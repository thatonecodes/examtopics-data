{"pageProps":{"questions":[{"id":"XgWKW9f3cHmPkxLLbVgT","choices":{"C":"Memory mismatch error","B":"Operating system memory limit","D":"Dynamic memory allocation","A":"Insufficient memory on the hypervisor"},"url":"https://www.examtopics.com/discussions/comptia/view/82459-exam-cv0-003-topic-1-question-174-discussion/","answers_community":["B (50%)","D (38%)","13%"],"isMC":true,"topic":"1","answer":"B","unix_timestamp":1663356960,"question_images":[],"question_id":81,"discussion":[{"timestamp":"1679002560.0","comment_id":"671094","upvote_count":"11","poster":"jiminycriminal","content":"Jesus these questions are so bad. You don't \"upgrade\" VM memory, you allocate more to them. Hypervisors don't have \"memory\", but Hyper-V will not even let you start a VM if there isn't enough host memory so the that conflicts with the suggested answer.\n\nBut the other answers make even less sense lol."},{"comment_id":"1364896","upvote_count":"1","content":"Selected Answer: B\nExplanation:\nMany operating systems, especially older versions or specific editions, have memory limits based on licensing or architecture.\nFor example:\n32-bit OS: Typically cannot use more than 4GB of RAM.\nCertain editions of Windows Server: Have licensing-based RAM limitations.\nIf the OS cannot recognize or utilize the additional memory, there will be no performance improvement.","timestamp":"1741091940.0","poster":"BigM"},{"comment_id":"1186756","timestamp":"1727697780.0","poster":"[Removed]","content":"This question... If the VM already has 8GB of RAM then it has to be a 64bit/x86 OS. If it is upgraded to 16GB then it still has to be an x86 OS.. How can it be an operating system memory limit? I know I referring to Windows here but D seems like a valid answer. I know you can argue in all kinds of different contextual points of view but this is mine.. First answer D, next answer would be B.","upvote_count":"1"},{"upvote_count":"4","timestamp":"1722780960.0","content":"Selected Answer: D\nOperating system memory limit seems silly. It's 2024 man.","comment_id":"1140316","poster":"FrancisDrake"},{"upvote_count":"2","timestamp":"1710218280.0","comment_id":"1005323","content":"Operating System Memory Limit (Option B): Many operating systems have memory limits or restrictions on how much memory an application or process can utilize. If the operating system has a limit set at 8GB, for example, even if you allocate 16GB to a VM, the operating system will only use up to its configured limit, and the additional memory won't provide any performance benefit.","poster":"Pongsathorn"},{"comment_id":"999961","content":"Selected Answer: D\nI think the answer is Dynamic Memory Allocation. Even though more RAM was installed, the VMs instance will have to be powered off. Memory allocated, then turned back on to realize any benefit. Currently, the extra memory is only available to the host.","timestamp":"1709683860.0","upvote_count":"2","poster":"dcdc1000"},{"upvote_count":"4","comment_id":"927201","timestamp":"1702973160.0","content":"Selected Answer: B\nWhen upgrading the memory of VMs, it is crucial to consider the limitations imposed by the operating system running within the VM. If the operating system has a memory limit or restriction that is lower than the upgraded memory size, it will not utilize the additional memory beyond its limit, resulting in no performance benefit for users.","poster":"Wanna_Pass_Forever"},{"upvote_count":"3","timestamp":"1702889100.0","poster":"ROCompTIA","comment_id":"926480","content":"Selected Answer: B\nIf there was insufficient memory on the hypervisor, the VMs would likely not be able to start or function properly after the upgrade.\nEach operating system has a maximum amount of RAM that it can handle, and if the VM's operating system has a limit of 8GB, then increasing the VM's memory to 16GB would not provide any additional benefit because the operating system wouldn't be able to utilize the extra memory."},{"content":"Selected Answer: A\nIts A. The Vms are on premises. Therefore its very likely you have not got enough RAM to double the memory of the VMs. Yes the wording isnt perfect but I feel its pretty obvious what they meant","upvote_count":"2","timestamp":"1700739780.0","poster":"PatrickH","comment_id":"904791"},{"comment_id":"815109","timestamp":"1692519480.0","poster":"Markedexam","content":"Seems the reference to \"recently upgraded the memory of its on-premises VMs from 8GB to 16GB\" is referring to the physical servers hosting the VM's & not the hypervisor/VM's settings, (since upgrading memory is only a physical attribute.) This denotes a physical upgrade (dedicated server - hosted in house & not cloud at all!) Take that premise as the foundation & then look at the Dynamic Memory Allocation - which could have remained unchanged since the \"upgrade\" - hence you need to up the allocation. Put simply, if you upgrade the physical memory - don't forget to increase dynamic memory resources to the VM's since you have more to distribute.","upvote_count":"2"},{"comment_id":"780306","upvote_count":"1","poster":"CapJackSparrow","timestamp":"1689697440.0","content":"Maybe B? Over allocation? Wish they would stop word playing.."},{"upvote_count":"2","content":"Maybe the VM doesn't need it and has dynamically assigned it back to the HV? \\","comment_id":"780303","poster":"CapJackSparrow","timestamp":"1689697320.0"},{"poster":"strale","upvote_count":"2","comments":[{"content":"no on B. limitations for ram on OS are based on 32 and 64bit architecture. they already had 8gb so they were already 64bit OS's.","poster":"solutionz","upvote_count":"3","comment_id":"744340","timestamp":"1686672300.0"}],"content":"Shouldn't it be B? Question does not specify which OS is on VM and OSs do have memory limitations https://www.compuram.de/blog/en/how-much-ram-can-be-addressed-under-the-current-32-bit-and-64-bit-operating-systems/\n\nQuestion states that users are not experiencing any performance benefit, which means that VMs are running, but are not faster, which for me, eliminates A.\n\nI am going for B","comment_id":"718163","timestamp":"1684081260.0"}],"timestamp":"2022-09-16 21:36:00","exam_id":349,"answer_images":[],"question_text":"An enterprise recently upgraded the memory of its on-premises VMs from 8GB to 16GB. However, users are not experiencing any performance benefit. Which of the following is the MOST likely reason?","answer_description":"","answer_ET":"B"},{"id":"gemoEQAyvYSLTvjabYpC","topic":"1","isMC":true,"discussion":[{"content":"No description of the error. Could be compute. Could be license.","timestamp":"1725292620.0","upvote_count":"3","comment_id":"1164287","poster":"FrancisDrake"},{"comment_id":"1015378","comments":[{"upvote_count":"1","poster":"Pongsathorn","content":"Issues related to storage (Option B) or licensing (Option C) would typically manifest differently and might not be directly related to a vertical-scaling operation. Scripts (Option D) could potentially cause issues, but they would usually generate specific error messages related to the script execution rather than a general error during vertical scaling.\n\nTo resolve this issue, the administrator should ensure that there are sufficient CPU resources available on the physical host, or consider adjusting the resource allocation and limitations for the VMs to match the available physical resources. Additionally, monitoring and performance analysis tools can help identify resource bottlenecks and guide resource allocation decisions.","comment_id":"1015379","timestamp":"1711245120.0"}],"content":"Selected Answer: A\nThe error occurring during a vertical-scaling test of the vCPU on the VMs suggests that the issue is likely related to compute resources. Vertical scaling, also known as scaling up, involves increasing the capacity of an individual VM, which primarily involves adding more CPU and memory resources to the VM.\n\n**Answer: A. Compute**\n\nIn a virtualized environment, compute resources such as CPU and memory are allocated to VMs. When you perform vertical scaling by adding more vCPUs to a VM, it requires that the underlying physical host has available CPU resources to allocate. If there are not enough available CPU resources on the host, you may encounter errors or limitations when trying to vertically scale the VMs.","upvote_count":"3","timestamp":"1711245120.0","poster":"Pongsathorn"},{"poster":"Wanna_Pass_Forever","content":"Selected Answer: A\nIf the administrator encounters an error during the vertical-scaling test, it suggests that there may be insufficient compute resources available in the three-server cluster to accommodate the increased vCPU allocation. This could be due to limitations in the underlying hardware, such as the number of physical CPU cores or the amount of available processing power.","timestamp":"1702973340.0","comment_id":"927208","upvote_count":"2"},{"comment_id":"926485","content":"Selected Answer: A\nVertical scaling, also known as scaling up, is the process of adding resources such as processing power (CPU) or memory to a server. If the administrator is getting an error while trying to scale the vCPU on the VMs, it's likely that there's a compute resource limitation","timestamp":"1702889280.0","poster":"ROCompTIA","upvote_count":"1"},{"upvote_count":"2","content":"gets an error ?? wow , I'm supposed to be a magician to know what is the error shit that caused by compute or license issue ...shitttttt on comptia","timestamp":"1701694920.0","comment_id":"914443","poster":"Maged_nader12"},{"content":"It could be a licensing issue if the Hypervisor is a per socket/core license. Vertically scaling the CPU may not work if there are no more valid CPU's left to use and the new ones can't be allocated.","timestamp":"1697393820.0","comment_id":"871204","upvote_count":"1","poster":"BeauChateau"},{"timestamp":"1697393520.0","comment_id":"871201","upvote_count":"1","content":"Selected Answer: A\nIf a systems administrator gets an error while executing a vertical-scaling test of the vCPU on VMs in a virtualized environment, the issue is MOST likely related to \"Compute\" (A).\n\nIn a virtualized environment, the \"Compute\" refers to the processing power (CPU and RAM) of the physical host machine that runs multiple virtual machines. Vertical scaling is the process of adding more resources to a virtual machine to increase its processing power. If the administrator is encountering an error while executing a vertical-scaling test of the vCPU on VMs, it indicates that there may be an issue with the compute resources available to the VMs, such as insufficient processing power or limited capacity on the physical host.","poster":"BeauChateau"},{"timestamp":"1696530240.0","poster":"Blatzzy","upvote_count":"1","comment_id":"862382","content":"A. Compute is the most likely issue occurring if the systems administrator is getting an error while executing a vertical-scaling test of the vCPU on the VMs in a three-server cluster with 12 VMs running on each server.\n\nVertical scaling refers to increasing the resources of a single virtual machine, such as increasing the number of vCPUs. This requires additional resources from the physical host, specifically compute resources. If the compute resources of the physical host are already being fully utilized by the existing VMs, then there may not be enough resources available to support the additional vCPUs required by the newly scaled VMs.\n\nStorage and licensing issues are less likely to cause errors while executing a vertical-scaling test of the vCPU on the VMs. Scripts can cause issues, but they are unlikely to be the primary cause of an error related to vertical scaling of vCPUs."},{"poster":"Daymeyon","comment_id":"793197","content":"Sounds like a per core licensing issue. \nEven if you added more vCPU's per VM than you had available, you'd just be over subscribed... and wouldn't get a error on start.","upvote_count":"1","timestamp":"1690735980.0"},{"content":"if the licensing is based on a per core situation and you add more vCPU's then you could run into licensing issues","timestamp":"1689088200.0","upvote_count":"1","comment_id":"772747","poster":"quick1unc"},{"comment_id":"672453","poster":"ryanzou","content":"Selected Answer: A\nA is correct.","upvote_count":"3","timestamp":"1679156400.0"},{"content":"What does licensing have to do with vertical scaling? I'd go with compute since that's what we're dealing with. Licensing is an issue with horizontal scaling.","upvote_count":"3","timestamp":"1679003040.0","comments":[{"timestamp":"1701433860.0","upvote_count":"1","content":"SO what about per core/socket license? adding more available cores will be considered as vertical","poster":"Maged_nader12","comment_id":"911968"}],"poster":"jiminycriminal","comment_id":"671098"}],"answer_description":"","question_id":82,"answer_images":[],"url":"https://www.examtopics.com/discussions/comptia/view/82461-exam-cv0-003-topic-1-question-175-discussion/","answers_community":["A (100%)"],"exam_id":349,"timestamp":"2022-09-16 21:44:00","question_text":"A systems administrator is deploying a new virtualized environment. The setup is a three-server cluster with 12 VMs running on each server. While executing a vertical-scaling test of the vCPU on the VMs, the administrator gets an error. Which of the following issues is MOST likely occurring?","question_images":[],"answer":"A","unix_timestamp":1663357440,"answer_ET":"A","choices":{"B":"Storage","A":"Compute","D":"Scripts","C":"Licensing"}},{"id":"2dkvlMrjQhsYwp2qNeTv","unix_timestamp":1666358220,"isMC":true,"answers_community":["C (53%)","A (47%)"],"exam_id":349,"question_id":83,"topic":"1","url":"https://www.examtopics.com/discussions/comptia/view/86137-exam-cv0-003-topic-1-question-176-discussion/","answer_description":"","answer_ET":"C","question_images":[],"choices":{"D":"Create /26 subnets in three regions and run 80 instances on each one.","A":"Create /25 subnets in two regions and run 80 instances on each one.","B":"Create /26 subnets in two regions and run 40 instances on each one.","C":"Create /26 subnets in three regions and run 40 instances on each one."},"answer":"C","timestamp":"2022-10-21 15:17:00","answer_images":[],"question_text":"A company would like to migrate its current on-premises workloads to the public cloud. The current platform requires at least 80 instances running at all times to work properly. The company wants the workload to be highly available, even if the cloud provider loses one region due to a catastrophe, and the costs to be kept to a minimum. Which of the following strategies should the company implement?","discussion":[{"comment_id":"1364521","content":"Selected Answer: C\nExplanation:\nThe company's requirements are:\n\nHigh availability – The workload must survive a regional outage.\nMinimum cost – Resources should be optimized to avoid unnecessary expenses.\nAt least 80 instances running at all times.","upvote_count":"1","poster":"BigM","timestamp":"1741025640.0"},{"poster":"BlueMan93","timestamp":"1733884500.0","upvote_count":"2","content":"Selected Answer: C\nVVV4WIN explains it best.","comment_id":"1228190"},{"poster":"Jhonattan0032","content":"Selected Answer: C\nC is the correct answer","comment_id":"1132267","timestamp":"1721963220.0","upvote_count":"1"},{"content":"Selected Answer: A\nI like A because of the simplicity. C would be my alternate. As to cost I'm not sure if it's cheaper to run the extra instances (A) or if it's cheaper to run in an extra region (C).","timestamp":"1720960200.0","upvote_count":"4","comment_id":"1122601","poster":"FrancisDrake"},{"content":"40 in 3 regions (40x3=120) vs 80 in 2 regions (80x2=160) -> means that it is cheaper to host the 40 in 3 regions, while also being able to lose an entire region and still have the required 80 instances.","timestamp":"1716365640.0","upvote_count":"2","comment_id":"1077204","poster":"VVV4WIN"},{"content":"Selected Answer: A\n\"at all times\" Implies that less than 80 will not suffice. Thus we need at least 2 sets of 80 for HA","timestamp":"1707946080.0","comment_id":"981045","poster":"No5172685","upvote_count":"2"},{"comment_id":"926489","timestamp":"1702889880.0","content":"Selected Answer: C\nThe question said : \n1.at least 80 instances running at all times to work\n2.wants the workload to be highly available, even if the cloud provider loses one region due to a catastrophe\n3.costs to be kept to a minimum\n\nThe choice of /26 subnets allows for up to 64 IP addresses, which is sufficient for running 40 instances in each region - Respect al 3 requiraments\n\nBy running 40 instances in each region, the company can achieve the desired level of availability while keeping costs lower - So, we have cost optimization !\n\nBy distributing the workload across three regions, the company ensures redundancy and fault tolerance. They want AT LEAST 80 instances.","poster":"ROCompTIA","upvote_count":"3"},{"timestamp":"1701623820.0","comment_id":"913726","content":"Selected Answer: A\nFirst, the question states that in order to operate correctly, the platform needs 80 instances at ALL times in order to operate correctly. This rules out \"B\" (if one of your 2 regions goes down, you're left with only 40). You could have 40 instances in 3 regions and if 1 goes down you still have your 80, but it states to keep costs down so you're not going to have 3 regions. This rules out \"C\" and \"D\". In addition, the /26 unequivocally rules out \"D\". Answer \"A\" fits the bill perfectly with regard to the subnet requirement, the low cost requirement and the 80 instances requirement should one region go down.","upvote_count":"3","poster":"SecPlus2022"},{"comment_id":"904793","timestamp":"1700740320.0","content":"I think A is correct. Its not B or D as the maths just dont work. The question specifically states they want redundancy if one region goes down, so A covers that. Creating 3 regions cost MORE than 2 regions so thats my logic for choosing A.","upvote_count":"1","poster":"PatrickH"},{"timestamp":"1697393880.0","poster":"BeauChateau","comment_id":"871206","upvote_count":"3","content":"Selected Answer: C\nTo migrate the current on-premises workloads to the public cloud, with high availability and cost minimization, the company should implement the strategy of \"Create /26 subnets in three regions and run 40 instances on each one\" (C).\n\nRunning 80 instances on each region would be more expensive and less efficient in terms of resource utilization. Creating two regions with /25 subnets or /26 subnets with 40 instances each would not provide sufficient redundancy in case of a catastrophe in one region.\n\nBy creating /26 subnets in three regions and running 40 instances on each one, the company would have a total of 120 instances running, which satisfies the requirement of at least 80 instances running at all times. This strategy also provides high availability since the workload can be replicated in multiple regions, ensuring that it continues to function even if one region is lost due to a catastrophe."},{"content":"You cant run 80 on a /26","poster":"DocHacker","comment_id":"738485","upvote_count":"4","timestamp":"1686177120.0"},{"timestamp":"1682083020.0","poster":"tonytonyyyyy","comment_id":"700952","upvote_count":"3","content":"I think the answer is C. You'd be running a total of 120 instances vs 160 instances and you'd have high availability because if one subnet goes down you still have 80 instances."}]},{"id":"3nJhoRmBrm0zaWhkcvfu","timestamp":"2023-01-21 21:55:00","question_id":84,"exam_id":349,"answer_description":"","answer_images":[],"isMC":true,"question_images":[],"choices":{"C":"Changing the automation tool because it is incompatible","A":"Reducing the maximum threshold in the auto-scaling configuration","B":"Debugging the script and redeploying it","D":"Modifying the script to shut down the VM after five minutes"},"topic":"1","unix_timestamp":1674334500,"question_text":"A web application has been configured to use auto-scaling for provisioning and deprovisioning more VMs according to the workload. The systems administrator deployed a new Cl/CD tool to automate new releases of the web application. During the night, a script was deployed and configured to be executed by the VMs during bootstrapping. Now, the auto-scaling configuration is creating a new VM every five minutes. Which of the following actions will MOST likely resolve the issue?","url":"https://www.examtopics.com/discussions/comptia/view/96395-exam-cv0-003-topic-1-question-177-discussion/","answer_ET":"B","answers_community":["B (69%)","A (31%)"],"answer":"B","discussion":[{"poster":"Daymeyon","comment_id":"793237","content":"A. was my first thought. But that seems more like a temp fix/workaround than a resolution. Something is clearly wrong with the script to be making new VM's every 5 minutes and that needs to be addressed for the issue to be resolved. Fix the script and redeploy.","upvote_count":"7","timestamp":"1690738440.0"},{"upvote_count":"2","comment_id":"1132269","timestamp":"1721963460.0","poster":"Jhonattan0032","content":"Selected Answer: B\nThe problem is most likely related to the script that was deployed during bootstrapping"},{"comment_id":"1005324","poster":"Pongsathorn","timestamp":"1710218940.0","upvote_count":"3","content":"Selected Answer: B\nDebugging the Script (Option B): The problem is most likely related to the script that was deployed during bootstrapping. Debugging the script to identify and fix any issues causing the repeated VM creation is the first step to resolving the problem. Once the script is debugged and corrected, you can redeploy it to the VMs.\n\nReducing the Maximum Threshold (Option A): Reducing the maximum threshold in the auto-scaling configuration would limit the number of VMs that can be created simultaneously, but it doesn't address the root cause of the issue, which is the faulty script. It's better to fix the script itself."},{"content":"Selected Answer: B\nBy debugging the script, you can identify any errors or issues within the script that may be causing the auto-scaling process to trigger repeatedly. Once the issues are identified and resolved, you can redeploy the corrected script to the VMs during bootstrapping.","comment_id":"926497","upvote_count":"2","timestamp":"1702890720.0","poster":"ROCompTIA"},{"poster":"SecPlus2022","upvote_count":"2","content":"Selected Answer: B\n\"B\" will resolve it as the question requires, \"A\" is only a containment method.","comment_id":"913736","timestamp":"1701624420.0"},{"content":"Selected Answer: A\nA. Reducing the maximum threshold in the auto-scaling configuration would most likely resolve the issue. This would prevent the auto-scaling configuration from continuously creating new VMs every five minutes. It would also be a quick and easy solution to implement. The other options may also help to resolve the issue, but they would likely take more time and effort to implement.","upvote_count":"4","poster":"TheGinjaNinja","comments":[{"comment_id":"926495","upvote_count":"1","timestamp":"1702890540.0","poster":"ROCompTIA","content":"And the problem will not be solved doing so, because it will make fewer virtual machines."}],"comment_id":"783729","timestamp":"1689965700.0"}]},{"id":"0dVsaW8Ks0ASgCN7H6MH","answer_ET":"E","answer":"E","topic":"1","question_text":"A systems administrator is troubleshooting an application that is configured to auto-scale with a minimum of two nodes and a maximum of four. The application will scale out if the CPU utilization of one of the nodes exceeds 80% for more than five minutes and will scale in if the CPU utilization of one of the nodes drops under\n20% for more than ten minutes. There is a reverse proxy in front of the application. The systems administrator notices two of the nodes are often running over 80% for a long period of time, which is triggering the creation of the other two nodes; however, they are being created and terminated while the load in the first two remains over 50% all the time. Which of the following should the administrator configure to fix this issue?","question_images":[],"choices":{"A":"Disable DNS caching in the reverse proxy.","B":"Reduce the minimum node count to one.","D":"Reduce the scale-out rule to 50%.","C":"Disable TLS tickets.","E":"Increase the scale-in rule to 50%."},"answer_images":[],"discussion":[{"poster":"BigM","comment_id":"1361936","content":"Selected Answer: E\nE\n\nThe issue described suggests that the auto-scaling adds nodes when CPU usage is high but removes them too aggressively, leading to constant scaling activity without stabilizing the load.\n\nCurrent Scale-In Rule is Too Aggressive:\n\nThe system scales in (removes nodes) when CPU drops below 20% for more than 10 minutes.\nThis is too low of a threshold, meaning nodes get terminated too soon, which then forces the remaining nodes to handle more load again.\nWhy Raising the Scale-In Threshold to 50% Helps:\n\nBy increasing the scale-in threshold to 50%, extra nodes stay active longer and prevent premature termination.\nThis allows the system to stabilize under a balanced load rather than oscillating between scaling up and down.","upvote_count":"1","timestamp":"1740576720.0"},{"content":"The problem being described is that the application is scaling out at 50% instead of the intended 80%. \n\nZero of the solutions fixes this issue. Which is what you get when CompTIA outsources their questions and testing to India and literally no one reads what they come up with and put into the exams.","poster":"Pisces225","comment_id":"1189224","upvote_count":"4","timestamp":"1712225280.0"},{"poster":"FasterN8","comments":[{"timestamp":"1720800300.0","comment_id":"1246886","content":"It is sending traffic to the new 2 machines. \"first two remains over 50% all the time\". None of the solution truly fix the issue but I'm going with E","poster":"lamborghini","upvote_count":"1"}],"timestamp":"1708916460.0","upvote_count":"3","comment_id":"1159311","content":"Selected Answer: A\nAt first there seemed to be no right answer. But the given answer I think is right. If the reverse proxy isn't sending any traffic to the 2 new machines, they themselves trigger the 20% rule and trigger the scale-in action. Disabling DNS caching will force the Rev.proxy to constantly look for new VMs in the pool and finally send traffic their way."},{"upvote_count":"2","content":"Selected Answer: E\nI think is e","comment_id":"1135298","poster":"Jhonattan0032","timestamp":"1706560500.0"},{"upvote_count":"2","comment_id":"1132272","content":"Selected Answer: E\nincreasing the scale-in rule to 50%, nodes will only be terminated when their CPU utilization drops below 50%","timestamp":"1706246280.0","poster":"Jhonattan0032"},{"upvote_count":"4","timestamp":"1705244220.0","content":"None of these answers seem to address the problem.","poster":"FrancisDrake","comment_id":"1122615"},{"poster":"ROCompTIA","content":"Selected Answer: D\nBy reducing the scale-out rule to 50%, the auto-scaling system will only trigger the creation of new nodes when the CPU utilization exceeds 50%, instead of the current threshold of 80%. This adjustment will prevent unnecessary creation of new nodes when the load is still relatively high but below 80%","upvote_count":"1","timestamp":"1687072560.0","comments":[{"comment_id":"931154","poster":"slcc99","upvote_count":"2","timestamp":"1687483500.0","content":"Since it says \"The systems administrator notices two of the nodes are often running over 80% for a long period of time\", wouldn't reducing the scale-out rule to 50% solve the problem?"}],"comment_id":"926501"}],"answer_description":"","unix_timestamp":1687072560,"url":"https://www.examtopics.com/discussions/comptia/view/112502-exam-cv0-003-topic-1-question-178-discussion/","answers_community":["E (56%)","A (33%)","11%"],"question_id":85,"isMC":true,"exam_id":349,"timestamp":"2023-06-18 09:16:00"}],"exam":{"numberOfQuestions":375,"lastUpdated":"12 Apr 2025","name":"CV0-003","provider":"Comptia","isBeta":false,"isMCOnly":false,"isImplemented":true,"id":349},"currentPage":17},"__N_SSP":true}