{"pageProps":{"questions":[{"id":"UNMmLVEdi5doqkl0G9nf","unix_timestamp":1673691060,"isMC":true,"question_images":[],"timestamp":"2023-01-14 11:11:00","answer_description":"","discussion":[{"timestamp":"1681627140.0","comment_id":"871518","content":"Selected Answer: C\nOption A, B and D have some similarities with Option C but also have some key differences:\n\nOption A uses a Network Load Balancer (NLB) instead of an Application Load Balancer (ALB) and does not use AWS Database Migration Service (AWS DMS) for continuous data replication. Instead, it sets up the Aurora MySQL database as a replication target for the on-premises database.\nOption B does use AWS DMS for continuous data replication and sets up collection endpoints behind an ALB as Amazon EC2 instances in an Auto Scaling group. However, it does not create an Aurora Replica for the Aurora MySQL database or use Amazon RDS Proxy to write to the Aurora MySQL database.\nOption D does not use AWS DMS for continuous data replication or set up collection endpoints behind an ALB. Instead, it sets up collection endpoints as an Amazon Kinesis data stream and uses Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database.","upvote_count":"19","poster":"OCHT"},{"comment_id":"1275499","content":"C. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS.","poster":"amministrazione","upvote_count":"1","timestamp":"1725094620.0"},{"upvote_count":"3","timestamp":"1703949960.0","comment_id":"1109808","content":"Selected Answer: C\nNot A. not clear how the on-premises database is replicated on the Aurora MySQL, also you cannot place Lambda behind NLB as BLB only supports private IPs, instances and ALB https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html\nNot B. this will keep executing the aggregation job and the load on the same database instance and this will not resolve loading issues\nNot D. using Kinesis Data Firehose to replicate the database is not recommended, the solution should involve DMS. also moving to Kinesis Data Stream for data load requires some changes on the customer side which is not part of the request.\n\nC is the right solution: use DMS to migrate on-premise database, move the aggregation job to the read replica, using Lambda (that supports node.js) behind ALB will not impact client side","poster":"ninomfr64"},{"content":"Selected Answer: C\nAnswer C","timestamp":"1701713880.0","upvote_count":"1","comment_id":"1087834","poster":"shaaam80"},{"comment_id":"940974","content":"Selected Answer: C\nIt's a c","poster":"NikkyDicky","timestamp":"1688308920.0","upvote_count":"1"},{"comment_id":"926740","timestamp":"1687098360.0","upvote_count":"2","content":"Selected Answer: C\nKeyworks = DMS & RDS Proxy\nThen C","poster":"SkyZeroZx"},{"poster":"leehjworking","comments":[{"comment_id":"911255","content":"why ...oh...why?","timestamp":"1685536740.0","poster":"chikorita","upvote_count":"1"}],"timestamp":"1683182160.0","comment_id":"889217","content":"Selected Answer: C\nAD: restart = interruption?\nB: ASG...Why?","upvote_count":"3"},{"poster":"mfsec","comment_id":"853152","content":"Selected Answer: C\nill go with C","timestamp":"1680001920.0","upvote_count":"1"},{"poster":"dev112233xx","comment_id":"841345","content":"Selected Answer: C\nC.. even though question didn’t mention the total time of each job. If the job takes more than 15m then Lambda can’t be used. Probably the solution with ASG and EC2 is better .. not sure!","upvote_count":"3","timestamp":"1678999620.0"},{"content":"Selected Answer: C\nALB because you are pointing to to Lambda function, not a network address\n\nLook at AWS DMS feature https://aws.amazon.com/dms/features/ \n\nMain requirement - needs the migration to occur w/out interruptions or changes to the company's customers.\n\nC keeps it stupid simple w/ no service interruption","comment_id":"838541","poster":"zejou1","timestamp":"1678766520.0","upvote_count":"1"},{"comment_id":"833666","comments":[{"timestamp":"1678766640.0","comment_id":"838542","content":"Application - you are using Lambda functions that will be sending api commands, you would use network when it is just about routing","upvote_count":"1","poster":"zejou1"}],"content":"Could anybody explain why ALB? I'd go with API Gateway","timestamp":"1678345020.0","poster":"vherman","upvote_count":"1"},{"upvote_count":"1","poster":"Sarutobi","timestamp":"1677532260.0","content":"Selected Answer: C\nI would say C.","comment_id":"824162"},{"comment_id":"822413","content":"I have a feeling that none of the approaches will work.\na) We have two sources that change the database: migration and new data coming in. In a relational database this results in inconsistent data. Constraints will not be fulfilled. \nb) until the database is fully synced the second database has inconsistent data. Some parts of relations and parts of entities are still missing. Constraints will not be fulfilled. \nNone if the approaches addresses that aggregation tasks fail because of inconsistency of the data base.","poster":"hobokabobo","comments":[{"comment_id":"822475","poster":"hobokabobo","timestamp":"1677419400.0","upvote_count":"1","content":"ACID principle: atomicity, consistency, isolation and durability. All solutions violate this basic principle of relational databases.\nhttps://en.wikipedia.org/wiki/ACID"}],"timestamp":"1677418200.0","upvote_count":"1"},{"timestamp":"1677392640.0","poster":"God_Is_Love","upvote_count":"2","content":"Issue could be because of same db used for writing and reading heavily. solution to separate this into\nread replica only for reading. DMS for data migration to aws from onpremises.Writing app to DB and Reading app from DB for reports. Writing app needs RDSProxy and saves data.Reading app reads from replica.\nB is wrong because, Reading job (aggregation) needs to use replica which is mentioned in C. C is correct.","comment_id":"822103"},{"content":"is it C or B?\nSame person answers two times two different answers","timestamp":"1676689200.0","poster":"Fatoch","comment_id":"812586","upvote_count":"1"},{"timestamp":"1675104060.0","poster":"zozza2023","comment_id":"793190","content":"Selected Answer: C\nC is corect","upvote_count":"3"},{"content":"Selected Answer: C\nC. \nThis option would meet the requirements of resolving the data loading issue and migrating without interruption or changes for the company's customers. By using AWS DMS for continuous data replication, the company can ensure that the data being migrated is up to date. By setting up an Aurora Replica and moving the aggregation jobs to run against it, the company can offload some of the read workload from the primary database and reduce the risk of issues with the load jobs. By using AWS Lambda functions behind an ALB and Amazon RDS Proxy to write to the Aurora MySQL database, the company can add an extra layer of security and scalability to the data collection process. Finally, by pointing the collector DNS record to the ALB after the databases are synced and disabling the AWS DMS sync task, the company can ensure a smooth cutover to the new environment.","comment_id":"779255","comments":[{"content":"A. \nThis option would not work as it would require to change the primary database and also it may cause interruption for the company's customers during the cutover process.\n\nB.\nThis option would not work as it would not include Aurora Replica to offload the read workload, this would result in aggregation jobs running on the primary database which can cause the load jobs to fail during heavy loads.\n\nD.\nThis option would not work as it would require to use kinesis data stream which may cause performance issues and also it may not be the best fit for this use case. Additionally, using Kinesis Data Firehose would add complexity to the data replication process, and may result in increased latency or data loss.","timestamp":"1673983320.0","upvote_count":"2","comment_id":"779256","poster":"masetromain"}],"poster":"masetromain","timestamp":"1673983320.0","upvote_count":"4"},{"comment_id":"776683","poster":"zhangyu20000","content":"C is correct. need more read replica for aggregation jobs to read data","timestamp":"1673794080.0","upvote_count":"3"},{"poster":"masetromain","comment_id":"775238","timestamp":"1673691060.0","upvote_count":"2","content":"Selected Answer: B\nThe correct answer is B. Setting up an Amazon Aurora MySQL database and using AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora will ensure that data is continuously replicated to the new environment with minimal interruption. Moving the aggregation jobs to run against the Aurora MySQL database will ensure that the data is being read from the same database that is being loaded, which will resolve the data loading issue. Setting up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group, and disabling the AWS DMS sync task after the cutover from on-premises to AWS, will ensure that the migration occurs without interruptions or changes for the company's customers.","comments":[{"comment_id":"775239","poster":"masetromain","upvote_count":"1","comments":[{"timestamp":"1674681060.0","comment_id":"788137","upvote_count":"13","poster":"andctygr","comments":[{"content":"hhhhhhhhhh.","comment_id":"794060","upvote_count":"2","timestamp":"1675163940.0","poster":"jojom19980"},{"comment_id":"898635","upvote_count":"2","poster":"Jesuisleon","timestamp":"1684176480.0","content":"Before I read your comments, I thought I was the only one so sick of it :)"}],"content":"Dude can u pls stop copy-pasting from chatgpt I am so sick of it. It is not a reliable source. Just stop it for the god sake."}],"content":"Answer A is incorrect because it's not necessary to set up an Aurora Replica for the Aurora MySQL database, doing this will introduce additional complexity and cost. Using Amazon RDS Proxy is not necessary for this scenario, and disabling the replication job and restarting the Aurora Replica as the primary instance will cause an interruption to the service.\n\nAnswer C is incorrect because it's not necessary to set up an Aurora Replica for the Aurora MySQL database, doing this will introduce additional complexity and cost. Using Amazon RDS Proxy is not necessary for this scenario.\n\nAnswer D is incorrect because it's not necessary to use Amazon Kinesis data stream and Firehose to replicate the data when AWS DMS can be used to perform continuous data replication. Also, disabling the replication job and restarting the Aurora Replica as the primary instance will cause an interruption to the service.","timestamp":"1673691060.0"}]}],"answer_images":[],"answer":"C","question_text":"A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js applications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into reports. When the aggregation jobs run, some of the load jobs fail to run correctly.\n\nThe company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the company’s customers.\n\nWhat should a solutions architect do to meet these requirements?","question_id":446,"url":"https://www.examtopics.com/discussions/amazon/view/95170-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["C (95%)","5%"],"topic":"1","choices":{"D":"Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream.","B":"Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALDisable the AWS DMS sync task after the cutover from on premises to AWS.","A":"Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB.","C":"Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS."},"answer_ET":"C","exam_id":33},{"id":"Hj5aX8WGnT3aVSeVBL4c","isMC":true,"exam_id":33,"question_images":[],"answers_community":["D (75%)","C (25%)"],"question_text":"A company runs an ecommerce web application on AWS. The web application is hosted as a static website on Amazon S3 with Amazon CloudFront for content delivery. An Amazon API\nGateway API invokes AWS Lambda functions to handle user requests and order processing for the web application The Lambda functions store data in an Amazon ROS for MySQL DB cluster that uses On-Demand instances. The DB cluster usage has been consistent in the past 12 months.\n\nRecently, the website has experienced SQL injection and web exploit attempts. Customers also report that order processing time has increased during periods of peak usage. During these periods, the Lambda functions often have cold starts. As the company grows, the company needs to ensure scalability and low-latency access during traffic peaks. The company also must optimize the database costs and add protection against the SQL injection and web exploit attempts.\n\nWhich solution will meet these requirements?","question_id":447,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/142934-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","answer_ET":"D","answer":"D","choices":{"C":"Use Lambda functions with provisioned concurrency for compute during peak periods, Transition to Amazon Aurora Serverless for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts.","A":"Configure the Lambda functions to have an increased timeout value during peak periods. Use RDS Reserved Instances for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts.","D":"Use Lambda functions with provisioned concurrency for compute during peak periods. Use RDS Reserved Instances for the database. Integrate AWS WAF with CloudFront to protect against the SQL injection and web exploit attempts.","B":"Increase the memory of the Lambda functions, Transition to Amazon Redshift for the database. Integrate Amazon Inspector with CloudFront to protect against the SQL injection and web exploit attempts."},"timestamp":"2024-06-25 20:35:00","unix_timestamp":1719340500,"answer_images":[],"discussion":[{"poster":"ebbff63","comment_id":"1237973","content":"Selected Answer: D\nD - AWS WAF for SQL injection and web exploit protection","upvote_count":"9","timestamp":"1719475140.0"},{"timestamp":"1733222580.0","poster":"nimbus_00","comment_id":"1321335","upvote_count":"1","content":"Selected Answer: D\n\"DB cluster that uses On-Demand instances. The DB cluster usage has been consistent in the past 12 months.\" suggests RDS Reserved Instances for the database.\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithReservedDBInstances.html"},{"content":"Selected Answer: D\nBy leveraging Lambda functions with provisioned concurrency, RDS Reserved Instances, and AWS WAF with CloudFront, Option D provides a comprehensive solution addressing low-latency access during traffic peaks, optimizing database costs, and adding protection against SQL injection and web exploit attempts, meeting all stated requirements.\n\nOption C: While Aurora Serverless addresses database scalability and cost, AWS Shield Advanced may be unnecessary if SQL injection and web exploits are the primary concern, which AWS WAF can mitigate.","upvote_count":"1","timestamp":"1731807960.0","comment_id":"1313334","poster":"0b43291"},{"comment_id":"1296684","upvote_count":"2","timestamp":"1728779460.0","content":"Selected Answer: D\nIt's D: Provisioned Concurrency ensures that Lambda functions are pre-warmed and ready to handle requests instantly, which reduces the \"cold start\" problem. RDS Reserved Instances for Amazon RDS will help reduce the database cost. Since the workload has been consistent over the past 12 months, Reserved Instances provide a cost-effective solution by offering significant discounts compared to On-Demand pricing. AWS WAF protects the application from web exploits such as SQL injection and cross-site scripting (XSS).","poster":"JoeTromundo"},{"content":"Selected Answer: D\nwaf for sql injection and web exploits","comment_id":"1289877","timestamp":"1727425080.0","upvote_count":"2","poster":"wbedair"},{"comment_id":"1286155","comments":[{"comment_id":"1286725","timestamp":"1726821120.0","upvote_count":"1","content":"It looksxlike I was looking at different question as no mention in the question to use serverless database. I will go with D","poster":"wbedair"}],"upvote_count":"1","timestamp":"1726723320.0","content":"Selected Answer: C\nexpanding business needs serverless database so Aurora in option C is the best","poster":"wbedair"},{"comment_id":"1271037","content":"using shield advanced will enable the basic features of WAF for free as well, so C","upvote_count":"1","timestamp":"1724384400.0","comments":[{"poster":"Daniel76","upvote_count":"1","content":"Shield Advanced is not free, and it's used against DDoS, not SQL injection.","comment_id":"1284054","timestamp":"1726396200.0"}],"poster":"Isaac_lin"},{"upvote_count":"3","comment_id":"1262905","content":"Selected Answer: C\nRegardless of the diabolical wording of the question. Forget about whether it's WAF or Shield Advance, it's 'C' because it drills down to saying \"the company is now expecting growth and needs to ensure scalability\", this pushes us to Aurora Serverless. DB usage was consistent last year, it no longer is.","poster":"asquared16","timestamp":"1723202220.0"},{"comment_id":"1243378","poster":"vip2","upvote_count":"3","timestamp":"1720272060.0","content":"Selected Answer: D\nAWS WAF instead of AWS Shield"},{"content":"D, for sure.\nTo protect against SQL injection attacks, AWS WAF (Web Application Firewall) is the appropriate service to use, not AWS Shield Advanced.","comment_id":"1242870","upvote_count":"2","timestamp":"1720194360.0","poster":"gfhbox0083"},{"content":"Selected Answer: C\nLambda functions with provisioned concurrency for compute during peak periods + Aurora Serverless + AWS Shield Advanced, I don't see any better choice. Answer C.","comment_id":"1238374","upvote_count":"2","timestamp":"1719517500.0","poster":"mifune"},{"content":"C - using Lambda concuraancy with Aurora Serverless solves a bunch of the issues","comment_id":"1237051","timestamp":"1719340500.0","poster":"zapper1234","comments":[{"upvote_count":"2","timestamp":"1719667740.0","content":"it is D, no need for AWS Shield Advanced, WAF is sufficient.","poster":"toma","comments":[{"content":"it is D, AWS Shield Advanced is not required; AWS WAF can be used to protect against common web exploits such as SQL injection and cross-site scripting (XSS) attacks.","timestamp":"1719734400.0","poster":"kupo777","comment_id":"1239568","upvote_count":"2"}],"comment_id":"1239291"}],"upvote_count":"1"}]},{"id":"g5r4lqRuRwPnx41I2BCI","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/142997-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1719518460,"answers_community":["D (70%)","B (20%)","10%"],"answer_description":"","isMC":true,"answer_ET":"D","answer_images":[],"topic":"1","discussion":[{"upvote_count":"1","timestamp":"1737625680.0","content":"Selected Answer: C\nAn instance maintenance policy in the context of AWS Auto Scaling governs how instances are handled before they are fully launched and available for use. It defines the actions that need to occur (such as running a user data script, applying patches, or installing software) to ensure the instance is ready. When combined with features like warm pools, maintenance policies can ensure instances are prepared in advance and reduce delays during scaling events. These policies help ensure instances are fully initialized before serving traffic.","comments":[{"comment_id":"1360076","upvote_count":"1","timestamp":"1740215460.0","content":"Nope. An instance maintenance policy affects Amazon EC2 Auto Scaling events that cause instances to be replaced.","poster":"altonh"}],"poster":"zhen234","comment_id":"1345281"},{"timestamp":"1734209160.0","upvote_count":"1","content":"Selected Answer: B\n@songilly provided an exhaustive comment explaining why B is the only viable answer","poster":"henrikhmkhitaryan59","comment_id":"1326607"},{"poster":"alexbraila","content":"Selected Answer: B\nDue to the link in songilly's comment, which clearly states D is out. I am almost sure they were looking for knowledge of \"warm pools\", but here is another poorly written AWS question","timestamp":"1733686680.0","upvote_count":"1","comment_id":"1323733"},{"content":"Selected Answer: D\nNo mention of any peak period so there’s no way to use predictive scaling\nThe problem occurs cause the VMs take too long to boot up and be ready to accept requests, the only thing to do is to have them already “warm”.\nAnd I’ve never heard of a “maintenance mode” and I know that lifecycle hooks are a common practice with ASGs\n\n\nWarm pools\nLifecycle hooks are how","poster":"horiuchi","upvote_count":"1","timestamp":"1733575140.0","comment_id":"1323100"},{"poster":"songilly","comment_id":"1311643","timestamp":"1731534720.0","content":"You can't use a warm pool in an Auto Scaling group with mixed instances policy or has spot instances:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html\n\nSo not sure how D can be write. Although B doesn't seem great it might be the only viable option.","upvote_count":"2"},{"timestamp":"1726448760.0","poster":"Daniel76","comment_id":"1284408","upvote_count":"3","content":"Selected Answer: D\nAgree with D\nThere is no mention of predictable peak period. Since there's a known metric where user experience skpwness, dynamic scaling should be used. \nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html#:~:text=A%20dynamic%20scaling%20policy%20instructs,CloudWatch%20alarm%20is%20in%20ALARM.\n\nUse warm pool to reduce latency and cost of unnecessary standby instance. \nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html\n\nUse lifecycle hook due to the need to install custom packages\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\n\nMore reference:\nhttps://aws.amazon.com/blogs/compute/introducing-instance-maintenance-policy-for-amazon-ec2-auto-scaling/","comments":[{"content":"But why do you need a lifecycle hook just to run a script? It is already a user data script, so there is no need for this.","poster":"altonh","comment_id":"1360084","upvote_count":"1","timestamp":"1740216420.0"}]},{"comment_id":"1243376","poster":"vip2","content":"Selected Answer: D\nD is correcyt","timestamp":"1720271820.0","upvote_count":"1"},{"timestamp":"1720044780.0","content":"Selected Answer: D\nAnswer : D","poster":"Alagong","upvote_count":"1","comment_id":"1241685"},{"timestamp":"1720027080.0","poster":"AhmedSalem","upvote_count":"1","content":"Selected Answer: D\nAnswer D","comment_id":"1241554"},{"upvote_count":"1","poster":"kupo777","comments":[{"poster":"wbedair","upvote_count":"3","content":"looks like this is an answer for different question","comment_id":"1239863","timestamp":"1719784620.0"}],"comment_id":"1239563","content":"B\nAWS Database Migration Service can convert Oracle and SQL Server to Amazon RDS for MySQL and Amazon RDS for PostgreSQL stored procedures.\n\nD\nAWS Database Migration Service (AWS DMS) performs data migration.\n\nThe answer is DB.","timestamp":"1719733740.0"}],"question_text":"A company runs a web application on a single Amazon EC2 instance. End users experience slow application performance during times of peak usage, when CPU utilization is consistently more than 95%.\n\nA user data script installs required custom packages on the EC2 instance. The process of launching the instance takes several minutes.\n\nThe company is creating an Auto Scaling group that has mixed instance groups, varied CPUs, and a maximum capacity limit. The Auto Scaling group will use a launch template for various configuration options. The company needs to decrease application latency when new instances are launched during auto scaling.\n\nWhich solution will meet these requirements?","choices":{"C":"Use a predictive scaling policy. Enable warm pools for the Auto Scaling group. Use an instance maintenance policy to run the user data script.","D":"Use a dynamic scaling policy. Enable warm pools for the Auto Scaling group. Use lifecycle hooks to run the user data script.","B":"Use a dynamic scaling policy. Use lifecycle hooks to run the user data script. Set the default instance warmup time to 0 seconds.","A":"Use a predictive scaling policy. Use an instance maintenance policy to run the user data script. Set the default instance warmup time to 0 seconds."},"question_id":448,"timestamp":"2024-06-27 22:01:00","question_images":[],"exam_id":33},{"id":"jqFOR2TpR03GzJVBZ8ym","topic":"1","answer_description":"","answer":"CD","exam_id":33,"question_images":[],"discussion":[{"poster":"ebbff63","comment_id":"1237980","content":"Selected Answer: CD\nAnswer CD","timestamp":"1719476220.0","upvote_count":"7"},{"poster":"AzureDP900","content":"CD \nUsing both AWS SCT and AWS DMS will provide the necessary tools for a successful database migration. AWS SCT helps with identifying the changes needed during migration, while AWS DMS provides a managed service for executing the migration.","comment_id":"1313343","upvote_count":"1","timestamp":"1731808860.0"},{"timestamp":"1726455060.0","upvote_count":"1","comment_id":"1284433","poster":"liuliangzhou","content":"Selected Answer: CD\nA. AWS does not have a service directly named 'Migration Evaluator Quick Insights'.\nB. AWS Application Migration Service is primarily used for migrating virtual machines rather than analyzing them.\nC. AWS SCT can analyze the source database and identify potential architecture changes needed for successful migration to Amazon RDS.\nD. AWS DMS is a data migration service.\nE. AWS DataSync is a service used for quickly migrating large amounts of data between local storage and AWS storage services, with a focus on file level data migration. But database migration requires maintaining data integrity, relationships, constraints, and so on."},{"timestamp":"1721631540.0","poster":"Moghite","content":"Selected Answer: CD\nanswer CD","comment_id":"1252887","upvote_count":"1"},{"poster":"AhmedSalem","comment_id":"1241555","timestamp":"1720027140.0","content":"Selected Answer: CD\nAnswer CD","upvote_count":"1"},{"timestamp":"1719733980.0","upvote_count":"1","comment_id":"1239567","content":"B\nAWS Database Migration Service can convert Oracle and SQL Server to Amazon RDS for MySQL and Amazon RDS for PostgreSQL stored procedures.\n\nD\nAWS Database Migration Service (AWS DMS) performs data migration.\n\nThe answer is DB.","poster":"kupo777"},{"upvote_count":"4","timestamp":"1719597900.0","poster":"awsaz","content":"Selected Answer: CD\nC and D","comment_id":"1238825"}],"question_id":449,"unix_timestamp":1719476220,"question_text":"A company needs to migrate its on-premises database fleet to Amazon RDS. The company is currently using a mixture of Microsoft SQL Server, MySQL, and Oracle databases. Some of the databases have custom schemas and stored procedures.\n\nWhich combination of steps should the company take for the migration? (Choose two.)","answer_images":[],"answer_ET":"CD","choices":{"B":"Use AWS Application Migration Service to analyze the source databases and to identify the stored procedures that need to be migrated.","E":"Use AWS DataSync to migrate the data from the source databases to Amazon RDS.","A":"Use Migration Evaluator Quick Insights to analyze the source databases and to identify the stored procedures that need to be migrated.","D":"Use AWS Database Migration Service (AWS DMS) to migrate the source databases to Amazon RDS.","C":"Use the AWS Schema Conversion Tool (AWS SCT) to analyze the source databases for changes that are required"},"answers_community":["CD (100%)"],"timestamp":"2024-06-27 10:17:00","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/142971-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"XmcDhSSxSDYQbjUDauTY","question_text":"A company is migrating its blog platform to AWS. The company's on-premises servers connect to AWS through an AWS Site-to-Site VPN connection. The blog content is updated several times a day by multiple authors and is served from a file share on a network-attached storage (NAS) server.\n\nThe company needs to migrate the blog platform without delaying the content updates. The company has deployed Amazon EC2 instances across multiple Availability Zones to run the blog platform behind an Application Load Balancer. The company also needs to move 200 TB of archival data from its on-premises servers to Amazon S3 as soon as possible.\n\nWhich combination of stops will meet these requirements? (Choose two.)","discussion":[{"poster":"altonh","content":"Selected Answer: CD\nSnowcone is discontinued.","upvote_count":"1","comment_id":"1360098","timestamp":"1740220320.0"},{"upvote_count":"1","content":"C & D\n\nhttps://aws.amazon.com/about-aws/whats-new/2023/05/aws-snow-family-multi-pb-data-migration-210tb-device/","timestamp":"1721239560.0","comment_id":"1249876","poster":"ITguy_10"},{"poster":"AhmedSalem","timestamp":"1720027320.0","content":"Selected Answer: CD\nAnswer is CD \nC. EFS provides a scalable, shared file storage that can be accessed by both on-premises servers and EC2 instances. This ensures that updates to the content are immediately available to the blog platform without the need for synchronization.\nD. Snowball Edge is designed for large-scale data transfers, providing an efficient way to move 200 TB of data to Amazon S3 quickly and securely.","upvote_count":"3","comment_id":"1241557"},{"comment_id":"1239555","poster":"kupo777","upvote_count":"1","timestamp":"1719733200.0","content":"A, B\nThe NAS server has not been migrated.\n\nE\nAWS Snowcone does not have enough capacity.\nSnowball Edge Storage Optimized can handle up to 210 TB of NVMe capacity.\n\nThe answers are C and D."},{"comment_id":"1238826","timestamp":"1719598200.0","content":"Selected Answer: CD\nC And D","poster":"awsaz","upvote_count":"4"},{"comment_id":"1237989","upvote_count":"4","poster":"ebbff63","timestamp":"1719477780.0","content":"Selected Answer: CD\nAnswer CD"}],"answer_images":[],"answers_community":["CD (100%)"],"isMC":true,"answer":"CD","answer_ET":"CD","topic":"1","unix_timestamp":1719477780,"answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/142972-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"question_id":450,"choices":{"D":"Order an AWS Snowball Edge Storage Optimized device. Copy the static data artifacts to the device. Ship the device to AWS.","B":"Configure an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume for the EC2 instances to share for content access. Write code to synchronize the EBS volume with the NAS server weekly.","C":"Mount an Amazon Elastic File System (Amazon EFS) file system to the on-premises servers to act as the NAS server. Copy the blog data to the EFS file system. Mount the EFS file system to the C2 instances to serve the content.","E":"Order an AWS Snowcons SSD device. Copy the static data artifacts to the device. Ship the device to AWS.","A":"Create a weekly cron job in Amazon EventBridge. Use the cron job to invoke an AWS Lambda function to update the EC2 instances from the NAS server."},"timestamp":"2024-06-27 10:43:00"}],"exam":{"isImplemented":true,"id":33,"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional SAP-C02","numberOfQuestions":529,"isBeta":false,"provider":"Amazon","isMCOnly":true},"currentPage":90},"__N_SSP":true}