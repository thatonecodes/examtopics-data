{"pageProps":{"questions":[{"id":"SZGlKKNQQX2JCLOvmC4e","timestamp":"2021-11-13 07:29:00","unix_timestamp":1636784940,"answer_ET":"C","answers_community":["C (100%)"],"exam_id":22,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/65925-exam-aws-certified-database-specialty-topic-1-question-162/","question_text":"A company is running its customer feedback application on Amazon Aurora MySQL. The company runs a report every day to extract customer feedback, and a team reads the feedback to determine if the customer comments are positive or negative. It sometimes takes days before the company can contact unhappy customers and take corrective measures. The company wants to use machine learning to automate this workflow.\nWhich solution meets this requirement with the LEAST amount of effort?","isMC":true,"topic":"1","question_id":71,"discussion":[{"poster":"shammous","content":"Selected Answer: C\n\"When you run an ML query, Aurora calls Amazon SageMaker for a wide variety of ML algorithms or Amazon Comprehend for sentiment analysis\".\nRef: https://aws.amazon.com/getting-started/hands-on/sentiment-analysis-amazon-aurora-ml-integration/","timestamp":"1660379580.0","comment_id":"646226","upvote_count":"3"},{"poster":"novice_expert","timestamp":"1651244220.0","upvote_count":"1","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.html\n\nAurora machine learning uses a highly optimized integration between the Aurora database and the AWS machine learning (ML) services SageMaker and Amazon Comprehend.","comment_id":"594532"},{"upvote_count":"2","content":"Selected Answer: C\nComprehend is a simple ML solution and takes the least effort","comment_id":"554956","poster":"tugboat","timestamp":"1645660260.0"},{"upvote_count":"1","comment_id":"502062","poster":"mnzsql365","content":"C for me","timestamp":"1639565520.0"},{"content":"C for me\nFor details about using Aurora and Amazon Comprehend together, see Using Amazon Comprehend for sentiment detection.\nAurora machine learning uses a highly optimized integration between the Aurora database and the AWS machine learning (ML) services SageMaker and Amazon Comprehend.","timestamp":"1639060500.0","upvote_count":"1","comment_id":"497804","poster":"nood"},{"timestamp":"1637178120.0","upvote_count":"1","poster":"jelongpark","comment_id":"480238","content":"Ans C. All answers are available. The Minimum amount of effort is the key."},{"comment_id":"477680","poster":"[Removed]","content":"Answer A. Comprehend is a simple ML solution and takes the least effort","upvote_count":"1","timestamp":"1636827240.0"},{"poster":"hemantr","upvote_count":"1","timestamp":"1636784940.0","comment_id":"477320","content":"Ans C https://www.stackovercloud.com/2019/11/27/new-for-amazon-aurora-use-machine-learning-directly-from-your-databases/"}],"question_images":[],"answer":"C","choices":{"C":"Set up Aurora native integration with Amazon Comprehend. Use SQL functions to extract sentiment analysis.","D":"Set up Aurora native integration with Amazon SageMaker. Use SQL functions to extract sentiment analysis.","A":"Export the Aurora MySQL database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Use Amazon Comprehend to run sentiment analysis on the exported files.","B":"Export the Aurora MySQL database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Use Amazon SageMaker to run sentiment analysis on the exported files."},"answer_images":[]},{"id":"5M3BB279dyJtWJlZoHDL","choices":{"C":"Change the DB instance to Multi-AZ with a standby instance in another AWS Region.","D":"Create a read replica of the DB instance. Use the read replica to distribute the read traffic.","A":"Create an Amazon ElastiCache cluster. Use a write-through strategy to populate the cache.","B":"Create an Amazon ElastiCache cluster. Use a lazy loading strategy to populate the cache."},"answer":"D","exam_id":22,"question_id":72,"url":"https://www.examtopics.com/discussions/amazon/view/68497-exam-aws-certified-database-specialty-topic-1-question-163/","question_images":[],"unix_timestamp":1640321700,"answers_community":["D (93%)","7%"],"question_text":"A bank plans to use an Amazon RDS for MySQL DB instance. The database should support read-intensive traffic with very few repeated queries.\nWhich solution meets these requirements?","answer_ET":"D","discussion":[{"upvote_count":"8","content":"Selected Answer: D\nfew repeated Queries. in this case elastic cache will have to reach to the database to get the information which defeats the purpose of Elasticache","timestamp":"1643645100.0","comment_id":"537221","poster":"sj143"},{"upvote_count":"1","poster":"khun","content":"D. keyword \"few repeated Queries\" caching wont help.","timestamp":"1671336240.0","comment_id":"748593"},{"poster":"novice_expert","content":"Selected Answer: D\nD. Create a read replica of the DB instance. Use the read replica to distribute the read traffic.","comment_id":"595349","upvote_count":"2","timestamp":"1651361760.0"},{"content":"Selected Answer: D\nvery bad wording used in this question ; because of \"with extremely few repeated queries\" which basically means \"with a lot of different queries\" Elasticache won't help","timestamp":"1649847780.0","comment_id":"585157","upvote_count":"2","poster":"kret"},{"upvote_count":"2","timestamp":"1646589600.0","comment_id":"562185","content":"Sorry I take it back completely. Im an idiot. \"Very few repeated read queries\" as in most queries are unique. Elasticache wont help here. I will change my option to (D)","poster":"RotterDam"},{"upvote_count":"3","comment_id":"562174","poster":"RotterDam","content":"Correct Answer is (B) Another question right here in ExamTopics\n\nFor its application database tier, a corporation uses Amazon RDS MySQL instances, and for its web tier, Apache Tomcat servers. Repeated read requests make up the majority of database queries from web apps.\nWhich AWS service would benefit from the addition of an in-memory store for repeated read queries?\n\nA. Amazon RDS Multi-AZ\nB. Amazon SQS\nC. Amazon ElastiCache <<< (Official Answer)\nD. Amazon RDS read replica","timestamp":"1646588640.0"},{"comments":[{"content":"Replica is for read intensive -- D","comment_id":"873149","poster":"Mintwater","upvote_count":"1","timestamp":"1681771260.0"}],"timestamp":"1646588400.0","upvote_count":"1","comment_id":"562172","poster":"RotterDam","content":"Selected Answer: B\nwhat does this even mean? \"...with extremely few repeated queries.\" -> English grammar is aborrant in these questions and they're artificially creating phrases they think makese sense!!\n\nBut if they are TRYING to ask prevent the repeated queries from hitting the database- then Elasticache with lazy loading will do the trick the first query will take a hit to the database but the repeated queries will be returned from the cache\n\nFeels like (B)"},{"comment_id":"533220","upvote_count":"3","poster":"awsmonster","content":"B\n\nElasticache is the best solution to \"handle high-volume read requests with extremely few repeated queries.\" \n\nNote that it is extremely few repeated queries. As per this link, https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html, the lazy loading strategy fits this requirement","comments":[{"timestamp":"1644770100.0","comment_id":"546576","content":"go D\nkeywork \"use query\"","poster":"oopsy","upvote_count":"1"}],"timestamp":"1643227320.0"},{"content":"'with extremely few repeated queries.' I believe its D","comment_id":"509289","upvote_count":"1","poster":"SMAZ","timestamp":"1640468400.0"},{"content":"Selected Answer: D\nIs it D?","comment_id":"508316","poster":"jove","upvote_count":"2","timestamp":"1640321700.0","comments":[{"poster":"jove","comment_id":"509936","content":"Yes, the answer is D .. extremely few repeated queries -> read replica","upvote_count":"1","timestamp":"1640568360.0"}]}],"topic":"1","answer_images":[],"timestamp":"2021-12-24 05:55:00","answer_description":"","isMC":true},{"id":"d2eWObpyKVLN7rEroiWQ","answer_ET":"C","question_images":[],"unix_timestamp":1636583280,"answer_images":[],"discussion":[{"timestamp":"1651253940.0","comment_id":"594623","content":"Selected Answer: C\nWhen you associate a new DB parameter group with a DB instance, the modified static and dynamic parameters are applied only after the DB instance is rebooted. However, if you modify dynamic parameters in the newly associated DB parameter group, these changes are applied immediately without a reboot.","poster":"novice_expert","upvote_count":"8"},{"content":"Selected Answer: C\nManual reboot is necessary","upvote_count":"3","poster":"tugboat","comment_id":"555553","timestamp":"1645736340.0"},{"poster":"user0001","timestamp":"1645024860.0","comment_id":"548714","content":"C\nWhen you associate a new DB parameter group with a DB instance, the modified static and dynamic parameters are applied only after the DB instance is rebooted. However, if you modify dynamic parameters in the newly associated DB parameter group, these changes are applied immediately without a reboot. For more information about changing the DB parameter group, see Modifying an Amazon RDS DB instance.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html","upvote_count":"3"},{"comment_id":"477822","upvote_count":"2","content":"C for sure","poster":"jove","timestamp":"1636841400.0"},{"comment_id":"475834","content":"Option C. When you associate a new DB parameter group with a DB instance, the modified static and dynamic parameters are applied only after the DB instance is rebooted.","poster":"leunamE","timestamp":"1636583280.0","upvote_count":"2"}],"topic":"1","timestamp":"2021-11-10 23:28:00","answers_community":["C (100%)"],"question_text":"A database specialist has a fleet of Amazon RDS DB instances that use the default DB parameter group. The database specialist needs to associate a custom parameter group with some of the DB instances.\nAfter the database specialist makes this change, when will the instances be assigned to this new parameter group?","question_id":73,"answer_description":"","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/65816-exam-aws-certified-database-specialty-topic-1-question-164/","choices":{"B":"In the next scheduled maintenance window of the DB instances","A":"Instantaneously after the change is made to the parameter group","C":"After the DB instances are manually rebooted","D":"Within 24 hours after the change is made to the parameter group"},"exam_id":22,"isMC":true},{"id":"XXEBIniixwIXn3uF4Vjq","answers_community":["DE (64%)","CD (21%)","14%"],"answer_description":"","discussion":[{"content":"Selected Answer: DE\nC, D, E ( does not allow to select 3 options)\n\nE. use SCT to convert schema\n-> C. Create a table setting rule with parallel load option, load using DMS\n-> D. Continue DMS till cutover date","poster":"novice_expert","comment_id":"594736","timestamp":"1651277100.0","upvote_count":"8"},{"comment_id":"755255","upvote_count":"3","poster":"lollyj","timestamp":"1671923520.0","content":"Selected Answer: CD\nCDE are the answers"},{"content":"DE is the correct","timestamp":"1671449280.0","upvote_count":"1","comment_id":"749761","poster":"jjyy80"},{"poster":"examineme","upvote_count":"2","content":"Selected Answer: CE\nCDE correct answer","comment_id":"730730","timestamp":"1669746780.0"},{"upvote_count":"3","timestamp":"1646461680.0","content":"D and E are obviously correct. \nBetween B and C -> C is correct - the aws recommendation is to load Large tables via partitioning. That means creating filters to load data in parallel. The rest of the smaller tables can use the MaxFullLoadSubTasks settings to a higher value (default is 8)\n\nAn example is if you have a schema that contains 90 tables, 1 of which is 100GB while rest are small tables Create multiple tasks that break up the 100GB based on primary key Create another task to load the smaller tables with \"MaxFullLoadSubtasks\" settings to 20","comment_id":"561261","poster":"RotterDam"},{"upvote_count":"1","comment_id":"555607","poster":"tugboat","content":"Selected Answer: DE\nCDE are valid options","timestamp":"1645741440.0"},{"comment_id":"528384","content":"I vote for CDE","timestamp":"1642679580.0","poster":"awsmonster","upvote_count":"1"},{"upvote_count":"3","timestamp":"1640603340.0","comment_id":"510248","poster":"Shunpin","content":"I prefer CDE\nFor option C, https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Tablesettings.html#CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Tablesettings.ParallelLoad"},{"upvote_count":"1","content":"CDE correct options","comment_id":"488483","poster":"jove","timestamp":"1638043800.0"},{"content":"I Think CDE","timestamp":"1637008440.0","comment_id":"478982","upvote_count":"3","poster":"PietraOra"},{"comment_id":"478132","timestamp":"1636895520.0","upvote_count":"1","content":"ACD :-https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-data-from-an-on-premises-oracle-database-to-aurora-postgresql.html","poster":"hemantr","comments":[{"content":"A: wrong: AWS SCT is not data extraction tool. CDE.","comments":[{"poster":"jove","content":"Yes, CDE are the correct options.","timestamp":"1638043860.0","upvote_count":"1","comment_id":"488484"}],"comment_id":"478721","timestamp":"1636986480.0","upvote_count":"3","poster":"toppic26"}]}],"question_text":"A company is planning on migrating a 500-GB database from Oracle to Amazon Aurora PostgreSQL using the AWS Schema Conversion Tool (AWS SCT) and\nAWS DMS. The database does not have any stored procedures to migrate but has some tables that are large or partitioned. The application is critical for business so a migration with minimal downtime is preferred.\nWhich combination of steps should a database specialist take to accelerate the migration process? (Choose three.)","topic":"1","answer_images":[],"answer":"DE","question_images":[],"answer_ET":"DE","timestamp":"2021-11-14 14:12:00","unix_timestamp":1636895520,"isMC":true,"question_id":74,"exam_id":22,"choices":{"A":"Use the AWS SCT data extraction agent to migrate the schema from Oracle to Aurora PostgreSQL.","E":"Use AWS SCT to convert the schema from Oracle to Aurora PostgreSQL.","F":"Use AWS DMS to convert the schema from Oracle to Aurora PostgreSQL and for continuous replication.","D":"Use AWS DMS to set up change data capture (CDC) for continuous replication until the cutover date.","C":"For the large tables, create a table settings rule with a parallel load option in AWS DMS, then perform a full load using DMS.","B":"For the large tables, change the setting for the maximum number of tables to load in parallel and perform a full load using AWS DMS."},"url":"https://www.examtopics.com/discussions/amazon/view/66015-exam-aws-certified-database-specialty-topic-1-question-165/"},{"id":"uihAmSDCxeJgf035RHUG","isMC":true,"question_images":[],"answer_ET":"A","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/67108-exam-aws-certified-database-specialty-topic-1-question-166/","discussion":[{"content":"A is the correct answer\nThe question is how to refresh stale data","poster":"2025flakyt","upvote_count":"13","timestamp":"1639047960.0","comment_id":"497654"},{"timestamp":"1693650420.0","upvote_count":"1","comment_id":"996802","content":"Selected Answer: A\nAAAAAAA","poster":"chen0305_099"},{"upvote_count":"1","comment_id":"992498","content":"Selected Answer: D\nA is wrong, MultiSubnetfailover is not the issue here.","poster":"marcelodba","timestamp":"1693245000.0"},{"upvote_count":"3","content":"A. correct\nD. is obsolete. Amazon RDS for SQL Server now supports SQL Server Agent job replication\nPosted On: Apr 7, 2022\n\nAmazon RDS for SQL Server now supports SQL Server Agent job replication. With this new feature, SQL Server Agent jobs created, modified, or deleted on the primary instance will be automatically synchronized to the secondary instance in a Multi-AZ configuration.","poster":"khun","comment_id":"748604","timestamp":"1671337260.0"},{"upvote_count":"4","comment_id":"738461","timestamp":"1670454780.0","poster":"Sab","content":"Selected Answer: A\nA\nSQL Agent jobs can be replicated by turning on SQL Server Agent job replication\n\nEXECUTE msdb.dbo.rds_set_system_database_sync_objects @object_types = 'SQLAgentJob';\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.CommonDBATasks.Agent.html"},{"upvote_count":"2","content":"Question prepared before the Job replication feature it seems, so now Answer D is no longer valid, and it should be A.\n\nhttps://aws.amazon.com/about-aws/whats-new/2022/04/amazon-rds-sql-server-sql-agent-job-replication/","comment_id":"709432","timestamp":"1667335860.0","poster":"rags1482"},{"upvote_count":"1","comment_id":"626521","poster":"kush_sumit","timestamp":"1656845520.0","content":"Selected Answer: D\nC is wrong due to\n\nYou can't configure the secondary DB instance to accept database read activity.\nLink: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html\nSection:\nMicrosoft SQL Server Multi-AZ deployment limitations, notes, and recommendations"},{"comment_id":"626520","poster":"kush_sumit","upvote_count":"1","timestamp":"1656845460.0","content":"D is correct answer\n\nC is wrong due to \nYou can't configure the secondary DB instance to accept database read activity.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html\nMicrosoft SQL Server Multi-AZ deployment limitations, notes, and recommendations:"},{"comment_id":"622865","poster":"sachin","content":"D is the answer as JOB replication needs to be enabled explicitly.","upvote_count":"1","timestamp":"1656290460.0"},{"poster":"elf78","comment_id":"622702","content":"Selected Answer: A\nIt's about refreshing stale data. @Jusfunda's comment has the correct link","upvote_count":"3","timestamp":"1656264420.0"},{"poster":"Jusfunda","upvote_count":"3","content":"Selected Answer: A\nAnswer is A according to this link.\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/sql-server-ec2-best-practices/configure-availability-groups.html#:~:text=Set%20HostRecordTTL%20to%2060%20or%20less%20when%20using%20Always%20On%20availability%20groups","comment_id":"609487","timestamp":"1653954900.0"},{"timestamp":"1651279200.0","content":"Selected Answer: D\nIf you have SQL Server Agent jobs, recreate them on the secondary.\n Create the jobs first in the original primary, then fail over, and create the same jobs in the new primary.","poster":"novice_expert","comment_id":"594751","upvote_count":"1"},{"comment_id":"510347","timestamp":"1640612760.0","upvote_count":"3","content":"Selected Answer: D\nAs grek mentioned URL.","poster":"Shunpin"},{"comments":[{"poster":"user0001","comment_id":"557463","timestamp":"1645982580.0","content":"answer is D\nA is Wrong because it is more than 30 seconds , AG does not replicate agent on databases","upvote_count":"1"}],"content":"Selected Answer: D\nI think the correct answer is D.. The SQL Server Agent jobs are used to synchronize data between the Informix and SQL Server databases and without those being created on secondary node the data won't be synchronized.","poster":"jove","upvote_count":"3","timestamp":"1640110740.0","comment_id":"506341"},{"comment_id":"492583","poster":"grekh001","content":"I think D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html\n\nIf you have SQL Server Agent jobs, recreate them on the secondary. You do so because these jobs are stored in the msdb database, and you can't replicate this database by using Database Mirroring (DBM) or Always On Availability Groups (AGs). Create the jobs first in the original primary, then fail over, and create the same jobs in the new primary.","comments":[{"content":"This is mentioned in the URL you have shared above that JOB are replicated \n\nIn Multi-AZ deployments, SQL Server Agent jobs are replicated from the primary host to the secondary host when the job replication feature is turned on. For more information, see Turning on SQL Server Agent job replication.","timestamp":"1656290100.0","upvote_count":"2","comment_id":"622864","poster":"sachin"}],"upvote_count":"3","timestamp":"1638455100.0"}],"unix_timestamp":1638455100,"question_text":"A company is migrating an IBM Informix database to a Multi-AZ deployment of Amazon RDS for SQL Server with Always On Availability Groups (AGs). SQL\nServer Agent jobs on the Always On AG listener run at 5-minute intervals to synchronize data between the Informix database and the SQL Server database. Users experience hours of stale data after a successful failover to the secondary node with minimal latency.\nWhat should a database specialist do to ensure that users see recent data after a failover?","topic":"1","exam_id":22,"timestamp":"2021-12-02 15:25:00","answers_community":["A (55%)","D (45%)"],"answer_images":[],"question_id":75,"choices":{"B":"Break up large transactions into multiple smaller transactions that complete in less than 5 minutes.","D":"Create the SQL Server Agent jobs on the secondary node from a script when the secondary node takes over after a failure.","C":"Set the databases on the secondary node to read-only mode.","A":"Set TTL to less than 30 seconds for cached DNS values on the Always On AG listener."},"answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","name":"AWS Certified Database - Specialty","id":22,"isMCOnly":false,"isImplemented":true,"isBeta":false,"provider":"Amazon","numberOfQuestions":359},"currentPage":15},"__N_SSP":true}