{"pageProps":{"questions":[{"id":"J2DmNVvoh3B8XZJ3uEQK","question_id":81,"answer_ET":"D","choices":{"A":"Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.","B":"Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.","D":"Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days.","C":"Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days."},"question_images":[],"answer_description":"","question_text":"A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users.\nWhich action should the company take to meet these requirements MOST cost-effectively?","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/86933-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"discussion":[{"comments":[{"comments":[{"poster":"MutiverseAgent","comment_id":"956681","content":"I am taking back my answer, the right is D) as the \"Archive access tier\" check present in the \"Intelligent-Tiering Archive configurations\" is for \"S3 Glacier flexible retrieval\" which is not instant retrieval.","timestamp":"1689773820.0","comments":[{"upvote_count":"2","content":"Archive Instant Access tier (automatic)\nIf an object is not accessed for 90 consecutive days, the object moves to the Archive Instant Access tier. The Archive Instant Access tier provides low latency and high-throughput performance. --- This says it moves to instant retrieval class. Price for this is lesser than standard IA. Answer should be B","poster":"Itsmetanmay","comment_id":"1201384","timestamp":"1713963900.0"}],"upvote_count":"8"}],"poster":"MutiverseAgent","content":"I do not agree. The MOST cheaper option is B, because by choosing:\nD) Files older than 90 days will live eternally in the S3 Infrequently access layer at $0.0125/GB.\nB) Using Intelligent-Tiering files older than 90 days can be moved DIRECTLY to the \"Archive access tier\" (Glacier instant retrieval) at $0.004/GB, avoiding/skipping the \"S3 Infrequently access layer\". The question also seems to be according this assuption as says \"and configure it to move objects to a less expensive storage tier after 90 days\".\n\nhttps://aws.amazon.com/s3/pricing/?nc=sn&loc=4","timestamp":"1689772740.0","comment_id":"956670","upvote_count":"4"}],"timestamp":"1668520320.0","upvote_count":"40","comment_id":"718790","poster":"rjam","content":"Selected Answer: D\nAnswer D\nWhy Optoin D ?\nThe Question talks about downloads are infrequent older than 90 days which means files less than 90 days are accessed frequently. Standard-Infrequent Access (S3 Standard-IA) needs a minimum 30 days if accessed before, it costs more.\nSo to access the files frequently you need a S3 Standard . After 90 days you can move it to Standard-Infrequent Access (S3 Standard-IA) as its going to be less frequently accessed"},{"comments":[{"timestamp":"1684776960.0","comment_id":"904262","upvote_count":"5","poster":"ruqui","content":"have you tried to implement B? how do you configure Intelligent Tiering to move objects to a less expensive storage tier after 90 days? and which storage tier is this 'less expensive' ? the answer is clearly wrong ... correct answer is D"},{"comment_id":"761422","timestamp":"1672342920.0","poster":"FNJ1111","upvote_count":"2","content":"also, there are probably several ringtones which aren't popular/used. Why keep them in S3 standard? The company would save money if s3 intelligent-tiering moves the unpopular ringtones to a more cost-effective tier than s3 standard."},{"comments":[{"poster":"javitech83","upvote_count":"1","content":"because of tha link it is D. \nThere are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they’ll always be charged at the Frequent Access tier","timestamp":"1670400360.0","comments":[{"upvote_count":"2","poster":"javitech83","comment_id":"737562","timestamp":"1670400480.0","content":"oh sorry it states objects are bigger than 128 KB. B is correct"}],"comment_id":"737558"}],"content":"This link also has me going with “B.” Specifying 128 KB in size is not a coincidence. https://aws.amazon.com/s3/storage-classes/intelligent-tiering/","comment_id":"725573","timestamp":"1669267740.0","upvote_count":"6","poster":"Wilson_S"},{"content":"S3 Intelligent tiering is used when the access frequency is not known. I think 128KB is a deflector.","timestamp":"1670131320.0","upvote_count":"7","poster":"Wajif","comment_id":"734856"},{"upvote_count":"2","content":"Intelligent Tiering moves files how it thinks is right, not at a configurable interval such as \"after 90 days\".","comment_id":"1105950","timestamp":"1703590860.0","poster":"pentium75"}],"upvote_count":"16","poster":"zeronine75","timestamp":"1669176720.0","content":"Selected Answer: B\nB/D seems possible answer. But, I'll go with \"B\". \nIn the following table, S3 Intelligent-Tiering seems not so expansive than S3 Standard.\nhttps://aws.amazon.com/s3/pricing/?nc1=h_ls\nAnd, in the question \"128KB\" size is talking about S3 Intelligent-Tiering stuff.","comment_id":"724877"},{"content":"Selected Answer: D\nOption D is the most cost-effective solution as it leverages an S3 Lifecycle policy to automatically transition infrequently accessed files to S3 Standard-IA after 90 days, saving costs while meeting the company’s requirements.","upvote_count":"1","poster":"Skyskilo","timestamp":"1737071220.0","comment_id":"1341914"},{"content":"Selected Answer: D\nB. Mova os arquivos para o S3 Intelligent-Tiering:\n\nO S3 Intelligent-Tiering é uma solução flexível, mas tem uma pequena taxa mensal para monitorar os objetos. No caso descrito, uma política de ciclo de vida seria mais econômica para o comportamento de acesso conhecido (raros acessos após 90 dias).","comment_id":"1341086","timestamp":"1736953140.0","upvote_count":"1","poster":"Rcosmos"},{"poster":"[Removed]","comment_id":"1318367","upvote_count":"2","timestamp":"1732670220.0","content":"Selected Answer: D\nWhy D over the other options?\nA. Configure S3 Standard-IA for initial storage: Not cost-effective for frequently accessed files during the first 90 days, as S3 Standard-IA is optimized for infrequent access.\nB. Use S3 Intelligent-Tiering: While Intelligent-Tiering is a good choice for unpredictable access patterns, it incurs a monthly monitoring fee per object. Since the company's pattern is predictable (infrequent access after 90 days), this additional cost is unnecessary.\nC. Use S3 inventory to manage objects manually: S3 inventory only generates reports and does not automate object transitions. Manually moving objects is less efficient and error-prone compared to automated lifecycle policies."},{"comment_id":"1298923","poster":"PaulGa","content":"Selected Answer: D\nAns D - as well explained by rjam (1 year, 11 months ago)","upvote_count":"2","timestamp":"1729107840.0"},{"content":"Selected Answer: D\nI am going with D instead of B because Intelligent-Tiering may work against you in this case. The company knows that the files are rarely accessed after 90 days. So even if a file is accessed on the 89th day, they would still want to \"archive\" that file the next day (again because they fully know the typical access behavior). However, Intelligent Tiering will reset the 90-day inactive clock starting on the 89th day (i.e. it will not archive the file until the 189th day assuming no other access).","poster":"Duckydoo","comment_id":"1237102","timestamp":"1719350580.0","upvote_count":"2"},{"comment_id":"1233379","poster":"ChymKuBoy","timestamp":"1718842740.0","content":"Selected Answer: D\nD for sure","upvote_count":"1"},{"poster":"lofzee","upvote_count":"1","timestamp":"1716887520.0","comments":[{"poster":"lofzee","timestamp":"1716888000.0","comment_id":"1220097","upvote_count":"1","content":"Actually scrap that. I'm changing my answer to B.\nYou can setup Intelligent Tiering to move data to an archive after 90 days.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html?icmpid=docs_amazons3_console#sc-dynamic-data-access\n\nhttps://aws.amazon.com/about-aws/whats-new/2021/11/s3-intelligent-tiering-archive-instant-access-tier/\n\nAlso the 128KB size is perfect for Intelligent Tiering.\n\nTricky question indeed, one of those pointless ones that doesn't really make you a better or worse AWS Engineer"}],"content":"Selected Answer: D\nYou can't configure Intelligent Tiering, that is the whole point of it. It's intelligent and moves things on its own so there is no 90 day configuration you can set.\nTherefore D is the answer.","comment_id":"1220091"},{"upvote_count":"1","content":"Selected Answer: B\nSome songs make comebacks. Lifecycle policy isn't intelligent enough to deal with a resurgence in popularity of a ringtone. 128KB in size makes files eligible for Intelligent-Tiering.","timestamp":"1715891940.0","poster":"NSA_Poker","comment_id":"1212581"},{"upvote_count":"1","timestamp":"1714835520.0","poster":"ManikRoy","content":"Selected Answer: D\nClearly option D is the correct. This is not a trick question.","comment_id":"1206540"},{"comment_id":"1205979","content":"Selected Answer: B\nI think this is a kind of tricky question (B VS D) However:\nThe Q stated \"most accessed files readily available for its users.\"\nFrom this and correct me if I am wrong, I think B is a better solution since Amazon S3 Intelligent-Tiering working on \"moving data to the most cost-effective access tier based on access frequency\" while the S3 Standard-IA doesn't. \nRef: https://aws.amazon.com/s3/storage-classes/","poster":"MomenAWS","timestamp":"1714722360.0","upvote_count":"1"},{"poster":"richiexamaws","upvote_count":"1","content":"Selected Answer: B\nB. Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.\n\nExplanation:\n\nS3 Intelligent-Tiering is a storage class that automatically moves objects between two access tiers: frequent access and infrequent access, based on access patterns.\nBy configuring S3 Intelligent-Tiering to move objects to a less expensive storage tier after 90 days of infrequent access, the company can save money on storage costs while ensuring that the most accessed files remain readily available.\nThis approach is more cost-effective than using S3 Standard-Infrequent Access (S3 Standard-IA) because it automatically adjusts storage tiers based on access patterns without the need for manual configuration or management.","timestamp":"1712567880.0","comment_id":"1191480"},{"comment_id":"1169340","timestamp":"1709972640.0","poster":"Kanagarajd","content":"Selected Answer: D\nRight answer is D","upvote_count":"1"},{"comment_id":"1124453","timestamp":"1705432500.0","poster":"awsgeek75","content":"Selected Answer: D\nD is cheapest and managed by S3 Lifecycle policy\nA: Not readily available\nC: Wrong product\nB: No choice of '90 days' so you'll be paying for Intelligent Tiering unnecessarily for files to drop out of frequent access after the first 90 days.","upvote_count":"1"},{"timestamp":"1704458100.0","comment_id":"1114491","poster":"Firdous586","content":"B is the correct answer Kindly follow the below link for more information as proof\nhttps://aws.amazon.com/s3/storage-classes/","upvote_count":"1","comments":[{"poster":"awsgeek75","timestamp":"1705432320.0","comment_id":"1124451","content":"So D is the correct answer as IA is cheaper than Intelligent tier.","upvote_count":"1"}]},{"content":"The key reasons:\n\nS3 Lifecycle policies can automatically transition objects from S3 Standard to S3 Standard-IA after 90 days.\nS3 Standard provides high performance for frequently accessed newer files.\nS3 Standard-IA costs 20-30% less than S3 Standard for infrequently accessed files.\nThis matches access patterns - high performance for new files, cost savings for older files.\nS3 Intelligent Tiering has higher request costs and complexity for this simple access pattern.\nS3 Inventory lists objects and their properties but does not directly transition objects.\nLifecycle policies provide automated transitions without manual intervention.","comment_id":"1081287","upvote_count":"1","poster":"Ruffyit","timestamp":"1701071340.0"},{"content":"Selected Answer: D\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) has a minimum billable object size, which currently is 128KB. This means that even if the stored object is smaller than 128KB, Amazon S3 will charge for a minimum of 128KB of data.","comment_id":"1064329","poster":"wearrexdzw3123","upvote_count":"1","timestamp":"1699312380.0"},{"upvote_count":"1","poster":"Wayne23Fang","content":"Selected Answer: B\nVery tricky case. Besides all the arguments for both camps. I lean to (B). There is an article about the adoption of Intelligent-Tiering in the recent years to save money. Had the following text is \"all files ready\", I would picked (D): keeping the most accessed files readily available . for its users. I hope AWS gives \"partial credit\" for both (B) and (D) regardless which is the MOST cost-effective.","comments":[{"poster":"pentium75","comments":[{"poster":"Olukjr","content":"S3 Intelligent-Tiering monitors access patterns and automatically moves objects to the Infrequent Access tier when they haven't been accessed for 30 consecutive days. After 90 days of no access, the objects are moved to the Archive Instant Access tier without performance impact or operational overhead.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html","upvote_count":"2","timestamp":"1713243540.0","comment_id":"1196345"}],"timestamp":"1703590980.0","upvote_count":"2","comment_id":"1105951","content":"How do you 'configure Intelligent Tiering to move objects to a less expensive storage tier after 90 days'? It's called \"intelligent\" because it moves files intelligently, not after a schedule you specify."}],"timestamp":"1696943820.0","comment_id":"1039539"},{"timestamp":"1694145360.0","poster":"TariqKipkemei","content":"Selected Answer: D\nImplement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days.\n\nI would not try to overthink this.","upvote_count":"2","comment_id":"1002079"},{"poster":"Valder21","content":"Selected Answer: D\nNot B because Intelligent-tiering = unkown patterns","comment_id":"997765","upvote_count":"2","timestamp":"1693756500.0"},{"upvote_count":"3","timestamp":"1692223080.0","comment_id":"983061","poster":"Guru4Cloud","content":"Selected Answer: D\nThe key reasons:\n\nS3 Lifecycle policies can automatically transition objects from S3 Standard to S3 Standard-IA after 90 days.\nS3 Standard provides high performance for frequently accessed newer files.\nS3 Standard-IA costs 20-30% less than S3 Standard for infrequently accessed files.\nThis matches access patterns - high performance for new files, cost savings for older files.\nS3 Intelligent Tiering has higher request costs and complexity for this simple access pattern.\nS3 Inventory lists objects and their properties but does not directly transition objects.\nLifecycle policies provide automated transitions without manual intervention."},{"content":"Selected Answer: D\nAs per AWS Best Practices, S3 Intelligent Tier is designed for [unknown & changing] access patterns. Alternatively, if you do know the access pattern, use lifecycle policies.","poster":"Smart","comment_id":"961927","upvote_count":"4","timestamp":"1690223940.0"},{"poster":"vini15","comment_id":"950905","content":"should be D","upvote_count":"1","timestamp":"1689269820.0"},{"content":"Selected Answer: B\nBy using S3 IT, the company can take advantage of automatic cost optimization. IT moves objects between two access tiers: frequent access and infrequent access. In this case, since downloads for ringtones older than 90 days are infrequent, IT will automatically move those objects to the less expensive infrequent access tier, reducing storage costs while keeping the most accessed files readily available.\n\nA is not the most cost-effective solution because it doesn't consider the requirement of keeping the most accessed files readily available. S3 Standard-IA is designed for data that is accessed less frequently, but it still incurs higher costs compared to IT.\n\nC is not the most suitable solution for reducing storage costs. S3 inventory provides a list of objects and their metadata, but it does not offer direct cost optimization features.\n\nD is not the most cost-effective solution because it only moves objects from S3 Standard to S3 Standard-IA after 90 days. It doesn't take advantage of the benefits of IT, which automatically optimizes costs based on access patterns.","poster":"cookieMr","upvote_count":"3","timestamp":"1687602000.0","comment_id":"932435","comments":[]},{"poster":"kelvintoys93","timestamp":"1685950800.0","comment_id":"915179","content":"Selected Answer: D\n128kB is a just a trap. \nIt cannot be B because:\n1. Intelligent-tiering requires no configuration for class transitions - your option is just whether to opt into Archive/Deep Archive Access tier, which does not make sense for the requirement. Those two classes are cheapest in terms of storage but charges high for retrieval.\n2. Nowhere has it mentioned that the access pattern is unpredictable. If we really have to assume, I would rather assume that new songs have higher access frequency. In this case, you dont really benefit from the auto-transition feature that Intel-tier provides. You will be paying the same rate as S3 Standard class + additional fee for using Intel-tiering. Since the req is to have the most cost-efficient solution, D is the answer.","comments":[{"comment_id":"915192","timestamp":"1685951280.0","upvote_count":"2","poster":"kelvintoys93","content":"To add to my point above, for intel-tiering to move a file from:\nFrequent tier > Infrequent tier - requires object to not be accessed for 30 consecutive days\nInfrequent tier > Archive/Deep Archive - requires object to not be accessed for 90 days and above. \nCan one guarantee that a new song will not be downloaded for 30 consecutive days in order to take advantage of intel-tier's automated storage class transition? Even if that's the case, there is nothing that user need to \"configure\".. B would only be a valid solution if the configuration part is taken out.\nhttps://aws.amazon.com/s3/storage-classes/intelligent-tiering/"}],"upvote_count":"3"},{"content":"Selected Answer: B\nS3 Intelligent-Tiering is designed to optimize costs by automatically moving objects between two access tiers: frequent access and infrequent access. By moving the files to S3 Intelligent-Tiering, the company can take advantage of the automatic tiering feature to save costs on storage. Initially, the files will be stored in the frequent access tier for quick and easy access. However, since downloads for ringtones older than 90 days are infrequent, after that period, the objects will automatically be moved to the infrequent access tier, which offers a lower storage cost compared to the frequent access tier","poster":"Deansylla","comment_id":"909630","upvote_count":"1","timestamp":"1685384580.0","comments":[{"timestamp":"1703591100.0","upvote_count":"1","comment_id":"1105955","content":"How do you 'configure Intelligent Tiering to move objects to a less expensive storage tier after 90 days'? It's called \"intelligent\" because it moves files intelligently, not after a schedule you specify.","poster":"pentium75"}]},{"timestamp":"1685253720.0","poster":"Abrar2022","comment_id":"908367","upvote_count":"1","content":"In the question it mentions that the files are stored in S3 Standard. So you need to transition them from S3 standard using S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days."},{"poster":"ccmc","comments":[],"upvote_count":"1","comment_id":"892811","content":"Selected Answer: B\nare at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users. -- means some most accessed files are can be more than 90 days old. so should go with intelligent tiering as the patterns are unpredictable","timestamp":"1683611340.0"},{"poster":"cheese929","content":"Selected Answer: B\nAnswer should be B. \nS3 Standard and S3 Intelligent - Tiering are both $0.023 per GB per month. \nHowever S3 Standard - Infrequent Access is $0.0125 per GB while S3 Intelligent - Tiering Archive Access Tier is $0.0036 per GB. S3 Intelligent - Tiering Deep Archive Access Tier is even cheaper at $0.00099 per GB. Thus the answer is B.","upvote_count":"1","comment_id":"885758","timestamp":"1682905620.0"},{"comment_id":"884192","poster":"sitro95","upvote_count":"2","content":"Selected Answer: D\nI vote for option D\nB is saying that it will move to less expensive storage which can be also Glacier but this does not fill requirements of the question","timestamp":"1682760180.0"},{"poster":"kruasan","content":"Selected Answer: D\nD is more cost effective and pattern is known","upvote_count":"1","comment_id":"881931","timestamp":"1682534040.0"},{"poster":"darn","comment_id":"876139","upvote_count":"1","content":"Selected Answer: B\nS3 Intelligent Tiering","timestamp":"1682044080.0"},{"timestamp":"1681776360.0","comment_id":"873187","upvote_count":"2","content":"Selected Answer: B\nFor S3 Intelligent-Tiering, objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they’ll always be charged at the Frequent Access tier rates and don’t incur the monitoring and automation charge. The question says \"The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size\", so all files will get the benefit from S3 Intelligent-Tiering. The question also says \"while keeping the most accessed files readily available for its users.\", consequently, B is the best choice","poster":"EricYu2023"},{"comment_id":"866056","timestamp":"1681109400.0","poster":"TECHNOWARRIOR","upvote_count":"1","content":"i STAND CORRECTED ANSWER -B :Based on the given information, the company can use Amazon S3 Intelligent-Tiering for storing its files containing ringtones. Since the files are at least 128 KB in size, they will not incur any minimum object storage charges. Additionally, the company can take advantage of the auto-tiering feature of S3 Intelligent-Tiering, which automatically moves objects between different storage tiers based on their access patterns. This can help reduce storage costs by moving infrequently accessed files to the lower-cost tiers.\n\nHowever, it is important to note that objects smaller than 128KB are not eligible for auto-tiering in S3 Intelligent-Tiering. Therefore, if the company has any files smaller than 128KB, they should continue to store them in Amazon S3 Standard."},{"comment_id":"866054","content":"ANSWER -D :S3 Standard-IA is designed for larger objects and has a minimum object storage charge of 128KB. Objects smaller than 128KB in size will incur storage charges as if the object were 128KB. For example, a 6KB object in S3 Standard-IA will incur S3 Standard-IA storage charges for 6KB and an additional minimum object size charge equivalent to 122KB at the S3 Standard-IA storage price. See the Amazon S3 pricing page for information about S3 Standard-IA pricing.\nThere is no minimum object size for S3 Intelligent-Tiering, but objects smaller than 128KB are not eligible for auto-tiering. These smaller objects may be stored in S3 Intelligent-Tiering, but will always be charged at the Frequent Access tier rates, and are not charged the monitoring and automation charge.","timestamp":"1681109100.0","upvote_count":"1","poster":"TECHNOWARRIOR"},{"content":"Selected Answer: B\n*B* makes more sense because you will not have to wait for 90 days to save costs for the ringtones that do not perform","comment_id":"865050","poster":"notacert","upvote_count":"1","timestamp":"1680987000.0"},{"poster":"jcramos","upvote_count":"2","content":"Selected Answer: D\nWhy not B? There are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than **128 KB** are not eligible for auto tiering. *** These smaller objects may be stored, but they’ll always be charged at the Frequent Access tier **** rates and don’t incur the monitoring and automation charge.\n\nhttps://aws.amazon.com/s3/storage-classes/intelligent-tiering/","comment_id":"863920","timestamp":"1680874020.0"},{"upvote_count":"1","timestamp":"1680122460.0","comment_id":"854936","content":"Selected Answer: B\n\"objects smaller than 128KB are not eligible for auto-tiering\": So B makes more sense. Since Intelligent tiering applies for 128KB+ files(atleast).","poster":"kraken21"},{"content":"Selected Answer: D\nS3 Intelligent-Tiering is designed for data with unknown or changing access patterns and automatically moves data between two access tiers based on access frequency, while S3 Standard-IA is designed for infrequently accessed data that still requires low latency access times when accessed.\nIn this scenario, already mentioned that \"the files are infrequent for ringtones older than 90 days and keeping the most access files readily available for the users\". So, it is sure that S3-AI.","poster":"KZM","upvote_count":"1","comment_id":"825508","timestamp":"1677642780.0"},{"upvote_count":"1","content":"Requirement is > The company needs to save money on storage while keeping the most accessed files readily available for its user . ( So after 90 days , they can wait for access ) .\n\nLooking at AI by default it will auto move between > Frequent Access > Infrequent Access >\nArchive Instant Access with an OPTIONAL param to park after 90 days to >\n\nArchive Access – S3 Intelligent-Tiering provides you with the option to activate the Archive Access tier for data that can be accessed asynchronously. After activation, the Archive Access tier automatically archives objects that have not been accessed for a minimum of 90 consecutive days.\n\nSo B","timestamp":"1676522760.0","poster":"AlmeroSenior","comment_id":"810259"},{"comment_id":"808520","timestamp":"1676388660.0","upvote_count":"1","content":"Selected Answer: D\nTo manage your objects so that they are stored cost effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html","poster":"Joxtat"},{"content":"Selected Answer: D\nIntelligent Tiering: Monitoring and Automation, All Storage / Month (Objects > 128 KB) $0.0025 per 1,000 objects","comment_id":"806390","upvote_count":"2","timestamp":"1676210040.0","poster":"Yelizaveta"},{"upvote_count":"1","content":"I think it is D. \nS3 Lifecycle policy to move the files to S3 Standard-IA after 90 days is more cost-effected.","poster":"KZM","timestamp":"1676022660.0","comment_id":"804194"},{"content":"Selected Answer: B\nKeeping most accessed file readily available.","upvote_count":"1","poster":"ProfXsamson","comment_id":"801674","timestamp":"1675832940.0"},{"comment_id":"790061","content":"Selected Answer: D\nI think that the cost of transition from the Intelligent to the Standard infrequent should be considered. In option D, going from standard to standard infrequent is free. In option B, the transfer of the files after 90 days has a cost. The question asks for most-cost effectly, I think it is D","timestamp":"1674861660.0","upvote_count":"5","poster":"egmiranda"},{"timestamp":"1673278740.0","comment_id":"770589","poster":"SilentMilli","content":"Selected Answer: D\nOption D suggests implementing an Amazon S3 Lifecycle policy that moves objects from the S3 Standard storage class to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class after 90 days. This would allow the company to save money on storage costs while keeping the most accessed files readily available for its users.\n\nS3 Standard-IA is a storage class that is designed for objects that are accessed less frequently, but still require rapid access when needed. It is generally less expensive than S3 Standard, but has higher retrieval fees. By implementing an S3 Lifecycle policy to move objects to S3 Standard-IA after 90 days, the company would be able to take advantage of the lower storage costs for less frequently accessed objects while still being able to access the files quickly when needed.","upvote_count":"1"},{"poster":"lfrad","timestamp":"1673278020.0","content":"Selected Answer: D\nIf the ringtones are accessed from the Archive Instant Access or Infrequent Access through Intelligent-Tiering, they will be put back on the Frequent Access tier. \nYet we know these ringtones, while being accessed sometime, do not need to move up again as it will be a very rare access. Therefore D preserving their status as Infrequent Access will prevent paying 90 days of Frequent Access rate for a ringtone accessed once every 6 months.","upvote_count":"2","comment_id":"770569"},{"timestamp":"1672830660.0","comment_id":"765533","content":"Selected Answer: B\nI you compare costs for a file that is infrequently used, it's very clear B is the correct answer:\nS3 Intelligent-Tiering\n---------------------------------\n0 --------------------> 30 ---------------------------> 90\nS3 Standard Infrequent Access Archive Instant Access tier\n$0.023 $0.0125 $0.004\n\nLifeCycle\n--------------\n0 ---------------------------------------------------> 90\nS3 Standard S3 Standard - Infrequent Access\n$0.023 $0.0125","comments":[{"comment_id":"766329","content":"Try again\n\nS3 Intelligent-Tiering\n\n0 ----- > 30 -----> 90\nS3 Std S3 IA S3 Arch IA\n$0.023 $0.0125 $0.004\n\nLifeCycle\n0 ----------------> 90\nS3 Std S3 IA\n$0.023 $0.0125","upvote_count":"2","poster":"JayBee65","timestamp":"1672904160.0"}],"upvote_count":"2","poster":"JayBee65"},{"comment_id":"765026","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html , both S3-Standard and S3-IA provide millisecond access.","timestamp":"1672778340.0","poster":"dan80","upvote_count":"2"},{"poster":"MegaMax","content":"Selected Answer: D\nD S3 IA minimum size 128kb","upvote_count":"1","comment_id":"756012","timestamp":"1672002780.0"},{"timestamp":"1671983400.0","comment_id":"755799","upvote_count":"1","poster":"techhb","content":"Selected Answer: B\nits D as objects larger then 128 kb,auto tiering,here are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they’ll always be charged at the Frequent Access tier rates and don’t incur the monitoring and automation charge."},{"poster":"duriselvan","comment_id":"752582","upvote_count":"1","timestamp":"1671641700.0","content":"Ans -D : -question itself infrequent access"},{"content":"Selected Answer: B\nOption B will be more cost effective than option D.","timestamp":"1671310020.0","poster":"career360guru","upvote_count":"1","comment_id":"748405"},{"timestamp":"1670937840.0","comment_id":"744065","upvote_count":"1","content":"B \nArchive Instant Access – With S3 Intelligent-Tiering, any existing objects that have not been accessed for 90 consecutive days are automatically moved to the Archive Instant Access tier.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html","poster":"JC1418","comments":[{"content":"Object not been accessed for 90 consecutive days-Statement itself does not fulfill stated objective. Hence B should be ruled out.","upvote_count":"1","comment_id":"764676","timestamp":"1672752360.0","poster":"Shreeshak"}]},{"poster":"AlaN652","comment_id":"743659","content":"Selected Answer: D\nD seems the correct answer. Since there is extra charge associated with using the intelligent tiering \"Monitoring and Automation, All Storage / Month (Objects > 128 KB)\". The company has millions of files so this will be extra cost. The cost for storage in S3 and intelligent tiering is the same (for intelligent frequent access and S3 standard) https://aws.amazon.com/s3/pricing/?nc1=h_ls","upvote_count":"1","timestamp":"1670913120.0"},{"poster":"Qjb8m9h","content":"Answer is D: ntelligent tier shuffles objects between two tiers based on high or low access to data. Data first uploaded into the intelligent tier is assigned to the “high access tier”, since data most recently uploaded is more likely than not to need high performance and frequent access. After a period of 30 days of infrequent access, data is moved to the low access tier. This is where the cost savings come into play. If for whatever reason access to that object picks back up, AWS will move it back into the high access tier.","upvote_count":"1","comment_id":"743501","timestamp":"1670897520.0"},{"content":"Selected Answer: D\nAnswer is D. B is an incomplete solution.","comment_id":"742147","poster":"lapaki","timestamp":"1670796720.0","upvote_count":"1","comments":[{"comment_id":"742150","content":"B also suggests moving to intelligent tiering first which unnecessarily increases costs.","poster":"lapaki","timestamp":"1670796840.0","upvote_count":"1"}]},{"poster":"RBSK","comment_id":"740024","upvote_count":"1","content":"Selected Answer: B\nB and D, both seems possible options. Prefer B in this case","timestamp":"1670584140.0"},{"upvote_count":"1","timestamp":"1670468520.0","comment_id":"738570","poster":"mikey2000","content":"Selected Answer: D\ns3 glacier is the cheapest option but the files doesn't need to be archived, so you wouldn't need it because the ringtones that are 90 days MIGHT need to be downloaded, they are just infrequently accessed ,we dont need to archive them"},{"timestamp":"1670400420.0","upvote_count":"1","comment_id":"737560","content":"Selected Answer: D\nThere are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering.","comments":[{"comment_id":"737567","upvote_count":"2","content":"sorry because of that it is B","timestamp":"1670400540.0","poster":"javitech83"}],"poster":"javitech83"},{"content":"Selected Answer: D\nAnswer is D since it says infrequently accessed.","upvote_count":"2","poster":"Wajif","comment_id":"734857","timestamp":"1670131380.0"},{"timestamp":"1669309680.0","upvote_count":"5","content":"Selected Answer: B\n\"128KB\" and \"90 Days\" keywords should point to Answer B","comment_id":"726072","poster":"taichun"},{"poster":"Wpcorgan","upvote_count":"1","comment_id":"724441","timestamp":"1669133100.0","content":"D is correct"},{"timestamp":"1668006240.0","upvote_count":"3","content":"Selected Answer: D\nI think D is answer","poster":"TaiTran1994","comment_id":"714678"},{"content":"Selected Answer: B\nAnother thought on B, Because it is asking for cost effective, it is possible to move the data to Glacier instant retrival after 90 days","upvote_count":"3","poster":"PS_R","comments":[{"poster":"PS_R","content":"but first 90 days we dont have infrequent data and hence intelligent tiering might not be required.","upvote_count":"4","timestamp":"1668077580.0","comment_id":"715158"}],"comment_id":"713955","timestamp":"1667923440.0"},{"poster":"Cynthia19","content":"answer : D","comment_id":"710717","timestamp":"1667499780.0","upvote_count":"3"}],"answer":"D","timestamp":"2022-11-03 19:23:00","isMC":true,"unix_timestamp":1667499780,"topic":"1","answers_community":["D (66%)","B (34%)"]},{"id":"FKunpSJoM2lqtFfDL6UZ","choices":{"A":"Use S3 Object Lock in governance mode with a legal hold of 1 year.","D":"Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the function to track the hash of the saved object so that modified objects can be marked accordingly.","C":"Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy to only allow the IAM role.","B":"Use S3 Object Lock in compliance mode with a retention period of 365 days."},"question_text":"A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date.\nWhich solution will meet these requirements?","discussion":[{"upvote_count":"57","comment_id":"908048","poster":"elmogy","comments":[{"upvote_count":"5","timestamp":"1710995580.0","poster":"Burrito69","content":"I liked that thought of yours.. can you do more of these please? Thank you","comment_id":"1178975"},{"timestamp":"1703167080.0","poster":"Praewwara","upvote_count":"12","content":"Amazon S3 Object Lock\n1. Governance mode - Only users with special permissions can overwrite, delete, or alter object ock settings\n2. Compliance mode - No user, including the root user in an AWS account, can overwrite, delete, or alter object lock settings","comment_id":"1102583"}],"content":"Selected Answer: B\nB,\nThe key is \"No users can have the ability to modify or delete any files\" and compliance mode supports that.\nI remember it this way: ( governance is like government, they set the rules but they can allow some people to break it :D )","timestamp":"1685197980.0"},{"comment_id":"718491","upvote_count":"22","content":"Answer : B\nReason: Compliance Mode. The key difference between Compliance Mode and Governance Mode is that there are NO users that can override the retention periods set or delete an object, and that also includes your AWS root account which has the highest privileges.","timestamp":"1668485580.0","comments":[{"poster":"Zerotn3","timestamp":"1672617300.0","comment_id":"763397","comments":[{"upvote_count":"8","poster":"JayBee65","comment_id":"766333","timestamp":"1672904400.0","content":"Adding is not the same as changing :)"}],"content":"How about: The repository must allow a few scientists to add new files","upvote_count":"1"},{"comment_id":"907777","content":"Compliance mode controls the object life span after creation. \nhow this option restricts all scientists from adding new file? please explain.","upvote_count":"4","timestamp":"1685164800.0","poster":"abhishek2021"}],"poster":"Qjb8m9h"},{"comment_id":"1349755","upvote_count":"1","poster":"satyaammm","timestamp":"1738386540.0","content":"Selected Answer: B\nCompliance mode supports the feature of \"No users can delete or modify the files\"."},{"comment_id":"1298925","content":"Selected Answer: B\nAns B - Compliance mode... but not sure that answers \"allow only a few scientists to add new files\"...?","timestamp":"1729108260.0","upvote_count":"3","poster":"PaulGa"},{"poster":"toyaji","upvote_count":"2","comment_id":"1262323","timestamp":"1723095600.0","content":"Selected Answer: B\nFirst of all, Regal hold has no expiration before you remove it. So A makes no sense.\nAfter that Governance mode is breakable with permission, but Compilance mode is not even for root user cannot delete it.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html"},{"comment_id":"1235113","poster":"ChymKuBoy","timestamp":"1719033240.0","upvote_count":"1","content":"Selected Answer: B\nB for sure"},{"upvote_count":"2","content":"Selected Answer: B\nI almost chose A for this deciving line lol but it would be compliance mode as no user should be able to change objects:-\nThe repository must allow a few scientists to add new files and must restrict all other users to read-only access.","timestamp":"1714835880.0","comment_id":"1206543","poster":"ManikRoy"},{"comment_id":"1143464","poster":"demigodnyi","timestamp":"1707319740.0","content":"Can someone please explain why the answer is not A. It said that The repository must allow a few scientists to add new files. So, i think some user must have permission to change it.","upvote_count":"1","comments":[{"poster":"NSA_Poker","upvote_count":"2","timestamp":"1715893140.0","comment_id":"1212583","content":"Legal hold – A legal hold provides the same protection as a retention period, but it HAS NO EXPIRATION DATE. Instead, a legal hold remains in place until you explicitly remove it. Legal holds are independent from retention periods and are PLACED ON INDIVIDUAL OBJECT VERSIONS."},{"timestamp":"1714835940.0","upvote_count":"2","content":"Write Once Read Many - in compliance mode there is no restriction in adding but only changing existing objects","comment_id":"1206544","poster":"ManikRoy"}]},{"content":"Unsure, B would meet the \"must keep every file for a minimum of 1 year\" requirement. (In theory C would too if you ignore the root user, but administrators could remove the policy.) But what about the 'a few scientists must be able to add new files'? None of the options mentions permissions for a special group.","comments":[{"upvote_count":"1","timestamp":"1705432680.0","content":"agree that something is missing for \"some users\".\nACD are not going to work flat out so B looks like right answer but with some language issues either in the question or the answer.","comment_id":"1124456","poster":"awsgeek75"},{"poster":"LoXoL","content":"Agree. It looks like it's missing sth here.","upvote_count":"1","comment_id":"1120756","timestamp":"1705062420.0"}],"timestamp":"1703591640.0","poster":"pentium75","upvote_count":"3","comment_id":"1105961"},{"content":"Both Compliance & Governance mode protect objects against being deleted or changed. But in Governance mode some people can have special permissions. In this question, no user can delete or modify files; so the answer is Compliance mode only. Neither of these modes restrict user from adding new files.","upvote_count":"3","comment_id":"1081311","timestamp":"1701073380.0","poster":"Ruffyit"},{"timestamp":"1694145660.0","comment_id":"1002087","upvote_count":"2","content":"Selected Answer: B\nCompliance Mode best suits this scenario because once an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened.","poster":"TariqKipkemei"},{"poster":"Guru4Cloud","comment_id":"983067","timestamp":"1692223560.0","upvote_count":"2","content":"Selected Answer: B\nB) seems to be the right option, because: Both option A) & B) allow to:\n- Scientists add new files & other users read-only access.\n- Keep files for a minimum of 1 year\nOnly option B allows to:\n- Disable all users the ability to modify or delete any file.\nIf A) were the correct option some scientis will be able to modify files, as if they were in charge of put an object lock same permission would allow them to remove the lock and consequently delete the file."},{"comment_id":"956716","timestamp":"1689775740.0","content":"Selected Answer: B\nB) seems to be the right option, because: Both option A) & B) allow to:\n- Scientists add new files & other users read-only access.\n- Keep files for a minimum of 1 year\nOnly option B allows to:\n- Disable all users the ability to modify or delete any file.\nIf A) were the correct option some scientis will be able to modify files, as if they were in charge of put an object lock same permission would allow them to remove the lock and consequently delete the file.","upvote_count":"1","poster":"MutiverseAgent"},{"timestamp":"1687604400.0","poster":"cookieMr","comment_id":"932470","content":"Selected Answer: B\nS3 Object Lock provides the necessary features to enforce immutability and retention of objects in an S3. Compliance mode ensures that the locked objects cannot be deleted or modified by any user, including those with write access. By setting a retention period of 365 days, the company can ensure that every file in the repository is kept for a minimum of 1 year after its creation date.\n\nA does not provide the same level of protection as compliance mode. In governance mode, there is a possibility for authorized users to remove the legal hold, potentially allowing objects to be modified or deleted.\n\nC can restrict users from deleting or changing objects, but it does not enforce the retention period requirement. It also does not provide the same level of immutability and protection against accidental or malicious modifications.\n\nD does not address the requirement of preventing users from modifying or deleting files. It provides a mechanism for tracking changes but does not enforce the desired access restrictions or retention period.","upvote_count":"4"},{"content":"Am I the only one to worry about leap years ?","upvote_count":"1","comment_id":"904329","timestamp":"1684782600.0","poster":"norris81"},{"poster":"cheese929","comment_id":"885765","timestamp":"1682906220.0","content":"Selected Answer: B\nIn compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account. When an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened. Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period.\nIn governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary. \nIn Governance mode, Objects can be deleted by some users with special permissions, this is against the requirement.","upvote_count":"2"},{"content":"Selected Answer: B\nits B, legal hold has no retention","comment_id":"876140","timestamp":"1682044140.0","upvote_count":"3","poster":"darn"},{"timestamp":"1682040120.0","comment_id":"876106","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html","poster":"Shrestwt","upvote_count":"1"},{"poster":"jaswantn","content":"Both Compliance & Governance mode protect objects against being deleted or changed. But in Governance mode some people can have special permissions. In this question, no user can delete or modify files; so the answer is Compliance mode only. Neither of these modes restrict user from adding new files.","comment_id":"857481","timestamp":"1680306780.0","upvote_count":"2"},{"timestamp":"1674979260.0","upvote_count":"1","content":"B. Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period.","comment_id":"791465","poster":"ProfXsamson"},{"poster":"aba2s","timestamp":"1672833960.0","comments":[{"poster":"aba2s","upvote_count":"4","comment_id":"769501","timestamp":"1673187240.0","content":"users cannot have the ability to modify or delete any files in the repository ==> Compliance Mode"}],"comment_id":"765595","upvote_count":"1","content":"Selected Answer: B\nusers can have the ability to modify or delete any files in the repository ==> Compliance Mode"},{"comments":[{"timestamp":"1680987120.0","upvote_count":"1","comment_id":"865052","poster":"notacert","content":"Legal hold needs to be removed manually.\n\n\n\"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. \""}],"upvote_count":"3","comment_id":"763391","content":"Selected Answer: A\nB would also meet the requirement to keep every file in the repository for at least 1 year after its creation date, as you can specify a retention period of 365 days. However, it would not meet the requirement to restrict all users except a few scientists to read-only access. S3 Object Lock in compliance mode only allows you to specify retention periods and does not have any options for controlling access to objects in the bucket.\n\nTo meet all the requirements, you should use S3 Object Lock in governance mode and use IAM policies to control access to the objects in the bucket. This would allow you to specify a legal hold with a retention period of at least 1 year and to restrict all users except a few scientists to read-only access.","poster":"Zerotn3","timestamp":"1672615740.0"},{"poster":"techhb","comment_id":"755800","upvote_count":"2","timestamp":"1671983640.0","content":"Selected Answer: B\nNo users can have the ability to modify or delete any files in the repository. hence it must be compliance mode."},{"upvote_count":"3","poster":"lazyyoung","content":"Selected Answer: B\nAnswer is B\nCompliance: \n- Object versions can't be overwritten or deleted by any user, including the root user \n- Objects retention modes can't be changed, and retention periods can't be shortened \n\nGovernance: \n- Most users can't overwrite or delete an object version or alter its lock settings \n- Some users have special permissions to change the retention or delete the object","comment_id":"751465","timestamp":"1671569880.0"},{"comment_id":"748409","content":"Selected Answer: B\nB is best answer but I feel none of the answers covers the requirement for only few users(scientiest) are able to upload(create) the file in the bucket and all other users has Read only access.","upvote_count":"3","poster":"career360guru","timestamp":"1671310380.0"},{"content":"It is B per \"No users can have the ability to modify or delete any files in the repository. \". Compliance mode supports that requirement whereas Governance mode does not as defined via https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html.","upvote_count":"1","comment_id":"747883","timestamp":"1671265860.0","poster":"SteveD15"},{"comment_id":"726038","poster":"Cizzla7049","timestamp":"1669307100.0","upvote_count":"1","comments":[{"poster":"JayBee65","timestamp":"1672904520.0","content":"Why is it not B?","comment_id":"766334","upvote_count":"1"}],"content":"Selected Answer: A\nANSWER IS DEFINITELY A"},{"content":"B i think. im not sure..thougts?","upvote_count":"1","comment_id":"724446","timestamp":"1669133220.0","poster":"Wpcorgan"},{"upvote_count":"1","poster":"mabotega","content":"Selected Answer: A\nhttps://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/object-lock/#:~:text=be%20deleted%20again.-,Compliance%20Mode.,which%20has%20the%20highest%20privileges.","comment_id":"715462","timestamp":"1668105060.0"},{"content":"Selected Answer: B\n\"No users can have the ability to modify or delete any files in the repository\" = Compliance mode.","poster":"ArielSchivo","comment_id":"714659","timestamp":"1668004500.0","upvote_count":"3"},{"timestamp":"1667500980.0","content":"Selected Answer: B\nB. Due to compliance","upvote_count":"2","comment_id":"710728","poster":"USalo"},{"content":"A is Correct \n\n\"In governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary.\"\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html","comments":[{"upvote_count":"2","timestamp":"1669095240.0","comment_id":"724110","poster":"Bobbybash","content":"if you have very specific permissions, including s3:BypassGovernanceMode, s3:GetObjectLockConfiguration, s3:GetObjectRetention, then a user will still be able to delete an object version within the retention period or change any retention settings set on the bucket."}],"comment_id":"710278","upvote_count":"1","timestamp":"1667455080.0","poster":"nikerlas"},{"poster":"bunnychip","upvote_count":"4","comment_id":"705571","content":"Selected Answer: B\n'No users\" can have the ability to modify or delete any files in the repository","timestamp":"1666876200.0"},{"comments":[{"comment_id":"704898","upvote_count":"9","poster":"Six_Fingered_Jose","timestamp":"1666805640.0","content":"actually i read the question again\n> No users can have the ability to modify or delete any files in the repository.\n\nanswer should be B ignore my comment"}],"poster":"Six_Fingered_Jose","upvote_count":"3","content":"Selected Answer: A\nAnswer should be A because a few scientist must be able to edit the file\n> In governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions.\n\nIt cant be B because in compliance mode, absolutely nobody can touch the file during its period\n> In compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html#object-lock-retention-modes","comment_id":"704897","timestamp":"1666805580.0"},{"poster":"dave9994","timestamp":"1666657740.0","upvote_count":"3","content":"Compliance mode is more restrictive : https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html","comment_id":"703447"}],"topic":"1","question_id":82,"answer_description":"","answer_ET":"B","question_images":[],"isMC":true,"exam_id":31,"answer_images":[],"timestamp":"2022-10-25 02:29:00","answers_community":["B (92%)","8%"],"unix_timestamp":1666657740,"url":"https://www.examtopics.com/discussions/amazon/view/86359-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"B"},{"id":"1PN1l6OOgGJFdJziIEN1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/86795-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"A":"Use AWS DataSync to connect the S3 buckets to the web application.","D":"Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application.","B":"Deploy AWS Global Accelerator to connect the S3 buckets to the web application.","C":"Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers."},"question_images":[],"timestamp":"2022-11-01 14:49:00","topic":"1","exam_id":31,"question_id":83,"discussion":[{"upvote_count":"15","poster":"rjam","comment_id":"709184","timestamp":"1667310540.0","content":"key :caching\nOption C"},{"comment_id":"983068","content":"Selected Answer: C\nThe reasons are:\n\nAmazon CloudFront is a content delivery network (CDN) that caches content at edge locations around the world.\nConnecting the S3 buckets containing the media files to CloudFront will cache the content at global edge locations.\nThis provides fast reliable access to users everywhere by serving content from the nearest edge location.\nCloudFront integrates tightly with S3 for secure, durable storage.\nGlobal Accelerator improves availability and performance for TCP/UDP traffic, not HTTP-based content delivery.\nDataSync and SQS are not technologies for a global CDN like CloudFront.","poster":"Guru4Cloud","timestamp":"1692223620.0","upvote_count":"8"},{"comment_id":"1349756","content":"Selected Answer: C\nCloudFront is most suitable here and is designed to do this.","poster":"satyaammm","timestamp":"1738386660.0","upvote_count":"1"},{"comment_id":"1298927","content":"Selected Answer: C\nAns C - Cloudfront is designed to do this...","poster":"PaulGa","timestamp":"1729108440.0","upvote_count":"2"},{"upvote_count":"2","poster":"TariqKipkemei","comment_id":"1002089","content":"Selected Answer: C\nAmazon CloudFront to the rescue","timestamp":"1694145780.0"},{"content":"Selected Answer: C\nCloudFront is a content delivery network (CDN) service provided by AWS. It caches content at edge locations worldwide, allowing users to access the content quickly regardless of their geographic location. By connecting the S3 to CloudFront, the media files can be cached at edge locations, ensuring reliable and fast delivery to users.\n\nA. is a data transfer service that is not designed for caching or content delivery. It is used for transferring data between on-premises storage systems and AWS services.\n\nB. is a service that improves the performance and availability of applications for global users. While it can provide fast and reliable access, it is not specifically designed for caching media files or connecting directly to S3.\n\nD. is a message queue service that is not suitable for caching or content delivery. It is used for decoupling and coordinating message-based communication between different components of an application.\n\nTherefore, the correct solution is option C, deploying CloudFront to connect the S3 to CloudFront edge servers.","timestamp":"1687604580.0","poster":"cookieMr","upvote_count":"5","comment_id":"932472"},{"timestamp":"1686745500.0","upvote_count":"1","poster":"jackky3123213","content":"Global Accelerator does not support Edge Caching","comment_id":"923180"},{"poster":"Bmarodi","comment_id":"905137","upvote_count":"1","content":"Selected Answer: C\nOption C is correct answer.","timestamp":"1684865700.0"},{"comment_id":"847305","timestamp":"1679504040.0","upvote_count":"2","poster":"warioverde","content":"As far as I understand, Global Accelerator does not have caching features, so CloudFront would be the recommended service for that purpose"},{"comment_id":"809734","poster":"Americo32","content":"Selected Answer: C\nC correto","timestamp":"1676478600.0","upvote_count":"1"},{"content":"C, Caching == Edge location == CloudFront","comment_id":"791466","timestamp":"1674979500.0","upvote_count":"3","poster":"ProfXsamson"},{"comment_id":"748411","upvote_count":"2","timestamp":"1671310500.0","content":"Selected Answer: C\nC right answer","poster":"career360guru"},{"upvote_count":"1","comment_id":"745895","content":"Selected Answer: C\nAgreed","poster":"k1kavi1","timestamp":"1671097320.0"},{"content":"C is correct","comment_id":"724447","upvote_count":"1","poster":"Wpcorgan","timestamp":"1669133280.0"},{"content":"Selected Answer: C\nAnswer is C","comment_id":"716946","timestamp":"1668288900.0","poster":"MyNameIsJulien","upvote_count":"1"}],"question_text":"A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate geographically.\nWhich solution will meet these requirements?","answer_images":[],"unix_timestamp":1667310540,"isMC":true,"answers_community":["C (100%)"],"answer_ET":"C","answer":"C"},{"id":"X4O7yOh7HgorgunN2fSW","answer_ET":"AE","timestamp":"2022-10-18 07:28:00","url":"https://www.examtopics.com/discussions/amazon/view/85770-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1666070880,"topic":"1","discussion":[{"upvote_count":"17","content":"Selected Answer: AE\nI believe AE makes the most sense","poster":"Wazhija","timestamp":"1666070880.0","comment_id":"697932"},{"upvote_count":"12","timestamp":"1666805880.0","content":"Selected Answer: AE\nyeah AE makes sense, only E is working with S3 here and questions wants them to be in S3","poster":"Six_Fingered_Jose","comment_id":"704901"},{"poster":"Dharmarajan","timestamp":"1738865040.0","comment_id":"1352563","upvote_count":"1","content":"Selected Answer: AE\nA&E - least operational overhead - E over D"},{"timestamp":"1737990600.0","content":"Selected Answer: AE\nA,E is ok","comment_id":"1347451","poster":"sdelena","upvote_count":"1"},{"content":"Selected Answer: AD\nAns A, D - A everyone seems to agree; I choose D over E because Parquet is aimed at columnar data - and that is not specified and may restrict query type access","timestamp":"1729108920.0","poster":"PaulGa","comment_id":"1298932","upvote_count":"2"},{"timestamp":"1722098100.0","comment_id":"1256361","content":"Selected Answer: AE\nAE satisfies the requirements that demand that the data should be stored in s3 and a one-time analytic will run on it.","poster":"jaradat02","upvote_count":"3"},{"comment_id":"1220111","timestamp":"1716889200.0","poster":"lofzee","upvote_count":"3","content":"Selected Answer: AE\nC and D = too much overhead\nB = incorrect because Athena is used for one time queries. \n\nThat leaves A and E"},{"timestamp":"1704054960.0","upvote_count":"4","content":"Selected Answer: AE\nA is a given due to Athena and QuickSight option.\nBetween C and E, the AWS Lake Formation is a more managed solution so it should have less operational overhead that writing Custom AWS Lambda.\nAE should be preferred over AC.","comments":[{"content":"E is only confusing because of Apache Parquet format (like a grid?) what's the point of that in the context of this quesiton?","timestamp":"1705433160.0","poster":"awsgeek75","upvote_count":"5","comment_id":"1124461"}],"comment_id":"1110811","poster":"awsgeek75"},{"poster":"Guru4Cloud","upvote_count":"8","content":"Selected Answer: AE\nThe reasons are:\n\nAWS Lake Formation and Glue provide automated data lake creation with minimal coding. Glue crawlers identify sources and ETL jobs load to S3.\nAthena allows ad-hoc queries directly on S3 data with no infrastructure to manage.\nQuickSight provides easy cloud BI for dashboards.\nOptions C and D require significant custom coding for ETL and queries.\nRedshift and OpenSearch would require additional setup and management overhead.","comment_id":"983070","timestamp":"1692223920.0"},{"timestamp":"1688616360.0","upvote_count":"4","poster":"Mia2009687","comment_id":"944304","content":"Selected Answer: AE\nIt combines data from database and stream data, so data lake needs to be used.\nAnd it wants to do one time query, so Athena is better."},{"timestamp":"1687375620.0","poster":"TTaws","upvote_count":"3","comment_id":"929850","content":"@Golcha once the data comes from different sources then you use GLUE"},{"upvote_count":"1","timestamp":"1685160840.0","content":"Selected Answer: AC\nLess Overhead with option AC .No need to manage","poster":"Jeeva28","comments":[{"content":"But C moves the data to Redshift while the question says you want it in S3 (and Athena from answer A also needs it in S3).","comment_id":"1105969","timestamp":"1703592360.0","upvote_count":"4","poster":"pentium75"}],"comment_id":"907739"},{"upvote_count":"1","poster":"Golcha","comments":[{"content":"C moves the data to Redshift while the question says you want it in S3 (and Athena from answer A also needs it in S3).","poster":"pentium75","timestamp":"1703592420.0","comment_id":"1105970","upvote_count":"2"}],"timestamp":"1681473480.0","comment_id":"870159","content":"Selected Answer: AC\nNo specific use case for GLUE"},{"upvote_count":"6","poster":"TECHNOWARRIOR","content":"The Apache Parquet format is a performance-oriented, column-based data format designed for storage and retrieval. It is generally faster for reads than writes because of its columnar storage layout and a pre-computed schema that is written with the data into the files. AWS Glue’s Parquet writer offers fast write performance and flexibility to handle evolving datasets. You can use AWS Glue to read Parquet files from Amazon S3 and from streaming sources as well as write Parquet files to Amazon S3. When using AWS Glue to build a data lake foundation, it automatically crawls your Amazon S3 data, identifies data formats, and then suggests schemas for use with other AWS analytic services[1][2][3][4].","comment_id":"866067","timestamp":"1681110300.0"},{"timestamp":"1681110060.0","comment_id":"866064","upvote_count":"5","content":"ANSWER - AE:Amazon Athena is the best choice for running one-time queries on streaming data. Although Amazon Kinesis Data Analytics provides an easy and familiar standard SQL language to analyze streaming data in real-time, it is designed for continuous queries rather than one-time queries[1]. On the other hand, Amazon Athena is a serverless interactive query service that allows querying data in Amazon S3 using SQL. It is optimized for ad-hoc querying and is ideal for running one-time queries on streaming data[2].AWS Lake Formation uses as a central place to have all your data for analytics purposes (E). Athena integrate perfect with S3 and can makes queries (A).","poster":"TECHNOWARRIOR"},{"comments":[{"content":"Why S3 in Apache Parquet? https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-s3-announces-parquet-output-format-for-inventory/","upvote_count":"2","timestamp":"1680877080.0","comment_id":"863974","poster":"jcramos"}],"upvote_count":"5","timestamp":"1680876780.0","comment_id":"863967","poster":"jcramos","content":"Selected Answer: AE\nAWS Lake Formation uses as a central place to have all your data for analytics purposes (E). Athena integrate perfect with S3 and can makes queries (A)."},{"content":"Can anyone please explain me why B cannot be an answer?","comment_id":"803880","poster":"JiyuKim","upvote_count":"6","comments":[{"timestamp":"1682040840.0","poster":"Shrestwt","content":"Kinesis Data Analytics is designed for continuous queries rather than one-time queries.","upvote_count":"8","comment_id":"876116"}],"timestamp":"1675992480.0"},{"content":"can anyone help me in below question\n36. A company has a Java application that uses Amazon Simple Queue Service (Amazon SOS) to parse messages. The application cannot parse messages that are large on 256KB size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.\nWhich solution will meet these requirements with the FEWEST changes to the code?\na) Use the Amazon SOS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.\nb) Use Amazon EventBridge to post large messages from the application instead of Aaron SOS\nc) Change the limit in Amazon SQS to handle messages that are larger than 256 KB\nd) Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS) Configure Amazon SQS to reference this location in the messages.","comment_id":"785665","poster":"ashishvineetlko","timestamp":"1674495360.0","upvote_count":"1","comments":[{"comment_id":"817573","upvote_count":"1","content":"I will do \"A\" as well.","timestamp":"1677050820.0","poster":"skondey"},{"content":"A would probably be the best answer. Sqs extended client library is for Java apps.","upvote_count":"1","timestamp":"1674980040.0","poster":"ProfXsamson","comment_id":"791471"}]},{"comment_id":"784485","poster":"bullrem","comments":[{"poster":"pentium75","comment_id":"1105971","upvote_count":"2","content":"Why use OpenSearch service?","timestamp":"1703592480.0"}],"timestamp":"1674405180.0","upvote_count":"1","content":"Selected Answer: DE\nI believe DE makes the most sense"},{"comment_id":"782462","upvote_count":"6","poster":"ShinobiGrappler","content":"Selected Answer: AE\nstored in s3 -> data lake -> athena (process the SQL parquet format)-> quicksight visualize","timestamp":"1674230820.0"},{"timestamp":"1672618620.0","comment_id":"763401","comments":[{"content":"\"Company needs to consolidate all the data into one place\" -> S3 bucket, which is happening in E, which means Athena would not have an issue, so A is ok.","upvote_count":"4","poster":"JayBee65","comments":[{"comment_id":"785152","content":"Absolutely, querying data is after staging and so Athena fits perfectly.","upvote_count":"1","timestamp":"1674464340.0","poster":"jainparag1"}],"timestamp":"1672904880.0","comment_id":"766339"}],"upvote_count":"2","content":"Selected Answer: BE\nWhile Amazon Athena is a fully managed service that makes it easy to analyze data stored in Amazon S3 using SQL, it is primarily designed for running ad-hoc queries on data stored in Amazon S3. It may not be the best choice for running one-time queries on streaming data, as it is not designed to process data in real-time.\n\nAdditionally, using Amazon Athena for one-time queries on streaming data could potentially lead to higher operational overhead, as you would need to set up and maintain the necessary infrastructure to stream the data into Amazon S3, and then query the data using Athena.\n\nUsing Amazon Kinesis Data Analytics, as mentioned in option B, would be a better choice for running one-time queries on streaming data, as it is specifically designed to process data in real-time and can automatically scale to match the incoming data rate.","poster":"Zerotn3"},{"content":"Selected Answer: AE\nC can work it out ,but has additional overhead.","poster":"techhb","upvote_count":"4","comment_id":"755803","timestamp":"1671984000.0"},{"comment_id":"748414","timestamp":"1671310620.0","poster":"career360guru","upvote_count":"2","content":"Selected Answer: AE\nA and E"},{"poster":"javitech83","comment_id":"737582","timestamp":"1670401440.0","upvote_count":"1","content":"Selected Answer: AC\nI would go for AE as information needs to be stored in S3"},{"upvote_count":"3","poster":"Swagata23","content":"Anser is AE : https://aws.amazon.com/blogs/big-data/enhance-analytics-with-google-trends-data-using-aws-glue-amazon-athena-and-amazon-quicksight/","timestamp":"1670338560.0","comment_id":"736922"},{"poster":"DivaLight","timestamp":"1669563300.0","upvote_count":"1","comment_id":"728379","content":"Selected Answer: AE\nOption AE"},{"poster":"Cizzla7049","content":"Selected Answer: AC\nA and C are correct","timestamp":"1669307280.0","comment_id":"726044","upvote_count":"1"},{"poster":"backbencher2022","timestamp":"1667747160.0","content":"Selected Answer: AE\nA&E is the correct answer","upvote_count":"1","comment_id":"712391"},{"content":"AC is correct. Ans E is also correct But in ans E: since Apache Parquer format is used, this is not correct answer as per AWS exam answer\nSix_Fingered_Jose","comments":[{"poster":"kmliuy73","content":"https://aws.amazon.com/tw/about-aws/whats-new/2018/12/amazon-s3-announces-parquet-output-format-for-inventory/","upvote_count":"1","comment_id":"741738","timestamp":"1670765460.0"}],"comment_id":"708950","timestamp":"1667289660.0","poster":"Dsouzaf","upvote_count":"4"}],"answers_community":["AE (89%)","5%"],"answer":"AE","answer_images":[],"choices":{"E":"Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.","C":"Create custom AWS Lambda functions to move the individual records from the databases to an Amazon Redshift cluster.","B":"Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.","A":"Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.","D":"Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters."},"answer_description":"","question_images":[],"question_text":"A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs).\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)","question_id":84,"exam_id":31,"isMC":true},{"id":"Eyrmimer13Q0V0YAyzjJ","topic":"1","question_text":"A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/87629-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":85,"answers_community":["DE (77%)","AD (18%)","5%"],"unix_timestamp":1668586560,"timestamp":"2022-11-16 09:16:00","answer_ET":"DE","isMC":true,"answer_description":"","discussion":[{"poster":"JayBee65","comments":[{"content":"100% agree","upvote_count":"7","poster":"aadityaravi8","comment_id":"944171","timestamp":"1688598960.0"}],"upvote_count":"52","timestamp":"1672906800.0","content":"I tend to agree D and E...\n\nA - Manual task that can be automated, so why make life difficult?\nB - The maximum retention period is 35 days, so would not help\nC - The maximum retention period is 35 days, so would not help\nD - Only option that deals with logs, so makes sense\nE - Partially manual but only option that achieves the 5 year goal","comment_id":"766371"},{"timestamp":"1669634040.0","poster":"kmaneith","comment_id":"729013","upvote_count":"22","content":"Selected Answer: DE\ndude trust me","comments":[{"timestamp":"1672905780.0","comment_id":"766351","poster":"JayBee65","upvote_count":"16","content":"No, please show your reasoning, you may be wrong. Remember, no one thinks they are wrong, but some always are :)"},{"poster":"Priyanshugpt486","upvote_count":"2","content":"hehe... hehe","comment_id":"1012821","timestamp":"1695275940.0"}]},{"poster":"PaulGa","content":"Selected Answer: DE\nAns D, E","upvote_count":"1","timestamp":"1729109220.0","comment_id":"1298934"},{"content":"D + E I think.\nD deals with the logs.\nAWS Backup for backups....\nWhy make backups more difficult by not using the built in backup tool?","upvote_count":"3","timestamp":"1716889440.0","comment_id":"1220115","poster":"lofzee"},{"poster":"rjjkc","comment_id":"1212501","content":"Selected Answer: AD\nI'd say \"A\" over \"E\" because in the option \"E\", it says use AWS Backup to take the \"backups\" not snapshot. \"If you use AWS CLI, this is set using the parameter DeleteAfterDays. The retention period for snapshots can range between 1 day and 100 years (or indefinitely if you don't enter one), while the retention period for continuous backups can range from 1 day to 35 days. The creation date of a backup is the date the backup job started, not the date it completed. If your backup job doesn't complete on the same date it started, use the date on which it began to help calculate retention periods.\"\n\nFrom here: https://docs.aws.amazon.com/aws-backup/latest/devguide/creating-a-backup-plan.html","upvote_count":"2","timestamp":"1715876760.0"},{"content":"Selected Answer: DE\nAgree with the reasoning of JayBee","comment_id":"1206550","upvote_count":"1","poster":"ManikRoy","timestamp":"1714837140.0"},{"poster":"EMPERBACH","comment_id":"1198082","timestamp":"1713464220.0","content":"Selected Answer: CD\nC - C - https://www.bing.com/ck/a?!&&p=7e852c106834b9bdJmltdHM9MTcxMzM5ODQwMCZpZ3VpZD0xNzJiMThiOS1kM2RiLTZlZGEtMWNhZC0wYjRlZDJiZDZmZDYmaW5zaWQ9NTQ0OQ&ptn=3&ver=2&hsh=3&fclid=172b18b9-d3db-6eda-1cad-0b4ed2bd6fd6&psq=does+Aurora+offer+auto+backup&u=a1aHR0cHM6Ly9kb2NzLmF3cy5hbWF6b24uY29tL0FtYXpvblJEUy9sYXRlc3QvQXVyb3JhVXNlckd1aWRlL0F1cm9yYS5NYW5hZ2luZy5CYWNrdXBzLmh0bWwjOn46dGV4dD1BdXJvcmElMjBiYWNrcyUyMHVwJTIweW91ciUyMGNsdXN0ZXIlMjB2b2x1bWUlMjBhdXRvbWF0aWNhbGx5JTIwYW5kLHRvJTIwYW55JTIwcG9pbnQlMjB3aXRoaW4lMjB0aGUlMjBiYWNrdXAlMjByZXRlbnRpb24lMjBwZXJpb2Qu&ntb=1\n\nD - Only answer for audit logs of activities on database","upvote_count":"2"},{"timestamp":"1707791760.0","poster":"jjcode","comment_id":"1148855","upvote_count":"2","content":"My thoughts:\n1. AWS backups is designed to make back ups \n2. \"configure backup retention for 5 years\" with what? a script? maybe AWS backups???? are the back ups done with DD and stored in S3? i cannot trust this answer\n3. \"Take a manual snapshot of the DB cluster\" this is not an amazon best practice they want us to use their tools AWS backups\n4. \"create a life cycle policy\" assuming the back ups are stored in S3 (which is not a best practice) cannot trust this\n\nthat leaves D and E"},{"upvote_count":"2","timestamp":"1701085200.0","poster":"Ruffyit","comment_id":"1081479","content":"D AND E- makes more sense as we automate backups in Aurora DB\n- Export data to CloudWatch to capture all log events and configure CloudWatch to retain logs indefinitely."},{"poster":"awashenko","timestamp":"1697136120.0","comment_id":"1042015","content":"Selected Answer: DE\nD and E. \n\nA would work as well, but D is the better option as its automated. \nE is the only option that gets you to the 5 year retention.","upvote_count":"2"},{"timestamp":"1694515980.0","content":"D AND E- makes more sense as we automate backups in Aurora DB \n- Export data to CloudWatch to capture all log events and configure CloudWatch to retain logs indefinitely.","poster":"kambarami","comment_id":"1005655","upvote_count":"2"},{"content":"Selected Answer: DE\nDE makes more sense","upvote_count":"1","comment_id":"1002098","timestamp":"1694146680.0","poster":"TariqKipkemei"},{"poster":"Guru4Cloud","content":"Selected Answer: CD\nThe reasons are:\n\nConfiguring the automated backups for the Aurora PostgreSQL DB cluster to retain backups for 5 years will meet the requirement to store all data for that duration.\nExporting the database logs to CloudWatch Logs will capture the audit logs of actions performed in the database. CloudWatch Logs retention can be configured to store logs indefinitely.\nThis meets the need to keep audit logs available beyond the 5 year data retention period.\nAdditional manual snapshots or using AWS Backup for backups is not necessary since automated backups are already enabled.\nA lifecycle policy is useful for transitioning storage classes but does not apply here for a set 5 year retention.","upvote_count":"2","timestamp":"1692224280.0","comment_id":"983072"},{"timestamp":"1679593020.0","comment_id":"848511","content":"Selected Answer: AD\nAutomated backup is limited 35 days","poster":"neverdie","upvote_count":"3"},{"comment_id":"787149","upvote_count":"5","comments":[{"comment_id":"787151","timestamp":"1674610140.0","content":"https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls","poster":"Training4aBetterLife","upvote_count":"3"}],"content":"Selected Answer: DE\nPreviously, you had to create custom scripts to automate backup scheduling, enforce retention policies, or consolidate backup activity for manual Aurora cluster snapshots, especially when coordinating backups across AWS services. With AWS Backup, you gain a fully managed, policy-based backup solution with snapshot scheduling and snapshot retention management. You can now create, manage, and restore Aurora backups directly from the AWS Backup console for both PostgreSQL-compatible and MySQL-compatible versions of Aurora.\nTo get started, select an Amazon Aurora cluster from the AWS Backup console and take an on-demand backup or simply assign the cluster to a backup plan.","timestamp":"1674610080.0","poster":"Training4aBetterLife"},{"content":"Selected Answer: DE\nA is not a valid option for meeting the requirements. A manual snapshot of the DB cluster is a point-in-time copy of the data in the cluster. While taking manual snapshots can be useful for creating backups of the data, it is not a reliable or efficient way to meet the requirement of storing all the data for 5 years and deleting it after 5 years. It would be difficult to ensure that manual snapshots are taken regularly and retained for the required period of time. It is recommended to use a fully managed backup service like AWS Backup, which can automate and centralize the process of taking and retaining backups.","comments":[{"timestamp":"1672619580.0","content":"Sorry, B and E that correct\nB. Create a lifecycle policy for the automated backups.\nThis would ensure that the backups taken using AWS Backup are retained for the desired period of time.","comment_id":"763410","poster":"Zerotn3","comments":[{"upvote_count":"3","timestamp":"1672906920.0","comment_id":"766374","poster":"JayBee65","content":"I think a lifecycle policy would only keep backups for 35 days"},{"timestamp":"1697135940.0","poster":"awashenko","upvote_count":"2","content":"Thats not correct ( i thought it was but I went and looked it up) Aurora only keeps backups from 1-35 days.","comment_id":"1042010"}],"upvote_count":"1"}],"timestamp":"1672619460.0","upvote_count":"4","poster":"Zerotn3","comment_id":"763407"},{"content":"Selected Answer: DE\nD and E only","timestamp":"1671985560.0","upvote_count":"2","poster":"techhb","comment_id":"755817"},{"upvote_count":"1","timestamp":"1671902100.0","poster":"Chirantan","content":"AD\n is correct as you can keep backup of snapshot indifferently.","comment_id":"755037"},{"upvote_count":"2","poster":"career360guru","comment_id":"748416","content":"Selected Answer: DE\nD and E","timestamp":"1671310800.0"},{"comment_id":"744752","timestamp":"1670996280.0","upvote_count":"5","poster":"Qjb8m9h","content":"Aurora backups are continuous and incremental so you can quickly restore to any point within the backup retention period. No performance impact or interruption of database service occurs as backup data is being written. You can specify a backup retention period, from 1 to 35 days, when you create or modify a DB cluster.\n\nIf you want to retain a backup beyond the backup retention period, you can also take a snapshot of the data in your cluster volume. Because Aurora retains incremental restore data for the entire backup retention period, you only need to create a snapshot for data that you want to retain beyond the backup retention period. You can create a new DB cluster from the snapshot."},{"poster":"Marge_Simpson","upvote_count":"3","content":"Selected Answer: DE\nD is the only one that resolves the logging situation\n\"automated backup\" = AWS Backup\nhttps://aws.amazon.com/backup/faqs/?nc=sn&loc=6\nAWS Backup provides a centralized console, automated backup scheduling, backup retention management, and backup monitoring and alerting. AWS Backup offers advanced features such as lifecycle policies to transition backups to a low-cost storage tier. It also includes backup storage and encryption independent from its source data, audit and compliance reporting capabilities with AWS Backup Audit Manager, and delete protection with AWS Backup Vault Lock.","timestamp":"1670910900.0","comment_id":"743634"},{"content":"AD\nReason: When creating Aurora back up, you will need to specify the retention period which is between 1-35days. This does not meet the 5years retention requirement in this case. \nHence taking a snap manual snap shot is the best solution. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html","timestamp":"1670900460.0","poster":"Qjb8m9h","comment_id":"743540","upvote_count":"2"},{"comment_id":"737284","comments":[{"comment_id":"741759","poster":"kmliuy73","timestamp":"1670767020.0","upvote_count":"3","content":"https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls AWS Backup"}],"poster":"Heyang","upvote_count":"4","timestamp":"1670370000.0","content":"Selected Answer: AD\nno more than 35 days"},{"poster":"VicBucket1996","upvote_count":"3","comments":[],"comment_id":"737254","timestamp":"1670365620.0","content":"We all are agree with letter D but based in this documentation I think A could be the other correct answer:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html\n\nBut if I wrong, let me know, please :)"},{"content":"Selected Answer: DE\nDE Option","timestamp":"1669563480.0","upvote_count":"3","poster":"DivaLight","comment_id":"728382"},{"content":"Selected Answer: DE\nD and E is the most sensible options here.","poster":"Phinx","comment_id":"727994","upvote_count":"3","timestamp":"1669533840.0"},{"upvote_count":"7","timestamp":"1669455960.0","comment_id":"727423","content":"Selected Answer: DE\nhttps://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls\nAWS Backup adds Amazon Aurora database cluster snapshots as its latest protected resource","poster":"justtry"},{"timestamp":"1669364160.0","upvote_count":"5","poster":"Nightducky","comment_id":"726495","content":"Selected Answer: DE\nThere is no sense with A if you can use AWS backup and keep snapshot for 5 years.","comments":[{"comment_id":"765645","timestamp":"1672837080.0","poster":"HayLLlHuK","content":"https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls%20AWS%20Backup","upvote_count":"1"},{"timestamp":"1670995800.0","content":"But the retention period is between 1-35 went creating Aurora backup using AWS backup.","comment_id":"744747","poster":"Qjb8m9h","upvote_count":"2"}]},{"comment_id":"725605","content":"DE, AWS Backup adds Amazon Aurora database cluster snapshots as its latest protected resource. Starting today, you can use AWS Backup to manage Amazon Aurora database cluster snapshots. AWS Backup can centrally configure backup policies, monitor backup activity, copy a snapshot within and across AWS regions, except for China regions, where snapshots can only be copied from one China region to another.","poster":"TECHNOWARRIOR","timestamp":"1669272360.0","upvote_count":"3"},{"comment_id":"725328","poster":"ds0321","upvote_count":"2","timestamp":"1669227780.0","content":"Selected Answer: AD\n35 days is the Maximum time for Backups aurora"},{"upvote_count":"1","poster":"Nigma","timestamp":"1669183680.0","content":"Selected Answer: AD\nA and D","comment_id":"724925"},{"upvote_count":"2","comment_id":"724449","content":"D and E","timestamp":"1669133520.0","poster":"Wpcorgan"},{"comment_id":"721289","content":"Selected Answer: DE\nI’m going for DE. Picked E because AWS backup does work to create and manage snapshots of Aurora DB instances.","timestamp":"1668776220.0","poster":"peneloco","upvote_count":"4"},{"upvote_count":"3","comment_id":"721241","timestamp":"1668769800.0","content":"Selected Answer: DE\nD for audit log\nE for backup","poster":"LeGloupier"},{"content":"Answer A and D\nManual DB Snapshots. Retention of backup for as long as you want\nAudit Logs can be enabled and sent to CloudWatch Logs for longer retention","poster":"rjam","timestamp":"1668595320.0","comment_id":"719552","upvote_count":"3"},{"timestamp":"1668586560.0","comments":[{"upvote_count":"1","poster":"Aamee","timestamp":"1669928220.0","comment_id":"733093","content":"What's the source of truth for this statement?"}],"upvote_count":"4","poster":"babaxoxo","comment_id":"719464","content":"Selected Answer: AD\nAnswer A&D\nIf you want to retain Aurora Backup beyond the maximum retention day (35) -> do manual snapshot"}],"answer_images":[],"choices":{"B":"Create a lifecycle policy for the automated backups.","D":"Configure an Amazon CloudWatch Logs export for the DB cluster.","C":"Configure automated backup retention for 5 years.","E":"Use AWS Backup to take the backups and to keep the backups for 5 years.","A":"Take a manual snapshot of the DB cluster."},"answer":"DE","question_images":[]}],"exam":{"provider":"Amazon","isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31,"numberOfQuestions":1019,"isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true},"currentPage":17},"__N_SSP":true}