{"pageProps":{"questions":[{"id":"Q9gtp9d3w4xkf18uPIlR","answer":"B","question_images":[],"answer_ET":"B","answer_description":"","choices":{"C":"Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application's Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.","A":"Reconfigure the application's Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.","D":"Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.","B":"Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application's Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs."},"exam_id":32,"discussion":[{"content":"The options provided to this question are literally foolish. You don't need an SNS notification and lambda function to update route 53. Router 53 can do the health checks on its own and failover. You just need one lambda function to promote the read-replica to the backup instance which can't be done automatically in the multi-region setup. In any case, from the options provided, B is the correct one, but whoever was the architect, he definitely didn't clear the SAP-C02 certification. LOL!","upvote_count":"1","poster":"Sin_Dan","comment_id":"1298638","timestamp":"1729073220.0"},{"content":"B\nThis solution recommends creating an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values, and then configuring Route 53 with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. Finally, the application's Route 53 record should be updated with a failover policy that routes traffic to the ALB in the backup region when a health check failure occurs.\n\nThis approach provides automatic failover to the backup region when a health check failure occurs, reducing the RTO to less than 15 minutes. Additionally, this approach is cost-effective as it does not require an active-active strategy.","timestamp":"1678169160.0","comment_id":"831593","upvote_count":"1","poster":"TajSidKazi"},{"poster":"syaldram","upvote_count":"1","timestamp":"1673196960.0","comment_id":"769642","content":"Selected Answer: B\nC is definitely in correct. The company does not have the budget!"},{"upvote_count":"3","poster":"Spavanko","timestamp":"1669895280.0","comment_id":"732553","content":"Selected Answer: B\nWhy not B:\n\nC is incorrect because \"The company does not have a large enough budget for an active-active strategy.\""},{"content":"Selected Answer: B\nStandard solution","poster":"SureNot","upvote_count":"3","comment_id":"727101","timestamp":"1669412400.0","comments":[{"comment_id":"730420","content":"why not A is not good?","upvote_count":"1","poster":"masetromain","timestamp":"1669727640.0","comments":[{"timestamp":"1669770900.0","content":"It;s NOT A because, It's not Active-Active, Latency based routing is for active-active mainly.\n\nWe need failover policy.\n\nRef - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","upvote_count":"3","comment_id":"730975","poster":"pixepe"},{"upvote_count":"1","timestamp":"1704915480.0","content":"You also cannot configure a cross-region alarm. An alarm based on a metric from another region. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Cross-Account-Cross-Region.html","poster":"3a632a3","comment_id":"1118989"}]}]}],"topic":"1","answers_community":["B (100%)"],"answer_images":[],"unix_timestamp":1669412400,"question_id":966,"question_text":"A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application's data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.\n\nThe company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company does not have a large enough budget for an active-active strategy.\n\nWhat should a solutions architect recommend to meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/88764-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2022-11-25 22:40:00","isMC":true},{"id":"6KrvzJqTE2a8I01lIJCo","discussion":[{"comments":[{"comment_id":"993073","upvote_count":"1","poster":"vn_thanhtung","timestamp":"1693312500.0","content":"1. The company already has created the database schema in an Amazon RDS for MySQL DB instance. \n2. A solutions architect needs to design a solution that will migrate the data to AWS with the least possible downtime\nAnd you are going to choose D. Stupid!"}],"upvote_count":"1","content":"Selected Answer: D\nWhy not option D?\nIt suit for the requirement","poster":"phattran","timestamp":"1685968080.0","comment_id":"915438"},{"comment_id":"831597","content":"B\nAWS Database Migration Service (DMS), which is designed to migrate databases to AWS with minimal downtime. It can use change data capture (CDC) to continuously replicate changes made to the on-premises database to the Amazon RDS for MySQL DB instance. VPC endpoints for AWS DMS ensure that the data is not transferred over the internet. Encryption at rest is provided by using the AWS KMS default key, and encryption in transit is provided by using TLS.","timestamp":"1678169820.0","poster":"TajSidKazi","upvote_count":"2"},{"timestamp":"1673197320.0","comment_id":"769647","content":"Selected Answer: B\nB makes more sense to me!","upvote_count":"2","poster":"syaldram"},{"content":"Selected Answer: B\nB is the one.","timestamp":"1671110880.0","upvote_count":"2","poster":"Kende","comment_id":"746114"},{"timestamp":"1670087580.0","comment_id":"734584","content":"Correct B.\nnew data is constantly updated in the database","poster":"ggrodskiy","upvote_count":"3"},{"content":"Selected Answer: B\nB is correct\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html\nhttps://aws.amazon.com/dms/\nhttps://aws.amazon.com/fr/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/","timestamp":"1669728360.0","upvote_count":"3","poster":"masetromain","comment_id":"730441"}],"answers_community":["B (88%)","13%"],"question_id":967,"question_text":"A company is migrating an on-premises application and a MySQL database to AWS. The application processes highly sensitive data, and new data is constantly updated in the database. The data must not be transferred over the internet. The company also must encrypt the data in transit and at rest.\n\nThe database is 5 TB in size. The company already has created the database schema in an Amazon RDS for MySQL DB instance. The company has set up a 1 Gbps AWS Direct Connect connection to AWS. The company also has set up a public VIF and a private VIF. A solutions architect needs to design a solution that will migrate the data to AWS with the least possible downtime.\n\nWhich solution will meet these requirements?","exam_id":32,"isMC":true,"timestamp":"2022-11-29 14:26:00","choices":{"A":"Perform a database backup. Copy the backup files to an AWS Snowball Edge Storage Optimized device. Import the backup to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.","B":"Use AWS Database Migration Service (AWS DMS) to migrate the data to AWS. Create a DMS replication instance in a private subnet. Create VPC endpoints for AWS DMS. Configure a DMS task to copy data from the on-premises database to the DB instance by using full load plus change data capture (CDC). Use the AWS Key Management Service (AWS KMS) default key for encryption at rest. Use TLS for encryption in transit.","D":"Use Amazon S3 File Gateway. Set up a private connection to Amazon S3 by using AWS PrivateLink. Perform a database backup. Copy the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.","C":"Perform a database backup. Use AWS DataSync to transfer the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."},"url":"https://www.examtopics.com/discussions/amazon/view/89247-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","answer_ET":"B","answer_images":[],"unix_timestamp":1669728360,"topic":"1","question_images":[],"answer":"B"},{"id":"eF2lJLJiH3LVwjGyz88s","discussion":[{"timestamp":"1735165500.0","upvote_count":"1","content":"Selected Answer: B\nEMRFS is designed for S3 integration. You should delete the cluster to save cost.","comment_id":"1331720","poster":"Zinnia_Wang"},{"poster":"WhyIronMan","comment_id":"1250467","content":"Selected Answer: D\nI agree with D","timestamp":"1721312940.0","upvote_count":"1"},{"content":"Selected Answer: D\nThis solutions architect is not very good. Basically, the compute savings plan eliminates any other options except D. The other thing that leads to D is \"business critical\" and no mention of capacity reservations to ensure on-demand capacity. BTW, EMR is very easy to shutdown and start back up even with a complex cross realm trust configured with AD. You can save a lot of money by doing this and all of your data is safely stored in S3.","comment_id":"1119006","timestamp":"1704917400.0","upvote_count":"1","poster":"3a632a3"},{"timestamp":"1687813140.0","upvote_count":"2","poster":"SkyZeroZx","content":"Selected Answer: D\nI agree with D. The cluster performs tasks that are critical to business needs so we can't turn off cluster.\nbut not seems for Compute savnings plans in this case","comment_id":"934788"},{"content":"Answer : B\n\nhttps://aws.amazon.com/blogs/big-data/optimize-amazon-emr-costs-with-idle-checks-and-automatic-resource-termination-using-advanced-amazon-cloudwatch-metrics-and-aws-lambda/","timestamp":"1686261840.0","comments":[{"comment_id":"993075","poster":"vn_thanhtung","content":"Terminate the cluster, including all instances, when the processing is completed. No way ?","upvote_count":"1","timestamp":"1693312740.0"}],"upvote_count":"2","comment_id":"918704","poster":"Roontha"},{"content":"Selected Answer: D\nI donÂ´t think we should terminate the on demand instances considering the job is executed daily. this might generate additional costs and we are already using a compute savings plan for those Reserved EC2.","upvote_count":"2","comment_id":"908295","timestamp":"1685242740.0","poster":"rbm2023"},{"comment_id":"791823","upvote_count":"1","poster":"zozza2023","content":"Selected Answer: D\nI agree with D","timestamp":"1675009200.0"},{"upvote_count":"2","timestamp":"1674930600.0","content":"Selected Answer: B\nTerminate when no longer needed.","comment_id":"790930","poster":"Vash2303"},{"comment_id":"734585","timestamp":"1670087640.0","content":"Correct D.","poster":"ggrodskiy","upvote_count":"3"},{"comment_id":"730452","content":"Selected Answer: D\nI agree with D. The cluster performs tasks that are critical to business needs so we can't turn off cluster.","poster":"masetromain","timestamp":"1669728900.0","upvote_count":"2"}],"unix_timestamp":1669728900,"question_images":[],"choices":{"C":"Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage","D":"Launch the master and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate only the task node instances when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.","B":"Launch the master and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.","A":"Launch all task, master, and core nodes on Spot Instances in an instance fleet. Terminate the duster, including all instances, when the processing is completed."},"answer_ET":"D","answer":"D","isMC":true,"timestamp":"2022-11-29 14:35:00","answer_description":"","question_id":968,"url":"https://www.examtopics.com/discussions/amazon/view/89250-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"answers_community":["D (75%)","B (25%)"],"answer_images":[],"topic":"1","question_text":"A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs tasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, master, and core nodes. The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is not a priority because the data is not referenced until late in the day.\n\nThe solutions architect must review the architecture and suggest a solution to minimize the compute costs.\n\nWhich solution should the solutions architect recommend to meet these requirements?"},{"id":"cR7es1DPLJysaKAuMMtO","choices":{"A":"Use a spread placement group. Set a minimum of eight instances for each Availability Zone.","D":"Launch the additional instances as Dedicated Hosts in the placement groups.","C":"Create a new placement group. Merge the new placement group with the original placement group.","B":"Stop and start all the instances in the placement group. Try the launch again."},"answer_images":[],"answer_description":"","question_id":969,"topic":"1","answer":"B","answers_community":["B (100%)"],"unix_timestamp":1669734660,"url":"https://www.examtopics.com/discussions/amazon/view/89258-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A solutions architect has launched multiple Amazon EC2 instances in a placement group within a single Availability Zone. Because of additional load on the system, the solutions architect attempts to add new instances to the placement group. However, the solutions architect receives an insufficient capacity error.\n\nWhat should the solutions architect do to troubleshoot this issue?","timestamp":"2022-11-29 16:11:00","answer_ET":"B","exam_id":32,"question_images":[],"discussion":[{"comment_id":"1250469","timestamp":"1721313060.0","poster":"WhyIronMan","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster","upvote_count":"1"},{"timestamp":"1687812720.0","poster":"SkyZeroZx","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster","upvote_count":"1","comment_id":"934787"},{"poster":"loustitech","timestamp":"1686481440.0","content":"C is wrong.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nGeneral rules and limitations > You can't merge placement groups.","upvote_count":"1","comment_id":"920592"},{"poster":"Vash2303","comment_id":"790935","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster","upvote_count":"1","timestamp":"1674930900.0"},{"poster":"ggrodskiy","content":"Correct B.","upvote_count":"2","comment_id":"734586","timestamp":"1670087640.0"},{"content":"Selected Answer: B\nThe correct answer is B:\nIf you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again.\nC - is wrong \"You can't merge placement groups.\"\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","comments":[{"comment_id":"730548","content":"If you try to add more instances to the placement group later, or if you try to launch more than one instance type in the placement group, you increase your chances of getting an insufficient capacity error.\n\nIf you stop an instance in a placement group and then start it again, it still runs in the placement group. However, the start fails if there isn't enough capacity for the instance.\n\nIf you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Starting the instances may migrate them to hardware that has capacity for all of the requested instances.\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","upvote_count":"1","poster":"masetromain","timestamp":"1669735020.0"}],"upvote_count":"1","timestamp":"1669734660.0","comment_id":"730541","poster":"masetromain"}],"isMC":true},{"id":"cfClSPbaY6XIuw0Efoty","question_id":970,"answer_images":[],"choices":{"C":"Use the Lambda Provisioned Concurrency feature.","E":"Change the API Gateway endpoint to an edge-optimized endpoint.","B":"Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.","D":"Move the code for opening the database connection in the Lambda function outside of the event handler.","A":"Use the cluster endpoint of the Aurora database."},"question_text":"A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.\n\nDuring testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application's performance.\n\nWhich actions should the solutions architect take to meet these requirements? (Choose two.)","exam_id":32,"timestamp":"2022-11-29 16:16:00","answer_description":"","question_images":[],"answer_ET":"BD","topic":"1","isMC":true,"answer":"BD","discussion":[{"upvote_count":"1","timestamp":"1704918480.0","poster":"3a632a3","comment_id":"1119012","content":"Selected Answer: BD\nB - enables connection pooling for RDS\nD - create the database connection outside of the handler to allow connections to be re-used by subsequent function invocations.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-lambda-tutorial.html"},{"content":"Correct BD","comment_id":"949213","upvote_count":"2","timestamp":"1689097440.0","poster":"ggrodskiy"},{"comment_id":"915304","timestamp":"1685959620.0","content":"Selected Answer: BD\nB: https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html#:~:text=Objects declared outside of the function's handler method remain initialized\nD: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html#:~:text=RDS%20Proxy%20establishes%20a%20database%20connection%20pool%20and%20reuses%20connections%20in%20this%20pool.%20This%20approach%20avoids%20the%20memory%20and%20CPU%20overhead%20of%20opening%20a%20new%20database%20connection%20each%20time","poster":"F_Eldin","upvote_count":"2"},{"comment_id":"884577","content":"Selected Answer: BC\nBC makes sense","upvote_count":"1","poster":"dev112233xx","timestamp":"1682790960.0"},{"poster":"zozza2023","comment_id":"791834","timestamp":"1675009740.0","content":"Selected Answer: BD\nB and D for me","upvote_count":"1"},{"content":"B and D are the ones.","timestamp":"1671111600.0","comment_id":"746132","poster":"Kende","upvote_count":"3"},{"timestamp":"1669734960.0","comments":[{"upvote_count":"1","content":"The answer cannot be E and A:\nfor A:\nThe Lambda function only queries an Amazon Aurora MySQL database.\n- You use the cluster endpoint for all write operations on the DB cluster, including inserts, updates, deletes, and DDL changes. You can also use the cluster endpoint for read operations, such as queries.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\n\nfor E:\nThe consumers of the web application are all close to the AWS Region where the application will be deployed\n- An edge-optimized API endpoint is best for geographically distributed clients.\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint-types.html","poster":"masetromain","comment_id":"730547","timestamp":"1669735020.0"}],"content":"Selected Answer: BD\nI go witch B and D.\nfor B:\nConnect to RDS outside of Lambda handler method to improve performance\n\nhttps://awstut.com/en/2022/04/30/connect-to-rds-outside-of-lambda-handler-method-to-improve-performance-en/\n\nfor D:\nUsing RDS Proxy, you can handle unpredictable surges in database traffic. Otherwise, these surges might cause issues due to oversubscribing connections or creating new connections at a fast rate. RDS Proxy establishes a database connection pool and reuses connections in this pool. This approach avoids the memory and CPU overhead of opening a new database connection each time. To protect the database against oversubscription, you can control the number of database connections that are created.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html","upvote_count":"2","poster":"masetromain","comment_id":"730545"}],"unix_timestamp":1669734960,"url":"https://www.examtopics.com/discussions/amazon/view/89259-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["BD (86%)","14%"]}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"isImplemented":true,"provider":"Amazon","isBeta":false,"id":32},"currentPage":194},"__N_SSP":true}