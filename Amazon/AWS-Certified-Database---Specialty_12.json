{"pageProps":{"questions":[{"id":"0NOC7OAOWTWjsWw73mhm","question_text":"A database specialist must load 25 GB of data files from a company's on-premises storage to an Amazon Neptune database.\nWhich approach to load the data is FASTEST?","timestamp":"2021-11-13 17:28:00","answers_community":["A (69%)","D (31%)"],"isMC":true,"answer":"A","exam_id":22,"answer_images":[],"question_id":56,"discussion":[{"timestamp":"1637445000.0","comment_id":"482852","content":"they point to the right place but the answer is wrong, should be A:\n1.Copy the data files to an Amazon Simple Storage Service (Amazon S3) bucket.\n2. Create an IAM role with Read and List access to the bucket.\n3. Create an Amazon S3 VPC endpoint.\n4. Start the Neptune loader by sending a request via HTTP to the Neptune DB instance.\n5. The Neptune DB instance assumes the IAM role to load the data from the bucket.","poster":"johnconnor","upvote_count":"8"},{"poster":"adelcold","comment_id":"939526","upvote_count":"3","content":"Selected Answer: A\nhttps://aws.amazon.com/datasync/\n\nD is not correct as DataSync cannot write into the Neptune directly!","timestamp":"1688179920.0"},{"poster":"aviathor","timestamp":"1684501560.0","upvote_count":"2","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html","comment_id":"901952"},{"timestamp":"1678819500.0","comment_id":"839144","poster":"sk1974","upvote_count":"2","content":"Selected Answer: A\nAnswer is A"},{"comment_id":"821713","comments":[{"poster":"aviathor","comment_id":"901956","upvote_count":"2","content":"The problem with that is that AWS DataSync cannot \"load data into Neptune\". So the solution of uploading data to S3 and then using the Neptune Loader to ingest the data is still the closest we get...","timestamp":"1684501860.0"}],"poster":"OCHT","timestamp":"1677347940.0","upvote_count":"4","content":"Selected Answer: D\nOption D, on the other hand, allows for direct transfer of data from the on-premises storage to Neptune using AWS DataSync, which can be a more efficient and cost-effective option. Additionally, DataSync can handle large data sets and can automatically handle network interruptions and failures during the data transfer process.\n\nTherefore, if speed is a top priority, option D is likely to be the fastest approach for loading the 25 GB of data files to the Neptune database."},{"timestamp":"1651367700.0","comment_id":"595382","content":"Selected Answer: A\nA. Upload the data to Amazon S3 and use the Loader command to load the data from Amazon S3 into the Neptune database.\n\n1.Copy the data files to an Amazon Simple Storage Service (Amazon S3) bucket.\n2. Create an IAM role with Read and List access to the bucket.\n3. Create an Amazon S3 VPC endpoint.\n4. Start the Neptune loader by sending a request via HTTP to the Neptune DB instance.\n5. The Neptune DB instance assumes the IAM role to load the data from the bucket.","poster":"novice_expert","upvote_count":"2"},{"content":"Got this question in my exam. (i cleared it). A is correct","poster":"RotterDam","upvote_count":"2","comment_id":"562589","timestamp":"1646654580.0"},{"poster":"leunamE","content":"Answer A.","timestamp":"1636820880.0","comment_id":"477631","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/amazon/view/65970-exam-aws-certified-database-specialty-topic-1-question-149/","choices":{"A":"Upload the data to Amazon S3 and use the Loader command to load the data from Amazon S3 into the Neptune database.","D":"Use AWS DataSync to load the data directly from the on-premises storage into the Neptune database.","B":"Write a utility to read the data from the on-premises storage and run INSERT statements in a loop to load the data into the Neptune database.","C":"Use the AWS CLI to load the data directly from the on-premises storage into the Neptune database."},"topic":"1","question_images":[],"answer_ET":"A","answer_description":"","unix_timestamp":1636820880},{"id":"IYE8i65y0bULiyqfpJzv","answer_description":"","question_text":"A company has a production Amazon Aurora Db cluster that serves both online transaction processing (OLTP) transactions and compute-intensive reports. The reports run for 10% of the total cluster uptime while the OLTP transactions run all the time. The company has benchmarked its workload and determined that a six- node Aurora DB cluster is appropriate for the peak workload.\nThe company is now looking at cutting costs for this DB cluster, but needs to have a sufficient number of nodes in the cluster to support the workload at different times. The workload has not changed since the previous benchmarking exercise.\nHow can a Database Specialist address these requirements with minimal user involvement?","question_images":[],"answer_ET":"D","answers_community":["D (100%)"],"unix_timestamp":1594178820,"question_id":57,"topic":"1","exam_id":22,"url":"https://www.examtopics.com/discussions/amazon/view/25069-exam-aws-certified-database-specialty-topic-1-question-15/","discussion":[{"poster":"chicagomassageseeker","content":"Answer D. You dont setup a seperate cluster for reporting. Aurora does OLTP and reporting from same cluster. just use autoscaling.","timestamp":"1632583380.0","comment_id":"129418","upvote_count":"15"},{"content":"Selected Answer: D\nOut of the provided options, D seems to be the best.","poster":"MultiAZ","timestamp":"1705074060.0","upvote_count":"1","comment_id":"1120895"},{"poster":"Pranava_GCP","timestamp":"1694741940.0","upvote_count":"2","comment_id":"1008023","content":"Selected Answer: D\nD. Set up Auto Scaling --> minimal user involvement\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html#Aurora.Integrating.AutoScaling.Concepts\n\n\"Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.\""},{"poster":"Zimboguru","comment_id":"681888","content":"Selected Answer: D\nD is correct","upvote_count":"1","timestamp":"1664380380.0"},{"comment_id":"595247","content":"Selected Answer: D\nAuto-scaling will scale-up/down number of read-replicas","poster":"novice_expert","upvote_count":"1","timestamp":"1651348860.0"},{"poster":"guru_ji","upvote_count":"1","timestamp":"1635840240.0","content":"D ==>> Correct Answer.","comment_id":"437449"},{"timestamp":"1635832020.0","poster":"aws4myself","upvote_count":"1","comment_id":"434506","content":"D => autoscaling on Aurora cluster"},{"content":"D ==>> Correct Answer.","comment_id":"425875","poster":"guru_ji","timestamp":"1635767640.0","upvote_count":"1"},{"timestamp":"1635055680.0","upvote_count":"2","comment_id":"314756","content":"D for sure.","poster":"LMax"},{"upvote_count":"1","poster":"myutran","timestamp":"1634847480.0","comment_id":"299601","content":"Ans: D"},{"upvote_count":"1","timestamp":"1634028420.0","content":"D is the correct answer. Auto-scaling will scale-up/down number of read-replicas automatically based on the workload.","poster":"JobinAkaJoe","comment_id":"252843"},{"poster":"Ebi","timestamp":"1632953580.0","content":"Answer is D","comment_id":"169860","upvote_count":"1"},{"content":"D is Correct","comment_id":"139726","poster":"BillyC","upvote_count":"2","timestamp":"1632952920.0"}],"timestamp":"2020-07-08 05:27:00","isMC":true,"choices":{"C":"Use the stop cluster functionality to stop all the nodes of the DB cluster during times of minimal workload. The cluster can be restarted again depending on the workload at the time.","A":"Split up the DB cluster into two different clusters: one for OLTP and the other for reporting. Monitor and set up replication between the two clusters to keep data consistent.","B":"Review all evaluate the peak combined workload. Ensure that utilization of the DB cluster node is at an acceptable level. Adjust the number of instances, if necessary.","D":"Set up automatic scaling on the DB cluster. This will allow the number of reader nodes to adjust automatically to the reporting workload, when needed."},"answer":"D","answer_images":[]},{"id":"mAah8ySm2GVFiAtkFV7J","unix_timestamp":1636805640,"isMC":true,"choices":{"A":"Use AWS Backup to build a backup plan for the required retention period. Assign the DB instances to the backup plan.","D":"Use AWS Lambda to schedule a daily manual snapshot of the DB instances. Delete snapshots that exceed the retention requirement.","B":"Modify the DB instances to enable the automated backup option. Select the required backup retention period.","C":"Automate a daily cron job on an Amazon EC2 instance to create MySQL dumps, transfer to Amazon S3, and implement an S3 Lifecycle policy to meet the retention requirement."},"question_images":[],"question_id":58,"answer_images":[],"question_text":"A finance company needs to make sure that its MySQL database backups are available for the most recent 90 days. All of the MySQL databases are hosted on\nAmazon RDS for MySQL DB instances. A database specialist must implement a solution that meets the backup retention requirement with the least possible development effort.\nWhich approach should the database specialist take?","exam_id":22,"discussion":[{"upvote_count":"5","content":"Ans: A\nAuto Backup is 35 Max. Creating Backup plan and Create on-demand backup are the solution.","comment_id":"510161","poster":"Shunpin","timestamp":"1640595240.0"},{"comments":[{"timestamp":"1704257220.0","content":"No. The answer is A. \nAuto backup is actually 35 days as maximum. You should manually backup for 90 days. Then you can two choices to control the manual backup (or snapshot) which you can define longer retention period than 35 days of RDS automated backup.\n- use AWS Backup so that you make a schedule to create a manual snapshot\n- use the Lambda Function to make a schedule to create a manual snapshot\nOf course, AWS Backup is easier than implements of Lambda.","comment_id":"1112500","poster":"Hisayuki","upvote_count":"1"}],"comment_id":"1054271","content":"Selected Answer: D\nnot A because, AWS backup has a 35 day retention policy","timestamp":"1698292740.0","poster":"KikiNoviandi","upvote_count":"2"},{"content":"Selected Answer: A\nA. Use AWS Backup to build a backup plan for the required retention period. Assign the DB instances to the backup plan.","timestamp":"1694495340.0","comment_id":"1005401","poster":"Pranava_GCP","upvote_count":"2"},{"upvote_count":"1","timestamp":"1691440320.0","comments":[{"comment_id":"1005399","timestamp":"1694495280.0","poster":"Pranava_GCP","upvote_count":"3","content":"AWS Backup can retain snapshots between 1 day and 100 years (or indefinitely, if you do not enter a retention period), and continuous backups between 1 and 35 days."}],"content":"Can't believe so many votes for A. AWS backup also has a 35 day retention policy and the same page references snapshots. A is 100% incorrect\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/point-in-time-recovery.html","poster":"CallMeHerb","comment_id":"974960"},{"poster":"MrAliMohsan","upvote_count":"2","timestamp":"1685095500.0","content":"Selected Answer: A\nNot Selecting D Since it is more development effort.","comment_id":"907274"},{"upvote_count":"2","timestamp":"1682343120.0","comment_id":"879380","content":"Ans: D","poster":"yyy"},{"content":"max limit for automated backups is 35 days hene manual snapshot is solution\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupRetention","upvote_count":"2","timestamp":"1658024700.0","comment_id":"632432","poster":"Chirantan"},{"content":"You can set the backup retention period when you create a DB instance. If you don't set the backup retention period, the default backup retention period is one day if you create the DB instance using the Amazon RDS API or the AWS CLI. The default backup retention period is seven days if you create the DB instance using the console.\n\nAfter you create a DB instance, you can modify the backup retention period. You can set the backup retention period to between 0 and 35 days. Setting the backup retention period to 0 disables automated backups. Manual snapshot limits (100 per Region) do not apply to automated backups.\nmax limit for automated backups is 35 days hen e manual snapshot is solution \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupRetention","poster":"Chirantan","upvote_count":"1","timestamp":"1657428120.0","comment_id":"629432"},{"upvote_count":"2","timestamp":"1655768760.0","comment_id":"619501","poster":"megadba","content":"RDS autobackup's retention period is max 35days"},{"timestamp":"1651249260.0","upvote_count":"4","content":"Selected Answer: A\nAWS Backup service + retention period set","poster":"novice_expert","comment_id":"594588"},{"content":"Selected Answer: A\nAgree with other comments","upvote_count":"2","comment_id":"554988","poster":"tugboat","timestamp":"1645663020.0"},{"comment_id":"502154","poster":"mnzsql365","content":"A for me - AWS Backup Service solution","upvote_count":"1","timestamp":"1639573500.0"},{"content":"A for me\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html \n \nB is wrong , because Automated backups with unsupported MySQL storage engines .","comment_id":"497816","upvote_count":"1","poster":"nood","timestamp":"1639061880.0"},{"poster":"Justu","comment_id":"490553","content":"Automatic backups are only for max 35 days","timestamp":"1638265380.0","upvote_count":"2"},{"content":"A https://aws.amazon.com/getting-started/hands-on/amazon-rds-backup-restore-using-aws-backup/","timestamp":"1636805640.0","comment_id":"477433","poster":"hemantr","upvote_count":"2"}],"timestamp":"2021-11-13 13:14:00","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/65935-exam-aws-certified-database-specialty-topic-1-question-150/","answers_community":["A (83%)","D (17%)"],"answer":"A","answer_description":"","topic":"1"},{"id":"0sHYCgoD5mB6syLnMyc0","discussion":[{"poster":"RotterDam","timestamp":"1646573520.0","content":"Selected Answer: BC\nBC is the correct Answer.\nA is NOT an issue. I've tested this by creating a different table name in a destination region with TWO GSIs one with the exact same name and indexes and one with a different name and index than the original table. GSIs are table specific. The Key constraint is a table with the same name should NOT exist and permissions to access should be there","comment_id":"562036","upvote_count":"10"},{"comment_id":"595653","poster":"novice_expert","comments":[{"upvote_count":"1","content":"Yes, A and C.","timestamp":"1653032340.0","comment_id":"604278","comments":[{"poster":"JasonZhu","timestamp":"1696091400.0","comments":[{"timestamp":"1702708920.0","comment_id":"1097961","poster":"jitesh_k","content":"B is not an issue. \n\"None of the new or existing replica tables in the global table can contain any data.\"\nfrom: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html","upvote_count":"1"}],"upvote_count":"1","comment_id":"1021701","content":"BC for me.\nA is not right. As long as same index_name don't exist & it will create new 2nd index.\nB. it can't create a new table if the table with the same name is existed."}],"poster":"khchan123"}],"upvote_count":"7","timestamp":"1651421220.0","content":"Selected Answer: AC\nA. A global secondary index with the same partition key but a different sort key exists in the new Region where replication is desired.\nIf global secondary indexes are specified, then the following conditions must also be met:\nThe global secondary indexes must have the same name.\nThe global secondary indexes must have the same hash key and sort key (if present).\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateGlobalTable.html\n\nThe table must have the same name as all of the other replicas. (means B is out)\nC. No role with the dynamodb:CreateGlobalTable permission exists in the account.\nTo create and maintain global tables in DynamoDB, you must have the dynamodb:CreateGlobalTable permission to access each of the following:\nThe replica table that you want to add.\nEach existing replica that's already part of the global table.\nThe global table itself.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/gt_IAM.html"},{"upvote_count":"1","comment_id":"1159560","timestamp":"1708940700.0","content":"AC\n\n\"Requirements for adding a new replica table\"\nIf you want to add a new replica table to a global table, each of the following conditions must be true:\nThe table must have the same partition key as all of the other replicas.\nThe table must have the same write capacity management settings specified.\nThe table must have the same name as all of the other replicas.\nThe table must have DynamoDB Streams enabled, with the stream containing both the new and the old images of the item.\nNone of the new or existing replica tables in the global table can contain any data.\n\nIf global secondary indexes are specified, the following conditions must also be met:\nThe global secondary indexes must have the same name.\nThe global secondary indexes must have the same partition key and sort key (if present).\n\nhttps://docs.aws.amazon.com/ja_jp/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html","poster":"kyo"},{"content":"Selected Answer: AC\nAC\nAn empty table is not a problem","poster":"MultiAZ","upvote_count":"1","comment_id":"1123535","timestamp":"1705338180.0"},{"upvote_count":"1","timestamp":"1700715720.0","poster":"yogitadb","comment_id":"1078073","content":"To create and maintain global tables in DynamoDB, you must have the dynamodb:CreateGlobalTable permission to access each of the following:\n\nThe global secondary indexes must have the same partition key and sort key (if present).\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html#globaltables_reqs_bestpractices.requirements"},{"upvote_count":"1","content":"Selected Answer: AC\nAccording doc: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html#globaltables_reqs_bestpractices.requirements\n\n1. The global secondary indexes must have the same partition key and sort key (if present). . Thus, A\n\nB: Doc: None of the new or existing replica tables in the global table can contain any data.\nThus, empty table with same name is OK","poster":"pek77","comment_id":"1038724","timestamp":"1696860180.0"},{"upvote_count":"1","comment_id":"945621","timestamp":"1688731620.0","content":"BC: GSI allow distinct combination of partition and sort key so A is not a problem. D and E are not an issue.","poster":"sju"},{"poster":"tsk9921","content":"BC for me","comment_id":"893474","timestamp":"1683672060.0","upvote_count":"1"},{"poster":"sk1974","timestamp":"1678116720.0","content":"Why not B & E . The CMK are region specific. There is no mention that there the DB specialst has the CMK for the new region --https://aws.amazon.com/about-aws/whats-new/2020/11/encrypt-your-amazon-dynamodb-global-tables-by-using-your-own-encryption-keys/","upvote_count":"1","comment_id":"830953","comments":[{"timestamp":"1695561720.0","poster":"Germaneli","upvote_count":"1","content":"A CMK is not a hinderance, but a prerequisite for expanding into the new region.\nAn Amazon owned key would not be possible because it cannot be shared across regions.","comment_id":"1015817"}]},{"timestamp":"1667082420.0","upvote_count":"1","content":"Anser AB\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html\nRequirements for adding a new replica table\nIf you want to add a new replica table to a global table, each of the following conditions must be true:\n\nThe table must have the same partition key as all of the other replicas.\n\nThe table must have the same write capacity management settings specified.\n\nThe table must have the same name as all of the other replicas.\n\nThe table must have DynamoDB Streams enabled, with the stream containing both the new and the old images of the item.\n\nNone of the new or existing replica tables in the global table can contain any data.\n\nIf global secondary indexes are specified, the following conditions must also be met:\n\nThe global secondary indexes must have the same name.\n\nThe global secondary indexes must have the same partition key and sort key (if present).","comment_id":"707521","poster":"rags1482","comments":[{"comment_id":"731020","content":"B is wrong as indicated \"None of the new or existing replica tables in the global table can contain any data.\" so \"An empty table with the same name exists in the new Region where replication is desired\" should be acceptable.","poster":"miles_chong","upvote_count":"2","timestamp":"1669774980.0"}]},{"comment_id":"681053","content":"A. No issue : as long as same index_name don't exist & it will create new 2nd index","poster":"Jiang_aws1","comments":[{"timestamp":"1665368520.0","content":"Question asked : \"creating a global table\" so \"A\" not related ( about indexes etc )","comment_id":"690658","upvote_count":"1","poster":"Jiang_aws1"}],"upvote_count":"1","timestamp":"1664302560.0"},{"comment_id":"540582","poster":"soyyodario","comments":[{"content":"Not only does the console check it, but the AWS CLI fails to enable replication if there is a pre-existing table in the replica region.\n\naws dynamodb update-table --table-name my-table --cli-input-json \\\n'{\n \"ReplicaUpdates\":\n [\n {\n \"Create\": {\n \"RegionName\": \"eu-central-1\"\n }\n }\n ]\n}' \\\n--region=eu-west-3\n\nAn error occurred (ValidationException) when calling the UpdateTable operation:\nFailed to create a the new replica of table with name: ‘my-table’ because one o\n more replicas already existed as tables.","upvote_count":"1","timestamp":"1684735860.0","comment_id":"903762","poster":"aviathor"}],"upvote_count":"4","timestamp":"1644000120.0","content":"Selected Answer: BC\nB and C\nC: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/gt_IAM.html\nTo create and maintain global tables in DynamoDB, you must have the dynamodb:CreateGlobalTable permission to access.\nB: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html\nThe console checks to ensure that a table with the same name doesn't exist in the selected Region. If a table with the same name does exist, you must delete the existing table before you can create a new replica table in that Region."},{"comment_id":"511120","timestamp":"1640697240.0","poster":"Shunpin","upvote_count":"2","content":"Selected Answer: AB\n1. No role with DynamoDB, but Account with CreateGlobalTable permiession, so no issues to create Global table.\n2. Change Sort key when replicate a table - don't think so\n3. Name exists in another region - Error pop-up."},{"comment_id":"510787","timestamp":"1640664720.0","upvote_count":"1","poster":"Dantehilary","content":"BC!!!!!"},{"timestamp":"1640479020.0","content":"B,C\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html\nFrom the Available replication Regions dropdown, choose US West (Oregon).\n\nThe console checks to ensure that a table with the same name doesn't exist in the selected Region. If a table with the same name does exist, you must delete the existing table before you can create a new replica table in that Region.","upvote_count":"2","comment_id":"509353","poster":"SMAZ"},{"comments":[{"content":"You are right","upvote_count":"2","comment_id":"483801","poster":"johnconnor","timestamp":"1637545680.0"}],"comment_id":"480666","poster":"Sp230","content":"Should be A and C","upvote_count":"3","timestamp":"1637236560.0"},{"upvote_count":"1","comment_id":"477677","content":"Options A and B.","poster":"leunamE","timestamp":"1636826940.0"}],"answer":"BC","choices":{"B":"An empty table with the same name exists in the Region where replication is desired.","E":"The table is encrypted using a KMS customer managed key.","D":"DynamoDB Streams is enabled for the table.","C":"No role with the dynamodb:CreateGlobalTable permission exists in the account.","A":"A global secondary index with the same partition key but a different sort key exists in the new Region where replication is desired."},"answer_images":[],"unix_timestamp":1636826940,"answer_description":"","timestamp":"2021-11-13 19:09:00","isMC":true,"question_text":"An online advertising company uses an Amazon DynamoDb table as its data store. The table has Amazon DynamoDB Streams enabled and has a global secondary index on one of the keys. The table is encrypted using an AWS Key Management Service (AWS KMS) customer managed key.\nThe company has decided to expand its operations globally and wants to replicate the database in a different AWS Region by using DynamoDB global tables.\nUpon review, an administrator notices the following:\n✑ No role with the dynamodb: CreateGlobalTable permission exists in the account.\n✑ An empty table with the same name exists in the new Region where replication is desired.\n✑ A global secondary index with the same partition key but a different sort key exists in the new Region where replication is desired.\nWhich configurations will block the creation of a global table or the creation of a replica in the new Region? (Choose two.)","question_id":59,"url":"https://www.examtopics.com/discussions/amazon/view/65971-exam-aws-certified-database-specialty-topic-1-question-151/","question_images":[],"answer_ET":"BC","topic":"1","answers_community":["BC (56%)","AC (36%)","8%"],"exam_id":22},{"id":"mJepQQYsTBWUCUmLyxxm","topic":"1","question_images":[],"choices":{"C":"Enable AWS CloudTrail logs on the table. Create an AWS Lambda function that reads the log files once an hour and filters DynamoDB API actions. Write the filtered log files to Amazon S3.","D":"Enable Amazon DynamoDB Streams on the table. Create an AWS Lambda function triggered by the stream. Write the log entries to an Amazon Kinesis Data Firehose delivery stream with buffering and Amazon S3 as the destination.","A":"Enable Amazon DynamoDB Streams on the table. Create an AWS Lambda function triggered by the stream. Write the log entries to an Amazon S3 object.","B":"Create a backup plan in AWS Backup to back up the DynamoDB table once a day. Create an AWS Lambda function that restores the backup in another table and compares both tables for changes. Generate the log entries and write them to an Amazon S3 object."},"question_text":"A large automobile company is migrating the database of a critical financial application to Amazon DynamoDB. The company's risk and compliance policy requires that every change in the database be recorded as a log entry for audits. The system is anticipating more than 500,000 log entries each minute. Log entries should be stored in batches of at least 100,000 records in each file in Apache Parquet format.\nHow should a database specialist implement these requirements with DynamoDB?","url":"https://www.examtopics.com/discussions/amazon/view/66017-exam-aws-certified-database-specialty-topic-1-question-152/","answer":"D","answer_ET":"D","exam_id":22,"answer_images":[],"discussion":[{"upvote_count":"7","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/big-data/streaming-amazon-dynamodb-data-into-a-centralized-data-lake/\n- Kinesis Data Firehose – Kinesis Data Firehose helps to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3 and other destinations. It’s a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt your data streams before loading, which minimizes the amount of storage used and increases security.","comment_id":"555579","timestamp":"1645738140.0","poster":"tugboat"},{"upvote_count":"1","comment_id":"1015826","poster":"Germaneli","timestamp":"1695562080.0","content":"Selected Answer: D\nB+C are distractors (\"once an hour\", \"once a day\" for 500000 entries *per minute* cannot work).\nA is difficult to achieve since a Lambda function is limited to 15 mins and there is no buffering. D provides this."},{"upvote_count":"1","comment_id":"893473","timestamp":"1683672000.0","poster":"tsk9921","content":"D makes sense to me."},{"content":"D make sense.","comment_id":"748572","poster":"khun","timestamp":"1671332400.0","upvote_count":"1"},{"timestamp":"1651258020.0","upvote_count":"1","comment_id":"594649","poster":"novice_expert","content":"Selected Answer: D\nDynamoDB Streams -> Lambda function ->log entries Kinesis Data Firehose delivery stream with buffering -> S3"},{"content":"Should be A as per https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html","poster":"CloudGuru99","comment_id":"527032","timestamp":"1642543080.0","comments":[{"poster":"user0001","comment_id":"548784","timestamp":"1645030680.0","content":"with A you cant batch them as per the requirements ,with D you can","upvote_count":"3","comments":[{"comment_id":"629419","upvote_count":"2","content":"You can write lambda to start writing to new S3 object ( file ) after it has writter 10000 records to it. Option A is more reasonable .","poster":"sachin","timestamp":"1657422840.0"}]}],"upvote_count":"1"},{"upvote_count":"4","comment_id":"492183","content":"DDDDDDDD","timestamp":"1638426300.0","poster":"Dantehilary"},{"upvote_count":"2","comment_id":"486351","poster":"jove","content":"Option D","timestamp":"1637800920.0"},{"comment_id":"485878","poster":"GMartinelli","timestamp":"1637752020.0","content":"Selected Answer: D\nOption D","upvote_count":"3"},{"timestamp":"1636897620.0","content":"Option D.","upvote_count":"2","comment_id":"478143","poster":"leunamE"}],"isMC":true,"answer_description":"","unix_timestamp":1636897620,"timestamp":"2021-11-14 14:47:00","answers_community":["D (100%)"],"question_id":60}],"exam":{"isImplemented":true,"name":"AWS Certified Database - Specialty","id":22,"provider":"Amazon","isMCOnly":false,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":359},"currentPage":12},"__N_SSP":true}