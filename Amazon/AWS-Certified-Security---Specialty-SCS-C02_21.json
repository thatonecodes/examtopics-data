{"pageProps":{"questions":[{"id":"nWEN3zaGVzoHx7Kkp3g5","discussion":[{"comments":[{"upvote_count":"1","comment_id":"1214786","timestamp":"1716277200.0","poster":"khuman_12","content":"There is nothing about \"access logs\" in the question."},{"timestamp":"1710978600.0","upvote_count":"1","comment_id":"1178855","content":"I think this is the key to the question -\nsave ELB access logs\nTo save ELB access logs, you can:\nCreate an S3 bucket\nAttach a policy to the S3 bucket that allows Elastic Load Balancing to write the logs to the bucket\nConfigure access logs to capture and deliver log files to the S3 bucket\nVerify bucket permissions","poster":"hro"}],"upvote_count":"13","poster":"Ghe","content":"Selected Answer: C\nYou can't send ELB access logs into CloudWatch Logs, but to S3 only:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/enable-access-logging.html\n\nRegarding the alarm, natively there is no way to use query result as a metric.We could use a Lambda Function for this.\n\nC remains the most valid option.","comment_id":"1175054","timestamp":"1710603180.0"},{"comment_id":"1076364","comments":[{"comment_id":"1083117","upvote_count":"3","content":"No, if you look at the reqs. of this question, it specifically asks for this query:\n\"All the load balancer logs are centralized and searchable for auditing\"..\nSo if you select A for CloudWatch Log groups, it has the default retention policy set. After which it will clear off all the saved logs!... so how would you be able to do the audit on the logs after 14 days lets say??\nThat's why I'm going with Option C here..","poster":"Aamee","timestamp":"1701234000.0","comments":[{"content":"By default, cloudWatch log retention is indefinite unless you set it to limited duration due to audit requirement.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html#SettingLogRetention","poster":"Daniel76","upvote_count":"4","comment_id":"1110984","timestamp":"1704091200.0"},{"timestamp":"1704091560.0","content":"However i tend to agree it is C after reviewing this qn: enable access log can keep logs in s3 where you can search for ssl_cipher string using SQL like query:\nhttps://docs.aws.amazon.com/athena/latest/ug/application-load-balancer-logs.html\nyou can indeed publish athena query metrics to cloudwatch by enabling the option.\nhttps://docs.aws.amazon.com/athena/latest/ug/query-metrics-viewing.html","comment_id":"1110986","poster":"Daniel76","comments":[{"poster":"03beafc","content":"The article you posted is about the metrics behind the athena query, nothing there about generating metrics from an athena query. Answer is A","upvote_count":"3","comment_id":"1162118","timestamp":"1709170740.0"}],"upvote_count":"3"}]}],"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html#view-metric-data\ns3 buckets and athena are not needed.","upvote_count":"12","timestamp":"1700576580.0","poster":"Daniel76"},{"timestamp":"1725286860.0","poster":"hb0011","content":"Selected Answer: C\nThe requirement for ciphers eliminates option A because you need ELB access logs which requires s3.","upvote_count":"2","comment_id":"1276801"},{"poster":"FunkyFresco","upvote_count":"1","content":"Selected Answer: C\nC fits with the requirements.","timestamp":"1724263740.0","comment_id":"1270328"},{"poster":"shailvardhan","timestamp":"1716917460.0","upvote_count":"1","comment_id":"1220455","content":"Selected Answer: C\nC because elb access logs can not be sent directly to CloudWatch"},{"upvote_count":"1","comment_id":"1213269","poster":"Just_Ninja","timestamp":"1716030360.0","content":"Selected Answer: C\nAmazon Application Load Balancers (ALBs) cannot send logs directly to Amazon CloudWatch Logs. ALB access logs can be sent to Amazon S3 buckets for storage and analysis. https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html"},{"timestamp":"1714115700.0","comments":[{"comment_id":"1220456","poster":"shailvardhan","content":"but ELB access logs can not be sent directly to CloudWarch.","upvote_count":"1","timestamp":"1716917520.0"}],"poster":"liuyomz","comment_id":"1202432","content":"Selected Answer: A\nIm between A and C, because probably A is less efficiente because searching through Athena is better? But i dont know, A still seems more correct.","upvote_count":"1"},{"poster":"CloudHell","upvote_count":"2","comment_id":"1196935","content":"Selected Answer: C\nC for sure.","timestamp":"1713318540.0"},{"content":"C. ALB only push directly to S3 + Amazon CloudWatch filters only for log on CW log stream not on S3","upvote_count":"1","comment_id":"1189141","timestamp":"1712213700.0","poster":"Sodev"},{"upvote_count":"2","comment_id":"1147273","timestamp":"1707660060.0","content":"B is not correct choice\nOption B involves using S3 for storage and Athena for searching, but it suggests creating CloudWatch filters on S3 log files, which isn't directly possible as CloudWatch filters work on logs stored in CloudWatch Logs, not on S3.","poster":"PareshBPatel"},{"upvote_count":"1","timestamp":"1707659940.0","content":"C\nFor ensuring centralized and searchable logging for auditing purposes after transitioning to AWS Elastic Load Balancers, and for generating metrics to show which ciphers are in use, the most effective solution among the provided options is:\n\nC. Create an Amazon S3 bucket. Configure the load balancers to send logs to the S3 bucket. Use Amazon Athena to search the logs that are in the S3 bucket. Create Athena queries for the required metrics. Publish the metrics to Amazon CloudWatch.","comment_id":"1147271","poster":"PareshBPatel"},{"timestamp":"1707524700.0","content":"Selected Answer: B\nWeird set of answers. Mixing between access logs and performance metrics.\nCheck out this \nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html\n\nCloudWatch is responsible about collecting performance metrics.\nWhereas, access logs are captured and sent to S3. You can use these logs to analyze traffic patterns, but NOT TO QUERY METRICS using Athena (it does not make sense even).\n\nTherefore, the closest answer to correctness is B!","comment_id":"1145871","poster":"Raphaello","comments":[{"comment_id":"1156836","timestamp":"1708653480.0","upvote_count":"1","poster":"Raphaello","content":"Correct answer is C.\n\nAthena is in fact capable of query metrics and publish them to CloudWatch.\nhttps://docs.aws.amazon.com/athena/latest/ug/athena-cloudwatch-metrics-enable.html"}],"upvote_count":"2"},{"timestamp":"1705610100.0","comment_id":"1126176","upvote_count":"1","content":"A or C?? I choose C because I think that AWS ELB cant send logs to CloudWatch","poster":"smanzana"},{"poster":"Gafa255","timestamp":"1705607580.0","comment_id":"1126149","upvote_count":"1","content":"Selected Answer: C\nELB cant send log to CloudWatch."},{"timestamp":"1702979280.0","upvote_count":"1","content":"Exam on 2023-12-18","poster":"trashbox","comment_id":"1100486"},{"upvote_count":"1","timestamp":"1702435980.0","comment_id":"1095093","poster":"Raphaello","content":"Correct Answer is B.\nWe're talking about ELB access logs, not metrics, which always get forwarded to S3 bucket.\nFrom there one can use Athena for SQL querying."},{"poster":"[Removed]","timestamp":"1698806880.0","upvote_count":"1","comment_id":"1059331","content":"Selected Answer: C\nAnswer is C"},{"poster":"lalee2","comment_id":"1056650","content":"Selected Answer: C\nI think C is right","upvote_count":"1","timestamp":"1698565920.0"},{"comment_id":"1054796","upvote_count":"2","content":"Selected Answer: C\nAgree with C.","poster":"pupsik","timestamp":"1698341040.0"},{"timestamp":"1698285720.0","content":"Selected Answer: C\nThe Correct Answer should be C. \nA,D is wrong as - ELB cannot publish log directly to CloudWatch Log. \nB is wrong as - CloudWatch filter on S3 is not available. The filter is for CloudWatch Log.\n\n\nInstead you can publish query-related metrics to CloudWatch with custom widget https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/add_custom_widget_samples.html","comment_id":"1054232","poster":"bhui","upvote_count":"4"},{"comments":[{"poster":"100fold","timestamp":"1699297380.0","content":"Correction to answer C.","comment_id":"1064179","upvote_count":"2"}],"upvote_count":"1","poster":"100fold","content":"Thinking also A","timestamp":"1698196140.0","comment_id":"1053347"},{"comment_id":"1051959","timestamp":"1698071040.0","upvote_count":"1","poster":"Sumi81","content":"I think A"}],"exam_id":30,"unix_timestamp":1698071040,"answer":"C","choices":{"A":"Create an Amazon CloudWatch Logs log group. Configure the load balancers to send logs to the log group. Use the CloudWatch Logs console to search the logs. Create CloudWatch Logs filters on the logs for the required metrics.","C":"Create an Amazon S3 bucket. Configure the load balancers to send logs to the S3 bucket. Use Amazon Athena to search the logs that are in the S3 bucket. Create Athena queries for the required metrics. Publish the metrics to Amazon CloudWatch.","B":"Create an Amazon S3 bucket. Configure the load balancers to send logs to the S3 bucket. Use Amazon Athena to search the logs that are in the S3 bucket. Create Amazon CloudWatch filters on the S3 log files for the required metrics.","D":"Create an Amazon CloudWatch Logs log group. Configure the load balancers to send logs to the log group. Use the AWS Management Console to search the logs. Create Amazon Athena queries for the required metrics. Publish the metrics to Amazon CloudWatch."},"answer_ET":"C","question_id":101,"url":"https://www.examtopics.com/discussions/amazon/view/124456-exam-aws-certified-security-specialty-scs-c02-topic-1/","timestamp":"2023-10-23 16:24:00","question_images":[],"answer_images":[],"topic":"1","answer_description":"","isMC":true,"question_text":"A company used a lift-and-shift approach to migrate from its on-premises data centers to the AWS Cloud. The company migrated on-premises VMs to Amazon EC2 instances. Now the company wants to replace some of components that are running on the EC2 instances with managed AWS services that provide similar functionality.\nInitially, the company will transition from load balancer software that runs on EC2 instances to AWS Elastic Load Balancers. A security engineer must ensure that after this transition, all the load balancer logs are centralized and searchable for auditing. The security engineer must also ensure that metrics are generated to show which ciphers are in use.\nWhich solution will meet these requirements?","answers_community":["C (66%)","A (30%)","5%"]},{"id":"CRhMjKZk1gfw5eRBso0E","answer_images":[],"question_images":[],"choices":{"D":"Encrypt the secrets in us-east-1 by using a customer managed KMS key. Replicate the secrets to us-west-1. Encrypt the secrets in us-west-1 by using the customer managed KMS key from us-east-1.","B":"Encrypt the secrets in us-east-1 by using an AWS managed KMS key. Configure resources in us-west-1 to call the Secrets Manager endpoint in us-east-1.","A":"Encrypt the secrets in us-east-1 by using an AWS managed KMS key. Replicate the secrets to us-west-1. Encrypt the secrets in us-west-1 by using a new AWS managed KMS key in us-west-1.","C":"Encrypt the secrets in us-east-1 by using a customer managed KMS key. Configure resources in us-west-1 to call the Secrets Manager endpoint in us-east-1."},"topic":"1","question_text":"A company runs workloads in the us-east-1 Region. The company has never deployed resources to other AWS Regions and does not have any multi-Region resources. The company needs to replicate its workloads and infrastructure to the us-west-1 Region.\n\nA security engineer must implement a solution that uses AWS Secrets Manager to store secrets in both Regions. The solution must use AWS Key Management Service (AWS KMS) to encrypt the secrets. The solution must minimize latency and must be able to work if only one Region is available.\n\nThe security engineer uses Secrets Manager to create the secrets in us-east-1.\n\nWhat should the security engineer do next to meet the requirements?","timestamp":"2024-10-03 22:22:00","discussion":[{"comment_id":"1335923","timestamp":"1735887780.0","poster":"TareDHakim","content":"Selected Answer: D\nthe question doesn't specify the type of KMS key (aws managed or customer managed), since AWS managed KMS keys are region-specific, option A won't work for cross-region replication of secrets. The correct solution is Option D, which uses a customer managed KMS key that can be replicated across multiple regions, fulfilling the requirements of cross region replicas.","upvote_count":"1"},{"content":"Selected Answer: A\nAWS managed key is less complicated","upvote_count":"1","timestamp":"1735839060.0","poster":"Asma2023","comment_id":"1335677"},{"poster":"SCSC02Q","content":"Selected Answer: A\nSince solution must work if a region is down.","timestamp":"1735488660.0","comment_id":"1333588","upvote_count":"1"},{"comment_id":"1306482","upvote_count":"2","content":"Selected Answer: A\nThe question does not state that a multi-region key is used.\nKMS is usually a regional service.\n\"must be able to work if only one Region is available\"","poster":"koo_kai","timestamp":"1730630280.0"},{"comment_id":"1301321","timestamp":"1729554060.0","content":"Selected Answer: D\nBy using a customer managed KMS key, you ensure that the same key is used for encryption in both Regions, maintaining consistency and security. Replicating the secrets ensures that they are available in both Regions, reducing latency and providing redundancy in case one Region becomes unavailable.","poster":"dhewa","upvote_count":"1"},{"comment_id":"1298979","upvote_count":"1","timestamp":"1729118160.0","poster":"gkaself","content":"Selected Answer: D\nD is correct Answer"},{"timestamp":"1727986920.0","poster":"mikelord","upvote_count":"3","comment_id":"1292926","content":"Selected Answer: D\nD should be the right answer."}],"unix_timestamp":1727986920,"question_id":102,"isMC":true,"answer_ET":"D","exam_id":30,"answers_community":["D (60%)","A (40%)"],"answer":"D","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/148616-exam-aws-certified-security-specialty-scs-c02-topic-1/"},{"id":"tHY6qbgSJcEOdLhej7DT","question_images":[],"answer_ET":"B","exam_id":30,"answer":"B","timestamp":"2024-10-06 16:28:00","answer_images":[],"choices":{"A":"Make the following changes to NACL3:\n• Add a rule that allows inbound traffic on port 5432 from NACL2.\n• Add a rule that allows outbound traffic on ports 1024-65536 to NACL2.\n• Remove the default rules that allow all inbound and outbound traffic.","C":"Make the following changes to NACL2:\n• Add a rule that allows outbound traffic on port 5432 to the CIDR blocks of the RDS subnets.\n• Remove the default rules that allow all inbound and outbound traffic.","B":"Make the following changes to NACL3:\n• Add a rule that allows inbound traffic on port 5432 from the Cl DR blocks of the application instance subnets.\n• Add a rule that allows outbound traffic on ports 1024-65536 to the application instance subnets.\n• Remove the default rules that allow all inbound and outbound traffic.","D":"Make the following changes to NACL2:\n• Add a rule that allows inbound traffic on port 5432 from the CIDR blocks of the RDS subnets.\n• Add a rule that allows outbound traffic on port 5432 to the RDS subnets."},"unix_timestamp":1728224880,"question_text":"A company operates a web application that runs on Amazon EC2 instances. The application listens on port 80 and port 443. The company uses an Application Load Balancer (ALB) with AWS WAF to terminate SSL and to forward traffic to the application instances only on port 80.\n\nThe ALB is in public subnets that are associated with a network ACL that is named NACL1. The application instances are in dedicated private subnets that are associated with a network ACL that is named NACL2. An Amazon RDS for PostgreSQL DB instance that uses port 5432 is in a dedicated private subnet that is associated with a network ACL that is named NACL3. All the network ACLs currently allow all inbound and outbound traffic.\n\nWhich set of network ACL changes will increase the security of the application while ensuring functionality?","topic":"1","answers_community":["B (75%)","C (25%)"],"url":"https://www.examtopics.com/discussions/amazon/view/148754-exam-aws-certified-security-specialty-scs-c02-topic-1/","discussion":[{"timestamp":"1742009220.0","content":"Selected Answer: B\nA: Referencing NACL2 (instead of CIDR blocks) is invalid—NACL rules require IP ranges, not ACL identifiers.\nC/D: Modifying NACL2 (application subnet) is unnecessary here. The ALB already forwards traffic to port 80 on the EC2 instances, which is managed by security groups. NACL2’s default allow-all rules are already sufficient for ALB-to-EC2 traffic.","comment_id":"1395773","upvote_count":"1","poster":"molerowan"},{"content":"Selected Answer: B\nWhy is B the Best Choice?\nRestricts Database Access (RDS - NACL3)\n\nThe RDS instance only needs to accept traffic from the application instances (NACL2) on port 5432 (PostgreSQL).\nThis prevents unauthorized access from other sources.\nEnsures Proper Response Traffic Flow\n\nPostgreSQL replies on ephemeral ports (1024-65536), so outbound traffic from NACL3 to NACL2 must be allowed for the connection to function.\nRemoves Open Access\n\nThe default allow-all rules are removed, improving security.\nOnly necessary inbound and outbound traffic is permitted.","upvote_count":"1","comment_id":"1349625","timestamp":"1738346880.0","poster":"Pat9595"},{"timestamp":"1736298480.0","content":"Selected Answer: C\nAnswer is C.\n\nBy default, AWS creates NACLs that allow all inbound and outbound traffic. To improve security, it is recommended to restrict access to only necessary traffic.\n\nThere is no need for DB subnet to open a broad range of ports. \n\nAnother problem with B is, how would you protect the application server if the NACL rule still allows all traffic? The question asked is to protect the application.","poster":"youonebe","comment_id":"1337773","upvote_count":"1"},{"poster":"TareDHakim","comment_id":"1336183","upvote_count":"1","content":"Selected Answer: B\nB. database will allow access from/to application subnet only","timestamp":"1735944840.0"},{"content":"I think B is correct. Inbound 5432 is allowed and outbound for ephemeral ports. Answer A is wrong as you can't use other NACL as a source.","poster":"maciekmacku","upvote_count":"2","comment_id":"1293876","timestamp":"1728224880.0"}],"answer_description":"","question_id":103,"isMC":true},{"id":"XaFVzMFbqqjHvOX09DQu","question_text":"AWS CloudTrail is being used to monitor API calls in an organization. An audit revealed that CloudTrail is failing to deliver events to Amazon S3 as expected.\n\nWhat initial actions should be taken to allow delivery of CloudTrail events to S3? (Choose two.)","question_images":[],"timestamp":"2024-10-04 17:31:00","answer_description":"","answers_community":["AD (86%)","14%"],"url":"https://www.examtopics.com/discussions/amazon/view/148648-exam-aws-certified-security-specialty-scs-c02-topic-1/","isMC":true,"answer":"AD","answer_images":[],"choices":{"A":"Verify that the S3 bucket policy allows CloudTrail to write objects.","C":"Remove any lifecycle policies on the S3 bucket that are archiving objects to S3 Glacier Flexible Retrieval.","E":"Verify that the log file prefix defined in CloudTrail exists in the S3 bucket.","B":"Verify that the IAM role used by CloudTrail has access to write to Amazon CloudWatch Logs.","D":"Verify that the S3 bucket defined in CloudTrail exists."},"unix_timestamp":1728055860,"answer_ET":"AD","topic":"1","question_id":104,"exam_id":30,"discussion":[{"timestamp":"1736298780.0","content":"Selected Answer: AD\nWhile the log file prefix is important for organizing logs within the S3 bucket, the prefix does not need to exist beforehand. CloudTrail will automatically create the necessary directories (based on the prefix) in the S3 bucket when logs are delivered. The existence of the prefix itself is not a critical requirement for the delivery of CloudTrail logs.","upvote_count":"2","comment_id":"1337775","poster":"youonebe"},{"content":"Selected Answer: AD\nTo address the issue of CloudTrail failing to deliver events to Amazon S3, the initial actions you should take are:\n\nA. Verify that the S3 bucket policy allows CloudTrail to write objects.\n\nD. Verify that the S3 bucket defined in CloudTrail exists.\n\nThese steps ensure that CloudTrail has the necessary permissions to write logs to the S3 bucket and that the specified bucket is correctly set up and accessible","upvote_count":"1","poster":"IPLogic","comment_id":"1321559","timestamp":"1733261460.0"},{"upvote_count":"1","poster":"723993f","comment_id":"1318355","timestamp":"1732668420.0","content":"Selected Answer: AD\nobvious"},{"comment_id":"1317973","content":"Selected Answer: AD\nAD. Prefix is optional when creating a trail","timestamp":"1732609140.0","upvote_count":"1","poster":"J0_e"},{"timestamp":"1731465720.0","content":"Selected Answer: AE\n(E) According to this, log file prefixes should be configured correctly both in the bucket policy and in CTrail configuration.\nI am assuming if the bucket does not exist, the Trail configuration should fail prematurely. \n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/turn-on-cloudtrail-in-additional-accounts.html","comment_id":"1311044","poster":"daburahjail","upvote_count":"1"},{"content":"AE read the documentation","comment_id":"1303108","timestamp":"1729924440.0","upvote_count":"1","poster":"Xelnak"},{"upvote_count":"1","content":"Selected Answer: AD\nAD it is.","timestamp":"1729551360.0","comment_id":"1301301","poster":"dhewa"},{"comment_id":"1301053","timestamp":"1729519740.0","poster":"Bad_Mat","upvote_count":"1","content":"It's AD"},{"timestamp":"1728055860.0","upvote_count":"1","comment_id":"1293153","poster":"SkyBlueUS","content":"AE might be the right answer?"}]},{"id":"M3CqzEH4CQK0jGFDxCsJ","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/150024-exam-aws-certified-security-specialty-scs-c02-topic-1/","isMC":true,"question_id":105,"answer_description":"","answer_images":[],"discussion":[{"content":"Selected Answer: D\nACM natively integrates with EventBridge and emits events for expiring certificates. A predefined pattern for Certificate Approaching Expiration simplifies monitoring.","upvote_count":"1","poster":"TareDHakim","comment_id":"1336187","timestamp":"1735945620.0"},{"timestamp":"1729551600.0","upvote_count":"1","poster":"dhewa","content":"Selected Answer: D\nThis approach leverages the built-in ACM Certificate Approaching Expiration event, which is automatically generated by ACM when a certificate is nearing its expiration date1. By using Amazon EventBridge to capture this event and route it to an SNS topic, you can efficiently set up email notifications without the need for custom Lambda functions or manual checks.","comment_id":"1301303"}],"answer_ET":"D","exam_id":30,"choices":{"D":"Create an Amazon EventBridge rule by using a predefined pattern for ACM Choose the metric in the ACM Certificate Approaching Expiration event as the event pattern. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target.","C":"Set up AWS Security Hub. Turn on the AWS Foundational Security Best Practices standard with integrated ACM to send findings. Configure and use a custom action by creating a rule to match the pattern from the ACM findings on the NotBefore attribute as the event source. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target.","B":"Create an Amazon CloudWatch alarm. Add all the certificate ARNs in the AWS/CertificateManager namespace to the DaysToExpiry metric. Configure the alarm to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic when the value for the DaysToExpiry metric is less than or equal to 31.","A":"Create an AWS Lambda function to list all certificates and to go through each certificate to describe the certificate by using the AWS SDK. Filter on the NotAfter attribute and send an email notification. Use an Amazon EventBridge rate expression to schedule the Lambda function to run daily."},"answers_community":["D (100%)"],"unix_timestamp":1729551600,"answer":"D","question_text":"A company has public certificates that are managed by AWS Certificate Manager (ACM). The certificates are either imported certificates or managed certificates from ACM with mixed validation methods. A security engineer needs to design a monitoring solution to provide alerts by email when a certificate is approaching its expiration date.\n\nWhat is the MOST operationally efficient way to meet this requirement?","topic":"1","timestamp":"2024-10-22 01:00:00"}],"exam":{"name":"AWS Certified Security - Specialty SCS-C02","lastUpdated":"11 Apr 2025","provider":"Amazon","isImplemented":true,"isMCOnly":true,"id":30,"numberOfQuestions":288,"isBeta":false},"currentPage":21},"__N_SSP":true}