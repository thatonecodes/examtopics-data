{"pageProps":{"questions":[{"id":"lgi645CH9rapFdbGNmZ5","topic":"1","question_text":"A media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_description":"","answer_images":[],"answers_community":["B (96%)","4%"],"discussion":[{"comment_id":"991858","content":"Selected Answer: B\nPetabyte scale- Redshift","timestamp":"1693204320.0","poster":"beginnercloud","upvote_count":"13"},{"poster":"Berny","timestamp":"1674131160.0","content":"Selected Answer: B\nData ingestion through Kinesis data streams will require manual intervention to provide more shards as data size grows. Kinesis firehose will ingest data with the least operational overhead.","comment_id":"781116","upvote_count":"8"},{"upvote_count":"3","timestamp":"1710089760.0","comment_id":"1170473","poster":"scar0909","content":"Selected Answer: B\nKinesis data stream cannot detined to s3"},{"comments":[{"poster":"Rhydian25","content":"Copy-paste from A1975's answer","upvote_count":"1","comment_id":"1230813","timestamp":"1718437680.0"}],"upvote_count":"5","timestamp":"1700479800.0","poster":"Ruffyit","content":"1- Kinesis Data Stream provides a fully managed platform for custom data processing and analysis. Or we can say that used for custom data processing and analysis which required more manual intervention.\n2- Kinesis Data Firehose simplifies the delivery of streaming data to various destinations without the need for complex transformations.\nOption B is more suitable for the given scenario.","comment_id":"1075354"},{"upvote_count":"3","poster":"David_Ang","timestamp":"1698427260.0","content":"Selected Answer: B\nalways if you have a service that is meant for a specific job, it the correct answer, is logic. \"A\" is not good enough for this situation","comment_id":"1055757"},{"content":"Selected Answer: B\nB. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.","timestamp":"1694448240.0","comment_id":"1004968","poster":"Guru4Cloud","upvote_count":"3"},{"content":"Selected Answer: B\nPetabyte scale- Redshift","comment_id":"989630","poster":"NVenkatS","timestamp":"1692930780.0","upvote_count":"5"},{"poster":"A1975","content":"Selected Answer: B\n1- Kinesis Data Stream provides a fully managed platform for custom data processing and analysis. Or we can say that used for custom data processing and analysis which required more manual intervention.\n2- Kinesis Data Firehose simplifies the delivery of streaming data to various destinations without the need for complex transformations.\nOption B is more suitable for the given scenario.","comment_id":"972702","upvote_count":"3","timestamp":"1691213220.0"},{"upvote_count":"3","comment_id":"941630","timestamp":"1688373540.0","poster":"sickcow","content":"Selected Answer: B\nPetabyte Scale sounds like Redshift!"},{"poster":"cookieMr","upvote_count":"3","timestamp":"1687949580.0","comment_id":"936513","content":"Selected Answer: B\nB provides a fully managed and scalable solution for data ingestion and analytics. KDF simplifies the data ingestion process by automatically scaling to handle large volumes of streaming data. It can directly load the data into an Redshift cluster, which is a powerful and fully managed data warehousing solution.\n\nA. While Kinesis can handle streaming data, it requires additional processing to load the data into an analytics solution.\n\nC. Although S3 and Lambda can handle the storage and processing of data, it requires more manual configuration and management compared to the fully managed solution offered by KDF and Redshift.\n\nD. This option involves more operational overhead, as it requires managing and scaling the EC2 instances and RDS database infrastructure manually.\n\nTherefore, option B with KDF delivering the data to Redshift cluster offers the most streamlined and operationally efficient solution for ingesting and analyzing the user activity data in the given scenario."},{"comment_id":"932320","poster":"pisica134","timestamp":"1687592760.0","content":"petabytes in size => redshift","upvote_count":"3"},{"poster":"mattcl","timestamp":"1686615660.0","comment_id":"921842","content":"It's A. Data Stream is better in this case, and you can query data in S3 with Athena","comments":[{"comment_id":"925932","content":"Data Stream Can't write to S3. That's why B is only left correct answer.","comments":[{"timestamp":"1687784520.0","content":"Answer A… key phrase’ least operational overhead’ \nKDF can write to S3 … https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html","comment_id":"934475","upvote_count":"1","poster":"baba365"}],"upvote_count":"1","timestamp":"1687001940.0","poster":"Yadav_Sanjay"},{"poster":"JoeGuan","timestamp":"1692540180.0","comment_id":"985835","content":"https://aws.amazon.com/streaming-data/ a good explanation of either option. firehose appears to be an option for Least operational overhead, as the streams product requires some building of apps etc.","upvote_count":"2"}],"upvote_count":"2"},{"comment_id":"909999","timestamp":"1685430360.0","content":"Selected Answer: B\nOption B is correct answer.","upvote_count":"2","poster":"Bmarodi"},{"poster":"kruasan","timestamp":"1682776140.0","upvote_count":"7","comment_id":"884370","content":"Selected Answer: B\nThis solution meets the requirements as follows:\n• Kinesis Data Firehose can scale to ingest and process multiple terabytes per hour of streaming data. This can easily handle the petabyte-scale data volumes.\n• Firehose can deliver the data to Redshift, a petabyte-scale data warehouse, enabling on-demand SQL analytics of the data.\n• Redshift is a fully managed service, minimizing operational overhead. Firehose is also fully managed, handling scalability, availability, and durability of the streaming data ingestion."},{"poster":"gold4otas","upvote_count":"3","content":"Selected Answer: B\nB: The answer is certainly option \"B\" because ingesting user activity data can easily be handled by Amazon Kinesis Data streams. The ingested data can then be sent into Redshift for Analytics. \n\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. Amazon Redshift Serverless lets you access and analyze data without all of the configurations of a provisioned data warehouse. \n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html","timestamp":"1679935380.0","comment_id":"852284"},{"upvote_count":"2","timestamp":"1678893780.0","comments":[{"upvote_count":"2","timestamp":"1703739120.0","content":"Why that? Analytics is done in Redshift, not by Kinesis.","poster":"pentium75","comment_id":"1107415"}],"comment_id":"840009","poster":"GalileoEC2","content":"the Key sentence here is: \"that facilitates on-demand analytics\", tthats the reason because we need to choose Kinesis Data streams over Data Firehose"},{"comment_id":"783974","timestamp":"1674368760.0","content":"Selected Answer: B\nB: Kinesis Data Firehose service automatically load the data into Amazon Redshift and is a petabyte-scale data warehouse service. It allows you to perform on-demand analytics with minimal operational overhead. Since the requirement didn't state what kind of analytics you need to run, we can assume that we do not need to set up additional services to provide further analytics. Thus, it has the least operational overhead.\n\nWhy not A: It is a viable solution, but storing the data in S3 would require you to set up additional services like Amazon Redshift or Amazon Athena to perform the analytics.","upvote_count":"3","poster":"alexleely"},{"content":"Selected Answer: A\nI think the key word in the question is \"ingestion\"...whish is data stream\n\nData Streams is a low latency streaming service in AWS Kinesis with the facility for ingesting at scale. On the other hand, Kinesis Firehose aims to serve as a data transfer service. The primary purpose of Kinesis Firehose focuses on loading streaming data to Amazon S3, Splunk, ElasticSearch, and RedShift","comments":[{"timestamp":"1703739180.0","content":"\"The primary purpose of Kinesis Firehose focuses on loading streaming data to ... RedShift\", isn't this exactly the requirement here?","poster":"pentium75","upvote_count":"2","comment_id":"1107417"}],"poster":"mp165","upvote_count":"3","comment_id":"778064","timestamp":"1673891880.0"},{"poster":"Aninina","upvote_count":"4","comment_id":"777833","timestamp":"1673881500.0","content":"Selected Answer: B\npetabytes: redshift"},{"poster":"wmp7039","content":"Selected Answer: B\nAmazon Kinesis Data Firehose + Redshift meets the requirements","upvote_count":"2","comment_id":"777655","timestamp":"1673873640.0"},{"timestamp":"1673760060.0","upvote_count":"2","comment_id":"776177","content":"Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This allows you to use your data to gain new insights for your business and customers.\n\nThe first step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift cluster. After you provision your cluster, you can upload your data set and then perform data analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query performance using the same SQL-based tools and business intelligence applications that you use today.","poster":"venice1234"},{"poster":"venice1234","timestamp":"1673759940.0","comment_id":"776174","upvote_count":"3","content":"Selected Answer: B\nfor Analytics of Petabyte size data, it should be Redshift cluster"},{"upvote_count":"5","poster":"Parsons","content":"Selected Answer: B\nB is the correct answer.\nWe cannot ingest data from KDS to S3 => A is rollout.","comment_id":"775997","timestamp":"1673736840.0"},{"poster":"mhmt4438","content":"Selected Answer: B\nhttps://www.examtopics.com/discussions/amazon/view/83853-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"775812","upvote_count":"3","timestamp":"1673720280.0"},{"upvote_count":"3","content":"Selected Answer: B\nNo it's B","poster":"Morinator","timestamp":"1673598600.0","comment_id":"774240"}],"exam_id":31,"unix_timestamp":1673598600,"url":"https://www.examtopics.com/discussions/amazon/view/94985-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"answer_ET":"B","question_id":161,"timestamp":"2023-01-13 09:30:00","answer":"B","question_images":[],"choices":{"D":"Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database.","C":"Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.","B":"Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.","A":"Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket."}},{"id":"2fNhQIbno9sWG4triFSd","answer_ET":"AE","url":"https://www.examtopics.com/discussions/amazon/view/95312-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"exam_id":31,"answers_community":["AE (100%)"],"isMC":true,"unix_timestamp":1673720400,"answer_description":"","answer":"AE","answer_images":[],"topic":"1","choices":{"A":"Use AWS Glue to process the raw data in Amazon S3.","C":"Add more EC2 instances to accommodate the increasing amount of incoming data.","E":"Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3.","B":"Use Amazon Route 53 to route traffic to different EC2 instances.","D":"Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process the data."},"discussion":[{"timestamp":"1673737020.0","upvote_count":"15","poster":"Parsons","content":"Selected Answer: AE\nA, E is the correct answer\n\n\"RESTful web services\" => API Gateway.\n\"EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket\" => GLUE with (Extract - Transform - Load)","comment_id":"776000"},{"content":"Selected Answer: AE\nA. It automatically discovers the schema of the data and generates ETL code to transform it.\n\nE. API Gateway can be used to receive the raw data from the remote devices via RESTful web services. It provides a scalable and managed infrastructure to handle the incoming requests. The data can then be sent to an Amazon Kinesis data stream, which is a highly scalable and durable real-time data streaming service. From there, Amazon Kinesis Data Firehose can be configured to use the data stream as a source and deliver the transformed data to Amazon S3. This combination of services allows for the seamless ingestion and processing of data while minimizing operational overhead.\n\nB. It does not directly address the need for scalable data processing and storage. It focuses on managing DNS and routing traffic to different endpoints.\n\nC. Adding more EC2 can lead to increased operational overhead in terms of managing and scaling the instances.\n\nD. Using SQS and EC2 for processing data introduces more complexity and operational overhead.","poster":"cookieMr","upvote_count":"5","comment_id":"936514","timestamp":"1687949820.0"},{"poster":"Noveo","upvote_count":"2","timestamp":"1726224000.0","comment_id":"1283117","content":"AE breaks the original workflow \"receive raw data - process - store\" to \"receive - store- process - store again\" which leads to additional storage consuming (and thus money consuming)."},{"comment_id":"1075359","upvote_count":"2","timestamp":"1700480280.0","content":"A - Use AWS Glue to process the raw data in Amazon S3\nE - Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3","poster":"Ruffyit"},{"comment_id":"1013638","poster":"TariqKipkemei","timestamp":"1695361620.0","content":"Selected Answer: AE\nE then A no doubt.","upvote_count":"2"},{"comment_id":"1004966","upvote_count":"2","content":"Selected Answer: AE\nA. It automatically discovers the schema of the data and generates ETL code to transform it.\n\nE. API Gateway can be used to receive the raw data from the remote devices via RESTful web services. It provides a scalable and managed infrastructure to handle the incoming requests. The data can then be sent to an Amazon Kinesis data stream, which is a highly scalable and durable real-time data streaming service. From there, Amazon Kinesis Data Firehose can be configured to use the data stream as a source and deliver the transformed data to Amazon S3. This combination of services allows for the seamless ingestion and processing of data while minimizing operational overhead.","poster":"Guru4Cloud","timestamp":"1694448120.0"},{"poster":"ibu007","content":"Selected Answer: AE\nA - Use AWS Glue to process the raw data in Amazon S3\nE - Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3","comment_id":"997793","timestamp":"1693758360.0","upvote_count":"3"},{"poster":"GCB1990","comment_id":"992634","timestamp":"1693258500.0","content":"Correct answer: D and E","upvote_count":"1"},{"content":"Why not BC?","timestamp":"1687173960.0","comment_id":"927413","poster":"wRhlH","upvote_count":"1"},{"content":"Why it not CE? \nAdd more EC2 instances to accommodate the increasing amount of incoming data?","comment_id":"922691","upvote_count":"1","comments":[{"timestamp":"1687461600.0","content":"EC2 is not server-less. they want to minimize overhead","upvote_count":"1","poster":"TTaws","comment_id":"930958"}],"poster":"AnnieTran_91","timestamp":"1686711360.0"},{"timestamp":"1684533840.0","comments":[{"timestamp":"1722716700.0","content":"facts!!","poster":"1e22522","upvote_count":"1","comment_id":"1260437"}],"comment_id":"902243","content":"Selected Answer: AE\nminimizes operational overhead = Serverless\nGlue, Kinesis Datastream, S3 are serverless","upvote_count":"3","poster":"studynoplay"},{"comment_id":"812703","poster":"KZM","content":"How about \"C\" to increase EC2 instances for the increased devices soon?","upvote_count":"1","timestamp":"1676700480.0"},{"upvote_count":"3","timestamp":"1673881620.0","poster":"Aninina","comment_id":"777835","content":"Selected Answer: AE\nGlue and API"},{"comment_id":"775815","timestamp":"1673720400.0","poster":"mhmt4438","content":"Selected Answer: AE\nhttps://www.examtopics.com/discussions/amazon/view/83387-exam-aws-certified-solutions-architect-associate-saa-c02/","upvote_count":"2"}],"question_id":162,"timestamp":"2023-01-14 19:20:00","question_text":"A company collects data from thousands of remote devices by using a RESTful web services application that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company needs a highly scalable solution that minimizes operational overhead.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)"},{"id":"i8KO6hEYvA2DTyPfiPLU","isMC":true,"timestamp":"2023-01-14 19:27:00","answer":"B","answer_ET":"B","unix_timestamp":1673720820,"answers_community":["B (94%)","6%"],"url":"https://www.examtopics.com/discussions/amazon/view/95314-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"poster":"Guru4Cloud","content":"Selected Answer: B\nThis is the most cost-effective option because:\n• Versioning has caused the number of objects to increase over time, even as current objects are deleted after 3 years. By deleting previous versions as well, this will clean up old object versions and reduce storage costs.\n• An S3 Lifecycle policy incurs no additional charges and requires no additional resources to configure and run. It is a native S3 tool for managing object lifecycles cost-effectively.","comment_id":"1004959","timestamp":"1710179460.0","upvote_count":"11"},{"content":"Selected Answer: B\nBy configuring the S3 Lifecycle policy to delete previous versions as well as current versions, the older versions of the CloudTrail logs will be deleted. This ensures that objects older than 3 years are removed from the S3 bucket, reducing the object count and controlling storage costs.\n\nA. This option is not directly related to managing objects in the S3. It focuses on configuring the expiration of CloudTrail trails, which may not address the need to delete objects from the S3 bucket.\n\nC. While it is technically possible to create a Lambda to delete objects older than 3 years, this approach would introduce additional complexity and operational overhead.\n\nD. Changing the ownership of the objects in the S3 bucket does not directly address the need to delete objects older than 3 years. Ownership does not affect the deletion behavior of the objects.","upvote_count":"5","comment_id":"936519","timestamp":"1703768460.0","poster":"cookieMr"},{"upvote_count":"3","timestamp":"1717615320.0","poster":"Mikado211","content":"Selected Answer: B\nI did something similar recently : Lifecycle is triggered more or less each 24 hours, in my case it removed hundreds of gigabytes and millions of small files in one shot. Using another mechanism like a script would have taken days if not weeks.","comment_id":"1088824"},{"timestamp":"1716198300.0","upvote_count":"3","poster":"Ruffyit","comment_id":"1075365","content":"This is the most cost-effective option because:\n• Versioning has caused the number of objects to increase over time, even as current objects are deleted after 3 years. By deleting previous versions as well, this will clean up old object versions and reduce storage costs.\n• An S3 Lifecycle policy incurs no additional charges and requires no additional resources to configure and run. It is a native S3 tool for managing object lifecycles cost-effectively."},{"timestamp":"1711093740.0","content":"Selected Answer: B\nEnsure to delete previous versions as well.","upvote_count":"2","comment_id":"1013640","poster":"TariqKipkemei"},{"timestamp":"1701336000.0","content":"Selected Answer: B\nI go for option B.","poster":"Bmarodi","upvote_count":"2","comment_id":"910006"},{"poster":"ruqui","content":"I don't think it's possible to configure an S3 lifecycle policy to delete all versions of an object, so B is wrong ... I think the question is improperly worded","comment_id":"905757","upvote_count":"2","timestamp":"1700829900.0"},{"content":"• Versioning has caused the number of objects to increase over time, even as current objects are deleted after 3 years. By deleting previous versions as well, this will clean up old object versions and reduce storage costs. • An S3 Lifecycle policy incurs no additional charges and requires no additional resources to configure and run. It is a native S3 tool for managing object lifecycles cost-effectively.","timestamp":"1699136580.0","comment_id":"889636","upvote_count":"2","poster":"Rahulbit34"},{"content":"Selected Answer: B\nThis is the most cost-effective option because:\n• Versioning has caused the number of objects to increase over time, even as current objects are deleted after 3 years. By deleting previous versions as well, this will clean up old object versions and reduce storage costs.\n• An S3 Lifecycle policy incurs no additional charges and requires no additional resources to configure and run. It is a native S3 tool for managing object lifecycles cost-effectively.","timestamp":"1698595020.0","comments":[{"content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/DeletingObjectVersions.html","comment_id":"884374","timestamp":"1698595020.0","poster":"kruasan","upvote_count":"3"}],"comment_id":"884373","upvote_count":"4","poster":"kruasan"},{"comment_id":"786967","timestamp":"1690225500.0","poster":"bullrem","comments":[{"upvote_count":"3","poster":"pentium75","comment_id":"1107426","timestamp":"1719543900.0","content":"As long as versioning on the S3 bucket is enabled, any deletion, whether performed by CloudTrail or by your custom Lambda function, will simply add a new version with a deletion market but will not delete the previous version."}],"content":"Selected Answer: C\nA more cost-effective solution would be to configure the organization's centralized CloudTrail trail to expire objects after 3 years. This would ensure that all objects, including previous versions, are deleted after the specified retention period.\nAnother option would be to create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years, this would allow you to have more control over the deletion process and to write a custom logic that best fits your use case.","upvote_count":"3"},{"upvote_count":"3","content":"Selected Answer: B\nThe question clearly says \"An S3 Lifecycle policy is in place to delete current objects after 3 years\". This implies that previous versions are not deleted, since this is a separate setting, and since logs are constantly changed, it would seem to make sense to delete previous versions so, so B. D is wrong, since the parent account (the management account) will already be the owner of all objects delivered to the S3 bucket, \"All accounts in the organization can see MyOrganizationTrail in their list of trails, but member accounts cannot remove or modify the organization trail. Only the management account or delegated administrator account can change or delete the trail for the organization.\", see https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html","poster":"JayBee65","comment_id":"786178","timestamp":"1690170480.0"},{"poster":"John_Zhuang","timestamp":"1689932580.0","upvote_count":"4","content":"Selected Answer: B\nB is the right answer. Ref: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/best-practices-security.html#:~:text=The%20CloudTrail%20trail,time%20has%20passed.\n\nOption A is wrong. No way to expire the cloudtrail logs","comment_id":"783217"},{"upvote_count":"3","timestamp":"1689552780.0","poster":"techhb","content":"Selected Answer: B\nConfigure the S3 Lifecycle policy to delete previous versions","comment_id":"778465"},{"poster":"Aninina","comment_id":"777841","timestamp":"1689512880.0","upvote_count":"2","content":"Selected Answer: B\nB. Configure the S3 Lifecycle policy to delete previous versions as well as current versions."},{"poster":"Parsons","timestamp":"1689368400.0","content":"Selected Answer: B\nB is correct answer","upvote_count":"3","comment_id":"776003"},{"content":"Ans: A \nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\nWhen you create an organization trail, a trail with the name that you give it is created in every AWS account that belongs to your organization. Users with CloudTrail permissions in member accounts can see this trail when they log into the AWS CloudTrail console from their AWS accounts, or when they run AWS CLI commands such as describe-trail. However, users in member accounts do not have sufficient permissions to delete the organization trail, turn logging on or off, change what types of events are logged, or otherwise change the organization trail in any way.","timestamp":"1689353880.0","poster":"AHUI","comment_id":"775848","upvote_count":"1","comments":[{"content":"correction: Ans D is the answer. \nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html","comment_id":"775852","timestamp":"1689354180.0","poster":"AHUI","upvote_count":"1"}]},{"upvote_count":"2","content":"Selected Answer: B\nB. Configure the S3 Lifecycle policy to delete previous versions as well as current versions.\n\nTo delete objects that are older than 3 years in the most cost-effective manner, the company should configure the S3 Lifecycle policy to delete previous versions as well as current versions. This will ensure that all versions of the objects, including the previous versions, are deleted after 3 years.","comment_id":"775820","poster":"mhmt4438","timestamp":"1689352020.0"}],"topic":"1","exam_id":31,"choices":{"D":"Configure the parent account as the owner of all objects that are delivered to the S3 bucket.","A":"Configure the organization’s centralized CloudTrail trail to expire objects after 3 years.","B":"Configure the S3 Lifecycle policy to delete previous versions as well as current versions.","C":"Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years."},"question_images":[],"answer_images":[],"answer_description":"","question_id":163,"question_text":"A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set of AWS accounts by using AWS Organizations from the parent account. The CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years.\n\nAfter the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new CloudTrail logs that are delivered to the S3 bucket has remained consistent.\n\nWhich solution will delete objects that are older than 3 years in the MOST cost-effective manner?"},{"id":"RGNQZLTkHcPpzEkxnlxy","answer_description":"","choices":{"A":"Increase the size of the DB instance to an instance type that has more available memory.","B":"Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB instances.","C":"Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.","D":"Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to write data from the topic to the database."},"question_text":"A company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the API fluctuates. During periods of heavy traffic, the API often returns timeout errors.\n\nAfter an inspection of the logs, the company determines that the database is not capable of processing the volume of write traffic that comes from the API. A solutions architect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic.\n\nWhich solution will meet these requirements?","exam_id":31,"unix_timestamp":1673721300,"question_images":[],"isMC":true,"answers_community":["C (100%)"],"timestamp":"2023-01-14 19:35:00","url":"https://www.examtopics.com/discussions/amazon/view/95318-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","answer_ET":"C","question_id":164,"answer_images":[],"discussion":[{"timestamp":"1724370540.0","poster":"toyaji","comment_id":"1270989","upvote_count":"5","content":"Selected Answer: C\nYou need to also use AWS RDS Proxy becuase lambda will increase parallel and it will cause connection error"},{"poster":"mwwt2022","content":"Selected Answer: C\n//minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic//\n\nI go for C","upvote_count":"3","timestamp":"1703503440.0","comment_id":"1105219"},{"comment_id":"1075369","poster":"Ruffyit","timestamp":"1700481060.0","upvote_count":"3","content":"Decouple the API and the DB with Amazon Simple Queue Service (Amazon SQS) queue."},{"comment_id":"1045801","poster":"oluolope","timestamp":"1697537400.0","upvote_count":"1","content":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html\nSQS can invoke lambda indeed. Initially I picked D because I wasn't sure it was possible but , this article shows it is. It makes this question even more confusing for me as it is also possible to trigger lambda from SNS:\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-lambda-as-subscriber.html\nI don't know which option between C and D makes more sense. I still have a preference for D as it seems less hacky than C."},{"poster":"TariqKipkemei","content":"Selected Answer: C\nDecouple the API and the DB with Amazon Simple Queue Service (Amazon SQS) queue.","timestamp":"1695361860.0","comment_id":"1013641","upvote_count":"4"},{"content":"Selected Answer: C\nC. Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.","comment_id":"1004956","timestamp":"1694447040.0","upvote_count":"2","poster":"Guru4Cloud"},{"comment_id":"936594","timestamp":"1687952700.0","upvote_count":"3","poster":"cookieMr","content":"Selected Answer: C\nBy leveraging SQS as a buffer and using an Lambda to process and write data from the queue to the database, the solution provides scalability, decoupling, and reliability while minimizing the number of connections to the database. This approach handles fluctuations in traffic and ensures data integrity during high-traffic periods.\n\nA. Increasing the size of the DB instance may provide more memory, but it does not address the issue of handling high write traffic efficiently and minimizing connections to the database.\n\nB. Modifying the DB instance to be a Multi-AZ instance and writing to all active instances can improve availability but does not address the issue of efficiently handling high write traffic and minimizing connections to the database.\n\nD. Using SNS and an Lambda can provide decoupling and scalability, but it is not suitable for handling heavy write traffic efficiently and minimizing connections to the database."},{"timestamp":"1687421820.0","comment_id":"930281","content":"I think D, \"Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database\" SQS can't invokes Lambda becouse SQS is pull.","upvote_count":"3","comments":[{"comment_id":"1107430","timestamp":"1703740260.0","content":"\"Invoke a Lambda function from an Amazon SQS trigger\" https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_section.html","poster":"pentium75","upvote_count":"2"}],"poster":"Moccorso"},{"comment_id":"924656","comments":[{"timestamp":"1703740380.0","comment_id":"1107431","poster":"pentium75","upvote_count":"2","content":"a) You can't write to multiple instances at the same time\nb) If you could, it would probably not increase performance\nc) if it would increase performance then it would probably double performance, which might not be enough\n\nDecoupling is the way to go - let clients submit data via APIs, and write it asynchronously to the database. Don't let clients wait until data has been written."}],"content":"Why not B","poster":"shivamrulz","timestamp":"1686872100.0","upvote_count":"2"},{"content":"C is in deed the correct answer for the use case","upvote_count":"2","timestamp":"1679019000.0","comment_id":"841516","poster":"Russs99"},{"content":"Selected Answer: C\nC is correct","poster":"kaushald","upvote_count":"2","timestamp":"1678281120.0","comment_id":"832982"},{"upvote_count":"2","content":"Selected Answer: C\nCis correct","timestamp":"1677633240.0","comment_id":"825447","poster":"Steve_4542636"},{"upvote_count":"2","timestamp":"1676221980.0","comment_id":"806595","content":"Selected Answer: C\nC looks ok","poster":"maciekmaciek"},{"timestamp":"1675731780.0","poster":"iamjaehyuk","content":"why not D?","comment_id":"800410","upvote_count":"1"},{"content":"Selected Answer: C\nC is correct.","timestamp":"1673737320.0","upvote_count":"2","comment_id":"776004","poster":"Parsons"},{"content":"Selected Answer: C\nC. Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.\n\nTo minimize the number of connections to the database and ensure that data is not lost during periods of heavy traffic, the company should modify the API to write incoming data to an Amazon SQS queue. The use of a queue will act as a buffer between the API and the database, reducing the number of connections to the database. And the use of an AWS Lambda function invoked by SQS will provide a more flexible way of handling the data and processing it. This way, the function will process the data from the queue and insert it into the database in a more controlled way.","comments":[{"poster":"Aninina","upvote_count":"6","comments":[{"timestamp":"1677408960.0","content":"same question as you :D","poster":"Nguyen25183","comment_id":"822275","upvote_count":"1"}],"comment_id":"777843","content":"Did you use ChatGPT?","timestamp":"1673881740.0"}],"upvote_count":"3","comment_id":"775831","timestamp":"1673721300.0","poster":"mhmt4438"}],"answer":"C"},{"id":"G5tQU4RJTfGrlUlPCfJe","isMC":true,"discussion":[{"content":"Selected Answer: A\nMigrating the databases to Aurora Serverless provides automated scaling and replication capabilities. Aurora Serverless automatically scales the capacity based on the workload, allowing for seamless addition or removal of compute capacity as needed. It also offers improved performance, durability, and high availability without requiring manual management of replication and scaling.\n\nB. Incorrect because it suggests migrating to a different database engine, which may introduce compatibility issues and require significant code modifications.\n\nC. Incorrect because consolidating into a larger MySQL database on larger EC2 instances does not provide the desired scalability and automation.\n\nD. Incorrect because using EC2 Auto Scaling groups for the database tier still requires manual management of replication and scaling.","poster":"cookieMr","timestamp":"1719575220.0","comment_id":"936600","upvote_count":"9"},{"comment_id":"1358271","upvote_count":"1","content":"Selected Answer: A\nAurora Serverless is the most suitable option here.","poster":"satyaammm","timestamp":"1739880180.0"},{"poster":"TariqKipkemei","comment_id":"1013643","timestamp":"1726984320.0","upvote_count":"2","content":"Selected Answer: A\nMigrate the databases to Amazon Aurora Serverless for Aurora MySQL"},{"content":"Selected Answer: A\nAurora MySQL","poster":"Undisputed","upvote_count":"2","comment_id":"965615","timestamp":"1722176100.0"},{"comment_id":"910018","content":"Selected Answer: A\nOption A is right answer.","poster":"Bmarodi","upvote_count":"2","timestamp":"1717054800.0"},{"poster":"Bhrino","content":"Selected Answer: A\nA is correct because aurora might be more expensive but its serverless and is much faster","upvote_count":"2","timestamp":"1708480920.0","comment_id":"816106"},{"comment_id":"778092","poster":"mp165","upvote_count":"4","timestamp":"1705429620.0","content":"Selected Answer: A\nA is porper\n\nhttps://aws.amazon.com/rds/aurora/serverless/"},{"timestamp":"1705417860.0","poster":"Aninina","comment_id":"777846","upvote_count":"2","content":"Selected Answer: A\nAurora MySQL"},{"timestamp":"1705257360.0","poster":"mhmt4438","comment_id":"775833","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/51509-exam-aws-certified-solutions-architect-associate-saa-c02/","upvote_count":"2"}],"question_id":165,"url":"https://www.examtopics.com/discussions/amazon/view/95319-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-01-14 19:36:00","exam_id":31,"question_text":"A company manages its own Amazon EC2 instances that run MySQL databases. The company is manually managing replication and scaling as demand increases or decreases. The company needs a new solution that simplifies the process of adding or removing compute capacity to or from its database tier as needed. The solution also must offer improved performance, scaling, and durability with minimal effort from operations.\n\nWhich solution meets these requirements?","answer_description":"","choices":{"C":"Combine the databases into one larger MySQL database. Run the larger database on larger EC2 instances.","D":"Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the new environment.","A":"Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.","B":"Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL."},"question_images":[],"unix_timestamp":1673721360,"answer_ET":"A","answer":"A","answer_images":[],"topic":"1","answers_community":["A (100%)"]}],"exam":{"isBeta":false,"id":31,"provider":"Amazon","isImplemented":true,"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03"},"currentPage":33},"__N_SSP":true}