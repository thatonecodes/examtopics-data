{"pageProps":{"questions":[{"id":"S5105pODCQcPGjuCHjvK","url":"https://www.examtopics.com/discussions/amazon/view/49012-exam-aws-certified-database-specialty-topic-1-question-135/","isMC":true,"choices":{"A":"Create a custom script that exports archival data from the DB cluster to Amazon S3 using a SQL view, then deletes the archival data from the DB cluster. Launch an Amazon EC2 instance with a weekly cron job to execute the custom script.","C":"Configure two AWS Lambda functions: one that exports archival data from the DB cluster to Amazon S3 using the mysqldump utility, and another that deletes the archival data from the DB cluster. Schedule both Lambda functions to run weekly using Amazon EventBridge (Amazon CloudWatch Events).","D":"Use AWS Database Migration Service (AWS DMS) to continually export the archival data from the DB cluster to Amazon S3. Configure an AWS Data Pipeline process to run weekly that executes a custom SQL script to delete the archival data from the DB cluster.","B":"Configure an AWS Lambda function that exports archival data from the DB cluster to Amazon S3 using a SELECT INTO OUTFILE S3 statement, then deletes the archival data from the DB cluster. Schedule the Lambda function to run weekly using Amazon EventBridge (Amazon CloudWatch Events)."},"unix_timestamp":1617515040,"exam_id":22,"answer_ET":"B","answers_community":["B (100%)"],"answer":"B","question_id":41,"answer_images":[],"discussion":[{"comment_id":"328531","content":"Going for B. since SELECT INTO OUTFILE S3 is available on Aurora. \nOption C. uses mysqldump who does not dump directly to S3","upvote_count":"14","timestamp":"1633573080.0","poster":"Jaypdv"},{"poster":"MultiAZ","comment_id":"1121356","upvote_count":"1","content":"Selected Answer: B\nAnser is B. The data should be readily accessible (e.g. via Athena), so mysqldump is not useful","timestamp":"1705129020.0"},{"upvote_count":"1","poster":"Pranava_GCP","timestamp":"1693435860.0","content":"Selected Answer: B\nB. Configure an AWS Lambda function that exports archival data from the DB cluster to Amazon S3 using a SELECT INTO OUTFILE S3 statement, then deletes the archival data from the DB cluster. Schedule the Lambda function to run weekly using Amazon EventBridge (Amazon CloudWatch Events).","comment_id":"994512"},{"poster":"rags1482","comment_id":"707515","content":"If the amount of data to be selected is large (more than 25 GB), we recommend that you use multiple SELECT INTO OUTFILE S3 statements to save the data to Amazon S3\n\nAnswer: B","upvote_count":"1","timestamp":"1667081760.0"},{"comment_id":"626455","timestamp":"1656832080.0","upvote_count":"1","content":"B is correct approch.\nC mysqldump can not dump into S3 \nhttps://aws.amazon.com/blogs/database/best-practices-for-exporting-and-importing-data-from-amazon-aurora-mysql-to-amazon-s3/","poster":"sachin"},{"content":"Selected Answer: B\nB because:\n1. Lambda function max run time is 15 min\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/\n2. SELECT INTO OUTFILES3 is there, and 10GB data per week sounds reasonable to finish copying within 15 min\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.SaveIntoS3.html\n\nAWS DMS can copy to S3 but option D says continuilly export, while we need weekly\nhttps://aws.amazon.com/blogs/database/archiving-data-from-relational-databases-to-amazon-glacier-via-aws-dms/","poster":"novice_expert","timestamp":"1651194480.0","comment_id":"594140","upvote_count":"1"},{"poster":"pcpcpc888","upvote_count":"3","content":"running a continually DMS job would NOT be operationally efficient, when talk about which, serverless options combined with Lambda and EventBridge would be a much better choice; considering the volume of the weekly archival, the duration would not hit Lambda timeout; however, it seems like more development would be needed for C, cause Select Into Outfile S3 directly integrates with S3. So B.","timestamp":"1644937980.0","comment_id":"547830"},{"poster":"Raj12131","comment_id":"519325","content":"Option A requires more effort and hence can be ruled out. Option B uses same lambda function for data migration and deletion thereafter. It doesn't work as lambda might timeout. Option C uses mysqldump which is ok but not as efficient as DMS. Option D is the correct solution in my view.","timestamp":"1641619560.0","upvote_count":"1"},{"comment_id":"510796","poster":"Shunpin","timestamp":"1640667000.0","upvote_count":"1","content":"Selected Answer: B\nFor option D, I will consider how DMS export data export to S3 looks like and also how DMS handle \"delete\" CDC statements. With DMS option, you need additional tasks to filter data and not easy to maintain."},{"content":"I think its D\nhttps://aws.amazon.com/blogs/database/archiving-data-from-relational-databases-to-amazon-glacier-via-aws-dms/","upvote_count":"1","poster":"SMAZ","timestamp":"1640536920.0","comment_id":"509707"},{"content":"Lambda functions have 15mins max execution time. If the extract and delete takes longer than 15 mins using a Lambda function won't work. This limitation might rule out option B and C. Option D will work but \"continually export the archival data\" is not a requirement. \n\nThoughts?","upvote_count":"1","timestamp":"1640284560.0","poster":"jove","comments":[{"comment_id":"542096","content":"Good catch on the 15 min limit for Lambda! But in the context of the question - \" Each month, around 10 GB of fresh data is uploaded to the database.\" - I would assume 2.5 GB weekly data volume - seems reasonable to assume that the export and delete will be done within 15 min. so B is still an option here","poster":"VPup","upvote_count":"1","timestamp":"1644196020.0"}],"comment_id":"508091"},{"poster":"Aesthet","comment_id":"361586","upvote_count":"2","content":"B final answer","timestamp":"1634694180.0"},{"content":"B is correct.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.SaveIntoS3.html","timestamp":"1634060040.0","upvote_count":"4","poster":"manan728","comment_id":"342574"},{"timestamp":"1633877940.0","poster":"Chhotu_DBA","upvote_count":"2","comment_id":"332735","content":"Option B correct"},{"timestamp":"1632199200.0","comments":[{"poster":"faramawi","content":"I think it should be D too. I think it provides \"most operationally efficient solution to migrate the archival data to Amazon S3 \". \nhttps://aws.amazon.com/blogs/database/archiving-data-from-relational-databases-to-amazon-glacier-via-aws-dms/\nhttps://aws.amazon.com/blogs/database/replicate-data-from-amazon-aurora-to-amazon-s3-with-aws-database-migration-service/","timestamp":"1635425940.0","comment_id":"448106","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"491841","poster":"Justu","comments":[{"upvote_count":"1","comment_id":"508090","content":"Yes you can but I'm not sure if using DMS is the right option","poster":"jove","timestamp":"1640284260.0","comments":[{"comment_id":"688195","content":"DMS is for DB migration tools & very $$$ so we just use time by time but let it run as job tools. so Lambda is right tools for this .","poster":"Jiang_aws1","upvote_count":"1","timestamp":"1665105300.0"}]}],"content":"Can you use AWS Data Pipeline process Custom SQL Query to delete data from RDS?","timestamp":"1638379680.0"}]}],"poster":"novak18","content":"Answer should be D?\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html\n\nhttps://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-sqlactivity.html","upvote_count":"1","comment_id":"327782"}],"question_text":"To meet new data compliance requirements, a company needs to keep critical data durably stored and readily accessible for 7 years. Data that is more than 1 year old is considered archival data and must automatically be moved out of the Amazon Aurora MySQL DB cluster every week. On average, around 10 GB of new data is added to the database every month. A database specialist must choose the most operationally efficient solution to migrate the archival data to\nAmazon S3.\nWhich solution meets these requirements?","question_images":[],"answer_description":"","topic":"1","timestamp":"2021-04-04 07:44:00"},{"id":"j9msCTLpg33xwnyp5WKf","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/48876-exam-aws-certified-database-specialty-topic-1-question-136/","choices":{"C":"Only allow incoming traffic from the subnet of the application servers on port 3306.","B":"Only allow incoming traffic from the sg-application-servers security group on port 443.","A":"Only allow incoming traffic from the sg-application-servers security group on port 3306.","D":"Only allow incoming traffic from the subnet of the application servers on port 443."},"discussion":[{"timestamp":"1632468840.0","comment_id":"327092","poster":"shantest1","comments":[{"timestamp":"1651416240.0","content":"Do we allow traffic from security group \nOR\nfrom resources that are assigned to the same security group ?","upvote_count":"1","poster":"novice_expert","comment_id":"595626"}],"content":"A. Answer\n\nDatabase port 3306 and better to allow only the specific subnet instead of the entire subnet.","upvote_count":"14"},{"comments":[{"comment_id":"992745","upvote_count":"1","poster":"Pranava_GCP","content":"Because 1) port 3306 is default port number for mySQL 2) a security group has to be explicitly assigned to an EC2 instance.","timestamp":"1693278900.0"}],"comment_id":"988199","poster":"Pranava_GCP","content":"Selected Answer: A\nA. Only allow incoming traffic from the sg-application-servers security group on port 3306.","timestamp":"1692787620.0","upvote_count":"1"},{"timestamp":"1684495740.0","poster":"aviathor","upvote_count":"2","comment_id":"901874","content":"Selected Answer: A\nWhere on earth do these \"Correct answers\" come from?\n\nAllowing connections only from members of sg-application-servers is more restrictive than allowing traffic from the whole subnet. 3306 is probably the correct port for RDS. Therefore A"},{"comments":[{"content":"C is wrong. Because allow the whole subnets mean allow all application on that subnet --> not secure.\nFYI: security group doesn't have 'Deny' rule.\nA is the answer. Security A allow traffic from Security B mean that Security A allow all resources using Security B","upvote_count":"1","comment_id":"804940","timestamp":"1676084820.0","poster":"im_not_robot"}],"content":"I'll go with C:\nSecurity groups contains rules allowing or denying access to specified IP address and TCP Ports. Then they are associated with resources (such as ec2, rds, etc).\nThe question says that EC2 use the security group sg-application-servers. That is the sg-application-servers contains the rules that at this moment make the ec2 communication work. To allow the RDS instance talk with this ec2, is necessary to create a SG and specify the address of the ec2 instances in the ingress rules, referencing the port 3306. Or, allow the traffic from the entire subnet at this same port.\nI've read the security group doc again and dont see nothing about grouping aws resources to reference as some kind of \"security resource group\" as the A answer say.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html","comment_id":"710833","upvote_count":"1","timestamp":"1667519760.0","poster":"Bernardes"},{"content":"Selected Answer: A\nA. Only allow incoming traffic from the sg-application-servers security group on port 3306.","timestamp":"1651416360.0","poster":"novice_expert","comment_id":"595628","upvote_count":"3"},{"comment_id":"522166","poster":"awsmonster","timestamp":"1641992640.0","upvote_count":"1","content":"Ans: A"},{"content":"Answer A. most restrictive approach is to allow only incoming connections from SG of EC2 instance on port 3306","poster":"Hits_23","timestamp":"1636153200.0","upvote_count":"1","comment_id":"409230"},{"content":"AAAAAAAAAAAA","timestamp":"1635160200.0","comment_id":"380637","upvote_count":"2","poster":"Suresh108"},{"content":"A final answer","poster":"Aesthet","upvote_count":"1","timestamp":"1633306860.0","comment_id":"361587"}],"isMC":true,"unix_timestamp":1617414900,"answers_community":["A (100%)"],"timestamp":"2021-04-03 03:55:00","topic":"1","exam_id":22,"question_id":42,"answer_images":[],"answer":"A","question_text":"A company developed a new application that is deployed on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances use the security group named sg-application-servers. The company needs a database to store the data from the application and decides to use an Amazon RDS for MySQL DB instance. The DB instance is deployed in a private DB subnet.\nWhat is the MOST restrictive configuration for the DB instance security group?","answer_ET":"A","answer_description":""},{"id":"1npeNi1zHTm8eDpFOxa7","url":"https://www.examtopics.com/discussions/amazon/view/48845-exam-aws-certified-database-specialty-topic-1-question-137/","question_images":[],"isMC":true,"choices":{"B":"Use AWS Database Migration Service (AWS DMS) to transfer the data from on premises to the S3 bucket. Use the Loader command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.","C":"Use AWS DataSync to transfer the data from on premises to the S3 bucket. Use the Loader command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.","D":"Use the AWS CLI to transfer the data from on premises to the S3 bucket. Use the Copy command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.","A":"Use an AWS SDK with a multipart upload to transfer the data from on premises to the S3 bucket. Use the Copy command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance."},"discussion":[{"timestamp":"1634251200.0","poster":"Jaypdv","content":"Answer is C. since DMS can only use databases (and S3) as sources, and the question does not specify that the on-prem data resides in a DB. In which case, Datasync is a more likely choice.","upvote_count":"8","comment_id":"328533"},{"timestamp":"1635958800.0","poster":"manan728","comment_id":"344988","content":"This question was asked in my exam. I went with C.","upvote_count":"6"},{"upvote_count":"1","content":"Selected Answer: C\nC. Use AWS DataSync to transfer the data from on premises to the S3 bucket. Use the Loader command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.\n\nAWS DataSync is used to transfer data from On-prem to S3, no source db is mentioned. \nTo load data from S3 to AWS Neptune to use loader command\n\n\nhttps://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html","timestamp":"1694828160.0","comment_id":"1008788","poster":"Pranava_GCP"},{"timestamp":"1651420380.0","content":"Selected Answer: C\nDMS target can not be S3\n\nAWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS storage services.\n\nC. Use AWS DataSync to transfer the data from on premises to the S3 bucket. Use the Loader command for Neptune to move the data in bulk from the S3 bucket to the Neptune DB instance.\n\naws DataSync is the way to transfer data from on-prem to S3, no source db is mentioned.\nOnly way to load data from S3 to AWS Neptune is using loader\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html","comments":[{"poster":"awsjjj","timestamp":"1665749280.0","content":"Answer is C. But DMS target can be S3. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html","comment_id":"694725","upvote_count":"1"}],"poster":"novice_expert","upvote_count":"2","comment_id":"595650"},{"comment_id":"562595","poster":"RotterDam","upvote_count":"2","timestamp":"1646654880.0","content":"Got this question in my exam. (i cleared it). C is correct"},{"upvote_count":"1","timestamp":"1641993900.0","comment_id":"522185","content":"Answer should be B, use DMS.\n\n\nData Sync does not read from databases (https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html).\n\n\"AWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS storage services, and also between AWS storage services.\"","comments":[{"upvote_count":"1","comment_id":"522189","poster":"awsmonster","content":"Sorry .. Answer is C. \n\nMy error, the questions says move fraud data, nothing is mentioned about database.","timestamp":"1641993960.0"}],"poster":"awsmonster"},{"upvote_count":"3","poster":"Hits_23","timestamp":"1636236240.0","comment_id":"409233","content":"C is correct answer. \naws DataSync is the way to transfer data from on-prem to S3, no source db is mentioned. \nOnly way to load data from S3 to AWS Neptune is using loader\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"},{"upvote_count":"1","poster":"Aesthet","content":"C is the answer\nIf on-premise DB was mentioned I would choose B","comment_id":"361591","timestamp":"1636045320.0"},{"poster":"db_interest","content":"C makes sense","timestamp":"1635825420.0","upvote_count":"1","comment_id":"344832"},{"content":"Between B and C that are the only plausible options C seems more legit for the given scenario.","timestamp":"1635216360.0","poster":"manan728","upvote_count":"1","comment_id":"342609"},{"content":"Answer should be B I guess","upvote_count":"1","comment_id":"326886","timestamp":"1633324200.0","poster":"novak18"}],"unix_timestamp":1617386820,"answers_community":["C (100%)"],"timestamp":"2021-04-02 20:07:00","topic":"1","exam_id":22,"question_id":43,"answer_images":[],"answer":"C","question_text":"A company is moving its fraud detection application from on premises to the AWS Cloud and is using Amazon Neptune for data storage. The company has set up a 1 Gbps AWS Direct Connect connection to migrate 25 TB of fraud detection data from the on-premises data center to a Neptune DB instance. The company already has an Amazon S3 bucket and an S3 VPC endpoint, and 80% of the company's network bandwidth is available.\nHow should the company perform this data load?","answer_ET":"C","answer_description":""},{"id":"n4RJfFJ9oiJSDr3mRSwE","exam_id":22,"question_id":44,"answer_description":"","timestamp":"2021-04-04 07:51:00","choices":{"A":"Set the max_connections parameter to 16,000 in the instance-level parameter group.","D":"Enable the query cache at the instance level.","B":"Modify the client connection timeout to 300 seconds.","C":"Create an Amazon RDS Proxy database proxy and update client connections to point to the proxy endpoint."},"unix_timestamp":1617515460,"answer_ET":"C","discussion":[{"upvote_count":"15","timestamp":"1635544380.0","poster":"manan728","content":"C looks to be the winner. \nAmazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66% and database credentials, authentication, and access can be managed through integration with AWS Secrets Manager and AWS Identity and Access Management (IAM).","comment_id":"342618"},{"comment_id":"327786","timestamp":"1632306120.0","poster":"novak18","comments":[{"comment_id":"328537","poster":"Jaypdv","content":"I'm sorry, you're correct. Answer is C.","upvote_count":"1","timestamp":"1634017380.0"}],"content":"Can the answer be C?\n\nhttps://aws.amazon.com/rds/proxy/","upvote_count":"6"},{"comment_id":"1006091","poster":"Pranava_GCP","timestamp":"1694564400.0","content":"C. Amazon RDS Proxy database proxy\n\n\"Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66% and database credentials, authentication, and access can be managed through integration with AWS Secrets Manager and AWS Identity and Access Management (IAM).\"\n\nhttps://aws.amazon.com/rds/proxy/","upvote_count":"1"},{"comment_id":"597946","timestamp":"1651889940.0","poster":"KaranGandhi30","upvote_count":"1","content":"Selected Answer: C\nWhen your primary instance is down ABD won't work."},{"timestamp":"1651189980.0","comment_id":"594110","upvote_count":"1","poster":"novice_expert","content":"Selected Answer: C\nhttps://aws.amazon.com/rds/proxy/"},{"content":"Selected Answer: C\nDefinitely C. Proxies maintain a pool of client connections that respond to failovers and actually IMPROVE failover times by 66% https://aws.amazon.com/rds/proxy/","comment_id":"561709","poster":"RotterDam","timestamp":"1646525400.0","upvote_count":"2"},{"comment_id":"557684","timestamp":"1646000760.0","content":"i vote for C,\nthe key is \"application recovery time after database failovers\"","poster":"user0001","upvote_count":"1"},{"timestamp":"1636033740.0","content":"C is correct. A D is not relevant, B even makes failover time slower.","upvote_count":"1","poster":"ChauPhan","comment_id":"433936"},{"timestamp":"1635935220.0","comment_id":"361593","upvote_count":"2","content":"C final answer","poster":"Aesthet"},{"comments":[{"upvote_count":"1","timestamp":"1634292060.0","content":"Oops. Answer is C.","comment_id":"328538","poster":"Jaypdv"}],"comment_id":"327814","poster":"Jaypdv","content":"B. Answer","timestamp":"1633879200.0","upvote_count":"1"}],"answer":"C","isMC":true,"question_images":[],"topic":"1","answers_community":["C (100%)"],"question_text":"A company migrated one of its business-critical database workloads to an Amazon Aurora Multi-AZ DB cluster. The company requires a very low RTO and needs to improve the application recovery time after database failovers.\nWhich approach meets these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/49013-exam-aws-certified-database-specialty-topic-1-question-138/","answer_images":[]},{"id":"6Fb2EfWMNJ74z3whxNlD","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/48844-exam-aws-certified-database-specialty-topic-1-question-139/","answer_description":"","isMC":true,"question_id":45,"question_images":[],"discussion":[{"comment_id":"328541","content":"C. Answer","timestamp":"1634443020.0","poster":"Jaypdv","upvote_count":"8"},{"comment_id":"327093","timestamp":"1634134500.0","content":"C. Answer","poster":"shantest1","upvote_count":"5"},{"content":"Selected Answer: C\nC is correct. You can only enable encryption during copy of the snapshot or when you create DB instance .","timestamp":"1694570280.0","comment_id":"1006100","upvote_count":"2","poster":"Pranava_GCP"},{"comment_id":"707135","timestamp":"1667040600.0","poster":"sirfans","upvote_count":"2","content":"Selected Answer: C\nC is the right choice. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html"},{"poster":"novice_expert","upvote_count":"3","timestamp":"1651404420.0","comment_id":"595568","content":"Selected Answer: C\nC. Stop the DB instance and create a snapshot. Copy the snapshot into another encrypted snapshot. Restore the encrypted snapshot to a new encrypted DB instance. Delete the original DB instance, and update the applications to point to the new encrypted DB instance."},{"upvote_count":"4","timestamp":"1649833980.0","comment_id":"585071","content":"Selected Answer: C\nC is the way. You can only enable encryption during copy operation of the snapshot.","poster":"kret"},{"timestamp":"1636071540.0","poster":"ChauPhan","comment_id":"433938","content":"C is correct","upvote_count":"1"},{"poster":"Aesthet","comment_id":"361594","upvote_count":"1","content":"C final answer","timestamp":"1634877420.0"},{"upvote_count":"1","timestamp":"1634464560.0","comments":[{"upvote_count":"1","comment_id":"342623","content":"Eh, maybe required for compliance I suppose but C it is.","poster":"manan728","timestamp":"1634639040.0"}],"comment_id":"342622","content":"Deleting the old database was kind of an unnecessary step added there.","poster":"manan728"},{"timestamp":"1633385220.0","upvote_count":"2","comment_id":"326878","poster":"novak18","content":"Answer should be C"}],"question_text":"A company is using an Amazon RDS for MySQL DB instance for its internal applications. A security audit shows that the DB instance is not encrypted at rest. The company's application team needs to encrypt the DB instance.\nWhat should the team do to meet this requirement?","choices":{"A":"Stop the DB instance and modify it to enable encryption. Apply this setting immediately without waiting for the next scheduled RDS maintenance window.","B":"Stop the DB instance and create an encrypted snapshot. Restore the encrypted snapshot to a new encrypted DB instance. Delete the original DB instance, and update the applications to point to the new encrypted DB instance.","C":"Stop the DB instance and create a snapshot. Copy the snapshot into another encrypted snapshot. Restore the encrypted snapshot to a new encrypted DB instance. Delete the original DB instance, and update the applications to point to the new encrypted DB instance.","D":"Create an encrypted read replica of the DB instance. Promote the read replica to master. Delete the original DB instance, and update the applications to point to the new encrypted DB instance."},"answer_images":[],"answer":"C","exam_id":22,"answer_ET":"C","timestamp":"2021-04-02 19:57:00","unix_timestamp":1617386220,"topic":"1"}],"exam":{"isImplemented":true,"id":22,"numberOfQuestions":359,"isMCOnly":false,"isBeta":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","name":"AWS Certified Database - Specialty"},"currentPage":9},"__N_SSP":true}