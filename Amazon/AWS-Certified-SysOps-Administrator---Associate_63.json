{"pageProps":{"questions":[{"id":"yCX7NqOJZFuJCSRneycx","topic":"1","question_text":"A company has a private Amazon S3 bucket that contains sensitive information. A SysOps administrator needs to keep logs of the IP addresses from authentication failures that result from attempts to access objects in the bucket. The logs must be stored so that they cannot be overwritten or deleted for 90 days.\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/79126-exam-aws-certified-sysops-administrator-associate-topic-1/","answer":"D","choices":{"A":"Create an AWS CloudTrail trail. Configure the log files to be saved to Amazon CloudWatch Logs. Configure the log group with a retention period of 90 days.","C":"Turn on access logging for the S3 bucket. Configure the access logs to be saved to Amazon CloudWatch Logs. Configure the log group with a retention period of 90 days.","D":"Turn on access logging for the S3 bucket. Configure the access logs to be saved in a second S3 bucket. Turn on S3 Object Lock on the second S3 bucket, and configure a default retention period of 90 days.","B":"Create an AWS CloudTrail trail. Configure the log files to be saved to a different S3 bucket. Turn on CloudTrail log file integrity validation for 90 days."},"question_images":[],"discussion":[{"comment_id":"657910","content":"Selected Answer: D\nD.\nLearn more here: https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","timestamp":"1662162300.0","poster":"kati2k22cz","upvote_count":"11"},{"content":"I would have thought is A, but after reading I found this: \"CloudTrail does not deliver logs for requests that fail authentication (in which the provided credentials are not valid). However, it does include logs for requests in which authorization fails (AccessDenied) and requests that are made by anonymous users.\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","poster":"princajen","comments":[{"comment_id":"731517","content":"You are right, but read again the question:”keep logs of the IP addresses from authentication failures” that “result from attempts to access objects in the bucket” . “That result from attempts to access objects in the bucket” , for me it’s mean authorization fails(AccessDenied).So, for me B,C,D technically are not possible. Vote for A","poster":"cosmogen","timestamp":"1669814880.0","upvote_count":"3"}],"timestamp":"1662056220.0","upvote_count":"10","comment_id":"656535"},{"comment_id":"1335682","poster":"Dinya_jui","content":"Selected Answer: D\nI would have thought is A, but after reading I found this: \"CloudTrail does not deliver logs for requests that fail authentication (in which the provided credentials are not valid). However, it does include logs for requests in which authorization fails (AccessDenied) and requests that are made by anonymous users.\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","timestamp":"1735840320.0","upvote_count":"1"},{"poster":"numark","upvote_count":"1","timestamp":"1732196940.0","comment_id":"1315854","content":"D: S3 Access Logging allows you to capture details of requests made to your S3 bucket, including failed attempts. This is critical for tracking authentication failures.By configuring the access logs to be saved to a second S3 bucket, you can separate the logs from the sensitive data in the original bucket, adding an extra layer of security and compliance.S3 Object Lock can be used to prevent object versions from being deleted or overwritten for a specified retention period. By turning on S3 Object Lock and setting the retention period to 90 days, you ensure that the access logs are immutable for the required duration.S3 Object Lock enforces a Write Once, Read Many (WORM) model, which is ideal for compliance and security use cases."},{"comment_id":"1180468","content":"Selected Answer: D\n\"CloudTrail does not deliver logs for requests that fail authentication (in which the provided credentials are not valid). However, it does include logs for requests in which authorization fails (AccessDenied) and requests that are made by anonymous users.\" \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html#:~:text=CloudTrail%20does%20not%20deliver%20logs%20for%20requests%20that%20fail%20authentication%20(in%20which%20the%20provided%20credentials%20are%20not%20valid).%20However%2C%20it%20does%20include%20logs%20for%20requests%20in%20which%20authorization%20fails%20(AccessDenied)%20and%20requests%20that%20are%20made%20by%20anonymous%20users.","timestamp":"1711160100.0","upvote_count":"4","poster":"joshnort"},{"comment_id":"1171126","content":"Selected Answer: D\nI think D is correct. Its the only option which prevents the logs from being deleted. Cloud watch log retention will prevent the logs from expiring, but they can still be deleted.","poster":"Rabbit117","timestamp":"1710173820.0","upvote_count":"3"},{"timestamp":"1707352020.0","upvote_count":"5","poster":"Learning4life","comment_id":"1143921","content":"Selected Answer: D\nSimple answer is D. S3 authentication failures are not logged by Cloud Trail. See chart in link. https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html Turning on S3object lock, protects against accidental deletion. See link https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html"},{"upvote_count":"2","content":"Selected Answer: D\nD is the correct answer to keep tracks of access logging for s3 bucket","poster":"ogogundare","timestamp":"1704903900.0","comment_id":"1118814"},{"upvote_count":"1","poster":"tamng","content":"D is Correct not A","comment_id":"1109261","timestamp":"1703893500.0"},{"upvote_count":"2","content":"Selected Answer: D\nB doesn't make sense with the log file integrity for 90 days bit - you don't configure log file integrity to only apply for a period of time.","comment_id":"1040898","poster":"Vinsmoke","timestamp":"1697041740.0"},{"poster":"callspace","upvote_count":"1","content":"Selected Answer: B\nLooks like it is B.\nServer access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill. But Cloud Trail is capable of recording theIPs.So IPs recording, CloudTrail log file integrity validation for 90 days","timestamp":"1695970020.0","comment_id":"1020548"},{"poster":"satamex","timestamp":"1692054240.0","content":"Its simple straightforward question . D it is","comment_id":"981142","upvote_count":"1"},{"timestamp":"1690625580.0","content":"Selected Answer: D\nOption D is the most appropriate solution because it covers the specific requirements mentioned in the question. By turning on access logging for the S3 bucket, you can capture the IP addresses from authentication failures. You then configure these access logs to be saved in a separate S3 bucket, ensuring data durability and separation from the source bucket. By enabling S3 Object Lock on the second S3 bucket and setting a default retention period of 90 days, you ensure that the logs cannot be deleted or overwritten for the specified duration.","comment_id":"966312","poster":"Christina666","upvote_count":"8"},{"content":"Selected Answer: B\nmaybe Create an AWS CloudTrail trail. Configure the log files to be saved to a different S3 bucket. Turn on CloudTrail log file integrity validation for 90 days.\n\nNotes:\nD is wrong because S3 buckets with S3 Object Lock can't be used as destination buckets for server access logs.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html","comment_id":"890500","timestamp":"1683350820.0","poster":"noahsark","comments":[{"content":"Integrity validation doesn't stop someone form changing/deleting logs, it just detects it. The real solution requires protection of the files through versioning or object lock. After wasting a day analyzing all the solutions, I found real problems with each one as I listed. That is why the voting is fairly balanced. Something is wrong with the responses given","timestamp":"1683658260.0","comment_id":"893369","upvote_count":"1","poster":"Gomer"}],"upvote_count":"1"},{"upvote_count":"4","content":"NOT A: Can't configure CloudTrail to store logs in CloudWatch Logs. CloudTrail uses S3 bucket. CloudWatch Logs is not applicable.\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/get-and-view-cloudtrail-log-files.html\n\nNOT B: \"Integrity validation\" is only designed to detect changes or deletions of CloudTrail logs. It depends on other security measures to block this.\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\n\nNOT C: Server access logging only delivers access logs for a source bucket to a target bucket. CloudWatch log group is not applicable.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.htm\n\nNOT D: \"S3 buckets with S3 Object Lock can't be used as destination buckets for server access logs.\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html","comment_id":"889189","comments":[{"timestamp":"1683180480.0","comment_id":"889192","content":"Both \"A\" and \"C\" are clearly not possible, and are excluded (IMHO). However, \"B\" and \"D\" also appear to be excluded for reasons cited. I do lean towards \"D\" because one clear requirement is to block log file deletion, not just detect it or automate it after 90 days.\nIMHO the real solution (not listed) would create a CloudTrail trail that logs S3 Data Events in seperate bucket, enable S3 Object Lock on that bucket with a retention period of 90 days, and enable Integrity Validation to detect any possible changes/deletions. Then I'd also figure out a lifecycle policy or some method to delete the logs sometimes after the 90 day requirement.","upvote_count":"1","poster":"Gomer"}],"timestamp":"1683180060.0","poster":"Gomer"},{"timestamp":"1680776220.0","comment_id":"862847","content":"Selected Answer: B\nBy creating a CloudTrail trail, you can log all API calls made to the S3 bucket, including authentication failures. The logs can be saved to a separate S3 bucket to isolate them from the main bucket and provide an additional layer of security. Turning on CloudTrail log file integrity validation ensures that the logs cannot be modified or deleted without detection. The retention period for the logs can be set to 90 days to meet the requirements specified in the question.","upvote_count":"1","poster":"vherman"},{"timestamp":"1677433740.0","content":"This question is really tricky, but after reding the question very carefully, I will definitely go with BBBBBB.","comment_id":"822768","upvote_count":"4","poster":"braveheart22"},{"poster":"Hisayuki","timestamp":"1677384540.0","content":"Selected Answer: D\nS3 Access logs and S3 Object lock - You can store S3 Access logs in a different S3 buckets so that you can analysis through Athena. Using S3 Object lock and specify retention period, files are blocked to be deleted for a amount of time.","comment_id":"822064","comments":[{"upvote_count":"2","comment_id":"823458","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\n\nCan't be D.","timestamp":"1677489180.0","poster":"defmania00"}],"upvote_count":"4"},{"comment_id":"821571","content":"Selected Answer: A\nB - CloudTrail log file integrity validation will NOT prevent logs to be overwritten or deleted.\nC - S3 Server Access Logging only uses S3 buckets as the destination\nD - S3 buckets with Object Lock enabled cannot be used as target buckets for Server Access Logging","upvote_count":"4","poster":"defmania00","timestamp":"1677336840.0"},{"timestamp":"1675522980.0","poster":"renekton","comment_id":"798083","content":"Selected Answer: B\nCorrect Answer is B\n\nA - wrong because the question need a bucket\nC - wrong because s3 access logging need another bucket for target logging\nD - wrong because S3 buckets with S3 Object Lock can't be used as destination buckets for server access logs","upvote_count":"1"},{"comment_id":"796023","content":"Selected Answer: B\nCorrect answer is B.\nYou can configure the period for validating CloudTrail log file integrity.\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\nOption A and C are wrong. Storing logs in CloudWatch Logs is not sufficient for preventing to be overwritten or deleted.\nOption D is wrong. S3 buckets with S3 Object Lock can't be used as destination buckets for server access logs.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html","timestamp":"1675342140.0","upvote_count":"3","poster":"Untamables"},{"timestamp":"1671102060.0","comment_id":"745971","comments":[{"content":"he got the answer : \nlog IP address, 90days,","poster":"jipark","timestamp":"1691660220.0","comment_id":"977471","upvote_count":"1"}],"content":"the answer is C:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/external-ip-address-s3-bucket/","upvote_count":"2","poster":"zolthar_z"},{"poster":"BietTuot","timestamp":"1671020940.0","comment_id":"745055","upvote_count":"2","content":"Selected Answer: D\nI think the correct answer is D.\nThe other answers (A, B and C) don't allow you to lock objects and you can overwrite or delete them. If you read carefully: \"The logs must be stored so that they cannot be overwritten or deleted for 90 days\".\n\nNow why we need to store logs in second bucket? That's because \"S3 buckets with S3 Object Lock can't be used as destination buckets for server access logs\".\nReference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html"},{"upvote_count":"1","comment_id":"745053","poster":"BietTuot","timestamp":"1671020880.0","content":"I think the correct answer is D.\nThe other answers (A, B and C) don't allow you to lock objects and you can overwrite or delete them. If you read carefully: \"The logs must be stored so that they cannot be overwritten or deleted for 90 days\".\n\nNow why we need to store logs in second bucket? That's because \"S3 buckets with S3 Object Lock can't be used as destination buckets for server access logs\".\nReference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html"},{"timestamp":"1670958000.0","poster":"beznika","comment_id":"744384","upvote_count":"3","content":"I think it's D. They specify that the logs cannot be overwritten or deleted for 90 days. Only D option with the object lock allows this. https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html The other 3 options don't protect objects from deletion or overwriting."},{"poster":"cosmogen","upvote_count":"1","comment_id":"731522","comments":[{"comment_id":"731855","upvote_count":"1","content":"Logging options for Amazon S3:\nYou can record the actions that are taken by users, roles, or AWS services on Amazon S3 resources and maintain log records for auditing and compliance purposes. To do this, you can use server-access logging, AWS CloudTrail logging, or a combination of both. We recommend that you use CloudTrail for logging bucket-level and object-level actions for your Amazon S3 resources. For more information about each option, see the following sections:\n* Logging requests using server access logging\n* Logging Amazon S3 API calls using AWS CloudTrail\nThe following table lists the key properties of CloudTrail logs and Amazon S3 server-access logs. To make sure that CloudTrail meets your security requirements, review the table and notes.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","timestamp":"1669833240.0","poster":"cosmogen"}],"content":"Selected Answer: A\nRead again the question:”keep logs of the IP addresses from authentication failures” that “result from attempts to access objects in the bucket” . “That result from attempts to access objects in the bucket” , for me it’s mean authorization fails(AccessDenied).So, for me B,C,D technically are not possible. Vote for A","timestamp":"1669815120.0"},{"poster":"CloudHandsOn","comment_id":"731507","upvote_count":"4","content":"D is the correct answer. Here is why..\nWhen logs are enabled for the S3 bucket, the destination needed is another s3 bucket (1). If logs are sent to CloudWatch, S3 Data Events will need to be enabled and delivered to CloudTrail, which in turn can be delivered to CloudWatch Logs (2).\n\nReferences:\n1.) https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html\n2.) https://repost.aws/questions/QUktXj6H2NT1mPM3ZTulhTOA/s-3-server-access-logs-to-cloudwatch","timestamp":"1669814460.0","comments":[{"comment_id":"731531","timestamp":"1669815420.0","content":"CORRECTION: I think the answer is A, and agree with princajen. \nC is NOT the answer because you cant send directly to CW from S3, you will need CT.\nD is NOT the answer because you cant enable S3 Object lock on the target bucket.","poster":"CloudHandsOn","upvote_count":"1"}]},{"poster":"cosmogen","upvote_count":"1","comment_id":"725574","comments":[{"upvote_count":"1","content":"As I read it agin now, may be D is not correct, because as you can read from the article, when you enable logging:”The target bucket must be in the same AWS Region and AWS account as the source bucket, and must not have a default retention period configuration.“ So target bucket must not have default retention period configuration. And one more:\nNote:\nS3 buckets with S3 Object Lock can't be used as destination buckets for server access logs.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\nSo may be A is correct. It’s a tricky question.","poster":"cosmogen","timestamp":"1669271400.0","comment_id":"725598"}],"timestamp":"1669267860.0","content":"When you enable s3 server access logging, you must specify target bucket. I can’t see any option to configure logs to be saved to Cloud watch.\n‘By default, Amazon S3 doesn't collect server access logs. When you enable logging, Amazon S3 delivers access logs for a source bucket to a target bucket that you choose. The target bucket must be in the same AWS Region and AWS account as the source bucket, and must not have a default retention period configuration.“\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html\nSo for me , may be D is correct."},{"timestamp":"1668927900.0","poster":"mlantonis2","comment_id":"722441","upvote_count":"2","content":"C is correct."},{"poster":"Liongeek","content":"Selected Answer: C\nAns: C","upvote_count":"1","comment_id":"721499","timestamp":"1668796260.0"},{"upvote_count":"2","content":"Selected Answer: C\nA - CloudTrail does not deliver logs for requests that fail authentication\nC - Correct\nD - Not possible - You can't enable S3 Object Lock on the target bucket.(see https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html)","comments":[{"poster":"defmania00","content":"You can't forward those logs to CW Logs with access logging.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html","comment_id":"823505","timestamp":"1677491940.0","upvote_count":"2"}],"comment_id":"717889","timestamp":"1668421200.0","poster":"fedorian"}],"answers_community":["D (74%)","12%","9%"],"answer_description":"","answer_images":[],"isMC":true,"timestamp":"2022-09-01 20:17:00","unix_timestamp":1662056220,"answer_ET":"D","question_id":311,"exam_id":34},{"id":"Qlw4FXTo9KYB6QmCvHaK","answer_images":[],"answer_ET":"AE","question_text":"A SysOps administrator notices that the cache hit ratio for an Amazon CloudFront distribution is less than 10%. The SysOps administrator needs to increase the cache hit ratio for the distribution, improve network performance, and reduce the load on the origin.\n\nWhich combination of actions should the SysOps administrator take to meet these requirements? (Choose two.)","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/130029-exam-aws-certified-sysops-administrator-associate-topic-1/","isMC":true,"question_images":[],"answer":"AE","timestamp":"2023-12-31 22:51:00","topic":"1","choices":{"A":"Enable CloudFront Origin Shield for the required AWS Regions.","E":"Increase the CloudFront TTL values in the cache behavior settings.","D":"Turn on automatic compression of objects in the cache behavior settings.","B":"Change the viewer protocol policy to use HTTPS only.","C":"Add a second origin. Create an origin group that includes both origins. Activate CloudFront origin failover."},"question_id":312,"unix_timestamp":1704059460,"answers_community":["AE (100%)"],"discussion":[{"upvote_count":"5","timestamp":"1719819600.0","poster":"WinAndWin","content":"Selected Answer: AE\nA is correct as document from https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/origin-shield.html\nE is second one.","comment_id":"1111068"},{"timestamp":"1728097980.0","upvote_count":"3","content":"Selected Answer: AE\nA: Enable CloudFront Origin Shield for the required AWS Regions. \nProvides: better cache hit ratio, reduced origin load, and better network performance.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/origin-shield.html\n\nE: Increase the CloudFront TTL values in the cache behavior settings.\nIncreasing the Time To Live (TTL) values in the cache behavior settings allows CloudFront to cache objects for a longer period, reducing the number of requests that need to go to the origin server. This can significantly improve the cache hit ratio and reduce the load on the origin server.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html","poster":"joshnort","comment_id":"1189633"},{"poster":"LemonGremlin","timestamp":"1719777060.0","comment_id":"1110831","upvote_count":"3","content":"Selected Answer: AE\nA. Enable CloudFront Origin Shield for the required AWS Regions.\n\nE. Increase the CloudFront TTL values in the cache behavior settings."}],"exam_id":34},{"id":"vo8xQV9Cfq4kmrp2DjDE","unix_timestamp":1704102240,"answer":"A","answer_images":[],"exam_id":34,"question_text":"A custom application must be installed on all Amazon EC2 instances. The application is small, updated frequently, and can be installed automatically.\n\nHow can the application be deployed on new EC2 instances?","timestamp":"2024-01-01 10:44:00","question_images":[],"answer_description":"","question_id":313,"url":"https://www.examtopics.com/discussions/amazon/view/130047-exam-aws-certified-sysops-administrator-associate-topic-1/","topic":"1","discussion":[{"upvote_count":"1","content":"Selected Answer: D\nOption D is the answer since it addresses the frequent updates issue as well whereas A does not.","comment_id":"1320618","poster":"Slays","timestamp":"1733069820.0"},{"timestamp":"1719754680.0","poster":"SysOps4","content":"Option A - key word \"NEW EC2 instances\" . We are interested only this app to be present in newly created EC2, question is not how to maintain app version in long-term...\n\nFor me \"update frequently\" is to guide that this app should not be part of the AMI - (thus not option C).","upvote_count":"3","comment_id":"1239662"},{"comment_id":"1225021","poster":"Student013657","timestamp":"1717625940.0","content":"Selected Answer: A\nThis is the most appropriate solution for your use case. Amazon EC2 user data is a feature that allows you to pass a script or a set of commands to the instance at launch time. This script can be used to download and install the custom application automatically when a new EC2 instance is launched. This is a simple and effective way to deploy the application, especially since it is small, updated frequently, and can be installed automatically.\n\nKey words: new instance -> user data scripts run only during the first boot cycle when an EC2 instance is launched.","upvote_count":"1"},{"timestamp":"1716811200.0","comment_id":"1219526","poster":"ovladan","content":"Selected Answer: D\nNot A => By default, user data scripts run only during the first boot cycle when an EC2 instance is launched. \"A\" does not solve \"update frequently\"","upvote_count":"1"},{"poster":"mestule","upvote_count":"2","content":"Selected Answer: A\ndeployed on NEW ec2 instances","timestamp":"1714349460.0","comment_id":"1203742"},{"comments":[{"content":"New EC2 instances means it should be on a user data script. Hence A","timestamp":"1711493340.0","comment_id":"1183679","poster":"Yowie351","upvote_count":"5"}],"timestamp":"1710085440.0","comment_id":"1170420","poster":"vivanchyk","content":"Selected Answer: D\nvoted D. is it a good practice to install app using instance user data at all? sounds really weird to me.","upvote_count":"3"},{"poster":"Kipalom","content":"deployed on NEW EC2 instances. So it really seems to be about the user data. But for every update, we need to create a new instance. So alternative answer would be D. What do you think guys?","upvote_count":"3","comments":[{"content":"Yea I agree to ur point on this cuz it's never a good idea to spin up a new EC2 everytime the app. needs a frequent update... For that, CodeDeploy sounds much robust and long term solution IMO.","timestamp":"1729520040.0","poster":"Aamee","comment_id":"1301057","upvote_count":"1"}],"comment_id":"1111415","timestamp":"1704139740.0"},{"comment_id":"1111155","timestamp":"1704112800.0","comments":[{"upvote_count":"1","content":"but u're missing the point of 'frequently updated' requirement in the question. How would you fulfil that through option A??..","poster":"Aamee","comment_id":"1306651","timestamp":"1730668860.0"}],"upvote_count":"4","poster":"nharaz","content":"Selected Answer: A\nUser data allows you to run scripts or execute commands on an EC2 instance during launch. You can include the script that downloads and installs the application in the user data section when launching an EC2 instance. This allows the application to be automatically deployed when the instance is launched"},{"content":"Selected Answer: A\nAs What I search from examtopic.com. A is correct one.","timestamp":"1704102240.0","poster":"WinAndWin","upvote_count":"3","comment_id":"1111069"}],"answers_community":["A (71%)","D (29%)"],"choices":{"D":"Configure AWS CodePipeline to deploy code changes and updates.","B":"Create a custom API using Amazon API Gateway to call an installation executable from an AWS CloudFormation template.","C":"Use AWS Systems Manager to inject the application into an AMI.","A":"Launch a script that downloads and installs the application using Amazon EC2 user data."},"answer_ET":"A","isMC":true},{"id":"KrxYbNOQZfQwQPjh9K6c","isMC":true,"unix_timestamp":1704102480,"answer":"D","answers_community":["D (100%)"],"choices":{"C":"Create an IAM user that has access to the object. Share the credentials with the users.","A":"Attach an S3 bucket policy that only allows object downloads from the users' IP addresses.","D":"Generate a presigned URL for the object. Share the URL with the users.","B":"Create an IAM role that has access to the object. Instruct the users to assume the role."},"exam_id":34,"topic":"1","question_text":"A SysOps administrator wants to securely share an object from a private Amazon S3 bucket with a group of users who do not have an AWS account.\n\nWhat is the MOST operationally efficient solution that will meet this requirement?","url":"https://www.examtopics.com/discussions/amazon/view/130048-exam-aws-certified-sysops-administrator-associate-topic-1/","answer_ET":"D","discussion":[{"upvote_count":"1","comment_id":"1197806","content":"Selected Answer: D\nPresigned is the way to go!","poster":"tgv","timestamp":"1729241880.0"},{"comment_id":"1140534","timestamp":"1722798900.0","poster":"Learning4life","upvote_count":"2","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html"},{"upvote_count":"3","comment_id":"1111190","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html","poster":"nharaz","timestamp":"1719833160.0"},{"poster":"WinAndWin","comment_id":"1111070","content":"Selected Answer: D\nD is correct. https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html","timestamp":"1719820080.0","upvote_count":"3"}],"timestamp":"2024-01-01 10:48:00","answer_images":[],"answer_description":"","question_id":314,"question_images":[]},{"id":"8DcvZ3OQuxYziEUXvgjN","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/130049-exam-aws-certified-sysops-administrator-associate-topic-1/","answers_community":["B (100%)"],"answer_images":[],"unix_timestamp":1704102780,"choices":{"D":"Modify the DB cluster to use a burstable instance type.","A":"Increase the read capacity units (RCUs) and the write capacity units (WCUs) on the database.","C":"Turn on enhanced networking for the DB instances.","B":"Configure RDS Proxy. Update the application with the RDS Proxy endpoint."},"exam_id":34,"question_images":[],"question_text":"A company is running an ecommerce application on AWS. The application maintains many open but idle connections to an Amazon Aurora DB cluster. During times of peak usage, the database produces the following error message: \"Too many connections.\" The database clients are also experiencing errors.\n\nWhich solution will resolve these errors?","timestamp":"2024-01-01 10:53:00","discussion":[{"poster":"dinuts","comment_id":"1152228","timestamp":"1723835820.0","upvote_count":"2","content":"Selected Answer: B\nB-->Using RDS Proxy, you can handle unpredictable surges in database traffic that otherwise might cause issues due to oversubscribing connections or creating new connections at a fast rate. RDS Proxy establishes a database connection pool and reuses connections in this pool without the memory and CPU overhead of opening a new database connection each time. To protect the database against oversubscription, you can control the number of database connections that are created. https://aws.amazon.com/rds/proxy/"},{"poster":"Learning4life","comment_id":"1140535","content":"Selected Answer: B\nToo many connections -> RDS Proxy","timestamp":"1722799080.0","upvote_count":"2"},{"upvote_count":"3","comment_id":"1111074","content":"Selected Answer: B\nB is correct. \nRDS proxy is solved the issue https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.howitworks.html","poster":"WinAndWin","timestamp":"1719820380.0"}],"question_id":315,"answer_ET":"B","answer_description":"","topic":"1","answer":"B"}],"exam":{"isMCOnly":false,"id":34,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":477,"provider":"Amazon","name":"AWS Certified SysOps Administrator - Associate"},"currentPage":63},"__N_SSP":true}