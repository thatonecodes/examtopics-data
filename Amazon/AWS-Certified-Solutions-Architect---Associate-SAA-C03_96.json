{"pageProps":{"questions":[{"id":"K2tPMnxVwAZAggLMMLP2","url":"https://www.examtopics.com/discussions/amazon/view/109531-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"B","question_id":476,"answer":"B","exam_id":31,"topic":"1","choices":{"D":"Modify the network ACL for the application tier subnets. Add an inbound deny rule for the IP addresses that are consuming resources.","B":"Modify the network ACL for the web tier subnets. Add an inbound deny rule for the IP addresses that are consuming resources.","C":"Modify the inbound security group for the application tier. Add a deny rule for the IP addresses that are consuming resources.","A":"Modify the inbound security group for the web tier. Add a deny rule for the IP addresses that are consuming resources."},"unix_timestamp":1684324680,"question_text":"A company operates a two-tier application for image processing. The application uses two Availability Zones, each with one public subnet and one private subnet. An Application Load Balancer (ALB) for the web tier uses the public subnets. Amazon EC2 instances for the application tier use the private subnets.\n\nUsers report that the application is running more slowly than expected. A security audit of the web server log files shows that the application is receiving millions of illegitimate requests from a small number of IP addresses. A solutions architect needs to resolve the immediate performance problem while the company investigates a more permanent solution.\n\nWhat should the solutions architect recommend to meet this requirement?","answer_images":[],"isMC":true,"answer_description":"","answers_community":["B (90%)","10%"],"discussion":[{"upvote_count":"26","timestamp":"1701162540.0","poster":"lucdt4","content":"Selected Answer: B\nA wrong because security group can't deny (only allow)","comment_id":"908396"},{"upvote_count":"10","timestamp":"1700400780.0","poster":"cloudenthusiast","content":"Selected Answer: B\nIn this scenario, the security audit reveals that the application is receiving millions of illegitimate requests from a small number of IP addresses. To address this issue, it is recommended to modify the network ACL (Access Control List) for the web tier subnets.\n\nBy adding an inbound deny rule specifically targeting the IP addresses that are consuming resources, the network ACL can block the illegitimate traffic at the subnet level before it reaches the web servers. This will help alleviate the excessive load on the web tier and improve the application's performance.","comment_id":"901876"},{"comments":[{"comment_id":"1352252","poster":"Besisco","upvote_count":"1","content":"agree, even if an app tier was under attack, the block rule should be applied at a web tier ACL","timestamp":"1738829040.0"}],"comment_id":"1126818","upvote_count":"3","timestamp":"1721400240.0","poster":"awsgeek75","content":"Selected Answer: B\nA: Wrong as SG cannot deny. By default everything is deny in SG and you allow stuff\nCD: App tier is not under attack so these are irrelevant options\nB: Correct as NACL is exactly for this access control list to define rules for CIDR or IP addresses"},{"timestamp":"1715230560.0","content":"Selected Answer: B\nModify the network ACL for the web tier subnets. Add an inbound deny rule for the IP addresses that are consuming resources.","upvote_count":"3","poster":"TariqKipkemei","comment_id":"1066196"},{"timestamp":"1714946400.0","content":"Selected Answer: B\nA is wrong\nSecurity groups act at the network interface level, not the subnet level, and they support Allow rules only.","poster":"potomac","comment_id":"1063399","upvote_count":"3"},{"comments":[{"upvote_count":"3","content":"Security group rules are always permissive; you can't create rules that deny access.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html","timestamp":"1720401900.0","comment_id":"1116365","poster":"OSHOAIB"},{"upvote_count":"3","timestamp":"1716078600.0","poster":"Goutham4981","comment_id":"1074408","content":"Security group can't deny."}],"upvote_count":"1","comment_id":"1016254","poster":"Devsin2000","timestamp":"1711333860.0","content":"Selected Answer: A\nThe security Group can be applied to an ALB at web tier."},{"content":"Selected Answer: B\nSince the bad requests are targeting the web tier, adding ACL deny rules for those IP addresses on the web subnets will block the traffic before it reaches the instances.\n\nSecurity group changes (Options A and C) would not be effective since the requests are not even reaching those resources.\n\nModifying the application tier ACL (Option D) would not stop the bad traffic from hitting the web tier.","upvote_count":"3","timestamp":"1708693920.0","poster":"Guru4Cloud","comment_id":"988218"},{"upvote_count":"3","poster":"fakrap","content":"Selected Answer: B\nA is wrong because you cannot put any deny in security group","timestamp":"1700644920.0","comment_id":"903808"},{"poster":"Rob1L","timestamp":"1700555220.0","content":"Selected Answer: B\nYou cannot Deny on SG, so it's B","upvote_count":"6","comment_id":"902944"},{"content":"Selected Answer: A\nOption B is not as effective as option A","comments":[{"timestamp":"1700292540.0","poster":"y0","comments":[{"content":"yeah, my mistake. B should be","upvote_count":"2","comment_id":"901813","poster":"nosense","timestamp":"1700394060.0"}],"upvote_count":"3","content":"Security group only have allow rules","comment_id":"900771"},{"upvote_count":"3","comment_id":"901877","content":"A and C out due to the fact that SG does not have deny on allow rules.","timestamp":"1700400840.0","poster":"cloudenthusiast"}],"upvote_count":"5","timestamp":"1700229480.0","poster":"nosense","comment_id":"900090"}],"question_images":[],"timestamp":"2023-05-17 13:58:00"},{"id":"uQ3dNE5iuspxVi3QJDW5","discussion":[{"comments":[{"comments":[{"timestamp":"1691588640.0","upvote_count":"4","poster":"Clouddon","comment_id":"976703","content":"When you send an email with Amazon SES, the email information you need to provide depends on how you call Amazon SES. You can provide a minimal amount of information and have Amazon SES take care of all of the formatting for you. Or, if you want to do something more advanced like send an attachment, you can provide the raw message yourself. https://docs.aws.amazon.com/ses/latest/dg/send-email-concepts-email-format.html"}],"upvote_count":"7","timestamp":"1680323460.0","poster":"apchandana","comment_id":"857573","content":"this document is talking about the SES API. not ses. SES does not format data. just sending emails.\nhttps://aws.amazon.com/ses/"}],"upvote_count":"33","content":"Selected Answer: BD\nYou can use SES to format the report in HTML.\nhttps://docs.aws.amazon.com/ses/latest/dg/send-email-formatted.html","comment_id":"696300","timestamp":"1665931560.0","poster":"whosawsome"},{"content":"Selected Answer: BD\nB&D are the only 2 correct options. If you are choosing option E then you missed the daily morning schedule requirement mentioned in the question which cant be achieved with S3 events for SNS. Event Bridge can used to configure scheduled events (every morning in this case). Option B fulfills the email in HTML format requirement (by SES) and D fulfills every morning schedule event requirement (by EventBridge)","timestamp":"1667523360.0","comments":[{"comment_id":"1282064","content":"Option B doesn't specify an API interface...","timestamp":"1726055400.0","upvote_count":"1","poster":"PaulGa"},{"comments":[{"content":"But E does not include a glue action. We need either C or D for the scheduling, plus B or E for the email sending.","upvote_count":"2","timestamp":"1704436380.0","poster":"pentium75","comment_id":"1114312"}],"content":"I don't believe you are correct when you say that E cannot meet the scheduling requirement. If the glue action is scheduled and outputs to S3, then as the S3 event destination is SNS, in effect you have a way of getting SNS to have a scheduled release.","comment_id":"950518","timestamp":"1689242160.0","upvote_count":"2","poster":"RupeC"},{"upvote_count":"2","content":"the daily schedule can be achieve with event bridge\n - schedule and event bridge to trigger daily\n - the event briodge will trigger a lambda function that will collect data and save it in s3\n - once data in s3 the event noitification will trigger SNS to send emails","poster":"slimen","timestamp":"1698728700.0","comment_id":"1058457"}],"upvote_count":"26","comment_id":"710854","poster":"backbencher2022"},{"poster":"RamanadhRavinuthala","comments":[{"content":"https://docs.aws.amazon.com/ses/latest/dg/send-email-concepts-email-format.html","upvote_count":"1","timestamp":"1731081780.0","poster":"JA2018","comment_id":"1308861"}],"timestamp":"1721719980.0","upvote_count":"3","content":"Selected Answer: BD\nD Explanation: EventBridge can be used to schedule regular invocations of a Lambda function that retrieves the required data from the application's API. This step sets up the process to collect the data at the specified time every morning.\n\nB Explanation: Amazon SES can format the data into an easy-to-read HTML report and send the email to multiple recipients efficiently.","comment_id":"1253472"},{"upvote_count":"5","poster":"hieunt.hus","content":"Selected Answer: BC\nSNS & Glue","timestamp":"1720869180.0","comment_id":"1247274"},{"comment_id":"1239346","content":"BC - Question says extract the data - which is the job of AWS Glue and then invoke SES endpoint to send email. SES support html format.\n\nBut BD also seems possible.","upvote_count":"5","poster":"jatric","timestamp":"1719676500.0"},{"content":"Defo B and D\nLook at the aws notes on Sns\nNotes\nYou can't customize the body of the email message. The email delivery feature is intended to provide internal system alerts, not marketing messages.\n\nIt can't send anything but notifications. SES can send normal emails therefore, BD is the answer.","timestamp":"1716654660.0","upvote_count":"3","comment_id":"1218449","poster":"lofzee"},{"timestamp":"1711934820.0","comment_id":"1187119","content":"Selected Answer: BE\nOption B suggests using Amazon SES, which allows you to format the data and email the report to multiple recipients in an efficient and scalable way.\n\nOption E proposes storing application data in Amazon S3, which is scalable and durable storage. By configuring an Amazon SNS topic as an S3 event destination, you can automatically trigger the report to be sent via email whenever new data is added to S3.","poster":"jhoiti","upvote_count":"1"},{"content":"Selected Answer: BD\nChat GPT says :\nWith Amazon SES, you can send rich, formatted email content, including text, HTML, attachments, and embedded images, suitable for email communication.\nAmazon SNS is primarily used for sending plain-text or JSON-formatted messages, suitable for notifications and alerts across different channels.\n\nThis can suggest that we need to use SES if we want to use HTML content.","timestamp":"1711305480.0","poster":"Monster07","upvote_count":"1","comment_id":"1181915"},{"upvote_count":"10","content":"Answer: B and D\nOther options\nA. Amazon Kinesis Data Firehose: This service is typically used for real-time streaming data processing rather than for scheduled tasks like generating a morning report.\n\nC. Amazon EventBridge to invoke an AWS Glue job: AWS Glue is a data integration service that's more focused on ETL (extract, transform, load) operations, often involving large datasets and complex transformations, which might be more than needed for this scenario.\n\nE. Amazon S3 with SNS topic: Storing data in S3 and using SNS for notification is viable, but this doesn't directly address the need to format the data into HTML and send it as an email report. SNS is better suited for sending notifications rather than formatted reports.","timestamp":"1705240740.0","comment_id":"1122573","poster":"wyejay","comments":[{"poster":"tonybuivannghia","content":"Some mistakes that Kinesis Data Firehose is used for near real-time streaming data processing. But totally I agree with you that B & D are correct.","upvote_count":"2","timestamp":"1726634340.0","comment_id":"1285577"}]},{"poster":"awsgeek75","content":"Selected Answer: BD\nVery detailed question so let's break it down:\n\"send the report to several email addresses at the same time every morning\" this locks B as nothing else can do it.\nA: Firehose to collect data from API will work but it cannot generate a report\nC: Glue is ETL, it cannot extract data from an API\nE: Store data in S3. No idea what this will help with\n\nThe API provides order shipping data so you can query it. Lambda can be used to query the API easily so D is good choice that works with B.\n\nBD is correct combination","comment_id":"1122564","timestamp":"1705240200.0","upvote_count":"3"},{"content":"Selected Answer: BD\n\"At the same time every morning\" requires scheduling, which is only mentioned in C and D. AWS Glue has no native functionality to query REST APIs, thus we need a Lambda function -> D.\n\nFor email we need SES or SNS, but as we want \"an easy-to-read HTML format\", SNS is out. SNS can send notifications, not formatted emails. Thus B.","upvote_count":"5","comment_id":"1114317","poster":"pentium75","timestamp":"1704436980.0"},{"timestamp":"1700523720.0","comment_id":"1075933","content":"Key: Send email every morning same time - 1. Simple email 2. AWS Event Bridge with lambda","upvote_count":"2","poster":"[Removed]"},{"poster":"wearrexdzw3123","content":"Selected Answer: B\nI think there is a problem with the answer. It should be that ses sends the email processed by lambda.","upvote_count":"1","timestamp":"1699780740.0","comment_id":"1068389"},{"content":"Selected Answer: BD\nKey: retrieval by a REST API, that's why use lambda","comment_id":"1051929","upvote_count":"2","poster":"tom_cruise","timestamp":"1698069480.0"},{"upvote_count":"4","comments":[{"comment_id":"1105094","content":"Why would I need to \"store the data\"? Wouldn't the Lamba function just call the SES API?","upvote_count":"2","poster":"pentium75","timestamp":"1703490840.0"}],"content":"Selected Answer: DE\nBoth SES and SNS can format html, but there is a disconnection between B and D. Where do you store the data between the steps?","poster":"tom_cruise","timestamp":"1696852140.0","comment_id":"1028853"},{"content":"Selected Answer: BD\nthe reason why \"B\" is more correct than \"E\" is because is more simple and you don't have to store data is not what they want, also SES is a service that is meant for sending the data through email, and is exactly what the company wants. is not the first time the admin is wrong with the answer","comment_id":"1020960","upvote_count":"3","timestamp":"1696006440.0","poster":"David_Ang"},{"poster":"hieulam","content":"Selected Answer: DE\nE should be correct:\nhttps://saturncloud.io/blog/how-to-send-html-mails-using-amazon-sns/","upvote_count":"1","comment_id":"1010984","timestamp":"1695089520.0","comments":[{"upvote_count":"3","content":"I believe BD are the answers. E can't be used, because, in E can't help with email formatting. E won't be the best choice even for scheduling.","timestamp":"1695271440.0","comment_id":"1012781","poster":"h_sahu"}]},{"comment_id":"974420","timestamp":"1691386620.0","content":"Selected Answer: BD\nCreate an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data. Then use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.","poster":"TariqKipkemei","upvote_count":"2"},{"poster":"miki111","timestamp":"1689779280.0","content":"Option BD is the correct answer","comment_id":"956748","upvote_count":"1"},{"poster":"miki111","timestamp":"1689554520.0","content":"Option BD is the right answer.","upvote_count":"1","comment_id":"953746"},{"poster":"RupeC","timestamp":"1689184800.0","upvote_count":"2","comment_id":"950070","content":"Selected Answer: CE\nGlue - is scheduled to prep the docs using its ETL functionality. Then E. puts the data into S3 and uses sns to send it out by email.","comments":[{"content":"On review, I think DE. D is better than C and glue is ETL but actually, the data needs to be queried, so Lambda is better. The eventbridge is scheduled so S3 and SNS will also by default be run immediately after the eventbridge rule has run.","comment_id":"957243","poster":"RupeC","timestamp":"1689837000.0","upvote_count":"2"}]},{"comments":[{"timestamp":"1703490900.0","comment_id":"1105095","poster":"pentium75","upvote_count":"1","content":"Why? Wouldn't the Lambda function just call the SES API?"}],"timestamp":"1688964600.0","comment_id":"947729","poster":"Mia2009687","content":"Selected Answer: DE\nB- Neither Lambda or SEM could hold the data. After the data being handled by Lambda, needs to store it in S3 before publishing to the end users.","upvote_count":"2"},{"poster":"MutiverseAgent","comments":[{"upvote_count":"1","comment_id":"1105097","timestamp":"1703490960.0","poster":"pentium75","content":"\"Save emails for future reference\" is NOT a requirement."}],"comment_id":"944752","upvote_count":"4","timestamp":"1688653920.0","content":"A: NOT (Firehose not needed here)\nB: NOT (SES supports HTML but does NOT explicitly format data)\nD: YES (Schedule process, extract & format)\nE: YES (Save emails in S3 for further reference, Send email)"},{"upvote_count":"1","poster":"Dhaysindhu","content":"Selected Answer: DE\nD: To schedule the event every morning and format the HTML\nE: To store the HTML in S3 and send the email using SNS","timestamp":"1687760400.0","comment_id":"934100"},{"comments":[{"comment_id":"1105099","poster":"pentium75","content":"But the Lambda function can ...","timestamp":"1703491020.0","upvote_count":"1"}],"timestamp":"1687760280.0","upvote_count":"1","comment_id":"934098","poster":"Mia2009687","content":"Selected Answer: DE\nSES cannot format the data."},{"upvote_count":"2","content":"Selected Answer: BD\nD: Create an EventBridge (CloudWatch Events) scheduled event that invokes Lambda to query API for data. This scheduled event can be set to trigger at desired time every morning to fetch shipping statistics from API.\n\nB: Use SES to format data and send report by email. In Lambda, after retrieving shipping statistics, you can format data into an easy-to-read HTML format using any HTML templating framework.\n\nOptions A, C, and E are not necessary for achieving the desired outcome. Ooption A is typically used for real-time streaming data ingestion and delivery to data lakes or analytics services. Glue (C) is a fully managed extract, transform, and load (ETL) service, which may be an overcomplication for this scenario. Storing the application data in S3 and using SNS (E) can be an alternative approach, but it adds unnecessary complexity.","comment_id":"929356","poster":"cookieMr","timestamp":"1687344420.0"},{"upvote_count":"1","poster":"Agustinr10","timestamp":"1686869040.0","content":"Selected Answer: CE\nExplanation:\n\nD. By creating an Amazon EventBridge scheduled event that triggers an AWS Lambda function, you can automate the process of querying the application's API for shipping statistics. The Lambda function can retrieve the data and perform any necessary formatting or transformation before proceeding to the next step.\n\nE. Storing the application data in Amazon S3 allows for easy accessibility and further processing. You can configure an S3 event notification to trigger an Amazon Simple Notification Service (SNS) topic whenever new data is uploaded to the S3 bucket. The SNS topic can be configured to send the report by email to the desired email addresses.","comment_id":"924621"},{"upvote_count":"1","timestamp":"1685962680.0","content":"Selected Answer: BD\nI go for BD options.","poster":"Bmarodi","comment_id":"915370"},{"comment_id":"888937","comments":[{"comment_id":"1114316","timestamp":"1704436680.0","upvote_count":"1","poster":"pentium75","content":"But Glue can't query REST APIs natively"}],"poster":"korn666","content":"Selected Answer: BC\nextract and transform the data\nAWS Glue is not always used for ETL processes that deal with unstructured data. Glue can also be used for ETL processes that deal with structured data. Glue provides a fully managed ETL service that makes it easy to move data between data stores. It can be used to transform and clean data in a scalable and cost-effective manner, and it supports a wide range of data formats, including both structured and unstructured data.","upvote_count":"4","timestamp":"1683146160.0"},{"upvote_count":"2","poster":"korn666","content":"BC\nextract and transform the data\nAWS Glue is not always used for ETL processes that deal with unstructured data. Glue can also be used for ETL processes that deal with structured data. Glue provides a fully managed ETL service that makes it easy to move data between data stores. It can be used to transform and clean data in a scalable and cost-effective manner, and it supports a wide range of data formats, including both structured and unstructured data.","comment_id":"888931","timestamp":"1683146040.0"},{"poster":"kruasan","comment_id":"875643","content":"Selected Answer: DE\nIn summary, option E is chosen because it provides a way to extract and organize the data into an easy-to-read HTML format and send it via email using Amazon S3 and Amazon SNS, respectively. While option B can be used to send the email, it does not provide a way to extract, organize, or store the data, which is a requirement in this case.","timestamp":"1681997100.0","upvote_count":"3"},{"content":"Selected Answer: DE\nSES is not used for formatting data . Whereas Email service can subscribe from SNS Topic. \nAnswer is DE","poster":"DIptyParashar","timestamp":"1680628800.0","comment_id":"861326","upvote_count":"2"},{"timestamp":"1674124920.0","content":"Selected Answer: BD\nYou can't use SNS for HTML e-mails","upvote_count":"5","poster":"BlueVolcano1","comment_id":"781026"},{"comment_id":"779624","upvote_count":"1","timestamp":"1674019320.0","poster":"john626","content":"Selected Answer: BD\nhttps://kennbrodhagen.net/2016/01/31/how-to-return-html-from-aws-api-gateway-lambda/"},{"timestamp":"1673855700.0","poster":"John_Zhuang","content":"Selected Answer: BD\nFor anyone confused with Option E, I don't think the issue comes from the first part, i.e. using S3 notification every time in the morning. It may not be 100% right as the lambda function needs the help of EventBridge Rule to run on a schedule. But in general, the S3 notification can be triggered as the new object is uploaded by the lambda function.\n\nThe REAL problem comes from the second part of the statement, i.e. using SNS to send email. It is true that SNS can send emails, BUT it cannot be used to send HTML formatted emails as SNS could handle. \nhttps://stackoverflow.com/questions/32241928/sending-html-content-in-aws-snssimple-notification-service-emails-notification","upvote_count":"3","comment_id":"777427"},{"timestamp":"1673055780.0","comment_id":"768190","upvote_count":"1","content":"Selected Answer: BD\nTo meet the requirements, the solutions architect can create an Amazon EventBridge (formerly known as Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data. The scheduled event can be configured to run at the desired time every morning. The Lambda function can be responsible for querying the API, formatting the data into an HTML format, and sending the report by email using Amazon Simple Email Service (Amazon SES).","poster":"SilentMilli"},{"upvote_count":"4","content":"Selected Answer: BC\nWhy is no one noticing the 'extract' key word? That's key for using Glue. Eventbridge can trigger Glue which extracts from the API and transforms the data to send it to SES.","comment_id":"765303","comments":[{"poster":"goodmail","upvote_count":"6","timestamp":"1673186940.0","content":"AWS Glue is always used for ETL processes that deal with unstructured data. When using Glue, usually the data will be sent to big data storage like Redshift. It is seldomly used for just sending email.\nLambda can easily get API data and do any filtering, let say some python code to extract JSON from API.","comment_id":"769498"}],"poster":"Trix786","timestamp":"1672812240.0"},{"timestamp":"1672351020.0","poster":"Mars2k","content":"Selected Answer: BD\nWith SNS you can't customize the body of the email message. The email delivery feature is intended to provide internal system alerts","comment_id":"761518","upvote_count":"2"},{"upvote_count":"3","comment_id":"752353","content":"Selected Answer: BD\nD, Eventbridge = scheduled events, lambda = function that queries API for the data\nB, SES (simple email service) = formats the data which then can be sent via email\nA, Firehose = streaming\nC, Glue = ETL service\nE, S3 = SSS \nA, C and E don't solve the problem of querying REST API for the data","timestamp":"1671629160.0","poster":"pazabal"},{"comment_id":"751152","upvote_count":"2","content":"Selected Answer: BD\nD. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.\n\nB. Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.\n\nTo meet the requirements, a solutions architect could create a scheduled event using Amazon EventBridge (formerly known as Amazon CloudWatch Events) that invokes an AWS Lambda function at a specific time every morning. The Lambda function could then query the application's API to retrieve the shipping statistics, format the data into an easy-to-read HTML format, and send the report by email using Amazon Simple Email Service (Amazon SES). This would allow the company to automate the process of retrieving and sending the shipping statistics report.","poster":"Buruguduystunstugudunstuy","timestamp":"1671551400.0"},{"upvote_count":"1","poster":"duriselvan","comment_id":"750709","timestamp":"1671528360.0","content":"BD is correct ANs \nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html"},{"poster":"career360guru","content":"Selected Answer: DE\nD and E is better choice given the e-mail needs to contain a report data.\nCan be done using integrating Lambda with SES but that will require some code to invoke SES from Lambda. SNS provides the e-mail as publishing functionality and it can even retry mechanism etc...","timestamp":"1671420120.0","upvote_count":"1","comment_id":"749410"},{"poster":"arseyam","comment_id":"748735","content":"Selected Answer: BC\nThis is a typical scenario for Extract-Transform-Load which means AWS GLUE\n\nThe below article shows how you can extract data from a web API\nhttps://blog.clairvoyantsoft.com/extracting-data-from-a-web-service-via-aws-glue-570035b38988\n\nYou can start AWS Glue using AWS EventBridge\nhttps://docs.aws.amazon.com/glue/latest/dg/starting-workflow-eventbridge.html","timestamp":"1671358860.0","upvote_count":"3"},{"poster":"shw1981","timestamp":"1670440500.0","comment_id":"738272","content":"Selected Answer: BD\nB and D","upvote_count":"2"},{"content":"D E, SES for the marketing email. DE combined will do the JOB.","poster":"reeba_908","upvote_count":"2","comment_id":"737299","timestamp":"1670371320.0"},{"content":"B and D","timestamp":"1669038780.0","comment_id":"723564","upvote_count":"1","poster":"Wpcorgan"},{"poster":"pepgua","comment_id":"722510","timestamp":"1668937560.0","upvote_count":"7","content":"We seem to agree option D is correct. The second choice is between B (SES) and E (SNS). SES is the best answer as it's specifically designed for Email service. SNS can also deliver notifications via email but it's not designed for that HTML format. BD is correct.\nhttps://stackoverflow.com/questions/32241928/sending-html-content-in-aws-snssimple-notification-service-emails-notification"},{"content":"Selected Answer: BD\nNot C because there is no direct connector available for Glue to connect to the internet world (REST API), you can set up a VPC, with a public and a private subnet.","comment_id":"711854","comments":[{"content":"How could we know that the REST API is only Internet-facing and not e.g. accessible via an VPC endpoint. I wouldn't rule out Glue based on this assumption since the question does not specify the REST API further.","timestamp":"1672676580.0","poster":"Associate999","upvote_count":"1","comment_id":"763843"}],"poster":"baiy","upvote_count":"3","timestamp":"1667662200.0"},{"upvote_count":"1","timestamp":"1667483520.0","comment_id":"710536","content":"Selected Answer: DE\nYou can use SES to format the report in HTML.","poster":"santosh2316"},{"upvote_count":"4","content":"Selected Answer: BC\ni think BC or CE\nGlue should be the winner here with for ETL\ni personally think SES is the winner here too because of this\nhttps://docs.aws.amazon.com/ses/latest/dg/send-email-formatted.html","timestamp":"1666631160.0","poster":"Six_Fingered_Jose","comment_id":"703220","comments":[{"timestamp":"1670251200.0","comment_id":"736012","content":"Glue is event-driven whereas the requirement is for a scheduled task (run every morning), so is wrong.","upvote_count":"2","poster":"JayBee65"}]},{"timestamp":"1666309800.0","poster":"BoboChow","content":"Selected Answer: DE\nAbout sending the report to several email addresses, I would rather go for SNS than SES;\nAbout extracting the shipping statistics, I would rather go for Lambda than Glue;","comment_id":"700389","upvote_count":"2"},{"comment_id":"696014","timestamp":"1665898740.0","comments":[{"poster":"Newptone","upvote_count":"3","timestamp":"1667846700.0","comment_id":"713244","content":"You didn't check SNS limitation: By contrast, Amazon Simple Notification Service (Amazon SNS) is for messaging-oriented applications, the body of an Amazon SNS notification is limited to 8192 characters of UTF-8 strings, and isn't intended to support multimedia content.\nIt does not support \"easy-to-read HTML format\" email.\nhttps://aws.amazon.com/ses/faqs/#Amazon_SES_and_Other_AWS_Services"}],"poster":"KVK16","upvote_count":"3","content":"Selected Answer: DE\nGlue cannot output HTML Report, you'll need a Lambda. Also there is need to invoke the Lambda at a specific time - CRON Job - Thumbrule - Cloudwatch Events + Lambda. The reports are stored in S3 and SNS is invoke to email the reports to same selected emails everyday. \n\nSNS vs SES - \nSES Cannot extract data and it sends the emails without the consent of the receivers only take verification of the sender \nSchedule Lambda to"},{"upvote_count":"2","timestamp":"1665867660.0","content":"C.E.\nGlue job is good for ETL (extract transform load).","poster":"FFORTUNATE","comment_id":"695729"}],"url":"https://www.examtopics.com/discussions/amazon/view/85557-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","question_text":"A company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)","choices":{"B":"Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.","D":"Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.","A":"Configure the application to send the data to Amazon Kinesis Data Firehose.","C":"Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.","E":"Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email."},"answer_images":[],"answers_community":["BD (69%)","14%","Other"],"exam_id":31,"timestamp":"2022-10-15 23:01:00","topic":"1","answer_ET":"BD","question_id":477,"isMC":true,"unix_timestamp":1665867660,"answer":"BD","question_images":[]},{"id":"aDKu1241wnHoUoL9Tz61","answer_description":"","choices":{"D":"Create a transit gateway with a peering attachment between the eu-west-1 VPC and the ap-southeast-2 VPC. After the transit gateways are properly peered and routing is configured, create an inbound rule in the database security group that references the security group ID of the application servers in eu-west-1.","C":"Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables. Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.","A":"Create a VPC peering connection between the eu-west-1 VPC and the ap-southeast-2 VPC. Create an inbound rule in the eu-west-1 application security group that allows traffic from the database server IP addresses in the ap-southeast-2 security group.","B":"Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC. Update the subnet route tables. Create an inbound rule in the ap-southeast-2 database security group that references the security group ID of the application servers in eu-west-1."},"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/109708-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["C (89%)","11%"],"question_text":"A global marketing company has applications that run in the ap-southeast-2 Region and the eu-west-1 Region. Applications that run in a VPC in eu-west-1 need to communicate securely with databases that run in a VPC in ap-southeast-2.\n\nWhich network design will meet these requirements?","unix_timestamp":1684496220,"answer":"C","exam_id":31,"question_id":478,"topic":"1","answer_images":[],"discussion":[{"comment_id":"945330","poster":"VellaDevil","content":"Selected Answer: C\nAnswer: C -->\"You cannot reference the security group of a peer VPC that's in a different Region. Instead, use the CIDR block of the peer VPC.\"\nhttps://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html","upvote_count":"48","timestamp":"1688706900.0","comments":[{"comment_id":"1283560","timestamp":"1726304640.0","poster":"MatAlves","upvote_count":"2","content":"Wow, big thanks!"},{"timestamp":"1690347360.0","upvote_count":"4","content":"Thanks for this clarification!","comment_id":"963357","poster":"hsinchang"}]},{"poster":"Axeashes","comment_id":"924685","timestamp":"1686877440.0","upvote_count":"12","content":"Selected Answer: C\n\"You cannot reference the security group of a peer VPC that's in a different Region. Instead, use the CIDR block of the peer VPC.\"\nhttps://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html"},{"timestamp":"1699229100.0","content":"Selected Answer: C\nAfter establishing the VPC peering connection, the subnet route tables need to be updated in both VPCs to route traffic to the other VPC's CIDR blocks through the peering connection.","upvote_count":"3","comment_id":"1063401","poster":"potomac"},{"poster":"Bennyboy789","upvote_count":"4","comment_id":"992324","content":"Selected Answer: C\nVPC Peering Connection: This allows communication between instances in different VPCs as if they are on the same network. It's a straightforward approach to connect the two VPCs.\n\nSubnet Route Tables: After establishing the VPC peering connection, the subnet route tables need to be updated in both VPCs to route traffic to the other VPC's CIDR blocks through the peering connection.\n\nInbound Rule in Database Security Group: By creating an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses, you ensure that only the specified application servers from the eu-west-1 VPC can access the database servers in the ap-southeast-2 VPC.","timestamp":"1693235040.0"},{"poster":"Guru4Cloud","comments":[{"timestamp":"1705687680.0","poster":"awsgeek75","content":"No, you cannot use a SG reference from another region so last part \"Create an inbound rule in the ap-southeast-2 database security group that references the security group ID of the application servers in eu-west-1\" cannot be setup. This is why B is wrong.","comment_id":"1126866","upvote_count":"3"},{"comment_id":"988213","content":"Options A, C, D have flaws:\nOption A peer direction is wrong\nOption C opens databases to application server IP addresses rather than SG\nOption D uses transit gateway which is unnecessary for just two VPCs","poster":"Guru4Cloud","timestamp":"1692788520.0","upvote_count":"2"}],"timestamp":"1692788520.0","comment_id":"988212","upvote_count":"2","content":"Selected Answer: B\nB) Configure VPC peering between ap-southeast-2 and eu-west-1 VPCs. Update routes. Allow traffic in ap-southeast-2 database SG from eu-west-1 application server SG.\n\nThis option establishes the correct network connectivity for the applications in eu-west-1 to reach the databases in ap-southeast-2:\n\nVPC peering connects the two VPCs across regions - https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html#:~:text=You%20can%20create%20a%20VPC,%2DRegion%20VPC%20peering%20connection).\n\nUpdating route tables enables routing between the VPCs\nSecurity group rule allowing traffic from eu-west-1 application server SG to ap-southeast-2 database SG secures connectivity"},{"upvote_count":"1","timestamp":"1689569520.0","comment_id":"953834","content":"Selected Answer: C\nSelected C but B can also work","poster":"TariqKipkemei"},{"comments":[{"poster":"TariqKipkemei","upvote_count":"4","timestamp":"1699514880.0","comments":[{"comment_id":"1126867","content":"This is why B is wrong. You can never access cross region security group id","poster":"awsgeek75","upvote_count":"1","timestamp":"1705687740.0"}],"comment_id":"1066214","content":"Correction, You cannot reference the security group of a peer VPC that's in a different Region. Instead, use the CIDR block of the peer VPC. \nThe C is the only option here.\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html#:~:text=You%20cannot-,reference,-the%20security%20group"}],"content":"I just tried from the the console, You can specify the name or ID of another security group in the same region. To specify a security group in another AWS account (EC2-Classic only), prefix it with the account ID and a forward slash, for example: 111122223333/OtherSecurityGroup.\nYou can Specify a single IP address, or an IP address range in CIDR notation in the same/other region.\n\nIn the exam both option B and C would be a pass. In the real world both option will work.","timestamp":"1689569400.0","poster":"TariqKipkemei","upvote_count":"3","comment_id":"953831"},{"comments":[{"timestamp":"1688892780.0","content":"However, there was also a part of \"create an inbound rule in the database security group that references the security group ID of the application servers in eu-west-1\"\n\ntherefore, still C because we cannot reference SG ID of diff VPC, we should use the CIDR block","poster":"Iragmt","comment_id":"947024","upvote_count":"1"}],"content":"I realize D is right as ChatGpt indicates.Because here is not a problem just one application in a VPC connection to another in different region. Actually there many applications in different VPCs in a region which need to connect any other application crossingly in other region. So two transit gateway need to installed in two regions for multiple to multiple VPCs connections.","timestamp":"1688074320.0","comment_id":"938584","upvote_count":"2","poster":"Chris22usa"},{"content":"post it on ChaptGpt and it give me answer D. what heck with this?","timestamp":"1687918560.0","comment_id":"935993","upvote_count":"2","poster":"Chris22usa"},{"comment_id":"934782","timestamp":"1687812060.0","upvote_count":"1","content":"Selected Answer: C\nB is wrong because It is in a different region, so reference to the security group ID will not work. A is wrong because you need to update the route table. The answer should be C.","poster":"haoAWS"},{"comment_id":"932849","poster":"mattcl","upvote_count":"1","timestamp":"1687632360.0","content":"is B. what happens if application server IP addresses changes (Option C). You must change manually the IP in the security group again."},{"poster":"antropaws","upvote_count":"1","content":"Selected Answer: C\nI thought B, but I vote C after checking Axeashes response.","comment_id":"930271","timestamp":"1687420980.0"},{"upvote_count":"2","comment_id":"912548","content":"Selected Answer: C\nI think the answer is C because the security groups are in different VPCs. When the question wants to allow traffic from app vpc to database vpc i think using peering connection you will be able to add the security groups rules using private ip addresses of app servers. I don't think the database VPC will identify the security group id of another VPC.","poster":"HelioNeto","timestamp":"1685683080.0"},{"content":"D You cannot create a VPC peering connection between VPCs in different regions.","upvote_count":"3","timestamp":"1684518840.0","comment_id":"902159","poster":"REzirezi","comments":[{"content":"You can peer any two VPCs in different Regions, as long as they have distinct, non-overlapping CIDR blocks\nhttps://docs.aws.amazon.com/devicefarm/latest/developerguide/amazon-vpc-cross-region.html","comment_id":"904005","poster":"[Removed]","upvote_count":"3","timestamp":"1684756560.0"},{"timestamp":"1684740360.0","comment_id":"903814","content":"You can peer any two VPCs in different Regions, as long as they have distinct, non-overlapping CIDR blocks. This ensures that all of the private IP addresses are unique, and it allows all of the resources in the VPCs to address each other without the need for any form of network address translation (NAT).","upvote_count":"2","poster":"fakrap"}]},{"upvote_count":"2","timestamp":"1684504140.0","poster":"nosense","content":"Selected Answer: B\nb for me. bcs correct inbound rule, and not overhead","comment_id":"901983"},{"comment_id":"901880","content":"Selected Answer: B\nOption B suggests configuring a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC. By establishing this peering connection, the VPCs can communicate with each other over their private IP addresses.\n\nAdditionally, updating the subnet route tables is necessary to ensure that the traffic destined for the remote VPC is correctly routed through the VPC peering connection.\n\nTo secure the communication, an inbound rule is created in the ap-southeast-2 database security group. This rule references the security group ID of the application servers in the eu-west-1 VPC, allowing traffic only from those instances. This approach ensures that only the authorized application servers can access the databases in the ap-southeast-2 VPC.","upvote_count":"5","timestamp":"1684496220.0","poster":"cloudenthusiast"}],"answer_ET":"C","timestamp":"2023-05-19 13:37:00"},{"id":"Quhl6wCt9nHxeENSU5Jz","discussion":[{"comment_id":"901882","comments":[{"upvote_count":"6","poster":"cloudenthusiast","content":"Option B suggests using Amazon RDS for PostgreSQL Single-AZ DB instances for each development environment. While Amazon RDS is a reliable and cost-effective option, it may have slightly higher costs compared to Amazon Aurora On-Demand instances.","comments":[{"poster":"Iragmt","content":"I'm thinking that it should be B, since question does not mention any requirement only cost effective and this is just an development environment I guess we can leverage the use of RDS free tier also","comment_id":"947056","timestamp":"1688894340.0","upvote_count":"3"}],"timestamp":"1684496460.0","comment_id":"901883"}],"poster":"cloudenthusiast","timestamp":"1684496460.0","upvote_count":"12","content":"Selected Answer: C\nOption C suggests using Amazon Aurora On-Demand PostgreSQL-Compatible databases for each development environment. This option provides the benefits of Amazon Aurora, which is a high-performance and scalable database engine, while allowing you to pay for usage on an on-demand basis. Amazon Aurora On-Demand instances are typically more cost-effective for individual development environments compared to the provisioned capacity options."},{"comment_id":"1157386","timestamp":"1708713540.0","content":"Selected Answer: C\nGuys, when you use the pricing calculator the cost between option B and C is really close. I doubt anyone wants to test on your knowledge of exact pricings in your region. I think that \"On Demand\" being explicitly specified in option C and not being specified in option B is the main difference here the exam wants to test. In that case I'd assume that option B means a constantly running instance and not \"On Demand\" which would make the choice pretty obvious. Again, I don't think AWS exam will test you on knowing that a single AZ is cheaper by 0,005 cents than Aurora :D","poster":"Stranko","upvote_count":"11"},{"comment_id":"1388133","timestamp":"1741828620.0","content":"Selected Answer: C\nThere is no upfront commitment with Aurora. You pay an hourly charge for each instance that you launch, and when youâ€™re finished with an Aurora DB instance, you can delete it. You do not need to overprovision storage as a safety margin, and you only pay for the storage you actually consume- from AWS official link","poster":"tch","upvote_count":"1"},{"timestamp":"1737514680.0","upvote_count":"1","content":"Selected Answer: B\nB RDS single az is the cheapest. Aurora needs at least two azs.","poster":"zdi561","comment_id":"1344587"},{"content":"Selected Answer: C\nI choose C because Aurora on-Demand is Aurora Serverless:\nThe Aurora Serverless is cost effective. Scale out fine-grained increments to provide just the right number of database resources and pay only for capacity consumed.","poster":"tonybuivannghia","upvote_count":"2","comment_id":"1298999","timestamp":"1729125300.0"},{"poster":"a7md0","timestamp":"1720032060.0","comment_id":"1241594","upvote_count":"1","content":"Selected Answer: B\nSingle-AZ DB instances cheaper"},{"comment_id":"1202992","timestamp":"1714203300.0","content":"Selected Answer: B\nSingle AZ more cost effective","poster":"trinh_le","upvote_count":"2"},{"content":"Selected Answer: B\n1 instance(s) x 0.245 USD hourly x (4 / 24 hours in a day) x 730 hours in a month = 29.8083 USD ---> Amazon RDS PostgreSQL instances cost (monthly)\n1 instance(s) x 0.26 USD hourly x (4 / 24 hours in a day) x 730 hours in a month = 31.6333 USD ---> Amazon Aurora PostgreSQL-Compatible DB instances cost (monthly)","comment_id":"1139070","upvote_count":"2","timestamp":"1706947080.0","poster":"chasingsummer"},{"upvote_count":"3","comment_id":"1126211","content":"Selected Answer: C\nC is correct because B is cheaper but they don't mention to stop the DB when not in use","poster":"upliftinghut","timestamp":"1705612680.0"},{"upvote_count":"2","content":"Selected Answer: C\nOn-Demand is cheaper that Aurora or RDS because of low weekly usage","poster":"awsgeek75","timestamp":"1704911940.0","comment_id":"1118933"},{"comments":[{"comment_id":"1331136","poster":"Salilgen","timestamp":"1735047960.0","upvote_count":"1","content":"Question doesn't talk about serverless. Moreover, you can select on-demand pricing model for RDS for PostgreSQL Single-AZ too.\nIMO answer is B"}],"poster":"pentium75","timestamp":"1704107100.0","upvote_count":"4","comment_id":"1111106","content":"Selected Answer: C\nWe have environments that are used on average 4 hours per workday = 20 hours per week. So with option C (Aurora on-demand aka serverless) we pay for 20 hours per week. With option B (RDS) we pay for 168 hours per week (the answer does not mention anything about automating shutdown etc.).\n\nSo even if Aurora Serverless is slightly more expensive than RDS, C is cheaper because we pay only 20 (not 168) hours per week."},{"upvote_count":"1","content":"Selected Answer: B\nAurora on demand is (a little) more expensive than Aurora\nAurora is more expensive than RDS single instance\n\nSo cost effectiveness == RDS.\n\n(B)","timestamp":"1702334940.0","poster":"Mikado211","comment_id":"1093901","comments":[{"poster":"pentium75","timestamp":"1704106860.0","upvote_count":"3","content":"But if you use the database only 20 hours per week (5 x 4), wouldn't you pay way less with Aurora serverless than with RDS?","comment_id":"1111104"}]},{"poster":"Murtadhaceit","upvote_count":"7","content":"Selected Answer: B\nAWS Services Calculator is showing B cheaper by less than a dollar for the same settings for both. I used \"db.r6g.large\" for RDS (Single-AZ) and Aurora and put 4 hours/day.","timestamp":"1702306560.0","comments":[{"timestamp":"1708713240.0","poster":"Stranko","upvote_count":"2","content":"I used the calculator, single AZ is cheaper for the exact same usage duration, if you pick On-Demand option for it too. In Aurora case (option C) you have \"On Demand\" explicitly specified, so if it has to be specified then I suppose that B option is about a constantly running instance. If B had an \"On Demand\" added, I'd vote B too.","comment_id":"1157382"}],"comment_id":"1093592"},{"upvote_count":"1","content":"Selected Answer: B\nAmazon RDS Single AZ is cheaper than Aurora Multi-AZ","comment_id":"1091389","poster":"JoseVincent68","timestamp":"1702083420.0"},{"content":"Selected Answer: B\nAurora instances will cost you ~20% more than RDS MySQL Given the running hours the same.\nAlso Aurora is HA.","poster":"Wayne23Fang","comment_id":"1049872","timestamp":"1697924640.0","upvote_count":"1"},{"upvote_count":"6","comments":[{"timestamp":"1697572500.0","upvote_count":"1","poster":"Anmol_1010","content":"that is good piece of infroamtion","comment_id":"1046330"}],"content":"â€¦ just trying to trick you. Aurora on demand is Aurora Serverless.","comment_id":"1019180","timestamp":"1695841920.0","poster":"baba365"},{"timestamp":"1693578060.0","poster":"deechean","content":"Selected Answer: C\nAurora allows you to pay for the hours used. 4 hour every day, you only need 1/6 cost of 24 hours per day. You can check the Aurora pricing calculator.","upvote_count":"4","comment_id":"996131"},{"comments":[{"timestamp":"1704692820.0","poster":"OSHOAIB","upvote_count":"2","comment_id":"1116432","content":"Aurora is FULLY compatible with PostgreSQL, allowing existing applications and tools to run without requiring modification.\nhttps://aws.amazon.com/rds/aurora/features/#:~:text=Aurora%20is%20fully%20compatible%20with,to%20run%20without%20requiring%20modification"}],"timestamp":"1692787740.0","content":"Selected Answer: B\nThe key factors:\n\nRDS Single-AZ instances only run the DB instance when in use, minimizing costs for dev environments not used full-time\nRDS charges by the hour for DB instance hours used, versus Aurora clusters that have hourly uptime charges\nPostgreSQL is natively supported by RDS so no compatibility issues\nS3 Object Select (Option D) does not provide full database functionality\nAurora (Options A and C) has higher minimum costs than RDS even when not fully utilized","comment_id":"988202","upvote_count":"2","poster":"Guru4Cloud"},{"comment_id":"954934","timestamp":"1689653880.0","content":"Selected Answer: C\nPutting into consideration that the environments will only run 4 hours everyday and the need to save on costs, then Amazon Aurora would be suitable because it supports auto-scaling configuration where the database automatically starts up, shuts down, and scales capacity up or down based on your application's needs. So for the rest of the 4 hours everyday when not in use the database shuts down automatically when there is no activity.\nOption C would be best, as this is the name of the service from the aws console.","upvote_count":"3","poster":"TariqKipkemei"},{"upvote_count":"1","comment_id":"950502","content":"is A not the serverless ?","timestamp":"1689240300.0","poster":"dddddddddddww12"},{"comment_id":"927849","timestamp":"1687203060.0","upvote_count":"3","content":"Selected Answer: C\nC, more specific \"Aurora Serverless V2\", check the link: https://aws.amazon.com/rds/aurora/serverless/","poster":"MrAWSAssociate"},{"comment_id":"924424","timestamp":"1686849600.0","upvote_count":"2","content":"Selected Answer: B\nAnswer is B.","poster":"nuri92"},{"poster":"Bill1000","timestamp":"1686085680.0","content":"Selected Answer: C\nWith Aurora Serverless, you create a database, specify the desired database capacity range, and connect your applications. You pay on a per-second basis for the database capacity that you use when the database is active, and migrate between standard and serverless configurations with a few steps in the Amazon Relational Database Service (Amazon RDS) console.","comment_id":"916646","upvote_count":"2"},{"timestamp":"1685920980.0","comment_id":"914954","upvote_count":"2","poster":"Felix_br","content":"Selected Answer: C\nAmazon Aurora On-Demand is a pay-per-use deployment option for Amazon Aurora that allows you to create and destroy database instances as needed. This is ideal for development environments that are only used for part of the day, as you only pay for the database instance when it is in use.\n\nThe other options are not as cost-effective. Option A, configuring each development environment with its own Amazon Aurora PostgreSQL database, would require you to pay for the database instance even when it is not in use. Option B, configuring each development environment with its own Amazon RDS for PostgreSQL Single-AZ DB instance, would also require you to pay for the database instance even when it is not in use. Option D, configuring each development environment with its own Amazon S3 bucket by using Amazon S3 Object Select, is not a viable option as Amazon S3 is not a database."},{"timestamp":"1685335500.0","upvote_count":"2","content":"Selected Answer: B\nOption B would be the most cost-effective solution for configuring development environments. Amazon RDS for PostgreSQL Single-AZ DB instances would provide a cost-effective solution for a development environment. Amazon Aurora has higher cost than RDS (20% more)","poster":"elmogy","comments":[{"comment_id":"1111103","content":"But with C you'd pay 20 % more for 4 hours on average, while for B you'd pay 20 % less for 8 hours.","upvote_count":"2","poster":"pentium75","timestamp":"1704106740.0"}],"comment_id":"909041"},{"timestamp":"1684650720.0","upvote_count":"4","comment_id":"902949","content":"Selected Answer: B\nAmazon Aurora, whether On-Demand or not (Option A and C), provides higher performance and is more intended for production environments. It also typically has a higher cost compared to RDS,","poster":"Rob1L"},{"content":"Its B the most cost effective if it was preformance then it would be option A","comment_id":"902283","poster":"Anmol_1010","upvote_count":"1","timestamp":"1684542060.0"},{"comment_id":"900100","content":"Selected Answer: C\nc cost effectively","timestamp":"1684325400.0","poster":"nosense","upvote_count":"3"}],"question_text":"A company is developing software that uses a PostgreSQL database schema. The company needs to configure multiple development environments and databases for the company's developers. On average, each development environment is used for half of the 8-hour workday.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_ET":"C","choices":{"B":"Configure each development environment with its own Amazon RDS for PostgreSQL Single-AZ DB instances","C":"Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-Compatible database","D":"Configure each development environment with its own Amazon S3 bucket by using Amazon S3 Object Select","A":"Configure each development environment with its own Amazon Aurora PostgreSQL database"},"timestamp":"2023-05-17 14:10:00","unix_timestamp":1684325400,"answer":"C","question_images":[],"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/109532-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"question_id":479,"answers_community":["C (67%)","B (33%)"],"answer_description":"","exam_id":31},{"id":"JdtfyqhNnP3R6sJl34ri","question_id":480,"exam_id":31,"discussion":[{"poster":"cloudenthusiast","content":"Selected Answer: A\nThis solution allows you to leverage AWS Config to identify any untagged resources within your AWS Organizations accounts. Once identified, you can programmatically apply the necessary tags to indicate the backup requirements for each resource. By using tags in the backup plan configuration, you can ensure that only the tagged resources are included in the backup process, reducing operational overhead and ensuring all necessary resources are backed up.","comment_id":"901884","timestamp":"1684496640.0","upvote_count":"5"},{"poster":"Gape4","comment_id":"1236673","upvote_count":"2","timestamp":"1719283680.0","content":"Selected Answer: A\nI will go for A. C and D doesn't make sense. B- resources not running? No"},{"content":"Selected Answer: A\nUse AWS config to deploy the tag rule and remediate resources that are not compliant.","comment_id":"1066959","poster":"TariqKipkemei","upvote_count":"4","timestamp":"1699594080.0"},{"poster":"Guru4Cloud","comment_id":"988194","timestamp":"1692787020.0","content":"Selected Answer: A\nThis option has the least operational overhead:\n\nAWS Config continuously evaluates resource configurations and can identify untagged resources\nResources can be programmatically tagged via the AWS SDK based on Config data\nBackup plans can use tag criteria to automatically back up newly tagged resources\nNo manual review or resource discovery needed","upvote_count":"3"},{"poster":"Bill1000","upvote_count":"3","content":"Selected Answer: A\nVote A","timestamp":"1686085920.0","comment_id":"916647"},{"poster":"nosense","timestamp":"1684504260.0","comment_id":"901984","upvote_count":"4","content":"Selected Answer: A\na valid for me"}],"question_text":"A company uses AWS Organizations with resources tagged by account. The company also uses AWS Backup to back up its AWS infrastructure resources. The company needs to back up all AWS resources.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","topic":"1","choices":{"C":"Require all AWS account owners to review their resources to identify the resources that need to be backed up.","D":"Use Amazon Inspector to identify all noncompliant resources.","B":"Use AWS Config to identify all resources that are not running. Add those resources to the backup vault.","A":"Use AWS Config to identify all untagged resources. Tag the identified resources programmatically. Use tags in the backup plan."},"answer_images":[],"answer_ET":"A","answer":"A","question_images":[],"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/109709-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-05-19 13:44:00","unix_timestamp":1684496640,"answers_community":["A (100%)"]}],"exam":{"isImplemented":true,"id":31,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"provider":"Amazon","isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false},"currentPage":96},"__N_SSP":true}