{"pageProps":{"questions":[{"id":"oAu6SIOn4mGQrLcC1yyk","isMC":true,"topic":"1","answer_ET":"B","question_id":96,"unix_timestamp":1615582620,"timestamp":"2021-03-12 21:57:00","question_images":["https://www.examtopics.com/assets/media/exam-media/04239/0011600001.png","https://www.examtopics.com/assets/media/exam-media/04239/0011700001.png"],"answers_community":["B (80%)","A (20%)"],"choices":{"D":"The s3:PutObject and s3:PutObjectAcl permissions should be applied at the S3 bucket level.","C":"The Security Engineer's IAM policy does not grant permissions to read objects in the S3 bucket.","A":"The S3 bucket policy does not explicitly allow the Security Engineer access to the objects in the bucket.","B":"The object ACLs are not being updated to allow the users within the centralized account to access the objects."},"question_text":"A company's Security Engineer is copying all application logs to centralized Amazon S3 buckets. Currently, each of the company's application is in its own AWS account, and logs are pushed into S3 buckets associated with each account. The Engineer will deploy an AWS Lambda function into each account that copies the relevant log files to the centralized S3 bucket.\nThe Security Engineer is unable to access the log files in the centralized S3 bucket. The Engineer's IAM user policy from the centralized account looks like this:\n//IMG//\n\nThe centralized S3 bucket policy looks like this:\n//IMG//\n\nWhy is the Security Engineer unable to access the log files?","exam_id":29,"url":"https://www.examtopics.com/discussions/amazon/view/46776-exam-aws-certified-security-specialty-topic-1-question-185/","answer_description":"","answer":"B","answer_images":[],"discussion":[{"comment_id":"325084","content":"It's B\nthe remote account when putting objects in the central bucket they don't grant the bucket owner access\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/","poster":"[Removed]","upvote_count":"26","timestamp":"1633561440.0"},{"timestamp":"1655214420.0","upvote_count":"10","poster":"trongod05","comment_id":"616266","content":"Selected Answer: B\nI think it is B for these reasons:\n\n1. Both the bucket and user are in the same account. It says in the question that the Security Engineer's account is defined in the centralized account. The bucket policy is also in the centralized account. So we are not assuming any roles that would apply to the answer.\n\n2. You have to look at all applicable policies and evaluate together. We start with an explicit deny. Then we look at IAM, there is an explicit allow for the Security Engineer for s3:Get and s3:List. Then we look at bucket policy. There's no explicit deny's there and we still have the explicit allow being applied. Finally, we look at bucket ACL. Since it isn't presented in the question, we can only assume that there is an ACL that explicitly denies any principal's other than the ones listed in the ACL."},{"timestamp":"1709572380.0","comment_id":"1165835","poster":"Raphaello","content":"Selected Answer: B\nObject ACL.\n(s3:x-amz-acl | s3:x-amz-grant-read | s3:x-amz-grant-full-control)","upvote_count":"1"},{"poster":"Raphaello","timestamp":"1709042220.0","comment_id":"1160670","upvote_count":"1","content":"Selected Answer: A\nBest practice is to set bucket policy to control cross-account access to S3 bucket.\nA is the correct answer.","comments":[{"comment_id":"1165833","content":"Not sure how I missed \"The Engineer's IAM user policy from the CENTRALIZED account looks like this\".\nIt is not cross-account access.\n\nTherefore, B is the right answer.\n\nMy mistake.","upvote_count":"1","poster":"Raphaello","timestamp":"1709572200.0"}]},{"comment_id":"999494","upvote_count":"2","timestamp":"1693917300.0","poster":"addy_prepare","content":"Selected Answer: A\nA - \"It's also a best practice to use IAM policies and bucket policies (instead of ACLs) to manage cross-account access to buckets and objects.\" As well as it said that ACL is used for specific cases https://repost.aws/knowledge-center/cross-account-access-s3"},{"poster":"BlissfulCheetah","upvote_count":"2","timestamp":"1687178700.0","content":"Looks to me like A is the answer. \n\nWhen applying both IAM and S3 bucket policies, the resultant policy will be the intersection of both. The IAM policy grants the engineer get access, but the bucket policy's default deny policy doesn't grant access to anyone. Only the \"logcopier\" role and the IAM user that created the bucket can access the bucket.\n\nhttps://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/ \n\nS3 bucket policies (as the name would imply) only control access to S3 resources, whereas IAM policies can specify nearly any AWS action. One of the neat things about AWS is that you can actually apply both IAM policies and S3 bucket policies simultaneously, with the ultimate authorization being the least-privilege union of all the permissions (more on this in the section below titled “How does authorization work with multiple access control mechanisms?”).","comment_id":"927487"},{"poster":"TerrenceC","comment_id":"759456","content":"Another useful material for demystification.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-auth-workflow-object-operation.html","upvote_count":"1","timestamp":"1672210320.0"},{"upvote_count":"1","comment_id":"651408","timestamp":"1661365080.0","content":"Selected Answer: B\nWhy can't I access an object that was uploaded to my Amazon S3 bucket by another AWS account?\nFor existing Amazon S3 buckets with the default object ownership settings, the object owner is the AWS account which uploaded the object to the bucket. For these existing buckets, an object owner had to explicitly grant permissions to an object (by attaching an access control list). \n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/\nWith S3 Object Ownership, bucket owners can now manage the ownership of any objects uploaded to their buckets. By default, all newly created S3 buckets have the bucket owner enforced setting enabled.","poster":"sapien45"},{"timestamp":"1639631040.0","upvote_count":"1","poster":"tinyflame","content":"Answer = A / Multi-account requires both IAM and bucket policy permissions","comment_id":"502667"},{"upvote_count":"2","content":"It is B.","poster":"wahlbergusa","comment_id":"415299","timestamp":"1636188060.0"},{"upvote_count":"2","timestamp":"1635367800.0","content":"I think it's A","poster":"nainakaexam","comments":[{"comment_id":"387222","comments":[{"content":"It's B, it's clear to me now. Disregard my previous comment.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-8","timestamp":"1635999300.0","comment_id":"387227","upvote_count":"1","poster":"nainakaexam"}],"poster":"nainakaexam","content":"\"A\" would make sense if the centralised S3 bucket is in a different cross account for security engineer. From the question, it's not quite clear if it's actually a cross account","upvote_count":"1","timestamp":"1635956280.0"}],"comment_id":"387218"},{"poster":"DerekKey","timestamp":"1635189060.0","upvote_count":"2","content":"I think A\nTo allow cross-account access, you attach a resource-based policy to the resource that you want to share. You must also attach an identity-based policy to the identity that acts the principal in the request. The resource-based policy in the trusting account must specify the principal of the trusted account that will have access to the resource. You can specify the entire account or its IAM users, federated users, IAM roles, or assumed-role sessions.","comment_id":"385461"},{"timestamp":"1635183120.0","content":"I think its B\nA: no need to explicit allow IAM user. Either IAM policy or bucket policy allows the principal is enough. In this case, there is no explicit deny\nB: you can deny bucket owner account to access bucket/object via ACLs\nC: S3:Get* and S3:List* are allowed already in IAM policy\nD: User just need to read.","upvote_count":"3","comment_id":"384725","poster":"rhinozD","comments":[{"timestamp":"1666435020.0","upvote_count":"1","content":"S3:Get* and S3:List* are NOT allowed in IAM policy","comment_id":"701473","poster":"thuyeinaung"},{"comment_id":"701474","poster":"thuyeinaung","timestamp":"1666435200.0","content":"sorry, I was misread","upvote_count":"1"}]},{"upvote_count":"1","poster":"Haz56","comment_id":"378214","timestamp":"1635072180.0","content":"Cant be B, as the user and bucket are in the same centralised account\n\"Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account.\"\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html"},{"content":"My view is B.\nhttps://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/\n\"Whenever an AWS principal issues a request to S3, the authorization decision depends on the union of all the IAM policies, S3 bucket policies, and S3 ACLs that apply.\"\n\nA- S3 bucket does not need to explicitly allow. There is no explicit deny either.\nC- S3:Get* and S3:List* are allowed. \nD- The required access by security engineer is read, so S3:Put* are irrelevant. Also, the level of the Resource given should be acceptable.","timestamp":"1634911500.0","comment_id":"356176","upvote_count":"2","poster":"Daniel76"},{"poster":"sanjaym","comment_id":"352667","upvote_count":"3","timestamp":"1634062920.0","content":"A for sure. Security Engineer has read permission by IAM policy but bucket policy not allowing Engineer to get objects from bucket."},{"comment_id":"343706","content":"yeah, B\nFor the people that think it's a, it's not.\n\nthe IAM policy allows to get object, but not PUT\nbucket policy allows put from some roles.\nthere's no deny anywhere, both policies even evaluated together do not deny the engineer to read.","timestamp":"1633795260.0","upvote_count":"2","poster":"disposable1989"},{"content":"yeah, B\nFor the people that think it's a, it's not.\n\nthe IAM policy allows to get object, but not but\nbucket policy allows put from some roles.\nthere's no deny anywhere, both policies even evaluated together do not deny the engineer.","timestamp":"1633710540.0","comment_id":"343705","upvote_count":"1","poster":"disposable1989"},{"upvote_count":"1","content":"B is the answer","timestamp":"1633682760.0","comment_id":"341302","poster":"Hungdv"},{"poster":"cldy","comment_id":"316877","timestamp":"1632937920.0","upvote_count":"4","content":"A. Bucket policy should explicitly Allow access to Engineer principle."},{"poster":"Hudda","timestamp":"1632688200.0","comment_id":"315922","content":"any other comments please ? final answer is D or A?","upvote_count":"1"},{"upvote_count":"1","content":"any other comments please ?","poster":"Hudda","comment_id":"315920","timestamp":"1632660180.0"},{"comment_id":"309169","upvote_count":"4","comments":[{"poster":"ChinkSantana","comment_id":"316902","content":"This is true. Bucket policy does not allow read for the Engineer","timestamp":"1633136100.0","upvote_count":"1"},{"content":"It mentions the engineers IAM policy has Get* and List*, it shows it right there. The bucket policy doesn't need Get* and List* because the user policy and the bucket are in the same account. I think its B","comment_id":"404153","upvote_count":"1","timestamp":"1636090500.0","poster":"skipbaylessfor3"}],"timestamp":"1632490080.0","poster":"DayQuil","content":"A.\n\nSince a bucket policy is used, it will be evaluated alongside the engineer's IAM policy to determine if access is allowed. The bucket policy will need to allow Get* and List* actions for the engineer's principal in order for this to work."}]},{"id":"TuQv21yozUWny99Twt0L","question_text":"An application running on Amazon EC2 instances generates log files in a folder on a Linux file system. The instances block access to the console and file transfer utilities, such as Secure Copy Protocol (SCP) and Secure File Transfer Protocol (SFTP). The Application Support team wants to automatically monitor the application log files so the team can set up notifications in the future.\nA Security Engineer must design a solution that meets the following requirements:\n✑ Make the log files available through an AWS managed service.\nAllow for automatic monitoring of the logs.\n//IMG//\n\n✑ Provide an interface for analyzing logs.\n✑ Minimize effort.\nWhich approach meets these requirements?","question_id":97,"url":"https://www.examtopics.com/discussions/amazon/view/46777-exam-aws-certified-security-specialty-topic-1-question-186/","exam_id":29,"answer_description":"","timestamp":"2021-03-12 22:00:00","topic":"1","isMC":true,"unix_timestamp":1615582800,"choices":{"B":"Install the unified Amazon CloudWatch agent on the instances. Configure the agent to collect the application log files on the EC2 file system and send them to Amazon CloudWatch Logs.","C":"Install AWS Systems Manager Agent on the instances. Configure an automation document to copy the application log files to AWS DeepLens.","A":"Modify the application to use the AWS SDK. Write the application logs to an Amazon S3 bucket.","D":"Install Amazon Kinesis Agent on the instances. Stream the application log files to Amazon Kinesis Data Firehose and set the destination to Amazon Elasticsearch Service."},"answer_ET":"B","answer":"B","discussion":[{"comment_id":"352668","upvote_count":"13","timestamp":"1633817280.0","content":"B for sure.","poster":"sanjaym","comments":[{"comment_id":"420451","upvote_count":"1","timestamp":"1634645520.0","content":"B does \"minimum effort\" and \"make log files available\" B does not provide an interface for analyzing logs unless you think metrics monitor fits the bill. I guess that's good enough, -B- it is.","poster":"DahMac"}]},{"upvote_count":"7","content":"B. \n\nTechnically D is correct too, but minimal effort is a requirement here. You can use Cloudwatch Logs Insights to query CloudWatch logs in the console.\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html","timestamp":"1632153780.0","comment_id":"309172","poster":"DayQuil"},{"timestamp":"1709042700.0","content":"Selected Answer: B\nFor this case, I would go with B. Using a single AWS service that checks all the boxes of the requirements.\n\nThat being said, option D is NOT wrong, but it does not fit perfectly with this case.\nKinesis agent can be used to collect app logs, and can be used as source for Kinesis Data Firehose. \nI would've picked this option had the request is to send the logs to custom HTTP endpoint, Splunk, or Datadog. All are valid KDF destinations and would give the edge to this option over B.\n\nHowever for the current requirements, option B is the answer.","comment_id":"1160680","poster":"Raphaello","upvote_count":"1"},{"comment_id":"1068730","content":"Selected Answer: B\nB. Install the unified Amazon CloudWatch agent on the instances. Configure the agent to collect the application log files on the EC2 file system and send them to Amazon CloudWatch Logs.\n\nExplanation:\n\n Amazon CloudWatch:\n Amazon CloudWatch is an AWS managed service that provides monitoring for AWS resources and applications.\n The CloudWatch agent can be installed on EC2 instances to collect logs, system metrics, and other data.\n\n CloudWatch Logs:\n CloudWatch Logs is a service for monitoring, storing, and accessing log files from Amazon EC2 instances, AWS CloudTrail, and other sources.\n The CloudWatch agent can be configured to collect and send log files from EC2 instances to CloudWatch Logs.\n\n Monitoring and Analysis:\n Once the logs are in CloudWatch Logs, you can set up alarms and notifications for specific log events.\n You can also use CloudWatch Logs Insights for interactive, real-time log analysis.\n\n Minimizing Effort:\n Installing and configuring the CloudWatch agent is relatively straightforward, and it integrates well with other AWS services.","upvote_count":"1","poster":"RosenYordanov","timestamp":"1699810920.0"},{"upvote_count":"1","comment_id":"946040","poster":"pk0619","timestamp":"1688771100.0","content":"Selected Answer: B\nB is the easy way"},{"timestamp":"1685083320.0","poster":"Tofu13","content":"Selected Answer: D\nSame reason as tipzzz\nProvide an interface for analyzing logs. --> Elasticsearch\nDoesn't make sense to value minimum effort over fulfilling the requirements.\nUnless there is an interface for analyzing logs in CW the answer should be D.","comment_id":"907125","upvote_count":"1"},{"timestamp":"1682068380.0","comment_id":"876313","poster":"ITGURU51","upvote_count":"1","content":"If we install the Cloudwatch agent we can significantly reduces the amount of effort it takes to monitor the EC2 instance. B"},{"upvote_count":"1","poster":"[Removed]","timestamp":"1667138940.0","comment_id":"707846","content":"Selected Answer: B\nB&D are both good. But D is definitely does not sound like\" Minimize effort.\"\nI think answer is B"},{"comment_id":"651409","content":"Selected Answer: B\nb it is","upvote_count":"2","poster":"sapien45","timestamp":"1661365380.0"},{"timestamp":"1660673520.0","upvote_count":"1","content":"Selected Answer: D\nI go for option D.","poster":"dcasabona","comment_id":"647782"},{"upvote_count":"1","timestamp":"1639454160.0","comment_id":"501048","poster":"AzureDP900","content":"B is right"},{"poster":"kiev","upvote_count":"2","timestamp":"1636201680.0","comment_id":"438006","content":"Monitoring is always Cloudwatch and therefore answer is B"},{"content":"D is the answer.\nKinesis agent can send logs to firehose.\n✑ Provide an interface for analyzing logs. --> Elasticsearch","poster":"tipzzz","timestamp":"1635176040.0","comments":[{"comment_id":"475163","comments":[{"poster":"peddyua","content":"well you can use OpenSearch which is AWS service, but it's a stretch and much more complicated then B, while B does the job perfectly, you can even configure AWS Dashboard if regular cloudwatch is not enough.","comment_id":"836673","timestamp":"1678587060.0","upvote_count":"1"}],"content":"is Elasticsearch a 'AWS managed service'?","upvote_count":"2","poster":"acloudguru","timestamp":"1636508820.0"}],"comment_id":"429662","upvote_count":"1"},{"poster":"skipbaylessfor3","upvote_count":"3","content":"Maybe I'm thinking too deeply into this, but it says \"provide an interface for analyzing logs\" wouldn't that be better with ElasticSearch? Since it has integration with Kibana and Logstash etc... CloudWatch just shows the logs plainly?\nAlso it says AWS managed service, Elasticsearch is explicitly managed? (Although technically CloudWatch is managed too)\nI'm wondering if there's anything to do with SCP and SFTP, not sure why those are mentioned","comment_id":"405063","timestamp":"1634639520.0"},{"poster":"DerekKey","comment_id":"385465","timestamp":"1633958220.0","upvote_count":"2","content":"Crazy answer C -> AWS DeepLens :)","comments":[{"upvote_count":"2","content":"Lol you're joking right","timestamp":"1634143440.0","poster":"skipbaylessfor3","comment_id":"392092"}]},{"timestamp":"1633471380.0","upvote_count":"2","content":"B is the only correct answer here","poster":"ChinkSantana","comment_id":"316907"},{"poster":"cldy","timestamp":"1632540660.0","upvote_count":"3","comment_id":"316883","content":"B. Classic case of AWS ...."}],"answers_community":["B (75%)","D (25%)"],"answer_images":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04239/0011700003.png"]},{"id":"WgZvzqBjK8NI8QehI9Wg","answer_images":[],"timestamp":"2021-03-12 22:06:00","question_text":"A company has multiple AWS accounts that are part of AWS Organizations. The company's Security team wants to ensure that even those Administrators with full access to the company's AWS accounts are unable to access the company's Amazon S3 buckets.\nHow should this be accomplished?","topic":"1","exam_id":29,"answer_ET":"A","unix_timestamp":1615583160,"question_id":98,"question_images":[],"answer":"A","answers_community":["A (100%)"],"discussion":[{"upvote_count":"21","comment_id":"309175","content":"A.\n\nUse service control policies to deny or permit access at the account level. This will be precede IAM policies that permit access.","poster":"DayQuil","timestamp":"1632568860.0"},{"content":"Selected Answer: A\nSCP control/restrict even root user in member AWS accounts.","upvote_count":"1","poster":"Raphaello","timestamp":"1709572500.0","comment_id":"1165836"},{"poster":"G4Exams","upvote_count":"1","comment_id":"877765","timestamp":"1682214600.0","content":"Selected Answer: A\nA.This is a typical SCP usecase."},{"upvote_count":"1","timestamp":"1682068620.0","content":"Service control policies can be used to limit of scope and permissions granted to admin accounts within AWS. A","poster":"ITGURU51","comment_id":"876318"},{"poster":"acloudguru","upvote_count":"4","content":"such easy one, SCP for root. hope i can have such easy ones in my exam","timestamp":"1636514280.0","comment_id":"475201"},{"poster":"skipbaylessfor3","comment_id":"405024","content":"One problem with B is that they are only attaching it to roles... (which is also inefficient to attach it to potentially hundreds of roles) but what if there's a user who has access to S3 from their access/secret access key and they don't even use a role, then what? It wouldn't stop them. But an SCP would\nDid that make sense or am I tripping?","timestamp":"1635615660.0","comments":[{"upvote_count":"2","content":"Admins in those accounts with \"full permissions\" could change the permission boundaries. They can't do so with an SCP.","poster":"EricR17","timestamp":"1635985440.0","comment_id":"414639"}],"upvote_count":"3"},{"poster":"Daniel76","comments":[{"upvote_count":"1","timestamp":"1635390660.0","content":"So that means A is correct right? That's what I'm leaning towards","poster":"skipbaylessfor3","comment_id":"405022"}],"content":"B - permission boundary only effective within the account whereas the scenario requires multiple AWS accounts in the AWS Organization.","upvote_count":"1","comment_id":"373319","timestamp":"1634759820.0"},{"comments":[{"poster":"rhinozD","upvote_count":"2","comment_id":"384751","timestamp":"1634880600.0","content":"How many roles you have? If the number is 100? Or 1000"},{"comment_id":"414638","upvote_count":"1","timestamp":"1635841560.0","content":"Because Admins in those accounts with \"full permissions\" could change the permission boundaries. They can't do so with an SCP.","poster":"EricR17"}],"comment_id":"359440","timestamp":"1634729340.0","upvote_count":"2","poster":"rainit2006","content":"I know A works，but why B is incorrect？"},{"content":"\"A\" without doubt.","upvote_count":"4","timestamp":"1634153760.0","poster":"sanjaym","comment_id":"352669"},{"comments":[{"timestamp":"1635132960.0","content":"C should work but you have to take a lot of efforts to do it if the number S3 buckets is large number.","upvote_count":"1","comment_id":"384752","poster":"rhinozD"}],"content":"A and C will work.\nBut should use SCP here.","timestamp":"1633930980.0","comment_id":"341303","upvote_count":"1","poster":"Hungdv"},{"poster":"Edgecrusher77","timestamp":"1633496460.0","upvote_count":"2","comment_id":"320989","content":"A\nOf course SCP is not enougth, you will need a specific Policy, but SCP is the key point here"},{"timestamp":"1632674400.0","upvote_count":"1","content":"I don't think it's A as it does not block cross account access. C definitely works. Not sure about B.","comments":[{"poster":"ChinkSantana","comment_id":"316908","timestamp":"1633424100.0","upvote_count":"5","content":"A is the only correct answer here. \n\nUse service control policies to deny or permit access at the account level. This will be precede IAM policies that permit access."}],"poster":"aawwss","comment_id":"312802"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/46778-exam-aws-certified-security-specialty-topic-1-question-187/","answer_description":"Reference:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html","choices":{"B":"Add a permissions boundary to deny access to Amazon S3 and attach it to all roles.","D":"Create a VPC endpoint for Amazon S3 and deny statements for access to Amazon S3.","A":"Use SCPs.","C":"Use an S3 bucket policy."}},{"id":"Tm2Hgbpy3VV3a5JCuqTX","answers_community":["CD (100%)"],"question_text":"A Security Engineer has several thousand Amazon EC2 instances split across production and development environments. Each instance is tagged with its environment. The Engineer needs to analyze and patch all the development EC2 instances to ensure they are not currently exposed to any common vulnerabilities or exposures (CVEs).\nWhich combination of steps is the MOST efficient way for the Engineer to meet these requirements? (Choose two.)","answer_ET":"CD","topic":"1","isMC":true,"answer_description":"","answer_images":[],"timestamp":"2021-03-12 22:10:00","url":"https://www.examtopics.com/discussions/amazon/view/46779-exam-aws-certified-security-specialty-topic-1-question-188/","discussion":[{"upvote_count":"14","comment_id":"309177","timestamp":"1632240360.0","content":"C and D.\n\nUse Inspector to scan each instance for vulnerabilities. Then use the SSM RunCommand to patch the fleet of dev EC2 instances.","poster":"DayQuil"},{"timestamp":"1682069700.0","poster":"ITGURU51","comment_id":"876369","content":"We need to deploy AWS Systems manager to patch the development environment. C\nIn addition, Amazon Inspector provides security assessments to detect software vulnerabilities which can used to compromise the integrity of information systems. CD","upvote_count":"1"},{"comment_id":"656575","poster":"sapien45","content":"Selected Answer: CD\nThe rules in this package help verify whether the EC2 instances in your assessment targets are exposed to common vulnerabilities and exposures (CVEs). \nThe CVE rules package is updated regularly;\nAttacks can exploit unpatched vulnerabilities to compromise the confidentiality, integrity, or availability of your service or data. The CVE system provides a reference method for publicly known information security vulnerabilities and exposures\nIf a particular CVE appears in a finding that is produced by an Amazon Inspector Classic assessment, you can search https://cve.mitre.org/ for the ID of the CVE","timestamp":"1662058020.0","upvote_count":"2"},{"upvote_count":"3","content":"easy question, CD, hope i can get it in my exam","comment_id":"475189","timestamp":"1636513260.0","poster":"acloudguru"},{"poster":"kiev","upvote_count":"2","timestamp":"1635662460.0","comment_id":"438013","content":"Inspector for CVE and System manager of patching update and therefore CD is the answer"},{"timestamp":"1634648940.0","poster":"sanjaym","content":"Agree. C & D","upvote_count":"4","comment_id":"352670"},{"poster":"JAWS1600","upvote_count":"4","comment_id":"311589","content":"Agree C and D https://docs.aws.amazon.com/inspector/latest/userguide/inspector_cves.html","timestamp":"1633166640.0"}],"answer":"CD","question_id":99,"unix_timestamp":1615583400,"choices":{"D":"Install the Amazon EC2 System Manager agent on all development instances. Issue the Run command to EC2 System Manager to update all instances.","E":"Use AWS Trusted Advisor to check that all EC2 instances have been patched to the most recent version of operating system and installed software.","C":"Install the Amazon Inspector agent on all development instances. Configure Inspector to perform a scan using this CVE rule package on all instances tagged as being in the development environment.","B":"Install the Amazon Inspector agent on all development instances. Build a custom rule package, and configure Inspector to perform a scan using this custom rule on all instances tagged as being in the development environment.","A":"Log on to each EC2 instance, check and export the different software versions installed, and verify this against a list of current CVEs."},"exam_id":29,"question_images":[]},{"id":"vToIPTqHt4QFyd30TMZE","discussion":[{"content":"Answer: C\nhttps://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html\n\nA. Doesn't work because KMS by default can't replicate across regions.\nB. Doesn't work at the very least because storing a key in S3 violates the FIPS 140 requirement.\nD. Doesn't work because it violates the 'server-side encryption' requirement. Using JCE and PKCS11 would be client-side encryption.","timestamp":"1636176900.0","comment_id":"414701","poster":"EricR17","upvote_count":"24"},{"poster":"DerekKey","timestamp":"1635981720.0","comment_id":"385473","upvote_count":"9","content":"C - must be - since we want to use S3 that works only with KMS and can not work directly with HSM\n1. S3 - AWS Key Management Service key (SSE-KMS)\n2. Choose from your AWS KMS keys\n3. Key material origin\n4. Custom key store (CloudHSM)"},{"content":"Selected Answer: C\nBoth A & C are actually correct.\nCloudHSM support clone and sync to another region.\nAnd KMS support multi-region key (MRK).\n\nBoth of them are FIPS 140-2 Level 3 now.","poster":"Raphaello","timestamp":"1709043960.0","comment_id":"1160714","upvote_count":"1"},{"upvote_count":"4","timestamp":"1696234260.0","poster":"Salah21","comment_id":"1022928","content":"Selected Answer: A\nKMS is now FIPS 140-2 Security Level 3 compliant and supports Multi-Region keys.\n\nhttps://aws.amazon.com/kms/features/ \n\"All key material for KMS keys generated within AWS KMS HSMs and all operations that require decrypted KMS key material occur strictly within FIPS 140-2 Security Level 3 boundary of these HSMs.\" (they talking about KMS's multi-tenant HSMs not the dedicated CloudHSM)\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\n\"AWS KMS supports multi-Region keys, which are AWS KMS keys in different AWS Regions that can be used interchangeably – as though you had the same key in multiple Regions....\nLike all KMS keys, multi-Region keys never leave AWS KMS unencrypted.\""},{"timestamp":"1693916580.0","poster":"addy_prepare","content":"Selected Answer: A\nA - now KMS is 140-2 Level 3 compliant + Multi-Region key support (Check this in KMS key creation process)","comment_id":"999487","upvote_count":"5"},{"poster":"mamila","timestamp":"1693215180.0","upvote_count":"2","content":"Selected Answer: A\nThe answer is now A, KMS is 140-2 Level 3 compliant and supports multi-region replication.","comment_id":"991994"},{"content":"As of 2023 you use a KMS multi-region key, so the answer has changed to A https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html","comment_id":"916536","timestamp":"1686075660.0","upvote_count":"1","poster":"freddyman"},{"timestamp":"1682071620.0","upvote_count":"1","poster":"ITGURU51","comment_id":"876383","content":"The question states that key material must be replicated between regions and use (FIPS) 140-2 Level 3 encryption. Therefore C fits the bill."},{"timestamp":"1669727640.0","poster":"haris14","comment_id":"730422","content":"Answer: C\nUpdate (not relevant to the question): KMS supports multi-region now.","upvote_count":"2"},{"comment_id":"637569","poster":"dcasabona","timestamp":"1658860140.0","upvote_count":"2","content":"Selected Answer: C\nOption C..."},{"comment_id":"614882","timestamp":"1654933500.0","poster":"lotfi50","upvote_count":"2","content":"Selected Answer: C\nC is correct"},{"comment_id":"556464","timestamp":"1645841700.0","upvote_count":"1","poster":"RaySmith","content":"C is correct"},{"content":"\"key material must be created and kept on a FIPS 140-2 Level 3 certified computer\" --> CloudHSM. Due to data size --> KMS Envelope Encryption","timestamp":"1643071620.0","poster":"Radhaghosh","comment_id":"531711","upvote_count":"1"},{"comments":[{"comment_id":"491797","upvote_count":"1","content":"??? Explain Please","poster":"argol","timestamp":"1638375840.0"},{"upvote_count":"2","comment_id":"480195","poster":"munish3420","content":"Can you please paste link of refernce here?","timestamp":"1637172060.0"}],"content":"C is not relevant now as KMS allows copies of KMS keys in DynamoDB global tables.","comment_id":"477559","poster":"dumma","timestamp":"1636815840.0","upvote_count":"2"},{"comment_id":"385100","poster":"Joanale","content":"https://aws.amazon.com/es/blogs/security/aws-key-management-service-now-offers-fips-140-2-validated-cryptographic-modules-enabling-easier-adoption-of-the-service-for-regulated-workloads/\nANS: C","upvote_count":"2","timestamp":"1635517440.0"},{"comment_id":"357886","content":"Should it be D. Use AWS CloudHSM to generate the key material and backup keys across Regions.... ?\nKMS is FIPS 140-2 and Cloud HSM is FIPS 140-3\n\nhttps://aws.amazon.com/kms/faqs/?nc1=h_ls\n\n\"Q: What geographic region are my keys stored in?\nKeys generated by AWS KMS are only stored and used in the region in which they were created. They cannot be transferred to another region.\"","comments":[{"timestamp":"1635869280.0","content":"\"However, if you require even more control of the HSMs, you can create a custom key store that is backed by FIPS 140-2 Level 3 HSMs in an AWS CloudHSM cluster that you own and manage.\"\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-store-concepts.html","poster":"rhinozD","upvote_count":"1","comment_id":"385121"}],"timestamp":"1635300720.0","upvote_count":"2","poster":"eskimolander"},{"timestamp":"1635088620.0","upvote_count":"1","poster":"sanjaym","comment_id":"352673","content":"I'll go with C"},{"poster":"continent34","timestamp":"1635061320.0","content":"KMS is only within region. CloudHSM can be across regions","comment_id":"339613","upvote_count":"3"},{"upvote_count":"2","poster":"ChinkSantana","comment_id":"316912","timestamp":"1634691480.0","content":"I think its C. CloudHSM is a FIPS 140-2 & EAL-4 rating"},{"comment_id":"316012","content":"ans is C","upvote_count":"2","poster":"[Removed]","timestamp":"1634611740.0"},{"content":"C or D final answer friends pls confirm.","poster":"Hudda","comment_id":"315925","upvote_count":"1","timestamp":"1634594880.0"},{"comment_id":"314897","content":"C.\ncoz it needs server-side encryption. D uses client-side encryption.","upvote_count":"2","timestamp":"1633962840.0","poster":"cldy"},{"comments":[{"poster":"ChinkSantana","content":"Link does not say anything related to the question","upvote_count":"1","timestamp":"1634765100.0","comment_id":"321616"}],"poster":"JAWS1600","content":"I support chronoler - D .https://docs.aws.amazon.com/cloudhsm/latest/userguide/manage-keys.html","comment_id":"311591","timestamp":"1632846600.0","upvote_count":"1"},{"content":"This is D. Keyword FIPS2-LVL3. Only CloudHSM complies this requirement.","comments":[{"upvote_count":"2","comments":[{"upvote_count":"1","content":"I Studied again this question, and It's C. My bad sorry, this is a server-side request and you are right.","timestamp":"1636282620.0","comment_id":"447893","poster":"chronoler"}],"comment_id":"311915","timestamp":"1633889520.0","poster":"DayQuil","content":"Would C not work? The backend is configured to point towards CloudHSM clusters."}],"timestamp":"1632444120.0","poster":"chronoler","comment_id":"311115","upvote_count":"3"},{"upvote_count":"4","comment_id":"309206","timestamp":"1632158460.0","comments":[{"content":"Support C. \nD does not involve KMS custom key. However according to AWS FAQ below, to have Server side encryption on CloudHSM (for FIPS2 Lvl 3)- you need to use KMS custom key:\n\nQ: Can other AWS services use CloudHSM to store and manage keys?\nAWS services integrate with AWS Key Management Service, which in turn is integrated with AWS CloudHSM through the KMS custom key store feature. If you want to use the server-side encryption offered by many AWS services (such as EBS, S3, or Amazon RDS), you can do so by configuring a custom key store in AWS KMS. \n\nhttps://aws.amazon.com/cloudhsm/faqs/","upvote_count":"2","poster":"Daniel76","timestamp":"1635247260.0","comment_id":"356306"}],"poster":"DayQuil","content":"C.\n\nUse KMS custom key stores that point to CloudHSM if you want to use AWS services with CloudHSM. As for key material being available cross-region, a CloudHSM cluster will need to be deployed in desired target regions."}],"question_images":[],"topic":"1","choices":{"D":"Use AWS CloudHSM to generate the key material and backup keys across Regions. Use the Java Cryptography Extension (JCE) and Public Key Cryptography Standards #11 (PKCS #11) encryption libraries to encrypt and decrypt the data.","C":"Use an AWS KMS custom key store backed by AWS CloudHSM clusters, and copy backups across Regions.","B":"Use an AWS customer managed key, import the key material into AWS KMS using in-house AWS CloudHSM, and store the key material securely in Amazon S3.","A":"Use an AWS KMS customer managed key and store the key material in AWS with replication across Regions."},"answer_ET":"A","answer_images":[],"isMC":true,"unix_timestamp":1615585080,"url":"https://www.examtopics.com/discussions/amazon/view/46784-exam-aws-certified-security-specialty-topic-1-question-189/","timestamp":"2021-03-12 22:38:00","question_text":"A company has decided to use encryption in its AWS account to secure the objects in Amazon S3 using server-side encryption. Object sizes range from 16,000 B to 5 MB. The requirements are as follows:\n✑ The key material must be generated and stored in a certified Federal Information Processing Standard (FIPS) 140-2 Level 3 machine.\n✑ The key material must be available in multiple Regions.\nWhich option meets these requirements?","answer_description":"","question_id":100,"answers_community":["A (69%)","C (31%)"],"answer":"A","exam_id":29}],"exam":{"isImplemented":true,"name":"AWS Certified Security - Specialty","isMCOnly":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","id":29,"isBeta":false,"numberOfQuestions":509},"currentPage":20},"__N_SSP":true}