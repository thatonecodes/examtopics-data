{"pageProps":{"questions":[{"id":"SuAbkYtXXuKkn6QPUNlT","exam_id":33,"choices":{"A":"Increase the size of the EC2 instances.","C":"Change the health check path for the ALB.","B":"Increase the health check timeout for the ALB.","D":"Increase the health check grace period for the Auto Scaling group."},"isMC":true,"answer_images":[],"unix_timestamp":1719339720,"answer":"D","answer_description":"","answer_ET":"D","topic":"1","answers_community":["D (90%)","10%"],"question_images":[],"question_text":"A company hosts an application that uses several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). During the initial startup of the EC2 instances, the EC2 instances run user data scripts to download critical content for the application from an Amazon S3 bucket.\n\nThe EC2 instances are launching correctly. However, after a period of time, the EC2 instances are terminated with the following error message: “An instance was taken out of service in response to an ELB system health check failure.” EC2 instances continue to launch and be terminated because of Auto Scaling events in an endless loop.\n\nThe only recent change to the deployment is that the company added a large amount of critical content to the S3 bucket. The company does not want to alter the user data scripts in production.\n\nWhat should a solutions architect do so that the production environment can deploy successfully?","question_id":441,"url":"https://www.examtopics.com/discussions/amazon/view/142932-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"timestamp":"1719473640.0","poster":"ebbff63","content":"Selected Answer: D\nD. Increase the health check grace period","comment_id":"1237960","upvote_count":"9"},{"upvote_count":"7","content":"Selected Answer: D\nAnswer D. \nExtending the grace period allows the instances more time to complete their startup tasks, including downloading the additional content from S3, before health checks start. This solution does not require altering the user data scripts in production, which aligns with the company’s requirements.","comment_id":"1241343","timestamp":"1720003620.0","poster":"AhmedSalem"},{"poster":"nimbus_00","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/health-check-grace-period.html#:~:text=In%20the%20console%2C%20by%20default,the%20health%20check%20grace%20period.","timestamp":"1733220480.0","upvote_count":"1","comment_id":"1321323"},{"timestamp":"1732757760.0","poster":"AzureDP900","comment_id":"1318978","upvote_count":"1","content":"D is right.\nSince the EC2 instances are being terminated due to an ELB system health check failure, it's likely that the health checks are timing out while the user data scripts are still running and downloading content from S3. This causes the ALB to think the instance is unhealthy, leading to termination.\n\n\nBy increasing the health check grace period for the Auto Scaling group, you give the instances more time to complete their startup process, including the execution of the user data scripts, before considering them healthy or unhealthy. This should prevent the endless loop of launching and terminating EC2 instances due to health check failures."},{"poster":"JoeTromundo","timestamp":"1728776940.0","content":"Selected Answer: D\nCorrect answer: D - The grace period in Auto Scaling allows new instances time to finish initialization (like downloading data or running user scripts) before health checks start. By increasing this grace period, you provide more time for the EC2 instances to complete the startup process and avoid premature termination.\nIncorrect answer: B - This only extends the time the ALB waits for a response on a health check, but if the EC2 instance isn't ready to serve requests due to its long initialization time, it will still fail the health check.","upvote_count":"1","comment_id":"1296675"},{"content":"Selected Answer: B\nI prefer B over D:\nThe problem is that EC2 instances need to download content from S3 during the startup process, which may take some time. If ALB's health check is performed during this period and the EC2 instance is unable to respond to the health check request due to incomplete downloads, ALB may consider the instance unhealthy and remove it from the service. This may trigger the auto scaling group to start new instances, creating an endless loop.","upvote_count":"2","comments":[{"poster":"AzureDP900","timestamp":"1732757880.0","upvote_count":"1","content":"D is right\nIncreasing the health check timeout might allow more time for the user data scripts to run, but it doesn't give the instance a chance to recover and become healthy before termination.","comment_id":"1318981"}],"poster":"liuliangzhou","comment_id":"1284434","timestamp":"1726455420.0"}],"timestamp":"2024-06-25 20:22:00"},{"id":"BEfsoVgtRmSrzAuN3ThR","isMC":true,"answer_ET":"D","choices":{"D":"Create an Amazon RDS for PostgreSQL DB instance. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to the DB instance. Use PostgreSQL native spatial data support. Run cron jobs on the DB instance for maintenance. Use AWS Direct Connect to connect the DB instance to the on-premises environment for foreign table support.","C":"Launch Amazon EC2 instances to host the Oracle databases. Place the EC2 instances in an Auto Scaling group. Use AWS Application Migration Service to move the data from on premises to the EC2 instances and for real-time bidirectional change data capture (CDC) synchronization. Use Oracle native spatial data support. Create an AWS Lambda function to run maintenance jobs as part of an AWS Step Functions workflow. Create an internet gateway for foreign table support.","A":"Create Amazon DynamoDB global tables with auto scaling enabled. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to DynamoDB. Create an AWS Lambda function to move the spatial data to Amazon S3. Query the data by using Amazon Athena. Use Amazon EventBridge to schedule jobs in DynamoDB for maintenance. Use Amazon API Gateway for foreign table support.","B":"Create an Amazon RDS for Microsoft SQL Server DB instance. Use native replication to move the data from on premises to the DB instance. Use the AWS Schema Conversion Tool (AWS SCT) to modify the SQL Server schema as needed after replication. Move the spatial data to Amazon Redshift. Use stored procedures for system maintenance. Create AWS Glue crawlers to connect to the on-premises Oracle databases for foreign table support."},"unix_timestamp":1719516600,"question_images":[],"timestamp":"2024-06-27 21:30:00","discussion":[{"comment_id":"1296676","timestamp":"1728777240.0","content":"Selected Answer: D\nPostgreSQL natively supports spatial data through the PostGIS extension. This makes it well-suited for handling spatial data from the Oracle databases. AWS Schema Conversion Tool (SCT) and AWS Database Migration Service (DMS) are both effective tools for migrating schema and data from Oracle to Amazon RDS for PostgreSQL, ensuring minimal disruption during the migration process. Amazon RDS for PostgreSQL allows the use of cron jobs directly on the instance through extensions like pg_cron for scheduling maintenance tasks. AWS Direct Connect provides a dedicated, secure connection between the on-premises systems and the AWS environment. This low-latency link will allow querying data from on-premises Oracle databases as foreign tables in the PostgreSQL instance without going through the internet, which supports compliance and performance needs.","upvote_count":"4","poster":"JoeTromundo"},{"comment_id":"1266539","upvote_count":"2","poster":"neta1o","timestamp":"1723736820.0","content":"Selected Answer: D\nI didn't realize SCT could convert from Oracle to things like Aurora MySQL and PostgreSQL. But since that is true and a direct connect also makes sense, I'd go D. https://aws.amazon.com/dms/schema-conversion-tool/"},{"poster":"Helpnosense","comment_id":"1239738","upvote_count":"2","timestamp":"1719764640.0","content":"Selected Answer: D\nIt's Data Migration Service that provides real-time bidirectional change data capture (CDC) synchronization"},{"timestamp":"1719530760.0","upvote_count":"3","poster":"goldeneye","comment_id":"1238425","content":"Selected Answer: D\nOption D is the most appropriate because it leverages AWS services effectively to migrate and manage the databases while ensuring compatibility with spatial data types and maintaining connectivity for querying on-premises data."},{"upvote_count":"2","content":"Option D is the most appropriate because it leverages AWS services effectively to migrate and manage the databases while ensuring compatibility with spatial data types and maintaining connectivity for querying on-premises data.","poster":"goldeneye","comment_id":"1238424","timestamp":"1719530700.0"},{"timestamp":"1719516600.0","comment_id":"1238371","upvote_count":"3","poster":"mifune","content":"Selected Answer: D\nPostgreSQL has native support for spatial data, which is required by the company, so answer for me is D"}],"question_id":442,"answer_images":[],"answers_community":["D (100%)"],"exam_id":33,"answer":"D","question_text":"A company needs to move some on-premises Oracle databases to AWS. The company has chosen to keep some of the databases on premises for business compliance reasons.\n\nThe on-premises databases contain spatial data and run cron jobs for maintenance. The company needs to connect to the on-premises systems directly from AWS to query data as a foreign table.\n\nWhich solution will meet these requirements?","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/142995-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1"},{"id":"yrmppsLsMrMtV7bKrqiE","question_text":"Accompany runs an application on Amazon EC2 and AWS Lambda. The application stores temporary data in Amazon S3. The S3 objects are deleted after 24 hours.\n\nThe company deploys new versions of the application by launching AWS CloudFormation stacks. The stacks create the required resources. After validating a new version, the company deletes the old stack. The deletion of an old development stack recently failed. A solutions architect needs to resolve this issue without major architecture changes.\n\nWhich solution will meet these requirements?","choices":{"C":"Update the CloudFormation stack to add a DeletionPolicy attribute with a value of Snapshot for the S3 bucket resource","A":"Create a Lambda function to delete objects from an S3 bucket. Add the Lambda function as a custom resource in the CloudFormation stack with a DependsOn attribute that points to the S3 bucket resource.","D":"Update the CloudFormation template to create an Amazon Elastic File System (Amazon EFS) file system to store temporary files instead of Amazon S3. Configure the Lambda functions to run in the same VPC as the EFS file system.","B":"Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket."},"unix_timestamp":1719516900,"discussion":[{"content":"Selected Answer: A\nA, for sure.\nDeletionPolicy: Delete: The DeletionPolicy attribute in CloudFormation is used to specify what should happen to a resource when the stack is deleted. The value Delete indicates that CloudFormation should delete the resource (in this case, the S3 bucket) when the stack is deleted.\nNon-Empty Buckets: The problem with this approach is that CloudFormation cannot delete an S3 bucket if it contains any objects. The DeletionPolicy: Delete does not change this behavior; it only specifies that the bucket should be deleted, which will still fail if the bucket is not empty.","timestamp":"1720786380.0","comment_id":"1246723","upvote_count":"6","poster":"gfhbox0083"},{"upvote_count":"1","poster":"SIJUTHOMASP","content":"Selected Answer: A\nRepeated question.","comment_id":"1332589","timestamp":"1735328340.0"},{"poster":"nimbus_00","upvote_count":"2","comment_id":"1321329","content":"Selected Answer: A\ncommon scenario.\nhttps://repost.aws/questions/QUvAaCd6J7To-Fs-eReXMgNg/to-add-an-aws-custom-resource-to-cloudformation-template-and-provide-an-aws-lambda-function","timestamp":"1733221380.0"},{"content":"A is right\n\nBy creating a Lambda function that deletes objects from the S3 bucket, you can ensure that the old CloudFormation stack is deleted even if the deletion process fails.\n\nAttaching this Lambda function as a custom resource in the CloudFormation stack allows CloudFormation to wait for the delete operation to complete before proceeding with the deletion of the old stack.\n\n\nThe DependsOn attribute ensures that the Lambda function runs after the S3 bucket has been deleted, preventing any potential issues with deleting objects that may not be removed yet.\n\n\nAttaching this custom resource solves the issue without requiring major architecture changes.","poster":"AzureDP900","upvote_count":"1","timestamp":"1731808620.0","comment_id":"1313342"},{"poster":"JoeTromundo","upvote_count":"2","comment_id":"1296677","timestamp":"1728777600.0","content":"Selected Answer: A\nBy using a Lambda function as a custom resource, you can ensure that the Lambda function deletes the objects in the S3 bucket before CloudFormation attempts to delete the bucket itself. Adding the DependsOn attribute ensures that the S3 bucket resource will not be deleted until the Lambda function has completed its task of clearing out all objects from the bucket, thus avoiding any errors caused by attempting to delete a non-empty S3 bucket. Options B and C: These options will not work because the DeletionPolicy attribute does NOT trigger the deletion of the OBJECTS INSIDE THE BUCKET. It ONLY determines what happens to the BUCKET RESOURCE ITSELF, not its contents. The stack deletion will still fail if objects remain in the bucket. Option D introduces significant architectural changes, which are unnecessary for solving the stack deletion issue."},{"comment_id":"1242868","poster":"gfhbox0083","content":"A, for sure.\nDeletionPolicy: Delete: The DeletionPolicy attribute in CloudFormation is used to specify what should happen to a resource when the stack is deleted. The value Delete indicates that CloudFormation should delete the resource (in this case, the S3 bucket) when the stack is deleted.\nNon-Empty Buckets: The problem with this approach is that CloudFormation cannot delete an S3 bucket if it contains any objects. The DeletionPolicy: Delete does not change this behavior; it only specifies that the bucket should be deleted, which will still fail if the bucket is not empty.","upvote_count":"2","timestamp":"1720194000.0"},{"upvote_count":"3","timestamp":"1720168080.0","content":"Selected Answer: A\nyou can´t delete a S3 bucket with objects in it. So A is correct","poster":"ahrentom","comment_id":"1242616"},{"comment_id":"1241734","timestamp":"1720055760.0","upvote_count":"1","poster":"Russs99","content":"Selected Answer: B\nBy setting the Deletion Policy attribute to Delete in the stack, you ensure that the S3 bucket and its contents are deleted when the CloudFormation stack is deleted. This best option for the scenario and and aligns with the desired behavior of removing old resources when the stack is deleted."},{"comment_id":"1241686","poster":"Alagong","content":"Selected Answer: A\nIT SHOULD BE A","timestamp":"1720044960.0","upvote_count":"3"},{"poster":"AhmedSalem","content":"Selected Answer: A\nI will go for A.\nUsing Lambda function as a custom resource ensures that the S3 bucket is emptied before the stack is deleted. DependsOn Attribute ensures the Lambda function runs and completes before attempting to delete the S3 bucket, thus preventing deletion failure.","comment_id":"1241349","upvote_count":"4","timestamp":"1720004100.0"},{"timestamp":"1719976020.0","poster":"grandcanyon","comment_id":"1241130","upvote_count":"1","content":"Selected Answer: B\nWhen you specify a DeletionPolicy attribute with a value of Delete for an S3 bucket in a CloudFormation template, CloudFormation will delete the bucket and all its contents during stack deletion. This approach addresses the issue of the stack deletion failing due to the bucket not being empty."},{"upvote_count":"1","content":"Selected Answer: C\nVotes C. After s3 snapshot, cloud formation will proceed s3 bucket deletion. A is right but compare to c it doesn't match the requirement in the question. \"resolve this issue without major architecture changes.\"\nAlso the data become useless only after 24 hours. A delete everything regardless. C is better.","comment_id":"1239746","poster":"Helpnosense","timestamp":"1719765360.0"},{"timestamp":"1719667140.0","poster":"toma","content":"it should be A","upvote_count":"4","comment_id":"1239282"},{"timestamp":"1719516900.0","content":"Selected Answer: A\n\"DependsOn\" attribute ensures that the Lambda function will always be invoked before the S3 bucket is deleted in a CloudFormation. Answer A.","upvote_count":"1","poster":"mifune","comment_id":"1238372"}],"answer_description":"","answer_images":[],"answers_community":["A (88%)","8%"],"question_id":443,"isMC":true,"answer_ET":"A","exam_id":33,"topic":"1","timestamp":"2024-06-27 21:35:00","answer":"A","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/142996-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"aDdd89FviQiPeEpY8uRd","answers_community":["CE (88%)","13%"],"answer_ET":"CE","topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/142933-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"exam_id":33,"unix_timestamp":1719340260,"answer":"CE","question_text":"A company has an application that stores user-uploaded videos in an Amazon S3 bucket that uses S3 Standard storage. Users access the videos frequently in the first 180 days after the videos are uploaded. Access after 180 days is rare. Named users and anonymous users access the videos.\n\nMost of the videos are more than 100 MB in size. Users often have poor internet connectivity when they upload videos, resulting in failed uploads. The company uses multipart uploads for the videos.\n\nA solutions architect needs to optimize the S3 costs of the application.\n\nWhich combination of actions will meet these requirements? (Choose two.)","question_images":[],"timestamp":"2024-06-25 20:31:00","discussion":[{"timestamp":"1739316600.0","content":"Selected Answer: CE\nCouldn't have said it better than JoeTromundo","poster":"d401c0d","upvote_count":"1","comment_id":"1355224"},{"comment_id":"1296678","upvote_count":"3","timestamp":"1728777960.0","content":"Selected Answer: CE\nC: Incomplete multipart uploads consume unnecessary storage and increase costs, especially with large files like videos. Setting a lifecycle policy to automatically delete incomplete uploads after 7 days helps reduce unnecessary storage costs without manual intervention.\nE: The videos are frequently accessed in the first 180 days and rarely accessed after that. Transitioning the videos to S3 Standard-IA after 180 days reduces costs while still providing immediate retrieval when needed. S3 Standard-IA is designed for infrequently accessed data and offers lower storage costs than S3 Standard while maintaining fast access times.\nWhy not B? S3 Transfer Acceleration improves upload speeds for users with poor internet connectivity, but it INCURS ADDITIONAL COSTS. While this can help with uploads, it does NOT directly optimize STORAGE COSTS, which is the main goal here.","poster":"JoeTromundo"},{"content":"Selected Answer: CE\nCE for sure.\n\nWhy not B?\nThe root cause for failed upload is due to that fact that user has poor internet connectivity. That's not something transfer accelerator can help with, maybe the user need to find a better internet provider, or use Elon's star link","comment_id":"1266959","timestamp":"1723803120.0","upvote_count":"1","poster":"kgpoj"},{"timestamp":"1720060140.0","poster":"Russs99","content":"Selected Answer: CE\nCE are the best options. In option C, incomplete upload should be deleted to save costs.","comment_id":"1241771","upvote_count":"1"},{"comment_id":"1241351","upvote_count":"1","timestamp":"1720004280.0","poster":"AhmedSalem","content":"Selected Answer: CE\nFrom the cost management perspective, the answer should be CE"},{"poster":"ujizane","content":"Selected Answer: CE\nCost Optimization so CE","upvote_count":"3","comment_id":"1239207","timestamp":"1719655620.0"},{"timestamp":"1719655560.0","content":"Cost Optimization so CE","poster":"ujizane","upvote_count":"2","comment_id":"1239206"},{"comment_id":"1238373","content":"Selected Answer: BE\nB - for transfers of files over long distances between your client and an S3 bucket | E - for reducing cost for data that is accessed less frequently.","timestamp":"1719517200.0","upvote_count":"2","poster":"mifune"},{"content":"Selected Answer: CE\nC - optimizes the S3 storage costs effectively \nE - address frequent access in the first 180 days and infrequent access afterward","poster":"ebbff63","timestamp":"1719474720.0","comment_id":"1237971","upvote_count":"4"},{"content":"B and E are the only ones that make sense to me","poster":"zapper1234","comments":[{"poster":"toma","timestamp":"1719667440.0","comment_id":"1239287","content":"B is not related to cost optimization.","upvote_count":"1"}],"timestamp":"1719340260.0","comment_id":"1237049","upvote_count":"3"}],"question_id":444,"answer_images":[],"choices":{"A":"Configure the S3 bucket to be a Requester Pays bucket.","C":"Create an S3 Lifecycle configuration o expire incomplete multipart uploads 7 days after initiation.","D":"Create an S3 Lifecycle configuration to transition objects to S3 Glacier Instant Retrieval after 1 day.","E":"Create an S3 Lifecycle configuration to transition objects to S3 Standard-infrequent Access (S3 Standard- IA) after 180 days.","B":"Use S3 Transfer Acceleration to upload the videos to the S3 bucket."}},{"id":"uXt7vOCV9iDlsIYnExaC","question_images":[],"answers_community":["A (41%)","D (27%)","B (17%)","Other"],"unix_timestamp":1670692620,"isMC":true,"answer_ET":"A","exam_id":33,"timestamp":"2022-12-10 18:17:00","discussion":[{"poster":"EricZhang","comment_id":"746619","content":"A. The only difference between A and D is CloudFront function vs Lambda@Edge. In this case the CloudFront function can remove the response header based on request header and much faster/light-weight.","timestamp":"1671149580.0","upvote_count":"63","comments":[{"comments":[{"timestamp":"1738429620.0","poster":"RyGuy2025","upvote_count":"1","comment_id":"1349991","content":"Where the function is integrated - it is already past the ALB."}],"comment_id":"1349989","upvote_count":"1","poster":"RyGuy2025","timestamp":"1738429440.0","content":"If you read the solution - it does not reference a CloudFront Function, it references a Cloud Front Distribution, which is not the same. That is why B is the best answer."},{"upvote_count":"13","content":"After read, answer A \"Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header\" not really clear and fuzzy, \"The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices\" => \"Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header\" => D make sence","poster":"vn_thanhtung","comment_id":"986137","timestamp":"1692578100.0"}]},{"timestamp":"1670692620.0","poster":"masetromain","content":"I think this is answer D: Lambda@Edge can modify headers\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html","comments":[{"poster":"ninomfr64","comment_id":"1091722","upvote_count":"3","timestamp":"1702116360.0","content":"Agree on D, but also CloudFront Function can manipulate headers https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html#:~:text=cache%20hit%20ratio.-,Header%20manipulation,-%E2%80%93%20You%20can%20insert"}],"upvote_count":"29","comment_id":"741134"},{"timestamp":"1744332120.0","poster":"chucky41_1","comment_id":"1559755","upvote_count":"1","content":"Selected Answer: C\nAnswer: C\nQuestion mentions to use serverless technologies, \n\nAPI Gateway and Lambda considerd as serverless. so answer A & D can be ommited and B or C are the remaining options, but since Option C mentions mapping templates, which has ability to modify the request. that is the probable answer."},{"timestamp":"1743189120.0","upvote_count":"1","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions-choosing.html","comment_id":"1411454","poster":"cloudlab"},{"upvote_count":"1","timestamp":"1741808640.0","comment_id":"1388055","poster":"ParamD","content":"Selected Answer: A\nCloudFront functions are the easiest way to manipulate response headers"},{"upvote_count":"1","poster":"BennyMao","content":"Selected Answer: D\nCloudFront functions cannot modify response headers, which is a key requirement.","timestamp":"1741418100.0","comment_id":"1366517"},{"comment_id":"1359629","poster":"soulation","timestamp":"1740114960.0","content":"Selected Answer: A\nWhile D might work, it's overkill for this use case. Cloudfront function is 600% cheaper, less latency, less code size. Correct answer is A.","upvote_count":"1"},{"comment_id":"1349988","comments":[{"content":"Following this up to say after further research I believe B is wrong - no way to remove though.. sry ppl - API Gateway REST APIs cannot natively modify response headers based on the User-Agent header in a conditional way - sry ppl","timestamp":"1738430040.0","upvote_count":"1","comment_id":"1349994","poster":"RyGuy2025"}],"upvote_count":"2","content":"Selected Answer: B\nI'm going to wade in here and vote B as the most efficient and cost effective deployment option - here's why:\nREST APIs natively support header modifications and API Gateway responses can be customized based on conditions (major feature).\nYou can easily integrate the already moved Lamda functions - no additional components required.\nLastly, the solution meets the serverless requirement and is more cost effective than the others.","poster":"RyGuy2025","timestamp":"1738429140.0"},{"timestamp":"1734298440.0","content":"Selected Answer: A\nCloudFront Functions is ideal for lightweight, short-running functions for the following use cases: \n\nHeader manipulation – Insert, modify, or delete HTTP headers in the request or response. For example, you can add a True-Client-IP header to every request","upvote_count":"1","comment_id":"1327067","poster":"pk0619"},{"content":"The correct answer is D.\n\nHere’s why:\n\nRequirement: The solution must:\n\nUse serverless technologies.\nRetain the ability to support older devices by removing problematic headers based on the User-Agent header.\nAnalysis of the options:\n\nA. CloudFront with an ALB and a CloudFront function:\n\nCloudFront functions can modify headers, but they are limited to request handling and cannot modify response headers.\nThis does not fully meet the requirement to remove response headers based on the User-Agent header.","comment_id":"1313894","timestamp":"1731917220.0","upvote_count":"1","comments":[{"comment_id":"1336125","timestamp":"1735931580.0","upvote_count":"1","content":"According to the docs, CloudFront functions can, \"Insert, modify, or delete HTTP headers in the request or response.\" So A is the better choice.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions-choosing.html","poster":"Kirkster"}],"poster":"julianocaldeira"},{"poster":"ad11934","upvote_count":"1","content":"Option A as per https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions-choosing.html -- seems lambda@edge is not needed as cloudfront function can be quick for modifying the response headers and handles higher request rates","timestamp":"1731895920.0","comment_id":"1313793"},{"upvote_count":"3","content":"Selected Answer: A\nIt's A.\nCloudfront funtions are faster and cheaper than Lambda@Edge.\nThe use case in the question (add headers if missing from old devices) is documented on the AWS samples\nhttps://github.com/aws-samples/amazon-cloudfront-functions/tree/main/add-origin-header","poster":"CGarces","timestamp":"1731841560.0","comment_id":"1313480"},{"content":"Option A or D are viable. I went with D because Lambda@Edge offers better performance since it runs closer to the end-users, reducing latency. Lambda@Edge can also handle more complex logic and is always more suitable for modifying headers based on User-Agent. While A could work, D offers a more robust and scalable solution that better aligns with the company's requirements to support older devices and leverage serverless technologies effectively.","poster":"MChal","timestamp":"1730832540.0","comment_id":"1307519","upvote_count":"1"},{"comment_id":"1304860","content":"Selected Answer: A\nCloudFront lets you choose whether you want CloudFront to forward headers to your origin and to cache separate versions of a specified object based on the header values in viewer requests. This allows you to serve different versions of your content based on the device the user is using, the location of the viewer, the language the viewer is using, and a variety of other criteria.","timestamp":"1730264400.0","upvote_count":"1","poster":"TariqKipkemei"},{"timestamp":"1730245320.0","content":"Removing User-Agent headers is generally a lightweight task. Since it involves simple string manipulation to detect and remove headers based on predefined conditions, CloudFront Functions (Option A) are perfectly suited for this purpose. They provide fast, low-latency execution, making them ideal for straightforward operations like header manipulation.","poster":"AloraCloud","upvote_count":"1","comment_id":"1304770"},{"content":"It says a serverless solution. ALBs are not serverless.","comment_id":"1304131","upvote_count":"1","timestamp":"1730142960.0","poster":"0b43291"},{"upvote_count":"1","comment_id":"1291067","timestamp":"1727604240.0","content":"Selected Answer: D\nLambda@Edge is integrated with CloudFront and allows you to manipulate requests and responses closer to the user, such as removing unsupported headers. This is ideal for addressing the issue with older devices by inspecting the User-Agent header and removing the problematic HTTP headers.","poster":"Syre"}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/90939-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_images":[],"answer":"A","question_id":445,"answer_description":"","choices":{"B":"Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header.","C":"Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API.","A":"Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header.","D":"Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header."},"question_text":"A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers.\nThe company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions.\nWhich solution will meet these requirements?"}],"exam":{"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isMCOnly":true,"id":33,"provider":"Amazon","lastUpdated":"11 Apr 2025","isImplemented":true,"numberOfQuestions":529},"currentPage":89},"__N_SSP":true}