{"pageProps":{"questions":[{"id":"ESNRsAZkqUPVjOjn3oqF","timestamp":"2023-01-16 14:51:00","isMC":true,"answer_images":[],"question_text":"A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region:\n\n• Amazon S3 bucket that stores game assets\n• Amazon DynamoDB table that stores player scores\n\nA solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.\n\nWhat should the solutions architect do to meet these requirements?","question_images":[],"answer_description":"","choices":{"C":"Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region.","D":"Create another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.","B":"Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC).","A":"Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables."},"answer_ET":"C","topic":"1","answers_community":["C (90%)","10%"],"question_id":41,"discussion":[{"timestamp":"1675038480.0","upvote_count":"14","poster":"zozza2023","comment_id":"792203","content":"Selected Answer: C\nDynamoDB global tables + S3 replication+Cloudfront"},{"upvote_count":"7","poster":"masetromain","timestamp":"1673891340.0","comment_id":"778044","content":"Option C is the correct answer because it meets the requirements of reducing latency, improving reliability and requiring minimal effort to implement.\n\nBy creating another S3 bucket in a new Region, and configuring S3 Cross-Region Replication between the buckets, the game assets will be replicated to the new Region, reducing latency for users accessing the assets from that region. Additionally, by creating an Amazon CloudFront distribution and configuring origin failover with two origins accessing the S3 buckets in each Region, it ensures that the game assets will be served to users even if one of the regions becomes unavailable.\n\nConfiguring DynamoDB global tables by enabling Amazon DynamoDB Streams, and adding a replica table in a new Region, will also improve reliability by allowing the player scores to be replicated and updated in multiple regions, ensuring that the scores are available even in the event of a regional failure.","comments":[{"upvote_count":"3","poster":"masetromain","content":"Option A is not correct because using the new table as a replica target for DynamoDB global tables will not improve reliability. The same applies for Option D, which only uses S3 Same-Region Replication, which will not reduce latency for users in other regions.\n\nOption B is not correct because configuring asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) is not the best solution for this use case. It would require additional configuration and management effort.","timestamp":"1673891340.0","comment_id":"778047"}]},{"content":"Selected Answer: C\nJust to add for DynamoDB, indeed you will need to create replica in the new region when creating global table, making it accessible in the new region nearer to the user.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables.tutorial.html","comment_id":"1293765","poster":"Daniel76","timestamp":"1728197340.0","upvote_count":"1"},{"poster":"JoeTromundo","content":"Selected Answer: C\nOption C is correct.\nJust to clarify: AWS uses DynamoDB Streams to replicate DynamoDB Global Tables. Using the Console, it is enabled automatically. Using the CLI, you must enable it explicitly by using StreamEnabled=true.","timestamp":"1728162180.0","comment_id":"1293606","upvote_count":"1"},{"timestamp":"1705936380.0","comment_id":"1128806","poster":"ninomfr64","upvote_count":"1","content":"Selected Answer: C\nA = \"Configure S3 Cross-Region Replication\" but doesn't create a new bucket in another region.\nB = \"Configure S3 Same-Region Replication\" without creating a second bucket and this should be cross-region. AWS DMS with CDC is not a good fit here, global table is the right option here\nC = correct\nD = we need the new bucket in a different region"},{"upvote_count":"2","timestamp":"1703375760.0","poster":"career360guru","content":"Selected Answer: C\nOption C","comment_id":"1104366"},{"comment_id":"1088155","timestamp":"1701744600.0","content":"Selected Answer: C\nAnswer C.\nRegarding DynamoDB Streams - \nGlobal tables use DynamoDB Streams to replicate data across different Regions. When you create a replica for a global table, a stream is created by default. Any changes to a replica are replicated to all the other replicas within the same global table within a second using DynamoDB Streams.","upvote_count":"2","poster":"shaaam80"},{"comments":[{"timestamp":"1725242880.0","comment_id":"1276297","upvote_count":"1","poster":"helloworldabc","content":"just C"},{"content":"Option A doesn't mention creating a new bucket in a different region","poster":"ninomfr64","comment_id":"1128809","timestamp":"1705936440.0","upvote_count":"1"},{"comment_id":"1074001","content":"I initially thought it was C, but I was torn between A and C. You may be right.","timestamp":"1700313000.0","poster":"Jay_2pt0_1","upvote_count":"1"}],"timestamp":"1699480020.0","comment_id":"1065984","poster":"blackgamer","content":"The answer is A. C added unnecessary complexities such as Amazon DynamoDB Streams and Origin Failover.","upvote_count":"1"},{"comment_id":"1003096","poster":"uC6rW1aB","comments":[{"poster":"ninomfr64","timestamp":"1705936560.0","content":"C is correct, Origin Group allows failover see https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html","upvote_count":"2","comment_id":"1128814"}],"timestamp":"1694256060.0","upvote_count":"2","content":"Selected Answer: A\nother option are incorrect.\nB: Configure S3 Same-Region Replication.---> It's not meet multi-region requirement.\nC: Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. ---> It's not support for this kinda failover\nD: Create another S3 bucket in the same Region, and configure S3 Same-Region Replication between the buckets. ---> It's not meet multi-region requirement."},{"poster":"dkcloudguru","timestamp":"1694075040.0","content":"option c is the easiest way to do","upvote_count":"1","comment_id":"1001330"},{"comment_id":"995945","content":"Selected Answer: A\nCreating an Amazon CloudFront distribution will reduce latency for global users by serving assets from the closest edge location. S3 Cross-Region Replication will ensure that game assets are available in another region, improving reliability. Creating a new DynamoDB table in a new region and using it as a replica target for DynamoDB global tables will enable multi-region replication, improving reliability.","timestamp":"1693564020.0","upvote_count":"1","poster":"ProMax"},{"comment_id":"985133","timestamp":"1692443040.0","poster":"SK_Tyagi","content":"Selected Answer: C\nOption C has another differentiator - DynamoDBStreams that will assist in Reliability","upvote_count":"2"},{"upvote_count":"1","poster":"ggrodskiy","comment_id":"966375","timestamp":"1690631400.0","comments":[{"upvote_count":"1","comment_id":"996789","content":"Referred to your AWS doc link. I don't see any condition that states that the origins in the origin group cannot be from two different regions. Can you provide the statement from the AWS doc that you are referring to please ?","timestamp":"1693648860.0","poster":"venvig"}],"content":"Correct A.\nCloudFront does not support origin failover with two origins accessing the S3 buckets in each Region. According to the AWS documentationhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html, origin failover only works within the same Region, not across Regions. This means that you can only configure origin failover with two origins that are in the same Region as the CloudFront distribution. If you want to use origin failover with S3 buckets in different Regions, you need to create multiple CloudFront distributions, one for each Region, and configure them to use the same domain name with geolocation routinghttps://blog.ippon.tech/when-a-cloudfront-origin-must-fail-for-testing-high-availability/."},{"comment_id":"942025","timestamp":"1688399640.0","upvote_count":"1","content":"Selected Answer: C\nweird question wording, but C fit more","poster":"NikkyDicky"},{"content":"Selected Answer: C\nCreate another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets","timestamp":"1679896380.0","poster":"mfsec","upvote_count":"2","comment_id":"851773"},{"timestamp":"1673877060.0","comment_id":"777727","content":"C is correct. S3 cross replicate, CloudFront, Dynamodb global database and origin failover","upvote_count":"2","poster":"zhangyu20000"}],"url":"https://www.examtopics.com/discussions/amazon/view/95548-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1673877060,"exam_id":33,"answer":"C"},{"id":"ZUiS9Fr9HaO98Qz5x8kI","unix_timestamp":1673877360,"topic":"1","answer_description":"","question_id":42,"answers_community":["C (86%)","14%"],"timestamp":"2023-01-16 14:56:00","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/95549-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"exam_id":33,"question_text":"A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.\n\nThe company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application.\n\nWhich solution will meet these requirements?","answer_ET":"C","answer_images":[],"discussion":[{"upvote_count":"12","timestamp":"1694257680.0","poster":"uC6rW1aB","content":"Selected Answer: C\nC correct\nDocumentDB only have on-demand instance but not on-demand capacity mode, the mode is for DynamoDB","comment_id":"1003113"},{"poster":"ninomfr64","content":"Selected Answer: C\nA = Aurora supports MySQL and PostgreSQL, not MongoDB. App changes are not allowed\nB = This could work but DocumentDB provides managed MongoDB instance that is preferable\nC = correct\nD = there isn't on-demand capacity mode, in 2022 launched MondoDB Elastic Cluster that eliminates the need to choose, manage or upgrade instances and allows to scale up to 4PiB storage whereas instance based scales up to 128TiB.\n\nI thing this question is pre elastic cluster as this is ambiguous between C and D","comment_id":"1128860","timestamp":"1705939740.0","upvote_count":"5"},{"poster":"cnethers","timestamp":"1720111260.0","comment_id":"1242184","upvote_count":"1","content":"D is the correct answer https://aws.amazon.com/documentdb/pricing/\non-demand instance is supported by DocumentDB","comments":[{"content":"just C","poster":"helloworldabc","upvote_count":"2","comment_id":"1276299","timestamp":"1725242940.0"}]},{"comment_id":"1175938","timestamp":"1710691740.0","upvote_count":"1","poster":"gofavad926","content":"Selected Answer: C\nC, documented. No exists the on-demand capacity mode"},{"timestamp":"1706783160.0","comment_id":"1137498","upvote_count":"3","content":"'Appropriately sized instances' Means on-demand ? that is quite vague..","poster":"AimarLeo"},{"upvote_count":"2","timestamp":"1704642360.0","poster":"jpa8300","content":"Selected Answer: D\nDocumentDB does indeed support on-demand capacity mode (Contrary to what other users say here) https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/\nOn-Demand is ideally to a use case where you have unpredictable or variable database workloads, like this case, it is not said anywhere the expected workload, so it is better to start with On-demand , and later when you know the workload you can cahnge it.","comments":[{"comments":[{"timestamp":"1705937400.0","upvote_count":"1","poster":"ninomfr64","comment_id":"1128830","content":"There is no on-demand capacity for DocumentDB, however Elastic Cluster option is provided \"Elastic Clusters enables you to elastically scale your document database to handle millions of writes and reads, with petabytes of storage capacity\" see https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more.-,Elastic%20Clusters,-What%20is%20Amazon"}],"upvote_count":"1","timestamp":"1705062540.0","comment_id":"1120763","poster":"buriz","content":"what you have linked here is a dynamodb article not a documentDB one, documentDB does not support on-demand capacity mode - https://aws.amazon.com/documentdb/faqs/\n\n\"You can scale the compute resources allocated to your instance in the AWS Management Console by selecting the desired instance and clicking the “modify” button. Memory and CPU resources are modified by changing your instance class.\""},{"poster":"chicagobeef","upvote_count":"1","content":"This is DynamoDB, not DocumentDB. The choices only mention DocumentDB.","timestamp":"1705068660.0","comment_id":"1120827"}],"comment_id":"1115980"},{"upvote_count":"1","poster":"career360guru","timestamp":"1703376600.0","comments":[{"timestamp":"1723494000.0","content":"DocumentDB does support on-demand capacity:\nhttps://aws.amazon.com/documentdb/pricing/#:~:text=On-demand%20instances%20let%20you%20pay%20per%20second%2C,and%20having%20to%20guess%20the%20correct%20capacity","comment_id":"1264805","poster":"2aa2222","upvote_count":"1"}],"content":"Selected Answer: C\nThere is no on-demand capacity mode for DocumentDB, though there is on-demand vCPU based pricing available.","comment_id":"1104371"},{"comments":[],"timestamp":"1693567740.0","comment_id":"995992","upvote_count":"3","poster":"ProMax","content":"Selected Answer: C\nAmazon DocumentDB does NOT have on-demand capacity mode, so its option C."},{"timestamp":"1692443880.0","content":"Selected Answer: D\nI was leaning towards Option C but \"Appropriately sized instances\" is vague since the question does not state the size of Mongo DB. On-demand instances serve the purpose here, they are offered by DocumentDB, see the link\nhttps://aws.amazon.com/documentdb/pricing/","comment_id":"985141","upvote_count":"2","poster":"SK_Tyagi"},{"upvote_count":"2","content":"Selected Answer: C\nits a c","comment_id":"942123","poster":"NikkyDicky","timestamp":"1688408640.0"},{"poster":"easytoo","comment_id":"927780","timestamp":"1687197120.0","content":"c-c-c-c-c-c-c-c\n\nOn-demand capacity mode as suggested in D may not provide the same level of high availability as multi-Availability Zone deployments. So it's c-c-c-c-c-c-c for me.","upvote_count":"2"},{"comment_id":"920946","upvote_count":"2","content":"Selected Answer: C\nSee best practices for amazon documentdb - instance sizing in docs.\nAddicionally there is no on-demand capacity mode.","timestamp":"1686513000.0","poster":"SkyZeroZx"},{"poster":"F_Eldin","comments":[{"timestamp":"1684617540.0","content":"The correct link https://www.applytosupply.digitalmarketplace.service.gov.uk/g-cloud/services/743016963590682","poster":"F_Eldin","upvote_count":"2","comment_id":"902779","comments":[{"upvote_count":"1","content":"The content mentioned in your link and the original comment are both mentioning things related to DynamoDB. Your link is even worse which is describing DynamoDB but say it is for DocumentDB. Please study hard","poster":"[Removed]","comment_id":"1073085","timestamp":"1700206560.0"}]}],"upvote_count":"3","timestamp":"1684617420.0","content":"Selected Answer: C\nDocumentDB does indeed support on-demand capacity mode (Contrary to what other users say here) https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/\n\nbut this mode is good for spikey workloads and does not address the high availablity requirement","comment_id":"902777"},{"comment_id":"902370","upvote_count":"1","poster":"leehjworking","content":"Selected Answer: C\nSee best practices for amazon documentdb - instance sizing in docs.","timestamp":"1684560720.0"},{"comment_id":"876549","timestamp":"1682081520.0","upvote_count":"2","poster":"Sarutobi","content":"Selected Answer: C\nGoing wit C. I still call the DocumentDB used in mode C \"on-demand mode\" because you have to select the Ec2 instance; the pricing documentation still uses that name. There is an Elastic cluster for DocumentDB. Could it be that option D \"on-demand capacity mode\" is referring to Elastic mode?"},{"upvote_count":"1","poster":"OCHT","comment_id":"867065","timestamp":"1681199100.0","content":"Selected Answer: C\nAmazon DocumentDB does not support an on-demand capacity mode. You can only choose from different instance classes that have fixed compute and memory resources. However, you can scale your instances up or down as needed, and you can also pause and resume your instances to save costs. Amazon DocumentDB also automatically scales your storage and I/O based on your data size and workload."},{"timestamp":"1679896740.0","content":"Selected Answer: C\nC - there is no on-demand capacity mode.","poster":"mfsec","comment_id":"851777","upvote_count":"1"},{"upvote_count":"1","timestamp":"1679455320.0","poster":"zejou1","comment_id":"846633","content":"Selected Answer: C\nAmazon DocumentDB best practice to choose an instance type with enough RAM to fit your working set (i.e., data and indexes) in memory. Having properly sized instances will help optimize for overall performance and potentially minimize I/O cost.\n https://docs.aws.amazon.com/documentdb/latest/developerguide/best_practices.html\n\nAlso, you would already need to have it as on-demand; first thing is to size it appropriately"},{"content":"Selected Answer: C\nNo on-demand capacity mode for DocumentDB","comment_id":"834317","timestamp":"1678390020.0","upvote_count":"2","poster":"kiran15789"},{"upvote_count":"1","poster":"sambb","comment_id":"832671","timestamp":"1678263300.0","content":"Selected Answer: C\nNo on-demand capacity mode for DocumentDB"},{"content":"Selected Answer: C\nhttps://dynobase.dev/dynamodb-vs-documentdb/","timestamp":"1678258560.0","poster":"andras","comment_id":"832616","upvote_count":"1"},{"content":"Selected Answer: C\nIs C, DocumentDB On-Demand is not a thing. You need to create On-Demand instances as part of the cluster, but nothing like DynamoDB. The cluster can either be Instance base or Elastic.","timestamp":"1677805320.0","upvote_count":"2","comment_id":"827572","poster":"Sarutobi"},{"timestamp":"1676775960.0","comment_id":"813683","comments":[{"timestamp":"1676835360.0","poster":"spd","content":"Is this ref for DocumentDB ?","comment_id":"814442","upvote_count":"1"},{"content":"That blog is for Dynamodb, not document DB. Nowhere is mentioned capacity mode for documentDB, there's on-demand https://aws.amazon.com/documentdb/pricing/.","poster":"c73bf38","timestamp":"1677133500.0","comment_id":"818859","upvote_count":"3"}],"upvote_count":"1","content":"Selected Answer: D\nOn-demand capacity mode is there for document DB\nhttps://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/","poster":"Mahakali"},{"comment_id":"811019","poster":"lunt","timestamp":"1676573580.0","upvote_count":"3","content":"Selected Answer: C\nThere is no on-demand capacity mode. FAQ itself states capacity is reference to CPU and not Storage. C is right in my eyes."},{"poster":"spd","upvote_count":"2","timestamp":"1676331300.0","comment_id":"807912","content":"Selected Answer: C\nNo on-demand - so its C"},{"timestamp":"1676312640.0","poster":"[Removed]","upvote_count":"1","comment_id":"807690","content":"It's D. On Demand Instances with DocumentDB let you provision instances without knowing the capacity. https://aws.amazon.com/documentdb/pricing"},{"content":"Selected Answer: C\nThere is no on-demand mode for DocumentDB. It is C.","timestamp":"1676057940.0","upvote_count":"2","comment_id":"804685","poster":"Ilk"},{"upvote_count":"2","content":"Selected Answer: C\n\"DocumentDB in on-demand capacity mode\" is an invented thing","timestamp":"1675633680.0","poster":"Musk","comment_id":"799203"},{"poster":"ExamTopix01","upvote_count":"3","timestamp":"1675580100.0","content":"C\nOn-demand capacity mode is the function of Dynamodb. \nhttps://aws.amazon.com/blogs/news/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/\n\nAmazon DocumentDB Elastic Clusters\nhttps://aws.amazon.com/blogs/news/announcing-amazon-documentdb-elastic-clusters/","comments":[{"comment_id":"799201","poster":"Musk","timestamp":"1675633560.0","content":"Very good catch","upvote_count":"1"},{"timestamp":"1676775540.0","content":"Both these URLs are not found. Are you sure?","upvote_count":"1","comment_id":"813681","poster":"Mahakali"}],"comment_id":"798656"},{"timestamp":"1673891460.0","comment_id":"778050","upvote_count":"3","content":"Selected Answer: D\nD is correct because it uses Amazon DocumentDB with MongoDB compatibility, which allows for a seamless migration of the subscriber data from the on-premises MongoDB database to the cloud. Additionally, by using DocumentDB in on-demand capacity mode, the company can easily scale the database based on the actual load and usage of the application, without the need to provision instances in advance.\n\nOption A is not a good fit because Aurora is a relational database and not compatible with MongoDB.\nOption B is not a good fit because it uses MongoDB on a single EC2 instance, which does not provide high availability for the subscriber data.\nOption C is not a good fit because it uses Amazon DocumentDB with MongoDB compatibility, but it is deployed on instances, which may not be able to handle the load and usage of the application.","poster":"masetromain"},{"poster":"zhangyu20000","content":"D is correct with Document on demand mode because it is new deployment. load is unknown\nA: Aurora is not compatiable with Mongodb\nB: Mongodb on single EC2 not support HA\nC: DocumentDB on instance may not support real load","upvote_count":"2","timestamp":"1673877360.0","comment_id":"777731"}],"isMC":true,"choices":{"B":"Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.","A":"Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.","D":"Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.","C":"Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."}},{"id":"C59KCtNuVKwb8OVLdjC0","answers_community":["ACF (100%)"],"exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/95550-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","choices":{"C":"Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.","F":"Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.","D":"Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.","E":"Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.","B":"Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.","A":"Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account."},"discussion":[{"comments":[{"timestamp":"1694233980.0","comment_id":"833652","upvote_count":"3","poster":"God_Is_Love","content":"first the source bucket needs to give grant access thru bucket policy and KMS key policy (A,C options)\nSecondly, Strategy IAM role needs to give access to read from S3 bucket and also KMS key (Option F)"}],"upvote_count":"13","timestamp":"1694233800.0","comment_id":"833649","poster":"God_Is_Love","content":"Selected Answer: ACF\nB wrong - full permissions ? when question asks for minimum permissions.\nD wrong - anonymous user ? anonymous does not work\nE wrong - encrypt permissions ? No Strategy account needs decrypt permissions\nSo, A,C,F"},{"upvote_count":"6","content":"Selected Answer: ACF\nB full permission ? X\nD anonymous? X\nE encryption not needed for strategy team","comment_id":"903647","poster":"leehjworking","timestamp":"1700625840.0"},{"poster":"career360guru","content":"Selected Answer: ACF\nA, C and F","upvote_count":"1","timestamp":"1719180960.0","comment_id":"1104374"},{"content":"Selected Answer: ACF\nBy rule of elimination\nBDE are wrong. God_Is_Love is spot on","timestamp":"1708349040.0","poster":"SK_Tyagi","upvote_count":"1","comment_id":"985145"},{"poster":"NikkyDicky","timestamp":"1704313680.0","comment_id":"942129","upvote_count":"2","content":"Selected Answer: ACF\nits ACF"},{"timestamp":"1696751400.0","poster":"OCHT","comment_id":"864529","content":"Selected Answer: ACF\nOption B suggests updating the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key. This option is not ideal because it grants more permissions than necessary. The requirement is to provide users with only the minimum permissions they need to view objects in the S3 bucket.\n\nOption D suggests creating a bucket policy that includes read permissions for the S3 bucket and setting the principal of the bucket policy to an anonymous user. This option is not ideal because it would allow anyone to read objects in the S3 bucket, which could pose a security risk.\n\nOption E suggests updating the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role. This option is not necessary because the requirement is for users in the Strategy account to be able to view objects in the S3 bucket, not to encrypt them.","upvote_count":"3"},{"timestamp":"1695795540.0","poster":"mfsec","comment_id":"851788","upvote_count":"2","content":"Selected Answer: ACF\nACF is the best choice"},{"upvote_count":"2","poster":"taer","timestamp":"1695008160.0","comment_id":"842490","content":"Selected Answer: ACF\nA. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.\nC. Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.\nF. Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."},{"timestamp":"1690670940.0","upvote_count":"3","comment_id":"792215","poster":"zozza2023","content":"Selected Answer: ACF\nA C AND F"},{"upvote_count":"3","comment_id":"789348","timestamp":"1690429560.0","poster":"Untamables","content":"Selected Answer: ACF\nhttps://repost.aws/knowledge-center/cross-account-access-denied-error-s3"},{"poster":"masetromain","comment_id":"781389","timestamp":"1689780240.0","content":"Selected Answer: ACF\nA, C, and F are the correct options.","upvote_count":"4"},{"upvote_count":"3","poster":"masetromain","content":"A, C, and F are the correct options.\n\nOption A creates a bucket policy that includes read permissions for the S3 bucket and sets the principal of the bucket policy to the account ID of the Strategy account. This ensures that users in the Strategy account have the necessary permissions to access the S3 bucket.\n\nOption C updates the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role. This ensures that the users in the Strategy account have the necessary permissions to decrypt the objects stored in the S3 bucket.\n\nOption F updates the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key. This ensures that the users in the Strategy account have the necessary permissions to read the objects in the S3 bucket and to decrypt them using the custom KMS key.\n\nThe other options are not correct because they either grant unnecessary permissions (B, D) or grant permissions in the wrong way (E).","timestamp":"1689522780.0","comment_id":"778055"},{"timestamp":"1689508740.0","comment_id":"777736","poster":"zhangyu20000","content":"ACF is correct","upvote_count":"2"}],"isMC":true,"question_text":"A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company’s marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects.\n\nA solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.\n\nThe solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)","question_id":43,"answer":"ACF","question_images":[],"answer_images":[],"answer_description":"","unix_timestamp":1673877540,"timestamp":"2023-01-16 14:59:00","answer_ET":"ACF"},{"id":"CKIT7iSW4zP6aDPIZr6R","answer_description":"","answer":"C","discussion":[{"comment_id":"865660","timestamp":"1681056720.0","upvote_count":"23","poster":"dev112233xx","content":"Selected Answer: C\nAlmost voted D because of the Storage Gateway + SAN combination.. but seems like it's not correct since S3 events cannot trigger Batch jobs directly, you need a Lambda function! S3 events can be only Lambda,SNS or SQS..","comments":[{"comment_id":"875791","upvote_count":"3","poster":"Kampton","content":"Agree - The Lambda function acts as a bridge between the S3 event and AWS Batch, allowing you to trigger AWS Batch jobs in response to S3 events.","timestamp":"1682010060.0"}]},{"timestamp":"1678384320.0","comments":[{"poster":"AWSum1","timestamp":"1728318840.0","comment_id":"1294391","upvote_count":"1","content":"Nope, you need S3 events to trigger Lambda. S3 events cannot trigger batch"},{"comment_id":"1276300","poster":"helloworldabc","upvote_count":"1","content":"just C","timestamp":"1725243240.0"},{"poster":"God_Is_Love","comment_id":"834248","timestamp":"1678384740.0","upvote_count":"4","content":"On Premise NAS and file servers to S3. --> Use DataSync solution\nOn Premise SMB or NFS file share to S3 --> Use Storage/File Gateway solution"},{"poster":"titi_r","content":"@God_Is_Love, both articles you've provided are NOT mentioning \"SAN\" at all. You cannot copy data from SAN using storage GW, but you do it with DataSync ran from within a server, which is connected to that SAN. Research more on what SAN is and how does it work :)","timestamp":"1710713220.0","comment_id":"1176083","upvote_count":"1"}],"upvote_count":"9","content":"Selected Answer: D\nGuys its Tricky one between C and D and answer is D! (Modernization question)\nLook at this two below blogs : \nhttps://aws.amazon.com/blogs/storage/using-aws-storage-gateway-to-modernize-next-generation-sequencing-workflows/\n\nThanks to tinyflame who made me do my research on this :-)\nYes, SAN -> Storage Gateway Only\nNAS -> Data Sync or Storage Gateway\nhttps://aws.amazon.com/blogs/storage/from-on-premises-to-aws-hybrid-cloud-architecture-for-network-file-shares/","comment_id":"834243","poster":"God_Is_Love"},{"timestamp":"1729100820.0","upvote_count":"1","content":"Selected Answer: C\nDataSync + Direct Connect\nS3 => Lambda => SF\nDocker => ECR => Batch","comment_id":"1298843","poster":"FZA24"},{"timestamp":"1723664220.0","comment_id":"1265960","content":"lambda solo dura 900 segundos me voy por la D","upvote_count":"1","comments":[],"poster":"k10training02"},{"poster":"trungtd","upvote_count":"1","timestamp":"1716854700.0","comment_id":"1219902","content":"Selected Answer: C\nCurrently, S3 events can only push to three different types of destinations:\nSNS topic, SQS Queue, AWS Lamba. \nYou cannot directly trigger a Batch job by S3 Event"},{"comment_id":"1128872","upvote_count":"3","timestamp":"1705941060.0","content":"Selected Answer: C\nA = 200GB very now and then doesn't need Snowball Edge\nB = Data Pipeline is ETL and not suitable in hybrid scenarios\nC = correct (DataSync does the job, also the app is already container based and it works well with Batch that is suited for HPC kind of workload - genomic sequencing is a typical HPC workload)\nD = even tough Storage Gateway does the job you cannot directly trigger a AWS Batch job from an S3 event, you need either a Lambda in the middle or enable EventBrdige notification and create a rule that triggers the AWS Batch Job","poster":"ninomfr64"},{"timestamp":"1705499820.0","comment_id":"1125015","upvote_count":"1","content":"... \"The main requirement is that the data needs to be accessible over the network in a file format like NFS that DataSync supports.\"","poster":"cox1960"},{"timestamp":"1705499700.0","content":"C - Amazon Q says \"While it does not directly support SAN (storage area network), you can use AWS DataSync to transfer data from files stored on a SAN volume to AWS storage services like Amazon S3.\"","poster":"cox1960","upvote_count":"1","comment_id":"1125012"},{"content":"Selected Answer: C\nOption C is better option. Though D is also possible but as the jobs are already container based C would be better.\nQuestion is not clear whether containers used on-premise are docker based containers.","comment_id":"1104379","timestamp":"1703377440.0","poster":"career360guru","upvote_count":"2"},{"timestamp":"1703342820.0","poster":"mosalahs","content":"Selected Answer: C\nData Transfer --- > Data Sync\nData Integration --- > Storage GW\nData Orchestration --- > Data Pipeline","upvote_count":"3","comment_id":"1104076"},{"content":"Selected Answer: C\nD doesn't seem to be correct as AWS Batch is not a destination for AWS S3 events.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html","timestamp":"1702091280.0","comment_id":"1091414","upvote_count":"2","poster":"Maygam"},{"content":"Selected Answer: C\nOption C: Use AWS DataSync to transfer data to Amazon S3. DataSync is designed for fast, easy and secure data transfer. This option also uses S3 events to trigger an AWS Lambda function, which launches an AWS Step Functions workflow and runs a Docker container using AWS Batch. This option takes into account data transfer, processing and container management, and should be the most suitable solution.\nOption D: Use AWS Storage Gateway's file gateway to transfer data to Amazon S3. Storage Gateway is suitable for hybrid cloud environments, but in this case, since the company already has a high-speed AWS Direct Connect connection, it will be more efficient to use DataSync.","comment_id":"1003131","poster":"uC6rW1aB","upvote_count":"2","timestamp":"1694258880.0"},{"content":"C.\nOf the given options C is probably the closest. Step Functions can be used to model the workflow. D does not specify this. DataSync can be used to transfer data [https://docs.aws.amazon.com/datasync/latest/userguide/s3-cross-account-transfer.html].","timestamp":"1692645660.0","comment_id":"986883","upvote_count":"1","poster":"Ganshank"},{"content":"Selected Answer: D\nI choose D. My rationale - 200GB data for 1 genome sequence, Lets say DirectConnect is 1Gbps line, DataSync cannot efficiently transfer the data to get the processing under 1 day.\nAgree with God_Is_Love's hypothesis","comment_id":"985153","comments":[{"poster":"vn_thanhtung","timestamp":"1693549920.0","content":"S3 event can't trigger direct AWS Batch job. => C","comment_id":"995761","upvote_count":"1"},{"poster":"ninomfr64","comment_id":"1128876","upvote_count":"1","content":"Assuming DX is 1Gbps, it takes about 27 minutes to transfer 200GB. also, I don't see how Storage Gateway can speedup things. My point is that here both DataSynch and Storage Gateway can di the job, but you cannot trigger Batch job directly from S3 object event. Thus C","timestamp":"1705941540.0"}],"timestamp":"1692445020.0","upvote_count":"1","poster":"SK_Tyagi"},{"poster":"RGR21","upvote_count":"1","timestamp":"1691592240.0","comment_id":"976760","content":"Does the AWS DataSync support SAN?"},{"content":"Correct D.","upvote_count":"1","timestamp":"1690629720.0","poster":"ggrodskiy","comment_id":"966357"},{"poster":"NikkyDicky","upvote_count":"1","content":"Selected Answer: C\nC\nD would be an option if using volume gateway and lambda to trigger batch\ndatasync dont need to support NAS. agent can copy off of NFS or SMB mount of the NAS drive.","comment_id":"942144","timestamp":"1688410800.0"},{"poster":"Jackhemo","timestamp":"1687213980.0","comment_id":"927994","content":"Selected Answer: C\nI answered D, but Olabiba.ai says C, because:\nHere's why option C is the most suitable choice:\n\n\n\nOverall, option C provides a scalable and efficient solution for the company to process genomics data on AWS, meeting their capacity and turnaround time requirements.","upvote_count":"1"},{"upvote_count":"3","comment_id":"921007","poster":"Buggie","timestamp":"1686520740.0","content":"Lambda can run only for 15 minutes."},{"comment_id":"902772","content":"Selected Answer: C\nYou should use the datasync option. The option that says, to use the S3 events to trigger the batch is not fully correct, you need a lambda in order to have this type of integration in this case option D is incorrect.","poster":"rbm2023","upvote_count":"1","timestamp":"1684617000.0"},{"content":"Selected Answer: D\nOption D is a valid solution to the given scenario. \nBy using an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3, the company can leverage the Direct Connect connection to transfer data quickly and securely. \nUsing S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data, the company can take advantage of the scalability and flexibility of AWS Batch to process genomics data. \nThis solution allows the company to process the data quickly and scale based on workload demands while reducing the turnaround time from weeks to days.","upvote_count":"2","poster":"Parsons","timestamp":"1682522580.0","comment_id":"881768"},{"timestamp":"1680091140.0","content":"Cannot be D. S3 events cannot trigger Batch jobs. Only Eventbridge can trigger but thats not an option in D. Both Storage FileGW and Datasync dont support SAN. File GS supports NAS vis NFS/SMB. DataSync NAS vis NFS/SMB.\nData Pipeline can be an option.","poster":"Eshu2009","comment_id":"854351","upvote_count":"2"},{"content":"Selected Answer: C\n正解はCです。\nSANについての記載がありますが、それはあくまで現状の説明であって、次期の仕組みの話ではないです。\nまた、S3イベントで起動できるものにAWS Batchはありません。","comment_id":"854216","upvote_count":"5","timestamp":"1680082080.0","poster":"Asagumo","comments":[{"comments":[{"upvote_count":"1","content":"translate for others too","comment_id":"983232","poster":"chikorita","timestamp":"1692249780.0"}],"poster":"easytoo","content":"summed it up nicely.","comment_id":"927789","upvote_count":"4","timestamp":"1687197840.0"}]},{"comment_id":"848695","upvote_count":"4","timestamp":"1679607900.0","poster":"Arnaud92","content":"For me, none of these answer are correct.\nC: DataSync is not working with SAN\nD: Storage gateway have multiple gateway type. Answer is talking about \"file gateway\" which is not compatible with SAN. The gateway compatible would be \"Volume gateway\".\nB: Data Pipeline, i'm not sure it's working with SAN.\nA: snow is not a solution for regular and automatic process .."},{"timestamp":"1679491980.0","comment_id":"847103","upvote_count":"1","comments":[{"upvote_count":"1","content":"Storage file gateway with SAN?","comment_id":"1141309","poster":"AWSLord32","timestamp":"1707154200.0"}],"poster":"mfsec","content":"Selected Answer: D\nD because of the SAN. Its more efficient to use Storage Gateway."},{"timestamp":"1679117820.0","poster":"taer","upvote_count":"2","content":"Selected Answer: C\nC is correct","comment_id":"842492"},{"comment_id":"842403","poster":"Damijo","timestamp":"1679102460.0","upvote_count":"7","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/storage/how-to-move-and-store-your-genomics-sequencing-data-with-aws-datasync/"},{"comment_id":"836932","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/filegateway/latest/filefsxw/using-dx.html","timestamp":"1678619640.0","poster":"krishccie","upvote_count":"1"},{"timestamp":"1678466460.0","content":"Selected Answer: C\nC is correct","poster":"vherman","comment_id":"835235","upvote_count":"2"},{"timestamp":"1677411840.0","poster":"kiran15789","comment_id":"822317","content":"Selected Answer: C\nOption D uses an AWS Storage Gateway file gateway, which is not a good fit for the high-speed Direct Connect connection, and it introduces additional complexity with an extra gateway layer.","upvote_count":"5"},{"timestamp":"1677134280.0","upvote_count":"2","poster":"c73bf38","comment_id":"818873","content":"Selected Answer: C\nOption C would be the best solution to meet the requirements.\n\nAWS DataSync can be used to transfer the sequencing data to Amazon S3, which will make the data accessible to the processing applications. S3 events can then be used to trigger an AWS Lambda function, which starts an AWS Step Functions workflow. The Docker images can be stored in Amazon Elastic Container Registry (Amazon ECR), which can then trigger AWS Batch to run the containers and process the sequencing data. This approach makes the system scalable, and the Docker containers can be run on multiple EC2 instances to handle the workload."},{"comment_id":"816137","upvote_count":"4","poster":"tinyflame","timestamp":"1676947800.0","content":"Selected Answer: D\nSAN -> Storage Gateway Only\nNAS -> Data Sync or Storage Gateway"},{"timestamp":"1675039920.0","comment_id":"792218","content":"Selected Answer: C\nC is the correct solution","poster":"zozza2023","upvote_count":"2"},{"content":"The correct solution is C.\n\nAWS DataSync can be used to transfer the sequencing data to Amazon S3, which is a more efficient and faster method than using Snowball Edge devices. Once the data is in S3, S3 events can trigger an AWS Lambda function that starts an AWS Step Functions workflow. The Docker images can be stored in Amazon Elastic Container Registry (Amazon ECR) and AWS Batch can be used to run the container and process the sequencing data.\n\nOption A is not the best solution because it would take a long time to transfer the data to AWS and process the data, and AWS Snowball Edge is not ideal for high-speed data transfer.\nOption B is not the best solution because EC2 Auto Scaling group is not a cost-effective solution for running short-lived jobs\nOption D is not the best solution because AWS Batch is not the best service for running short-lived jobs and it may not be cost-effective","upvote_count":"5","poster":"masetromain","timestamp":"1673891640.0","comment_id":"778058"},{"poster":"zhangyu20000","comment_id":"777739","upvote_count":"1","timestamp":"1673877660.0","content":"C is correct"}],"answer_images":[],"question_text":"A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days.\n\nThe company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day.\n\nWhich solution meets these requirements?","choices":{"A":"Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.","D":"Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.","B":"Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.","C":"Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data."},"question_id":44,"exam_id":33,"unix_timestamp":1673877660,"timestamp":"2023-01-16 15:01:00","isMC":true,"question_images":[],"topic":"1","answers_community":["C (78%)","D (22%)"],"url":"https://www.examtopics.com/discussions/amazon/view/95551-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"C"},{"id":"urvPkYmHK8rH2WNRiESr","unix_timestamp":1673878740,"answer_description":"","answer_ET":"C","question_id":45,"answer":"C","isMC":true,"question_images":[],"timestamp":"2023-01-16 15:19:00","discussion":[{"content":"Selected Answer: C\nEFS is Linux/Mac based, So, A,D are out.\nLustre stands for Linux cluster, So B is out. Left is C which is correct (Amazon FSx for Windows )","upvote_count":"15","timestamp":"1694293620.0","comment_id":"834504","poster":"God_Is_Love"},{"content":"Selected Answer: C\nC for windows, AD and ACLs","comment_id":"1200543","timestamp":"1729670340.0","poster":"julmarcas","upvote_count":"1"},{"comment_id":"1143347","timestamp":"1723030560.0","poster":"rootcode","upvote_count":"1","content":"Selected Answer: C\nC is the correct option"},{"content":"Selected Answer: C\nOption C as it is windows based OS.","upvote_count":"1","timestamp":"1719181800.0","comment_id":"1104382","poster":"career360guru"},{"content":"Selected Answer: C\nOption B FSx for Lustre is not for Linux POSIX-compliant\nOption C correct","comment_id":"1003144","upvote_count":"2","timestamp":"1709991600.0","poster":"uC6rW1aB"},{"comment_id":"1001624","timestamp":"1709827200.0","poster":"dkcloudguru","upvote_count":"1","content":"C FSx for windows is a good fit for this"},{"comment_id":"949348","content":"FSx for Lustre can only be used by Linux-based instances.","poster":"Sam202","timestamp":"1705021980.0","upvote_count":"1"},{"timestamp":"1704315720.0","upvote_count":"1","poster":"NikkyDicky","content":"Selected Answer: C\nC for windows","comment_id":"942146"},{"comment_id":"920953","content":"Selected Answer: C\nEFS and FSx for Lustre == Linux\nFSx Windows File == Windows","upvote_count":"3","timestamp":"1702332180.0","poster":"SkyZeroZx"},{"upvote_count":"2","timestamp":"1695382800.0","comment_id":"847112","poster":"mfsec","content":"Selected Answer: C\nEFS and Windows is not straight forward. C is the best solution."},{"poster":"zejou1","comment_id":"846655","content":"Selected Answer: C\nAmazon FSx is built on Windows Server... Access Control Lists (ACLs)... To control user access, Amazon FSx integrates with your on-premises Microsoft Active Directory as well as with AWS Microsoft Managed AD.\nhttps://aws.amazon.com/fsx/windows/features/?nc=sn&loc=2\n\nAll others don't work - forget about the \"least management\" statement - it says \"implement Windows ACLS to control...\" all others are thrown out.","upvote_count":"3","timestamp":"1695348180.0"},{"poster":"kiran15789","comment_id":"822326","upvote_count":"2","content":"Selected Answer: C\nOption D suggests using an EFS file system, which is a shared file system that can be mounted on multiple EC2 instances, but this requires additional configuration to keep the content in sync across all instances.\n\nOption C is the optimal choice because Amazon FSx for Windows File Server supports Windows ACLs and seamlessly integrates with Active Directory to join instances to a domain. This option minimizes management overhead by reducing the complexity of managing multiple EFS file shares or writing scripts to synchronize content across EC2 instances.","timestamp":"1693043820.0"},{"poster":"Musk","timestamp":"1691339940.0","content":"Selected Answer: C\nFSX for WIndows is the only option. The rest of options are not supported.","upvote_count":"2","comment_id":"800124"},{"poster":"jojom19980","upvote_count":"2","timestamp":"1691040840.0","comment_id":"796839","content":"Selected Answer: C\nFSx for Lustre can only be used by Linux-based instances."},{"content":"Selected Answer: D\ngood answer are C or D but as it says LEAST management overhead ==> D as in C we will need a user data script","upvote_count":"1","timestamp":"1690671480.0","poster":"zozza2023","comments":[{"content":"sorry D is uncorrect as it use Elastic File System (Amazon EFS) itch is not windows so Iswitch to C","comments":[{"comment_id":"943495","content":"Also that means each instance launched from the AMI will have 2TB EBS volume.. which is not ideal","timestamp":"1704451620.0","poster":"lxrdm","upvote_count":"1"}],"upvote_count":"1","comment_id":"792227","poster":"zozza2023","timestamp":"1690671600.0"}],"comment_id":"792226"},{"upvote_count":"1","timestamp":"1690473120.0","content":"@masetromain is this a good exam study guide? Like how many questions were from here. Any help would be appreciated. Thank you","poster":"ARLV","comment_id":"789855"},{"upvote_count":"1","poster":"Untamables","timestamp":"1690432620.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_join_instance.html","comment_id":"789372"},{"timestamp":"1689607800.0","upvote_count":"4","comments":[{"comments":[{"upvote_count":"3","timestamp":"1689607800.0","poster":"masetromain","content":"The other choices are not correct because:\n\nOption A: An Amazon Elastic File System (Amazon EFS) file share is not a windows file system and it does not support Windows ACLs.\n\nOption B: Amazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, it is not a windows file system and it does not support Windows ACLs.\n\nOption D: An Amazon Elastic File System (Amazon EFS) file share is not a windows file system and it does not support Windows ACLs.\n\nIn both cases, creating a new AMI from the current EC2 instance that is running it doesn't help to solve the problem as it won't provide a scalable solution that runs on at least three instances across multiple Availability Zones.","comment_id":"779139"}],"comment_id":"779138","upvote_count":"2","poster":"masetromain","timestamp":"1689607800.0","content":"This solution meets the requirements with the least management overhead because it utilizes Amazon FSx for Windows File Server, which is a fully managed service that allows you to easily set up a highly available and scalable file server. The Auto Scaling group ensures that the application is running on at least three instances across multiple Availability Zones, providing high availability and fault tolerance. The user data script can be used to automate the setup and configuration of the instances when they are launched, and it can be used to join the instances to the AD domain, so that the instances can be managed and access to the file contents can be controlled using Windows ACLs."}],"comment_id":"779136","content":"Selected Answer: C\nI switch for C: Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.","poster":"masetromain"},{"content":"Selected Answer: D\nThe correct answer is D, as it meets all of the requirements with the least management overhead.\n\nIn this solution, an Amazon Elastic File System (Amazon EFS) file system is created and an Auto Scaling group is created that extends across three Availability Zones and maintains a minimum size of three instances. A new AMI is created from the current EC2 instance that is running, and the instances in the Auto Scaling group are then launched from this new AMI.\n\nA seamless domain join is then performed to join the instances to the AD domain, and the Amazon EFS file system is mounted on the instances. This solution uses an existing EC2 instance, so there is no need to use a user data script to install the application or join the instances to the AD domain, which reduces the management overhead.","comment_id":"778061","timestamp":"1689523020.0","upvote_count":"1","poster":"masetromain","comments":[{"comment_id":"778062","upvote_count":"1","timestamp":"1689523020.0","content":"The other choices are not correct because they either require a user data script to install the application or to join the instances to the AD domain, which increases the management overhead, or they use a different file system that may not be compatible with the application or the AD domain.","poster":"masetromain"}]},{"content":"D is only one make sense\nA: No AMI creation, have to use user data to install app, more complex\nB: need user data\nC: need user data\nD: has least management overhead","poster":"zhangyu20000","comments":[{"upvote_count":"1","poster":"Musk","comment_id":"800123","content":"D: EFS does not work for WIndows.","timestamp":"1691339880.0"}],"upvote_count":"1","timestamp":"1689509940.0","comment_id":"777758"}],"url":"https://www.examtopics.com/discussions/amazon/view/95555-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","answer_images":[],"choices":{"D":"Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain.","B":"Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system.","C":"Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.","A":"Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share."},"exam_id":33,"question_text":"A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones.\n\nA solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in time.\n\nWhich solution will meet these requirements with the LEAST management overhead?","answers_community":["C (95%)","5%"]}],"exam":{"name":"AWS Certified Solutions Architect - Professional SAP-C02","id":33,"isBeta":false,"provider":"Amazon","isImplemented":true,"numberOfQuestions":529,"lastUpdated":"11 Apr 2025","isMCOnly":true},"currentPage":9},"__N_SSP":true}