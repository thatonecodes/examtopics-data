{"pageProps":{"questions":[{"id":"7jqMynXfBwIN0C946IhQ","answer_ET":"B","choices":{"B":"Use a target tracking policy to dynamically scale the Auto Scaling group.","C":"Use an AWS Lambda function ta update the desired Auto Scaling group capacity.","D":"Use scheduled scaling actions to scale up and scale down the Auto Scaling group.","A":"Use a simple scaling policy to dynamically scale the Auto Scaling group."},"discussion":[{"comment_id":"759433","upvote_count":"22","content":"Selected Answer: B\nThe correct answer is B. To maintain the desired performance across all instances in the Amazon EC2 Auto Scaling group, the solutions architect should use a target tracking policy to dynamically scale the Auto Scaling group.\n\nA target tracking policy allows the Auto Scaling group to automatically adjust the number of EC2 instances in the group based on a target value for a metric. In this case, the target value for the CPU utilization metric could be set to 40% to maintain the desired performance of the application. The Auto Scaling group would then automatically scale the number of instances up or down as needed to maintain the target value for the metric.\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html","timestamp":"1672208160.0","poster":"Buruguduystunstugudunstuy"},{"timestamp":"1697045040.0","poster":"tom_cruise","content":"Selected Answer: B\ntarget tracking policy = maintain","upvote_count":"6","comment_id":"1040963"},{"content":"Selected Answer: B\nJustificativa:\nO que é uma política de rastreamento de destino?\nUma política de rastreio de destino (Target Tracking Scaling Policy) ajusta automaticamente o tamanho do grupo de Auto Scaling para manter uma métrica-alvo específica, como a utilização da CPU, próxima a um valor desejado.\nNesse caso, a métrica-alvo seria 40% de utilização da CPU.\nPor que a opção B é ideal?\nAutomação baseada em métricas:\nO rastreamento de destino monitora continuamente a utilização da CPU das instâncias e ajusta a capacidade do grupo para manter o desempenho próximo ao alvo configurado (40%).","poster":"Rcosmos","upvote_count":"1","timestamp":"1736873340.0","comment_id":"1340447"},{"timestamp":"1736338320.0","content":"Selected Answer: B\nSince it is mentioned that it works best when CPU usage is around 40% hence using target tracking within ASG's is the most suitable option here.","upvote_count":"1","poster":"satyaammm","comment_id":"1337931"},{"comments":[{"content":"Useful, jaradat02. Thanks.","poster":"1e22522","timestamp":"1722606420.0","upvote_count":"1","comment_id":"1259917"}],"comment_id":"1253527","upvote_count":"3","poster":"jaradat02","content":"Selected Answer: B\nB is the correct answer, since target scaling monitors cloudwatch metrics, while simple/step scaling monitors cloudwatch alarms.","timestamp":"1721724180.0"},{"timestamp":"1705426980.0","upvote_count":"3","comment_id":"1124394","poster":"awsgeek75","content":"Selected Answer: B\n40% CPU for best performance is a \"target tracking\" policy for scaling so B is correct.\nA: Wrong policy\nCD: Won't achieve 40% CPU"},{"comment_id":"1057343","content":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html","upvote_count":"3","timestamp":"1698639960.0","poster":"Ruffyit"},{"poster":"youdelin","timestamp":"1696806540.0","comment_id":"1028340","upvote_count":"2","content":"Selected Answer: B\nI really don't get what kind of software running like a car with the most economical fuel speed range, but well, the answer is B"},{"timestamp":"1693544340.0","comment_id":"995696","upvote_count":"2","content":"Selected Answer: B\nThe application performs best when the CPU utilization of the EC2 instances is at or near 40%. \nTarget tracking will maintain CPU utilization at 40%. When CloudWatch detects that the average CPU utilization is beyond 40%, it will trigger the target tracking policy to scale out the auto scaling group to meet this target utilization. Once everything is settled and the average CPU utilization has gone below 40%, another scale in action will kick in and reduce the number of auto scaling instances in the auto scaling group.","poster":"TariqKipkemei"},{"comment_id":"982834","timestamp":"1692208560.0","upvote_count":"4","content":"Selected Answer: B\nThe key reasons are:\n\nA target tracking policy allows defining a specific target metric value to maintain, in this case 40% CPU utilization.\nAuto Scaling will automatically add or remove instances to keep utilization at the target level, without manual intervention.\nThis will dynamically scale the group to maintain performance as load changes.\nA simple scaling policy only responds to breaching thresholds, not maintaining a target.\nScheduled actions and Lambda would require manual calculation and updates to track utilization.\nTarget tracking policies are the native Auto Scaling feature designed to maintain a metric at a target value.","poster":"Guru4Cloud"},{"upvote_count":"3","timestamp":"1687505520.0","poster":"cookieMr","content":"Selected Answer: B\nTarget tracking policy is the most appropriate choice. This policy allows ASG to automatically adjust the desired capacity based on a target metric, such as CPU utilization. By setting the target metric to 40%, ASG will scale the number of instances up or down as needed to maintain the desired CPU utilization level. This ensures that the application's performance remains optimal.\n\nA suggests using a simple scaling policy, which allows for scaling based on a fixed metric or threshold. However, it may not be as effective as a target tracking policy in dynamically adjusting the capacity to maintain a specific CPU utilization level.\n\nC suggests using an Lambda to update the desired capacity. While this can be done programmatically, it would require custom scripting and may not provide the same level of automation and responsiveness as a target tracking policy.\n\nD suggests using scheduled scaling actions to scale up and down ASG at predefined times. This approach is not suitable for maintaining the desired performance in real-time based on actual CPU utilization.","comment_id":"931314"},{"comment_id":"872971","upvote_count":"1","content":"Selected Answer: B\nB of course.","timestamp":"1681755780.0","poster":"Robrobtutu"},{"poster":"aba2s","comment_id":"764697","timestamp":"1672753380.0","content":"Selected Answer: B\nB seem to the correct response.\n\nWith a target tracking scaling policy, you can increase or decrease the current capacity of the group based on a target value for a specific metric. This policy will help resolve the over-provisioning of your resources. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern.","upvote_count":"4"},{"upvote_count":"2","poster":"orionizzie","comment_id":"754817","timestamp":"1671879120.0","content":"Selected Answer: B\ntarget tracking - CPU at 40%"},{"poster":"career360guru","content":"Selected Answer: B\nOption B","comment_id":"748618","timestamp":"1671340620.0","upvote_count":"1"},{"poster":"Wpcorgan","timestamp":"1669066560.0","upvote_count":"1","comment_id":"723954","content":"B is correct"},{"upvote_count":"5","poster":"ArielSchivo","timestamp":"1667919600.0","comment_id":"713914","content":"Selected Answer: B\nOption B. Target tracking policy.\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"},{"content":"B\n\nCPU utilization = target tracking","poster":"Nigma","timestamp":"1667893140.0","upvote_count":"3","comment_id":"713571"},{"upvote_count":"1","poster":"SimonPark","comment_id":"707625","content":"Selected Answer: B\nB is the answer","timestamp":"1667107800.0"}],"answer":"B","topic":"1","isMC":true,"question_images":[],"question_id":56,"url":"https://www.examtopics.com/discussions/amazon/view/86659-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer_images":[],"unix_timestamp":1667107800,"question_text":"An application runs on Amazon EC2 instances across multiple Availability Zonas. The instances run in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%.\nWhat should a solutions architect do to maintain the desired performance across all instances in the group?","answers_community":["B (100%)"],"timestamp":"2022-10-30 06:30:00","answer_description":""},{"id":"lc75meYkIcq96ylKCgAv","question_id":57,"answer_description":"","topic":"1","choices":{"B":"Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to CloudFront.","A":"Write individual policies for each S3 bucket to grant read permission for only CloudFront access.","C":"Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns the target S3 bucket as the Amazon Resource Name (ARN).","D":"Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 bucket permissions so that only the OAI has read permission."},"isMC":true,"exam_id":31,"discussion":[{"comment_id":"699608","comments":[{"poster":"SimonPark","comment_id":"707626","content":"Thanks it convinces me","upvote_count":"1","timestamp":"1698644100.0"}],"content":"Selected Answer: D\nI want to restrict access to my Amazon Simple Storage Service (Amazon S3) bucket so that objects can be accessed only through my Amazon CloudFront distribution. How can I do that?\nCreate a CloudFront origin access identity (OAI)\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/","upvote_count":"40","timestamp":"1697783760.0","poster":"123jhl0"},{"poster":"Guru4Cloud","comment_id":"982838","content":"Selected Answer: D\nThe key reasons are:\n\nAn OAI provides secure access between CloudFront and S3 without exposing the S3 bucket publicly.\nThe OAI is associated with the CloudFront distribution.\nThe S3 bucket policy limits access only to that OAI.\nThis ensures only CloudFront can access the objects, not direct S3 access.\nOption A is complex to manage individual bucket policies.\nOption B exposes credentials that aren't needed.\nOption C works but OAI is the preferred method.\nSo using an origin access identity provides the most secure way to serve private S3 content through CloudFront. The OAI prevents direct public access to the S3 bucket.","timestamp":"1723831260.0","upvote_count":"10"},{"content":"Selected Answer: D\nOAI's are designed specifically for S3 origins. It helps provide security for CloudFront.","poster":"satyaammm","timestamp":"1736338560.0","upvote_count":"1","comment_id":"1337933"},{"content":"Selected Answer: D\nC would also work but missing important details in the answer\nD is legacy and architect should not recommend it","upvote_count":"4","timestamp":"1730808960.0","comment_id":"1062863","poster":"xdkonorek2"},{"upvote_count":"2","comment_id":"1041004","content":"Selected Answer: D\n\"If your users try to access objects using Amazon S3 URLs, they're denied access. The origin access identity has permission to access objects in your Amazon S3 bucket, but users don't.\"\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html","poster":"tom_cruise","timestamp":"1728669240.0"},{"content":"Selected Answer: D\nTo meet the requirements of serving files through CloudFront while restricting direct access to the S3 bucket URL, the recommended approach is to use an origin access identity (OAI). By creating an OAI and assigning it to the CloudFront distribution, you can control access to the S3 bucket.\nThis setup ensures that the files stored in the S3 bucket are only accessible through CloudFront and not directly through the S3 bucket URL. Requests made directly to the S3 URL will be blocked.\n\n\nOption A suggests writing individual policies for each S3 bucket, which can be cumbersome and difficult to manage, especially if there are multiple buckets involved.\n\nOption B suggests creating an IAM user and assigning it to CloudFront, but this does not address restricting direct access to the S3 bucket URL.\n\nOption C suggests writing an S3 bucket policy with CloudFront distribution ID as the Principal, but this alone does not provide the necessary restrictions to prevent direct access to the S3 bucket URL.","comment_id":"931316","timestamp":"1719128040.0","poster":"cookieMr","upvote_count":"4"},{"upvote_count":"2","comment_id":"911835","timestamp":"1717225080.0","poster":"antropaws","content":"DECEMBER 2022 UPDATE:\n\nRestricting access to an Amazon S3 origin:\n\nCloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI). We recommend using OAC because it supports:\n\nAll Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022\nAmazon S3 server-side encryption with AWS KMS (SSE-KMS)\nDynamic requests (PUT and DELETE) to Amazon S3\n\nOAI doesn't work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html"},{"poster":"Buruguduystunstugudunstuy","comment_id":"759452","upvote_count":"6","timestamp":"1703745900.0","content":"Selected Answer: D\nThe correct answer is D. To meet the requirements, the solutions architect should create an origin access identity (OAI) and assign it to the CloudFront distribution. The S3 bucket permissions should be configured so that only the OAI has read permission.\n\nAn OAI is a special CloudFront user that is associated with a CloudFront distribution and is used to give CloudFront access to the files in an S3 bucket. By using an OAI, the company can serve the files through the CloudFront distribution while preventing direct access to the S3 bucket.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html"},{"timestamp":"1702876740.0","content":"Selected Answer: D\nD is the right answer","upvote_count":"1","comment_id":"748619","poster":"career360guru"},{"comments":[{"comment_id":"872977","poster":"Robrobtutu","upvote_count":"1","content":"Thanks, I didn't know about OAC.","timestamp":"1713378480.0"}],"timestamp":"1702495260.0","comment_id":"744398","content":"Selected Answer: D\nD is correct but instead of OAI using OAC would be better since OAI is legacy","poster":"gloritown","upvote_count":"3"},{"comment_id":"724300","upvote_count":"1","poster":"Wpcorgan","content":"D is correct","timestamp":"1700655060.0"}],"question_text":"A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The company wants to serve all the files through an Amazon CloudFront distribution. The company does not want the files to be accessible through direct navigation to the S3 URL.\nWhat should a solutions architect do to meet these requirements?","answer_images":[],"question_images":[],"timestamp":"2022-10-20 08:36:00","unix_timestamp":1666247760,"answer_ET":"D","answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/85992-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"D"},{"id":"QhDUwsowQIdhrdkxQ2C7","question_text":"A company’s website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company’s website demands globally. The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.\nWhich combination should a solutions architect recommend to meet these requirements?","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/86654-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"poster":"G3","comment_id":"792040","upvote_count":"21","content":"Selected Answer: A\nHistorical reports = Static content = S3","timestamp":"1675024200.0"},{"upvote_count":"11","comment_id":"707602","content":"A is the correct answer\n The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.","poster":"dokaedu","timestamp":"1667104920.0"},{"content":"Selected Answer: A\nQuestion mentioned cost effective and S3 is the most cost effective answer","upvote_count":"1","poster":"Manimgh","comment_id":"1559811","timestamp":"1744358160.0"},{"poster":"PaulGa","comment_id":"1285740","upvote_count":"2","timestamp":"1726664640.0","content":"Selected Answer: A\nAns A - S3 is designed to optimise costs, highly scalable and can store static content such as website. CloudFront is designed to securely deliver content with low latency and high transfer rate."},{"poster":"huaze_lei","upvote_count":"2","timestamp":"1725540600.0","comment_id":"1278935","content":"Selected Answer: A\nHistorical data will not change, and hence they are static content. So the answer is S3 with distributed content (Cloudfront)"},{"upvote_count":"2","content":"Selected Answer: A\nA is the most suitable choice because the content is static and downloadable","comment_id":"1253575","timestamp":"1721727600.0","poster":"jaradat02"},{"timestamp":"1720311840.0","comment_id":"1243620","content":"Selected Answer: A\nA. S3 is designed to optimize storage costs, is highly scalable and can hold static content e.g. a website. CloudFront is designed to securely deliver content with low latency and high transfer speeds.","poster":"KTEgghead","upvote_count":"2"},{"content":"Bringing content closer to users, Answer is A","timestamp":"1705318440.0","poster":"MrPCarrot","upvote_count":"3","comment_id":"1123289"},{"timestamp":"1693800240.0","poster":"TariqKipkemei","comment_id":"998178","content":"Selected Answer: A\nGlobal, cost-effective, serverless, low latency = CloudFront with S3\nStatic content = S3","upvote_count":"6"},{"upvote_count":"3","timestamp":"1692208980.0","comment_id":"982841","poster":"Guru4Cloud","content":"Selected Answer: A\nHistorical reports = Static content = S3"},{"comment_id":"931319","content":"By using CloudFront, the website can leverage the global network of edge locations to cache and deliver the performance reports to users from the nearest edge location, reducing latency and providing fast response times. Amazon S3 serves as the origin for the files, where the reports are stored.\n\nOption B is incorrect because AWS Lambda and Amazon DynamoDB are not the most suitable services for serving downloadable files and meeting the website demands globally.\n\nOption C is incorrect because using an Application Load Balancer with Amazon EC2 Auto Scaling may require more infrastructure provisioning and management compared to the CloudFront and S3 combination. Additionally, it may not provide the same level of global scalability and fast response times as CloudFront.\n\nOption D is incorrect because while Amazon Route 53 is a global DNS service, it alone does not provide the caching and content delivery capabilities required for serving the downloadable reports. Internal Application Load Balancers do not address the global scalability and caching requirements specified in the scenario.","comments":[{"comment_id":"949565","timestamp":"1689146640.0","upvote_count":"1","poster":"Bmarodi","content":"Very good explanations!"}],"timestamp":"1687505820.0","poster":"cookieMr","upvote_count":"5"},{"poster":"Buruguduystunstugudunstuy","timestamp":"1672211160.0","content":"Selected Answer: A\nThe correct answer is Option A. To meet the requirements, the solutions architect should recommend using Amazon CloudFront and Amazon S3.\n\nBy combining Amazon CloudFront and Amazon S3, the solutions architect can provide a scalable and cost-effective solution that limits the provisioning of infrastructure resources and provides the fastest possible response time.\n\nhttps://aws.amazon.com/cloudfront/\n\nhttps://aws.amazon.com/s3/","upvote_count":"4","comment_id":"759463"},{"content":"A is correct","upvote_count":"1","timestamp":"1672076760.0","comment_id":"757704","poster":"techhb"},{"timestamp":"1671340920.0","content":"Selected Answer: A\nA is the best and most cost effective option if only download of the static pre-created report(no data processing before downloading) is a requirement.","comment_id":"748620","upvote_count":"2","poster":"career360guru"},{"poster":"Wpcorgan","upvote_count":"1","content":"A is correct","comment_id":"724301","timestamp":"1669119120.0"},{"timestamp":"1668355020.0","comment_id":"717410","upvote_count":"2","poster":"sdasdawa","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/27935-exam-aws-certified-solutions-architect-associate-saa-c02/"},{"timestamp":"1668348480.0","comment_id":"717343","upvote_count":"2","poster":"Nirmal3331","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/27935-exam-aws-certified-solutions-architect-associate-saa-c02/"},{"content":"Selected Answer: A\nSee this discussion:\nhttps://www.examtopics.com/discussions/amazon/view/27935-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"samplunk","upvote_count":"2","comment_id":"717166","timestamp":"1668326460.0"},{"content":"Selected Answer: C\nload balancing + scalability + cost effective","comments":[{"upvote_count":"4","poster":"pentium75","content":"Why use EC2 instances to serve historical report files? Those belong in S3. No need to run in VM to let users download static content.","timestamp":"1703579280.0","comment_id":"1105827"}],"comment_id":"717054","upvote_count":"1","poster":"manu427","timestamp":"1668304800.0"},{"comment_id":"716880","timestamp":"1668279900.0","upvote_count":"1","content":"Selected Answer: B\nI think the answer is B","poster":"MyNameIsJulien","comments":[]}],"timestamp":"2022-10-30 05:42:00","answers_community":["A (96%)","2%"],"answer":"A","choices":{"A":"Amazon CloudFront and Amazon S3","B":"AWS Lambda and Amazon DynamoDB","C":"Application Load Balancer with Amazon EC2 Auto Scaling","D":"Amazon Route 53 with internal Application Load Balancers"},"answer_description":"","question_id":58,"unix_timestamp":1667104920,"answer_ET":"A","answer_images":[],"exam_id":31,"topic":"1"},{"id":"XISRPjMRkB8GFnMd5gPN","topic":"1","isMC":true,"answer_description":"","answer_ET":"C","choices":{"A":"Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS Region.","B":"Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to replicate the snapshots to another AWS Region.","D":"Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another Availability Zone.","C":"Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region."},"unix_timestamp":1665688800,"question_images":[],"question_id":59,"question_text":"A company runs an Oracle database on premises. As part of the company’s migration to AWS, the company wants to upgrade the database to the most recent available version. The company also wants to set up disaster recovery (DR) for the database. The company needs to minimize the operational overhead for normal operations and DR setup. The company also needs to maintain access to the database's underlying operating system.\nWhich solution will meet these requirements?","answer_images":[],"answers_community":["C (51%)","A (39%)","9%"],"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/85423-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"timestamp":"2022-10-13 21:20:00","discussion":[{"content":"Option C since RDS Custom has access to the underlying OS and it provides less operational overhead. Also, a read replica in another Region can be used for DR activities.\n\nhttps://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/","comments":[{"timestamp":"1686810600.0","content":"You can't create cross-Region replicas in RDS Custom for Oracle: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-rr.html#custom-rr.limitations","poster":"KalarAzar","upvote_count":"23","comment_id":"923781"}],"timestamp":"1667994180.0","poster":"ArielSchivo","upvote_count":"45","comment_id":"714519"},{"poster":"brushek","content":"Selected Answer: C\nIt should be C:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-custom.html\nand\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/working-with-custom-oracle.html","upvote_count":"21","comments":[{"content":"how it is C when the read replica is not meant for DR","comment_id":"1021334","upvote_count":"3","comments":[{"comment_id":"1055392","timestamp":"1698399540.0","poster":"wsdasdasdqwdaw","content":"If the source DB instance fails, you can promote your Read Replica to a standalone source server.","upvote_count":"10"}],"timestamp":"1696060500.0","poster":"bhgt"}],"comment_id":"694231","timestamp":"1665688800.0"},{"poster":"CloudExpert01","upvote_count":"1","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/working-with-custom-oracle.html","comment_id":"1539315","timestamp":"1743863460.0"},{"upvote_count":"1","comment_id":"1352878","content":"Selected Answer: C\nThe company also needs to maintain access to the database's underlying operating system --> only RDS Custom can provide this facility","timestamp":"1738912620.0","poster":"AshishDhole"},{"poster":"zdi561","upvote_count":"1","timestamp":"1738207920.0","comment_id":"1348895","content":"Selected Answer: C\nyou can create a read replica in another region which is less operational cost for DR"},{"content":"Selected Answer: C\nAmazon RDS Custom for Oracle:\n\nAmazon RDS Custom for Oracle provides a managed database service while also allowing access to the underlying operating system, which is required in this scenario.\nThis option strikes a balance between minimizing operational overhead (as AWS manages backups, patching, and scaling) and retaining flexibility for customizations or specific access requirements.","comment_id":"1316079","upvote_count":"2","poster":"0de7d1b","timestamp":"1732236420.0"},{"poster":"TheTeaBoy","content":"A: Incorrect - Question states to minimise operational overhead, building an EC2 instance and managing that doesn't reduce the operational overhead.\nB: Incorrect - Yes that ticks the DR aspect, but Amazon RDS for Oracle doesn't provide access to the underlying operating system of the database server.\nC: Correct - Amazon RDS Custom provides a limited amount of operating system access of the database server(s), so that's good. Creating a read replica in another AWS Region covers DR. Its not asking for High Availability (HA) in this case.\nD: Incorrect - Amazon RDS for Oracle doesn't provide access to the underlying operating system of the database server. A standby database in another availability, could be considered as covering DR.","upvote_count":"3","comment_id":"1310225","timestamp":"1731346800.0"},{"timestamp":"1729791000.0","poster":"mzeynalli","upvote_count":"2","comment_id":"1302567","content":"Selected Answer: C\nAmazon RDS Custom for Oracle is the best choice because it allows access to the underlying OS, provides an easier path for upgrades, minimizes operational overhead, and supports setting up cross-region read replicas for a DR solution."},{"poster":"19d92c7","comment_id":"1301620","upvote_count":"1","content":"It should be standby:\n. When the primary database fails, \nAmazon RDS promotes the secondary database to primary. Because it assumes the primary databases endpoint, \nthe EC2 instances can resume traffic with the new primary database. Meanwhile, a new standby database is \ncreated in the other Availability Zone.","timestamp":"1729607460.0"},{"comments":[{"upvote_count":"2","content":"D is out since DR is more about \"different region\", not just \"different AZ\"","poster":"LuongTo","timestamp":"1729747500.0","comment_id":"1302313"}],"content":"Selected Answer: D\nAns D - use regular RDS Oracle; no need for RDS Custom because the question doesn't state and special customisations... the only bit unanswered is \"...maintain access to company's underlying o/s\"","comment_id":"1285742","timestamp":"1726664940.0","poster":"PaulGa","upvote_count":"2"},{"content":"Selected Answer: C\nOption C provides the right balance of managed service convenience, access to the underlying OS, and effective disaster recovery with cross-region read replicas","upvote_count":"2","poster":"bignatov","comment_id":"1275002","timestamp":"1725009300.0"},{"content":"Selected Answer: A\n\"You can't create cross-Region RDS Custom for Oracle replicas.\" So it can't be C.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-rr.html\n\nB and D don't provide access to underlying OS. So the only option left is A, which won't help minimize the operational overhead though.","poster":"MatAlves","timestamp":"1724739540.0","upvote_count":"4","comment_id":"1273212"},{"comments":[{"timestamp":"1724739420.0","comment_id":"1273211","content":"\"You can't create cross-Region RDS Custom for Oracle replicas.\"\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-rr.html","upvote_count":"1","poster":"MatAlves"}],"comment_id":"1264961","timestamp":"1723521000.0","content":"Selected Answer: C\nB, D - do not have access to underlying OS.\nBoth A and C could work, but C is less Overhead.\n\nC - Amazon RDS Custom for Oracle actually supports creating read replicas. \"Creating an RDS Custom for Oracle replica is similar to creating an RDS for Oracle replica, but with important differences.\" (https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-rr.html). The read replica is not meant for disaster recovery, but it could work as such when no better options are available.","upvote_count":"2","poster":"scaredSquirrel"},{"comment_id":"1263574","timestamp":"1723304880.0","upvote_count":"3","content":"Selected Answer: A\nYou can't create cross-Region RDS Custom for Oracle replicas.","poster":"Abbas_Abi_AWS"},{"poster":"jaradat02","content":"Selected Answer: C\nSince we need to have access to the underlying infrastructure, C makes sense.","timestamp":"1721727660.0","comment_id":"1253577","upvote_count":"1"},{"content":"Selected Answer: C\nIts C\nKey : Access to underlying O.S - RDS custom can give you this feature\nGeneral RDS for Oracle( or any) - you can't access underlying O.S\nSo, definitely C considering this point in the question","upvote_count":"1","poster":"ChinthaGurumurthi","comment_id":"1253178","timestamp":"1721665020.0"},{"upvote_count":"1","content":"Defintely not C. You cannot create a read replica of RDS custom for Oracle in a different region: https://aws.amazon.com/blogs/database/build-high-availability-for-amazon-rds-custom-for-oracle-using-read-replicas/","comment_id":"1246378","poster":"freedafeng","timestamp":"1720742460.0"}]},{"id":"2XVX3fOl140TDia2B1mD","answer":"C","answer_images":[],"answer_description":"","isMC":true,"unix_timestamp":1666250280,"choices":{"C":"Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.","A":"Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.","D":"Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon RDS to query the data.","B":"Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data."},"answers_community":["C (54%)","A (46%)","0%"],"question_text":"A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region.\nWhich solution will meet these requirements with the LEAST operational overhead?","question_images":[],"answer_ET":"C","exam_id":31,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/85993-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":60,"discussion":[{"comments":[{"content":"See comment from Nicknameinvalid below. You get your answer.","upvote_count":"2","poster":"markw92","comment_id":"925613","timestamp":"1686958860.0"},{"comments":[{"timestamp":"1699561320.0","content":"Answer A has you move the data before you enable replication, therefore there is no difference between A and C when it comes to the point in time you enable replication. I agree A would be a better choice if the order of operations said, create a bucket->Enable encryption->move files...but it doesn't. It has you create the bucket and move the files.","poster":"Chiznitz","upvote_count":"3","comment_id":"1066704"}],"poster":"MutiverseAgent","comment_id":"952396","upvote_count":"1","content":"It'a since replication works for new objects but not for the existing ones, untless you use batch replication which is not the case.","timestamp":"1689426000.0"}],"upvote_count":"63","comment_id":"699631","timestamp":"1666250280.0","poster":"123jhl0","content":"Selected Answer: C\nSSE-KMS vs SSE-S3 - The last seems to have less overhead (as the keys are automatically generated by S3 and applied on data at upload, and don't require further actions. KMS provides more flexibility, but in turn involves a different service, which finally is more \"complex\" than just managing one (S3). So A and B are excluded. If you are in doubt, you are having 2 buckets in A and B, while just keeping one in C and D.\nhttps://s3browser.com/server-side-encryption-types.aspx \nDecide between C and D is deciding on Athena or RDS. RDS is a relational db, and we have documents on S3, which is the use case for Athena. Athena is also serverless, which eliminates the need of controlling the underlying infrastructure and capacity. So C is the answer.\nhttps://aws.amazon.com/athena/"},{"comment_id":"707618","upvote_count":"29","content":"Answer is A: \nAmazon S3 Bucket Keys reduce the cost of Amazon S3 server-side encryption using AWS Key Management Service (SSE-KMS). This new bucket-level key for SSE can reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS. With a few clicks in the AWS Management Console, and without any changes to your client applications, you can configure your bucket to use an S3 Bucket Key for AWS KMS-based encryption on new objects.\nThe Existing S3 bucket might have uncrypted data - encryption will apply new data received after the applying of encryption on the new bucket.","timestamp":"1667107200.0","comments":[{"content":"But in server side encryption Multi Region Keys is not possible which leaves to Option C","upvote_count":"3","comments":[{"upvote_count":"1","timestamp":"1715814360.0","poster":"NSA_Poker","content":"\"you manage the multi-Region key in each Region independently. Neither AWS nor AWS KMS ever automatically creates or replicates multi-Region keys into any Region on your behalf. AWS managed keys, the KMS keys that AWS services create in your account for you, are always single-Region keys.\"\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html","comment_id":"1212158","comments":[{"upvote_count":"1","timestamp":"1720635960.0","comment_id":"1245678","poster":"SAgang","content":"from your link you missed this part: \n\nyou can encrypt data in one AWS Region and decrypt it in a different AWS Region without re-encrypting or making a cross-Region call to AWS KMS"}]}],"comment_id":"991738","poster":"AKBM7829","timestamp":"1693186560.0"},{"upvote_count":"1","timestamp":"1683849120.0","poster":"s50600822","content":"Don't know what \"kays\" are, could they be a trap?","comment_id":"895491","comments":[{"poster":"Bmarodi","content":"Kays = keys, mistype i think.","upvote_count":"1","comment_id":"918306","timestamp":"1686229860.0"}]},{"upvote_count":"2","timestamp":"1670565540.0","poster":"RBSK","comment_id":"739828","content":"Cost reduction is in comparison bet Bucket level KMS key and object level KMS key. Not between SSE-KMS and SSE-S3. Hence its a wrong comparison"},{"poster":"RODROSKAR","content":"Reducing cost was never the target, it's LEAST operational. In that regard SSE-S3 AWS fully managed.","upvote_count":"6","comment_id":"713354","timestamp":"1667864700.0"},{"poster":"LuckyAro","timestamp":"1673818500.0","content":"I didn't read anywhere in the question where cost was an issue of consideration, so how you made it a main issue here is beyond me.","upvote_count":"6","comment_id":"777087"},{"content":"If you want to use the cost argument: SSE-S3 is free so it's cheaper than any other encryption solution (all of the others have a cost), so the answer should be C","timestamp":"1684755000.0","comments":[{"comments":[{"poster":"pentium75","upvote_count":"3","comment_id":"1105848","timestamp":"1703582100.0","content":"Which is the same in A and C. In A you create a new bucket, but still you load the existing data into the new bucket before enabling replication."}],"poster":"MutiverseAgent","comment_id":"952397","timestamp":"1689426060.0","upvote_count":"2","content":"Replication does not work for existing objects, only for new ones."}],"upvote_count":"2","poster":"ruqui","comment_id":"903990"},{"content":"Both answers A & C can be possible from the certificate perspective because in both regions will be certificates to encrypt/decrypt, SSE-KMS and SSE-S3 respectively. But the difference is that replication works for new objects and not existing ones, so that leaves answer A as the only right option.","upvote_count":"2","comment_id":"952410","poster":"MutiverseAgent","comments":[],"timestamp":"1689426960.0"}],"poster":"dokaedu"},{"comment_id":"1539457","content":"Selected Answer: C\nsince the requirement is \"The serverless solution needs to analyze existing and new data\", the answer C seems to be more appropriate since all the data will be in 1 bucket","timestamp":"1743863640.0","upvote_count":"1","poster":"CloudExpert01"},{"poster":"Mimine87","comment_id":"1505041","timestamp":"1743818880.0","content":"Selected Answer: A\nAmazon S3 + Athena is a fully serverless data analytics solution (no infrastructure to manage).\n\nS3 Cross-Region Replication (CRR) enables automatic, asynchronous replication of objects across AWS Regions, satisfying the replication requirement.\n\nSSE-KMS with multi-Region keys ensures that the data is encrypted and can be decrypted in the destination Region, which is crucial for cross-region analytics and compliance.\n\nAthena is used for analyzing data directly in S3 using SQL, which meets the requirement to analyze existing and new data using SQL (SL was a typo).\n\nThis option delivers everything with the least operational overhead — no server provisioning, no DB management, and built-in encryption + replication.","upvote_count":"1"},{"poster":"Faraz999","comment_id":"1414791","upvote_count":"1","content":"Selected Answer: A\nyou can apply cross-region replication to an existing Amazon S3 bucket, but it will only replicate new objects after the replication rule is configured. To replicate existing objects, you'll need to use S3 Batch Replication.","timestamp":"1743417840.0"},{"comment_id":"1410860","timestamp":"1743074580.0","upvote_count":"1","content":"Selected Answer: A\nI went for A because, if you start to encrypt only new data then \n1. You will have an inconsistently solution for security. I think it fair to assume that if they want all new data encrypted, then all data should be encrypted\n2. the serverless function will become more complex as it will have to establish whether the data is encrypted or not every time it retrieves an object. \n\nTherefore you need to create a new S3 bucket so that all data will be encrypted","poster":"MPG1970"},{"content":"Selected Answer: C\nfewer services = less operational overhead\nmanaged services/serverless = less operational overhead","comment_id":"1401489","poster":"SirDNS","timestamp":"1742549340.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1741704780.0","poster":"tch","content":"Selected Answer: A\nIf you want a straightforward approach to encrypting your S3 data without managing your own encryption keys, SSE-S3 is a good option. \n\nyou need AWS KMS for this complex solution","comment_id":"1387477"},{"comment_id":"1349729","upvote_count":"1","timestamp":"1738376760.0","content":"Selected Answer: A\nA new bucket is needed to encrypt objects. C uses existing bucket.","poster":"zdi561"},{"timestamp":"1736907600.0","upvote_count":"1","content":"Selected Answer: A\nAbout typos: \"kays\" should be \"keys\", and \"SL\" should be \"SQL\". So you gotta choose between A and C (We need Athena!).\n\nAbout option C: SSE-S3 is a valid encryption method, just not as suitable as SSE-KMS with multi-Region keys for CRR. SSE-KMS with multi-Region keys simplifies key management in the destination Region.","poster":"LeonSauveterre","comment_id":"1340614"},{"comment_id":"1340465","upvote_count":"1","timestamp":"1736875320.0","poster":"Rcosmos","content":"Selected Answer: U\nA melhor opção para atender aos requisitos com a menor sobrecarga operacional é a opção A:\n\nA. Crie um novo bucket do S3. Carregue os dados no novo bucket do S3. Use a replicação entre regiões (CRR) do S3 para replicar objetos criptografados para um bucket do S3 em outra região. Use a criptografia no lado do servidor com chaves multirregionais do AWS KMS (SSE-KMS). Use o Amazon Athena para consultar os dados.Conclusão:\nA opção A é a escolha ideal, pois combina os recursos sem servidor do Amazon Athena, a segurança avançada do SSE-KMS com chaves multirregionais, e a replicação automática entre regiões com o S3 CRR, tudo com a menor sobrecarga operacional."},{"poster":"skylerwhite","comment_id":"1331634","content":"Selected Answer: A\nwhat is the different between create a new S3 bucket and load data into the existing S3 bucket? I don't get it this. Please..","timestamp":"1735142880.0","upvote_count":"1"},{"poster":"Mischi","upvote_count":"1","content":"Selected Answer: A\nOption A provides a serverless solution, advanced encryption with AWS KMS multi-region keys and cross-region replication with CRR, all with the lowest operational overhead. Amazon Athena is the ideal tool for analyzing data in S3 without the need for additional infrastructure.","comment_id":"1330671","timestamp":"1734928380.0"},{"upvote_count":"1","poster":"Chr1s_Mrg","timestamp":"1727955000.0","content":"Selected Answer: C\nRDS is relational DB so we need Athena for this","comment_id":"1292768"},{"poster":"tonybuivannghia","content":"Selected Answer: A\nI think A is correct because SSE-S3 doesn't support multi-region key management, but SSE-KMS has.","upvote_count":"1","comment_id":"1288418","timestamp":"1727156520.0"},{"comment_id":"1285746","poster":"PaulGa","content":"Selected Answer: A\nAns A - once you realise \"SL\" is a typo for \"ML\" then its only the Athena options, and in the case of option it means setting up a new S3 bucket","comments":[{"content":"I'm curious that if the SL is the typo of \"SQL\"?","poster":"llccing","comment_id":"1331724","upvote_count":"2","timestamp":"1735168200.0"}],"upvote_count":"3","timestamp":"1726665360.0"},{"comment_id":"1273223","content":"Selected Answer: C\n\"Unencrypted objects and objects encrypted with SSE-S3 are replicated by default.\" (stephane maarek course)","poster":"MatAlves","timestamp":"1724740800.0","upvote_count":"2"},{"poster":"appltsla","upvote_count":"1","timestamp":"1722654600.0","comment_id":"1260118","content":"Selected Answer: A\ngpt-4 says A"},{"poster":"ChymKuBoy","timestamp":"1718763360.0","content":"Selected Answer: C\nC for sure","upvote_count":"2","comment_id":"1232680"},{"content":"Selected Answer: C\nSince S3 has provides the automatic encryption for the storage objects, create another bucket is redundant, C has the least operational overhead.","timestamp":"1718153220.0","comment_id":"1228751","upvote_count":"3","poster":"CCCat"},{"content":"Selected Answer: A\nWhat do mean by 'load the data into the existing bucket' ! the data is already staying in the existing bucket !","timestamp":"1714655400.0","poster":"ManikRoy","comments":[{"upvote_count":"1","timestamp":"1721665200.0","poster":"ChinthaGurumurthi","content":"The question says 'existing and new data'","comment_id":"1253180"}],"comment_id":"1205562","upvote_count":"1"},{"timestamp":"1713360540.0","content":"Selected Answer: C\nfrom @pentium75\n\nData in S3 is queried with Athena, not RDS, thus B and D are out.\n\nA requires a new bucket and loading data into that - Why, since data is already in S3? It says to enable CRR only after loading the data, so existing data won't be replicated anyway.\n\nC uses existing data (less operational overhead compared to loading data into a new bucket) and SSE-E3 (less operational overhead than SSE-KMS).","poster":"[Removed]","comment_id":"1197257","upvote_count":"4"},{"comment_id":"1189895","content":"Selected Answer: A\nOption B suggests using Amazon RDS to query the data, which introduces additional complexity compared to using Amazon Athena.\nOption C suggests using server-side encryption with Amazon S3 managed encryption keys (SSE-S3) instead of AWS KMS multi-Region keys, which might not meet the encryption requirements.\nOption D also suggests using Amazon RDS to query the data, which, as mentioned earlier, is not the best choice for a serverless solution and would result in higher operational overhead.","upvote_count":"1","timestamp":"1712317320.0","poster":"Solomon2001"},{"content":"The answer is A because SSE-S3 does not support cross-region replication of encrypted data. If you perform cross-region replication, you will have to re-encrypt the data.","timestamp":"1709977560.0","comment_id":"1169382","upvote_count":"2","poster":"cheroh_tots"},{"content":"Selected Answer: A\nawai it is correct","timestamp":"1708806240.0","poster":"suryansb","upvote_count":"1","comment_id":"1158140"},{"poster":"thewalker","timestamp":"1706528760.0","comments":[{"content":"5. Monitor the job completion to ensure all objects were encrypted. You can optionally delete the original unencrypted versions after verifying successful encryption.\nThis approach minimizes disruption and performs the encryption without having to rewrite existing data or code. \n\nAlso Refer: \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-copy-example-bucket-key.html","timestamp":"1706528820.0","poster":"thewalker","upvote_count":"2","comment_id":"1134894"}],"comment_id":"1134893","content":"Selected Answer: C\nAs per Amazon Q:\nThe easiest way to encrypt existing objects in S3 is to use server-side encryption with S3-managed keys (SSE-S3). Here are the basic steps:\n1. Enable SSE-S3 on the target S3 bucket if it is not already enabled. This will ensure all new or copied objects are encrypted automatically.\n2. Create an S3 inventory report for the source bucket containing the objects. This will generate a CSV file with metadata of all objects.\n3. Use S3 Select or AWS Athena to query the inventory report and filter for only unencrypted objects.\n4. Create an S3 Batch Operations job to copy the filtered unencrypted objects to the target bucket. The copy operation will automatically encrypt the objects using the bucket's SSE-S3 configuration.","upvote_count":"4"},{"comment_id":"1105853","poster":"pentium75","content":"Selected Answer: C\nData in S3 is queried with Athena, not RDS, thus B and D are out.\n\nA requires a new bucket and loading data into that - Why, since data is already in S3? It says to enable CRR only after loading the data, so existing data won't be replicated anyway. \n\nC uses existing data (less operational overhead compared to loading data into a new bucket) and SSE-E3 (less operational overhead than SSE-KMS).","upvote_count":"7","timestamp":"1703582340.0","comments":[{"upvote_count":"2","timestamp":"1704961740.0","comment_id":"1119509","content":"Most clear explanation. Thanks!","poster":"LoXoL"}]},{"upvote_count":"2","poster":"DHADD003","comments":[{"comment_id":"1105849","poster":"pentium75","upvote_count":"4","comments":[{"upvote_count":"1","comment_id":"1340617","content":"True, but how do you use SSE-S3 with multi-Region keys for CRR?","poster":"LeonSauveterre","timestamp":"1736907720.0"}],"content":"It says \"data requires encryption\", not that it must use same key in both regions.","timestamp":"1703582160.0"}],"content":"Selected Answer: A\nI selected A because SSE-S3 keys are not multi-regional keys. You must use SSE-KMS for the multi-regional keys and then for serverless its Aurora.","comment_id":"1103532","timestamp":"1703264400.0"},{"upvote_count":"3","comment_id":"1102070","content":"Selected Answer: A\nThe most suitable solution with the least operational overhead for the company's requirements is:\n\nOption A:\n\nCreate a new S3 bucket.\nLoad the data into the new S3 bucket.\nUse S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region.\nUse server-side encryption with AWS KMS multi-Region keys (SSE-KMS).\nUse Amazon Athena to query the data.\n\nThis option aligns with the specified requirements of encrypting the data, replicating it to a different AWS Region, and utilizing serverless querying with Amazon Athena. It also minimizes operational overhead by leveraging AWS managed services.","timestamp":"1703118720.0","poster":"djgodzilla"},{"timestamp":"1703063340.0","content":"Selected Answer: A\nA is correct - SSE-KMS is multi region keys and Athena is serverless for analyze\n\nC is incorrect - SSE-S3 is region specific for encryption","poster":"SaurabhTiwari1","comment_id":"1101402","upvote_count":"1"},{"comment_id":"1098011","timestamp":"1702716960.0","content":"Selected Answer: C\nC. is correct after January 2023 because \"Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 are automatically encrypted at no additional cost and with no impact on performance. \"","upvote_count":"4","poster":"kmargaronis"},{"content":"Selected Answer: C\nSSE-S3 is the easiest to use and offers strong encryption, while SSE-C provides more control over your encryption keys (and much more admin overhead)","poster":"chasingsummer","comment_id":"1095583","upvote_count":"2","timestamp":"1702480380.0"},{"poster":"ale_brd_111","timestamp":"1701971040.0","upvote_count":"2","content":"Selected Answer: C\nTherefore, the most appropriate solution to meet the requirements of the serverless application is to load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.\n\nThis solution effectively leverages the existing S3 bucket, S3 Cross-Region Replication for data replication, SSE-S3 for encryption, and Amazon Athena for efficient data querying, enabling the company to analyze existing and new data with minimal management effort and a serverless architecture.","comment_id":"1090504"},{"comment_id":"1090503","upvote_count":"1","timestamp":"1701970980.0","poster":"ale_brd_111","content":"Selected Answer: A\nthe most appropriate solution to meet the requirements of the serverless application is to load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.\n\nThis solution effectively leverages the existing S3 bucket, S3 Cross-Region Replication for data replication, SSE-S3 for encryption, and Amazon Athena for efficient data querying, enabling the company to analyze existing and new data with minimal management effort and a serverless architecture."},{"comment_id":"1088173","timestamp":"1701747960.0","poster":"BhavyaMPatel","content":"Selected Answer: C\nright answer should be c as we need less overhead and if we are using sse-s3\nthen encrypted object and object encrypted with sse-s3 replicate by default for object encrypted with sse-kms Specify which KMS Key to encrypt the objects within the target bucket we need to adapt the KMS Key Policy for the target key,An IAM Role with kms:Decrypt for the source KMS Key and kms:Encrypt for the target KMS Key,we might get KMS throttling errors, in which case you can ask for a Service Quotas increase so less operation is in c option so right answer should be c","upvote_count":"2"},{"timestamp":"1698465240.0","upvote_count":"2","poster":"sofodofo","comment_id":"1055968","content":"Selected Answer: C\nSeems like C - refer to https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html#replication-default-encryption\n\n<How default bucket encryption affects replication>\nWhen you enable default encryption for a replication destination bucket, the following encryption behavior applies:\n- If objects in the source bucket are not encrypted, the replica objects in the destination bucket are encrypted by using the default encryption settings of the destination bucket. As a result, the entity tags (ETags) of the source objects differ from the ETags of the replica objects. If you have applications that use ETags, you must update those applications to account for this difference."},{"upvote_count":"1","poster":"RNess","timestamp":"1697640660.0","comment_id":"1047012","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/#:~:text=To%20encrypt%20an%20existing%20object,data%20using%20server%2Dside%20encryption."},{"upvote_count":"4","content":"Selected Answer: A\n\"To encrypt an existing object using SSE, you replace the object. To encrypt existing objects in place, you can use the Copy Object or Copy Part API. This copies the objects with the same name and encrypts the object data using server-side encryption.\"\nhttps://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/#:~:text=To%20encrypt%20an%20existing%20object,data%20using%20server%2Dside%20encryption.","comment_id":"1041116","timestamp":"1697058720.0","poster":"tom_cruise"},{"content":"Selected Answer: C\nAnswer C I think","comment_id":"1022512","upvote_count":"1","timestamp":"1696179240.0","poster":"DamyanG"},{"timestamp":"1695529920.0","content":"Selected Answer: C\nAthena to query from S3.\nSSE-S3 is least operation overhead than SSE-KMS\nso, C.","upvote_count":"3","comment_id":"1015486","poster":"JKevin778"},{"timestamp":"1695172920.0","upvote_count":"1","comment_id":"1011832","content":"Selected Answer: A\nThe question should be A.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-what-is-isnot-replicated.html#:~:text=Objects%20created%20after%20you%20add%20a%20replication%20configuration.","poster":"hieulam"},{"comment_id":"1006952","timestamp":"1694633940.0","poster":"XCheng","upvote_count":"2","content":"C\nhttps://docs.aws.amazon.com/zh_cn/AmazonS3/latest/userguide/bucket-encryption.html#bucket-encryption-replication"},{"upvote_count":"1","timestamp":"1694087340.0","content":"Selected Answer: C\ni think C is correct answer,i asked chatgpt","comment_id":"1001494","poster":"frankie270299"},{"upvote_count":"5","poster":"TariqKipkemei","timestamp":"1693802880.0","content":"Selected Answer: A\nTechnically both A and C will work, but there is a requirement for 'LEAST operational overhead'.\nMulti-Region keys are a flexible and powerful solution for many common data security scenarios such as this:\nGlobal data management\nBusinesses that operate globally need globally distributed data that is available consistently across AWS Regions. You can create multi-Region keys in all Regions where your data resides, then use the keys as though they were a single-Region key without the latency of a cross-Region call or the cost of re-encrypting data under a different key in each Region.","comment_id":"998211"},{"timestamp":"1693299300.0","content":"If you use S3 managed encryption key , it will apply to newly uploaded objects, not to existing objects. C & D is wrong. which state use existing bucket.\nAthena is to query in S3 so no need of RDS. B is wrong.\nCorrect Answer is A - use KMS key","upvote_count":"5","poster":"Jeyaluxshan","comment_id":"992930"},{"comments":[{"timestamp":"1692174660.0","comment_id":"982331","content":"Please remember that enabling encryption on a bucket does not retroactively encrypt existing objects. You would need to perform a copy operation to re-upload existing objects with encryption enabled if you want to ensure that all objects are encrypted(from chatGPT)","upvote_count":"1","poster":"GC2023"}],"poster":"sohailn","upvote_count":"2","timestamp":"1691729400.0","content":"C is the best answer because encrypted s3 replication is not as simple, \nif you have an unencrypted data or encrypted with sse-s3 it will replicate by default.\nif you have encrypted sse-c client side encryption it will not replicate at all because you need to access the key all the time.\nif you encrypted with sse-kms by default it will not encrypt from source to target by default you ll need to perform addition steps and we cant use KMS-Multi key because aws s3 still consider it independent key, so you must first need to decrypt the data in source bucket and reencrypt in target bucket this solution is 100% true as per stephen udemy instrcutor.","comment_id":"978273"},{"comment_id":"975271","content":"As a side note, if a bucket already exists and you enable replication, you CAN actually now also replicate the existing object in the bucket with \"Amazon S3 Batch Replication\". \nhttps://aws.amazon.com/blogs/aws/new-replicate-existing-objects-with-amazon-s3-batch-replication/#:~:text=S3%20Replication%20is%20a%20fully,or%20to%20multiple%20destination%20buckets.","poster":"Fielies23","upvote_count":"3","timestamp":"1691477820.0"},{"comment_id":"959483","timestamp":"1690028220.0","poster":"RupeC","upvote_count":"3","content":"Selected Answer: C\nA and C are valid, but C has less overhead and the key management is also serverless."},{"content":"Selected Answer: A\nwithout any changes to your client applications","comment_id":"950749","upvote_count":"1","poster":"fuzzycr","timestamp":"1689258600.0"},{"comment_id":"950746","upvote_count":"1","poster":"MNotABot","content":"A\nKMS will give least operational overhead as it needs key rotation in 3 years which is 1 year in S3-SSE","timestamp":"1689258420.0"},{"timestamp":"1689126060.0","comment_id":"949401","upvote_count":"2","content":"Selected Answer: C\nSSE-S3 less operational overhead","poster":"sosda"},{"poster":"jaydesai8","content":"Selected Answer: A\nA, since we can use multi-keys in another region with aws kms keys","upvote_count":"1","comment_id":"946256","timestamp":"1688799540.0"},{"timestamp":"1688037780.0","content":"Selected Answer: C\nThis option minimizes operational overhead because it uses the existing S3 bucket, avoiding the need to create a new one. The data is encrypted using server-side encryption with SSE-S3, which simplifies key management as the encryption keys are managed by Amazon S3. The replication of encrypted objects to another region is handled through S3 Cross-Region Replication (CRR), which automates the process without requiring additional configuration or maintenance.\n\nFor data analysis, Amazon Athena can be used to query the data directly from the S3 bucket, providing a serverless analytics solution. Athena supports querying data in S3 using standard SQL queries, making it a suitable choice for analyzing the data without the need for provisioning and managing database infrastructure.","comment_id":"937962","comments":[{"poster":"MutiverseAgent","timestamp":"1689426240.0","upvote_count":"1","comment_id":"952401","content":"Replication does not work for existing objects, only for new ones."}],"upvote_count":"2","poster":"ajchi1980"},{"upvote_count":"1","timestamp":"1687506600.0","poster":"cookieMr","content":"Selected Answer: A\nIt creates a new S3, allowing for isolation and organization of the data in a serverless solution.\nS3 CRR is used to automatically replicate the encrypted objects to an S3 in another Region, providing data replication and disaster recovery capability.\nSSE-KMS ensures the encryption of data at rest with a secure key management service.\nAthena is a serverless query service that enables analyzing data in S3 using SQL queries without the need for managing infrastructure. It allows for easy analysis of the existing and new data.\n\nOption B suggests using Amazon RDS to query the data. However, Amazon RDS is a managed relational database service and not suitable for analyzing data stored in S3 directly.\n\nOption C suggests using server-side encryption with Amazon S3 managed encryption keys (SSE-S3). While SSE-S3 provides encryption, using SSE-KMS with multi-Region keys offers better control and security for data encryption.\n\nOption D also suggests using Amazon RDS to query the data, which is not the most suitable service for analyzing data in S3.","comment_id":"931331"},{"comment_id":"923846","upvote_count":"1","timestamp":"1686816060.0","content":"Selected Answer: C\nC is correct.\n\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html","poster":"beginnercloud"},{"poster":"northyork","comment_id":"922453","upvote_count":"1","content":"answer c: https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html","timestamp":"1686681660.0"},{"content":"Answer is C: By default, Amazon S3 doesn't replicate objects that are stored at rest using server-side encryption with KMS keys. To replicate encrypted objects, you modify the bucket replication configuration to tell Amazon S3 to replicate these objects. \nhttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c03/view/14/","poster":"northyork","timestamp":"1685613780.0","comment_id":"911944","upvote_count":"1","comments":[{"upvote_count":"1","poster":"northyork","timestamp":"1685613840.0","comment_id":"911946","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-4.html"}]},{"content":"Selected Answer: A\nSince sse-s3 is owned by aws, it will take care of decryption upon replication because you need to re-encrypt the object again in the new region\n\nso, A","timestamp":"1684211340.0","comment_id":"898871","poster":"th3k33n","upvote_count":"2"},{"poster":"pedroso","timestamp":"1684084740.0","upvote_count":"1","content":"Selected Answer: C\nQuestion says \"LEAST operational overhead\" and that is the one that is configured by default.\n\nThe KMS is other level of security but have additional cost and additional operational effort. To upload the object you will need to identify the KMS key you want to use and to access the object you will (again) to identify the KMS key you want to use.\n\nAll Amazon S3 buckets have encryption configured by default, and objects are automatically encrypted by using server-side encryption with Amazon S3 managed keys (SSE-S3). This encryption setting applies to all objects in your Amazon S3 buckets.\n\nIf you need more control over your keys, such as managing key rotation and access policy grants, you can choose to use server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS).","comment_id":"897765"},{"comments":[{"content":"You need to creat a new bucket in a different Region because the replication works for new objects and not existing ones, so that leaves answer A as the only right option.","comment_id":"988286","poster":"D10SJoker","upvote_count":"1","timestamp":"1692792780.0"}],"content":"Folks who chose A, why do you need a new S3 bucket? Please explain","comment_id":"893439","poster":"studynoplay","upvote_count":"2","timestamp":"1683667080.0"},{"timestamp":"1683274620.0","poster":"rushi0611","content":"Selected Answer: C\nAnswer C:\nNo where asked to have the same keys to be used for decryption and also to audit, main aim is to reduce operational overhead- so only with sse-s3 this is possible.","comment_id":"889892","upvote_count":"2"},{"poster":"Nicknameinvalid","timestamp":"1682924340.0","upvote_count":"18","content":"Selected Answer: A\nOption C is not the best solution because it uses server-side encryption with Amazon S3 managed encryption keys (SSE-S3), which is not multi-Region. This means that the encryption key will only be available in the Region where the S3 bucket resides and cannot be replicated to another Region. As a result, if the bucket needs to be replicated to another Region, it will require extra steps to ensure that the encryption keys are also available in the new Region. In contrast, option A uses server-side encryption with AWS KMS multi-Region keys (SSE-KMS), which can be replicated to another Region and makes it easier to manage the encryption keys in a multi-Region setup.","comment_id":"885937"},{"timestamp":"1682503500.0","poster":"chibaniMed","content":"Selected Answer: A\nits A and not C beacause the question dosent mention if data is already encrypdted or not.\nthat's why the solution A can be used for both cases.","upvote_count":"3","comment_id":"881417"},{"timestamp":"1682099280.0","upvote_count":"2","content":"• S3 Replication Encryption Considerations: \n ○ Unencrypted objects and objects encrypted with SSE-S3 are replicated by default \n ○ Objects encrypted with SSE-C (customer provided key) are never replicated \n ○ For objects encrypted with SSE-KMS, you need to enable the option cause by default is not replicated you should enable and decrypt before send to target and encrypt in the target:\n § Specify which KMS Key to encrypt the objects within the target bucket \n § Adapt the KMS Key Policy for the target key \n § An IAM Role with kms:Decrypt for the source KMS Key and kms:Encrypt for the target KMS Key \n § You might get KMS throttling errors, in which case you can ask for a Service Quotas increase \n ○ You can use multi-region AWS KMS Keys, but they are currently treated as independent keys by Amazon S3 (the object will still be decrypted and then encrypted) \nso it's C","poster":"KarimaMaf","comment_id":"876742"},{"timestamp":"1681932060.0","comment_id":"875041","content":"Selected Answer: C\nexisting bucket and SSE-S3","poster":"darn","upvote_count":"1"},{"content":"Crap man! I hate these at 51/49 % where's the truth man? I'm just here to study... lol :)","timestamp":"1681824660.0","comments":[{"upvote_count":"3","content":"the thoughts of my mind as well:)","poster":"cokutan","comment_id":"905592","timestamp":"1684912620.0"},{"timestamp":"1683667020.0","content":"So many controversial questions","poster":"studynoplay","upvote_count":"3","comment_id":"893438"}],"poster":"kels1","comment_id":"873666","upvote_count":"13"},{"comments":[{"comment_id":"907499","timestamp":"1685118480.0","upvote_count":"1","content":"You can replicate existing bucket with all the objects, you do not need to create a new one for that challenge","poster":"ErnShm"}],"upvote_count":"3","comment_id":"873053","timestamp":"1681761360.0","content":"This is a tricky one. C would have less operational overhead, but CRR doesn't encrypt already existing objects, so you'd need to create a new bucket, upload the existing data to the new bucket, and then encrypt the bucket.","poster":"Robrobtutu"},{"content":"A key issue in this question is the fact that the existing data on S3 requires encryption. Meaning it is currently unencrypted. \nSo which is easier? to create a new S3 and move the data or go through the process of encrypting already existing unencrypted data.\nI will just create new S3.","comment_id":"872237","timestamp":"1681689780.0","upvote_count":"1","poster":"C_M_M"},{"poster":"Musti35","comment_id":"871500","content":"Selected Answer: A\nSSE-S3 only encrypt new data but you can encrpyt existing and new data with SSE-KMS","upvote_count":"2","timestamp":"1681624260.0"},{"content":"Selected Answer: C\nOption C provides a correct solution that meets the given requirements with the least operational overhead. Data is loaded into the existing S3 bucket, and the S3 Cross-Region Replication (CRR) feature is used to replicate the data to an S3 bucket in a different AWS region. The data is encrypted server-side using Amazon S3 managed encryption keys (SSE-S3). Amazon Athena can be used to query the data.","timestamp":"1681361520.0","comment_id":"869074","poster":"nazirlia","upvote_count":"3"},{"timestamp":"1681041900.0","comment_id":"865472","upvote_count":"2","poster":"channn","content":"Selected Answer: C\nchoose C as no need to create a new S3."},{"poster":"TECHNOWARRIOR","comment_id":"865460","upvote_count":"2","content":"Solution A and C are both workable options, while Solution B is not a feasible option for the requirements given. Solution A and C both utilize S3 Cross-Region Replication and server-side encryption with Amazon S3, and both use Amazon Athena for querying the data. However, Solution A involves creating a new S3 bucket, which requires extra effort and network costs, while Solution C uses the existing S3 bucket. On the other hand, Solution C uses SSE-S3 encryption, which is limited to the region where the data resides, while Solution A uses AWS KMS multi-region keys for encryption, which ensures secure data handling across regions. Therefore, Solution A may be a better option if disaster recovery across regions is a priority.","timestamp":"1681040220.0"},{"upvote_count":"1","timestamp":"1680251040.0","comment_id":"856783","poster":"nauman001","content":"Read the question carefully: I think the only difference is Bucket Presence in ....\nA: There is no bucket create the bucket and upload the data. and in \nC: Bucket already created and customer upload the data into existing bucket. \nIn both ways New data is uploaded which is encrypted and CRR applies on it."},{"content":"Selected Answer: A\nSelected A as for S3 CRR only new objects are replicated and this means additional overhead using Answer:C.","poster":"kraken21","timestamp":"1680114720.0","comment_id":"854834","upvote_count":"2"},{"poster":"Erbug","upvote_count":"1","content":"Selected Answer: C\nwhy do we need SSE-S3 and not SSE-KMS for this solution? What are the differences between them?","timestamp":"1679519880.0","comment_id":"847534"},{"upvote_count":"3","timestamp":"1679316180.0","comment_id":"844869","poster":"asoli","content":"Selected Answer: A\nit says existing objects and new objects. When you enable cross-region replication on an s3 bucket, it only replicates the new objects and you have to take care of the existing objects to copy them to the new bucket. which has more operational overhead."},{"poster":"ChandraPrabu","content":"Selected Answer: A\nExisting objects in the source bucket will not be replicated to the destination bucket unless you manually copy them to the destination bucket or use another method such as Amazon S3 inventory and Amazon S3 batch operations.\n\nIn that case option A makes sense to copy the exiting data to new bucket & make them replicated in destination bucket.","timestamp":"1678930980.0","comment_id":"840479","upvote_count":"4"},{"comment_id":"835433","poster":"rdss11","timestamp":"1678482240.0","upvote_count":"1","content":"Selected Answer: C\nS3 buckets are ecrypted by SSE-S3 by default"},{"timestamp":"1678431060.0","poster":"[Removed]","upvote_count":"2","content":"i would go with A\ni dont understand what loading data into existing s3 means","comment_id":"834694"},{"poster":"athiha","upvote_count":"7","timestamp":"1677920760.0","comment_id":"828768","content":"Selected Answer: A\nThe only reason why I choose option A is that the question states \"Serverless Solutions needs to analyze existing and new data\". And when you turn on the Cross-Region Replication (CRR), the existing data will not be replicated automatically. It only replicates the new data added to the source bucket from the point you turn on CRR. So it would make more sense to have a new bucket to load the data and then turn on the CRR."},{"comments":[{"poster":"KZM","timestamp":"1677633780.0","comment_id":"825453","upvote_count":"3","content":"A,\nSorry, I wrongly clicked on C. I mean option A, AWS KMS multi-Region kays."}],"comment_id":"825397","poster":"KZM","content":"Selected Answer: C\nMulti-Regions Key in AWS KMS\nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html","timestamp":"1677629100.0","upvote_count":"1"},{"comment_id":"822503","upvote_count":"1","content":"Selected Answer: C\nIt says that they already have a S3 bucket, option A indicates to create a new one. Why would they create a new bucket when they already have one? option c is better","comments":[{"upvote_count":"1","content":"Cross region replication is not retro-active.","poster":"CapJackSparrow","timestamp":"1678734660.0","comment_id":"838203"}],"timestamp":"1677420900.0","poster":"Ja13"},{"upvote_count":"2","timestamp":"1677144600.0","content":"Selected Answer: A\nA is the best solution that meets the company's requirements with the least operational overhead.\n\nA recommends creating a new S3 bucket, loading the data into the new S3 bucket, using S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another region, using server-side encryption with AWS KMS multi-Region keys (SSE-KMS), and using Amazon Athena to query the data.","comment_id":"819021","poster":"LuckyAro"},{"upvote_count":"2","content":"Selected Answer: A\nIt doesn't state whether the Existing S3 bucket might have uncrypted data - encryption will apply new data received after the applying of encryption on the new bucket.","timestamp":"1677117780.0","poster":"bdp123","comment_id":"818664"},{"content":"Selected Answer: C\nI vote C , key word existing data","timestamp":"1675695300.0","poster":"joric","comment_id":"799867","upvote_count":"1"},{"upvote_count":"1","timestamp":"1675695120.0","content":"The serverless solution needs to analyze existing and new data by using SL. \n(SQL) there is a misstype.","comment_id":"799864","poster":"joric"},{"poster":"KZM","timestamp":"1675551240.0","comment_id":"798442","content":"Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance.\n\nServer-side encryption with AWS KMS keys (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service.\n\nIf we think about the LEAST operational overhead, SSE-S3 is more reasonable, I think.","upvote_count":"2"},{"upvote_count":"1","content":"I mean I askesd whats SL? is it a typing mistake? admins unlock my comment please.","comment_id":"796559","timestamp":"1675384140.0","poster":"joric","comments":[{"comment_id":"808067","timestamp":"1676347860.0","upvote_count":"2","content":"Its a TYPO for SQL","poster":"Joxtat"}]},{"upvote_count":"1","comments":[{"poster":"Lemmij","comment_id":"803842","upvote_count":"1","timestamp":"1675990200.0","content":"Machine Language"}],"comment_id":"796558","timestamp":"1675384080.0","poster":"joric","content":"I asked what is ML. admins unlock my comment please."},{"comment_id":"791501","poster":"kerl","timestamp":"1674984240.0","content":"Answer is C, i have manually tested using SS3-S3 for encryption and test the relication and it work. For existing data, before u save your replication rules, S3 will asked if u want to replicate existing data to a new S3 bucket using Batch Operation Job.","upvote_count":"6"},{"content":"Selected Answer: C\n\nUnencrypted objects and objects encrypted with SSE-S3 are replicated by default","timestamp":"1674903960.0","comment_id":"790540","poster":"ASODAD","upvote_count":"1"},{"timestamp":"1674876300.0","comment_id":"790228","poster":"joric","content":"\" analyze existing and new data by using SL.\" what is SL ? never heard before","upvote_count":"2"},{"poster":"micola","comment_id":"781124","upvote_count":"1","timestamp":"1674131520.0","content":"Selected Answer: C\nBy default, Amazon S3 doesn't replicate objects that are stored at rest using server-side encryption with customer managed keys stored in AWS KMS. \n...\nYou can use multi-Region AWS KMS keys in Amazon S3. However, Amazon S3 currently treats multi-Region keys as though they were single-Region keys, and does not use the multi-Region features of the key. For more information, see Using multi-Region keys in AWS Key Management Service Developer Guide.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html\n\nso between A and C I'm choosing C, as it has less overhead"},{"upvote_count":"3","timestamp":"1674102660.0","poster":"simplimarvelous","comment_id":"780726","content":"So would have to go with answer A this with this question.\n- with no specific information telling that existing data is encrypted a new bucket should be created so that new data will be encrypted. By default existing encrypted at rest object are not replicated without having to having to do additional work to tell S3 to replicate it.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-4.html\n\nThe documentation does say the following:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html\nChanges to note before enabling default encryption SSE-S3\nAfter you enable default encryption for a bucket, the following encryption behavior applies:\n\n\"There is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.\""},{"upvote_count":"1","content":"Selected Answer: C\nThe solution that meets these requirements with the least operational overhead is option C.\n\nBy loading the data into the existing S3 bucket, it eliminates the need to create a new S3 bucket and move data to it. S3 Cross-Region Replication (CRR) can then be used to replicate encrypted objects to an S3 bucket in another Region. Server-side encryption with Amazon S3 managed encryption keys (SSE-S3) can be used to encrypt the data. Finally, Amazon Athena can be used to query the data which is a serverless, interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.","timestamp":"1673984280.0","comment_id":"779279","poster":"remand"},{"content":"Selected Answer: C\nAthena is Used to analyse and query petabytes of data or build application on S3, on-premises and other.\nServerless \nOnly pay when run","comment_id":"777899","upvote_count":"1","timestamp":"1673884380.0","poster":"Ello2023"},{"poster":"lfrad","content":"Selected Answer: A\nMy question is about CRR: since we need to replicate all data both new and old, shouldn't the existing bucket option be out of the window as CRR does not replicate already present data but only new PUTs ? \n\nIn this case the question would trick you into answering C as SSE-S3 is less operational overhead over SSE-KMS, BUT it not a feasible architecture as the CRR choice (instead of say batch replication or s3 sync command) would not allow existing data to be transferred.\n\nSo SSE-S3 better, but C is not feasible... so A ?","upvote_count":"4","comment_id":"770423","timestamp":"1673271000.0"},{"upvote_count":"2","comment_id":"764784","timestamp":"1672758300.0","content":"Selected Answer: A\nI vote for A","poster":"aba2s"},{"upvote_count":"2","content":"Selected Answer: C\nLEAST operational overhead","comment_id":"763110","timestamp":"1672562040.0","poster":"Zerotn3"},{"comment_id":"759518","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: C\nThe solution that meets the requirements with the least operational overhead is Option C:\n\n* Load the data into the existing S3 bucket.\n* Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region.\n* Use server-side encryption with Amazon S3-managed encryption keys (SSE-S3).\n* Use Amazon Athena to query the data.\n\nOption C avoids the need to create a new S3 bucket, which reduces operational overhead. Additionally, using Amazon Athena to query the data allows for efficient querying of the data stored in the S3 bucket without the need to set up and maintain a separate database instance.","upvote_count":"2","timestamp":"1672213500.0"},{"timestamp":"1672056780.0","upvote_count":"3","content":"Can anyone tell me a solid answer on this lol. The voting is way too close as 50/50","poster":"waiyiu9981","comments":[{"comment_id":"757788","content":"I am goingvwith SSE-S3.","upvote_count":"1","timestamp":"1672084080.0","poster":"techhb"},{"upvote_count":"2","comment_id":"759529","timestamp":"1672213800.0","poster":"Buruguduystunstugudunstuy","content":"The correct answer is Option C. To meet the requirements with the least operational overhead, the company should load the data into the existing S3 bucket and use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. The company should use server-side encryption with Amazon S3-managed encryption keys (SSE-S3) and use Amazon Athena to query the data.\n\nUsing S3 Cross-Region Replication (CRR) will allow the company to replicate the encrypted data to a different AWS Region automatically, which will provide the company with a disaster recovery solution. Using server-side encryption with Amazon S3 managed encryption keys (SSE-S3) will allow the company to encrypt the data without having to manage the keys.\n\nAmazon Athena is a fully managed, serverless query service that allows users to analyze data stored in S3 using SQL. By using Athena, the company can analyze the data without having to provision and manage any infrastructure."}],"comment_id":"757384"},{"upvote_count":"2","content":"Selected Answer: A\nEnabling CRR requires versioning to be enabled on the bucket so it is better option to create a new bucket as it may have undesired effect on existing bucket.\nAthena is serverless with least operational overhead for Analytics that can directly read data from S3.","comment_id":"748642","timestamp":"1671346200.0","poster":"career360guru"},{"poster":"Certified101","comment_id":"745377","upvote_count":"3","timestamp":"1671043500.0","content":"Selected Answer: A\n\"The serverless solution needs to analyze existing and new data by using SL\"\n\nSo this means there is unencrypted data in the existing s3 bucket due to the encryption solution not yet in place. We will need a new S3 bucked"},{"content":"Selected Answer: A\nA. Agree with others that it’s reasonable to assume current setup has unencrypted data. Must create new bucket to set default encryption on all items going forward.","poster":"lapaki","upvote_count":"3","comment_id":"740772","timestamp":"1670659620.0","comments":[{"timestamp":"1670738520.0","content":"No mention of existing data being encrypted !!!","poster":"kmliuy73","comment_id":"741451","upvote_count":"2"}]},{"upvote_count":"3","content":"Selected Answer: A\nit's not C because it is not achieving the goal of encrypting all objects, hence A","poster":"Amaris","timestamp":"1670170320.0","comment_id":"735226"},{"timestamp":"1670124360.0","upvote_count":"6","comment_id":"734819","content":"Selected Answer: A\nAthena being serverless and offers less operational overhead so ruling out B&D.\nC: SSE-S3 has higher operational overhead but it doesnt achieve the GOAL which is to encrypt all objects.\nHence the answer is A:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html\n\n\"Changes to note before enabling default encryption\n\nAfter you enable default encryption for a bucket, the following encryption behavior applies:\n\nThere is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.\"","poster":"Wajif"},{"poster":"JayanKuruwita","content":"Selected Answer: C\nC would be hard if we have to replicate set of existing objects, because we need AWS support assistance for that. But this is about new objects that we are going to upload to S3. Thinking about setting up permissions for KMS key with SSE-KMS mode. I would go with C.","comment_id":"727986","timestamp":"1669533420.0","upvote_count":"1"},{"poster":"justtry","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html\nThere is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.That's the reason why new bucket should be created.","timestamp":"1669448040.0","comment_id":"727371","upvote_count":"3"},{"poster":"Wpcorgan","content":"A is correct for me","comment_id":"724305","timestamp":"1669119300.0","upvote_count":"2"},{"poster":"Onimole","timestamp":"1668787620.0","upvote_count":"1","comment_id":"721397","content":"C appears to be right"},{"content":"It is A:\nhttps://aws.amazon.com/blogs/storage/replicating-existing-objects-between-s3-buckets/","poster":"Ohnet","comment_id":"718650","timestamp":"1668505440.0","upvote_count":"2"},{"content":"The issue with Option C) is the existing data before setting up CRR will not be replicated and that is why option A) looks good.\nThe question does not talk of employing S3 Batch Replication before CRR is setup.","comment_id":"715917","upvote_count":"4","poster":"study_aws1","timestamp":"1668159180.0"},{"upvote_count":"3","comment_id":"712033","content":"Selected Answer: A\nA is the solution with least operational overhead","poster":"backbencher2022","timestamp":"1667687280.0"},{"poster":"masetromain","timestamp":"1667381280.0","content":"Selected Answer: A\nfor me the answer is A:\n\nThe Existing S3 bucket might have uncrypted data - encryption will apply new data received after the applying of encryption on the new bucket.","upvote_count":"5","comment_id":"709730"},{"content":"The question should read \"SQL\" not \"SL\"","comment_id":"705699","upvote_count":"4","poster":"UWSFish","timestamp":"1666885380.0"},{"upvote_count":"2","timestamp":"1666654740.0","poster":"dave9994","content":"Selected Answer: C\nC has fewer ops. overhead. and the question says the customer has an existing S3 bucket. Then there is no need to create a new S3 bucket.","comment_id":"703420"}],"timestamp":"2022-10-20 09:18:00"}],"exam":{"id":31,"isImplemented":true,"provider":"Amazon","isMCOnly":true,"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false},"currentPage":12},"__N_SSP":true}