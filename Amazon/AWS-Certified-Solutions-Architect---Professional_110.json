{"pageProps":{"questions":[{"id":"iJkB9yHrWCmP5srcEnqy","answers_community":["B (100%)"],"question_id":546,"answer_ET":"B","exam_id":32,"question_text":"A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB. The\nDevelopers account resides in a dedicated organizational unit (OU). The Solutions Architect has implemented the following SCP on the Developers account:\n//IMG//\n\nWhen this policy is deployed, IAM users in the Developers account are still able to use AWS services that are not listed in the policy.\nWhat should the Solutions Architect do to eliminate the Developers' ability to use services outside the scope of this policy?","timestamp":"2021-03-13 16:31:00","topic":"1","answer_images":[],"question_images":["https://www.examtopics.com/assets/media/exam-media/04241/0037900001.png"],"answer_description":"","isMC":true,"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/46899-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1615649460,"choices":{"A":"Create an explicit deny statement for each AWS service that should be constrained.","B":"Remove the FullAWSAccess SCP from the Developer account's OU.","D":"Add an explicit deny statement using a wildcard to the end of the SCP.","C":"Modify the FullAWSAccess SCP to explicitly deny all services."},"discussion":[{"upvote_count":"19","poster":"Waiweng","timestamp":"1634678340.0","content":"B is correct","comment_id":"350115"},{"content":"Answer is A - You cannot remove the FullAWSAccess SCP that is inherited from root. Test it and see.","comments":[{"poster":"joe16","content":"Yes, you can.(Ans - B)\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist\n\"To use SCPs as an allow list, you must replace the AWS managed FullAWSAccess SCP with an SCP that explicitly permits only those services and actions that you want to allow. By removing the default FullAWSAccess SCP, all actions for all services are now implicitly denied. Your custom SCP then overrides the implicit Deny with an explicit Allow for only those actions that you want to permit.\"","timestamp":"1636246920.0","comment_id":"456073","upvote_count":"11"},{"comment_id":"423623","content":"Answer is A, because as soon as an SCP was created, the FullAWSAccess SCP was already overruled (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist) and (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist), because Explicit Deny > Explicit Allow > Implicit Deny > Implicit Allow, the only way to overcome Explicit Allow is to add Explicit Deny statements. Answers C and D would work too good, of course everything would be blocked !","poster":"tekkart","comments":[{"upvote_count":"1","comment_id":"1088327","content":"This link says that \"The organization administrator can detach the FullAWSAccess policy and attach this one instead.\". So the FullAWSAccess policy needs to be detached explicitly.","poster":"sumaju","timestamp":"1701769380.0"}],"upvote_count":"4","timestamp":"1636086720.0"}],"poster":"student2020","comment_id":"415682","upvote_count":"8","timestamp":"1634960820.0"},{"poster":"3a632a3","timestamp":"1705849800.0","upvote_count":"1","comment_id":"1127899","content":"Selected Answer: B\nSCP evaluation starts with an implicit Deny (soft deny). The default SCP allows full access so removing this policy causes any service to be implicitly denied unless an allow statement is used. Explicit Deny should be used for organizational rules that must be strictly enforced (hard deny). An example would be to deny a service that doesn't meet a specific compliance requirement to be used in regulated accounts."},{"content":"Answer is B.\n\n\"To use SCPs as an allow list, you must replace the AWS managed FullAWSAccess SCP with an SCP that explicitly permits only those services and actions that you want to allow. By removing the default FullAWSAccess SCP, all actions for all services are now implicitly denied. Your custom SCP then overrides the implicit Deny with an explicit Allow for only those actions that you want to permit. For a permission to be enabled for a specified account, every SCP from the root through each OU in the direct path to the account, and even attached to the account itself, must allow that permission.\"","upvote_count":"1","poster":"TravelKo","comment_id":"991775","timestamp":"1693193400.0"},{"upvote_count":"1","comments":[{"upvote_count":"1","content":"** I meant D is wrong (typo, not work)","timestamp":"1677204120.0","poster":"God_Is_Love","comment_id":"820021"}],"comment_id":"820018","timestamp":"1677204060.0","content":"Logical answer : C is ruining the good policy and not efficient. A is ridiculously inefficient, how many services need to be denied, thousands ? D is work because SCPs does not work like that. SCPs work like inverted Tree hierarchy.A deny list strategy makes use of the FullAWSAccess SCP that is attached by default to every OU and account. This SCP overrides the default implicit deny, and explicitly allows all permissions to flow down from the root to every account, unless you explicitly deny a permission with an additional SCP that you create and attach to the appropriate OU or account. So B should be correct\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html","poster":"God_Is_Love"},{"content":"100% B","upvote_count":"1","comment_id":"713303","timestamp":"1667852280.0","poster":"mrgreatness"},{"comment_id":"692252","poster":"wassb","content":"This question doesnt make sense AT ALL. \n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"ec2:*\",\n \"cloudwatch:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n}\nAn allow list policy might look like the following example, which enables account users to perform operations for Amazon Elastic Compute Cloud (Amazon EC2) and Amazon CloudWatch, ****but no other service****. \n + The FullAWSAccess SCP doesnt need to be deleted, the fact defining a new SCP is enough..\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist","timestamp":"1665505380.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"626224","poster":"aandc","content":"B\nTo support this, AWS Organizations attaches an AWS managed SCP named FullAWSAccess to every root and OU when it's created. This policy allows all services and actions. It's always available for you to attach or detach from the entities in your organization as needed. Because the policy is an AWS managed SCP, you can't modify or delete it.","timestamp":"1656778260.0"},{"poster":"tkanmani76","timestamp":"1642595640.0","upvote_count":"1","content":"B is correct - An allow list strategy has you remove the FullAWSAccess SCP that is attached by default to every OU and account. This means that no APIs are permitted anywhere unless you explicitly allow them.","comment_id":"527484"},{"upvote_count":"1","content":"it should be B","poster":"AzureDP900","timestamp":"1638752880.0","comment_id":"494784"},{"timestamp":"1637766180.0","content":"Ans is B","upvote_count":"1","poster":"pcops","comment_id":"486053"},{"upvote_count":"2","comment_id":"441794","timestamp":"1636101840.0","content":"B\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html","poster":"student22"},{"comment_id":"329315","poster":"RedKane","content":"Ignore the messages below - it looks like access has to be granted at each level : root, any intermediate OUs and ACCOUNT so removing FullAWSAccess SCP from any of the nodes in the hierarchy will do the job.","upvote_count":"2","timestamp":"1633831980.0"},{"comment_id":"329309","upvote_count":"1","poster":"RedKane","content":"To add to the previous post - each higher OU higher in the hierarchy, including organization root will also have FullAWSAccess SCP attached and each of those SCPs will be inherited by each account below in the hierarchy. So each account inherits multiple copies of FullAWSAccess SCP. In order to get rid of it one would need to remove FullAWSAccess SCP from every OU (higher in the hierarchy) and the root as well as the ACCOUNT itself.","timestamp":"1633775820.0"},{"comment_id":"329304","poster":"RedKane","content":"FullAWSAccess SCP is attached automatically by default not only to each OU but also to each account individually so removing FullAWSAccess SCP from Developers-OU will change nothing as the one attached directly to the Developers-ACCOUNT will still remain. That would only leave option A as valid although I'm not sure if the author of this question considered the behavior I described. Also in real scenarios one would rather attach SCP with DENY's and leave FullAWSAccess SCP untouched.","upvote_count":"1","timestamp":"1633496760.0"},{"timestamp":"1633182060.0","comment_id":"321754","content":"it should B","upvote_count":"1","poster":"alisyech"},{"timestamp":"1632663000.0","content":"answ B","comment_id":"314992","poster":"didek1986","upvote_count":"1"},{"comment_id":"314378","timestamp":"1632444600.0","content":"B. allow list \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_denylist","poster":"beber3564","upvote_count":"2"},{"content":"I Support D","poster":"M_Asep","comment_id":"311938","upvote_count":"1","timestamp":"1632416040.0"},{"poster":"nitinz","comments":[{"content":"D cannot be the answer. It will also override the allowed statements.\n\nA request results in an explicit deny if an applicable policy includes a Deny statement. If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html","timestamp":"1632707880.0","comments":[{"upvote_count":"4","comment_id":"318406","timestamp":"1632820260.0","content":"You are absolutely correct. Thanks for the link I change my answer to B.","poster":"nitinz"}],"comment_id":"316698","upvote_count":"4","poster":"heyheyhei"}],"content":"D is correct answer.","upvote_count":"1","comment_id":"309805","timestamp":"1632385140.0"}]},{"id":"k4X3No677iVyz8JpZGtS","url":"https://www.examtopics.com/discussions/amazon/view/28227-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"timestamp":"1634526060.0","content":"B\nStunning how few people can read. Opsworks isn't even mentioned anywhere in the question. This question has nothing to do with Opsworks. It's cloudformation + Chef Automate (which indicates a Chef Serve, NOT a full Opsworks stack). Chef isn't an aws tools and doesn't require Opsworks to work, and it supports Blue/Green, Phoenix, and Canary deployments (https://blog.chef.io/watch-chef-aws-your-path-to-devops). The answer is B.","poster":"Trap_D0_r","upvote_count":"26","comment_id":"282976","comments":[{"timestamp":"1648997460.0","comment_id":"580337","upvote_count":"5","poster":"sashsz","content":"Stunning how you are judging the other people and you yourself didn't provide any reason behind your answer."}]},{"upvote_count":"12","content":"A. Blue/green deployment with Opsworks for Chef Automate, using separate stacks for each environment.","poster":"Nemer","timestamp":"1632240360.0","comment_id":"156199","comments":[{"upvote_count":"2","comment_id":"200168","poster":"Gmail78","content":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html","timestamp":"1632594780.0"},{"content":"https://aws.amazon.com/opsworks/chefautomate/","comment_id":"563836","timestamp":"1646812200.0","upvote_count":"1","poster":"Alexey79"}]},{"upvote_count":"1","poster":"3a632a3","content":"Selected Answer: A\nA is the answer\nCanary and Blue/Green are very similar deployment styles. I've been doing DevOps since before it was a term. You can learn about canary in this link where they even use blue/green to show the environments: https://martinfowler.com/bliki/CanaryRelease.html?ref=wellarchitected\n\nThe difference here is with canary you need to be able to control traffic, BUT there isn't a method listed in the infrastructure to do this such as an ALB or route53. Since you can't shape traffic the next best thing is blue/green.","timestamp":"1705851840.0","comment_id":"1127918"},{"upvote_count":"1","poster":"Jesuisleon","timestamp":"1684627500.0","comment_id":"902818","content":"Selected Answer: A\nA.\nThe problem is from Apache tomcat server, it's from your environment, so you can't use canary method to detect. canary method is usually for testing your changes or features by putting one new application into your environment with your old applications, if new application works as normal, then you gradually replace all your old applications with new application."},{"content":"Selected Answer: A\nBlue/green will ensure the upgrade is tested before launch. - better reliability = A is right\nCanary will track the change, but without doing anything else will not make it more reliable. = b is not the better answer.","timestamp":"1659881220.0","poster":"nexus2020","comment_id":"643746","upvote_count":"3"},{"upvote_count":"1","timestamp":"1654736940.0","poster":"Anhdd","comments":[{"upvote_count":"1","timestamp":"1654737000.0","poster":"Anhdd","comment_id":"613565","content":"It's say that with the blue/green deployment, the cons is: \"Cost is a drawback to blue-green deployments. Replicating a production environment can be complex and expensive, especially when working with microservices. Quality assurance and user acceptance testing may not identify all of the anomalies or regressions either, and so shifting all user traffic at once can present risks. An outage or issue could also have a wide-scale business impact before a rollback is triggered, and depending on the implementation, in-flight user transactions may be lost when the shift in traffic is made.\""}],"comment_id":"613563","content":"Selected Answer: B\nB good"},{"comment_id":"605045","poster":"user0001","timestamp":"1653174480.0","upvote_count":"2","content":"it is A in this case, it would be B if the question is about cost"},{"content":"Selected Answer: A\nDon't let the top comment fool you.. the answer is A. OpsWorks has 3 modes: Puppet Enterprise, Chef Automate, and OpsWorks..\n\nFor the exam you will default for OpsWorks if you see those keywords.","upvote_count":"3","comment_id":"552840","poster":"futen0326","timestamp":"1645446180.0"},{"content":"A is the answer.\nB: is not suitable in this scenario as they are facing issues with upgrading Apache TOMCAT server. You cannot do Canary deployments for server infrastructure. Canary is best suited for toggle features/releases that can we toggled on /off. Since this is TOMCAT, you cannot opt in or opt out.","poster":"AMKazi","upvote_count":"6","comment_id":"531376","timestamp":"1643035440.0"},{"comment_id":"529646","timestamp":"1642832280.0","upvote_count":"1","poster":"GeniusMikeLiu","content":"after read so many comment, I still confused. what the main diffenrent between Blue/green and canary deloyment?"},{"timestamp":"1642489980.0","comment_id":"526366","content":"A is reliable. nothing is mentioned about costs.","poster":"cannottellname","upvote_count":"2"},{"poster":"GeniusMikeLiu","comment_id":"515368","upvote_count":"3","content":"Selected Answer: A\nquesion is care about 'reliable', so A is best then B. Blue/Green deployment can roll back to old version if something goes wrong.","timestamp":"1641178560.0"},{"poster":"cldy","timestamp":"1640870940.0","content":"B is correct.","comment_id":"513403","upvote_count":"1"},{"comment_id":"500998","poster":"Juks","content":"Blue/Green deployment is more reliable as it will never cause an outage. Using Canary you are still causing outage for a set of users.","comments":[{"timestamp":"1654737120.0","comment_id":"613567","content":"Blue/green deployment: An outage or issue could also have a wide-scale business impact before a rollback is triggered, and depending on the implementation, in-flight user transactions may be lost when the shift in traffic is made","upvote_count":"1","poster":"Anhdd"}],"upvote_count":"5","timestamp":"1639448400.0"},{"comment_id":"496448","upvote_count":"7","timestamp":"1638927300.0","poster":"tkanmani76","content":"A - The answer is Blue/Green. The question requires a \"Reliable\" soluton - With Canary you would still be routing to a small subset of user base who would be impacted if there is an issue with upgrade. With Blue/Green you would test in one environment and once it works fine you could swing over - that way there will be no customer impact or production issue."},{"poster":"sappers","content":"Its B Canary - yep stunning that so many head-in-cloud Architects dont understand DevOps - Think if YOU were responsible for \"service outages occur(ing) OFTEN as a result of unanticipated issues\" e.g. a known intermittently flawed App - would you really do Blue/Green (no mention in Q of testing) ? then just swap over ? Good luck w that :@)","comment_id":"492679","timestamp":"1638462960.0","upvote_count":"2"},{"comment_id":"492595","upvote_count":"1","poster":"sappers","timestamp":"1638456600.0","content":"Its B Canary - yep stunning that so many head-in-cloud Architects dont understand DevOps - Think if YOU were responsible for \"service outages occur(ing) OFTEN as a result of unanticipated issues\" e.g. a known intermittently flawed App - would you really do Blue/Green (no mention in Q of testing) ? then just swap over ? Good lcuck w that :@)"},{"poster":"denccc","content":"Issue: \"The Engineering team reports there are frequent service disruptions due to unexpected errors when updating the application of the Apache Tomcat server.\" This can be prevented by using B/G deployments. Only when everything is fine a switch will happen. So will go with A.","upvote_count":"4","timestamp":"1635887460.0","comment_id":"430136"},{"comments":[{"comments":[{"upvote_count":"1","timestamp":"1636230660.0","content":"and this -\nhttps://circleci.com/blog/canary-vs-blue-green-downtime/","comment_id":"456088","poster":"joe16"}],"comment_id":"456086","poster":"joe16","timestamp":"1635916680.0","upvote_count":"1","content":"A and B are similar but B more suited in this scenario. Please read this - \nhttps://martinfowler.com/bliki/CanaryRelease.html"}],"timestamp":"1635850740.0","comment_id":"427944","poster":"Cotter","upvote_count":"3","content":"A or B I am very confuse!"},{"comment_id":"411858","poster":"WhyIronMan","upvote_count":"6","content":"I'll go with A\nDoesn't matter if is Cheff, Puppet, OpsView, Opsworks, Ansible, BeansTalk or other Hell...\n\nBlue/Green and Canary are Deployment Models.\nThe question is not saying to save cost or \"be cost effective\" so you can easily create a separate environment, do the updates, do the tests do anything you want without touch in the live environment (Blue) and if is everything ok you can change to Blue to Green (new Blue)\n\nI could be B, but A is a better answer.","timestamp":"1635760140.0"},{"comment_id":"397852","timestamp":"1635310320.0","poster":"Pb55","content":"Chef NOT Opworks therefore B.","upvote_count":"2"},{"content":"Should be blue/green deploy. Reason is it mentioned code is well tested but deploy often has accident. So Blue/Green deploy allow people do some test after deploy done but before it goes alive. I believe this is the right way.","timestamp":"1635185940.0","upvote_count":"4","poster":"oscargee","comment_id":"362547"},{"timestamp":"1634938860.0","comment_id":"350118","poster":"Waiweng","upvote_count":"4","content":"it's A"},{"comment_id":"292511","poster":"Kian1","content":"Which solution will increase the reliability of all releases?????? I really do not know which one is more reliable between Blue/Green and Canary. if someone has a good reference please share with us. but If I have to choose, then it is \"A\" Blue/Green as it is more frequently used with cloudFormation than with Canary.","upvote_count":"1","timestamp":"1634642580.0"},{"timestamp":"1634377560.0","comments":[{"content":"great note from Trap_D0_r, in here there is no OpsWorks, I change my answer to B","upvote_count":"3","comment_id":"286105","timestamp":"1634583840.0","poster":"Ebi"}],"upvote_count":"5","poster":"Ebi","comment_id":"275202","content":"Answer is A\nOpsWorks does not support canary deployment"},{"poster":"petebear55","upvote_count":"1","comment_id":"253659","comments":[{"content":"Sounds great but if you can't do it, you can't do it! No support for canary here: https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html\nActually, if you read carefully you'll notice that the unexpected errors happen when updating the application, not afterwards. With blue/green you update, test, then release which solves this issue.","upvote_count":"4","poster":"ncloud1","comments":[{"timestamp":"1634819640.0","content":"OpsWorks is not mentioned. Chef does support canary.","poster":"sarah_t","upvote_count":"2","comment_id":"333945"}],"timestamp":"1633959660.0","comment_id":"262227"}],"timestamp":"1633936320.0","content":"Think guys !!! ? \"Which solution will increase the reliability of all releases?\" If errors are happening then blue/green deployments will lead to an oil slick of errors!! Canary would allow you to stick the cork back in the bottle ... And so lessen impact and thus the \"RELIABILITY\" of all releases"},{"content":"The answer is B chaps !!! Well done kanavPeer!! .... Remember lads the idea of thios test is to separate the wheat from the chaff. Answer A is too easy and would typically be grasped by an ASSOCIATE level candidate. But this is the Professional study!! .. Thus answer A is a red herring ... be careful chaps .. Amazon like to put these slippery eels in to trip you up and separate the wheat from the chaff !!! Read the question again and THINK about what its asking ? .. imagine the situation ... and you will see that Canary is the best approach to protect and save costs","poster":"petebear55","comments":[{"poster":"AMKazi","upvote_count":"1","content":"The question is not to save costs, its to improve reliability. Its about what solves the problem not the level of exam. You cannot to CANARY deployments for APACHE TOMCAT. Its not a feature release , its platform upgrade. canary deployments are suited for feature releases not platform upgrades","comment_id":"531378","timestamp":"1643035680.0"}],"comment_id":"253655","timestamp":"1633926780.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1632828060.0","comment_id":"253168","content":"Answer is A since OpsWork doesn't support Canary deployment.","poster":"Bulti"},{"comment_id":"244054","poster":"T14102020","upvote_count":"1","timestamp":"1632739620.0","content":"Correct is A. Blue/green deployment"},{"timestamp":"1632733560.0","upvote_count":"3","content":"I'll go with A","poster":"jackdryan","comment_id":"234469"},{"comments":[{"comment_id":"179519","content":"There is no canary for opsworks","timestamp":"1632575040.0","comments":[{"poster":"jobe42","comment_id":"411842","content":"And OpsWorks isn't mentioned here... just Chef, and Chef supports Canary","timestamp":"1635480900.0","upvote_count":"1"}],"upvote_count":"4","poster":"hglopes"},{"timestamp":"1633069680.0","upvote_count":"2","poster":"petebear55","content":"Yes i am inclined to agree with you ... B","comment_id":"253645"}],"comment_id":"168974","poster":"kanavpeer","upvote_count":"4","timestamp":"1632404040.0","content":"B - https://dev.to/mostlyjason/intro-to-deployment-strategies-blue-green-canary-and-more-3a3#:~:text=Canary%20deployment%20is%20like%20blue,part%20of%20the%20production%20infrastructure.\n\nCanary deployment is like blue-green, except it’s more risk-averse. Instead of switching from blue to green in one step, you use a phased approach.\n\nWith canary deployment, you deploy a new application code in a small part of the production infrastructure. Once the application is signed off for release, only a few users are routed to it. This minimizes any impact."},{"upvote_count":"2","comment_id":"157033","timestamp":"1632386880.0","content":"A is correct.","poster":"Anila_Dhharisi"},{"content":"A - Quick and easy rollback","comment_id":"156320","upvote_count":"2","poster":"zeronine","timestamp":"1632345660.0"}],"unix_timestamp":1597213440,"isMC":true,"answer_ET":"A","choices":{"D":"Implement the all at once deployment methodology.","C":"Configure Amazon CloudFront to serve all requests from the cache while deploying the updates.","A":"Implement a blue/green deployment methodology.","B":"Implement the canary release methodology."},"topic":"1","answer_images":[],"question_images":[],"question_id":547,"question_text":"A company developed a Java application and deployed it to an Apache Tomcat server that runs on Amazon EC2 instances. The company's Engineering team has implemented AWS CloudFormation and Chef Automate to automate the provisioning of and updates to the infrastructure and configuration of the application in the development, test, and production environments. These implementations have led to significantly improves reliability in releasing changes. The Engineering team reports there are frequent service disruptions due to unexpected errors when updating the application of the Apache Tomcat server.\nWhich solution will increase the reliability of all releases?","exam_id":32,"answer":"A","timestamp":"2020-08-12 08:24:00","answer_description":"","answers_community":["A (92%)","8%"]},{"id":"CzNpVmPqGAwyEtxsx90x","answer":"AB","topic":"1","isMC":true,"timestamp":"2020-08-12 08:38:00","answer_ET":"BD","question_images":[],"choices":{"C":"Create a Lambda function to rotate the credentials every hour by deploying a new Lambda version with the updated credentials.","B":"Configure Lambda to use the stored database credentials in AWS Secrets Manager and enable automatic rotation.","A":"Configure Lambda to assume a role in the management account with appropriate access to AWS.","D":"Use an SCP on the management account's OU to prevent IAM users from accessing resources in the Service team's account.","E":"Enable AWS Shield Advanced on the management account to shield sensitive resources from unauthorized IAM access."},"question_text":"During a security audit of a Service team's application, a Solutions Architect discovers that a username and password for an Amazon RDS database and a set of\nAWS IAM user credentials can be viewed in the AWS Lambda function code. The Lambda function uses the username and password to run queries on the database, and it uses the IAM credentials to call AWS services in a separate management account.\nThe Solutions Architect is concerned that the credentials could grant inappropriate access to anyone who can view the Lambda code. The management account and the Service team's account are in separate AWS Organizations organizational units (OUs).\nWhich combination of changes should the Solutions Architect make to improve the solution's security? (Choose two.)","answer_images":[],"unix_timestamp":1597214280,"exam_id":32,"answer_description":"","question_id":548,"discussion":[{"comment_id":"168699","upvote_count":"27","content":"A & B are correct.\nConcenus on B being correct.\nRegarding A verse D: SCP is too restrictive. As mentioned by khksoma, the issue is only around the Lambda function. D also does not provide a way to support the Lambda calling AWS services in the separate account. As such, D is not correct. Option \"A\" addresses this and is supported by the link given by balisongjam.","timestamp":"1632427140.0","poster":"LunchTime"},{"comment_id":"275809","timestamp":"1634933400.0","upvote_count":"6","content":"Answer is AB","poster":"Ebi"},{"timestamp":"1688311620.0","comment_id":"941012","upvote_count":"1","poster":"SkyZeroZx","content":"Selected Answer: AB\nA/B\nAssuming the role is the right way to do it. And SSM is good for storing DB credentials\nhttps://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/\n\nD is wrong as users from one account cannot access resources from another account if not allowed through cross-account access using assumed roles. There's no need to use SCP for deny\n\nOnly question in my case in this case usually case in lambdas not is need usage credentials is more apropiate use IAM Role avoid use of credentials.\nBut correct is AB"},{"poster":"tartarus23","content":"Selected Answer: AB\nA. Seems a better option than using AWS organizations to address the requirements\nB. AWS Secrets Manager enables lifecycle management, key rotation and securely storing the database credentials.","timestamp":"1651911480.0","comment_id":"598028","upvote_count":"1"},{"content":"My answer is B and D.\n\nin the question, mentioned as “The Solutions Architect is afraid that the credentials might be misused by anybody who can examine the Lambda code”, so proper access control is needed here. We need D for this.","upvote_count":"3","timestamp":"1643684520.0","comment_id":"537609","poster":"HellGate"},{"timestamp":"1642222200.0","upvote_count":"2","poster":"CloudChef","content":"Seems AWS has people who put a bunch of wrong answers at about the same time. Careful what you believe.","comment_id":"523931"},{"timestamp":"1638753180.0","upvote_count":"1","poster":"AzureDP900","comment_id":"494786","content":"A, B is right"},{"upvote_count":"1","poster":"tonikus","timestamp":"1636099200.0","content":"Q: Answers here.. marked as \"Correct\" with randomizer?","comment_id":"455499"},{"timestamp":"1636045200.0","comment_id":"411861","poster":"WhyIronMan","content":"I'll go with A,B","upvote_count":"2"},{"poster":"ss160700","content":"A&B - D will prevent Lambda to function correctly","upvote_count":"1","timestamp":"1635578040.0","comment_id":"371457"},{"comments":[{"timestamp":"1635998460.0","poster":"pradhyumna","upvote_count":"3","content":"Changing to A & B rds credentials in secrets manager, use roles to eliminate mgmt creds","comment_id":"398742"}],"poster":"pradhyumna","comment_id":"360339","content":"B and D is correct. The question says, \"which combination\", obviously both AB are solving the same lambda problem, hence not a good \"combination\". On top of it, it does not help lambda assuming a role in mgmt account while the application is completely running in service account with lambda and RDS. Second part of the problem is how to prevent users from using the IAM credentials which can be viewed in the code. This is what SCP is addressing, anyways SCP doesn't affect the IAM users in the mgmt account and so this SCP would prevent IAM users from the service account . I would go with B and D \"combination\".","upvote_count":"3","timestamp":"1635504180.0"},{"upvote_count":"6","comment_id":"350120","timestamp":"1635242340.0","poster":"Waiweng","content":"it's A&B"},{"poster":"Kian1","comment_id":"292516","content":"going with AB","timestamp":"1634995500.0","upvote_count":"3"},{"content":"A & B is the most secure approach","upvote_count":"2","timestamp":"1634961420.0","comment_id":"281210","poster":"Firststack"},{"comment_id":"268998","timestamp":"1634916300.0","poster":"Justu","upvote_count":"1","content":"AB, You need to fix lambda getting credentials directly from the code and allow it to use mgmt account resources. \n\nD: There's no need to restrict ServiceAccount resources by SCP. \n\nKanavpeer is right."},{"comment_id":"254553","timestamp":"1634806800.0","upvote_count":"2","content":"A/B\nAssuming the role is the right way to do it. And SSM is good for storing DB credentials\nhttps://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/\n\nD is wrong as users from one account cannot access resources from another account if not allowed through cross-account access using assumed roles. There's no need to use SCP for deny\nE is wrong as shield is used for ddos protection\nC does not make sense with hourly redeploying of lambda","poster":"Cantaloupe"},{"comment_id":"253665","content":"BEST PRACTICE WOULD BE B AND D","upvote_count":"3","poster":"petebear55","timestamp":"1634491860.0"},{"comment_id":"253440","timestamp":"1634400780.0","poster":"Bulti","upvote_count":"2","content":"A & B is the right answer. SCP will prevent an IAM user credentials to access the services which will cause the Lambda function to fail. We don't want the Lambda function to fail wile calling AWS services. Option A provides an elegant and standard solution to allow Lambda in one account to access AWS services in another account by assuming the IAM role that provides it access to call those AWS services."},{"upvote_count":"2","poster":"arulrajjayaraj","timestamp":"1634302500.0","content":"A&B - No SCP needed (High level restriction)","comment_id":"248764"},{"content":"Correct is BD. Secrets Manager + SCP","timestamp":"1634244180.0","comment_id":"244061","upvote_count":"2","poster":"T14102020"},{"comment_id":"234476","timestamp":"1634197320.0","upvote_count":"3","poster":"jackdryan","content":"I'll go with A,B"},{"content":"My friend A,B","comment_id":"233116","upvote_count":"1","timestamp":"1633909620.0","poster":"gookseang"},{"content":"B and D","comment_id":"233114","upvote_count":"2","poster":"gookseang","timestamp":"1633763100.0"},{"upvote_count":"2","comment_id":"209278","timestamp":"1633397880.0","poster":"CYL","comments":[{"upvote_count":"2","comment_id":"229034","poster":"cloudgc","timestamp":"1633522860.0","content":"SCP will improve the security but will not enable the Lambda access to management account.","comments":[{"poster":"cloudgc","content":"+ can impact other IAM users.","upvote_count":"1","comment_id":"229035","timestamp":"1633554120.0"}]}],"content":"B & D. Password protection using secrets manager. Use SCP to control access rights grant activities."},{"content":"A & B are correct.","poster":"ishuiyutian","timestamp":"1633351320.0","upvote_count":"2","comment_id":"206240"},{"content":"Sorry... changed mind. A&B is right answer. Of course A and B mean adopt those changes after deleting the credentials on lambda function.\nI thought there is a credential for DB connection on lambda function and lambda already assume roles for AWS services on management account. I read it again and now I can see why A&B is right answer.","poster":"hyemi","timestamp":"1633324380.0","upvote_count":"1","comment_id":"203940"},{"comment_id":"203501","timestamp":"1633267200.0","content":"I agree SCP could be too restrictive. If there was another option for deleting the credentials from lambda function, then I'll choose B and that option.","upvote_count":"1","poster":"hyemi"},{"upvote_count":"2","content":"B&D. Lambda already assumes a role in management account because it's working!! It can call the service in management account. We need to prevent the users in management from looking the credentials in Lambda function, thus here comes the SCP.","timestamp":"1633182960.0","poster":"hyemi","comment_id":"203498"},{"comment_id":"199487","timestamp":"1633172700.0","poster":"Paitan","upvote_count":"1","content":"Options A and B"},{"comment_id":"168844","poster":"Momon","upvote_count":"2","content":"on second thought A and B may be better =)\nhttps://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/","timestamp":"1632516000.0"},{"content":"B and D. There is this concern that needs D answer. \"The Solutions Architect concerned that the credentials could grant inappropriate access to anyone who can view the Lambda code.\"","comment_id":"168840","upvote_count":"2","poster":"Momon","timestamp":"1632505320.0"},{"comment_id":"166377","upvote_count":"2","poster":"[Removed]","content":"B&D are correct","timestamp":"1632351900.0"},{"poster":"directconnect","upvote_count":"3","timestamp":"1632300060.0","comment_id":"160157","content":"A & B are correct."},{"timestamp":"1632278580.0","upvote_count":"2","poster":"Anila_Dhharisi","content":"B & D are correct.","comment_id":"157036"},{"upvote_count":"4","comment_id":"156203","timestamp":"1632092340.0","poster":"Nemer","comments":[{"comment_id":"156599","timestamp":"1632142500.0","poster":"khksoma","comments":[{"poster":"kanavpeer","content":"A,B - \nC - wrong because, it can impact other IAM users which may be used.\nQuestion's end line says \"...improve the solution's security?\"","upvote_count":"1","timestamp":"1632616620.0","comment_id":"168975"}],"upvote_count":"10","content":"It should be A and B. The concern is only around Lambda."},{"content":"D can not be a correct answer as there is no requirement to restrict cross account access via SCP but question asks to improve the solution security when accessing services across accounts which is solved by option B.","comment_id":"338727","poster":"Amitv2706","comments":[{"timestamp":"1635148740.0","upvote_count":"2","comment_id":"338728","poster":"Amitv2706","content":"I mean to say which is solved by option A.\nAnswer is A and B."}],"timestamp":"1635023820.0","upvote_count":"1"}],"content":"B & D - Secrets Manager supports automatic rotation of credentials for RDS. SCP to control cross-OU access to Service team’s account."}],"url":"https://www.examtopics.com/discussions/amazon/view/28229-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["AB (100%)"]},{"id":"KkSF6JCHAvtESs6KYMPY","question_text":"A company is having issues with a newly deployed serverless infrastructure that uses Amazon API Gateway, Amazon Lambda, and Amazon DynamoDB.\nIn a steady state, the application performs as expected. However, during peak load, tens of thousands of simultaneous invocations are needed and user requests fail multiple times before succeeding. The company has checked the logs for each component, focusing specifically on Amazon CloudWatch Logs for Lambda.\nThere are no errors logged by the services or applications.\nWhat might cause this problem?","answer_ET":"C","choices":{"D":"DynamoDB is set up in an auto scaling mode. During peak load, DynamoDB adjusts capacity and throughput behind the scenes, which is causing the temporary downtime. Once the scaling completes, the retries go through successfully.","C":"The throttle limit set on API Gateway is very low. During peak load, the additional requests are not making their way through to Lambda.","B":"Lambda is in a subnet that uses a NAT gateway to reach out of the internet, and the function instance does not have sufficient Amazon EC2 resources in the VPC to scale with the load.","A":"Lambda has very low memory assigned, which causes the function to fail at peak load."},"answer_description":"Reference:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html","question_images":[],"topic":"1","answers_community":["C (100%)"],"question_id":549,"discussion":[{"timestamp":"1632942960.0","poster":"proxyolism","comment_id":"191938","upvote_count":"18","content":"the answer is C. question says\n\nThe company has checked the logs for each component, focusing specifically on Amazon CloudWatch Logs for Lambda.\n\nand it means there is no error log from lambda. the company actually did not check API gateway's cloudwatch log. if lambda fails, the company could check it is the lambda problem with cloudwatch logs. furthermore, A is completely wrong because lambda runs pararell with concurrency. and question says this problem only occurs when during maximum loads. if lambda memory is the cause of problem, it can be failed whenever under maximum loads or not."},{"timestamp":"1632121560.0","content":"Fine with C. Nothing wrong with Lambda. Increase API gateway throttle limits.","upvote_count":"9","comments":[{"comments":[{"timestamp":"1635950640.0","upvote_count":"2","comment_id":"436820","poster":"student22","content":"C\nNo errors from Lambda because requests were throttled at API Gateway."}],"timestamp":"1632243180.0","comment_id":"184096","poster":"sam422","upvote_count":"2","content":"There are no errors logged in from services or application, why we think gateway throttle errors, it will log if an issue. I go with A"}],"poster":"Nemer","comment_id":"154674"},{"timestamp":"1688311860.0","comment_id":"941014","content":"Selected Answer: C\nCorrect is C. If no errors in logs so need increase API gateway throttle limits","upvote_count":"1","poster":"SkyZeroZx"},{"comment_id":"640144","content":"C makes perfect sense","timestamp":"1659277740.0","poster":"Ni_yot","upvote_count":"1"},{"poster":"AMKazi","content":"Ans is C: \ncannot be D: https://aws.amazon.com/about-aws/whats-new/2017/06/announcing-amazon-dynamodb-auto-scaling/#:~:text=Starting%20today%2C%20when%20you%20create,request%20volumes%2C%20with%20zero%20downtime.","comment_id":"532239","upvote_count":"1","timestamp":"1643124960.0"},{"upvote_count":"1","comment_id":"513947","timestamp":"1640939280.0","poster":"cldy","content":"C is correct."},{"poster":"AzureDP900","comment_id":"494787","upvote_count":"1","content":"C is correct","timestamp":"1638753300.0"},{"timestamp":"1635575400.0","content":"I'll go with C","comment_id":"411866","poster":"WhyIronMan","upvote_count":"2"},{"comment_id":"404542","content":"Im for A, was for D but doesnt make sense as if it was scaling issue on Dynamo it will log errors on lambda.","comments":[{"poster":"Kopa","comment_id":"478683","content":"sorry i mean C","upvote_count":"1","timestamp":"1636982280.0"}],"timestamp":"1635530460.0","upvote_count":"2","poster":"Kopa"},{"content":"it's C","comment_id":"350125","upvote_count":"2","poster":"Waiweng","timestamp":"1635395940.0"},{"upvote_count":"1","poster":"kiev","comment_id":"295261","content":"It is C. I thought it was A but reading again the last line that says lambda has cloud watch has been checked and there is no problem with lambda implies the problem isn't with lambda. Now between API gateway and Dynamodb, I think it is clear there is a problem with throttle limit in API gateway that's causing the issue.","timestamp":"1635291000.0"},{"poster":"Kian1","comment_id":"292518","content":"going with C","upvote_count":"1","timestamp":"1634987520.0"},{"timestamp":"1634942940.0","content":"C is correct","poster":"Firststack","upvote_count":"2","comment_id":"281211"},{"upvote_count":"3","content":"Answer is C","comment_id":"275814","poster":"Ebi","timestamp":"1634899860.0"},{"timestamp":"1634797440.0","content":"C API GW limit","upvote_count":"2","poster":"kopper2019","comment_id":"258878"},{"poster":"Bulti","content":"Answer is C. When throttle limits are low on API Gateway, concurrent requests beyond that threshold limit are dropped and they need to be retried. As a result after repeated retries the request succeeds when the concurrent request count drops below the throttle limit.","timestamp":"1634669580.0","comment_id":"253449","upvote_count":"2"},{"content":"Correct is C. If no errors in logs so need increase API gateway throttle limits","upvote_count":"1","comment_id":"244062","timestamp":"1634634540.0","poster":"T14102020"},{"upvote_count":"2","timestamp":"1634555520.0","comment_id":"239050","content":"A right","poster":"joos"},{"comment_id":"233121","content":"Seems C","upvote_count":"1","timestamp":"1634550840.0","poster":"gookseang"},{"content":"I'll go with C","upvote_count":"3","poster":"jackdryan","timestamp":"1634471280.0","comment_id":"231631"},{"timestamp":"1634002020.0","upvote_count":"1","comment_id":"209282","poster":"CYL","content":"C. Since no errors shown in other services, the problem probably starts at the source item, which is the API gateway."},{"upvote_count":"2","poster":"cpd","content":"If dynamo or Lambda is having problems, we will see them in cloudwatch logs of lamnda. \nSo choice is C","timestamp":"1633788840.0","comment_id":"206305"},{"poster":"hyemi","content":"....focusing specifically on Amazon CloudWatch Logs \"FOR LAMBDA\".","comment_id":"203511","timestamp":"1633699140.0","upvote_count":"1"},{"upvote_count":"3","content":"Why not D ? In all cases all services have shown no issues in logs","comment_id":"201002","poster":"Mikey123","timestamp":"1633210620.0"},{"timestamp":"1632686700.0","poster":"perio","content":"I would go A. API GW throttle can be burst, but not infinitely. Default throttle is 5000 and can be burst to 10000. If you need more capacity, should contact Service Quota.\nhttps://docs.aws.amazon.com/ko_kr/apigateway/latest/developerguide/limits.html","upvote_count":"1","comments":[{"poster":"hyemi","comment_id":"203506","upvote_count":"1","content":"The question is asking what is the problem...no requiring for solution. More over, the lambda received requests sent and couldn't deal with it, why there is nothing left in CloudWatch log?","timestamp":"1633529040.0"}],"comment_id":"186125"},{"upvote_count":"1","timestamp":"1632364140.0","poster":"sam422","comment_id":"184097","content":"I go with A , There are no errors logged in from services or application, why we think gateway throttle errors? it will log if an issue."},{"timestamp":"1632157500.0","comment_id":"157039","poster":"Anila_Dhharisi","content":"C is roght answer. The API Gateway limits should be increased. No issues with Lambda nad Dynamodb as the user requests are failing and no error logs related to any of the services used.","upvote_count":"4"}],"exam_id":32,"timestamp":"2020-08-10 17:50:00","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/27953-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1597074600,"isMC":true,"answer_images":[]},{"id":"KIRJmEFNcbcfXXGJ4VVL","answer_description":"","topic":"1","answer_ET":"AD","isMC":true,"exam_id":32,"answer_images":[],"discussion":[{"comment_id":"154673","upvote_count":"33","poster":"Nemer","timestamp":"1632222240.0","content":"A&D. AWS Org operating in all features mode, to be able to use SCP with deny list (blacklist).\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp-strategies.html","comments":[{"content":"Well done Nemer","timestamp":"1634958900.0","upvote_count":"2","poster":"petebear55","comment_id":"253678"}]},{"upvote_count":"1","comment_id":"941018","poster":"SkyZeroZx","timestamp":"1688312040.0","content":"Selected Answer: AD\nA & D for sure"},{"poster":"davidy2020","content":"A&D\nAll features – The default feature set that is available to AWS Organizations. It includes all the functionality of consolidated billing, plus advanced features that give you more control over accounts in your organization. For example, when all features are enabled the management account of the organization has full control over what member accounts can do. The management account can apply SCPs to restrict the services and actions that users (including the root user) and roles in an account can access.\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html#feature-set","timestamp":"1674275220.0","upvote_count":"2","comment_id":"782993"},{"timestamp":"1648577220.0","upvote_count":"1","comment_id":"577806","poster":"jj22222","content":"Selected Answer: AD\nA & D for sure"},{"content":"Selected Answer: AD\nI agree with nemer","timestamp":"1644940860.0","comment_id":"547862","poster":"pititcu667","upvote_count":"1"},{"timestamp":"1638760020.0","content":"A, D is right","comment_id":"494830","poster":"AzureDP900","upvote_count":"1"},{"comment_id":"411868","timestamp":"1636290660.0","poster":"WhyIronMan","content":"I'll go with A,D","upvote_count":"1"},{"upvote_count":"4","timestamp":"1635974580.0","content":"it's A&D","comment_id":"350177","poster":"Waiweng"},{"content":"A and D","upvote_count":"1","comment_id":"349043","poster":"blackgamer","timestamp":"1635675480.0"},{"upvote_count":"2","timestamp":"1635545280.0","poster":"kiev","comment_id":"295271","content":"AD is the right answer. Now to those saying why A and not E, it is because AWS organisations in all feature mode include consolidated billing."},{"timestamp":"1635470700.0","content":"going with AD","comment_id":"292522","poster":"Kian1","upvote_count":"1"},{"poster":"Ebi","timestamp":"1635435180.0","comment_id":"275819","upvote_count":"3","content":"A and D are correct answer"},{"poster":"kopper2019","timestamp":"1635359760.0","comment_id":"269328","upvote_count":"2","content":"A and D, Orgs and SCP the way to go"},{"poster":"Bulti","content":"A & D is the right answer. Forst put all accounts into OU and the apply SCP to deny access to the EC2 API that procure new reserved instances or modify existing reserved instances.","comment_id":"253451","timestamp":"1634729220.0","upvote_count":"1"},{"content":"With D in place, I wonder how the procurement team would now be able to purchase reserved instances...I know you can have exceptions in SCPs but c'mon AWS...","timestamp":"1634572320.0","poster":"darthvoodoo","comment_id":"251319","upvote_count":"3"},{"timestamp":"1634381640.0","upvote_count":"1","content":"Correct is AD. A and D. Use AWS Organization together with SCP","comment_id":"244065","poster":"T14102020"},{"poster":"rscloud","comment_id":"242021","timestamp":"1634367660.0","upvote_count":"1","content":"A and D\nSCPs are available only in an organization that has all features enabled.\nAn SCP restricts permissions for IAM users and roles in member accounts"},{"comment_id":"231633","poster":"jackdryan","content":"I'll go with A,D","upvote_count":"3","timestamp":"1633530420.0"},{"timestamp":"1633516140.0","poster":"cloudgc","content":"A - https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html\nD","upvote_count":"1","comment_id":"229078"},{"timestamp":"1633464240.0","content":"A & D \nhttps://docs.aws.amazon.com/organizations/latest/APIReference/API_EnableAllFeatures.html","comment_id":"211064","upvote_count":"1","poster":"liono"},{"comment_id":"209286","upvote_count":"1","poster":"CYL","timestamp":"1633429260.0","content":"A and D. Use AWS Organization together with SCP for enforcement of purchase capability."},{"timestamp":"1633266060.0","content":"Answer should be D and E.\n\nD is agreed by everyone, but E is better than A as consolidated billing will save money. \nThoughts?","poster":"sonalshenoy","comment_id":"201033","upvote_count":"1","comments":[{"poster":"drexciya28","upvote_count":"1","timestamp":"1633413240.0","comment_id":"204222","comments":[{"comment_id":"253681","poster":"petebear55","upvote_count":"1","timestamp":"1635350520.0","content":"consolidated billing by itself would not allow you to apply scp to effectively 'deny'"},{"content":"All Features(is a default features of Organization account)\n 1. Includes consolidated billing features\n 2. Invited accounts(child account) must approve enables all features\n 3. You can use SCP\n 4. Ability to apply an SCP to prevent members account from leaving the organization.\n 5. Once All Feature is enabled, can't switch back to consolidated billing only","poster":"GopiSivanathan","timestamp":"1633725540.0","upvote_count":"5","comment_id":"232575"}],"content":"I prefer E because Consolidated billing enables sharing of volume pricing discounts. From the official documentation:\nYou can combine the usage across all accounts in the organization to share the volume pricing discounts, Reserved Instance discounts, and Savings Plans. This can result in a lower charge for your project, department, or company than with individual standalone accounts."},{"comment_id":"203513","upvote_count":"2","timestamp":"1633296600.0","content":"I thought like you at first, but A&D seems right. Read the page below. It says \"....If you enable all features in an organization, then you can apply service control policies (SCPs) to any or all of your accounts.\"\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/security-iam.html","poster":"hyemi"}]},{"comment_id":"196134","content":"With all the given answers, permission wil be denied to the Team authorised to purchase EC2 as well","upvote_count":"1","poster":"SanjeevB","timestamp":"1633222500.0"},{"content":"How about AC?","upvote_count":"1","comment_id":"176606","poster":"Phat","comments":[{"upvote_count":"1","comment_id":"182099","timestamp":"1633193580.0","content":"SCP gives more greater flexibility to implement permission on top level","poster":"sam422"}],"timestamp":"1633005720.0"},{"comment_id":"157041","content":"A&D are correct. Instead of creating IAM policy to each account better to create a deny policy and attach to SCP in AWS Org.","upvote_count":"2","poster":"Anila_Dhharisi","timestamp":"1632596400.0"}],"question_id":550,"timestamp":"2020-08-10 17:49:00","question_images":[],"unix_timestamp":1597074540,"answers_community":["AD (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/27952-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"AD","question_text":"A large company with hundreds of AWS accounts has a newly established centralized internal process for purchasing new or modifying existing Reserved\nInstances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement or execution. Previously, business units would directly purchase or modify Reserved Instances in their own respective AWS accounts autonomously.\nWhich combination of steps should be taken to proactively enforce the new process in the MOST secure way possible? (Choose two.)","choices":{"B":"Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions.","E":"Ensure that all AWS accounts are part of an AWS Organizations structure operating in consolidated billing features mode.","D":"Create an SCP that contains a deny rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions. Attach the SCP to each organizational unit (OU) of the AWS Organizations structure.","C":"In each AWS account, create an IAM policy with a DENY rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions.","A":"Ensure all AWS accounts are part of an AWS Organizations structure operating in all features mode."}}],"exam":{"isImplemented":true,"numberOfQuestions":1019,"isMCOnly":false,"isBeta":false,"provider":"Amazon","id":32,"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025"},"currentPage":110},"__N_SSP":true}