{"pageProps":{"questions":[{"id":"VwVkQrlm60zTc0vcxgL2","answer_images":[],"discussion":[{"timestamp":"1698737340.0","content":"Selected Answer: AC\nFor those who struggle on why A but not D as they are almost identical like I did:\nA: Create an S3 access point for each application in THE AWS account\nD: Create an S3 access point for each application in EACH AWS account\n\nNot sure if this is technical or English exam.","poster":"joleneinthebackyard","comments":[{"poster":"a54b16f","content":"A: in the AWS account that owns the S3 bucket","comment_id":"1159250","timestamp":"1708903680.0","upvote_count":"1"}],"upvote_count":"18","comment_id":"1058535"},{"comment_id":"1254343","upvote_count":"4","timestamp":"1721819460.0","poster":"vip2","content":"Selected Answer: AC\nsee details step in below link where 'Create an Amazon S3 gateway endpoint in your VPC'\n\nhttps://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/"},{"timestamp":"1716483720.0","poster":"sse69","content":"Selected Answer: AC\nhttps://repost.aws/knowledge-center/s3-access-bucket-restricted-to-vpc","upvote_count":"2","comments":[{"content":"The linked post describes the scenario of creating an S3 access point in the data lake account (answer A) and a gateway VPC endpoint in the application's account (answer C).","poster":"fartosh","comment_id":"1221762","timestamp":"1717093500.0","upvote_count":"2"}],"comment_id":"1216764"},{"comment_id":"1206858","content":"Selected Answer: AC\nA and C in my opinion. Interface Endpoint is for EC2 generally, when we need a private IP. Gateway Endpoint is suitable in 95% cases when there are DynamoDB and S3 secure connectivity.","poster":"red_panda","upvote_count":"1","timestamp":"1714908300.0"},{"comment_id":"1205256","upvote_count":"2","poster":"BrijMohan08","timestamp":"1714612200.0","content":"Selected Answer: AC\nA & C\nWhy not B?\nInterface endpoints are used for services that require a private IP address within the VPC, such as Amazon EC2, Amazon ECS, or Amazon SNS.\n\nGateway endpoints, on the other hand, are used for services that are accessed using their public endpoint, such as Amazon S3 and Amazon DynamoDB.\n\nSince the scenario involves accessing an S3 bucket, a gateway endpoint is the appropriate choice, not an interface endpoint."},{"upvote_count":"1","comment_id":"1194778","timestamp":"1712995140.0","content":"Correct:A,B\nhttps://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/","poster":"trap"},{"poster":"VerRi","upvote_count":"2","timestamp":"1711902600.0","content":"Selected Answer: AB\nGateway Endpoint only allows resources within the VPC to connect to S3.\nIt is not possible to provide the gateway endpoint across many AWS accounts","comment_id":"1186888"},{"timestamp":"1711287720.0","comment_id":"1181624","upvote_count":"2","content":"Selected Answer: AB\nI don't think C can achieve the requirement. At least according to this https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html. Here's why.\n\n\"100's of AWS Accounts\" hints about possibility of cross region access. Gateway Endpoints can't allow access from VPCs in other regions. Gateway endpoint is to access from own VPC.","poster":"kz407"},{"upvote_count":"3","comment_id":"1179522","poster":"Dgix","timestamp":"1711082400.0","content":"Selected Answer: AB\nIt's A+B. A sets up S3 Access Points, one for each accessing application, in the data lake account (the S3 account) which are configured with policies giving each application least-privilege access. B then sets up PrivateLink access (==interface endpoints) in each of the application accounts. \n\nC is out because gateway endpoints can't take policies.\nD is less efficient than A+B\nE is too simplistic - one gateway endpoint is not enough.."},{"comment_id":"1177527","content":"Selected Answer: AB\nA is valid, but C can't be configured for fine-grained access since it involves a gateway endpoint. Therefore: B as this is possible with a PrivateLink (==interface endpoint)","timestamp":"1710864900.0","poster":"Dgix","upvote_count":"1"},{"poster":"blackgamer","content":"Answer is A & B. \n\nC is not suitable based on AWS Gateway endpoints documentation - \n\"Endpoint connections cannot be extended out of a VPC. Resources on the other side of a VPN connection, VPC peering connection, transit gateway, or AWS Direct Connect connection in your VPC cannot use a gateway endpoint to communicate with Amazon S3.\"\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html","comment_id":"1093038","timestamp":"1702263660.0","upvote_count":"3","comments":[{"content":"With a gateway endpoint, you can access Amazon S3 from your VPC (https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html)","upvote_count":"1","timestamp":"1707031800.0","poster":"zhooon","comment_id":"1139865"}]},{"upvote_count":"1","content":"Selected Answer: AC\nA & C are right.","comment_id":"1077676","timestamp":"1700678400.0","poster":"career360guru"},{"comments":[{"upvote_count":"2","timestamp":"1704330480.0","poster":"Mehrannn","content":"considering this blog post, do you agree with A&B or A&C?","comment_id":"1113272"}],"timestamp":"1698783780.0","upvote_count":"3","comment_id":"1059180","poster":"Sab","content":"Selected Answer: AC\nhttps://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/"},{"comment_id":"1053191","upvote_count":"2","content":"Selected Answer: AB\nAnswer is AB, because gateway VPC does not have access to S3 access point.\nAnd interface VPC endpoint allos access to S3 access point.\nNote from ChatGPT:\nAs of my last knowledge update in September 2021, Gateway VPC Endpoints for Amazon S3 do not support direct access to S3 access points. Gateway VPC Endpoints are designed to provide private connectivity from your Amazon Virtual Private Cloud (VPC) to S3, but they do not inherently support access to S3 access points.","poster":"KCjoe","timestamp":"1698186420.0"},{"timestamp":"1697125560.0","comment_id":"1041885","poster":"totten","upvote_count":"1","content":"Selected Answer: AC\nA. By creating an S3 access point for each application in the AWS account that owns the S3 bucket and configuring it to be accessible only from the application's VPC, you ensure that each application has the minimum necessary permissions and can access the data lake securely.\n\nC. Creating a gateway endpoint for Amazon S3 in each application's VPC and configuring the endpoint policy to allow access to an S3 access point ensures that traffic from each VPC is directed through the S3 access point and adheres to the security requirements. Specifying the route table that is used to access the access point is an essential part of the configuration.\n\nThis combination of steps helps you meet your security and access requirements by using S3 access points and VPC endpoints for each application. It ensures that the data lake is accessed securely and that access permissions are correctly configured."},{"timestamp":"1693448160.0","upvote_count":"1","comment_id":"994648","content":"Selected Answer: BD\nGateway endpoint is public where as S3 access point and Interface endpoint can be private and limited to VPC. https://aws.amazon.com/s3/features/access-points/","poster":"Gabehcoud"},{"content":"can anyone tell me why B is incorrect\nfrom what i know\ngateway endpoint resolves to Public AWS IP\ninterface endpoint is completely private\nplease correct me if wrong","timestamp":"1692892560.0","poster":"chikorita","comment_id":"989318","upvote_count":"3","comments":[{"timestamp":"1693569780.0","content":"interface endpoint is completely private, you are wrong interface endpoint is public","upvote_count":"1","poster":"vn_thanhtung","comment_id":"996023"},{"timestamp":"1693569720.0","comment_id":"996022","content":"Because To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application => using access endpoint instead of interface endpoints","upvote_count":"1","comments":[{"content":"thanks, got it","poster":"chikorita","timestamp":"1694093820.0","upvote_count":"1","comment_id":"1001611"}],"poster":"vn_thanhtung"}]},{"upvote_count":"1","comment_id":"976506","timestamp":"1691577780.0","comments":[{"upvote_count":"1","comment_id":"976519","content":"however I think E also has problem \"route table that is used to access the bucket\" should be access point","timestamp":"1691591280.0","poster":"softarts"}],"content":"Selected Answer: AE\nStephane sap-c02 practice test2-Q72.\nC is worong. There is no need to create separate VPCs for each application, as just a single data lake VPC can house all applications, which allows you to configure a single S3 gateway endpoint having a policy with a condition to limit access via a common prefix for the access points of all the S3 buckets for the data lake. So this option is not the best fit.","poster":"softarts"},{"timestamp":"1690904520.0","comment_id":"969174","upvote_count":"1","poster":"Arnaud92","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html\nYou can access Amazon S3 from your VPC using gateway VPC endpoints. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to Amazon S3."},{"comment_id":"945053","poster":"NikkyDicky","upvote_count":"2","content":"Selected Answer: AC\nAC - https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/","timestamp":"1688684100.0"},{"comment_id":"944099","timestamp":"1688589180.0","poster":"Christina666","upvote_count":"1","content":"Selected Answer: AC\nA C is correct.\nS3: gateway endpoint, policy-> allow access point DNS\naccess point: choose S3 vpc endpoint as origin"},{"content":"Selected Answer: AC\nA ) manage with granular permissions from the master account the connection to the bucket sounds like a good idea and according to what is required\nB ) interface endpoint , usually use case is for enable public connection , not is required is incorrect in this case \nC) Gateway Endpoint, it is usually used for the internal AWS network which would be useful in this additional case that is configured for each account and client application which is granular, sounds like a good idea\nD ) use access point in the clients, but it does not make sense because the one who will grant the permissions has to be the owner of the bucket so we discard it\nE ) gateway endpoint , doesn't sound appropriate in the owner's bucket because you have to use granular permissions as directed with the access point\n\nThen correct is AC","comment_id":"936932","poster":"SkyZeroZx","upvote_count":"3","timestamp":"1687971420.0"},{"timestamp":"1687881600.0","content":"Selected Answer: DE\nI vote D and E. D - because we need to create the endpoint in the account, which hosts the application and to limit it to VPC and E - since we need to create gateway endpoint in the data lake's account so the traffic does not traverse via the internet.\nBelow is the official documentation\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-access-points.html","comment_id":"935570","poster":"Maria2023","upvote_count":"1"},{"timestamp":"1687811700.0","content":"Selected Answer: AC\nAC - Correct","comment_id":"934776","upvote_count":"2","poster":"SmileyCloud","comments":[{"poster":"SmileyCloud","comment_id":"944857","content":"https://repost.aws/knowledge-center/s3-access-bucket-restricted-to-vpc","upvote_count":"1","timestamp":"1688662320.0"}]},{"content":"a-c-a-c-a-c","upvote_count":"1","timestamp":"1687783440.0","poster":"easytoo","comment_id":"934458"},{"upvote_count":"1","comment_id":"934239","poster":"awscerts023","content":"Selected Answer: AC\nAdding the weightage on right answer , AC according to https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/","timestamp":"1687771140.0"},{"timestamp":"1687666440.0","comment_id":"933172","content":"Selected Answer: AC\nAgree with A and C","upvote_count":"1","poster":"PhuocT"},{"content":"Selected Answer: AC\nIt's AC. https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/","comment_id":"932189","upvote_count":"3","timestamp":"1687583520.0","poster":"ozelllll"},{"comment_id":"932046","content":"Selected Answer: AC\nGPT : Step A enables each application to have its own access point, which can be configured to allow only the necessary permissions for that application. This satisfies the requirement for least privilege access.\nStep C involves creating a gateway VPC endpoint for S3 in each application's VPC. This endpoint provides a private path for traffic between the VPC and the S3 bucket, ensuring that the data does not traverse the public internet. The endpoint policy should be configured to allow access to the specific S3 access point created for the application, maintaining the least privilege principle.","upvote_count":"2","timestamp":"1687567560.0","poster":"gd1","comments":[{"poster":"Jackhemo","comment_id":"932919","timestamp":"1687639800.0","upvote_count":"1","content":"Bud, GPT Regenerate and check again."}]},{"comment_id":"932029","poster":"Alabi","content":"Selected Answer: AB\nExplanation:\n\nStep A involves creating an S3 access point for each application in the AWS account that owns the S3 bucket. By configuring each access point to be accessible only from the application's VPC, access to the bucket is restricted to specific VPCs. Updating the bucket policy to require access from an access point ensures that applications can only access the bucket through the designated access point.\n\nStep B is about creating an interface endpoint for Amazon S3 in each application's VPC. By configuring the endpoint policy to allow access to an S3 access point, the applications can communicate with the S3 bucket through the access point. Creating a VPC gateway attachment for the S3 endpoint enables connectivity between the VPC and the S3 service.","upvote_count":"2","timestamp":"1687566360.0"},{"content":"Selected Answer: AD\nGateway Endpoint = VPC connecting VPC privately with no internet connectivity\nS3 access point = unique DNS address for applications to interact with a specific portion of a bucket, while allowing granular control over permissions and policies.\nInterface Endpoint = AWS Service connecting VPC privately without internet connectivity\n\nSo the question is application accessing S3, so Gateway endpoint is out (CE out), also Interface Endpoint is out as well (B).\n\nonly AD left.","comment_id":"931882","timestamp":"1687546800.0","poster":"nexus2020","upvote_count":"4","comments":[{"poster":"task_7","upvote_count":"1","content":"Thanks","timestamp":"1696132920.0","comment_id":"1021994"}]}],"choices":{"C":"Create a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point. Specify the route table that is used to access the access point.","B":"Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint.","A":"Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application’s VPC. Update the bucket policy to require access from an access point.","E":"Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket.","D":"Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point."},"exam_id":33,"question_text":"A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function.\n\nTo meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.\n\nWhich combination of steps should the solutions architect take to implement this solution? (Choose two.)","question_images":[],"timestamp":"2023-06-23 21:00:00","unix_timestamp":1687546800,"answer_description":"","answer":"AC","question_id":161,"topic":"1","answers_community":["AC (72%)","AB (18%)","6%"],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/113133-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"AC"},{"id":"qpA6QDFXkEN75NuOVy87","exam_id":33,"answer_images":[],"unix_timestamp":1687457520,"url":"https://www.examtopics.com/discussions/amazon/view/112977-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":162,"question_text":"A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that send application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.\n\nThe company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring solution that uses Splunk on premises. A solutions architect needs to determine how to send networking traffic to Splunk.\n\nHow should the solutions architect meet these requirements?","isMC":true,"choices":{"B":"Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extracts individual log events from records sent by CloudWatch Logs subscription filters. Enable VPC flows logs, and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to the Kinesis Data Firehose delivery stream.","A":"Enable VPC flows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. Generate ACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucket by using those credentials.","C":"Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs grouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda function to automatically send any new file that is put in the S3 bucket to Splunk.","D":"Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1-minute sliding window to collect the events. Create a SQL query that uses the anomaly detection template to monitor any networking traffic anomalies in near-real time. Send the result to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination."},"answers_community":["B (100%)"],"timestamp":"2023-06-22 20:12:00","answer_description":"","topic":"1","question_images":[],"answer_ET":"B","discussion":[{"content":"Selected Answer: B\nAnswer is B\nQuestion asks for \"near real time\" analysis\nFor near real time -->use Kinesis Datafirehose. \nFor real time ---> use Kineses data streams\nreal-time is instant, whereas near real-time is delayed","poster":"bhanus","comment_id":"930885","timestamp":"1703275920.0","upvote_count":"16"},{"poster":"adelynllllllllll","content":"B:\n\nWhy do they answer the solution backwards. it does no follow the workflow, it is hard to put the picture together. but , anyway.","comment_id":"1111937","timestamp":"1719920340.0","upvote_count":"4"},{"poster":"career360guru","content":"Selected Answer: B\nB is right answer as KDF supports Splunk integration.","comment_id":"1077709","comments":[{"comment_id":"1077710","timestamp":"1716397380.0","upvote_count":"1","poster":"career360guru","content":"and Requirement is Near Real time."}],"upvote_count":"1","timestamp":"1716397380.0"},{"poster":"joleneinthebackyard","upvote_count":"3","comment_id":"1058511","content":"Selected Answer: B\nMonitoring solution -> VPC flow logs\nNear real time analysis -> Firehose\nFirehose also can have spunk as destination -> eye on B\nA: giving access key normally a secondary considered option\nC: too complex to get logs while we have vpc flow logs\nD: same","timestamp":"1714453620.0"},{"upvote_count":"1","comment_id":"959796","content":"correct B.","poster":"ggrodskiy","timestamp":"1705959180.0"},{"upvote_count":"1","comment_id":"945055","timestamp":"1704589020.0","poster":"NikkyDicky","content":"Selected Answer: B\nits a B"},{"content":"Selected Answer: B\nB, in this link https://docs.aws.amazon.com/firehose/latest/dev/creating-the-stream-to-splunk.html#:~:text=In%20this%20part%20of%20the%20Kinesis%20Data%20Firehose%20tutorial%2C%20you%20create%20an%20Amazon%20Kinesis%20Data%20Firehose%20delivery%20stream%20to%20receive%20the%20log%20data%20from%20Amazon%20CloudWatch%20and%20deliver%20that%20data%20to%20Splunk., the traffic flow is: CW logs-> Kinesis Datafirehose delivery-> Splunk. In our case, we need custom logs, so need to subscribe VPC flow logs to send to splunk for specific monitoring","upvote_count":"1","poster":"Christina666","comment_id":"944104","timestamp":"1704494280.0"},{"content":"Selected Answer: B\nAnswer is B\nQuestion asks for \"near real time\" analysis\nFor near real time -->use Kinesis Datafirehose.\nFor real time ---> use Kineses data streams\nreal-time is instant, whereas near real-time is delayed","comment_id":"941322","upvote_count":"2","poster":"SkyZeroZx","timestamp":"1704247800.0"},{"comment_id":"934778","timestamp":"1703630220.0","poster":"SmileyCloud","upvote_count":"3","content":"Selected Answer: B\nIt's B - Rest is too complex. https://docs.aws.amazon.com/firehose/latest/dev/creating-the-stream-to-splunk.html"},{"poster":"PhuocT","timestamp":"1703409480.0","content":"Selected Answer: B\nB is answer, I think","upvote_count":"1","comment_id":"932298"},{"content":"Selected Answer: B\nB. https://docs.aws.amazon.com/firehose/latest/dev/vpc-splunk-tutorial.html","poster":"ozelllll","upvote_count":"2","timestamp":"1703403120.0","comment_id":"932199"},{"comment_id":"932055","timestamp":"1703386620.0","poster":"gd1","upvote_count":"4","content":"Selected Answer: B\nGPT - Amazon VPC Flow Logs can be enabled to capture information about the IP traffic going to and from network interfaces in the VPC. Flow log data can be published to Amazon CloudWatch Logs and Amazon S3. Once the logs are in CloudWatch, you can create a subscription filter that forwards events to a Kinesis Data Firehose stream.\nAWS Lambda can preprocess records in the Kinesis Data Firehose stream before they are delivered to Splunk.This solution provides near-real-time delivery of VPC Flow Logs to Splunk.Other options are less optimal because they involve unnecessary complexity or do not provide near-real-time monitoring."}],"answer":"B"},{"id":"sUibEnqev08S7pKIcsmf","url":"https://www.examtopics.com/discussions/amazon/view/113142-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-06-24 03:08:00","choices":{"C":"Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the United States. Apply the SCP to the OU.","B":"Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation.","D":"Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU.","E":"Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost.","F":"Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role.","A":"Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket."},"topic":"1","answers_community":["BDE (85%)","Other"],"question_text":"A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the development teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide the information to the company's finance team.\n\nThe company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States. However, some resources have been created in other Regions.\n\nA solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the accounts. The solution also must ensure that the company can create resources only in Regions in the United States.\n\nWhich combination of steps will meet these requirements in the MOST operationally efficient way? (Choose three.)","discussion":[{"content":"Selected Answer: BDE\nB - You need AWS Orgs to manage all other accts\nD - You need to deny creating resources\nE - You create the role in the mgmt acct not in each AWS acct. That's the point of the mgmt acct.","poster":"SmileyCloud","comments":[{"poster":"Arnaud92","timestamp":"1708791600.0","comment_id":"989257","content":"I'm not sure for E. The management account in AWS Organisations is to manage membres account and policies but not roles. I'll go for F instead.","upvote_count":"2"}],"upvote_count":"9","timestamp":"1703630520.0","comment_id":"934784"},{"content":"Selected Answer: BDE\nRemember SCP Only deny not allow ( in definition )","upvote_count":"8","poster":"SkyZeroZx","timestamp":"1704752880.0","comment_id":"946726"},{"timestamp":"1732201800.0","upvote_count":"2","comment_id":"1214964","content":"Selected Answer: BDE\nAnswer is BDE withouth any doubt!","poster":"red_panda"},{"poster":"Wardove","content":"Selected Answer: BDE\nNot C because there is no word about default SCP removal.\nFullAWSAccess - without an explicit deny SCP would not suffice the requirement","upvote_count":"2","comment_id":"1146346","timestamp":"1723292040.0"},{"timestamp":"1722868080.0","poster":"veyisceylan","upvote_count":"1","comment_id":"1141278","content":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html\nNotes\nAn Allow statement in an SCP permits the Resource element to only have a \"*\" entry.\nAn Allow statement in an SCP can't have a Condition element at all.\n\nTherefore Option C is not possible"},{"content":"BCE and it aligns with what ChatGpt thinks","timestamp":"1718938020.0","upvote_count":"1","comment_id":"1102186","poster":"GoKhe"},{"content":"ABD -ANS\nA. Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket.\nB. Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation.\nD. Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU.","upvote_count":"2","poster":"duriselvan","timestamp":"1717901220.0","comment_id":"1091457"},{"upvote_count":"1","comment_id":"1084564","poster":"shaaam80","timestamp":"1717080660.0","content":"Selected Answer: BDE\nAnswer - BDE"},{"comment_id":"1077960","content":"Selected Answer: BDE\nExplicit Deny is more strict than Explicit Allow - As member account can add allow creation of resources in other regions.","poster":"career360guru","upvote_count":"6","timestamp":"1716420240.0"},{"comments":[{"comment_id":"1072264","upvote_count":"2","poster":"Tofu13","content":"Not exactly overwritten. If you allow the creation in certain regions in the SCP, all member accounts are allowed to create instances in the region. But each member account can add IAM policies to allow to create them in different regions as well, unless there is an explicit deny. Therefore only D works.","timestamp":"1715840100.0"}],"poster":"NikkyDicky","upvote_count":"1","timestamp":"1704602100.0","content":"Selected Answer: BDE\nBDE - going with the crowd, although C seems like it'd work too. Is the issue that it can be overriden at account level?","comment_id":"945176"},{"timestamp":"1704494580.0","comment_id":"944106","upvote_count":"1","content":"Selected Answer: BDE\nBDE\n\nOrg -> enable all feature-> invite all member account-> member account accept invitation\nOrg-> mgmt account-> create IAM role to access to member account-> login member account assume this role to view billings","poster":"Christina666"},{"timestamp":"1704248820.0","comment_id":"941328","poster":"SkyZeroZx","content":"Selected Answer: BDE\nFor C, do an allow statement with StringEqual, for D, do a deny statement with StringNotEqual of US region. So C & D are both right.\nCost Explorer has all the reports, creating a S3 is NOT operationally efficient – A is out\nIAM role is needed to view billing - E","upvote_count":"1"},{"comment_id":"938387","poster":"javitech83","timestamp":"1703876400.0","upvote_count":"1","content":"Selected Answer: BDE\ncorrect answer is BDE"},{"poster":"easytoo","comment_id":"934478","upvote_count":"1","content":"b-c-e...b-c-e","timestamp":"1703602980.0"},{"content":"Selected Answer: BDE\nFor C, do an allow statement with StringEqual, for D, do a deny statement with StringNotEqual of US region. So C & D are both right.\nCost Explorer has all the reports, creating a S3 is NOT operationally efficient – A is out\nIAM role is needed to view billing - E","poster":"nexus2020","timestamp":"1703428020.0","comment_id":"932536","upvote_count":"2"},{"comment_id":"932301","poster":"PhuocT","content":"B, D an E","timestamp":"1703409720.0","upvote_count":"1"},{"comment_id":"932204","timestamp":"1703403360.0","upvote_count":"2","content":"Selected Answer: BDF\nit's BDF","poster":"ozelllll"},{"content":"Selected Answer: ABD\nOption A suggests using AWS Cost and Usage Reports to automatically generate and store consolidated monthly cost reports in an S3 bucket that is accessible to the finance team. B. Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. D. Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU.","comment_id":"932066","poster":"gd1","upvote_count":"4","timestamp":"1703387280.0"}],"answer_description":"","answer_ET":"BDE","question_images":[],"answer_images":[],"isMC":true,"answer":"BDE","exam_id":33,"question_id":163,"unix_timestamp":1687568880},{"id":"qh9fFOkRWqsOCm8rl9GL","exam_id":33,"unix_timestamp":1687464540,"answer_description":"","question_text":"A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires read-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security team.\n\nHow should a solutions architect meet these requirements?","answer_ET":"B","answer_images":[],"topic":"1","discussion":[{"poster":"bhanus","comment_id":"930994","timestamp":"1687464540.0","upvote_count":"7","content":"Selected Answer: B\nB is right\nA is incorrect as you CANNOT establish a trust relationship between the IAM policy and account \nC and D does NOT talk about readonly access"},{"comment_id":"944112","timestamp":"1688590080.0","content":"Selected Answer: B\nSo there is 3 parts, security account, member account, org account\n\nGoal: Security account-> member account\nIn org account, use org crossAccountAccessRole-> create ReadOnlyRole in member account\nBuild trust: security account & member account\nSecurity account assume member account ReadOnlyRole","upvote_count":"5","poster":"Christina666"},{"timestamp":"1741333620.0","poster":"albert_kuo","upvote_count":"1","comment_id":"1366178","content":"Selected Answer: B\nD is incorrect. OrganizationAccountAccessRole will have AdministratorAccess privileage."},{"poster":"duriselvan","upvote_count":"1","comment_id":"1093657","comments":[{"comment_id":"1271555","upvote_count":"1","poster":"helloworldabc","content":"just B","timestamp":"1724483160.0"},{"upvote_count":"3","timestamp":"1702599780.0","poster":"0c118eb","content":"OrganizationAccountAccessRole by default has AdministratorAccess IAM policy attached. The security team should only get Read Only. Best practice for accounts within an organization is B.","comment_id":"1096927"}],"content":"D Ans\nD. STS AssumeRole with OrganizationAccountAccessRole in Member Account:\n\nPros:\nFollows best practices for cross-account access using temporary credentials.\nMinimizes complexity by leveraging the pre-existing OrganizationAccountAccessRole.\nCons:\nSecurity team needs access to each member account to assume the role.\nTherefore, option D, using AWS STS to call the AssumeRole API for the OrganizationAccountAccessRole in each member account from the security account, is the most secure and efficient solution. This approach leverages existing IAM roles, minimizes configuration overhead, and adheres to best practices for cross-account access using temporary credentials.","timestamp":"1702311120.0"},{"upvote_count":"1","poster":"career360guru","timestamp":"1700702940.0","content":"Selected Answer: B\nOption B","comment_id":"1077965"},{"comment_id":"959789","content":"Correct B.","timestamp":"1690053120.0","poster":"ggrodskiy","upvote_count":"1"},{"content":"Selected Answer: B\nits a b","poster":"NikkyDicky","timestamp":"1688697600.0","comment_id":"945182","upvote_count":"2"},{"comment_id":"934792","timestamp":"1687813680.0","upvote_count":"1","poster":"SmileyCloud","content":"Selected Answer: B\nB - You need a role."},{"upvote_count":"1","comment_id":"934485","poster":"easytoo","content":"b-b-b-b-b-b-b","timestamp":"1687784940.0"},{"content":"Selected Answer: B\nB is classic usage of Cross Account Role","poster":"SkyZeroZx","upvote_count":"1","comment_id":"933829","timestamp":"1687719420.0"},{"timestamp":"1687641060.0","content":"oh labiba is 'B'\n\nTo meet the requirements, a solutions architect should choose option B. Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.\n\nBy using the OrganizationAccountAccessRole IAM role, the solutions architect can create a new IAM role with read-only access in each member account. This allows the security team to have read-only access to all accounts from their own AWS account. The trust relationship between the IAM role in each member account and the security account ensures that the security team can assume the IAM role and access the necessary resources.","upvote_count":"2","comment_id":"932935","poster":"Jackhemo"},{"timestamp":"1687591380.0","comment_id":"932304","poster":"PhuocT","upvote_count":"1","content":"B is the answer"},{"poster":"gd1","comment_id":"932068","timestamp":"1687569240.0","upvote_count":"3","content":"Selected Answer: B\nGPT: This approach aligns with the AWS best practice of using IAM roles to delegate permissions across AWS accounts. The OrganizationAccountAccessRole is a role that is automatically created when you create a new account in an organization. This role can be assumed by the master account, but it can also be assumed by other accounts if a trust relationship is established."},{"poster":"Alabi","upvote_count":"2","timestamp":"1687568700.0","comment_id":"932064","content":"Selected Answer: B\nOption B suggests using the OrganizationAccountAccessRole IAM role to create a new IAM role in each member account. This IAM role will have read-only access permissions. By establishing a trust relationship between the IAM role in each member account and the security account, the security team's AWS account is granted access to the member accounts."}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/113012-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":164,"isMC":true,"choices":{"B":"Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.","D":"Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the member account from the security account. Use the generated temporary credentials to gain access.","A":"Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a trust relationship between the IAM policy in each member account and the security account. Ask the security team to use the IAM policy to gain access.","C":"Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the management account from the security account. Use the generated temporary credentials to gain access."},"answer":"B","timestamp":"2023-06-22 22:09:00","answers_community":["B (100%)"]},{"id":"SGDu2BOcYuQbRyxFbu1c","topic":"1","question_text":"A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.\n\nA solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.\n\nWhich set of additional steps should the solutions architect take to meet these requirements?","question_id":165,"answer_ET":"B","choices":{"B":"Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet.","A":"Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.","D":"Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.","C":"Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet."},"answer_description":"","discussion":[{"timestamp":"1741117500.0","comment_id":"1365090","upvote_count":"1","content":"Selected Answer: A\nthere are a lot of unknowns ( IS there AWS organization so we can use Ram to share TGW or if these VPC's are in the same region). If we think about sharing there are supposed be AWS organizations and RAM","poster":"sergza888"},{"comment_id":"1222000","poster":"mns0173","content":"With hundreds of VPCs you will inevitably face CIDR overlapping conflict so better to use Transit Gateway","upvote_count":"1","timestamp":"1717141080.0"},{"comment_id":"1208744","timestamp":"1715240700.0","content":"Selected Answer: A\nThere's a key information that is not mentioned in the question, if the VPCs are in the same region or in different regions, as we're talking of hundreds of AWS accounts the answer will be VPC peering as a single transit gateway doesn´t support different regions so A. If all VPCs are in the same region, the answer would be transit gateway so B. Saying that I go for A","poster":"teo2157","comments":[{"comment_id":"1271557","timestamp":"1724483220.0","content":"just B","poster":"helloworldabc","upvote_count":"1"}],"upvote_count":"1"},{"timestamp":"1700703300.0","content":"Selected Answer: B\nOption B","upvote_count":"1","comment_id":"1077968","poster":"career360guru"},{"comment_id":"1058475","timestamp":"1698732480.0","upvote_count":"4","content":"Selected Answer: B\n\"hundreds of AWS account\" - think of transit gateway, VPC peering, PrivateLink should be out\noption C: add transit gateway to each account -> out","poster":"joleneinthebackyard"},{"comment_id":"959788","timestamp":"1690053060.0","poster":"ggrodskiy","upvote_count":"1","content":"Correct B."},{"upvote_count":"1","poster":"NikkyDicky","comment_id":"945186","content":"Selected Answer: B\nb for sure","timestamp":"1688697720.0"},{"poster":"Christina666","comment_id":"944113","content":"Selected Answer: B\nhundreds of VPCs-> TGW\nthen we only have B and C\nC: create TGW in each account, wrong","timestamp":"1688590380.0","upvote_count":"4"},{"content":"Selected Answer: B\nB - Hub and spoke is based on transit GW","poster":"SmileyCloud","comment_id":"934799","upvote_count":"2","timestamp":"1687815420.0"},{"poster":"easytoo","comment_id":"934486","upvote_count":"2","timestamp":"1687785060.0","content":"b-b-b-b-b-b-b"},{"upvote_count":"1","poster":"PhuocT","comment_id":"932306","timestamp":"1687591500.0","content":"yep, it's B"},{"timestamp":"1687570260.0","poster":"Alabi","comment_id":"932081","upvote_count":"3","content":"Selected Answer: B\nOption B suggests creating a transit gateway, which acts as a hub for connectivity between multiple VPCs and on-premises networks. By sharing the transit gateway with the existing AWS accounts, the solutions architect can attach the VPCs, including the spoke VPCs, to the transit gateway. The required routing can then be configured to direct traffic from the spoke VPCs to the transit gateway, which will route it to the egress VPC with the NAT gateway. This allows for centralized routing and connectivity to the internet for the spoke VPCs."},{"content":"Selected Answer: B\nGPT = B; AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. It simplifies the management of network connectivity across a large number of accounts/VPCs.","poster":"gd1","upvote_count":"1","timestamp":"1687569540.0","comment_id":"932072"},{"poster":"jubileu84","timestamp":"1687539240.0","upvote_count":"1","comment_id":"931800","content":"B is correct because we have hundreds of vpcs and default quota for peering peer vpc is = 50"},{"timestamp":"1687464420.0","upvote_count":"1","comment_id":"930992","content":"Selected Answer: B\nSHould be B","poster":"bhanus"}],"answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/113010-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["B (89%)","11%"],"timestamp":"2023-06-22 22:07:00","question_images":[],"answer":"B","exam_id":33,"unix_timestamp":1687464420}],"exam":{"id":33,"isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","numberOfQuestions":529,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isMCOnly":true,"isBeta":false},"currentPage":33},"__N_SSP":true}