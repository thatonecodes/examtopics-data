{"pageProps":{"questions":[{"id":"PHpvy94feqZA3kGcHsg9","url":"https://www.examtopics.com/discussions/amazon/view/74005-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":61,"exam_id":20,"answers_community":["C (100%)"],"answer_images":[],"choices":{"D":"Archive indices that are older than 3 months by using Index State Management (ISM) to create a policy to migrate the indices to Amazon OpenSearch Service (Amazon Elasticsearch Service) UltraWarm storage. When the audit team requires the older data, migrate the indices in UltraWarm storage back to hot storage.","A":"Archive indices that are older than 3 months by using Index State Management (ISM) to create a policy to store the indices in Amazon S3 Glacier. When the audit team requires the archived data, restore the archived indices back to the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.","B":"Archive indices that are older than 3 months by taking manual snapshots and storing the snapshots in Amazon S3. When the audit team requires the archived data, restore the archived indices back to the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.","C":"Archive indices that are older than 3 months by using Index State Management (ISM) to create a policy to migrate the indices to Amazon OpenSearch Service (Amazon Elasticsearch Service) UltraWarm storage."},"answer_description":"","isMC":true,"question_images":[],"answer":"C","question_text":"A retail company stores order invoices in an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster Indices on the cluster are created monthly.\nOnce a new month begins, no new writes are made to any of the indices from the previous months. The company has been expanding the storage on the Amazon\nOpenSearch Service (Amazon Elasticsearch Service) cluster to avoid running out of space, but the company wants to reduce costs. Most searches on the cluster are on the most recent 3 months of data, while the audit team requires infrequent access to older data to generate periodic reports. The most recent 3 months of data must be quickly available for queries, but the audit team can tolerate slower queries if the solution saves on cluster costs\nWhich of the following is the MOST operationally efficient solution to meet these requirements?","timestamp":"2022-04-21 12:20:00","discussion":[{"content":"C is okay. No need to return back to hot storage\n\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/ultrawarm.html\n\n\"UltraWarm nodes use Amazon S3 and a sophisticated caching solution to improve performance. For indexes that you are not actively writing to, query less frequently, and don't need the same performance from, UltraWarm offers significantly lower costs per GiB of data. Because warm indexes are read-only unless you return them to hot storage, UltraWarm is best-suited to immutable data, such as logs.\n\nIn OpenSearch, warm indexes behave just like any other index. You can query them using the same APIs or use them to create visualizations in OpenSearch Dashboards.\"","comment_id":"590704","timestamp":"1650730080.0","upvote_count":"6","poster":"CHRIS12722222"},{"upvote_count":"5","comment_id":"590952","poster":"astalavista1","content":"Selected Answer: C\nC - As ISM can move it from Hot to Ultrawarm and uses S3 in the background for Storage(hence not A to reduce operational overhead), you don't need to move it back to Hot as it is used by the Audit team for auditing meaning only read hence the reason it's not D.","timestamp":"1650791280.0"},{"upvote_count":"4","poster":"pk349","timestamp":"1682970240.0","comment_id":"886680","content":"C: I passed the test"},{"content":"Selected Answer: C\nSelected Answer: C","upvote_count":"2","poster":"rocky48","comment_id":"643521","timestamp":"1659817980.0"},{"poster":"rb39","content":"Selected Answer: C\nC - data is always available in ultra warm storage, just queries are slower, which matches the statement requirements","timestamp":"1650536400.0","comment_id":"589254","upvote_count":"2"}],"unix_timestamp":1650536400,"answer_ET":"C","topic":"1"},{"id":"rITlcefGbNc6TNNVp8DN","question_images":[],"answers_community":["B (100%)"],"answer_images":[],"question_id":62,"answer_description":"","answer_ET":"B","choices":{"C":"Register the S3 locations with AWS Lake Formation. Create an AWS Glue job to create an ETL workflow that removes the PII columns from the data and creates a separate copy of the data in another data lake S3 bucket. Register the new S3 locations with Lake Formation. Grant users the permissions to each data lake data based on whether the users are authorized to see PII data.","A":"Define a bucket policy for each S3 bucket of the data lake to allow access to users who have authorization to see PII data. Catalog the data by using AWS Glue. Create two IAM roles. Attach a permissions policy with access to PII columns to one role. Attach a policy without these permissions to the other role.","B":"Register the S3 locations with AWS Lake Formation. Create two IAM roles. Use Lake Formation data permissions to grant Select permissions to all of the columns for one role. Grant Select permissions to only columns that contain non-PII data for the other role.","D":"Register the S3 locations with AWS Lake Formation. Create two IAM roles. Attach a permissions policy with access to PII columns to one role. Attach a policy without these permissions to the other role. For each downstream analytics service, use its native security functionality and the IAM roles to secure the PII data."},"discussion":[{"timestamp":"1650536220.0","comment_id":"589250","poster":"rb39","comments":[{"comment_id":"807156","upvote_count":"1","timestamp":"1676274900.0","poster":"Merrick","content":"https://docs.aws.amazon.com/lake-formation/latest/dg/managing-permissions.html\nLake Formation provides central access controls for data in your data lake. You can define security policy-based rules for your users and applications by role in Lake Formation, and integration with AWS Identity and Access Management authenticates those users and roles. Once the rules are defined, Lake Formation enforces your access controls at table and column-level granularity for users of Amazon Redshift Spectrum and Amazon Athena."}],"upvote_count":"9","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/lake-formation-permissions.html"},{"upvote_count":"6","poster":"CHRIS12722222","content":"B is good","timestamp":"1650733140.0","comment_id":"590720"},{"poster":"pk349","upvote_count":"3","timestamp":"1682970300.0","comment_id":"886681","content":"B: I passed the test"},{"content":"I agree B, but we actually use C in my current workplace datalake. Reason being retention/purging/privacy requests - need to isolate the data not just manage permissions to view it.","poster":"np2021","timestamp":"1679144580.0","upvote_count":"1","comment_id":"842783"},{"content":"Answer B AWS Lake Formation column-level permissions can be used to restrict access to specific columns in a table. When a user retrieves metadata about the table using the console or an API like glue:GetTable , the column list in the table object contains only the fields to which they have access.","poster":"VijiTu","timestamp":"1667971260.0","comment_id":"714312","upvote_count":"2"},{"timestamp":"1658381460.0","poster":"rocky48","comment_id":"634378","upvote_count":"2","content":"Selected Answer: B\nAnswer: B"},{"poster":"ru4aws","upvote_count":"2","content":"Selected Answer: B\nColumn Level permissions is possible only with Lake Formation Fine grained access controls","comment_id":"631615","timestamp":"1657860780.0"},{"comment_id":"595267","content":"Answer: B","timestamp":"1651350420.0","upvote_count":"1","poster":"jrheen"}],"topic":"1","answer":"B","unix_timestamp":1650536220,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74004-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2022-04-21 12:17:00","question_text":"A financial services company is building a data lake solution on Amazon S3. The company plans to use analytics offerings from AWS to meet user needs for one- time querying and business intelligence reports. A portion of the columns will contain personally identifiable information (PII) Only authorized users should be able to see plaintext PII data.\nWhat is the MOST operationally efficient solution that meets these requirements?","exam_id":20},{"id":"T7vZhCnnq4008wiVrV8g","question_images":[],"answers_community":["DE (90%)","10%"],"answer_images":[],"question_id":63,"answer_description":"","choices":{"A":"Place the small files into one S3 folder. Define one single table for the small S3 files in AWS Glue Data Catalog. Rerun the AWS Glue ETL jobs against this AWS Glue table.","D":"Use the groupFiles setting in the AWS Glue ETL job to merge small S3 files and rerun AWS Glue ETL jobs.","B":"Create an AWS Lambda function to merge small S3 files and invoke them periodically. Run the AWS Glue ETL jobs after successful completion of the Lambda function.","E":"Update the Kinesis Data Firehose S3 buffer size to 128 MB. Update the buffer interval to 900 seconds.","C":"Run the S3DistCp utility in Amazon EMR to merge a large number of small S3 files before running the AWS Glue ETL jobs."},"answer_ET":"DE","discussion":[{"content":"DE: I passed the test","upvote_count":"6","comment_id":"886682","poster":"pk349","timestamp":"1682970420.0"},{"comment_id":"773808","content":"The buffer sizes hints range from 1 MbB to 128 MbB for Amazon S3 delivery. For Amazon OpenSearch Service (OpenSearch Service) delivery, they range from 1 MB to 100 MB. For AWS Lambda processing, you can set a buffering hint between 0.2 MB and up to 3 MB using the BufferSizeInMBs processor parameter. The size threshold is applied to the buffer before compression. These options are treated as hints. Kinesis Data Firehose might choose to use different values when it is optimal.\n\nThe buffer interval hints range from 60 seconds to 900 seconds.","upvote_count":"1","timestamp":"1673550480.0","poster":"Chelseajcole"},{"timestamp":"1666285920.0","comments":[{"poster":"JoellaLi","comment_id":"704755","content":"Link: https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html","upvote_count":"1","timestamp":"1666794120.0"}],"poster":"MultiCloudIronMan","comment_id":"700154","upvote_count":"1","content":"DE see extract from AWS \"You can configure the values for Amazon S3 Buffer size (1–128 MB) or Buffer interval (60–900 seconds). The condition satisfied first triggers data delivery to Amazon S3. When data delivery to the destination falls behind data writing to the delivery stream, Kinesis Data Firehose raises the buffer size dynamically.\""},{"comment_id":"689840","upvote_count":"3","timestamp":"1665279960.0","content":"DE. \nA – Won’t resolve this issue.\nB – Lambda has additional cost.\nC – S3DistCp has an additional cost.\nD – It’s covered in Glue ETL cost.\nE – Setting won’t have an additional cost.","poster":"jazzok"},{"content":"Selected Answer: BD\nI doubt \"E\" is an answer, because max buffer time of KDF is 5 mins. I would go with BD","upvote_count":"1","timestamp":"1659756600.0","poster":"dushmantha","comments":[{"content":"DE - KDF max buffer time is 900 seconds (15 mins)\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/#:~:text=Kinesis%20Data%20Firehose%20buffers%20incoming,data%20delivery%20to%20Amazon%20S3.","upvote_count":"2","poster":"mawsman","comment_id":"846253","timestamp":"1679424480.0"}],"comment_id":"643174"},{"content":"Selected Answer: DE\nSelected Answer: DE","poster":"rocky48","comment_id":"634935","upvote_count":"2","timestamp":"1658454360.0"},{"content":"Selected Answer: DE\nGrouping files together reduces the memory footprint on the Spark driver as well as simplifying file split orchestration.\nBuffer size increase avoids creating small size files","upvote_count":"2","comment_id":"631700","poster":"ru4aws","timestamp":"1657882440.0"},{"comment_id":"617217","upvote_count":"1","timestamp":"1655379240.0","content":"Another good article discussing Out of memory issue; https://aws.amazon.com/premiumsupport/knowledge-center/glue-oom-java-heap-space-error/","poster":"Ramshizzle"},{"comment_id":"612250","timestamp":"1654502520.0","upvote_count":"2","content":"Selected Answer: DE\nDE.\nGrouping files together reduces the memory footprint on the Spark driver as well as simplifying file split orchestration. Without grouping, a Spark application must process each file using a different Spark task. Each task must then send mapStatus object containing the location information to the Spark driver. \nhttps://aws.amazon.com/blogs/big-data/optimize-memory-management-in-aws-glue/","poster":"khchan123"},{"timestamp":"1651355100.0","content":"Answer-D,E","comment_id":"595306","poster":"jrheen","upvote_count":"1"},{"timestamp":"1651213440.0","upvote_count":"3","content":"Selected Answer: DE\nbuffer increase increases file size\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/?nc1=h_ls\ngroupFiles helps reading small files\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html","comment_id":"594264","poster":"Teraxs"},{"comment_id":"590722","poster":"CHRIS12722222","content":"DE seems good","upvote_count":"1","timestamp":"1650733560.0"}],"topic":"1","answer":"DE","unix_timestamp":1650733560,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74257-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2022-04-23 19:06:00","exam_id":20,"question_text":"A gaming company is building a serverless data lake. The company is ingesting streaming data into Amazon Kinesis Data Streams and is writing the data to\nAmazon S3 through Amazon Kinesis Data Firehose. The company is using 10 MB as the S3 buffer size and is using 90 seconds as the buffer interval. The company runs an AWS Glue ETL job to merge and transform the data to a different format before writing the data back to Amazon S3.\nRecently, the company has experienced substantial growth in its data volume. The AWS Glue ETL jobs are frequently showing an OutOfMemoryError error.\nWhich solutions will resolve this issue without incurring additional costs? (Choose two.)"},{"id":"GtYbI032FTxfFrBi5ZKP","answer":"D","timestamp":"2022-04-21 12:01:00","answers_community":["D (100%)"],"topic":"1","answer_description":"","unix_timestamp":1650535260,"answer_images":[],"question_images":[],"answer_ET":"D","discussion":[{"poster":"rb39","upvote_count":"15","timestamp":"1650535260.0","content":"Selected Answer: D\nThe FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly.\nReference:\nhttps://docs.aws.amazon.com/glue/latest/dg/machine-learning.html","comment_id":"589243"},{"comment_id":"886683","upvote_count":"1","timestamp":"1682970480.0","content":"D: I passed the test","poster":"pk349"},{"comment_id":"833107","timestamp":"1678288980.0","upvote_count":"1","poster":"Hypnoss","content":"why not C?","comments":[{"poster":"rsn","upvote_count":"3","content":"partitioning based on name may not be a great idea. It will result in too many partitions","timestamp":"1679190780.0","comment_id":"843344"}]},{"poster":"rocky48","content":"Selected Answer: D\nSelected Answer: D","upvote_count":"3","timestamp":"1658807520.0","comment_id":"637130"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74002-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":64,"choices":{"D":"Train and use the AWS Glue FindMatches ML transform in the ETLjob","C":"Partition tables and use the ETL job to partition the data on patient name","B":"Train and use the AWS Glue PySpark filter class in the ETLjob","A":"Use Amazon Macie pattern matching as part of the ETLjob"},"question_text":"A healthcare company ingests patient data from multiple data sources and stores it in an Amazon S3 staging bucket. An AWS Glue ETL job transforms the data, which is written to an S3-based data lake to be queried using Amazon Athena. The company wants to match patient records even when the records do not have a common unique identifier.\nWhich solution meets this requirement?","exam_id":20},{"id":"apNOlJVAYKsDaNEwSFmL","answer":"B","question_images":[],"choices":{"D":"Configure the Kafka-Kinesis-Connector to publish the data to an Amazon Kinesis Data Firehose delivery stream that is configured to store the data in Amazon S3. Configure an AWS Glue crawler to crawl the data. Use an Amazon Athena data source with QuickSight Standard edition to refresh the data in SPICE hourly and create a dynamic dashboard with forecasting and ML insights.","B":"Replace Kafka with an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to consume the data and store the data in Amazon S3. Use QuickSight Enterprise edition to refresh the data in SPICE from Amazon S3 hourly and create a dynamic dashboard with forecasting and ML insights.","A":"Replace Kafka with Amazon Managed Streaming for Apache Kafka. Ingest the data by using AWS Lambda, and store the data in Amazon S3. Use QuickSight Standard edition to refresh the data in SPICE from Amazon S3 hourly and create a dynamic dashboard with forecasting and ML insights.","C":"Configure the Kafka-Kinesis-Connector to publish the data to an Amazon Kinesis Data Firehose delivery stream that is configured to store the data in Amazon S3. Use QuickSight Enterprise edition to refresh the data in SPICE from Amazon S3 hourly and create a dynamic dashboard with forecasting and ML insights."},"answer_description":"","topic":"1","unix_timestamp":1650492000,"discussion":[{"upvote_count":"2","timestamp":"1704652320.0","poster":"eccc7e6","comment_id":"1116085","content":"Selected Answer: B\nA is incorrect because QuickSight Standard Edition does not support ML insights.\nC is incorrect because this does not solve the scalability problems since it keeps part of the current architecture.\nD is incorrect for the same reasons as A and C.\nAlso KDS is cheaper than MSK."},{"content":"kafka-connecter is very expensive.","poster":"LocalHero","upvote_count":"1","comment_id":"1066962","timestamp":"1699594860.0"},{"timestamp":"1699236780.0","poster":"LocalHero","content":"A and B is not correct . \nQuickSight SE is not supported ML insights.\nML insights needs QuickSIght EE.\nC is not correct.\nKafka-connecter is high cost solution.","upvote_count":"1","comment_id":"1063463"},{"content":"Selected Answer: B\n\"The solution also must correct the scalability problems\": so, can't be C","upvote_count":"1","comment_id":"946260","poster":"ccpmad","timestamp":"1688799720.0"},{"content":"B: I passed the test","upvote_count":"1","comment_id":"886684","timestamp":"1682970540.0","poster":"pk349"},{"poster":"Chelseajcole","timestamp":"1673551740.0","content":"Selected Answer: B\nC is wrong as it stated: The solution also must correct the scalability problems that the company experiences when it uses its current architecture to ingest data.\n\nWe gonna using pure AWS solution. Using connector cannot solve Apache Kafka scale problem unless they using MSK","upvote_count":"3","comment_id":"773828"},{"poster":"nadavw","timestamp":"1671005400.0","upvote_count":"1","content":"Selected Answer: C\nKafka-Kinesis-Connector for Firehose is used to publish messages from Kafka to one of the following destinations: Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service and in turn enabling near real time analytics with existing business intelligence tools and dashboards. Amazon Kinesis Firehose has ability to transform, batch, archive message onto S3 and retry if destination is unavailable. https://github.com/awslabs/kinesis-kafka-connector\n\n\nML insights available in enterprise edition","comment_id":"744857"},{"poster":"rav009","upvote_count":"1","comment_id":"706266","content":"Selected Answer: C\ncost-effectively is the key\nC will save the cost and B need change the codes from kafka to KDS.","timestamp":"1666945440.0"},{"timestamp":"1661364960.0","poster":"muhsin","upvote_count":"1","comments":[{"poster":"muhsin","comment_id":"651407","upvote_count":"2","content":"pardon, it is replacing kafka with KDS. so it is b","timestamp":"1661365020.0"}],"content":"I think it is C. Because we don't need to use KDS to get streams. \nKafka can publish the data to KDF directly with the connector.\n\nhttps://github.com/awslabs/kinesis-kafka-connector","comment_id":"651406"},{"comment_id":"634880","timestamp":"1658444880.0","content":"Selected Answer: B\nSelected Answer: B","upvote_count":"1","poster":"rocky48"},{"upvote_count":"3","comment_id":"628698","timestamp":"1657271880.0","poster":"dushmantha","content":"Selected Answer: B\nML insights available in enterprise edition. And hourly refresh of SPICE is also available in enterprise edition. So is gotta be B or C. Since the current solution is not scalable it need to be replaced. Hence B is the answer."},{"upvote_count":"1","content":"Answer - B","comment_id":"595276","timestamp":"1651351200.0","poster":"jrheen"},{"comment_id":"593835","poster":"Teraxs","upvote_count":"4","content":"Selected Answer: B\nEnterprise edition needed because of ML capabilities. Current solution not scalable, thus needs to be replaced, i.e. B","timestamp":"1651153020.0"},{"poster":"CHRIS12722222","timestamp":"1650734520.0","content":"B seems okay. ML insight is in enterprise edition","upvote_count":"2","comment_id":"590727"},{"upvote_count":"2","poster":"rb39","comment_id":"589242","content":"Selected Answer: B\nB - Kinesis is cheaper than Kafka","timestamp":"1650535200.0"}],"answer_ET":"B","exam_id":20,"answers_community":["B (89%)","11%"],"question_id":65,"timestamp":"2022-04-21 00:00:00","question_text":"A social media company is using business intelligence tools to analyze its data for forecasting. The company is using Apache Kafka to ingest the low-velocity data in near-real time. The company wants to build dynamic dashboards with machine learning (ML) insights to forecast key business trends. The dashboards must provide hourly updates from data in Amazon S3. Various teams at the company want to view the dashboards by using Amazon QuickSight with ML insights. The solution also must correct the scalability problems that the company experiences when it uses its current architecture to ingest data.\nWhich solution will MOST cost-effectively meet these requirements?","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74001-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_images":[]}],"exam":{"provider":"Amazon","numberOfQuestions":164,"lastUpdated":"11 Apr 2025","name":"AWS Certified Data Analytics - Specialty","isImplemented":true,"isMCOnly":true,"id":20,"isBeta":false},"currentPage":13},"__N_SSP":true}