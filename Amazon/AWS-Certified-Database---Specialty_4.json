{"pageProps":{"questions":[{"id":"Gdk98HCh2s4A8p1mpd6e","answer_ET":"A","topic":"1","question_text":"A large retail company recently migrated its three-tier ecommerce applications to AWS. The company's backend database is hosted on Amazon Aurora\nPostgreSQL. During peak times, users complain about longer page load times. A database specialist reviewed Amazon RDS Performance Insights and found a spike in IO:XactSync wait events. The SQL attached to the wait events are all single INSERT statements.\nHow should this issue be resolved?","exam_id":22,"url":"https://www.examtopics.com/discussions/amazon/view/48815-exam-aws-certified-database-specialty-topic-1-question-112/","choices":{"A":"Modify the application to commit transactions in batches","D":"Change the Aurora DB cluster storage to Provisioned IOPS (PIOPS).","C":"Add an Amazon ElastiCache for Redis cluster and change the application to write through.","B":"Add a new Aurora Replica to the Aurora DB cluster."},"unix_timestamp":1617370140,"discussion":[{"comments":[{"upvote_count":"4","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html\n\"This wait most often arises when there is a very high rate of commit activity on the system. You can sometimes alleviate this wait by modifying applications to commit transactions in batches. \"","poster":"gelsm","comment_id":"414408","timestamp":"1636119600.0"}],"poster":"shantest1","upvote_count":"14","timestamp":"1632637560.0","content":"A. answer","comment_id":"326682"},{"comment_id":"696119","content":"Selected Answer: A\nD - There is no option to setup PIOPS for Aurora. Only RDS has it","timestamp":"1665910920.0","comments":[{"poster":"Mintwater","upvote_count":"2","timestamp":"1681741800.0","content":"Thanks for your explanation for D","comment_id":"872792"}],"poster":"RBSK","upvote_count":"6"},{"poster":"adelcold","upvote_count":"1","timestamp":"1688169720.0","comment_id":"939462","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html\n\n Monitor your resources\n Scale up the CPU\n Increase network bandwidth\n Reduce the number of commits"},{"poster":"novice_expert","content":"Selected Answer: A\navoid high rate of commits by using batch commit","upvote_count":"2","comment_id":"595008","timestamp":"1651323120.0"},{"content":"Selected Answer: A\nAnswer is A.\n\nActions\n\nWe recommend different actions depending on the causes of your wait event.\n1.- Monitor your resources\n2.- Scale up the CPU\n3.- Increase network bandwidth\n4.- Reduce the number of commits\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html","comment_id":"538933","timestamp":"1643819820.0","upvote_count":"6","poster":"soyyodario"},{"upvote_count":"3","poster":"jove","comment_id":"506409","content":"Selected Answer: A\nTo reduce the number of commits, combine statements into transaction blocks.","timestamp":"1640117340.0"},{"comment_id":"491333","upvote_count":"1","poster":"Justu","timestamp":"1638341160.0","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html"},{"poster":"Suresh108","timestamp":"1634110800.0","upvote_count":"1","comment_id":"379455","content":"AAAAAA"},{"timestamp":"1633894980.0","content":"A final answer","comment_id":"360646","poster":"Aesthet","upvote_count":"1"},{"timestamp":"1633270440.0","content":"Yes, A is correct","upvote_count":"1","poster":"agrawalachin","comment_id":"344246"},{"comment_id":"342311","upvote_count":"1","poster":"manan728","timestamp":"1633132560.0","content":"Yup A is right for this use case"}],"answers_community":["A (100%)"],"question_id":16,"question_images":[],"timestamp":"2021-04-02 15:29:00","isMC":true,"answer_images":[],"answer":"A","answer_description":""},{"id":"w42247PL50rXutuCTSqV","question_id":17,"choices":{"C":"Use provisioned capacity. Create an AWS Application Auto Scaling policy to update capacity based on consumption.","D":"Use on-demand capacity.","B":"Use provisioned capacity. Set it to the capacity levels required for peak daytime throughput.","A":"Use reserved capacity. Set it to the capacity levels required for peak daytime throughput."},"question_images":[],"unix_timestamp":1617512940,"answers_community":["C (67%)","D (33%)"],"answer_description":"","exam_id":22,"discussion":[{"timestamp":"1696582380.0","upvote_count":"2","poster":"Sathish_dbs","content":"reduce cost - on-demand costlier\npredictable, gradual increase - auto scaling","comment_id":"1026375"},{"comment_id":"950600","poster":"leotoras","timestamp":"1689246840.0","upvote_count":"1","content":"C is correct based on documentation: When you create a DynamoDB table, auto scaling is the default capacity setting, but you can also enable auto scaling on any table that does not have it active. Behind the scenes, as illustrated in the following diagram, DynamoDB auto scaling uses a scaling policy in Application Auto Scaling. \nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/"},{"upvote_count":"1","comment_id":"916275","content":"Selected Answer: D\nIf C had been worded differently, like \"Provisioned capacity with auto-scaling\", I might have chosen C. But \"AWS Application Auto-Scaling\" does not make sense in the DynamoDB context. Therefore I choose \"On-Demand\" which is very flexible in terms of throughput.","poster":"aviathor","timestamp":"1686057120.0"},{"content":"CCCCCCCCCCCC","poster":"guau","comment_id":"801397","timestamp":"1675801080.0","upvote_count":"1"},{"upvote_count":"1","poster":"Arun32","timestamp":"1669277040.0","comment_id":"725648","content":"C for me too..\n\nDynamoDB auto scaling uses a scaling policy in Application Auto Scaling. To configure auto scaling in DynamoDB, you set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage. Auto scaling uses Amazon CloudWatch to monitor a table’s read and write capacity metrics. To do so, it creates CloudWatch alarms that track consumed capacity."},{"timestamp":"1666308180.0","comment_id":"700369","upvote_count":"2","content":"Selected Answer: D\nno traffic at night and orders of magnitude high traffic in off peak. I will go with D","poster":"awsjjj"},{"comment_id":"626528","upvote_count":"1","timestamp":"1656846000.0","poster":"kush_sumit","content":"Selected Answer: C\nC: As the pattern is predictable load with mangitude of spikes this could be handled by autoscaling. As per aws autoscaling is cost saving as compared to ondemand. If the pattern was unpredictable On-demand would be good"},{"content":"but it also says \"traffic increase during peak hours is steady and predictable\" and the firm also wants to reduce the throttling \" so it is either C or D \nI think .. \nC is more likely as traffic pattern is predictable","timestamp":"1656292800.0","comment_id":"622874","poster":"sachin","upvote_count":"1"},{"comment_id":"622871","content":"It states that there is lot of throttling though out the day and the firm wants to minimize the expenditure and cost. Provisioned is costlier and during night hours the traffic is almost nill.\nSo what ever small ( average of day ) capacity you are provisioning will be wasted at night. \nI believe on-demand in this case is better choice.","upvote_count":"1","timestamp":"1656291600.0","poster":"sachin"},{"poster":"Dantas","upvote_count":"1","content":"Selected Answer: C\n\"... traffic increase during peak hours is steady and predictable ...\"","comment_id":"608902","timestamp":"1653852180.0"},{"poster":"novice_expert","upvote_count":"3","content":"Selected Answer: C\n- traffic increase during peak hours is steady and predictable=> provisioned\n- no uses in night. => on demand (but costly)\n\nso provision some + auto scale with a target 70% utilization set \n\nD will also work but costly","timestamp":"1651279680.0","comment_id":"594755"},{"comment_id":"564835","poster":"mike3g2000","upvote_count":"2","content":"I go with C.\n\"To understand how DynamoDB auto scaling works, suppose that you have a table named ProductCatalog. The table is bulk-loaded with data infrequently, so it doesn't incur very much write activity. However, it does experience a high degree of read activity, which varies over time. By monitoring the Amazon CloudWatch metrics for ProductCatalog, you determine that the table requires 1,200 read capacity units (to avoid DynamoDB throttling read requests when activity is at its peak). You also determine that ProductCatalog requires 150 read capacity units at a minimum, when read traffic is at its lowest point.\n\nWithin the range of 150 to 1,200 read capacity units, you decide that a target utilization of 70 percent would be appropriate for the ProductCatalog table. Target utilization is the ratio of consumed capacity units to provisioned capacity units, expressed as a percentage. Application Auto Scaling uses its target tracking algorithm to ensure that the provisioned read capacity of ProductCatalog is adjusted as required so that utilization remains at or near 70 percent.\"","timestamp":"1646920980.0"},{"comments":[{"poster":"whn","content":"Remember for Provisioned with Auto-Scaling you are basically paying for throughput 24/7. Whereas for On-Demand Scaling you pay per request. This means for applications still in development or low traffic applications, it might be more economical to use On-Demand Scaling and not worry about provisioning throughput. However, at scale, this can quickly shift once you have a more consistent usage pattern.\nhttps://dynobase.dev/dynamodb-on-demand-vs-provisioned-scaling/#:~:text=Remember%20for%20Provisioned%20with%20Auto,not%20worry%20about%20provisioning%20throughput.","upvote_count":"2","comment_id":"582060","timestamp":"1649284020.0"},{"timestamp":"1685918160.0","upvote_count":"1","content":"The key is \"The traffic growth during peak hours is gradual and predictable on a daily basis\" provisioned with auto scaling","poster":"Paulv82003","comment_id":"914937"}],"content":"Very tricky. Feels like (D)\nEven of you enable Autoscaling - but you will still pay the same rate during off-peak hours. the provisioned capacity will never fall below the preconfigured one during night when theres no traffic. Plus the question WANTS to reduce expenditure - which means with OnDemand theres barely any charge during off peak hours compared to fully charged if u use provisioned capacity using thebaseline configured RCU/WCUs.","poster":"RotterDam","upvote_count":"3","comment_id":"561549","timestamp":"1646501760.0"},{"comment_id":"557469","content":"C\nProvisioned Mode\nIf you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto scaling to adjust your table’s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate in order to obtain cost predictability.\n\nProvisioned mode is a good option if any of the following are true:\n\nYou have predictable application traffic.\n\nYou run applications whose traffic is consistent or ramps gradually.\n\nYou can forecast capacity requirements to control costs.","poster":"user0001","timestamp":"1645983180.0","upvote_count":"4"},{"comment_id":"555625","timestamp":"1645742700.0","content":"Selected Answer: C\nThey know peak hours\nAWS Application Auto Scaling policy is good for it.","upvote_count":"1","poster":"tugboat"},{"upvote_count":"2","timestamp":"1642771140.0","poster":"Shinytopology","content":"C. DynamoDB autoscaling saves costs comparing to on-demand (which costs a bit higher for the NoOps benefit.) \nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/","comment_id":"529150"},{"content":"\"traffic increase during peak hours is steady and predictable\"\nC is the answer.","upvote_count":"1","poster":"wcx","timestamp":"1641583380.0","comment_id":"519176"},{"content":"C seems to be the best option :\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","timestamp":"1640112120.0","upvote_count":"2","poster":"jove","comment_id":"506359"},{"content":"C is the correct answer\nOn-demand mode is a good option if any of the following are true:\nYou create new tables with unknown workloads. You have unpredictable application traffic.\nYou prefer the ease of paying for only what you use. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","upvote_count":"2","timestamp":"1639048620.0","poster":"2025flakyt","comment_id":"497665"},{"poster":"Scunningham99","upvote_count":"1","content":"I would go with d","comment_id":"449467","timestamp":"1635457020.0"},{"content":"very close.. C or D","upvote_count":"2","poster":"guru_ji","timestamp":"1635447780.0","comment_id":"439810"},{"timestamp":"1635190020.0","upvote_count":"2","poster":"ChauPhan","comment_id":"433761","content":"\"The company wants to reduce the amount of throttling while minimizing costs.\"\nC and D both are OK, but C is saving cost solution."},{"upvote_count":"2","timestamp":"1635185340.0","content":"D is correct in this scenario, night load is very low, morning load is predicatable but going above and above. If you use provisioned capacity, you need to provide average of day and night utilization as fixed and autoscaling. Where capacity wasted with unused night times. Also provisioned capacity will not scale with in one hour to Random high spike. So, ondemand will give us as needed, also charges are high traffic means high, low traffic means low.","poster":"aws4myself","comment_id":"433375"},{"content":"C is correct\nAmazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","comment_id":"405437","timestamp":"1634823360.0","poster":"ExtHo","upvote_count":"4"},{"content":"I will go with C. Workload is predicted. So D cannot be the answer here.","poster":"AM","timestamp":"1634338320.0","comment_id":"375508","upvote_count":"2"},{"poster":"Aesthet","content":"C I guess","comment_id":"360652","upvote_count":"1","timestamp":"1634320320.0"},{"timestamp":"1634079360.0","poster":"AM","upvote_count":"1","content":"C looks correct. D was OK if cost minimizing was not the critetia","comment_id":"353345"},{"content":"I think this is a trick question: \nThe company initially provisioned capacity based on its average volume during the day without accounting for the variability in traffic patterns. However, the website is experiencing a significant amount of throttling during peak hours. \nThe company initially provisioned capacity but still experiencing a significant amount of throttling.\nIf they used provisioned capacity initially and it did not work well, then i think on-demand capacity might solve the problem.\nWhat do you think guys?\nAnswer is D if you agree with me.\nThanks!","timestamp":"1633880220.0","poster":"gsm1984","upvote_count":"2","comment_id":"350606"},{"content":"C is correct","timestamp":"1633839960.0","poster":"agrawalachin","comment_id":"344248","upvote_count":"1"},{"comment_id":"342314","timestamp":"1633053060.0","poster":"manan728","content":"C also because provisioned capacity is better for a cost optimization scenario.","upvote_count":"1"},{"poster":"Jaypdv","comment_id":"327753","timestamp":"1632963120.0","content":"C. since load and load profile are generally known","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/49004-exam-aws-certified-database-specialty-topic-1-question-113/","isMC":true,"topic":"1","question_text":"A company uses Amazon DynamoDB as the data store for its ecommerce website. The website receives little to no traffic at night, and the majority of the traffic occurs during the day. The traffic growth during peak hours is gradual and predictable on a daily basis, but it can be orders of magnitude higher than during off- peak hours.\nThe company initially provisioned capacity based on its average volume during the day without accounting for the variability in traffic patterns. However, the website is experiencing a significant amount of throttling during peak hours. The company wants to reduce the amount of throttling while minimizing costs.\nWhat should a database specialist do to meet these requirements?","timestamp":"2021-04-04 07:09:00","answer_ET":"C","answer":"C","answer_images":[]},{"id":"z7CicqI0Dm9cz9sKuSrB","topic":"1","question_text":"A company uses an Amazon RDS for PostgreSQL DB instance for its customer relationship management (CRM) system. New compliance requirements specify that the database must be encrypted at rest.\nWhich action will meet these requirements?","question_id":18,"answers_community":["A (89%)","11%"],"exam_id":22,"discussion":[{"upvote_count":"13","comment_id":"326705","poster":"shantest1","content":"A. Answer","comments":[{"upvote_count":"1","timestamp":"1638143280.0","comment_id":"489497","poster":"scottkerker","content":"This page has detailed steps for MySQL and Maria in terms of the encryption of an unencrypted RDS instance.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-encrypt-instance-mysql-mariadb/"},{"poster":"toppic26","comment_id":"476866","comments":[{"comment_id":"514503","comments":[{"comment_id":"565718","timestamp":"1647022080.0","upvote_count":"1","content":"A. Answer -> Answering to my own question: No! I've just tried to restore unencrypted manual and automatic snapshots into an encrypted db instance and it isn't allowed. If you want to launch an encrypted rds instance, you need to create an encrypted copy of the unencrypted snapshot.","poster":"Dantas"}],"timestamp":"1641040260.0","upvote_count":"1","poster":"Dantas","content":"Don't the words \"manual snapshot\" invalidate the answer \"A\"?"}],"content":"From the reference: You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created.\n\nHowever, because you can encrypt a copy of an unencrypted snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance. For more information, see Copying a snapshot.","upvote_count":"5","timestamp":"1636713420.0"}],"timestamp":"1632888120.0"},{"upvote_count":"1","content":"Sign in to the AWS Management Console and navigate to the Amazon RDS dashboard.\nSelect the DB instance that you want to encrypt.\nClick the \"Modify\" button.\nIn the \"Encryption\" section, select the option to \"Enable encryption\".\nChoose the KMS encryption key that you want to use or create a new one.\nClick \"Continue\" and review the summary of changes.\nClick \"Modify DB instance\" to apply the changes.\nNote that the encryption process will initiate a snapshot of the DB instance, encrypt it, and restore the encrypted data from the snapshot, so there will be a brief period of downtime while the encryption process is completed.","comment_id":"849535","poster":"redman50","timestamp":"1679682300.0"},{"upvote_count":"1","comment_id":"625565","content":"Selected Answer: D\nD. is correct. \nCreate Read Replica encrypted enable and promote standalone instance.\n\nA. The snapshot doesn't encrypred option.\nB. Unencrypted instance is not enable encrypted.\nC. Also automated snapshot is not enable encrypted.","timestamp":"1656648180.0","poster":"megadba"},{"content":"Selected Answer: A\nA. Answer","upvote_count":"1","comment_id":"625245","poster":"minhntm","timestamp":"1656592020.0"},{"upvote_count":"1","content":"Selected Answer: A\nYou can only encrypt an Amazon RDS DB instance when you create it, not after the DB instance is created.\n\nHowever, because you can encrypt a copy of an unencrypted snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance","poster":"niau","comment_id":"613598","timestamp":"1654742880.0"},{"timestamp":"1652412480.0","comment_id":"600943","upvote_count":"1","content":"A is correct, from the page DMS used for ongoing replication.\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\n\nAWS DMS – You can use AWS Database Migration Service (AWS DMS) to replicate changes from the source DB to the target DB. It is important to keep the source and target DB in sync to keep downtime to a minimum. For information about setting up AWS DMS and creating tasks, see the AWS DMS documentation.","poster":"praffuln"},{"timestamp":"1651238520.0","upvote_count":"1","poster":"novice_expert","comment_id":"594486","content":"Selected Answer: A\nManual Snapshot -> Create an encrypted copy -> Restore a new DB instance from the encrypted snapshot."},{"upvote_count":"2","comment_id":"554944","content":"Selected Answer: A\ncorrect","poster":"tugboat","timestamp":"1645659240.0"},{"timestamp":"1645659180.0","content":"Selected Answer: A\nCorrect option","poster":"tugboat","upvote_count":"1","comment_id":"554943"},{"timestamp":"1645653720.0","poster":"kped21","content":"A - is wrong, something has changed recently.\nI took a snapshot and tried to copy and encrypt it, it does not allows unencrypted to encrypted.\nThe best option is C, C works as is take any snapshot or manual snapshot and restore to new encrypted cluster.","comment_id":"554910","upvote_count":"1"},{"timestamp":"1639759860.0","poster":"Sisun","upvote_count":"2","content":"Selected Answer: A\nA - correct\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\nYou can enable encryption for an Amazon RDS DB instance when you create it, but not after it's created. However, you can add encryption to an unencrypted DB instance by creating a snapshot of your DB instance, and then creating an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot to get an encrypted copy of your original DB instance. The pattern uses AWS Database Migration Service (AWS DMS) to migrate data and AWS Key Management Service (AWS KMS) for encryption.","comment_id":"503799"},{"timestamp":"1638839760.0","upvote_count":"1","poster":"akiraklaus","comment_id":"495498","content":"All aswer is erro, is necessary utilization DMS \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html"},{"poster":"ChauPhan","upvote_count":"1","comment_id":"433763","timestamp":"1635599400.0","content":"A is correct"},{"upvote_count":"1","comment_id":"360654","content":"A final answer","poster":"Aesthet","timestamp":"1634851680.0"},{"upvote_count":"1","content":"A is correct","timestamp":"1633264440.0","comment_id":"344250","poster":"agrawalachin"}],"timestamp":"2021-04-02 16:02:00","isMC":true,"answer_images":[],"question_images":[],"unix_timestamp":1617372120,"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/48817-exam-aws-certified-database-specialty-topic-1-question-114/","answer_ET":"A","choices":{"C":"Restore a DB instance from the most recent automated snapshot and enable encryption.","D":"Create an encrypted read replica of the DB instance. Promote the read replica to a standalone instance.","A":"Create an encrypted copy of manual snapshot of the DB instance. Restore a new DB instance from the encrypted snapshot.","B":"Modify the DB instance and enable encryption."},"answer_description":""},{"id":"PEroP0BRfFeaqBOMSYGj","question_id":19,"answer":"D","question_text":"A database specialist was alerted that a production Amazon RDS MariaDB instance with 100 GB of storage was out of space. In response, the database specialist modified the DB instance and added 50 GB of storage capacity. Three hours later, a new alert is generated due to a lack of free space on the same DB instance.\nThe database specialist decides to modify the instance immediately to increase its storage capacity by 20 GB.\nWhat will happen when the modification is submitted?","answer_images":[],"topic":"1","isMC":true,"discussion":[{"poster":"Jaypdv","comment_id":"327756","upvote_count":"14","content":"D. answer. instance can't have any more storage modifications for six hours","timestamp":"1634034180.0"},{"timestamp":"1667277360.0","poster":"Satprave","content":"D - Storage shouldn't be extended immediately","comment_id":"708839","upvote_count":"1"},{"timestamp":"1651324620.0","upvote_count":"4","poster":"novice_expert","comment_id":"595027","content":"Selected Answer: D\nneeds 6 hour gap"},{"poster":"RotterDam","content":"Selected Answer: D\nAnswer is D - cannot modify storage until EITHER 6 hours have passed OR the \"storage-optimization\" status is complete (instance will show \"storage-optimization\" happens after previous storage capacity has increased -it CAN take more than 6 hours)\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html","upvote_count":"4","timestamp":"1646503860.0","comment_id":"561562"},{"poster":"kped21","upvote_count":"2","content":"D\nStorage optimization can take several hours. You can't make further storage modifications for either six (6) hours or until storage optimization has completed on the instance, whichever is longer.","comment_id":"555727","timestamp":"1645759260.0"},{"upvote_count":"2","content":"Selected Answer: D\nOption D","poster":"GMartinelli","comment_id":"492429","timestamp":"1638446460.0"},{"comment_id":"449546","timestamp":"1636169820.0","upvote_count":"1","content":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html - either 6 hours or when the first job completes - I would go with D too","poster":"Scunningham99"},{"upvote_count":"2","timestamp":"1635487260.0","poster":"Aesthet","content":"D\nYou can't make further storage modifications until six (6) hours after storage optimization has completed on the instance.","comment_id":"360661"},{"comment_id":"344252","timestamp":"1634642100.0","content":"D is correct. 6 hours duration needs to pass","upvote_count":"1","poster":"agrawalachin"},{"upvote_count":"2","timestamp":"1632114840.0","comments":[{"comment_id":"326749","upvote_count":"2","poster":"shantest1","timestamp":"1633178580.0","content":"Ignore the answer, that is for auto scaling, has to pass 6 hours.","comments":[{"poster":"shantest1","comment_id":"326752","timestamp":"1633196280.0","upvote_count":"1","content":"Well, that condition applies both to manual as well as auto scaling, 6 hours has to pass. So I think it is D still"}]}],"poster":"shantest1","content":"D. answer\nI think it needs to pass 6 hours to increase another storage space increase.","comment_id":"326747"}],"exam_id":22,"question_images":[],"unix_timestamp":1617374520,"answer_description":"","answer_ET":"D","answers_community":["D (100%)"],"timestamp":"2021-04-02 16:42:00","choices":{"D":"The request will fail as the most recent modification was too soon.","A":"The request will fail because this storage capacity is too large.","B":"The request will succeed only if the primary instance is in active status.","C":"The request will succeed only if CPU utilization is less than 10%."},"url":"https://www.examtopics.com/discussions/amazon/view/48820-exam-aws-certified-database-specialty-topic-1-question-115/"},{"id":"ieOdZhqsq9pM5V0yKaLr","question_id":20,"timestamp":"2021-04-04 07:24:00","answer":"BD","answer_description":"","unix_timestamp":1617513840,"discussion":[{"comment_id":"331190","upvote_count":"14","poster":"shantest1","timestamp":"1633134900.0","content":"B and D."},{"timestamp":"1645661580.0","content":"Selected Answer: BD\nPer - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html\n\nNot A as - You can't create an encrypted Aurora Replica from an unencrypted Aurora DB cluster. You can't create an unencrypted Aurora Replica from an encrypted Aurora DB cluster.\nB is good for in-transit replication\nNot C as - You can't convert an unencrypted DB cluster to an encrypted one. \nD as - You can restore an unencrypted snapshot to an encrypted Aurora DB cluster. To do this, specify a KMS key when you restore from the unencrypted snapshot.\nNot E as - KMS does not perform encryption for data in transit or in motion. If you want to encrypt data while in transit, then you would need to use a different method such as SSL.\n\nSo, B and D is correct.","poster":"tugboat","comment_id":"554967","upvote_count":"6"},{"upvote_count":"1","poster":"redman50","content":"Selected Answer: AC\nIn Aurora you can encrypt at rest without copying the snapshot. So A and C for sure","comments":[{"content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html#Overview.Encryption.Limitations","comment_id":"858875","upvote_count":"1","timestamp":"1680442200.0","poster":"Piccaso"}],"comment_id":"849542","timestamp":"1679682780.0"},{"comment_id":"839675","content":"Selected Answer: BD\nI reckon is B and D","upvote_count":"1","timestamp":"1678868820.0","poster":"dougporto1988"},{"content":"Selected Answer: BD\nB and D. D is right. Take snapshot of cluster > and (keyword here) > enable encryption. You cannot take a snapshot and encrypt it at the same time, this where the 'and' comes into play, you can encrypt just a snapshot + you can encrypt the snapshot on restore.","poster":"lunt","comment_id":"601502","timestamp":"1652518800.0","upvote_count":"2"},{"comments":[{"poster":"novice_expert","upvote_count":"1","comment_id":"594540","content":"D. as - You can NOT restore an unencrypted snapshot to an encrypted Aurora DB cluster","timestamp":"1651244820.0"}],"poster":"novice_expert","content":"Selected Answer: BD\nB. SSL/TLS is good for in-transit replication\nD. as - You can restore an unencrypted snapshot to an encrypted Aurora DB cluster","upvote_count":"2","timestamp":"1651244760.0","comment_id":"594538"},{"timestamp":"1645655760.0","upvote_count":"3","poster":"kped21","content":"B,D\nC: Wrong, you cannot modify an unencrypted to encrypted","comment_id":"554925"},{"content":"A and B .. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html\n\nD is incorrect: \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_CopySnapshot.html\n\nFor Amazon Aurora DB cluster snapshots, you can't encrypt an unencrypted DB cluster snapshot when you copy the snapshot.","comment_id":"522250","timestamp":"1642000500.0","upvote_count":"4","poster":"awsmonster"},{"upvote_count":"1","poster":"ChauPhan","comment_id":"433767","timestamp":"1635655320.0","content":"B and D is correct"},{"timestamp":"1635156120.0","upvote_count":"1","content":"B and D are correct choice.","comment_id":"409077","poster":"Hits_23"},{"content":"BD\nB. is obvious. For D. I thought it's possible to directly restore the unencrypted snapshot into an encrypted cluster so somehow one step looks unnecessary. But A, C and E are incorrect so I pick D. by default","timestamp":"1632125760.0","comment_id":"327766","poster":"Jaypdv","upvote_count":"5"}],"choices":{"A":"Create an Aurora Replica with encryption enabled using AWS Key Management Service (AWS KMS). Then promote the replica to master.","C":"Modify the existing Aurora DB cluster and enable encryption using an AWS Key Management Service (AWS KMS) encryption key. Apply the changes immediately.","D":"Take a snapshot of the Aurora DB cluster and encrypt the snapshot using an AWS Key Management Service (AWS KMS) encryption key. Restore the snapshot to a new DB cluster and update the financial application database endpoints.","E":"Use AWS Key Management Service (AWS KMS) to secure the in-transit connection between the financial application and the Aurora DB cluster.","B":"Use SSL/TLS to secure the in-transit connection between the financial application and the Aurora DB cluster."},"question_images":[],"answer_ET":"BD","answers_community":["BD (92%)","8%"],"answer_images":[],"question_text":"A company uses Amazon Aurora for secure financial transactions. The data must always be encrypted at rest and in transit to meet compliance requirements.\nWhich combination of actions should a database specialist take to meet these requirements? (Choose two.)","exam_id":22,"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/49006-exam-aws-certified-database-specialty-topic-1-question-116/"}],"exam":{"provider":"Amazon","name":"AWS Certified Database - Specialty","isMCOnly":false,"lastUpdated":"11 Apr 2025","isImplemented":true,"id":22,"isBeta":false,"numberOfQuestions":359},"currentPage":4},"__N_SSP":true}