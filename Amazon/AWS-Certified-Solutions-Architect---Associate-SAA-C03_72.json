{"pageProps":{"questions":[{"id":"UgM5lsGTZHILw6AXwYup","timestamp":"2023-03-10 16:55:00","question_id":356,"answer_images":[],"answers_community":["C (100%)"],"question_images":[],"question_text":"A meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does not want this new service to affect the performance of the current application.\n\nWhat should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?","discussion":[{"comments":[{"poster":"Buruguduystunstugudunstuy","upvote_count":"6","timestamp":"1711424760.0","content":"Answer A is not a suitable solution because it requires additional configuration to notify the internal teams, and it could add operational overhead to the application.\n\nAnswer B is not the best solution because it requires changes to the current application, which may affect its performance, and it creates additional work for the teams to subscribe to multiple topics.\n\nAnswer D is not a good solution because it requires a cron job to scan the table every minute, which adds additional operational overhead to the system.\n\nTherefore, the correct answer is C. Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon SNS topic to which the teams can subscribe.","comment_id":"850655"}],"upvote_count":"14","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: C\nThe best solution to meet these requirements with the least amount of operational overhead is to enable Amazon DynamoDB Streams on the table and use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe. This solution requires minimal configuration and infrastructure setup, and Amazon DynamoDB Streams provide a low-latency way to capture changes to the DynamoDB table. The triggers automatically capture the changes and publish them to the SNS topic, which notifies the internal teams.","comment_id":"850654","timestamp":"1711424700.0"},{"content":"Selected Answer: C\nEnable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe","poster":"Guru4Cloud","upvote_count":"4","comment_id":"994049","timestamp":"1725018960.0"},{"poster":"james2033","timestamp":"1721451480.0","comment_id":"957132","content":"Selected Answer: C\nQuestion keyword: \"sends an alert\", a new weather event is recorded\". Answer keyword C \"Amazon DynamoDB Streams on the table\", \"Amazon Simple Notification Service\" (Amazon SNS). Choose C. Easy question.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\n\nhttps://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","upvote_count":"4"},{"content":"Selected Answer: C\nBest answer is C","poster":"TariqKipkemei","timestamp":"1716525660.0","upvote_count":"3","comments":[{"upvote_count":"6","poster":"TariqKipkemei","comment_id":"1052461","content":"DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. This capture activity can also invoke triggers to write the event to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe to.","timestamp":"1729742100.0"}],"comment_id":"905473"},{"comment_id":"849314","content":"C is correct","poster":"Hemanthgowda1932","timestamp":"1711286760.0","upvote_count":"2"},{"content":"definitely C","upvote_count":"2","comment_id":"847244","timestamp":"1711124160.0","poster":"Santosh43"},{"upvote_count":"4","comment_id":"845254","poster":"Bezha","timestamp":"1710970020.0","content":"Selected Answer: C\nDynamoDB Streams"},{"timestamp":"1710181800.0","poster":"sitha","content":"Selected Answer: C\nAnswer : C","comment_id":"836394","upvote_count":"2"},{"content":"Selected Answer: C\ncccccccc","timestamp":"1710086100.0","poster":"[Removed]","upvote_count":"2","comment_id":"835189"}],"choices":{"A":"Use DynamoDB transactions to write new event data to the table. Configure the transactions to notify internal teams.","D":"Add a custom attribute to each record to flag new items. Write a cron job that scans the table every minute for items that are new and notifies an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe.","B":"Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Have each team subscribe to one topic.","C":"Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe."},"answer":"C","exam_id":31,"unix_timestamp":1678463700,"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/102169-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"answer_ET":"C"},{"id":"dRGMooNGKTb93CRV4hnD","answer":"A","unix_timestamp":1678464120,"topic":"1","choices":{"D":"Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances.","A":"Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.","B":"Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.","C":"Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails."},"answers_community":["A (93%)","3%"],"answer_images":[],"answer_ET":"A","question_text":"A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage.\n\nThe company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/102170-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"discussion":[{"upvote_count":"7","content":"Selected Answer: A\nB has app servers in a single AZ and a database on a single instance\nC has both DB replicas in a single AZ\nD does not work (EBS Multi-Attach requires EC2 instances in same AZ), and if it would work then the EBS volume would be an SPOF","poster":"pentium75","timestamp":"1719725340.0","comment_id":"1110350"},{"poster":"Guru4Cloud","comment_id":"994044","timestamp":"1709214660.0","content":"Selected Answer: A\nDeploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration","upvote_count":"2"},{"comments":[{"upvote_count":"4","poster":"Guru4Cloud","comment_id":"994047","timestamp":"1709214780.0","content":"C is incorrect because the read replica also resides in a single AZ"}],"timestamp":"1707242100.0","poster":"czyboi","upvote_count":"1","comment_id":"973993","content":"Why is C incorrect ?"},{"content":"Selected Answer: A\nA most def.","timestamp":"1701335400.0","poster":"antropaws","upvote_count":"3","comment_id":"910000"},{"timestamp":"1700891100.0","comment_id":"906316","content":"Selected Answer: A\nDeploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.","poster":"TariqKipkemei","upvote_count":"3"},{"upvote_count":"3","timestamp":"1695692280.0","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: A\nThe correct answer is A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.\n\nTo make an existing application highly available and resilient while avoiding any single points of failure and giving the application the ability to scale to meet user demand, the best solution would be to deploy the application servers using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones and use an Amazon RDS DB instance in a Multi-AZ configuration.\n\nBy using an Amazon RDS DB instance in a Multi-AZ configuration, the database is automatically replicated across multiple Availability Zones, ensuring that the database is highly available and can withstand the failure of a single Availability Zone. This provides fault tolerance and avoids any single points of failure.","comment_id":"850647"},{"comment_id":"846892","upvote_count":"1","poster":"Thief","timestamp":"1695368340.0","comments":[{"poster":"Guru4Cloud","comment_id":"994048","timestamp":"1709214840.0","content":"D is incorrect because using Multi-Attach EBS adds complexity and doesn't provide automatic DB failover","upvote_count":"2"},{"timestamp":"1719725220.0","comment_id":"1110347","upvote_count":"2","poster":"pentium75","content":"Multi-Attach does not work across Availability Zones."},{"poster":"Buruguduystunstugudunstuy","content":"Answer D, deploying the primary and secondary database servers on EC2 instances across multiple Availability Zones and using Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances, may provide high availability for the database but may introduce additional complexity, and management overhead, and potential performance issues.","upvote_count":"2","timestamp":"1695692040.0","comment_id":"850643"}],"content":"Selected Answer: D\nWhy not D?"},{"comment_id":"844532","poster":"WherecanIstart","timestamp":"1695177060.0","upvote_count":"3","content":"Selected Answer: A\nHighly available = Multi-AZ approach"},{"upvote_count":"1","content":"Selected Answer: A\nAnswers is A","timestamp":"1694918880.0","comment_id":"841585","poster":"nileshlg"},{"comment_id":"841177","timestamp":"1694874360.0","poster":"[Removed]","content":"Selected Answer: A\nOption A is the correct solution. Deploying the application servers in an Auto Scaling group across multiple Availability Zones (AZs) ensures high availability and fault tolerance. An Auto Scaling group allows the application to scale horizontally to meet user demand. Using Amazon RDS DB instance in a Multi-AZ configuration ensures that the database is automatically replicated to a standby instance in a different AZ. This provides database redundancy and avoids any single point of failure.","upvote_count":"2"},{"upvote_count":"1","comment_id":"837972","timestamp":"1694608200.0","poster":"quentin17","comments":[],"content":"Selected Answer: C\nHighly available"},{"poster":"KAUS2","content":"Selected Answer: A\nYes , agree with A","timestamp":"1694430240.0","comment_id":"836005","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: A\nagree with that","comment_id":"835381","timestamp":"1694369160.0","poster":"cegama543"}],"question_images":[],"isMC":true,"answer_description":"","timestamp":"2023-03-10 17:02:00","question_id":357},{"id":"k0MQh5UXwK1hIUaLZV0G","answer":"A","unix_timestamp":1678464540,"topic":"1","choices":{"C":"Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.","D":"Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.","B":"Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.","A":"Update the Kinesis Data Streams default settings by modifying the data retention period."},"answers_community":["A (69%)","C (28%)","2%"],"answer_images":[],"answer_ET":"A","question_text":"A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams.\n\nWhat should a solutions architect do to resolve this issue?","url":"https://www.examtopics.com/discussions/amazon/view/102175-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"discussion":[{"comment_id":"844535","upvote_count":"31","timestamp":"1679287020.0","content":"Selected Answer: A\n\"A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).\"\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html\n\nThe question mentioned Kinesis data stream default settings and \"every other day\". After 24hrs, the data isn't in the Data stream if the default settings is not modified to store data more than 24hrs.","comments":[{"content":"Thank you for the link","poster":"babayomi","comment_id":"1304453","timestamp":"1730206800.0","upvote_count":"1"}],"poster":"WherecanIstart"},{"poster":"cegama543","comment_id":"835380","content":"Selected Answer: C\nC. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.\n\nThe best option is to update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams. Kinesis Data Streams scales horizontally by increasing or decreasing the number of shards, which controls the throughput capacity of the stream. By increasing the number of shards, the application will be able to send more data to Kinesis Data Streams, which can help ensure that S3 receives all the data.","comments":[{"upvote_count":"2","poster":"Buruguduystunstugudunstuy","content":"Answer C:\nC. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.\n\n- Answer C updates the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams. By increasing the number of shards, the data is distributed across multiple shards, which allows for increased throughput and ensures that all data is ingested and processed by Kinesis Data Streams.\n- Monitoring the Kinesis Data Streams and adjusting the number of shards as needed to handle changes in data throughput can ensure that the application can handle large amounts of streaming data.","comments":[{"upvote_count":"2","poster":"Buruguduystunstugudunstuy","timestamp":"1679793480.0","comment_id":"850633","content":"@cegama543, my apologies. Moderator if you can disapprove of the post above? I made a mistake. It is supposed to be intended on the post that I submitted.\n\nThanks."}],"comment_id":"850630","timestamp":"1679793120.0"},{"poster":"CapJackSparrow","comment_id":"842791","content":"lets say you had infinity shards... if the retention period is 24 hours and you get the data every 48 hours, you will lose 24 hours of data no matter the amount of shards no?","timestamp":"1679145180.0","comments":[{"timestamp":"1679242200.0","upvote_count":"6","comment_id":"843922","content":"Amazon Kinesis Data Streams supports changes to the data record retention period of your data stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).","poster":"enzomv"}],"upvote_count":"15"}],"timestamp":"1678478640.0","upvote_count":"17"},{"upvote_count":"1","content":"Selected Answer: U\nC. Atualizar o número de fragmentos do Kinesis:\n\nAumentar o número de fragmentos melhora a capacidade de processamento e a taxa de transferência do stream. No entanto, este não é o problema aqui, pois a perda de dados ocorre devido ao período de retenção insuficiente, e não devido a limitações de capacidade.","poster":"Rcosmos","timestamp":"1737576420.0","comment_id":"1344944"},{"content":"Selected Answer: C\nAnswer is C\n\nIssue with A) Update the Kinesis Data Streams default settings by modifying the data retention period. is below\n\nLimitation: Modifying the data retention period affects how long data is kept in the stream, but it does not address the issue of the stream's capacity to ingest data. If the stream is unable to handle the incoming data volume, extending the retention period will not resolve the data loss issue.","poster":"abriggy","comment_id":"1254619","upvote_count":"1","timestamp":"1721862900.0"},{"timestamp":"1705612380.0","content":"Selected Answer: A\nEvery other day, = 48 hours\nDefault settings = 24 hours\n\nB: Development library so won't help\nC: More shards may retain more data but they will have same limitation of 24 hours retention\nD: Irrelevant\n\nA: Increase the default limit from 24 hours to 48 hours","poster":"awsgeek75","comment_id":"1126207","upvote_count":"6"},{"comment_id":"1110352","timestamp":"1704007860.0","content":"Selected Answer: A\n\"Default settings\" = 24 hour retention","upvote_count":"5","poster":"pentium75"},{"comment_id":"1090363","poster":"Murtadhaceit","timestamp":"1701959100.0","upvote_count":"3","content":"Selected Answer: A\nKDS has two modes:\n1. Provisioned Mode: Answer C would be correct if KDS runs in this mode. We need to increase the number of shards.\n2. On-Demand: Scales automatically, which means it doesn't need to adjust the number of shards based on observed throughput.\n\nAnd since the question does not mention which type, I would go with On-demand. Therefore, A is the correct answer."},{"comment_id":"1052485","timestamp":"1698121200.0","content":"Selected Answer: A\nData records are stored in shards in a kinesis data stream temporarily. The time period from when a record is added, to when it is no longer accessible is called the retention period. This time period is 24 hours by default, but could be adjusted to 365 days.\nKinesis Data Streams automatically scales the number of shards in response to changes in data volume and traffic, so this rules out option C.\n\nhttps://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html#:~:text=the%20number%20of-,shards,-in%20response%20to","upvote_count":"2","poster":"TariqKipkemei"},{"timestamp":"1695641520.0","content":"Selected Answer: A\nI have only voted A because it mentions the default setting in Kinesis, if it did not mention that then I would look to increase the Shards. By default it is 24 hours and can go to 365 days. I think the question should be rephrased slightly. I had trouble deciding between A & C. Also apparently the most voted answer is the correct answer as per some advice I was given.","upvote_count":"3","poster":"Ramdi1","comment_id":"1016746"},{"comment_id":"997236","upvote_count":"3","content":"Selected Answer: A\nDefault retention is 24 hrs, but the data read is every other day, so the S3 will never receive the data, Change the default retention period to 48 hours.","poster":"BrijMohan08","timestamp":"1693706040.0"},{"content":"Selected Answer: C\nBy default, a Kinesis data stream is created with one shard. If the data throughput to the stream is higher than the capacity of the single shard, the data stream may not be able to handle all the incoming data, and some data may be lost.\nTherefore, to handle the high volume of data that the application sends to Kinesis Data Streams, the number of Kinesis shards should be increased to handle the required throughput.\nKinesis Data Streams shards are the basic units of scalability and availability. Each shard can process up to 1,000 records per second with a maximum of 1 MB of data per second. If the application is sending more data to Kinesis Data Streams than the shards can handle, then some of the data will be dropped.","upvote_count":"1","poster":"Guru4Cloud","comment_id":"994028","timestamp":"1693395660.0","comments":[{"content":"If you have doubts, Please read about Kinesis Data Streams shards.\nAns: A is not the correct answer here","timestamp":"1693395780.0","comment_id":"994031","upvote_count":"1","poster":"Guru4Cloud"}]},{"poster":"Amycert","timestamp":"1691803380.0","comment_id":"979062","content":"Selected Answer: A\nthe default retention period is 24 hours \"The default retention period of 24 hours covers scenarios where intermittent lags in processing require catch-up with the real-time data. \"\nso we should increment this","upvote_count":"2"},{"timestamp":"1690278660.0","content":"Selected Answer: A\nAs \"Default settings\" is mentioned here, I vote for A.","comment_id":"962563","poster":"hsinchang","upvote_count":"2"},{"content":"Selected Answer: A\nkeyword here is - default settings and every other day and since \"A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).\"\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html\n\nWill go with A","poster":"jaydesai8","comment_id":"947487","timestamp":"1688927400.0","upvote_count":"2"},{"content":"Selected Answer: A\nC is wrong because even if you update the number of Kinesis shards, you still need to change the default data retention period first. Otherwise, you would lose data after 24 hours.","upvote_count":"3","comment_id":"913922","timestamp":"1685827860.0","poster":"jayce5"},{"content":"Selected Answer: C\nA is unrelated to the issue. The correct answer is C.","timestamp":"1685430960.0","poster":"antropaws","comment_id":"910005","upvote_count":"1"},{"comment_id":"909528","upvote_count":"1","timestamp":"1685376240.0","content":"Correct Ans. is B","poster":"omoakin"},{"upvote_count":"2","content":"By default, a Kinesis data stream is created with one shard. If the data throughput to the stream is higher than the capacity of the single shard, the data stream may not be able to handle all the incoming data, and some data may be lost.\n\nTherefore, to handle the high volume of data that the application sends to Kinesis Data Streams, the number of Kinesis shards should be increased to handle the required throughput","comment_id":"891375","timestamp":"1683463080.0","poster":"smd_"},{"comment_id":"879414","timestamp":"1682345040.0","upvote_count":"2","content":"both Option A and Option C could be valid solutions to resolving the issue of data loss, depending on the root cause of the problem. It would be best to analyze the root cause of the data loss issue to determine which solution is most appropriate for this specific scenario.","poster":"arjundevops"},{"comment_id":"876153","upvote_count":"2","poster":"neosis91","content":"Selected Answer: C\nCCCCCCCCC","timestamp":"1682045940.0"},{"comment_id":"859033","upvote_count":"2","timestamp":"1680452160.0","content":"Also: https://www.examtopics.com/discussions/amazon/view/61067-exam-aws-certified-solutions-architect-associate-saa-c02/ for Option A.","poster":"kraken21"},{"comment_id":"859030","content":"Selected Answer: A\nIt comes down to is it a compute issue or a storage issue. Since the keywords of \"Default\", \"every other day\" were used and the issue is some data is missing, I am voting for Option A.","timestamp":"1680452100.0","upvote_count":"6","poster":"kraken21"},{"poster":"channn","comment_id":"858509","upvote_count":"3","timestamp":"1680408780.0","content":"Selected Answer: C\nChapGPT gives answer B or C. also mention that Option A and option D are not directly related to the issue of data loss and may not help to resolve the problem."},{"comments":[{"timestamp":"1679793180.0","comment_id":"850632","upvote_count":"2","content":"In comparison, while both options can help address the issue of data not being ingested by Kinesis Data Streams, Answer C is a more direct solution that addresses the underlying issue of insufficient capacity to handle the data throughput. Answer A may delay the issue of incomplete data ingestion by increasing the retention period, but it does not address the root cause of the problem.\n\nIn conclusion, Answer C is a more effective solution for handling large amounts of streaming data and ensuring that all data is ingested and processed by Kinesis Data Streams.","comments":[{"comment_id":"910568","timestamp":"1685476080.0","poster":"ruqui","upvote_count":"2","content":"your analysis is wrong ... the real problem is that the application ingested the data every 48 hours, if Kinesis holds only the latest 24 hours, then all data that is ingested by Kinesis in hours 0 to 23 are not present (it will only have the data from hours 24 to 48) . For this reason, C is completely wrong, there's no way that having an infinite number of shards would be able to process data that is already gone"}],"poster":"Buruguduystunstugudunstuy"}],"comment_id":"850629","content":"Selected Answer: C\nA comparison of Answer A and Answer C:\n\nAnswer A:\nA. Update the Kinesis Data Streams default settings by modifying the data retention period.\n\n- Answer A modifies the data retention period of Kinesis Data Streams, which defines how long the data is retained in the stream. Increasing the retention period may ensure that all data is eventually ingested and processed by Kinesis Data Streams, but it does not address the immediate issue of data not being ingested by Kinesis Data Streams.\n- Modifying the data retention period may also lead to increased storage costs if the data is retained for a longer period of time.","upvote_count":"3","poster":"Buruguduystunstugudunstuy","timestamp":"1679793060.0"},{"upvote_count":"2","comment_id":"845549","content":"A is the correct answer","poster":"Grace83","timestamp":"1679376240.0"},{"upvote_count":"6","content":"Selected Answer: A\nCorrect answer is A. Keywords to consider are,\n1. Default Parameters\n2. Every Other Day","comment_id":"841583","timestamp":"1679028240.0","poster":"nileshlg"},{"poster":"[Removed]","comment_id":"841174","upvote_count":"4","timestamp":"1678983720.0","content":"Selected Answer: C\nC. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.\n\nThe issue is that the Amazon S3 bucket is not receiving all the data sent to Kinesis Data Streams. This indicates that the bottleneck is most likely in the Kinesis Data Streams configuration.\n\nTo resolve this issue, a solutions architect should increase the number of Kinesis shards. Kinesis Data Streams partitions data into shards, and each shard can handle a specific amount of data throughput. By default, Kinesis Data Streams is configured with a single shard, which may not be enough to handle the application's data throughput.\n\nIncreasing the number of shards will distribute the data more evenly and improve the throughput, allowing all the data to be processed and sent to Amazon S3 for further analysis."},{"upvote_count":"3","content":"Selected Answer: A\nNeed to increase default retention period","comment_id":"840665","poster":"kampatra","timestamp":"1678953600.0"},{"poster":"UnluckyDucky","content":"Selected Answer: A\nBy default, Kinesis Data Streams hold your data for 24 hours. Everything that is 24 hours and 1 second old gets deleted unless the retention policy is changed.\n\nKey words: Every other day and default settings for that Kinesis streams.","upvote_count":"5","comment_id":"838154","timestamp":"1678732140.0"},{"timestamp":"1678637160.0","poster":"Karlos99","upvote_count":"3","comment_id":"837191","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html\n\nRole of the KPL\nThe KPL is an easy-to-use, highly configurable library that helps you write to a Kinesis data stream. It acts as an intermediary between your producer application code and the Kinesis Data Streams API actions. The KPL performs the following primary tasks:\n Writes to one or more Kinesis data streams with an automatic and configurable retry mechanism\n Collects records and uses PutRecords to write multiple records to multiple shards per request\n Aggregates user records to increase payload size and improve throughput"},{"upvote_count":"2","poster":"KAUS2","comment_id":"836001","timestamp":"1678539720.0","content":"Selected Answer: C\nC is the correct answer. Agree with cegama543's explanation."},{"comments":[{"timestamp":"1678674960.0","content":"AAAAA what?","poster":"fkie4","comment_id":"837561","upvote_count":"1"}],"content":"Selected Answer: A\naaaaaaa","upvote_count":"3","comment_id":"835199","poster":"[Removed]","timestamp":"1678464540.0"}],"question_images":[],"isMC":true,"answer_description":"","timestamp":"2023-03-10 17:09:00","question_id":358},{"id":"dyk2J2v8Pd0mLwTBwgTt","discussion":[{"content":"To grant the necessary permissions to an AWS Lambda function to upload files to Amazon S3, a solutions architect should create an IAM execution role with the required permissions and attach the IAM role to the Lambda function. This approach follows the principle of least privilege and ensures that the Lambda function can only access the resources it needs to perform its specific task.\n\nTherefore, the correct answer is D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.","comments":[{"timestamp":"1724849700.0","content":"Oh you're here","upvote_count":"3","comment_id":"1274062","poster":"AWSSURI"}],"timestamp":"1679783700.0","comment_id":"850525","poster":"Buruguduystunstugudunstuy","upvote_count":"7"},{"comment_id":"993988","timestamp":"1693393020.0","upvote_count":"4","content":"Selected Answer: D\nCreate Lambda execution role and attach existing S3 IAM role to the lambda function","poster":"Guru4Cloud"},{"content":"D. Créez un rôle d'exécution IAM avec les autorisations requises et attachez le rôle IAM à la fonction Lambda.\n\nL'architecte de solutions doit créer un rôle d'exécution IAM ayant les autorisations nécessaires pour accéder à Amazon S3 et effectuer les opérations requises (par exemple, charger des fichiers). Ensuite, le rôle doit être associé à la fonction Lambda, de sorte que la fonction puisse assumer ce rôle et avoir les autorisations nécessaires pour interagir avec Amazon S3.","timestamp":"1679408100.0","comment_id":"846049","upvote_count":"4","poster":"Bilalglg93350"},{"content":"Selected Answer: D\nAnswer is D","comment_id":"841576","timestamp":"1679027700.0","upvote_count":"3","poster":"nileshlg"},{"upvote_count":"3","timestamp":"1678953780.0","content":"Selected Answer: D\nD - correct ans","comment_id":"840669","poster":"kampatra"},{"timestamp":"1678557660.0","content":"Selected Answer: D\nCreate Lambda execution role and attach existing S3 IAM role to the lambda function","poster":"sitha","upvote_count":"3","comment_id":"836356"},{"timestamp":"1678530960.0","comment_id":"835834","upvote_count":"3","poster":"ktulu2602","content":"Selected Answer: D\nDefinitely D"},{"poster":"Nithin1119","timestamp":"1678507080.0","upvote_count":"2","content":"Selected Answer: D\nddddddd","comment_id":"835638"},{"poster":"[Removed]","timestamp":"1678464660.0","comment_id":"835202","content":"Selected Answer: D\ndddddddd","upvote_count":"2"}],"question_images":[],"timestamp":"2023-03-10 17:11:00","answers_community":["D (100%)"],"question_id":359,"answer_ET":"D","unix_timestamp":1678464660,"isMC":true,"topic":"1","choices":{"C":"Create a new IAM user and use the existing IAM credentials in the Lambda function.","A":"Add required IAM permissions in the resource policy of the Lambda function.","B":"Create a signed request using the existing IAM credentials in the Lambda function.","D":"Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function."},"question_text":"A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3.\n\nWhat should a solutions architect do to grant the permissions?","url":"https://www.examtopics.com/discussions/amazon/view/102178-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"D","exam_id":31,"answer_description":"","answer_images":[]},{"id":"6dUQLSWgACZUHvWKToKj","discussion":[{"upvote_count":"5","content":"Selected Answer: D\nTo improve the architecture of this application, the best solution would be to use Amazon Simple Queue Service (Amazon SQS) to buffer the requests and decouple the S3 bucket from the Lambda function. This will ensure that the documents are not lost and can be processed at a later time if the Lambda function is not available.\n\nThis will ensure that the documents are not lost and can be processed at a later time if the Lambda function is not available. By using Amazon SQS, the architecture is decoupled and the Lambda function can process the documents in a scalable and fault-tolerant manner.","comment_id":"850511","poster":"Buruguduystunstugudunstuy","timestamp":"1679782020.0"},{"comment_id":"993972","timestamp":"1693392240.0","poster":"Guru4Cloud","content":"Selected Answer: D\nD. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambd","upvote_count":"3"},{"content":"Selected Answer: D\nD is the best approach","upvote_count":"3","timestamp":"1684987320.0","poster":"TariqKipkemei","comment_id":"906322"},{"comment_id":"850645","timestamp":"1679794560.0","upvote_count":"3","poster":"Russs99","content":"Selected Answer: D\nD is the correct answer"},{"comments":[{"poster":"JA2018","timestamp":"1732167480.0","content":"Frrom Google Translate:\n\nCreate an Amazon Simple Queue Service (Amazon SQS) queue. Send requests to the queue. Configure the queue as an event source for Lambda.\n\nThis solution makes it possible to effectively manage load peaks and avoid the loss of documents in the event of a sudden increase in traffic. When new documents are uploaded to the Amazon S3 bucket, requests are sent to the Amazon SQS queue, which acts as a buffer. The Lambda function is triggered based on events in the queue, which provides balanced processing and prevents the application from being overwhelmed by a large number of concurrent documents.","comment_id":"1315660","upvote_count":"1"},{"timestamp":"1679794500.0","comment_id":"850644","poster":"Russs99","content":"exactement. si je pouvais explique come cela en Francais aussi","upvote_count":"1"}],"comment_id":"846044","timestamp":"1679407860.0","content":"D. Créez une file d’attente Amazon Simple Queue Service (Amazon SQS). Envoyez les demandes à la file d’attente. Configurez la file d’attente en tant que source d’événement pour Lambda.\n\nCette solution permet de gérer efficacement les pics de charge et d'éviter la perte de documents en cas d'augmentation soudaine du trafic. Lorsque de nouveaux documents sont chargés dans le compartiment Amazon S3, les demandes sont envoyées à la file d'attente Amazon SQS, qui agit comme un tampon. La fonction Lambda est déclenchée en fonction des événements dans la file d'attente, ce qui permet un traitement équilibré et évite que l'application ne soit submergée par un grand nombre de documents simultanés.","poster":"Bilalglg93350","upvote_count":"1"},{"comment_id":"844538","poster":"WherecanIstart","upvote_count":"1","timestamp":"1679287200.0","content":"Selected Answer: D\nD is the correct answer."},{"content":"Selected Answer: D\nD is correct","upvote_count":"1","timestamp":"1678953900.0","comment_id":"840671","poster":"kampatra"},{"comment_id":"837533","timestamp":"1678671600.0","poster":"[Removed]","upvote_count":"1","content":"Selected Answer: D\nD is correct"},{"comment_id":"835205","poster":"[Removed]","upvote_count":"2","timestamp":"1678464720.0","content":"Selected Answer: D\ndddddddd"}],"question_images":[],"unix_timestamp":1678464720,"timestamp":"2023-03-10 17:12:00","exam_id":31,"question_id":360,"question_text":"A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents.\n\nWhat should a solutions architect do to improve the architecture of this application?","answer_description":"","answer_ET":"D","isMC":true,"answer_images":[],"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/102180-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","answer":"D","choices":{"D":"Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.","C":"Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.","B":"Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.","A":"Set the Lambda function's runtime timeout value to 15 minutes."}}],"exam":{"numberOfQuestions":1019,"isImplemented":true,"isBeta":false,"provider":"Amazon","name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025","isMCOnly":true,"id":31},"currentPage":72},"__N_SSP":true}