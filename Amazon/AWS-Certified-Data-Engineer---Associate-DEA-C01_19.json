{"pageProps":{"questions":[{"id":"7mDRLgkNgFtYj8PDK1R6","answer_description":"","timestamp":"2024-11-25 07:31:00","question_text":"A company saves customer data to an Amazon S3 bucket. The company uses server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the bucket. The dataset includes personally identifiable information (PII) such as social security numbers and account details.\n\nData that is tagged as PII must be masked before the company uses customer data for analysis. Some users must have secure access to the PII data during the pre-processing phase. The company needs a low-maintenance solution to mask and secure the PII data throughout the entire engineering pipeline.\n\nWhich combination of solutions will meet these requirements? (Choose two.)","answer":"AD","answers_community":["AD (100%)"],"discussion":[{"upvote_count":"1","content":"Selected Answer: AD\nA will find and mask the PII\nD for access","timestamp":"1736965680.0","poster":"MerryLew","comment_id":"1341195"},{"timestamp":"1734952080.0","poster":"HagarTheHorrible","comment_id":"1330789","upvote_count":"1","content":"Selected Answer: AD\nA for data maskin and D for access"},{"upvote_count":"1","content":"Selected Answer: AD\nhttps://aws.amazon.com/tw/blogs/big-data/build-a-data-pipeline-to-automatically-discover-and-mask-pii-data-with-aws-glue-databrew/","timestamp":"1732516260.0","comment_id":"1317347","poster":"emupsx1"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151945-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","isMC":true,"answer_images":[],"question_id":91,"answer_ET":"AD","choices":{"B":"Use Amazon GuardDuty to monitor access patterns for the PII data that is used in the engineering pipeline.","D":"Use AWS Identity and Access Management (IAM) to manage permissions and to control access to the PII data.","E":"Write custom scripts in an application to mask the PII data and to control access.","C":"Configure an Amazon Macie discovery job for the S3 bucket.","A":"Use AWS Glue DataBrew to perform extract, transform, and load (ETL) tasks that mask the PII data before analysis."},"exam_id":21,"unix_timestamp":1732516260},{"id":"GzSWlvFbGBQUgT5LhYAR","isMC":true,"answer_images":[],"answer_ET":"C","answers_community":["C (50%)","D (50%)"],"unix_timestamp":1732516980,"topic":"1","discussion":[{"upvote_count":"1","poster":"italiancloud2025","timestamp":"1740320400.0","comment_id":"1360519","content":"Selected Answer: D\nD: S칤, porque crea una 칰nica configuraci칩n de seguridad que especifica encriptaci칩n en reposo (con KMS) y en tr치nsito (usando el archivo PEM), y se adjunta al cl칰ster durante su creaci칩n."},{"timestamp":"1732516980.0","upvote_count":"1","poster":"emupsx1","comment_id":"1317352","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-specify-security-configuration.html"}],"timestamp":"2024-11-25 07:43:00","choices":{"A":"Create an Amazon EMR security configuration. Specify the appropriate AWS KMS key for at-rest encryption for the S3 bucket. Create a second security configuration. Specify the Amazon S3 path of the PEM file for in-transit encryption. Create the EMR cluster, and attach both security configurations to the cluster.","B":"Create an Amazon EMR security configuration. Specify the appropriate AWS KMS key for local disk encryption for the S3 bucket. Specify the Amazon S3 path of the PEM file for in-transit encryption. Use the security configuration during EMR cluster creation.","C":"Create an Amazon EMR security configuration. Specify the appropriate AWS KMS key for at-rest encryption for the S3 bucket. Specify the Amazon S3 path of the PEM file for in-transit encryption. Use the security configuration during EMR cluster creation.","D":"Create an Amazon EMR security configuration. Specify the appropriate AWS KMS key for at-rest encryption for the S3 bucket. Specify the Amazon S3 path of the PEM file for in-transit encryption. Create the EMR cluster, and attach the security configuration to the cluster."},"question_text":"A data engineer is launching an Amazon EMR cluster. The data that the data engineer needs to load into the new cluster is currently in an Amazon S3 bucket. The data engineer needs to ensure that data is encrypted both at rest and in transit.\n\nThe data that is in the S3 bucket is encrypted by an AWS Key Management Service (AWS KMS) key. The data engineer has an Amazon S3 path that has a Privacy Enhanced Mail (PEM) file.\n\nWhich solution will meet these requirements?","answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151948-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"answer":"C","question_id":92},{"id":"anmm1tWi66DStCtixWBQ","choices":{"A":"Use Amazon Redshift ML to generate inventory recommendations.","C":"Use Amazon Redshift ML to schedule regular data exports for offline model training.","D":"Use SageMaker Autopilot to create inventory management dashboards in Amazon Redshift.","B":"Use SQL to invoke a remote SageMaker endpoint for prediction.","E":"Use Amazon Redshift as a file storage system to archive old inventory management reports."},"answer":"AB","answer_images":[],"unix_timestamp":1732537260,"question_id":93,"discussion":[{"timestamp":"1736965980.0","comment_id":"1341198","upvote_count":"1","poster":"MerryLew","content":"Selected Answer: AB\nA and B \nRedshift ML for data exports? Nah. \nSageMaker autopilot is for building/training/deploying models\nRedshift for file storage?"},{"upvote_count":"1","poster":"emupsx1","content":"Selected Answer: AB\nThe company wants to make real-time inventory recommendations. Select (A) recommendations.\nThe company also wants to make predictions about future inventory needs. Select (B) prediction.","comment_id":"1319983","timestamp":"1732927320.0"}],"timestamp":"2024-11-25 13:21:00","topic":"1","answers_community":["AB (100%)"],"isMC":true,"question_text":"A retail company is using an Amazon Redshift cluster to support real-time inventory management. The company has deployed an ML model on a real-time endpoint in Amazon SageMaker.\n\nThe company wants to make real-time inventory recommendations. The company also wants to make predictions about future inventory needs.\n\nWhich solutions will meet these requirements? (Choose two.)","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151958-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_description":"","answer_ET":"AB","exam_id":21},{"id":"CdT3G4xWMQKLJuduAHAd","choices":{"D":"Use AWS Glue DataBrew recipes to read and transform the CSV files.","C":"Use an AWS Glue workflow to build a set of jobs to crawl and transform the CSV files.","A":"Use AWS Glue Python jobs to read and transform the CSV files.","B":"Use an AWS Glue custom crawler to read and transform the CSV files."},"topic":"1","answer_ET":"D","question_id":94,"question_images":[],"answer_images":[],"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/152017-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"D","discussion":[{"timestamp":"1734950760.0","comment_id":"1330781","content":"Selected Answer: D\nall more or less common operations all avilalble in data brew.","upvote_count":"1","poster":"HagarTheHorrible"},{"upvote_count":"1","poster":"emupsx1","comment_id":"1317807","timestamp":"1732575900.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/databrew/latest/dg/recipes.html"}],"exam_id":21,"timestamp":"2024-11-26 00:05:00","answers_community":["D (100%)"],"question_text":"A company stores CSV files in an Amazon S3 bucket. A data engineer needs to process the data in the CSV files and store the processed data in a new S3 bucket.\n\nThe process needs to rename a column, remove specific columns, ignore the second row of each file, create a new column based on the values of the first row of the data, and filter the results by a numeric value of a column.\n\nWhich solution will meet these requirements with the LEAST development effort?","unix_timestamp":1732575900},{"id":"D3tIYWgYonaVQ0E3J6JQ","choices":{"C":"Run the VACUUM REINDEX command against the identified tables.","B":"Run the ANALYZE COMPRESSION command against the identified tables. Manually update the compression encoding of columns based on the output of the command.","D":"Run the VACUUM RECLUSTER command against the identified tables.","A":"Run the ANALYZE command against the identified tables. Manually update the compression encoding of columns based on the output of the command."},"isMC":true,"answer_description":"","answer_ET":"B","topic":"1","timestamp":"2024-11-04 23:37:00","exam_id":21,"question_id":95,"question_text":"A company uses Amazon Redshift as its data warehouse. Data encoding is applied to the existing tables of the data warehouse. A data engineer discovers that the compression encoding applied to some of the tables is not the best fit for the data.\n\nThe data engineer needs to improve the data encoding for the tables that have sub-optimal encoding.\n\nWhich solution will meet this requirement?","url":"https://www.examtopics.com/discussions/amazon/view/150749-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"B","answers_community":["B (100%)"],"answer_images":[],"question_images":[],"discussion":[{"content":"Selected Answer: B\nAmazon Redshift uses columnar storage with compression encoding to optimize query performance and reduce storage costs. Over time, sub-optimal encoding may lead to poor performance.\n\nTo determine the best compression encoding for a table, use the ANALYZE COMPRESSION command, which:\n游댳 Scans the table's data and suggests optimal encoding types for each column.\n游댳 Helps reduce storage size and improve query efficiency.\n游댳 Requires a manual column update because Amazon Redshift does not automatically apply new encodings.","timestamp":"1742049060.0","poster":"Ramdi1","comment_id":"1398880","upvote_count":"1"},{"comment_id":"1307120","content":"Correct Answer: B\n\nANALYZE COMPRESSION Command: This command analyzes the data in the specified tables and provides recommendations for the best compression encoding for each column. It evaluates the current encoding and suggests more efficient options based on the actual data distribution.\nManual Update: After running the command, the data engineer can manually apply the recommended compression encodings to optimize storage and query performance.","poster":"kupo777","timestamp":"1730759820.0","upvote_count":"2"}],"unix_timestamp":1730759820}],"exam":{"numberOfQuestions":207,"name":"AWS Certified Data Engineer - Associate DEA-C01","isImplemented":true,"id":21,"isMCOnly":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":19},"__N_SSP":true}