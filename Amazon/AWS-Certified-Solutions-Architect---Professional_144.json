{"pageProps":{"questions":[{"id":"cq6wrR3qczleJ4PV83Rk","topic":"1","exam_id":32,"question_id":716,"isMC":true,"answer_images":[],"answers_community":["D (68%)","B (32%)"],"timestamp":"2021-04-30 12:12:00","question_images":[],"unix_timestamp":1619777520,"choices":{"A":"Create RDS MySQL read replicas. Deploy the application to multiple AWS Regions. Use a Route 53 latency-based routing policy to route to the application.","C":"Replace the DB instance with Amazon DynamoDB global tables. Deploy the application in multiple AWS Regions. Use a Route 53 latency-based routing policy to route to the application.","D":"Replace the DB instance with Amazon Aurora with Aurora Replicas. Deploy the application to multiple smaller EC2 instances across multiple Availability Zones in an Auto Scaling group behind an ALB.","B":"Configure the DB instance as Multi-AZ. Deploy the application to two additional EC2 instances in different Availability Zones behind an ALB."},"url":"https://www.examtopics.com/discussions/amazon/view/51225-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"comment_id":"359482","comments":[{"upvote_count":"5","comment_id":"439662","timestamp":"1635298020.0","poster":"Coffeinerd","content":"For sure D! Besides Aurora we have autoscaling -> no operational overhead on load events."},{"upvote_count":"1","comments":[{"poster":"mnsait","timestamp":"1734276600.0","content":"If instance size was a constraint, the question will need to indicate it. Else, we go with the use case presented. Plus B does not even have ASG. How will you address reliability without ASG in place?\n\nAnswer is D","comment_id":"1326941","upvote_count":"1"}],"poster":"czarno","timestamp":"1648747260.0","comment_id":"579078","content":"I don't think so.\nThere might be a reason why there are 2 HUGE ec2 instances running.\nMaybe the application needs this kind of a performance as it can't run in parallel.\nIn this case you can't just scale out... while scaling in.\nAurora would be nice, but not this time.\n\nAnswer B is correct"}],"timestamp":"1632529680.0","poster":"Waiweng","content":"it's D Aurora which provides the least amount of operational overhead","upvote_count":"21"},{"comment_id":"352194","poster":"beebatov","upvote_count":"6","content":"Answer: C\n\nB doesn't offer MAXIMUM resiliency, following the well architected framework's resiliency pillar, DR scenario must be considered. In this scenario we have a near real-time application, we would need DynamoDB + multi region for maximum resiliency for both App and DB. Moreover, we are working the development team that can switch from RDS to NoSQL.","comments":[{"upvote_count":"10","comment_id":"354953","content":"changing to D\n\nhttps://youtu.be/ZCt3ctVfGIk?t=111","poster":"beebatov","timestamp":"1632274860.0"},{"comments":[{"poster":"kadev","timestamp":"1662186540.0","content":"And you want to double large EC2 to another region + add more money for changing coding lol","upvote_count":"1","comment_id":"658127"}],"poster":"DashL","timestamp":"1633145160.0","upvote_count":"2","comment_id":"399485","content":"The question says \"achieve maximum reliability with the least amount of operational overhead\". RDS/Aurora has much higher operational overhead than DynamoDB."}],"timestamp":"1632101940.0"},{"upvote_count":"1","poster":"[Removed]","comment_id":"827473","timestamp":"1677794100.0","content":"Selected Answer: D\nNothing to do with cost, operation overhead will be reduced with option D, autoscaling and more smaller instances mean more resiliency. If the question was cost related or least changes required upfront i would probably go B."},{"comment_id":"763751","poster":"evargasbrz","upvote_count":"1","timestamp":"1672666980.0","content":"Selected Answer: D\nI'll go with D"},{"timestamp":"1662523680.0","comment_id":"661958","upvote_count":"1","poster":"cale","content":"Selected Answer: D\nIt's D"},{"timestamp":"1662186300.0","upvote_count":"1","poster":"kadev","content":"\"operational overhead\" => saving cost\nB/D\n\n1. Currently, App in \"two large Amazon EC2\" in 1 AZ, we can saving cost by smaller EC2 + Autoscaling in multi A-Z , not adds more large EC2 \n2. RDS multi AZ, that mean 2 instance equaly, double cost. With replicas, you can chose a maller RDS type for savign cost.\n\n===> Finally, D","comment_id":"658124"},{"poster":"KiraguJohn","comment_id":"640363","content":"Which one is more costly? \n1. Changing existing RDS to multi AZ or\n2. Converting the existing RDS to Amazon Aurora","comments":[{"poster":"kadev","content":"RDS to multi AZ => two instance equally , double cost\nAmazon Aurora or RDS ( with replicas , read Q carefully ) => you can pick small RDS type for replica instance","timestamp":"1662186660.0","comment_id":"658128","upvote_count":"1"}],"timestamp":"1659332940.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nB for me\nSeem that the question has change, it's say that it needs minimize operational expense, which made me choose B over D. Using Aurora will cost you more","poster":"TechX","timestamp":"1656396360.0","comment_id":"623756","comments":[{"upvote_count":"2","timestamp":"1667337420.0","poster":"Cal88","comment_id":"709442","content":"Operational overhead not expense.\nRead the question carefully\nCost is not a concern here but availability is.\nAdding two large nodes will not guarantee HA , but auto scaling will\nSo D is better I am sorry"}]},{"poster":"kangtamo","comment_id":"623327","timestamp":"1656338940.0","content":"Selected Answer: D\nAgree with D: Aurora.","upvote_count":"1"},{"content":"Selected Answer: D\nAns: D","timestamp":"1647735360.0","comment_id":"571319","poster":"azure_kai","upvote_count":"2"},{"comment_id":"564941","upvote_count":"2","poster":"razerlg","timestamp":"1646936460.0","content":"Selected Answer: B\nIn my opinion D doesnt guarantee multi-AZ unless the replica is placed in a different AZ, and that is not specified. I would choose B"},{"poster":"johnnsmith","upvote_count":"2","comment_id":"553444","timestamp":"1645504500.0","content":"B is correct. A and C are wrong because Route 53 health check is missing. D is wrong because it doesn't say \"replica in a different AZ\" or Multi-AZ. Only B can still function when an AZ fails,"},{"upvote_count":"1","poster":"pititcu667","comment_id":"547085","content":"Selected Answer: B\nB because they specifically mention least amount of effort while minimizing costs.","timestamp":"1644838620.0"},{"content":"Selected Answer: B\nDid they change the question ..? I'm reading \"The development team is tasked with enhancing the solution's dependability while minimizing operating expenses.\"\n\nSwitching to Aurora will incur a big cost.. you can simply setup Multi-AZ, and switch the instances to be in different AZs. It is not the most resilient architecture but it is improved and the most cost-effective one here.","poster":"futen0326","timestamp":"1644744600.0","upvote_count":"2","comment_id":"546356"},{"poster":"kyo","timestamp":"1644041220.0","upvote_count":"1","comment_id":"540800","content":"D is better than B."},{"comment_id":"532281","content":"Selected Answer: D\nD my opinion","timestamp":"1643129820.0","poster":"kubala","upvote_count":"3"},{"content":"Selected Answer: D\nDDDDDDDDDDD","timestamp":"1643089680.0","comment_id":"531886","upvote_count":"4","poster":"cannottellname"},{"timestamp":"1641025740.0","comment_id":"514432","content":"D: Aurora + ASG + ELB + MultiAZ","upvote_count":"1","poster":"cldy"},{"timestamp":"1639192680.0","content":"My Answer: D \nFor sure","comment_id":"499066","upvote_count":"1","poster":"challenger1"},{"comment_id":"497136","poster":"AzureDP900","content":"D is right answer","timestamp":"1638992400.0","upvote_count":"1"},{"poster":"Kopa","comment_id":"474200","timestamp":"1636364640.0","content":"Im going for D, Aura the best for this scenario and also small instances are more high available then bigger instances on ASG.","upvote_count":"1"},{"poster":"Viper57","content":"D is clearly the best option here. The question asks for reliability. While having the app deployed in multiple regions might provide this, none of them mention using an auto scaling group other than D. This would involve manually scaling instances!\n\nD. Multi AZ ASG + ALB + Aurora = Less over head and automatic scaling","upvote_count":"4","comment_id":"451356","timestamp":"1636267680.0"},{"comment_id":"448274","content":"It's D","upvote_count":"1","poster":"andylogan","timestamp":"1636105980.0"},{"upvote_count":"1","comment_id":"445519","content":"It has to be C. Maximum reliability means multi-region deployment. Only C does that. D doesn't do that as EC2 are in the same region.","poster":"johnnsmith","timestamp":"1636052100.0"},{"comment_id":"440517","content":"D is correct\n1. Amazon Aurora with Aurora Replicas that will be added or removed depending on a workload parameter\n2. Auto-scaling of EC2 instances that will be added or removed\n3. Smaller instances that will be scaled in/down\n4. All AZ used to protect high availability\n\nA&B wrong - nonsense\nC wrong - why you really need DynamoDB if you only want high availability","timestamp":"1635422460.0","upvote_count":"1","poster":"DerekKey"},{"timestamp":"1635348240.0","content":"D\nMax Reliability = Aurora Replicas for DB + Auto Scaling Group for the app","poster":"student22","comment_id":"440080","upvote_count":"1"},{"poster":"denccc","timestamp":"1635047640.0","upvote_count":"1","content":"would go for D","comment_id":"435626"},{"upvote_count":"1","comment_id":"435022","timestamp":"1634923140.0","poster":"Suresh108","content":"I am choosing DDDDD. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html#Concepts.AuroraHighAvailability.Data"},{"timestamp":"1634560980.0","poster":"tgv","upvote_count":"2","comment_id":"434686","content":"DDD\n---\nI was inclined to choose B because of the \"least amount of operational overhead\", but the questions is also asking for \"maximum reliability\" so I'm not comfortable with not having an Auto Scaling group."},{"poster":"blackgamer","timestamp":"1634066640.0","content":"It is D. B will work too but D give highest reliability with least operational overhead.","comment_id":"434382","upvote_count":"3"},{"poster":"sergioandreslq","content":"For me it is B, Reliability with least amount of operational overhead:\n1. Enable Multi-AZ for reliability for DB.\n2. Create ASG with EC2 in different AZs for Apps.\nThe others are good too, especially D, however, I believe B is the easier.","upvote_count":"1","comment_id":"434097","comments":[{"content":"B doesn't say they are using ASG. So D.","upvote_count":"2","timestamp":"1635913020.0","poster":"Kitty0403","comment_id":"444075"}],"timestamp":"1633879620.0"},{"poster":"tiffanny","timestamp":"1633872120.0","content":"A. possible, but if something happen there is a lot of operational work to do (ex : promoting read replica. and also this one is not very certain since read replica will replicate inside region unless you configure it to global read replica\nB. adding 2 additional Ec2 is not correct and not best practice. its vertical scaling , and aws suggest to use horizontal scaling\nC. Not every database suit DynamoDb , will choose this if they say more about the data structure\nD. Aurora is managed. This one is the best over all the choicess \nans : D","comments":[{"timestamp":"1634833140.0","upvote_count":"2","content":"B. adding 2 additional Ec2 is not correct and not best practice. its vertical scaling , and aws suggest to use horizontal scaling\n\n-- it is called horizontal scaling. scaling out.","poster":"Suresh108","comment_id":"435015"}],"comment_id":"431366","upvote_count":"1"},{"upvote_count":"4","comments":[{"content":"Also, DynamoDB require schema conversion AND refactor the Application to work with Dynamo and this is a lot of effort","upvote_count":"2","poster":"WhyIronMan","comment_id":"414146","timestamp":"1633749840.0"},{"content":"Also, Replacing the DB instance with Amazon Aurora with Aurora requires a lot of effort than compared with just setting the RDS as multiAZ.","poster":"WhyIronMan","comment_id":"414147","upvote_count":"2","timestamp":"1633781940.0"},{"poster":"Shanmahi","comment_id":"438115","content":"I feel that the condition of \"least operational overhead\" is intended for how to manage from operations perspective post deployment - i.e. building operationally efficient solution. Hence, \"maximum resiliency\" will take over in this specific question & option D does provide that solve. It is worth noting as you have mentioned, D will require more \"development\" effort however operationally it will require least effort to maintain.","timestamp":"1635285180.0","upvote_count":"3"}],"timestamp":"1633716960.0","content":"I'll go with B.\nMAXIMUM resiliency INSIDE the least amount of operational overhead\nD requires a lot more of operation overhead compared to B","poster":"WhyIronMan","comment_id":"414144"},{"comment_id":"403270","upvote_count":"3","timestamp":"1633671240.0","content":"It is D","poster":"qurren"},{"content":"I think the question is somewhat ambiguous. In my mind “improve” is mostly “keep existing arch at core, but improve upon it” which is B. However the ask is for the least operational overhead, which is C.","comment_id":"400467","upvote_count":"1","timestamp":"1633610760.0","poster":"vimgoru24"},{"timestamp":"1633554300.0","comment_id":"400459","content":"I go for B. Others are also great, but the least amount of effort to keep the current arch and performance baseline is B","poster":"vimgoru24","upvote_count":"1"},{"poster":"hk436","content":"D is my answer!!","upvote_count":"2","timestamp":"1633020600.0","comment_id":"385733"},{"upvote_count":"2","poster":"mustpassla","content":"A & C are also offer MAX resiliency, but the question mentioned \"least amount of operational overhead.\", so A.","comments":[{"content":"A seems tempting but in generate read replica is multi AZ. Though Application is deployed in multi-region so it may not work. To me both C and D are good but C would require to re-factor the application. Will stick with D. Not sure though as the question is open ended.","comment_id":"392486","upvote_count":"1","timestamp":"1633033500.0","poster":"Gladabhi"}],"timestamp":"1632771480.0","comment_id":"367686"},{"timestamp":"1632712980.0","content":"the answer is D,","comment_id":"364732","upvote_count":"1","poster":"vkbajoria"},{"upvote_count":"1","timestamp":"1632463320.0","poster":"tvs","comment_id":"356017","content":"it is D. near real time data , so need replicas . in event of zone failure you can promote it as primary and autoscaling will take care EC2."},{"comments":[{"poster":"gsw","comment_id":"346087","timestamp":"1632100860.0","content":"the answer is B","upvote_count":"5","comments":[{"content":"least amount of operational overhead. There is no ASG in B, so we must manually scale them which includes operational overhead. B is the wrong answer.!","comment_id":"385735","timestamp":"1633029480.0","upvote_count":"3","poster":"hk436"}]}],"timestamp":"1632075000.0","content":"why not A?","upvote_count":"1","poster":"gsw","comment_id":"346086"}],"answer_ET":"D","answer":"D","question_text":"A development team has created a new flight tracker application that provides near-real-time data to users. The application has a front end that consists of an\nApplication Load Balancer (ALB) in front of two large Amazon EC2 instances in a single Availability Zone. Data is stored in a single Amazon RDS MySQL DB instance. An Amazon Route 53 DNS record points to the ALB.\nManagement wants the development team to improve the solution to achieve maximum reliability with the least amount of operational overhead.\nWhich set of actions should the team take?","answer_description":""},{"id":"kAcdcsFRdKbw05UhsiVf","isMC":true,"answer":"AC","url":"https://www.examtopics.com/discussions/amazon/view/74168-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"poster":"snakecharmer2","content":"Selected Answer: AC\nA & C\neasy one...","timestamp":"1650717240.0","upvote_count":"10","comment_id":"590614"},{"timestamp":"1699589220.0","content":"Selected Answer: AC\nStorage is unpredictable,so with ebs this solution is incomplete","poster":"Rs1084","comment_id":"1066936","upvote_count":"1"},{"poster":"ravisar","content":"Answer is AC. AWS Transit Gateway is Enables for attaching VPNs and Different VPC on the same region for different AWS accounts. Nothing to do content distribution.","comment_id":"590233","timestamp":"1650658560.0","upvote_count":"4"}],"question_id":717,"unix_timestamp":1650658560,"answer_images":[],"choices":{"E":"Use AWS Shield Standard to protect the application","D":"Use AWS WAF and Amazon Detective to protect the application","A":"Use Amazon Route 53 and Amazon CloudFront for content distribution. Use Amazon S3 to store static content","B":"Use Amazon Route 53 and AWS Transit Gateway for content distribution. Use an Amazon Elastic Block Store (Amazon EBS) volume to store static content","C":"Use AWS WAF with AWS Shield Advanced to protect the application"},"question_text":"A multimedia company with a single AWS account is launching an application for a global user base. The application storage and bandwidth requirements are unpredictable. The application will use Amazon EC2 instances behind an Application Load Balancer as the web tier and will use Amazon DynamoDB as the database tier. The environment for the application must meet the following requirements:\n✑ Low latency when accessed from any part of the world\n✑ WebSocket support\n✑ End-to-end encryption\nProtection against the latest security threats\n//IMG//\n\n✑ Managed layer 7 DDoS protection\nWhich actions should the solutions architect take to meet these requirements? (Choose two.)","timestamp":"2022-04-22 22:16:00","exam_id":32,"topic":"1","answer_description":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04241/0047600004.png"],"answer_ET":"AC","answers_community":["AC (100%)"]},{"id":"eL9cPXKkzmvZiCdTIgYz","isMC":true,"question_text":"A company is using AWS Organizations to manage 15 AWS accounts. A solutions architect wants to run advanced analytics on the company's cloud expenditures. The cost data must be gathered and made available from an analytics account. The analytics application runs in a VPC and must receive the raw cost data each night to run the analytics.\nThe solutions architect has decided to use the Cost Explorer API to fetch the raw data and store the data in Amazon S3 in JSON format. Access to the raw cost data must be restricted to the analytics application. The solutions architect has already created an AWS Lambda function to collect data by using the Cost Explorer\nAPI.\nWhich additional actions should the solutions architect take to meet these requirements?","topic":"1","exam_id":32,"discussion":[{"upvote_count":"17","content":"Agreed, A is correct. C could be correct except for the part about restricting access using a bucket policy with aws:SourceIp which leaves A.\nSee https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html#vpc-endpoints-s3-bucket-policies","timestamp":"1633054500.0","comment_id":"382145","poster":"wowznuz"},{"poster":"beebatov","timestamp":"1632665400.0","upvote_count":"11","content":"Answer: A\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html\n\nthe administrator in the management account can create a role to grant cross-account permissions to a user in a member account as follows:\n\nThe management account administrator creates an IAM role and attaches a permissions policy to the role that grants permissions to the organization's resources.\n\nThe management account administrator attaches a trust policy to the role that identifies the member account ID as the Principal who can assume the role.\n\nThe member account administrator can then delegate permissions to assume the role to any users in the member account. Doing this allows users in the member account to create or access resources in the management account and the organization. The principal in the trust policy can also be an AWS service principal if you want to grant permissions to an AWS service to assume the role.","comment_id":"352260"},{"upvote_count":"1","content":"Selected Answer: A\nC could have been correct if aws:VpcSourceIp was mentioned instead of aws:SourceIp\nhttps://repost.aws/knowledge-center/block-s3-traffic-vpc-ip","poster":"rsn","comment_id":"991086","timestamp":"1693090200.0"},{"poster":"[Removed]","content":"Selected Answer: A\nC almost looks right, although you would use Aws:sourceip in a bucket policy when accessing it via internet. Use sourcevpc or sourcevpce when using an interface/gateway endpoint.","timestamp":"1677799620.0","comment_id":"827533","upvote_count":"2"},{"content":"https://docs.aws.amazon.com/cost-management/latest/userguide/ce-api-best-practices.html\n\nAn IAM user must be granted explicit permission to query the Cost Explorer API. Granting an IAM user access to the Cost Explorer API gives that user query access to any cost and usage data available to that account.\nB","timestamp":"1672839300.0","upvote_count":"1","poster":"maxh8086","comment_id":"765697"},{"upvote_count":"2","timestamp":"1663250400.0","comment_id":"669990","poster":"pinhead900","content":"answer is A - but still lambda would need to run in VPC for it to be able to use the Gateway endpoint, that part is missing."},{"timestamp":"1661623380.0","upvote_count":"3","comment_id":"652655","poster":"Andykris","content":"C has interface endpoint which is incorrect for S3.","comments":[{"content":"Correct answer is A","timestamp":"1661623440.0","upvote_count":"1","comment_id":"652657","poster":"Andykris"}]},{"upvote_count":"1","comment_id":"622767","content":"Selected Answer: A\nAgree with A. Gateway endpoint for S3.","poster":"kangtamo","timestamp":"1656271680.0"},{"content":"A is correct, S3 gateway endpoint for access within analytics VPC from analytics application.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html","upvote_count":"2","comment_id":"497143","timestamp":"1638992880.0","poster":"AzureDP900"},{"upvote_count":"1","content":"It's A","comment_id":"448279","timestamp":"1636299180.0","poster":"andylogan"},{"timestamp":"1636214400.0","content":"AAA\n---\nThe cost of the AWS Organization is visible in the master account so B & D are out.\nA is a more best practice approach. No need for an interface endpoint","poster":"tgv","comment_id":"434738","upvote_count":"4"},{"upvote_count":"1","timestamp":"1635798180.0","comment_id":"434735","content":"AAA\n---\nThe cost of the AWS Organization is visible in the master account so B & D are out.\nA is a more best practice approach. No need for an interface endpoint","poster":"tgv"},{"poster":"blackgamer","timestamp":"1635659520.0","content":"A for sure","upvote_count":"1","comment_id":"434389"},{"timestamp":"1635149820.0","upvote_count":"1","content":"It is D. In A, the Lambda does not have permissions to write in the S3 bucket because it is not able to access the endpoint","comment_id":"428243","poster":"pablobairat"},{"timestamp":"1634614140.0","upvote_count":"2","comment_id":"422391","content":"agreed A is correct cause S3 supported only Gateway and DynamoDB endpoints\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway.html","poster":"rodolfo2020"},{"comment_id":"415482","content":"going for A","poster":"Kopa","timestamp":"1634423340.0","upvote_count":"1"},{"poster":"WhyIronMan","timestamp":"1634407140.0","comment_id":"414157","content":"I'll go with A","upvote_count":"3"},{"timestamp":"1633811820.0","content":"I believe A is not the only option here, however it is the most secure one, so A.","poster":"vimgoru24","comments":[{"upvote_count":"1","timestamp":"1634007420.0","content":"Even though the new interface VPC endpoint option seems correct, that last part of the answer to use VPC CIDR range as sourceIP is incorrect. Only public Ip addresses are supported.","poster":"student2020","comment_id":"408041"}],"comment_id":"400484","upvote_count":"2"},{"timestamp":"1633669320.0","content":"A is my answer!!","poster":"hk436","upvote_count":"4","comment_id":"385737"},{"poster":"mustpassla","comment_id":"368386","timestamp":"1632886800.0","upvote_count":"2","content":"A, org acc and gateway endpoint"},{"timestamp":"1632689760.0","content":"A sounds right because it's in an organization and master account has all the cost related data. Analysis account wants to run the api by assuming the role.","poster":"vkbajoria","comment_id":"364741","upvote_count":"3"},{"content":"A is correct, S3 gateway endpoint for access within analytics VPC from analytics application. IAM role in Master management account with trust relationship policy for the analytics account and sts:AssumeRole from Lambda application from analytics VPC to call the cost explorer APIs.","upvote_count":"3","timestamp":"1632502740.0","poster":"kpcert","comment_id":"351373"},{"poster":"miniso8153","timestamp":"1632476760.0","comment_id":"350885","content":"I choose A","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/51974-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"A","unix_timestamp":1620293580,"question_images":[],"choices":{"A":"Create an IAM role in the Organizations master account with permissions to use the Cost Explorer API, and establish trust between the role and the analytics account. Update the Lambda function role and add sts:AssumeRole permissions. Assume the role in the master account from the Lambda function code by using the AWS Security Token Service (AWS STS) AssumeRole API call. Create a gateway endpoint for Amazon S3 in the analytics VPC. Create an S3 bucket policy that allows access only from the S3 endpoint.","B":"Create an IAM role in the analytics account with permissions to use the Cost Explorer API. Update the Lambda function and assign the new role. Create a gateway endpoint for Amazon S3 in the analytics VPC. Create an S3 bucket policy that allows access only from the analytics VPC by using the aws:SourceVpc condition.","C":"Create an IAM role in the Organizations master account with permissions to use the Cost Explorer API, and establish trust between the role and the analytics account. Update the Lambda function role and add sts:AssumeRole permissions. Assume the role in the master account from the Lambda function code by using the AWS Security Token Service (AWS STS) AssumeRole API call. Create an interface endpoint for Amazon S3 in the analytics VPC. Create an S3 bucket policy that allows access only from the analytics VPC private CIDR range by using the aws:SourceIp condition.","D":"Create an IAM role in the analytics account with permissions to use the Cost Explorer API. Update the Lambda function and assign the new role. Create an interface endpoint for Amazon S3 in the analytics VPC. Create an S3 bucket policy that allows access only from the S3 endpoint."},"question_id":718,"answer_description":"","answer_images":[],"answers_community":["A (100%)"],"timestamp":"2021-05-06 11:33:00","answer_ET":"A"},{"id":"BLb59MAfNpOUaFoyQ4Ze","topic":"1","discussion":[{"content":"D seems correct","upvote_count":"12","comment_id":"346094","poster":"gsw","timestamp":"1632666960.0","comments":[{"comment_id":"460256","content":"Adding one more link for architecture: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html","poster":"StelSen","upvote_count":"1","timestamp":"1635678000.0"},{"poster":"ExtHo","upvote_count":"9","timestamp":"1632723840.0","content":"Yes its D :)\nTransfer 30TB over 50Mbps will take around 53Days that ruled out ABC\nhttps://www.calctool.org/CALC/prof/computing/transfer_time","comment_id":"349827"}]},{"comment_id":"939399","timestamp":"1688160540.0","content":"Selected Answer: D\n50Mbps = 6.25 MBPS = 22GB per hour = 540 (.5TB) per day. 30TB / .5 = 60 days... Snowball is the only option here","poster":"SkyZeroZx","upvote_count":"1"},{"content":"Selected Answer: D\n50Mbps = 6.25 MBPS = 22GB per hour = 540 (.5TB) per day. 30TB / .5 = 60 days... Snowball is the only option here","upvote_count":"1","timestamp":"1677799800.0","poster":"[Removed]","comment_id":"827535"},{"content":"D. Create a job in AWS Snowball Edge to import data into Amazon S3. Install AWS SCT extraction agents on the on-premises servers. Define the local and AWS Database Migration Service (AWS DMS) tasks to send the data to the Snowball Edge device. When the Snowball Edge device is returned to AWS and the data is available in Amazon S3, run the AWS DMS subtask to copy the data to Amazon Redshift.","poster":"cldy","comment_id":"495622","timestamp":"1638854100.0","upvote_count":"1"},{"timestamp":"1638264900.0","comment_id":"490550","content":"I go with D","poster":"backfringe","upvote_count":"1"},{"timestamp":"1638050100.0","poster":"AzureDP900","comment_id":"488524","upvote_count":"1","content":"D is best option, data transfer using corporate internet takes around 58 days!"},{"comment_id":"448280","poster":"andylogan","upvote_count":"1","content":"It's D","timestamp":"1635112200.0"},{"upvote_count":"3","poster":"tgv","content":"DDD\n---\nYou cannot migrate 20 TB over 50 Mbps in 2 weeks without Snowball","comment_id":"434740","timestamp":"1634925420.0"},{"upvote_count":"2","comment_id":"414159","poster":"WhyIronMan","timestamp":"1634734500.0","content":"I'll go with D\nfor data > 20TB use Snowball"},{"poster":"vimgoru24","upvote_count":"1","content":"D. This is the way.","timestamp":"1634603160.0","comment_id":"400485"},{"comment_id":"385742","upvote_count":"1","poster":"hk436","content":"D is my answer!!","timestamp":"1634569440.0"},{"comment_id":"359488","poster":"Waiweng","content":"it's D","timestamp":"1634078340.0","upvote_count":"2"},{"comment_id":"356032","timestamp":"1634021640.0","content":"it is D . Since there is no online data to sync , snowball is best option. Also with 50Mbs you can able to transfer 7Tb only in 14 days.","upvote_count":"3","poster":"tvs"},{"upvote_count":"4","poster":"beebatov","content":"Answer: D\n\nAWS Database Migration Service (AWS DMS) can use Snowball Edge and Amazon S3 to migrate large databases more quickly than by other methods\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html","comment_id":"352207","timestamp":"1632994980.0"}],"choices":{"C":"Install the AWS SCT extraction agents on the on-premises servers. Create a Site-to-Site VPN connection. Create an AWS Database Migration Service (AWS DMS) replication instance that is the appropriate size. Authorize the IP address of the replication instance to be able to access the on-premises data warehouse through the VPN connection.","B":"Install the AWS SCT extraction agents on the on-premises servers. Define the extract, upload, and copy tasks to send the data to an Amazon S3 bucket. Copy the data into the Amazon Redshift cluster. Run the tasks at the beginning of the data freeze period.","A":"Create an AWS Database Migration Service (AWS DMS) replication instance. Authorize the public IP address of the replication instance to reach the data warehouse through the corporate firewall. Create a migration task to run at the beginning of the fata freeze period.","D":"Create a job in AWS Snowball Edge to import data into Amazon S3. Install AWS SCT extraction agents on the on-premises servers. Define the local and AWS Database Migration Service (AWS DMS) tasks to send the data to the Snowball Edge device. When the Snowball Edge device is returned to AWS and the data is available in Amazon S3, run the AWS DMS subtask to copy the data to Amazon Redshift."},"unix_timestamp":1619779860,"answer_images":[],"answer":"D","exam_id":32,"question_text":"A company wants to migrate a 30 TB Oracle data warehouse from on premises to Amazon Redshift. The company used the AWS Schema Conversion Tool (AWS\nSCT) to convert the schema of the existing data warehouse to an Amazon Redshift schema. The company also used a migration assessment report to identify manual tasks to complete.\nThe company needs to migrate the data to the new Amazon Redshift cluster during an upcoming data freeze period of 2 weeks. The only network connection between the on-premises data warehouse and AWS is a 50 Mbps internet connection.\nWhich migration strategy meets these requirements?","answers_community":["D (100%)"],"answer_ET":"D","timestamp":"2021-04-30 12:51:00","question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/51230-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true,"question_id":719},{"id":"aOO3UmQjDOYBKA6V16Dz","isMC":true,"question_text":"A company that tracks medical devices in hospitals wants to migrate its existing storage solution to the AWS Cloud. The company equips all of its devices with sensors that collect location and usage information. This sensor data is sent in unpredictable patterns with large spikes. The data is stored in a MySQL database running on premises at each hospital. The company wants the cloud storage solution to scale with usage.\nThe company's analytics team uses the sensor data to calculate usage by device type and hospital. The team needs to keep analysis tools running locally while fetching data from the cloud. The team also needs to use existing Java application and SQL queries with as few changes as possible.\nHow should a solutions architect meet these requirements while ensuring the sensor data is secure?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/51232-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"discussion":[{"poster":"Jaypdv","timestamp":"1632138300.0","content":"Going with C. because it works with the app still running on-prem. You will still need a few code changes but the question allows that.\nD. would sound plausible but PrivateLink is for VPC only","comments":[{"timestamp":"1632233340.0","content":"Existing MySQL database can't be easy to serve from S3 (object storage) that ruled out S3 option and NLB is also not good option that leaves only C","comment_id":"349832","upvote_count":"4","poster":"ExtHo"},{"poster":"justfmm","comment_id":"425626","content":"https://aws.amazon.com/blogs/aws/aws-privatelink-for-amazon-s3-now-available/","timestamp":"1634965440.0","upvote_count":"2"}],"comment_id":"346138","upvote_count":"16"},{"content":"Answer: C\n\nhttps://aws.amazon.com/blogs/aws/new-data-api-for-amazon-aurora-serverless/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html","poster":"beebatov","timestamp":"1632662040.0","upvote_count":"7","comment_id":"352242"},{"timestamp":"1708879680.0","comment_id":"1158972","poster":"pkaarthick","content":"Selected Answer: C\nOption C (Aurora Data API), as B & D are about S3 (SQL query needed here) - ruled out","upvote_count":"1"},{"upvote_count":"2","timestamp":"1664511540.0","comment_id":"683221","poster":"JohnPi","content":"I wonder how option C satisfies the statement \"use existing Java application and SQL queries with as few changes as possible\"? btw Aurora Serverless has an \"invisible\" NLB in front"},{"poster":"dcdcdc3","content":"Selected Answer: C\nB&D - no, as SQL queries are desired..\nA - no, unless anyone can show me how this works: \"Authenticate users using the NLB with credentials stored in AWS Secrets Manager.\"","comment_id":"681312","upvote_count":"1","timestamp":"1664324160.0"},{"comment_id":"497151","timestamp":"1638993300.0","content":"C is right choice.","upvote_count":"2","poster":"AzureDP900"},{"poster":"cldy","comment_id":"496737","content":"C. Store the data in an Amazon Aurora Serverless database. Serve the data through the Aurora Data API using an IAM user authorized with AWS Identity and Access Management (IAM) and the AWS Secrets Manager ARN.","upvote_count":"1","timestamp":"1638960240.0"},{"content":"C) Store the data in an Amazon Aurora Serverless database. Serve the data through the Aurora Data API using an IAM user authorized with AWS Identity and Access Management (IAM) and the AWS Secrets Manager ARN.","comment_id":"488521","timestamp":"1638049560.0","upvote_count":"1","poster":"AzureDP900"},{"upvote_count":"1","content":"It's C","poster":"andylogan","timestamp":"1636239360.0","comment_id":"448285"},{"content":"going for C, as app code needs no change","comment_id":"440995","poster":"Kopa","upvote_count":"1","timestamp":"1636195920.0"},{"content":"C is correct - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html#data-api.access","timestamp":"1636185600.0","upvote_count":"2","comment_id":"440529","poster":"DerekKey"},{"upvote_count":"1","poster":"tgv","timestamp":"1635703920.0","comment_id":"434748","content":"CCC\n---\nThe data is currently stored in a MySQL database running on-prem. Storing MySQL data in S3 doesn't sound good so B & D are out.\nAurora Data API \"enables the SQL HTTP endpoint, a connectionless Web Service API for running SQL queries against this database. When the SQL HTTP endpoint is enabled, you can also query your database from inside the RDS console (these features are free to use).\""},{"comment_id":"434392","content":"C for me","timestamp":"1635609180.0","poster":"blackgamer","upvote_count":"1"},{"upvote_count":"2","content":"All of this answers are bad in real world, but for the purpose of the exam - C is the answer","poster":"vimgoru24","comment_id":"400494","timestamp":"1634823780.0"},{"content":"My answer is D. \n The team also needs to use existing Java application and SQL queries with as few changes as possible.\nTHere is a requirement to be able to execute sql queries. Athena provides the way!!","comment_id":"385745","poster":"hk436","comments":[{"content":"s3 through private link? i dont think so matey","timestamp":"1633714440.0","poster":"MrCarter","comments":[{"content":"i stand corrected s3 and privatelink is a thing","poster":"MrCarter","upvote_count":"2","comment_id":"396316","timestamp":"1633899660.0"},{"poster":"MrCarter","upvote_count":"2","timestamp":"1634542980.0","comment_id":"396317","content":"i stand corrected s3 and privatelink is a thing"}],"comment_id":"396315","upvote_count":"1"},{"poster":"MrCarter","comment_id":"396320","upvote_count":"1","comments":[{"comment_id":"440523","timestamp":"1635724620.0","content":"Wrong if you think it is not possible.","upvote_count":"1","poster":"DerekKey"}],"content":"The team needs to keep analysis tools running locally while fetching data from the cloud. That is why it has to be C","timestamp":"1634617260.0"}],"timestamp":"1633706220.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1633501200.0","content":"Going with C.","poster":"Kukkuji","comment_id":"371533"},{"comments":[{"timestamp":"1633473360.0","comment_id":"368399","content":"Change to C, coz A use NLB","poster":"mustpassla","upvote_count":"1"}],"upvote_count":"1","comment_id":"368393","poster":"mustpassla","timestamp":"1633414260.0","content":"A, unpredictable patterns with large spikes & less change."},{"upvote_count":"2","comment_id":"359493","timestamp":"1633359720.0","poster":"Waiweng","content":"it's C"},{"poster":"PLAINsboro","upvote_count":"2","comment_id":"357478","timestamp":"1633060980.0","content":"Going with C: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html#data-api.access\nhe AmazonRDSDataFullAccess policy also includes permissions for the user to get the value of a secret from AWS Secrets Manager. Users need to use Secrets Manager to store secrets that they can use in their calls to the Data API. Using secrets means that users don't need to include database credentials for the resources that they target in their calls to the Data API."},{"content":"Answer: A https://aws.amazon.com/blogs/aws/aurora-serverless-ga/","comments":[{"timestamp":"1636080360.0","comment_id":"440526","upvote_count":"2","poster":"DerekKey","content":"Authenticate users using the NLB with credentials stored in AWS Secrets Manager.\nInteresting"}],"timestamp":"1632698220.0","upvote_count":"1","poster":"ben2020","comment_id":"353316"},{"upvote_count":"1","content":"None of these make sense - you can't serve data through a Network Load Balancer the answer can't be A","poster":"gsw","timestamp":"1632092160.0","comment_id":"346097"}],"answer":"C","unix_timestamp":1619780220,"question_images":[],"choices":{"C":"Store the data in an Amazon Aurora Serverless database. Serve the data through the Aurora Data API using an IAM user authorized with AWS Identity and Access Management (IAM) and the AWS Secrets Manager ARN.","D":"Store the data in an Amazon S3 bucket. Serve the data through Amazon Athena using AWS PrivateLink to secure the data in transit.","B":"Store the data in an Amazon S3 bucket. Serve the data through Amazon QuickSight using an IAM user authorized with AWS Identity and Access Management (IAM) with the S3 bucket as the data source.","A":"Store the data in an Amazon Aurora Serverless database. Serve the data through a Network Load Balancer (NLB). Authenticate users using the NLB with credentials stored in AWS Secrets Manager."},"question_id":720,"answer_description":"","answer_images":[],"answers_community":["C (100%)"],"timestamp":"2021-04-30 12:57:00","answer_ET":"A"}],"exam":{"isMCOnly":false,"isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","id":32,"numberOfQuestions":1019,"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon"},"currentPage":144},"__N_SSP":true}