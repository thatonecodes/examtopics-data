{"pageProps":{"questions":[{"id":"IchOgpcSrfdy2OclNMZ4","exam_id":33,"answer_images":[],"question_images":[],"question_text":"To abide by industry regulations, a solutions architect must design a solution that will store a company's critical data in multiple public AWS Regions, including in the United States, where the company's headquarters is located. The solutions architect is required to provide access to the data stored in AWS to the companyâ€™s global WAN network. The security team mandates that no traffic accessing this data should traverse the public internet.\n\nHow should the solutions architect design a highly available solution that meets the requirements and is cost-effective?","answer_description":"","answer":"D","topic":"1","answer_ET":"D","unix_timestamp":1707168900,"url":"https://www.examtopics.com/discussions/amazon/view/132963-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions.","C":"Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use an AWS transit VPC solution to access data in other AWS Regions.","B":"Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use inter-region VPC peering to access the data in other AWS Regions.","A":"Establish AWS Direct Connect connections from the company headquarters to all AWS Regions in use. Use the company WAN to send traffic over to the headquarters and then to the respective DX connection to access the data."},"answers_community":["D (85%)","Other"],"question_id":366,"timestamp":"2024-02-05 22:35:00","discussion":[{"upvote_count":"5","comment_id":"1229053","timestamp":"1718195340.0","content":"Selected Answer: D\nDX + DXGW","poster":"trungtd"},{"timestamp":"1732577520.0","content":"Selected Answer: B\nThe most cost-effective option for accessing data across multiple AWS Regions while using AWS Direct Connect is B.","poster":"TomTom","upvote_count":"1","comment_id":"1317818"},{"poster":"AzureDP900","timestamp":"1731354420.0","comment_id":"1310287","upvote_count":"3","content":"This solution meets the requirements and is cost-effective.\n\n\nOption D\nAWS Direct Connect: This service allows you to establish a dedicated network connection between your premises and an AWS Region, which bypasses the public internet.\nTwo connections: Establishing two connections provides redundancy, ensuring that if one connection fails, the other can take over. This meets the requirement for high availability.\nDirect Connect Gateway: By using a Direct Connect Gateway, you can extend the connectivity to multiple AWS Regions, allowing your global WAN network to access data in those regions without traversing the public internet."},{"comment_id":"1213109","poster":"sarlos","timestamp":"1715999760.0","content":"between C and D: TGW is a regional service. So D is the answer","upvote_count":"3"},{"comment_id":"1169369","upvote_count":"1","timestamp":"1709975460.0","content":"Selected Answer: D\nOption D","poster":"career360guru"},{"poster":"adelynllllllllll","comment_id":"1157589","timestamp":"1708738860.0","upvote_count":"4","content":"D:\n\nNeed direct connect gateway to share with other regions"},{"comment_id":"1146122","poster":"nharaz","content":"Selected Answer: D\nD - offers a blend of high availability (through redundancy with two DX connections), cost-effectiveness (by reducing the number of DX connections required), and simplicity (by avoiding the complexity of managing a transit VPC or multiple peering connections).","timestamp":"1707562560.0","upvote_count":"3"},{"timestamp":"1707328320.0","comments":[{"timestamp":"1712437020.0","upvote_count":"4","poster":"pangchn","content":"D\nTransit gateway is regional service.\nWe need DX gateway here","comment_id":"1190617"}],"upvote_count":"1","comment_id":"1143607","content":"Selected Answer: C\nC. transit VPC","poster":"TheCloudGuruu"},{"comment_id":"1142866","timestamp":"1707266880.0","poster":"kejam","upvote_count":"2","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html","comments":[{"comment_id":"1144117","poster":"07c2d2a","timestamp":"1707371280.0","comments":[{"content":"\"With the previous two options, you pay for Direct Connect pricing. For this option, you also pay for the Transit Gateway attachment and data processing charges.\"","comment_id":"1144119","timestamp":"1707371340.0","upvote_count":"2","poster":"07c2d2a"}],"content":"It seems to suggest you pay extra to do it that way. This is asking for the most cost-effective option.","upvote_count":"1"}]},{"upvote_count":"2","poster":"alexis123456","content":"Correct Answer is D","comment_id":"1141507","timestamp":"1707168900.0"}],"isMC":true},{"id":"H1uTMEsMu37s5CdeGYUC","topic":"1","answers_community":["B (94%)","6%"],"exam_id":33,"answer":"B","answer_description":"","question_images":[],"discussion":[{"timestamp":"1732577940.0","content":"Selected Answer: B\nThe best solution for the company's disaster recovery needs with minimal operational overhead is B. Configure AWS Elastic Disaster Recovery. \nThis option enables continuous data replication to Amazon EC2 instances attached to Amazon EBS volumes, ensuring the RPO of 5 minutes is met. When the on-premises environment fails, Elastic Disaster Recovery can automatically launch the EC2 instances using the replicated volumes, streamlining the recovery process and reducing manual intervention compared to other options.","comment_id":"1317823","upvote_count":"1","poster":"TomTom"},{"upvote_count":"2","poster":"AzureDP900","timestamp":"1731353880.0","content":"Option B is right \nElastic Disaster Recovery: This service allows you to create a replication instance in AWS, which can be used as a target for your application data. When your on-premises environment is unavailable, Elastic Disaster Recovery can automatically launch EC2 instances using the replicated volumes.\nZero-configuration restore: With Elastic Disaster Recovery, there's no need to configure any additional services or scripts to restore the application. The service takes care of launching the correct EC2 instances with the necessary EBS volumes attached.\nRPO of 5 minutes: By replicating your data regularly (e.g., every 5 minutes), you can ensure that your RPO is met, and in case of a disaster, the replicated data will be available on AWS.","comment_id":"1310283"},{"timestamp":"1711545180.0","upvote_count":"3","poster":"TonytheTiger","content":"Selected Answer: B\nOption B: AWS Elastic Disaster Recovery performance Failover and Failback \n\nhttps://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html","comment_id":"1184121"},{"content":"Selected Answer: B\nB is the correct answer. AWS Elastic Disaster Recovery aligns with low operational overhead and 5-minute RPO. It takes care of ongoing replication\n\nA: Incorrect, DataSync lacks comprehensive DR capabilities and requires manual provisioning.\nC: Incorrect, introduces complexity and doesn't support a 5-minute RPO for DR.\nD: Incorrect, FSx lacks automated DR solutions for VMware vSphere VMs, increasing overhead.","timestamp":"1711019220.0","upvote_count":"3","poster":"Dgix","comment_id":"1179140"},{"poster":"career360guru","content":"Selected Answer: B\nOption B","comment_id":"1169370","upvote_count":"1","timestamp":"1709975520.0"},{"comment_id":"1146577","timestamp":"1707590340.0","comments":[{"upvote_count":"1","content":"The RPO is 5 minutes.","comment_id":"1152779","poster":"marszalekm","timestamp":"1708197420.0"}],"upvote_count":"1","poster":"Russs99","content":"Selected Answer: D\nI selected option D, option B is requires enabling and configuring AWS disaster recovery service, monitoring replication status. In the even of a disaster. The replication of EC2 instances needs to be managed and maintained even where not in used."},{"comment_id":"1143612","content":"Selected Answer: B\nAnswer is B","upvote_count":"2","poster":"TheCloudGuruu","timestamp":"1707328440.0"},{"comment_id":"1143261","content":"Selected Answer: B\nCorrect answer is B - because only this option will provide LEAST amount of operational overhead.\n\nA - is out, because DataSync can't replicate data to EBS volumes\nC - is out, because AWS Backup can't restore not managed data from S3 to EBS\nD - is out, because it is not provide a way HOW we will replicate data from on-premise to FSx. Also, it is require additional amount of operational overhead","upvote_count":"4","poster":"HunkyBunky","timestamp":"1707304080.0"},{"upvote_count":"1","timestamp":"1707267000.0","poster":"kejam","comment_id":"1142867","content":"Selected Answer: B\nhttps://aws.amazon.com/disaster-recovery/"},{"poster":"alexis123456","upvote_count":"2","timestamp":"1707169620.0","content":"Correct Answer is D","comment_id":"1141517"}],"answer_images":[],"isMC":true,"question_text":"A company has developed an application that is running Windows Server on VMware vSphere VMs that the company hosts on premises. The application data is stored in a proprietary format that must be read through the application. The company manually provisioned the servers and the application.\n\nAs part of its disaster recovery plan, the company wants the ability to host its application on AWS temporarily if the company's on-premises environment becomes unavailable. The company wants the application to return to on-premises hosting after a disaster recovery event is complete. The RPO is 5 minutes.\n\nWhich solution meets these requirements with the LEAST amount of operational overhead?","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/132965-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":367,"timestamp":"2024-02-05 22:47:00","choices":{"A":"Configure AWS DataSync. Replicate the data to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and attach the EBS volumes.","C":"Provision an AWS Storage Gateway file gateway. Replicate the data to an Amazon S3 bucket. When the on-premises environment is unavailable, use AWS Backup to restore the data to Amazon Elastic Block Store (Amazon EBS) volumes and launch Amazon EC2 instances from these EBS volumes.","D":"Provision an Amazon FSx for Windows File Server file system on AWS. Replicate the data to the file system. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and use AWS::CloudFormation::Init commands to mount the Amazon FSx file shares.","B":"Configure AWS Elastic Disaster Recovery. Replicate the data to replication Amazon EC2 instances that are attached to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes."},"unix_timestamp":1707169620},{"id":"cHgE17MvSTf3zHZrlHzj","question_images":[],"unix_timestamp":1673644620,"answer_description":"","answer_ET":"B","discussion":[{"content":"Selected Answer: B\nB is the most cost-effective solution as it reduces the number of data nodes in the cluster to 2 and adds UltraWarm nodes to handle the expected capacity. By configuring the indexes to transition to UltraWarm when OpenSearch Service ingests the data, the company can take advantage of the lower storage costs of UltraWarm. Additionally, by transitioning the input data to S3 Glacier Deep Archive after 1 month using an S3 Lifecycle policy, the company can further reduce costs by using the lower storage costs of S3 Glacier Deep Archive for long-term data retention.","timestamp":"1673644620.0","comments":[{"comment_id":"774863","poster":"masetromain","timestamp":"1673644620.0","comments":[{"poster":"God_Is_Love","upvote_count":"5","comment_id":"821204","comments":[{"timestamp":"1677301800.0","comment_id":"821205","poster":"God_Is_Love","content":"* I meant C says..","upvote_count":"5"}],"content":"B says to delete but question asks for saving on compliance purposes.","timestamp":"1677301800.0"}],"content":"Option C can meet the requirements of reducing the number of data nodes in the cluster and using UltraWarm and cold storage nodes to handle the expected capacity and moving the data to lower cost storage after 1 month. However, it may not be the most cost-effective solution as it involves additional complexity in configuring the indexes to transition between different storage tiers, and may also require additional management and maintenance of the cold storage nodes. Option B, where the data is transitioned from S3 Standard to S3 Glacier Deep Archive using an S3 Lifecycle policy is simpler and more cost-effective as it eliminates the need for additional storage tiers and management.","upvote_count":"3"}],"poster":"masetromain","upvote_count":"21","comment_id":"774862"},{"comment_id":"1275488","poster":"amministrazione","content":"B. Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy.","timestamp":"1725093960.0","upvote_count":"1"},{"poster":"Malcnorth59","comment_id":"1214962","timestamp":"1716296880.0","content":"Why can't I switch all nodes to ultrawarm. I can't find it anywhere in the documentation and it's not listed in the pre-requisites. \n\nAlso why can the number of nodes be reduced from 10 to 2? is that because Ultrawarm use S3?","upvote_count":"1"},{"comment_id":"1199418","upvote_count":"1","timestamp":"1713837180.0","content":"why not D?","poster":"sarlos"},{"comment_id":"1104718","poster":"ninomfr64","timestamp":"1703434680.0","upvote_count":"1","comments":[{"timestamp":"1703434920.0","content":"I think I got it, UltraWarm is for read-only data. Thus you still need to have at least a data node","poster":"ninomfr64","comment_id":"1104721","upvote_count":"1"}],"content":"I need help here:\n\nTo use UltraWarm storage, domains must have dedicated master nodes as per doc https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ultrawarm.html\n\nThe scenario mentions \"an OpenSearch Service cluster with 10 data nodes\". Assuming you only have these nodes in the cluster, in all answers you need to add dedicated master node(s). Assuming we also have dedicated master node why not replacing all data nodes with UltraWarm nodes?"},{"content":"Selected Answer: B\nOption A says to replace all Data Nodes with ultra warm nodes. But this is NOT possible. There has to be atleast one data node","poster":"venvig","upvote_count":"3","timestamp":"1692536100.0","comment_id":"985783"},{"content":"Selected Answer: B\nB I think :/","timestamp":"1688144880.0","comment_id":"939272","upvote_count":"2","poster":"NikkyDicky"},{"comments":[{"content":"I think you are referring All AWS Certified Solutions Architect - Professional SAP-C02 Questions, question 44. yes, I changed from D to A after reading this link.","upvote_count":"1","timestamp":"1686759060.0","comment_id":"923374","poster":"Jesuisleon"},{"upvote_count":"1","comment_id":"943279","timestamp":"1688529420.0","content":"You can specify the IP address with the CIDR parameter\n\nhttps://ec2.amazonaws.com/?Action=AuthorizeSecurityGroupIngress\n&GroupId=sg-112233\n&IpPermissions.1.IpProtocol=tcp\n&IpPermissions.1.FromPort=3389\n&IpPermissions.1.ToPort=3389\n&IpPermissions.1.IpRanges.1.CidrIp=192.0.2.0/24\n&IpPermissions.1.IpRanges.1.Description=Access from New York office\n\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_AuthorizeSecurityGroupIngress.html","poster":"eddylynx"}],"upvote_count":"2","poster":"Damijo","timestamp":"1679351880.0","content":"Selected Answer: A\nIf you look at the IAM documentation here, you can see that the ec2:AuthorizeSecurityGroupIngress action doesn't have any conditions that would allow you to specify the ip addresses in the inbound/outbound rules.https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonec2.html","comment_id":"845307"},{"comment_id":"840402","content":"Selected Answer: B\nB - makes more sense","poster":"dev112233xx","upvote_count":"4","timestamp":"1678922040.0"},{"timestamp":"1677907260.0","comment_id":"828687","upvote_count":"3","poster":"Ajani","content":"UltraWarm provides a cost-effective way to store large amounts of read-only data on Amazon OpenSearch Service. Standard data nodes use \"hot\" storage, which takes the form of instance stores or Amazon EBS volumes attached to each node. Hot storage provides the fastest possible performance for indexing and searching new data."},{"timestamp":"1676066940.0","poster":"moota","upvote_count":"2","content":"I asked ChatGPT. Can I use all UltraWarm nodes in AWS OpenSearch instead of data nodes? :)\n\nNo, UltraWarm nodes in AWS OpenSearch are designed for storage and retrieval of infrequently accessed data, while data nodes are optimized for faster indexing and searching of data. While UltraWarm nodes can be used as a complement to data nodes, they are not a replacement for them.","comments":[{"content":"This eliminates option A","poster":"hobokabobo","upvote_count":"2","timestamp":"1677372000.0","comment_id":"821954"}],"comment_id":"804820"},{"comment_id":"791760","timestamp":"1675005120.0","poster":"Musk","upvote_count":"4","content":"Selected Answer: B\nOption B is the most cost-effective solution that meets the requirements. Reducing the number of data nodes in the cluster and adding UltraWarm nodes will help to reduce the ongoing costs of running the OpenSearch Service cluster. Configuring the indexes to transition to UltraWarm when OpenSearch Service ingests the data will further reduce costs. Additionally, transitioning the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy will lower the storage costs of retaining the input data for compliance purposes."}],"isMC":true,"exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/95091-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["B (94%)","6%"],"answer":"B","question_text":"A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes from an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.\n\nThe company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.\n\nWhich solution will meet these requirements MOST cost-effectively?","timestamp":"2023-01-13 22:17:00","answer_images":[],"topic":"1","choices":{"A":"Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.","C":"Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Add cold storage nodes to the cluster Transition the indexes from UltraWarm to cold storage. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy.","D":"Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.","B":"Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy."},"question_id":368},{"id":"d2VcnsgassnncFtMyc9R","question_images":[],"answers_community":["C (68%)","B (32%)"],"url":"https://www.examtopics.com/discussions/amazon/view/132972-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2024-02-05 23:22:00","isMC":true,"unix_timestamp":1707171720,"exam_id":33,"answer_description":"","topic":"1","question_text":"A company runs a highly available data collection application on Amazon EC2 in the eu-north-1 Region. The application collects data from end-user devices and writes records to an Amazon Kinesis data stream and a set of AWS Lambda functions that process the records. The company persists the output of the record processing to an Amazon S3 bucket in eu-north-1. The company uses the data in the S3 bucket as a data source for Amazon Athena.\n\nThe company wants to increase its global presence. A solutions architect must launch the data collection capabilities in the sa-east-1 and ap-northeast-1 Regions. The solutions architect deploys the application, the Kinesis data stream, and the Lambda functions in the two new Regions. The solutions architect keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis.\n\nDuring testing of the new setup, the solutions architect notices a significant lag on the arrival of data from the new Regions to the S3 bucket.\n\nWhich solution will improve this lag time the MOST?","question_id":369,"choices":{"B":"Turn on S3 Transfer Acceleration on the S3 bucket in eu-north-1. Change the application to use the new S3 accelerated endpoint when the application uploads data to the S3 bucket.","A":"In each of the two new Regions, set up the Lambda functions to run in a VPC. Set up an S3 gateway endpoint in that VPC.","D":"Increase the memory requirements of the Lambda functions to ensure that they have multiple cores available. Use the multipart upload feature when the application uploads data to Amazon S3 from Lambda.","C":"Create an S3 bucket in each of the two new Regions. Set the application in each new Region to upload to its respective S3 bucket. Set up S3 Cross-Region Replication to replicate data to the S3 bucket in eu-north-1."},"discussion":[{"content":"Selected Answer: C\ns3 transfer acceleration is not supported in eu-north-1 region yet\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html","upvote_count":"18","poster":"wyeedh1","timestamp":"1707632580.0","comment_id":"1147011"},{"poster":"VerRi","upvote_count":"7","content":"Selected Answer: C\n\"improve this lag time the MOST\" means improve the time for \"uploading files\" not \"uploading the files to the destination.\" Uploading the files to the bucket in the same region is faster than transferring them to other regions.","comment_id":"1180989","timestamp":"1711209180.0"},{"timestamp":"1741866960.0","poster":"itsjunukim","content":"Selected Answer: B\n\"keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis\" B","comment_id":"1388280","upvote_count":"1"},{"timestamp":"1731641340.0","poster":"0b43291","comment_id":"1312393","content":"Selected Answer: C\nBy implementing Option C, the company can leverage the proximity of local S3 buckets in each Region, while still maintaining a centralized data store in eu-north-1 through Cross-Region Replication. This approach minimizes the lag time for data arrival and ensures that the data analysis in Amazon Athena is performed on the complete and up-to-date dataset.\n\nThe other options have limitations or may not address the lag time issue effectively:\n Option A (VPC and S3 gateway endpoint) does not directly address the latency issue caused by the long distances between Regions.\n Option B (S3 Transfer Acceleration) can improve transfer speeds but may not be as effective as having local S3 buckets in each Region.\n Option D (increasing Lambda memory and using multipart uploads) may improve Lambda performance but does not address the underlying network latency issue.","upvote_count":"1"},{"upvote_count":"1","poster":"AzureDP900","content":"Considering the current limitations of S3 Transfer Acceleration in the eu-north-1 Region, I would say that Option C is actually the best choice.","comment_id":"1310280","timestamp":"1731353520.0"},{"poster":"AloraCloud","upvote_count":"2","comment_id":"1306639","timestamp":"1730666460.0","content":"Except if RTC is enabled you cannot guarantee that the data replicated to the central S3 bucket in eu-north-1 will be up to date.\n\nI'll be dammed if AWS is really expecting folks to remember the regions which do not have the Amazon S3 Transfer Acceleration feature."},{"upvote_count":"2","timestamp":"1729054800.0","content":"I think the answer is C because Amazon Kinesis Data Streams cannot directly leverage S3 Transfer Acceleration. S3 Transfer Acceleration is typically used for accelerating transfers from clients to S3 and doesn't support Kinesis Data Streams directly.","comment_id":"1298555","poster":"AloraCloud"},{"content":"Selected Answer: B\nWhile C would work, it adds complexity by requiring separate S3 buckets in each Region and replicating data, which could increase cost and delay due to replication schedules.","comment_id":"1283511","timestamp":"1726295520.0","upvote_count":"2","poster":"Syre"},{"poster":"trungtd","timestamp":"1717457940.0","content":"Selected Answer: C\nNo matter how fast you run, you cannot run from Tokyo to Stockholm as fast as someone 10km from Stockholm.","upvote_count":"2","comment_id":"1223831"},{"timestamp":"1715200500.0","content":"Answer is C but feel this is one they definitely want people to get wrong. No one is going to memorize which regions every feature is supported in, it is something anyone would look up.","upvote_count":"2","comment_id":"1208536","poster":"e4bc18e"},{"upvote_count":"3","timestamp":"1714733220.0","comment_id":"1206049","content":"Selected Answer: C\nI would choose B but it cannot be B because S3 Transfer Acceleration is not supported in eu-north-1. This leaves C as the only viable option.","poster":"seetpt"},{"timestamp":"1714553280.0","upvote_count":"3","content":"Selected Answer: C\nagree with bjexamprep","comment_id":"1204959","poster":"qaz12wsx"},{"timestamp":"1713058800.0","poster":"bjexamprep","upvote_count":"6","content":"Selected Answer: C\nS3 Transfer Acceleration achieves acceleration by AWS routing traffic to AWS cloudfront edge location and transfer aws network backbone. The key speed improvement is AWS network backbone. \nWhile, in this case, the application in the two new regions is the client to write the output to S3 in EU, which means the client is already on AWS, the data transportation is already on AWS network backbone. So S3 Transfer Acceleration is not helping at all. B is out.\nC is the best answer.","comment_id":"1195199"},{"poster":"leliodesouza","timestamp":"1712432280.0","comment_id":"1190595","upvote_count":"4","content":"Selected Answer: C\nCorrect Answer is C.\n\nS3 transfer acceleration is not yet supported in the eu-north-1 region, as wyeedh1 commented.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html"},{"content":"Selected Answer: C\nwyeedh1","poster":"akiakiaki","timestamp":"1711810560.0","comment_id":"1186150","upvote_count":"2"},{"poster":"failexamonly","comment_id":"1183913","upvote_count":"3","timestamp":"1711522740.0","content":"Selected Answer: C\nsee wyeedh1 answer"},{"content":"Selected Answer: B\nOption B","comment_id":"1169376","timestamp":"1709976180.0","poster":"career360guru","upvote_count":"3"},{"comment_id":"1165185","timestamp":"1709512320.0","poster":"thotwielder","upvote_count":"3","content":"Selected Answer: B\nA: wrong. for s3 gateway endpoint it's not possible to access the S3 buckets from another VPC/Region\nB: typical scenario for s3 transfer acceleration.\nC: possible. But extra steps. And not sure s3 replication will be faster. So is wrong.\nD: wrong."},{"poster":"sat2008","comment_id":"1164087","upvote_count":"2","content":"Selected Answer: B\nI agree for the reason \"requirement to centralize the data analysis\" we cant have S3 s in other regions","timestamp":"1709379780.0"},{"content":"Selected Answer: B\nas they said \"The solutions architect keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis\". So, B should be the answer","upvote_count":"4","poster":"cf9e355","timestamp":"1708702860.0","comment_id":"1157273"},{"upvote_count":"2","poster":"veyisceylan","content":"I think that the LAG 7is caused due to network path running to S3 public interfaces. The gateway endpoint can enchance the network path and reduce the LAG.\n\nhttps://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/","comment_id":"1149527","timestamp":"1707853020.0"},{"poster":"arberod","upvote_count":"3","content":"Selected Answer: B\nIt is B","comment_id":"1144371","timestamp":"1707389820.0"},{"upvote_count":"2","poster":"07c2d2a","comment_id":"1144134","timestamp":"1707371880.0","content":"B. It's not efficient to upload to 1 S3 bucket, then have it replicated across. It will be faster to use acceleration to get to the actual destination bucket. Additionally, they don't need 2 extra copies of all the data. B makes the most sense from all perspectives."},{"content":"Selected Answer: B\nB. Transfer Accelertation","poster":"TheCloudGuruu","comment_id":"1143615","upvote_count":"3","timestamp":"1707328560.0"},{"comment_id":"1143257","upvote_count":"4","poster":"HunkyBunky","timestamp":"1707303720.0","content":"Selected Answer: C\nAnswer is C - this option will provide MOST lag-improve solution for application"},{"content":"Selected Answer: B\nhttps://aws.amazon.com/s3/transfer-acceleration/","timestamp":"1707267180.0","upvote_count":"4","poster":"kejam","comment_id":"1142868"},{"upvote_count":"3","comment_id":"1141577","poster":"alexis123456","timestamp":"1707171720.0","content":"Correct Answer is C"}],"answer":"C","answer_images":[],"answer_ET":"C"},{"id":"eTJ9YcopRgRAi0Qir1og","question_images":[],"answer_description":"","choices":{"A":"Create an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway.","D":"Configure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection.","C":"Create a VPC peering connection from each business unit VPC to the shared VPAccept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection.","B":"Create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console."},"discussion":[{"upvote_count":"7","poster":"sat2008","content":"B is the answer for me \nOnly way to get around overlapping IP range is using endpoint service","timestamp":"1708181040.0","comment_id":"1152611"},{"comment_id":"1316122","timestamp":"1732243560.0","upvote_count":"3","content":"Selected Answer: B\nBy using a VPC endpoint service with the \"require endpoint acceptance\" option, the company can securely and efficiently provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC, while addressing the requirements of overlapping CIDR blocks and controlled access.\n\nA. AWS Transit Gateway: While Transit Gateway can connect multiple VPCs, it does not provide a mechanism to control access or handle overlapping CIDR blocks between VPCs.\n\nC. VPC Peering: VPC peering does not support overlapping CIDR blocks between VPCs, which is a requirement in this scenario. Additionally, managing multiple VPC peering connections can become complex and difficult to maintain as the number of VPCs increases.\n\nD. Site-to-Site VPN: No","poster":"0b43291"},{"content":"By choosing Option B, you get secure, private connectivity between the client applications in the business unit VPCs and the centralized application in the shared VPC without introducing unnecessary complexity or costs.\nThis configuration provides secure, private connectivity between the client applications in the business unit VPCs and the centralized application in the shared VPC.","poster":"AzureDP900","comment_id":"1310276","upvote_count":"1","timestamp":"1731353100.0"},{"upvote_count":"2","poster":"Moghite","timestamp":"1721365800.0","content":"Selected Answer: B\nonly option to get around of IP overlapping \nhttps://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/","comment_id":"1250854"},{"comments":[{"comment_id":"1240446","timestamp":"1719872580.0","upvote_count":"1","poster":"toma","content":"\"This requires that automatic route propagation to Transit Gateway be disabled as not all of the subnets in each VPC should be advertised.\" so it is B"},{"timestamp":"1716342360.0","comment_id":"1215338","poster":"sarlos","content":"Not possible, because TGW does not support overlapping ranges","upvote_count":"3"}],"poster":"43c89f4","upvote_count":"1","content":"A is actually. they never mentioned cost effect or less effort solution.\n\nwhen they are not mentioned anything we need to prefer best option","timestamp":"1714569420.0","comment_id":"1205103"},{"poster":"career360guru","upvote_count":"1","content":"Selected Answer: B\noption B","comment_id":"1169834","timestamp":"1710018780.0"},{"upvote_count":"2","timestamp":"1707399660.0","poster":"arberod","content":"Selected Answer: B\nB is the answer","comment_id":"1144493"},{"content":"Selected Answer: B\nAnswer is B\nApplication already uses NLB so this is a best way for solve that task","comment_id":"1143252","timestamp":"1707303000.0","upvote_count":"2","poster":"HunkyBunky"},{"timestamp":"1707276840.0","poster":"kejam","content":"Selected Answer: B\nhttps://www.examtopics.com/discussions/amazon/view/46708-exam-aws-certified-solutions-architect-professional-topic-1/\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/","comment_id":"1142924","upvote_count":"4"},{"upvote_count":"1","content":"Selected Answer: B\nVPC Endpoint Service can do the job","poster":"master9","comment_id":"1142839","timestamp":"1707262560.0"},{"comment_id":"1141697","timestamp":"1707184680.0","upvote_count":"4","poster":"alexis123456","content":"Correct Answer is A"}],"question_text":"A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability.\n\nUp to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only.\n\nWhich network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?","isMC":true,"answer":"B","unix_timestamp":1707184680,"topic":"1","answer_ET":"B","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/132984-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"answers_community":["B (100%)"],"timestamp":"2024-02-06 02:58:00","question_id":370}],"exam":{"isImplemented":true,"isMCOnly":true,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","lastUpdated":"11 Apr 2025","numberOfQuestions":529,"provider":"Amazon","id":33},"currentPage":74},"__N_SSP":true}