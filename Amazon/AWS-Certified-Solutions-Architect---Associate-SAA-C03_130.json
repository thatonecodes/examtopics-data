{"pageProps":{"questions":[{"id":"pKDmOyIjoV0cweRJiAiG","exam_id":31,"question_images":[],"answer":"D","answer_description":"","choices":{"D":"Delete all nonessential snapshots. Use Amazon Data Lifecycle Manager to create and manage the snapshots according to the company's snapshot policy requirements.","A":"Use logs in Amazon CloudWatch Logs to monitor the storage utilization of Amazon EBS. Use Amazon EBS Elastic Volumes to reduce the size of the EBS volumes.","C":"Delete all expired and unused snapshots to reduce snapshot costs.","B":"Use a custom script to monitor space usage. Use Amazon EBS Elastic Volumes to reduce the size of the EBS volumes."},"answer_ET":"D","answers_community":["D (100%)"],"topic":"1","question_text":"A company uses AWS Cost Explorer to monitor its AWS costs. The company notices that Amazon Elastic Block Store (Amazon EBS) storage and snapshot costs increase every month. However, the company does not purchase additional EBS storage every month. The company wants to optimize monthly costs for its current storage usage.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","discussion":[{"upvote_count":"8","content":"Selected Answer: D\nThis option involves managing snapshots efficiently to optimize costs with minimal operational overhead.\n\nDelete all nonessential snapshots: This reduces costs by eliminating unnecessary snapshot storage.\nUse Amazon Data Lifecycle Manager (DLM): DLM can automate the creation and deletion of snapshots based on defined policies. This reduces operational overhead by automating snapshot management according to the company's snapshot policy requirements.","comment_id":"1077150","poster":"t0nx","timestamp":"1700643480.0"},{"upvote_count":"1","comment_id":"1318359","poster":"JA2018","timestamp":"1732669260.0","content":"Selected Answer: D\nAllows the user to manage snapshots efficiently for cost optimization with the LEAST operational overhead.\n\n#1: Delete all nonessential snapshots: This reduces costs by eliminating unnecessary storage used by such snapshots.\n\n#2: Amazon Data Lifecycle Manager (DLM): This AWS managed service can automate the creation and deletion process of snapshots based on defined policies. Automating snapshot management according to snapshot policy requirements helps to cut down operational overhead ."},{"upvote_count":"4","poster":"awsgeek75","timestamp":"1705758660.0","comment_id":"1127300","content":"Selected Answer: D\nLeast operational overhead for your snapshot management is https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\n\nC will just do it once but assuming they want an ongoing solution.\nA: It will help with EBS size but won't fix the snapshot problems\nB: Same as A, nothing to do with snapshos","comments":[{"timestamp":"1712196960.0","poster":"xBUGx","comments":[{"poster":"KennethNg923","content":"I think \"optimize monthly costs\" include future monthly cost as well, so D is better than C","upvote_count":"2","comment_id":"1231633","timestamp":"1718590320.0"}],"content":"Q says The company wants to optimize monthly costs for its current storage usage. I think they only want to do it once?","upvote_count":"1","comment_id":"1189032"}]},{"poster":"TariqKipkemei","timestamp":"1702021800.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html","upvote_count":"4","comment_id":"1090852"}],"timestamp":"2023-11-22 09:58:00","isMC":true,"answer_images":[],"unix_timestamp":1700643480,"question_id":646,"url":"https://www.examtopics.com/discussions/amazon/view/126865-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"JLWvFFLa4GbsMLooLpyW","exam_id":31,"question_text":"A company is developing a new application on AWS. The application consists of an Amazon Elastic Container Service (Amazon ECS) cluster, an Amazon S3 bucket that contains assets for the application, and an Amazon RDS for MySQL database that contains the dataset for the application. The dataset contains sensitive information. The company wants to ensure that only the ECS cluster can access the data in the RDS for MySQL database and the data in the S3 bucket.\n\nWhich solution will meet these requirements?","discussion":[{"content":"Selected Answer: A\nWe're asked to restrict access to both, RDS and S3, to \"the ECS cluster\" (not to a subnet or endpoint).\n\nNot B: Does not restrict RDS at all. Wording about S3 is unusual.\nNot C: Would work for S3, but would allow RDS access from whole subnet which may contain other resources besides the ECS cluster\nNot D: Would allow RDS access from whole subnet which may contain other resources besides the ECS cluster. Would allow S3 access from VPC endpoint which might be accessed by other resources besides the ECS cluster.","timestamp":"1704708180.0","comments":[{"comment_id":"1363640","poster":"GOTJ","upvote_count":"1","content":"You are right with your option \"D\" concerns about letting other resources running on the same subnet that your ECS cluster accessing the database. But, likewise, the \"task execution role\" mentioned in your selected answer \"A\" is applied to a specific ECS task, not to the whole cluster (requirement)","timestamp":"1740846300.0"},{"content":"A doesn't restricy access. It only encrypts at rest. D resctricts access. Encryption and Access Control are 2 different things","comment_id":"1332517","timestamp":"1735316160.0","poster":"deacon967","upvote_count":"2"}],"poster":"pentium75","upvote_count":"13","comment_id":"1116529"},{"comment_id":"1077140","timestamp":"1700642940.0","poster":"t0nx","content":"Selected Answer: D\nOption D is the most comprehensive solution as it leverages VPC endpoints for both Amazon RDS and Amazon S3, along with proper network-level controls to restrict access to only the necessary resources from the ECS cluster.","upvote_count":"9","comments":[{"upvote_count":"3","comment_id":"1121811","poster":"awsgeek75","timestamp":"1705160700.0","content":"D only secures access to RDS and S3, it does not secure the sensitive data inside the RDS and S3."}]},{"content":"Selected Answer: A\nBut security groups allow you to control inbound and outbound traffic at the instance level, while network ACLs offer similar capabilities at the VPC subnet level.","poster":"tch","upvote_count":"1","comments":[{"upvote_count":"1","poster":"tch","content":"look portion of answer D: Update the RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in.\n\n\"allow access from only the subnets \".. is this correct for security group?","comment_id":"1409602","timestamp":"1742805540.0"}],"comment_id":"1409601","timestamp":"1742805420.0"},{"poster":"Dantecito","content":"Selected Answer: C\nI think the word \"restricts\" is confusing. But I interpret it as allow the ECS task execution role to access the S3 bucket and deny everything else.","comment_id":"1357021","upvote_count":"1","timestamp":"1739649300.0"},{"content":"Selected Answer: D\nThe dataset contains sensitive information, nothing to be done here, just stating that it contains sensitive info.","poster":"dragossky","upvote_count":"1","timestamp":"1733770380.0","comment_id":"1324175"},{"timestamp":"1732669800.0","comments":[{"content":"Why the other options are not as suitable:\n\nOption A and B:\nWhile using a KMS key to encrypt data is a good practice, it doesn't directly control access to the data. Attaching the KMS key to the ECS task execution role in the policy would still allow any entity with the key to decrypt the data, not just the ECS cluster.","poster":"JA2018","comment_id":"1318363","upvote_count":"1","timestamp":"1732669860.0","comments":[{"poster":"JA2018","upvote_count":"1","content":"Option D:\nThis option only addresses the RDS access by creating a VPC endpoint, but it doesn't restrict access to the S3 bucket effectively, which is also mentioned in the question","timestamp":"1732669860.0","comment_id":"1318364"}]}],"poster":"JA2018","content":"Selected Answer: C\nWhy not C?\n\nKey point:\n\nThe question specifically asks to ensure that only the ECS cluster can access the data, which means restricting access to the S3 bucket and RDS database solely to the ECS task execution role.\n\nBreakdown of option C:\n\nS3 bucket policy: By creating a policy that only allows the ECS task execution role to access the S3 bucket, you effectively limit access to the data stored there.\n\nVPC endpoint for RDS: A VPC endpoint creates a private connection to the RDS database, preventing any external access and ensuring that only resources within the VPC can connect.\n\nRDS security group update: Further restricting access by configuring the RDS security group to only allow connections from the subnets where the ECS cluster will launch tasks.","comment_id":"1318362","upvote_count":"1"},{"upvote_count":"3","content":"I cannot believe how many people vote A. \n\nthe questions is asking only allow ECS cluster access RDS and access to S3.\n\n2 keys here: 1. security group is usually used to security access between RDS and ECS cluster 2. access data in S3 securely, imemdiately, we should think about S3 VPC Gateway endpoints because this secures the traffic only travel via private network.\n\nAnswer A is just talking about encrpt data at rest, and that is not what the question is asking about","poster":"XXXXXlNN","timestamp":"1728132180.0","comment_id":"1293480"},{"poster":"MandAsh","comment_id":"1232666","timestamp":"1718757300.0","content":"After reading comments changed to A. D will not protect data at rest it will only give n/w level security","upvote_count":"1"},{"comment_id":"1202038","content":"Selected Answer: A\nAccording to me \"The dataset contains sensitive information\" is the main information that motivate the real requirement which is \"The company wants to ensure that only the ECS cluster can access the data in the RDS for MySQL database and the data in the S3 bucket\". So we have to take these two assertions into consideration.\nAnd knowing that, as S3 default encryption capabilities, RDS Mysql DB Instance encryption is not active by default (check this link for details https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html ), option A is the best option to meet the requirements of accessing the datasets and the assets only from ECS cluster tasks and preserve, at the same time, data confidentiality and integrity. In other words, option A is the best one to ensure the data protection at REST for S3 and RDS and only accessed by ECS cluster.","upvote_count":"3","poster":"bujuman","timestamp":"1714053060.0"},{"upvote_count":"1","poster":"Hung23","comment_id":"1199990","timestamp":"1713765600.0","content":"Selected Answer: C\nTry to chat GPT Please"},{"comment_id":"1168762","upvote_count":"2","content":"Selected Answer: A\nA seems right","poster":"seetpt","timestamp":"1709899080.0"},{"poster":"[Removed]","comment_id":"1122190","timestamp":"1705198320.0","content":"Selected Answer: A\nVote for A. Keywords: “sensitive information” and “data in…”\nD: only network control, can’t control data access on sensitive information.","upvote_count":"4"},{"content":"Selected Answer: C\nI did not get how does D achieves the only access from ECS cluster to S3 VPC endpoint.","timestamp":"1704639600.0","comment_id":"1115935","upvote_count":"1","poster":"Marco_St"},{"content":"Selected Answer: A\nA; When Only the ECS task execution role is able to encrypt and decrypt the data in the RDS and in the S3 bucket by means of the KMS key policy, you ensure that nothing else can read or modify the data.\nB: this answer doesn’t state that only the ECS cluster can reach the data.\nC: Creating a VPC endpoint for RDS does not mean that only the ECS cluster can reach the data\nD: The S3 VPC endpoint does not guarantee that only the ECS cluster can reach the data. Also allowing a subnet to have access to the RDS sounds too open to me","timestamp":"1704451440.0","upvote_count":"5","comment_id":"1114439","poster":"1rob"},{"timestamp":"1703736960.0","upvote_count":"1","poster":"Min_93","content":"Options A and B involve using AWS Key Management Service (AWS KMS) for encryption but do not directly address the requirement to restrict access to the ECS cluster. Option C is not the most direct approach for restricting access to the RDS database, as it focuses on the S3 bucket.\n\nTherefore, option D is the most appropriate solution for ensuring that only the ECS cluster can access the data in the RDS for MySQL database and the data in the S3 bucket.","comment_id":"1107390"},{"upvote_count":"3","comment_id":"1090860","timestamp":"1702022340.0","content":"Selected Answer: D\nA VPC endpoint enables customers to privately connect to supported AWS services and VPC endpoint services powered by AWS PrivateLink.","poster":"TariqKipkemei"},{"timestamp":"1701729240.0","comments":[{"poster":"Salilgen","timestamp":"1735399920.0","comment_id":"1333028","content":"So you control access to S3 bucket but anyone could still access to RDS from the ECS cluster's subnet","upvote_count":"1"}],"upvote_count":"2","comment_id":"1088048","poster":"SHAAHIBHUSHANAWS","content":"C\nneed to restrict access from ECS cluster"},{"upvote_count":"4","comment_id":"1076873","comments":[{"timestamp":"1701729180.0","content":"I agree this will allow only resources from VPC but will not restrict only ECS cluster. I suggest we use bucket policy to use ECS cluster role on top of network settings.","poster":"SHAAHIBHUSHANAWS","comment_id":"1088045","upvote_count":"1"}],"timestamp":"1700621280.0","content":"Selected Answer: D\nCreate a VPC endpoint for Amazon RDS for MySQL: This ensures that the ECS cluster can access the RDS database directly within the same Virtual Private Cloud (VPC), without having to go over the internet. By updating the security group to allow access only from the specific subnets that the ECS cluster will generate tasks in, you limit access to only the authorized entities.\n\nCreate a VPC endpoint for Amazon S3: This allows the ECS cluster to access the S3 bucket directly within the same VPC. By updating the S3 bucket policy to allow access only from the S3 VPC endpoint, you restrict access to the designated VPC, ensuring that only authorized resources can access the S3 bucket.","poster":"LemonGremlin"}],"choices":{"A":"Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt both the S3 bucket and the RDS for MySQL database. Ensure that the KMS key policy includes encrypt and decrypt permissions for the ECS task execution role.","D":"Create a VPC endpoint for Amazon RDS for MySQL. Update the RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in. Create a VPC endpoint for Amazon S3. Update the S3 bucket policy to allow access from only the S3 VPC endpoint.","C":"Create an S3 bucket policy that restricts bucket access to the ECS task execution role. Create a VPC endpoint for Amazon RDS for MySQL. Update the RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in.","B":"Create an AWS Key Management Service (AWS KMS) AWS managed key to encrypt both the S3 bucket and the RDS for MySQL database. Ensure that the S3 bucket policy specifies the ECS task execution role as a user."},"topic":"1","answer_images":[],"question_images":[],"question_id":647,"unix_timestamp":1700621280,"url":"https://www.examtopics.com/discussions/amazon/view/126798-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["A (57%)","D (35%)","8%"],"answer":"A","answer_ET":"A","answer_description":"","timestamp":"2023-11-22 03:48:00","isMC":true},{"id":"NkgPrWGFCDetv6uJGuob","question_text":"A company has a web application that runs on premises. The application experiences latency issues during peak hours. The latency issues occur twice each month. At the start of a latency issue, the application's CPU utilization immediately increases to 10 times its normal amount.\n\nThe company wants to migrate the application to AWS to improve latency. The company also wants to scale the application automatically when application demand increases. The company will use AWS Elastic Beanstalk for application deployment.\n\nWhich solution will meet these requirements?","answer_images":[],"question_images":[],"topic":"1","choices":{"C":"Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the environment to scale on a schedule.","A":"Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode. Configure the environment to scale based on requests.","B":"Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the environment to scale based on requests.","D":"Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode. Configure the environment to scale on predictive metrics."},"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/126800-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"A","answers_community":["A (64%)","D (34%)","2%"],"exam_id":31,"discussion":[{"upvote_count":"11","comment_id":"1112669","content":"Selected Answer: A\n\"Scale on predictive metrics\" does not sound like something that Beanstalk can do. In EC2 you can create a \"predictive scaling policy\", but apparently this is not supported by Beanstalk. That would rule out D.\n\nWe have no indication that the application is CPU-intensive in general. If CPU utilization \"increases to 10 times its normal amount\" then the \"normal amount\" cannot be higher than 10 %. This would rule out B and C.","poster":"pentium75","timestamp":"1704278280.0"},{"poster":"LemonGremlin","upvote_count":"10","comments":[{"timestamp":"1703245800.0","content":"Traffic is \"immediately increases\". We can't predict and can not use Predictive Metrics. \nAnd requirement need auto scaling","comment_id":"1103344","upvote_count":"2","poster":"ftaws"}],"content":"Selected Answer: D\nBurstable Performance Instances (T3 or T3a): These instances are designed for burstable workloads and provide a baseline level of CPU performance with the ability to burst above that baseline when needed. Bursting is particularly beneficial for handling sudden spikes in CPU utilization, such as those described in the scenario.\n\nUnlimited Mode: Enabling \"unlimited\" mode allows instances to burst beyond their baseline performance without accumulating CPU credits. This is important for handling sudden and sustained increases in CPU utilization during peak hours.\n\nScale on Predictive Metrics: Configuring the environment to scale on predictive metrics allows AWS Elastic Beanstalk to proactively adjust the number of instances based on anticipated demand. This can help ensure that the environment is scaled up before the latency issues occur, addressing them in advance.","comment_id":"1076876","timestamp":"1700621460.0"},{"upvote_count":"1","poster":"Salilgen","comment_id":"1333039","content":"Selected Answer: A\nIMO answer is A.\n\"The Auto Scaling group uses two Amazon CloudWatch alarms to trigger scaling operations. The default triggers scale when the average outbound network traffic from each instance is higher than 6 MiB or lower than 2 MiB over a period of five minutes. To use Auto Scaling effectively, configure triggers that are appropriate for your application, instance type, and service requirements. You can scale based on several statistics including latency, disk I/O, CPU utilization, and request count.\"\nThis is dynamic scaling.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.as.html","timestamp":"1735402020.0"},{"poster":"3c6417b","upvote_count":"1","comments":[{"comment_id":"1326355","upvote_count":"1","poster":"LeonSauveterre","content":"B is incorrect because CPU usage percentage rising does not necessarily have anything to do with computing methods. Especially during peak hours, the jobs are coming in hot, but it's quite possible that each job can be done within seconds. It's just too many jobs, and that's not something compute optimized instances can tackle.","timestamp":"1734154320.0"},{"upvote_count":"1","timestamp":"1720055340.0","comment_id":"1241730","content":"I have the same question.","poster":"Gape4"}],"content":"Selected Answer: B\nExplain to me why it's not B?","timestamp":"1718122140.0","comment_id":"1228523"},{"poster":"sandordini","upvote_count":"6","timestamp":"1713961680.0","comment_id":"1201352","content":"Selected Answer: A\nD - No such service as Elastic Beanstalk Predictive Scaling, And even if there was, no historical data in AWS for an application we are just about to migrate to AWS. Therefore: A"},{"comment_id":"1185323","poster":"lenotc","content":"Selected Answer: A\nD is incorrect Predictive scaling not fit","timestamp":"1711705320.0","upvote_count":"3"},{"comment_id":"1127246","content":"For those voting D, predictive scaling analyses historic data to predict the scaling needs. This scenario is a migration scenario so there won't be any historic data which is why D won't work. A (burst) is the only option after migration.","poster":"awsgeek75","timestamp":"1705751340.0","upvote_count":"5"},{"comment_id":"1121826","upvote_count":"6","timestamp":"1705161720.0","poster":"awsgeek75","content":"Selected Answer: A\nBC are compute optimised instances which don't solve 10x CPU issues at start of the latency.\nAD are burstable performance which will help with 10x increase CPU usage\nD is not an available feature of Elastic Beanstalk (yet) or I cannot find it in config/docs. Happy to be corrected\nA makes sense due to burst performance. Scale based on requests is possible and I'm assuming that latency is related to requests."},{"content":"Selected Answer: A\nFollowing https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.as.html I see: \" You can scale based on several statistics including latency, disk I/O, CPU utilization, and request count. \" So no 'scale on predictive metrics, so D is not okay.\n\nAlso, the company also wants to scale the application automatically when application demand increases, so scale on a schedule is not appropriate here. So C is not okay.\nBurstable performance instances in unlimited mode can sustain high CPU utilization for any period of time whenever required, so an immediate demand of CPU resources is 'covered'. So I go for A.","timestamp":"1704909660.0","upvote_count":"4","comment_id":"1118903","poster":"1rob"},{"upvote_count":"4","timestamp":"1703737140.0","comment_id":"1107392","content":"Selected Answer: D\nOption A, which suggests using burstable performance instances in unlimited mode, is appropriate. However, option D is more specific to the requirement of scaling based on predictive metrics, which is crucial for handling the latency issues that occur at specific times each month.\n\nOptions B and C suggest using compute optimized instances and scaling based on requests or on a schedule. While these options might work for general scalability, they may not address the immediate and intense spikes in CPU utilization that are mentioned in the scenario.\n\nTherefore, option D is the most appropriate solution for improving latency and automatically scaling the application based on predictive metrics using AWS Elastic Beanstalk.","poster":"Min_93"},{"poster":"evelynsun","timestamp":"1702738620.0","content":"Selected Answer: A\nThis solution meets the requirements because it allows the company to automatically scale the application's CPU capacity based on the number of requests it receives. The burstable performance instances provide high CPU performance when needed, which can help to reduce latency during peak hours. \n\nnot D: this solution has some drawbacks. First, it can be expensive to use burstable performance instances in unlimited mode, as the instances are charged per hour. Second, it can be difficult to predict the exact CPU requirements of the application, which can lead to over- or under-provisioning of CPU resources.","comment_id":"1098255","upvote_count":"3"},{"poster":"TariqKipkemei","timestamp":"1702022940.0","comment_id":"1090877","content":"Selected Answer: A\nThe company also wants to scale the application automatically when application demand increases = Scale based on requests","upvote_count":"3"},{"poster":"SHAAHIBHUSHANAWS","upvote_count":"2","timestamp":"1701729900.0","content":"B\nQuestion is asking scale based on demand so better scale based on requests. Predictive metrics not defined and may be interpreted differently by many users.","comment_id":"1088061"},{"content":"Selected Answer: D\nGiven the scenario described, the best solution among the provided options to meet the requirements of migrating the application to AWS, improving latency, and scaling the application automatically during increased demand would be:\n\nD. Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode. Configure the environment to scale on predictive metrics.","poster":"reika1914","timestamp":"1700873520.0","upvote_count":"2","comment_id":"1079657"},{"timestamp":"1700641260.0","poster":"t0nx","upvote_count":"4","comment_id":"1077121","content":"Selected Answer: D\nIn this scenario, the application experiences latency issues during peak hours with a sudden increase in CPU utilization. Using burstable performance instances in unlimited mode allows the application to burst beyond the baseline performance when needed. Configuring the environment to scale on predictive metrics enables proactive scaling based on anticipated increases in demand."}],"unix_timestamp":1700621460,"isMC":true,"question_id":648,"answer_description":"","timestamp":"2023-11-22 03:51:00"},{"id":"bd9DVZxfBSMKe5LJADaW","answer_images":[],"answer":"B","discussion":[{"content":"Selected Answer: B\nuse automation to secure its systems and network infrastructure = AWS CloudFormation\ntrack and audit all incremental changes to the infrastructure = AWS Config","comment_id":"1090885","timestamp":"1702023240.0","poster":"TariqKipkemei","upvote_count":"10"},{"poster":"Min_93","content":"Selected Answer: B\nOption B is the most suitable because it combines the benefits of infrastructure as code (CloudFormation) with tracking and auditing capabilities (AWS Config). With CloudFormation, the company can define and deploy its infrastructure in a repeatable and automated way, ensuring consistency and adherence to security standards. AWS Config then complements this by providing visibility into changes and configuration details.","timestamp":"1703737320.0","comment_id":"1107394","upvote_count":"5"},{"timestamp":"1720055700.0","upvote_count":"2","content":"Selected Answer: B\nI will go for B","comment_id":"1241733","poster":"Gape4"},{"poster":"awsgeek75","timestamp":"1705759200.0","comments":[{"content":"The difference is in wording: \"The company's security team must be able to track and audit all incremental changes to the infrastructure\"\n\nIf this has to be done BEFORE the deployment then D is the option\nIf this is AFTER the deployment then B is the option\n\nHopefully exam will have better language. Good luck!","poster":"awsgeek75","upvote_count":"3","comment_id":"1127306","timestamp":"1705759260.0"}],"upvote_count":"3","comment_id":"1127305","content":"Selected Answer: B\nOrganisations is not really related to this\nAWS Service Catalog is like a IaaC source control so D is a close option. However B looks more logical."}],"choices":{"A":"Use AWS Organizations to set up the infrastructure. Use AWS Config to track changes.","B":"Use AWS CloudFormation to set up the infrastructure. Use AWS Config to track changes.","D":"Use AWS CloudFormation to set up the infrastructure. Use AWS Service Catalog to track changes.","C":"Use AWS Organizations to set up the infrastructure. Use AWS Service Catalog to track changes."},"topic":"1","isMC":true,"exam_id":31,"answer_description":"","question_id":649,"answer_ET":"B","answers_community":["B (100%)"],"question_text":"A company has customers located across the world. The company wants to use automation to secure its systems and network infrastructure. The company's security team must be able to track and audit all incremental changes to the infrastructure.\n\nWhich solution will meet these requirements?","question_images":[],"timestamp":"2023-12-08 09:14:00","unix_timestamp":1702023240,"url":"https://www.examtopics.com/discussions/amazon/view/128070-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"vsoFjFTm4zAjUtMmcKZ5","url":"https://www.examtopics.com/discussions/amazon/view/128269-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","timestamp":"2023-12-11 09:08:00","answers_community":["BE (100%)"],"question_images":[],"isMC":true,"answer_ET":"BE","question_text":"A startup company is hosting a website for its customers on an Amazon EC2 instance. The website consists of a stateless Python application and a MySQL database. The website serves only a small amount of traffic. The company is concerned about the reliability of the instance and needs to migrate to a highly available architecture. The company cannot modify the application code.\n\nWhich combination of actions should a solutions architect take to achieve high availability for the website? (Choose two.)","question_id":650,"answer_description":"","unix_timestamp":1702282080,"choices":{"C":"Migrate the database to Amazon DynamoDB, and enable DynamoDB auto scaling.","B":"Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.","E":"Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2 instances that are distributed across two Availability Zones.","A":"Provision an internet gateway in each Availability Zone in use.","D":"Use AWS DataSync to synchronize the database data across multiple EC2 instances."},"answer":"BE","answer_images":[],"exam_id":31,"discussion":[{"upvote_count":"8","poster":"TariqKipkemei","comment_id":"1093234","content":"Selected Answer: BE\nTo achieve high availability for the website, Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance and Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2 instances that are distributed across two Availability Zones.","timestamp":"1718086080.0"},{"poster":"Sergiuss95","upvote_count":"6","timestamp":"1730477940.0","comment_id":"1205123","content":"Selected Answer: BE\nI sold my soul to the devil to pass the exam"},{"timestamp":"1720879620.0","content":"Selected Answer: BE\nB: RDS HA\nE: Application HA\n\nC: Company cannot change code so this won't work\nA: Does not make sense with other options\nD: Makes no sense with other options","comment_id":"1121831","upvote_count":"3","poster":"awsgeek75"},{"timestamp":"1718925960.0","content":"A. no failed over mechanism\nC. DynamoDB is no SQL, cannot use with MySQL\nD. Not HA, just sync/replication tools.\n\nAnswer BE.","poster":"Cyberkayu","upvote_count":"3","comment_id":"1102088"}]}],"exam":{"isBeta":false,"numberOfQuestions":1019,"provider":"Amazon","isImplemented":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31,"isMCOnly":true,"lastUpdated":"11 Apr 2025"},"currentPage":130},"__N_SSP":true}