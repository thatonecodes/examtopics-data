{"pageProps":{"questions":[{"id":"GH3QqtynWByUxS4KonxJ","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/99796-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"D":"Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.","C":"Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.","A":"Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.","B":"Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule."},"answer_description":"","timestamp":"2023-02-18 14:05:00","exam_id":31,"discussion":[{"poster":"elearningtakai","upvote_count":"10","comment_id":"855789","content":"Selected Answer: D\nAmazon Inspector is a security assessment service that automatically assesses applications for vulnerabilities or deviations from best practices. It can be used to scan the EC2 instances for software vulnerabilities. AWS Systems Manager Patch Manager can be used to patch the EC2 instances on a regular schedule. Together, these services can provide a solution that meets the requirements of running regular security scans and patching EC2 instances on a regular schedule. Additionally, Patch Manager can provide a report of each instance’s patch status.","timestamp":"1696079880.0"},{"timestamp":"1692754080.0","poster":"LuckyAro","upvote_count":"5","comment_id":"818742","content":"Selected Answer: D\nAmazon Inspector is a security assessment service that helps improve the security and compliance of applications deployed on Amazon Web Services (AWS). It automatically assesses applications for vulnerabilities or deviations from best practices. Amazon Inspector can be used to identify security issues and recommend fixes for them. It is an ideal solution for running regular security scans across a large fleet of EC2 instances.\n\nAWS Systems Manager Patch Manager is a service that helps you automate the process of patching Windows and Linux instances. It provides a simple, automated way to patch your instances with the latest security patches and updates. Patch Manager helps you maintain compliance with security policies and regulations by providing detailed reports on the patch status of your instances."},{"content":"Selected Answer: D\nA handy reference page for such questions is:\nhttps://aws.amazon.com/products/security/\nAmazon Inspector = vulnerability detection = patching\nhttps://aws.amazon.com/inspector/","upvote_count":"2","timestamp":"1720022220.0","poster":"awsgeek75","comment_id":"1112997"},{"comment_id":"999519","timestamp":"1709650800.0","poster":"Guru4Cloud","upvote_count":"2","content":"Selected Answer: D\ndddddddddd"},{"comment_id":"828407","content":"Selected Answer: D\nInspecter is for EC2 instances and network accessibility of those instances\nhttps://portal.tutorialsdojo.com/forums/discussion/difference-between-security-hub-detective-and-inspector/","upvote_count":"2","timestamp":"1693766580.0","poster":"Steve_4542636"},{"content":"Selected Answer: D\nAmazon Inspector for EC2\nhttps://aws.amazon.com/vi/inspector/faqs/?nc1=f_ls\nAmazon system manager Patch manager for automates the process of patching managed nodes with both security-related updates and other types of updates.\n\nhttp://webcache.googleusercontent.com/search?q=cache:FbFTc6XKycwJ:https://medium.com/aws-architech/use-case-aws-inspector-vs-guardduty-3662bf80767a&hl=vi&gl=kr&strip=1&vwsrc=0","poster":"TungPham","comment_id":"816239","upvote_count":"3","timestamp":"1692589140.0"},{"comment_id":"813488","timestamp":"1692383760.0","content":"answer - D\nhttps://aws.amazon.com/inspector/faqs/","upvote_count":"3","poster":"jennyka76"},{"comment_id":"812977","timestamp":"1692356700.0","content":"D as AWS Systems Manager Patch Manager can patch the EC2 instances.","upvote_count":"2","poster":"Neha999"}],"answer":"D","answer_images":[],"answers_community":["D (100%)"],"answer_ET":"D","unix_timestamp":1676725500,"topic":"1","question_id":276,"question_text":"A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large fleet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance’s patch status.\n\nWhich solution will meet these requirements?","isMC":true},{"id":"CU3EVlaEheHdDOsc46ER","url":"https://www.examtopics.com/discussions/amazon/view/85201-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1665529320,"answer_images":[],"discussion":[{"comment_id":"697437","upvote_count":"101","content":"Selected Answer: C\nI would go for C. The tricky phrase is \"near-real-time solution\", pointing to Firehouse, but it can't send data to DynamoDB, so it leaves us with C as best option. \n\nKinesis Data Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumologic, LogicMonitor, MongoDB, and HTTP End Point as destinations.\n\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/#:~:text=Kinesis%20Data%20Firehose%20currently%20supports,HTTP%20End%20Point%20as%20destinations.","timestamp":"1666014240.0","poster":"ArielSchivo","comments":[{"content":"This was a really tough one. But you have the best explanation on here with reference point. Thanks. I’m going with answer C!","comment_id":"788074","upvote_count":"4","timestamp":"1674675600.0","poster":"Lonojack"},{"poster":"SaraSundaram","timestamp":"1679205960.0","comments":[{"comment_id":"950853","content":"Stream is used if you want real time results , but with firehose , you generally use the data at a later point of time by storing it somewhere. Hence if you see \"REAL TIME\" the answer is most probably Kinesis Data Streams.","upvote_count":"20","poster":"diabloexodia","timestamp":"1689265920.0"}],"comment_id":"843470","upvote_count":"4","content":"There are many questions having Firehose and Stream. Need to know them in detail to answer. Thanks for the explanation"},{"comments":[{"timestamp":"1677058740.0","comment_id":"817683","content":"\"easily stream data at any scale\" \nThis is a description of Kinesis Data Stream. I think you can configure its quantity but still not provision and manage scalability by yourself.","poster":"habibi03336","upvote_count":"1"}],"upvote_count":"1","poster":"lizzard812","content":"Sorry but I still can't see how Kinesis Data Stream is 'scalable', since you have to provision the quantity of shards in advance?","comment_id":"797320","timestamp":"1675454160.0"}]},{"timestamp":"1666138920.0","content":"The answer is C, because Firehose does not suppport DynamoDB and another key word is \"data\" Kinesis Data Streams is the correct choice. Pay attention to key words. AWS likes to trick you up to make sure you know the services.","poster":"JesseeS","comment_id":"698574","upvote_count":"33"},{"timestamp":"1738525200.0","upvote_count":"1","comment_id":"1350626","poster":"kyd0nix","content":"Selected Answer: B\nIMO \"near-real-time\" is key for Firehose, BUT since of all the discussions B vs C (Firehose can't have DynamoDB as destination, I think the question is misswritten and has to be reviewed to avoid the confusion)"},{"content":"Selected Answer: C\nC - Kinesis Data Streams allows for low-latency processing, which is crucial for near-real-time requirements.","timestamp":"1737276840.0","comment_id":"1342931","upvote_count":"1","poster":"FlyingHawk"},{"upvote_count":"1","content":"Selected Answer: C\nScalable processing: The system must scale to handle hundreds of thousands of users and millions of transactions during peak hours.\nNear-real-time sharing of transactions: Transactions should be shared with internal applications in near-real time.\nSensitive data removal: Sensitive information must be processed and removed before storage.\nLow-latency retrieval: The processed data must be stored in a document database (Amazon DynamoDB) for quick access.","poster":"MGKYAING","timestamp":"1735280040.0","comment_id":"1332252"},{"poster":"aefuen1","comments":[{"content":"The question says it needs to have sensitive information removed before DB storage. Other apps could consume transaction data but not necessarily store, or consume only non-sensitive data.","upvote_count":"1","comment_id":"1357078","timestamp":"1739660220.0","poster":"kernel1"}],"upvote_count":"2","content":"Selected Answer: B\nIt's B. You can write to the DynamoDB table from the lambda preprocessing function. Also option C can't be correct because if \"Other applications can consume the transactions data off the Kinesis data stream\", this means they will consume data with sensitive values, which is a constraint for the solution.","comment_id":"1328844","timestamp":"1734579660.0"},{"upvote_count":"1","poster":"engnrshon","content":"C :","timestamp":"1732345980.0","comment_id":"1316591"},{"upvote_count":"2","poster":"Mauro0001","content":"Selected Answer: C\nOne of the tricky phrases is 'near-real-time solutions' because it points to the fact that every time a write is made to a database, it incurs a delay, and then retrieving it with an API call adds another latency. With Kinesis Data Streams, that process is optimized because the intermediary that gives you the ability to write to DynamoDB also provides that data to other services due to the retention period of Kinesis Data Streams.","timestamp":"1725388980.0","comment_id":"1277785"},{"upvote_count":"2","content":"Selected Answer: C\nAns C. \nHigh level difference between the Kinesis and DynamoDB:\nKinesis Streams allows production/ consumption of large volumes of data (web data, logs, etc); DynamoDB Streams is a feature local to DynamoDB to track the granular changes to DynamoDB table items.\n(Note also: data latency for Firehose is 60 seconds or higher; Streams is for custom processing and has sub-second processing latency).","comment_id":"1274092","poster":"PaulGa","timestamp":"1724853060.0"},{"poster":"Lin878","upvote_count":"2","comment_id":"1243688","content":"Selected Answer: C\nQ: What is a destination in Firehose?\n\nA destination is the data store where your data will be delivered. Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumo Logic, LogicMonitor, MongoDB, and HTTP End Point as destinations.\n\nhttps://aws.amazon.com/firehose/faqs/","timestamp":"1720324560.0"},{"content":"Selected Answer: C\nwith multiple consumers and on the fly modification, it seems like the most logical choice","poster":"the_mellie","comment_id":"1217081","timestamp":"1716504960.0","upvote_count":"2"},{"timestamp":"1709930400.0","content":"I chose B. The \"near real time\" is very specific to Kinesis firehose which is a better option anyway. The rest of the answer makes sense too. C is wrong : \"sensitive data removed by Lambda & then store transaction data in DynamoDB\" , while it continues to say other applications are accessing the transaction data from kinesis Data stream !!","upvote_count":"3","comment_id":"1169095","poster":"vi24"},{"timestamp":"1708269780.0","poster":"Pics00094","upvote_count":"5","content":"Selected Answer: C\nneed to know..\n1) Lambda Integration\n2) Difference between Real time(Kinesis Data Stream) vs Near Real time(Kinesis Fire House)\n3) Firehouse can't target DynamoDB","comment_id":"1153376"},{"poster":"JulianWaksmann","comment_id":"1141433","timestamp":"1707164160.0","content":"i think c are bad too, because it isn't near real time.","upvote_count":"2"},{"content":"Selected Answer: C\nA: DynamoDB streams are logs, not fit for real-time sharing.\nB: S3 is not document database, it's BLOB\nD: S3 and files are not database\nC: Kinesis + Lambda + DynamoDB is high performance, low latency scalable solution.","upvote_count":"3","poster":"awsgeek75","timestamp":"1705185720.0","comment_id":"1122095"},{"upvote_count":"1","poster":"A_jaa","content":"Selected Answer: C\nAnswer-C","timestamp":"1705149720.0","comment_id":"1121647"},{"timestamp":"1703169600.0","poster":"bujuman","upvote_count":"1","content":"Selected Answer: C\nData Stream can handle near-real-time and is able to store to DynamoDB","comment_id":"1102616"},{"poster":"djgodzilla","timestamp":"1702580700.0","upvote_count":"1","comment_id":"1096743","content":"Selected Answer: C\nKinesis Data Streams stores data for later processing by applications , key difference with Firehose which delivers data directly to AWS services."},{"timestamp":"1699712880.0","upvote_count":"1","comment_id":"1067895","content":"Selected Answer: C\nCorrect answer is C.\nAs some commented already, 'near-real-time' could make you think abut Firehose but its consumers are 3rd-party partners destinations, Amazon S3, Amazon Redshift, Amazon OpenSearch and HTTP endpoint so DynamoDB can't be used in this scenario.","poster":"wabosi"},{"content":"C is the best solution for the following reasons:\n\n1. Real-time Data Stream: To share millions of financial transactions with other apps, you need to be able to ingest data in real-time, which is made possible by Amazon Kinesis Data Streams.\n\n2. Data Transformation: You can cleanse and eliminate sensitive data from transactions before storing them in Amazon DynamoDB by utilizing AWS Lambda with Kinesis Data Streams. This takes care of the requirement to handle sensitive data with care.\n3. Scalability: DynamoDB and Amazon Kinesis are both extremely scalable technologies that can manage enormous data volumes and adjust to the workload.","poster":"Ruffyit","upvote_count":"1","timestamp":"1698332640.0","comment_id":"1054688"},{"comment_id":"1048066","content":"C is the best solution for the following reasons:\n\n 1. Real-time Data Stream: To share millions of financial transactions with other apps, you need to be able to ingest data in real-time, which is made possible by Amazon Kinesis Data Streams.\n\n 2. Data Transformation: You can cleanse and eliminate sensitive data from transactions before storing them in Amazon DynamoDB by utilizing AWS Lambda with Kinesis Data Streams. This takes care of the requirement to handle sensitive data with care.\n 3. Scalability: DynamoDB and Amazon Kinesis are both extremely scalable technologies that can manage enormous data volumes and adjust to the workload.\n\n 4. Low-Latency retrieval: Applications requiring real-time data can benefit from low-latency retrieval, which is ensured by storing the processed data in DynamoDB.","comments":[{"upvote_count":"2","timestamp":"1697729820.0","content":"Choices A, B, and D are limited in certain ways:\n • Real-time data streaming is not provided by Option A (DynamoDB with Streams); additional components would need to be implemented in order to handle data in real-time.\n • Kinesis Data Firehose, Option B, lacks the real-time processing capabilities of Kinesis Data Streams and is primarily used for data distribution to destinations like as S3.\n • For near-real-time use cases, Option D (Batch processing with S3) is not the best choice. It adds latency and overhead associated with batch processing, which is incompatible with the need for real-time data sharing.\nUsing the advantages of Lambda, DynamoDB, and Kinesis Data Streams, Option C offers a scalable, real-time, and effective solution for the given use case.","comment_id":"1048069","poster":"AWSStudyBuddy"}],"timestamp":"1697729640.0","poster":"AWSStudyBuddy","upvote_count":"4"},{"comments":[{"comment_id":"1055585","content":"firehose can not send data to dynamoDB","poster":"spw7","timestamp":"1698409860.0","upvote_count":"1"}],"content":"I picked B. We need to understand how Kinesis Data Warehouse works to answer this question right.","timestamp":"1695557820.0","comment_id":"1015741","poster":"Ak9kumar","upvote_count":"1"},{"content":"kinesis Data Firhouse optionally support lambda for transformation","timestamp":"1691552580.0","upvote_count":"1","comment_id":"976211","poster":"sohailn"},{"poster":"TariqKipkemei","upvote_count":"9","content":"Selected Answer: C\nScalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications = Amazon Kinesis Data Streams.\nRemove sensitive data from transactions = AWS Lambda.\nStore transaction data in a document database for low-latency retrieval = Amazon DynamoDB.","comment_id":"970726","timestamp":"1691035980.0"},{"upvote_count":"3","poster":"cookieMr","content":"Selected Answer: C\nTo meet the requirements of sharing financial transaction details with several other internal applications, and processing and storing the transactions data in a scalable and near-real-time manner, a solutions architect should recommend option C: Stream the transactions data into Amazon Kinesis Data Streams, use AWS Lambda integration to remove sensitive data, and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.\n\nOption A (storing transactions data in DynamoDB and using DynamoDB Streams) may not provide the same level of scalability and real-time data sharing as Kinesis Data Streams. Option B (using Kinesis Data Firehose to store data in DynamoDB and S3) adds unnecessary complexity and additional storage costs. Option D (storing batched transactions data in S3 and processing with Lambda) may not provide the required near-real-time data sharing and low-latency retrieval compared to the streaming-based solution.","timestamp":"1687087260.0","comment_id":"926640"},{"content":"Selected Answer: C\nits c because yes","comment_id":"924222","timestamp":"1686835140.0","poster":"oiccic99","upvote_count":"1"},{"comment_id":"921894","timestamp":"1686625500.0","content":"I think it is B. Kinesis data stream can import data from DynamoDB, but can not export data to DynamoDB. Data stream only support to export to Lamda, Kinesis Firehose,Kinesis Analytics or AWS Glue. Data stream's exporting to other object need to ETL transform process , which is Firehose's function.","upvote_count":"1","poster":"Chris22usa"},{"comment_id":"921505","timestamp":"1686577800.0","poster":"konieczny69","upvote_count":"1","content":"Selected Answer: B\nnear real time - firehose\nbesides - dynamo is no the destination, lambda is\nand lambda can be used since you can expose it behind http"},{"content":"Selected Answer: B\nThat is definitely B:\n\nIt is saying \"near real time\" that makes sense :\n\nnear real time : Kinesis Data Firehose\nreal time : Kinesis Data Stream\n\nAlso, Kinesis Data Firehose supports DynamoDB. The link is below :\n\nhttps://dynobase.dev/dynamodb-faq/can-firehose-write-to-dynamodb/#:~:text=Answer,data%20to%20a%20DynamoDB%20table.","comment_id":"897404","upvote_count":"1","timestamp":"1684053780.0","poster":"VIad","comments":[{"poster":"ruqui","comment_id":"899523","upvote_count":"3","content":"The problem says that Firehose will store data in Amazon DynamoDB and Amazon S3, I think it's not possible to have more than one consumer, so B solution is impossible","timestamp":"1684264560.0"},{"comments":[{"poster":"sakurali","comment_id":"1046631","timestamp":"1697610600.0","upvote_count":"1","content":"disagree with ur point, cuz Firehose is all about delivering data. It's like a reliable courier that ensures your data gets to its destination securely and promptly. While Firehose itself doesn't store data, it can deliver it to various AWS services, including Amazon DynamoDB and Amazon S3."}],"comment_id":"976029","upvote_count":"1","poster":"Clouddon","timestamp":"1691526540.0","content":"I disagree withe statement about firehose as stated from this source because aws says \"Kinesis Data Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumo Logic, LogicMonitor, MongoDB, and HTTP End Point as destinations.\""}]},{"poster":"plutonash","upvote_count":"5","timestamp":"1683186180.0","comment_id":"889263","content":"Selected Answer: B\nfor me the awswer is B. kinesis data firehose can transfert data to dynamoDB and the key world in the question : Near Real Time\nReal Time = Kinesis Data Stream\nNear Real Time = Kinesis Data Firehose"},{"upvote_count":"2","timestamp":"1680688860.0","content":"Selected Answer: B\nKinesis Data Firehose does have integration with Lambda. Kinesis Data Strems does not have that integration so B is correct","poster":"jcramos","comment_id":"861955"},{"upvote_count":"5","comment_id":"858618","poster":"bakamon","timestamp":"1680422040.0","content":"Selected Answer: C\nNear Real Time : Kinesis Data Stream & Kinesis Data Firehouse\nKinesis Data Stream :: used for streaming live data\nKinesis Data Firehouse :: used when you have to store the streaming data into S3, Redshift etc"},{"comment_id":"857083","content":"Selected Answer: C\nThis solution meets the requirements for scalability, near-real-time processing, and sharing data with several internal applications. Kinesis Data Streams is a fully managed service that can handle millions of transactions per second, making it a scalable solution. Using Lambda to process the data and remove sensitive information provides a fast and efficient method to perform data transformation in near-real-time. Storing the processed data in DynamoDB allows for low-latency retrieval, and the data can be shared with other applications using the Kinesis data stream.","poster":"linux_admin","upvote_count":"2","timestamp":"1680267120.0"},{"comment_id":"856579","upvote_count":"1","content":"C : B is incorrect , coz firehouse can't work with lambda","timestamp":"1680235200.0","poster":"will2will"},{"upvote_count":"1","content":"Selected Answer: C\nKinesis Data Firehose doesn't support DynamoDB as a destination. \nhttps://docs.aws.amazon.com/firehose/latest/dev/create-name.html","poster":"Abhineet9148232","comment_id":"854321","timestamp":"1680088740.0"},{"poster":"bilel500","comment_id":"830925","upvote_count":"1","content":"Selected Answer: C\nKinesis Data Streams focuses on ingesting and storing data streams. Kinesis Data Firehose focuses on delivering data streams to select destinations. Both can ingest data streams but the deciding factor in which to use depends on where your streamed data should go to.","timestamp":"1678114560.0"},{"comment_id":"826563","poster":"Daiking","timestamp":"1677740520.0","upvote_count":"1","content":"Selected Answer: C\nI was confused B because it's the phrase \"near-real-time\", but the destination of Firehose can not be DynamoDB.\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-destination.html"},{"comment_id":"820626","content":"Answer B. Question says: \"The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database\". So, only the data stored in database needs to be sensitized NOT the ones which is to be stored in S3. Option C is wrong because option C says: \"Use AWS Lambda integration to remove sensitive data from every transaction\" which is NOT what the question asks for.","upvote_count":"1","poster":"Bhawesh","timestamp":"1677250140.0"},{"upvote_count":"2","poster":"Bhawesh","content":"Selected Answer: B\nMy vote is: option B. Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.\nThis question has 2 requirements:\n1. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. \n2. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.","comment_id":"818111","timestamp":"1677087240.0"},{"comment_id":"794508","content":"Selected Answer: C\n\"Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.\" \nYou can't do it with Firehose.","timestamp":"1675190940.0","poster":"pgomess","upvote_count":"1"},{"content":"Selected Answer: B\nKDS doesnt remove sensitive information as required.\nB is correct","timestamp":"1674714420.0","upvote_count":"1","comments":[{"timestamp":"1674848460.0","upvote_count":"1","poster":"dark_firzen","content":"Lambda does.","comment_id":"789952"}],"poster":"sassy2023","comment_id":"788435"},{"poster":"LuckyAro","timestamp":"1674607560.0","upvote_count":"4","content":"Selected Answer: B\nOther applications can consume from Kinesis Data Streams with the sensitive information still unremoved ? The question requires that sensitive information be purged from the Data Stream.","comment_id":"787125"},{"comment_id":"784406","upvote_count":"2","comments":[{"comment_id":"784407","content":"Option B, using Amazon Kinesis Data Firehose, is a more cost-effective solution for storing and processing large amounts of data in near real-time. This service automatically scales based on the incoming data rate and it can automatically store the data in Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service, and it can also invoke a Lambda function to process the data before storing it. This option eliminates the need for additional management, monitoring and maintenance of Kinesis data streams.","upvote_count":"3","timestamp":"1674399180.0","poster":"bullrem"}],"content":"Selected Answer: B\nKinesis Data Streams is a service that allows you to collect, process, and analyze streaming data in real-time. It can handle a large number of transactions and it can scale to match the rate of incoming data. However, it comes with additional costs for data retention, data throughput, and number of shards. Additionally, it requires additional management and maintenance to set up, configure, and monitor the Kinesis data streams.","poster":"bullrem","timestamp":"1674399180.0"},{"timestamp":"1674366840.0","comments":[{"timestamp":"1674366900.0","poster":"kdinesh95","content":"Kinesis data stream\n\n• A massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can configure hundreds of thousands of data producers to continuously put data into a Kinesis data stream.","comment_id":"783945","upvote_count":"1"}],"upvote_count":"1","content":"Kinesis data analytics : Option c the question has the in the first line.\n• Analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. You can quickly build SQL queries and Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale.\n\nKinesis Data Firehose\n• It can capture, transform, and load streaming data into S3, Redshift, Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards being used today.","comment_id":"783943","poster":"kdinesh95"},{"content":"Selected Answer: C\nStream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.","upvote_count":"1","comments":[{"upvote_count":"1","content":"Other applications can consume from Kinesis Data Streams with the sensitive information still unremoved ? The question requires that sensitive information be purged from the Data Stream.","timestamp":"1674607440.0","poster":"LuckyAro","comment_id":"787123"}],"timestamp":"1673043780.0","poster":"SilentMilli","comment_id":"768115"},{"content":"Selected Answer: C\nC seems right","poster":"techhb","timestamp":"1672701480.0","comment_id":"764113","upvote_count":"1"},{"poster":"Idriss10","timestamp":"1672321020.0","content":"Selected Answer: C\nThis is what made the difference : to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB.","upvote_count":"2","comment_id":"761065"},{"comment_id":"751848","timestamp":"1671594420.0","upvote_count":"2","poster":"duriselvan","content":"C- ans:- Data storage for 1 to 365 days - Data storage for 1 to 365 days and \nKinesis Data Firehose >No data storage"},{"upvote_count":"3","poster":"pazabal","content":"Selected Answer: C\nlow-latency retrieval = dynamodb","timestamp":"1671553260.0","comment_id":"751177"},{"upvote_count":"2","content":"Selected Answer: C\nTo meet the requirements of a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications, while also processing the transactions to remove sensitive data before storing them in a document database for low-latency retrieval, a solutions architect should recommend streaming the transactions data into Amazon Kinesis Data Streams and using AWS Lambda integration to remove sensitive data from every transaction before storing the data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.\n\nAmazon Kinesis Data Streams is a fully managed service for real-time processing of streaming data at scale. You can use Kinesis Data Streams to continuously capture and store large amounts of data in real-time, such as financial transactions in this case. You can then process the data using a variety of real-time analytics and processing tools, such as AWS Lambda.","poster":"Buruguduystunstugudunstuy","comments":[{"comments":[{"timestamp":"1671504960.0","upvote_count":"2","comment_id":"750428","poster":"Buruguduystunstugudunstuy","content":"Option A, storing the transactions data in Amazon DynamoDB and using DynamoDB Streams to share the data with other applications, would not meet the requirement to remove sensitive data from the transactions before storing them in a document database.\n\nOption B, streaming the transactions data into Amazon Kinesis Data Firehose to store the data in Amazon DynamoDB and Amazon S3, and using AWS Lambda integration with Kinesis Data Firehose to remove sensitive data, would not allow other applications to consume the data in real-time.\n\nOption D, storing the batched transactions data in Amazon S3 as files and using AWS Lambda to process and remove sensitive data before updating the files in Amazon S3 and storing the data in Amazon DynamoDB, would not meet the requirement for a near-real-time solution as it involves batch processing of the transactions data."}],"comment_id":"750427","content":"AWS Lambda is a serverless computing platform that allows you to run code in response to events and automatically scale to meet demand. By using Lambda integration with Kinesis Data Streams, you can process the transactions data as it is streamed into the data stream, removing sensitive data from each transaction before storing it in Amazon DynamoDB. This will allow you to meet the requirement to remove sensitive data from the transactions before storing them in a document database.\n\nOther applications can consume the transactions data off the Kinesis data stream in real-time, allowing you to meet the requirement to share the transactions data with other internal applications.","timestamp":"1671504900.0","poster":"Buruguduystunstugudunstuy","upvote_count":"2"}],"timestamp":"1671504840.0","comment_id":"750426"},{"poster":"career360guru","content":"Selected Answer: C\nOption C","upvote_count":"1","comment_id":"749545","timestamp":"1671432000.0"},{"upvote_count":"2","poster":"icaniwill","timestamp":"1670090040.0","comment_id":"734613","content":"Selected Answer: C\nAnswer is C. Kinesis Streams is good for low latency streaming ingested at scale."},{"poster":"9014","content":"Selected Answer: B\nIt clearly mentions near-real-time so the answer is B. Firehose support near-real-time and kinesis support real-time data process","upvote_count":"3","comment_id":"732722","timestamp":"1669905540.0"},{"comment_id":"723510","content":"C is correct","timestamp":"1669035840.0","upvote_count":"1","poster":"Wpcorgan"},{"upvote_count":"2","content":"Selected Answer: C\nWill vote C due to lack of support of DynamoDB in Firehose. Solution A caught my eye for a second but then ignoring it becaue DynamoDB streams would only send data changes to an existing table.","poster":"Wajif","comment_id":"717095","timestamp":"1668315540.0"},{"timestamp":"1667644260.0","content":"This question is messed up. AWS question points out \"Near real time\" which points to Kinesis Firehose and then went ahead to mess it up with a solution support pointing to DynamoDB which is inaccurate and leave us with the C option with Kinesis Data Stream, which is a \"real time\" streams.","upvote_count":"4","poster":"keezbadger","comment_id":"711682"},{"content":"Selected Answer: C\nA and D are out since they are bad answers.\nB is impossible as Kinesis Firehose cannot directly store data in DynamoDB.\nThis only leaves C","comment_id":"695539","upvote_count":"1","timestamp":"1665852900.0","poster":"Gatt"},{"poster":"KVK16","content":"B vs C \nB : Pro- Sensitive information removal by Lambda , near real-time\nCon - Ingesting into Dynamo DB although has low latency is complex and other application using S3 for data \n \nC: Pro - Other Apps using data off DB Streams\nCon : Sensitive data removal lambda , near real-time \nMy Pick is B for being more managed service - a Rule of thumb","timestamp":"1665821700.0","comment_id":"695275","upvote_count":"1"},{"timestamp":"1665721920.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/zh_cn/streams/latest/dev/tutorial-stock-data-lambda.html","upvote_count":"1","poster":"BoboChow","comment_id":"694489"},{"content":"Please explain C","poster":"340trunkhawk","upvote_count":"1","comment_id":"694366","timestamp":"1665705960.0"},{"comment_id":"694022","timestamp":"1665673560.0","upvote_count":"1","comments":[{"comment_id":"694361","upvote_count":"2","timestamp":"1665705120.0","content":"It's C. There is no support for Firehose to DynamoDB.","comments":[{"comment_id":"701868","upvote_count":"2","content":"that's correct, I missed that point, thanks :)","timestamp":"1666492800.0","poster":"Sinaneos"}],"poster":"herculian_effort"}],"content":"Selected Answer: B\nNear-real-time directly points to kinesis firehose, it's fully managed and scalable, along with dynamoDB. Transforming data with lambda function integration is one of its main use cases. Therefore the answer has got to be B \nhttps://aws.amazon.com/kinesis/data-firehose/faqs/?nc=sn&loc=5","poster":"Sinaneos"},{"content":"The answer is C","poster":"Lilibell","timestamp":"1665529320.0","comment_id":"692471","upvote_count":"4"}],"question_text":"A company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.\nWhat should a solutions architect recommend to meet these requirements?","choices":{"A":"Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications.","C":"Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.","D":"Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3.","B":"Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3."},"question_images":[],"answers_community":["C (87%)","13%"],"topic":"1","exam_id":31,"answer_description":"","answer":"C","timestamp":"2022-10-12 01:02:00","answer_ET":"C","isMC":true,"question_id":277},{"id":"1O0D61MAC6QiAzdtljaI","exam_id":31,"answer_description":"","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/99702-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest.\n\nWhat should a solutions architect do to meet this requirement?","answer_images":[],"answers_community":["A (100%)"],"choices":{"C":"Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances by using the certificate.","D":"Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certificate.","A":"Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.","B":"Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances."},"question_images":[],"isMC":true,"answer":"A","discussion":[{"comment_id":"1113002","content":"Selected Answer: A\nA: Enable encryption\nB: KMS is for storage and doesn't directly integrate to DB without further work\nC and D are for data encryption in transit not at rest","poster":"awsgeek75","timestamp":"1720022400.0","comments":[{"comment_id":"1125957","timestamp":"1721305740.0","content":"Actually, D is total nonsense and no idea what it is saying","poster":"awsgeek75","upvote_count":"1"}],"upvote_count":"3"},{"upvote_count":"4","comment_id":"1076444","content":"Selected Answer: A\nKMS only generates and manages encryption keys. That's it. That's all it does. It's a fundamental service that you as well as other AWS Services (like Secrets Manager) use it to encrypt or decrypt.\nKey Management Service. Secrets Manager is for database connection strings.\n upvoted 3 times","poster":"robpalacios1","timestamp":"1716300900.0"},{"content":"OK, but why not B???","timestamp":"1701011340.0","poster":"antropaws","comments":[{"poster":"aaroncelestin","comment_id":"986036","content":"KMS only generates and manages encryption keys. That's it. That's all it does. It's a fundamental service that you as well as other AWS Services (like Secrets Manager) use it to encrypt or decrypt. \n\nSecrets Manager stores actual secrets like passwords, pass phrases, and anything else you want encrypted. SM uses KMS to encrypt its secrets, it would be circular to get an encryption key from KMS to use SM to encrypt the encryption key.","timestamp":"1708465020.0","upvote_count":"5"}],"upvote_count":"1","comment_id":"907379"},{"timestamp":"1698436620.0","comment_id":"883009","content":"Selected Answer: A\nANSWER - A","poster":"SkyZeroZx","upvote_count":"1"},{"poster":"datz","timestamp":"1695589620.0","comment_id":"849667","upvote_count":"2","content":"Selected Answer: A\nA for sure"},{"timestamp":"1694051100.0","content":"A is 100% Crt","poster":"PRASAD180","comment_id":"831532","upvote_count":"2"},{"timestamp":"1693766700.0","poster":"Steve_4542636","upvote_count":"4","content":"Selected Answer: A\nKey Management Service. Secrets Manager is for database connection strings.","comment_id":"828409"},{"content":"Selected Answer: A\nA is the correct solution to meet the requirement of encrypting the data at rest. \n\nTo encrypt data at rest in Amazon RDS, you can use the encryption feature of Amazon RDS, which uses AWS Key Management Service (AWS KMS). With this feature, Amazon RDS encrypts each database instance with a unique key. This key is stored securely by AWS KMS. You can manage your own keys or use the default AWS-managed keys. When you enable encryption for a DB instance, Amazon RDS encrypts the underlying storage, including the automated backups, read replicas, and snapshots.","upvote_count":"4","timestamp":"1692754260.0","comment_id":"818746","poster":"LuckyAro"},{"poster":"bdp123","content":"Selected Answer: A\nAWS Key Management Service (KMS) is used to manage the keys used to encrypt and decrypt the data.","upvote_count":"2","comment_id":"818215","timestamp":"1692721500.0"},{"poster":"pbpally","timestamp":"1692554160.0","comment_id":"815766","upvote_count":"2","content":"Selected Answer: A\nOption A"},{"upvote_count":"2","timestamp":"1692458100.0","content":"A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances is the correct answer to encrypt the data at rest in Amazon RDS DB instances.\n\nAmazon RDS provides multiple options for encrypting data at rest. AWS Key Management Service (KMS) is used to manage the keys used to encrypt and decrypt the data. Therefore, a solution architect should create a key in AWS KMS and enable encryption for the DB instances to encrypt the data at rest.","poster":"NolaHOla","comment_id":"814302"},{"upvote_count":"2","content":"ANSWER - A\nhttps://docs.aws.amazon.com/whitepapers/latest/efs-encrypted-file-systems/managing-keys.html","timestamp":"1692440160.0","poster":"jennyka76","comment_id":"814050"},{"timestamp":"1692293880.0","upvote_count":"3","content":"Selected Answer: A\nA. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.\n\nhttps://www.examtopics.com/discussions/amazon/view/80753-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"812285","poster":"Bhawesh"}],"unix_timestamp":1676662680,"topic":"1","question_id":278,"timestamp":"2023-02-17 20:38:00"},{"id":"vtpIZsbudw7wPSANDwHE","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/99603-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"A","answer_description":"","question_text":"A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company’s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization.\n\nWhat should a solutions architect do to meet these requirements?","answers_community":["A (93%)","7%"],"answer_images":[],"choices":{"C":"Use a secure VPN connection.","B":"Use AWS DataSync.","A":"Use AWS Snowball.","D":"Use Amazon S3 Transfer Acceleration."},"question_images":[],"isMC":true,"answer":"A","discussion":[{"poster":"kruasan","timestamp":"1698658320.0","comment_id":"884956","upvote_count":"13","content":"Selected Answer: A\nDon't mix up between Mbps and Mbs. \nThe proper calculation is:\n\n10 MB/s x 86,400 seconds per day x 30 days/8 = 3,402,000 MB or approximately 3.4 TB"},{"content":"Selected Answer: A\nHonestly, the company has bigger problem with that slow connection :)\n30 days is the first clue so you can get snowball shipped and sent back (5 days each way)","poster":"awsgeek75","upvote_count":"6","comment_id":"1113004","timestamp":"1720022580.0"},{"content":"Selected Answer: A\naws snowball은 대용량 데이터 이전하기 위한 것 입니다.","upvote_count":"1","timestamp":"1719758820.0","poster":"cabta","comment_id":"1110707"},{"timestamp":"1713341700.0","content":"(15/8) = 1.875 MB/s\n1.875 MB/s x 0.7 = 1.3125 (70% NW utilization) MB/s\n1.3125 MB/s x 3600 = 4725 MB (MB per 1 hour)\n4725 x 24 = 113400 MB per 1 full day (24h)\n113400 x 30 = 3402000 MB for 30 days \n3402000 / 1024 = 3322.265625 GB for 30 days \n3322.265625 / 1024 ~ 3.24 TB for 30 days => not enough for NW => Snowball which is A","poster":"wsdasdasdqwdaw","upvote_count":"4","comment_id":"1045697"},{"timestamp":"1712891700.0","content":"Selected Answer: A\nI wont try to think to much about it, AWS Snowball was designed for this","comment_id":"1041311","poster":"TariqKipkemei","upvote_count":"4"},{"poster":"Guru4Cloud","timestamp":"1710523980.0","comment_id":"1008559","content":"Selected Answer: A\n° 15 Mbps bandwidth with 70% max utilization limits the effective bandwidth to 10.5 Mbps or 1.31 MB/s.\n° 20 TB of data at 1.31 MB/s would take approximately 193 days to transfer over the network. ° This far exceeds the 30 day requirement.\n° AWS Snowball provides a physical storage device that can be shipped to the data center. Up to 80 TB can be loaded onto a Snowball device and shipped back to AWS.\nThis allows the 20 TB of data to be transferred much faster by shipping rather than over the limited network bandwidth.\n° Snowball uses tamper-resistant enclosures and 256-bit encryption to keep the data secure during transit.\n° The data can be imported into Amazon S3 or Amazon Glacier once the Snowball is received by AWS.","upvote_count":"5"},{"comment_id":"842915","timestamp":"1695045360.0","comments":[{"timestamp":"1695783480.0","comments":[{"content":"3,402,000","timestamp":"1695783660.0","poster":"Zox42","upvote_count":"2","comment_id":"851643"},{"upvote_count":"1","comment_id":"954232","content":"How can 15 * 0.7 be 1.3125 LMAO","poster":"hozy_","timestamp":"1705502880.0","comments":[{"poster":"hozy_","comment_id":"954233","upvote_count":"3","content":"OMG it was Mbps! Not MBps. You are right! awesome!!!","timestamp":"1705502940.0"}]}],"poster":"Zox42","upvote_count":"3","comment_id":"851642","content":"15 Mbps * 0.7 = 1.3125 MB/s and 1.3125 * 86,400 * 30 = 3.402.000 MB\nAnswer A is correct."}],"content":"Selected Answer: B\n10 MB/s x 86,400 seconds per day x 30 days = 25,920,000 MB or approximately 25.2 TB\n\nThat's how much you can transfer with a 10 Mbps link (roughly 70% of the 15 Mbps connection).\n\nWith a consistent connection of 8~ Mbps, and 30 days, you can upload 20 TB of data.\n\nMy math says B, my brain wants to go with A. Take your pick.","poster":"UnluckyDucky","upvote_count":"3"},{"upvote_count":"3","comment_id":"820620","poster":"Bilalazure","timestamp":"1692881100.0","content":"Selected Answer: A\nAws snowball"},{"content":"A is 100% Crt","comment_id":"818800","poster":"PRASAD180","timestamp":"1692758400.0","upvote_count":"2"},{"comment_id":"818756","upvote_count":"2","poster":"LuckyAro","timestamp":"1692754680.0","content":"Selected Answer: A\nAWS Snowball"},{"poster":"pbpally","content":"Selected Answer: A\nOption a","comment_id":"815761","upvote_count":"2","timestamp":"1692554100.0"},{"content":"ANSWER - A\nhttps://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html","upvote_count":"2","timestamp":"1692439620.0","poster":"jennyka76","comment_id":"814042"},{"content":"Selected Answer: A\noption A","comment_id":"811776","poster":"AWSSHA1","upvote_count":"4","timestamp":"1692262860.0"}],"unix_timestamp":1676631660,"topic":"1","timestamp":"2023-02-17 12:01:00","question_id":279},{"id":"hm9oPiACZb1WwzUJ5CQ6","isMC":true,"answer_ET":"B","topic":"1","answer_images":[],"exam_id":31,"question_text":"A company needs to provide its employees with secure access to confidential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices.\n\nThe files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity.\n.\nWhich solution will meet these requirements?","answers_community":["B (93%)","7%"],"answer_description":"","question_images":[],"answer":"B","timestamp":"2023-02-18 13:46:00","url":"https://www.examtopics.com/discussions/amazon/view/99792-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.","A":"Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound traffic to the employees’ IP addresses.","B":"Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.","D":"Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On)."},"discussion":[{"timestamp":"1680182460.0","poster":"elearningtakai","comment_id":"855793","content":"Selected Answer: B\nThis solution addresses the need for secure access to confidential and sensitive files, as well as the increase in remote usage. Migrating the files to Amazon FSx for Windows File Server provides a scalable, fully managed file storage solution in the AWS Cloud that is accessible from on-premises and cloud environments. Integration with the on-premises Active Directory allows for a consistent user experience and centralized access control. AWS Client VPN provides a secure and managed VPN solution that can be used by employees to access the files securely.","upvote_count":"8"},{"timestamp":"1703863080.0","content":"Selected Answer: B\nC has \"signed URL\", everyone who has the URL could download. Plus, only B ensure the \"must be downloaded securely\" part by using VPN.","upvote_count":"5","poster":"pentium75","comment_id":"1108836"},{"upvote_count":"3","comment_id":"1152595","timestamp":"1708179840.0","content":"Selected Answer: B\nMy money is on B, but it's still not mentioned that the customer used an on-prem Active Directory.","poster":"NayeraB"},{"comments":[{"comment_id":"1315321","content":"\"Data must be downloaded securely to users' devices\" => AWS Client VPN","upvote_count":"1","poster":"JA2018","timestamp":"1732114320.0"}],"poster":"TariqKipkemei","timestamp":"1697080980.0","upvote_count":"4","content":"Selected Answer: B\nWindows file server = Amazon FSx for Windows File Server file system\nFiles can be accessed only by authorized users = On-premises Active Directory","comment_id":"1041318"},{"comments":[{"timestamp":"1705588500.0","comment_id":"1125965","content":"But then how do you download the files to user's machine in a secure way?","poster":"awsgeek75","upvote_count":"2"},{"poster":"pentium75","timestamp":"1703862960.0","upvote_count":"4","content":"That's why we're using FSX for Windows File Server in AWS.\n\n\"Signed URL to allow download\" would allow everyone who has the URL to download the files, but we must \"ensure that the files can be accessed only by authorized users\". Plus, the \"private VPC endpoint\" is not really of use here, it's still S3 and the users are not in AWS.","comment_id":"1108833"}],"poster":"BrijMohan08","timestamp":"1693877460.0","comment_id":"998938","content":"Selected Answer: C\nRemember: The file server is running out of capacity.","upvote_count":"2"},{"content":"Selected Answer: B\nB is the correct answer","comment_id":"891716","poster":"SkyZeroZx","timestamp":"1683501120.0","upvote_count":"2"},{"comment_id":"818808","content":"Selected Answer: B\nB is the best solution for the given requirements. It provides a secure way for employees to access confidential and sensitive files from anywhere using AWS Client VPN. The Amazon FSx for Windows File Server file system is designed to provide native support for Windows file system features such as NTFS permissions, Active Directory integration, and Distributed File System (DFS). This means that the company can continue to use their on-premises Active Directory to manage user access to files.","upvote_count":"4","timestamp":"1677129000.0","poster":"LuckyAro"},{"comment_id":"818479","upvote_count":"2","content":"B is the correct answer","poster":"Bilalazure","timestamp":"1677102660.0"},{"upvote_count":"2","content":"Answer - B\n1- https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\n2- https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html","timestamp":"1676807820.0","poster":"jennyka76","comment_id":"814032"},{"timestamp":"1676724360.0","content":"B \nAmazon FSx for Windows File Server file system","poster":"Neha999","upvote_count":"3","comment_id":"812950"}],"unix_timestamp":1676724360,"question_id":280}],"exam":{"isBeta":false,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isMCOnly":true,"isImplemented":true,"provider":"Amazon","id":31,"lastUpdated":"11 Apr 2025"},"currentPage":56},"__N_SSP":true}