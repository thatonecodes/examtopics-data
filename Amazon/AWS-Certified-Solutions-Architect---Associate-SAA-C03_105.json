{"pageProps":{"questions":[{"id":"yOKvdihK2ZEKkDGVOugy","unix_timestamp":1665676800,"question_text":"A solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases.\nWhich solution will meet these requirements?","discussion":[{"comment_id":"694071","poster":"Sinaneos","timestamp":"1665676800.0","content":"Selected Answer: C\nA: doesn't fully configure the traffic flow\nB: security groups don't have deny rules\nD: peering is mostly between VPCs, doesn't really help here\n\nanswer is C, most mainstream way","upvote_count":"53"},{"comment_id":"826283","comments":[{"upvote_count":"5","poster":"orhan64","comment_id":"964374","timestamp":"1690435740.0","content":"Hey bro, did you buy premium access?"}],"timestamp":"1677706560.0","poster":"Gary_Phillips_2007","upvote_count":"34","content":"Just took the exam today and EVERY ONE of the questions came from this dump. Memorize it all. Good luck."},{"poster":"PaulGa","upvote_count":"2","content":"Selected Answer: C\nAns C - only one that makes sense: \"...a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets\" - operative word: \"allows\"","timestamp":"1726056480.0","comment_id":"1282077"},{"poster":"awsgeek75","comment_id":"1122586","timestamp":"1705241640.0","content":"Selected Answer: C\nA: route table that connect... no idea what this option is trying to do but won't work for RDS\nB: SG are deny by default\nD: Peering connection between subnets? No idea what this is but happy to learn if such a thing exists. \n\nC: SG to allow input to private subnet means everything else will be blocked. Attaching this SG to DB instance means it will block everything except the private subnet instances which is where the required EC2 instances are.","upvote_count":"5"},{"poster":"AWSStudyBuddy","upvote_count":"2","comment_id":"1048799","timestamp":"1697806440.0","content":"Selected Answer: C\nRDS databases can only be accessed by EC2 instances located in private subnets: From the security group given to instances in the private subnets, the DB instances' security group will permit incoming traffic. Because of this, the RDS databases will only be accessible by EC2 instances located on the private subnets.\nBecause of its safe architecture, Every other source of incoming traffic will be blocked by the security group that is linked to the database instances. The RDS databases will be better shielded from unwanted access thanks to this."},{"comment_id":"976844","upvote_count":"3","timestamp":"1691599020.0","content":"Selected Answer: C\nThe key reasons are:\n\nUsing security groups to control access between resources is a standard practice in VPCs.\nThe security group attached to the RDS DB instances can allow inbound traffic from the security group for the EC2 instances in the private subnets.\nThis allows only those EC2 instances in the private subnets to connect to the databases, meeting the requirements.\nRoute tables, peering connections, and denying public subnet access would not achieve the needed selectivity of allowing only the private subnet EC2 instances.\nSecurity groups provide stateful filtering at the instance level for precise access control.","poster":"Guru4Cloud"},{"upvote_count":"3","content":"Selected Answer: C\nSecurity groups only have allow rules","comment_id":"975187","timestamp":"1691469180.0","poster":"TariqKipkemei"},{"timestamp":"1690991580.0","content":"Selected Answer: C\noptoin C","comment_id":"970338","poster":"praveenvky83","upvote_count":"1"},{"poster":"miki111","comment_id":"956757","timestamp":"1689780060.0","content":"Option C is the correct answer","upvote_count":"1"},{"content":"Selected Answer: C\nCreating security group that allows inbound traffic from security group assigned to instances in private subnets ensures that only EC2 running in private subnets can access the RDS databases. By associating security group with DB, you restrict access to only instances that belong to designated security group.\n\nA: This approach may help control routing within VPC, it does not address the specific access requirement between EC2 instances and RDS databases.\n\nB: Using a deny rule in a security group can lead to complexities and potential misconfigurations. It is generally recommended to use allow rules to explicitly define access permissions.\n\nD: Peering connections enable communication between different VPCs or VPCs in different regions, and they are not necessary for restricting access between subnets within the same VPC.","comment_id":"929388","poster":"cookieMr","upvote_count":"5","timestamp":"1687345800.0"},{"content":"Selected Answer: C\nOption C meets the requirements.","timestamp":"1685965320.0","comment_id":"915398","upvote_count":"1","poster":"Bmarodi"},{"timestamp":"1684398660.0","upvote_count":"2","comment_id":"900924","poster":"Abrar2022","content":"By default, a security group is set up with rules that deny all inbound traffic and permit all outbound traffic."},{"comment_id":"886078","content":"Selected Answer: C\nCCCCCCCCCCC","timestamp":"1682935680.0","upvote_count":"1","poster":"water314"},{"content":"Selected Answer: C\nCreate a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances. This will allow the EC2 instances in the private subnets to have access to the RDS databases while denying access to the EC2 instances in the public subnets.","comment_id":"768200","poster":"SilentMilli","upvote_count":"2","timestamp":"1673056500.0"},{"comment_id":"751198","comments":[{"upvote_count":"1","content":"Option A, creating a new route table that excludes the route to the public subnets' CIDR blocks and associating it with the database subnets, would not meet the requirements because it would block all traffic to the database subnets, not just traffic from the public subnets.\n\nOption B, creating a security group that denies inbound traffic from the security group assigned to instances in the public subnets and attaching it to the DB instances, would not meet the requirements because it would allow all traffic from the private subnets to reach the DB instances, not just traffic from the security group assigned to instances in the private subnets.\n\nOption D, creating a new peering connection between the public subnets and the private subnets and a different peering connection between the private subnets and the database subnets, would not meet the requirements because it would allow all traffic from the private subnets to reach the DB instances, not just traffic from the security group assigned to instances in the private subnets.","timestamp":"1671553920.0","comment_id":"751199","poster":"Buruguduystunstugudunstuy"}],"poster":"Buruguduystunstugudunstuy","upvote_count":"3","timestamp":"1671553860.0","content":"Selected Answer: C\nThe solution that meets the requirements described in the question is option C: Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.\n\nIn this solution, the security group applied to the DB instances allows inbound traffic from the security group assigned to instances in the private subnets. This ensures that only EC2 instances running in the private subnets can have access to the RDS databases."},{"poster":"Nandan747","upvote_count":"4","comment_id":"749740","timestamp":"1671447360.0","content":"Selected Answer: C\nThe real trick is between B and C. A and D are ruled out for obvious reasons.\nB is wrong as you cannot have deny type rules in Security groups.\nSo- C is the right answer."},{"content":"Selected Answer: C\nThe key is \"Only EC2 instances that run in the private subnets can have access to the RDS databases\"\nThe answer is C.","poster":"ashish_t","timestamp":"1669396020.0","comment_id":"726973","upvote_count":"2"},{"timestamp":"1669039200.0","poster":"Wpcorgan","content":"C is correct","comment_id":"723572","upvote_count":"1"},{"upvote_count":"2","poster":"17Master","timestamp":"1667494980.0","content":"Selected Answer: C\nAns correct.","comment_id":"710668"},{"comment_id":"696061","poster":"KVK16","upvote_count":"6","content":"Selected Answer: C\nInside a VPC, traffic locally between different subnets cannot be restricted by routing but incase they are in different VPCs then it would be possible. This is imp Gain in VPC\n- So only method is Security Groups - like EC2 also RDS also has Security Groups to restrict traffic to database instances","timestamp":"1665905880.0"}],"answer_description":"","exam_id":31,"topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/85409-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"answer_ET":"C","answers_community":["C (100%)"],"question_id":521,"timestamp":"2022-10-13 18:00:00","answer":"C","answer_images":[],"choices":{"D":"Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets.","A":"Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.","B":"Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.","C":"Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances."}},{"id":"ATnQI23toajaQTFplbj2","exam_id":31,"discussion":[{"timestamp":"1708545900.0","content":"Selected Answer: BD\nTo decrypt environment variables encrypted with AWS KMS, Lambda needs to be granted permissions to call KMS APIs. This is done in two places:\n\nThe Lambda execution role needs kms:Decrypt and kms:GenerateDataKey permissions added. The execution role governs what AWS services the function code can access.\nThe KMS key policy needs to allow the Lambda execution role to have kms:Decrypt and kms:GenerateDataKey permissions for that specific key. This allows the execution role to use that particular key.","poster":"Guru4Cloud","comment_id":"986813","upvote_count":"7"},{"timestamp":"1730495220.0","comment_id":"1205201","poster":"wizcloudifa","content":"Selected Answer: BD\nAs per the principle of least privilege, granting permissions = role level","upvote_count":"5"},{"comment_id":"1073100","upvote_count":"3","timestamp":"1715926500.0","poster":"TariqKipkemei","content":"Selected Answer: BD\nAllow the Lambda execution role in the AWS KMS key policy then add AWS KMS permissions in the role."},{"upvote_count":"3","timestamp":"1709405460.0","poster":"ssa03","content":"Selected Answer: BD\nCorrect Answer: BD","comment_id":"997012"},{"content":"its B & D","comment_id":"975128","upvote_count":"2","timestamp":"1707368580.0","poster":"Nirav1112"},{"comment_id":"972590","timestamp":"1707100140.0","poster":"mrsoa","upvote_count":"2","content":"Selected Answer: BD\nBD BD BD BD"},{"poster":"Deepakin96","content":"Selected Answer: BD\nIts B and D","comment_id":"970915","timestamp":"1706957760.0","upvote_count":"2"},{"timestamp":"1706793060.0","comment_id":"968989","poster":"Bmaster","upvote_count":"2","content":"My choice is B,D"}],"answer":"BD","url":"https://www.examtopics.com/discussions/amazon/view/116979-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"D":"Allow the Lambda execution role in the AWS KMS key policy.","E":"Allow the Lambda resource policy in the AWS KMS key policy.","C":"Add AWS KMS permissions in the Lambda function policy.","B":"Add AWS KMS permissions in the Lambda execution role.","A":"Add AWS KMS permissions in the Lambda resource policy."},"unix_timestamp":1690888260,"question_images":[],"answer_ET":"BD","answer_images":[],"answer_description":"","question_text":"A company is using AWS Key Management Service (AWS KMS) keys to encrypt AWS Lambda environment variables. A solutions architect needs to ensure that the required permissions are in place to decrypt and use the environment variables.\n\nWhich steps must the solutions architect take to implement the correct permissions? (Choose two.)","timestamp":"2023-08-01 13:11:00","question_id":522,"answers_community":["BD (100%)"],"isMC":true,"topic":"1"},{"id":"jNS4Re1Ljwypq5mXJxTX","answer_description":"","answer":"A","unix_timestamp":1690806300,"answers_community":["A (74%)","C (25%)","2%"],"question_text":"A company has a financial application that produces reports. The reports average 50 KB in size and are stored in Amazon S3. The reports are frequently accessed during the first week after production and must be stored for several years. The reports must be retrievable within 6 hours.\n\nWhich solution meets these requirements MOST cost-effectively?","question_images":[],"topic":"1","answer_images":[],"question_id":523,"url":"https://www.examtopics.com/discussions/amazon/view/116896-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-07-31 14:25:00","choices":{"A":"Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier after 7 days.","C":"Use S3 Intelligent-Tiering. Configure S3 Intelligent-Tiering to transition the reports to S3 Standard-Infrequent Access (S3 Standard-IA) and S3 Glacier.","D":"Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier Deep Archive after 7 days.","B":"Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days."},"discussion":[{"poster":"zjcorpuz","timestamp":"1691154960.0","content":"Answer is A\nAmazon S3 Glacier:\nExpedited Retrieval: Provides access to data within 1-5 minutes.\nStandard Retrieval: Provides access to data within 3-5 hours.\nBulk Retrieval: Provides access to data within 5-12 hours.\nAmazon S3 Glacier Deep Archive:\nStandard Retrieval: Provides access to data within 12 hours.\nBulk Retrieval: Provides access to data within 48 hours.","upvote_count":"25","comment_id":"972183"},{"comments":[{"timestamp":"1733635020.0","upvote_count":"2","content":"Objects must be stored for at least 30 days before transitioning to S3 Standard-IA or S3 One Zone-IA.\n\nOption A is to S3 Glacier. A is correct.","poster":"LeonSauveterre","comment_id":"1323370"},{"upvote_count":"2","timestamp":"1726383720.0","content":"After reading the document from your link, it's clear that NONE of the restrictions apply to S3 standard, but to S3 Standard IA.\n\nA is the correct answer.","poster":"MatAlves","comment_id":"1283944"},{"content":"This is worth noticing! Glad I came across your comment 1 day before my test.","comments":[{"upvote_count":"2","poster":"Marco_St","timestamp":"1704890520.0","content":"so Could I ask is A or C for this question? I voted for A but it seems you had the same question in the exam and it was C? Thanks! I will attend the exam soon.","comment_id":"1118547"}],"poster":"Hades2231","upvote_count":"4","comment_id":"993718","timestamp":"1693372440.0"},{"poster":"franbarberan","timestamp":"1695736020.0","content":"the 7 days limitation is only if you want to move from s3 standart to S3 Standard-IA or S3 One Zone-IA, if you move to s3 glacier dont have this limitation, correct answer is A","upvote_count":"14","comment_id":"1017821"}],"comment_id":"988667","content":"Selected Answer: C\nAll the \"....after 7 days\" options are wrong.\nBefore you transition objects to S3 Standard-IA or S3 One Zone-IA, you must store them for at least 30 days in Amazon S3\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html#:~:text=Minimum%20Days%20for%20Transition%20to%20S3%20Standard%2DIA%20or%20S3%20One%20Zone%2DIA","poster":"oayoade","upvote_count":"11","timestamp":"1692823020.0"},{"comment_id":"1322507","timestamp":"1733428560.0","upvote_count":"1","poster":"rosanna","content":"Selected Answer: A\nthere's a period before you can transition to any storage class (down the hierarchy) \"except for glacier of all types\" a minimum of 30 days. \nHowever, when transitioning to Glacier directly there's no minimum period to wait so you can choose \"A\" safely"},{"comment_id":"1261780","upvote_count":"3","content":"Selected Answer: A\nIts A ya bunch of nerds","poster":"1e22522","timestamp":"1722966000.0"},{"comment_id":"1237800","upvote_count":"1","timestamp":"1719448140.0","poster":"Gape4","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"},{"poster":"Linuslin","comment_id":"1208717","content":"Selected Answer: A\nC is incorrect.\nUnsupported lifecycle transitions\nAmazon S3 does not support any of the following lifecycle transitions.\nYou can't transition from the following:\n\nAny storage class to the S3 Standard storage class.\nAny storage class to the Reduced Redundancy Storage (RRS) class.\nThe S3 Intelligent-Tiering storage class to the S3 Standard-IA storage class.\nThe S3 One Zone-IA storage class to the S3 Intelligent-Tiering, S3 Standard-IA, or S3 Glacier Instant Retrieval storage classes.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html","timestamp":"1715235420.0","upvote_count":"4"},{"timestamp":"1704997140.0","comment_id":"1120053","upvote_count":"4","content":"Selected Answer: A\nBC are lifecycle with tiering and infrequent access which are not required here.\nD is deep archive and can take hours to retrieve so it is not suitable\nA is cheapest workable option","poster":"awsgeek75"},{"timestamp":"1704890820.0","content":"Selected Answer: A\nfrequent access pattern- Standard.","upvote_count":"2","poster":"Marco_St","comment_id":"1118550"},{"timestamp":"1704173640.0","upvote_count":"4","poster":"pentium75","comment_id":"1111615","content":"Selected Answer: A\nNot B - More expensive than A\nNot C - Intelligent-Tiering moves only objects of at least 128 KB\nNot D - Glacier Deep Archive takes more than 6 hours to retrieve"},{"poster":"TariqKipkemei","timestamp":"1700463840.0","content":"Selected Answer: A\nAny option with S3 Intelligent-Tiering is out, this is only required when the access patterns are unknown. \nFrom the question the access patterns are well known, enough to tie the frequently accessed reports to S3 standard and transition them to S3 glacier after 7days.","upvote_count":"4","comment_id":"1075216"},{"poster":"iwannabeawsgod","content":"Selected Answer: A\nits A for me","upvote_count":"3","comment_id":"1046508","timestamp":"1697598060.0"},{"upvote_count":"2","comment_id":"1027604","poster":"Carlos_O","content":"Selected Answer: A\nTiene mas sentido","timestamp":"1696709160.0"},{"comment_id":"1027297","content":"Selected Answer: A\nOption A\nAmazon S3 Glacier Standard Retrieval: Provides access to data within 3-5 hours.","timestamp":"1696679400.0","poster":"sl2man","upvote_count":"4"},{"content":"Selected Answer: A\nmost cost effective has to be glacier so A \nWith C it is using intelligence tiering which is 30 days minimum from what I have read, I may be wrong on how I read that.","comment_id":"1018726","upvote_count":"2","poster":"Ramdi1","timestamp":"1695812460.0"},{"upvote_count":"2","timestamp":"1695146880.0","poster":"tabbyDolly","content":"answer A\nfrequent access during the first week -> keeps data in s3 standard for 7 days\nstored for several year and retrievable within 6 hours -> can be moved to s3 glacier for data archive purpose","comment_id":"1011608"},{"timestamp":"1694274060.0","content":"Selected Answer: A\nIts A. Data cannot be transitioned from Intelligent Tiering to Standard IA \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html","comment_id":"1003345","upvote_count":"4","poster":"anikety123"},{"comment_id":"1002609","upvote_count":"2","content":"Selected Answer: C\nCheck Oayoade comment, before transition, 30 days in S3 the files have to be, young padawans","poster":"Mll1975","timestamp":"1694189940.0"},{"poster":"ssa03","timestamp":"1693673700.0","upvote_count":"1","content":"Selected Answer: C\nCorrect Answer: C","comment_id":"997016"},{"poster":"ersin13","content":"I agree with zjcorpuz the answer is A","comment_id":"975153","timestamp":"1691467020.0","upvote_count":"2"},{"content":"Selected Answer: A\nOption A","timestamp":"1691163540.0","comment_id":"972292","poster":"D10SJoker","upvote_count":"4"},{"timestamp":"1691069760.0","poster":"mrsoa","upvote_count":"4","content":"Selected Answer: A\nFor me its A because S3 glacier Flexible retrieval standard can retrieve files in 3 to 5 hours \n\n\nD is incorrect because S3 glacier deep archive needs 12 hours minimum to retrieve files \n\nB and C are more expensive comparing to A and D","comment_id":"971153"},{"comments":[{"timestamp":"1692971520.0","poster":"PLN6302","comment_id":"990125","comments":[{"comment_id":"1015980","content":"The Amazon S3 Glacier Deep Archive storage class provides two retrieval options ranging from 12-48 hours.","upvote_count":"2","timestamp":"1695571620.0","poster":"darekw"}],"content":"I think option D also.because we have to retrieve the data within 6 hours that can be possible with S3 glacier deep archive","upvote_count":"2"}],"comment_id":"970399","poster":"RazSteel","content":"Selected Answer: D\nFor me its D coz the size of files are 50kb","timestamp":"1690995420.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1690806300.0","poster":"Josantru","comment_id":"968096","content":"Correct C. \nis halting the storage of data for a number of years"}],"isMC":true,"answer_ET":"A","exam_id":31},{"id":"4bad91oVnJyDZXCbBtLH","exam_id":31,"discussion":[{"content":"Selected Answer: B\nThe key considerations are:\n\nThe company needs flexibility to change EC2 instance types and families every 2-3 months. This rules out Reserved Instances which lock you into an instance type and family for 1-3 years.\nA Compute Savings Plan allows switching instance types and families freely within the term as needed. No Upfront is more flexible than All Upfront.\nA 1-year term balances commitment and flexibility better than a 3-year term given the company's changing needs.\nWith No Upfront, the company only pays for usage monthly without an upfront payment. This optimizes cost.","poster":"Guru4Cloud","comment_id":"986798","timestamp":"1724262180.0","upvote_count":"10"},{"poster":"Kiki_Pass","content":"Selected Answer: B\n\"EC2 Instance Savings Plans give you the flexibility to change your usage between instances WITHIN a family in that region. \"\nhttps://aws.amazon.com/savingsplans/compute-pricing/","comment_id":"972830","upvote_count":"5","timestamp":"1722849420.0"},{"timestamp":"1732086360.0","poster":"TariqKipkemei","content":"Selected Answer: B\nOnly Compute Savings Plan allows you to change instance family.","comment_id":"1075218","upvote_count":"4"},{"comment_id":"979228","content":"Selected Answer: B\n\" needs to change the type and family of its EC2 instances\". that means B I think.","poster":"avkya","timestamp":"1723449480.0","upvote_count":"3"},{"comment_id":"971159","poster":"mrsoa","timestamp":"1722692280.0","upvote_count":"3","content":"Selected Answer: B\nB is the right answer"},{"timestamp":"1722512520.0","upvote_count":"2","comment_id":"969020","content":"B is correct..\n'EC2 Instance Savings Plans' can't change 'family'.","poster":"Bmaster"},{"poster":"Josantru","comment_id":"968098","content":"Correct B. \nTo change 'Family' always Compute saving plan, right?","upvote_count":"4","timestamp":"1722428880.0"}],"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/116897-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Purchase All Upfront Reserved Instances for a 1-year term.","D":"Purchase an All Upfront EC2 Instance Savings Plan for a 1-year term.","A":"Purchase Partial Upfront Reserved Instances for a 3-year term.","B":"Purchase a No Upfront Compute Savings Plan for a 1-year term."},"unix_timestamp":1690806480,"question_images":[],"answer_images":[],"answer_ET":"B","answer_description":"","question_text":"A company needs to optimize the cost of its Amazon EC2 instances. The company also needs to change the type and family of its EC2 instances every 2-3 months.\n\nWhat should the company do to meet these requirements?","timestamp":"2023-07-31 14:28:00","question_id":524,"answers_community":["B (100%)"],"isMC":true,"topic":"1"},{"id":"8ve1nH12r1zhHdqWdgiJ","question_id":525,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/117206-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"choices":{"C":"Configure Amazon Inspector to analyze the data that is in Amazon S3.","A":"Configure Amazon Macie in each Region. Create a job to analyze the data that is in Amazon S3.","B":"Configure AWS Security Hub for all Regions. Create an AWS Config rule to analyze the data that is in Amazon S3.","D":"Configure Amazon GuardDuty to analyze the data that is in Amazon S3."},"discussion":[{"timestamp":"1708544220.0","poster":"Guru4Cloud","comment_id":"986796","upvote_count":"6","content":"Selected Answer: A\nThe key reasons are:\n\nAmazon Macie is designed specifically for discovering and classifying sensitive data like PII in S3. This makes it the optimal service to use.\nMacie can be enabled directly in the required Regions rather than enabling it across all Regions which is unnecessary. This minimizes overhead.\nMacie can be set up to automatically scan the specified S3 buckets on a schedule. No need to create separate jobs.\nSecurity Hub is for security monitoring across AWS accounts, not specific for PII discovery. More overhead than needed.\nInspector and GuardDuty are not built for PII discovery in S3 buckets. They provide broader security capabilities."},{"comment_id":"1120057","content":"Selected Answer: A\nPII = Macie\nSecurity Hub: Organisation security and logging not for PII\nInspector: Infra vulnerability management\nGuardDuty: Network protection","timestamp":"1720715220.0","upvote_count":"4","poster":"awsgeek75"},{"content":"Selected Answer: A\nAmazon Macie = PII","poster":"TariqKipkemei","timestamp":"1716181680.0","comment_id":"1075220","upvote_count":"2"},{"content":"Selected Answer: A\nAWS Macie = PII detection","poster":"mrsoa","upvote_count":"4","comment_id":"971160","timestamp":"1706974800.0"},{"poster":"Deepakin96","timestamp":"1706958000.0","comment_id":"970920","content":"Selected Answer: A\nAmazon Macie will identify all PII","upvote_count":"3"}],"answer":"A","answer_description":"","question_text":"A solutions architect needs to review a company's Amazon S3 buckets to discover personally identifiable information (PII). The company stores the PII data in the us-east-1 Region and us-west-2 Region.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"A","answers_community":["A (100%)"],"exam_id":31,"timestamp":"2023-08-03 11:00:00","answer_images":[],"unix_timestamp":1691053200,"isMC":true}],"exam":{"name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"id":31,"lastUpdated":"11 Apr 2025","isMCOnly":true,"isBeta":false,"provider":"Amazon","numberOfQuestions":1019},"currentPage":105},"__N_SSP":true}