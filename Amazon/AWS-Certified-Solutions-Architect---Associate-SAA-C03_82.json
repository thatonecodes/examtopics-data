{"pageProps":{"questions":[{"id":"hsyiU3u7xzlYKka9TXVH","isMC":true,"timestamp":"2023-05-16 14:27:00","choices":{"C":"Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.","B":"Turn on S3 Object Lock with governance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.","A":"Turn on the S3 Versioning feature for the S3 bucket. Configure S3 Lifecycle to delete the data after 7 years. Configure multi-factor authentication (MFA) delete for all S3 objects.","D":"Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Use S3 Batch Operations to bring the existing data into compliance."},"unix_timestamp":1684240020,"answer_images":[],"question_id":406,"topic":"1","answer_description":"","discussion":[{"poster":"omarshaban","comment_id":"1124540","timestamp":"1705443780.0","comments":[{"upvote_count":"4","timestamp":"1705620240.0","content":"Did you pass?","poster":"awsgeek75","comment_id":"1126281"}],"content":"THIS WAS IN MY EXAM","upvote_count":"10"},{"timestamp":"1704965640.0","poster":"iapps369","content":"D\nas S3 batch operations reduce risk and manual copy/paste overhead.","comment_id":"1119554","upvote_count":"6"},{"poster":"Lin878","timestamp":"1718549940.0","comment_id":"1231405","upvote_count":"3","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html"},{"comments":[{"comment_id":"1115921","content":"With option C, you have to copy the object for it to be complaint and then delete the original as only the new copy will be compliant. So D is the only option","timestamp":"1704638580.0","poster":"awsgeek75","upvote_count":"2"}],"comment_id":"1115919","poster":"awsgeek75","upvote_count":"5","content":"Selected Answer: D\nA: Versioning, not relevant\nB: Governance, it won't enforce object lock \nC: Recopy existing objects may work but lots of operational overhead (see link)\nD: Compliance on existing objects with batch operations is least operational overhead\n\nhttps://repost.aws/questions/QUGKrl8XRLTEeuIzUHq0Ikew/s3-object-lock-on-existing-s3-objects","timestamp":"1704638460.0"},{"comment_id":"1112424","content":"Selected Answer: A\nTo enable Object Lock on an Amazon S3 bucket, you must first enable versioning on that bucket. other 3 option did not enable versioning first","upvote_count":"2","timestamp":"1704248160.0","poster":"mr123dd"},{"content":"Selected Answer: D\nRecopying offers more control but requires users to manage the process. S3 Batch Operations automates the process at scale but with less granular control - LEAST operational overhead","timestamp":"1702487460.0","upvote_count":"3","comment_id":"1095678","poster":"fb4afde"},{"upvote_count":"2","comment_id":"1069871","content":"Its C because you only need to recopy all existing objects one time, so why use S3 batch operations if new datas going to be in compliance retention mode? I can see why its C although my initial gut answer was D.","comments":[{"upvote_count":"3","timestamp":"1704029880.0","content":"What if I don't have the original files anymore? Where should I copy them from?","comment_id":"1110594","poster":"pentium75"}],"timestamp":"1699922880.0","poster":"moonster"},{"timestamp":"1694777580.0","upvote_count":"2","comment_id":"1008415","poster":"kwang312","comments":[{"comment_id":"1110597","content":"You need a token from AWS Support, but you CAN enable Object Lock for an existing bucket.","timestamp":"1704030120.0","upvote_count":"3","poster":"pentium75"}],"content":"You can only enable Object Lock for new buckets. If you want to turn on Object Lock for an existing bucket, contact AWS Support."},{"comment_id":"989361","upvote_count":"2","content":"Selected Answer: D\nTurn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Use S3 Batch Operations to bring the existing data into compliance.","timestamp":"1692897300.0","poster":"Guru4Cloud"},{"timestamp":"1687021320.0","poster":"MrAWSAssociate","upvote_count":"2","comment_id":"926147","content":"Selected Answer: D\nTo replicate existing object/data in S3 Bucket to bring them to compliance, optionally we use \"S3 Batch Replication\", so option D is the most appropriate, especially if we have big data in S3."},{"comment_id":"923670","content":"Selected Answer: D\nFor minimum ops D is best","upvote_count":"2","timestamp":"1686799500.0","poster":"TariqKipkemei"},{"comment_id":"915240","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html","poster":"DrWatson","timestamp":"1685954820.0","upvote_count":"5"},{"comment_id":"915189","poster":"antropaws","comments":[{"comment_id":"1110596","poster":"pentium75","timestamp":"1704029940.0","upvote_count":"2","content":"And gathering all the files for copying them again does not?"}],"timestamp":"1685951160.0","upvote_count":"3","content":"Selected Answer: C\nBatch operations will add operational overhead."},{"content":"Use Object Lock in Compliance mode. Then Use Batch operation. \nWRONG>>manual work and not automated>>>Recopy all existing objects to bring the existing data into compliance.","timestamp":"1685860320.0","comment_id":"914205","upvote_count":"1","comments":[{"content":"Batch IS automated. You just need to create the batch which is a one-time operation.\n\"Recopy all existing objects\" is not operational overhead?","timestamp":"1704030240.0","comment_id":"1110601","poster":"pentium75","upvote_count":"2"}],"poster":"Abrar2022"},{"poster":"omoakin","content":"C\nWhen an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened. Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period.","comment_id":"901491","upvote_count":"3","timestamp":"1684442760.0","comments":[{"poster":"omoakin","upvote_count":"2","comment_id":"901493","content":"error i meant to type D\ni wont do recopy","timestamp":"1684442880.0"},{"poster":"lucdt4","content":"No, D for me because the requirement is LEAST operational overhead\nSo RECOPy .......... is the manual operation -> C is wrong\nD is correct","comment_id":"907075","timestamp":"1685078760.0","upvote_count":"3"}]},{"content":"Recopying vs. S3 Batch Operations: In Option C, the recommendation is to recopy all existing objects to ensure they have the appropriate retention settings. This can be done using simple S3 copy operations. On the other hand, Option D suggests using S3 Batch Operations, which is a more advanced feature and may require additional configuration and management. S3 Batch Operations can be beneficial if you have a massive number of objects and need to perform complex operations, but it might introduce more overhead for this specific use case.\n\nOperational complexity: Option C has a straightforward process of recopying existing objects. It is a well-known operation in S3 and doesn't require additional setup or management. Option D introduces the need to set up and configure S3 Batch Operations, which can involve creating job definitions, specifying job parameters, and monitoring the progress of batch operations. This additional complexity may increase the operational overhead.","comment_id":"901204","timestamp":"1684413420.0","upvote_count":"2","poster":"cloudenthusiast"},{"content":"Selected Answer: D\nYou need AWS Batch to re-apply certain config to files that were already in S3, like encryption","poster":"Efren","comment_id":"899932","timestamp":"1684316160.0","upvote_count":"5"},{"poster":"nosense","comments":[{"upvote_count":"2","content":"So does C.","timestamp":"1704030180.0","comment_id":"1110600","poster":"pentium75"}],"upvote_count":"3","content":"Selected Answer: D\nD for me, bcs no sense to recopy all data","comment_id":"899176","timestamp":"1684240020.0"}],"answers_community":["D (86%)","9%"],"question_text":"A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal requirement to retain all new and existing data in Amazon S3 for 7 years.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/109404-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"D","question_images":[],"answer_ET":"D"},{"id":"BfTcvxgCwumaRUci5BQZ","answer_description":"","choices":{"D":"Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.","A":"Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.","C":"Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.","B":"Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route traffic."},"unix_timestamp":1684240260,"answer_ET":"A","exam_id":31,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/109405-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","answer_images":[],"timestamp":"2023-05-16 14:31:00","question_id":407,"answers_community":["A (89%)","11%"],"question_text":"A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities.\n\nWhat should a solutions architect do to route traffic to multiple Regions?","discussion":[{"poster":"TariqKipkemei","content":"Selected Answer: A\nGlobal, Reduce latency, health checks, no failover = Amazon CloudFront\nGlobal ,Reduce latency, health checks, failover, Route traffic = Amazon Route 53\noption A has more weight.","upvote_count":"31","comment_id":"923678","comments":[{"timestamp":"1715329440.0","upvote_count":"3","comment_id":"1209274","poster":"ManikRoy","content":"Cloud front does have failover capabilities.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html#:~:text=the%20secondary%20origin.-,Note,Choose%20Create%20origin%20group."},{"content":"nicley explained","upvote_count":"3","comment_id":"1045343","poster":"Anmol_1010","timestamp":"1697498460.0"}],"timestamp":"1686800280.0"},{"content":"Selected Answer: A\nA. I'm not an expert in this area, but I still want to express my opinion. After carefully reviewing the question and thinking about it for a long time, I actually don't know the reason. As I mentioned at the beginning, I'm not an expert in this field.","poster":"examtopictempacc","upvote_count":"19","comments":[{"timestamp":"1705620540.0","upvote_count":"3","poster":"awsgeek75","comment_id":"1126283","content":"All the explanation you need for this question and option A is in this article:\nhttps://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/"}],"timestamp":"1684777140.0","comment_id":"904264"},{"poster":"MatAlves","upvote_count":"3","comment_id":"1283036","timestamp":"1726208520.0","content":"Selected Answer: A\nCorrect me if I'm wrong but CloudFront DOES NOT have health check capabilities out of the box. Route 53 and Global Accelerator do."},{"upvote_count":"2","poster":"ChymKuBoy","content":"Selected Answer: A\nA for sure","timestamp":"1724667360.0","comment_id":"1272616"},{"comment_id":"1115930","comments":[],"upvote_count":"4","poster":"awsgeek75","timestamp":"1704639120.0","content":"Selected Answer: A\nB: Caching solution. Not ideal for failover although it will work. Would have been a correct answer if A wasn't an option\nC: Transit gateway is for VPC connectivity not AWS API or Lambda\nD: Even if it was possible, there is a primary region dependency of ALB\nA: correct because R53 health checks can failover across regions\n\nGood explanation here:\nhttps://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/"},{"comment_id":"1113336","poster":"tosuccess","upvote_count":"2","timestamp":"1704342480.0","content":"Selected Answer: B\nwe can set primary and secondry regions in cloud front for failover."},{"poster":"pentium75","comment_id":"1110604","upvote_count":"3","content":"Selected Answer: A\nApplication is serverless, it doesn't matter where it runs, so can be active-active setup and run wherever the request comes in. Route 53 with health checks will route to a healthy region.\n\nB, could work too, but CloudFront is for caching which does not seem to help with an API. The goal here is \"failover capabilities\", not caching/performance/latency etc.","timestamp":"1704030600.0"},{"content":"Selected Answer: A\nIn activ active failover config, route53 continuously monitors its endpoints and if one of them is unhealthy, it excludes the region/endpoint from its valid traffic route - Only Sensible option\nCloudfront is a content delivery network - not used to route traffic\nTransit gateway for traffic routing - aws devs will hit us with a stick on hearing this option\nYou cant use a load balancer for cross region load balancing - invalid","upvote_count":"2","comment_id":"1073776","poster":"Goutham4981","timestamp":"1700277000.0"},{"content":"Selected Answer: A\nGlobal ,Reduce latency, health checks, failover, Route traffic = Amazon Route 53","timestamp":"1699210020.0","poster":"potomac","comment_id":"1063180","upvote_count":"2"},{"comment_id":"1042141","upvote_count":"1","content":"\"What the?\" yeah I know right","timestamp":"1697156700.0","poster":"youdelin"},{"comments":[{"poster":"deechean","comment_id":"995752","content":"its not static content, actually they deployed a API Gateway backed by lambda","timestamp":"1693549140.0","upvote_count":"3"}],"content":"Selected Answer: B\n\"Stateless applications provide one service or function and use content delivery network (CDN), web, or print servers to process these short-term requests.\nhttps://docs.aws.amazon.com/architecture-diagrams/latest/multi-region-api-gateway-with-cloudfront/multi-region-api-gateway-with-cloudfront.html","comment_id":"993179","timestamp":"1693319340.0","poster":"jrestrepob","upvote_count":"1"},{"poster":"MrAWSAssociate","comment_id":"926164","timestamp":"1687022580.0","content":"Selected Answer: A\nA option does make sense.","upvote_count":"2"},{"timestamp":"1686859260.0","comment_id":"924557","poster":"Sangsation","upvote_count":"2","comments":[{"comment_id":"1110603","timestamp":"1704030480.0","upvote_count":"1","content":"Option A does not speak of Route 53 failover routing policies.","poster":"pentium75"}],"content":"Selected Answer: B\nBy creating an Amazon CloudFront distribution with origins in each AWS Region where the application is deployed, you can leverage CloudFront's global edge network to route traffic to the closest available Region. CloudFront will automatically route the traffic based on the client's location and the health of the origins using CloudFront health checks.\n\nOption A (creating Amazon Route 53 health checks with an active-active failover configuration) is not suitable for this scenario as it is primarily used for failover between different endpoints within the same Region, rather than routing traffic to different Regions."},{"poster":"Axeashes","comment_id":"922733","comments":[{"poster":"Gooniegoogoo","upvote_count":"1","timestamp":"1688137380.0","comment_id":"939162","content":"that is from 2017.. i wonder if it is still relevant.."}],"content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/","upvote_count":"4","timestamp":"1686715020.0"},{"comment_id":"915253","timestamp":"1685955720.0","upvote_count":"2","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html","poster":"DrWatson"},{"upvote_count":"1","poster":"antropaws","comment_id":"915194","timestamp":"1685951520.0","content":"Selected Answer: A\nI understand that you can use Route 53 to provide regional failover."},{"timestamp":"1685903100.0","upvote_count":"3","comment_id":"914870","poster":"alexandercamachop","content":"Selected Answer: A\nTo route traffic to multiple AWS Regions and provide regional failover capabilities for a stateless web application running on AWS Lambda functions invoked by Amazon API Gateway, you can use Amazon Route 53 with an active-active failover configuration.\n\nBy creating Amazon Route 53 health checks for each Region and configuring an active-active failover configuration, Route 53 can monitor the health of the endpoints in each Region and route traffic to healthy endpoints. In the event of a failure in one Region, Route 53 automatically routes traffic to the healthy endpoints in other Regions.\n\nThis setup ensures high availability and failover capabilities for your web application across multiple AWS Regions."},{"timestamp":"1685865240.0","upvote_count":"2","comment_id":"914243","content":"I think it's A because the keyword is \"route\" traffic.","poster":"udo2020"},{"poster":"omoakin","content":"BBBBBBBBBBBBB","timestamp":"1685395440.0","comment_id":"909726","upvote_count":"1","comments":[{"poster":"karbob","upvote_count":"2","timestamp":"1685471700.0","comment_id":"910530","content":"CloudFront does not support health checks for routing traffic. is designed primarily for content distribution and caching, rather than for load balancing or traffic routing based on health checks."}]},{"poster":"Rob1L","comment_id":"902518","timestamp":"1684579500.0","content":"Selected Answer: A\nIt's A\nIt's not B because Amazon CloudFront can distribute traffic to multiple origins, but it does not support automatic failover between regions based on health checks. CloudFront is primarily a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds.","upvote_count":"5"},{"timestamp":"1684490580.0","poster":"y0","comment_id":"901827","content":"I agree with A - active-active failover means considering resources across all regions. So, in this case, to distribute traffic across all regions, Route 53 seems good. Cloudfront usage is more towards reducing latency for applications used globally by caching content at edge locations. It somehow does not fit the use case for distributing traffic. Also, not sure of the term \"cloudfront healthchecks\"","upvote_count":"2"},{"comment_id":"901496","content":"A \ncheck this out Qtn 3\nhttps://dumpsgate.com/wp-content/uploads/2021/01/SAA-C02.pdf","poster":"omoakin","timestamp":"1684443240.0","upvote_count":"2"},{"comment_id":"901207","content":"Selected Answer: B\nThis approach leverages the capabilities of CloudFront's intelligent routing and health checks to automatically distribute traffic across multiple AWS Regions and provide failover capabilities in case of Regional disruptions or unavailability.","upvote_count":"2","timestamp":"1684413600.0","poster":"cloudenthusiast"},{"content":"Selected Answer: B\nB, bcs a cant' provide regional failover","comments":[{"poster":"Efren","comment_id":"899934","content":"Agreed","upvote_count":"1","timestamp":"1684316340.0"}],"poster":"nosense","upvote_count":"3","timestamp":"1684240260.0","comment_id":"899178"}],"question_images":[],"answer":"A"},{"id":"BzXBeSrspr5yff3OJeMI","url":"https://www.examtopics.com/discussions/amazon/view/109499-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-05-17 10:44:00","question_text":"A company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications.\n\nWhat should a solutions architect do to mitigate any single point of failure in this architecture?","isMC":true,"choices":{"B":"Add a second virtual private gateway and attach it to the Management VPC.","D":"Add a second VPC peering connection between the Management VPC and the Production VPC.","A":"Add a set of VPNs between the Management and Production VPCs.","C":"Add a second set of VPNs to the Management VPC from a second customer gateway device."},"question_images":[],"answer_ET":"C","answer":"C","answers_community":["C (95%)","5%"],"answer_images":[],"unix_timestamp":1684313040,"exam_id":31,"discussion":[{"timestamp":"1724511660.0","content":"Selected Answer: C\nC is the correct option to mitigate the single point of failure.\n\nThe Management VPC currently has a single VPN connection through one customer gateway device. This is a single point of failure.\n\nAdding a second set of VPN connections from the Management VPC to a second customer gateway device provides redundancy and eliminates this single point of failure.","comments":[{"timestamp":"1724512320.0","comment_id":"989293","content":"As @Abrar2022 explains \n(production) VPN 1--------------> cgw 1\n(management) VPN 2--------------> cgw","upvote_count":"3","poster":"Guru4Cloud"}],"comment_id":"989284","poster":"Guru4Cloud","upvote_count":"7"},{"poster":"Abrar2022","comment_id":"914214","upvote_count":"6","content":"(production) VPN 1--------------> cgw 1 \n(management) VPN 2--------------> cgw 2","comments":[{"comment_id":"914216","poster":"Abrar2022","upvote_count":"2","content":"ANSWER IS C","timestamp":"1717483500.0"}],"timestamp":"1717483440.0"},{"timestamp":"1738969740.0","content":"Selected Answer: D\nbut what if the VPC peering connection fails? how The Management and Production vpc communicate?","poster":"Dantecito","upvote_count":"1","comment_id":"1353152"},{"timestamp":"1727711940.0","comment_id":"1021651","poster":"bsbs1234","content":"C,\n\n(production) --PrivateGateway-------->Direct Connect Gateway 1 ---> cgw 1 ---> DataCenter\n(production) -- PrivateGateway ------> Direct Connect Gateway 2 --->cgw 2 --> DataCenter\n(Management) -- > VPN ---- > (Direct Connect Gateway 1?) --- >cgw1 ---> dataCenter---> device in dataCenter","upvote_count":"2"},{"comment_id":"901497","content":"I agree to C","timestamp":"1716066120.0","upvote_count":"2","poster":"omoakin"},{"poster":"cloudenthusiast","content":"Selected Answer: C\noption D is not a valid solution for mitigating single points of failure in the architecture. I apologize for the confusion caused by the incorrect information.\n\nTo mitigate single points of failure in the architecture, you can consider implementing option C: adding a second set of VPNs to the Management VPC from a second customer gateway device. This will introduce redundancy at the VPN connection level for the Management VPC, ensuring that if one customer gateway or VPN connection fails, the other connection can still provide connectivity to the data center.","timestamp":"1716036300.0","upvote_count":"5","comment_id":"901211"},{"comment_id":"899968","upvote_count":"5","poster":"Efren","content":"Selected Answer: C\nRedundant VPN connections: Instead of relying on a single device in the data center, the Management VPC should have redundant VPN connections established through multiple customer gateways. This will ensure high availability and fault tolerance in case one of the VPN connections or customer gateways fails.","timestamp":"1715940300.0"},{"poster":"nosense","upvote_count":"3","content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/53908-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"899886","timestamp":"1715935440.0"}],"question_id":408,"topic":"1","answer_description":""},{"id":"XXtPZIwgRL9uBaqgWJjZ","timestamp":"2023-05-16 19:08:00","answer_description":"","answer":"B","question_text":"A company runs its application on an Oracle database. The company plans to quickly migrate to AWS because of limited resources for the database, backup administration, and data center maintenance. The application uses third-party database features that require privileged access.\n\nWhich solution will help the company migrate the database to AWS MOST cost-effectively?","question_images":[],"answer_images":[],"topic":"1","unix_timestamp":1684256880,"url":"https://www.examtopics.com/discussions/amazon/view/109432-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize the database settings to support third-party features.","D":"Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to remove dependency on Oracle APEX.","B":"Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to support third-party features.","A":"Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud services."},"isMC":true,"exam_id":31,"discussion":[{"content":"Selected Answer: B\nRDS Custom since it's related to 3rd vendor\nRDS Custom since it's related to 3rd vendor\nRDS Custom since it's related to 3rd vendor","poster":"Abrar2022","comment_id":"914220","upvote_count":"6","timestamp":"1701679680.0"},{"comments":[{"upvote_count":"2","poster":"awsgeek75","timestamp":"1720359780.0","content":"Action ignore the last line of my previous comment, A is not a valid option in any case as it suggest replacing 3rd party features with cloud services which is not possible without more details.","comment_id":"1115977"}],"content":"Selected Answer: B\nKey constraints: Limited resources for DB admin and cost. 3rd party db features with privileged access.\nA: Won't work due to 3rd party features \nC: AMI with Oracle may work but again overhead of backed, maintenance etc\nD: Too much overhead in rewrite\nB: Actually supports Oracle 3rd party features\nCaution: If this is only about APEX as suggested in option D, then A is also a possible answer: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.APEX.html","upvote_count":"3","poster":"awsgeek75","comment_id":"1115976","timestamp":"1720359660.0"},{"timestamp":"1719748440.0","upvote_count":"2","comment_id":"1110607","content":"Selected Answer: B\n\"Amazon RDS Custom is a managed database service for applications that require customization of the underlying operating system and database environment. Benefits of RDS automation with the access needed for legacy, packaged, and custom applications.\"\n\nThat should allow the \"privileged access\".","poster":"pentium75"},{"poster":"Guru4Cloud","content":"Selected Answer: B\nMigrate the database to Amazon RDS Custom for Oracle. Customize the database settings to support third-party features.","comment_id":"989280","timestamp":"1708793640.0","upvote_count":"4"},{"content":"Selected Answer: B\nCustom database features = Amazon RDS Custom for Oracle","poster":"TariqKipkemei","comment_id":"923688","upvote_count":"4","timestamp":"1702619760.0"},{"upvote_count":"2","poster":"antropaws","timestamp":"1701770340.0","comment_id":"915201","content":"Selected Answer: B\nMost likely B."},{"upvote_count":"1","content":"CCCCCCCCCCCCCCCCCCCCC","poster":"omoakin","comment_id":"909753","timestamp":"1701303000.0"},{"comment_id":"902646","poster":"aqmdla2002","upvote_count":"3","content":"Selected Answer: B\nhttps://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/","timestamp":"1700500500.0"},{"comment_id":"902271","timestamp":"1700444760.0","comments":[{"content":"Amazon RDS Custom for Oracle, which is not an actual service. !!!!","timestamp":"1701379740.0","upvote_count":"2","poster":"karbob","comment_id":"910558"}],"upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/ja_jp/AmazonRDS/latest/UserGuide/Oracle.Resources.html","poster":"hiroohiroo"},{"poster":"nosense","comment_id":"901871","upvote_count":"3","timestamp":"1700400420.0","content":"Option C is also a valid solution, but it is not as cost-effective as option B. \nOption C requires the company to manage its own database infrastructure, which can be expensive and time-consuming. Additionally, the company will need to purchase and maintain Oracle licenses."},{"upvote_count":"2","timestamp":"1700395740.0","comment_id":"901829","poster":"y0","content":"RDS Custom enables the capability to access the underlying database and OS so as to configure additional settings to support 3rd party. This feature is applicable only for Oracle and Postgresql","comments":[{"poster":"y0","upvote_count":"2","content":"Sorry Oracle and sql server (not posstgresql)","comment_id":"901830","timestamp":"1700395800.0"}]},{"upvote_count":"1","content":"I will say C cos of this\n \"application uses third-party \"","comment_id":"901499","timestamp":"1700348880.0","poster":"omoakin"},{"comment_id":"901216","poster":"cloudenthusiast","timestamp":"1700319240.0","upvote_count":"1","comments":[],"content":"Selected Answer: C\nShould not it be since for Ec2, the company will have full control over the database and this is the reason that they are moving to AWS in the first place \"The company plans to quickly migrate to AWS because of limited resources for the database, backup administration, and data center maintenance?\""},{"content":"Selected Answer: B\nRDS Custom when is something related to 3rd vendor, for me","comment_id":"899969","poster":"Efren","timestamp":"1700222820.0","upvote_count":"2"},{"upvote_count":"3","poster":"nosense","content":"not sure, but b probably","comment_id":"899402","timestamp":"1700161680.0"}],"answers_community":["B (97%)","3%"],"question_id":409,"answer_ET":"B"},{"id":"qdJ0nVVvpO0BbmKRUXfA","discussion":[{"comment_id":"696243","poster":"Incognito013","content":"A, C, D options are out, since Lambda is fully managed service which provides high availability and scalability by its own\n\nAnswers are B and E","comments":[{"comment_id":"837382","timestamp":"1678653900.0","content":"There are times you do have to increase lambda memory for improved performance though. But not in this case.","poster":"Oluseun","upvote_count":"4"}],"timestamp":"1665924480.0","upvote_count":"27"},{"poster":"Sinaneos","comment_id":"694057","content":"Selected Answer: BE\nBE so that the lambda function reads the SQS queue and nothing gets lost","timestamp":"1665675900.0","upvote_count":"12"},{"timestamp":"1736759460.0","content":"Selected Answer: BE\nbest solution is to decouple the applications using SQS queue.","poster":"AshishDhole","comment_id":"1339844","upvote_count":"1"},{"content":"Selected Answer: BE\nExplicação:\nCriar uma fila do SQS e inscrevê-la no tópico do SNS (Opção B):\n\nQuando o tópico do SNS envia notificações, elas são colocadas na fila do SQS.\nIsso garante que as mensagens sejam armazenadas de forma durável na fila, mesmo que a função do Lambda não esteja disponível para processá-las imediatamente.\nModificar a função do Lambda para ler da fila do SQS (Opção E):\n\nA função do Lambda é configurada para ser acionada por mensagens na fila do SQS.\nSe ocorrer um problema de conectividade ou falha na função, as mensagens permanecerão na fila do SQS até que sejam processadas com sucesso.\nIsso evita a perda de mensagens e elimina a necessidade de intervenção manual.","poster":"Rcosmos","comment_id":"1338986","upvote_count":"1","timestamp":"1736546160.0"},{"poster":"satyaammm","timestamp":"1735668300.0","upvote_count":"1","comment_id":"1334966","content":"Selected Answer: BE\nEnabling SQS with Lambda is the solution here as we need all data to be ingested in future."},{"comment_id":"1332308","content":"Selected Answer: BE\nB. Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic:\nBy adding an SQS queue between the SNS topic and the Lambda function, you decouple the notification and processing layers.\nMessages are stored reliably in the SQS queue, and they are not lost even if the Lambda function fails temporarily due to network connectivity or other issues.\n\nE. Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue:\nConfiguring the Lambda function to poll the SQS queue ensures that messages are processed asynchronously. If a processing failure occurs, the message remains in the queue and can be retried without manual intervention.\nThis ensures all data is eventually processed.","upvote_count":"3","poster":"MGKYAING","timestamp":"1735287480.0"},{"comment_id":"1311841","poster":"friday_test","content":"B,E : SQS is a pull mechanism broker, with at least one delivery, so by using SQS no messages get lost.","upvote_count":"1","timestamp":"1731581700.0"},{"poster":"Chr1s_Mrg","comment_id":"1292010","timestamp":"1727795580.0","upvote_count":"1","content":"Selected Answer: BE\nCreate an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic: This will decouple the data ingestion process and ensure that messages are not lost if the Lambda function fails to process them immediately\n\nModify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue: This ensures that the Lambda function can process the messages from the SQS queue, providing a reliable way to handle data ingestion even if there are network issues."},{"content":"Selected Answer: BE\nBE for sure","comment_id":"1230726","poster":"ChymKuBoy","upvote_count":"1","timestamp":"1718411580.0"},{"comment_id":"1122492","content":"Selected Answer: BE\nBE is correct as SQS ensures the messages are stored in a queue for processing.\nA: No issue with Lambda availability so this solution is wrong\nC: No issues with CPU or memory so no value added by this step also\nD: This is not a provisioning issue so provisioning more Lambda won't solve the re-execution issues. The missed messages will still be lost","poster":"awsgeek75","timestamp":"1705232280.0","upvote_count":"4"},{"comment_id":"1072997","upvote_count":"4","comments":[{"content":"Nice explaination","comments":[{"content":"explanation*","comment_id":"1271567","timestamp":"1724484600.0","poster":"SaurabhTiwari1","upvote_count":"1"}],"timestamp":"1724484540.0","upvote_count":"2","poster":"SaurabhTiwari1","comment_id":"1271566"}],"timestamp":"1700190060.0","content":"Since network timeout is the issue here, introduce SQS and read from it , that way when network goes down, data still remains in the queue and when connectivity is back, the lambda function can continue from the last data in the queue","poster":"OmegaLambda7XL9"},{"upvote_count":"1","comment_id":"1055062","content":"the correct combination of actions to ensure that the Lambda function ingests all data in the future is to create an SQS queue and subscribe it to the SNS topic (option B) and modify the Lambda function to read from the SQS queue (option E).","poster":"Ruffyit","timestamp":"1698373020.0"},{"comment_id":"1028719","poster":"tom_cruise","content":"Selected Answer: BE\nKey: network connectivity issues","timestamp":"1696845540.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"1023454","content":"Selected Answer: BE\nThis one told you the answer in the answer choices. Just add the word THEN between B and E and there ya go.","poster":"awashenko","timestamp":"1696281000.0"},{"content":"B and E , the FAN out model , SQS will help to retrie the work and delayed processing","upvote_count":"1","timestamp":"1691667720.0","comment_id":"977585","poster":"Abdou1604"},{"comment_id":"976536","timestamp":"1691579220.0","content":"Selected Answer: BE\nB) Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.\n\nE) Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.","upvote_count":"1","poster":"Guru4Cloud"},{"upvote_count":"1","timestamp":"1691123820.0","comment_id":"971684","poster":"TariqKipkemei","content":"Selected Answer: BE\nBE is most logical answer."},{"timestamp":"1689552720.0","content":"Option BE is the right answer.","upvote_count":"1","poster":"miki111","comment_id":"953733"},{"content":"Selected Answer: BE\nA. Deploying the Lambda function in multiple Availability Zones improves availability and fault tolerance but does not guarantee ingestion of all data.\n\nC. Increasing CPU and memory allocated to the Lambda function may improve its performance but does not address the issue of connectivity failures.\n\nD. Increasing provisioned throughput for the Lambda function is not applicable as Lambda functions are automatically scaled by AWS and provisioned throughput is not configurable.\n\nTherefore, the correct combination of actions to ensure that the Lambda function ingests all data in the future is to create an SQS queue and subscribe it to the SNS topic (option B) and modify the Lambda function to read from the SQS queue (option E).","comment_id":"929226","poster":"cookieMr","upvote_count":"7","timestamp":"1687337160.0"},{"upvote_count":"1","comment_id":"913323","timestamp":"1685772000.0","poster":"Bmarodi","content":"Selected Answer: BE\nThe combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future, are by Creating an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic, and Modifying the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue"},{"content":"Selected Answer: BE\nB. Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic. This will decouple the ingestion workflow and provide a buffer to temporarily store the data in case of network connectivity issues.\n\nE. Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue. This will allow the Lambda function to process the data from the SQS queue at its own pace, decoupling the data ingestion from the data delivery and providing more flexibility and fault tolerance.","comment_id":"857151","timestamp":"1680271320.0","poster":"linux_admin","upvote_count":"3"},{"comment_id":"805294","poster":"Ello2023","timestamp":"1676125320.0","content":"Help \nCan SQS Queue have multiple consumers so SNS and Lambda can consume at the same time?","upvote_count":"1"},{"poster":"Lonojack","comments":[{"content":"What does connectivity have to do with Provisioned IOPS which is supposed to enhance I/O rate?","timestamp":"1675568160.0","comment_id":"798590","upvote_count":"2","poster":"ProfXsamson"}],"timestamp":"1674753900.0","comment_id":"788966","content":"How come no one’s acknowledged the connection issue? Obviously we know we need SQS as a buffer for messages when the system fails. But shouldn’t we consider provisioned iops to handle the the connectivity so maybe it will be less likely to lose connectivity and fail in the first place?","upvote_count":"2"},{"timestamp":"1673053440.0","upvote_count":"3","content":"Selected Answer: BE\nTo ensure that the Lambda function ingests all data in the future, the solutions architect can create an Amazon Simple Queue Service (Amazon SQS) queue and subscribe it to the SNS topic. This will allow the data notifications to be queued in the event of a network connectivity issue, rather than being lost. The solutions architect can then modify the Lambda function to read from the SQS queue, rather than from the SNS topic directly. This will allow the Lambda function to process any queued data as soon as the network connectivity issue is resolved, without the need for manual intervention.\n\nBy using an SQS queue as a buffer between the SNS topic and the Lambda function, the company can improve the reliability and resilience of the ingestion workflow. This approach will help ensure that the Lambda function ingests all data in the future, even when there are network connectivity issues.","comment_id":"768171","poster":"SilentMilli"},{"content":"Selected Answer: BE\nB and E, allow the data to be queued up in the event of a failure, rather than being lost, then by reading from the queue, the Lambda function will be able to process the data\nA, improves reliability but doesn't ensure all data is ingested\nC and D, they improve performance but not ensure all data is ingested","upvote_count":"3","poster":"pazabal","timestamp":"1671573780.0","comment_id":"751580"},{"comment_id":"750492","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: BE\n***CORRECT***\nB. Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.\nE. Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.\n\nAn Amazon Simple Queue Service (SQS) queue can be used to decouple the data ingestion workflow and provide a buffer for data deliveries. By subscribing the SQS queue to the SNS topic, you can ensure that notifications about new data deliveries are sent to the queue even if the Lambda function is unavailable or experiencing connectivity issues. When the Lambda function is ready to process the data, it can read from the SQS queue and process the data in the order in which it was received.","timestamp":"1671510180.0","upvote_count":"2","comments":[{"content":"***WRONG***\nOption A, deploying the Lambda function in multiple Availability Zones, would not directly address the issue of connectivity failures. \nOption C, increasing the CPU and memory that are allocated to the Lambda function, would not directly address the issue of connectivity failures. Option D, increasing provisioned throughput for the Lambda function, would not directly address the issue of connectivity failures.","poster":"Buruguduystunstugudunstuy","comment_id":"750493","timestamp":"1671510180.0","upvote_count":"2"}]},{"timestamp":"1671429360.0","poster":"career360guru","comment_id":"749502","upvote_count":"1","content":"Selected Answer: BE\nB and E"},{"upvote_count":"1","comment_id":"723546","content":"B and E","timestamp":"1669037460.0","poster":"Wpcorgan"},{"comment_id":"703186","timestamp":"1666627680.0","poster":"Six_Fingered_Jose","upvote_count":"4","content":"Selected Answer: BE\nB and E is the obvious answer here,\nSQS ensures that message does not get lost"},{"upvote_count":"1","poster":"D2w","timestamp":"1666299900.0","content":"Selected Answer: AB\nWhy not AB","comments":[{"content":"lambda is serverless, it does not need to be multi-AZ..","comment_id":"703185","timestamp":"1666627560.0","poster":"Six_Fingered_Jose","upvote_count":"1"}],"comment_id":"700301"}],"timestamp":"2022-10-13 17:45:00","isMC":true,"answer_images":[],"answers_community":["BE (98%)","2%"],"topic":"1","answer_ET":"BE","answer":"BE","question_text":"A company has a data ingestion workflow that consists of the following:\n• An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries\n• An AWS Lambda function to process the data and record metadata\nThe company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job.\nWhich combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? (Choose two.)","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/85408-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"D":"Increase provisioned throughput for the Lambda function.","E":"Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.","B":"Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.","A":"Deploy the Lambda function in multiple Availability Zones.","C":"Increase the CPU and memory that are allocated to the Lambda function."},"answer_description":"","exam_id":31,"unix_timestamp":1665675900,"question_id":410}],"exam":{"id":31,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false,"isMCOnly":true,"numberOfQuestions":1019,"provider":"Amazon","isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":82},"__N_SSP":true}