{"pageProps":{"questions":[{"id":"gb3JX1BCHX6yW0uMJ0Kv","question_text":"A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting the traffic away using an Amazon Route\n53 weighted routing policy.\nFor its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base.\nWhich deployment strategy will meet these requirements?","isMC":true,"exam_id":35,"question_id":1,"answer_ET":"B","choices":{"D":"Use AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be changed, use OpsWorks to perform a blue/green deployment and shift traffic gradually.","A":"Use AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.","B":"Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.","C":"Use AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the API and Lambda functions. Shift traffic gradually using an Elastic Beanstalk blue/green deployment."},"topic":"1","unix_timestamp":1617382740,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/48835-exam-aws-devops-engineer-professional-topic-1-question-1/","question_images":[],"discussion":[{"timestamp":"1634620020.0","content":"B\nnoting SAM is built over CloudFormation.","comment_id":"330985","upvote_count":"12","poster":"devopp"},{"comment_id":"1279415","poster":"[Removed]","timestamp":"1725605160.0","upvote_count":"9","content":"B - Canary Deployment"},{"content":"Selected Answer: B\nB IS HIGHLY CORRECT","upvote_count":"1","comment_id":"1293423","timestamp":"1728123060.0","poster":"agbor_tambe"},{"content":"B is the correct answer\n\nThis is a Legit dumps I passed my Exam on June 15th, 2023 with a 870 score. I studied both DOP-C01 and DOP-C02, 95% of the questions came from them. Most questions came from DOP-C01 with 80% and DOP-C02 with 15%. Get contributor access to read all comments and maybe access other exams if you plan to take more exams. Go through the questions at least twice or more to get familiar with it!","comment_id":"952216","poster":"b620a50","upvote_count":"5","timestamp":"1689412440.0"},{"poster":"mdg3501","content":"anyone took the exam lately? is this still valid or should I go ahead with C02?","comment_id":"894526","upvote_count":"2","timestamp":"1683772200.0"},{"poster":"Hamza5","content":"B - Canary Deployment","comment_id":"793291","upvote_count":"1","timestamp":"1675110240.0"},{"poster":"Bulti","content":"B- Canary deployment of Lambda is done using AWS SAM that comes built in with CodeDeploy. https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html","timestamp":"1671946320.0","upvote_count":"1","comment_id":"755414"},{"comment_id":"696953","upvote_count":"2","poster":"developer_404","timestamp":"1665985560.0","content":"Selected Answer: B\nCanary Deployment is the use case for this scenario while the other two are Blue green deployment. Option A is using Route53 failover which is not necessary for the scenario."},{"comment_id":"661937","comments":[{"comment_id":"1204397","poster":"vn_thanhtung","upvote_count":"1","timestamp":"1714462320.0","content":"ANS: C, D check topic https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html"},{"poster":"Goozian","upvote_count":"2","content":"D AND E","timestamp":"1664058480.0","comments":[{"content":"what about the other ones new? thanks","timestamp":"1664295240.0","upvote_count":"1","comments":[{"timestamp":"1667869680.0","content":"Passed with a new question?","upvote_count":"1","poster":"ZZIN","comment_id":"713386"}],"poster":"kopper2019","comment_id":"680943"}],"comment_id":"678268"},{"poster":"Goozian","comment_id":"689028","content":"it came up in my exam","upvote_count":"1","timestamp":"1665205140.0"},{"comments":[{"comment_id":"738544","poster":"huyrk102","content":"Agree CD\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed","upvote_count":"1","timestamp":"1670466720.0"}],"upvote_count":"2","content":"Ans: CD\nhttps://aws.amazon.com/tw/premiumsupport/knowledge-center/cloudformation-update-rollback-failed/","poster":"EnWu","comment_id":"690933","timestamp":"1665394260.0"}],"timestamp":"1662522120.0","upvote_count":"2","poster":"kopper2019","content":"New as of September 6th, 2022\nNO.265 A company updated the AWS CloudFormation template tor a critical business application. The stack update process Tailed due to an error in me updated template, and CloudFormation automatically began the stack rollback process Later, a DevOps engineer found the application was still unavailable, and that the stack was in the UPDATE_ROLLBACK_FALED state Which combination of actions will allow the stack rollback to complete successful/? (Select 2)\nA. Attach the AWSCloudFormationFulAccess IAM policy to the CloudFormation role \nB. Automatically heal the stack resources using CloudFormation drift detection.\nC. Issue a ContinueUpdateRolback command from the CloudFormation console or AWS CLI\nD. Manually the resources to match the expectations of the stack.\nE. Update the existing CloudFormation stack using the original template"},{"poster":"kopper2019","content":"New as of September 6th, 2022\nA company's application is running on Amazon EC2 instances in an Auto Scaling group. A DevOps engineer needs to ensure there are at least four application servers running at all times. Whenever an update has to be made to the application, the engineer creates a new AMI with the updated configuration and updates the AWS CloudFormation template with the new AMI ID. After the stack update finishes, the engineer manually terminates the old instances one by one. verifying that the new instance is operational before proceeding. The engineer needs to automate this process.\nWhich action will allow for the LEAST number of manual steps moving forward?\nA. Update the CloudFormation template to include the UpdatePolicy attribute with the AutoScalingRollingUpdate policy.\nB. Update the CloudFormation template to include the UpdatePolicy attribute with the AutoScalingReplacingUpdate policy.","comment_id":"661935","upvote_count":"1","comments":[{"upvote_count":"1","poster":"kopper2019","content":"C. Use an Auto Scaling lifecycle hook to verify that the previous instance is operational before allowing the DevOps engineer's selected instance to terminate.\nD. Use an Auto Scaling lifecycle hook to confirm there are at least four running instances before allowing the DevOps engineer's selected instance to terminate.","comment_id":"661936","comments":[{"timestamp":"1665394560.0","poster":"EnWu","content":"Ans: A","comment_id":"690940","upvote_count":"4"}],"timestamp":"1662522060.0"}],"timestamp":"1662522060.0"},{"comments":[{"timestamp":"1666708260.0","comment_id":"703948","content":"Ans: ADE","upvote_count":"1","comments":[{"comment_id":"703949","content":"sr, I think ADF","comments":[{"timestamp":"1670469900.0","comment_id":"738585","content":"The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. \n=> A\nThe entire CI/CD pipeline must be capable of being re- provisioned in alternate AWS accounts or Regions within minutes. \n=> DF","upvote_count":"3","poster":"huyrk102"}],"upvote_count":"6","timestamp":"1666708320.0","poster":"huynd6793"}],"poster":"huynd6793"}],"comment_id":"661933","upvote_count":"1","content":"New as of September 6th, 2022\nA company needs to implement a robust CI/CD pipeline to automate the deployment of an application in AWS. The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. The entire CI/CD pipeline must be capable of being re- provisioned in alternate AWS accounts or Regions within minutes. A DevOps engineer has already created an AWS CodeCommit repository to store the source code.\nWhich combination of actions should be taken when building this pipeline to meet these requirements? (Select THREE.)\nA. Configure an AWS CodePipehne pipeline with a build stage using AWS CodeBuild. \nB. Copy the build artifact from CodeCommit to Amazon S3.\nC. Create an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline.\nD. Create an AWS Elastic Beanstalk environment as the deployment target in AWS CodePipeline.\nE. Implement an Amazon SQS queue to decouple the pipeline components. \nF. Provision all resources using AWS CloudFormation.","poster":"kopper2019","timestamp":"1662522000.0"},{"poster":"kopper2019","comments":[{"content":"Ans: BD","upvote_count":"4","comment_id":"690943","timestamp":"1665394680.0","poster":"EnWu","comments":[{"upvote_count":"1","poster":"huyrk102","content":"remediate this issue\n=> D\nprevent this issue from happening in the future.\n=> B","comment_id":"738586","timestamp":"1670469960.0"}]}],"upvote_count":"2","content":"New as of September 6th, 2022\nA company runs several applications across multiple AWS accounts in an organization in AWS Organizations.\nSome of the resources are not tagged properly, and the company's finance team cannot determine which costs are associated with which applications. A DevOps engineer must remediate this issue and prevent this issue from happening in the future.\nWhich combination of actions should the DevOps engineer take to meet these requirements? (Select TWO.)\n\nA. Activate the user-defined cost allocation tags in each AWS account. \nB. Create and attach an SCP that requires a specific tag.\nC. Define each line of business (LOB) in AWS Budgets. Assign the required tag to each resource. \nD. Scan all accounts with Tag Editor. Assign the required tag to each resource.\nE. Use the budget report to find untagged resources. Assign the required tag to each resource.","comment_id":"661930","timestamp":"1662521880.0"},{"comment_id":"661927","timestamp":"1662521820.0","upvote_count":"1","poster":"kopper2019","comments":[{"timestamp":"1662521820.0","content":"B. Establish a permission boundary in the master account to restrict Regions and authorized services. Use AWS CloudFormation StackSet to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.\nC. Establish a service control policy in the master account to restrict Regions and authorized services. Use AWS Resource Access Manager to share master account roles with permissions for each job function, including AWS SSO for authentication in each account.\nC. Establish a service control policy in the master account to restrict Regions and authorized services. Use CloudFormation StackSet to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.","poster":"kopper2019","upvote_count":"1","comments":[{"timestamp":"1665394800.0","comments":[{"timestamp":"1670470380.0","upvote_count":"1","content":"Agree D","comment_id":"738588","poster":"huyrk102"}],"poster":"EnWu","upvote_count":"3","comment_id":"690946","content":"Ans: D"},{"comment_id":"1098678","comments":[{"comment_id":"1204407","poster":"vn_thanhtung","content":"RAM to share resource not policy i think","timestamp":"1714464000.0","upvote_count":"1"}],"upvote_count":"1","poster":"z_inderjot","content":"C has to be the answer , since D does not support use of Active Directory , but C has AWS SSO for authencation which leverage Active Directory for authentication . While both RAM and StackSet can be used to share role and permessions","timestamp":"1702791120.0"}],"comment_id":"661928"}],"content":"New as of September 6th, 2022\nA company is using AWS Organizations and wants to implement a governance strategy with the following requirements:\n\n- AWS resource access is restricted to the same two Regions for all accounts.\n- AWS services are limited to a specific group of authorized services for all accounts.\n- Authentication is provided by Active Directory.\n- Access permissions are organized by job function and are identical in each account. Which solution will meet these requirements?\n\nA. Establish an organizational unit (OU) with group policies in the master account to restrict Regions and authorized services. Use AWS Cloud Formation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account."},{"timestamp":"1662521640.0","comment_id":"661923","content":"NEw Q as of September 6th, 2022\nA DevOps engineer is currently running a container-based workload on-premises The engineer wants to move the application to AWS, but needs to keep the on-premises solution active because not all APIs will move at the same time. The traffic between AWS and the on-premises network should be secure and encrypted at all times. Low management overload is also a requirement.\n\nWhich combination of actions will meet these criteria? (Select THREE.)\n\nCreate a Network Load Balancer and. for each service, create a listener that points to the correct set of containers either in AWS or on-premises.\n\nCreate an Application Load Balancer and, for each service, create a listener that points to the correct set of containers either in AWS or on-premises.","upvote_count":"1","poster":"kopper2019","comments":[{"upvote_count":"1","timestamp":"1662521700.0","poster":"kopper2019","comments":[{"comment_id":"703959","timestamp":"1666709280.0","upvote_count":"5","comments":[{"content":"Agree BDE","timestamp":"1670470740.0","upvote_count":"1","poster":"huyrk102","comment_id":"738592"},{"timestamp":"1668833220.0","poster":"ZZIN","upvote_count":"1","comment_id":"721760","content":"It says three, but what is the correct answer, ABC?"}],"poster":"huynd6793","content":"B: Create an Application Load Balancer and, for each service, create a listener that points to the correct set of containers either in AWS or on-premises.\nD: Host the AWS containers in Amazon ECS with a Fargate launch type\nE. Use Amazon API Gateway to front the workload, and create a VPC link so API Gateway can forward API calls to the on-premises network through a VPN connection."}],"content":"A. Create a Network Load Balancer and. for each service, create a listener that points to the correct set of containers either in AWS or on-premises.\nB . Create an Application Load Balancer and, for each service, create a listener that points to the correct set of containers either in AWS or on-premises.\nC. Host the AWS containers in Amazon ECS with an EC2 launch type. (D). Host the AWS containers in Amazon ECS with a Fargate launch type\nD. Use Amazon API Gateway to front the workload, and create a VPC link so API Gateway can forward API calls to the on-premises network through a VPN connection.\nE. Use Amazon API Gateway to front the workload, and set up public endpoints for the on-premises APIs so API Gateway can access them.","comment_id":"661926"}]},{"poster":"Manh","upvote_count":"3","content":"B make sense","timestamp":"1661910900.0","comment_id":"654607"},{"upvote_count":"1","poster":"sanc","comment_id":"327195","content":"B .............","timestamp":"1633432320.0"},{"upvote_count":"2","content":"I'll go with B","timestamp":"1632314160.0","comment_id":"326828","poster":"WhyIronMan"}],"answer":"B","answers_community":["B (100%)"],"answer_description":"","timestamp":"2021-04-02 18:59:00"},{"id":"sOp2yMqEjaeNkS5nm3yP","answer_description":"","answers_community":["C (100%)"],"choices":{"B":"Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.","C":"Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.","D":"Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases.","A":"Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases."},"timestamp":"2019-11-04 18:30:00","unix_timestamp":1572888600,"question_images":[],"discussion":[{"comment_id":"20679","content":"since its minimal changes, stick with Aurora","timestamp":"1632208500.0","upvote_count":"19","poster":"marwan"},{"timestamp":"1632576420.0","content":"minimal changes, so change to dynamodb may need schema change, C makes more sense.","upvote_count":"13","comment_id":"21243","poster":"BeastX"},{"timestamp":"1727466420.0","content":"C, Single Product Catalog: Using Aurora with read replicas allows you to maintain a single source of truth for your product catalog that can be accessed across multiple regions. Read replicas can help distribute the read load and provide faster access to the catalog.\n\nRegional Customer Information: Setting up additional local Aurora instances in each region for customer information and purchases ensures that sensitive data remains compliant with local regulations while allowing the application to scale regionally.\n\nThis approach minimizes application changes because you can leverage the existing Aurora database structure and simply configure the necessary replicas and local instances.","upvote_count":"1","poster":"4b18f59","comment_id":"1290325"},{"poster":"xhi158","upvote_count":"1","content":"I will go with C\n\nTo meet the company’s requirements with the least amount of application changes, the company should use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases 1. This approach will ensure that the company has a single product catalog across all regions while keeping customer information and purchases in each region for compliance purposes.\n\nOption A is incorrect because Amazon Redshift is not designed for storing product catalogs.\n\nOption B is incorrect because Amazon DynamoDB global tables are not designed for storing product catalogs.\n\nOption D is incorrect because Amazon DynamoDB global tables are not designed for storing customer information and purchases.\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.DBInstanceClass.html","timestamp":"1700400120.0","comment_id":"1074629"},{"poster":"hp298","timestamp":"1679266860.0","upvote_count":"1","comment_id":"844318","content":"Selected Answer: C\nC because LEAST amount of changes. DB is already aurora."},{"content":"Due to least amount of changes needed, the answer is C.","poster":"Bulti","timestamp":"1672549680.0","comment_id":"763055","upvote_count":"2"},{"content":"Selected Answer: C\nc for me","upvote_count":"2","poster":"colinquek","timestamp":"1662683760.0","comment_id":"664097"},{"content":"Selected Answer: C\nC. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.","upvote_count":"1","timestamp":"1649162700.0","poster":"jj22222","comment_id":"581252"},{"timestamp":"1636071840.0","content":"Go C -1","upvote_count":"1","poster":"oopsy","comment_id":"447382"},{"poster":"certking","timestamp":"1636002540.0","upvote_count":"2","comment_id":"446803","content":"if you have developed with both SQL-like databases, i.e.: RDS, Aurora, and with DynamoDB, you should know the database schema design and CRUD operations are VASTLY different between SQL and DynamoDB."},{"poster":"WhyIronMan","upvote_count":"2","timestamp":"1636000920.0","content":"I'll go with C)\nThe question wants \"LEAST amount of application changes\", so ANY option with includes DynamoDB (even for half portion) will require a LOT of changes.","comment_id":"320215"},{"timestamp":"1635977400.0","content":"C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.","comment_id":"318032","poster":"glam","upvote_count":"2"},{"comment_id":"226980","poster":"Coffeinerd","timestamp":"1635278340.0","upvote_count":"1","content":"Key info: \"LEAST amount of application changes\", so not changing DB type or engine, no code refactoring! \nRight: C\nWrong: \n-A: require code changes, Redshift makes no sense here as it is for data warehousing\n-B: require again code changes\n-D: would imply code changes as well."},{"upvote_count":"4","comment_id":"223180","content":"I'll go with C","timestamp":"1634994780.0","poster":"jackdryan"},{"upvote_count":"1","poster":"Dr_Wells","timestamp":"1634807880.0","content":"Its Option C cause, we only need a LEAST amount of changes.","comment_id":"207067"},{"comments":[{"poster":"Piccaso","content":"Amazing comment involving real life scenarios. Thanks.","timestamp":"1675428420.0","comment_id":"797031","upvote_count":"1"}],"timestamp":"1634692440.0","upvote_count":"7","content":"It's C - all the others require application changes to accommodate a different DB, which is undesirable since the question is asking for minimal application changes. It's worth adding that in the real world things are often much more complicated than this, so you'd want to revisit the business requirements, validate them, forecast future requirements and make a decision. Having regions of US, Asia and Europe is a gross simplification, because what you really need to consider is the data protection regulations in specific jurisdictions, not continents. e.g. Indonesia is in Asia and up until Oct 2019 required customer data to be held in-country, but there is no AWS data centre there. What do you do?","poster":"cowshield","comment_id":"144587"},{"comment_id":"122123","upvote_count":"1","poster":"AKD","timestamp":"1634446980.0","content":"Can't be B as it requires application changes.\nC is correct"},{"content":"My Selection C","poster":"Kuang","timestamp":"1634107980.0","upvote_count":"1","comment_id":"111092"},{"comment_id":"106962","upvote_count":"1","poster":"un","content":"C is correct","timestamp":"1634008740.0"},{"comment_id":"80916","poster":"Raj9","content":"Except C, all will need schema changes, so C makes more sense","timestamp":"1633528080.0","upvote_count":"3"},{"comment_id":"64206","upvote_count":"1","poster":"Ebi","content":"C is the answer","timestamp":"1633258260.0"},{"content":"C is correct","poster":"yassu","timestamp":"1633254540.0","comment_id":"62117","upvote_count":"1"},{"upvote_count":"1","timestamp":"1633018260.0","content":"LEAST amount of application changes so answer should be C","poster":"ppshein","comment_id":"61643"},{"timestamp":"1632820080.0","upvote_count":"2","poster":"xaocho","comment_id":"54534","content":"It is C"},{"timestamp":"1632087180.0","poster":"G3","comment_id":"19194","upvote_count":"3","content":"I feel its B.\nhttps://aws.amazon.com/dynamodb/global-tables/","comments":[{"upvote_count":"1","comments":[{"comment_id":"185028","poster":"NKnab","timestamp":"1634754600.0","upvote_count":"1","content":"change to c"}],"comment_id":"114511","poster":"NKnab","content":"Aurora also has global option - read replicas in multi region","timestamp":"1634122320.0"},{"upvote_count":"1","timestamp":"1634981940.0","comment_id":"221341","content":"Global tables are for reading and writing data as well. It must be C.","poster":"mrcaique"}]}],"answer_images":[],"question_id":2,"answer_ET":"C","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/7681-exam-aws-devops-engineer-professional-topic-1-question-10/","exam_id":35,"question_text":"An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on\nAmazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance.\nWhen the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region.\nHow should the company meet these requirements with the LEAST amount of application changes?","answer":"C","topic":"1"},{"id":"NSLB2Lm8B3JFPGUJ9yxS","answers_community":["A (57%)","B (43%)"],"topic":"1","answer_description":"","question_id":3,"choices":{"B":"Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.","A":"Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.","D":"Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack.","C":"Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks."},"discussion":[{"content":"Selected Answer: B\nThe correct answer is B. Creating a CloudFormation nested stack allows the software development team to make cross-stack resource references and parameters available in both stacks, while still maintaining separate review and lifecycle management processes for each team. In this way, the software development team can use resources maintained by the database engineering team in their CloudFormation template, and can deploy changes to their template using their CI/CD pipeline.","poster":"SatenderRathee","comment_id":"741576","upvote_count":"11","timestamp":"1670752380.0"},{"content":"Selected Answer: A\nA is correct\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html\n\nWhen you organize your AWS resources based on lifecycle and ownership, you might want to build a stack that uses resources that are in another stack. You can hardcode values or use input parameters to pass resource names and IDs. However, these methods can make templates difficult to reuse or can increase the overhead to get a stack running. Instead, use cross-stack references to export resources from a stack so that other stacks can use them. Stacks can use the exported resources by calling them using the Fn::ImportValue function.","poster":"saeidp","comment_id":"770939","timestamp":"1673309640.0","upvote_count":"7"},{"timestamp":"1705562700.0","upvote_count":"1","poster":"failexamonly","comment_id":"1125595","content":"Selected Answer: B\nB will satisfy both teams"},{"comment_id":"870072","poster":"daheck","content":"Selected Answer: A\nWhen you organize your AWS resources based on lifecycle and ownership, you might want to build a stack that uses resources that are in another stack. You can hardcode values or use input parameters to pass resource names and IDs. However, these methods can make templates difficult to reuse or can increase the overhead to get a stack running. Instead, use cross-stack references to export resources from a stack so that other stacks can use them. Stacks can use the exported resources by calling them using the Fn::ImportValue function.","upvote_count":"1","timestamp":"1681462380.0"},{"timestamp":"1680532860.0","comment_id":"860028","upvote_count":"1","poster":"merki","content":"Chatgpt said: Option B: Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks would be the best solution to meet these requirements.\n\nA CloudFormation nested stack is a stack that can be used as a resource within another stack, which allows for cross-stack resource references and parameters. This will enable the software development team to reference and use resources from the database engineering team's CloudFormation template without having to maintain them in their own template.\n\nUsing a nested stack also allows both teams to maintain their own review and lifecycle management processes, as each team can manage their own stack independently. Additionally, the resource-level change-set reviews can be implemented on both the parent and nested stack, which provides a comprehensive review process."},{"poster":"easytoo","comment_id":"846303","timestamp":"1679429400.0","content":"It's B and here's why:\nA CloudFormation nested stack allows you to create a stack as a set of AWS resources within another stack, enabling you to break down complex stacks into smaller, more manageable stacks. The nested stack can be used to create a set of resources that are managed and maintained by a different team or individual while still maintaining control over the resources by the parent stack. By using nested stacks, both teams can maintain their own review and lifecycle management processes while still enabling cross-stack resource references and parameter sharing","upvote_count":"1"},{"comment_id":"818504","timestamp":"1677105780.0","content":"The answer is not nested stacks as you cannot perform change sets on your own. \nYou need a root stack for that and who manages the root stack?","upvote_count":"3","poster":"BelloMio"},{"timestamp":"1677036480.0","upvote_count":"2","comment_id":"817432","poster":"BelloMio","comments":[{"upvote_count":"2","timestamp":"1677036900.0","comment_id":"817435","content":"https://blog.shikisoft.com/cloudformation-nested-stacks-vs-cross-stack-references/\n\n“Alternatively, if you need to manage your stacks as separate entities, you should use cross-stack references.”","poster":"BelloMio"}],"content":"B does not make sense to me. If you want the straps to be independent from one another you don’t use nested stacks"},{"content":"Selected Answer: B\nB.\nThis will satisfy both teams' requirements.","upvote_count":"2","timestamp":"1676935800.0","comment_id":"816012","poster":"CloudFloater"},{"timestamp":"1676839680.0","upvote_count":"1","comment_id":"814527","content":"The correct answer is B\nNested Stacks are a great way to deploy your infrastructure in a modular fashion\"","poster":"LoveToronto"},{"upvote_count":"1","comment_id":"814525","poster":"LoveToronto","timestamp":"1676839620.0","content":"Nested Stacks are a great way to deploy your infrastructure in a modular fashion\""},{"poster":"Mark1000","upvote_count":"2","comment_id":"809712","comments":[{"upvote_count":"1","comment_id":"855959","poster":"AkaAka4","content":"People select the answer when they comment. If they don't, then their answer is not considered in the \"proportion\"... which is why even many prefer A, they didn't select the answer and B is still shown as the most selected answer.","timestamp":"1680193920.0"}],"timestamp":"1676476980.0","content":"I'm not quite sure how this website calculates the proportions of the answers, as in this question there are more users who have voted A than B, at least a tie, and yet it gives more percentage to B...\nIn fact, in my opinion, the correct answer is A.\nBut come on, I'm not saying it's A, it's what I think, I'm saying that I don't know how to get the proportion....."},{"timestamp":"1676421360.0","upvote_count":"2","comment_id":"808970","poster":"joseribas89","content":"Selected Answer: B\nOption B is the best solution in this case. A nested stack is a stack that is created and managed as part of another stack. By creating a nested stack, both the database engineering team and the software development team can maintain their own CloudFormation templates and associated processes. The database engineering team can deploy their database stack as usual, and the software development team can create a nested stack that references the database stack's resources using cross-stack references."},{"upvote_count":"3","comments":[{"comment_id":"808972","upvote_count":"1","poster":"joseribas89","timestamp":"1676421480.0","content":"Option A, using stack exports and imports, can allow cross-stack references, but it does not provide a way to maintain separate CloudFormation templates or allow for resource-level change-set reviews.\n\nOption D, passing resource names and IDs as input parameters from the database stack to the web application stack, can work, but it requires manual updates to the web application stack whenever the database stack changes. This solution can be error-prone and time-consuming to maintain.\n\nSo, option B"}],"timestamp":"1675945320.0","comment_id":"803196","content":"Selected Answer: A\nB and C are eliminated because software development team needs to use database engineering team, not vice verse. \nA is more reasonable than D.","poster":"Piccaso"},{"comment_id":"794618","poster":"rrshah83","timestamp":"1675199460.0","content":"Selected Answer: A\nDB and web teams want to maintain the stack lifecycle separately ==> cross-stacks...","upvote_count":"5"},{"content":"Selected Answer: A\nA is correct.","upvote_count":"4","timestamp":"1674443280.0","comment_id":"784911","poster":"Bulti"},{"timestamp":"1673878560.0","poster":"Oleg_gol","comment_id":"777753","upvote_count":"5","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack"},{"timestamp":"1671259260.0","comment_id":"747820","upvote_count":"5","poster":"benjl","content":"I think answer is A.\nnested stack is creating its own resources for software development team. The question ask for \"needs to use resources maintained by the database engineering team\". From this perspective, stack export and import is valid approach. In this case, development team get affected when there is a change in database team.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack"},{"poster":"nsvijay04b1","timestamp":"1669988340.0","upvote_count":"1","content":"Selected Answer: B\nA is wrong when DB team deletes stack , application team gets effected.\nB is right , use nested stack and deploy own DB for application team and own resources. Even DB template changes further it wont effect already created resources or template application already referenced.\n\n\"However, both teams have their own review and lifecycle management processes that they want to keep\"","comment_id":"733763"},{"poster":"quixo","upvote_count":"3","comment_id":"733420","timestamp":"1669952160.0","content":"Instead, use cross-stack references to export resources from a stack so that other stacks can use them. Stacks can use the exported resources by calling them using the Fn::ImportValue function. \nRefer: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html \nhttps://blog.shikisoft.com/cloudformation-nested-stacks-vs-cross-stack-references/ \nhttps://www.bogotobogo.com/DevOps/AWS/aws-Cloudformation-CrossStck-Reference.php \n\nA"},{"poster":"SmileyCloud","content":"Selected Answer: B\n\"Nested Stacks are a great way to deploy your infrastructure in a modular fashion\"","comment_id":"732320","timestamp":"1669875240.0","upvote_count":"2"},{"comment_id":"731744","upvote_count":"2","poster":"Subhasis_Pattnayak","timestamp":"1669825980.0","content":"B is right"}],"unix_timestamp":1669825980,"answer_ET":"A","isMC":true,"answer_images":[],"question_text":"A company is developing a web application's infrastructure using AWS CloudFormation. The database engineering team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. As the scope of the application grows, the software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to keep. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/89418-exam-aws-devops-engineer-professional-topic-1-question-100/","question_images":[],"timestamp":"2022-11-30 17:33:00","exam_id":35,"answer":"A"},{"id":"wGKkavmYZGaLVuVExbqh","answers_community":["B (100%)"],"choices":{"B":"Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.","C":"Enable AWS Trusted Advisor and configure automatic remediation using Amazon CloudWatch Events.","D":"Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents.","A":"Enable AWS CloudTrail and configure automatic remediation using AWS Lambda."},"unix_timestamp":1669875300,"exam_id":35,"url":"https://www.examtopics.com/discussions/amazon/view/89516-exam-aws-devops-engineer-professional-topic-1-question-101/","answer":"B","question_images":[],"answer_description":"","answer_ET":"B","question_text":"A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or write accessible.\n\nWhat should a DevOps engineer do to meet these requirements?","isMC":true,"question_id":4,"timestamp":"2022-12-01 07:15:00","topic":"1","discussion":[{"timestamp":"1670752500.0","poster":"SatenderRathee","comment_id":"741578","content":"Selected Answer: B\nB. Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.\n\nTo meet the requirements specified in the question, the DevOps engineer should enable AWS Config rules and use AWS Systems Manager documents to automate the process of ensuring that all existing and future Amazon S3 buckets have encryption, logging, and versioning enabled, and that no buckets are publicly readable or writable. AWS Config rules allow the security team to specify rules for how resources should be configured in their AWS environment, and AWS Systems Manager documents can be used to automate the process of remedying any non-compliant resources.","upvote_count":"5"},{"upvote_count":"2","timestamp":"1679429580.0","poster":"easytoo","content":"b-b-b-b-b-b-b-\nBy using AWS Config rules, the DevOps engineer can ensure that all existing and future S3 buckets have encryption, logging, and versioning enabled. The DevOps engineer can then use AWS Systems Manager documents to automatically remediate any noncompliant resources, ensuring that all S3 buckets remain secure.","comment_id":"846305"},{"comment_id":"829318","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/config/latest/developerguide/remediation.html","upvote_count":"1","poster":"bgc1","timestamp":"1677958260.0"},{"upvote_count":"1","timestamp":"1675946580.0","comment_id":"803222","content":"Selected Answer: B\nA : CloudTrail+Lambda is ....\nB : Looks nice\nC : ....\nD : ....","poster":"Piccaso"},{"upvote_count":"1","timestamp":"1674443760.0","content":"Selected Answer: B\nAnswer is B","poster":"Bulti","comment_id":"784927"},{"upvote_count":"1","comment_id":"754182","poster":"Imstack","content":"BBBBBBBBBBBBBB","timestamp":"1671797820.0"},{"poster":"SmileyCloud","comment_id":"732321","upvote_count":"4","content":"Selected Answer: B\nB - correct. Anytime there is something regarding compliance and enforcement, your best bet is AWS Config.","timestamp":"1669875300.0"}],"answer_images":[]},{"id":"6LUPUgizMp4P5B4GxJuT","url":"https://www.examtopics.com/discussions/amazon/view/89072-exam-aws-devops-engineer-professional-topic-1-question-102/","answer_ET":"B","isMC":true,"exam_id":35,"question_id":5,"answer":"B","answers_community":["B (77%)","A (23%)"],"choices":{"C":"Use EC2 Auto Recovery to automatically stop and start the instance in case of a failure. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.","B":"Configure AWS OpsWorks, and use the auto healing feature to stop and start the instance. Use a lifecycle event in OpsWorks to pull the metadata from Amazon S3 and update it on the instance.","D":"Use AWS CloudFormation to create an EC2 instance that includes the UserData property for the EC2 resource. Add a command in UserData to retrieve the application metadata from Amazon S3.","A":"Create an Amazon CloudWatch alarm for the StatusCheckFailed metric. Use the recover action to stop and start the instance. Use an S3 event notification to push the metadata to the instance when the instance is back up and running."},"topic":"1","timestamp":"2022-11-28 14:12:00","answer_description":"","discussion":[{"content":"Identical to Question #: 24","poster":"Gomer","upvote_count":"1","comment_id":"1221270","timestamp":"1717018320.0"},{"upvote_count":"1","comment_id":"829321","content":"Selected Answer: B\nSame reasons as Bulti","timestamp":"1677958380.0","poster":"bgc1"},{"upvote_count":"1","content":"Selected Answer: B\nA and C are eliminated, because S3 event notification cannot satisfy the context that the instance will be started when it does not respond. \nD does not work.","timestamp":"1676642100.0","poster":"Piccaso","comment_id":"811940"},{"comment_id":"785994","content":"Selected Answer: B\nB is correct because A and C are technically incorrect and D is incomplete in terms of meeting the requirements.","upvote_count":"2","timestamp":"1674518640.0","poster":"Bulti"},{"comment_id":"777784","timestamp":"1673879760.0","content":"Selected Answer: B\nhttps://www.examtopics.com/discussions/amazon/view/47002-exam-aws-devops-engineer-professional-topic-1-question-199/","upvote_count":"1","poster":"Oleg_gol"},{"upvote_count":"4","comment_id":"746207","timestamp":"1671115560.0","content":"A and C - S3 event notification are triggered when the objects change in S3.\nD - Only half of solution. No mention of how the instance will be recovered\nB - Is the correct option.","poster":"saggy4"},{"poster":"nsvijay04b1","content":"Selected Answer: B\nAgree with @smileyCloud","comment_id":"733536","upvote_count":"2","timestamp":"1669965960.0"},{"comment_id":"733484","timestamp":"1669961340.0","poster":"quixo","content":"I'll go with B\n\nA) and C) are wrong because there is no such thing like:\n\"Use a trigger in Amazon S3 to push the metadata to the instance when it is back up and running\"\n\nThere is no information about updating or putting a new metadata file to S3, so you can't create an event if nothing happens to the bucket.\nAlso there is no way to push from s3 to ec2 instance, that's not the case\nD) is incomplete","upvote_count":"2"},{"upvote_count":"3","comment_id":"732328","poster":"SmileyCloud","content":"Selected Answer: B\nWhile A and C seem straightforward, it doesn't say who's gonna trigger the S3 event notification. This event happens only if there is a change in S3, not EC2. \nLooks strange, but B is the correct answer.","timestamp":"1669875960.0"},{"upvote_count":"3","comment_id":"729114","poster":"Maygam","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/UsingAlarmActions.html","timestamp":"1669641120.0"}],"unix_timestamp":1669641120,"answer_images":[],"question_text":"A company runs an application on one Amazon EC2 instance. Application metadata is stored in Amazon S3 and must be retrieved if the instance is restarted. The instance must restart or relaunch automatically if the instance becomes unresponsive.\n\nWhich solution will meet these requirements?","question_images":[]}],"exam":{"isBeta":false,"isMCOnly":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":208,"name":"AWS DevOps Engineer Professional","provider":"Amazon","isImplemented":true,"id":35},"currentPage":1},"__N_SSP":true}