{"pageProps":{"questions":[{"id":"PsDxCWs9gzQ6dhFrYv3X","answer_images":[],"answer_description":"","timestamp":"2024-09-05 03:02:00","question_images":[],"choices":{"C":"Configure Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to send data directly to a Redshift provisioned cluster table.","D":"Use Amazon Redshift streaming ingestion from Kinesis Data Streams and to present data as a materialized view.","B":"Set up an Amazon Kinesis Data Firehose delivery stream to send data to Amazon S3. Configure a Redshift provisioned cluster to load data every minute.","A":"Set up an Amazon Kinesis Data Firehose delivery stream to send data to a Redshift provisioned cluster table."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/146967-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"D","answer":"D","unix_timestamp":1725498120,"exam_id":21,"discussion":[{"poster":"dashapetr","content":"Selected Answer: D\nAmazon Redshift supports streaming ingestion from Amazon Kinesis Data Streams. The Amazon Redshift streaming ingestion feature provides low-latency, high-speed ingestion of streaming data from Amazon Kinesis Data Streams into an Amazon Redshift materialized view. Amazon Redshift streaming ingestion removes the need to stage data in Amazon S3before ingesting into Amazon Redshift.\n\nlink: https://docs.aws.amazon.com/streams/latest/dev/using-other-services-redshift.html","upvote_count":"2","comment_id":"1278716","timestamp":"1725515760.0"},{"upvote_count":"1","comment_id":"1278568","timestamp":"1725498120.0","poster":"EJGisME","content":"Selected Answer: D\nD. Use Amazon Redshift streaming ingestion from Kinesis Data Streams and to present data as a materialized view."}],"topic":"1","question_id":51,"question_text":"A technology company currently uses Amazon Kinesis Data Streams to collect log data in real time. The company wants to use Amazon Redshift for downstream real-time queries and to enrich the log data.\n\nWhich solution will ingest data into Amazon Redshift with the LEAST operational overhead?","answers_community":["D (100%)"]},{"id":"v5QOi5ue0qH9cz3eaZQQ","unix_timestamp":1726704000,"answer_images":[],"answer":"A","choices":{"A":"Use an AWS Database Migration Service (AWS DMS) full load plus CDC job to load tables that contain monotonically increasing data columns from the on-premises data warehouse to Amazon S3. Use custom logic in AWS Glue to append the daily incremental data to a full-load copy that is in Amazon S3.","D":"Use AWS Glue to load a full copy of the data warehouse tables into Amazon S3 every day. Overwrite the previous day's full-load copy every day.","C":"Use an AWS Database Migration Service (AWS DMS) full load migration to load the data warehouse tables into Amazon S3 every day. Overwrite the previous day's full-load copy every day.","B":"Use an AWS Glue Java Database Connectivity (JDBC) connection. Configure a job bookmark for a column that contains monotonically increasing values. Write custom logic to append the daily incremental data to a full-load copy that is in Amazon S3."},"topic":"1","question_images":[],"timestamp":"2024-09-19 02:00:00","answer_ET":"A","exam_id":21,"question_id":52,"discussion":[{"content":"Answer:A\nUse an AWS Database Migration Service (AWS DMS) full load plus CDC job to load tables that contain monotonically increasing data columns from the on-premises data warehouse to Amazon S3.","comment_id":"1303473","poster":"Parandhaman_Margan","upvote_count":"1","timestamp":"1730001540.0"},{"comment_id":"1290748","poster":"LR2023","timestamp":"1727548920.0","upvote_count":"1","content":"Selected Answer: C\nA seems to be an overkill using custom logic"},{"comment_id":"1286062","timestamp":"1726704000.0","upvote_count":"4","poster":"Fawk","content":"Selected Answer: A\nDMS is definitely the service, and C is obviously wrong"}],"question_text":"A company maintains a data warehouse in an on-premises Oracle database. The company wants to build a data lake on AWS. The company wants to load data warehouse tables into Amazon S3 and synchronize the tables with incremental data that arrives from the data warehouse every day.\n\nEach table has a column that contains monotonically increasing values. The size of each table is less than 50 GB. The data warehouse tables are refreshed every night between 1 AM and 2 AM. A business intelligence team queries the tables between 10 AM and 8 PM every day.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?","answers_community":["A (80%)","C (20%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/147821-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true},{"id":"hgCWfMTVT0ilpNJSphI6","discussion":[{"timestamp":"1725516840.0","poster":"dashapetr","upvote_count":"8","comment_id":"1278727","content":"Selected Answer: C\nC: You can use S3 as a target and configure files to be in Parquet format https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html"},{"timestamp":"1731675540.0","content":"Selected Answer: C\nA and D wrong. B also wrong because Bookmark is used to mantain files that you don't want to re-analyze in case of a re-run about a glue job.","poster":"michele_scar","upvote_count":"1","comment_id":"1312632"},{"comment_id":"1279578","timestamp":"1725628740.0","upvote_count":"1","content":"Selected Answer: B\nVOTE B","comments":[{"comment_id":"1285741","timestamp":"1726664880.0","upvote_count":"2","content":"You can no see any updates to the data after the records are ingested by Glue. B is not correct","poster":"PashoQ"}],"poster":"siheom"}],"url":"https://www.examtopics.com/discussions/amazon/view/146993-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"choices":{"D":"Create an Oracle database in Amazon RDS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises Oracle database to Amazon RDS. Configure triggers on the tables to invoke AWS Lambda functions to write changed records to Amazon S3 in Parquet format.","C":"Create an AWS Database Migration Service (AWS DMS) task for ongoing replication. Set the Oracle database as the source. Set Amazon S3 as the target. Configure the task to write the data in Parquet format.","A":"Create an Apache Sqoop job in Amazon EMR to read the data from the Oracle database. Configure the Sqoop job to write the data to Amazon S3 in Parquet format.","B":"Create an AWS Glue connection to the Oracle database. Create an AWS Glue bookmark job to ingest the data incrementally and to write the data to Amazon S3 in Parquet format."},"answer_description":"","question_id":53,"timestamp":"2024-09-05 08:14:00","answer_images":[],"answers_community":["C (90%)","10%"],"answer":"C","unix_timestamp":1725516840,"answer_ET":"C","topic":"1","question_images":[],"question_text":"A company is building a data lake for a new analytics team. The company is using Amazon S3 for storage and Amazon Athena for query analysis. All data that is in Amazon S3 is in Apache Parquet format.\n\nThe company is running a new Oracle database as a source system in the companyâ€™s data center. The company has 70 tables in the Oracle database. All the tables have primary keys. Data can occasionally change in the source system. The company wants to ingest the tables every day into the data lake.\n\nWhich solution will meet this requirement with the LEAST effort?","isMC":true},{"id":"56sTPw8xF2S7bPyXdOgu","answer":"B","timestamp":"2024-09-08 09:38:00","unix_timestamp":1725781080,"discussion":[{"upvote_count":"2","content":"Selected Answer: B\nKPL automatically batches and aggregates multiple records into a single payload before sending them to Kinesis Data Streams. This reduces the number of records sent and optimizes shard throughput usage.","comment_id":"1330817","poster":"HagarTheHorrible","timestamp":"1734959520.0"},{"poster":"EJGisME","content":"Selected Answer: B\nB. Kinesis Producer Library (KPL)","comment_id":"1280260","timestamp":"1725781080.0","upvote_count":"3"}],"question_text":"A transportation company wants to track vehicle movements by capturing geolocation records. The records are 10 bytes in size. The company receives up to 10.000 records every second. Data transmission delays of a few minutes are acceptable because of unreliable network conditions.\n\nThe transportation company wants to use Amazon Kinesis Data Streams to ingest the geolocation data. The company needs a reliable mechanism to send data to Kinesis Data Streams. The company needs to maximize the throughput efficiency of the Kinesis shards.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/147168-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"question_images":[],"answer_ET":"B","exam_id":21,"answers_community":["B (100%)"],"question_id":54,"isMC":true,"answer_description":"","choices":{"B":"Kinesis Producer Library (KPL)","A":"Kinesis Agent","D":"Kinesis SDK","C":"Amazon Kinesis Data Firehose"}},{"id":"iMNFkht8C1aqZAUngi6K","choices":{"C":"Use Amazon Neptune ML and an Apache Gremlin script to remove duplicate records.","A":"Use the FindMatches feature of AWS Glue to remove duplicate records.","D":"Use the global tables feature of Amazon DynamoDB to prevent duplicate data.","B":"Use non-Windows functions in Amazon Athena to remove duplicate records."},"answer":"A","unix_timestamp":1726704240,"url":"https://www.examtopics.com/discussions/amazon/view/147823-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"An investment company needs to manage and extract insights from a volume of semi-structured data that grows continuously.\n\nA data engineer needs to deduplicate the semi-structured data, remove records that are duplicates, and remove common misspellings of duplicates.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"discussion":[{"comment_id":"1358479","poster":"italiancloud2025","timestamp":"1739916660.0","content":"Selected Answer: A\nA: SÃ­, porque AWS Glue FindMatches utiliza machine learning para deduplicar datos y corregir errores ortogrÃ¡ficos con mÃ­nima sobrecarga operativa.\nB: No, usar Athena requiere escribir consultas manuales y no maneja bien las variaciones de escritura.\nC: No, Neptune ML estÃ¡ orientado a anÃ¡lisis en grafos, no a la deduplicaciÃ³n de datos semi-estructurados.\nD: No, global tables en DynamoDB se usan para replicaciÃ³n, no para eliminar duplicados.","upvote_count":"1"},{"content":"Selected Answer: A\nA - The other options are dumb and hardly make sense","poster":"Fawk","timestamp":"1726704240.0","upvote_count":"2","comment_id":"1286065"}],"question_images":[],"exam_id":21,"topic":"1","isMC":true,"question_id":55,"answer_ET":"A","answer_description":"","answers_community":["A (100%)"],"timestamp":"2024-09-19 02:04:00"}],"exam":{"provider":"Amazon","numberOfQuestions":207,"isImplemented":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","lastUpdated":"11 Apr 2025","id":21,"isMCOnly":true,"isBeta":false},"currentPage":11},"__N_SSP":true}