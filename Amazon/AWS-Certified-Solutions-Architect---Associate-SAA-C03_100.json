{"pageProps":{"questions":[{"id":"niFA8tr8xXOQflK98Q24","answer_images":[],"discussion":[{"timestamp":"1690176360.0","comment_id":"961147","content":"Selected Answer: D\nAuto Scaling groups can span Availability Zones, but not AWS regions.\nHence the best option is to deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed.","poster":"TariqKipkemei","upvote_count":"21"},{"content":"Selected Answer: D\nA: Not possible for autoscaling across regions\nBC: Using PostgreSQL, not sure why?\nD: MOST fault tolerant != MOST scalable. This gives least downtime.","upvote_count":"4","timestamp":"1704922320.0","comment_id":"1119052","poster":"awsgeek75"},{"content":"Selected Answer: D\nEC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions","upvote_count":"3","timestamp":"1699236840.0","poster":"potomac","comment_id":"1063464"},{"upvote_count":"4","comments":[{"poster":"wsdasdasdqwdaw","comment_id":"1056839","upvote_count":"2","timestamp":"1698586080.0","content":"Simple as that."}],"comment_id":"1049523","content":"527: \nD is correct:\n- B & C is not correct because it mentions Aurora PostgreSQL which is not mentioned in the question\n- A is not correct because Auto scaling group can not span regions","poster":"thanhnv142","timestamp":"1697895360.0"},{"content":"Selected Answer: D\nUsing an Aurora global database that spans both the primary and secondary regions provides automatic replication and failover capabilities for the database tier.\nDeploying the web and application tiers to a second region provides fault tolerance for those components.\nUsing Route53 health checks and failover routing will route traffic to the secondary region if the primary region becomes unavailable.\nThis provides fault tolerance across all tiers of the architecture while minimizing downtime. Promoting the secondary database to primary ensures the second region can continue operating if needed.\nA is close, but doesn't provide an automatic database failover capability.\nB and C provide database replication, but not automatic failover.\nSo D is the most comprehensive and fault tolerant architecture.","poster":"Guru4Cloud","timestamp":"1692725100.0","upvote_count":"4","comment_id":"987639","comments":[{"poster":"JA2018","content":"PostgreSQL in options b & C could be a distractor.... PostgreSQL not mentioned in stem","timestamp":"1732339140.0","upvote_count":"1","comment_id":"1316568"}]},{"comment_id":"946796","content":"Selected Answer: D\nAnswer D","timestamp":"1688861580.0","poster":"Zox42","upvote_count":"2"},{"timestamp":"1687962480.0","comment_id":"936769","content":"Selected Answer: D\nD seems fitting: Global Databbase and deploying it in the new region","poster":"Zuit","upvote_count":"2"},{"timestamp":"1687261380.0","upvote_count":"1","content":"Selected Answer: B\nB is correct!","comments":[{"timestamp":"1687907940.0","content":"Replicated db doesnt mean they will act as a single db once the transfer is completed. Global db is the correct approach","poster":"manuh","upvote_count":"2","comments":[],"comment_id":"935899"}],"poster":"MrAWSAssociate","comment_id":"928414"},{"poster":"r3mo","timestamp":"1686596700.0","content":"\"D\" is the answer: because Aws Aurora Global Database allows you to read and write from any region in the global cluster. This enables you to distribute read and write workloads globally, improving performance and reducing latency. Data is replicated synchronously across regions, ensuring strong consistency.","comment_id":"921672","upvote_count":"4"},{"poster":"Henrytml","comment_id":"920643","timestamp":"1686485340.0","content":"Selected Answer: A\nA is the only answer remain using ELB, both Web/App/DB has been taking care with replicating in 2nd region, lastly route 53 for failover over multiple regions","comments":[{"content":"i will revoke my answer to standby web in 2nd region, instead of trigger to scale out","poster":"Henrytml","comment_id":"925162","timestamp":"1686919260.0","upvote_count":"1"},{"comment_id":"935898","upvote_count":"2","content":"also Asg cant span beyond a region","timestamp":"1687907880.0","poster":"manuh"}],"upvote_count":"1"},{"comment_id":"917473","content":"Selected Answer: D\nB&C are discarted.\nThe answer is between A and D. \nI would go with D because it explicitley created this web / app tier in second region, instead A just autoscales into a secondary region, rather then always having resources in this second region.","upvote_count":"4","poster":"alexandercamachop","comments":[{"poster":"JA2018","content":"as mentioned ASG cannot span across different AWS regions","upvote_count":"1","comment_id":"1316570","timestamp":"1732339200.0"}],"timestamp":"1686160620.0"}],"answer_description":"","question_text":"A company has a regional subscription-based streaming service that runs in a single AWS Region. The architecture consists of web servers and application servers on Amazon EC2 instances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The architecture includes an Amazon Aurora global database cluster that extends across multiple Availability Zones.\n\nThe company wants to expand globally and to ensure that its application has minimal downtime.\n\nWhich solution will provide the MOST fault tolerance?","question_images":[],"question_id":496,"choices":{"C":"Deploy the web tier and the application tier to a second Region. Create an Aurora PostgreSQL database in the second Region. Use AWS Database Migration Service (AWS DMS) to replicate the primary database to the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region.","A":"Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in Availability Zones in a second Region. Use an Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region.","D":"Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed.","B":"Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL cross-Region Aurora Replica in the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed."},"unix_timestamp":1686160620,"topic":"1","isMC":true,"answers_community":["D (95%)","2%"],"url":"https://www.examtopics.com/discussions/amazon/view/111428-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer_ET":"D","answer":"D","timestamp":"2023-06-07 19:57:00"},{"id":"str6h22iYCpbl2V0jYMK","url":"https://www.examtopics.com/discussions/amazon/view/111317-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"choices":{"A":"Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in Amazon S3 Glacier Flexible Retrieval. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the objects nightly from S3 Glacier Flexible Retrieval. Delete the objects after the job has processed the objects.","D":"Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive.","C":"Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use an Amazon S3 event notification when each file arrives to invoke the job in AWS Batch. Delete the files after the job has processed the files.","B":"Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files nightly from the EBS volume. Delete the files after the job has processed the files."},"timestamp":"2023-06-06 22:59:00","question_id":497,"isMC":true,"unix_timestamp":1686085140,"question_text":"A data analytics company wants to migrate its batch processing system to AWS. The company receives thousands of small data files periodically during the day through FTP. An on-premises batch job processes the data files overnight. However, the batch job takes hours to finish running.\n\nThe company wants the AWS solution to process incoming data files as soon as possible with minimal changes to the FTP clients that send the files. The solution must delete the incoming data files after the files have been processed successfully. Processing for each file needs to take 3-8 minutes.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?","discussion":[{"comments":[{"content":"well if you opt for unmanaged compute option in AWS Batch in that case you dont have to worry about the compute, AWS batch automatically provisions it for you","comment_id":"1204533","upvote_count":"2","timestamp":"1730294400.0","poster":"wizcloudifa"}],"poster":"pentium75","timestamp":"1719832380.0","comment_id":"1111178","upvote_count":"5","content":"Selected Answer: D\nObviously we choose AWS Transfer Family over hosting the FTP server ourselves on an EC2 instance. And \"process incoming data files as soon as possible\" -> trigger Lambda when files arrive. Lambda functions can run up to 15 minutes, it takes \"3-8 minutes\" per file -> works.\n\nAWS Batch just schedules jobs, but these still need to run somewhere (Lambda, Fargate, EC2)."},{"comment_id":"1056843","poster":"wsdasdasdqwdaw","content":"FTP => AWS Transfer Family, => C or D, but in C is used EBS not S3 which needs EC2 and in general is more complex => very clear D.","timestamp":"1714390320.0","upvote_count":"2"},{"poster":"Guru4Cloud","timestamp":"1708628340.0","content":"Selected Answer: D\nThe key points:\n\nUse AWS Transfer Family for the FTP server to receive files directly into S3. This avoids managing FTP servers.\nProcess each file as soon as it arrives using Lambda triggered by S3 events. Lambda provides fast processing time per file.\nLambda can also delete files after processing succeeds.\nOptions A, B, C involve more operational overhead of managing FTP servers and batch jobs. Processing latency would be higher waiting for batch windows.\nStoring files in Glacier (Option A) adds latency for retrieving files.","comment_id":"987627","upvote_count":"2"},{"timestamp":"1706275320.0","comment_id":"963648","upvote_count":"2","poster":"hsinchang","content":"Selected Answer: D\nProcessing for each file needs to take 3-8 minutes clearly indicates Lambda functions."},{"comment_id":"961158","timestamp":"1706082000.0","upvote_count":"4","content":"Selected Answer: D\nProcess incoming data files with minimal changes to the FTP clients that send the files = AWS Transfer Family.\nProcess incoming data files as soon as possible = S3 event notification.\nProcessing for each file needs to take 3-8 minutes = AWS Lambda function.\nDelete file after processing = AWS Lambda function.","poster":"TariqKipkemei"},{"content":"Selected Answer: D\nMost likely D.","upvote_count":"2","poster":"antropaws","comment_id":"930519","timestamp":"1703256420.0"},{"upvote_count":"2","comment_id":"921674","content":"\"D\" Since each file takes 3-8 minutes to process the lambda function can process the data file whitout a problem.","poster":"r3mo","timestamp":"1702415400.0"},{"comment_id":"921340","timestamp":"1702384260.0","comments":[{"upvote_count":"2","content":"https://aws.amazon.com/aws-transfer-family/","poster":"oras2023","comment_id":"923328","timestamp":"1702574220.0"}],"content":"Selected Answer: D\nYou cannot setup AWS Transfer Family to save files into EBS.","poster":"maver144","upvote_count":"3"},{"comment_id":"921285","upvote_count":"2","poster":"secdgs","timestamp":"1702380180.0","content":"Selected Answer: D\nD. Because \n1. process immediate when file transfer to S3 not wait for process several file in one time.\n2. takes 3-8 can use Lamda.\n\nC. Wrong because AWS Batch is use for run large-scale or large amount of data in one time."},{"comment_id":"918374","poster":"Aymanovitchy","timestamp":"1702054440.0","upvote_count":"2","content":"To meet the requirements of processing incoming data files as soon as possible with minimal changes to the FTP clients, and deleting the files after successful processing, the most operationally efficient solution would be:\n\nD. Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and delete them after processing. Use an S3 event notification to invoke the Lambda function when the files arrive."},{"timestamp":"1702032000.0","content":"Selected Answer: D\nIt should be D as lambda is more operationally viable solution given the fact each processing takes 3-8 minutes that lambda can handle","poster":"bajwa360","upvote_count":"2","comment_id":"918023"},{"poster":"alexandercamachop","comment_id":"917477","timestamp":"1701979080.0","content":"Selected Answer: C\nAnswer has to be between C or D. \nBecause Transfer Family is obvious do to FTP.\nNow i would go with C because it uses AWS Batch, which makes more sense for Batch processing rather then AWS Lambda.","comments":[{"timestamp":"1719832320.0","comment_id":"1111177","upvote_count":"2","content":"Why? \"Process incoming data files as soon as possible\", by triggering the Lambda function when files arrive. Batch is for scheduled jobs.","poster":"pentium75"}],"upvote_count":"1"},{"content":"I am between C and D. My reason is: \n\n\"The company wants the AWS solution to process incoming data files <b>as soon as possible</b> with minimal changes to the FTP clients that send the files.\"","poster":"Bill1000","timestamp":"1701903540.0","comment_id":"916644","upvote_count":"3"}],"answers_community":["D (96%)","4%"],"answer":"D","answer_description":"","exam_id":31,"answer_ET":"D","question_images":[],"topic":"1"},{"id":"zNvdApqHbGFQY3rZtN30","answers_community":["B (100%)"],"topic":"1","choices":{"A":"Migrate the databases to Amazon EC2. Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.","D":"Migrate the database to Amazon RDS. Use Amazon CloudWatch Logs for data security and protection.","C":"Migrate the data to Amazon S3 Use Amazon Macie for data security and protection","B":"Migrate the databases to Amazon RDS Configure encryption at rest."},"answer":"B","question_id":498,"timestamp":"2023-06-06 09:26:00","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/111246-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer_ET":"B","unix_timestamp":1686036360,"question_text":"A company is migrating its workloads to AWS. The company has transactional and sensitive data in its databases. The company wants to use AWS Cloud solutions to increase security and reduce operational overhead for the databases.\n\nWhich solution will meet these requirements?","answer_images":[],"question_images":[],"discussion":[{"timestamp":"1686036360.0","poster":"AshishRocks","upvote_count":"11","content":"B is the answer\nWhy not C - Option C suggests migrating the data to Amazon S3 and using Amazon Macie for data security and protection. While Amazon Macie provides advanced security features for data in S3, it may not be directly applicable or optimized for databases, especially for transactional and sensitive data. Amazon RDS provides a more suitable environment for managing databases.","comment_id":"916007"},{"poster":"oras2023","upvote_count":"5","timestamp":"1686057240.0","content":"Selected Answer: B\nB. Migrate the databases to Amazon RDS Configure encryption at rest.\nLooks like best option","comment_id":"916276"},{"timestamp":"1731757860.0","content":"Selected Answer: B\nS3 and Macie works too. However, the question mentioned about transactional data. RDS is much better for running transactional workloads. Thus, RDS is the correct answer here.","comment_id":"1313050","upvote_count":"2","poster":"venomblade"},{"content":"Selected Answer: B\nA: Operational overhead of EC2 and whatever DB is running on it\nC: Macie is not for data security, it's for identifying PII and sensitive data\nD: CloudWatch is for cloud events and does not secure databases\nB: RDS is managed so least operational overhead. Encryption at rest means security","comment_id":"1119054","timestamp":"1704922620.0","upvote_count":"3","poster":"awsgeek75"},{"comment_id":"987620","comments":[{"comment_id":"1039042","timestamp":"1696896540.0","content":"down voted.","poster":"wendaz","upvote_count":"1"}],"upvote_count":"4","content":"Selected Answer: B\nMigrate the databases to Amazon RDS Configure encryption at rest.","poster":"Guru4Cloud","timestamp":"1692722760.0"},{"comment_id":"961159","poster":"TariqKipkemei","upvote_count":"4","timestamp":"1690177380.0","content":"Selected Answer: B\nReduce Ops = Migrate the databases to Amazon RDS Configure encryption at rest"},{"poster":"alexandercamachop","content":"Selected Answer: B\nB for sure.\nFirst the correct is Amazon RDS, then encryption at rest makes the database secure.","comment_id":"917480","upvote_count":"4","timestamp":"1686160740.0"}],"isMC":true},{"id":"6tnabHZ0w1JqDTJa2kNp","exam_id":31,"timestamp":"2022-10-15 19:05:00","choices":{"D":"Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 years.","A":"Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion of the records for a period of 10 years.","C":"Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.","B":"Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. After 10 years, change the IAM policy to allow deletion."},"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/85532-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C","unix_timestamp":1665853500,"answer_ET":"C","answer_images":[],"question_id":499,"answer_description":"","isMC":true,"question_text":"A company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9 years. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be stored with maximum resiliency.\nWhich solution will meet these requirements?","topic":"1","discussion":[{"poster":"awsgeek75","upvote_count":"22","comment_id":"1122572","timestamp":"1705240680.0","content":"Selected Answer: C\nOnly CD provides Object Lock options which is required for stopping admin/root users from deleting.\nD is governance mode which is like government, pay enough money and you can do anything. This is not what we want so compliance is the option.\nC is right choice.\n\nFor future, remember\nS3 Lock Governance = corrupt government official\nS3 Lock Compliance = honest solution architect!"},{"content":"Selected Answer: C\nThe key reasons are:\n\nThe S3 Lifecycle policy transitions the data to Glacier Deep Archive after 1 year for long-term archival.\nS3 Object Lock in compliance mode prevents any user from deleting or overwriting objects for the specified retention period.\nGlacier Deep Archive provides very high durability and the lowest storage cost for long-term archival.\nCompliance mode ensures no one can override or change the retention settings even if policies change.\nThis meets all the requirements - immediate access for 1 year, archived for 9 years, unable to delete for 10 years, maximum resiliency","upvote_count":"9","poster":"Guru4Cloud","timestamp":"1691597520.0","comment_id":"976825"},{"upvote_count":"1","timestamp":"1735745400.0","poster":"satyaammm","comment_id":"1335215","content":"Selected Answer: C\nS3 lifecycle policies are required to transition from immediate availability to archived form. Also the S3 object lock helps prevent deletion of the bucket."},{"upvote_count":"2","poster":"PaulGa","timestamp":"1726055880.0","content":"Selected Answer: C\nAns C - S3 Glacier after year 1 in compliance mode with object lock (=immutable lock)","comment_id":"1282067"},{"poster":"Ruffyit","comment_id":"1055093","upvote_count":"5","content":"No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period = Compliance Mode","timestamp":"1698379200.0"},{"timestamp":"1694406180.0","poster":"axelrodb","content":"Selected Answer: C\nTo meet the requirements of immediately accessible records for 1 year and then archived for an additional 9 years with maximum resiliency, we can use S3 Lifecycle policy to transition records from S3 Standard to S3 Glacier Deep Archive after 1 year. And to ensure that the records cannot be deleted by anyone, including administrative and root users, we can use S3 Object Lock in compliance mode for a period of 10 years. Therefore, the correct answer is option C.\nReference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.htmls","comment_id":"1004439","upvote_count":"5"},{"comment_id":"975164","content":"Selected Answer: C\nNo one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period = Compliance Mode","poster":"TariqKipkemei","timestamp":"1691468160.0","upvote_count":"2"},{"upvote_count":"2","content":"Option C is the correct answer","timestamp":"1689779460.0","comment_id":"956752","poster":"miki111"},{"comment_id":"944872","timestamp":"1688663520.0","poster":"MutiverseAgent","comments":[{"comment_id":"1040203","poster":"dhax12","timestamp":"1696999560.0","content":"Put entire 10 years to Glacier means it's not accessible for the 1 year window. Hence wrong answer.","upvote_count":"2"}],"upvote_count":"1","content":"Why not A? Move all files to S3 Glacier instant retrieval (Cheaper than S3) and then move files older than a year to S3 Deep archive."},{"upvote_count":"4","comment_id":"929372","poster":"cookieMr","timestamp":"1687344960.0","content":"Selected Answer: C\nTo prevent deletion of records during the entire 10-year period, you can utilize S3 Object Lock feature. By enabling it in compliance mode, you can set a retention period on the objects, preventing any user, including administrative and root users, from deleting records.\n\nA: S3 Glacier is suitable for long-term archival, it may not provide immediate accessibility for the first year as required.\n\nB: Intelligent-Tiering may not offer the most cost-effective archival storage option for extended 9-year period. Changing the IAM policy after 10 years to allow deletion also introduces manual steps and potential human error.\n\nD: While S3 One Zone-IA can provide cost savings, it doesn't offer the same level of resiliency as S3 Glacier Deep Archive for long-term archival."},{"content":"Selected Answer: C\nIn compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html","poster":"11pantheman11","upvote_count":"4","comment_id":"882085","timestamp":"1682552040.0"},{"content":"Selected Answer: C\nRetention Period: A period is specified by Days & Years.\nWith Retention Compliance Mode, you can’t change/adjust (even by the account root user) the retention mode during the retention period while all objects within the bucket are Locked.\nWith Retention Governance mode, a less restrictive mode, you can grant special permission to a group of users to adjust the Lock settings by using S3:BypassGovernanceRetention.\n\nLegal Hold: It’s On/Off setting on an object version. There is no retention period. If you enable Legal Hole on specific object version, you will not be able to delete or override that specific object version. It needs S:PutObjectLegalHole as a permission.","poster":"athiha","upvote_count":"5","comment_id":"835742","timestamp":"1678523580.0"},{"comment_id":"823083","content":"Selected Answer: C\nS3 Glacier Deep Archive all day....","poster":"WherecanIstart","upvote_count":"1","timestamp":"1677458220.0"},{"poster":"SilentMilli","content":"Selected Answer: C\nUse an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.","upvote_count":"2","timestamp":"1673056260.0","comment_id":"768196"},{"content":"Selected Answer: C\nUse S3 Object Lock in compliance mode\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html","timestamp":"1671873540.0","poster":"k1kavi1","upvote_count":"4","comment_id":"754784"},{"upvote_count":"2","comment_id":"752902","poster":"pazabal","content":"Selected Answer: C\nC, A lifecycle set to transition from standard to Glacier deep archive and use lock for the delete requirement \nA, B and D don't meet the requirements","timestamp":"1671670800.0"},{"upvote_count":"4","timestamp":"1671552420.0","content":"Selected Answer: C\nC. Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.\n\nTo meet the requirements, the company could use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. S3 Glacier Deep Archive is Amazon's lowest-cost storage class, specifically designed for long-term retention of data that is accessed rarely. This would allow the company to store the records with maximum resiliency and at the lowest possible cost.","comments":[{"poster":"Buruguduystunstugudunstuy","timestamp":"1671552420.0","comment_id":"751166","upvote_count":"2","content":"To ensure that the records are not deleted during the entire 10-year period, the company could use S3 Object Lock in compliance mode. S3 Object Lock allows the company to apply a retention period to objects in S3, preventing the objects from being deleted until the retention period expires. By using S3 Object Lock in compliance mode, the company can ensure that the records are not deleted by anyone, including administrative users and root users, during the entire 10-year period."}],"comment_id":"751165","poster":"Buruguduystunstugudunstuy"},{"comment_id":"749737","content":"Selected Answer: C\nA and B are ruled out as you need them to be accessible for 1 year and using control policy or IAM policies, the administrator or root still has the ability to delete them.\nD is ruled out as it uses One Zone-IA, but requirement says max- resiliency. \nSO- C should be the right answer.","poster":"Nandan747","timestamp":"1671447000.0","upvote_count":"5"},{"comment_id":"749413","content":"Selected Answer: C\nOption C","poster":"career360guru","upvote_count":"1","timestamp":"1671420600.0"},{"comment_id":"743396","content":"Selected Answer: C\nThey should've put Glacier Vault Lock into Option C to make it even more obvious","upvote_count":"1","poster":"Marge_Simpson","timestamp":"1670885820.0"},{"comment_id":"737496","content":"Selected Answer: C\nC is the answer that fulfill the requirements of immediate access for one year and data durability for 10 years","upvote_count":"3","poster":"AlaN652","timestamp":"1670394900.0"},{"timestamp":"1669038960.0","upvote_count":"1","comment_id":"723570","content":"C is correct","poster":"Wpcorgan"},{"poster":"airraid2010","upvote_count":"3","timestamp":"1667726160.0","comment_id":"712221","content":"Selected Answer: C\nA-Wrong as the records must be immediately accessble for the first year.\nB-The question never mentioned about the records can be deleted or modified after 10-year period.\nD-It does not fulfill the condition of securing resiliency; you need multi-AZ to guarantee it.\n\nTherefore, the answer is C."},{"comment_id":"710123","content":"Selected Answer: C\nans is C","poster":"17Master","timestamp":"1667430420.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"700474","timestamp":"1666320420.0","poster":"BoboChow","content":"Selected Answer: C\nsure for C"},{"comment_id":"697632","content":"CCCCCCCCC","poster":"queen101","timestamp":"1666032060.0","upvote_count":"1"},{"poster":"ninjawrz","content":"Selected Answer: C\nThis is C","timestamp":"1665859800.0","comment_id":"695616","upvote_count":"1"},{"timestamp":"1665853500.0","upvote_count":"5","content":"Selected Answer: C\ncompliance lock cant be removed unlike governance","poster":"Rachness","comment_id":"695546"}],"answers_community":["C (100%)"]},{"id":"096ooJiiDSuKSWPAfInK","answers_community":["C (100%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/111271-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1686057360,"choices":{"B":"Replace the NLBs with Application Load Balancers (ALBs). Configure Route 53 to use latency-based routing.","C":"Add AWS Global Accelerator in front of the NLBs. Configure a Global Accelerator endpoint to use the correct listener ports.","D":"Add an Amazon API Gateway endpoint behind the NLBs. Enable API caching. Override method caching for the different stages.","A":"Add an Amazon CloudFront distribution in front of the NLBs. Increase the Cache-Control max-age parameter."},"discussion":[{"poster":"Guru4Cloud","upvote_count":"6","content":"Selected Answer: C\nThe key considerations are:\n\nThe application uses TCP and UDP for multiplayer gaming, so Network Load Balancers (NLBs) are appropriate.\nAWS Global Accelerator can be added in front of the NLBs to improve performance and reduce latency by intelligently routing traffic across AWS Regions and Availability Zones.\nGlobal Accelerator provides static anycast IP addresses that act as a fixed entry point to application endpoints in the optimal AWS location. This improves availability and reduces latency.\nThe Global Accelerator endpoint can be configured with the correct NLB listener ports for TCP and UDP.","timestamp":"1708626360.0","comment_id":"987604"},{"content":"Selected Answer: C\nTCP ,UDP, Gaming = global accelerator and Network Load Balancer","comment_id":"961164","timestamp":"1706082360.0","poster":"TariqKipkemei","upvote_count":"5"},{"content":"Selected Answer: C\nA: CloudFront is for caching. Not required\nB: ALB is for HTTP layer, won't help with TCP UDP issues\nD: API Gateway, API Caching total rubbish, ignore this option\nC: Is correct as Global Accelerator uses unicast for reducing latency globbally.","timestamp":"1720640520.0","upvote_count":"2","comment_id":"1119058","poster":"awsgeek75"},{"poster":"Henrytml","comments":[{"upvote_count":"1","timestamp":"1703726760.0","poster":"manuh","content":"Does alb handle udp? Can u share a source?","comment_id":"935902"}],"content":"Selected Answer: C\nonly b and c handle TCP/UDP, and C comes with accelerator to enhance performance","comment_id":"925172","upvote_count":"2","timestamp":"1702738080.0"},{"poster":"alexandercamachop","upvote_count":"3","content":"Selected Answer: C\nUDP and TCP is AWS Global accelarator as it works in the Transportation layer.\nNow this with NLB is perfect.","timestamp":"1701979200.0","comment_id":"917482"},{"comment_id":"916278","timestamp":"1701875760.0","poster":"oras2023","upvote_count":"3","content":"Selected Answer: C\nC is helping to reduce latency for end clients"}],"timestamp":"2023-06-06 15:16:00","question_images":[],"question_id":500,"question_text":"A company has an online gaming application that has TCP and UDP multiplayer gaming capabilities. The company uses Amazon Route 53 to point the application traffic to multiple Network Load Balancers (NLBs) in different AWS Regions. The company needs to improve application performance and decrease latency for the online game in preparation for user growth.\n\nWhich solution will meet these requirements?","topic":"1","isMC":true,"exam_id":31,"answer_ET":"C","answer":"C","answer_images":[]}],"exam":{"isMCOnly":true,"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"isBeta":false,"provider":"Amazon","id":31},"currentPage":100},"__N_SSP":true}