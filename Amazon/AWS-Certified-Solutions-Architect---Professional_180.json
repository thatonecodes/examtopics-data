{"pageProps":{"questions":[{"id":"mLrWixRV45OWzBszcTvQ","timestamp":"2022-09-05 18:22:00","unix_timestamp":1662394920,"answers_community":["ABD (100%)"],"question_text":"A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances, Amazon Elastic File System\n(Amazon EFS) file systems, and Amazon RDS DB instances.\nTo meet regulatory and business requirements, the company must make the following changes for data backups:\n* Backups must be retained based on custom daily, weekly, and monthly requirements.\n* Backups must be replicated to at least one other AWS Region immediately after capture.\n* The backup solution must provide a single source of backup status across the AWS environment.\n* The backup solution must send immediate notifications upon failure of any resource backup.\nWhich combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)","isMC":true,"answer_images":[],"question_images":[],"exam_id":32,"answer_description":"","choices":{"D":"Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED.","A":"Create an AWS Backup plan with a backup rule for each of the retention requirements","B":"Configure an AWS Backup plan to copy backups to another Region.","E":"Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.","C":"Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.","F":"Setup RDS snapshots on each database."},"discussion":[{"timestamp":"1662394920.0","poster":"AwsBRFan","comment_id":"660353","content":"Selected Answer: ABD\nCross region with AWS Backup: https://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html","upvote_count":"7"},{"poster":"wassb","comment_id":"697452","content":"Selected Answer: ABD\nhttps://docs.aws.amazon.com/efs/latest/ug/awsbackup.html","timestamp":"1666014960.0","upvote_count":"1"},{"poster":"AwsBRFan","upvote_count":"3","content":"DLM is for EBS and AMI snaps https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html","comment_id":"660356","timestamp":"1662394980.0"}],"answer":"ABD","question_id":896,"answer_ET":"ABD","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/80388-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"NqrdhFWf9qhw2OzqRVYi","unix_timestamp":1662395340,"question_text":"A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to\n20 minutes. A solutions architect wants to revise the architecture for the solution.\nWhich strategy should the solutions architect use?","choices":{"D":"Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours.","A":"Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.","C":"Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.","B":"Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours."},"answer_description":"","answers_community":["C (61%)","B (39%)"],"answer":"C","exam_id":32,"discussion":[{"timestamp":"1663927920.0","poster":"pinhead900","content":"Selected Answer: C\nprobably C, \nstep function could run a scheduled task when triggered by eventbrige, but why would you add that layer of complexity just to run aws batch when you could directly invoke it through eventbridge. \nThe link provided - https://aws.amazon.com/pt/blogs/compute/orchestrating-high-performance-computing-with-aws-step-functions-and-aws-batch/ makes sense only for HPC, this is a single instance that needs to be run","upvote_count":"10","comment_id":"676982"},{"timestamp":"1729054980.0","poster":"nimbus_00","content":"Selected Answer: B\nKey word: Scheduling\nWhile Fargate could work, it is more suited to containerized applications and doesn't offer the batch job scheduling capabilities that AWS Batch provides. AWS Batch also has better handling for compute-intensive, scheduled jobs.","upvote_count":"1","comment_id":"1298557"},{"poster":"rodrod","timestamp":"1694848680.0","upvote_count":"1","comment_id":"1008977","content":"Selected Answer: C\nC is the answer"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/step-functions/latest/dg/connect-batch.html It's B","timestamp":"1674651480.0","poster":"pitakk","comment_id":"787642","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: B\nI'll go with B\nWhy not C-> It says \"Fargate task\" and not \"ECS tasks\" using Fargate.","timestamp":"1672781760.0","poster":"evargasbrz","comment_id":"765083"},{"content":"Selected Answer: C\nA & D don't make sense. As for B --- it's wrong for two reasons. Firstly, this is just a single job ... you don't an added workflow by using StepFunctions. Secondly, Step Functions cannot be scheduled. Even if you want to use it for this case, you would at least need to use CloudWatch Events to trigger the process. C solves everything and is this only feasible option.","upvote_count":"2","poster":"Jonfernz","timestamp":"1667616420.0","comment_id":"711516"},{"content":"Answer B\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduled_tasks.html","comment_id":"705653","comments":[{"content":"Sorry its C via Fargate\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduled_tasks.html","timestamp":"1666880700.0","comment_id":"705655","upvote_count":"1","poster":"fais1985"}],"poster":"fais1985","timestamp":"1666880700.0","upvote_count":"1"},{"timestamp":"1665102000.0","poster":"dcdcdc3","comment_id":"688170","upvote_count":"3","content":"Selected Answer: B\nI would like to go with B:\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/orchestrate-an-etl-pipeline-with-validation-transformation-and-partitioning-using-aws-step-functions.html\n\nas the question is asking for an ETL solution\n\nMaybe the incorrect piece in C is around invoking a \"Fargate task\" every 4 hours where it should be invoke \"ECS task\" (aka your app) every 4 hours","comments":[{"comment_id":"688178","timestamp":"1665102660.0","content":"Or maybe because source code cannot be modified, we are stuck with a fargate container with unclear answer C and we cannot re-architect the solution to B ?.?","poster":"dcdcdc3","upvote_count":"1"}]},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/step-functions/latest/dg/connect-batch.html","upvote_count":"1","comment_id":"677021","timestamp":"1663930440.0","poster":"Yashar1691"},{"upvote_count":"4","comment_id":"673095","timestamp":"1663577580.0","poster":"kapara","content":"Selected Answer: C\nC.\nthose who say B - how can step function state can run schedule task?"},{"comment_id":"664132","timestamp":"1662687420.0","poster":"cale","upvote_count":"1","content":"Selected Answer: B\nIt is B."},{"comment_id":"660362","upvote_count":"1","comments":[],"timestamp":"1662395460.0","content":"Selected Answer: B\nB considering this link https://aws.amazon.com/pt/blogs/compute/orchestrating-high-performance-computing-with-aws-step-functions-and-aws-batch/","poster":"AwsBRFan"},{"comment_id":"660360","poster":"AwsBRFan","timestamp":"1662395340.0","content":"Selected Answer: C\nMaybe C https://aws.amazon.com/pt/blogs/compute/orchestrating-high-performance-computing-with-aws-step-functions-and-aws-batch/","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/80390-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"C","answer_images":[],"timestamp":"2022-09-05 18:29:00","isMC":true,"question_id":897,"question_images":[],"topic":"1"},{"id":"1PyollvvQvwQeppwRbZ9","answers_community":["D (100%)"],"topic":"1","question_text":"You need a persistent and durable storage to trace call activity of an IVR (Interactive Voice Response) system. Call duration is mostly in the 2-3 minutes timeframe. Each traced call can be either active or terminated. An external application needs to know each minute the list of currently active calls. Usually there are a few calls/second, but once per month there is a periodic peak up to 1000 calls/second for a few hours. The system is open 24/7 and any downtime should be avoided. Historical data is periodically archived to files. Cost saving is a priority for this project.\nWhat database implementation would better fit this scenario, keeping costs as low as possible?","discussion":[{"comment_id":"1266979","poster":"amministrazione","content":"D. Use DynamoDB with a \"Calls\" table and a Global Secondary Index on a \"IsActive\" attribute that is present for active calls only. In this way the Global Secondary Index is sparse and more effective.","upvote_count":"1","timestamp":"1723804980.0"},{"comment_id":"927933","poster":"SkyZeroZx","content":"Selected Answer: D\nThe best database implementation for this scenario is D.\n\nThis implementation uses DynamoDB with a \"Calls\" table and a Global Secondary Index on a \"IsActive\" attribute that is present for active calls only. This ensures that the Global Secondary Index is sparse and more effective.","upvote_count":"1","timestamp":"1687208220.0","comments":[{"timestamp":"1687208280.0","content":"The other implementations are not as suitable for this scenario. For example, implementation A uses a Global Secondary Index on the \"State\" attribute. This means that all calls are indexed, even the terminated calls. This can make the index large and inefficient.\n\nImplementation B uses a relational database. Relational databases are not as scalable as DynamoDB. They can also be more complex to manage.\n\nImplementation C uses two tables. This can make it more difficult to manage the data. It can also be more expensive than using a single table.","comment_id":"927935","poster":"SkyZeroZx","upvote_count":"1"},{"comment_id":"927934","upvote_count":"2","timestamp":"1687208220.0","poster":"SkyZeroZx","content":"Here are the reasons why this implementation is the best choice:\n\nDynamoDB is a good choice for this scenario because it is a highly scalable and durable database. It can handle the periodic peak of 1000 calls/second without any problems.\nThe Global Secondary Index on the \"IsActive\" attribute ensures that only the active calls are indexed. This helps to keep the index small and efficient.\nThe \"IsActive\" attribute is a sparse attribute. This means that only the active calls have a value for this attribute. This helps to keep the index even smaller and more efficient.\nThe historical data can be periodically archived to files. This helps to reduce the cost of storing the data"}]},{"comment_id":"761759","upvote_count":"1","content":"why not B?","poster":"hollie","timestamp":"1672386660.0"},{"content":"D. Use DynamoDB with a \"Calls\" table and a Global Secondary Index on a \"IsActive\" attribute that is present for active calls only. In this way the Global Secondary Index is sparse and more effective.","upvote_count":"1","comment_id":"496523","poster":"cldy","timestamp":"1638937380.0"},{"upvote_count":"1","comment_id":"367766","content":"Davis correct","comments":[{"poster":"01037","timestamp":"1634281620.0","upvote_count":"3","comment_id":"367767","content":"D is correct"}],"timestamp":"1632664320.0","poster":"01037"}],"timestamp":"2021-05-27 11:49:00","answer":"D","question_id":898,"choices":{"C":"Use RDS Multi-AZ with two tables, one for \"ACTIVE_CALLS\" and one for \"TERMINATED_CALLS\". In this way the \"ACTIVE_CALLS\" table is always small and effective to access.","B":"Use RDS Multi-AZ with a \"CALLS\" table and an indexed \"STATE\" field that can be equal to \"ACTIVE\" or 'TERMINATED\". In this way the SQL query is optimized by the use of the Index.","D":"Use DynamoDB with a \"Calls\" table and a Global Secondary Index on a \"IsActive\" attribute that is present for active calls only. In this way the Global Secondary Index is sparse and more effective.","A":"Use DynamoDB with a \"Calls\" table and a Global Secondary Index on a \"State\" attribute that can equal to \"active\" or \"terminated\". In this way the Global Secondary Index can be used for all items in the table."},"answer_ET":"D","question_images":[],"answer_images":[],"unix_timestamp":1622108940,"answer_description":"Q: Can a global secondary index key be defined on non-unique attributes?\nYes. Unlike the primary key on a table, a GSI index does not require the indexed attributes to be unique.\nQ: Are GSI key attributes required in all items of a DynamoDB table?\nNo. GSIs are sparse indexes. Unlike the requirement of having a primary key, an item in a DynamoDB table does not have to contain any of the GSI keys. If a GSI key has both hash and range elements, and a table item omits either of them, then that item will not be indexed by the corresponding GSI. In such cases, a GSI can be very useful in efficiently locating items that have an uncommon attribute.\nReference:\nhttps://aws.amazon.com/dynamodb/faqs/","isMC":true,"exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/53643-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"Dhd3Ehz6xv9lYE4iXpf3","unix_timestamp":1662350340,"url":"https://www.examtopics.com/discussions/amazon/view/80192-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"C":"Create new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB to connect to the DynamoDB tables.","B":"Create new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables.","D":"Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the cluster endpoint to connect to Amazon DocumentDB.","A":"Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the instance endpoint to connect to Amazon DocumentDB."},"answer_images":[],"question_images":[],"exam_id":32,"isMC":true,"answer_description":"","answer":"B","timestamp":"2022-09-05 05:59:00","discussion":[{"comment_id":"686963","upvote_count":"7","comments":[{"timestamp":"1666515060.0","poster":"fdoxxx","comment_id":"702020","content":"What about \"all Amazon EC2 instances must be hosted in a private subnet without an internet connection.\" It clearly states that it can not be a Gateway - it needs to be Interface endpoint - so C imho","upvote_count":"1","comments":[{"content":"Apps will run in the EC2s and connect to DynamobDB using Gateway endpoint.","upvote_count":"2","poster":"Rocketeer","timestamp":"1667182320.0","comment_id":"708136"}]}],"timestamp":"1664979840.0","content":"Both answers with provisioned IOPS don't scale on demand. Eliminate A and D. DynamoDB and S3 both use Gateway Endpoints, so it is B. For debate on Dynamo vs Document, it works in Mongo as key/value outside, so it will work with either Dynamo or Document. Would have gone with DocumentDB if not for provisioned IOPS and requirement it scale on-demand.","poster":"joanneli77"},{"timestamp":"1662350340.0","poster":"rajvee","comments":[{"content":"happy with that explanation","timestamp":"1663008240.0","poster":"Ni_yot","comment_id":"667367","upvote_count":"2"}],"upvote_count":"6","content":"D, seems about right. https://docs.aws.amazon.com/documentdb/latest/developerguide/endpoints.html \nCluster endpoint\nA cluster endpoint is an endpoint for an Amazon DocumentDB cluster that connects to the current primary instance for the cluster. Each Amazon DocumentDB cluster has a single cluster endpoint and one primary instance. In case of a failover, the cluster endpoint is remapped to the new primary instance.\n\nVs\n\nInstance endpoint\nAn instance endpoint is an endpoint that connects to a specific instance. Each instance in a cluster, regardless of whether it is a primary or replica instance, has its own unique instance endpoint. It is best to not use instance endpoints in your application. This is because they can change roles in case of a failover, thus requiring code changes in your application.","comment_id":"659708"},{"timestamp":"1721074020.0","comment_id":"1248556","content":"Selected Answer: B\nB) because *The database must be able to scale based on demand* this is a must to have requirement so change all legacy application code to persist and request data from/to dynamodb.\nquestion does not say to minimize development overhead sol.. poor developrs ✅✅","upvote_count":"1","poster":"WhyIronMan"},{"timestamp":"1688932860.0","content":"Selected Answer: D\nD, seems about right. https://docs.aws.amazon.com/documentdb/latest/developerguide/endpoints.html\nCluster endpoint\nA cluster endpoint is an endpoint for an Amazon DocumentDB cluster that connects to the current primary instance for the cluster. Each Amazon DocumentDB cluster has a single cluster endpoint and one primary instance. In case of a failover, the cluster endpoint is remapped to the new primary instance.\n\nAdicionally reference use MongoDB how key value , migration not it's this case in question \nThen D","poster":"SkyZeroZx","upvote_count":"1","comment_id":"947529"},{"poster":"RotterDam","comment_id":"913885","upvote_count":"1","timestamp":"1685819820.0","content":"Selected Answer: B\n(B) Is correct based on the Original Author of this question. I didn't know you CANNOT connect to DocumentDB with either Interface or Gateway EP. I also DIDNT know you can choose a different Database (dynamo is purely Key/Value not Document) even though the question doesnt mention this."},{"poster":"Jesuisleon","content":"Selected Answer: D\nI preferred D as the answer.\nFirst C is apparently wrong, DynamoDB ONLY supports gateway endpoint not supports interface endpoint. you can search dynamodb in this link https://docs.aws.amazon.com/vpc/latest/privatelink/aws-services-privatelink-support.html\n\nThe question doesn't refer the bottleneck client meets, so we can not assume client needs on-demand capacity to improve the capacity during high peak time.\n\nso compatibility to mongodb makes sense to client.","upvote_count":"1","timestamp":"1685277600.0","comment_id":"908645"},{"upvote_count":"1","poster":"dev112233xx","comment_id":"883796","timestamp":"1682704020.0","content":"Selected Answer: B\nI totally agree... B is the correct answer✅"},{"timestamp":"1680555660.0","content":"Selected Answer: D\nBad question. \nIf a legacy application is written to use Mongo DB it will not work with DynamoDB.\nYes it seems possible that the company we bought/lisenced that application could create a new version for DynamoDB as it only needs a key-value store.\nSo we have to options:\nConvince and pay some third party to write a new version of the application that is compatible with DynamoDB or use DocumentDB\nDepending on that decision the answer would is B or D.\nIn the meantime, until the new application for Dynamodb is available. I would suggest to use MongoDB. That is D.","comment_id":"860342","poster":"hobokabobo","upvote_count":"2"},{"content":"I think B is correct, and ChatGPT also confirmed B. I am going with B","poster":"Cloudyheema","comment_id":"798394","timestamp":"1675548240.0","upvote_count":"1"},{"poster":"evargasbrz","upvote_count":"1","comment_id":"765085","timestamp":"1672782300.0","content":"Selected Answer: B\nI'll go with B\nWhy not A and D -> They use \"provisioned IOPS\", so they don't scale on demand. \nB-> DynamoDB works with key/value and uses Gateway Endpoints"},{"poster":"alxjandroleiva","comments":[{"content":"what about scaling?","comments":[{"content":"What about it Dynamodb does not work. It never gets any data and so does not scale the slightest. It stays at zero utilization. \n\nWithout refactoring/rewriting the application no way to make an application that speaks Mongo work with dynamodb.","poster":"hobokabobo","comment_id":"860331","upvote_count":"2","timestamp":"1680554220.0"}],"timestamp":"1668121980.0","poster":"mrgreatness","comment_id":"715607","upvote_count":"1"}],"upvote_count":"2","timestamp":"1667285880.0","content":"D: Legacy application, You can not request the team to change all application to make request to dynamo instead of Mongo","comment_id":"708913"},{"poster":"dmscountera","timestamp":"1666846140.0","comments":[{"poster":"alxjandroleiva","comment_id":"708915","content":"And change all legacy application code to persist and request data to dynamo...?","timestamp":"1667285940.0","upvote_count":"2"}],"upvote_count":"1","content":"Selected Answer: B\nGoing for B\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/aws-services-privatelink-support.html\nDynamoDB not supported by private link\nGateway endpoint not relies on private link\ntherefore B","comment_id":"705230"},{"poster":"skywalker","comment_id":"691713","upvote_count":"4","content":"Selected Answer: B\nB, DynamoDB uses Gateway Point to allow connection from VPC","timestamp":"1665461880.0"},{"content":"B Vpc gateway","comment_id":"686753","timestamp":"1664963160.0","poster":"firstabed","upvote_count":"2"},{"timestamp":"1664735040.0","poster":"JohnPi","upvote_count":"1","content":"Selected Answer: C\nVpc gateway + scale on demand + being exchanged over an AWS PrivateLink is also encrypted","comments":[{"poster":"JohnPi","timestamp":"1664735100.0","content":"interface VPC endpoint uses private link encripted","comment_id":"685007","upvote_count":"1"}],"comment_id":"685005"},{"upvote_count":"4","comment_id":"684997","poster":"JohnPi","content":"Selected Answer: B\nVpc gateway + scale on demand","timestamp":"1664734440.0"},{"timestamp":"1664734260.0","comment_id":"684995","upvote_count":"1","poster":"JohnPi","content":"Selected Answer: D\nAmazon DocumentDB is virtual private cloud (VPC)-only and does not currently support public endpoints. Hence, if you are trying to connect from a Node server running locally in your machine, it will not be able to reach Amazon DocumentDB publicly."},{"comment_id":"682223","timestamp":"1664410860.0","comments":[{"poster":"sb333","timestamp":"1664688420.0","upvote_count":"5","content":"It's actually D. DocumentDB (Mongo compatible) is just that - compatible with MongoDB. This is a legacy application that is built to use MongDB. The AWS exam is testing if you know that this is an available option for this use case. There is no requirement in the question that would force you to convert the database type to DynamoDB (or any other for that matter).\n\nhttps://www.mongodb.com/databases/key-value-database","comment_id":"684615"}],"upvote_count":"2","poster":"BEN_TUTU","content":"Selected Answer: C\nANSWER IS : C The question above is tricky, it's a \"bait\", a trap ! the question says : using mongo db as a\" key-value\" database, the term key-value database changes it from a DOCUMENT DB(Mongo compatible) to a \"key-value database, of course which DYNAMO DB is an example of NOSQL KEY VALUE database, which will make it \"scalable\" as the question requires.\n\nFor further proof, \n1. ) google the meaning of \"key-value\" database\n2.) google examples of NOSQL \"KEY VALUE\" DATABASE : YOU WILL SEE \"DYNAMO DB \"\n3.) Then, compare \"Mongo db as a key-value database\" to dynamo db as an example of nosql key-value database ? they both belong to the same group of key-value database and it will scale as the question required. (QED)\n\nAnswer is : C \n\nNOTE : AVOID JUMPING AT ANSWERS THAT LOOK SO STRAIGHT FORWARD. ALSO TRY TO FIND OUT THE MEANING OF ANY STRANGE UNKNOWN \"TERMINOLOGIES\" BEFORE ARRIVING AT YOUR ANSWERS."}],"answers_community":["B (59%)","D (23%)","C (18%)"],"answer_ET":"B","question_id":899,"topic":"1","question_text":"A company is migrating a legacy application from an on-premises data center to AWS. The application uses MongoDB as a key-value database. According to the company's technical guidelines, all Amazon EC2 instances must be hosted in a private subnet without an internet connection. In addition, all connectivity between applications and databases must be encrypted. The database must be able to scale based on demand.\nWhich solution will meet these requirements?"},{"id":"XrHCTLFmVgZeSKU4ilep","answers_community":["B (100%)"],"topic":"1","timestamp":"2022-09-05 00:41:00","question_id":900,"unix_timestamp":1662331260,"url":"https://www.examtopics.com/discussions/amazon/view/80168-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"B","question_text":"A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2, AWS Fargate, and\nAWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in other months.\nThe company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts across the organization to calculate usage.\nWhich solution will provide the MOST cost savings for all the organization's compute usage?","answer":"B","choices":{"B":"Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level.","D":"Purchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6 months.","A":"Purchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member accounts.","C":"Purchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months."},"answer_description":"","isMC":true,"exam_id":32,"discussion":[{"timestamp":"1662331260.0","upvote_count":"10","comment_id":"659605","content":"Selected Answer: B\nB - Savings Plan, which apply to EC2+Fargate+Lambda","poster":"epomatti"},{"content":"Selected Answer: B\nB - Savings Plan, which applies to EC2+Fargate+Lambda","upvote_count":"1","poster":"WhyIronMan","comment_id":"1248532","timestamp":"1721070060.0"},{"timestamp":"1688006460.0","upvote_count":"2","comment_id":"937402","poster":"SkyZeroZx","content":"B - Savings Plan, which apply to EC2+Fargate+Lambda"},{"comment_id":"711663","upvote_count":"2","content":"The answer is in the question: \"compute cost\"","poster":"Costi","timestamp":"1667643000.0"},{"upvote_count":"1","poster":"Ni_yot","comment_id":"667371","timestamp":"1663008480.0","content":"B. Savings plan will defo save more money and is cost optimized"}],"answer_images":[],"question_images":[]}],"exam":{"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":false,"name":"AWS Certified Solutions Architect - Professional","numberOfQuestions":1019,"provider":"Amazon","isBeta":false,"id":32},"currentPage":180},"__N_SSP":true}