{"pageProps":{"questions":[{"id":"POjliky7vY1cSOhizJln","topic":"1","question_id":176,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/132697-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true,"answer_ET":"D","unix_timestamp":1706878560,"answer_description":"","question_images":[],"discussion":[{"upvote_count":"12","poster":"GiorgioGss","comment_id":"1177258","timestamp":"1710848100.0","content":"Selected Answer: D\nAlthough C is more cost-effective, because of \"must be able to retrieve all data within milliseconds\" will go with D"},{"upvote_count":"2","content":"Selected Answer: D\nBased on this docs https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html\nD will be appropriate as it allows for instant retrieval","poster":"andrologin","comment_id":"1247687","timestamp":"1720945440.0"},{"poster":"rpwags","timestamp":"1719078720.0","upvote_count":"3","content":"Selected Answer: D\nStaying with \"D\"... The Amazon S3 Glacier Deep Archive storage class is designed for long-term data archiving where data retrieval times are flexible. It does not offer millisecond retrieval times. Instead, data retrieval from S3 Glacier Deep Archive typically takes 12 hours or more. For millisecond retrieval times, you would use the S3 Standard, S3 Standard-IA, or S3 One Zone-IA storage classes, which are designed for frequent or infrequent access with low latency.","comment_id":"1235533"},{"content":"Selected Answer: D\nI am confused with C or D","timestamp":"1713798300.0","comment_id":"1200232","upvote_count":"2","poster":"raghumvj"},{"content":"Selected Answer: C\nC is correct.\n\n\"Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds.\"\nhttps://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/","comment_id":"1197928","comments":[{"content":"But C doesn't say anything about Instant Retrieval.","timestamp":"1716889380.0","upvote_count":"3","comment_id":"1220114","poster":"tgv"}],"timestamp":"1713441840.0","poster":"chris_spencer","upvote_count":"1"},{"comment_id":"1194774","content":"Selected Answer: D\nleast operation overhead, D","timestamp":"1712995080.0","upvote_count":"2","poster":"Christina666"},{"timestamp":"1712396400.0","content":"The correct answer may be D. Intelligent tiering's default access tier is:\n1. accessed less than 30 days: frequent access tier\n2. not accessed in 30-90 days: Infrequent Access tier\n3. not accessed more than 90 days: Archive Instant Access tier\nOther tiers require more retrieve time need activation.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html","poster":"arvehisa","upvote_count":"2","comment_id":"1190327"},{"upvote_count":"2","timestamp":"1710565080.0","comment_id":"1174738","poster":"helpaws","content":"Selected Answer: C\nAmazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds"},{"upvote_count":"1","poster":"kj07","content":"A few remarks: Data should be retrieved in ms. This means all the options with Glacier are wrong: BC\n\nFor D how you can set the S3 intelligent-Tiering if the current class is Standard? \nI guess you need a lifecycle policy.\n\nWhich leaves only A as an option.\n\nThoughts?","timestamp":"1710413160.0","comment_id":"1173314"},{"timestamp":"1709828220.0","content":"D. is correct","upvote_count":"1","poster":"damaldon","comment_id":"1168162"},{"content":"Option C. Use S3 Intelligent-Tiering. Activate the Deep Archive Access tier.\n\n\n\nBy using S3 Intelligent-Tiering and activating the Deep Archive Access tier, the company can optimize S3 storage costs with minimal operational overhead. S3 Intelligent-Tiering automatically moves objects between four access tiers, including the Deep Archive Access tier, based on changing access patterns and cost optimization. This eliminates the need for manual lifecycle policies and constant refinement, as the storage class is adjusted automatically based on data access patterns, resulting in cost savings while ensuring quick access to all data when needed.","upvote_count":"1","timestamp":"1709503260.0","comment_id":"1165126","poster":"Felix_G"},{"content":"Selected Answer: D\nOption D, using S3 Intelligent-Tiering with the default access tier, will meet the requirements best. It provides a hands-off approach to storage cost optimization while ensuring that data is available for analytics workloads within the required timeframe.","upvote_count":"3","poster":"rralucard_","comment_id":"1138543","timestamp":"1706878560.0"}],"timestamp":"2024-02-02 13:56:00","answers_community":["D (89%)","11%"],"exam_id":21,"question_text":"A company stores petabytes of data in thousands of Amazon S3 buckets in the S3 Standard storage class. The data supports analytics workloads that have unpredictable and variable data access patterns.\nThe company does not access some data for months. However, the company must be able to retrieve all data within milliseconds. The company needs to optimize S3 storage costs.\nWhich solution will meet these requirements with the LEAST operational overhead?","choices":{"C":"Use S3 Intelligent-Tiering. Activate the Deep Archive Access tier.","A":"Use S3 Storage Lens standard metrics to determine when to move objects to more cost-optimized storage classes. Create S3 Lifecycle policies for the S3 buckets to move objects to cost-optimized storage classes. Continue to refine the S3 Lifecycle policies in the future to optimize storage costs.","B":"Use S3 Storage Lens activity metrics to identify S3 buckets that the company accesses infrequently. Configure S3 Lifecycle rules to move objects from S3 Standard to the S3 Standard-Infrequent Access (S3 Standard-IA) and S3 Glacier storage classes based on the age of the data.","D":"Use S3 Intelligent-Tiering. Use the default access tier."},"answer":"D"},{"id":"kF7JkGwSBIWPnFZRnYHI","answer_description":"","choices":{"D":"Store the credentials in AWS Secrets Manager.","E":"Grant the AWS Glue job IAM role access to the stored credentials.","C":"Access the credentials from a configuration file that is in an Amazon S3 bucket by using the AWS Glue job.","B":"Store the credentials in a configuration file that is in an Amazon S3 bucket.","A":"Store the credentials in the AWS Glue job parameters."},"answer_ET":"DE","topic":"1","question_images":[],"question_id":177,"answer_images":[],"exam_id":21,"unix_timestamp":1706878860,"discussion":[{"comment_id":"1177260","timestamp":"1726738620.0","upvote_count":"9","poster":"GiorgioGss","content":"Selected Answer: DE\nD because it's AWS best practice for securing creds and E because after you put cred in secrets you will need permissions for accesing"},{"comment_id":"1168164","content":"Ans, DE","poster":"damaldon","timestamp":"1725718740.0","upvote_count":"1"},{"comment_id":"1138547","timestamp":"1722596460.0","poster":"rralucard_","upvote_count":"3","content":"Selected Answer: DE\nD. Store the credentials in AWS Secrets Manager: AWS Secrets Manager is a service that helps you protect access to your applications, services, and IT resources without the upfront investment and on-going maintenance costs of operating your own infrastructure. It's specifically designed for storing and retrieving credentials securely, and therefore, it is an appropriate choice for handling the Redshift cluster credentials.\n\nE. Grant the AWS Glue job IAM role access to the stored credentials: IAM roles for AWS Glue will allow the job to assume a role with the necessary permissions to access the credentials in AWS Secrets Manager. This method avoids embedding credentials directly in the script or a configuration file and allows for centralized management of the credentials."}],"timestamp":"2024-02-02 14:01:00","question_text":"During a security review, a company identified a vulnerability in an AWS Glue job. The company discovered that credentials to access an Amazon Redshift cluster were hard coded in the job script.\nA data engineer must remediate the security vulnerability in the AWS Glue job. The solution must securely store the credentials.\nWhich combination of steps should the data engineer take to meet these requirements? (Choose two.)","answer":"DE","isMC":true,"answers_community":["DE (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/132698-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"sMTE6fsGvzkykm9R4oKT","answer":"B","answer_ET":"B","answer_description":"","discussion":[{"upvote_count":"5","timestamp":"1728806640.0","content":"Selected Answer: B\nFully Managed, Serverless: Redshift Serverless eliminates the need to manually create, manage, or delete clusters. It automatically scales resources based on the workload, reducing operational overhead significantly.\nCost-Effective for Infrequent Workloads: Since the analytics processes run only once a month, Redshift Serverless's pay-per-use model is ideal for minimizing costs during downtime.\nSeamless S3 Integration: Redshift Serverless natively integrates with S3 for backup and restore operations, ensuring compatibility with the existing process.","comment_id":"1194785","poster":"Christina666"},{"content":"Selected Answer: B\nB is correct","comment_id":"1219522","upvote_count":"1","timestamp":"1732716000.0","poster":"4c78df0"},{"comment_id":"1177263","content":"Selected Answer: B\n\"does not require to manage the infrastructure manually\" = Serverless","upvote_count":"2","timestamp":"1726738680.0","poster":"GiorgioGss"},{"timestamp":"1725720300.0","comment_id":"1168183","poster":"damaldon","content":"Ans. B","upvote_count":"1"},{"content":"Selected Answer B: Options A, C and D still involve manual tasks like administering CloudFormation stacks, using AWS CLI commands, or configuring Step Function state machines.\n\nBy leveraging Redshift Serverless, the data engineer avoids all cluster and infrastructure administration effort. This has the least operational overhead to run the monthly","poster":"Felix_G","timestamp":"1725394380.0","comment_id":"1165128","upvote_count":"3"},{"timestamp":"1722596640.0","poster":"rralucard_","upvote_count":"2","comment_id":"1138549","content":"Selected Answer: B\nUse Amazon Redshift Serverless. This option allows the data engineer to focus on the analytics processes themselves without worrying about cluster provisioning, scaling, or management. It provides an on-demand, serverless solution that can handle variable workloads and is cost-effective for intermittent and irregular processing needs like those described."}],"answers_community":["B (100%)"],"exam_id":21,"question_images":[],"question_id":178,"choices":{"B":"Use Amazon Redshift Serverless to automatically process the analytics workload.","D":"Use AWS CloudFormation templates to automatically process the analytics workload.","C":"Use the AWS CLI to automatically process the analytics workload.","A":"Use Amazon Step Functions to pause the Redshift cluster when the analytics processes are complete and to resume the cluster to run new processes every month."},"isMC":true,"topic":"1","unix_timestamp":1706879040,"question_text":"A data engineer uses Amazon Redshift to run resource-intensive analytics processes once every month. Every month, the data engineer creates a new Redshift provisioned cluster. The data engineer deletes the Redshift provisioned cluster after the analytics processes are complete every month. Before the data engineer deletes the cluster each month, the data engineer unloads backup data from the cluster to an Amazon S3 bucket.\nThe data engineer needs a solution to run the monthly analytics processes that does not require the data engineer to manage the infrastructure manually.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"timestamp":"2024-02-02 14:04:00","url":"https://www.examtopics.com/discussions/amazon/view/132699-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"MlyJ2jo9DsR0Arvs0ms1","timestamp":"2024-02-02 14:07:00","answer":"D","answer_images":[],"topic":"1","unix_timestamp":1706879220,"url":"https://www.examtopics.com/discussions/amazon/view/132700-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_description":"","answer_ET":"D","question_id":179,"discussion":[{"timestamp":"1706879220.0","comment_id":"1138552","poster":"rralucard_","upvote_count":"9","content":"Selected Answer: D\nAWS Glue DataBrew: AWS Glue DataBrew is a visual data preparation tool that allows data engineers and data analysts to clean and normalize data without writing code. Using DataBrew, a data engineer could create a recipe that includes the concatenation of the customer first and last names and then use the COUNT_DISTINCT function. This would not require complex code and could be performed through the DataBrew user interface, representing a lower operational effort."},{"content":"Selected Answer: D\nDataBrew supports various transformations,\nincluding the COUNT_DISTINCT function, which is ideal for calculating the number of unique values in a column (combined first and last names in this case).","poster":"pypelyncar","comment_id":"1228711","timestamp":"1718149320.0","upvote_count":"2"},{"poster":"Ousseyni","upvote_count":"2","timestamp":"1713353400.0","content":"Selected Answer: D\ngo in D","comment_id":"1197192"},{"comment_id":"1188250","content":"Selected Answer: D\nsince it's less operational effort, I would go in D","upvote_count":"2","poster":"lucas_rfsb","timestamp":"1712083800.0"}],"question_text":"A company receives a daily file that contains customer data in .xls format. The company stores the file in Amazon S3. The daily file is approximately 2 GB in size.\nA data engineer concatenates the column in the file that contains customer first names and the column that contains customer last names. The data engineer needs to determine the number of distinct customers in the file.\nWhich solution will meet this requirement with the LEAST operational effort?","choices":{"A":"Create and run an Apache Spark job in an AWS Glue notebook. Configure the job to read the S3 file and calculate the number of distinct customers.","D":"Use AWS Glue DataBrew to create a recipe that uses the COUNT_DISTINCT aggregate function to calculate the number of distinct customers.","C":"Create and run an Apache Spark job in Amazon EMR Serverless to calculate the number of distinct customers.","B":"Create an AWS Glue crawler to create an AWS Glue Data Catalog of the S3 file. Run SQL queries from Amazon Athena to calculate the number of distinct customers."},"exam_id":21,"question_images":[],"isMC":true,"answers_community":["D (100%)"]},{"id":"tyDYP36owfIY0DtoHaYv","discussion":[{"poster":"rralucard_","timestamp":"1707033900.0","comment_id":"1139892","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\nUse the Streaming Ingestion Feature of Amazon Redshift: Amazon Redshift recently introduced streaming data ingestion, allowing Redshift to consume data directly from Kinesis Data Streams in near real-time. This feature simplifies the architecture by eliminating the need for intermediate steps or services, and it is specifically designed to support near real-time analytics. The operational overhead is minimal since the feature is integrated within Redshift.","upvote_count":"7"},{"upvote_count":"1","comment_id":"1304366","timestamp":"1730193600.0","content":"option B","poster":"ssnei"},{"content":"Selected Answer: B\nB is correct","upvote_count":"1","timestamp":"1716811260.0","comment_id":"1219527","poster":"4c78df0"},{"poster":"lucas_rfsb","content":"Selected Answer: B\nI'd go in B","timestamp":"1712083980.0","upvote_count":"1","comment_id":"1188254"}],"timestamp":"2024-02-04 09:05:00","url":"https://www.examtopics.com/discussions/amazon/view/132765-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","unix_timestamp":1707033900,"exam_id":21,"question_id":180,"answer_description":"","question_text":"A healthcare company uses Amazon Kinesis Data Streams to stream real-time health data from wearable devices, hospital equipment, and patient records.\nA data engineer needs to find a solution to process the streaming data. The data engineer needs to store the data in an Amazon Redshift Serverless warehouse. The solution must support near real-time analytics of the streaming data and the previous day's data.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"B","isMC":true,"answer":"B","question_images":[],"answer_images":[],"topic":"1","choices":{"C":"Load the data into Amazon S3. Use the COPY command to load the data into Amazon Redshift.","A":"Load data into Amazon Kinesis Data Firehose. Load the data into Amazon Redshift.","B":"Use the streaming ingestion feature of Amazon Redshift.","D":"Use the Amazon Aurora zero-ETL integration with Amazon Redshift."},"answers_community":["B (100%)"]}],"exam":{"provider":"Amazon","isMCOnly":true,"isImplemented":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","isBeta":false,"numberOfQuestions":207,"id":21,"lastUpdated":"11 Apr 2025"},"currentPage":36},"__N_SSP":true}