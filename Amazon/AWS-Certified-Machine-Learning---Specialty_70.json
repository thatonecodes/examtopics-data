{"pageProps":{"questions":[{"id":"ArDOZw9FGgUsYv7U6q6A","choices":{"C":"Serialize the trained model so the format is compressed for deployment. Build the image and upload it to Docker Hub.","D":"Build the Docker image with the inference code. Configure Docker Hub and upload the image to Amazon ECR.","B":"Serialize the trained model so the format is compressed for deployment. Tag the Docker image with the registry hostname and upload it to Amazon S3.","A":"Build the Docker image with the inference code. Tag the Docker image with the registry hostname and upload it to Amazon ECR."},"exam_id":26,"question_id":346,"topic":"1","answer":"A","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/43938-exam-aws-certified-machine-learning-specialty-topic-1/","question_images":[],"answer_description":"","question_text":"A Machine Learning Specialist previously trained a logistic regression model using scikit-learn on a local machine, and the Specialist now wants to deploy it to production for inference only.\nWhat steps should be taken to ensure Amazon SageMaker can host a model that was trained locally?","isMC":true,"answer_ET":"A","timestamp":"2021-02-03 19:44:00","answers_community":["A (100%)"],"unix_timestamp":1612377840,"discussion":[{"content":"Ans : A Refer the below : \n https://sagemaker-workshop.com/custom/containers.html","poster":"arulrajjayaraj","timestamp":"1665128820.0","comment_id":"282927","upvote_count":"19"},{"comment_id":"282901","content":"A\nhttps://sagemaker-workshop.com/custom/containers.html","timestamp":"1663644840.0","upvote_count":"7","poster":"Paul_NoName"},{"timestamp":"1732721100.0","poster":"endeesa","upvote_count":"1","content":"Selected Answer: A\nYou need the container to be hosted on ECR.","comment_id":"1081697"},{"content":"Selected Answer: A\nA. YES - the inference code is built after inspecting the coefficient of the Linear Model (or, alternatively, the model can be serialized via pickle and the inference code is simply to unserialized the mode); ECR is only registry supported by SageMaer; tagging the Docker image with the registry hostname (eg. docker tag image1 public.ecr.aws/g6h7x5m5/image1) is required so that the docker push command knows where to push the image\nB. NO - no need to compress; image must be on ECR\nC. NO - no need to compress; image must be on ECR\nD. NO - image must be on ECR","timestamp":"1726386180.0","upvote_count":"5","comment_id":"1008245","poster":"loict"},{"content":"Selected Answer: A\nA is the right answer","timestamp":"1712221860.0","poster":"Khalil11","comment_id":"860842","upvote_count":"3"},{"timestamp":"1709903580.0","content":"Selected Answer: A\nFor SageMaker to run a container for training or hosting, it needs to be able to find the image hosted in the image repository, Amazon Elastic Container Registry (Amazon ECR). The three main steps to this process are building locally, tagging with the repository location, and pushing the image to the repository.","comment_id":"832983","poster":"Nadia0012","upvote_count":"4"},{"comment_id":"519454","upvote_count":"3","timestamp":"1673174640.0","poster":"geekgirl007","content":"Selected Answer: A\nA for sure."},{"poster":"astonm13","timestamp":"1666995600.0","upvote_count":"3","content":"Answer is A.","comment_id":"291638"},{"timestamp":"1666882680.0","poster":"cnethers","comment_id":"284086","content":"Docker Hub is a repository so ANS D makes no sense. Option A is the way to go.","upvote_count":"2"}]},{"id":"zvtQwmYR0nnHAWrQBmQK","discussion":[{"poster":"Paul_NoName","timestamp":"1633319160.0","content":"B is the right answer","comment_id":"282906","upvote_count":"33"},{"content":"Selected Answer: B\nB to use as storage with policies","comment_id":"535362","timestamp":"1643449860.0","upvote_count":"7","poster":"clawo"},{"poster":"xicocaio","timestamp":"1727468580.0","upvote_count":"1","comment_id":"1290343","content":"Selected Answer: B\n- Amazon S3-backed data lake: S3 is the best storage option for large and rapidly growing datasets like images from trucks. S3 scales easily, handles large volumes of data, and is cost-effective for long-term storage, making it a natural choice for this scenario.\n- IAM access control: You can use bucket policies in S3 to set very specific access controls, ensuring that only certain IAM users have permission to access or modify the data. This satisfies the requirement for access control using IAM.\n- Processing flexibility: Storing the images in S3 offers flexibility for future machine learning use cases. The data stored in S3 can easily be integrated with other AWS services like SageMaker, Athena, EMR, and more for processing and analysis."},{"poster":"endeesa","upvote_count":"1","timestamp":"1701098940.0","content":"Selected Answer: B\nEMR/HDFS is not more 'flexible' than S3","comment_id":"1081703"},{"comment_id":"1008250","content":"Selected Answer: B\nA. NO - volume too big for a DB\nB. YES\nC. NO - instance access will not control HDFS access\nD. NO - EFS does not use IAM policies (it is unix)","upvote_count":"1","timestamp":"1694764080.0","poster":"loict"},{"comment_id":"991564","upvote_count":"1","timestamp":"1693153500.0","content":"Selected Answer: B\nS3 indeed","poster":"Mickey321"},{"timestamp":"1684849800.0","comment_id":"904964","upvote_count":"1","content":"Selected Answer: B\nS3 always","poster":"JK1977"},{"timestamp":"1678353900.0","content":"Selected Answer: B\nI would say the answer is B not because of the cost on EMR,. that is also a current answer. however: \"most processing flexibility\" indicates that S3 is a better option. because all ML solutions and work flows integrate with S3. it hasn't spoken what the ML solution and which services so I take the safe side and go with S3","poster":"Nadia0012","upvote_count":"2","comment_id":"833770"},{"timestamp":"1658231400.0","poster":"KlaudYu","upvote_count":"4","comments":[{"timestamp":"1682536200.0","upvote_count":"1","content":"the question does not require long-term storage.","comment_id":"881958","poster":"ZSun"}],"content":"Selected Answer: B\nC is not affordable because it is ephemeral storage. https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html\n\"HDFS is used by the master and core nodes. One advantage is that it's fast; a disadvantage is that it's ephemeral storage which is reclaimed when the cluster ends. It's best used for caching the results produced by intermediate job-flow steps.\"","comment_id":"633525"},{"comment_id":"519029","comments":[{"poster":"ovokpus","upvote_count":"5","timestamp":"1656455700.0","content":"Why will you need to spin up servers (EMR) just to store visual data for ML?","comment_id":"624303"}],"upvote_count":"4","content":"Selected Answer: C\nC is correct. it says real time data and to be used for ml process so EMR more suitable. also S3 bucket policies not same as IAM users so B is not correct.","poster":"geekgirl007","timestamp":"1641567120.0"},{"upvote_count":"3","comment_id":"427062","content":"I think Amazon EMR is more appropriate, as the data scheme stated is a big data scheme.\n\nhttps://aws.amazon.com/emr/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc","poster":"Abdo702","comments":[{"timestamp":"1642380180.0","content":"IAM support is required for storage feature , that is not possible as per options described as IAM is supported for HDFS for the instance running on top of it, hence B should be correct","poster":"Sourabh1703","comment_id":"525370","upvote_count":"2"}],"timestamp":"1636225920.0"},{"content":"B is the right answer","comment_id":"324894","poster":"Vita_Rasta84444","timestamp":"1634907480.0","upvote_count":"2"},{"timestamp":"1634883960.0","comment_id":"320486","upvote_count":"1","content":"S3 is the easy, scalable and secure option to store the image data.","poster":"srinu3054"},{"timestamp":"1634543760.0","poster":"astonm13","comment_id":"291640","upvote_count":"1","content":"B is the right answer"},{"timestamp":"1633669320.0","comment_id":"283586","upvote_count":"2","poster":"zzaibis","content":"B is an appropriate choice"}],"topic":"1","unix_timestamp":1612377960,"exam_id":26,"question_id":347,"answer_description":"","question_text":"A trucking company is collecting live image data from its fleet of trucks across the globe. The data is growing rapidly and approximately 100 GB of new data is generated every day. The company wants to explore machine learning uses cases while ensuring the data is only accessible to specific IAM users.\nWhich storage option provides the most processing flexibility and will allow access control with IAM?","answer":"B","choices":{"C":"Setup up Amazon EMR with Hadoop Distributed File System (HDFS) to store the files, and restrict access to the EMR instances using IAM policies.","A":"Use a database, such as Amazon DynamoDB, to store the images, and set the IAM policies to restrict access to only the desired IAM users.","D":"Configure Amazon EFS with IAM policies to make the data available to Amazon EC2 instances owned by the IAM users.","B":"Use an Amazon S3-backed data lake to store the raw images, and set up the permissions using bucket policies."},"url":"https://www.examtopics.com/discussions/amazon/view/43940-exam-aws-certified-machine-learning-specialty-topic-1/","answer_images":[],"answer_ET":"B","isMC":true,"timestamp":"2021-02-03 19:46:00","question_images":[],"answers_community":["B (82%)","C (18%)"]},{"id":"5WS072OG4d0WZcW77mEE","answer_ET":"B","answer":"B","answer_description":"","unix_timestamp":1578757440,"answers_community":["B (100%)"],"discussion":[{"comment_id":"37751","timestamp":"1632820980.0","content":"B is correct","poster":"cybe001","upvote_count":"24"},{"timestamp":"1634027940.0","poster":"dhs227","content":"The correct answer HAS TO be B\nUsing Glue Use AWS Glue to catalogue the data and Amazon Athena to run queries against data on S3 are very typical use cases for those services. \n\nD is not ideal, Lambda can surely do many things but it requires development/testing effort, and Amazon Kinesis Data Analytics is not ideal for ad-hoc queries.","comment_id":"73106","upvote_count":"8"},{"content":"Selected Answer: B\nB. Use AWS Glue to catalog the data and Amazon Athena to run queries. \nWhy is this the best choice?\nAWS Glue can automatically catalog both structured and unstructured data in S3.\nAmazon Athena is a serverless SQL query service that allows direct SQL queries on S3 data without moving it.\nNo infrastructure setup is required—just define a Glue Data Catalog and start querying with Athena.","upvote_count":"1","comment_id":"1357051","timestamp":"1739654340.0","poster":"JonSno"},{"content":"Selected Answer: B\nS3 query === athena , to catalog data glue","poster":"reginav","timestamp":"1734367500.0","upvote_count":"1","comment_id":"1327517"},{"poster":"AjoseO","upvote_count":"3","content":"Selected Answer: B\nAWS Glue is a fully managed ETL service that makes it easy to move data between data stores. It can automatically crawl, catalogue, and classify data stored in Amazon S3, and make it available for querying and analysis. With AWS Glue, you don't have to worry about the underlying infrastructure and can focus on your data.\n\nAmazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. It integrates with AWS Glue, so you can use the catalogued data directly in Athena without any additional data movement or transformation.","comment_id":"803128","timestamp":"1727164080.0"},{"comment_id":"973040","content":"Selected Answer: B\nThe reason for this choice is that AWS Glue is a fully managed service that provides a data catalogue to make your data in S3 searchable and queryable1. AWS Glue crawls your data sources, identifies data formats, and suggests schemas and transformations1. You can use AWS Glue to catalogue both structured and unstructured data, such as relational data, JSON, XML, CSV files, images, or media files2.","timestamp":"1691240400.0","upvote_count":"1","poster":"Mickey321"},{"poster":"Venkatesh_Babu","content":"Selected Answer: B\nI think it should be b","timestamp":"1690207380.0","upvote_count":"1","comment_id":"961674"},{"upvote_count":"2","poster":"SK27","comment_id":"740702","content":"Selected Answer: B\nB is the easiest. We can use Glue crawler.","timestamp":"1670651280.0"},{"timestamp":"1663475400.0","poster":"ryuhei","content":"Selected Answer: B\nAnswer B","upvote_count":"2","comment_id":"671982"},{"comment_id":"525043","upvote_count":"3","content":"Selected Answer: B\nQuerying data in S3 with SQL is almost always Athena.","poster":"vetaal","timestamp":"1642347300.0"},{"poster":"gcpwhiz","content":"If AWS asks the question of querying unstructured data in an efficient manner, it is almost always Athena","comment_id":"333926","upvote_count":"2","timestamp":"1636188660.0"},{"timestamp":"1636002300.0","upvote_count":"2","content":"B. I don't think that you even need Glue to transform anything. Just use Glue to define the schemas and then use Athena to query based on those schemas.","poster":"cloud_trail","comment_id":"278018"},{"content":"answer is B","timestamp":"1635445620.0","upvote_count":"1","poster":"Willnguyen22","comment_id":"173410"},{"upvote_count":"1","poster":"syu31svc","content":"SQL on S3 is Athena so answer is B for sure","timestamp":"1635135300.0","comment_id":"165033"},{"content":"B is right","timestamp":"1634434500.0","poster":"roytruong","upvote_count":"2","comment_id":"98666"},{"timestamp":"1633370220.0","poster":"Jayraam","upvote_count":"1","comment_id":"72457","content":"Answer is B.\n\nQueries Against an Amazon S3 Data Lake\nData lakes are an increasingly popular way to store and analyze both structured and unstructured data. If you want to build your own custom Amazon S3 data lake, AWS Glue can make all your data immediately available for analytics without moving the data.\n\nhttps://aws.amazon.com/glue/"},{"comments":[{"poster":"Urban_Life","timestamp":"1634948520.0","content":"May I know why you are taking complex route?","comment_id":"123823","upvote_count":"10"}],"comment_id":"65511","timestamp":"1633012860.0","upvote_count":"1","content":"Correct Ans is D...Kinesis Data Analytics can use Lambda to transform and then run the SQL queries..","poster":"PRC"},{"comments":[{"upvote_count":"3","poster":"mawsman","content":"https://aws.amazon.com/glue/ - See Use cases;\nQueries against an Amazon S3 data lake\nData lakes are an increasingly popular way to store and analyze both structured and unstructured data. If you want to build your own custom Amazon S3 data lake, AWS Glue can make all your data immediately available for analytics without moving the data.","timestamp":"1634395980.0","comment_id":"88383"},{"content":"You could use a glue job to transform your unstructured data into more appropriate formats. Also, depending on your data, you might be able to create a custom classifier in glue, which will be able to crawl your data - this works particularly well in semi-structured cases, say for log files.","timestamp":"1634906760.0","upvote_count":"1","poster":"ExamTaker123456789","comment_id":"104662"}],"upvote_count":"1","comment_id":"41521","content":"Can Glue Crawler process unstructured data?","timestamp":"1632956940.0","poster":"BigEv"}],"isMC":true,"answer_images":[],"timestamp":"2020-01-11 16:44:00","exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/11771-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":348,"question_images":[],"question_text":"A manufacturing company has structured and unstructured data stored in an Amazon S3 bucket. A Machine Learning Specialist wants to use SQL to run queries on this data.\nWhich solution requires the LEAST effort to be able to query this data?","choices":{"B":"Use AWS Glue to catalogue the data and Amazon Athena to run queries.","A":"Use AWS Data Pipeline to transform the data and Amazon RDS to run queries.","D":"Use AWS Lambda to transform the data and Amazon Kinesis Data Analytics to run queries.","C":"Use AWS Batch to run ETL on the data and Amazon Aurora to run the queries."},"topic":"1"},{"id":"iV1CJeLw85EqQ1lWOgdA","isMC":true,"question_id":349,"timestamp":"2021-02-03 14:10:00","unix_timestamp":1612357800,"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/43919-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"C","discussion":[{"comments":[{"poster":"Dr_Kiko","timestamp":"1636300380.0","content":"without losing a lot of information from the original dataset\nsince when PCA retains information?","comment_id":"434161","upvote_count":"3"},{"upvote_count":"4","comment_id":"718524","content":"PCA helps to speed up the training","timestamp":"1668491820.0","poster":"VinceCar"}],"timestamp":"1632331740.0","content":"Answer C. Need reduce the features preserving the information on it this is achieve using PCA.","comment_id":"282736","poster":"ahquiceno","upvote_count":"26"},{"comment_id":"283154","upvote_count":"6","timestamp":"1633965540.0","poster":"[Removed]","comments":[{"poster":"SophieSu","content":"If you REMOVE highly correlated features(that means in pairs), the model lost a lot of information.","upvote_count":"4","timestamp":"1634828700.0","comment_id":"298724"},{"timestamp":"1678760460.0","upvote_count":"2","content":"A doesn't have sense. Self-correlation is for times series data, not for pair correlation","poster":"rodrigus","comment_id":"838495"}],"content":"Answer is A, because one must avoid information loss that PCA or autoencoders introduce through new features (https://www.i2tutorials.com/what-are-the-pros-and-cons-of-the-pca/). Otherwise, I would perform C."},{"content":"Selected Answer: A\nThis question can be misleading.\n\nI would choose A if self-correlation in the dataset is meaning pair-wise correlation, this is the most typical approach in real life. But if self-correlation means auto-correlation as in the time-series treatment, then it is wrong.\n\nIssues with answer C: Autoencoders are notorious for being hard to interpret. With PCA it is possible, but definitely not easy if you have a large dataset. In real life with this scenario, you would always go with pairwise correlation as the most simple yet effective approach.","comment_id":"1290353","poster":"xicocaio","upvote_count":"1","timestamp":"1727469960.0"},{"content":"Selected Answer: C\nAnswer is C","upvote_count":"1","comment_id":"1258162","timestamp":"1722338400.0","poster":"Giodefa96"},{"poster":"geoan13","upvote_count":"1","comment_id":"1069156","content":"Answer C \nPCA (Principal Component Analysis) takes advantage of multicollinearity and combines the highly correlated variables into a set of uncorrelated variables. Therefore, PCA can effectively eliminate multicollinearity between features.\nhttps://towardsdatascience.com/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b#:~:text=PCA%20(Principal%20Component%20Analysis)%20takes,effectively%20eliminate%20multicollinearity%20between%20features.","timestamp":"1699864380.0"},{"poster":"Mickey321","comment_id":"991599","timestamp":"1693157160.0","upvote_count":"1","content":"Selected Answer: C\nOption C"},{"upvote_count":"1","poster":"Mickey321","comment_id":"991570","timestamp":"1693153680.0","content":"Selected Answer: C\nAn autoencoder is a type of neural network that can learn a compressed representation of the input data, called the latent space, by encoding and decoding the data through multiple hidden layers1. PCA is a statistical technique that can reduce the dimensionality of the data by finding a set of orthogonal axes, called the principal components, that capture the most variance in the data2. Both methods can transform the original features into new features that are lower-dimensional, uncorrelated, and informative."},{"upvote_count":"1","timestamp":"1690923600.0","content":"Selected Answer: C\nC is the correct.\nSelf-correlation is for time series, which is not mention here. Besides that, even if was correlation only, try to do this in thousand features...","comment_id":"969414","poster":"kaike_reis"},{"timestamp":"1685126040.0","poster":"vbal","upvote_count":"1","comment_id":"907543","content":"A . run correlation matrix and remove highly correlated features."},{"poster":"JK1977","upvote_count":"1","comment_id":"904965","timestamp":"1684849920.0","content":"Selected Answer: C\nPCA for feature reduction"},{"content":"is it just me or is every 15th answer here PCA?","poster":"GOSD","comment_id":"890012","timestamp":"1683288120.0","upvote_count":"2"},{"timestamp":"1680710580.0","upvote_count":"1","comment_id":"862266","poster":"oso0348","content":"Selected Answer: C\nUsing an autoencoder or PCA can help reduce the dimensionality of the dataset by creating new features that capture the most important information in the original dataset while discarding some of the noise and highly correlated features. This can help speed up the training time and reduce overfitting issues without losing a lot of information from the original dataset. Option A may remove too many features and may not capture all the important information in the dataset, while option B only rescales the data and does not address the issue of highly correlated features. Option D is not a feature engineering technique and may not be an effective way to reduce the dimensionality of the dataset."},{"upvote_count":"1","content":"Selected Answer: C\nPCA builds new features starting from high correlated ones. So it matches the question","timestamp":"1679760000.0","poster":"Paolo991","comment_id":"850268"},{"poster":"Sneep","timestamp":"1673259420.0","comment_id":"770236","content":"It's C.\n\nThe Data Scientist should use principal component analysis (PCA) to replace the original features with new features. PCA is a technique that reduces the dimensionality of a dataset by projecting it onto a lower-dimensional space, while preserving as much of the original variation as possible. This can help to speed up the training time of the model and reduce overfitting issues, without losing a significant amount of information from the original dataset.","upvote_count":"1"},{"comment_id":"763803","upvote_count":"1","timestamp":"1672672500.0","content":"Selected Answer: C\nC: PCA is the solution","poster":"Aninina"},{"content":"Selected Answer: C\nCorrection to C. Removing correlated features from hundreds of columns will be tedious and time consuming. PCA is the way to go here.\n\nApologies for the flip","upvote_count":"2","comment_id":"622968","timestamp":"1656301680.0","poster":"ovokpus"},{"upvote_count":"1","comment_id":"621895","poster":"ovokpus","timestamp":"1656109320.0","content":"Selected Answer: A\nAnswer is A. Eliminate features that are highly correlated. This will not compromise the quality of the feature space as much as PCA would."},{"timestamp":"1636022280.0","content":"Answer is C","poster":"Vita_Rasta84444","upvote_count":"1","comment_id":"324899"},{"poster":"SophieSu","timestamp":"1635210600.0","upvote_count":"2","comment_id":"298726","content":"C is the correct answer. PCA reduces the dimensionality, solves the overfitting, at the mean time does not cause information loss."},{"timestamp":"1634771760.0","poster":"astonm13","content":"Answer is C","comment_id":"291641","upvote_count":"2"},{"upvote_count":"2","content":"I think it should be C?","timestamp":"1633819920.0","comment_id":"282932","poster":"wolf90"},{"content":"C seems to be correct.","comment_id":"282907","poster":"Paul_NoName","upvote_count":"3","timestamp":"1633384440.0"},{"upvote_count":"2","content":"Ans. is C.","comment_id":"282846","timestamp":"1632375360.0","poster":"takahirokoyama"}],"answer_images":[],"question_images":[],"exam_id":26,"choices":{"D":"Cluster raw data using k-means and use sample data from each cluster to build a new dataset","C":"Use an autoencoder or principal component analysis (PCA) to replace original features with new features","B":"Normalize all numerical values to be between 0 and 1","A":"Run self-correlation on all features and remove highly correlated features"},"topic":"1","answers_community":["C (83%)","A (17%)"],"question_text":"A credit card company wants to build a credit scoring model to help predict whether a new credit card applicant will default on a credit card payment. The company has collected data from a large number of sources with thousands of raw attributes. Early experiments to train a classification model revealed that many attributes are highly correlated, the large number of features slows down the training speed significantly, and that there are some overfitting issues.\nThe Data Scientist on this project would like to speed up the model training time without losing a lot of information from the original dataset.\nWhich feature engineering technique should the Data Scientist use to meet the objectives?","answer_description":""},{"id":"V5a5dhoAXL7N9TVBOmYN","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/43921-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"A":"Gather more data using Amazon Mechanical Turk and then retrain","D":"Add class weights to the MLP's loss function and then retrain","C":"Train an XGBoost model instead of an MLP","B":"Train an anomaly detection model instead of an MLP"},"isMC":true,"unix_timestamp":1612358940,"topic":"1","discussion":[{"comment_id":"283160","poster":"[Removed]","upvote_count":"36","timestamp":"1664268120.0","content":"For me answer is D, adjust to higher weight for class of interest: https://androidkt.com/set-class-weight-for-imbalance-dataset-in-keras/. More data may/may not be available and a data labeling job will take time."},{"content":"I believe is C, because we already made all changes possible in MLP hidden layers and the results have not improved then we must change model so XGBoot seems the best option","poster":"rhuanca","comment_id":"605516","upvote_count":"5","timestamp":"1684759920.0"},{"timestamp":"1724779860.0","upvote_count":"2","content":"Selected Answer: D\nIn this case, the data scientist is training a multilayer perceptron (MLP), which is a type of neural network, on a dataset with multiple classes. The target class of interest is unique compared to the other classes within the dataset, but it does not achieve an acceptable recall metric. Recall is a measure of how well the model can identify the relevant examples from the minority class. The data scientist has already tried varying the number and size of the MLP’s hidden layers, which has not significantly improved the results. A solution to improve recall must be implemented as quickly as possible.","comment_id":"991603","poster":"Mickey321"},{"content":"Selected Answer: D\nThe fastest one is D","upvote_count":"1","poster":"kaike_reis","timestamp":"1722623160.0","comment_id":"970454"},{"upvote_count":"1","poster":"ADVIT","comment_id":"939388","timestamp":"1719781260.0","content":"\"quickly as possible\" mean do not change to new stuff, so it's D."},{"upvote_count":"1","comment_id":"912771","poster":"kukreti18","timestamp":"1717327080.0","content":"Not C, as the question ask for a quick solution.\nI accept D."},{"timestamp":"1717117080.0","poster":"vbal","content":"Answer C : https://towardsdatascience.com/boosting-techniques-in-python-predicting-hotel-cancellations-62b7a76ffa6c","upvote_count":"1","comment_id":"910684"},{"comment_id":"804790","poster":"AjoseO","content":"Selected Answer: D\nAdding class weights to the MLP's loss function balances the class frequencies in the cost function during training, so the optimization process focuses more on the underrepresented class, improving recall.","timestamp":"1707599040.0","upvote_count":"3"},{"poster":"Tomatoteacher","content":"Selected Answer: D\nI have done this before, class weights help with unbalanced data. Only logical solution that would help if not done, XGBoost could be different, but who knows, both NNs and XGBoost have comparable performance. Answer D!","upvote_count":"4","comment_id":"778159","timestamp":"1705433700.0"},{"content":"Selected Answer: D\nIn this example, it is necessary to improve recall as soon as possible, so instead of creating additional datasets, it is effective to change the weight of each class during learning.","poster":"hamuozi","comment_id":"684941","upvote_count":"4","timestamp":"1696264320.0"},{"timestamp":"1692473760.0","poster":"victorlifan","upvote_count":"2","content":"C: 'distinct' indicates we can simplify this as a binary classification problem; then, NN is just overkill. plus, retraining a NN is much slower than training an XGboost model","comment_id":"649094"},{"upvote_count":"2","timestamp":"1685446200.0","content":"I feel answer is B. Question says Target is different than the input data which is hint for anomaly detection.","comments":[{"upvote_count":"1","content":"stop overthink","timestamp":"1722623100.0","poster":"kaike_reis","comment_id":"970453"}],"poster":"exam_prep","comment_id":"609158"},{"content":"I believe the answer is C because we need to use hyperparameters to improve model performance.","comment_id":"508813","upvote_count":"2","timestamp":"1671911460.0","poster":"KM226"},{"timestamp":"1666637340.0","comment_id":"306339","upvote_count":"4","poster":"ksarda11","content":"In case of the quickest possible way, D seems fine. For XGBoost, it will take a bit of time to code again"},{"upvote_count":"2","comments":[{"comment_id":"298728","content":"A is incorrect. Even if you hire Amazon Mechanical Turk, you won't have more data. This question is NOT asking about \"labeling\".","timestamp":"1666088880.0","upvote_count":"2","poster":"SophieSu"}],"poster":"ahquiceno","comment_id":"282750","content":"For me Answer A. Why no other model instead xgBoost, the model need more labeled data to be trained and learn more positive examples.","timestamp":"1663774800.0"}],"timestamp":"2021-02-03 14:29:00","exam_id":26,"question_text":"A Data Scientist is training a multilayer perception (MLP) on a dataset with multiple classes. The target class of interest is unique compared to the other classes within the dataset, but it does not achieve and acceptable recall metric. The Data Scientist has already tried varying the number and size of the MLP's hidden layers, which has not significantly improved the results. A solution to improve recall must be implemented as quickly as possible.\nWhich techniques should be used to meet these requirements?","answer_description":"","answer":"D","answers_community":["D (100%)"],"answer_images":[],"question_id":350,"answer_ET":"D"}],"exam":{"isImplemented":true,"isBeta":false,"isMCOnly":false,"lastUpdated":"11 Apr 2025","id":26,"provider":"Amazon","numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty"},"currentPage":70},"__N_SSP":true}