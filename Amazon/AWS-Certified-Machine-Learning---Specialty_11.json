{"pageProps":{"questions":[{"id":"Gte5NJGzmF5MXOB0Xxsb","question_text":"A machine learning specialist stores IoT soil sensor data in Amazon DynamoDB table and stores weather event data as JSON files in Amazon S3. The dataset in\nDynamoDB is 10 GB in size and the dataset in Amazon S3 is 5 GB in size. The specialist wants to train a model on this data to help predict soil moisture levels as a function of weather events using Amazon SageMaker.\nWhich solution will accomplish the necessary transformation to train the Amazon SageMaker model with the LEAST amount of administrative overhead?","timestamp":"2022-04-25 01:15:00","topic":"1","answer_images":[],"question_images":[],"discussion":[{"upvote_count":"19","timestamp":"1682378100.0","poster":"cron0001","content":"Selected Answer: D\nD. AWS Glue can connect with DynamoDB and join both data sets together via Glue Studio. Requiring minimal overheads","comment_id":"591287"},{"comment_id":"1046844","content":"Selected Answer: D\nD. AWS Glue can connect with DynamoDB and join both data sets together via Glue Studio. Requiring minimal overheads","poster":"DimLam","timestamp":"1729249860.0","upvote_count":"1"},{"upvote_count":"2","poster":"ccpmad","comment_id":"968920","content":"Selected Answer: D\nOption D with AWS Glue crawlers and ETL job provides a straightforward and efficient way to merge the data from DynamoDB and Amazon S3 into a format suitable for training the Amazon SageMaker model with minimal administrative overhead.","timestamp":"1722505320.0"},{"poster":"injoho","comment_id":"873337","upvote_count":"3","content":"D.\nhttps://aws.amazon.com/blogs/big-data/accelerate-amazon-dynamodb-data-access-in-aws-glue-jobs-using-the-new-aws-glue-dynamodb-elt-connector/","timestamp":"1713422400.0"},{"content":"12-sep exam","upvote_count":"1","poster":"Shailendraa","timestamp":"1694543640.0","comment_id":"667351"}],"answer_ET":"D","question_id":51,"choices":{"A":"Launch an Amazon EMR cluster. Create an Apache Hive external table for the DynamoDB table and S3 data. Join the Hive tables and write the results out to Amazon S3.","D":"Crawl the data using AWS Glue crawlers. Write an AWS Glue ETL job that merges the two tables and writes the output in CSV format to Amazon S3.","B":"Crawl the data using AWS Glue crawlers. Write an AWS Glue ETL job that merges the two tables and writes the output to an Amazon Redshift cluster.","C":"Enable Amazon DynamoDB Streams on the sensor table. Write an AWS Lambda function that consumes the stream and appends the results to the existing weather files in Amazon S3."},"exam_id":26,"isMC":true,"answers_community":["D (100%)"],"unix_timestamp":1650842100,"answer_description":"","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/74392-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"FOrv8CaUoiN2JxbCtUni","exam_id":26,"answer_images":[],"discussion":[{"upvote_count":"15","comments":[{"timestamp":"1679065980.0","upvote_count":"4","comment_id":"671542","poster":"rb39","content":"it's a sentiment analysis problem => comprehend"}],"timestamp":"1668109980.0","content":"Selected Answer: A\nA should be the answer","comment_id":"599724","poster":"ayatkhrisat"},{"content":"Selected Answer: C\nBlazingText can also do supervised text classification","upvote_count":"5","timestamp":"1668010680.0","comments":[{"timestamp":"1743400560.0","upvote_count":"1","content":"yes but only in TextClassification mode, note W2V mode... so A","comment_id":"1413962","poster":"ef12052"}],"comment_id":"599083","poster":"ckkobe24"},{"content":"Built-in BlazingText model using Word2Vec mode in Amazon SageMaker would likely be quicker to set up compared to using Amazon Comprehend for this specific use case. Since the problem statement mentions that the review data is already labeled with the correct durability result, preparing the training data should be relatively straightforward.\nAdditionally, as a built-in algorithm, BlazingText is optimized and pre-configured for text classification tasks, reducing the need for extensive customization and configuration compared to using Amazon Comprehend for this specific use case.\nIt's important to note that while BlazingText may be quicker to set up for this particular task, Amazon Comprehend offers a broader range of NLP capabilities and may be more suitable for other NLP tasks or scenarios where more customization and flexibility are required.\nHowever, given the time constraint of 2 days and the specific requirement of identifying product durability concerns from reviews, training a built-in BlazingText model using Word2Vec mode in Amazon SageMaker is likely to be the more direct and quicker approach to get a working solution set up and running.","comments":[],"comment_id":"1183446","upvote_count":"1","timestamp":"1727361360.0","poster":"F1Fan"},{"timestamp":"1723890840.0","content":"Selected Answer: C\nGiven the time constraint of 2 days and the need for a quick solution, the most direct approach would be to choose an option that provides a ready-to-use solution without the need for extensive customization or training.\n\nAmong the given options, the most direct approach would be:\n\nC. Train a built-in BlazingText model using Word2Vec mode in Amazon SageMaker.\n\nThis option allows you to leverage a pre-built model (BlazingText) that is optimized for text classification tasks. Word2Vec mode is suitable for analyzing text data and can quickly provide insights into sentiment or, in this case, concerns over product durability. This approach minimizes the need for extensive data preprocessing and model tuning, allowing you to focus on training and deploying the model within the given timeframe.","comment_id":"1152539","upvote_count":"2","poster":"3eb0542"},{"upvote_count":"2","content":"Selected Answer: A\nUsing a existing model to do the task in 2 days.\nA","poster":"rav009","comment_id":"1116438","timestamp":"1720411080.0"},{"content":"Selected Answer: A\nI would say A","timestamp":"1713438840.0","poster":"DimLam","upvote_count":"2","comment_id":"1046846"},{"poster":"loict","content":"Selected Answer: A\nA. YES - Amazon Comprehend with multi-class mode and Augmented manifest file\nB. NO - Gluon is for timeseries\nC. NO - still a lot of work after generating embedding\nD. NO - seq2seq is to generate text, we want to classify","comment_id":"1011200","timestamp":"1710852660.0","upvote_count":"2"},{"poster":"teka112233","content":"Selected Answer: A\nTo solve the problem in 2 days, and dealing with sentiment analysis so A will be the right answer using the comprehend \nAWS Comprehend is a natural language processing (NLP) service that uses machine learning to discover insights from text. It provides a range of functionalities, including detecting language and sentiment, extracting named entities and key phrases, and tagging parts of speech5. AWS Comprehend can automatically break down concepts like entities, phrases, and syntax in a document, which is particularly helpful for identifying events, organizations, persons, or products referenced in a document","timestamp":"1710147420.0","upvote_count":"2","comment_id":"1004504"},{"content":"Selected Answer: A\nThe most direct approach to solve this problem within 2 days is option A, train a custom classifier by using Amazon Comprehend. By doing so, you can use Amazon Comprehend, a natural language processing (NLP) service that uses machine learning to find insights and relationships in text, to create a custom classifier that can identify reviews expressing concerns over product durability. You can use the labeled reviews as your training data and specify the durability result as the class label. Amazon Comprehend will automatically preprocess the text, extract features, and train the classifier for you. You can also use Amazon Comprehend to evaluate the performance of your classifier and deploy it as an endpoint. This way, you can train a model to solve this problem within 2 days without requiring much coding or infrastructure management.","timestamp":"1709040060.0","comment_id":"991409","poster":"Mickey321","upvote_count":"2"},{"content":"A: You can customize Amazon Comprehend for your specific requirements without the skillset required to build machine learning-based NLP solutions. Using automatic machine learning, or AutoML, Comprehend Custom builds customized NLP models on your behalf, using training data that you provide.","timestamp":"1702011960.0","comment_id":"917751","poster":"vbal","upvote_count":"1"},{"poster":"Mllb","comment_id":"860039","timestamp":"1696344300.0","upvote_count":"2","content":"Selected Answer: A\nComprehend can do Custom Classification"},{"comment_id":"860035","timestamp":"1696344240.0","upvote_count":"1","content":"Selected Answer: A\nComprehend can do Sentiment Analysis","poster":"Mllb"},{"comment_id":"837524","poster":"fez_2312","timestamp":"1694561340.0","upvote_count":"1","content":"The answer is C, because of the amount of data, and the time constraint. C is the most efficient solution. Conventionally A would be the right answer, but given the time constraint the answer is C."},{"content":"I would say blaze text. Cuz comprehend needs custom code, so we have only 2 days.","upvote_count":"2","poster":"alp_ileri","comment_id":"833522","timestamp":"1694216760.0"},{"timestamp":"1685189580.0","poster":"ystotest","comment_id":"728324","upvote_count":"1","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html"},{"poster":"cron0001","comments":[{"upvote_count":"8","poster":"f4bi4n","content":"its exactly the opposite, because its needs to be ready in 2 day I would use Comprehend ;) You don't need to write code, you have the data already available, so its faster then D","comment_id":"619420","timestamp":"1671567120.0"}],"comment_id":"591292","upvote_count":"3","content":"Selected Answer: D\nIf the problem needs to be solved in 2 days I would avoid going with any customised solution which would eliminate A and B. As the data is labelled already we don't need an unsupervised algorithm therefore eliminating C. Which leaves us with D","timestamp":"1666653900.0"}],"answer":"A","question_images":[],"answer_ET":"A","unix_timestamp":1650842700,"url":"https://www.examtopics.com/discussions/amazon/view/74394-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":52,"question_text":"A company sells thousands of products on a public website and wants to automatically identify products with potential durability problems. The company has\n1.000 reviews with date, star rating, review text, review summary, and customer email fields, but many reviews are incomplete and have empty fields. Each review has already been labeled with the correct durability result.\nA machine learning specialist must train a model to identify reviews expressing concerns over product durability. The first model needs to be trained and ready to review in 2 days.\nWhat is the MOST direct approach to solve this problem within 2 days?","timestamp":"2022-04-25 01:25:00","choices":{"B":"Build a recurrent neural network (RNN) in Amazon SageMaker by using Gluon and Apache MXNet.","D":"Use a built-in seq2seq model in Amazon SageMaker.","A":"Train a custom classifier by using Amazon Comprehend.","C":"Train a built-in BlazingText model using Word2Vec mode in Amazon SageMaker."},"topic":"1","answer_description":"","answers_community":["A (74%)","C (18%)","8%"],"isMC":true},{"id":"9wuavxhsUVN9ZjCCacq2","exam_id":26,"question_images":[],"question_id":53,"answer_images":[],"choices":{"C":"Use the AMAZON.SearchQuery built-in slot types for custom searches in the database.","A":"Add the unrecognized words in the enumeration values list as new values in the slot type.","B":"Create a new custom slot type, add the unrecognized words to this slot type as enumeration values, and use this slot type for the slot.","D":"Add the unrecognized words as synonyms in the custom slot type."},"answer_ET":"D","answer":"D","answers_community":["D (100%)"],"timestamp":"2022-04-22 04:50:00","url":"https://www.examtopics.com/discussions/amazon/view/74078-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer_description":"","unix_timestamp":1650595800,"discussion":[{"content":"Selected Answer: D\nD is the answer.\n\nThe unrecognized words are synonyms for \"comedy\", so they should be added as synonyms under the comedy slot type\n\nsee the excerpt:\n\n\"For each intent, you can specify parameters that indicate the information that the intent needs to fulfill the user's request. These parameters, or slots, have a type. A slot type is a list of values that Amazon Lex uses to train the machine learning model to recognize values for a slot. For example, you can define a slot type called \"Genres.\" Each value in the slot type is the name of a genre, \"comedy,\" \"adventure,\" \"documentary,\" etc. You can define a synonym for a slot type value. For example, you can define the synonyms \"funny\" and \"humorous\" for the value \"comedy.\"\"\n\nhttps://docs.aws.amazon.com/lex/latest/dg/howitworks-custom-slots.html","comment_id":"622863","timestamp":"1687825620.0","poster":"ovokpus","upvote_count":"12"},{"comment_id":"589693","poster":"knightknt","upvote_count":"9","comments":[{"content":"https://docs.aws.amazon.com/lex/latest/dg/howitworks-custom-slots.html","timestamp":"1683526740.0","poster":"cognito_22","upvote_count":"5","comment_id":"598457"}],"timestamp":"1682131800.0","content":"D? can not be C.Amazon Lex doesn't support the AMAZON.LITERAL or the AMAZON.SearchQuery built-in slot types. https://docs.aws.amazon.com/lex/latest/dg/howitworks-builtins-slots.html"},{"content":"Selected Answer: D\nD is the answer.","timestamp":"1729874460.0","upvote_count":"1","poster":"cyberfriends","comment_id":"1053879"},{"poster":"Mickey321","comment_id":"991410","content":"Selected Answer: D\nThe best way to fix the problem is option D, add the unrecognized words as synonyms in the custom slot type. By doing so, you can map different words that have the same meaning to the same slot value, without changing the Lambda code or data in DynamoDB. For example, you can add “funny”, “fun”, and “humor” as synonyms for the slot value “comedy”. This way, Amazon Lex can understand the category spoken by users and pass it to the Lambda function that queries the DynamoDB table for a list of book titles.\n\nOption A, adding the unrecognized words in the enumeration values list as new values in the slot type, is not a good choice because it would create new slot values that do not match the existing categories in the DynamoDB table. For example, if you add “funny” as a new value in the slot type, Amazon Lex would pass it to the Lambda function, which would not find any book titles for that category in the DynamoDB table.","timestamp":"1724757780.0","upvote_count":"2"},{"content":"C is the answer\n\nAMAZON.SearchQuery\n\nAs you think about what users are likely to ask, consider using a built-in or custom slot type to capture user input that is more predictable, and the AMAZON.SearchQuery slot type to capture less-predictable input that makes up the search query.\n\nThe following example shows an intent schema for SearchIntent, which uses the AMAZON.SearchQuery slot type and also includes a CityList slot that uses the AMAZON.City slot type.\n\nMake sure that your skill uses no more than one AMAZON.SearchQuery slot per intent. The Amazon.SearchQuery slot type cannot be combined with another intent slot in sample utterances.\n\nEach sample utterance must include a carrier phrase. The exception is that you can omit the carrier phrase in slot samples. A carrier phrase is the word or words that are part of the utterance, but not the slot, such as \"search for\" or \"find out\".","upvote_count":"1","timestamp":"1719888840.0","poster":"worldboss","comment_id":"940477"},{"comment_id":"811796","timestamp":"1708168800.0","content":"Selected Answer: D\nThe ML specialist should add the unrecognized words as synonyms in the custom slot type. This will allow Amazon Lex to understand the user's intent even if they use synonyms for the predefined slot values. By adding the synonyms, Amazon Lex will recognize them as variations of the predefined slot values and map them to the appropriate slot value. This approach can be a quick and effective way to improve the accuracy of the chatbot's understanding of user requests without having to change the Lambda code or the data in DynamoDB.","upvote_count":"2","poster":"AjoseO"},{"upvote_count":"2","comment_id":"616490","timestamp":"1686792300.0","content":"B is correct","poster":"[Removed]"}],"question_text":"A company that runs an online library is implementing a chatbot using Amazon Lex to provide book recommendations based on category. This intent is fulfilled by an AWS Lambda function that queries an Amazon DynamoDB table for a list of book titles, given a particular category. For testing, there are only three categories implemented as the custom slot types: \"comedy,\" \"adventure,` and \"documentary.`\nA machine learning (ML) specialist notices that sometimes the request cannot be fulfilled because Amazon Lex cannot understand the category spoken by users with utterances such as \"funny,\" \"fun,\" and \"humor.\" The ML specialist needs to fix the problem without changing the Lambda code or data in DynamoDB.\nHow should the ML specialist fix the problem?","isMC":true},{"id":"sHhgk3MYHSGkncdySfSF","answer_ET":"D","answer_images":[],"answer":"D","exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/74395-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","question_text":"A manufacturing company uses machine learning (ML) models to detect quality issues. The models use images that are taken of the company's product at the end of each production step. The company has thousands of machines at the production site that generate one image per second on average.\nThe company ran a successful pilot with a single manufacturing machine. For the pilot, ML specialists used an industrial PC that ran AWS IoT Greengrass with a long-running AWS Lambda function that uploaded the images to Amazon S3. The uploaded images invoked a Lambda function that was written in Python to perform inference by using an Amazon SageMaker endpoint that ran a custom model. The inference results were forwarded back to a web service that was hosted at the production site to prevent faulty products from being shipped.\nThe company scaled the solution out to all manufacturing machines by installing similarly configured industrial PCs on each production machine. However, latency for predictions increased beyond acceptable limits. Analysis shows that the internet connection is at its capacity limit.\nHow can the company resolve this issue MOST cost-effectively?","question_images":[],"choices":{"B":"Extend the long-running Lambda function that runs on AWS IoT Greengrass to compress the images and upload the compressed files to Amazon S3. Decompress the files by using a separate Lambda function that invokes the existing Lambda function to run the inference pipeline.","A":"Set up a 10 Gbps AWS Direct Connect connection between the production site and the nearest AWS Region. Use the Direct Connect connection to upload the images. Increase the size of the instances and the number of instances that are used by the SageMaker endpoint.","C":"Use auto scaling for SageMaker. Set up an AWS Direct Connect connection between the production site and the nearest AWS Region. Use the Direct Connect connection to upload the images.","D":"Deploy the Lambda function and the ML models onto the AWS IoT Greengrass core that is running on the industrial PCs that are installed on each machine. Extend the long-running Lambda function that runs on AWS IoT Greengrass to invoke the Lambda function with the captured images and run the inference on the edge component that forwards the results directly to the web service."},"isMC":true,"unix_timestamp":1650843180,"question_id":54,"discussion":[{"comment_id":"591298","poster":"cron0001","upvote_count":"17","timestamp":"1698190380.0","comments":[{"upvote_count":"3","comment_id":"671544","poster":"rb39","timestamp":"1710688620.0","content":"A-C: excluded out, Direct Connect is expensive"}],"content":"Selected Answer: D\nD is correct according to official documentation.\nhttps://docs.aws.amazon.com/greengrass/v1/developerguide/ml-inference.html"},{"content":"Selected Answer: D\nOption D eliminates the need for internet connection since the inference is done on the edge component, and the results are directly forwarded to the web service. \n\nThis approach also reduces the need for larger instances and direct connect connections, thus being the most cost-effective solution.","upvote_count":"6","timestamp":"1723886700.0","comment_id":"811797","poster":"AjoseO"}],"answers_community":["D (100%)"],"topic":"1","timestamp":"2022-04-25 01:33:00"},{"id":"IKzOAycUfQuIJQzf3xyn","question_text":"A data scientist is using an Amazon SageMaker notebook instance and needs to securely access data stored in a specific Amazon S3 bucket.\nHow should the data scientist accomplish this?","timestamp":"2022-05-02 20:08:00","topic":"1","answer_images":[],"question_images":[],"discussion":[{"poster":"tgaos","upvote_count":"9","comment_id":"608233","timestamp":"1669609500.0","content":"Agree with the Answer C. Attach the policy to the IAM roal associated with the notebook."},{"poster":"salads","comment_id":"648269","upvote_count":"8","timestamp":"1676700120.0","content":"Selected Answer: C\nc is the right answer"},{"timestamp":"1731938400.0","upvote_count":"1","content":"Selected Answer: C\nAmazon SageMaker notebook ARN , I don't think there is such a thing.\nSo A is not right .\nSo C","comment_id":"1213291","poster":"rav009"},{"upvote_count":"1","timestamp":"1720639740.0","content":"Selected Answer: C\nC. Attach policy to IAM role associated with the notebook: This is a standard and recommended approach in AWS. By attaching a policy to the IAM role that the SageMaker notebook instance assumes, you can precisely control the notebook's access to the specific S3 bucket. This method follows the AWS best practice of using IAM roles for managing permissions and also allows for easier management and scalability.\n\nA. Add an S3 bucket policy: This approach involves modifying the S3 bucket policy to grant permissions directly to the SageMaker notebook instance's ARN. While this method can effectively grant access, it is less flexible and scalable compared to using IAM roles. It directly ties the bucket's access policy to a specific resource (the notebook instance), which might not be ideal for managing access in a larger environment.","poster":"CloudHandsOn","comment_id":"1119050"},{"poster":"Mickey321","timestamp":"1709040600.0","content":"Selected Answer: C\nThe best way for the data scientist to securely access data stored in a specific Amazon S3 bucket from an Amazon SageMaker notebook instance is option C, attach the policy to the IAM role associated with the notebook that allows GetObject, PutObject, and ListBucket operations to the specific S3 bucket. By doing so, the data scientist can use IAM role-based access control to grant permissions to the notebook instance to access the S3 bucket without exposing any credentials or keys. The data scientist can also limit the scope of the permissions to only the necessary operations and resources, following the principle of least privilege.","upvote_count":"1","comment_id":"991413"},{"content":"Selected Answer: C\nOption A suggests adding an S3 bucket policy, but it is not the recommended way to grant permissions to specific IAM roles associated with SageMaker notebook instances. Bucket policies are generally used for granting cross-account access or public access, not for specifying access for specific IAM roles.","timestamp":"1706788140.0","upvote_count":"1","comment_id":"968926","poster":"ccpmad"},{"upvote_count":"2","timestamp":"1705694040.0","poster":"tigercorp","content":"An IAM policy cannot attach to an ARN. An IAM policy can only attach to an IAM role or an IAM user. So the answer is C","comment_id":"956883"},{"comment_id":"947514","timestamp":"1704835440.0","upvote_count":"1","comments":[{"poster":"mirik","comment_id":"947516","timestamp":"1704835680.0","upvote_count":"1","content":"On the other hand, in C they state \"specific S3 bucket\" and in the A - only \"an S3 bucket\". Maybe in A they add global policy to allow access to all S3 buckets?"}],"poster":"mirik","content":"Selected Answer: A\nA - we allow access to specific notebook. AIM role policy can be global and related to all user notebooks."},{"timestamp":"1697573220.0","content":"AC are both correct answer, but A is better than C, mostly due to the limitation of IAM policy.\nIAM policies: The maximum size of an IAM policy document is 6,144 characters. You can attach up to 10 policies to an IAM user, role, or group.","upvote_count":"1","poster":"ZSun","comment_id":"873061"},{"timestamp":"1692264420.0","comments":[{"timestamp":"1698351240.0","comment_id":"881999","content":"I dont agree with this. Restrict bucket access only to limited principal is much secure than grant specific IAM prinicap. Restrict specific principal eliminate other visits, but grant specific IAM user permission does not exclude other visit.","upvote_count":"1","poster":"ZSun"}],"comment_id":"811798","poster":"AjoseO","upvote_count":"1","content":"Selected Answer: C\nOption C ensures that the notebook instance is granted permission to access the S3 bucket without the need to provide credentials.\n\nOption A is incorrect because it suggests adding a bucket policy that grants permission to a specific IAM principal, which is less secure than granting permission to an IAM role."},{"content":"12-sep exam","poster":"Shailendraa","comment_id":"667354","upvote_count":"4","timestamp":"1678653300.0"},{"poster":"[Removed]","upvote_count":"4","content":"C is correct","timestamp":"1671062760.0","comment_id":"616417"},{"timestamp":"1668594060.0","comment_id":"602493","poster":"edvardo","comments":[{"comment_id":"722347","poster":"VinceCar","upvote_count":"1","content":"For A, only some operations are allowed, no specified users or roles have been granted this permission for these operations.","timestamp":"1684544640.0"},{"upvote_count":"1","content":"I am not sure but in question we don't have cross-account situation?","poster":"dunhill","timestamp":"1683385680.0","comment_id":"712490"},{"timestamp":"1682672460.0","poster":"colin1919","comment_id":"706297","upvote_count":"1","content":"Based on this logic indeed A would be better."}],"content":"Selected Answer: A\nQuoting the book \"Data Science on AWS\":\n\"Generally, we would use IAM identity-based policies if we need to define permissions for more than just S3, or if we have a number of S3 buckets, each with different permissions requirements. We might want to keep access control policies in the IAM environment.\n\nWe would use S3 bucket policies if we need a simple way to grant cross-account access to our S3 environment without using IAM roles, or if we reach the size limit for our IAM policy. We might want to keep access control policies in the S3 environment.\"\n\nA would be the choice then.","upvote_count":"3"},{"poster":"ayatkhrisat","comments":[{"comment_id":"716411","content":"Only \"securely access\" is required, not encryption.","upvote_count":"1","timestamp":"1683845760.0","poster":"VinceCar"}],"content":"Selected Answer: B\nB is the answer","timestamp":"1668028080.0","upvote_count":"1","comment_id":"599238"},{"poster":"bluer1","upvote_count":"2","comment_id":"596172","timestamp":"1667419680.0","content":"A - for me"}],"answer_ET":"C","question_id":55,"choices":{"B":"Encrypt the objects in the S3 bucket with a custom AWS Key Management Service (AWS KMS) key that only the notebook owner has access to.","D":"Use a script in a lifecycle configuration to configure the AWS CLI on the instance with an access key ID and secret.","C":"Attach the policy to the IAM role associated with the notebook that allows GetObject, PutObject, and ListBucket operations to the specific S3 bucket.","A":"Add an S3 bucket policy allowing GetObject, PutObject, and ListBucket permissions to the Amazon SageMaker notebook ARN as principal."},"exam_id":26,"isMC":true,"answers_community":["C (72%)","A (22%)","6%"],"unix_timestamp":1651514880,"answer_description":"","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/75091-exam-aws-certified-machine-learning-specialty-topic-1/"}],"exam":{"name":"AWS Certified Machine Learning - Specialty","lastUpdated":"11 Apr 2025","id":26,"numberOfQuestions":369,"isBeta":false,"isImplemented":true,"isMCOnly":false,"provider":"Amazon"},"currentPage":11},"__N_SSP":true}