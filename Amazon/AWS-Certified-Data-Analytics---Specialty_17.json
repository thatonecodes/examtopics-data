{"pageProps":{"questions":[{"id":"x04YkJIsrSOk8h4PNBJl","discussion":[{"timestamp":"1650538560.0","comment_id":"589270","content":"Selected Answer: A\nAgree with answer A as C&D was eliminated due to the last accessed rather than created for Lifecycle policy.\nBy compressing you save cost and converting to columnar data, performance is increased.","upvote_count":"11","poster":"astalavista1"},{"upvote_count":"6","timestamp":"1667584020.0","comment_id":"711296","poster":"cloudlearnerhere","content":"Selected Answer: A\nCorrect answer is A as columnar data format store data efficiently by employing column-wise compression and enables split and parallel processing. Storing processed data in S3 in SA-IA and moving raw data in Glacier would help reduce costs.\n\nOption B & D is wrong as it is recommended to use columnar data format for processing.\n\nOptions C is wrong as lifecycle rules are based on Object creation data and not last date when the object was accessed."},{"content":"Selected Answer: A\ncolumnar and based on object creation time","poster":"GLam123","upvote_count":"1","timestamp":"1698525420.0","comment_id":"1056435"},{"timestamp":"1690915200.0","poster":"NikkyDicky","comment_id":"969305","upvote_count":"1","content":"Selected Answer: A\nA make sense"},{"content":"A: I passed the test","upvote_count":"2","comment_id":"886291","poster":"pk349","timestamp":"1682947500.0"},{"content":"Selected Answer: A\nIt should be based on object creation, not based on object access","poster":"Arka_01","upvote_count":"1","comment_id":"678434","timestamp":"1664081700.0"},{"timestamp":"1658380080.0","comment_id":"634365","poster":"rocky48","content":"Selected Answer: A\nAnswer is A","upvote_count":"1"},{"content":"Selected Answer: A\nshould be 5 years after object creation to Infrequent for processed data\nand 7 days after object creation to glacier for raw data\n\nThere is no point of counting days from \"Last accessed\"","poster":"ru4aws","comment_id":"631609","upvote_count":"2","timestamp":"1657858440.0"},{"comment_id":"626910","poster":"dushmantha","content":"Selected Answer: A\nColumnar data is a way of optimizing (eleminate B, D). And the lifecycle policy should be assigned after object creation (eleminate C). Ans is A","timestamp":"1656924060.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nAnswer is A","timestamp":"1653205800.0","poster":"Bik000","comment_id":"605260"},{"timestamp":"1650463740.0","content":"ans should be A","comment_id":"588791","comments":[{"comment_id":"589269","poster":"astalavista1","content":"Agree with answer A as C&D was eliminated due to the last accessed rather than created for Lifecycle policy.\nBy compressing you save cost and converting to columnar data, performance is increased.","upvote_count":"1","timestamp":"1650538500.0"}],"poster":"azi_2021","upvote_count":"2"}],"question_text":"A company wants to optimize the cost of its data and analytics platform. The company is ingesting a number of .csv and JSON files in Amazon S3 from various data sources. Incoming data is expected to be 50 GB each day. The company is using Amazon Athena to query the raw data in Amazon S3 directly. Most queries aggregate data from the past 12 months, and data that is older than 5 years is infrequently queried. The typical query scans about 500 MB of data and is expected to return results in less than 1 minute. The raw data must be retained indefinitely for compliance requirements.\nWhich solution meets the company's requirements?","question_id":81,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/73913-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1","answer_images":[],"choices":{"D":"Use an AWS Glue ETL job to partition and convert the data into a row-based data format. Use Athena to query the processed dataset. Configure a lifecycle policy to move the data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after the object was last accessed. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after the last date the object was accessed.","A":"Use an AWS Glue ETL job to compress, partition, and convert the data into a columnar data format. Use Athena to query the processed dataset. Configure a lifecycle policy to move the processed data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after object creation. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after object creation.","B":"Use an AWS Glue ETL job to partition and convert the data into a row-based data format. Use Athena to query the processed dataset. Configure a lifecycle policy to move the data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after object creation. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after object creation.","C":"Use an AWS Glue ETL job to compress, partition, and convert the data into a columnar data format. Use Athena to query the processed dataset. Configure a lifecycle policy to move the processed data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after the object was last accessed. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after the last date the object was accessed."},"isMC":true,"exam_id":20,"timestamp":"2022-04-20 16:09:00","answer_description":"","answer_ET":"A","answer":"A","answers_community":["A (100%)"],"unix_timestamp":1650463740},{"id":"887FbwChH2BoGbtzPubX","answer_ET":"C","question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/73985-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_text":"An energy company collects voltage data in real time from sensors that are attached to buildings. The company wants to receive notifications when a sequence of two voltage drops is detected within 10 minutes of a sudden voltage increase at the same building. All notifications must be delivered as quickly as possible. The system must be highly available. The company needs a solution that will automatically scale when this monitoring feature is implemented in other cities. The notification system is subscribed to an Amazon Simple Notification Service (Amazon SNS) topic for remediation.\nWhich solution will meet these requirements?","choices":{"D":"Create an Amazon Kinesis data stream to capture the incoming sensor data. Create another stream for notifications. Set up AWS Application Auto Scaling on both streams. Create an Amazon Kinesis Data Analytics for Java application to detect the known event sequence, and add a message to the message stream Configure an AWS Lambda function to poll the message stream and publish to the SNS topic.","C":"Create an Amazon Kinesis Data Firehose delivery stream to capture the incoming sensor data. Use an AWS Lambda transformation function to detect the known event sequence and send the SNS message.","A":"Create an Amazon Managed Streaming for Apache Kafka cluster to ingest the data. Use an Apache Spark Streaming with Apache Kafka consumer API in an automatically scaled Amazon EMR cluster to process the incoming data. Use the Spark Streaming application to detect the known event sequence and send the SNS message.","B":"Create a REST-based web service by using Amazon API Gateway in front of an AWS Lambda function. Create an Amazon RDS for PostgreSQL database with sufficient Provisioned IOPS to meet current demand. Configure the Lambda function to store incoming events in the RDS for PostgreSQL database, query the latest data to detect the known event sequence, and send the SNS message."},"isMC":true,"discussion":[{"timestamp":"1650520920.0","content":"Answer = D","comment_id":"589125","poster":"CHRIS12722222","upvote_count":"28"},{"upvote_count":"16","poster":"morpheus23","timestamp":"1656021480.0","content":"It's C. Your confusing the \"immediately\" and ignoring that prior to that it's an event that happens as a sequence within 10 minutes, so firehose is a viable option.\n\nAWS auto scaling does not support Amazon Kinesis so that rules out D\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/integrated-services-list.html","comments":[{"comments":[{"timestamp":"1695132480.0","poster":"chinmayj213","content":"for Kinesis Data Stream we can use on demand option instead of provision that will able to handle","upvote_count":"1","comment_id":"1011409"}],"content":"It seems AWS does support application auto-scaling for Kinesis Data streams since Nov 2018. https://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/","poster":"Soumya92","upvote_count":"2","timestamp":"1691499360.0","comment_id":"975677"},{"timestamp":"1678113240.0","comment_id":"830905","upvote_count":"1","content":"I think C is correct: https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html#services-kinesis-windows","poster":"flanfranco","comments":[{"poster":"flanfranco","comments":[{"timestamp":"1694495100.0","poster":"zbyroger0902","comment_id":"1005396","content":"The two links seems to be evidence of why lambda is not the right choice for the question. It is stated in the two links you provided that lambda cannot deal with stateful computing except for tumbling window in the streams , and pattern recognition as the question asked for is obvious example of stateful computing that tumbling window is not applicable.","upvote_count":"3"}],"timestamp":"1678113360.0","upvote_count":"1","content":"https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/","comment_id":"830910"}]},{"poster":"siju13","comment_id":"630028","timestamp":"1657544040.0","content":"no, within the 10 minutes, the user wants to be notified immediately when the second event happens","upvote_count":"6"}],"comment_id":"621342"},{"comment_id":"1183033","poster":"tsangckl","upvote_count":"2","timestamp":"1711429020.0","content":"Selected Answer: D\nBing \nOption D is correct because it provides a highly available, scalable, and real-time solution. Amazon Kinesis Data Streams can capture and process large streams of data records in real time. AWS Application Auto Scaling can automatically adjust capacity to maintain steady, predictable performance at the lowest possible cost. Amazon Kinesis Data Analytics can process and analyze streaming data using standard SQL, and AWS Lambda can run your code in response to events and automatically manage the compute resources for you.\nOther options are not the best solutions for this scenario. For example, Options A and B involve using technologies that may not provide the real-time processing required for this use case. Option C does not provide a mechanism for detecting the known event sequence in real time."},{"poster":"NarenKA","upvote_count":"4","timestamp":"1708508400.0","content":"Selected Answer: D\nI will go with D as KDS is designed to handle massive streams of real-time data and can scale automatically to match the volume of data input and KDA processes of streaming data using SQL and analyse the incoming data stream for patterns, such as the sequence of voltage drops within 10 minutes of a voltage increase. KDA can scale based on demand, supporting the expansion to other cities. AWS Lambda to poll a notification stream and then publish to the Amazon SNS topic for immediate action upon detecting the specified event.\nOption A involves managing an Apache Kafka cluster and an EMR cluster, which adds complexity and operational overhead. B uses a REST-based web service and RDS, might not scale as seamlessly and could introduce latency in detecting and notifying about the events. C serverless, does not offer the same level of real-time processing and pattern detection capabilities needed for this specific use case as KDA.","comment_id":"1155395"},{"content":"kinesis data stream don't support aws auto-scaling, but like everything that happens in the cloud, there exists a way to scale your kds based on lambda and aws auto-scaling... then the D question is not completely wrong...\n\nemr is not highly available, then the A question is killed in this fact...\n\nthis problem announces real-time data streaming, but we discard kdf... not because kdf, but because lambda function... in near-real-time data processing using kdf and lambda with a consumer, just the records streamed in 1MB or 60s are processed by lambda...\n\nb makes no sense...\n\nthis is a poor question and they should be removed from this dump...","poster":"GCPereira","comment_id":"1119127","timestamp":"1704927660.0","upvote_count":"2"},{"upvote_count":"2","content":"The answer is D. It has a requirement of \" All notifications must be delivered as quickly as possible.\". That requirement rules out Firehose as it can have a delay for buffering and Kinesis data stream is real time.","comment_id":"1105800","timestamp":"1703577600.0","poster":"blackgamer"},{"upvote_count":"2","content":"Selected Answer: C\nC. Option D mention \"Set up AWS Application Auto Scaling on both streams\" and this is not possible. Here is not mention that you enable on demand capacity... no, it mentions \"AWS Application Auto Scaling\". And what about this part? \"Create another stream for notifications.\" It is not needed!\nOption C is the correct one","comment_id":"1043687","timestamp":"1697312220.0","poster":"gofavad926"},{"poster":"zanhsieh","upvote_count":"3","comment_id":"995851","content":"Selected Answer: C\nVote C.\nA: No, as EMR is not HA. Single region only.\nB: No, RDS is not ideal for streaming.\nC: Yes. All systems are fully managed.\nD: No. Although there is a solution in 2018 (https://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/) but it can’t scale more 10 times per 24-hrs nor scale double current shard.","timestamp":"1693556760.0"},{"poster":"geekfrosty","timestamp":"1691896320.0","upvote_count":"1","content":"C can't be right, it asks for \"real-time\", Firehouse collects before it pushes the data out which makes it \"near real-time\". Answer should be D","comment_id":"979714"},{"timestamp":"1690915680.0","poster":"NikkyDicky","comment_id":"969311","content":"Selected Answer: C\nC, because of non-HA EMR","upvote_count":"2"},{"comment_id":"929322","timestamp":"1687342200.0","content":"Selected Answer: C\nEMR is an incorrect answer because it is not high-available","poster":"EueChan","upvote_count":"2"},{"poster":"theriderzone","comment_id":"926514","upvote_count":"2","content":"Selected Answer: C\nEMR clusters dy default is not highly available.","timestamp":"1687074180.0"},{"timestamp":"1684820700.0","poster":"Debi_mishra","upvote_count":"2","comment_id":"904599","content":"Very good question. Both A and C are correct. But A can be ruled out because it uses EMR and out of box its not a highly available service, which is a critical requirement here."},{"comment_id":"886294","upvote_count":"2","timestamp":"1682947560.0","content":"C: I passed the test","poster":"pk349"},{"comment_id":"882788","upvote_count":"2","poster":"uk_dataguy","content":"Selected Answer: C\nAnswer = C\nAccording to https://aws.amazon.com/kinesis/data-firehose/faqs/\nIt is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.","timestamp":"1682610840.0"},{"content":"Selected Answer: C\nC is correct","timestamp":"1679435280.0","comment_id":"846398","upvote_count":"2","poster":"rich_knp"},{"timestamp":"1677048780.0","poster":"srirnag","comments":[{"content":"but emr is not high available","poster":"GCPereira","upvote_count":"1","timestamp":"1704927060.0","comment_id":"1119117"}],"content":"Its A By Elimination. \n\nB is not ideal for streams. \nC. Is not immediate. We need real time thing. \nD. Way too complex. Not sure if we would need additional Stream, Application auto scalling will scale Lambda. Where did Java come from? I believe there is a hard limit for KDS too. \n\nMSK wins on the scalability front. Spark streaming Application can call SNS SDK. Unbelievable, but it is A.","upvote_count":"3","comment_id":"817541"},{"comment_id":"815201","poster":"chokirock","timestamp":"1676895840.0","content":"I think it's D, I found you can scale Kinesis Data Streams with the commented feature:\n\nhttps://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/","upvote_count":"4"},{"comments":[{"content":"Sure, that's not D.\nAWS doesn't have any built-in Autoscaling for Kinesis Data Streams; it is available using AWS Lambda or some third-party apps.","upvote_count":"1","poster":"enoted","comment_id":"806801","timestamp":"1676234700.0"}],"poster":"ethaniel","content":"Selected Answer: D\nAnswer D: AWS Application Autoscaling is available for Kinesis Data Streams; Data Analytics can be developed with Scala / Java / SQL + analyses/logics to be applied based on time windows point to using Data Analytics.","comment_id":"797404","timestamp":"1675460340.0","upvote_count":"5"},{"poster":"rocky48","content":"Selected Answer: D\n\nThe voltage data can be sent to a Kinesis Data Stream in real-time from the sensors.\nA Lambda function can be triggered by the data stream and perform the logic to detect the sequence of voltage drops and sudden voltage increase within 10 minutes, using the Kinesis Data Streams API for consuming the data.\nIf a match is found, the Lambda function can publish a message to an SNS topic for remediation.\nThe Lambda function can be configured to automatically scale based on the amount of incoming data.\nKinesis Data Streams can automatically scale to handle the data stream, and it is highly available.\nThis solution allows the company to quickly receive notifications and automatically scale as the monitoring feature is implemented in other cities.","comment_id":"777412","upvote_count":"3","timestamp":"1673853600.0","comments":[{"poster":"rocky48","upvote_count":"1","timestamp":"1675362000.0","content":"Writing only Java application looks like trap answer, switching to A","comment_id":"796316"}]},{"upvote_count":"1","poster":"gopi_data_guy","comment_id":"775510","content":"Though Option D is not an answer for this question, option D architecture looks ideal. There is no autoscaling shard option for Kinesis data stream but we can implement it using Lambda, SNS to call UpdateShardCount API\nSee this: https://aws.amazon.com/blogs/big-data/auto-scaling-amazon-kinesis-data-streams-using-amazon-cloudwatch-and-aws-lambda/\n\nEliminating C since the use case need to calculate based on 10 minutes time window. I don't think Firehose with lambda can achieve this ordering. \n\nAnswer: A","timestamp":"1673706660.0"},{"poster":"DeerSong","content":"Answer is D. Since KDS doesn't support auto scaling, you have to configure auto scaling manually.","timestamp":"1673098860.0","comment_id":"768590","upvote_count":"2"},{"timestamp":"1672783380.0","comment_id":"765095","upvote_count":"1","poster":"sarycz","content":"In the AWS Skill Builder exam preparatory course, there was one question and EMR was an incorrect answer because it is not high-available. Thus D, even though the autoscaling statement looks strange."},{"timestamp":"1670978520.0","comment_id":"744591","upvote_count":"2","content":"Selected Answer: A\nAnswer = A","poster":"jpneves"},{"upvote_count":"1","comments":[{"upvote_count":"1","poster":"silvaa360","timestamp":"1670855700.0","comment_id":"742915","content":"Just read a comment, and even with the transformation lambda trying to workaround this, it is not possible to have more than 5 minutes of execution. I'm even more certain that must be A.\n\nPS: if it is A, it is one of the first questions where MSK will be choosen over Kinesis, and that is the only thing that will remain in my head to not have 100% certainty."},{"poster":"silvaa360","content":"This question confused me so much that, after so much study and exam questions, I was thinking about a Lambda as a delivery destination. This is WRONG, as KDF will not deliver to lambda, so for me C is completely ruled out. A must be the answer.","comment_id":"742924","timestamp":"1670856120.0","upvote_count":"1"}],"content":"- There is no auto-scaling in Kinesis. There is a way to do it, but not via \"AWS Application\" (tbh I don't even know what this means). So this rules out D.\n- Lambda transformation function in KDF is meant to do transformations and not to deliver data. Please note that they say \"transformation lambda\". I'm hopping that they are writing that on purpose to tell us that they are mentioning the lambda that can be used do do transformations in KDF and not the actual sink/deliver service that can be used. This would rule out C.\n- Between B and A, I know for a fact that Spark Streaming can be used to do that, but I very unsure if a KDF with a lambda function (as deliver and not as transformation) using a 10min batch would not work as well.\n\nConclusion: between A and C, if we want to risk on AWS services, should be C, but for me the best solution is A.","timestamp":"1670855460.0","poster":"silvaa360","comment_id":"742906"},{"upvote_count":"2","content":"Selected Answer: A\nAlthough not optimal from AWS services perspective this is the technologies that are build for such scenarios Kafka and Spark Streaming","poster":"nadavw","timestamp":"1668604440.0","comment_id":"719671"},{"upvote_count":"4","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/msk/latest/developerguide/serverless.html","comment_id":"717261","poster":"Quekky","timestamp":"1668337860.0"},{"content":"C is incorrect - what if 2nd event comes in 10 mins and firehose will take at least 1 minute to process and send notification via lambda. The requirement of 10 minutes is for window not the SLA for notification.\nD - Seems to be write but Data Stream is pub sub, why the hell one would use two data streams for same data?\nA - is not ideal specially in AWS context but this is feasible. this is not managed but can have automated by programming. But not sure.","comment_id":"712174","poster":"Ashish1101","timestamp":"1667718240.0","upvote_count":"1"},{"poster":"cloudlearnerhere","comments":[{"content":"no,\nminimal buffer interval for Data Firehose is 60 seconds - that's breaking the requirement to transfer data immediately.","comment_id":"806807","timestamp":"1676234940.0","upvote_count":"1","poster":"enoted"}],"timestamp":"1667584620.0","comment_id":"711297","upvote_count":"4","content":"Selected Answer: C\nC is the correct as only Amazon Kinesis Data Firehose can automatically scales from given options"},{"timestamp":"1666702680.0","poster":"thirukudil","comment_id":"703878","content":"Selected Answer: D\nD. kinesis data stream will stream the data in real-time. Kinesis data analytics can analyze the data for last 10 mins using tumbling window and write in DEST stream which in turn process by lamba and send to SNS. Auto scaling of the data streams are managed by JAVA app. This solution is simple and effective than other solutions.","upvote_count":"1","comments":[{"comment_id":"829971","timestamp":"1678026540.0","poster":"Paulv82003","content":"You answer is correct for most part but unfortunately:\n1. answer D does not call for Kinesis data analytics but for some weird 2 KDSs\n2. we should use Sliding Windows instead of tumbling window","upvote_count":"1"}]},{"content":"Selected Answer: A\nIt should be A, Kafka can do storage scale when new cities joined.\nWhy so many people select C, firehose cannot even guarantee the order of the events.\nAnd the 10 minutes is the time window, not the time of notification.","comment_id":"690654","upvote_count":"2","timestamp":"1665367980.0","poster":"rav009"},{"comment_id":"678437","timestamp":"1664081880.0","upvote_count":"3","content":"Selected Answer: C\nOther answers do not make sense. AWS Kinesis data firehose can utilize lambda to send notification to SNS and can scale automatically.","poster":"Arka_01"},{"upvote_count":"2","comment_id":"677524","timestamp":"1663977900.0","poster":"he11ow0rId","content":"Selected Answer: D\nAnswer = D"},{"poster":"Abep","content":"Selected Answer: A\nSelected A. Kafka and Spark streaming can scale when new cities are added. Though this might seem over engineered. \n\nMy thoughts why option \"C\" didn't appeal to me was\n1. Kinesis Firehose transformation lambda has a timeout of 5 mins instead of the entire 15 mins. This prevents the lambda from caching an entire 10 minute rolling window for stream analysis. https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html#data-transformation-execution-duration\n\n2. if the data is ingested to an external datastore, and doing a rolling window query will be expensive as the table grows, not to mention the growth in cities in future.\n\n3. Firehose is a delivery stream service, where target should be a sink. Option \"C\" projects it as data stream solution instead.","comments":[{"comments":[{"comment_id":"1190070","content":"AWS Application Auto Scaling lets you define scaling policies that can automatically add or remove shards to an Amazon Kinesis Data Stream. You can create a CloudWatch alarm that monitors Kinesis Data Stream shard metrics. When a custom threshold of the alarm is reached, for example because the number of requests has grown, the alarm is fired. This firing sends a notification to an Application Auto Scaling policy that responds based on the scaling preference. When the scaling policy is triggered, Application Auto Scaling calls the UpdateShardCount API operation. The call passes the new number of Kinesis Data Stream shards for the desired capacity.(FOM JON BONSSO DODJ PRACTICE TEST)","timestamp":"1712347320.0","upvote_count":"1","poster":"TheEnquirer"},{"poster":"enoted","upvote_count":"1","content":"MSK is able to scale automatically\nhttps://docs.aws.amazon.com/msk/latest/developerguide/msk-autoexpand.html","timestamp":"1676235720.0","comment_id":"806815"}],"poster":"Rejju","comment_id":"706993","upvote_count":"2","content":"but Kafka and KDS can not scale automatically, and only KDF can. and that's what makes me pick option C.","timestamp":"1667022840.0"}],"upvote_count":"5","timestamp":"1662437700.0","comment_id":"660794"},{"upvote_count":"1","timestamp":"1661343000.0","comment_id":"651235","content":"Selected Answer: D\nanswer is def D. However, A would also work.","poster":"redwan123"},{"comment_id":"638496","comments":[{"poster":"rocky48","comments":[],"comment_id":"777410","upvote_count":"1","timestamp":"1673853540.0","content":"The voltage data can be sent to a Kinesis Data Stream in real-time from the sensors.\nA Lambda function can be triggered by the data stream and perform the logic to detect the sequence of voltage drops and sudden voltage increase within 10 minutes, using the Kinesis Data Streams API for consuming the data.\nIf a match is found, the Lambda function can publish a message to an SNS topic for remediation.\nThe Lambda function can be configured to automatically scale based on the amount of incoming data.\nKinesis Data Streams can automatically scale to handle the data stream, and it is highly available.\nThis solution allows the company to quickly receive notifications and automatically scale as the monitoring feature is implemented in other cities.\nAnswer: D"}],"content":"Selected Answer: C\nSelected Answer: C","timestamp":"1658986860.0","upvote_count":"2","poster":"rocky48"},{"comments":[{"content":"the 10 min is the time window of the event analysis, not the time to notification...","upvote_count":"2","comment_id":"690655","timestamp":"1665368040.0","poster":"rav009"}],"comment_id":"621567","poster":"Ramshizzle","content":"Selected Answer: C\nThe answer is C. \nFirehose + Lambda + SNS works fast enough (they want to be notified within 10min)\nFirehose+Lambda are highly available and scale very well. Nothing has to be done for them to scale.","upvote_count":"3","timestamp":"1656066060.0"},{"comment_id":"605560","content":"D\n\"immediately\" rules out firehose","timestamp":"1653229500.0","upvote_count":"2","poster":"fl0resi3nsis"},{"poster":"jazztarou","comment_id":"602713","content":"C　https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html","timestamp":"1652741820.0","upvote_count":"1"},{"comments":[{"comments":[{"upvote_count":"1","content":"the lambda in KDF is supposed to do transformations and not serve as sink. AWS wouldn't put a option that would makes us use their services in the wrong way.","timestamp":"1670709660.0","poster":"silvaa360","comment_id":"741265"}],"upvote_count":"1","content":"\"Kinesis Data Firehose supports a Lambda invocation time of up to 5 minutes.\"","comment_id":"698047","poster":"JoellaLi","timestamp":"1666081860.0"}],"timestamp":"1652658000.0","upvote_count":"1","comment_id":"602336","poster":"jazztarou","content":"Selected Answer: A\nd may make another upload for transmission, but the reason is unknown.\nI think c is wrong because lamda cannot be selected as the destination of firehose."},{"content":"Selected Answer: A\nKafka is alternative to Kinesis and it has auto scaling workers and High Availability. Hence A is the Answer","comments":[{"content":"Change mind to D","comments":[],"timestamp":"1650779580.0","upvote_count":"2","poster":"sanjee000","comment_id":"590900"}],"comment_id":"590894","timestamp":"1650778140.0","poster":"sanjee000","upvote_count":"2"},{"upvote_count":"7","poster":"rb39","content":"Selected Answer: C\nC - The company needs a solution that will automatically scale => Firehose","comment_id":"589990","timestamp":"1650631860.0"}],"exam_id":20,"answers_community":["C (51%)","A (27%)","D (22%)"],"timestamp":"2022-04-21 08:02:00","topic":"1","answer":"C","question_id":82,"unix_timestamp":1650520920,"answer_images":[]},{"id":"pyBFGanWlEbaoVTvvl9C","timestamp":"2022-04-22 04:08:00","discussion":[{"timestamp":"1650593280.0","content":"Selected Answer: D\nhttps://aws.amazon.com/kinesis/data-analytics/features/?pg=ln&sec=hs","poster":"ay12","upvote_count":"12","comment_id":"589681"},{"timestamp":"1708511700.0","content":"Selected Answer: D\nOptions A and C, which involve Kinesis Data Firehose with delivery to Amazon S3 and subsequent processing by AWS Lambda, are not optimized for near-real-time feedback due to the inherent latency in delivering data to S3 and then processing it. Option B, involving Amazon Managed Streaming for Apache Kafka and Kinesis Data Analytics for SQL, could also be a viable solution for real-time analytics, but the specific choice of Apache Flink in Option D is more directly aligned with the company's need for complex event processing and near-real-time analysis capabilities.","poster":"NarenKA","comment_id":"1155441","upvote_count":"1"},{"poster":"gofavad926","content":"Selected Answer: D\nD. Amazon Managed Service for Apache Flink","upvote_count":"1","timestamp":"1697312460.0","comment_id":"1043688"},{"content":"A & C cannot be option as it uses firehose, which has min record buffered time as 60 second / 1 minute and here we have to do processing in 30 second. So we left with B and D","upvote_count":"1","comment_id":"1011420","comments":[{"timestamp":"1695134160.0","content":"D seems correct as KDA using flink for processing (sql style)","poster":"chinmayj213","upvote_count":"1","comment_id":"1011425"}],"poster":"chinmayj213","timestamp":"1695133800.0"},{"content":"D. AWS KDA doesn't take input from Kafka, only if KDA is provisioned using Flink, then Kafka can be a source.","timestamp":"1690180860.0","upvote_count":"1","comment_id":"961232","poster":"penguins2"},{"content":"D: I passed the test","comment_id":"886297","comments":[{"poster":"roymunson","content":"Was the \"test\" about typing the same comment in every single discussion to show people how you realy wasting your time with stupid actions on the internet?","upvote_count":"5","timestamp":"1699991640.0","comment_id":"1070802"}],"timestamp":"1682947620.0","poster":"pk349","upvote_count":"1"},{"timestamp":"1678636320.0","comments":[{"poster":"AwsNewPeople","upvote_count":"2","comment_id":"837177","content":"But since the question never ask for Cost optimisation, I will go with D","timestamp":"1678636380.0"}],"upvote_count":"1","comment_id":"837176","poster":"AwsNewPeople","content":"Option B is the most suitable solution as it allows the company to stream the data in real-time to Amazon Managed Streaming for Apache Kafka. Then, the data can be processed and analyzed using Amazon Kinesis Data Analytics for SQL Application, which can handle data in JSON format with dynamic schema changes. This solution allows the company to identify playback issues in near-real-time within 30 seconds.\n\nOption A is not optimal because it requires an S3 event to trigger the Lambda function, which may introduce some latency. Option C is similar to option A, but with an additional step of writing the data to S3, which may not be necessary. Option D uses Apache Flink, which may be overkill for this use case and can be more complex to set up compared to the SQL-based Kinesis Data Analytics application in option B."},{"content":"Selected Answer: B\nAccording to https://aws.amazon.com/kinesis/data-analytics/faqs/\nSome keywords are \"json formatted data\", \"schema update\". These are what Kinesis Data Analytics SQL applications do. And Kinesis Data Analytics can also get data from MSK. Also MSK is a valid data source for KDA SQL Application based on below. https://aws.amazon.com/kinesis/data-analytics/?nc=sn&loc=1","poster":"ota123","comment_id":"768838","upvote_count":"1","timestamp":"1673118180.0","comments":[{"upvote_count":"1","poster":"nadavw","content":"The preferred service may be the Kinesis Video stream, which is intended for playback. The problem with D is the message size limit in KDS (1MB), while media can be larger.\n\nhttps://aws.amazon.com/kinesis/video-streams/?nc=sn&loc=0&amazon-kinesis-video-streams-resources-blog.sort-by=item.additionalFields.createdDate&amazon-kinesis-video-streams-resources-blog.sort-order=desc#:~:text=Amazon%20Kinesis%20Video%20Streams%20makes%20it%20easy%20to%20securely%20stream%20video%20from%20connected%20devices%20to%20AWS%20for%20analytics%2C%20machine%20learning%20(ML)%2C%20playback%2C%20and%20other%20processing","comment_id":"776550","timestamp":"1673786820.0"}]},{"timestamp":"1665371520.0","upvote_count":"4","poster":"rav009","content":"D \nFirehose can be ruled out for it has 60 sec data latency.\nKDA for SQL cannot support MSK as source.","comment_id":"690679"},{"upvote_count":"2","comment_id":"678438","content":"Selected Answer: D\nAs the allowed time offset is of 30 seconds, we can eliminate options with fireshose. Kafka is third party, and not a preferred answer.","comments":[{"comment_id":"698064","upvote_count":"2","timestamp":"1666083360.0","poster":"JoellaLi","content":"This is not the reason for not selecting B, since 'Amazon Managed Streaming for Apache Kafka(MSK)' is a AWS service not third party."}],"timestamp":"1664082000.0","poster":"Arka_01"},{"content":"Ans is D.\nOption A & C - Firehose and its minimum buffer time is 60 sec\nOption B - Will work, But JSON schema changes over time. Kinesis Data Analytics requires manual intervention to update column mapping if input changes.","timestamp":"1656711240.0","upvote_count":"4","poster":"Sen5476","comment_id":"625873"},{"poster":"Bik000","timestamp":"1653205860.0","upvote_count":"1","content":"Selected Answer: D\nAnswer is D","comment_id":"605261"},{"content":"How does Managed Kafka and Kinesis Analytics even interact with each other? Correct ans is D.","timestamp":"1653123780.0","poster":"certificationJunkie","comment_id":"604774","upvote_count":"1"},{"poster":"MWL","timestamp":"1652272320.0","comment_id":"600121","upvote_count":"2","content":"Selected Answer: D\nChanged to D.\nKDA for SQL can not set MSK as source. KDF can not process in 30 seconds."},{"comment_id":"597175","comments":[{"comments":[{"upvote_count":"1","timestamp":"1670138700.0","poster":"shammous","content":"Also, data is in JSON format, and the schema can change. SQL is useless in this case","comment_id":"734908"}],"upvote_count":"3","content":"Change to D, KDA for SQL can not set MSK as source. KDA for Flink can.","comment_id":"600117","poster":"MWL","timestamp":"1652272260.0"}],"content":"Selected Answer: B\nVote for B.\nAccording to https://aws.amazon.com/kinesis/data-analytics/faqs/\nSome keywords are \"json formatted data\", \"schema update\". These are what Kinesis Data Analytics SQL applications do. And Kinesis Data Analytics can also get data from MSK.","upvote_count":"2","timestamp":"1651735080.0","poster":"MWL"},{"upvote_count":"1","timestamp":"1651349220.0","comment_id":"595251","poster":"jrheen","content":"D- looks good"}],"choices":{"B":"Send the data to Amazon Managed Streaming for Apache Kafka. Configure Amazon Kinesis Data Analytics for SQL Application as the consumer application to process and analyze the data.","A":"Send the data to Amazon Kinesis Data Firehose with delivery to Amazon S3. Configure an S3 event to invoke an AWS Lambda function to process and analyze the data.","C":"Send the data to Amazon Kinesis Data Firehose with delivery to Amazon S3. Configure Amazon S3 to initiate an event for AWS Lambda to process and analyze the data.","D":"Send the data to Amazon Kinesis Data Streams. Configure an Amazon Kinesis Data Analytics for Apache Flink application as the consumer application to process and analyze the data."},"topic":"1","question_text":"A media company has a streaming playback application. The company needs to collect and analyze data to provide near-real-time feedback on playback issues within 30 seconds. The company requires a consumer application to identify playback issues, such as decreased quality during a specified time frame. The data will be streamed in JSON format. The schema can change over time.\nWhich solution will meet these requirements?","isMC":true,"exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/74075-exam-aws-certified-data-analytics-specialty-topic-1-question/","unix_timestamp":1650593280,"question_id":83,"answer":"D","answer_ET":"D","answers_community":["D (86%)","14%"],"question_images":[],"answer_images":[],"answer_description":""},{"id":"Nzgidp8AHYcmhGlBn7Wb","answer_description":"","question_id":84,"question_text":"An ecommerce company stores customer purchase data in Amazon RDS. The company wants a solution to store and analyze historical data. The most recent 6 months of data will be queried frequently for analytics workloads. This data is several terabytes large. Once a month, historical data for the last 5 years must be accessible and will be joined with the more recent data. The company wants to optimize performance and cost.\nWhich storage solution will meet these requirements?","answer_ET":"D","question_images":[],"discussion":[{"comment_id":"162833","upvote_count":"20","content":"D seems correct","timestamp":"1632772020.0","poster":"abhineet"},{"poster":"Shraddha","comments":[{"poster":"Donell","upvote_count":"6","comment_id":"386297","timestamp":"1635107940.0","content":"Correct, answer is D.\nRedshift is suitable for running complex analytical queries. Athena is suitable for small ad-hoc queries."}],"upvote_count":"17","content":"Ans D\nNote: A and B are immediately out because RDS is not for analysis. C and D both work, but D is balanced between performance and cost. C may cost less (depending on data compression, frequency of queries) but query to recent data will be slower.","timestamp":"1634997780.0","comment_id":"383904"},{"comment_id":"1119162","timestamp":"1704931320.0","content":"when we say \"analytics queries\", \"analytics workloads\" or \"complex analysis\", in 80% of the cases we call redshift... if we sum a short period of analysis (6 months in this case) redshift is a better option... rds will continue as a relational database, don't run analytics queries","poster":"GCPereira","upvote_count":"1"},{"comment_id":"886298","poster":"pk349","upvote_count":"2","content":"D: I passed the test","timestamp":"1682947680.0","comments":[{"comments":[{"poster":"DipeshGandhi131","content":"hahah!!","upvote_count":"2","timestamp":"1687926240.0","comment_id":"936069"}],"upvote_count":"5","comment_id":"904118","timestamp":"1684764300.0","poster":"Espa","content":"I see you have been posting only I passed the test :)"}]},{"content":"Selected Answer: D\nOption D is the most suitable solution for this scenario.\n\nExplanation:\n\nIncrementally copy data from Amazon RDS to Amazon S3: This allows for storing of historical data in a cost-effective manner while allowing frequent querying of the more recent data in RDS.\nLoad and store the most recent 6 months of data in Amazon Redshift: This provides a performant solution for frequent queries of the most recent data.\nConfigure an Amazon Redshift Spectrum table to connect to all historical data: This enables joining of historical data with the more recent data in Redshift, providing the required analysis capability.\nOption A does not address the requirement to optimize performance for querying the most recent data. Option B involves creating a read replica of RDS, which may not be efficient for frequently queried data. Option C also does not provide a solution for frequent querying of the most recent data.","upvote_count":"2","timestamp":"1678637340.0","comment_id":"837197","poster":"AwsNewPeople"},{"upvote_count":"1","comment_id":"822005","timestamp":"1677377940.0","content":"Option D suggests copying data from RDS to S3 incrementally, storing the most recent 6 months of data in Amazon Redshift, and configuring an Amazon Redshift Spectrum table to connect to all historical data. This approach allows the company to optimize cost and performance as Redshift is a cost-effective data warehousing solution that can handle large volumes of data. Additionally, using Redshift Spectrum enables the company to query both the recent and historical data sets together in real-time.\n\nOption A suggests creating a read replica of the RDS database to store the most recent 6 months of data and copying the historical data into Amazon S3. This approach does not allow for real-time querying of the historical data and may result in increased query latency.","poster":"rags1482"},{"poster":"murali12180","content":"Selected Answer: A\nA. By moving the data to S3 and Glue Catalog that carries both RDS and S3 schema will enable them to use the same schema for queries. Remember the requirement says \"low cost\". Redshift is out of the picture.","comment_id":"806876","upvote_count":"1","timestamp":"1676242380.0","comments":[{"content":"I don't think read replicas for certain months can be created. Read replicas will replicate entire db. Unlikely A is the answer","timestamp":"1678538820.0","upvote_count":"1","comment_id":"835983","poster":"aws_kid"}]},{"comment_id":"797122","upvote_count":"1","timestamp":"1675436280.0","content":"Selected Answer: D\nD for the win","poster":"BtotheJ"},{"comment_id":"711800","timestamp":"1667657640.0","upvote_count":"3","content":"D is the right answer as loading and querying recent 6 months of data via Redshift gives better performance and old data can be queried via Redshift spectrum\nC is wrong though it's possible to query the entire data in S3 using Athena, however, it will not be able to match the high performance offered by Redshift to query the last six months of data. So this option is not the best fit for the given use case.\n\nOptions A & B are wrong as RDS is not an ideal solution to store and query historical data. Also, 6 months data may be several terabytes large.","poster":"cloudlearnerhere"},{"poster":"aefuen1","content":"Selected Answer: D\nD seems correct","comment_id":"704289","upvote_count":"1","timestamp":"1666746120.0"},{"poster":"rocky48","comment_id":"634941","timestamp":"1658455020.0","upvote_count":"1","content":"Selected Answer: D\nAnswer-D"},{"poster":"jrheen","upvote_count":"1","timestamp":"1651356480.0","comment_id":"595316","content":"Answer-D"},{"content":"Selected Answer: D\nD seems correct","timestamp":"1648394400.0","poster":"simonaque","upvote_count":"1","comment_id":"576283"},{"content":"D is correct...","poster":"ShilaP","upvote_count":"1","comment_id":"571558","timestamp":"1647774840.0"},{"content":"Selected Answer: D\neffective way to query across S3 and RDS is using redshift spectrum","comment_id":"562286","poster":"Agn3001","upvote_count":"1","timestamp":"1646603040.0"},{"content":"Historical Data points to Redshift Spectrum. Hence D","upvote_count":"1","comment_id":"507294","timestamp":"1640197740.0","poster":"umatrilok"},{"comment_id":"482597","upvote_count":"1","poster":"aws2019","timestamp":"1637417760.0","content":"answer is D."},{"timestamp":"1635304380.0","content":"B is clearly wrong because data need to join.\nC is quite wrong because data in S3 is not up to date with the RDS.\nD is very nice architecture but launching a Redshift Cluster for just 6 month data is not practical.\nA is correct. You can use federated query. It is cost-saving and for ecommerce site, there is not much complicated analytics.","upvote_count":"3","comments":[{"content":"the question says:\nMost recent Data will be queried often with analytical workloads. If this is not suitable for redshift what else?\nAnd for the historical it is recommending Spectrum.\nJust perfect answer in my opinion.","upvote_count":"1","timestamp":"1635990480.0","comment_id":"411353","poster":"Hariru"},{"content":"You can't create subset of a data in read replicas. A clearly is wrong","timestamp":"1682060280.0","poster":"aws_kid","upvote_count":"1","comment_id":"876247"}],"poster":"Huy","comment_id":"388596"},{"timestamp":"1634764080.0","upvote_count":"1","content":"This link gives good comparison https://www.upsolver.com/blog/aws-athena-pricing-redshift-comparison","comment_id":"337197","poster":"SuperSundra"},{"content":"Answer C. If you are using redshift cluster, then no need to use S3 to store historical data. Redshift is made to store historical data and analyze. Refer to FAQs Q: When should you use a full featured enterprise data warehouse, like Amazon Redshift vs. a query service like Amazon Athena? .. With Option D, data gets stored in S3 along with storing 6 months of data in Redshift cluster. Along with S3 storage cost, redshift cluster costs and Redshift spectrum costs.. It is not optimized solution for cost.. Athena can do complex joins, analysis as well. With option C, it is S3 storage cost and Athena pay as you go query scan cost.","upvote_count":"2","poster":"SuperSundra","comment_id":"337189","timestamp":"1634337840.0","comments":[{"poster":"Hariru","timestamp":"1636130220.0","content":"Redshift Spectrum is created for historical data, and this data will be stored in s3.","upvote_count":"1","comment_id":"411354"}]},{"content":"Answer is A. loading data from S3 to Redshift is not optimal for performance and not cost-effective compared to just using the read replica. A cheaper option is to use the AWS Glue Data Catalog to query data on both Amazon RDS and Amazon S3.\nhttps://aws.amazon.com/blogs/big-data/analyzing-data-in-s3-using-amazon-athena/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ExportSnapshot.html","timestamp":"1634048820.0","poster":"kalyan_krishna742020","comment_id":"291325","upvote_count":"3"},{"poster":"lostsoul07","content":"D is the right answer","upvote_count":"2","comment_id":"274280","timestamp":"1633710360.0"},{"timestamp":"1633660560.0","content":"Going for D.\nC can be OK, but the question does not indicate the use of Glue crawler for RDS or use of federated queries to allow Athena to query RDS.","poster":"Draco31","comment_id":"237295","upvote_count":"2"},{"comment_id":"226765","content":"Most likely is D. Most cost optimized is C, but \"queried frequently for analytics\" is not possible with athena. Athena is \"adhoc-queries\" and not \"analytics\". so its D.","timestamp":"1633620840.0","poster":"hans1234","upvote_count":"2"},{"poster":"BillyC","comment_id":"216783","content":"D is corret for me","upvote_count":"2","timestamp":"1633443840.0"},{"upvote_count":"2","timestamp":"1633353120.0","poster":"jove","content":"D - This is perfect use case for Redshift and Spectrum.","comment_id":"206807"},{"content":"Task can be achieved by C and D both. But it requires one in month so Option-C is more cost effective than Option-D BUT Option-D would perform better than Option-C. With that confusion, I will go with option-C (Cost effective)\n\nTo validate option-C works:\nhttps://www.amazonaws.cn/en/athena/faqs/\nQ: Why should I upgrade to AWS Glue Data Catalog?","upvote_count":"3","timestamp":"1633250580.0","comment_id":"204820","poster":"sanjaym"},{"poster":"syu31svc","upvote_count":"2","timestamp":"1633168320.0","content":"Answer is C\nS3 would provide a cost effective storage solution and with GlueData Catalog and Athena would allow to query the data both for 6 months and historical","comment_id":"192836","comments":[{"comments":[{"content":"redshift is optimized for petabytes data queries, for terabytes of data performance is almost equal between athenand redshift. Athena is cheaper than redshift","comment_id":"408362","timestamp":"1635797220.0","upvote_count":"1","poster":"Gekko"}],"upvote_count":"1","content":"The most recent 6 months of data will be queried frequently for analytics workloads...note \"queried frequently\" and hence for better performance and cost move to redshift","comment_id":"204738","timestamp":"1633247040.0","poster":"Warrior001"}]},{"timestamp":"1633104900.0","content":"Option D, historical data should be stored on s3 and joined with most recent data from Redshift using spectrum (external tables)","poster":"Karan_Sharma","comment_id":"179075","upvote_count":"4"},{"comment_id":"175523","upvote_count":"1","poster":"Paitan","timestamp":"1632870420.0","content":"I will go with option D."},{"upvote_count":"1","poster":"GauravM17","timestamp":"1632862320.0","content":"D I believe","comment_id":"174183"}],"exam_id":20,"timestamp":"2020-08-20 17:31:00","unix_timestamp":1597937460,"isMC":true,"answers_community":["D (88%)","13%"],"answer_images":[],"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/29144-exam-aws-certified-data-analytics-specialty-topic-1-question/","choices":{"D":"Incrementally copy data from Amazon RDS to Amazon S3. Load and store the most recent 6 months of data in Amazon Redshift. Configure an Amazon Redshift Spectrum table to connect to all historical data.","B":"Use an ETL tool to incrementally load the most recent 6 months of data into an Amazon Redshift cluster. Run more frequent queries against this cluster. Create a read replica of the RDS database to run queries on the historical data.","A":"Create a read replica of the RDS database to store the most recent 6 months of data. Copy the historical data into Amazon S3. Create an AWS Glue Data Catalog of the data in Amazon S3 and Amazon RDS. Run historical queries using Amazon Athena.","C":"Incrementally copy data from Amazon RDS to Amazon S3. Create an AWS Glue Data Catalog of the data in Amazon S3. Use Amazon Athena to query the data."},"topic":"1"},{"id":"uF7ZQNIqtXmbdOyBN2OZ","choices":{"B":"Create an Athena workgroup for each given use case, apply tags to the workgroup, and create an IAM policy using the tags to apply appropriate permissions to the workgroup.","C":"Create an IAM role for each given use case, assign appropriate permissions to the role for the given use case, and add the role to associate the role with Athena.","D":"Create an AWS Glue Data Catalog resource policy for each given use case that grants permissions to appropriate individual IAM users, and apply the resource policy to the specific tables used by Athena.","A":"Create an S3 bucket for each given use case, create an S3 bucket policy that grants permissions to appropriate individual IAM users. and apply the S3 bucket policy to the S3 bucket."},"discussion":[{"timestamp":"1632313860.0","content":"B any thoughts?\nhttps://docs.aws.amazon.com/athena/latest/ug/user-created-workgroups.html","poster":"Priyanka_01","comment_id":"158912","upvote_count":"29"},{"upvote_count":"10","poster":"jersyl","content":"I think it's B. based on this link:\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/","comment_id":"158987","timestamp":"1632320700.0"},{"comment_id":"1155448","content":"Selected Answer: B\nCorrect answer is B - creating Athena workgroups for each use case, tagging those workgroups, and applying IAM policies based on those tags is the most effective way to meet the company's security and compliance requirements.\n\nOption A focuses on S3 bucket policies, which do not directly address the separation of query execution within Athena.\nOption C involves creating IAM roles for use cases but does not inherently separate query execution and history within Athena itself.\nOption D pertains to AWS Glue Data Catalog resource policies, which, while important for controlling access to data, do not directly manage the separation of Athena query execution and history.","upvote_count":"1","poster":"NarenKA","timestamp":"1708512780.0"},{"poster":"pk349","timestamp":"1682947740.0","content":"B: I passed the test","comment_id":"886300","upvote_count":"2"},{"poster":"AwsNewPeople","timestamp":"1678637460.0","upvote_count":"3","content":"Selected Answer: B\nThe solution that meets the requirements to separate query execution and query history among users, teams, or applications running in the same AWS account is to create an Athena workgroup for each given use case, apply tags to the workgroup, and create an IAM policy using the tags to apply appropriate permissions to the workgroup. This allows the company to control access to specific workgroups and apply different permissions to different groups. Option B is therefore the correct answer.\n\nOption A is not a suitable solution as creating S3 buckets for each use case would not effectively control access to Athena queries and history.\n\nOption C is not a suitable solution as creating an IAM role for each use case would not allow for granular control over permissions and would not effectively separate query execution and history.\n\nOption D is not a suitable solution as creating an AWS Glue Data Catalog resource policy would not effectively separate query execution and history within Athena.","comment_id":"837198"},{"timestamp":"1678539000.0","poster":"aws_kid","comment_id":"835988","content":"Does this site deliberately provide wrong answer choice?","upvote_count":"4"},{"comment_id":"711801","timestamp":"1667657760.0","poster":"cloudlearnerhere","content":"Correct answer is B as Athena Workgroup can help comply with the internal security policies by separating execution for users, teams, applications and applying access control and auditing.","upvote_count":"4"},{"upvote_count":"1","comment_id":"641338","poster":"lordpizzo","content":"Selected Answer: B\nwithout a doubt, the best way to guarantee isolation and better control over history, cost and the lowest possible level of access is to create workgroups for athena. Letter B without a doubt!","timestamp":"1659456180.0"},{"poster":"msa11a","upvote_count":"1","content":"Selected Answer: B\nAthena group","timestamp":"1659153600.0","comment_id":"639465"},{"content":"Selected Answer: B\nB is correct","poster":"awsexpert69","upvote_count":"1","timestamp":"1658847060.0","comment_id":"637489"},{"upvote_count":"1","poster":"bp339","timestamp":"1658338620.0","content":"Selected Answer: B\nAthena Workgroups","comment_id":"634161"},{"timestamp":"1654458780.0","upvote_count":"1","poster":"abdelawwal","comment_id":"612014","content":"B \nbased on that link:\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/"},{"poster":"Bik000","content":"Selected Answer: B\nAnswer should be B","comment_id":"605275","upvote_count":"1","timestamp":"1653206340.0"},{"timestamp":"1649765280.0","poster":"jrheen","content":"B is correct","upvote_count":"1","comment_id":"584697"},{"comment_id":"578065","poster":"jrheen","content":"Selected Answer: B\nB is correct","timestamp":"1648618140.0","upvote_count":"3"},{"content":"I think B is the correct answer","comment_id":"575668","poster":"pidkiller","timestamp":"1648314540.0","upvote_count":"1"},{"poster":"PravinT","comment_id":"556565","timestamp":"1645862700.0","upvote_count":"1","content":"B is the right answer"},{"content":"B is the correct Answer","comment_id":"546376","timestamp":"1644746700.0","poster":"RSSRAO","upvote_count":"1"},{"timestamp":"1642883220.0","comment_id":"530067","upvote_count":"1","content":"Answer is B","poster":"vkbajoria"},{"poster":"hess","content":"Selected Answer: B\nB is correct","upvote_count":"1","comment_id":"524475","timestamp":"1642285560.0"},{"timestamp":"1637849880.0","content":"Correct B!","comment_id":"486712","poster":"danigunawan","upvote_count":"1"},{"poster":"aws2019","timestamp":"1636926180.0","upvote_count":"1","content":"Answer is B","comment_id":"478351"},{"comment_id":"411357","content":"It has to be B","poster":"Hariru","timestamp":"1635739020.0","upvote_count":"1"},{"upvote_count":"5","poster":"Shraddha","content":"Ans B\nhttps://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html","timestamp":"1635725520.0","comment_id":"383906"},{"content":"B is the right answer","upvote_count":"3","comment_id":"274281","poster":"lostsoul07","timestamp":"1635621660.0"},{"upvote_count":"4","comment_id":"216780","timestamp":"1635365400.0","content":"B is correct for me","poster":"BillyC"},{"timestamp":"1635356760.0","content":"B it is..","upvote_count":"3","comment_id":"206810","poster":"jove"},{"timestamp":"1634418480.0","poster":"sanjaym","comment_id":"204848","upvote_count":"2","content":"Answer B.\n Amazon Athena Workgroups - A new resource type that can be used to separate query execution and query history between Users, Teams, or Applications running under the same AWS account\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/"},{"upvote_count":"2","timestamp":"1633154580.0","content":"Option B, Athena Workgroup is designed for this","comment_id":"179077","poster":"Karan_Sharma"},{"content":"I think it is B\nhttps://docs.aws.amazon.com/athena/latest/ug/tags-access-control.html","comment_id":"177178","poster":"KoMo","upvote_count":"2","timestamp":"1632888780.0"},{"upvote_count":"3","timestamp":"1632482040.0","comment_id":"175524","content":"Athena workgroup will do the trick. So option B for me.","poster":"Paitan"},{"poster":"zeronine","content":"B is my answer - https://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/\nhttps://docs.aws.amazon.com/athena/latest/ug/workgroups.html","upvote_count":"3","comment_id":"159895","timestamp":"1632424440.0"}],"answer_ET":"B","question_id":85,"answer":"B","answers_community":["B (100%)"],"question_text":"A company leverages Amazon Athena for ad-hoc queries against data stored in Amazon S3. The company wants to implement additional controls to separate query execution and query history among users, teams, or applications running in the same AWS account to comply with internal security policies.\nWhich solution meets these requirements?","topic":"1","unix_timestamp":1597538760,"question_images":[],"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/28685-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"answer_images":[],"timestamp":"2020-08-16 02:46:00"}],"exam":{"isMCOnly":true,"id":20,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":164,"provider":"Amazon","name":"AWS Certified Data Analytics - Specialty","isImplemented":true},"currentPage":17},"__N_SSP":true}