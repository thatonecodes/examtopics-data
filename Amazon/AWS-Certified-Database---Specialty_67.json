{"pageProps":{"questions":[{"id":"KlZOEyXy8AgZSDJliNoC","exam_id":22,"isMC":true,"question_images":[],"answer_description":"","answer":"C","answers_community":["C (100%)"],"question_id":331,"unix_timestamp":1595854440,"question_text":"A company is developing a multi-tier web application hosted on AWS using Amazon Aurora as the database. The application needs to be deployed to production and other non-production environments. A Database Specialist needs to specify different MasterUsername and MasterUserPassword properties in the AWS\nCloudFormation templates used for automated deployment. The CloudFormation templates are version controlled in the company's code repository. The company also needs to meet compliance requirement by routinely rotating its database master password for production.\nWhat is most secure solution to store the master password?","answer_ET":"C","answer_images":[],"discussion":[{"timestamp":"1633059120.0","content":"Agree with C\n\"By using the secure string support in CloudFormation with dynamic references you can better maintain your infrastructure as code. You’ll be able to avoid hard coding passwords into your templates and you can keep these runtime configuration parameters separated from your code. Moreover, when properly used, secure strings will help keep your development and production code as similar as possible, while continuing to make your infrastructure code suitable for continuous deployment pipelines.\"\nhttps://aws.amazon.com/blogs/mt/using-aws-systems-manager-parameter-store-secure-string-parameters-in-aws-cloudformation-templates/\n\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/","upvote_count":"10","poster":"BillyMadison","comment_id":"146960"},{"upvote_count":"7","comment_id":"144975","timestamp":"1632131100.0","poster":"BillyC","content":"C is correct"},{"content":"Selected Answer: C\nagree with C","timestamp":"1706777160.0","poster":"ychaabane","upvote_count":"1","comment_id":"1137406"},{"comment_id":"992592","poster":"Pranava_GCP","content":"Selected Answer: C\nC. Use the secretsmanager dynamic reference to retrieve the master password stored in AWS Secrets Manager and enable automatic rotation.","timestamp":"1693252380.0","upvote_count":"2"},{"timestamp":"1656076320.0","upvote_count":"1","poster":"ryuhei","comment_id":"621667","content":"Selected Answer: C\nAnswer:C"},{"poster":"novice_expert","upvote_count":"2","timestamp":"1651363080.0","comment_id":"595358","content":"Selected Answer: C\nC. Use the secretsmanager dynamic reference to retrieve the master password stored in AWS Secrets Manager and enable automatic rotation.\n\n\"By using the secure string support in CloudFormation with dynamic references you can better maintain your infrastructure as code. You’ll be able to avoid hard coding passwords into your templates and you can keep these runtime configuration parameters separated from your code. Moreover, when properly used, secure strings will help keep your development and production code as similar as possible, while continuing to make your infrastructure code suitable for continuous deployment pipelines.\""},{"upvote_count":"1","comment_id":"515962","poster":"selva1982","content":"C is correct","timestamp":"1641234300.0"},{"upvote_count":"1","content":"C is correct","timestamp":"1635696000.0","poster":"swarndeep","comment_id":"342255"},{"content":"C is the right answe","comment_id":"253507","poster":"JobinAkaJoe","timestamp":"1634510460.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1634136900.0","content":"Ans - C","comment_id":"212783","poster":"Ashoks"},{"comment_id":"159091","content":"Answer id C\nSSM does not supported automatic rotation","timestamp":"1634076900.0","upvote_count":"6","poster":"Ebi"},{"poster":"BillyC","timestamp":"1632376680.0","comment_id":"146528","upvote_count":"1","content":"Sorry D"}],"timestamp":"2020-07-27 14:54:00","url":"https://www.examtopics.com/discussions/amazon/view/26783-exam-aws-certified-database-specialty-topic-1-question-73/","topic":"1","choices":{"C":"Use the secretsmanager dynamic reference to retrieve the master password stored in AWS Secrets Manager and enable automatic rotation.","A":"Store the master password in a parameter file in each environment. Reference the environment-specific parameter file in the CloudFormation template.","B":"Encrypt the master password using an AWS KMS key. Store the encrypted master password in the CloudFormation template.","D":"Use the ssm dynamic reference to retrieve the master password stored in the AWS Systems Manager Parameter Store and enable automatic rotation."}},{"id":"m1weHkscGrVBYn08pnTu","url":"https://www.examtopics.com/discussions/amazon/view/26784-exam-aws-certified-database-specialty-topic-1-question-74/","answers_community":["AE (100%)"],"question_images":[],"unix_timestamp":1595854500,"answer_description":"","timestamp":"2020-07-27 14:55:00","isMC":true,"answer_images":[],"question_text":"A company is writing a new survey application to be used with a weekly televised game show. The application will be available for 2 hours each week. The company expects to receive over 500,000 entries every week, with each survey asking 2-3 multiple choice questions of each user. A Database Specialist needs to select a platform that is highly scalable for a large number of concurrent writes to handle the anticipated volume.\nWhich AWS services should the Database Specialist consider? (Choose two.)","exam_id":22,"answer":"AE","topic":"1","choices":{"A":"Amazon DynamoDB","E":"Amazon ElastiCache","D":"Amazon Elasticsearch Service","B":"Amazon Redshift","C":"Amazon Neptune"},"answer_ET":"AE","discussion":[{"comment_id":"144977","content":"Ans A and E are correct","poster":"BillyC","upvote_count":"8","timestamp":"1632525780.0"},{"comment_id":"446040","upvote_count":"5","poster":"guru_ji","timestamp":"1636271580.0","content":"I got this Question in exam."},{"comment_id":"992595","timestamp":"1693254000.0","poster":"Pranava_GCP","upvote_count":"1","content":"Selected Answer: AE\nA. Amazon DynamoDB\nE. Amazon ElastiCache"},{"poster":"f___16","comment_id":"902454","content":"My answer is AE too. But my question is can DynamoDB and Elasticache be used together? Are there many scenarios of using them both in the same application?","upvote_count":"2","timestamp":"1684571940.0"},{"timestamp":"1651415100.0","upvote_count":"4","poster":"novice_expert","comment_id":"595623","content":"Selected Answer: AE\nA. Amazon DynamoDB (for high responses 500K in 2 hours)\nE. Amazon ElastiCache (for high static reads 500K in 2 hours for few questions + options)"},{"upvote_count":"1","poster":"Shunpin","content":"Selected Answer: AE\nAdvantages and disadvantages of write-through\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.WriteThrough","comment_id":"511066","timestamp":"1640693400.0"},{"content":"Selected Answer: AE\nhttps://aws.amazon.com/products/databases/real-time-apps-elasticache-for-redis/","timestamp":"1640379600.0","comment_id":"508865","upvote_count":"3","poster":"jove"},{"comment_id":"427451","timestamp":"1636227180.0","content":"it is asking about the use case of each DB\nC. Amazon Neptune\nIt is not relevant, AWS Neptune is Graph database\nD. Amazon Elasticsearch Service\nit is not a use case, ES is using for log store/search/metric/config info/document list/ etc\nE. Amazon ElastiCache\nIt is used to cache the data for faster query (read) performance, usually using before a database\nA, B is my choice. Redshift is dataware house, but can be used as a data lake as high concurrent write.","comments":[{"comment_id":"801992","content":"redshift is not good with write-intensive use case","timestamp":"1675858440.0","upvote_count":"1","poster":"im_not_robot"},{"timestamp":"1646543460.0","upvote_count":"4","comment_id":"561784","poster":"RotterDam","content":"definitely not Redshift"},{"upvote_count":"1","poster":"jove","timestamp":"1640379420.0","content":"Redshift is not a good idea","comment_id":"508864"}],"upvote_count":"2","poster":"ChauPhan"},{"content":"AE would make sense if not this part: \"needs to select a platform that is highly scalable for a large number of concurrent writes\".\nI think AD is a better choice here.","timestamp":"1636047000.0","comment_id":"358850","poster":"Aesthet","upvote_count":"1"},{"timestamp":"1635638880.0","content":"Ans: AE","comment_id":"299098","upvote_count":"1","poster":"myutran"},{"content":"A, E for sure\nhttps://aws.amazon.com/products/databases/real-time-apps-elasticache-for-redis/","comment_id":"294444","upvote_count":"2","poster":"Exia","timestamp":"1635417240.0"},{"comment_id":"253511","upvote_count":"2","poster":"JobinAkaJoe","timestamp":"1634794200.0","content":"A. Amazon DynamoDB -- ideal for this requirement\nB. Amazon Redshift -- Wrong choice\nC. Amazon Neptune -- Not a requirement to have graph database\nD. Amazon Elasticsearch Service -- Not requirement for ElasticSearch\nE. Amazon ElastiCache -- The requirement is write intensive.. Not sure how Elasticache can help.\n\nA,E seem to be the best choice.","comments":[{"poster":"addixion","upvote_count":"1","content":"ElastiCache store the questions and multiple answers","comment_id":"287832","timestamp":"1635080280.0"}]},{"upvote_count":"2","comment_id":"212785","timestamp":"1633620900.0","content":"yes, A, E","poster":"Ashoks"},{"comment_id":"161770","poster":"BillyMadison","content":"AE \nhttps://aws.amazon.com/elasticache/\nBuilding real-time apps across versatile use cases like gaming, geospatial service, caching, session stores, or queuing, with advanced data structures, replication, and point-in-time snapshot support.\nhttps://aws.amazon.com/dynamodb/\nBuild powerful web applications that automatically scale up and down. You don't need to maintain servers, and your applications have automated high availability.","upvote_count":"4","timestamp":"1633568280.0"},{"content":"My ans is A and D. Why not E, E is store data in memory and normally we use it as a buffer server but not the server to store the data.\nD, Amazon Elasticsearch is Highly scalable.","timestamp":"1633164900.0","poster":"szmulder","upvote_count":"2","comments":[{"poster":"awsmonster","upvote_count":"1","content":"Amazon OpenSearch Service (successor to Amazon Elasticsearch Service)\nmakes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. \n\nSeems like an overkill for a 2-3 multiple question. \n\nI vote for AE","comment_id":"520447","timestamp":"1641759960.0"},{"content":"My opinion is the same as yours.","poster":"rootkim","comment_id":"246216","timestamp":"1633693500.0","upvote_count":"1"}],"comment_id":"159498"},{"upvote_count":"1","timestamp":"1632989640.0","poster":"Ebi","content":"Answer is AE","comment_id":"159095"}],"question_id":332},{"id":"vxWkVpgsyhIbcgps5A7v","answer":"A","isMC":true,"answers_community":["A (60%)","D (40%)"],"question_images":[],"question_id":333,"answer_images":[],"answer_description":"","unix_timestamp":1595869320,"question_text":"A company has migrated a single MySQL database to Amazon Aurora. The production data is hosted in a DB cluster in VPC_PROD, and 12 testing environments are hosted in VPC_TEST using the same AWS account. Testing results in minimal changes to the test data. The Development team wants each environment refreshed nightly so each test database contains fresh production data every day.\nWhich migration approach will be the fastest and most cost-effective to implement?","answer_ET":"A","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/26802-exam-aws-certified-database-specialty-topic-1-question-75/","timestamp":"2020-07-27 19:02:00","choices":{"B":"Run the master in Amazon Aurora MySQL. Take a nightly snapshot, and restore it into 12 databases in VPC_TEST using Aurora Serverless.","D":"Run the master in Amazon Aurora MySQL using Aurora Serverless. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly.","C":"Run the master in Amazon Aurora MySQL. Create 12 Aurora Replicas in VPC_TEST, and script the replicas to be deleted and re-created nightly.","A":"Run the master in Amazon Aurora MySQL. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly."},"discussion":[{"upvote_count":"13","timestamp":"1632735600.0","comment_id":"149677","comments":[{"poster":"Huy","timestamp":"1634534520.0","comment_id":"360085","upvote_count":"5","content":"Cloning is not supported on Aurora Serverless nor Cross-Region. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html","comments":[{"poster":"ChauPhan","upvote_count":"1","content":"No, it is cross-account, not cross-region. The question mentions same account\nYou can create an Aurora provisioned clone from a provisioned Aurora DB cluster. You can create an Aurora Serverless v1 clone from an Aurora Serverless v1 DB cluster. But you can also create Aurora Serverless v1 clones from Aurora provisioned DB clusters, and you can create provisioned clones from Aurora Serverless v1 DB clusters.\n\"CROSS-ACCOUNT cloning currently doesn't support cloning Aurora Serverless v1 DB clusters\"","comment_id":"427471","timestamp":"1634534820.0"}]}],"poster":"zanhsieh","content":"A. \nB dropped due to snapshot is slower (full disk dump) than clone (copy-on-write)\nC dropped due to no write on Aurora Replicas\nD dropped due to there’s no option for cloning in the console."},{"timestamp":"1705151280.0","content":"Selected Answer: A\nAnswer is A\nThe question does not mention variable workload, so I see no need for Serverless (D)","comment_id":"1121671","upvote_count":"1","poster":"MultiAZ"},{"upvote_count":"2","timestamp":"1691226720.0","content":"Selected Answer: A\nChoose between A and D.\nTo have Aurora Serverless we need to convert RDS Aurora MySQL to Aurora Serverless MySQL.\nWe can do it 3 ways:\n - Snapshot restore\n - Logical backup and restore\n - A new serverless reader (for Amazon Aurora PostgreSQL-Compatible Edition versions 13.6 and later). Add a serverless reader, force a failover. This promotes the reader instance to a writer instance.\nhttps://repost.aws/knowledge-center/aurora-migrate-provisioned-serverless\nWe are asked for \"the fastest\" solution, so, answer A.","poster":"IhorK","comment_id":"972829"},{"comment_id":"895163","poster":"Pankaj24hrs","content":"D \n\nhttps://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/\n\nAurora Serverless supports fast database cloning. You only pay for additional storage if you make data changes in the cloned DB cluster.\n\nIn question it mentioned \"Testing results in minimal changes to the test data\" so there will be a minimal cost for dev env databases. Most cost-effective.","upvote_count":"1","timestamp":"1683816900.0"},{"content":"A.\nit is possible to create an Aurora Cluster with a replica and then use it to create an Aurora Serverless cluster. Then use the Serverless cluster as the source to clone 12 DEV DB.\nNot choosing D is because D -- we can not choose Serverless as master primary DB because severless is for infrequently use","comment_id":"844415","poster":"Mintwater","upvote_count":"1","timestamp":"1679276520.0"},{"content":"A is fasted but B is most cost effective\nD is wrong due to it is not recommend to use serverless db for production.","timestamp":"1675858860.0","comment_id":"802000","poster":"im_not_robot","upvote_count":"2","comments":[{"timestamp":"1681119780.0","upvote_count":"1","content":"Agree A\nAgree your point - \" not recommend to use serverless db for production\"","comment_id":"866160","poster":"Mintwater"}]},{"poster":"guau","upvote_count":"2","content":"Selected Answer: D\nD Serverless is fastest and clone is supported","timestamp":"1675803180.0","comment_id":"801438"},{"content":"Selected Answer: D\nGo for D as Aurora Serverless support clonning since June 2021 \nhttps://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/","timestamp":"1674137280.0","comment_id":"781209","poster":"teo2157","upvote_count":"2"},{"content":"Selected Answer: A\nI chose A over D because I don't believe serverless will be cheaper necessarily.","comment_id":"751426","timestamp":"1671567060.0","upvote_count":"3","poster":"lollyj"},{"upvote_count":"2","timestamp":"1665960840.0","poster":"awsjjj","comment_id":"696625","content":"Selected Answer: D\naurora server less now supports cloning since june 2021. Question is abount cost effective. i am leanng towards D","comments":[{"comment_id":"696626","poster":"awsjjj","content":"Although Personally I wouldn't recommend Serverless for production workload with the limitations comes with aurora serverless . A is not a wrong answer either","upvote_count":"1","timestamp":"1665960960.0"}]},{"upvote_count":"3","comment_id":"683680","timestamp":"1664549340.0","content":"Selected Answer: A\nGoing to go with A. Aurora Serverless is a good fit for applications that are not expected to serve traffic on a regular basis, such as development or test environments. So in this case, moving the master to serverless seems kind of backwards.","poster":"JeanGat"},{"poster":"niau","comment_id":"632218","timestamp":"1657983300.0","content":"Selected Answer: A\nA. D It s not correct. Why move to serverless?","upvote_count":"3"},{"timestamp":"1651413240.0","comment_id":"595619","poster":"novice_expert","upvote_count":"2","content":"Selected Answer: D\nx A. Run the master in Amazon Aurora MySQL. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly. (right answer before June 2021 as option D's serverless did not allow clone earlier)\nx B. snapshot slow\nx C. Replica not for testing\nD. Run the master in Amazon Aurora MySQL using Aurora Serverless. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly. (right answer after 6/2021, serverless v1 supports cloning to same account)\n\nhttps://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html"},{"content":"A.\n\nAlthough the question does not mention any info about the production database. I am not convinced to move the production to Aurora Serverless, with these limitations in place:\n\n\nAurora Serverless v1 doesn't support the following features:\n Aurora global databases\n Aurora multi-master clusters\n Aurora Replicas\n AWS Identity and Access Management (IAM) database authentication\n Backtracking in Aurora\n Database activity streams\n Performance Insights","upvote_count":"4","comments":[{"content":"Severless V2 has not most of that limitations","comment_id":"801443","timestamp":"1675803300.0","poster":"guau","upvote_count":"1"}],"poster":"awsmonster","timestamp":"1641759540.0","comment_id":"520442"},{"content":"The best answer will be D after Jun. 21, 2021. According to https://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/,\nafter Jun. 21, 2021, Amazon Aurora allows you to create clones between Aurora Serverless v1 and provisioned Aurora DB clusters to enable quick sharing of data, i.e., you can create Aurora Serverless v1 clones from Aurora provisioned DB clusters, and you can also create provisioned clones from Aurora Serverless v1 DB clusters.","comment_id":"493809","poster":"scottkerker","timestamp":"1638636540.0","upvote_count":"3"},{"poster":"guru_ji","content":"I got this Question in exam.","upvote_count":"2","comment_id":"446056","timestamp":"1635763860.0"},{"content":"Answer: A","upvote_count":"2","timestamp":"1635505620.0","poster":"guru_ji","comment_id":"439024"},{"poster":"grekh001","upvote_count":"2","timestamp":"1635429300.0","comment_id":"435696","content":"https://aws.amazon.com/about-aws/whats-new/2021/06/amazon-aurora-serverless-v1-supports-fast-database-cloning/\n\nThis link seems to support D for \"get quick access to production data for development and testing\"\n\nI say D"},{"upvote_count":"2","poster":"aws4myself","comment_id":"433350","timestamp":"1635271680.0","content":"I will go with B, because snapshots are incremental, only 1st snapshot will take time and rest all are incremental, just changes will be added. We have snapshot now, will use aurora serverless for less cost.\n\nDefinitely, D is not, because it is asking about the clone solution to Test vpc, not prod vpc."},{"upvote_count":"1","comments":[{"content":"The load is NOT unpredictable.\nI don't agree with D.","comment_id":"430318","upvote_count":"1","poster":"guru_ji","timestamp":"1635212460.0"}],"content":"D is more cost-effective than C\nYou pay on a per-second basis for the database capacity you use when the database is active, and migrate between standard and serverless configurations","comment_id":"427478","poster":"ChauPhan","timestamp":"1634970600.0"},{"upvote_count":"2","comment_id":"345012","poster":"manan728","content":"Yup A is correct and this question was asked in my exam.","timestamp":"1634428080.0"},{"poster":"swarndeep","upvote_count":"2","comment_id":"342256","timestamp":"1634399220.0","content":"A looks like the correct answer"},{"content":"Ans: A","poster":"myutran","comment_id":"299104","timestamp":"1634371380.0","upvote_count":"2"},{"poster":"GeeBeeEl","timestamp":"1634216040.0","upvote_count":"1","comments":[{"timestamp":"1634652960.0","content":"D is more cost-effective because \"You pay on a per-second basis for the database capacity you use when the database is active, and migrate between standard and serverless configurations\"","comment_id":"427477","upvote_count":"3","poster":"ChauPhan"}],"comment_id":"294814","content":"To confirm A, see https://aws.amazon.com/blogs/aws/amazon-aurora-fast-database-cloning/ where it shows that cloning of 2TB can happen in 2 hours --- “If I had a 2TB database it could take hours just waiting for a copy of the data to be ready before I could perform my tasks. Even within RDS MySQL” With cloning, “my 2TB snapshot restore job that used to take an hour is now ready in about 5 minutes “ \nWhile A or D may be correct in terms of speed. D serverless is more expensive \nA is the answer"},{"comment_id":"273378","poster":"Glendon","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"427479","content":"Aurora supports many different types of cloning. You can create an Aurora provisioned clone from a provisioned Aurora DB cluster. You can create an Aurora Serverless v1 clone from an Aurora Serverless v1 DB cluster?????","timestamp":"1635041280.0","poster":"ChauPhan"}],"content":"Answer is A or D as the use of clones is correct (due to minimal changes to the test data). D is incorrect as you can't clone a Aurora Serverless cluster (Link: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html) \n\nTherefore, the answer is A.","timestamp":"1634145300.0"},{"content":"A is my choice","comment_id":"253512","poster":"JobinAkaJoe","upvote_count":"1","timestamp":"1633779840.0"},{"poster":"Ashoks","content":"Ans is A","comment_id":"212796","timestamp":"1633764000.0","upvote_count":"2"},{"comment_id":"161774","upvote_count":"4","timestamp":"1633087920.0","poster":"BillyMadison","comments":[{"poster":"ChauPhan","content":"The description: The production data is hosted in a DB cluster in VPC_PROD, and 12 testing environments are hosted in VPC_TEST USING THE SAME AWS ACCOUNT.","timestamp":"1635079920.0","upvote_count":"1","comment_id":"427480"}],"content":"Going with A. Definitely not B that is slow. Not D you cant clone serverless.\nhttps://www.jeremydaly.com/aurora-serverless-the-good-the-bad-and-the-scalable/\n“You can’t clone an Aurora Serverless cluster across AWS accounts.”\nhttps://aws.amazon.com/blogs/aws/amazon-aurora-fast-database-cloning/\n“ Fast Database Cloning. By taking advantage of Aurora’s underlying distributed storage engine you’re able to quickly and cheaply create a copy-on-write clone of your database.”\n” Even within RDS MySQL, I would still have to wait several hours for a snapshot copy to complete before I was able to test a schema migration or perform some analytics.”\nhttps://aws.amazon.com/rds/aurora/mysql-features/\nAmazon Aurora supports quick, efficient cloning operations, where entire multi-terabyte database clusters can be cloned in minutes. Cloning is useful for a number of purposes including application development, testing, database updates, and running analytical queries."},{"upvote_count":"4","content":"Answer is A","comment_id":"159098","poster":"Ebi","timestamp":"1632927960.0"},{"timestamp":"1632443760.0","poster":"steves","comments":[{"upvote_count":"1","comments":[{"upvote_count":"1","poster":"RBSK","timestamp":"1665875700.0","content":"If its least expensive, Is not Serverless is cheaper??","comment_id":"695780"}],"content":"Question is about the least expensive option which is option A","comment_id":"510472","poster":"jove","timestamp":"1640623200.0"}],"upvote_count":"2","content":"Ans: A or D.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html","comment_id":"145165"}],"exam_id":22},{"id":"INtQmEMCP3L1fr3Qh2VZ","answer_images":[],"answer_ET":"D","timestamp":"2020-07-13 19:33:00","exam_id":22,"unix_timestamp":1594661580,"question_images":[],"question_id":334,"choices":{"C":"Set an AWS Application Auto Scaling policy for the table to handle the increase in traffic","A":"Ensure the table is always provisioned to meet peak needs","D":"Preprovision additional capacity for the known peaks and then reduce the capacity after the event","B":"Allow burst capacity to handle the additional load"},"answers_community":["D (54%)","C (46%)"],"url":"https://www.examtopics.com/discussions/amazon/view/25628-exam-aws-certified-database-specialty-topic-1-question-76/","answer_description":"","topic":"1","answer":"D","question_text":"A large ecommerce company uses Amazon DynamoDB to handle the transactions on its web portal. Traffic patterns throughout the year are usually stable; however, a large event is planned. The company knows that traffic will increase by up to 10 times the normal load over the 3-day event. When sale prices are published during the event, traffic will spike rapidly.\nHow should a Database Specialist ensure DynamoDB can handle the increased traffic?","isMC":true,"discussion":[{"content":"C is the correct.\nD is not correct because in Dynamodb when you scale up the capacity your data partition will increase accorssing to your RCU and WCU, but when you scale down the partition remain unchnaged, so the per table RCU and WCU will give poor performance. \nI think Auto Scaling is the correct way is such situation.","timestamp":"1656655020.0","comments":[{"timestamp":"1656935940.0","content":"correct, I'm surprised that no one talk about it. Once you add more capacity, it's really hard to reduce","upvote_count":"7","comment_id":"626975","poster":"minhntm"},{"upvote_count":"2","comment_id":"870876","poster":"Mintwater","comments":[{"comments":[{"upvote_count":"1","content":"https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html and https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\n\nWith Application Auto Scaling, you create a scaling policy for a table or a global secondary index. The scaling policy specifies whether you want to scale read capacity or write capacity (or both), and the minimum and maximum provisioned capacity unit settings for the table or index.\n\n\nAnswer: C","timestamp":"1691909040.0","comment_id":"979818","poster":"kerl"}],"timestamp":"1689190320.0","poster":"leotoras","upvote_count":"1","content":"this document regards EC2 auto scaling, not DynamoDB scaling","comment_id":"950140"}],"timestamp":"1681558260.0","content":"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-tutorial.html\nAfter completing this tutorial, you’ll know how to:\n\nUse scheduled scaling to add extra capacity to meet a heavy load before it arrives, and then remove the extra capacity when it's no longer required.\n\nUse a target tracking scaling policy to scale your application based on current resource utilization.\nVote for C"}],"comment_id":"625593","poster":"sachin","upvote_count":"22"},{"poster":"BillyMadison","comment_id":"146972","upvote_count":"15","content":"I'm going with D because we know about the increased traffic in advance because it will be due to a sale. \nBurst capacity is fine for unknown spikes up to 5 minutes. This even is for 3 days. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html#bp-partition-key-throughput-bursting\n\"DynamoDB provides some flexibility in your per-partition throughput provisioning by providing burst capacity. Whenever you're not fully using a partition's throughput, DynamoDB reserves a portion of that unused capacity for later bursts of throughput to handle usage spikes.\nDynamoDB currently retains up to 5 minutes (300 seconds) of unused read and write capacity. During an occasional burst of read or write activity, these extra capacity units can be consumed quickly—even faster than the per-second provisioned throughput capacity that you've defined for your table.\nDynamoDB can also consume burst capacity for background maintenance and other tasks without prior notice.\nNote that these burst capacity details might change in the future.\"","timestamp":"1633070880.0"},{"timestamp":"1705151520.0","comment_id":"1121675","poster":"MultiAZ","content":"Selected Answer: D\nThe answer is D, as we know about the event beforehand\nFurthermore, C will have an issue getting 10x performance quickly enough because of the cooldown.","upvote_count":"1"},{"comment_id":"1112136","poster":"sonu6252","timestamp":"1704220140.0","content":"D.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","upvote_count":"1"},{"content":"Selected Answer: C\nscheduled auto-scaling","poster":"rrshah83","upvote_count":"1","comment_id":"1106136","timestamp":"1703604600.0"},{"upvote_count":"1","content":"D is correct, because prewarm:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand","timestamp":"1698104580.0","poster":"Santix","comment_id":"1052355"},{"upvote_count":"1","poster":"Germaneli","content":"Selected Answer: C\nScheduled scaling, as one way of Application Auto Scaling, is available for DynamoDB tables and global secondary indexes. It allows to \"scale a resource one time only or on a recurring schedule\". I understand that this is what we need for the one-time event, and it's even automated (option D is not automated).\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html","comment_id":"1010728","timestamp":"1695053100.0"},{"comment_id":"988367","timestamp":"1692798120.0","poster":"orlvas","content":"D\nIn summary, Autoscaling requires consecutive data points where the target utilization value is being breached to scale up a DynamoDB table. For this reason Autoscaling is not recommended as a solution for dealing with spiked workloads.","upvote_count":"1"},{"comment_id":"973729","upvote_count":"1","timestamp":"1691317860.0","content":"Selected Answer: C\nAmazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","poster":"IhorK"},{"comment_id":"912805","upvote_count":"2","content":"Have everyone forgot cooldown during AWS Application Auto Scaling policy? We know we need to increase by 10x for AWS Application Auto Scaling policy with cooldowns it will take time to get there.","poster":"Paulv82003","timestamp":"1685707860.0","comments":[{"timestamp":"1699538880.0","upvote_count":"1","poster":"sguinales","comment_id":"1066461","content":"agree and in questo said \"traffic will spike rapidly\" autoscaling here bad performance instead provisioned, because you know when is going to be a spike and can be prepared."}]},{"poster":"aviathor","content":"Selected Answer: D\nA. This is achieved by D, but D is more precise.\nB. Does DynamoDB support burst capacity?\nC.The question is not about the application, but about DynamoDB\nD. Using provisioned capacity to meet the expected demand is one way of doing it. Using provisioned capacity with auto-scaling would also work. And of course on-demand would be an option.","comment_id":"912591","timestamp":"1685688840.0","upvote_count":"1"},{"timestamp":"1676221260.0","comments":[{"content":"because the usage will spike rapidly, if you have pre previsioned, you dont waste time scaling","timestamp":"1689190380.0","upvote_count":"1","poster":"leotoras","comment_id":"950142"}],"poster":"guau","comment_id":"806588","upvote_count":"1","content":"Selected Answer: C\nC- I will go with autoscaling. Why change 2 times config, when autoscaling is designed for that.."},{"upvote_count":"1","timestamp":"1675860540.0","content":"D is wrong because on-demand to maximum 2 times of previous peak, it can not scale to 10x.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/on-demand-table-throttling-dynamodb/","comment_id":"802032","poster":"im_not_robot"},{"content":"Selected Answer: C\nC is correct. AWS always recomend to use auto scaling when you can predict usage. I now that I excpect 10x more traffic.","timestamp":"1674869580.0","poster":"renfdo","comment_id":"790149","upvote_count":"1"},{"upvote_count":"1","poster":"lollyj","content":"Selected Answer: D\nI\"m going with D because auto scaling on the application doesn't mean the DB can accommodate the increased RW on the DB. Since the peak traffic is predictable then it may be best to pre-provision ahead and reduce after sale is over. I may be wrong though","timestamp":"1671567360.0","comment_id":"751428"},{"poster":"awsjjj","timestamp":"1665961380.0","upvote_count":"1","content":"Selected Answer: D\nI go with D. although If the answer D also includes autoscaling it would have been easy to choose D","comment_id":"696628"},{"upvote_count":"1","poster":"SachinGoel","content":"Selected Answer: C\nC. Set an AWS Application Auto Scaling policy for the table to handle the increase in traffic","timestamp":"1665475980.0","comment_id":"691890"},{"upvote_count":"5","timestamp":"1656955380.0","comments":[{"content":"I agree because of the possibility of \"Scheduled scaling for Application Auto Scaling\" (https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html)","poster":"DevoteamAnalytix","comment_id":"640410","timestamp":"1659337440.0","upvote_count":"4"}],"poster":"kush_sumit","comment_id":"627093","content":"Selected Answer: C\nAnswer: C\nDynamodb falls under the category of application scaling as per link and we can use schedule scaling option which can be used for one time or recurring schedule.\n\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html\nFeatures of Application Auto Scaling\nApplication Auto Scaling allows you to automatically scale your scalable resources according to conditions that you define.\n\nTarget tracking scaling – Scale a resource based on a target value for a specific CloudWatch metric.\n\nStep scaling – Scale a resource based on a set of scaling adjustments that vary based on the size of the alarm breach.\n\nScheduled scaling – Scale a resource one time only or on a recurring schedule.\n\nLink: https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html"},{"comment_id":"595122","poster":"novice_expert","timestamp":"1651332480.0","comments":[{"timestamp":"1681558080.0","poster":"Mintwater","comment_id":"870873","upvote_count":"1","content":"After completing this tutorial, you’ll know how to:\n\nUse scheduled scaling to add extra capacity to meet a heavy load before it arrives, and then remove the extra capacity when it's no longer required.\n\nUse a target tracking scaling policy to scale your application based on current resource utilization."}],"content":"Selected Answer: D\nx A. table is always provisioned to meet peak needs (bad solution)\nx B. burst capacity (good for 5 min)\nx C. Application Auto Scaling policy ( first it says Application not DB, second why auto scale when event is non-recurring)\nD. Preprovision additional capacity for the known peaks and then reduce the capacity after the event (better add DB auto scaling too)","upvote_count":"3"},{"timestamp":"1646511180.0","comment_id":"561620","upvote_count":"6","content":"Selected Answer: D\nD sounds correct. \n(1) we know in advance there is going to be an enormous increase and we can pre-provision a large capacity before the event\n(2) Autoscaling does NOT kick in immediately. ITs gradual and may not be able to keep up with the spike. Fairly high chance of throttling as even Burst Capacity wont be able to keep up.","poster":"RotterDam"},{"upvote_count":"2","poster":"user0001","comment_id":"557655","timestamp":"1645998360.0","content":"i vote for C , anticipating is not an option you consider on production \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"},{"timestamp":"1642772160.0","upvote_count":"2","poster":"awsmonster","content":"C\n\nApplication Auto Scaling supports DynamoDB. \nhttps://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html","comment_id":"529157"},{"content":"Selected Answer: D\nOption D","upvote_count":"2","comment_id":"496057","timestamp":"1638884040.0","poster":"GMartinelli"},{"poster":"Suresh108","content":"I am with DDDDDD. \n\nchoosing D over C because of the words \"will spike rapidly.\" and traffic is known by 10 times, prerevision should be good.\n\nWhen sale prices are published during the event, *****traffic will spike rapidly.******","timestamp":"1636056600.0","upvote_count":"2","comment_id":"378641"},{"timestamp":"1635972960.0","poster":"Dip11","comment_id":"365473","upvote_count":"2","content":"D looks to be more appropriate. C is also possible but when its a one time event why do want to create policies and schedule it. If its recurring event then C is more appropriate."},{"comment_id":"358858","timestamp":"1635837780.0","content":"D for me","upvote_count":"1","poster":"Aesthet"},{"timestamp":"1635436920.0","upvote_count":"4","comments":[{"timestamp":"1635648480.0","comment_id":"330369","upvote_count":"1","poster":"shantest1","content":"Good find, makes sense, C is the answer"},{"comment_id":"427484","timestamp":"1636265760.0","upvote_count":"1","content":"As we can predict the traffic in the event 10 times, there is no need to setup a application autoscaling. Will you delete it after the event is done?","poster":"ChauPhan"}],"comment_id":"308547","content":"Definitely C. See the link, you can use this for scheduled scaling.\n\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html","poster":"jyrajan"},{"poster":"JobinAkaJoe","timestamp":"1635048060.0","content":"D is my choice","upvote_count":"1","comment_id":"253516"},{"timestamp":"1634935860.0","content":"I would go with D. Provisioned capacity is preferable for known workload","poster":"Ashoks","comment_id":"212802","upvote_count":"4"},{"content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ProvisionedThroughput.Throttling","comment_id":"207063","poster":"Awsexpert2020","timestamp":"1634929260.0","upvote_count":"1"},{"content":"C for me.. According to https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/, you can save 30.8% cost using auto scaling instead of pre-provisioning additional capacity.","poster":"vicks316","upvote_count":"2","comment_id":"201985","timestamp":"1634473920.0","comments":[{"content":"answer says application not database, C is not correct","poster":"tamagogo","timestamp":"1634973240.0","comment_id":"225481","upvote_count":"3"}]},{"upvote_count":"1","content":"C is correct because the amount of traffic increase can be predicted.","comment_id":"179890","timestamp":"1633794900.0","poster":"saki0915"},{"content":"Answer is D\nBurst capacity is only on per partition basis, so B is not correct.","upvote_count":"2","poster":"Ebi","timestamp":"1633581120.0","comment_id":"159102"},{"upvote_count":"1","poster":"awscamus","content":"Agree with B","comment_id":"143402","timestamp":"1632942540.0"},{"upvote_count":"1","comment_id":"141888","content":"agreed with D","timestamp":"1632877320.0","poster":"SaulGoodman"},{"comment_id":"139582","content":"B - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html","comments":[{"comment_id":"507337","timestamp":"1640202240.0","poster":"jove","content":"Burst capacity can help for only up to 5 minutes not for 3 days.","upvote_count":"1"}],"upvote_count":"1","timestamp":"1632389760.0","poster":"aws_geek"},{"content":"D. Preprovision DynamoDB. (assign RCU and WCU 10 times higher than normal load using provisioned mode)","timestamp":"1632297420.0","comment_id":"134128","poster":"chicagomassageseeker","upvote_count":"8"}]},{"id":"GIFvkGxzWUiz0D50x1Nk","topic":"1","unix_timestamp":1596197400,"answer_images":[],"discussion":[{"comment_id":"158702","timestamp":"1633255500.0","content":"Sorry, I mean A. We cannot define full load or CDC in Replication instance","upvote_count":"10","comments":[{"content":"B is the answer.....You define full load and CDC \nwhen creating the replication task","poster":"Kamazani","upvote_count":"2","timestamp":"1634194320.0","comment_id":"257428"}],"poster":"awscamus"},{"upvote_count":"5","poster":"novice_expert","content":"Selected Answer: A\nC&D unrelated\nA. \"AWS DMS full load with ongoing change data capture (CDC)\" (Note WITH means 1 task)\nB. allow both full load and ongoing change data capture (CDC)\n(note AND, means 2 tasks)\nThere are two types of ongoing replication tasks:\n\nFull load plus CDC – The task migrates existing data and then updates the target database based on changes to the source database.\n\nCDC only – The task migrates ongoing changes after you have data on your target database.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html","timestamp":"1651417620.0","comment_id":"595637"},{"poster":"aviathor","timestamp":"1685689560.0","content":"Selected Answer: A\nThe answers are not very clearly stated, but A is the best option\n\nA suggests that the source instance needs to be configured for AWS DMS full load and data capture. There is some truth to that since binary logging (transaction logs) need to be enabled.\nxB suggest that it is the instance that is configured for full load and CDC, but it is really the task\nxC suggests that the task should be configured to generate full logs?\nxD configure two-way communication","comment_id":"912593","upvote_count":"1"},{"content":"whoever makes these questions is probably not sober - \nanswer choice (A) is super apparent to the point of silliness - what ELSE would be the source?\nanswer choice (B) says replication instance needs to be configured to allow Full Load + CDC. The INSTANCE doesnt do this. The Task it hosts does! Do they intend both to be the one and the same?","timestamp":"1646540760.0","comment_id":"561778","poster":"RotterDam","upvote_count":"4"},{"timestamp":"1641992820.0","content":"Ans A: Full load and CDC are defined in a DMS Tasks.","upvote_count":"2","poster":"awsmonster","comment_id":"522171"},{"timestamp":"1636262880.0","poster":"Dip11","content":"Question not clear. A seems more relevant.","upvote_count":"4","comment_id":"365475"},{"content":"\"requires minimal downtime when the RDS DB instance goes live\"\nin order to do CDC: \"you must first ensure that ARCHIVELOG MODE is on to provide information to LogMiner. AWS DMS uses LogMiner to read information from the archive logs so that AWS DMS can capture changes\"\nSo my answer is A","comment_id":"358866","poster":"Aesthet","timestamp":"1635756180.0","upvote_count":"2"},{"poster":"Zhongkai","comment_id":"356051","timestamp":"1635570600.0","content":"https://docs.aws.amazon.com/dms/latest/sbs/chap-oracle2postgresql.steps.configureoracle.html says \"If you want to capture and apply changes (CDC), then you also need the following privileges.\"\nso A is the correct answer","upvote_count":"1"},{"comment_id":"253519","poster":"JobinAkaJoe","upvote_count":"1","timestamp":"1633990200.0","content":"This question is bit vague. I will go with A"},{"comment_id":"212852","timestamp":"1633730160.0","content":"A is the answer","poster":"Ashoks","upvote_count":"2"},{"upvote_count":"2","comments":[{"content":"Answer is A. You need to to a DMS task that is \"full+cdc\", this has nothing to do with the DMS instance in B","comment_id":"273147","poster":"MultiAZ","upvote_count":"2","timestamp":"1634604840.0"},{"comments":[{"content":"https://docs.aws.amazon.com/dms/latest/sbs/chap-rdsoracle2aurora.steps.createreplicationinstance.html\n\nA DMS replication instance performs the actual data migration between source and target. The replication instance also caches the transaction logs during the migration. How much CPU and memory capacity a replication instance has influences the overall time required for the migration.","comment_id":"691321","upvote_count":"1","poster":"Jiang_aws1","timestamp":"1665416760.0"}],"timestamp":"1635219300.0","upvote_count":"2","comment_id":"278601","poster":"See111","content":"Answer is A replication instance can't do full and cdc task .only dms do."}],"comment_id":"158698","timestamp":"1632935040.0","poster":"awscamus","content":"B since Migrate existing data and replicate ongoing changes (full load + change data capture (CDC)) – To migrate data with minimal downtime, AWS DMS can migrate the existing data and replicate the data changes from the source to the target until the cutover. This migration type is best for small and medium databases that require minimal downtime, which only lasts for the duration of the cutover."},{"upvote_count":"4","content":"B i think... im not sure","poster":"BillyC","timestamp":"1632571020.0","comment_id":"148058"},{"upvote_count":"2","timestamp":"1632087360.0","content":"A looks fine","poster":"Mohitrecdgp","comment_id":"147999"}],"choices":{"B":"Configure the AWS DMS replication instance to allow both full load and ongoing change data capture (CDC)","A":"Configure the on-premises application database to act as a source for an AWS DMS full load with ongoing change data capture (CDC)","C":"Configure the AWS DMS task to generate full logs to allow for ongoing change data capture (CDC)","D":"Configure the AWS DMS connections to allow two-way communication to allow for ongoing change data capture (CDC)"},"exam_id":22,"answers_community":["A (100%)"],"answer_ET":"A","timestamp":"2020-07-31 14:10:00","url":"https://www.examtopics.com/discussions/amazon/view/27045-exam-aws-certified-database-specialty-topic-1-question-77/","answer":"A","answer_description":"","question_images":[],"question_text":"A Database Specialist is migrating an on-premises Microsoft SQL Server application database to Amazon RDS for PostgreSQL using AWS DMS. The application requires minimal downtime when the RDS DB instance goes live.\nWhat change should the Database Specialist make to enable the migration?","isMC":true,"question_id":335}],"exam":{"isImplemented":true,"lastUpdated":"11 Apr 2025","name":"AWS Certified Database - Specialty","isBeta":false,"id":22,"numberOfQuestions":359,"isMCOnly":false,"provider":"Amazon"},"currentPage":67},"__N_SSP":true}