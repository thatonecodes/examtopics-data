{"pageProps":{"questions":[{"id":"tlYRiZCTzLcjunppXmyF","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/28167-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2020-08-11 19:37:00","answers_community":["AE (100%)"],"answer":"AE","question_images":[],"question_id":541,"discussion":[{"timestamp":"1633404600.0","comment_id":"183212","comments":[{"content":"Did you check about CMK??","comments":[{"timestamp":"1633776240.0","content":"He is right, i think you are talking about costumer managed CMK which isnt mentioned in the answers.","comments":[{"comment_id":"488721","timestamp":"1638064980.0","poster":"tiana528","upvote_count":"2","content":"The question mentioned KMS CMK, which has two types, AWS-managed and customer-managed. So A is correct. Using KMS CMK as the firm's key, no problem at all."}],"comment_id":"205806","upvote_count":"2","poster":"angelsrp"}],"comment_id":"197446","timestamp":"1633708740.0","poster":"oraldevel","upvote_count":"2"},{"upvote_count":"1","poster":"arulrajjayaraj","content":"I think the requirement here is \" All data at rest must be encrypted using keys controlled by the firm \" ,I think KMS would do that , CloudHSM may be ideal for Customer Supplied Encryption keys with extra hardware security with no one has access to that .","timestamp":"1634101500.0","comment_id":"226847"},{"content":"There is AWS managed CMK and customer managed CMKs. In this case as it applies to the question, the firm can use customer managed CMK. Based on this, A is correct","comment_id":"284852","poster":"QCO","upvote_count":"8","comments":[{"upvote_count":"3","timestamp":"1635598260.0","poster":"Sunflyhome","content":"By default, AWS KMS creates the key material for a CMK. You cannot extract, export, view, or manage this key material. Also, you cannot delete this key material; you must delete the CMK. **** However, you can import your own key material into a CMK ****","comment_id":"331366"}],"timestamp":"1635054600.0"},{"content":"A.E - Correct\nC is incorrect. Here is the snippted from CloudHSM FAQ page that clearly states that you need to import the CloudHSM managed key into the AWS KMS to use SSE -\n\"AWS services integrate with AWS Key Management Service, which in turn is integrated with AWS CloudHSM through the KMS custom key store feature. If you want to use the server-side encryption offered by many AWS services (such as EBS, S3, or Amazon RDS), you can do so by configuring a custom key store in AWS KMS.\"","comment_id":"456038","timestamp":"1636236540.0","poster":"joe16","upvote_count":"5"}],"poster":"ipindado2020","content":"A. KEYS not controlled by the firm(AWS KMS). KO\nB. KEYS not controlled by the firm(AWS KMS) and access through internet. KO\nC. KEYS controlled by the firm (CloudHSM) and access to AWS public resources trhough internal VPC endpoints. OK.\nD. This restricts that financial service users can access just to this bucket trhough the vpc link, does not prevent anybody else to read the bucket. KO\nE. This will enforce the access to the bucket from the financial users vpc. OK.\n\nThen CE","upvote_count":"29"},{"upvote_count":"27","poster":"Nemer","comment_id":"155735","content":"A & E. VPC endpoints and bucket policies...without removing the existing PutObject permissions for the users who are uploading. \nhttps://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/","timestamp":"1632289560.0"},{"upvote_count":"2","timestamp":"1678622220.0","comment_id":"836970","content":"Selected Answer: AE\nkeys controlled by the firm doesn't mean CloudHSM. If there is no specific requirement, always CMK. So A is correct.\nE over D is obvious.\n\nAAAA EEEE","poster":"milofficial"},{"content":"I would go with C and E\nReason for C: Encryption in transit is required not encryption at rest\nhttps://docs.aws.amazon.com/cloudhsm/latest/userguide/data-protection.html\nApplication connects to CloudHSM using interface endpoint and S3 with gateway endpoint\nReason for E: Gateway endpoint need bucket policy to restrict from VPCE\n\nReason for E:","poster":"Student1950","timestamp":"1657995660.0","comment_id":"632322","upvote_count":"1"},{"upvote_count":"1","comment_id":"626979","content":"Selected Answer: AE\nvote AE","poster":"aandc","timestamp":"1656936300.0"},{"content":"A. Launch the Amazon EMR cluster in a private subnet configured to use an AWS KMS CMK for at-rest encryption. Configure a gateway VPC endpoint for Amazon S3 and an interface VPC endpoint for AWS KMS.\nE. Configure the S3 bucket policies to permit access using an aws:sourceVpce condition to match the S3 endpoint ID.","poster":"cldy","comment_id":"495825","upvote_count":"1","timestamp":"1638869220.0"},{"upvote_count":"1","timestamp":"1638751740.0","content":"A,E is correct","comment_id":"494770","poster":"AzureDP900"},{"upvote_count":"1","content":"Perhaps the issue is wrong.\nI found that there were the following releases for CloudHSM:\nhttps://aws.amazon.com/about-aws/whats-new/2021/02/introducing-amazon-vpc-endpoints-aws-cloudhsm/?nc1=h_ls\nIn other words, until February of this year, it was not possible to create a VPC endpoint in CloudHSM.\nTherefore A & E is correct.","timestamp":"1636234380.0","comment_id":"441011","poster":"wakame"},{"poster":"Suresh108","comment_id":"438233","upvote_count":"3","timestamp":"1636162440.0","content":"SSE-S3: AWS manages both data key and master key\n\nSSE-KMS: AWS manages data key and you manage master key\n\nSSE-C: You manage both data key and master key\n\nSee this doc for more details: http://amzn.to/2iVsGvM\n\n\nA ) Server-Side Encryption\n\nSSE-S3 (AWS-Managed Keys) => When the requirement is to keep the encryption work simple and minimise the maintenance overhead then use SSE-S3.\n\nSSE-KMS (AWS KMS Keys) => When the requirement is to maintain a security audit trail then use SSE-KMS Keys.\n\nSSE-C (Customer-Provided Keys) => When end-to-end encryption is not required and the client wants full control of his/her security keys, then use SSE-C.\n\nB) Client-Side Encryption\n\nAWS KMS-managed, customer master key => When the requirement is to maintain end-to-end encryption plus a security audit trail, then use AWS KMS Keys.\n\nClient Managed Master Key => When the requirement is to maintain end-to-end encryption but the client wants full control of his/her security keys, then use Client Managed Master Key."},{"upvote_count":"2","comment_id":"430120","content":"A and E","poster":"denccc","timestamp":"1636125840.0"},{"poster":"WhyIronMan","timestamp":"1636045380.0","content":"I'll go with A,E","comment_id":"411772","upvote_count":"2"},{"poster":"pradhyumna","timestamp":"1636037760.0","upvote_count":"2","content":"A E\nWhile C looks like a close one it is not a complete one, the cluster instances would need HSM client software to make it work which is missing from the answer. On the otherhand A just meets the requirements.\nhttps://aws.amazon.com/cloudhsm/features/","comment_id":"398713"},{"comment_id":"350082","timestamp":"1636036440.0","upvote_count":"4","poster":"Waiweng","content":"it's A ,E"},{"content":"For those who are voting for C, \nDoubt that EMR supports CloudHSM based encryption option.\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html","comment_id":"338294","upvote_count":"3","poster":"Amitv2706","timestamp":"1635895560.0"},{"content":"Guys inpindado is correct. I have confirmed with my materials from Neal Davis. The key requirements is to keep environment isolated from the Internet and with that we could use AWS CLOUDHSM and VPC condition should match S3 endpoints ID.","comment_id":"300097","upvote_count":"6","poster":"kiev","timestamp":"1635377940.0"},{"poster":"kiev","comment_id":"295100","content":"A and E for me. CMK is managed by firm and E is no question.","timestamp":"1635238140.0","upvote_count":"2"},{"comment_id":"292489","content":"going for A,E","poster":"Kian1","timestamp":"1635194760.0","upvote_count":"2"},{"upvote_count":"4","comments":[{"content":"KMS CMK is managed by firm, you don't need CloudHSM,\nAlso interface endpoint does not support CloudHSM, so C is NOT CORRECT answer","comments":[{"timestamp":"1635535740.0","content":"CloudHSM does support interface endpoint\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/integrated-services-vpce-list.html","poster":"RedKane","upvote_count":"1","comment_id":"329270"}],"upvote_count":"4","timestamp":"1635077040.0","comment_id":"286574","poster":"Ebi"}],"comment_id":"277552","timestamp":"1635049080.0","content":"I will go with AE","poster":"Ebi"},{"comment_id":"260771","timestamp":"1635013140.0","upvote_count":"1","poster":"khan11","content":"A is out...AWS KMS CMK is not managed by the firm.... CLOUD HSM is managed by the firm\nA and E are the right answers","comments":[{"timestamp":"1635052800.0","poster":"ele","content":"CE are the right --> AWS KMS CMK is not managed by the firm.","upvote_count":"1","comment_id":"279902"}]},{"timestamp":"1634912280.0","comment_id":"253631","upvote_count":"1","content":"A and E for me .. tough one .. but that was my choice before reading the below ... so am confident in my answer ..","poster":"petebear55"},{"comment_id":"253131","timestamp":"1634622480.0","content":"A & E. The VPCE policies are meant to restrict access to the endpoint, not to the S3 bucket. Therefore D is incorrect.","upvote_count":"2","poster":"Bulti"},{"content":"Correct AE. CMK KMS + S3 endpoint + aws:sourceVpce","timestamp":"1634527800.0","upvote_count":"1","poster":"T14102020","comment_id":"244040"},{"poster":"cloudgc","timestamp":"1634217720.0","content":"A,E\nonly SSE-S3 and SSE-KMS are supported by EMR","comment_id":"240970","upvote_count":"1"},{"poster":"jackdryan","upvote_count":"2","timestamp":"1634208300.0","comment_id":"231626","content":"I'll go with A,E"},{"poster":"cpd","comment_id":"218222","upvote_count":"1","content":"A is obvious but b.w D and E, i think E will ensure data to s3 can only be uploaded using vpce, thus making sure traffic is isolated from internet","timestamp":"1633934160.0"},{"timestamp":"1633887360.0","comment_id":"209600","poster":"CYL","upvote_count":"1","content":"A, E. VCE will require configuration both at the VPC level and at the bucket policy level."},{"poster":"bbnbnuyh","comment_id":"206840","content":"https://aws.amazon.com/about-aws/whats-new/2020/07/amazon-emr-supports-encrypting-log-file-with-aws-kms-customer-management-cmks-with-more-flexible-security/.. There is no integration with cloudHSM afaik. A& E","upvote_count":"3","timestamp":"1633797780.0"},{"comment_id":"199222","upvote_count":"2","poster":"Paitan","content":"I will go with options A & E","timestamp":"1633722900.0","comments":[{"content":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html#vpc-endpoints-s3-bucket-policies","upvote_count":"2","comment_id":"199509","timestamp":"1633756740.0","poster":"Paitan"}]},{"timestamp":"1633505700.0","poster":"pddddd","upvote_count":"5","content":"AD. With E, S3 access is limited to VPC, where we have the EMR. It is unclear if the users are uploading from within the VPC or not, hence D.","comment_id":"192910"},{"upvote_count":"2","comment_id":"157017","timestamp":"1632739920.0","content":"As the firm requires to be isolated from the internet, its better to go with endpoints so option A. In B, it uses NAT which is in public subnet . Next option is to use bucket policies with association with VPC endpoint. so E works out. A,E are the options.","comments":[{"comment_id":"168956","upvote_count":"2","comments":[{"poster":"kanavpeer","content":"revisited the question again, i think it will work.\nAE are the right ones","timestamp":"1633170480.0","comment_id":"168961","upvote_count":"1"}],"timestamp":"1633131540.0","poster":"kanavpeer","content":"E will not work, as aws:sourceVpce expects the vpc endpoint id instead of s3 endpoint id. my vote is for AD"}],"poster":"Anila_Dhharisi"}],"topic":"1","answer_ET":"AE","unix_timestamp":1597167420,"answer_description":"","exam_id":32,"isMC":true,"choices":{"D":"Configure the S3 endpoint policies to permit access to the necessary data buckets only.","C":"Launch the Amazon EMR cluster in a private subnet configured to use an AWS CloudHSM appliance for at-rest encryption. Configure a gateway VPC endpoint for Amazon S3 and an interface VPC endpoint for CloudHSM.","B":"Launch the Amazon EMR cluster in a private subnet configured to use an AWS KMS CMK for at-rest encryption. Configure a gateway VPC endpoint for Amazon S3 and a NAT gateway to access AWS KMS.","E":"Configure the S3 bucket policies to permit access using an aws:sourceVpce condition to match the S3 endpoint ID.","A":"Launch the Amazon EMR cluster in a private subnet configured to use an AWS KMS CMK for at-rest encryption. Configure a gateway VPC endpoint for Amazon S3 and an interface VPC endpoint for AWS KMS."},"question_text":"An advisory firm is creating a secure data analytics solution for its regulated financial services users. Users will upload their raw data to an Amazon S3 bucket, where they have PutObject permissions only. Data will be analyzed by applications running on an Amazon EMR cluster launched in a VPC. The firm requires that the environment be isolated from the internet. All data at rest must be encrypted using keys controlled by the firm.\nWhich combination of actions should the Solutions Architect take to meet the user's security requirements? (Choose two.)"},{"id":"DL8zXsZxT4fESCBu0v3E","answer":"A","answer_ET":"A","answers_community":["A (100%)"],"topic":"1","timestamp":"2020-08-10 15:27:00","isMC":true,"exam_id":32,"unix_timestamp":1597066020,"answer_description":"","question_id":542,"question_text":"While debugging a backend application for an IoT system that supports globally distributed devices, a Solutions Architect notices that stale data is occasionally being sent to user devices. Devices often share data, and stale data does not cause issues in most cases. However, device operations are disrupted when a device reads the stale data after an update.\nThe global system has multiple identical application stacks deployed in different AWS Regions. If a user device travels out of its home geographic region, it will always connect to the geographically closest AWS Region to write or read data. The same data is available in all supported AWS Regions using an Amazon\nDynamoDB global table.\nWhat change should be made to avoid causing disruptions in device operations?","discussion":[{"content":"A. DynamoDB does not support strongly consistent reads ACROSS REGIONS. The stale data comes from writing to one region & reading from another.","poster":"Nemer","upvote_count":"15","timestamp":"1632947460.0","comment_id":"154541"},{"comment_id":"941054","timestamp":"1688314860.0","content":"Selected Answer: A\nA. DynamoDB does not support strongly consistent reads ACROSS REGIONS. The stale data comes from writing to one region & reading from another.","poster":"SkyZeroZx","upvote_count":"1"},{"poster":"et22s","content":"Selected Answer: A\nAn application can read and write data to any replica table. If your application only uses eventually consistent reads and only issues reads against one AWS Region, it will work without any modification. However, if your application requires strongly consistent reads, it must perform all of its strongly consistent reads and writes in the same Region. DynamoDB does not support strongly consistent reads across Regions. Therefore, if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html#V2globaltables_HowItWorks.conflict-resolution","timestamp":"1668315060.0","comment_id":"717093","upvote_count":"1"},{"comment_id":"496728","upvote_count":"1","poster":"cldy","content":"A. Update the backend to use strongly consistent reads. Update the devices to always write to and read from their home AWS Region.","timestamp":"1638959700.0"},{"poster":"AzureDP900","content":"A is right","upvote_count":"1","timestamp":"1638751980.0","comment_id":"494771"},{"content":"Answer is A","timestamp":"1635922920.0","upvote_count":"1","comment_id":"448526","poster":"moon2351"},{"upvote_count":"2","content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html","poster":"Goram113","timestamp":"1635621960.0","comment_id":"441828"},{"comment_id":"411774","content":"I'll go with A","poster":"WhyIronMan","timestamp":"1635522180.0","upvote_count":"2"},{"content":"it's A","upvote_count":"2","poster":"Waiweng","timestamp":"1635246420.0","comment_id":"350089"},{"content":"A is correct","timestamp":"1634883180.0","poster":"Firststack","upvote_count":"2","comment_id":"281197"},{"upvote_count":"3","poster":"Ebi","comment_id":"277559","timestamp":"1634645580.0","content":"A is the correct answer"},{"timestamp":"1634415480.0","content":"A - However, if your application requires strongly consistent reads, it must perform all of its strongly consistent reads and writes in the same Region. DynamoDB does not support strongly consistent reads across Regions. Therefore, if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region.\nDoc link - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html","comment_id":"263537","upvote_count":"4","poster":"SD13"},{"poster":"Bulti","upvote_count":"3","comment_id":"253142","timestamp":"1634086680.0","content":"A is the right answer. Dynamo DB doesn't support strong consistency on global tables cross-region. In order for strong consistency to work , the application needs to write and read data from the same region."},{"poster":"Britts","comment_id":"250740","upvote_count":"2","timestamp":"1633648680.0","content":"A can't be right unless the the dynamodb global table gets replaced by a regional table first?"},{"comment_id":"244043","poster":"T14102020","timestamp":"1633558500.0","upvote_count":"1","content":"Correct is A. Read only from Home Region"},{"timestamp":"1633445280.0","content":"I'll go with A","poster":"jackdryan","comment_id":"231627","upvote_count":"2"},{"comment_id":"215973","upvote_count":"2","content":"A. ——> \n“ An application can read and write data to any replica table. If your application only uses eventually consistent reads and only issues reads against one AWS Region, it will work without any modification. However, if your application requires strongly consistent reads, it must perform all of its strongly consistent reads and writes in the same Region. DynamoDB does not support strongly consistent reads across Regions. Therefore, if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region.\n\nIf applications update the same item in different Regions at about the same time, conflicts can arise. To help ensure eventual consistency, DynamoDB global tables use a last writer wins reconciliation between concurrent updates, in which DynamoDB makes a best effort to determine the last writer. With this conflict resolution mechanism, all the replicas will agree on the latest update and converge toward a state in which they all have identical data. “","timestamp":"1633360680.0","poster":"smartassX"},{"content":"A. This combination allows for less restriction and impact to overall performance and allows for consistent read requirements. Tie a particular user back to the home region.","comment_id":"209602","poster":"CYL","upvote_count":"1","timestamp":"1633352280.0"},{"content":"Answer is A. Yes, DynamoDB does not support strongly consistent reads across Regions. Therefore, if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region.","upvote_count":"2","timestamp":"1632982020.0","comment_id":"157021","poster":"Anila_Dhharisi"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/27927-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"B":"Enable strong consistency globally on a DynamoDB global table. Update the backend to use strongly consistent reads.","C":"Switch the backend data store to Amazon Aurora MySQL with cross-region replicas. Update the backend to always write to the master endpoint.","A":"Update the backend to use strongly consistent reads. Update the devices to always write to and read from their home AWS Region.","D":"Select one AWS Region as a master and perform all writes in that AWS Region only. Update the backend to use strongly consistent reads."},"answer_images":[]},{"id":"VeHSZXouIZ1qXIH3uXoZ","timestamp":"2021-04-01 08:23:00","question_id":543,"discussion":[{"content":"A. Implement IDS/IPS agents on each Instance running in VPC\nD. Implement a reverse proxy layer in front of web servers and configure IDS/IPS agents on each reverse proxy server.","upvote_count":"2","poster":"amministrazione","comment_id":"1266654","timestamp":"1723751820.0"},{"timestamp":"1656466740.0","content":"Selected Answer: AD\nIt's AD\nExplanation:\nEC2 does not allow promiscuous mode, and you cannot put something in between the ELB and the web server (like a listener or IDP)","poster":"TechX","upvote_count":"2","comment_id":"624360"},{"timestamp":"1635965400.0","content":"A and D","upvote_count":"2","poster":"01037","comment_id":"349110"},{"comment_id":"340761","timestamp":"1633987920.0","content":"A and D for me.","poster":"Malcnorth59","upvote_count":"1"},{"upvote_count":"2","poster":"cldy","timestamp":"1632401880.0","content":"A.D.\nthese are the two ways to implement IDS/IPS.","comment_id":"325568"}],"topic":"1","choices":{"C":"Implement Elastic Load Balancing with SSL listeners in front of the web applications","B":"Configure an instance in each subnet to switch its network interface card to promiscuous mode and analyze network traffic.","D":"Implement a reverse proxy layer in front of web servers and configure IDS/IPS agents on each reverse proxy server.","A":"Implement IDS/IPS agents on each Instance running in VPC"},"unix_timestamp":1617258180,"url":"https://www.examtopics.com/discussions/amazon/view/48616-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"EC2 does not allow promiscuous mode, and you cannot put something in between the ELB and the web server (like a listener or IDP)","answer_ET":"AD","answers_community":["AD (100%)"],"exam_id":32,"answer_images":[],"question_text":"You are designing an intrusion detection prevention (IDS/IPS) solution for a customer web application in a single VPC. You are considering the options for implementing IOS IPS protection for traffic coming from the Internet.\nWhich of the following options would you consider? (Choose two.)","answer":"AD","question_images":[],"isMC":true},{"id":"sSPGbaus2i38kR6vVgXY","answer_ET":"C","discussion":[{"timestamp":"1635246420.0","upvote_count":"15","comment_id":"277560","poster":"Ebi","content":"Answer is C"},{"timestamp":"1705849140.0","content":"Selected Answer: C\nTo use selective cross region replication, the regular objects need a tag. D does not tag regular items so cross region replication wouldn't work.\nhttps://aws.amazon.com/about-aws/whats-new/2018/09/amazon-s3-announces-selective-crr-based-on-object-tags/\nI don't like that Lambda + S3 events are used for SRR as that could be a compliance issue if the Lambda function fails. And SRR in S3 has been around for a long time with one of the use cases being data sovereignty compliance.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html#srr-scenario","comment_id":"1127895","upvote_count":"1","poster":"3a632a3"},{"timestamp":"1684625640.0","poster":"Jesuisleon","comments":[{"poster":"Jesuisleon","comment_id":"920228","comments":[{"content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html#:~:text=Overview%20of%20Amazon%20S3%20Event%20Notifications","upvote_count":"1","poster":"vn_thanhtung","timestamp":"1693058400.0","comment_id":"990836"}],"content":"I change to D since in the link above, search \"Notification Details\", there is no tag item inside.\n(Etag is just checksum not our tag here), so ordinary s3 event doesn't have tag item. So C is out, Still need CloudWatch events to trigger.","timestamp":"1686420420.0","upvote_count":"1"}],"upvote_count":"1","comment_id":"902808","content":"Selected Answer: C\nThe answer is C.\nWe need tag to selectively copy objects, normal objects to another region S3 bucket and secrets objects to a S3 bucket in the same region, so A, B are out.\nNew object in S3 event can directly trigger lambda, no need to use CloudWatch, \nsee https://aws.amazon.com/blogs/aws/s3-event-notification/, so D is out."},{"poster":"Blackfry","comment_id":"712159","content":"Selected Answer: C\nThe difference between C and D is that both regular and confidential documents are tagged or only confidential documents are tagged. But we can use Object tags, if there are any tag.\nSo when we wants to use Selective Cross-Region Replication based on Object Tags about regular documents, we should tag 'regular documents'(or both).","upvote_count":"2","timestamp":"1667711520.0"},{"upvote_count":"1","comment_id":"688969","poster":"WayneYi","comments":[{"content":"In fact, to move regular documents is an implicit requirement from the question. Even option C says \"...... to move regular documents to an S3 bucket in a different AWS Region\".","upvote_count":"1","timestamp":"1665277560.0","comments":[{"poster":"wassb","content":"@Bulti answer : Answer is C. D looks like an option except for the fact that the regular objects are not tagged.","comment_id":"692220","timestamp":"1665502380.0","upvote_count":"1"}],"poster":"tomosabc1","comment_id":"689804"}],"content":"I will go with C. Because option D says that we are moving regular documents into a different region, it makes no sense at all","timestamp":"1665191760.0"},{"comment_id":"646526","content":"Selected Answer: D\nThe only difference between C and D is S3 events/Cloud watch events - In case of C, S3 events cannot be triggered selectively based on tag, so it would call Lambda for all documents - hence D is better.","upvote_count":"3","poster":"Harithareddynn","timestamp":"1660443420.0"},{"content":"I think answer is D cloudwatch verify that the application can adapt to this new demand","poster":"ciki","upvote_count":"1","timestamp":"1639742220.0","comment_id":"503662"},{"upvote_count":"2","poster":"vbal","timestamp":"1639676220.0","content":"https://aws.amazon.com/blogs/mt/monitor-tag-changes-on-aws-resources-with-serverless-workflows-and-amazon-cloudwatch-events/\nEven after reading above page I am still not sure if it would work or not. But If I can trigger even based upon Each Object's Tag being Put into S3, I would prefer D just because this is more efficient as Lambda would be triggered only for Secret documents and not for ALL the PUT Object Events which is in-efficient IMO.","comment_id":"503094"},{"comment_id":"495685","poster":"cldy","timestamp":"1638859500.0","upvote_count":"1","content":"C. Tag documents as either regular or secret in Amazon S3. Create an individual S3 backup bucket in the same AWS account and AWS Region. Use S3 selective cross-region replication based on object tags to move regular documents to an S3 bucket in a different AWS Region. Configure an AWS Lambda function that triggers when new S3 objects are created in the main bucket to replicate only documents tagged as secret into the S3 bucket in the same AWS Region."},{"content":"Use S3 selective cross-region replication , Answer is C.","comment_id":"494777","poster":"AzureDP900","upvote_count":"1","timestamp":"1638752340.0"},{"comment_id":"486043","upvote_count":"1","timestamp":"1637765460.0","content":"I will go for C","poster":"pcops"},{"timestamp":"1637464860.0","content":"Selected Answer: C\nAnswer = C.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html","poster":"acloudguru","comment_id":"482993","upvote_count":"2"},{"comment_id":"411775","poster":"WhyIronMan","timestamp":"1636081560.0","upvote_count":"3","content":"I'll go with C"},{"upvote_count":"2","poster":"aws_arn_name","comments":[{"content":"Wrong. Althought I got confused on the same point as well. The trick is for CloudWatch Event Rule you first need to enable CloudTrail Data Events = > https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html\n\nHence C is correct.","upvote_count":"2","poster":"wahlbergusa","comment_id":"496189","timestamp":"1638895620.0"}],"comment_id":"353855","timestamp":"1635629760.0","content":"I think answer is D. With C only new confidential object will be backup, what about old confidential object. Addition , S3 event has lag , although rarely but still can cause lost data, CloudWatch is more reliable"},{"poster":"Waiweng","upvote_count":"4","comment_id":"350093","content":"it's C","timestamp":"1635463920.0"},{"content":"The answer is C. https://aws.amazon.com/about-aws/whats-new/2018/09/amazon-s3-announces-selective-crr-based-on-object-tags/","poster":"blackgamer","comment_id":"348484","upvote_count":"3","timestamp":"1635376020.0"},{"upvote_count":"1","timestamp":"1635045300.0","comment_id":"263223","content":"C.\nRegion is treated as a country, though there are several Regions in US.","poster":"01037"},{"poster":"Bulti","comments":[{"timestamp":"1650256980.0","comment_id":"587469","upvote_count":"1","content":"Thank you for pointing this out","poster":"LiamNg"}],"comment_id":"253151","content":"Answer is C. D looks like an option except for the fact that the regular objects are not tagged. Only highly confidential objects are tagged. Otherwise it's possible to setup a CloudWatch Event rule on an S3 object load event and specify in the action to invoke Lambda function to copy the secret files into a backup S3 bucket.","upvote_count":"3","timestamp":"1634160480.0"},{"timestamp":"1633865580.0","comment_id":"250739","poster":"Britts","content":"Not sure why lambda is needed here as we can do cross zone replication in AWS in the same account. Anyway will go with C","upvote_count":"1"},{"timestamp":"1633747440.0","poster":"T14102020","content":"Correct is C. Backup for secret and regular documents + Lambda without CloudWatch event","upvote_count":"2","comment_id":"244045"},{"upvote_count":"2","timestamp":"1633545300.0","comment_id":"231628","poster":"jackdryan","content":"I'll go with C"},{"timestamp":"1633134900.0","comment_id":"209605","upvote_count":"2","poster":"CYL","content":"C. Have separate replication methods for secret and normal documents."},{"timestamp":"1632651660.0","upvote_count":"4","comment_id":"157030","content":"C is right answer. No need of CloudWatch events. if any errors occur then we can check for the Lambda CloudWatch logs.","poster":"Anila_Dhharisi"},{"upvote_count":"3","timestamp":"1632537840.0","content":"C. S3 publishing new object created event to Lambda.","poster":"Nemer","comment_id":"156142"},{"upvote_count":"4","content":"Answer = C.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html","poster":"wood_wood","timestamp":"1632402000.0","comment_id":"155468"}],"exam_id":32,"question_images":[],"question_text":"A software as a service (SaaS) company offers a cloud solution for document management to private law firms and the public sector. A local government client recently mandated that highly confidential documents cannot be stored outside the country. The company CIO asks a Solutions Architect to ensure the application can adapt to this new requirement. The CIO also wants to have a proper backup plan for these documents, as backups are not currently performed.\nWhat solution meets these requirements?","choices":{"B":"Tag documents as either regular or secret in Amazon S3. Create an individual S3 backup bucket in the same AWS account and AWS Region. Create a cross- region S3 bucket in a separate AWS account. Set proper IAM roles to allow cross-region permissions to the S3 buckets. Configure an AWS Lambda function triggered by Amazon CloudWatch scheduled events to copy objects that are tagged as secret to the S3 backup bucket and objects tagged as normal to the cross-region S3 bucket.","C":"Tag documents as either regular or secret in Amazon S3. Create an individual S3 backup bucket in the same AWS account and AWS Region. Use S3 selective cross-region replication based on object tags to move regular documents to an S3 bucket in a different AWS Region. Configure an AWS Lambda function that triggers when new S3 objects are created in the main bucket to replicate only documents tagged as secret into the S3 bucket in the same AWS Region.","D":"Tag highly confidential documents as secret in Amazon S3. Create an individual S3 backup bucket in the same AWS account and AWS Region. Use S3 selective cross-region replication based on object tags to move regular documents to a different AWS Region. Create an Amazon CloudWatch Events rule for new S3 objects tagged as secret to trigger an AWS Lambda function to replicate them into a separate bucket in the same AWS Region.","A":"Tag documents that are not highly confidential as regular in Amazon S3. Create individual S3 buckets for each user. Upload objects to each user's bucket. Set S3 bucket replication from these buckets to a central S3 bucket in a different AWS account and AWS Region. Configure an AWS Lambda function triggered by scheduled events in Amazon CloudWatch to delete objects that are tagged as secret in the S3 backup bucket."},"url":"https://www.examtopics.com/discussions/amazon/view/28100-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":544,"answer_description":"","isMC":true,"answers_community":["C (67%)","D (33%)"],"unix_timestamp":1597151820,"timestamp":"2020-08-11 15:17:00","answer":"C","answer_images":[],"topic":"1"},{"id":"n3oPhGZaj5PrbQdw2ieH","question_images":[],"isMC":true,"timestamp":"2020-08-12 07:03:00","answer_ET":"A","question_text":"A company has an application that runs on a fleet of Amazon EC2 instances and stores 70 GB of device data for each instance in Amazon S3. Recently, some of the S3 uploads have been failing. At the same time, the company is seeing an unexpected increase in storage data costs. The application code cannot be modified.\nWhat is the MOST efficient way to upload the device data to Amazon S3 while managing storage costs?","answers_community":["A (100%)"],"question_id":545,"discussion":[{"poster":"Ebi","comment_id":"275185","timestamp":"1634717640.0","upvote_count":"11","content":"Answer is A","comments":[{"timestamp":"1634973300.0","poster":"ExtHo","comments":[{"upvote_count":"1","poster":"kirrim","comment_id":"461283","timestamp":"1636208700.0","content":"Technically, you can view failed multi-part uploads in the console using AWS Storage Lens:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-lens-optimize-storage.html#locate-incomplete-mpu\nhttps://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/\n\nI still think A is the best answer, though!"}],"upvote_count":"10","content":"Supporting Ebi It should be A, because the most critical problem is that the console cannot display the information that your multipart upload failed. This can only be viewed through the SDK/API.\nAnd the title said that there are many unexpected data costs, which should refer to the storage fee caused by the failure of multipart upload (because if you don’t use multipart upload, the entire file upload will fail if it fails, and there is no such part of the cost). It can be concluded that the original program has already written code for multipart upload. No additional code changes are required.","comment_id":"326243"}]},{"comments":[{"content":"On reviewing the Option D again, I realized that it is assuming we are using multipart upload with S3 TA. This will also require a code change. The only option then which will not require a code change is option B. So my final answer is Option B.","comment_id":"258186","upvote_count":"3","comments":[{"comment_id":"263233","poster":"01037","content":"Yes, only B doesn't need code change.","upvote_count":"1","timestamp":"1634575620.0"},{"poster":"01037","comment_id":"263236","content":"But how to find out a failed upload?\nIsn't the upload is a 0 or 1 operation if it isn't multi part upload?","upvote_count":"1","timestamp":"1634712480.0"}],"timestamp":"1634443680.0","poster":"Bulti"}],"comment_id":"253158","timestamp":"1634302620.0","poster":"Bulti","upvote_count":"6","content":"Between A and D, I will go with D only because A will require a code change. It is assumed that the application currently does not use multi-part upload. Using S3 Transfer acceleration does not require code change. Identifying multi-part object failures is possible using both CLI and console so I will go with D."},{"content":"Selected Answer: A\nchoose A","timestamp":"1692064620.0","upvote_count":"1","poster":"ibu007","comment_id":"981213"},{"comment_id":"713065","content":"Answer is DDDD","upvote_count":"1","timestamp":"1667828100.0","poster":"sou123454"},{"poster":"TechX","upvote_count":"1","timestamp":"1656468000.0","content":"Selected Answer: A\nAgree with A, best solution here","comment_id":"624368"},{"timestamp":"1656276480.0","upvote_count":"1","comment_id":"622813","content":"Selected Answer: A\nAgree with A: AWS CLI.","poster":"kangtamo"},{"poster":"CloudChef","comment_id":"520373","upvote_count":"1","content":"A is correct","timestamp":"1641749520.0"},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1638752640.0","comment_id":"494781","content":"A is right"},{"content":"I will go with A","comment_id":"486046","poster":"pcops","timestamp":"1637765760.0","upvote_count":"1"},{"comments":[{"poster":"sashenka","upvote_count":"3","timestamp":"1636935300.0","comment_id":"478395","content":"AAA - CORRECTION. It appears that if using the AWS SDK/CLI by DEFAULT when uploading a >5Mb file to an AWS S3 bucket multipart upload will be used. That and I missed that listing of failed multipart upload objects CAN'T be viewed in the Management Console."}],"comment_id":"478392","upvote_count":"2","content":"DDD - main reason it is NOT A is because \"Modifications to the application's code are not permitted.\" and taking advantage of S3 multipart uploads REQUIRES modification to your code. SDK/API is provided and the S3 multipart upload function is different than the PUT of the S3 upload. Take a look here:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mpu-upload-object.html\n\nAdditionally, TA is best practice for transferring large files to S3 buckets. As data arrives at the closest edge location, the data is routed to Amazon S3 over an optimized network path. This will insure more device uploads will not end up in a failed state.","timestamp":"1636934700.0","poster":"sashenka"},{"comment_id":"448531","timestamp":"1636175100.0","upvote_count":"1","content":"I think Answer is A","poster":"moon2351"},{"comment_id":"428762","content":"\"Use the AWS Management Console to list incomplete parts to address the failed S3 uploads\" - not possible with Management Console\nC & D - wrong\n\"Upload device data using S3 Transfer Acceleration\" - can be used to move data between Regions. Not in this case\nB & D - wrong\n\"Use the AWS Management Console to address the failed S3 uploads.\" - there is no functionality\nB - wrong\n\"Use the AWS CLI to list incomplete parts to address the failed S3 uploads\" - correct\n\"Enable the lifecycle policy for the incomplete multipart uploads on the S3 bucket to delete the old uploads and prevent new failed uploads from accumulating.\" - correct\nA - correct. I assume that they will not change the application and use CLI to upload files","upvote_count":"5","poster":"DerekKey","timestamp":"1636058280.0"},{"comment_id":"411796","content":"I'll go with A","poster":"WhyIronMan","upvote_count":"1","timestamp":"1635937920.0"},{"upvote_count":"1","timestamp":"1635915780.0","content":"Well described here https://aws.amazon.com/blogs/aws-cost-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/","comment_id":"402439","poster":"Desailly"},{"content":"If some of uploads are failing and cost is getting increased means upload is already multipart\nhence only ask is to how to reduce the cost and that can be done by deleting failed uploads\nfrom S3. Hence A makes sense.","timestamp":"1635893520.0","poster":"SPRao","upvote_count":"2","comment_id":"397754"},{"content":"Note: You aren’t able to view the parts of your incomplete multipart upload in the AWS Management Console.\nhttps://aws.amazon.com/cn/blogs/aws-cost-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/\n\nSo A is the only one.","upvote_count":"2","comment_id":"362542","comments":[{"comment_id":"377738","content":"Lifecycle policies for failed uploads discussed in this blog: https://aws.amazon.com/blogs/aws-cost-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/\n(A)","timestamp":"1635537480.0","upvote_count":"1","poster":"Rich_Rich"},{"poster":"bharadhwaj","timestamp":"1635779760.0","content":"s3 storage lens is through the console. The real thing is deletion of the file and not a move to glacier. hence A is correct","upvote_count":"2","comment_id":"380966"}],"timestamp":"1635425640.0","poster":"oscargee"},{"poster":"Waiweng","content":"it's A","comment_id":"350103","upvote_count":"4","timestamp":"1635139080.0"},{"upvote_count":"3","timestamp":"1635065040.0","content":"S3 Transfer Acceleration is used for data transfer from remote clients by routing them through AWS edge locations. How would that help when the data is already uploaded from within an AWS region?","comment_id":"333936","poster":"sarah_t"},{"upvote_count":"1","content":"Torn between A and B... Does multipart upload require code changes on the application level?","comment_id":"326765","timestamp":"1635035760.0","poster":"awsnoob"},{"content":"going with A","comment_id":"292504","upvote_count":"1","poster":"Kian1","timestamp":"1634944980.0"},{"timestamp":"1634756160.0","comment_id":"281204","poster":"Firststack","comments":[{"content":"key point is the code can't be modified","comment_id":"354152","timestamp":"1635161100.0","upvote_count":"1","poster":"jason0011"}],"content":"B is correct","upvote_count":"2"},{"comment_id":"253642","content":"D 'Use the AWS CLI' (A) is not the \"MOST EFFICIENT\" .. that requires more user input than just using the management console","timestamp":"1634382240.0","poster":"petebear55","comments":[{"upvote_count":"1","timestamp":"1634919540.0","comment_id":"286076","content":"If you use \"S3 Transfer Acceleration\" for upload then why you will need to delete incomplete multi-part? \nAnswer is A","poster":"Ebi"}],"upvote_count":"1"},{"comment_id":"250734","upvote_count":"1","timestamp":"1634265960.0","poster":"Britts","content":"Will go with D, as the multipart upload will require application changes. Lifecycle policies can address the cleanup of unfinished big files"},{"comment_id":"245852","upvote_count":"1","timestamp":"1634155560.0","poster":"kj07","content":"Answer A make sense, but if you use multi part upload you will need to change the code. Also using the CLI doesn't seem like the AWS way.\n\nD is also ok, but the question doesn't mention something that requires transfer acceleration.\n\nFrom A and D I will pick D, looks more suitable for question needs."},{"content":"Correct is D. without multipart uploads+ delete incomplete multipart uploads","timestamp":"1634092080.0","poster":"T14102020","comment_id":"244052","upvote_count":"1"},{"timestamp":"1633949460.0","content":"I choose C , my Friend choose A","comment_id":"233107","poster":"gookseang","upvote_count":"1"},{"poster":"jackdryan","upvote_count":"2","content":"I'll go with A","comment_id":"231630","timestamp":"1633873560.0"},{"upvote_count":"1","timestamp":"1633623120.0","comment_id":"229641","content":"A makes sense + https://aws.amazon.com/premiumsupport/knowledge-center/s3-multipart-upload-cli/","poster":"taoteching1"},{"timestamp":"1633568040.0","comment_id":"211045","upvote_count":"2","content":"There is no need for S3 transfer acceleration and is never the requirement in the question, hence A fulfills the requirement.","poster":"liono"},{"upvote_count":"2","timestamp":"1633325400.0","content":"Between A and D , I will go for D . The reason being , If LifeCycle policy is to delete Incomplete MPU then the application should already be using it ? hence no point of A ..S3 TA s what they are looking for","poster":"Mikey123","comment_id":"210144"},{"timestamp":"1633189440.0","comment_id":"207766","poster":"SamAWSExam99","content":"A. Upload device data using a multipart upload. Use the AWS CLI to list incomplete parts to address the failed S3 uploads. Enable the lifecycle policy for the incomplete multipart uploads on the S3 bucket to delete the old uploads and prevent new failed uploads from accumulating.","upvote_count":"1"},{"upvote_count":"1","content":"S3 lifecycle policy is for changing the storage class....A, D are wrong. C is correct.","comments":[{"upvote_count":"1","poster":"hyemi","content":"Change to A. Link from zeronine's comment is helpful! Thanks.","timestamp":"1633172700.0","comment_id":"203936"},{"content":"You are wrong. Intelligent-Tiering is for storage class. Lifecycle covers: Delete expired delete markers or incomplete multipart uploads","timestamp":"1636016520.0","upvote_count":"1","comment_id":"428726","poster":"DerekKey"}],"poster":"hyemi","timestamp":"1632919320.0","comment_id":"203490"},{"upvote_count":"2","comment_id":"196122","timestamp":"1632720960.0","content":"Correction: between B and D, D makes more sense","poster":"SanjeevB"},{"upvote_count":"2","content":"muli part upload not possible as no code change is possible in application. For mulipart upload you need to include the Oject id provided by AWSand th epart number when you upload. D is the only option thats available.","comment_id":"196121","poster":"SanjeevB","timestamp":"1632612780.0"},{"poster":"Anila_Dhharisi","upvote_count":"2","comment_id":"157032","comments":[{"comment_id":"217858","upvote_count":"1","content":"multipart upload you need to adjust the code?","timestamp":"1633586520.0","poster":"beso"}],"content":"Yes. A is right answer.","timestamp":"1632289080.0"},{"content":"A - https://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/","upvote_count":"3","poster":"zeronine","comment_id":"156319","timestamp":"1632278400.0"},{"comment_id":"156153","poster":"Nemer","timestamp":"1632106080.0","comments":[{"poster":"loen","upvote_count":"8","comment_id":"191274","timestamp":"1632594780.0","content":"Hi, the question includes :\n The application code cannot be modified\nif use multipart upload , need to modify source code.\nI support D."}],"upvote_count":"3","content":"A. More efficient to use the AWS API than the console, to list & abort incomplete uploads. ListMultipartUploads/AbortMultipartUpload \n+ life cycle policies to reduce costs.\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/API_ListMultipartUploads.html"}],"topic":"1","exam_id":32,"unix_timestamp":1597208580,"answer_description":"","answer":"A","choices":{"C":"Upload device data using a multipart upload. Use the AWS Management Console to list incomplete parts to address the failed S3 uploads. Configure a lifecycle policy to archive continuously to Amazon S3 Glacier.","B":"Upload device data using S3 Transfer Acceleration. Use the AWS Management Console to address the failed S3 uploads. Use the Multi-Object Delete operation nightly to delete the old uploads.","D":"Upload device data using S3 Transfer Acceleration. Use the AWS Management Console to list incomplete parts to address the failed S3 uploads. Enable the lifecycle policy for the incomplete multipart uploads on the S3 bucket to delete the old uploads and prevent new failed uploads from accumulating.","A":"Upload device data using a multipart upload. Use the AWS CLI to list incomplete parts to address the failed S3 uploads. Enable the lifecycle policy for the incomplete multipart uploads on the S3 bucket to delete the old uploads and prevent new failed uploads from accumulating."},"url":"https://www.examtopics.com/discussions/amazon/view/28220-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[]}],"exam":{"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","isMCOnly":false,"isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","provider":"Amazon","id":32,"isBeta":false},"currentPage":109},"__N_SSP":true}