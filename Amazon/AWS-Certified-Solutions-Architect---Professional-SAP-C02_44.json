{"pageProps":{"questions":[{"id":"wbdf6IaMglPntLFoKlUQ","question_images":[],"question_id":216,"exam_id":33,"answer":"A","unix_timestamp":1687338600,"isMC":true,"answers_community":["A (89%)","11%"],"choices":{"C":"Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC.","D":"Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.","A":"Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.","B":"Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets."},"discussion":[{"comment_id":"944620","upvote_count":"53","timestamp":"1688645220.0","poster":"YodaMaster","content":"This question was painful to read."},{"upvote_count":"14","content":"Selected Answer: A\nAnswer - A\nD is closest, but wrong as you subnets cannot be modified. They have to be deleted and re-created.","comment_id":"1084843","timestamp":"1701389280.0","poster":"shaaam80"},{"content":"Selected Answer: D\nD is the answer.","comment_id":"1318382","timestamp":"1732672440.0","upvote_count":"1","poster":"youonebe"},{"comment_id":"1281139","upvote_count":"1","timestamp":"1725902400.0","content":"Selected Answer: D\nA is wrong. It involves a complex process of deleting and recreating subnets, which could lead to downtime and operational complexity. Also, the approach of creating new subnets from the old address space is risky and can be prone to errors.","poster":"Syre"},{"comment_id":"1095198","timestamp":"1702448100.0","upvote_count":"2","poster":"duriselvan","content":"D is ans\nere's why this option is the most suitable:\n\nMinimal downtime: It minimizes downtime by gradually shifting instances between subnets within the same VPC, ensuring continuous connectivity to the on-premises environment.\nNo additional address space: It utilizes the existing IPv4 address space by splitting the subnets, avoiding the need for additional resources.\nPhased approach: It implements the changes in manageable steps, minimizing risk and allowing for rollback if necessary."},{"content":"For the CIDR range, what's after '-'? Is something missing?","poster":"gary_gary","upvote_count":"1","timestamp":"1701064320.0","comment_id":"1081235"},{"timestamp":"1700414520.0","upvote_count":"8","poster":"Mikado211","content":"Selected Answer: A\nB do not follow the need of no downtime\nC will force you to migrate to a new CIDR\n\nA and D are similar except that in A you recreate the subnets while in D you update the subnets.\nBut you cannot update the subnets, you have to remove and recreate them.\n\nSo A is the correct answer.","comment_id":"1074769"},{"content":"Selected Answer: A\nIn a scenario where you must add a new AZ without service downtime, option A, which progressively transitions to new subnets in the new AZ while keeping the existing infrastructure running, is a better choice. This approach ensures high availability and minimal disruption to your services.\nOption D is not correct. You cannot update the CIDR block of an existing Amazon VPC subnet without recreating it.","comment_id":"1045598","poster":"totten","upvote_count":"4","timestamp":"1697523780.0"},{"content":"The question though lol had to look for the difference in the options to remember the answer. When it comes to a “delete “","poster":"Blingy","upvote_count":"2","comment_id":"1013883","timestamp":"1695373080.0"},{"content":"Selected Answer: D\nD is easier, no need to delete the subnet. https://docs.aws.amazon.com/vpc/latest/userguide/subnet-cidr-reservation.html","timestamp":"1693400640.0","poster":"Arnaud92","upvote_count":"2","comment_id":"994122"},{"comment_id":"986786","poster":"SK_Tyagi","upvote_count":"3","timestamp":"1692639000.0","content":"Selected Answer: A\nSurely wasn't a 3 min ques. Thankfully they did not throw CIDR reservations into the mix"},{"timestamp":"1688827200.0","upvote_count":"3","comment_id":"946565","content":"Selected Answer: A\nA. can't update subnet","poster":"NikkyDicky"},{"comment_id":"946140","poster":"Christina666","timestamp":"1688788260.0","upvote_count":"5","content":"These answers are big pain to read"},{"content":"Selected Answer: A\nA - Correct. You can't modify subnet as D says.","comment_id":"935735","timestamp":"1687891740.0","upvote_count":"2","poster":"SmileyCloud"},{"upvote_count":"4","poster":"nexus2020","timestamp":"1687821900.0","comment_id":"934862","content":"Selected Answer: A\nD: \"Update the AZ1 subnet\" in D is not possible. you have to delete and recreate a subnet, there is no update option\nB: service intruption \nC: is a joke....."},{"upvote_count":"1","timestamp":"1687623660.0","comment_id":"932746","content":"Selected Answer: A\nolabiba.ai says \"A\". Chatgpt kept bouncing between \"B\" & \"D\".","poster":"Jackhemo"},{"comment_id":"932599","upvote_count":"2","content":"Selected Answer: A\nA is answer","poster":"bhanus","timestamp":"1687614180.0"},{"content":"yep, A is correct.","poster":"PhuocT","upvote_count":"2","comment_id":"932476","timestamp":"1687605000.0"},{"upvote_count":"3","comment_id":"932022","poster":"jubileu84","content":"A is correct. https://repost.aws/knowledge-center/vpc-ip-address-range","timestamp":"1687565340.0"},{"poster":"MoussaNoussa","timestamp":"1687531080.0","content":"A is the correct answer","upvote_count":"1","comment_id":"931706"},{"content":"Selected Answer: D\nCorrect Answer is D","comment_id":"929255","timestamp":"1687338600.0","poster":"psyx21","upvote_count":"1","comments":[{"upvote_count":"2","poster":"nexus2020","timestamp":"1687821840.0","comment_id":"934861","content":"\"Update the AZ1 subnet\" in D is not possible. you have to delete and recreate a subnet, there is no update option","comments":[{"timestamp":"1688131320.0","upvote_count":"2","poster":"javitech83","comment_id":"939110","content":"psyx21 answer wrong on purpose"},{"upvote_count":"1","timestamp":"1693400520.0","content":"i think it's feasible (ndlr changing the cidr reservation: https://docs.aws.amazon.com/vpc/latest/userguide/subnet-cidr-reservation.html). So for me D can be easier to adopt.","poster":"Arnaud92","comment_id":"994118"}]}]}],"timestamp":"2023-06-21 11:10:00","url":"https://www.examtopics.com/discussions/amazon/view/112791-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:\n\n\nVPC CIDR: 10.0.0.0/23 -\n\nAZ1 subnet CIDR: 10.0.0.0/24 -\n\nAZ2 subnet CIDR: 10.0.1.0/24 -\n\nSince deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime. Which solution will meet these requirements?","answer_images":[],"topic":"1","answer_description":"","answer_ET":"A"},{"id":"hOTU4g8OE0sKolTbVaQh","answer_images":[],"answer_ET":"A","answer":"A","question_text":"A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to deploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using a predefined list of project values.\n\nWhen the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant project values. The company wants to enforce the use of project tags for new resources.\n\nWhich solution will meet these requirements with the LEAST effort?","exam_id":33,"discussion":[{"poster":"ayadmawla","comment_id":"1098825","content":"Selected Answer: A\nThe key to the answer is in the first sentence of A and B. You can create a Tag Policy in the Management Account not OU since the OU is not an \"Account\" but a target where a policy is applied. Tag Policy is not the same as an SCP. \n\nSee: https://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/","timestamp":"1718614980.0","upvote_count":"7"},{"poster":"bhanus","comment_id":"932609","comments":[{"content":"The tags are different for each OU.","comment_id":"942684","poster":"SmileyCloud","timestamp":"1704374940.0","upvote_count":"5"}],"content":"Selected Answer: A\nA is correct BUT I did NOT like the last line in option A. It says \"Attach the SCP to each OU\". Why should you attach SCP to each OU. Can't you just attach to RootOU so it gets inherited to child OUs","upvote_count":"7","timestamp":"1703432880.0"},{"comment_id":"1074815","timestamp":"1716134700.0","upvote_count":"1","content":"Ok this is strange if you do not use this stuff regularly as AWS uses \"tag policy\" for several different configuration services.\n\nYou can apply a tag policy on the management account through AWS Organization. If you do it all child OUs will inherit the tag policy.\n\nIf you do the same \"tag policy\" on the management account using AWS Resource Groups Tag Editor it will not be inherited. \n\nB was a very seductive answer, even chatGPT made a mistake here by defining this answer as good in first occurence.\n\nBut considering we use AWS Organization to manage everything, it's clearly an AWS Organization Tag Policy which is used here. So a tag policy applied on the management account will be inherited by the child OUs.\n\nAnswer is A.\n\nAWS terminology can be really bad.","poster":"Mikado211"},{"upvote_count":"1","timestamp":"1705799340.0","poster":"ggrodskiy","comment_id":"957931","content":"Correct A."},{"content":"Selected Answer: A\nA. tag policy create in management account","poster":"NikkyDicky","upvote_count":"3","comment_id":"946567","timestamp":"1704732660.0"},{"timestamp":"1703778240.0","comment_id":"936725","upvote_count":"2","poster":"SkyZeroZx","content":"Selected Answer: A\nA) in management account for tag policy and SCP , Sounds Good\nB) for each account ? more overhead\nC ) IAM for account in cloudformation ? is incorrect in this case\nD) AWS Service Catalog ? why ? incorrect"},{"content":"Selected Answer: A\nA - Correct. You create an SCP with allowed tags in the root OU and then attach the SCP to all OUs.","comment_id":"935753","poster":"SmileyCloud","upvote_count":"1","timestamp":"1703711280.0"},{"timestamp":"1703709540.0","poster":"Jonalb","content":"Selected Answer: A\nAAAAAAAAAAAAA","upvote_count":"1","comment_id":"935721"},{"timestamp":"1703383860.0","poster":"jubileu84","upvote_count":"1","comment_id":"932025","content":"Correct Answer is A"},{"upvote_count":"3","timestamp":"1703359740.0","content":"Selected Answer: A\nA) Is correct in the master account of all organization use SCP is less overhead than B\nB ) is more overhead than A because in each OU create SCP\nC ) IAM in all account is more overhead\nD) is valid but not restrict other options o create with CLI or console the rest service without tags\n\nThen A is correct","poster":"SkyZeroZx","comment_id":"931823"},{"timestamp":"1703288760.0","content":"Selected Answer: A\nolabiba.ai says 'A'","comment_id":"931072","upvote_count":"1","poster":"Jackhemo"},{"timestamp":"1703157060.0","poster":"psyx21","upvote_count":"1","comment_id":"929256","content":"Selected Answer: A\nCorrect Answer is A"},{"content":"Selected Answer: A\nWhat not use SCP?","comment_id":"928607","poster":"bmdf","timestamp":"1703097960.0","upvote_count":"1"}],"timestamp":"2023-06-20 18:46:00","question_id":217,"question_images":[],"isMC":true,"unix_timestamp":1687279560,"answer_description":"","answers_community":["A (100%)"],"choices":{"D":"Use AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share the portfolio with all OUs that are in the organization.","C":"Create a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the cloudformation:CreateStack API operation unless a project tag is added. Assign the policy to each user.","A":"Create a tag policy that contains the allowed project tag values in the organization's management account. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.","B":"Create a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU."},"url":"https://www.examtopics.com/discussions/amazon/view/112695-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1"},{"id":"JqBhokO8RqbmYit8glXl","choices":{"C":"Use the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group's launch template. Remove the current instance type from the configuration.","D":"Create a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a new revision of the Auto Scaling group's launch template.","B":"Use the information about the application's CPU and memory utilization to select an instance type that matches the requirements. Modify the Auto Scaling group's configuration by adding the new instance type. Remove the current instance type from the configuration.","A":"List instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group's launch template configuration to use multiple instance types from the list."},"unix_timestamp":1687338660,"answer_images":[],"answer":"C","answers_community":["C (62%)","B (34%)","4%"],"question_images":[],"topic":"1","answer_description":"","question_text":"An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type of instance.\n\nCPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently reduce the EC2 cost and increase the utilization.\n\nWhich solution will meet these requirements with the LEAST number of configuration changes in the future?","isMC":true,"discussion":[{"timestamp":"1687893360.0","comment_id":"935761","upvote_count":"15","comments":[{"upvote_count":"5","content":"I’ve tested it myself in the AWS Console – correct answer is “B”. To change the instance type you have 3 options, and all of them require modifying the ASG’s config:\n1. Create a new revision of the current launch template, then change the ASG config to use it.\n2. Create a new launch template, then change the ASG config to use it.\n3. Use the option “Override launch template” in the ASG config.\nIf you only create a new revision of the launch template, the ASG will continue to use the old revision. The state that you cannot change the instance type from the ASG config is NOT true and anybody can verify it in the AWS Console.","poster":"titi_r","timestamp":"1712945400.0","comment_id":"1194466","comments":[{"timestamp":"1724224980.0","comment_id":"1269922","poster":"helloworldabc","upvote_count":"1","content":"just C"}]}],"content":"Selected Answer: C\nIt's C. You change the instance type/size in the launch template not the ASG. ASG can change the min/max size, not instance type.","poster":"SmileyCloud"},{"comment_id":"1135229","comments":[{"timestamp":"1717421700.0","poster":"fartosh","comment_id":"1223611","upvote_count":"2","content":"I believe \"The Auto Scaling group configuration uses only one type of instance.\" is just a badly phrased sentence and the question designer only meant that instances running under AutoScaling Group are of one instance type. I understand the sentence as a suggestion to improve the state by specifying multiple types instead.\n\nApart from the wording, there's nothing wrong with answer C. It lets you stop worrying about the future instance generations, too, compared to B where you have to modify the instance type whenever a new generation is released. Also, as specific instance types can be temporarily unavailable (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity), C can smoothly use another available instance automatically."}],"poster":"LazyAutonomy","content":"Selected Answer: B\nThe answer used to be C, but now it's B. But not for the reasons others here have mentioned. The question states that \"The Auto Scaling group configuration uses only one type of instance\". This implies the ASG config has implemented instance overrides, which - you guessed it - overrides the instance type that's specified in the launch template. You could cut new versions of launch templates until you're blue in the face, it wont make a lick of difference if the ASG config is overriding the instance type. And because ASGs can be modified, I reckon that puts a nail in C's coffin, making B the new correct answer. I think this is the first question (out of 400+) where the moderator-selected solution was correct and the community-voted solution was incorrect.","upvote_count":"8","timestamp":"1706554440.0"},{"content":"Selected Answer: C\nA out since “similar” still be under utilized.\nB reported information about utilization look sexy, B is feasible, but specify “an instance type” would not be flexible as C\nC using launch template, specify CPU and memory instead of instance type => AWS will select suitable instance type, this is for the \"future\" requirement\nD seems overkill","poster":"LuongTo","timestamp":"1733445420.0","upvote_count":"1","comment_id":"1322575"},{"poster":"Syre","content":"Selected Answer: A\nWhy Option C is Less Optimal:\nSingle Instance Type Limitation: By specifying CPU and memory requirements for a single instance type, you limit the flexibility of your Auto Scaling group. If the chosen instance type does not fully match the application's varying load, it may result in either underutilization or performance issues.\n\nNo Cost Optimization: This option does not take advantage of cost differences among different instance types. Without multiple instance types, you miss out on opportunities to select more cost-effective options based on current pricing and utilization needs.\n\nFuture Configuration Changes: While specifying the right instance type initially is good, it doesn't address changing application requirements or price fluctuations over time. It could still require adjustments in the future if the chosen instance type becomes less cost-effective or if application requirements change.","timestamp":"1725904740.0","comment_id":"1281167","upvote_count":"1"},{"upvote_count":"1","comment_id":"1271599","timestamp":"1724489460.0","content":"Selected Answer: B\nPerhaps it’s an older question that AWS intended for us to solve using C.\nBut nowadays days its B for the reasons explained by some of the people in this thread.\n\nI would be really surprised to see this question in the exam, but if I will - I'll definitely go with B","poster":"tgv"},{"timestamp":"1720005540.0","comment_id":"1241367","upvote_count":"2","content":"Selected Answer: A\nCorrect answer is A:\nThis solution allows for a mix of instance types, which can help optimize costs and increase utilization.\nBy using similar instance types, it ensures compatibility with the application's requirements.\nThis approach requires the least number of configuration changes in the future as it provides flexibility to automatically use different instance types as they become available or as prices change.\n\nB. This option limits the Auto Scaling group to a single instance type again, which doesn't provide flexibility for future changes.\nC. Specifying CPU and memory requirements without instance types may lead to unexpected instance selections and potential compatibility issues.\nD. Using a script with the AWS Price List Bulk API could lead to frequent changes and may select instance types that aren't optimal for the application's needs.","poster":"053081f"},{"comment_id":"1206000","upvote_count":"2","content":"Selected Answer: C\nC for me","poster":"seetpt","timestamp":"1714725900.0"},{"content":"Selected Answer: B\nI’ve tested it myself in the AWS Console – correct answer is “B”. To change the instance type you have 3 options, and all of them require modifying the ASG’s config:\n1. Create a new revision of the current launch template, then change the ASG config to use it.\n2. Create a new launch template, then change the ASG config to use it.\n3. Use the option “Override launch template” in the ASG config.\nIf you only create a new revision of the launch template, the ASG will continue to use the old revision. The state that you cannot change the instance type from the ASG config is NOT true and anybody can verify it in the AWS Console.","poster":"titi_r","comment_id":"1194467","upvote_count":"4","timestamp":"1712945460.0"},{"comment_id":"1174824","upvote_count":"3","poster":"career360guru","content":"Selected Answer: C\nOption C","timestamp":"1710574680.0"},{"content":"C:\n\nAuto scaling group is built on top of launch template, you can reference AMI in template, but not in auto scaling group","timestamp":"1708634340.0","poster":"adelynllllllllll","comment_id":"1156708","upvote_count":"1"},{"upvote_count":"2","timestamp":"1707571080.0","poster":"igor12ghsj577","content":"AWS does not allow to edit launch configuration. If you notice, we define instance type at time of launch configuration. So if you want to change instance type in Auto Scaling group than you need to create new launch configuration for that.","comment_id":"1146326"},{"content":"Selected Answer: B\nC is wrong.\n\nLet's assume a scenario where the optimal hardware requirement for a program under load is 4GB of RAM for every 1 CPU. \n\nHowever, you have specified only one type of instance with 1CPU and 1GB RAM.\n\nEven if you choose Option C and apply load balancing, having 4 instances of 1CPU and 1GB RAM (totaling 4CPU and 4GB RAM) will still result in an issue of low CPU utilization.","comment_id":"1123057","poster":"tmlong18","upvote_count":"2","timestamp":"1705292880.0"},{"comment_id":"1098837","timestamp":"1702811520.0","content":"Selected Answer: C\nKey to the Answer is \"Modify\". Launch templates are immutable; after you create a launch template, you can't modify it. Instead, you can create a new version of the launch template that includes any changes you require.\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-launch-template-versions.html","upvote_count":"6","poster":"ayadmawla"},{"content":"b IS ANS\nMinimal configuration changes: This solution only requires modifying the Auto Scaling group configuration to add the new, more efficient instance type and remove the old, underutilized type. This minimizes future maintenance and reduces the risk of introducing errors.\n\nScalability and flexibility: The Auto Scaling group will automatically scale up and down based on demand, even with the new instance type. This ensures high availability and cost-effectiveness.\n\nFuture-proof: This approach doesn't rely on specific instance types or the AWS Price List Bulk API, making it more adaptable to future changes and updates in the AWS ecosystem.","timestamp":"1702135620.0","comment_id":"1091878","poster":"duriselvan","upvote_count":"1"},{"timestamp":"1700394300.0","poster":"severlight","content":"Selected Answer: C\nwe cannot change/modify launch config or launch template","upvote_count":"4","comment_id":"1074594"},{"upvote_count":"5","comment_id":"997471","timestamp":"1693731480.0","poster":"cmoreira","content":"Selected Answer: C\nIt could be B or C, but \"LEAST number of configuration changes in the future\" makes it C."},{"content":"Selected Answer: B\nIn the launch template, you can only select one instance type. You can however override the Launch Template in the ASG configuration and specify multiple instance types.","timestamp":"1693311000.0","comment_id":"993057","poster":"aviathor","upvote_count":"7"},{"comment_id":"982626","upvote_count":"4","content":"Selected Answer: C\nattribute-based instance types","timestamp":"1692192660.0","poster":"softarts"},{"upvote_count":"1","poster":"ggrodskiy","timestamp":"1689894360.0","content":"Correct C.","comment_id":"957927"},{"comments":[{"comment_id":"946573","poster":"NikkyDicky","upvote_count":"2","content":"ah, damn, clicked the wrong one.\n\nIt's a C! not B","timestamp":"1688828220.0"}],"comment_id":"946570","poster":"NikkyDicky","timestamp":"1688828220.0","upvote_count":"2","content":"Selected Answer: B\nits a B"},{"timestamp":"1687963380.0","comments":[{"timestamp":"1717421160.0","content":"I do agree with the approach, it's the most robust way. I was also going to quote AWS docs that only AutoScaling Group can specify instance attributes until I found this resource:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-launchtemplate-launchtemplatedata.html#cfn-ec2-launchtemplate-launchtemplatedata-instancerequirements. From the linked page you can see that Launch Template can define instance requirements as well.","comment_id":"1223599","upvote_count":"1","poster":"fartosh"}],"content":"Selected Answer: C\n\" with the LEAST number of configuration changes in the future?\" means we need to use attribute-based instance types. Otherwise as new instance types get created, and older ones get retired, we need to re-configure launch config again.","poster":"pupsik","upvote_count":"5","comment_id":"936785"},{"timestamp":"1687823220.0","content":"Selected Answer: B\nC: Application recommend to use X, but the real utilizationi is low, aka underutilized. so C is NOT addressing the cost saving part.\n\nB would be the answer addressing the right sizing. utilization is also what AWS recommend to check when doing right sizing, such as using Trusted Advisor to see the under utilization, using compute optimizer, cloudwatch log, etc","comment_id":"934873","poster":"nexus2020","upvote_count":"1"},{"comment_id":"934794","poster":"Alabi","upvote_count":"2","content":"Selected Answer: C\nBy using the information about the application's CPU and memory utilization, you can determine the CPU and memory requirements of the application.\n In this solution, you create a new revision of the Auto Scaling group's launch template and specify the CPU and memory requirements in the template. This ensures that the new instances launched by the Auto Scaling group meet the application's requirements.\n By removing the current instance type from the configuration, you ensure that only instances with the specified CPU and memory requirements are launched, effectively increasing utilization and optimizing costs.\n This solution requires minimal configuration changes as you are primarily modifying the launch template with the updated CPU and memory requirements.","timestamp":"1687814340.0"},{"content":"c-c-c-c-c-c-c","upvote_count":"1","timestamp":"1687806780.0","poster":"easytoo","comment_id":"934734"},{"poster":"Maria2023","upvote_count":"3","timestamp":"1687694520.0","comments":[{"poster":"Maria2023","upvote_count":"1","timestamp":"1687695240.0","comment_id":"933545","content":"This might be available only for Spot instances though, needs to be confirmed"}],"content":"Selected Answer: C\nI vote for C because of the attribute-based instance type selection, available in the ASG configuration","comment_id":"933539"},{"comments":[{"poster":"rxhan","comment_id":"933889","upvote_count":"1","content":"I am thinking so too, since Launch Templates are what AWS recommends. Launch Configs will be retired.","timestamp":"1687726500.0"},{"timestamp":"1692871500.0","poster":"skyhiker","upvote_count":"1","content":"Spot on with this link, thank you!","comment_id":"989063"}],"poster":"ozelllll","upvote_count":"3","comment_id":"933201","content":"Selected Answer: C\nIt's C: https://aws.amazon.com/blogs/aws/new-attribute-based-instance-type-selection-for-ec2-auto-scaling-and-ec2-fleet/","timestamp":"1687667400.0"},{"upvote_count":"1","timestamp":"1687627680.0","poster":"gd1","comment_id":"932811","content":"Selected Answer: B\nB is correct"},{"timestamp":"1687338660.0","comment_id":"929257","upvote_count":"3","content":"Selected Answer: B\nCorrect Answer is B","poster":"psyx21"}],"url":"https://www.examtopics.com/discussions/amazon/view/112792-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"timestamp":"2023-06-21 11:11:00","question_id":218,"answer_ET":"C"},{"id":"0rPL9E5LV9pH0SZitTot","unix_timestamp":1687338720,"url":"https://www.examtopics.com/discussions/amazon/view/112793-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"answer_description":"","discussion":[{"content":"I think the key here is to focus on the requirements. It is clearly stated that the requirement is that the strategy meet an RPO of 2 hours and an RTO of 4 hours. Even though option C is the most cost-effective, it is contingent on a few external factors, like the size of the data, the data change rate, etc., which cannot be assumed at the risk of breaching RPO and RTO requirements. So based on that, the most effective option is D.","timestamp":"1692251400.0","poster":"finesse_999","comment_id":"983255","comments":[{"poster":"mike5656","upvote_count":"1","comment_id":"1320860","content":"agree with titi_r","timestamp":"1733131080.0"},{"comment_id":"1046407","timestamp":"1697582640.0","content":"Agreed","comments":[{"poster":"titi_r","upvote_count":"4","comment_id":"1194483","timestamp":"1712946840.0","content":"\"C\" does not mention a restore operation at all. Where will Route 53 route the traffic in the secondary Region: to the DB snapshots in the AWS Backup vault maybe?\nSo, D should be the right option.\n\nP.S. Very badly written question btw."}],"upvote_count":"1","poster":"backtorod"}],"upvote_count":"23"},{"content":"Selected Answer: C\nAnswer: C\nWeird question. Sometimes I think there is no BEST answer and that they were created just to confuse people.\nAnyway, thinking on cost and the mentioned RPO and RTO, I would still go with C (if they were longer, it would be easier to choose among the questions).","upvote_count":"7","comment_id":"978958","comments":[{"comment_id":"1269927","poster":"helloworldabc","comments":[{"upvote_count":"1","poster":"Halliphax","comment_id":"1309143","timestamp":"1731171000.0","content":"just C"}],"timestamp":"1724225220.0","upvote_count":"1","content":"just D"}],"timestamp":"1691785320.0","poster":"chico2023"},{"timestamp":"1741438680.0","comment_id":"1366559","upvote_count":"1","content":"Selected Answer: D\nAWS Backup has the lowest cost but cannot fulfill RTO requirement.","poster":"albert_kuo"},{"upvote_count":"2","timestamp":"1735984080.0","content":"Selected Answer: D\nI voted for D.\n\nWhile option C (using AWS Backup) might seem cost-effective, it may not consistently meet RTO/RPO requirements, especially for large datasets. The reliability of AWS Backup, while generally good, cannot guarantee meeting specific recovery time and recovery point objectives.\n\nThe question might be designed to trick you into choosing the cheapest option, but a reliable disaster recovery solution is crucial. Therefore, I chose option D.\"**","poster":"PSPaul","comment_id":"1336322"},{"poster":"SIJUTHOMASP","content":"Selected Answer: C\nI think we need to assume the ideal situation whenever there is no specific mention about the database size. In ideal situation 2 hours is more than sufficient to take backup. Since the question has the key for cost-effectiveness, the answer would be C. In addition, there are multiple options available to expedite the recovery from backup if the normal path can't meet RTO.","upvote_count":"2","comment_id":"1330939","timestamp":"1734986220.0"},{"comment_id":"1301846","poster":"sashenka","upvote_count":"1","content":"Selected Answer: D\nWhy AWS Backup (Option C) May Not Be Suitable:\nCross-region snapshot copies \"can take hours to complete\" depending on database size and regions involved.Since the database size is unknown Cross-region copies have variable completion times. We need to guarantee an RPO of 2 hours. AWS Backup cannot reliably guarantee the 2-hour RPO requirement due to these uncertainties.\nBetter Solution: \nAurora Global Database (Option D) would be more appropriate because:\nIt provides replication lag of typically less than 1 second, easily meeting the 2-hour RPO requirement. It enables fast global failover to secondary regions in minutes, meeting the 4-hour RTO requirement. Route 53 failover routing provides automated traffic switching during recovery. While Aurora Global Database may be more expensive than AWS Backup, it's the only solution among the options that can definitively guarantee meeting both the RPO and RTO requirements regardless of database size. Therefore, Option D is the correct choice as it's the only solution that can reliably meet both the RPO and RTO requirements with certainty.","timestamp":"1729653000.0"},{"upvote_count":"1","content":"The ONLY reason I am going with C is that AWS Backup is generally more cost-effective compared to continuous replication setups like Aurora Global Database or DynamoDB Global Tables1.\n\nIt allows you to create point-in-time backups and restore them in a secondary region, meeting the RPO requirement of 2 hours and RTO requirement of 4 hours","poster":"AloraCloud","timestamp":"1727736960.0","comment_id":"1291695"},{"content":"Selected Answer: D\nvote for D. Global table","timestamp":"1723599240.0","poster":"tsangckl","comment_id":"1265470","upvote_count":"2"},{"timestamp":"1714725960.0","upvote_count":"1","content":"Selected Answer: C\nC for me","comment_id":"1206002","poster":"seetpt"},{"upvote_count":"1","timestamp":"1714528680.0","content":"My answer is B.\nB is cheapest and it will create only when event occurs. it will complete within 2hours.\nC and D are costly options compare to BH","comment_id":"1204812","poster":"43c89f4"},{"content":"Selected Answer: C\nAWS often publish this kind of bad framed question. The question is looking for most cost effective solution. So I believe C is the expected answer even it is not complete answer. But C has three big problems: \n1. a backup is a backup, if it doesn't provide a way to restore, it is only a backup and is not a complete DR. \n2. It doesn't mention the frequency of the backup nor the continuous backup, which means we don't know whether it can meet the 2hr RPO. \n3. It doesn’t mention the ECS DR. Well, neither does the other answers.\n.\nAurora global db and DynamoDB global table are apparently more expensive. With the question design, they should be excluded even they are actually complete answers.","upvote_count":"5","poster":"bjexamprep","timestamp":"1711080360.0","comment_id":"1179841"},{"poster":"teo2157","timestamp":"1710502020.0","upvote_count":"2","content":"Selected Answer: B\nGithub Copilot answer:\nThe solution you proposed is a good approach for implementing a disaster recovery (DR) strategy. Here's a breakdown of how it works:\n1. **AWS Database Migration Service (DMS)**: This service can be used to replicate data from your Amazon Aurora databases in the primary region to the secondary region. This ensures that you have a backup of your data in case of a disaster in the primary region.\n2. **Amazon EventBridge and AWS Lambda**: These services can be used together to trigger the replication process whenever there is a change in the Aurora databases.\n3. **DynamoDB Streams, EventBridge, and Lambda**: DynamoDB Streams capture table activity, and you can use Lambda functions triggered by EventBridge to process the stream and replicate the changes to DynamoDB tables in the secondary region.","comment_id":"1174236"},{"timestamp":"1709743740.0","comment_id":"1167350","poster":"hogtrough","upvote_count":"1","content":"Selected Answer: D\nWe have no idea the size of the db thus we can't assume we can reach an RTO of 4 hours using backups. D is the cheapest solution out of A, B and D."},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\nThus the correct answer is C, as the minimum RPO for AWS Backup (unless you use Point-in-time recovery) is exactly 2 hours.","poster":"6a03ffb","upvote_count":"4","timestamp":"1709196720.0","comment_id":"1162356"},{"poster":"chelbsik","content":"Selected Answer: D\nBackup restore can take more than 4 hours, so D","comments":[{"timestamp":"1708165320.0","poster":"ele","upvote_count":"1","comment_id":"1152483","content":"In general time to restore from a recovery point using AWS Backup depends on the size of the data and type of resource being restored, is it a single DB, or an entire aurora cluster, a time frame cannot be estimated, it may take 5 minutes or 1 hour"}],"upvote_count":"3","comment_id":"1145966","timestamp":"1707549240.0"},{"timestamp":"1707224580.0","content":"Selected Answer: D\nThe correct answer is in fact D. Though the question asks for a cost effective option. Option C does not guarantee on the mentioned RPO and RTO.\nSo between A and D what is the most cost effective way.\nD wins as it does not have cost of Cloudfront","poster":"saggy4","upvote_count":"2","comment_id":"1142158"},{"poster":"bjexamprep","upvote_count":"5","content":"Selected Answer: C\nAWS often publish this kind of bad framed question. The question is looking for most cost effective solution. So I believe C is the expected answer even it is not complete answer. But C has two big problems: \n1. a backup is a backup, if it doesn't provide a way to restore, it is only a backup and is not a complete DR. \n2. It doesn't mention the frequency of the backup nor the continues backup, which means we don't whether it can meet the 2hr RPO. \n.\nAurora global db and DynamoDB global table are apparently more expensive. With the question design, they should be excluded even they are actually complete answers.","comment_id":"1110024","timestamp":"1703971500.0"},{"timestamp":"1702051080.0","upvote_count":"1","poster":"duriselvan","comment_id":"1091171","content":"D is ans\nDatabase replication: Aurora global databases and DynamoDB global tables provide automatic, continuous replication across Regions, ensuring an RPO of 2 hours or less. This eliminates the need for manual database setup or complex replication processes.\nRegional API endpoints: Configuring API Gateway APIs with Regional endpoints in both Regions ensures availability in either Region, supporting a quick RTO of 4 hours.\nRoute 53 failover routing: Route 53 provides a cost-effective and efficient way to switch traffic between Regions during a DR event. It eliminates the need for more expensive services like CloudFront for failover."},{"upvote_count":"1","poster":"duriselvan","timestamp":"1702051020.0","content":"d ANS","comment_id":"1091170"},{"content":"Selected Answer: C\nIts C based on the: \"Which solution will meet these requirements MOST cost-effectively?\"","comment_id":"1091020","poster":"ayadmawla","upvote_count":"3","timestamp":"1702035300.0"},{"upvote_count":"4","content":"Selected Answer: C\nThe AWS backup based approach is highly cost-effective, employs a backup and restore strategy, and can be designed to comply with cross region backup regulatory requirements. I also explained Aurora Global Database, an Aurora feature which can be utilized when you have strict RTO and RPO requirements.\nhttps://aws.amazon.com/es/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/","poster":"D10SJoker","comment_id":"1075284","timestamp":"1700471580.0"},{"upvote_count":"4","timestamp":"1700394900.0","comment_id":"1074596","poster":"severlight","content":"Selected Answer: C\nthey mentioned cloud formation, code pipeline and made rto and rpo less strict"},{"comment_id":"1069663","poster":"enk","timestamp":"1699902420.0","content":"Selected Answer: B\nChatGPT","comments":[{"content":"ChatGPT Isn't perffect. You need to think more and write less with chatGPT.","timestamp":"1700137320.0","upvote_count":"5","poster":"nublit","comment_id":"1072398"}],"upvote_count":"1"},{"comment_id":"1028809","poster":"ggrodskiy","timestamp":"1696850280.0","upvote_count":"2","content":"Selected Answer: D\nRTO 4 + RPO 2 + MOST cost-effectively = D\nRTO 4 + RPO ????? + MOST cost-effectively= C"},{"timestamp":"1696785120.0","poster":"7f37374","upvote_count":"2","content":"Selected Answer: D\nthe most effective option is D.","comment_id":"1028184"},{"comment_id":"998082","timestamp":"1693792080.0","content":"Answer is C for the RTO and RPO provided","upvote_count":"3","poster":"Explorer_30"},{"content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/","comment_id":"986809","upvote_count":"3","poster":"SK_Tyagi","timestamp":"1692640800.0"},{"poster":"punj","content":"Selected Answer: C\nTo implement C backups must be taken at the interval of 2 hours to satisy RPO which might not be very efficient. \nD is efficient but costs slightly more.","comment_id":"976157","timestamp":"1691545080.0","upvote_count":"2","comments":[{"upvote_count":"1","poster":"kejam","timestamp":"1705881840.0","content":"Not necessarily. Both Aurora and DynamoDB support Point In Time Recovery from their Backups.","comment_id":"1128256"}]},{"comment_id":"968617","poster":"rxhan","content":"Selected Answer: C\nC and D are crorect solution\nC is the answer as it is cost less https://aws.amazon.com/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/","upvote_count":"1","timestamp":"1690848660.0"},{"content":"Correct D.","poster":"ggrodskiy","upvote_count":"2","timestamp":"1689894000.0","comment_id":"957924"},{"upvote_count":"2","timestamp":"1689002460.0","content":"Selected Answer: D\nI am not sure about that the C can satisfy requirement for RPO and RTO.","poster":"Piccaso","comment_id":"948234"},{"timestamp":"1688828700.0","poster":"NikkyDicky","upvote_count":"3","comment_id":"946575","content":"Selected Answer: C\nwhat a bizarre question... no mention about failing over application components in answers.. \nlooking at just the DBs and with RPO/RTO targets, C is the answer for cost-effective"},{"timestamp":"1688644680.0","upvote_count":"1","content":"Selected Answer: C\nC is cheaper than D","poster":"YodaMaster","comment_id":"944610"},{"comment_id":"939126","upvote_count":"2","content":"Selected Answer: C\nThe most cost effective would be C. Although RPO of 2 hours will requeire to a backup every 2 hours, but is chepaer than global databases","poster":"javitech83","timestamp":"1688132700.0"},{"comment_id":"936799","content":"Selected Answer: C\n\"RTO of 4 hours\" and \"MOST cost effective\" - use AWS Backup.","poster":"pupsik","upvote_count":"1","timestamp":"1687964520.0"},{"comment_id":"935776","timestamp":"1687894260.0","poster":"SmileyCloud","content":"Selected Answer: D\nD - When you see DR, your best bet is Rt53 and failover. In this case, global tables and regional GW API also contributes.","upvote_count":"2"},{"comment_id":"935448","poster":"CloudInfrastructures","upvote_count":"1","content":"Selected Answer: C\nBackup is cheaper and it probably fits in the RTO","timestamp":"1687874160.0"},{"poster":"nexus2020","upvote_count":"1","timestamp":"1687823820.0","content":"Selected Answer: C\nGlobal table (D) cost more than using Backup (C)","comment_id":"934881"},{"timestamp":"1687774560.0","poster":"zhaogster","content":"C is correct","upvote_count":"3","comment_id":"934284"},{"content":"Selected Answer: D\nThe answer is D.\n\nThis solution is the most cost-effective because it uses Amazon Aurora global databases and DynamoDB global tables to replicate the databases to a secondary AWS Region. This replication is done automatically and in real time, so the RPO is 0 seconds. The RTO is also low, because the secondary Region is already configured and ready to take over in the event of a disaster.","poster":"SkyZeroZx","timestamp":"1687725240.0","comments":[{"content":"The other solutions are more expensive because they require manual replication or backups. For example, solution B uses AWS DMS, DynamoDB Streams, EventBridge, and Lambda to replicate the databases to a secondary Region. This replication is not done in real time, so the RPO is greater than 0 seconds. The RTO is also higher, because the secondary Region must be manually configured and provisioned before it can be used.\n\nSolution C uses AWS Backup to create backups of the databases in a secondary AWS Region. This solution has a lower RPO than solution B, because the backups can be restored to a point in time within the backup retention period. However, the RTO is still higher than solution D, because the backups must be restored manually before the databases can be used.","comments":[{"comment_id":"939127","upvote_count":"3","poster":"javitech83","content":"Option C RTO is higher, but the solution is cheaper and we need to sleect the most COST effective","timestamp":"1688132820.0"}],"poster":"SkyZeroZx","timestamp":"1687725300.0","comment_id":"933880","upvote_count":"2"},{"comment_id":"934880","content":"Question is asking for RPO of 2 hours and an RTO of 4 hours. it is not asking which option provides the lowest RPO RTO. it is asking which one cost less","poster":"nexus2020","upvote_count":"3","timestamp":"1687823760.0"}],"comment_id":"933879","upvote_count":"5"},{"comment_id":"933594","content":"Selected Answer: D\nD is correct","upvote_count":"2","poster":"psyx21","timestamp":"1687697160.0"},{"comment_id":"933443","poster":"shree2023","content":"Selected Answer: D\nD is correct answer","timestamp":"1687687380.0","upvote_count":"2"},{"comment_id":"933373","poster":"nicecurls","upvote_count":"2","timestamp":"1687680840.0","content":"Selected Answer: D\ni believe in D"},{"timestamp":"1687627980.0","poster":"gd1","content":"Selected Answer: D\nThe solution includes setting up an Aurora Global Database and DynamoDB Global Tables to replicate the databases to a secondary AWS Region. The proposed solution also involves configuring an API Gateway API with a regional endpoint in both the primary and secondary regions. use of Amazon Route 53 failover routing to switch traffic from the primary region to the secondary region.","upvote_count":"2","comment_id":"932813"},{"timestamp":"1687605840.0","upvote_count":"1","comment_id":"932485","content":"D is answer.","poster":"PhuocT"},{"upvote_count":"1","content":"Selected Answer: B\nCorrect Answer is B","timestamp":"1687338720.0","poster":"psyx21","comment_id":"929258"}],"answer_images":[],"answer_ET":"C","exam_id":33,"question_id":219,"choices":{"A":"Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront with origin failover to route traffic to the secondary Region during a DR scenario.","C":"Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.","B":"Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB Streams, EventBridge. and Lambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.","D":"Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."},"answer":"C","question_text":"A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway The application data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning by using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.\n\nA solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.\n\nWhich solution will meet these requirements MOST cost-effectively?","question_images":[],"topic":"1","answers_community":["C (57%)","D (38%)","5%"],"timestamp":"2023-06-21 11:12:00"},{"id":"0dlxJGEaCSikb1L6MBuS","answers_community":["A (92%)","8%"],"answer_images":[],"exam_id":33,"discussion":[{"timestamp":"1689165000.0","upvote_count":"12","content":"A. Yes, because Amazon CloudFront considers the case of parameter names and values when caching based on query string parameters , thus inconsistent query strings may cause CloudFront to forward mixed-cased/misordered requests to the origin.\nTriggering a Lambda@Edge function based on a viewer request event to sort parameters by name and force them to be lowercase is the best choice.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html#query-string-parameters-optimizing-caching\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-cloudfront-trigger-events.html\n\nB. No, because this will exacerbate the caching issue by sending all query string parameters requests to the origin\nC. No, because this won't help increase the cache hit ratio\nD. No, because a CloudFront distribution specifies information about the origin/source of your content and how to track and manage content delivery.","comment_id":"949764","poster":"dkx"},{"content":"Selected Answer: A\nCloudFront does NOT support to specify casing-insensitive query string","timestamp":"1741438980.0","upvote_count":"1","comment_id":"1366560","poster":"albert_kuo"},{"poster":"ChanduWodeyar","content":"Answer:D is best and cheap.","upvote_count":"1","comment_id":"1236781","timestamp":"1719305280.0","comments":[{"upvote_count":"1","timestamp":"1724225340.0","comment_id":"1269933","poster":"helloworldabc","content":"just A"}]},{"timestamp":"1706556180.0","poster":"LazyAutonomy","content":"Selected Answer: C\nOMG so now I have to invoke and pay for a Lambda for every single GET request that traverses my CDN? No, F*** that. If D isn't supported then ciao bella s/Cloudfront/Cloudflare/g and say hello to Apache running mod_substitute thank you very much.","upvote_count":"1","comment_id":"1135250"},{"timestamp":"1702443360.0","content":"Caching based on query string parameters\n\nIf you configure CloudFront to cache based on query string parameters, you can improve caching if you do the following:\n\nConfigure CloudFront to forward only the query string parameters for which your origin will return unique objects.","comment_id":"1095141","poster":"duriselvan","upvote_count":"1"},{"upvote_count":"1","timestamp":"1702443300.0","comment_id":"1095140","poster":"duriselvan","content":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html"},{"poster":"duriselvan","content":"D is ans\nD. Casing-insensitive query string processing:\n\nThis is the simplest and fastest solution to implement.\nIt will treat requests with the same query string but different character casing as identical, boosting the cache hit ratio.\nIt utilizes built-in functionality of CloudFront without requiring additional services or configurations.\nRemember, while other options might offer additional functionalities, the primary goal is to quickly improve the cache hit ratio. Specifying casing-insensitive query string processing achieves this with minimal impact and complexity.","timestamp":"1702443180.0","upvote_count":"1","comment_id":"1095137"},{"timestamp":"1689893880.0","comment_id":"957920","content":"Correct A.","upvote_count":"2","poster":"ggrodskiy"},{"upvote_count":"2","poster":"NikkyDicky","timestamp":"1688829000.0","comment_id":"946580","content":"Selected Answer: A\nits an A\nD would be nice if was supported"},{"timestamp":"1687894500.0","comment_id":"935780","content":"Selected Answer: A\nA - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters","poster":"SmileyCloud","upvote_count":"3"},{"timestamp":"1687894440.0","upvote_count":"1","poster":"SmileyCloud","content":"A - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters","comment_id":"935779"},{"comment_id":"934887","poster":"nexus2020","upvote_count":"1","timestamp":"1687824120.0","content":"Selected Answer: A\nD is out: CloudFront distributions do not have built-in support for specifying a case-insensitive query string. By default, CloudFront treats query strings as case-sensitive, meaning that a URL with a different case in the query string parameter would be treated as a separate object and potentially result in a cache miss."},{"comment_id":"933881","poster":"SkyZeroZx","upvote_count":"2","content":"Selected Answer: A\nA , same questions this version 1 \nhttps://www.examtopics.com/discussions/amazon/view/27789-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"1687725360.0"},{"content":"Selected Answer: A\nA is the answer -to sort parameters by name and force them to be lowercase","upvote_count":"2","timestamp":"1687628340.0","poster":"gd1","comment_id":"932819"},{"timestamp":"1687616040.0","comment_id":"932634","upvote_count":"1","content":"A\ncheck for the example in the below documentation\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html","poster":"bhanus"},{"timestamp":"1687606080.0","poster":"PhuocT","upvote_count":"1","content":"A is answer","comment_id":"932492"},{"timestamp":"1687338780.0","content":"Selected Answer: A\nCorrect Answer is A","upvote_count":"1","comment_id":"929259","poster":"psyx21"}],"url":"https://www.examtopics.com/discussions/amazon/view/112794-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"unix_timestamp":1687338780,"answer_description":"","question_id":220,"answer":"A","timestamp":"2023-06-21 11:13:00","question_text":"A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that the web application is slowing down.\n\nThe company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that query strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.\n\nWhich set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?","question_images":[],"topic":"1","choices":{"D":"Update the CloudFront distribution to specify casing-insensitive query string processing.","A":"Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function.","B":"Update the CloudFront distribution to disable caching based on query string parameters.","C":"Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase."},"answer_ET":"A"}],"exam":{"isMCOnly":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","numberOfQuestions":529,"isImplemented":true,"id":33,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02"},"currentPage":44},"__N_SSP":true}