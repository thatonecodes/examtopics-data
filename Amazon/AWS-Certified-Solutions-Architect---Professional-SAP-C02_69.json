{"pageProps":{"questions":[{"id":"vB00GEilZqprPbyk8uFh","answer_images":[],"answer_description":"","unix_timestamp":1700688480,"question_text":"A company is preparing to deploy an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for a workload. The company expects the cluster to support an unpredictable number of stateless pods. Many of the pods will be created during a short time period as the workload automatically scales the number of replicas that the workload uses.\n\nWhich solution will MAXIMIZE node resilience?","url":"https://www.examtopics.com/discussions/amazon/view/126963-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"A":"Use a separate launch template to deploy the EKS control plane into a second cluster that is separate from the workload node groups.","D":"Configure the workload to use topology spread constraints that are based on Availability Zone.","C":"Configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned.","B":"Update the workload node groups. Use a smaller number of node groups and larger instances in the node groups."},"isMC":true,"question_images":[],"answer_ET":"D","timestamp":"2023-11-22 22:28:00","discussion":[{"poster":"thala","content":"Selected Answer: D\nUse Topology Spread Constraints Based on Availability Zone","timestamp":"1716416940.0","comment_id":"1077943","comments":[{"content":"To maximize node resilience for an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is expected to support an unpredictable number of stateless pods, the best solution would be to configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned.","upvote_count":"5","poster":"devalenzuela86","comment_id":"1078831","timestamp":"1716492480.0"}],"upvote_count":"11"},{"timestamp":"1716528120.0","poster":"HunkyBunky","content":"Selected Answer: D\nI guess D, because question requres to MAXIMIZE NODE resilience. Node not workload, so we need to spread nodes across AZs.","upvote_count":"9","comment_id":"1079091"},{"comment_id":"1168999","upvote_count":"1","timestamp":"1725809100.0","poster":"career360guru","content":"Selected Answer: D\nOption D"},{"content":"Selected Answer: D\nOption D","poster":"career360guru","timestamp":"1720719240.0","comment_id":"1120120","upvote_count":"2"},{"timestamp":"1718209200.0","poster":"MegalodonBolado","comment_id":"1094821","upvote_count":"5","content":"\"To achieve high availability, customers deploy Amazon EKS worker nodes (Amazon EC2 instances) across multiple distinct AZs. To complement this approach, we recommend customers to implement Kubernetes primitives, such as pod topology spread constraints to achieve pod-level high availability as well as efficient resource utilization.\"\n\nhttps://aws.amazon.com/blogs/containers/getting-visibility-into-your-amazon-eks-cross-az-pod-to-pod-network-bytes/"},{"comment_id":"1084279","upvote_count":"2","content":"Selected Answer: D\nAnswer D. \nFrom GPT - This approach ensures that the stateless pods are distributed across different Availability Zones, maximizing node resilience. If a failure occurs in one Availability Zone, the impact on the workload is minimized because other pods are spread across different zones. Makes sense for Node Resilience!","timestamp":"1717060560.0","poster":"shaaam80"},{"upvote_count":"4","content":"Selected Answer: D\nD is the answer. Configuring the workload to use topology spread constraints based on Availability Zone â€” is the best solution to maximize node resilience. This approach enhances the stability and availability of the EKS cluster by ensuring that the workload is evenly spread across different Availability Zones, thereby mitigating the risks associated with zone-specific failures or performance issues.\n\nRemember, it's asking about Node Resilience, not Pod Resilience","timestamp":"1716775980.0","poster":"heatblur","comment_id":"1081196"},{"timestamp":"1716424320.0","comment_id":"1078001","content":"Selected Answer: D\nD. Configure a carga de trabalho para usar restriÃ§Ãµes de propagaÃ§Ã£o de topologia baseadas na zona de disponibilidade.","poster":"Jonalb","upvote_count":"2","comments":[{"poster":"devalenzuela86","upvote_count":"1","content":"To maximize node resilience for an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is expected to support an unpredictable number of stateless pods, the best solution would be to configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned.","comment_id":"1078830","timestamp":"1716492420.0"}]},{"content":"Selected Answer: C\nC for sure","timestamp":"1716406080.0","comment_id":"1077841","upvote_count":"3","poster":"devalenzuela86"}],"answer":"D","answers_community":["D (91%)","9%"],"exam_id":33,"question_id":341,"topic":"1"},{"id":"gU4ETwd1wdGEYIcUOQaN","answers_community":["C (100%)"],"exam_id":33,"answer_ET":"C","answer_images":[],"discussion":[{"timestamp":"1701343380.0","content":"Selected Answer: C\nAnswer C. Configure RDS read-replica instead of Snapshots. Invoke Lambda function to promote read-replica to primary and update Route53 to point to secondary region incase of DR","poster":"shaaam80","upvote_count":"5","comment_id":"1084288"},{"content":"Option C provides a suitable DR solution by:\n\n\n\nSetting up a second ECS cluster and ECS service in the separate Region, which allows for containerless deployment of the application.\n\nCreating a cross-Region read replica of the RDS DB instance in the separate Region, ensuring that the data is available in the new Region even if the primary database fails.\n\nCreating an AWS Lambda function to promote the read replica to the primary database, which ensures that both Regions have an up-to-date copy of the data.\n\nConfiguring the Lambda function to update Route 53 to route traffic to the second ECS cluster, ensuring seamless failover.\n\n\nThis solution provides several benefits, including:\n\nRapid application recovery (less than 30 minutes)\nHigh availability and low downtime\nEasy disaster recovery with minimal manual intervention","timestamp":"1731367200.0","poster":"AzureDP900","upvote_count":"2","comment_id":"1310368"},{"upvote_count":"1","content":"C, for sure.\nIt's not straight forward to Convert the RDS MySQL snapshot to an Amazon DynamoDB global table.","comment_id":"1246454","poster":"gfhbox0083","timestamp":"1720755120.0"},{"upvote_count":"1","timestamp":"1709918880.0","poster":"career360guru","comment_id":"1169002","content":"Selected Answer: C\nOption C"},{"upvote_count":"1","comment_id":"1120124","content":"Selected Answer: C\nOption C","timestamp":"1705002060.0","poster":"career360guru"},{"timestamp":"1702519440.0","comment_id":"1095949","content":"C. read replica","poster":"_Juwon","upvote_count":"2"},{"timestamp":"1701169920.0","content":"Selected Answer: C\nAnswer c","poster":"GabrielDeBiasi","upvote_count":"2","comment_id":"1082469"},{"poster":"salazar35","content":"Selected Answer: C\nThe solution must minimize the time that is necessary to recover from a failure","comment_id":"1080904","upvote_count":"2","timestamp":"1701019800.0"},{"poster":"thala","content":"Selected Answer: C\nSecond ECS Cluster and RDS Read Replica with Lambda","upvote_count":"1","timestamp":"1700699580.0","comment_id":"1077944"},{"upvote_count":"1","poster":"devalenzuela86","comment_id":"1077848","timestamp":"1700688720.0","content":"Selected Answer: C\nAnswer c"},{"upvote_count":"1","comment_id":"1077075","timestamp":"1700638140.0","poster":"cypkir","content":"Selected Answer: C\nAnswer: C"}],"timestamp":"2023-11-22 08:29:00","topic":"1","unix_timestamp":1700638140,"question_id":342,"answer_description":"","question_text":"A company needs to implement a disaster recovery (DR) plan for a web application. The application runs in a single AWS Region.\n\nThe application uses microservices that run in containers. The containers are hosted on AWS Fargate in Amazon Elastic Container Service (Amazon ECS). The application has an Amazon RDS for MySQL DB instance as its data layer and uses Amazon Route 53 for DNS resolution. An Amazon CloudWatch alarm invokes an Amazon EventBridge rule if the application experiences a failure.\n\nA solutions architect must design a DR solution to provide application recovery to a separate Region. The solution must minimize the time that is necessary to recover from a failure.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/126859-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"answer":"C","choices":{"D":"Setup a second ECS cluster and ECS service on Fargate in the separate Region. Take a snapshot of the RDS DB instance. Convert the snapshot to an Amazon DynamoDB global table. Create an AWS Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.","B":"Create an AWS Lambda function that creates a second ECS cluster and ECS service in the separate Region. Configure the Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.","C":"Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create a cross-Region read replica of the RDS DB instance in the separate Region. Create an AWS Lambda function to promote the read replica to the primary database. Configure the Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.","A":"Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create an AWS Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."},"isMC":true},{"id":"42gA5rjcnfZYFLTjXpjz","answer_description":"","discussion":[{"timestamp":"1701343740.0","poster":"shaaam80","upvote_count":"14","comment_id":"1084292","content":"Selected Answer: A\nAnswer A. C cannot be correct because Cost Anomaly detection is for a surprise cost exceeds. A is a perfect use case for this scenario."},{"upvote_count":"9","content":"Selected Answer: A\n\"A\" describe perfectly the process to create this kind of control. Besides Cost Anomaly is very focused on \"Cost\", while the question ask to control the \"usage\" (ex:hours), not exactly $ cost. I suggest doing a demo. \"A\" for sure","timestamp":"1701287340.0","poster":"37b2ab7","comment_id":"1083781"},{"poster":"eesa","content":"Selected Answer: B\nOption B: Configure AWS Cost Anomaly Detection in the organization's management account. Configure a monitor type of AWS Service. Apply a filter of Amazon EC2. Configure an alert subscription to notify the architecture team if the usage is 10% more than the average usage for the last 30 days.\nðŸ›  Explanation:\n\nThe company needs a solution that:\n\n Tracks EC2 usage over time.\n\n Calculates the average usage for the past 30 days.\n\n Alerts the architecture team if usage exceeds 10% of the average.\n\nâœ… AWS Cost Anomaly Detection is designed for this exact purpose. It:\n\n Monitors usage and cost metrics at the AWS service level (e.g., EC2).\n\n Uses machine learning to detect anomalies based on historical usage.\n\n Sends alerts when usage exceeds a defined threshold (in this case, 10% more than the last 30-day average).","upvote_count":"1","timestamp":"1742983080.0","comment_id":"1410309"},{"poster":"Deztroyer88","upvote_count":"1","timestamp":"1741567020.0","content":"Selected Answer: B\nAWS Cost Anomaly Detection is designed to automatically track usage trends and detect anomalies in EC2 usage patterns.\nYou can set up a monitoring rule to compare daily EC2 usage against a 30-day average.\nIf usage exceeds 10% over the historical average, it will trigger an alert.\nThis solution is automated, scalable, and requires minimal operational overhead.","comment_id":"1374257"},{"poster":"AloraCloud","upvote_count":"2","timestamp":"1729029600.0","comment_id":"1298452","content":"AWS Budgets can be used to set custom budget based on your expected usage and notify you when a threshold is exceeded. AWS Cost Anomaly Detection uses advanced machine learning (ML) technologies to identify anomalous spend and root causes."},{"timestamp":"1728664500.0","comment_id":"1296182","upvote_count":"3","poster":"JoeTromundo","content":"Selected Answer: A\nFor those who think the correct answer is B: \nhttps://docs.aws.amazon.com/cost-management/latest/userguide/getting-started-ad.html\n\"For Threshold, enter a number to configure the anomalies that you want to generate alerts for. There are two types of thresholds: absolute and percentage. Absolute thresholds trigger alerts when an anomaly's total COST impact exceeds your chosen threshold. Percentage thresholds trigger alerts when an anomaly's total impact percentage exceeds your chosen threshold. Total impact percentage is the percentage difference between the total expected SPEND and total actual SPEND.\"\"\nAWS COST Anomaly Detection primarily focuses on COST anomalies rather than specific usage metrics."},{"upvote_count":"2","content":"Selected Answer: B\nExplanation:\nAWS Cost Anomaly Detection: This service can monitor your AWS usage and costs, identifying anomalies and deviations from normal usage patterns. By setting up a monitor for Amazon EC2 usage, you can detect if the usage is significantly higher than usual, such as exceeding 10% of the average usage over the past 30 days.\n\nMonitor Type: Choosing \"AWS Service\" as the monitor type allows you to focus specifically on EC2 usage.\n\nAlert Subscription: You can configure alerts to notify the architecture team when the detected usage anomaly exceeds the threshold, such as a 10% increase over the historical average.","poster":"Chakanetsa","timestamp":"1723824240.0","comment_id":"1267248"},{"content":"Selected Answer: A\nIt has to be A.\nI have tried to do it in AWS Budget Console. Here's a step by step breakdown of what I have done:\n\nStep 1: Click on the \"Create budget\" button and choose the \"Usage budget\" type\nStep 2: Set the Usage type groups as `EC2 Running hours`, Set budget amount's baseline timerange as `Last 30 days` with a `daily` period\nStep 3: Configure alerts with 110% of budgeted amount","timestamp":"1723593660.0","comments":[{"comment_id":"1399192","upvote_count":"1","content":"I can not reproduce your steps. For usage type group, there are 3 types: fixed/planned/auto-adjusted. both of them are absolute number and don't support percentage setting.","timestamp":"1742119680.0","poster":"GabrielShiao"},{"timestamp":"1723593960.0","upvote_count":"2","poster":"kgpoj","content":"When setting the time range, even if the label says `Last 30 days`, but if you hover on it, it expands and says `last-30 day average`\n\nSo really now AWS Budget can help us collect daily average using a 30-day sliding window. You can use this as baseline, and use 110% of baseline value to trigger the alert","comment_id":"1265442"}],"comment_id":"1265440","upvote_count":"2","poster":"kgpoj"},{"upvote_count":"1","poster":"gfhbox0083","content":"A, for sure.\nService Monitor tracks spend across all deployed services, but not for a specific service (like ec2)","comment_id":"1242488","timestamp":"1720156020.0"},{"comment_id":"1222674","poster":"9f02c8d","content":"It should be D as AWS Cost Anomaly Detection is a service that monitors your cost and usage data to detect anomalies based on machine learning models. It can identify unusual spending patterns and notify you when anomalies are detected based on historical usage patterns","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1717250040.0","content":"I mean B","poster":"9f02c8d","comment_id":"1222675"}],"timestamp":"1717250040.0"},{"timestamp":"1711476000.0","content":"Selected Answer: A\nOption A - Maybe I am the problem here, I don't why people are selecting option \"B\", when the the first line in AWS Cost Management documentation Under AWS Budget states - \"You can use AWS Budgets to track and take action on your AWS costs and usage. You can use AWS Budgets to monitor your aggregate utilization and coverage metrics for your Reserved Instances (RIs) or Savings Plans.\" \nhttps://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html\nAWS Blog - https://aws.amazon.com/blogs/mt/manage-cost-overruns-part-1/","comment_id":"1183500","poster":"TonytheTiger","upvote_count":"5"},{"poster":"Dgix","upvote_count":"1","timestamp":"1711016640.0","content":"Selected Answer: B\nIt's B. Cost Anomaly detection _can_ do this kind of thing. AWS Budgets is for overall costs and is a less sharp tool here.","comment_id":"1179111"},{"poster":"career360guru","comment_id":"1120140","content":"Selected Answer: A\nOption A - Cost Anomaly detection does not allow to filter based on EC2 type only.","timestamp":"1705003500.0","upvote_count":"5"},{"content":"A:\n\nCost dedection is for cost, not for EC2 metrix","upvote_count":"1","comment_id":"1117678","timestamp":"1704817620.0","poster":"adelynllllllllll"},{"upvote_count":"2","content":"This questions is weird. The best soltuion should be AWS CloudWatch. No such answer to choose !","comment_id":"1097013","poster":"GaryQian","timestamp":"1702608960.0"},{"comments":[{"content":"steps to set up an AWS Budget to track EC2 usage and receive an alert if it's more than 10% higher than the average usage from the last 30 days:\n\nGo to the AWS Management Console |Open the \"Budgets\" service |Create a New Budget:|Choose \"Cost budget\" as the budget type.|Choose the time period for the budget (e.g., Monthly).|Set the start and end dates for the budget.\nConfigure Cost and Usage Details:|Choose the \"Cost and usage\" option.|Specify the \"Service\" as \"Amazon EC2\" to focus on EC2 costs.|Choose the \"Usage type\" as \"Usage Quantity.\"|Set Budgeted Amount:|Set the budgeted amount to be 110% of the average EC2 usage from the last 30 days. \nConfigure Alerts:|Enable the alert threshold.|Set the alert threshold to be \"Actual > Forecasted\" and \"More than 0%\" to be alerted when the actual usage exceeds the forecast.","upvote_count":"2","poster":"vibzr2023","comments":[{"comment_id":"1265437","comments":[{"comment_id":"1265438","upvote_count":"1","poster":"kgpoj","timestamp":"1723592940.0","content":"In AWS Budget Console:\nUsage budget\nMonitor your usage of one or more specified usage types or usage type groups and receive alerts when your user-defined thresholds are met. Using usage budgets, the budgeted amount represents your expected usage. For example, you can use a usage budget to monitor the usage of certain services such as Amazon EC2 and Amazon S3."}],"content":">|Choose \"Cost budget\" as the budget type.\n\nIt should be \"Usage budget\"","timestamp":"1723592940.0","upvote_count":"1","poster":"kgpoj"}],"timestamp":"1704150060.0","comment_id":"1111487"}],"content":"I recommend doing a demo.\nFor sure it is A. It describe perfectly the process.","comment_id":"1083775","timestamp":"1701286740.0","poster":"37b2ab7","upvote_count":"3"},{"comments":[{"comment_id":"1099988","poster":"0c118eb","upvote_count":"1","content":"You're right on most, but on this one, it is A.","timestamp":"1702927860.0"}],"timestamp":"1701058980.0","poster":"heatblur","upvote_count":"2","comment_id":"1081201","content":"Selected Answer: B\nB is the answer, AWS Cost Anomaly Detection is specifically designed to monitor AWS service usage, identify anomalies based on historical patterns, and can be configured to send alerts when the usage exceeds a certain threshold compared to the average of the last 30 days. This aligns well with the requirement to receive daily alerts if EC2 usage is more than 10% higher than the average usage from the past 30 days."},{"poster":"George88","comment_id":"1080500","upvote_count":"3","timestamp":"1700977800.0","content":"Answer: A\nhttps://aws.amazon.com/blogs/aws-cloud-financial-management/launch-daily-cost-and-usage-budgets/"},{"poster":"Jonalb","comment_id":"1077991","upvote_count":"3","content":"Selected Answer: B\nB. Configure o AWS Cost Anomaly Detection na conta de gerenciamento da organizaÃ§Ã£o. Configure um tipo de monitor de serviÃ§o AWS. Aplique um filtro do Amazon EC2. Configure uma assinatura de alerta para notificar a equipe de arquitetura se o uso for 10% maior que o uso mÃ©dio dos Ãºltimos 30 dias.","comments":[{"timestamp":"1700774160.0","content":"Option B is incorrect because AWS Cost Anomaly Detection is not designed to track EC2 usage as a metric. It is used to detect anomalies in your AWS costs and usage patterns.","poster":"devalenzuela86","upvote_count":"4","comment_id":"1078820"}],"timestamp":"1700706000.0"},{"comment_id":"1077946","timestamp":"1700699760.0","poster":"thala","upvote_count":"3","content":"Selected Answer: B\nAWS Cost Anomaly Detection for EC2","comments":[]},{"upvote_count":"3","comment_id":"1077858","content":"Selected Answer: A\nAnswer A","poster":"devalenzuela86","timestamp":"1700688900.0"},{"content":"Selected Answer: B\nAnswer: B","upvote_count":"2","timestamp":"1700638200.0","poster":"cypkir","comment_id":"1077076"}],"unix_timestamp":1700638200,"question_text":"A company has AWS accounts that are in an organization in AWS Organizations. The company wants to track Amazon EC2 usage as a metric. The companyâ€™s architecture team must receive a daily alert if the EC2 usage is more than 10% higher the average EC2 usage from the last 30 days.\n\nWhich solution will meet these requirements?","isMC":true,"answers_community":["A (73%)","B (27%)"],"exam_id":33,"answer_images":[],"question_id":343,"timestamp":"2023-11-22 08:30:00","answer_ET":"A","choices":{"C":"Enable AWS Trusted Advisor in the organization's management account. Configure a cost optimization advisory alert to notify the architecture team if the EC2 usage is 10% more than the reported average usage for the last 30 days.","B":"Configure AWS Cost Anomaly Detection in the organization's management account. Configure a monitor type of AWS Service. Apply a filter of Amazon EC2. Configure an alert subscription to notify the architecture team if the usage is 10% more than the average usage for the last 30 days.","D":"Configure Amazon Detective in the organization's management account. Configure an EC2 usage anomaly alert to notify the architecture team if Detective identifies a usage anomaly of more than 10%.","A":"Configure AWS Budgets in the organization's management account. Specify a usage type of EC2 running hours. Specify a daily period. Set the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer. Configure an alert to notify the architecture team if the usage threshold is met"},"answer":"A","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/126860-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1"},{"id":"93Sj10HroU56lEDk7tZS","answers_community":["B (83%)","C (17%)"],"choices":{"A":"Receive the orders in an Amazon EC2-hosted database and use EC2 instances to process them.","D":"Receive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them.","C":"Receive the orders using the AWS Step Functions program and launch an Amazon ECS container to process them.","B":"Receive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them."},"url":"https://www.examtopics.com/discussions/amazon/view/126964-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1700688960,"topic":"1","question_id":344,"timestamp":"2023-11-22 22:36:00","answer_description":"","discussion":[{"content":"Selected Answer: B\nLoosely coupled = SQS - Lambda is also the simplest to use","comment_id":"1083187","upvote_count":"6","poster":"tfl","timestamp":"1701241500.0","comments":[{"content":"Yip that key words \"loosely coupled\" has been repeated several times during the trainings","timestamp":"1728928200.0","upvote_count":"1","poster":"AWSum1","comment_id":"1297766"}]},{"comment_id":"1120144","upvote_count":"5","timestamp":"1705003860.0","poster":"career360guru","content":"Selected Answer: B\noption B"},{"upvote_count":"2","content":"Using an Amazon SQS queue (option B) and an AWS Lambda function is the most reliable approach for several reasons:\nScalability: AWS Lambda can automatically scale based on incoming requests, ensuring that orders are processed quickly even during peak traffic.\nHigh availability: By using a message queue like SQS, orders are stored temporarily until they can be processed, ensuring that no orders are lost due to system failures.\nLoosely coupled architecture: The use of an SQS queue and Lambda function decouples the order processing from the storage (Amazon DynamoDB) in a separate logical layer, making it easier to scale and maintain individual components independently.\nHandling sporadic traffic patterns: AWS Lambda provides a serverless computing experience that automatically scales based on demand, making it well-suited for handling variable workload spikes.","comment_id":"1310367","poster":"AzureDP900","timestamp":"1731366840.0"},{"poster":"career360guru","timestamp":"1709920980.0","upvote_count":"3","content":"Selected Answer: B\nOption B","comment_id":"1169013"},{"comments":[{"content":"what about loosely coupled? SQS required for it.","comment_id":"1175580","upvote_count":"1","timestamp":"1710649440.0","poster":"yog927"}],"upvote_count":"2","timestamp":"1707583140.0","comment_id":"1146436","poster":"ele","content":"Selected Answer: C\nAnswer is C\nOrder processing is a multi-step cycle not a two step one. Stepfunction and ECS is the most reliable way to go."},{"content":"Selected Answer: B\nOption B","poster":"ma23","comment_id":"1123829","timestamp":"1705375500.0","upvote_count":"3"},{"poster":"vibzr2023","upvote_count":"2","content":"Selected Answer: B -- SQS for sure coz you can't take a chance of loosing data.","timestamp":"1704148680.0","comment_id":"1111473"},{"upvote_count":"3","poster":"MegalodonBolado","comment_id":"1094880","timestamp":"1702410000.0","content":"\". The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table.\" We can't assume, without further information, that it's a multistep action. For now, it just processes one order and send info to Dynamo.\n\nLooks reasonable to use SQS+Lambda for a loosely coupled solution\nB"},{"timestamp":"1702288500.0","comment_id":"1093340","content":"Selected Answer: C\nHere is an example of a Step Function for a simple order flow. You can see how many lambda functions will be necessary that can't be replaced by a single SQS and Lambda\n\nhttps://dev.to/aws-builders/aws-step-functions-simple-order-flow-6gn","upvote_count":"2","poster":"ayadmawla"},{"poster":"ayadmawla","timestamp":"1702288200.0","content":"Selected Answer: C\nAnswer is C\nOrder processing is a multi-step cycle not a two step one.","comment_id":"1093338","upvote_count":"2"},{"timestamp":"1701343860.0","upvote_count":"2","poster":"shaaam80","comment_id":"1084294","content":"Selected Answer: B\nAnswer B"},{"poster":"salazar35","timestamp":"1701197460.0","content":"Selected Answer: B\nB is correct","upvote_count":"4","comment_id":"1082856"},{"content":"Selected Answer: B\nB for sure","timestamp":"1701170040.0","comment_id":"1082473","poster":"GabrielDeBiasi","upvote_count":"4"},{"timestamp":"1700688960.0","upvote_count":"3","comment_id":"1077870","poster":"devalenzuela86","content":"Selected Answer: B\nB for sure"}],"answer_images":[],"question_text":"An e-commerce company is revamping its IT infrastructure and is planning to use AWS services. The companyâ€™s CIO has asked a solutions architect to design a simple, highly available, and loosely coupled order processing application. The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table. The application has a sporadic traffic pattern and should be able to scale during marketing campaigns to process the orders with minimal delays.\n\nWhich of the following is the MOST reliable approach to meet the requirements?","answer_ET":"B","exam_id":33,"question_images":[],"answer":"B","isMC":true},{"id":"cbUOksG9ZxruDRDAKkkD","exam_id":33,"answers_community":["B (100%)"],"answer_ET":"B","answer_images":[],"discussion":[{"comment_id":"1084297","poster":"shaaam80","upvote_count":"11","timestamp":"1701344100.0","content":"Selected Answer: B\nAnswer B. Always remember - Automatic Password Rotation - AWS Secrets Manager!"},{"timestamp":"1731366660.0","comment_id":"1310366","poster":"AzureDP900","content":"B is perfect","upvote_count":"1"},{"comment_id":"1169015","poster":"career360guru","upvote_count":"1","content":"Selected Answer: B\nOption B","timestamp":"1709921160.0"},{"timestamp":"1705422900.0","poster":"SwapnilAWS","upvote_count":"1","content":"Option : B is the correct answer\n\nWhile AWS Systems Manager Parameter Store is a valid service for storing configuration data, including secrets, using AWS KMS for encryption and Boto3 for retrieval, it lacks the built-in support for automatic rotation of secrets\n\nAWS KMS is primarily designed for managing cryptographic keys and does not provide built-in support for storing and rotating secrets like database credentials.\n\nWhile AWS KMS key rotation is available, it is intended for cryptographic key rotation rather than the rotation of sensitive data like passwords.","comment_id":"1124357"},{"upvote_count":"1","content":"Selected Answer: B\nThe correct solution should be: \nStore the database credentials for both environments in AWS Secrets Manager with distinct key entry for the QA environment and the production environment. Enable a Lambda function to rotate the secrets regularly. Create a KMS key for each secret and use them to encrypt the credentials. Assign permissions to allow the business Lambda function to retrieve the credential from Secret manager and decrypt the credential with the KMS key.\n\nB is not ideal but is the only acceptable answer: \nâ€œTurn on rotation.â€: In Secret Manager, you must enable a Lambda function to rotate the credential\nâ€œProvide a reference to the Secrets Manager key as an environment variable for the Lambda functions. â€œ permission must be set to allow the Lambda function to use the Key to decrypt the credential.","poster":"bjexamprep","comment_id":"1120982","timestamp":"1705082400.0"},{"content":"Selected Answer: B\nOption B","comment_id":"1120150","poster":"career360guru","upvote_count":"1","timestamp":"1705004040.0"},{"comment_id":"1082475","content":"Selected Answer: B\n\"rotate passwords automatically\" -> AWS Secrets Manager","upvote_count":"3","poster":"GabrielDeBiasi","timestamp":"1701170100.0"},{"content":"Selected Answer: B\nAWS Secrets Manager with Rotation Enabled:","poster":"thala","upvote_count":"2","comment_id":"1077948","timestamp":"1700700000.0"},{"upvote_count":"1","poster":"devalenzuela86","comment_id":"1077877","timestamp":"1700689140.0","content":"Selected Answer: B\nB for sure"},{"comment_id":"1077772","poster":"321swa","upvote_count":"1","content":"Correct Answer is B","timestamp":"1700683980.0"},{"comment_id":"1077079","content":"Selected Answer: B\nAnswer: B","upvote_count":"1","timestamp":"1700638320.0","poster":"cypkir"}],"timestamp":"2023-11-22 08:32:00","topic":"1","question_id":345,"unix_timestamp":1700638320,"answer_description":"","question_text":"A company is deploying AWS Lambda functions that access an Amazon RDS for PostgreSQL database. The company needs to launch the Lambda functions in a QA environment and in a production environment.\n\nThe company must not expose credentials within application code and must rotate passwords automatically.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/126861-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"answer":"B","choices":{"C":"Store the database credentials for both environments in AWS Key Management Service (AWS KMS). Turn on rotation. Provide a reference to the credentials that are stored in AWS KMS as an environment variable for the Lambda functions.","A":"Store the database credentials for both environments in AWS Systems Manager Parameter Store. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Within the application code of the Lambda functions, pull the credentials from the Parameter Store parameter by using the AWS SDK for Python (Boto3). Add a role to the Lambda functions to provide access to the Parameter Store parameter.","D":"Create separate S3 buckets for the QA environment and the production environment. Turn on server-side encryption with AWS KMS keys (SSE-KMS) for the S3 buckets. Use an object naming pattern that gives each Lambda functionâ€™s application code the ability to pull the correct credentials for the function's corresponding environment. Grant each Lambda function's execution role access to Amazon S3.","B":"Store the database credentials for both environments in AWS Secrets Manager with distinct key entry for the QA environment and the production environment. Turn on rotation. Provide a reference to the Secrets Manager key as an environment variable for the Lambda functions."},"isMC":true}],"exam":{"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional SAP-C02","provider":"Amazon","id":33,"isBeta":false,"isMCOnly":true,"isImplemented":true,"numberOfQuestions":529},"currentPage":69},"__N_SSP":true}