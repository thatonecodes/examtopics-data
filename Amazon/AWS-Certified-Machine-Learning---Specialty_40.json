{"pageProps":{"questions":[{"id":"2yFsCCLoMuoabh2wXjrC","timestamp":"2023-12-14 15:30:00","answer_images":[],"choices":{"B":"Use a principal component analysis (PCA) model.","A":"Create new features and interaction variables.","C":"Apply normalization on the feature set.","D":"Use a multiple correspondence analysis (MCA) model."},"answer_ET":"B","question_text":"A company is building a new supervised classification model in an AWS environment. The company's data science team notices that the dataset has a large quantity of variables. All the variables are numeric.\n\nThe model accuracy for training and validation is low. The model's processing time is affected by high latency. The data science team needs to increase the accuracy of the model and decrease the processing time.\n\nWhat should the data science team do to meet these requirements?","answers_community":["B (100%)"],"answer":"B","question_images":[],"exam_id":26,"unix_timestamp":1702564200,"url":"https://www.examtopics.com/discussions/amazon/view/128591-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer_description":"","discussion":[{"upvote_count":"1","timestamp":"1719165360.0","content":"Selected Answer: B\nIt's PCA. It's not MCA because all the values are numeric and not categorical.","comment_id":"1104245","poster":"taustin2"},{"comment_id":"1097784","upvote_count":"1","poster":"xiaoeason","content":"Selected Answer: B\nneed to reduce the dimension of features in order to enhance accuracy on train and test data since # of features are huge.","timestamp":"1718491860.0"},{"timestamp":"1718368200.0","poster":"usamazubairi","content":"B. Use a principal component analysis (PCA) model. This is because PCA can help to reduce the number of variables while preserving the most important information, which can help to improve the accuracy of the model and reduce the processing time.","comment_id":"1096553","upvote_count":"3"}],"isMC":true,"question_id":196},{"id":"yfXIyWKRiNCTz4GMgJpT","discussion":[{"timestamp":"1735292640.0","comment_id":"1332333","poster":"data_sma","upvote_count":"1","content":"Selected Answer: A\nL1 so the weight for the noisy features can go to zero"},{"timestamp":"1734440820.0","poster":"LeoD","content":"Selected Answer: D\nAlthough this document (https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html#regularization) relate noise with L1 in its content, if you were to use L1 regularization, the algorithm might push too many weights to zero. This could inadvertently remove useful information, especially if the noisy features still contain some signal. So in this case it should be L2 instead of L1.","upvote_count":"2","comment_id":"1327893"},{"content":"Selected Answer: D\nL2 regularization is generally more effective for reducing overfitting in the presence of noisy data because it reduces the impact of all features proportionally, rather than eliminating some features entirely as L1 regularization does.","poster":"MultiCloudIronMan","timestamp":"1727202540.0","upvote_count":"2","comment_id":"1288736"},{"content":"Selected Answer: A\nBoth L1 & L2 help with overfitting. \nHowever, L1 regularization does feature selection by reducing weights of rerelenat featurtes to zero - reducing dimensionality and removing noisy features as in this case. L2 on the other hand keeps all features including noisy ones.","comment_id":"1167566","timestamp":"1709771220.0","poster":"AIWave","upvote_count":"2"},{"comment_id":"1125880","timestamp":"1705584180.0","content":"Selected Answer: A\nIf you suspect that some features are irrelevant, Option A (L1 Regularization) could be more effective as it can shrink some coefficients to zero, effectively performing feature selection be removing the noise.\n If you believe that most features are relevant but the model is too complex, Option D (L2 Regularization) is typically the better choice as it evenly shrinks all coefficients, thus reducing model complexity without eliminating features.\n\nIn this case, option A would be ideal to get rid of the irrelevant noise.","poster":"CloudHandsOn","upvote_count":"2"},{"poster":"xiaoeason","comment_id":"1097791","upvote_count":"4","content":"Selected Answer: D\nIt should be D as this is a overfitting problem.\nA might make the model oversimple in that case train acc will be bad. L2 is better than L1","timestamp":"1702688160.0"},{"content":"Selected Answer: A\nA. L1 Regularization reduces the amount of noise in the model, https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html","poster":"Ryan10000","timestamp":"1702626540.0","upvote_count":"4","comment_id":"1097097"},{"poster":"Aaabbk","content":"D\nL2 regularisation for overfitting and noise","timestamp":"1702588680.0","comment_id":"1096839","upvote_count":"3"}],"unix_timestamp":1702588680,"choices":{"D":"Add L2 regularization to the linear learner regression model.","C":"Perform feature engineering by including quadratic and cubic terms. Train the linear learner regression model.","B":"Perform a principal component analysis (PCA) on the dataset. Use the linear learner regression model.","A":"Add L1 regularization to the linear learner regression model."},"question_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/128608-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"exam_id":26,"question_text":"An exercise analytics company wants to predict running speeds for its customers by using a dataset that contains multiple health-related features for each customer. Some of the features originate from sensors that provide extremely noisy values.\n\nThe company is training a regression model by using the built-in Amazon SageMaker linear learner algorithm to predict the running speeds. While the company is training the model, a data scientist observes that the training loss decreases to almost zero, but validation loss increases.\n\nWhich technique should the data scientist use to optimally fit the model?","topic":"1","answer_images":[],"answers_community":["A (53%)","D (47%)"],"timestamp":"2023-12-14 22:18:00","question_id":197,"answer_ET":"A","answer_description":""},{"id":"evhnAuhFs0INkXbPHw2m","answer_images":[],"question_id":198,"discussion":[{"content":"Selected Answer: A\nThis is simple, assume we identify 100 customers who have spent atleast $1M dollars before. We want our marketing materil to reach atleast 90 of them - hence as many true positives as possible meaning FPs are also ok. This means we need to optimise for recall","poster":"endeesa","upvote_count":"2","timestamp":"1732826160.0","comment_id":"1082933"},{"comment_id":"1053678","content":"Selected Answer: A\nFor me it is A. We need to send the promo to as many potential buyers as possible. so we need to reduce FN","timestamp":"1729856460.0","poster":"DimLam","upvote_count":"3"},{"content":"Selected Answer: A\nFPs should be fine as it is not mentioned in the question that it is expensive to send the expensive marketing packages to some extra customers. \nBUT FNs more than 10% are not fine as they want at least 90% of potential customers to get the material. Hence we tune hyperparameters to increase recall to target 90%. \nAnd then find best precision with this 90% recall. Which makes precision_at_target_recall an appropriate hyperparameter to set. But option A has recall_at_target_precision. Still leaning towards A as target recall needs to be 90%.\nAs per docs, \"If binary_classifier_model_selection_criteria is recall_at_target_precision, then precision is held at this value while recall is maximized.\"","comment_id":"997341","timestamp":"1725341760.0","upvote_count":"1","poster":"goku58"},{"content":"we want to reduce FPs. Hence precision target should be 90%. Thus B.","comment_id":"992707","timestamp":"1724895300.0","poster":"ashii007","upvote_count":"1"},{"comment_id":"990678","content":"Selected Answer: B\nB\nWe want to reduce the false positive(We do not want to give the expensive marketing package to the costomer who won't buy our product)","upvote_count":"3","poster":"boledadian","timestamp":"1724671620.0","comments":[]},{"comment_id":"986854","content":"Selected Answer: A\n(A) We want to reduce FN as much as possible (customers who will buy, but who were predicted by the model as “non-buyers”), so the correct alternative is Letter A.","poster":"kaike_reis","timestamp":"1724266200.0","upvote_count":"1","comments":[{"poster":"kaike_reis","content":"c - d are decoy.","upvote_count":"1","comment_id":"986855","timestamp":"1724266200.0"}]},{"poster":"Mickey321","comment_id":"971116","timestamp":"1722689400.0","upvote_count":"2","content":"Selected Answer: A\nFor classification problems, such as the one here, you can specify the binary_classifier_model_selection_criteria hyperparameter to control how the best model is selected from the validation set. You can choose from several criteria, such as accuracy, precision, recall, F1 measure, or cross-entropy loss1."},{"timestamp":"1720413720.0","content":"I think it's A - Recall.","upvote_count":"2","poster":"ADVIT","comment_id":"946170"}],"question_images":[],"answer_ET":"A","exam_id":26,"unix_timestamp":1688791320,"answer_description":"","question_text":"A company is planning a marketing campaign to promote a new product to existing customers. The company has data for past promotions that are similar. The company decides to try an experiment to send a more expensive marketing package to a smaller number of customers. The company wants to target the marketing campaign to customers who are most likely to buy the new product. The experiment requires that at least 90% of the customers who are likely to purchase the new product receive the marketing materials.\n\nThe company trains a model by using the linear learner algorithm in Amazon SageMaker. The model has a recall score of 80% and a precision of 75%.\n\nHow should the company retrain the model to meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/114485-exam-aws-certified-machine-learning-specialty-topic-1/","answers_community":["A (75%)","B (25%)"],"answer":"A","isMC":true,"timestamp":"2023-07-08 06:42:00","choices":{"A":"Set the target_recall hyperparameter to 90%. Set the binary_classifier_model_selection_criteria hyperparameter to recall_at_target_precision.","B":"Set the target_precision hyperparameter to 90%. Set the binary_classifier_model_selection_criteria hyperparameter to precision_at_target_recall.","C":"Use 90% of the historical data for training. Set the number of epochs to 20.","D":"Set the normalize_label hyperparameter to true. Set the number of classes to 2."},"topic":"1"},{"id":"Ipyk8NvVCMuIkE8aGFJt","exam_id":26,"answer":"AC","timestamp":"2023-12-17 12:26:00","unix_timestamp":1702812360,"question_text":"A company's machine learning (ML) specialist is building a computer vision model to classify 10 different traffic signs. The company has stored 100 images of each class in Amazon S3, and the company has another 10,000 unlabeled images. All the images come from dash cameras and are a size of 224 pixels × 224 pixels. After several training runs, the model is overfitting on the training data.\n\nWhich actions should the ML specialist take to address this problem? (Choose two.)","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/128794-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"B":"Use image preprocessing to transform the images into grayscale images.","D":"Replace the activation of the last layer with a sigmoid.","A":"Use Amazon SageMaker Ground Truth to label the unlabeled images.","C":"Use data augmentation to rotate and translate the labeled images.","E":"Use the Amazon SageMaker k-nearest neighbors (k-NN) algorithm to label the unlabeled images."},"answer_images":[],"topic":"1","answer_description":"","answer_ET":"AC","isMC":true,"discussion":[{"timestamp":"1718751180.0","comment_id":"1100146","upvote_count":"4","poster":"xiaoeason","content":"Selected Answer: AC\nC. Use data augmentation to rotate and translate the labeled images.\n\nData augmentation involves creating new training data by applying transformations such as rotation, translation, scaling, etc. This helps to increase the diversity of the training data and makes the model more robust without requiring additional labeled data.\nA. Use Amazon SageMaker Ground Truth to label the unlabeled images.\n\nLeveraging Amazon SageMaker Ground Truth can help in labeling the unlabeled images to expand the training dataset and reduce overfitting. Adding more labeled data can improve the generalization of the model and reduce overfitting."},{"content":"Selected Answer: AC\nA. Use Amazon SageMaker Ground Truth to label the unlabeled images \nC. Helps address the over-fitting problem","timestamp":"1718620800.0","poster":"fa0d8b7","upvote_count":"2","comment_id":"1098893"},{"poster":"aquanaveen","content":"Ground Truth to label the unlabeled images and data augmentation to create multiple variations of the labeled images","upvote_count":"1","comment_id":"1098845","timestamp":"1718616420.0"},{"poster":"aquanaveen","upvote_count":"2","comment_id":"1098844","timestamp":"1718616360.0","content":"Selected Answer: AC\nGround Truth to label the unlabelled images"}],"answers_community":["AC (100%)"],"question_id":199},{"id":"gLcj8gCRtHPZ70NEeWZm","question_text":"A data science team is working with a tabular dataset that the team stores in Amazon S3. The team wants to experiment with different feature transformations such as categorical feature encoding. Then the team wants to visualize the resulting distribution of the dataset. After the team finds an appropriate set of feature transformations, the team wants to automate the workflow for feature transformations.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","timestamp":"2023-12-19 02:02:00","topic":"1","unix_timestamp":1702947720,"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/128943-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"A","answer_images":[],"choices":{"C":"Use AWS Glue Studio with custom code to experiment with different feature transformations. Save the transformations to Amazon S3. Use Amazon QuickSight for visualization. Package the feature processing steps into an AWS Lambda function for automation.","D":"Use Amazon SageMaker Data Wrangler preconfigured transformations to experiment with different feature transformations. Save the transformations to Amazon S3. Use Amazon QuickSight for visualization. Package each feature transformation step into a separate AWS Lambda function. Use AWS Step Functions for workflow automation.","A":"Use Amazon SageMaker Data Wrangler preconfigured transformations to explore feature transformations. Use SageMaker Data Wrangler templates for visualization. Export the feature processing workflow to a SageMaker pipeline for automation.","B":"Use an Amazon SageMaker notebook instance to experiment with different feature transformations. Save the transformations to Amazon S3. Use Amazon QuickSight for visualization. Package the feature processing steps into an AWS Lambda function for automation."},"question_images":[],"question_id":200,"answer_description":"","isMC":true,"discussion":[{"timestamp":"1718751720.0","upvote_count":"5","comment_id":"1100155","poster":"xiaoeason","content":"Selected Answer: A\nThis solution offers the following advantages:\n\nAmazon SageMaker Data Wrangler provides a user-friendly interface to explore and experiment with feature transformations, making it efficient for the data science team to try out different options.\n\nSageMaker Data Wrangler templates for visualization can quickly generate visualizations for the resulting distribution of the dataset, streamlining the visualization process.\n\nExporting the feature processing workflow to a SageMaker pipeline for automation automates the feature transformations efficiently within the SageMaker environment."},{"poster":"AIWave","timestamp":"1725663300.0","upvote_count":"1","comment_id":"1167577","content":"Selected Answer: A\nAmazon SageMaker Data Wrangler: provides preconfigured transformations that allow for easy exploration of feature transformations. This simplifies the experimentation process.\n\nSageMaker Data Wrangler templates for visualization: allows for visualizing the resulting distribution of the dataset, aiding in understanding the effects of feature transformations.\n\nExport the feature processing workflow to a SageMaker pipeline for automation: Once an appropriate set of feature transformations is identified, the workflow can be exported to a SageMaker pipeline for automation. This ensures reproducibility and scalability of the feature processing steps."},{"poster":"vkbajoria","upvote_count":"1","content":"Selected Answer: A\nData Wrangler is an amazing tool that take EDA to next level","timestamp":"1725404640.0","comment_id":"1165201"}],"answers_community":["A (100%)"],"exam_id":26}],"exam":{"provider":"Amazon","id":26,"isImplemented":true,"name":"AWS Certified Machine Learning - Specialty","isBeta":false,"numberOfQuestions":369,"isMCOnly":false,"lastUpdated":"11 Apr 2025"},"currentPage":40},"__N_SSP":true}