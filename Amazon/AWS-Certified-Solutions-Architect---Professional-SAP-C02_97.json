{"pageProps":{"questions":[{"id":"93LD0kE2eQiGZCYJWf4r","answer_images":[],"question_id":481,"url":"https://www.examtopics.com/discussions/amazon/view/95217-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company’s factory and automation applications are running in a single VPC. More than 20 applications run on a combination of Amazon EC2, Amazon Elastic Container Service (Amazon ECS), and Amazon RDS.\n\nThe company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for the cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM access for daily activities.\n\nThe company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must recommend an AWS Billing and Cost Management solution that provides these cost reports.\n\nWhich combination of actions will meet these requirements? (Choose three.)","discussion":[{"upvote_count":"42","timestamp":"1673697840.0","content":"Selected Answer: ACF\nA, C and F are the correct answers because they provide the required cost reports and analysis for the company's applications and teams.\n\nA. Activating user-defined cost allocation tags that represent the application and the team allows the company to assign costs to specific applications and teams. This allows the company to see how much each application and team is costing them, which is important for cost forecasting and budgeting.\n\nC. Creating a cost category for each application in Billing and Cost Management allows the company to group costs by application. This makes it easier to understand the costs associated with each application and to compare the costs of different applications over time.\n\nF. Enabling Cost Explorer allows the company to analyze costs and usage over time, and to create custom reports and forecasts. This is important for understanding the costs associated with each application and team, and for forecasting future costs.","comment_id":"775335","comments":[{"poster":"masetromain","timestamp":"1673697840.0","comments":[{"comment_id":"890978","poster":"a_c_","comments":[{"comments":[{"content":"So you are wrong, tags can be applied to applications so you can easily find them but unless they are actually activated as user defined billing tags then you will not be able to use those tags in cost analysis. Also you have to enable cost explorer it is not enabled by default and cost explorer lets you see the previous 12 months and creates projections for the next 12, so without that option you will not meet the objective.","poster":"e4bc18e","upvote_count":"3","timestamp":"1712411400.0","comment_id":"1190422"}],"timestamp":"1709063460.0","comment_id":"1160939","content":"In addition to the IAM access problem answer ACF will face, the problem statement already presents us with the information that resources are already tagged by team/application. Creating cost category seems redundant and even if you did create this redundancy, you are faced with the IAM access problem.\n\nIf each team is responsible for the cost and the performance, they would need access to the billing console for their team.","poster":"djeong95","upvote_count":"2"}],"timestamp":"1683402120.0","upvote_count":"10","content":"With out granting IAm Access, IAM users cannot access Billing console, so s cannot see the Cost explorer \nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html.\nQuestion says teams are responsible for cost\nI"}],"upvote_count":"7","comment_id":"775336","content":"B is not correct because AWS generated cost allocation tags are automatically created for some AWS resources, but it does not provide the required cost reports and analysis for the company's applications and teams.\n\nOption D is not correct because IAM access controls are used to limit access to the billing and cost management features, but it is not necessary to configure it to meet the requirements.\n\nE is not correct because Creating a cost budget allows the company to set a budget for their costs and to receive alerts when costs exceed the budget, but it does not provide the required cost reports and analysis for the company's applications and teams."}],"poster":"masetromain"},{"upvote_count":"18","content":"Selected Answer: ADF\nCorrect ADF - SInce resources are tagged, C may not require ?","comment_id":"811145","poster":"spd","timestamp":"1676585220.0"},{"comment_id":"1320741","timestamp":"1733101260.0","content":"Selected Answer: ACF\nACF\nUser defined tags to separate out billing. \nGrouping the costs. \nCost Explorer to analyze the costs.","upvote_count":"1","poster":"bhanus"},{"upvote_count":"3","content":"A. Activate the user-define cost allocation tags that represent the application and the team.\nC. Create a cost category for each application in Billing and Cost Management.\nE. Create a cost budget","timestamp":"1725094980.0","poster":"amministrazione","comment_id":"1275506"},{"upvote_count":"3","timestamp":"1721294580.0","comment_id":"1250272","poster":"neta1o","content":"Selected Answer: ACF\nThe company needs to determine which costs on the monthly AWS bill are attributable to each application or team. - Tagging and Cost Categories\n\n The company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. -Cost Explorer"},{"content":"Selected Answer: ADF\nA. User defined cost allocation tags: application, team \nD. Activate IAM access to Billing and Cost Management: \n\"The teams use IAM access for daily activities.\"\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/control-access-billing.html\nF. Enable Cost Explorer: https://docs.aws.amazon.com/cost-management/latest/userguide/ce-enable.html\n\n\nC is NOT needed, because A already will give a usage view by \"tags that represent their application and team\" \"The company needs to determine which costs on the monthly AWS bill are attributable to each application or team\"","upvote_count":"3","poster":"alex_heavy","timestamp":"1719979860.0","comment_id":"1241140"},{"timestamp":"1712068560.0","content":"Selected Answer: ACF\nOption ACF and NOT ADF - Cost allocation helps you identify who is spending what, within your organization. Cost categories is a cost allocation service to help you map your AWS costs, to your unique internal business structures.\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html","comment_id":"1188113","upvote_count":"3","poster":"TonytheTiger"},{"poster":"mav3r1ck","comments":[{"poster":"mav3r1ck","timestamp":"1711200960.0","comments":[{"timestamp":"1711201020.0","poster":"mav3r1ck","content":"E. Create a cost budget: Creating a cost budget is valuable for managing expenses and avoiding overspending, but it does not directly facilitate the attribution of costs to applications or teams, nor does it aid in the creation of historical comparison reports or forecasts in the manner required by the company.","comment_id":"1180895","upvote_count":"2"},{"poster":"mav3r1ck","content":"[correction for typo error above] Explanation of Exclusions: B, D, E","timestamp":"1711201080.0","comment_id":"1180900","upvote_count":"1"}],"comment_id":"1180894","upvote_count":"1","content":"Explanation of Exclusions: B, D, F"},{"upvote_count":"1","content":"here's the detailed recommendation:","poster":"mav3r1ck","comments":[{"content":"A. Activate user-defined cost allocation tags: User-defined tags need to be activated for cost allocation purposes. These tags, representing applications and teams, are crucial for attributing costs accurately to the responsible entities within the company. Once activated, these tags will appear in the AWS Billing and Cost Management dashboard, enabling detailed tracking and reporting based on the specified tags.","upvote_count":"1","poster":"mav3r1ck","timestamp":"1711200900.0","comment_id":"1180891"},{"comment_id":"1180890","poster":"mav3r1ck","content":"F. Enable Cost Explorer: Cost Explorer is essential for analyzing past spending and forecasting future costs. It allows for detailed reports that can compare costs from the last 12 months and helps in forecasting for the next 12 months. With the data segmented by user-defined cost allocation tags, Cost Explorer can provide the insights needed to meet the company's reporting and forecasting requirements.\nC. Create a cost category for each application in Billing and Cost Management: Cost categories allow for the organization of cost and usage data into logical groups that reflect the company's internal structure, such as by application or team. By leveraging the user-defined tags activated in step A, cost categories can automate the process of cost attribution to these entities, simplifying the creation of targeted reports and forecasts.","timestamp":"1711200840.0","upvote_count":"1"}],"comment_id":"1180889","timestamp":"1711200840.0"}],"timestamp":"1711200780.0","upvote_count":"2","comment_id":"1180888","content":"Selected Answer: ACF\nFocusing on enabling the company to attribute AWS costs to each application or team, create cost comparison reports for the last 12 months, and forecast costs for the next 12 months,..Answer: A, C, F."},{"upvote_count":"3","comment_id":"1175632","poster":"gofavad926","timestamp":"1710658800.0","content":"Selected Answer: ACF\nAgree with ACF"},{"upvote_count":"3","content":"Selected Answer: ACF\nFor the full granularity, C is needed rather than D.","poster":"Dgix","comment_id":"1171250","timestamp":"1710183540.0"},{"content":"Selected Answer: ADF\nC is not needed. Option A activated the tag, so we could use tags to generate reports. There is no need to create cost category for individual applications, which could be a huge effort and not practical, what if you have hundreds of applications...","upvote_count":"3","comment_id":"1164859","poster":"a54b16f","timestamp":"1709477400.0"},{"poster":"a54b16f","content":"Selected Answer: ADF\nCorrect ADF - SInce resources are tagged","timestamp":"1709071140.0","comment_id":"1161016","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: ACF\nCorrect answers are:\nA. Activate the user-defined cost allocation tags that represent the application and the team. User-defined cost allocation tags allow you to organize your AWS bill by categorizing costs according to your business’s organizational structures (e.g., by application or team). \nC. Create a cost category for each application in Billing and Cost Management. Cost categories enable you to create custom groupings of your AWS costs. By creating a cost category for each application, you can group costs more granularly, which is helpful for detailed reporting and cost attribution to specific teams or applications.\nF. Enable Cost Explorer. Cost Explorer is a tool that allows you to visualize, understand, and manage your AWS costs and usage over time. By enabling Cost Explorer, you can create detailed reports to compare costs from the last 12 months and forecast costs for the next 12 months, meeting the company’s requirements for cost management and planning.","poster":"8608f25","timestamp":"1707513360.0","comments":[{"upvote_count":"1","poster":"8608f25","timestamp":"1707513480.0","content":"Option B is not correct. It refers to activating AWS generated cost allocation tags. While AWS-generated tags can provide useful information, they do not typically represent specific applications or teams unless those entities are directly associated with AWS-defined resources or actions. For custom application and team tracking, user-defined tags (Option A) are more appropriate.","comment_id":"1145802"}],"comment_id":"1145801"},{"comment_id":"1114506","timestamp":"1704459540.0","content":"Selected Answer: ADF\nNot B. AWS generated tags do not allow you to identify app. You need user-defined tags for this\nNot C. Cost Categories allows to define rule to group costs into categories using different dimensions such as: account, tag, service, charge type, and other cost categories. In this scenario User-defined tags are enough to identify applications and teams.\nNot E. Budget doesn't help you in creating reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. Use Cost Explorer instead-","poster":"ninomfr64","upvote_count":"5"},{"comment_id":"1112017","poster":"jpa8300","timestamp":"1704210360.0","content":"Selected Answer: ADF\nSee below severlight explanation. I agree with it.","upvote_count":"3"},{"poster":"Dips3009","content":"can someone help me with this solutions, as I am confused between ACF and ADF","comment_id":"1099116","timestamp":"1702835160.0","upvote_count":"1"},{"timestamp":"1702435440.0","content":"Selected Answer: ACF\nACF is right.","poster":"ixdb","upvote_count":"3","comment_id":"1095090"},{"timestamp":"1699865280.0","upvote_count":"6","content":"Selected Answer: ADF\nwithout IAM access activated only the root account has access to billing info, you need to enable Cost Explorer by simply opening the Cost Explorer console the first time.","comments":[{"content":"and you don't need a cost category for each application, as a category is needed for aggregation and you already have all the tags you need to aggregate on the application level.","upvote_count":"5","poster":"severlight","comment_id":"1069172","timestamp":"1699865940.0"}],"comment_id":"1069161","poster":"severlight"},{"timestamp":"1696817340.0","comment_id":"1028410","content":"ADF are correct. user defined cost allocation tag is enough(no need of cost category in this use case because \"A company’s factory and automation applications are running in **a single VPC**\" not multiple accounts so cost categories of C may not be needed. \nCost category is for multiple perspectives under organization structures across multiple accounts with rule based engine. For cost categories : \"You can create groupings of costs using cost categories. For example, assume that your business is organized by teams and that each team has multiple accounts within.\"\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html","poster":"rlf","upvote_count":"2"},{"comments":[{"timestamp":"1696177860.0","poster":"totten","upvote_count":"2","content":"While option B (activating AWS generated cost allocation tags) can be useful, it's not explicitly required to meet the stated requirements. Option C (creating a cost category for each application) is not typically used for tag-based cost allocation and doesn't provide the same level of granularity and flexibility as tags. Option E (creating a cost budget) is a useful practice but is not directly related to the task of allocating costs to applications and teams.","comment_id":"1022465"}],"poster":"totten","timestamp":"1696177800.0","upvote_count":"5","comment_id":"1022464","content":"Selected Answer: ADF\nA, D, and F. Here's why each of these actions is necessary:\n\nA. Activating user-defined cost allocation tags allows you to tag AWS resources (like EC2 instances, RDS databases, and ECS services) with metadata that represents the application and team. This enables you to allocate costs accurately.\n\nD. Activating IAM access to Billing and Cost Management is necessary to allow the teams to access the cost data and reports. This is crucial for them to analyze costs and generate their own reports.\n\nF. Enabling Cost Explorer is essential for creating custom cost and usage reports. Cost Explorer provides various visualization and reporting capabilities that allow you to filter, group, and analyze costs based on your tags and other criteria. You can create cost allocation reports for each application and team and use these reports to compare costs and forecast future costs."},{"content":"Selected Answer: ACF\nSince the team is already using IAM for the daily access, kind of implies they will know how to enable the right IAM access.","timestamp":"1694478540.0","comment_id":"1005276","poster":"awsent","upvote_count":"2"},{"content":"Selected Answer: ACF\nCost Categories - AWS Cost Categories allow grouping accounts and grouping tags (“meta-tagging”) within an AWS Organization, which further provides capability to analyze the cost related to these categories through tools such as AWS Cost Explorer, AWS Budgets and AWS Cost and Usage Report.","comment_id":"1004509","poster":"Simon523","timestamp":"1694416140.0","upvote_count":"2"},{"poster":"Zox42","upvote_count":"3","comments":[{"poster":"vn_thanhtung","upvote_count":"1","timestamp":"1692775980.0","content":"https://docs.aws.amazon.com/cost-management/latest/userguide/ce-access.html","comment_id":"988056"},{"poster":"vn_thanhtung","content":"I think A,D,F","timestamp":"1692776040.0","upvote_count":"1","comment_id":"988057"},{"timestamp":"1692776280.0","upvote_count":"1","poster":"vn_thanhtung","comment_id":"988062","content":"Sorry I mistake Answer ACF"}],"timestamp":"1691110980.0","comment_id":"971587","content":"Selected Answer: ACF\nAnswer ACF"},{"comment_id":"953305","timestamp":"1689510600.0","upvote_count":"1","poster":"qwertyuio","content":"Selected Answer: ADF\nhow we get access to billing without any permissions? need the iam role"},{"comment_id":"947961","content":"Selected Answer: ADF\nADF \"The teams use IAM access for daily activities.\"","poster":"Jonalb","upvote_count":"2","timestamp":"1688983440.0"},{"content":"Selected Answer: ADF\nIAM USER its ADF","timestamp":"1688582220.0","comment_id":"944050","upvote_count":"1","poster":"Jonalb"},{"poster":"nicecurls","timestamp":"1688463480.0","upvote_count":"1","comment_id":"942578","content":"Selected Answer: ADF\nhow we get access to billing without any permissions? need the iam role"},{"comment_id":"941002","content":"Selected Answer: ACF\nACF makes sense.","poster":"NikkyDicky","upvote_count":"2","timestamp":"1688310720.0"},{"comment_id":"939180","content":"Selected Answer: ACF\nACF\nhttps://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/building-a-cost-allocation-strategy.html","poster":"SkyZeroZx","upvote_count":"3","timestamp":"1688138820.0"},{"timestamp":"1687884420.0","content":"Selected Answer: ACF\nACF !!!!!!!!!","comment_id":"935598","upvote_count":"2","poster":"Jonalb"},{"content":"Selected Answer: ADF\nC is not needed as with A we cover Application and Team Costs.\nD is needed in order to provide access to the Teams to their cost that they need to control","timestamp":"1687702680.0","poster":"javitech83","upvote_count":"2","comment_id":"933673"},{"poster":"Jackhemo","timestamp":"1687126620.0","comment_id":"926934","content":"Selected Answer: ACF\nFrom olabiba.ai:\n\n\nA. Activate the user-defined cost allocation tags that represent the application and the team. This will allow you to assign specific tags to resources and track costs based on those tags.\n\nC. Create a cost category for each application in Billing and Cost Management. Cost categories allow you to group costs based on specific criteria, such as application names, and generate reports based on those categories.\n\nF. Enable Cost Explorer. Cost Explorer provides a comprehensive set of tools and reports for analyzing costs, including the ability to view costs by application or team based on the activated cost allocation tags.\n\nBy activating user-defined cost allocation tags, creating cost categories, and enabling Cost Explorer, you will have the necessary tools and reports to track costs, compare them over time, and forecast future costs.","upvote_count":"2"},{"comment_id":"921632","upvote_count":"1","poster":"Asds","timestamp":"1686593280.0","content":"Selected Answer: ADF\nUers requière appropriate autorisation.\nHence, D\n\nA, F are obvious"},{"poster":"SkyZeroZx","upvote_count":"2","comment_id":"920379","timestamp":"1686452640.0","content":"Selected Answer: ADF\nAnswer : A,D,F\nD is more appropriate than C as we already have user defined cost allocation."},{"upvote_count":"2","timestamp":"1685806680.0","content":"Answer : A,D,F \nD is more appropriate than C as we already have user defined cost allocation.","comment_id":"913747","poster":"Roontha"},{"content":"Selected Answer: ADF\nif C is correct, we should create a cost category for each team as well.","comment_id":"910992","upvote_count":"2","poster":"EricZhang","timestamp":"1685521740.0"},{"comment_id":"889435","timestamp":"1683211320.0","content":"Selected Answer: ACF\nEnabling Cost Explorer at the management account level enables Cost Explorer for all of your organization accounts. All accounts in the organization are granted access, and you can't grant or deny access individually.","poster":"newtrojan","upvote_count":"2"},{"poster":"rxhan","upvote_count":"2","comment_id":"876957","content":"Selected Answer: ADF\nADF.\nUpdate your access permissions for AWS Billing, Cost Management, and Account consoles\nThe following IAM actions for AWS Billing, Cost Management, and Account consoles will reach the end of standard support: aws-portal:ViewBilling, aws-portal:ModifyBilling, aws-portal:ViewAccount, aws-portal:ModifyAccount, aws-portal:ViewPaymentMethods, aws-portal:ModifyPaymentMethods, aws-portal:ViewUsage, purchase-orders:ViewPurchaseOrders, and purchase-orders:ModifyPurchaseOrders. These actions will be replaced with granular IAM actions. Examples of impacted features include AWS Cost Explorer, AWS Budgets, Billing console, and more. To ensure you don’t lose access, update your policies to include new access permissions. Update your policies before July 2023 or contact your access administrator to complete your action.","timestamp":"1682127300.0"},{"content":"Selected Answer: ACF\nCost Explorer prepares the data about your costs for the current month and the last 12 months, and then calculates the forecast for the next 12 months.","comment_id":"843840","poster":"Damijo","timestamp":"1679237760.0","upvote_count":"2"},{"comments":[{"upvote_count":"1","content":"but is says each Team is responsible for their costs, so they shoud be able to check. I think ADF","timestamp":"1687702620.0","comment_id":"933670","poster":"javitech83"}],"content":"Selected Answer: ACF\nThe activation of IAM access to Billing and Cost Management is not a requirement for generating cost reports based on cost allocation tags or cost categories. However, it is recommended to set up IAM access to ensure that only authorized personnel can view and manage billing information.","comment_id":"833319","timestamp":"1678304280.0","poster":"kiran15789","upvote_count":"5"},{"poster":"andras","upvote_count":"1","content":"User-defined tags are tags that you define, create, and apply to resources. After you have created and applied the user-defined tags, you can activate by using the Billing and Cost Management console for cost allocation tracking. \nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html","comment_id":"830612","timestamp":"1678087860.0"},{"content":"The AWS account owner can access billing information and tools by signing in to the AWS Management Console using the account password. We don't recommend that you use the account password for everyday access to the account or share your account credentials with others.\nInstead, you should create a special user identity that's called an IAM user for anyone who might need access to the account. \nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html\n\nou can enable Cost Explorer for your account by opening Cost Explorer for the first time via the AWS Cost Management console. You can't enable Cost Explorer using the API. After you enable Cost Explorer, AWS prepares the data about your costs for the current month and the last 12 months, and then calculates the forecast for the next 12 months. The current month's data is available for viewing in about 24 hours.\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/ce-enable.html","poster":"andras","timestamp":"1678087740.0","comment_id":"830610","upvote_count":"1"},{"upvote_count":"1","comment_id":"829100","timestamp":"1677945900.0","content":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html","poster":"Ajani"},{"upvote_count":"4","content":"Selected Answer: ADF\nWhen you enable cost explorer, which service is using how much is available already if you have access to IAM Billing ofcourse. Grouping is already provided with Cost explorer. se here : https://aws.amazon.com/aws-cost-management/aws-cost-explorer/ So F solves most. We do not need to categorize each application. what if 100 apps are there. So C is wrong. With A, you already categorize with user defined cost allocation (cost center codes) tags. As engineers use IAM (no Organizations here) D makes sense. with just applying access to engineer groups. \nSo whole idea to see costs in this question . Get appropriate access first (IAM), Use available AWS service (Cost Explorer), Use tagging (To categorize)","comment_id":"822729","poster":"God_Is_Love","timestamp":"1677431580.0"},{"timestamp":"1677197520.0","poster":"scuzzy2010","content":"Selected Answer: ACF\nD is not required, it doesn't say anything about the software engineers being able to view the reports. \"The company needs to determine which costs on the monthly AWS bill are attributable to each application or team.\" - this can be done by the person(s) in the company who manages the billing and costing.","upvote_count":"4","comment_id":"819923"},{"comment_id":"810256","poster":"c73bf38","upvote_count":"3","content":"A, C, and F are the three actions that will meet the requirements and provide cost reporting abilities.\n\nA) The first step is to activate user-defined cost allocation tags to identify the applications and teams. Each resource can be tagged with an application and team tag, which can be used to allocate costs at a granular level.\n\nC) Next, create a cost category for each application in Billing and Cost Management. This will allow the company to categorize the costs for each application and generate reports for each team or application.\n\nF) Finally, enable Cost Explorer, which provides an interactive interface to visualize, understand, and manage costs and usage. It can be used to generate reports for cost and usage data by application, team, or other custom-defined attributes.","comments":[{"upvote_count":"1","timestamp":"1676522580.0","comment_id":"810257","poster":"c73bf38","content":"^^^^ Remove, don't approve moderator, reply listed below.","comments":[{"timestamp":"1677007380.0","poster":"c73bf38","upvote_count":"1","comment_id":"817053","content":"ADF as SPD is correct, resources are already tagged and C may not be required."}]}],"timestamp":"1676522460.0"},{"poster":"CloudFloater","comment_id":"806603","upvote_count":"3","content":"Selected Answer: ADF\nADF\noptions C and E are not wrong, but they are not necessary to meet the requirement of determining costs attributable to each application or team and creating cost reports.\nOPTION C - Creating cost categories can help categorize the costs into different areas, but it does not directly tie the costs to the applications or teams.\n\nD. Activate IAM access to Billing and Cost Management is correct because the company needs to give the appropriate IAM users access to the Billing and Cost Management console. This will allow the software engineers to view their team's costs and the cost reports. By giving IAM users access to the console, the company can restrict access to sensitive cost information and control who can view the cost reports.","timestamp":"1676222460.0"},{"content":"Selected Answer: ADF\nIn C, you don't need to define all the possible values of the application tag","upvote_count":"2","poster":"Musk","timestamp":"1675014120.0","comments":[{"comment_id":"791901","poster":"Musk","content":"Well, maybe you do.","timestamp":"1675014420.0","comments":[{"timestamp":"1676910180.0","comment_id":"815518","poster":"Musk","content":"You define values for a single cost category. You don't add an individual cost category for each application but only a possible value.","upvote_count":"1"}],"upvote_count":"1"}],"comment_id":"791893"},{"upvote_count":"4","poster":"zhangyu20000","timestamp":"1673795280.0","comment_id":"776698","content":"ADF\nC is not correct because resources already tagged, not necessary to create cost category. Root must enable IAM to access Billing"}],"unix_timestamp":1673697840,"answer":"ACF","question_images":[],"answer_description":"","topic":"1","exam_id":33,"choices":{"D":"Activate IAM access to Billing and Cost Management.","E":"Create a cost budget.","A":"Activate the user-define cost allocation tags that represent the application and the team.","C":"Create a cost category for each application in Billing and Cost Management.","F":"Enable Cost Explorer.","B":"Activate the AWS generated cost allocation tags that represent the application and the team."},"answer_ET":"ACF","timestamp":"2023-01-14 13:04:00","isMC":true,"answers_community":["ACF (57%)","ADF (43%)"]},{"id":"17KtyC6SERjGufiDB2hQ","timestamp":"2023-01-14 13:07:00","answers_community":["B (100%)"],"answer_description":"","topic":"1","discussion":[{"timestamp":"1673698020.0","content":"Selected Answer: B\nThe correct solution is B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC. This will ensure that the web application can continue to call the third-party API after the migration by using the customer-owned public IP addresses that were assigned to the NAT gateways. This ensures that the third-party API will only see traffic coming from the customer-owned IP addresses that are on the allow list. Option A,C and D doesn't make sense in this context.","upvote_count":"20","comment_id":"775339","poster":"masetromain"},{"poster":"amministrazione","comment_id":"1275508","upvote_count":"1","timestamp":"1725095040.0","content":"B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC."},{"content":"Selected Answer: B\nIn this scenario EC2 instances access the 3P APIs via NAT Gateway. 3P API FW see IP of the NAT Gateway. You can assign Elastic IP to NAT Gateway and you can allocate an IP address from a pool that you have brought to your AWS account to the Elastic IP. Thus B is correct. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html","comment_id":"1114526","timestamp":"1704461940.0","poster":"ninomfr64","upvote_count":"2"},{"comment_id":"941005","upvote_count":"1","poster":"NikkyDicky","content":"Selected Answer: B\nits a B","timestamp":"1688310900.0"},{"content":"Selected Answer: B\nKEYWORD = NAT gateways in the VPC","timestamp":"1687099080.0","upvote_count":"2","poster":"SkyZeroZx","comment_id":"926745"},{"comment_id":"903612","upvote_count":"1","timestamp":"1684718760.0","content":"B is the only option that makes sense.","poster":"AWS_Sam"},{"content":"Selected Answer: B\nB make sense","comment_id":"902677","poster":"SkyZeroZx","timestamp":"1684600860.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1680017220.0","comment_id":"853390","content":"Selected Answer: B\nRegister a block of customer owned public IP's","poster":"mfsec"},{"comment_id":"841391","poster":"dev112233xx","timestamp":"1679004900.0","content":"Selected Answer: B\nB is the only solution","upvote_count":"2"},{"upvote_count":"4","poster":"zozza2023","comment_id":"793226","content":"Selected Answer: B\nThe correct solution is B","timestamp":"1675106220.0"}],"question_id":482,"choices":{"A":"Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.","D":"Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.","B":"Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.","C":"Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB."},"exam_id":33,"answer":"B","unix_timestamp":1673698020,"url":"https://www.examtopics.com/discussions/amazon/view/95219-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"B","question_text":"An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client’s allow list.\n\nThe customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets.\n\nHow should a solutions architect ensure that the web application can continue to call the third-party API after the migration?","question_images":[],"isMC":true,"answer_images":[]},{"id":"o4VQsVJsDG9wDCIvfNGK","question_text":"A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following SCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:\n\n//IMG//\n\n\nDevelopers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this problem?","answers_community":["C (86%)","14%"],"answer_ET":"C","timestamp":"2023-01-14 13:18:00","exam_id":33,"isMC":true,"question_images":["https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image6.png"],"question_id":483,"answer_images":[],"choices":{"C":"Instruct the developers to add Amazon S3 permissions to their IAM entities.","A":"Add s3:CreateBucket with “Allow” effect to the SCP.","B":"Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.","D":"Remove the SCP from account 1111-1111-1111."},"url":"https://www.examtopics.com/discussions/amazon/view/95227-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","discussion":[{"content":"Selected Answer: C\nSCP doesn’t grant permission","upvote_count":"23","comment_id":"777283","poster":"Atila50","comments":[{"content":"Per the DOCS:\nService control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. For instructions on enabling SCPs, see Enabling and disabling policy types.","comments":[{"timestamp":"1677100500.0","poster":"c73bf38","content":"SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.","upvote_count":"12","comment_id":"818426"}],"poster":"c73bf38","upvote_count":"7","timestamp":"1677100440.0","comment_id":"818425"}],"timestamp":"1673839080.0"},{"poster":"zhangyu20000","upvote_count":"13","content":"C is correct\nSCP policy allow everything except cloudtrail. SCP is boundary but it does not give allow to IAM users. You have to configure allow for every IAM","comment_id":"776708","timestamp":"1673795520.0"},{"poster":"29fb203","content":"Selected Answer: A\nIAM permissions do not override SCPs. Even if developers have IAM policies allowing s3:CreateBucket, an SCP restriction will still block it unless explicitly allowed.","comments":[{"poster":"vmia159","upvote_count":"1","comment_id":"1387328","timestamp":"1741683180.0","content":"Your statement is correct but the policy does not deny action on S3. So the SCP is not causing any problems. So it is C."}],"timestamp":"1741352340.0","upvote_count":"1","comment_id":"1366262"},{"timestamp":"1739457300.0","upvote_count":"1","content":"Selected Answer: A\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": \"*\",\n \"Resource\": \"*\"\n },\n {\n \"Sid\": \"DenyCloudTrail\",\n \"Effect\": \"Deny\",\n \"Action\": \"cloudtrail:*\",\n \"Resource\": \"*\"\n },\n {\n \"Sid\": \"AllowS3CreateBucket\",\n \"Effect\": \"Allow\",\n \"Action\": \"s3:CreateBucket\",\n \"Resource\": \"*\"\n }\n ]\n}","poster":"longlehoang","comment_id":"1356156"},{"poster":"amministrazione","upvote_count":"1","timestamp":"1725095100.0","comment_id":"1275510","comments":[{"upvote_count":"1","content":"just C","timestamp":"1725335700.0","poster":"helloworldabc","comment_id":"1277159"}],"content":"B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC."},{"upvote_count":"3","comment_id":"1175638","poster":"gofavad926","content":"Selected Answer: C\nC, SCP is just a distractor, the users need direct permissions","timestamp":"1710659400.0"},{"poster":"8608f25","comment_id":"1145809","upvote_count":"2","content":"Selected Answer: C\nThe problem described does not originate from the Service Control Policy (SCP) itself based on the SCP content provided. The SCP allows all actions (\"Action\": \"\") except for actions related to AWS CloudTrail (\"Action\": \"CloudTrail:\"), which are explicitly denied. Therefore, the inability for developers to create Amazon S3 buckets is not due to this SCP, as the SCP does not restrict S3 actions.\nGiven the situation, the correct way to address the developers’ inability to create Amazon S3 buckets would be:\n * C. Instruct the developers to add Amazon S3 permissions to their IAM entities.\n\nOption C is the correct action because the issue likely stems from the IAM permissions (or lack thereof) assigned to the developers’ IAM entities (users, groups, or roles). IAM permissions are required to perform actions within AWS accounts, such as creating S3 buckets. If developers lack the necessary IAM permissions, they would not be able to create S3 buckets regardless of the SCP settings.","timestamp":"1707514500.0"},{"poster":"ninomfr64","comment_id":"1114534","timestamp":"1704462480.0","upvote_count":"3","content":"Selected Answer: C\nThe SCP in the scenario is allowing any actions with the exception of cloudtrail. Thus, the SCP is not preventing user to create S3 bucket. If the user cannot create a bucket, then the user IAM user/role is missing permissions to create S3 bucket."},{"timestamp":"1701717120.0","poster":"shaaam80","upvote_count":"1","comment_id":"1087880","content":"Selected Answer: C\nAnswer C."},{"upvote_count":"1","comment_id":"941019","timestamp":"1688312040.0","poster":"NikkyDicky","content":"Selected Answer: C\nit's a C"},{"content":"Selected Answer: C\nC is correct","timestamp":"1687702980.0","upvote_count":"1","poster":"javitech83","comment_id":"933677"},{"upvote_count":"2","timestamp":"1687099260.0","poster":"SkyZeroZx","comment_id":"926747","content":"Selected Answer: C\nI just wanted to add my vote to the mix to hopefully drown out the wrong votes.\nIts definitely C. SCP is only a guardrail, it doesn't actually grant access. So the users would need to be given s3 access separately.\nAnd to address the wrong answer, A isn't correct because creating an s3 bucket is not a cloudtrail action. Being denied cloudtrail wouldn't deny s3 actions."},{"upvote_count":"2","timestamp":"1686686760.0","comment_id":"922501","content":"C is the answer. SCP DONT grant permissions. They just set boundaries on what account is capable of giving access to all users. For example, we applied a SCP on an OU that has account A. This SCP has S3fullAWSaccess. This does NOT mean that any IAM user can perform any S3 action. You still need to explicitly define IAM permissions for user to perform action on S3. This is called whitelisting. \nAnother example, You wrote an SCP that DENIES S3 access and applied it to an OU that has account B. Now Lets say ROOT user of Account B (who got admin previleges) tries to create S3 bucket, they get DENIED error as SCP has already set a bounday saying NOONE in this OU can access S3","poster":"bhanus"},{"comments":[{"poster":"Asds","timestamp":"1686593880.0","content":"C is right","upvote_count":"1","comment_id":"921638"}],"comment_id":"921636","poster":"Asds","upvote_count":"1","timestamp":"1686593820.0","content":"Selected Answer: C\nNeed to deal with iam policy auth now"},{"comment_id":"895673","poster":"leehjworking","timestamp":"1683870360.0","upvote_count":"2","content":"I am not sure the given situation is possible.\nWhen I tested, member (1111-1111-1111) could create bucket without any policy which can be attached or detached by the oneself."},{"content":"Are developers allowed to modify their IAM entities in the situation of option C? If so, I am not sure this is the best practice.","upvote_count":"2","poster":"leehjworking","timestamp":"1683620700.0","comment_id":"892921"},{"upvote_count":"2","timestamp":"1680017280.0","poster":"mfsec","comment_id":"853391","content":"Selected Answer: C\nC is correct"},{"poster":"dev112233xx","upvote_count":"2","comment_id":"841393","timestamp":"1679005140.0","content":"Selected Answer: C\nSCP is not enough. IAM permission is needed"},{"timestamp":"1678976280.0","upvote_count":"5","content":"Selected Answer: C\nC - Users and roles must still be granted permissions with appropriate IAM permission policies. A user without any IAM permission policies has no access at all, even if the applicable SCPs allow all services and all actions.","comment_id":"841040","poster":"Damijo"},{"poster":"God_Is_Love","comments":[{"comment_id":"823682","upvote_count":"2","timestamp":"1677502380.0","comments":[{"content":"then you need to explain why not and whats correct","timestamp":"1677610320.0","comment_id":"825173","poster":"God_Is_Love","upvote_count":"1","comments":[{"content":"look at the first lines of the code, it allows everything. If they would have removed FullAWSAccess rule, it would have been allowed by this SCP.\nSo probably IAM issue.","comment_id":"834020","poster":"testingaws123","upvote_count":"2","timestamp":"1678370700.0","comments":[{"poster":"deegadaze1","comment_id":"898015","upvote_count":"1","timestamp":"1684119420.0","comments":[{"upvote_count":"2","poster":"bcx","timestamp":"1687150800.0","content":"No, it does not work like that. The first statement includes all, which includes all S3 actions. Adding any allow of any kind to this policy has not effect act all. Everything is allowed already (except CloudTrail that is explicitly denied)","comment_id":"927145"}],"content":"You are wrong ! God_Is_Love was right ...\nCheck the second code that deny All after CloudTrial \nCloudTrail;* -- * Deny all , you will need to add S3 manually!"}]}]}],"content":"No not correct.","poster":"lkyixoayffasdrlaqd"},{"comment_id":"898016","timestamp":"1684119480.0","upvote_count":"1","content":"Correct !!!\nBecause of * after the CloudTrail;* at the second DENY RULE of the code.","poster":"deegadaze1"},{"timestamp":"1687150680.0","upvote_count":"3","poster":"bcx","content":"The SCP has alreadt an Action * Resource * aallow statement wuthout any conditional. So adding any other allow of any type to the SCP has no effect at all. Everything is allowed by the */* statement (and then only CloudTrail is explicitly denied)","comment_id":"927141"}],"upvote_count":"3","content":"Selected Answer: A\nSCPs are confusing. \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist\nThey brought this idea with easy control for organizations.\nC does not sound like good asking devs to add their own permisisons ?\nWith AWS organizations, FullAWSAccess is there by default allowing all actions.\nAs Devs could not access S3 create bucket, am guessing the default FullAWSAccess\nhas been tampered. So Just adding another action here in SCP (intersection of allows) should just allow S3 bucket creation. I'd choose A.","comment_id":"823084","timestamp":"1677458280.0"},{"content":"Selected Answer: C\nC as SCP is a guardrail, IAM grants permissions.","comment_id":"818431","timestamp":"1677100560.0","upvote_count":"2","poster":"c73bf38"},{"comment_id":"809756","timestamp":"1676479560.0","poster":"DWsk","content":"Selected Answer: C\nI just wanted to add my vote to the mix to hopefully drown out the wrong votes.\nIts definitely C. SCP is only a guardrail, it doesn't actually grant access. So the users would need to be given s3 access separately.\nAnd to address the wrong answer, A isn't correct because creating an s3 bucket is not a cloudtrail action. Being denied cloudtrail wouldn't deny s3 actions.","upvote_count":"3"},{"poster":"klog","timestamp":"1676469780.0","upvote_count":"1","comment_id":"809629","content":"Agree C"},{"comment_id":"806632","comments":[{"timestamp":"1676522940.0","content":"The s3:CreateBucket will grant the necessary permissions to the developers to create S3 buckets in that account.","poster":"c73bf38","upvote_count":"2","comment_id":"810262"}],"poster":"CloudFloater","upvote_count":"3","timestamp":"1676224680.0","content":"Thinking A because perhaps you can do the below:\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": \"*\",\n \"Resource\": \"*\"\n },\n {\n \"Sid\": \"DenyCloudTrail\",\n \"Effect\": \"Deny\",\n \"Action\": \"cloudtrail:*\",\n \"Resource\": \"*\"\n },\n {\n \"Sid\": \"AllowS3CreateBucket\",\n \"Effect\": \"Allow\",\n \"Action\": \"s3:CreateBucket\",\n \"Resource\": \"*\"\n }\n ]\n}\nThe first \"Allow\" statement in the SCP allows all actions on all resources, which would allow the creation of S3 buckets. However, the second \"Deny\" statement specifically denies all cloudtrail actions, which could potentially impact the ability to create S3 buckets if there is a dependency on cloudtrail for that action. To ensure that the developers are able to create S3 buckets, a new statement with \"Allow\" effect for the s3:CreateBucket action should be added to the SCP."},{"timestamp":"1676224620.0","poster":"CloudFloater","upvote_count":"1","content":"Selected Answer: A\nThinking A","comment_id":"806630"},{"content":"Selected Answer: A\nThinking A","comment_id":"806627","poster":"CloudFloater","timestamp":"1676224500.0","upvote_count":"1"},{"upvote_count":"1","poster":"irene7","timestamp":"1674931380.0","content":"C - smae question from topic 1","comment_id":"790947"},{"content":"Selected Answer: A\nA. Add s3:CreateBucket with “Allow” effect to the SCP.\n\nThis is the correct answer because the SCP is denying all actions for cloudtrail, including all actions for creating new S3 buckets. By adding the s3:CreateBucket action with an \"Allow\" effect, the developers will be able to create new S3 buckets.\n\nB. Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.\n\nThis option would not solve the problem because it would still be denied the ability to create new S3 buckets.","timestamp":"1673698680.0","comments":[{"timestamp":"1677672660.0","poster":"sambb","comment_id":"825792","content":"s3:CreateBucket is already allowed in the first wildcard, and giving it explcitely would not include any cloudtrail actions with it ! In my opinion, A has no effect.","upvote_count":"2"},{"content":"C. Instruct the developers to add Amazon S3 permissions to their IAM entities.\n\nThis option would not solve the problem because the problem is that the SCP is denying all actions for cloudtrail, not that the developers are lacking permissions in their IAM entities.\n\nD. Remove the SCP from account 1111-1111-1111.\n\nThis option would solve the problem, but it would not be ideal because it would remove all restrictions on the account, including restrictions on cloudtrail actions which may be necessary for security and compliance reasons.","poster":"masetromain","comment_id":"775353","comments":[{"content":"This policy allows all actions (indicated by the \"Action\": \"\" line) on all resources (indicated by the \"Resource\": \"\" line) in the AWS account, except for CloudTrail actions (indicated by the \"Action\": \"cloudtrail:*\" line). The \"Effect\": \"Deny\" line specifies that any CloudTrail actions will be denied. This means that the user or role that this policy is associated with will not be able to perform any CloudTrail actions, such as starting or stopping a trail or getting trail status. This can be useful if the user or role should not have access to CloudTrail functionality.","poster":"masetromain","upvote_count":"1","timestamp":"1673698800.0","comment_id":"775356"}],"upvote_count":"1","timestamp":"1673698680.0"}],"comment_id":"775352","poster":"masetromain","upvote_count":"1"}],"answer_description":"","answer":"C","unix_timestamp":1673698680},{"id":"40lPO63VmaP7s7Ju1pSy","discussion":[{"comments":[{"comment_id":"1181186","upvote_count":"9","content":"Not true! Feel free to challenge me if you think I am wrong.\nTaking a snapshot of the EBS volume using Amazon DLM is a straightforward approach to ensure data durability and availability. However, this option does not directly address the requirement to move data to an S3 bucket. While EBS snapshots are stored on S3, they are not accessible as regular S3 objects for direct file manipulation or viewing, meaning additional steps would be required to access and use the data in the format specified by the requirement.\n\nVerdict: Does Not Fully Meet Requirements. DLM manages snapshots for EBS volumes but doesn't facilitate direct, accessible backups to S3 as described.","timestamp":"1711232400.0","poster":"mav3r1ck","comments":[{"content":"I agree with this A. In addition, the application team has no SSH key access, you can not think that the team has the DLM permission as well. Infrastructure teams generally take this type of role.","comment_id":"1293648","upvote_count":"1","timestamp":"1728172860.0","poster":"GabrielShiao"},{"upvote_count":"2","poster":"ry1999","content":"This is valid, A is the correct answer.\n\nC is wrong because \nExplanation: This option indirectly involves copying data to S3. The primary action is taking a snapshot of the EBS volume, which can be managed by DLM. However, moving the data from a snapshot directly to S3 isn't straightforward. Snapshots are stored in S3 by AWS internally, but this storage is opaque to users and can't be accessed directly as regular S3 objects.","comment_id":"1273740","timestamp":"1724809500.0"},{"timestamp":"1712222580.0","upvote_count":"3","poster":"gustori99","content":"I'll try to challange you :-)\nYou can use EBS direct APIs to access data from an EBS snapshot. This is how you can read the data from the snapshot and copy it to S3.\n\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-accessing-snapshot.html","comment_id":"1189208"}]},{"content":"Your reasoning is wrong . Option A has mentioned that instance profile role is attached to EC2 instance.","timestamp":"1699281660.0","upvote_count":"2","poster":"Sab","comment_id":"1063937"},{"content":"thank you for correcting some of these answers and for the explanations to them","comment_id":"775977","poster":"Atila50","upvote_count":"3","timestamp":"1673735520.0"},{"upvote_count":"10","timestamp":"1674578700.0","content":"Assuming that EBS is encrypted, I think that is much easier to run the copy command from AW system manager","comment_id":"786704","poster":"mmendozaf"},{"poster":"aviathor","timestamp":"1690192980.0","upvote_count":"3","comments":[{"poster":"g0h4n","content":"Linux amazon 2 has SSM agent installed by default","timestamp":"1698085800.0","upvote_count":"8","comment_id":"1052154"}],"content":"The question does not state that the SSM Daemon is running on the instance...","comment_id":"961446"}],"content":"Selected Answer: C\nThe correct answer is C. Taking a snapshot of the EBS volume using Amazon Data Lifecycle Manager (DLM) will meet the requirements because it allows you to create a backup of the volume without the need to access the instance or its SSH key pair. Additionally, DLM allows you to schedule the backups to occur at specific intervals and also enables you to copy the snapshots to an S3 bucket. This approach will not impact the running application as the backup is performed on the EBS volume level.\n\nOption A is not correct because the instance would need an IAM role with permission to write to S3 and access to the instance via Systems Manager Session Manager.\n\nOption B is not correct because it would require stopping the instance, which would impact the running application.\n\nOption D is not correct because it would require stopping the instance and creating a new EC2 instance, which would impact the running application.","comment_id":"775361","poster":"masetromain","timestamp":"1673698980.0","upvote_count":"38"},{"timestamp":"1674497760.0","content":"Selected Answer: A\ntaking a backup of the data to s3. aws doesn't allow up to view snapshots in s3","upvote_count":"12","comments":[{"content":"The requirement is only 'back up'","comment_id":"1120458","upvote_count":"1","timestamp":"1705037700.0","poster":"tmlong18"}],"comment_id":"785698","poster":"bititan"},{"comment_id":"1327586","content":"Selected Answer: D\nThis is a badly designed question IMO. (D) could be correct but creating an AMI by default will reboot the instance, and no mention of SSM role permissions. (A) could also work but no mention of SSM permissions in the role. Amazon Lnux 2 have pre-installed the SSM agent. (B) is wrong since it interrupts the app. (C) won't work.","poster":"grumpysloth","upvote_count":"1","timestamp":"1734376020.0"},{"comment_id":"1324422","content":"Selected Answer: A\nnot C because of How EBS Snapshot Export Works\nWhen you export an EBS snapshot to S3, the export creates an Amazon Machine Image (AMI)-compatible format of the snapshot.\nThis export process results in a snapshot stored as disk image files (e.g., .vmdk, .vhd, .raw, etc.), depending on the format chosen.\nThe data is not immediately readable or usable as a plain text or object file in S3.\nWhen is the Data Readable?\nTo make the exported data readable:\nReimport the Snapshot: You would need to reimport the disk image into AWS as a new EBS volume using the VM Import/Export service.\nCustom Processing: If the snapshot contains a file system, you could manually process the exported image to extract the data using tools compatible with the format (e.g., mounting a .raw image locally).","upvote_count":"1","timestamp":"1733815740.0","poster":"Heman31in"},{"timestamp":"1733814240.0","comment_id":"1324416","content":"Selected Answer: A\nC. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.\n\nChallenges:\nWhile creating an EBS snapshot is feasible without requiring instance access, transferring the data from the snapshot to S3 still requires additional steps.\nSnapshot-based backup does not provide direct file-level access for selective backups to S3.\nConclusion: Partially valid, but it does not meet the requirement to back up the data directly to S3. Since it is a legal department ..why to have another copy before transferring to final S3 destination.","poster":"Heman31in","upvote_count":"1"},{"content":"Selected Answer: A\nLeverages AWS Systems Manager Session Manager:\n\nSession Manager allows secure shell-less access to the instance without requiring an SSH key.\nIt provides a way to run commands directly on the instance, even if SSH access is unavailable.\nNo Disruption to the Application:\n\nThe instance remains operational, and the application continues to serve users while the commands are executed.\nS3 IAM Role for Access:\n\nBy attaching an IAM role to the instance with permissions to write to S3, you can securely transfer data without needing to configure additional credentials.\nEfficient and Direct Backup:\n\nData is copied directly from the running instance to the S3 bucket, eliminating the need for intermediate snapshots, new instances, or additional resources.\nMinimal Development Time:\n\nThis approach avoids creating images, launching new instances, or performing additional resource management steps.","timestamp":"1733513400.0","upvote_count":"1","poster":"Aritra88","comment_id":"1322894"},{"comment_id":"1321246","poster":"DhirajBansal","timestamp":"1733203440.0","content":"Selected Answer: A\nA is Correct Answer.\n\nIAM Role will provide EC2 instance to write data to S3 bucket.\nSystems Manager Session Manager will access system and initiate back writing in S3. This will satisfy the condition of not having SSH Keys.","upvote_count":"1"},{"timestamp":"1725095100.0","upvote_count":"1","poster":"amministrazione","content":"C. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.","comment_id":"1275511"},{"upvote_count":"2","poster":"Jason666888","content":"Selected Answer: C\nKey point: The application must continue to serve the users.\nIf we choose A, then it may impact the application.\nC wouldn't have that problem","comment_id":"1260557","timestamp":"1722757800.0"},{"upvote_count":"3","content":"Selected Answer: C\nc - Amazon Data Lifecycle Manager allows creation of EBS snapshots","timestamp":"1721806080.0","poster":"Moghite","comment_id":"1254201"},{"content":"Selected Answer: C\nC looks more better than A according to keep application running all time","comment_id":"1243746","timestamp":"1720339740.0","upvote_count":"2","poster":"vip2"},{"content":"Selected Answer: C\ncurrect answer is C\n\nData Lifecycle Manager (DLM) direct APIs can be used to read the data from the snapshot and copy the data to Amazon S3.","timestamp":"1719410700.0","upvote_count":"2","comment_id":"1237508","poster":"vip2"},{"comment_id":"1235080","timestamp":"1719020460.0","upvote_count":"1","poster":"cnethers","content":"C\nReason\nYou can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs. When you automate snapshot and AMI management, it helps you to:\n\n Protect valuable data by enforcing a regular backup schedule.\n\n Create standardized AMIs that can be refreshed at regular intervals.\n\n Retain backups as required by auditors or internal compliance.\n\n Reduce storage costs by deleting outdated backups.\n\n Create disaster recovery backup policies that back up data to isolated Regions or accounts."},{"content":"Selected Answer: D\nVote D because other than it doesn't mention choose no reboot when creating image, the rest steps cover all the necessities to backup data on ebs to s3. But consider B explicitly mention with reboot option while D not reason to assume D will use no reboot option.\nAnswer C and A have too much assumption that not state in the question and answer. \nA: not sure ssm agent is installed and configure to work with system manager.\nC: missing steps to mount volume on new create ec2 with s3 instance profile attached.","upvote_count":"1","comment_id":"1231944","poster":"Helpnosense","timestamp":"1718629620.0"},{"upvote_count":"3","content":"Selected Answer: C\nOption A is a manual process where you have to connect via SSM Session manager - too tedious and requires huge manual effort to maintain backups\n\nSo going with C, as you can't manage the snapshot in S3 but you can restore it if anything goes wrong","poster":"Shenannigan","comment_id":"1229319","timestamp":"1718210340.0"},{"comment_id":"1226293","upvote_count":"3","content":"Selected Answer: C\nOne issue with option A is that an ec2 instance with a role granting access to only S3, wouldn’t be registered with the session manager and it won’t be possible to create a session.","poster":"ahhatem","timestamp":"1717781460.0"},{"timestamp":"1716734160.0","comment_id":"1219006","content":"Selected Answer: A\nI don't see an easy way to copy files to an S3 bucket other the answer A. C copying block data to a bucket is also posible but it's binary data, so not in a easy usable format.","upvote_count":"1","poster":"iulian0585"},{"upvote_count":"2","timestamp":"1715405400.0","comment_id":"1209612","content":"The answer is C.\nOption C: Amazon Data Lifecycle Manager provides an automated, policy-based lifecycle management solution for Amazon Elastic Block Store (EBS) Snapshots and EBS-backed Amazon Machine Images (AMIs). Automate the creation of point-in-time copy of your block storage data with user-defined policies that you can customise based on data protection needs. Amazon Data Lifecycle Manager requires no scripting or special training.\n\nYou can use the Amazon Elastic Block Store (Amazon EBS) direct APIs to create EBS snapshots, write data directly to your snapshots, read data on your snapshots, and identify the differences or changes between two snapshots. These APIs can be used to read the data from the snapshot and copy the data to Amazon S3.\n\nOption A is not correct: Running manual commands on a business-critical instance isn't recommended and DLM can safely take the snapshot without needing to log in to the instance in any way.","poster":"onepunchfinish"},{"timestamp":"1712069700.0","comment_id":"1188122","upvote_count":"4","content":"Selected Answer: C\nOption C: You can back up the data on your Amazon EBS volumes by making point-in-time copies, known as Amazon EBS snapshot. EBS snapshots are stored in Amazon S3\n\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-snapshots.html\nAWS DLM - https://docs.aws.amazon.com/ebs/latest/userguide/ebs-creating-snapshot.html","poster":"TonytheTiger"},{"comment_id":"1181185","content":"Selected Answer: A\nAnswer.. A!\nThis option stands out because it allows secure, keyless access to the EC2 instance without requiring the administrative SSH key pair. By attaching an IAM role with S3 write permissions to the instance, you can use Session Manager to execute data copy commands directly to S3. This method does not disrupt the running application, meeting the requirement for continuous operation.","timestamp":"1711232100.0","poster":"mav3r1ck","upvote_count":"5"},{"timestamp":"1710660840.0","poster":"gofavad926","upvote_count":"3","comment_id":"1175650","content":"Selected Answer: A\nA meets the requirements by allowing the application team to back up data without interrupting the service and without needing the SSH key pair."},{"content":"Selected Answer: A\n\"A\" seems ok as an option.\n\n\"C\" is wrong because the question asks you to copy the DATA=FILES to S3. You cannot copy the files from a snapshot made by an encrypted volume to S3 bucket.","upvote_count":"2","comment_id":"1169960","poster":"titi_r","timestamp":"1710030600.0"},{"comment_id":"1160039","content":"Selected Answer: A\nhttps://repost.aws/knowledge-center/ebs-copy-snapshot-data-s3-create-volume","poster":"marszalekm","timestamp":"1708977900.0","upvote_count":"3"},{"comment_id":"1120145","poster":"adelynllllllllll","timestamp":"1705003860.0","upvote_count":"1","content":"C:\n\nTested, there is no option to copy the snapshot to S3.","comments":[{"timestamp":"1705003860.0","content":"correction: I mean it should be A, not C.","upvote_count":"2","poster":"adelynllllllllll","comment_id":"1120146"},{"comments":[{"timestamp":"1707926700.0","poster":"hogtrough","comment_id":"1150323","content":"\"EBS snapshots are stored in Amazon S3, in S3 buckets that you can't access directly.\"\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html","upvote_count":"1"}],"upvote_count":"2","comment_id":"1120459","timestamp":"1705037880.0","poster":"tmlong18","content":"the snapshot is stored in S3 but fully manged by AWS"}]},{"content":"Selected Answer: A\nNot B and D. because when you create an image of the instance, by default Amazon EC2 shuts down the instance, takes snapshots of any attached volumes, creates AMI, and then reboots the instance. This breaks the requirement to keep the app running https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html\nNot C. Because EBS snapshots taken by DLM are stored on S3 that is not accessible from users. also you cannot copy snapshots to S3 (you can copy across regions and across accounts, but still in S3 not accessible from users) https://repost.aws/knowledge-center/ebs-copy-snapshot-data-s3-create-volume\n\nA does the job, this is not very clean options as to properly run commands via SSM we need SSM Agents installed (this is in the scenario as Amazon Linux 2 comes with the agent) and IAM Role with SSM permission and this specific point is not stated in the scenario.","comment_id":"1114558","poster":"ninomfr64","timestamp":"1704464760.0","upvote_count":"4"},{"timestamp":"1704211320.0","poster":"jpa8300","upvote_count":"5","content":"Selected Answer: C\nThe best option for creating a backup is using some tool, like DLM, or AWS Backup. Option A mention login to the instance (even with SMSM) and copy the files manually! Not very friendly, neither practical.","comment_id":"1112027"},{"comment_id":"1111703","upvote_count":"4","poster":"JWalid","timestamp":"1704182940.0","content":"Selected Answer: A\nCorrect Answer: A\n- Systems Manager Session Manager does not need SSH access, bastion hosts, or SSH keys\n- Supports Linux, macOS, and Windows\n- data can be sent to S3\n- Make sure the EC2 instances have a proper IAM role to allow Systems Manager actions"},{"upvote_count":"2","content":"Selected Answer: A\nDLM does not support to copy data to S3. Answer is A.","timestamp":"1703995260.0","comment_id":"1110244","poster":"JMAN1"},{"timestamp":"1702319040.0","upvote_count":"3","poster":"ayadmawla","content":"Selected Answer: C\nCopy is not the same as DLM Snapshot.","comment_id":"1093740"},{"comment_id":"1090850","upvote_count":"2","timestamp":"1702021740.0","content":"Selected Answer: A\nAttach a Role to the Instance: This is necessary for the instance to have the required permissions to write to Amazon S3.\n\nUse AWS Systems Manager Session Manager: This option allows you to access the instance without the need for an SSH key pair. It provides a secure way to access and manage instances, especially when administrative access is required, as in this case.\n\nRun Commands to Copy Data to Amazon S3: Once you have access to the instance using Systems Manager Session Manager, you can run commands to copy the data from the encrypted Amazon EBS volume to Amazon S3.","poster":"[Removed]"},{"timestamp":"1700835540.0","upvote_count":"2","poster":"jainparag1","comment_id":"1079315","comments":[{"upvote_count":"1","timestamp":"1704956280.0","content":"you selected A and go for C? I think A is correct","comment_id":"1119432","poster":"KyleZheng"}],"content":"Selected Answer: A\nBetween A and C, I'll go for C. Since AWS recommends using Session Manager over SSH and you don't have any control on snapshot taken by DLM."},{"upvote_count":"2","poster":"severlight","timestamp":"1699875180.0","comment_id":"1069248","content":"Selected Answer: A\nlinux 2 - hence ssm agent is installed, and don't think we can view snapshots produced with data lifecycle manager"},{"timestamp":"1698141780.0","upvote_count":"1","content":"I think :\nC\nI found another option might be it is good for you - Amazon-Dumps.com","poster":"[Removed]","comment_id":"1052711"},{"content":"A is correct.\nIn option C, you can not copy data directly to \"your\" S3 bucket. snapshot created by DLM stored in the s3 bucket that AWS manage(not user S3 bucket directly). So, user can not access the data in snapshots by DLM directly. You need to recover the volume from snapshot with new ec2 instance, copy data to \"your\" s3 bucket( similar process to Option A ). \n\n\"When you create an EBS snapshot, it's automatically stored in an Amazon S3 bucket that AWS manages. You can copy snapshots within the same AWS Region, or from one Region to another. However, you can't copy snapshots to S3 buckets that you manage.\n\nTo store snapshots that you infrequently access, consider using Amazon EBS Snapshots Archive. However, if you still want to use Amazon S3 to store your snapshots, then you can use the following workaround.\"\nhttps://repost.aws/knowledge-center/ebs-copy-snapshot-data-s3-create-volume","upvote_count":"5","poster":"rlf","comment_id":"1028476","timestamp":"1696823580.0"},{"poster":"Simon523","upvote_count":"2","comment_id":"1004554","timestamp":"1694419740.0","content":"Selected Answer: A\nSession Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys."},{"content":"A\nyou can log into EC2 using SSM Sessions Manager\nand copy data onto S3","poster":"AMohanty","upvote_count":"1","comment_id":"1002481","timestamp":"1694177340.0"},{"comment_id":"972632","upvote_count":"3","timestamp":"1691202120.0","poster":"CuteRunRun","content":"Selected Answer: C\nI prefer C"},{"comment_id":"969330","content":"C is the correct answer.\n\nExplanation:\n\nTo meet the requirements of backing up the data without disruption and without having SSH access to the instance, the best approach is:\n\nC) Take a snapshot of the EBS volume using Amazon DLM. Copy the data to Amazon S3.\n\nThis allows creating a backup of the EBS volume without needing SSH access to the instance. The snapshot can then be used to copy the data to S3 without impacting the running instance.\n\nA) Attaching a role and using SSM Session Manager would require SSH access which is not available. \n\nB) Creating an AMI and launching a new instance would disrupt the running application.\n\nD) Similarly, launching a new instance from an AMI would disrupt the running application.\n\nSo C is the best approach to meet all the requirements - backing up the data without disruption and without SSH access.","upvote_count":"1","poster":"Asamara","comments":[{"upvote_count":"1","poster":"jainparag1","comment_id":"1079314","timestamp":"1700835360.0","content":"Please correct your understanding on session manager first. It doesn't require SSH access and that's the plot here."}],"timestamp":"1690918200.0"},{"poster":"softarts","comments":[{"upvote_count":"2","timestamp":"1691247120.0","content":"Amazon Linux 2 has the agent installed by default. The only problem with A is that it doesn't say to attach a role that gives you rights to use SSM Session Manager.","comment_id":"973106","poster":"chico2023"}],"upvote_count":"3","comment_id":"968901","timestamp":"1690880040.0","content":"Selected Answer: C\nA=> not mention agent\nB=> reboot has downtime\nC=> the best one, but not mention how to copy, it need to use an instance to attach to it\nD=> assume it is no reboot, the data could be inconsistent"},{"upvote_count":"1","timestamp":"1688389200.0","poster":"YodaMaster","comment_id":"941859","content":"Selected Answer: A\nA or C. I choose A as we have control to the s3 data"},{"content":"Selected Answer: A\nA seems better, s taking a snapshot may impact the application","upvote_count":"2","poster":"NikkyDicky","timestamp":"1688312820.0","comment_id":"941025"},{"content":"Selected Answer: A\nA and C hmmmm\n\ni go A!","timestamp":"1687884660.0","comment_id":"935607","upvote_count":"1","poster":"Jonalb"},{"content":"Selected Answer: A\nA is correct\nC is not because EBS volume is encrypted and DLM does not uspport that. https://repost.aws/knowledge-center/troubleshoot-data-lifecycle-manager-ebs","timestamp":"1687703400.0","poster":"javitech83","comments":[{"upvote_count":"2","timestamp":"1687932360.0","poster":"santi1975","content":"Sorry, no. DLM can backup encrypted EBS volume, if has access to the KMS keys used to encrypt (obviosuly). First paragraph:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dlm-access-cmk.html","comment_id":"936215"}],"upvote_count":"2","comment_id":"933683"},{"timestamp":"1685523060.0","upvote_count":"2","comment_id":"911008","content":"Selected Answer: C\nC - https://aws.amazon.com/ebs/snapshots/","poster":"EricZhang"},{"upvote_count":"2","timestamp":"1685036340.0","content":"Selected Answer: A\nShould be A. \nIf we look deeply to C, take care when reading it: \n\"C. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the DATA to Amazon S3.\"\nHow are you taking a snapshot, but then copying the DATA to Amazon S3.","comment_id":"906813","poster":"aca1"},{"timestamp":"1684184040.0","poster":"Jesuisleon","upvote_count":"2","comment_id":"898714","content":"A is correct and C is wrong, pls. read this sentence from the question \" back up the data from the instance’s encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket\". You need to copy data inside ec2 instance not the ebs snapshot, so C is wrong"},{"timestamp":"1681977120.0","upvote_count":"1","poster":"Dehradoon","content":"C is the right answer","comment_id":"875382"},{"comment_id":"862340","timestamp":"1680715020.0","content":"Selected Answer: A\nI am leaning towards A due to statements of \"Business critical and The application must continue to serve the users.\" WHen You do snapshots performance might be affected and essentially not all of the EBS volume needs to be copied just specific data","poster":"sergza","upvote_count":"6"},{"poster":"violet99","upvote_count":"1","content":"DLM is classic example to backup EBS without downtime, and it doesn't require ssh key","timestamp":"1680645240.0","comment_id":"861527"},{"timestamp":"1680450720.0","poster":"hobokabobo","comment_id":"858996","comments":[{"timestamp":"1687272600.0","comment_id":"928527","content":"Snapshots that are taken from encrypted volumes are automatically encrypted.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html#ebs-create-snapshot-encrypted","upvote_count":"1","poster":"erhard"},{"poster":"hobokabobo","upvote_count":"1","content":"And of course: no need got an ssh key to create a snapshot. Has nothing to do with SSH.","comment_id":"859000","timestamp":"1680450960.0"}],"upvote_count":"4","content":"Selected Answer: C\nIt may be either A or C depending how one reads the question.\nEbs snapshots are stored in S3 buckets however they are Amazon owned buckets not user owned buckets. \nIf one needs to put data in his own user owned bucket I guess A is the way to go.\n\nIf we just need a backup on S3 then C is the way to go. \nI lean towards this interpretation. EBS Snapshot goes to a S3 Bucket. It is an AWS owned Bucket: so what? Its an S3 Bucket which is all we need to achieve. Its a backup to S3. DLM is easy to use, works and most importantly easy to restore. And it is best practice."},{"content":"Selected Answer: A\nA for the answer","upvote_count":"2","poster":"taer","timestamp":"1679215320.0","comment_id":"843559"},{"poster":"taer","timestamp":"1679062140.0","content":"Selected Answer: A\nSelected Answer: A","upvote_count":"2","comment_id":"842040"},{"upvote_count":"2","comment_id":"839588","poster":"aqiao","timestamp":"1678861380.0","content":"Selected Answer: A\nAWS recommended to stop EC2 instances to create root volume snapshot\nWhen you create a snapshot for an EBS volume that serves as a root device, we recommend that you stop the instance before taking the snapshot.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html"},{"poster":"vherman","comment_id":"833967","upvote_count":"6","timestamp":"1678366200.0","content":"Selected Answer: A\nA is correct\nread carefully: a directive from the legal department to back up the data from the instance’s encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket.\n ... to back THE DATA from ... to S3. So, the legal department needs data itself, not the EBS that contains the data. That is why C is wrong!"},{"poster":"anita_student","upvote_count":"2","comment_id":"825220","timestamp":"1677613860.0","content":"Selected Answer: A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/ebs-copy-snapshot-data-s3-create-volume/"},{"upvote_count":"3","poster":"God_Is_Love","comment_id":"823747","content":"Selected Answer: A\nAnswer is A. Little config is needed to setup AWS Systems Manager Session Manager but its worth it here as the question says the app is critical to companies businessOnce you gain ec2 access, you can use all sorts of commands. just do >aws ec2 help and thats where I found this copy-snapshot command.\n\nAws doc about ec2 copy-snapshot command is below :\nDESCRIPTION\n Copies a point-in-time snapshot of an EBS volume and stores it in Ama-\n zon S3. You can copy a snapshot within the same Region, from one Region\n to another, or from a Region to an Outpost. You can't copy a snapshot\n from an Outpost to a Region, from one Outpost to another, or within the\n same Outpost.\n\n You can use the snapshot to create EBS volumes or Amazon Machine Images\n (AMIs).","comments":[{"timestamp":"1677505800.0","upvote_count":"1","poster":"God_Is_Love","comment_id":"823753","content":"moreover, instance does not need to be shut down which is the greatest requirement here as the app is critical to the business here in question."},{"comments":[{"content":"Same here, there is this command https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html but the destination bucket is not under user control.","poster":"Sarutobi","comment_id":"825175","timestamp":"1677610440.0","upvote_count":"1"}],"timestamp":"1677505740.0","poster":"God_Is_Love","upvote_count":"2","comment_id":"823751","content":"I explored DLM but did not find suitable commands to copy to S3 although other options are there to create volumes,snapshots, AMIs etc. So I choose A and not C"}],"timestamp":"1677505620.0"},{"content":"Selected Answer: A\nYes, Systems Manager Session Manager can access an instance without requiring SSH keys.\n\nSession Manager provides a secure, auditable way to access your instances using AWS Identity and Access Management (IAM) roles, rather than relying on SSH keys. When using Session Manager, you can control who has access to your instances, and you can audit their activity.\n\nTo use Session Manager, you need to have the required IAM permissions to start a session with an instance. Once you have those permissions, you can connect to an instance through the AWS Management Console, AWS CLI, or using the AWS SDKs. When you start a session, Session Manager establishes a secure connection between your computer and the instance.\n\nIn summary, Session Manager is an alternative to SSH that uses IAM roles for access control and doesn't require you to manage SSH keys.","upvote_count":"2","timestamp":"1677411360.0","comment_id":"822306","poster":"saurabh1805"},{"poster":"kiran15789","upvote_count":"2","content":"Selected Answer: D\nA- Mission critical application so not feasible\nC - Cant take snaptshot since its encrypted\nD- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonS3.html","timestamp":"1677255240.0","comment_id":"820701"},{"timestamp":"1676585640.0","content":"Selected Answer: A\nIts A - Log on to Instance through SSM. For DLM, it will store on amazon manage S3 and you might not have control on S3......","poster":"spd","upvote_count":"3","comment_id":"811156"},{"poster":"c73bf38","timestamp":"1676523180.0","upvote_count":"3","comment_id":"810264","content":"The answer is A, Amazon Linux 2 is mentioned because by default SSM manager is included with the AMI hence you can access the instances using Systems Manager. Just need to attach a role to the instances to allow it to write to an S3 bucket."},{"poster":"DWsk","timestamp":"1676480040.0","comment_id":"809766","upvote_count":"2","content":"Selected Answer: C\nThe answer is C. DLM is designed for EBS volume backups without downtime.\nThis is a classic case of AWS pointing you to a service that created just for this purpose."},{"upvote_count":"3","comment_id":"804918","content":"Selected Answer: C\nChanged my mind. I like C because it is ideal as an automated backup solution.","timestamp":"1676081100.0","poster":"moota"},{"comment_id":"804917","upvote_count":"2","poster":"moota","content":"Selected Answer: A\nAccording to ChatGPT, Option C, which involves taking a snapshot of the EBS volume using Amazon Data Lifecycle Manager (Amazon DLM) and then copying the data to Amazon S3, does not require the instance to be rebooted, so it should not cause downtime.\n\nHowever, it's worth noting that while the backup is being performed, the EBS volume may experience an increase in I/O latency, which could potentially impact the performance of the application during the backup process. Therefore, it is recommended to perform the backup during a maintenance window or a low-traffic period to minimize the impact on users.","timestamp":"1676080860.0"},{"comment_id":"795816","timestamp":"1675318920.0","poster":"tatdatpham","upvote_count":"2","content":"Selected Answer: A\nThe answer is A\nOption C (Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.) does not meet the requirement of not having the administrative SSH key pair for the instance, which is needed for accessing the data on the EBS volume. The application team does not have access to the instance and cannot take a snapshot or copy the data to Amazon S3."},{"poster":"zozza2023","comment_id":"793238","content":"Selected Answer: C\n\"critical application\" and DLM is designed to protect EC2 EBS any disruption","upvote_count":"4","timestamp":"1675107240.0"},{"timestamp":"1675016100.0","content":"Selected Answer: A\nI don't think C is correct because to copy the data to S3 you would need to create a volume from the snapshot and mount it in an EC2 instance.","comment_id":"791918","poster":"Musk","upvote_count":"4"},{"content":"C, you can’t directly copy from ebs to s3. You need to take a snapshot to copy to s3.","timestamp":"1674932760.0","poster":"irene7","upvote_count":"1","comment_id":"790982"}],"question_text":"A company has a monolithic application that is critical to the company’s business. The company hosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The company’s application team receives a directive from the legal department to back up the data from the instance’s encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the administrative SSH key pair for the instance. The application must continue to serve the users.\n\nWhich solution will meet these requirements?","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/95233-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-01-14 13:23:00","question_id":484,"answers_community":["A (51%)","C (47%)","2%"],"question_images":[],"answer_ET":"A","unix_timestamp":1673698980,"exam_id":33,"answer_description":"","answer_images":[],"choices":{"A":"Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.","D":"Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.","C":"Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.","B":"Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3."},"topic":"1","answer":"A"},{"id":"3HMwVmRJBval6FYhhQTq","answers_community":["BDF (95%)","3%"],"url":"https://www.examtopics.com/discussions/amazon/view/95265-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-01-14 13:54:00","question_text":"A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions architect must implement a solution that uses the AWS CLI.\n\nWhich combination of steps will successfully copy the data? (Choose three.)","exam_id":33,"question_images":[],"question_id":485,"answer":"BDF","discussion":[{"comment_id":"779908","comments":[{"comment_id":"780163","timestamp":"1674056700.0","poster":"masetromain","upvote_count":"5","content":"You are correct, step E should be executed using the IAM user credentials from the destination account. This is because when objects are copied from one bucket to another, the object's permissions (ACLs) are also copied. Therefore, if the objects are copied using the IAM user credentials from the source account, the objects will have the same permissions as they did in the source bucket, which may not include permissions for the user in the destination account. By using the IAM user credentials from the destination account, the objects will have the appropriate permissions for the user in the destination account once they are copied."}],"poster":"icassp","content":"Selected Answer: BDF\n\"The above command should be executed with destination AWS IAM user account credentials only otherwise the copied objects in destination S3 bucket will still have the source account permissions and won’t be accessible by destination account users.\" According to https://medium.com/tensult/copy-s3-bucket-objects-across-aws-accounts-e46c15c4b9e1.","timestamp":"1674040140.0","upvote_count":"27"},{"content":"Selected Answer: BDF\nI switch to BDF;\nStep B is necessary so that the user in the destination account has the necessary permissions to access the source bucket and list its contents, read its objects. \n\nStep D is needed so that the user in the destination account has the necessary permissions to access the destination bucket and list contents, put objects, and set object ACLs\n\nStep F is necessary because the aws s3 sync command needs to be run using the IAM user credentials from the destination account, so that the objects will have the appropriate permissions for the user in the destination account once they are copied.\n\nThe other choices are not correct because :\nA. and C. are about creating policies in the source account but the user who wants to access the data is in the destination account\nE. is about running the command with the source account, which is not suitable because it will lead to copied objects in destination S3 bucket still have the source account permissions and won’t be accessible by destination account users.","upvote_count":"16","timestamp":"1674056820.0","comment_id":"780164","poster":"masetromain"},{"timestamp":"1732047840.0","poster":"jAtlas7","upvote_count":"2","content":"BDF is the answer - see: https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-to-another-account-and-region-by-using-the-aws-cli.html","comment_id":"1314897"},{"comment_id":"1275513","poster":"amministrazione","timestamp":"1725095160.0","content":"B. Create a bucket policy to allow a user in the destination account to list the source bucket’s contents and read the source bucket’s objects. Attach the bucket policy to the source bucket.\nD. Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.\nF. Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data.","upvote_count":"1"},{"timestamp":"1707515880.0","comments":[{"content":"F. Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data. Performing the sync operation as a user in the destination account, who has been granted the appropriate permissions, ensures that the data can be copied from the source bucket to the destination bucket successfully.","upvote_count":"1","poster":"8608f25","timestamp":"1707515880.0","comment_id":"1145823"}],"upvote_count":"1","poster":"8608f25","content":"Selected Answer: BDF\nB. Create a bucket policy to allow a user in the destination account to list the source bucket’s contents and read the source bucket’s objects. Attach the bucket policy to the source bucket. This step ensures that the destination account has the necessary permissions to access the data in the source bucket.\nD. Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user. This step provides the necessary permissions for a user in the destination account to both access the source bucket’s contents and write to the destination bucket.","comment_id":"1145822"},{"upvote_count":"1","timestamp":"1704465420.0","content":"Selected Answer: BDF\nNot A. A bucket policy attached to destination bucket cannot allow the source bucket to execute actions\nNot C. Because we are picking option B which relies on a policy allowing a user in the destination account.\nNot E. Because we are picking options B and D which rely on a user in the destination account","comment_id":"1114565","poster":"ninomfr64"},{"timestamp":"1704211920.0","upvote_count":"1","content":"Selected Answer: BDF\nNo need for more explanations, the ones below are enough.","poster":"jpa8300","comment_id":"1112041"},{"content":"Selected Answer: BDF\nBD:\nhttps://repost.aws/knowledge-center/cross-account-access-s3\nF:\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html","comment_id":"1080514","upvote_count":"1","timestamp":"1700981100.0","poster":"edder"},{"content":"Selected Answer: BDF\nA is incorrect since a bucket policy cannot allow another bucket to do anything. B. Is however an option since you can indeed create a bucket policy to allow a user in another account to perform operations on the bucket.\n\nOnce you have chosen B, then D and F are the only possible choices.","upvote_count":"2","poster":"aviathor","timestamp":"1693487700.0","comment_id":"995136"},{"content":"Selected Answer: BCE\nBCE should also work \nCreate bucket policy at destination bucket to allow permission on source aws user\nCreate IAM policy for source aws user to list/get/put on both buckets\nRun s3 sync command from source bucket to destination bucket","upvote_count":"1","timestamp":"1692259500.0","poster":"H4des","comment_id":"983355"},{"content":"Selected Answer: BDF\nI prefer BDF, I do not know why the correct answer is ADF","comment_id":"972658","timestamp":"1691208600.0","poster":"CuteRunRun","upvote_count":"1"},{"comment_id":"945031","content":"Selected Answer: BDF\nsource bucket: allow destination user + list & get contents permission\ndestination bucket: allow IAM user to get source bucket contents + destination bucket get/list/put objects + aws sync command","upvote_count":"2","poster":"Christina666","timestamp":"1688680440.0"},{"comment_id":"941030","content":"Selected Answer: BDF\nit's BDF for sure","poster":"NikkyDicky","timestamp":"1688313180.0","upvote_count":"1"},{"poster":"Maria2023","upvote_count":"2","comment_id":"928543","content":"Selected Answer: BDF\nThe entire idea of A is wrong (you achieve nothing by giving rights from one bucket to another) so we start from B and the rest are a common sense","timestamp":"1687274700.0"},{"comment_id":"870041","timestamp":"1681457040.0","poster":"huanaws088","upvote_count":"3","content":"Selected Answer: BDF\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-to-another-account-and-region-by-using-the-aws-cli.html"},{"upvote_count":"6","comment_id":"825115","poster":"God_Is_Love","timestamp":"1677607500.0","content":"Logical answer : Who ever uploads to a bucket becomes its owner. So A should ring a flaw in it. Similar issue in C. So straight away, A, C are wrong. that points to B,D to be correct. Refer https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-in-one-account-and-region-to-another-account-and-region.html\n\nNow E or F ? the hint is in D. Destination account user has the necessary privileges to get/put objects permission. So choose destination account or run sync/copy commands. So the answer should be B, D , F"},{"upvote_count":"1","content":"The parts BDF fit together in a way that works. \n\nI think choosing this direction (pulling from the destination account) is slightly more secure than then the other other way round(pushing from source to destination) as only read access is granted to the foreign account but no write access - especially regarding human error: one cannot accidentally tamper with the source, so the worst thing that could happen is that one needs to sync again. The other options don't fit together with other parts.","timestamp":"1677513660.0","poster":"hobokabobo","comment_id":"823918"},{"poster":"zozza2023","upvote_count":"4","comment_id":"793246","content":"Selected Answer: BDF\nBDF are the answers","timestamp":"1675107720.0"},{"comment_id":"776816","upvote_count":"3","poster":"zhangyu20000","content":"BCE\nSource user must have role that can write to destination bucket","timestamp":"1673800680.0"},{"comment_id":"775424","upvote_count":"2","timestamp":"1673700840.0","poster":"masetromain","comments":[{"poster":"Nicocacik","comment_id":"779362","upvote_count":"3","content":"I think that the answer is BDF. If you select steps B and D, you must use a user in the destination account (option F)","timestamp":"1673993040.0"},{"content":"I think it is ADF especially option F as option D is using user in destination account.","comments":[{"poster":"pengpeng","comment_id":"781895","timestamp":"1674190860.0","content":"sorry, typo, BDF","upvote_count":"2"}],"upvote_count":"2","poster":"pengpeng","timestamp":"1674051420.0","comment_id":"780075"},{"comment_id":"777407","content":"If you are specifying Step D where you create an IAM policy in the destination account that allow a user in the destination account to access the source bucket, why are you choosing Step E instead of Step F where it specifies a user on the destination account rather in the source?","upvote_count":"3","poster":"lochesistemas","timestamp":"1673853300.0"}],"content":"Selected Answer: BDE\nThe question is asking for a combination of steps that will successfully copy the data using the AWS CLI.\n\nThe correct answer would be B, D and E.\n\nStep B: You must create a bucket policy in the source account that allows the user in the destination account to list and read the source bucket's contents.\nStep D: You must create an IAM policy in the destination account that allows the user to list, put and set object ACLs in the destination bucket\nStep E: Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.\nBy doing so, the solution architect will be able to copy the data from the source to the destination bucket."}],"answer_ET":"BDF","choices":{"B":"Create a bucket policy to allow a user in the destination account to list the source bucket’s contents and read the source bucket’s objects. Attach the bucket policy to the source bucket.","A":"Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket.","D":"Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.","F":"Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data.","C":"Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user.","E":"Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data."},"topic":"1","answer_images":[],"isMC":true,"unix_timestamp":1673700840,"answer_description":""}],"exam":{"numberOfQuestions":529,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isBeta":false,"provider":"Amazon","isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":true,"id":33},"currentPage":97},"__N_SSP":true}