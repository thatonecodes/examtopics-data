{"pageProps":{"questions":[{"id":"68a26s8I9yTLMIhAxz0f","url":"https://www.examtopics.com/discussions/amazon/view/112883-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"D","answer_description":"","discussion":[{"content":"Selected Answer: D\nThe steps to on How To - \n\nhttps://aws.amazon.com/blogs/storage/recovering-network-file-shares-with-aws-elastic-disaster-recovery-and-aws-datasync/","upvote_count":"1","comment_id":"1174441","timestamp":"1726415880.0","poster":"TonytheTiger"},{"upvote_count":"1","poster":"shaaam80","comment_id":"1084527","content":"Selected Answer: D\nAnswer D","timestamp":"1717077900.0"},{"comment_id":"1078887","timestamp":"1716497220.0","content":"Selected Answer: D\nOption D","poster":"career360guru","upvote_count":"1"},{"content":"Selected Answer: D\nFSX for Windows and Elastic Disaster Recovery","comment_id":"986173","poster":"SK_Tyagi","timestamp":"1708489740.0","upvote_count":"4"},{"upvote_count":"1","timestamp":"1704584340.0","comment_id":"945025","content":"Selected Answer: D\nits a D","poster":"NikkyDicky"},{"upvote_count":"2","content":"Selected Answer: D\nYou need FSx, not EFS and def not S3.","poster":"SmileyCloud","timestamp":"1703522340.0","comment_id":"933692"},{"timestamp":"1703405760.0","poster":"PhuocT","upvote_count":"1","content":"Selected Answer: D\nD is the answer","comment_id":"932250"},{"content":"Selected Answer: D\nD for sure\nB is wrong because you cannot use EFS for Windows EC2 Servers","comment_id":"931985","upvote_count":"1","timestamp":"1703378640.0","poster":"Alabi"},{"comment_id":"930812","upvote_count":"1","content":"D is the right answer","timestamp":"1703271660.0","poster":"MoussaNoussa"},{"poster":"bhanus","timestamp":"1703215500.0","upvote_count":"4","content":"Selected Answer: D\nConsidering RTO and RPO, D is correct answer\nA is incorrect because, thought backups are in s3, its not possible to recover ec2 within 15-minute RTO and a 5-minute RPO","comment_id":"930050"}],"question_id":151,"topic":"1","timestamp":"2023-06-22 03:25:00","choices":{"B":"Create a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by using AWS DataSync. During a disaster, use AWS CodePipeline to deploy the templates to restore the on-premises servers. Fail back the data by using DataSync.","A":"Create an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster, recover the on-premises servers from the backup. During tailback, run the on-premises servers on Amazon EC2 instances.","D":"Use AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file system by using AWS DataSync. Mount the file system to AWS servers. During a disaster, fail over the on-premises servers to AWS. Fail back to new or existing servers by using Elastic Disaster Recovery.","C":"Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into Amazon S3 by using the s3 sync command. During a disaster, swap DNS endpoints to point to AWS. Fail back the data by using the s3 sync command."},"isMC":true,"answer":"D","unix_timestamp":1687397100,"answer_images":[],"question_text":"A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run the application. All the servers mount a common share.\n\nThe company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities.\n\nWhich solution will meet these requirements MOST cost-effectively?","question_images":[],"answers_community":["D (100%)"],"exam_id":33},{"id":"5yUHYe5QmGsAHuBtB2dd","url":"https://www.examtopics.com/discussions/amazon/view/112884-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"ACF","discussion":[{"timestamp":"1724509020.0","content":"Selected Answer: ACF\nA. Ensure the HPC cluster is launched within a single Availability Zone: This choice ensures that the EC2 instances in the cluster have low network latency and high bandwidth, as they are located within the same data center.\nC. Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled: EFA is a network interface that provides low-latency, high-bandwidth communication between EC2 instances. By selecting instance types with EFA enabled, the cluster can benefit from improved inter-instance communication.\nF. Replace Amazon EFS with Amazon FSx for Lustre: Amazon FSx for Lustre is a high-performance file system optimized for HPC workloads. By using FSx for Lustre instead of Amazon EFS, the cluster can achieve better performance for the large number of shared files generated by the workload.\n\nAnd what about a cluster placement group?","poster":"aviathor","upvote_count":"12","comment_id":"989254"},{"timestamp":"1733720040.0","content":"C. Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.\nD. Ensure the cluster is launched across multiple Availability Zones.\nF. Replace Amazon EFS with Amazon FSx for Lustre.","comment_id":"1091460","poster":"duriselvan","upvote_count":"2"},{"comment_id":"1090524","timestamp":"1733596080.0","poster":"srv321","content":"Selected Answer: ACF\nGoing to ACF ,looks logical","upvote_count":"1"},{"upvote_count":"2","comment_id":"1078888","content":"Selected Answer: ACF\nA, C, F","poster":"career360guru","timestamp":"1732402200.0"},{"comment_id":"945028","timestamp":"1720302300.0","upvote_count":"1","poster":"NikkyDicky","content":"Selected Answer: ACF\nACF for performance"},{"upvote_count":"1","poster":"bhanus","timestamp":"1719720180.0","content":"Selected Answer: ACF\n@MODERATOR - Please remove my previous comment. I agree with ACF. Thank you MoussaNoussa for clarifying","comment_id":"938790"},{"upvote_count":"1","content":"Selected Answer: ACF\nACF is the correct answer","poster":"javitech83","comment_id":"938361","timestamp":"1719678120.0"},{"timestamp":"1719339180.0","poster":"SkyZeroZx","upvote_count":"1","comment_id":"933806","content":"Selected Answer: ACF\nA, C and F","comments":[{"comment_id":"933809","timestamp":"1719339300.0","upvote_count":"1","content":"B ) Not is correct because ENI not more performance in this case with HPC Cluster\nD ) sonds good but not is good option because performance is required in same AZ is the cluster placement group strategy more adecuate \nE ) replace EFS by EBS not is apropiate for performance","poster":"SkyZeroZx"}]},{"comment_id":"933701","upvote_count":"3","content":"Selected Answer: ACF\nA - Single AZ is better than multi AZ for performance\nC - Use EFA. https://aws.amazon.com/hpc/efa/ - It tells you that's HPC is a use case.\nF - Use FSx for Lustre - https://aws.amazon.com/fsx/lustre/. HPC is a use case.","poster":"SmileyCloud","timestamp":"1719327060.0"},{"timestamp":"1719248520.0","poster":"PhuocT","upvote_count":"1","comment_id":"932793","content":"Selected Answer: ACF\nA, C and F"},{"content":"Selected Answer: ACF\nACF is the correct answer","upvote_count":"1","comment_id":"932165","timestamp":"1719203940.0","poster":"ozelllll"},{"timestamp":"1719169860.0","comment_id":"931890","poster":"easytoo","content":"a-c-f...a-c-f...a-c-f\n\nTo achieve maximum performance from the HPC cluster, the following design choices should be made:\n\nA. Ensure the HPC cluster is launched within a single Availability Zone: This choice ensures that the EC2 instances in the cluster have low network latency and high bandwidth, as they are located within the same data center.\n\nC. Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled: EFA is a network interface that provides low-latency, high-bandwidth communication between EC2 instances. By selecting instance types with EFA enabled, the cluster can benefit from improved inter-instance communication.\n\nF. Replace Amazon EFS with Amazon FSx for Lustre: Amazon FSx for Lustre is a high-performance file system optimized for HPC workloads. By using FSx for Lustre instead of Amazon EFS, the cluster can achieve better performance for the large number of shared files generated by the workload.","upvote_count":"2"},{"poster":"nexus2020","upvote_count":"1","comment_id":"930952","timestamp":"1719083640.0","content":"Selected Answer: CDF\nB: more interface does not mean faster. so B is not a good choice.\nE: RAID? is often not recommended on Cloud Platform, aws has already raid the drive for you underlay. \nA: HPC recommended to use multiregion. \n\nso CDF"},{"timestamp":"1719075780.0","content":"ACF is the right answer","comment_id":"930814","poster":"MoussaNoussa","upvote_count":"2"},{"comment_id":"930051","content":"Selected Answer: CDF\nCDF are correct\n\nC - EFA provides low-latency and high-bandwidth communication between EC2 instances. It can optimize the network performance of the HPC cluster.\nD - Launching the HPC cluster across multiple Availability Zones allows you to distribute the workload and resources, reducing the chances of a single point of failure and increasing overall performance.\nF - FSx for Lustre is a high-performance file system optimized for HPC workloads.","upvote_count":"1","comments":[{"content":"changing my vote to ACF as per below suggestion","upvote_count":"1","timestamp":"1719721680.0","poster":"bhanus","comment_id":"938813"},{"upvote_count":"5","poster":"MoussaNoussa","content":"performance is the main goal. so running HPC in the same AZ is the right choice here","comments":[{"poster":"bhanus","upvote_count":"1","timestamp":"1719423300.0","content":"Thank you @ MoussaNoussa for clarifying. Agreed.","comment_id":"934679"}],"comment_id":"930820","timestamp":"1719075900.0"}],"timestamp":"1719019680.0","poster":"bhanus"}],"answer_description":"","question_id":152,"topic":"1","timestamp":"2023-06-22 03:28:00","choices":{"F":"Replace Amazon EFS with Amazon FSx for Lustre.","B":"Launch the EC2 instances and attach elastic network interfaces in multiples of four.","A":"Ensure the HPC cluster is launched within a single Availability Zone.","C":"Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.","D":"Ensure the cluster is launched across multiple Availability Zones.","E":"Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array."},"isMC":true,"answer":"ACF","unix_timestamp":1687397280,"answer_images":[],"question_images":[],"question_text":"A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1.000 EC2 instances, overall performance was well below expectations.\n\nWhich collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)","answers_community":["ACF (92%)","8%"],"exam_id":33},{"id":"6rKKbFoO3RS6nj5bWtDc","exam_id":33,"answer_images":[],"question_text":"A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire organization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique tag values.\n\nWhich solution will meet these requirements?","timestamp":"2023-06-22 03:33:00","answers_community":["A (84%)","B (16%)"],"url":"https://www.examtopics.com/discussions/amazon/view/112885-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"A","isMC":true,"choices":{"A":"Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.","C":"Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.","D":"Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs.","B":"Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization's management account."},"discussion":[{"upvote_count":"8","timestamp":"1717937460.0","comment_id":"1091861","poster":"duriselvan","content":"The most suitable solution for applying standardized tags across the organization with specific values for each OU is A. Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values for each OU. Attach the tag policies to the OUs.\n\nHere's why:\n\nEnforce tag standardization: An SCP applied to the entire organization denies resource creation unless the required tags are present, ensuring consistent tagging across all accounts.\n\nOU-specific tags: Tag policies attached to each OU define the specific tag values for that OU, allowing customization without compromising overall standardization.\n\nGranular control: Attaching tag policies to OUs instead of the management account provides more granular control and flexibility for managing tags within each OU."},{"poster":"Maria2023","upvote_count":"6","comment_id":"935552","content":"Selected Answer: A\nYou go to the management account -> Organizations console -> Policies -> Tag policies -> \"name of the policy\" -> attach to OU. That's it - A is correct","timestamp":"1703698680.0"},{"comment_id":"1078893","content":"Selected Answer: A\nOption A","upvote_count":"1","timestamp":"1716497760.0","poster":"career360guru"},{"upvote_count":"2","poster":"nicecurls","content":"Selected Answer: A\nFOR EACH OU's","timestamp":"1704800580.0","comment_id":"947063"},{"upvote_count":"1","poster":"NikkyDicky","content":"Selected Answer: A\nit's an A","timestamp":"1704585480.0","comment_id":"945034"},{"comment_id":"944630","upvote_count":"1","poster":"dkx","comments":[{"comment_id":"959387","poster":"santi1975","upvote_count":"4","comments":[{"upvote_count":"2","content":"Sorry, I mean cannot be B, and the correct answer is A!","comment_id":"959388","poster":"santi1975","timestamp":"1705923480.0"}],"content":"The question clearly says \"Each of the company's OUs will have unique tag values\", you cannot inherit what is different. The answer is B","timestamp":"1705923420.0"},{"poster":"43c89f4","timestamp":"1730297220.0","upvote_count":"2","content":"The Question mentions \"Each of the company's OUs will have unique tag values.\" the values list will change for OU's \n\nMy answer is A","comment_id":"1204551"}],"timestamp":"1704550620.0","content":"The correct answer is B.\n\nImagine if you had an AWS Organization with 50+ OUs, it would be very inefficient to manually apply a generic tagging policy to each OU, so that's why there is the concept of policy inheritance: when you attach a policy to the organization root, all OUs and accounts in the organization inherit that policy\n\nWhen you attach a tag policy to your organization root, the tag policy applies to all of that root's member OUs and accounts.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/attach-tag-policy.html\n\nUnderstanding policy inheritance: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance.html"},{"poster":"Piccaso","upvote_count":"1","timestamp":"1704364260.0","content":"Selected Answer: A\nC and D must be wrong, because of \"allow ... \"\nB is weird.","comment_id":"942521"},{"timestamp":"1704246720.0","poster":"SkyZeroZx","upvote_count":"1","comment_id":"941314","content":"Selected Answer: A\nEach of the company's OUs will have unique tag values.\nThen A because each OU unique tags A is the unique with approved this case"},{"timestamp":"1703625780.0","comment_id":"934738","poster":"SmileyCloud","content":"Selected Answer: A\nIt's A. The policies are different for each account, so you can't assign it to the management account. Exact same scenario: https://aws.amazon.com/blogs/mt/implement-aws-resource-tagging-strategy-using-aws-tag-policies-and-service-control-policies-scps/","upvote_count":"3"},{"comment_id":"934671","content":"Selected Answer: A\nMODERATOR - Please remove my previous comment. From the discussion it looks like A is the answer. Looks like the tag policies should be attached at the OU level to ensure that each OU has its own unique tag values.","poster":"bhanus","upvote_count":"1","timestamp":"1703618700.0"},{"poster":"PhuocT","timestamp":"1703444640.0","comment_id":"932794","upvote_count":"2","content":"I think it's A"},{"poster":"gd1","upvote_count":"2","timestamp":"1703371980.0","content":"GPT 4. 0 says A - I agree. Values per OU","comment_id":"931946"},{"timestamp":"1703365920.0","poster":"easytoo","comment_id":"931893","content":"b-b-b-b-b-b","upvote_count":"1"},{"upvote_count":"1","timestamp":"1703272440.0","comment_id":"930822","poster":"MoussaNoussa","content":"option A is the right answer, we need a have a list of allowed tag values per OU"},{"timestamp":"1703215980.0","comments":[{"poster":"bhanus","upvote_count":"1","timestamp":"1703917680.0","content":"changing my vote to A. The policies are different for each account, so you can't assign it to the management account.","comment_id":"938812"}],"content":"Selected Answer: B\nB - you don't have apply SCPs to each account or OU. Attaching the tag policies to the organization's management account ensures that the policies are applied consistently to all OUs within the organization.\nC is incorrect because SCP are NOT used for ALLOW action. They are used for DENY actions (setting boundaries)","comment_id":"930052","poster":"bhanus","upvote_count":"3"}],"question_images":[],"question_id":153,"topic":"1","answer_description":"","unix_timestamp":1687397580,"answer_ET":"A"},{"id":"9YVhBiOUc9YUx4cnwght","url":"https://www.examtopics.com/discussions/amazon/view/112886-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry Transport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket.\n\nRecently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new design on AWS that is highly available and scalable to prevent a similar occurrence.\n\nWhich solution will meet these requirements?","answer_images":[],"timestamp":"2023-06-22 03:38:00","answers_community":["C (87%)","13%"],"question_id":154,"answer":"C","choices":{"D":"Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS IoT Core.","C":"Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core.","A":"Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy. Route the sensors to send the data to the domain name.","B":"Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer (NLB) that points to the Amazon MSK broker. Enable NLB health checks. Route the sensors to send the data to the NLB."},"answer_description":"","question_images":[],"topic":"1","isMC":true,"exam_id":33,"answer_ET":"C","unix_timestamp":1687397880,"discussion":[{"upvote_count":"7","comment_id":"986178","timestamp":"1692585720.0","poster":"SK_Tyagi","content":"Selected Answer: C\nOption B is missing the Data Transformation to be done by Lambda"},{"timestamp":"1692106860.0","content":"Selected Answer: C\nC, because it said new design and obviously IoT is what aws recommend.","upvote_count":"6","poster":"softarts","comment_id":"981715"},{"comment_id":"1267411","content":"Must be C.\nhttps://aws.amazon.com/tw/blogs/iot/building-an-iot-solution-to-securely-transmit-mqtt-messages-under-private-networks/","poster":"Linuslin","upvote_count":"1","timestamp":"1723861800.0"},{"upvote_count":"1","comment_id":"1259284","timestamp":"1722504120.0","poster":"8693a49","content":"Selected Answer: C\nOption B could also work. The key why it is not correct is because it says there is a single broker, so it will not be HA, therefore it won't prevent the crashing problem. So C is the correct option."},{"content":"Selected Answer: C\nMSK can't transforms the data","poster":"tmlong18","upvote_count":"5","timestamp":"1705223940.0","comment_id":"1122417"},{"comment_id":"1094678","upvote_count":"1","content":"b ANS\nB. Amazon MSK with NLB:\n\nPros:\nHighly available and managed Kafka service.\nScalable to accommodate increasing data volume.\nNLB automatically distributes sensor data across healthy brokers.\nCons:\nRequires migration from on-premises Kafka server.\nPotential cost increase for managed service.","poster":"duriselvan","timestamp":"1702395840.0"},{"upvote_count":"1","comment_id":"1078894","poster":"career360guru","content":"Selected Answer: C\nOption C.","timestamp":"1700780280.0"},{"content":"C :Anshttps://docs.aws.amazon.com/lambda/latest/dg/services-kinesisfirehose.html","timestamp":"1695651720.0","poster":"duriselvan","upvote_count":"2","comment_id":"1016883"},{"comment_id":"976255","content":"Selected Answer: C\nAnswer: C\n\nTo me C is still the best option as it is not wrong and there is an uncertainty regarding NLB support for MQTT protocol. You can, yes, however, not out of the box, you would need solutions like HiveMQ, for example: https://github.com/mqtt/mqtt.org/wiki/Server%20support\n\nNow, when I read this part of the question \"Recently, the Kafka server crashed. The company lost sensor data while the server was being restored\", to me it seems that it would be OK for the company to look for different ways in having their data stored in S3, be it using a Kafka server or not.\n\nTherefore and, just because the question doesn't say anything regarding cost effectiveness, least operational overhead, least dev overhead and so on, it's safe to assume (to me) that IoT Core would be the option AWS wants us to think about.","timestamp":"1691557320.0","poster":"chico2023","upvote_count":"3"},{"poster":"andy7t","timestamp":"1690537800.0","upvote_count":"2","content":"Selected Answer: B\nBoth B and C will work? \nNLB + MSK is a well defined pattern. MSK is highly available and scaleable. MQTT will pass through NLB as it's just a network port. \nNo changes to the application. \n\nC would also work, but seems to involve more refactoring.","comment_id":"965454"},{"upvote_count":"2","timestamp":"1690257960.0","content":"Selected Answer: B\nIt's B,\nbecause MSK can handle the lightweight MQTT protocol.","comment_id":"962319","poster":"Just_Ninja"},{"comment_id":"945039","poster":"NikkyDicky","upvote_count":"1","timestamp":"1688680980.0","content":"Selected Answer: C\nits a C\nmqtt->IoT core"},{"poster":"javitech83","timestamp":"1688056380.0","content":"Selected Answer: C\nIoT perfect for MQTT. Option D could have the same problem as on-premises","comment_id":"938369","upvote_count":"2"},{"upvote_count":"3","timestamp":"1687808160.0","content":"Selected Answer: C\nIt's C. Anytime you see sensors, your best bet is IoT. It's not D because you'll have one Kafka EC2 instance and it's not HA.","comment_id":"934743","poster":"SmileyCloud"},{"poster":"bhanus","comment_id":"934683","upvote_count":"1","content":"Selected Answer: C\nMODERATOR - please remove my previous comment. Looks is C is correct answer","timestamp":"1687801260.0"},{"poster":"SkyZeroZx","comment_id":"933810","timestamp":"1687717080.0","content":"Selected Answer: C\nIOT core is designed to handle this. and NLB does not support MQTT.","upvote_count":"1"},{"poster":"PhuocT","upvote_count":"2","timestamp":"1687588020.0","content":"Agree with C","comment_id":"932254"},{"upvote_count":"3","content":"Selected Answer: C\nIOT core is designed to handle this. and NLB does not support MQTT.","poster":"nexus2020","timestamp":"1687521420.0","comments":[{"comment_id":"931948","content":"Agree and IPT Core supports MQTT","upvote_count":"1","poster":"gd1","timestamp":"1687553760.0"}],"comment_id":"931553"},{"poster":"MoussaNoussa","timestamp":"1687454700.0","comment_id":"930837","upvote_count":"2","content":"C is the correct Answer"},{"comments":[{"poster":"Just_Ninja","timestamp":"1690257900.0","comment_id":"962317","content":"Don't switch to C, B is right. MSK can handle MQTT. No special IOT Service is needed.","upvote_count":"1"},{"timestamp":"1687454820.0","comments":[{"comment_id":"934681","poster":"bhanus","content":"Thank you again MoussaNoussa. I might be wrong. I have read NLB supports MQTT and hence chose this answer. But I am also unsure","timestamp":"1687801020.0","upvote_count":"1"}],"content":"This is incorrect, the solution needs to use IoT Core","poster":"MoussaNoussa","upvote_count":"1","comment_id":"930839"},{"timestamp":"1688099220.0","poster":"bhanus","content":"changing my vote to C as per below suggestion","upvote_count":"1","comment_id":"938811"}],"upvote_count":"1","poster":"bhanus","timestamp":"1687397880.0","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/big-data/secure-connectivity-patterns-to-access-amazon-msk-across-aws-regions/","comment_id":"930054"}]},{"id":"uSgjGFvG6rKKL1oC8Buj","question_images":[],"exam_id":33,"isMC":true,"answer_images":[],"answer_ET":"ABD","timestamp":"2023-06-22 03:46:00","answer":"ABD","question_id":155,"answers_community":["ABD (100%)"],"answer_description":"","unix_timestamp":1687398360,"choices":{"F":"Set up RDS snapshots on each database.","B":"Configure an AWS Backup plan to copy backups to another Region.","D":"Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED.","C":"Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.","E":"Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.","A":"Create an AWS Backup plan with a backup rule for each of the retention requirements."},"question_text":"A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances. Amazon Elastic File System (Amazon EFS) file systems, and Amazon RDS DB instances.\n\nTo meet regulatory and business requirements, the company must make the following changes for data backups:\n\n• Backups must be retained based on custom daily, weekly, and monthly requirements.\n• Backups must be replicated to at least one other AWS Region immediately after capture.\n• The backup solution must provide a single source of backup status across the AWS environment.\n• The backup solution must send immediate notifications upon failure of any resource backup.\n\nWhich combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/112887-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"upvote_count":"8","poster":"bhanus","comment_id":"930058","timestamp":"1703216760.0","content":"Selected Answer: ABD\nABD\nE is incorrect because Amazon Data Lifecycle Manager to used to automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs. It CANNOT be used for backups for ec2, EFS, RDS\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/snapshot-lifecycle.html"},{"content":"Selected Answer: ABD\nAWS Backup Plan - https://docs.aws.amazon.com/aws-backup/latest/devguide/about-backup-plans.html \n\nBackup Copy across AWS Regions - https://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html\n\nBackup across AWS regions video - https://www.youtube.com/watch?v=qMN18Lpj3PE","comment_id":"1174449","timestamp":"1726417500.0","poster":"TonytheTiger","upvote_count":"1"},{"comment_id":"1078895","timestamp":"1716498120.0","content":"Selected Answer: ABD\nOptions A B D","upvote_count":"2","poster":"career360guru"},{"timestamp":"1704586020.0","content":"Selected Answer: ABD\nits ABD","poster":"NikkyDicky","upvote_count":"2","comment_id":"945040"},{"poster":"SkyZeroZx","upvote_count":"2","comment_id":"941317","content":"Selected Answer: ABD\nABD. You don't need Lambda for cross-region backup. You don't need RDS snaps.","timestamp":"1704246960.0"},{"upvote_count":"4","poster":"SmileyCloud","comment_id":"934746","timestamp":"1703626980.0","content":"Selected Answer: ABD\nABD. You don't need Lambda for cross-region backup. You don't need RDS snaps."},{"poster":"easytoo","timestamp":"1703366280.0","comment_id":"931896","content":"a-b-d...a-b-d","upvote_count":"1"},{"poster":"MoussaNoussa","upvote_count":"2","comment_id":"930853","timestamp":"1703273880.0","content":"ABD is the correct answer"}]}],"exam":{"provider":"Amazon","id":33,"isImplemented":true,"isBeta":false,"isMCOnly":true,"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional SAP-C02","numberOfQuestions":529},"currentPage":31},"__N_SSP":true}