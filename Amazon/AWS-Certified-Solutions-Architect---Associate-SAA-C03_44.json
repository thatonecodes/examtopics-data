{"pageProps":{"questions":[{"id":"KtwL72TpjeU0Epz3xD8D","question_text":"A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning.\n\nHow should the scaling be changed to address the staff complaints and keep costs to a minimum?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/99584-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":216,"answer_description":"","question_images":[],"answer":"C","unix_timestamp":1676625660,"choices":{"A":"Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.","B":"Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.","D":"Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens.","C":"Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period."},"timestamp":"2023-02-17 10:21:00","answer_images":[],"answers_community":["C (66%)","A (34%)"],"discussion":[{"upvote_count":"31","comment_id":"841416","timestamp":"1679009340.0","comments":[{"poster":"c10356a","comment_id":"1104783","upvote_count":"2","comments":[{"timestamp":"1703835240.0","comment_id":"1108431","poster":"pentium75","upvote_count":"4","content":"There is a cooldown period in the auto-scaling group, which helps 'keeping costs to a minimum' as instances would be removed sooner."}],"content":"There is no cooldown period in target tracking, but warm-up time.","timestamp":"1703443500.0"},{"timestamp":"1683616620.0","comments":[{"poster":"TheFivePips","comment_id":"1159969","timestamp":"1708969800.0","upvote_count":"3","content":"From what I can tell, you must specify an end time, or else it will run indefinitly. So I think A would be right, if they specified an end time. Otherwise C is more cost effective\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html"},{"content":"This is right, setting desired capacity doesn't turn off autoscaling policies","timestamp":"1700845320.0","comment_id":"1079435","poster":"xdkonorek2","upvote_count":"3"}],"content":"If it stars with 20 instances it will not keep it all day. It will scale down based on demand. The scheduled action in Option A simply ensures that there are enough instances running to handle the increased traffic when the day begins, while still allowing the Auto Scaling group to scale up or down based on demand during the rest of the day. https://docs.aws.amazon.com/autoscaling/ec2/userguide/scale-your-group.html","poster":"mandragon","comment_id":"892879","upvote_count":"10"}],"poster":"asoli","content":"Selected Answer: C\nAt first, I thought the answer is A. But it is C.\n\nIt seems that there is no information in the question about CPU or Memory usage. \nSo, we might think the answer is A. why? because what we need is to have the required (desired) number of instances. It already has scheduled scaling that works well in this scenario. Scale down after working hours and scale up in working hours. So, it just needs to adjust the desired number to start from 20 instances.\n\nBut here is the point it shows A is WRONG!!!\nIf it started with desired 20 instances, it will keep it for the whole day. What if the load is reduced? We do not need to keep the 20 instances always. That 20 is the MAXIMUM number we need, no the DESIRE number. So it is against COST that is the main objective of this question.\n\nSo, the answer is C"},{"content":"Selected Answer: C\nC. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.\n\nHere's the reasoning:\n\n Target Tracking Scaling Policy: With a target tracking scaling policy, you can set a target value for a specific metric, such as CPU utilization. The Auto Scaling group then adjusts the capacity to maintain that target.\n\n Lower CPU Threshold: By triggering the target tracking action at a lower CPU threshold, the Auto Scaling group can proactively add instances when the workload increases, helping to address the slowness at the beginning of the day.\n\n Decrease Cooldown Period: Reducing the cooldown period allows the Auto Scaling group to scale in and out more rapidly, making adjustments quicker in response to changing demand.","poster":"meowruki","upvote_count":"6","comment_id":"1083935","timestamp":"1701312120.0","comments":[{"upvote_count":"2","poster":"meowruki","content":"Options A and D involve scheduled actions, which are time-based and may not be as responsive to immediate changes in demand. They also do not dynamically respond to varying workloads.","comment_id":"1083936","timestamp":"1701312120.0"}]},{"poster":"zdi561","comment_id":"1350717","content":"Selected Answer: A\nDecreasing cpu threshold could cause more running instance(might have more than two at night) therefore higher cost. Schedule at desired to 20 minutes before the day will solve the issue that the app is very slow at begininng of the day","upvote_count":"1","timestamp":"1738546440.0"},{"poster":"LeonSauveterre","timestamp":"1732523940.0","content":"Selected Answer: A\nExam-wise, you gotta choose A.\n\nThe problem is explicitly described as slowness at the start of the day, which implies a predictable traffic pattern. Option C argues for a responsive approach (reacting dynamically to real-time demand changes) rather than a predictive approach (pre-scaling based on a known schedule).\n\nAlso, option A can minimize costs compared to option D as well, because the desired capacity is a flexible number that the Auto Scaling group targets at specific times but doesn’t enforce as a hard minimum.\n\nIf the emphasis is on handling varying traffic patterns throughout the day, Option C could be considered viable.","comment_id":"1317391","upvote_count":"1"},{"timestamp":"1725150600.0","content":"Question is about cost effectiveness hence Use target tracking scaling policies to maintain a specific metric, such as CPU utilization or request count per target. This allows the Auto Scaling group to dynamically adjust the number of instances based on real-time demand. We do not need to have 20 instances up and running during start of the day.","poster":"AbhiBK","comment_id":"1275759","upvote_count":"2"},{"poster":"maryam_sh","content":"Selected Answer: A\nNot C because: target tracking scaling attempts to maintain a target metric (like average CPU utilization). While more responsive, it would still react to increased load rather than pre-scaling. The initial slow period would persist as the scaling reacts to the increased demand rather than anticipating it.","timestamp":"1724858340.0","comment_id":"1274167","upvote_count":"2"},{"content":"Selected Answer: A\nA for sure.","timestamp":"1720309620.0","upvote_count":"3","poster":"ChymKuBoy","comment_id":"1243607"},{"upvote_count":"1","comment_id":"1129153","timestamp":"1705973280.0","poster":"foha2012","content":"Answer is A. Makes more sense to me."},{"content":"Selected Answer: C\nA is not cost effective, it would set the number of instances to maximum even before the first employee arrives. \nD is not cost effective, it would cause the permanent use of 20 instances\n\nB could almost work, but if you configure small steps then it scales too slowly in the morning; if you configure big steps (like \"add 8 instances at a time\") it would scale in the morning but not be cost-efficient during the day.\n\nC would address the requirement, it would scale to meet a certain CPU utilization. Decreasing the cooldown period (which is not possible for the scaling policy itself but for the auto-scaling group) would help 'keeping costs to a minimum'.","upvote_count":"3","comment_id":"1108427","poster":"pentium75","timestamp":"1703835120.0"},{"content":"Selected Answer: A\nThe question 369 is exactly the same problem,\nSince a scheduled scaling doesn't disable the autoscaling later in the day the A works perfectly well.","poster":"Mikado211","upvote_count":"3","comment_id":"1098404","timestamp":"1702751520.0"},{"comments":[{"content":"How would scaling up to the maximum number of instances \"keep costs to a minimum\"?","timestamp":"1703834520.0","poster":"pentium75","comment_id":"1108412","upvote_count":"2"}],"comment_id":"1095971","upvote_count":"1","timestamp":"1702521960.0","poster":"Cyberkayu","content":"A. since only a boot storm issue at 9am and settle down in mid morning, 20 instance is enough to support the workload\nNOT C. Reduce threshold to trigger (lets say 50% from 80% utilization) and lower cool down period, will still take time to ramp up to max 20 instance."},{"timestamp":"1702189680.0","comments":[{"poster":"pentium75","comment_id":"1108428","content":"WOuld not keep costs to a minimum","timestamp":"1703835180.0","upvote_count":"2"}],"comment_id":"1092301","upvote_count":"4","content":"Selected Answer: A\nI would go with A. Autoscaling is still there and the problem is clearly in morning.","poster":"[Removed]"},{"content":"My mistake, I should have chosen c. A lower threshold can expand in advance, and lowering cooling can increase the expansion frequency.","timestamp":"1700087280.0","poster":"wearrexdzw3123","upvote_count":"3","comment_id":"1071969"},{"poster":"wearrexdzw3123","content":"Selected Answer: A\nI choose option A because the root of the problem is the inability of the scaling speed in the morning to meet the demand, rather than what criteria to use for scaling.","upvote_count":"1","comments":[],"timestamp":"1699751160.0","comment_id":"1068249"},{"comments":[{"timestamp":"1695964200.0","poster":"TariqKipkemei","upvote_count":"2","content":"Option C is best","comment_id":"1020471"}],"comment_id":"1020469","timestamp":"1695964140.0","poster":"TariqKipkemei","upvote_count":"4","content":"To keep costs to a minimum target tracking is the best option.\nFor example the scaling metric is the average CPU utilization of the EC2 auto scaling instances, and their average during the day should always be 80%. When CloudWatch detects that the average CPU utilization is beyond 80% at start of day, it will trigger the target tracking policy to scale out the auto scaling group to meet this target utilization. Once everything is settled and the average CPU utilization has gone below 80% at night, another scale in action will kick in and reduce the number of auto scaling instances in the auto scaling group."},{"timestamp":"1695285960.0","comment_id":"1012931","comments":[],"poster":"Ramdi1","upvote_count":"3","content":"Selected Answer: A\nI am going A based on it stating upto 20 so you already know what they maximum they use which is n a sense consistent. however i can see why people have put C. I think they need more clarification on the questions."},{"timestamp":"1694700240.0","comment_id":"1007721","content":"Selected Answer: A\nA. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.","poster":"Uzbekistan","upvote_count":"2","comments":[{"upvote_count":"2","poster":"pentium75","timestamp":"1703834580.0","content":"How would scaling up to the maximum number of instances \"keep costs to a minimum\"?","comment_id":"1108417"}]},{"content":"CHATGPT says Answers is A\nA. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.","upvote_count":"1","poster":"Uzbekistan","comment_id":"1007718","timestamp":"1694700180.0"},{"poster":"BrijMohan08","upvote_count":"2","comment_id":"1000028","timestamp":"1693960860.0","content":"Selected Answer: A\nScaling Out: In the morning when you schedule the AWS EC2 scaling to have a minimum and maximum of 20 instances, if the load on your application increases beyond the current number of instances, AWS Auto Scaling will automatically launch new instances to meet the demand up to the maximum of 20 instances.\n\nScaling In: As the load on your application decreases in the afternoon or night, AWS Auto Scaling will continuously monitor the health and load of your instances. If the instances are underutilized and can be terminated without affecting your application's performance, AWS Auto Scaling will automatically scale in by terminating excess instances, \n\nWhy not D? If you specify the min instance, AWS will always keep the minimum number of instances (20 in this case) running.","comments":[{"poster":"JA2018","upvote_count":"1","timestamp":"1732027260.0","comment_id":"1314711","content":"borrowing pentium75's oft-repeated words for this thread ....\n\nHow would scaling up to the maximum number of instances \"keep costs to a minimum\"?"}]},{"poster":"LazyTs","upvote_count":"1","timestamp":"1693916520.0","content":"It's A, C will not be fast enough with the sudden influx of the users, if C is fast enough then the original scenario should already be good enough as the 20 is already the max which set to start at working hours(when CPU starts to spin up)","comment_id":"999483","comments":[]},{"timestamp":"1691506260.0","poster":"kapalulz","content":"Selected Answer: C\nC. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period","upvote_count":"2","comment_id":"975773"},{"timestamp":"1688773440.0","content":"Selected Answer: C\nI was in team A. But from the definition of desired capacity, it seems once we set it as 20, it will try to keep it as 20 which is not saving cost.\n\nDesired capacity: Represents the initial capacity of the Auto Scaling group at the time of creation. An Auto Scaling group attempts to maintain the desired capacity. It starts by launching the number of instances that are specified for the desired capacity, and maintains this number of instances as long as there are no scaling policies or scheduled actions attached to the Auto Scaling group.","poster":"Mia2009687","comment_id":"946046","upvote_count":"3"},{"comment_id":"912951","timestamp":"1685724120.0","poster":"DrWatson","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/consolidated-view-of-warm-up-and-cooldown-settings.html\nDefaultCooldown\nOnly needed if you use simple scaling policies.\nAPI operation: CreateAutoScalingGroup, UpdateAutoScalingGroup\nThe amount of time, in seconds, between one scaling activity ending and another one starting due to simple scaling policies. For more information, see Scaling cooldowns for Amazon EC2 Auto Scaling (https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scaling-cooldowns.html)\nDefault: 300 seconds.","comments":[],"upvote_count":"2"},{"comment_id":"902980","comments":[],"timestamp":"1684652580.0","upvote_count":"1","content":"Selected Answer: A\nI think the \"cost\" part that talks against A is a catch. No information why the EC2s are slow - maybe it's not CPU? \n\nOn the other hand we know that \"Auto Scaling group scales up to 20 instances during work hours\". A seems to be the only option that kinda satisfies requirements.","poster":"Konb"},{"upvote_count":"1","poster":"xmark443","timestamp":"1684257960.0","content":"There may be days when the demand is lower. So schedule scaling is more cost than target tracking.","comment_id":"899426"},{"timestamp":"1683825000.0","upvote_count":"1","content":"Selected Answer: A\nHave to go with A on this one","poster":"justhereforccna","comments":[],"comment_id":"895248"},{"upvote_count":"5","content":"Selected Answer: C\nThis option will scale up capacity faster in the morning to improve performance, but will still allow capacity to scale down during off hours. It achieves this as follows:\n• A target tracking action scales based on a CPU utilization target. By triggering at a lower CPU threshold in the morning, the Auto Scaling group will start scaling up sooner as traffic ramps up, launching instances before utilization gets too high and impacts performance.\n• Decreasing the cooldown period allows Auto Scaling to scale more aggressively, launching more instances faster until the target is reached. This speeds up the ramp-up of capacity.\n• However, unlike a scheduled action to set a fixed minimum/maximum capacity, with target tracking the group can still scale down during off hours based on demand. This helps minimize costs.","poster":"kruasan","timestamp":"1682781360.0","comment_id":"884424"},{"timestamp":"1681265340.0","content":"Selected Answer: A\nI'm going with A - it tells us that 20 instances is the normal capacity during the work day - so scheduling that at the start of the work day means you don't need to put load on the system to trigger scale-out. So this is like a warm start. Cool down has nothing to do with anything and it doesn't mention anything about CPU/resources for target setting.","upvote_count":"1","comments":[{"poster":"JA2018","upvote_count":"1","content":"borrowing pentium75's oft-repeated words for this thread ....\n\nHow would scaling up to the maximum number of instances \"keep costs to a minimum\"?","timestamp":"1732027320.0","comment_id":"1314716"}],"comment_id":"867806","poster":"Dr_Chomp"},{"upvote_count":"2","poster":"kraken21","timestamp":"1680291780.0","content":"Selected Answer: C\nHow should the scaling be changed to address the staff complaints and keep costs to a minimum? \"Option C\" scaling based on metrics and with the combination of reducing the cooldown the cost part is addressed.","comment_id":"857400"},{"timestamp":"1679355720.0","comment_id":"845358","content":"Selected Answer: A\nI will go with A based on this \"The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight.\"\nSetting the instances to 20 before the office hours start should address the issue.","upvote_count":"1","poster":"[Removed]","comments":[{"comment_id":"857398","poster":"kraken21","content":"How about the cost part :\"How should the scaling be changed to address the staff complaints and keep costs to a minimum?\". By scaling to 20 instances you are abusing instance cost. C is a better option.","upvote_count":"2","timestamp":"1680291660.0"}]},{"upvote_count":"4","poster":"FourOfAKind","timestamp":"1677929280.0","comment_id":"828852","content":"Selected Answer: C\nWith step scaling and simple scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that invoke the scaling process. You also define how your Auto Scaling group should be scaled when a threshold is in breach for a specified number of evaluation periods.\n\nWe strongly recommend that you use a target tracking scaling policy to scale on a metric like average CPU utilization or the RequestCountPerTarget metric from the Application Load Balancer.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html"},{"comments":[],"content":"Selected Answer: A\nI vote for A\nThe desired capacity does not statically fix the size of the group.\n\nDesired capacity: Represents the **initial capacity** of the Auto Scaling group at the time of creation. An Auto Scaling group attempts to maintain the desired capacity. It starts by launching the number of instances that are specified for the desired capacity, and maintains this number of instances **as long as there are no scaling policies** or scheduled actions attached to the Auto Scaling group.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html","upvote_count":"3","poster":"feddo","comment_id":"827004","timestamp":"1677767640.0"},{"content":"C:\ntarget tracking may be a better option for ensuring the application remains responsive during high-traffic periods while also minimizing costs during periods of low usage. The target tracking can be used without CloudWatch alarms, as it relies on CloudWatch metrics directly.","poster":"KZM","upvote_count":"2","timestamp":"1676997900.0","comment_id":"816865"},{"poster":"LuckyAro","timestamp":"1676896440.0","content":"Selected Answer: C\nBetween closing and opening times there'll be enough \"cooling down\" period if necessary, however, I don't see it's relationship with the solution.","comment_id":"815214","upvote_count":"2"},{"poster":"NolaHOla","comments":[{"content":"How is decreasing cooldown related to question?","upvote_count":"1","poster":"Rocky2023","comment_id":"812613","timestamp":"1676691600.0","comments":[{"content":"Honestly not completely sure, but the rest of the options either don't think for the MOST Cost effective solution (as when directly placed on 20 this will generate cost|) or are irrelevant","timestamp":"1676905320.0","poster":"NolaHOla","upvote_count":"2","comment_id":"815414"},{"timestamp":"1677070200.0","comment_id":"817829","content":"I think because by decreasing the cooldown, the scale up and down will be more sensitive, more in \"real time\" I would say.","poster":"leoattf","upvote_count":"2"}]}],"content":"I would personally go for C, Implementing a target tracking scaling policy would allow the Auto Scaling group to adjust its capacity in response to changes in demand while keeping the specified metric at the target value\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-strategies.html\n\nOption A is not the best solution because it sets the desired capacity to 20 shortly before the office opens, but it does not take into account the actual demand of the application. This means that the company will be paying for 20 instances all the time, even during the off-hours, which will result in unnecessary costs. Additionally, there may be days when the demand is lower or higher than expected, so it is not a scalable solution.","timestamp":"1676643480.0","comment_id":"811963","upvote_count":"4"},{"timestamp":"1676625660.0","content":"Selected Answer: C\nAnswer is C","upvote_count":"3","comment_id":"811681","poster":"zTopic"}],"answer_ET":"C","exam_id":31,"isMC":true},{"id":"1itUrlrOfBdOUNmYOEpM","answer":"AD","answer_description":"","choices":{"D":"Configure the Auto Scaling group to use the average CPU as the scaling metric.","E":"Configure the Auto Scaling group to use the average free memory as the scaling metric.","B":"Migrate the database to Amazon Aurora to use Auto Scaling storage.","A":"Configure storage Auto Scaling on the RDS for Oracle instance.","C":"Configure an alarm on the RDS for Oracle instance for low free storage space."},"discussion":[{"comments":[{"content":"Nicely Explained.","poster":"[Removed]","comment_id":"1158432","upvote_count":"3","timestamp":"1708843260.0"}],"timestamp":"1680345540.0","upvote_count":"29","content":"Selected Answer: AD\nA) Configure storage Auto Scaling on the RDS for Oracle instance.\n= Makes sense. With RDS Storage Auto Scaling, you simply set your desired maximum storage limit, and Auto Scaling takes care of the rest.\n\nB) Migrate the database to Amazon Aurora to use Auto Scaling storage.\n= Scenario specifies application's data layer uses Oracle-specific PL/SQL functions. This rules out migration to Aurora.\n\nC) Configure an alarm on the RDS for Oracle instance for low free storage space.\n= You could do this but what does it fix? Nothing. The CW notification isn't going to trigger anything.\n\nD) Configure the Auto Scaling group to use the average CPU as the scaling metric.\n= Makes sense. The CPU utilization is the precursor to the storage outage. When the ec2 instances are overloaded, the RDS instance storage hits its limits, too.","comment_id":"857810","poster":"klayytech"},{"timestamp":"1719672960.0","content":"Selected Answer: AE\nB mean db migration and only oracle specfic commands are allowed\nC is just notification not High avalibilty\nNow the toss up is between D and E since D is measuring CPU % and E is measuring memory and the question states \"This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage\" i will err on the side of option E","comment_id":"1239334","upvote_count":"1","poster":"BombArat"},{"poster":"Uzbekistan","timestamp":"1711583040.0","upvote_count":"3","content":"Selected Answer: AC\nTo ensure the system can automatically scale for the increased traffic, you can take the following steps:\nA. Configure storage Auto Scaling on the RDS for Oracle instance.\n\nBy enabling storage Auto Scaling on the RDS instance, you ensure that additional storage is provisioned automatically when the existing storage reaches capacity. This helps prevent the RDS instance from running out of storage due to increased traffic and data growth.\n\nC. Configure an alarm on the RDS for Oracle instance for low free storage space.\n\nSetting up an alarm for low free storage space on the RDS instance allows you to receive notifications when the storage capacity is approaching its limits. This proactive monitoring helps you take necessary actions, such as adding more storage or scaling resources, before it affects the application's performance.","comment_id":"1184460"},{"timestamp":"1703835540.0","poster":"pentium75","content":"Selected Answer: AD\nB is not possible because the application \"uses Oracle-specific PL/SQL functions\"\nC does not meet the \"automatically scale\" requirement\nE would require an agent on the hosts which we might not have, plus CPU is a better indicator than memory","upvote_count":"2","comment_id":"1108437"},{"comments":[{"timestamp":"1701312420.0","comments":[{"upvote_count":"1","comment_id":"1314721","timestamp":"1732027500.0","content":"STEM mentioned the application \"uses Oracle-specific PL/SQL functions\"","poster":"JA2018"}],"poster":"meowruki","content":"Option B (Migrate the database to Amazon Aurora): While Amazon Aurora provides benefits such as auto-scaling storage and high performance, it involves migrating from Oracle to Aurora, which might require application changes and data migration efforts.\n\n Option C (Configure an alarm on the RDS for Oracle instance for low free storage space): While it's good to have an alarm for low storage space, configuring storage Auto Scaling (Option A) is a more proactive solution that automatically adjusts storage before reaching a critical point.\n\n Option E (Configure the Auto Scaling group to use the average free memory as the scaling metric): While monitoring memory is important for application performance, CPU utilization is often a more direct and responsive metric for auto-scaling in many scenarios.","comment_id":"1083939","upvote_count":"3"}],"content":"Selected Answer: AD\nA. Configure storage Auto Scaling on the RDS for Oracle instance.\n\n This option allows the RDS instance to automatically scale its storage based on the actual storage usage, ensuring that you don't run out of storage.\n\nD. Configure the Auto Scaling group to use the average CPU as the scaling metric.\n\n By using CPU utilization as a scaling metric, the Auto Scaling group can dynamically adjust the number of EC2 instances based on the application's demand. This helps in handling increased traffic and preventing overload on existing instances.","timestamp":"1701312420.0","comment_id":"1083938","upvote_count":"2","poster":"meowruki"},{"timestamp":"1696218360.0","content":"Selected Answer: AD\nConfigure storage Auto Scaling on the RDS for Oracle instance and Configure the Auto Scaling group to use the average CPU as the scaling metric to accommodate the increased traffic automatically.","comment_id":"1022779","upvote_count":"2","poster":"TariqKipkemei"},{"content":"Selected Answer: AD\nOption B (Migrate the database to Amazon Aurora) may be a good long-term solution, but it involves database migration, which can be complex and time-consuming. For immediate scalability and to address the storage issue, configuring storage Auto Scaling on the existing RDS instance is a more immediate and straightforward solution.\n\nOption C (Configure an alarm on the RDS for Oracle instance for low free storage space) is useful for monitoring, but it doesn't proactively address the storage issue by automatically expanding storage as needed.\n\nOption E (Configure the Auto Scaling group to use the average free memory as the scaling metric) is less common as a scaling metric for EC2 instances compared to CPU utilization. While memory can be an important factor for application performance, CPU utilization is typically a more commonly used metric for scaling decisions. It also doesn't directly address the RDS storage issue.","comment_id":"1020869","upvote_count":"2","poster":"vijaykamal","comments":[{"content":"the application \"uses Oracle-specific PL/SQL functions\"\n\nRules out Option B in my mind","poster":"JA2018","timestamp":"1732027560.0","upvote_count":"1","comment_id":"1314722"}],"timestamp":"1695997980.0"},{"timestamp":"1694019240.0","upvote_count":"2","comment_id":"1000821","content":"Selected Answer: AD\nA. By enabling storage Auto Scaling on the RDS for Oracle instance, it will automatically add more storage when the existing storage is running out, ensuring the application's data layer can handle the increased data storage requirements.\n\nD. By configuring the Auto Scaling group to use the average CPU utilization as the scaling metric, it can automatically add more EC2 instances to the Auto Scaling group when the CPU utilization exceeds a certain threshold. This will help handle the increased traffic and workload on the EC2 instances in the multi-tier application.","poster":"Guru4Cloud"},{"content":"Selected Answer: AD\nA. By enabling storage Auto Scaling on the RDS for Oracle instance, it will automatically add more storage when the existing storage is running out, ensuring the application's data layer can handle the increased data storage requirements.\n\nD. By configuring the Auto Scaling group to use the average CPU utilization as the scaling metric, it can automatically add more EC2 instances to the Auto Scaling group when the CPU utilization exceeds a certain threshold. This will help handle the increased traffic and workload on the EC2 instances in the multi-tier application.","timestamp":"1690938600.0","comment_id":"969534","upvote_count":"2","poster":"A1975"},{"upvote_count":"3","poster":"kruasan","content":"Selected Answer: AD\nThese options will allow the system to scale both the compute tier (EC2 instances) and the data tier (RDS storage) automatically as traffic increases:\nA. Storage Auto Scaling will allow the RDS for Oracle instance to automatically increase its allocated storage when free storage space gets low. This ensures the database does not run out of capacity and can continue serving data to the application.\nD. Configuring the EC2 Auto Scaling group to scale based on average CPU utilization will allow it to launch additional instances automatically as traffic causes higher CPU levels across the instances. This scales the compute tier to handle increased demand.","timestamp":"1682781720.0","comment_id":"884427"},{"comment_id":"857411","content":"Selected Answer: AD\nAuto scaling storage RDS will ease storage issues and migrating Oracle Pl/Sql to Aurora is cumbersome. Also Aurora has auto storage scaling by default. \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling","upvote_count":"3","poster":"kraken21","timestamp":"1680292980.0"},{"upvote_count":"3","timestamp":"1677731280.0","content":"Selected Answer: BD\nMy answer is B & D... \nB. Migrate the database to Amazon Aurora to use Auto Scaling Storage. --- Aurora storage is also self-healing. Data blocks and disks are continuously scanned for errors and repaired automatically.\nD. Configurate the Auto Scaling group to sue the average CPU as the scaling metric. -- Good choice.\n\nI believe either A & C or B & D options will work.","comments":[{"upvote_count":"5","comment_id":"828856","poster":"FourOfAKind","timestamp":"1677929700.0","content":"In this question, you have Oracle DB, and Amazon Aurora is for MySQL/PostgreSQL. A and D are the correct choices.","comments":[{"upvote_count":"1","comments":[{"timestamp":"1703835480.0","content":"But the application \"uses Oracle-specific PL/SQL functions\".","comment_id":"1108435","poster":"pentium75","upvote_count":"2"},{"poster":"[Removed]","timestamp":"1679354760.0","upvote_count":"2","comment_id":"845345","content":"I still think A is the answer, because RDS for Oracle auto scaling once enabled it will automatically adjust the storage capacity."}],"comment_id":"845339","content":"You can migrate Oracle PL/SQL to Aurora:\nhttps://docs.aws.amazon.com/dms/latest/oracle-to-aurora-mysql-migration-playbook/chap-oracle-aurora-mysql.sql.html","poster":"[Removed]","timestamp":"1679354220.0"}]}],"poster":"Nel8","comment_id":"826495"},{"poster":"Ja13","upvote_count":"4","content":"Selected Answer: AD\na and d","comment_id":"817371","timestamp":"1677031440.0"},{"upvote_count":"4","poster":"KZM","timestamp":"1676998920.0","comment_id":"816892","content":"A and D."},{"timestamp":"1676978040.0","poster":"GwonLEE","comment_id":"816474","content":"Selected Answer: AD\na and d","upvote_count":"4"},{"comment_id":"815226","upvote_count":"3","poster":"LuckyAro","content":"Selected Answer: AD\nA and D","timestamp":"1676897040.0"},{"upvote_count":"2","poster":"Joan111edu","comment_id":"812667","timestamp":"1676697480.0","content":"Selected Answer: AD\nhttps://www.examtopics.com/discussions/amazon/view/46534-exam-aws-certified-solutions-architect-associate-saa-c02/"},{"timestamp":"1676694060.0","poster":"ChrisG1454","comments":[{"content":"https://www.examtopics.com/discussions/amazon/view/46534-exam-aws-certified-solutions-architect-associate-saa-c02/#:~:text=%22This%20overloads%20the%20EC2%20instances%20and%20causes%20the,the%20RDS%20for%20Oracle%20instance%20upvoted%202%20times","timestamp":"1676694060.0","upvote_count":"2","comment_id":"812638","poster":"ChrisG1454"}],"comment_id":"812637","content":"answer is A and D","upvote_count":"2"},{"poster":"rrharris","comment_id":"812604","content":"A and D are the Answers","upvote_count":"2","timestamp":"1676690880.0"}],"answer_images":[],"answers_community":["AD (90%)","4%"],"url":"https://www.examtopics.com/discussions/amazon/view/99739-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’ s data layer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.\n\nWhat should a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)","topic":"1","timestamp":"2023-02-18 04:28:00","isMC":true,"unix_timestamp":1676690880,"exam_id":31,"question_images":[],"question_id":217,"answer_ET":"AD"},{"id":"elCgVfjz1J36zDQa9W0m","isMC":true,"answers_community":["D (82%)","A (18%)"],"discussion":[{"timestamp":"1676661780.0","comments":[{"upvote_count":"3","timestamp":"1703846340.0","poster":"pentium75","content":"A doesn't say \"store content ON the gateway\", it says \"use AWS Storage Gateway for files\" (which is the product) \"to store the video content\" [on S3].","comments":[{"poster":"jaswantn","comment_id":"1143486","upvote_count":"1","timestamp":"1707321420.0","content":"Creating storage gateway for Files will mount S3 bucket as an NFS volume that can be shared among EC2 Instances in the same manner as EFS but more cost effectively."}],"comment_id":"1108591"}],"upvote_count":"28","content":"Selected Answer: D\nStorage gateway is not used for storing content - only to transfer to the Cloud","comment_id":"812269","poster":"bdp123"},{"poster":"kraken21","timestamp":"1680293460.0","comments":[{"content":"But how do you attach \"an EBS volume\" to all the servers, and how will you use the files on it then to serve customers.","poster":"pentium75","upvote_count":"3","timestamp":"1703846280.0","comment_id":"1108589"}],"content":"Selected Answer: D\nThere is no on-prem/non Aws infrastructure to create a gateway. Also, EFS+EBS is more expensive that EFS and S3. So D is the best option.","upvote_count":"12","comment_id":"857413"},{"comment_id":"1319523","timestamp":"1732849860.0","content":"Selected Answer: D\nOption D (S3 + EBS) is the most cost-effective and scalable choice for a service that involves storing video content and performing I/O-intensive transcoding for mobile platforms.\nOption A (S3 File Gateway) could work for applications with strict file system requirements but is less optimal for high-performance transcoding which is required by this online service","poster":"FlyingHawk","upvote_count":"1"},{"poster":"Uzbekistan","upvote_count":"4","comment_id":"1184461","content":"Selected Answer: D\nD. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.\n\nAmazon S3 (Simple Storage Service) is highly durable, scalable, and cost-effective for storing large volumes of data, such as video content. It offers lower storage costs compared to Amazon EFS and is suitable for storing large files like video content.\n\nFor processing the video content, you can temporarily move the files from Amazon S3 to an Amazon EBS volume attached to the EC2 instances. This approach allows you to leverage the high-performance storage of Amazon EBS for processing, while still benefiting from the cost-effectiveness of Amazon S3 for long-term storage. Once processing is complete, you can remove the temporary files from the EBS volume and store the final results back in S3.","timestamp":"1711583220.0"},{"content":"Selected Answer: A\nAnswer is closer to the following principle and D is near impossible to implement:\n\"Amazon S3 File Gateway – Amazon S3 File Gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). You deploy the gateway into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM), or as a hardware appliance that you order from your preferred reseller. You can also deploy the Storage Gateway VM in VMware Cloud on AWS, or as an AMI in Amazon EC2. The gateway provides access to objects in S3 as files or file share mount points. With a S3 File Gateway, you can do the following\"","comment_id":"1166587","poster":"bujuman","timestamp":"1709653920.0","upvote_count":"1"},{"poster":"vip2","upvote_count":"1","comment_id":"1152832","content":"Selected Answer: A\nA is correct\nFor D, how to move file from S3 to EBS temporarily????","timestamp":"1708205100.0"},{"upvote_count":"1","comment_id":"1125403","timestamp":"1705531980.0","content":"Selected Answer: A\nI was initially going for D but EBS part makes no sense as it is not possible. Closest explanation of A is in this article:\nhttps://aws.amazon.com/blogs/storage/mounting-amazon-s3-to-an-amazon-ec2-instance-using-a-private-connection-to-s3-file-gateway/ \n\nA is missing a lot of key steps but D is just impossible. Maybe it's just the wording?","poster":"awsgeek75"},{"comment_id":"1118883","upvote_count":"2","content":"EFS is already used, why EBS is an option in the answer?","timestamp":"1704908400.0","poster":"AzExam2020"},{"comments":[{"upvote_count":"1","comment_id":"1122508","content":"I agree. This documentation has convinced me. https://docs.aws.amazon.com/filegateway/latest/files3/what-is-file-s3.html","poster":"SVDK","timestamp":"1705233480.0"}],"comment_id":"1115716","poster":"anikolov","timestamp":"1704623040.0","upvote_count":"2","content":"Selected Answer: A\nAWS Storage Gateway S3 file gateway can be setup on EC2 ( https://repost.aws/knowledge-center/file-gateway-ec2 ). It use local disks/EBS for caching data.\nD: Can be used too, using attached EBS volume to each EC2 instance to process files. If require single EBS volume to be attached to multiple EC2, then it is possible too if they are in the same Availability Zone -> https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html.\n For me A and D both are possible, but expect AWS would like to select Storage GW"},{"upvote_count":"2","content":"Selected Answer: A\nA would work, \"Storage Gateway for files\" can provide access to S3 (cheap) via NFS (what the clients are using now). It has some additional cost in addition to the S3 charges, but would still be way cheaper than EFS.\n\nB would work for a single server, but as it provides a volume via iSCSI, it could be mounted only to a single server - does not meet the 'multiple instances can access' requirement.\n\nC and D do not meet the 'multiple instances can access' requirement because EBS can't be easily attached to all servers at the same time.","timestamp":"1703846160.0","comment_id":"1108588","comments":[{"poster":"pentium75","timestamp":"1704621480.0","comment_id":"1115695","content":"And even if ignoring the 'multiple instances can access' requirement, D would be against WAF; for temporary storage you'd use instance storage, not EBS.","upvote_count":"2"}],"poster":"pentium75"},{"upvote_count":"1","content":"Storage gateway is intended for on-premises applications to access cloud storage, so A, B is out. The question explicitly states that the files are uploaded and stored in EFS, not S3, so D is not correct. The answer is C. The EFS storage costs 10 times more than EBS, so moving files to EBS after processing is the solution.","comment_id":"1065153","poster":"liux99","timestamp":"1699389540.0"},{"content":"Selected Answer: D\nAnswer D is correct. \nStorage gateway is not used for storing content - only to transfer to the Cloud","poster":"beginnercloud","comments":[{"poster":"pentium75","upvote_count":"1","content":"But how do you attach \"an EBS volume\" to all the servers, and how will you use the files on it then to serve customers? \n\nA doesn't say \"store content ON the gateway\", it says \"use AWS Storage Gateway for files\" (which is the product) \"to store the video content\" [on S3]. And to be exact, Storage Gateway is not \"to transfer to the cloud\" but to provide access to S3 storage via SMB or NFS.","timestamp":"1703846460.0","comment_id":"1108592"}],"timestamp":"1698490080.0","comment_id":"1056129","upvote_count":"2"},{"poster":"TariqKipkemei","upvote_count":"2","comment_id":"1022782","timestamp":"1696218780.0","content":"Selected Answer: D\nCost effective = Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing"},{"upvote_count":"5","content":"Selected Answer: D\nAmazon S3 provides low-cost object storage for storing large amounts of unstructured data like videos. The videos can be stored in S3 durably and reliably.\n\nFor processing, the video files can be temporarily copied from S3 to an EBS volume attached to the EC2 instance. EBS provides low latency block storage for high performance video processing.\n\nOnce processing is complete, the output can be stored back in S3.","comment_id":"1000754","timestamp":"1694013900.0","poster":"Guru4Cloud"},{"timestamp":"1691468040.0","upvote_count":"2","comments":[{"comments":[{"comment_id":"1129173","timestamp":"1705975800.0","poster":"foha2012","upvote_count":"1","content":"We are ditching EFS in favor of S3. So there is no longer simultaneous access happening. Whoever needs the file, downloads it from S3, process it on their EC2 instance and save it back to S3."}],"upvote_count":"1","poster":"pentium75","content":"A mentions \"AWS Storage Gateway for files\" which implies S3 as the backend storage. D does not meet the 'multiple instances can access for processing' requirement.","comment_id":"1115698","timestamp":"1704621600.0"}],"content":"Selected Answer: D\nThe question doesn't give enough information. Well, quite a few AWS exam questions don't provide enough info.\nIdeally, A could be the best answer if it mentions S3 as the backend of storage gateway. Because if it doesn't mention S3 as the backend, that implies either Storage gateway as the storage(which is impossible) or continue using EFS(also impossible).\nD is not ideal, because it will introduce video download cost for downloading files from S3 to EBS temporary storage. But it is the best option we have.","comment_id":"975163","poster":"bjexamprep"},{"timestamp":"1690859580.0","content":"Selected Answer: D\nA more cost-effective storage solution for this scenario would be Amazon Simple Storage Service (Amazon S3). Amazon S3 is an object storage service that offers high scalability, durability, and availability at a lower cost compared to Amazon EFS. By using Amazon S3, you only pay for the storage you use, and it is typically more cost-efficient for scenarios where data is accessed less frequently, such as video storage for processing.","comment_id":"968681","upvote_count":"3","poster":"Undisputed"},{"timestamp":"1686975660.0","comment_id":"925759","upvote_count":"1","content":"Selected Answer: A\nThe result should be A.\n\nAmazon storage gateway has 4 types, S3 File Gateway, FSx file gateway, Type Gateway and Volume Gateway.\n\nIf not specific reference file gateway should be default as S3 gateway, which sent file over to S3 the most cost effective storage in AWS.\n\nWhy not D, the reason is last sentence, there are multiple EC2 servers for processing the video and EBS can only attach to 1 EC2 instance at a time, so if you use EBS, which mean for each EC2 instance you will have 1 EBS. This rule out D.","poster":"smartegnine","comments":[{"poster":"argl1995","content":"We can use multi-attach feature of EBS to attach one EBS volume to multiple Ec2 instances","comment_id":"940071","timestamp":"1688222460.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1687575360.0","content":"AWS Storage Gateway = extend storage to onprem","comment_id":"932124","poster":"[Removed]"},{"upvote_count":"1","content":"Storage Gateway is a GATEWAY, it does not store anything. You could use the gateway as a cache for content actually in S3. Btw, A and B even say that Storage Gateway would \"process the video content\" ...","timestamp":"1703835720.0","comment_id":"1108441","poster":"pentium75"}]},{"content":"Selected Answer: D\nD: MOST cost-effective of these options = S3","poster":"MostafaWardany","timestamp":"1686131820.0","comment_id":"917076","upvote_count":"2"},{"poster":"omoakin","content":"CCCCCCCCCCCCCCC","upvote_count":"1","comment_id":"908008","timestamp":"1685193480.0"},{"timestamp":"1682781900.0","poster":"kruasan","comment_id":"884434","content":"Selected Answer: D\nhe most cost-effective storage solution in this scenario would be:\nD. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.\nThis option provides the lowest-cost storage by using:\n• Amazon S3 for large-scale, durable, and inexpensive storage of the video content. S3 storage costs are significantly lower than EFS.\n• Amazon EBS only temporarily during processing. By mounting an EBS volume only when a video needs to be processed, and unmounting it after, the time the content spends on the higher-cost EBS storage is minimized.\n• The EBS volume can be sized to match the workload needs for active processing, keeping costs lower. The volume does not need to store the entire video library long-term.","upvote_count":"3"},{"upvote_count":"2","poster":"GalileoEC2","content":"Option A, which uses AWS Storage Gateway for files to store and process the video content, would be the most cost-effective solution.\n\nWith this approach, you would use an AWS Storage Gateway file gateway to access the video content stored in Amazon S3. The file gateway presents a file interface to the EC2 instances, allowing them to access the video content as if it were stored on a local file system. The video processing tasks can be performed on the EC2 instances, and the processed files can be stored back in S3.\n\nThis approach is cost-effective because it leverages the lower cost of Amazon S3 for storage while still allowing for easy access to the video content from the EC2 instances using a file interface. Additionally, Storage Gateway provides caching capabilities that can further improve performance by reducing the need to access S3 directly.","timestamp":"1679668740.0","comment_id":"849398"},{"upvote_count":"1","poster":"scs50","comment_id":"846412","comments":[{"upvote_count":"2","timestamp":"1703835780.0","comment_id":"1108442","content":"Why not use S3 directly, wouldn't that be more cost-efficient?","poster":"pentium75"}],"timestamp":"1679437440.0","content":"Selected Answer: A \nAmazon S3 File gateway is using S3 behind the scene. \nhttps://docs.aws.amazon.com/filegateway/latest/files3/what-is-file-s3.html"},{"poster":"CapJackSparrow","timestamp":"1678825740.0","upvote_count":"1","content":"Amazon S3 File Gateway\n\nAmazon S3 File Gateway presents a file interface that enables you to store files as objects in Amazon S3 using the industry-standard NFS and SMB file protocols, and access those files via NFS and SMB from your data center or Amazon EC2, or access those files as objects directly in Amazon S3. POSIX-style metadata, including ownership, permissions, and timestamps are durably stored in Amazon S3 in the user-metadata of the object associated with the file. Once objects are transferred to S3, they can be managed as native S3 objects and bucket policies such as lifecycle management and Cross-Region Replication (CRR), and can be applied directly to objects stored in your bucket. Amazon S3 File Gateway also publishes audit logs for SMB file share user operations to Amazon CloudWatch.\n\nCustomers can use Amazon S3 File Gateway to back up on-premises file data as objects in Amazon S3 (including Microsoft SQL Server and Oracle databases and logs), and for hybrid cloud workflows using data generated by on-premises applications for processing by AWS services such as machine learning or big data analytics.","comment_id":"839249"},{"timestamp":"1678165080.0","comments":[{"comment_id":"1105347","timestamp":"1703516640.0","content":"S3 is used for storage. EBS is just used for processing.","poster":"SohamSLP","upvote_count":"2"}],"poster":"Brak","comment_id":"831564","content":"Selected Answer: A\nIt can't be D, since there are multiple servers accessing the video files which rules out EBS. File Gateway provides a shared filesystem to replace EFS, but uses S3 for storage to reduce costs.","upvote_count":"5"},{"poster":"KZM","upvote_count":"4","content":"Using Amazon S3 for storing video content is the best way for cost-effectiveness I think. But I am still confused about why moved the data to EBS.","timestamp":"1676999760.0","comments":[{"comment_id":"816917","content":"A better solution would be to use a transcoding service like Amazon Elastic Transcoder to process the video content directly from Amazon S3. This would eliminate the need for storing the content on an EBS volume, reduce storage costs, and simplify the architecture by removing the need for managing EBS volumes.","timestamp":"1676999940.0","upvote_count":"3","poster":"KZM"}],"comment_id":"816914"},{"poster":"AlmeroSenior","timestamp":"1676971860.0","comment_id":"816397","content":"Selected Answer: A\nA looks right . File Gateway is S3 , but exposes it as NFS/SMB . So no need for costly retrieval like option D , or C consuming expensive EBS .","upvote_count":"2"},{"upvote_count":"1","timestamp":"1676905680.0","comments":[{"comment_id":"841431","upvote_count":"1","poster":"asoli","content":"EFS has a lower cost than EBS in general. So, moving from EFS to EBS will not reduce cost","timestamp":"1679010840.0"}],"poster":"NolaHOla","content":"Can someone please explain or provide information why not C? If we go with option D it states that we store the Content in S3 which is indeed cheaper, but then we move them to EBS for processing, how are multiple Linux instances, gonna process the same videos from EBS when they can't read them simultaneously. \nWhere for Option C, we indeed keep the EFS, then we process from there and move them to EBS for reading? seems more logical to me","comment_id":"815422"},{"comment_id":"815223","poster":"LuckyAro","timestamp":"1676896980.0","content":"Selected Answer: D\nUse Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.","upvote_count":"3"},{"content":"Most Cost Effective is S3","poster":"rrharris","upvote_count":"5","timestamp":"1676604120.0","comment_id":"811344"}],"answer_images":[],"answer_description":"","answer":"D","question_text":"A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive.\n\nWhich storage solution is MOST cost-effective?","question_images":[],"unix_timestamp":1676604120,"exam_id":31,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/99509-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).","B":"Use AWS Storage Gateway for volumes to store and process the video content.","D":"Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.","A":"Use AWS Storage Gateway for files to store and process the video content."},"timestamp":"2023-02-17 04:22:00","answer_ET":"D","question_id":218},{"id":"qsVRtE9LYFsKJfB0F1OZ","isMC":true,"exam_id":31,"topic":"1","answer":"BE","url":"https://www.examtopics.com/discussions/amazon/view/99940-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","answer_images":[],"question_text":"A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)","answers_community":["BE (100%)"],"question_images":[],"choices":{"B":"Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.","C":"Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.","D":"Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.","A":"Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.","E":"Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription."},"timestamp":"2023-02-19 14:10:00","unix_timestamp":1676812200,"discussion":[{"timestamp":"1692443400.0","content":"Selected Answer: BE\nData in hierarchies : Amazon DynamoDB\nB. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.\n\nSensitive Info: Amazon Macie\nE. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription.","comments":[{"comment_id":"853648","comments":[{"comment_id":"925765","timestamp":"1702794720.0","content":"C is half statement once event sent to Lambda what is next? Should send email right, but it does not say it.","upvote_count":"2","poster":"smartegnine"}],"upvote_count":"1","content":"Can someone please provide explanation why options \"B\" & \"C\" are the correct options?","poster":"gold4otas","timestamp":"1695930780.0"}],"upvote_count":"16","poster":"Bhawesh","comment_id":"814096"},{"upvote_count":"5","comment_id":"1000744","poster":"Guru4Cloud","content":"Selected Answer: BE\nB and E are the steps to meet all of the requirements.\n\nB meets the need to store hierarchical employee data in DynamoDB for low latency queries at high traffic. DynamoDB can handle the access patterns for hierarchical data. Exporting to S3 monthly provides an audit trail.\n\nE sets up Macie to analyze sensitive data and integrate with EventBridge to trigger monthly SNS notifications when financial data is present.","timestamp":"1709745180.0"},{"poster":"hro","comment_id":"1174538","timestamp":"1726431180.0","content":"B - because to store employee data in a hierarchical structured relationship. AmazonDB \"...Schema flexibility lets DynamoDB store complex hierarchical data within a single item.\"\nE - because C omits the monthly email notifications resolved by using Amazon SNS.\nJust my take.","upvote_count":"2"},{"poster":"[Removed]","upvote_count":"1","content":"Why not C and D? Can anyone explain please.","comment_id":"1158943","timestamp":"1724594700.0"},{"comment_id":"1095981","poster":"Cyberkayu","content":"A. Unload the data to Amazon S3 every month. \n\ndoesnt make sense to empty the employee data from redshift monthly","timestamp":"1718326740.0","upvote_count":"2"},{"content":"Selected Answer: BE\nB and E are the steps to meet all of the requirements.","timestamp":"1714301400.0","comment_id":"1056130","upvote_count":"2","poster":"beginnercloud"},{"content":"Selected Answer: BE\nUse Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription.","comment_id":"1022786","timestamp":"1712030340.0","poster":"TariqKipkemei","upvote_count":"2"},{"timestamp":"1706844540.0","upvote_count":"3","content":"Selected Answer: BE\n]B. Amazon DynamoDB is a fully managed NoSQL database service that provides low-latency, high-performance storage for hierarchical data. It handle high-traffic queries and delivering fast responses to retrieve employee data efficiently.\n\nE. Amazon Macie is a service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Integrating Macie with Amazon EventBridge allows you to receive events whenever any financial information is identified in the employee data. By using Amazon SNS, you can receive these notifications via email.","poster":"A1975","comment_id":"969546"},{"comment_id":"929568","poster":"cesargalindo123","upvote_count":"3","content":"AE\nhttps://aws.amazon.com/es/blogs/big-data/query-hierarchical-data-models-within-amazon-redshift/","timestamp":"1703172960.0"},{"comment_id":"884438","poster":"kruasan","comments":[{"timestamp":"1698600540.0","content":"Generally, for building a hierarchical relationship model, a graph database such as Amazon Neptune is a better choice. In some cases, however, DynamoDB is a better choice for hierarchical data modeling because of its flexibility, security, performance, and scale.\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-hierarchical-data-model/introduction.html","poster":"kruasan","comment_id":"884442","upvote_count":"4"}],"content":"Selected Answer: BE\n, the combination of DynamoDB for fast data queries, S3 for durable storage and backups, Macie for sensitive data monitoring, and EventBridge + SNS for email notifications satisfies all needs: fast query response, sensitive data protection, and monthly alerts. The solutions architect should implement DynamoDB with export to S3, and configure Macie with integration to send SNS email notifications.","timestamp":"1698600480.0","upvote_count":"2"},{"comment_id":"879840","content":"why Dynamo and not Redshift?","comments":[{"timestamp":"1698600600.0","poster":"kruasan","content":"1. Low latency - DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with single-digit millisecond latency. Redshift is a data warehouse solution optimized for complex analytical queries, so query latency would typically be higher. Since the requirements specify minimum latency for high-traffic queries, DynamoDB is better suited.\n2. Scalability - DynamoDB is highly scalable, able to handle very high read and write throughput with no downtime. Redshift also scales, but may experience some downtime during rescale operations. For a high-traffic application, DynamoDB's scalability and availability are better matched.","upvote_count":"5","comment_id":"884444"}],"upvote_count":"2","timestamp":"1698202380.0","poster":"darn"},{"timestamp":"1692758280.0","poster":"PRASAD180","comment_id":"818799","content":"BE is crt 100%","upvote_count":"2"},{"timestamp":"1692631980.0","comment_id":"816935","upvote_count":"3","poster":"KZM","content":"B and E\nTo send monthly email messages, an SNS service is required."},{"comment_id":"814168","upvote_count":"4","poster":"skiwili","content":"Selected Answer: BE\nB and E","timestamp":"1692448200.0"}],"answer_ET":"BE","question_id":219},{"id":"ndhkjNZTLlXHWxiidZy6","question_text":"A company has an application that is backed by an Amazon DynamoDB table. The company’s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years.\n\nWhich solution will meet these requirements?","choices":{"D":"Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years.","B":"Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.","C":"Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.","A":"Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years."},"question_id":220,"discussion":[{"comment_id":"1020877","upvote_count":"13","timestamp":"1695998760.0","poster":"vijaykamal","content":"Selected Answer: A\nOption B mentions using Amazon S3 Glacier Flexible Retrieval, but DynamoDB doesn't natively support transitioning backups to Amazon S3 Glacier. Options C and D involve custom scripts and EventBridge rules, which add complexity and may not be as reliable or efficient as using AWS Backup for this purpose."},{"comment_id":"947904","upvote_count":"6","timestamp":"1688977860.0","content":"All except A are \"On-demand\"","poster":"MNotABot"},{"timestamp":"1720310760.0","content":"Selected Answer: A\nA for sure","comment_id":"1243611","poster":"ChymKuBoy","upvote_count":"2"},{"poster":"Sadiya_Javid_Abbasi","upvote_count":"2","timestamp":"1704684600.0","comment_id":"1116366","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/creating-a-backup-plan.html"},{"comment_id":"1109733","poster":"mwwt2022","timestamp":"1703941020.0","upvote_count":"1","content":"Why B is wrong?"},{"comment_id":"1095984","content":"BCD, on-demand backup, manual work","timestamp":"1702522980.0","poster":"Cyberkayu","upvote_count":"5"},{"timestamp":"1698490260.0","comment_id":"1056131","upvote_count":"4","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/database/set-up-scheduled-backups-for-amazon-dynamodb-using-aws-backup/","poster":"beginnercloud"},{"timestamp":"1693185180.0","content":"Selected Answer: A\nA is right ans","poster":"chanchal133","upvote_count":"2","comment_id":"991729"},{"comment_id":"944494","poster":"narddrer","upvote_count":"4","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html\nUsing DynamoDB with AWS Backup, you can copy your on-demand backups across AWS accounts and Regions, add cost allocation tags to on-demand backups, and transition on-demand backups to cold storage for lower costs. To use these advanced features, you must opt in to AWS Backup.","comments":[{"content":"\"on demand\" (manual) backup -> hell no","poster":"pentium75","upvote_count":"4","comment_id":"1108603","timestamp":"1703846760.0"}],"timestamp":"1688636400.0"},{"poster":"kruasan","timestamp":"1682782440.0","content":"Selected Answer: A\nThis solution satisfies the requirements in the following ways:\n• AWS Backup will automatically take full backups of the DynamoDB table on the schedule defined in the backup plan (the first of each month).\n• The lifecycle policy can transition backups to cold storage after 6 months, meeting that requirement.\n• Setting a 7-year retention period in the backup plan will ensure each backup is retained for 7 years as required.\n• AWS Backup manages the backup jobs and lifecycle policies, requiring no custom scripting or management.","upvote_count":"3","comment_id":"884449"},{"content":"Answer is A","upvote_count":"2","comments":[{"content":"Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years","upvote_count":"2","comment_id":"1022787","poster":"TariqKipkemei","timestamp":"1696219380.0"}],"poster":"TariqKipkemei","timestamp":"1679976120.0","comment_id":"852747"},{"comment_id":"846622","upvote_count":"2","timestamp":"1679454180.0","comments":[{"poster":"mmustafa4455","content":"Its B. \nhttps://aws.amazon.com/blogs/database/set-up-scheduled-backups-for-amazon-dynamodb-using-aws-backup/","upvote_count":"2","comment_id":"846625","timestamp":"1679454240.0"}],"poster":"mmustafa4455","content":"Selected Answer: A\nThe correct Answer is A\n\nhttps://aws.amazon.com/blogs/database/set-up-scheduled-backups-for-amazon-dynamodb-using-aws-backup/"},{"upvote_count":"2","comment_id":"816625","content":"Selected Answer: A\nA is the answer","poster":"Wael216","timestamp":"1676986920.0"},{"content":"Selected Answer: A\nA is the answer.","comment_id":"816057","timestamp":"1676939760.0","upvote_count":"2","poster":"LuckyAro"},{"upvote_count":"1","content":"Selected Answer: A\nA is the correct answe","comment_id":"814171","poster":"skiwili","timestamp":"1676817120.0"},{"content":"A is the Answer\n\ncan be used to create backup schedules and retention policies for DynamoDB tables","comment_id":"813342","timestamp":"1676741580.0","poster":"rrharris","upvote_count":"3"},{"comment_id":"812952","poster":"kpato87","content":"Selected Answer: A\nA. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.","upvote_count":"4","timestamp":"1676724480.0"}],"answer_description":"","answer_images":[],"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/99793-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["A (90%)","10%"],"exam_id":31,"answer_ET":"A","timestamp":"2023-02-18 13:48:00","answer":"A","isMC":true,"unix_timestamp":1676724480}],"exam":{"name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31,"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Amazon","isBeta":false,"isMCOnly":true,"numberOfQuestions":1019},"currentPage":44},"__N_SSP":true}