{"pageProps":{"questions":[{"id":"wOSRZcNgkRScUtov6aqY","answers_community":["A (86%)","9%"],"unix_timestamp":1680650520,"question_id":1,"isMC":true,"answer":"A","answer_images":[],"exam_id":23,"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/105229-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"D":"Configure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric.","C":"Configure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.","A":"Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.","B":"Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify response code and application version as dimensions for the metric."},"answer_ET":"A","question_images":[],"discussion":[{"comment_id":"1209003","comments":[{"timestamp":"1744263240.0","poster":"alrayyan007","content":"valuable discussion Reference - ( lc.cx/30bEB- )","comment_id":"1559474","upvote_count":"1"}],"content":"Selected Answer: A\nThe other options are either incomplete or involve unnecessary complexity:\n\nOption B requires using CloudWatch Logs Insights queries, which may introduce additional complexity and potential performance overhead.\nOption C involves modifying the ALB access logs, which may not provide the required level of granularity or flexibility for capturing the application version information.\nOption D requires integrating AWS X-Ray, which is primarily designed for distributed tracing and may not be necessary for this specific use case of gathering metrics by response code and application version.\n\nTherefore, option A is the most appropriate and straightforward solution for the given requirements.","timestamp":"1727009940.0","upvote_count":"7","poster":"c3518fc"},{"poster":"BaburTurk","upvote_count":"5","comment_id":"995505","content":"Selected Answer: A\nOption A: This option is the most efficient way to gather the required metrics. It does not require any additional infrastructure and can be easily implemented.\nOption B: This option is more complex than Option A and requires configuring a CloudWatch Logs Insights query. This can be more time-consuming to set up and can be less efficient if the query is not optimized.\nOption C: This option requires configuring the ALB access logs to write to CloudWatch Logs. This can add additional latency to the requests.\nOption D: This option requires configuring AWS X-Ray integration. This is a more complex solution that is not necessary in this case.","timestamp":"1693518540.0"},{"upvote_count":"1","comment_id":"1359443","content":"Selected Answer: A\nWhy Option A is Correct:\nIt directly addresses the requirement to gather metrics for each API operation by response code and application version.\nCloudWatch Logs and metric filters are designed for this purpose and are easy to implement.\nIt provides a scalable and cost-effective solution.","poster":"festusbetd","timestamp":"1740081480.0"},{"comment_id":"1319068","timestamp":"1732778880.0","upvote_count":"1","content":"Selected Answer: A\nThat’s right","poster":"maikerusukofyirudo"},{"timestamp":"1732502460.0","content":"Selected Answer: D\ndsfdsf","upvote_count":"1","comment_id":"1317297","poster":"chan123"},{"content":"Selected Answer: B\nThe answer is b based on the scenario. \n\nWhen to Choose Which\n\nChoose A (Metric Filters) if you need basic metrics with straightforward patterns (e.g., counting occurrences of specific API operations and response codes).\n\nChoose B (Insights Queries) if you require more complex metric calculations, such as:\nAggregations (averages, sums, etc.) over time\nFiltering metrics based on conditions within the logs\nCreating custom metrics not directly defined in log lines","poster":"nothinmuch","upvote_count":"3","timestamp":"1708887420.0","comment_id":"1159035"},{"upvote_count":"1","content":"The correct answer is A.","timestamp":"1706698440.0","comment_id":"1136696","poster":"Gowtham5798"},{"timestamp":"1706410200.0","poster":"thanhnv142","content":"A is correct: Because only need to gather metric but not parse log or advanced analyzing the log.","upvote_count":"1","comment_id":"1133780"},{"upvote_count":"1","content":"A definitely","timestamp":"1706327640.0","comment_id":"1133047","poster":"thanhnv142"},{"comment_id":"1110948","upvote_count":"1","poster":"hoakhanh281","timestamp":"1704077640.0","content":"Cloudwatch log metrics filter can apply on pattern and populate metrics, CW insight significant to get it. B should be correct one"},{"content":"Selected Answer: A\nCloudWatch Logs Insights is a powerful tool for analyzing log data, but it's more suited for ad-hoc exploration and troubleshooting rather than continuous metric collection and monitoring.","upvote_count":"3","comment_id":"1108202","poster":"d262e67","timestamp":"1703808660.0"},{"timestamp":"1702390680.0","upvote_count":"1","poster":"DucSiu","comment_id":"1094605","content":"The company needs to gather a metric for each API operation by response code for each version of the application that is in use\n=> A"},{"upvote_count":"3","poster":"wem","comment_id":"1074186","timestamp":"1700329440.0","content":"Option A seems to be the most straightforward and effective method. By writing the required information as a log line to CloudWatch Logs and configuring a metric filter to increment metrics based on this data, the company can efficiently gather the metrics it needs with minimal complexity. This approach leverages existing AWS services in a simple and direct manner, aligning well with the company's requirements."},{"poster":"DevopsCircus","comment_id":"1060612","upvote_count":"2","content":"Answer A looks pretty good except one thing \n\"Configure a CloudWatch Logs metric filter that increments a metric for each API operation name.\"\nCan the metric filter \"increment\" the metric or just send metric value?\nAnswer B looks weird to me - I don't know the way to populate Logs Insights query result as the CW metric","timestamp":"1698934680.0"},{"content":"Selected Answer: A\nThe right option is A. In the question, the requirement is to get a metric. In option B, we are not creating any metric.","timestamp":"1698336180.0","upvote_count":"1","poster":"zain1258","comment_id":"1054736"},{"poster":"sivre","timestamp":"1697118060.0","comment_id":"1041772","content":"For me its B. From the question \"The company needs to gather a metric for each API operation by response code for each version of the application that is in use\". Only CloudWatch Logs Insight is querying for response code and application version. CloudWatch Logs metric filter is using API operation name to create an aggregate metric","upvote_count":"1"},{"upvote_count":"2","poster":"rbm2023","content":"Selected Answer: A\nIt should be between A and B but the thing is the difference use cases between CW Logs Metric and CW Logs Insights. Metrics will provide Aggregated data while insights give you direct access to raw log event data. Hence I would go for A","timestamp":"1696866720.0","comment_id":"1038797"},{"upvote_count":"1","timestamp":"1693981140.0","poster":"Aestebance","content":"Selected Answer: A\nMore efficient","comment_id":"1000244"},{"poster":"addapmr","timestamp":"1693745040.0","content":"real response : A","upvote_count":"1","comment_id":"997615"},{"upvote_count":"2","poster":"habros","content":"Selected Answer: A\nA is correct. For event-based tracking, metric filters in CW are more automatic. Insights query need some intervention still. Devops = AUTOMATION. \n\nX-Ray is used extensively for development, especially backend APIs. By production it should already had resolved any API health issues.","timestamp":"1688468760.0","comment_id":"942655"},{"upvote_count":"1","poster":"mywogunleye","comment_id":"932130","timestamp":"1687577040.0","content":"Option A is the correct Answer, \n\"Note that the metric filter is different from a log insights query, where the experience is interactive and provides immediate search results for the user to investigate. No automatic action can be invoked from an insights query. Metric filters, on the other hand, will generate metric data in the form of a time series. This lets you create alarms that integrate into your ITSM processes, execute AWS Lambda functions, or even create anomaly detection models.\"\nhttps://aws.amazon.com/blogs/mt/quantify-custom-application-metrics-with-amazon-cloudwatch-logs-and-metric-filters/"},{"poster":"King01","comment_id":"929089","content":"Option A is correct because it uses the metric filter.","upvote_count":"1","timestamp":"1687325220.0"},{"timestamp":"1686527820.0","upvote_count":"1","content":"B while it is possible to use CloudWatch Logs Insights to generate metrics from log data, it is not the most efficient or straightforward approach for this particular use case. Therefore, option A, which suggests using CloudWatch metric filters to generate metrics directly from the log data, is a better and more appropriate solution.","poster":"SanChan","comment_id":"921038"},{"poster":"SanChan","content":"I think correct answer is option A: Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.\n\nOption A provides a straightforward way to collect the necessary metrics by using CloudWatch Logs to capture the relevant information and then using a metric filter to aggregate the data into the required format. Option B suggests using CloudWatch Logs Insights query to populate CloudWatch metrics, which is a more complex approach than necessary. Option C suggests using ALB access logs to write to CloudWatch Logs and returning metadata from the Lambda function, which may be more complicated to implement. Option D suggests using X-Ray integration, which is a viable option for tracing and performance analysis, but it is not the best fit for this use case.","upvote_count":"2","timestamp":"1686527700.0","comment_id":"921036"},{"upvote_count":"1","poster":"Manny20","timestamp":"1686150480.0","content":"Option A is the most suitable choice for gathering the required metrics. By modifying the Lambda function to write the necessary information to CloudWatch Logs, you can track the API operation name, response code, and version number. To generate metrics from the log lines, you can configure a CloudWatch Logs metric filter that increments a metric for each API operation name. The response code and application version can be specified as dimensions for the metric, allowing you to group and analyze the data based on these attributes.","comment_id":"917331"},{"poster":"kacsabacsi78","upvote_count":"1","timestamp":"1686031920.0","comment_id":"915947","content":"CloudWatch Logs Insights query can't populate CloudWatch metrics.\nThe correct answer is A"},{"content":"Selected Answer: D\nD. AWS X-Ray allows you to analyze and debug your applications. It provides insights into the behavior of your applications. Moreover, X-Ray supports adding annotations and metadata to traces. Annotations are indexed for use with filter expressions and can be viewed in the AWS X-Ray console and API, which could be used to extract metrics. CloudWatch can ingest metrics from AWS X-Ray, allowing you to create and monitor custom metrics.","poster":"jjkcoins","timestamp":"1685828220.0","comment_id":"913924","upvote_count":"1"},{"comment_id":"909918","content":"Selected Answer: A\nOption A - use metric filter and API version, operation, return code as dimensions.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html","timestamp":"1685424600.0","poster":"madperro","upvote_count":"2"},{"content":"Option A is the correct answer, as the other options do not provide an effective way to gather the required metrics. Launching instances in a public subnet with Elastic IP addresses attached or setting up a NAT gateway in a private subnet do not address the need to gather the required metrics. Publishing application artifacts to an S3 bucket and creating a VPC endpoint for S3 would not be relevant for gathering the required metrics either. Creating a security group for the application instances and allowing only outbound traffic to the artifact repository also does not address the need to gather the required metrics.","comment_id":"870437","timestamp":"1681501320.0","upvote_count":"1","poster":"alce2020"},{"content":"A is correct","timestamp":"1681492140.0","poster":"jqso234","comment_id":"870349","upvote_count":"1"},{"content":"Selected Answer: A\nagree, it;s A","poster":"ele","upvote_count":"1","comment_id":"863661","timestamp":"1680856920.0"},{"poster":"Dimidrol","comment_id":"862431","upvote_count":"1","timestamp":"1680723600.0","content":"Selected Answer: A\nA for me"},{"content":"Selected Answer: A\nOption A is the correct choice to gather the required metrics. The DevOps engineer can modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Then, a CloudWatch Logs metric filter can be configured to increment a metric for each API operation name, with response code and application version specified as dimensions for the metric.","comment_id":"861650","timestamp":"1680656280.0","upvote_count":"3","poster":"5aga"},{"timestamp":"1680650520.0","comment_id":"861593","upvote_count":"2","content":"B or D","poster":"lqpO_Oqpl"}],"question_text":"A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API.\nAfter a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code.\nWhich additional set of actions should the DevOps engineer take to gather the required metrics?","timestamp":"2023-04-05 01:22:00"},{"id":"6y6hjph3hqnCWQAcTJll","question_images":[],"isMC":true,"answer_description":"","topic":"1","discussion":[{"timestamp":"1706369760.0","comment_id":"1133452","poster":"thanhnv142","upvote_count":"5","content":"C is correct:\n+ Remove unauthenticated access from the S3 bucket with a bucket policy\n+ Modify the service role for the CodeBuild project to include Amazon S3 access."},{"comment_id":"1260754","poster":"namtp","content":"Selected Answer: C\nC is a correct answer.\nInside AWS, using of service roles is the best option.","timestamp":"1722793800.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1702961280.0","comment_id":"1100296","content":"Selected Answer: C\nall these questions seem fairly to be part of aws devops exam","poster":"z_inderjot"},{"content":"Selected Answer: C\nC is correct","upvote_count":"1","comment_id":"1054768","poster":"zain1258","timestamp":"1698338640.0"},{"upvote_count":"2","comment_id":"1053689","timestamp":"1698234840.0","content":"Selected Answer: C\nInvolves using a service role also, which make it the most secure manner","poster":"Cervus18"},{"upvote_count":"4","poster":"SanChan","content":"Selected Answer: C\nC is the correct answer because it involves removing unauthenticated access from the S3 bucket with a bucket policy, which ensures that only authorized users or services can access the bucket.","timestamp":"1686545700.0","comment_id":"921127"},{"timestamp":"1686218760.0","poster":"madperro","upvote_count":"1","comment_id":"918100","content":"Selected Answer: C\nC is the best answer."},{"upvote_count":"2","poster":"alce2020","timestamp":"1681502880.0","comment_id":"870449","content":"c is the answer"},{"comment_id":"866505","content":"c is the answer.","upvote_count":"1","timestamp":"1681149600.0","poster":"ataince"},{"poster":"ele","content":"Selected Answer: C\nC most secure","comment_id":"863856","upvote_count":"1","timestamp":"1680870240.0"}],"question_id":2,"exam_id":23,"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/105505-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_ET":"C","question_text":"A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project.\nHow can this issue be corrected in the MOST secure manner?","timestamp":"2023-04-07 14:24:00","choices":{"D":"Remove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.","A":"Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.","B":"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.","C":"Remove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access. Use the AWS CLI to download the database population script."},"unix_timestamp":1680870240,"answer_images":[],"answers_community":["C (100%)"]},{"id":"S6gJ30vLZwKvnj4JRaNs","answer":"B","exam_id":23,"answers_community":["B (67%)","C (29%)","3%"],"unix_timestamp":1683634740,"answer_ET":"B","isMC":true,"question_text":"A DevOps engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect and process web logs. The DevOps engineer manages the Kinesis consumer application, which also runs on Amazon EC2.\n\nSudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records can be processed. The DevOps engineer must implement a solution to improve stream handling.\n\nWhich solution meets these requirements with the MOST operational efficiency?","question_images":[],"topic":"1","discussion":[{"timestamp":"1689587820.0","comment_id":"954054","content":"The answer is B because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose B.","poster":"emupsx1","comments":[{"upvote_count":"3","content":"Remember, only 50 of the questions are graded. \nFirst, there is no way what you said is true.\nSecond, your answer might not be graded. \nCorrect answer is a firm C.","timestamp":"1734734340.0","comment_id":"1329734","poster":"youonebe"},{"upvote_count":"2","poster":"[Removed]","content":"Were there any other questions from here in the exam?","comment_id":"1012165","timestamp":"1695202800.0"},{"content":"First of all Congratulations.\nNow how do you know that this question was not from those 10 questions that do not count towards your score. and your answer to this question was Wrong but not counted towards your score. Just Saying!!\nPeace :)","timestamp":"1700898960.0","comments":[{"comment_id":"1117412","content":"B is the Answer, let him show off, it is ok.","timestamp":"1704797280.0","poster":"Jaguaroooo","upvote_count":"5"}],"comment_id":"1079818","poster":"yorkicurke","upvote_count":"1"}],"upvote_count":"41"},{"poster":"yorkicurke","upvote_count":"6","timestamp":"1700899380.0","comment_id":"1079822","content":"Selected Answer: B\nwhy not C?\nbecause we just replace ONE ec2 with ONE lambda here. And no mention of aws lambda reserved concurrency or provisioned concurrency.\nIn the question were are asked for 'MOST operational efficiency'. that's my two cents.\nCiao"},{"timestamp":"1742212680.0","poster":"DKM","upvote_count":"1","content":"Selected Answer: B\nThe GetRecords.IteratorAgeMilliseconds metric in Amazon Kinesis Data Streams measures the age of the last record in all GetRecords calls made against a Kinesis stream. This age is calculated as the difference between the current time and when the last record of the GetRecords call was written to the stream.\n\nThis metric is crucial for monitoring the latency of your Kinesis consumer applications. If the IteratorAgeMilliseconds value is increasing, it could indicate issues such as slow record processing, read throttles, or connection timeouts. Monitoring this metric helps ensure that your consumer applications are processing records in a timely manner and not falling behind.","comment_id":"1399625"},{"poster":"ce0df07","content":"Selected Answer: B\nOption B because:\n1. It uses GetRecords.IteratorAgeMilliseconds metric\n - This metric indicates how far behind the consumer is processing\n - Perfect indicator for when scaling is needed\n - Automatically detects processing lag\n2. Horizontal scaling of EC2 consumers\n - Adds processing capacity when needed\n - Can scale down when demand decreases\n - Maintains existing application architecture\n 3. Increasing retention period\n - Provides buffer against temporary processing delays\n - Prevents data loss during scaling events\n - Default is 24 hours, can be increased up to 365 days","comment_id":"1352111","upvote_count":"1","timestamp":"1738795020.0"},{"upvote_count":"1","comment_id":"1344137","poster":"2d943d1","content":"Selected Answer: C\nI think most people are missing the part where the question focuses on MOST operational efficiency. Lambda fits this purpose, as ww are not managing instances and Lambda can scale to meet throughput demands.","timestamp":"1737462000.0"},{"poster":"ZinggieG87","comment_id":"1331009","timestamp":"1735009680.0","content":"Selected Answer: D\nA, sounds a lot effort towards a different architecture solution. \nB, I don't think the insufficient kinesis throughput has can be resolved.\nC, adding lambda is a big change, no mention about the concurrency limit isn't clear mentioned in the question.","upvote_count":"1"},{"comment_id":"1329731","content":"Selected Answer: C\nAnswer is C.\n\nB - This solution improves the consumer application’s ability to handle increased throughput, but it still requires manual scaling of EC2 instances, which can involve some complexity and operational overhead.\nC - Using Lambda is obviously more Operational Efficient.","timestamp":"1734734220.0","poster":"youonebe","upvote_count":"1"},{"content":"Selected Answer: B\nI think B makes more sence, because \"the Kinesis data streams drop records before the records can be processed\". It's not an intake throughput issue that needs more shards, but an outtake throughput issue.\n\"Consumer Record Processing Falling Behind\"\n\"After you identify how far behind your consumers are reading, look at the most common reasons why consumers fall behind. Start with the GetRecords.IteratorAgeMilliseconds metric, which tracks the read position across all shards and consumers in the stream. Note that if an iterator's age passes 50% of the retention period (by default, 24 hours...), there is risk for data loss due to record expiration.\"\n\"A quick stopgap solution is to increase the retention period.\"\n[...]\n\"An alternative approach is to increase your parallelism by increasing the number of shards.\"\n\"Finally, confirm you have an adequate amount of physical resources (memory, CPU utilization, etc.) on the underlying processing nodes during peak demand.\"\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","poster":"Gomer","upvote_count":"2","timestamp":"1718652060.0","comment_id":"1232080"},{"poster":"zijo","timestamp":"1717779540.0","comment_id":"1226284","content":"B is a good choice\nThe GetRecords.IteratorAgeMilliseconds metric in Amazon CloudWatch for Amazon Kinesis Data Streams measures the age of the last record returned by the GetRecords operation. Specifically, it represents the time difference between the current time and the approximate arrival timestamp of the last record processed by a consumer in milliseconds.\n\nPurpose: Measures the delay in processing data records from the time they are added to the stream to the time they are processed by a consumer.\n\nScaling: If you observe high iterator age values, consider increasing the number of shards or enhancing the processing capacity of your consumers (e.g., adding more instances or increasing the processing power of existing instances).","upvote_count":"1"},{"upvote_count":"1","comment_id":"1205450","poster":"seetpt","timestamp":"1714646400.0","content":"Selected Answer: B\nI choose B"},{"content":"Selected Answer: B\nConsumer Record Processing Falling Behind\n\nFor most use cases, consumer applications are reading the latest data from the stream. In certain circumstances, consumer reads may fall behind, which may not be desired. After you identify how far behind your consumers are reading, look at the most common reasons why consumers fall behind.\n\nStart with the GetRecords.IteratorAgeMilliseconds metric, which tracks the read position across all shards and consumers in the stream. Note that if an iterator's age passes 50% of the retention period (by default, 24 hours, configurable up to 365 days), there is risk for data loss due to record expiration. A quick stopgap solution is to increase the retention period. This stops the loss of important data while you troubleshoot the issue further. https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#record-processing-falls-behind","comment_id":"1198036","upvote_count":"4","timestamp":"1713458640.0","poster":"c3518fc"},{"poster":"Shasha1","upvote_count":"2","comment_id":"1165517","timestamp":"1709549880.0","content":"B\n GetRecords.IteratorAgeMilliseconds metric : its for track the progress of Kinese consumer, this cloud watch matric is use for the measuring the difference between current time and when the last record of GetRecords calls written to the stream. IteratorAgeMilliseconds metric is 0 means is processing fast enough and if its >0 its slow in processing. Therefore, Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams."},{"comment_id":"1151997","upvote_count":"2","poster":"kyuhuck","timestamp":"1708087620.0","content":"Selected Answer: C\noption C (Converting the Kinesis consumer application to run as an AWS Lambda function) is the most suitable solution. This approach automatically scales with the amount of incoming data, reduces the operational burden of managing EC2 instances, and leverages the serverless model to only incur costs for the actual compute time used for processing the data. This option provides a scalable, efficient, and cost-effective solution to the problem without the need for extensive infrastructure management."},{"poster":"thanhnv142","upvote_count":"1","content":"B is correct: <consumer application to fall behind> means we need to increase the power of the consumer. <Kinesis data streams drop records> means we should extend timeout. Only B match these requirements\nA: irrelevant\nC: Lambda is used only for short-lived tasks because its maximum execution time is 15 min. In this case, we need to process web logs. This is a time-consuming task, which is not suitable for Lambda\nD: No mention of increasing Consumer power","timestamp":"1706978760.0","comment_id":"1139470"},{"content":"B & D are correct. However, the question is looking for a solution for two issues\n\"application to fall behind, and the Kinesis data streams drop records before the records can be processed\"\nThen, B is the most appropriate solution","poster":"svjl","upvote_count":"3","comment_id":"1087217","timestamp":"1701648480.0"},{"comment_id":"1062929","content":"B is the answer: The data fall beind due to lack of pysical resources at consumer side. Icrease of more nodes will address this issue. https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#record-processing-falls-behind","poster":"2pk","upvote_count":"1","timestamp":"1699191660.0"},{"timestamp":"1696074240.0","content":"Selected Answer: B\nI would say B","comment_id":"1021423","poster":"buenos","upvote_count":"2"},{"content":"Selected Answer: D\nD. Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application processes the data faster.\n\nHere's the rationale for choosing this option:\n\nIncreasing Shards for Throughput:\nBy increasing the number of shards in the Kinesis data streams, you increase the overall throughput and the capacity to handle sudden increases in data. This directly addresses the issue of the consumer application falling behind during data spikes.\n\nOperational Efficiency:\nScaling the shards provides a more straightforward and efficient solution in terms of operation compared to modifying the consumer application, horizontally scaling instances, or converting the application to run as an AWS Lambda function.","comment_id":"1014425","timestamp":"1695410040.0","upvote_count":"1","poster":"Dushank"},{"poster":"Seoyong","content":"Selected Answer: C\nB is not correct. it manually scale EC2 instances.\nC is operationally efficiency .","comment_id":"994938","upvote_count":"1","timestamp":"1693471020.0"},{"content":"Selected Answer: B\nThe answer is B","poster":"Radeeka","comment_id":"991345","timestamp":"1693128780.0","upvote_count":"2"},{"upvote_count":"2","poster":"ProfXsamson","timestamp":"1693020840.0","comment_id":"990492","content":"Selected Answer: B\nB, Scaling based on GetRecords.IteratorAgeMilliseconds is right. GetRecords.IteratorAgeMilliseconds - The age of the last record in all GetRecords calls made against a Kinesis stream, measured over the specified time period. Age is the difference between the current time and when the last record of the GetRecords call was written to the stream. The Minimum and Maximum statistics can be used to track the progress of Kinesis consumer applications. A value of zero indicates that the records being read are completely caught up with the stream."},{"comment_id":"969593","upvote_count":"4","poster":"gigi_devops","timestamp":"1690945320.0","content":"Selected Answer: B\nLambda has some limitations like the execution time(15min), cocurrency limit."},{"timestamp":"1690389780.0","comment_id":"963988","content":"Selected Answer: C\nC. Converting the Kinesis consumer application to run as an AWS Lambda function is a highly efficient solution. AWS Lambda automatically scales based on the number of incoming events, so it can easily handle sudden increases in data without manual intervention. Additionally, AWS Lambda manages the underlying infrastructure, eliminating the need for the DevOps engineer to manage EC2 instances and their scaling.\n\nB. Horizontally scaling the Kinesis consumer application by adding more EC2 instances based on the GetRecords.IteratorAgeMilliseconds metric and increasing the retention period of the Kinesis data streams might improve the consumer application's ability to catch up with sudden data increases. However, it might not be the most efficient solution, as it requires manual intervention and additional resource management.","poster":"haazybanj","upvote_count":"4"},{"poster":"Just_Ninja","upvote_count":"2","comment_id":"950034","timestamp":"1689181980.0","content":"Selected Answer: B\nOption A: Storing the logs durably in Amazon S3 and using Amazon EMR to process the data adds complexity and potential cost. It also may not address the core issue of the Kinesis consumer application not keeping up with incoming data.\n\nOption C: AWS Lambda functions have some limitations (e.g., execution time, payload size, concurrency limits) which can create bottlenecks in processing large data streams.\n\nOption D: Increasing the number of shards in the Kinesis data stream may increase throughput, but it doesn't necessarily address the problem if the consumer application isn't able to process the data quickly enough. Moreover, this approach might incur higher costs."},{"poster":"Blueee","comment_id":"943596","content":"Selected Answer: B\nI will go with B","upvote_count":"2","timestamp":"1688554140.0"},{"upvote_count":"1","comment_id":"933526","poster":"FunkyFresco","content":"Selected Answer: C\nI will choose option C.","timestamp":"1687693080.0"},{"content":"I think B is correct. Operational Efficiency doesn't mean the best solution. In this case, they have develop the app using EC2 and lack autoscaling. The option B will be the simplest change to fulfil the requirement.","timestamp":"1687205820.0","comment_id":"927889","poster":"allen_devops","upvote_count":"2"},{"upvote_count":"3","poster":"ducluanxutrieu","timestamp":"1686987840.0","comment_id":"925842","content":"Selected Answer: B\nI'll go with option B"},{"comment_id":"924951","upvote_count":"3","poster":"rhinozD","timestamp":"1686900120.0","content":"Selected Answer: B\nI think the current flow is lack scaling.\nSo adding scaling will solve this problem.\nBy using GetRecords.IteratorAgeMilliseconds metric, you can track the progress of Kinesis Consumers and scale them.\nSo B."},{"timestamp":"1684561680.0","content":"Selected Answer: C\nC, with the MOST operational efficiency.","upvote_count":"4","poster":"PhuocT","comment_id":"902372"},{"content":"Selected Answer: B\nYes B, \nbecause it directly addresses the issue of the Kinesis consumer application falling behind due to sudden increases of data. By horizontally scaling the application with more EC2 instances, the application can handle more data and keep up with the stream. Increasing the retention period of the data streams ensures that the consumer application has more time to process the data.\nthe reason why not D, because increasing the number of shards in the Kinesis data streams will not necessarily solve the problem of the Kinesis consumer application falling behind.","timestamp":"1684101780.0","upvote_count":"2","poster":"2pk","comment_id":"897900"},{"comment_id":"897423","poster":"devnv","upvote_count":"1","timestamp":"1684055160.0","content":"B is the right answer"},{"comment_id":"894890","upvote_count":"2","content":"Selected Answer: B\nB is correct","timestamp":"1683797220.0","poster":"ParagSanyashiv"},{"timestamp":"1683634740.0","content":"Selected Answer: C\nI think it is C","upvote_count":"3","comment_id":"893078","poster":"Jeanphi72"}],"answer_images":[],"timestamp":"2023-05-09 14:19:00","choices":{"A":"Modify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on Amazon S3 to derive customer insights. Store the results in Amazon S3.","B":"Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams.","C":"Convert the Kinesis consumer application to run as an AWS Lambda function. Configure the Kinesis data streams as the event source for the Lambda function to process the data streams.","D":"Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application processes the data faster."},"question_id":3,"url":"https://www.examtopics.com/discussions/amazon/view/108805-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":""},{"id":"o5ZT4lCUHj9T88qKlTbc","question_images":[],"answer_description":"","isMC":true,"topic":"1","discussion":[{"comment_id":"954057","upvote_count":"17","content":"The answer is ACE because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose ACE.","timestamp":"1689587880.0","poster":"emupsx1","comments":[{"comment_id":"1002996","upvote_count":"7","poster":"BaburTurk","comments":[{"comment_id":"1232092","poster":"Gomer","upvote_count":"3","timestamp":"1718653860.0","content":"Either a bot or a bot for brains. Same useless comments made on multiple questions."}],"timestamp":"1694242500.0","content":"bot account, Pics or it didn't happen,"}]},{"upvote_count":"1","timestamp":"1733121120.0","comment_id":"1320809","content":"Selected Answer: ADE\nI prefer \"D\" over \"C\", because no-one asks to enable SSO (which is very complex to organise and maintain)","poster":"eugene2owl"},{"comment_id":"1254628","timestamp":"1721865600.0","poster":"auxwww","upvote_count":"3","content":"Selected Answer: ACE\nA - Only security team needs access to findings org wide - hence delegated account\nC - Allow security team members access to delegated account for Security hub using Identity center of control tower\nE - Each new account needs security hub for it's own users to access and also for aggregation across org"},{"content":"Automatic enablement in AWS Security Hub refers to the feature that allows AWS Security Hub to be automatically enabled for new and existing AWS accounts that are part of an organization in AWS Organizations. This feature simplifies the process of onboarding multiple accounts into Security Hub, ensuring consistent security posture and compliance across the organization.","poster":"zijo","upvote_count":"2","comment_id":"1227982","timestamp":"1718035500.0"},{"upvote_count":"2","comment_id":"1205451","timestamp":"1714646460.0","poster":"seetpt","content":"Selected Answer: ACE\nACE is correct"},{"comments":[],"timestamp":"1713256260.0","content":"Selected Answer: ACF\nACF\nE - ensures that all new accounts are automatically enrolled in Security Hub (same as F) but it does not address the requirement for specific users to view findings from their own accounts","upvote_count":"2","comment_id":"1196470","poster":"didek1986"},{"content":"ACF\nE - ensures that all new accounts are automatically enrolled in Security Hub (same as F) but it does not address the requirement for specific users to view findings from their own accounts","poster":"didek1986","comment_id":"1196469","upvote_count":"1","timestamp":"1713256200.0"},{"upvote_count":"2","timestamp":"1713070260.0","content":"Selected Answer: ACE\nace are correct answer","comment_id":"1195270","poster":"dkp"},{"timestamp":"1707018660.0","poster":"thanhnv142","content":"ACE are correct: <Only the security team can be allowed to view aggregated Security Hub findings> means we need a delegated admin. <All accounts must be enrolled in Security Hub after the accounts are created> and <in the MOST automated way> means we need enable automatic enablement\nB: no mention of delegated admin\nD: This options denied access of the security team, which is irrelevant\nF: This option's result is the same as in option E, but more complicated","upvote_count":"2","comment_id":"1139756"},{"content":"Selected Answer: ADE\nAccording to this article .. The Delegated account users have access in ANY account while the users under own account can view their own findings. So, there is no need to setup IAM policies for Security account users. https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-accounts-allowed-actions.html","poster":"2pk","comment_id":"1062916","timestamp":"1699190820.0","upvote_count":"1"},{"upvote_count":"3","poster":"YR4591","timestamp":"1698961980.0","comment_id":"1060938","content":"Selected Answer: ACE\nA - Create delegate account for the security hub\nC - Give access to users to security using permissions sets\nE - Use auto enable so every new account will be monitored by security hub"},{"comment_id":"952624","upvote_count":"2","timestamp":"1689448200.0","content":"Selected Answer: ACE\nACE are the correct answers.","poster":"sb333"},{"timestamp":"1689249840.0","content":"Selected Answer: ACE\nACE. Reason being, it is a landing zone and AWS SSO (IAM IC) is already part of the Control Tower product! Add security dept users as a SSO group and attach the security permission set to access security hub","poster":"habros","comment_id":"950644","upvote_count":"2"},{"content":"Selected Answer: ACE\nwith Control Tower comes the Identity Center implementation with default Identity Center directory.","comment_id":"934725","timestamp":"1687805640.0","upvote_count":"3","poster":"Wardove"},{"upvote_count":"2","content":"Selected Answer: ADE\nB is not the typical way AWS separates responsabilities in multi account (management, sec, audit)\nC is related with Active Directory \nE is more automated than F","comment_id":"906758","timestamp":"1685028720.0","comments":[{"poster":"jnv007","timestamp":"1688667360.0","comment_id":"944918","upvote_count":"1","content":"Identity Center is not exclusively related to Active Directory\nAn SCP can only prevent access but doesnt enable any access, so D is not sufficient\nACE for me \nhttps://docs.aws.amazon.com/securityhub/latest/userguide/accounts-orgs-auto-enable.html"}],"poster":"robotgeek"},{"comment_id":"905225","upvote_count":"4","timestamp":"1684875240.0","content":"ACF IS MORE EFFICIENT","poster":"Kodoma"},{"comment_id":"896465","content":"Selected Answer: ADE\nAde for me","timestamp":"1683963360.0","poster":"Dimidrol","upvote_count":"1"}],"question_id":4,"exam_id":23,"answer":"ACE","url":"https://www.examtopics.com/discussions/amazon/view/109130-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_ET":"ACE","question_text":"A company recently created a new AWS Control Tower landing zone in a new organization in AWS Organizations. The landing zone must be able to demonstrate compliance with the Center for Internet Security (CIS) Benchmarks for AWS Foundations.\n\nThe company’s security team wants to use AWS Security Hub to view compliance across all accounts. Only the security team can be allowed to view aggregated Security Hub findings. In addition, specific users must be able to view findings from their own accounts within the organization. All accounts must be enrolled in Security Hub after the accounts are created.\n\nWhich combination of steps will meet these requirements in the MOST automated way? (Choose three.)","timestamp":"2023-05-13 09:36:00","choices":{"C":"Create an AWS IAM Identity Center (AWS Single Sign-On) permission set that includes the required permissions. Use the CreateAccountAssignment API operation to associate the security team users with the permission set and with the delegated security account.","B":"Turn on trusted access for Security Hub in the organization’s management account. From the management account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.","D":"Create an SCP that explicitly denies any user who is not on the security team from accessing Security Hub.","F":"In the organization’s management account, create an Amazon EventBridge rule that reacts to the CreateManagedAccount event. Create an AWS Lambda function that uses the Security Hub CreateMembers API operation to add new accounts to Security Hub. Configure the EventBridge rule to invoke the Lambda function.","E":"In Security Hub, turn on automatic enablement.","A":"Turn on trusted access for Security Hub in the organization’s management account. Create a new security account by using AWS Control Tower. Configure the new security account as the delegated administrator account for Security Hub. In the new security account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards."},"unix_timestamp":1683963360,"answer_images":[],"answers_community":["ACE (71%)","ADE (21%)","8%"]},{"id":"KIwGrxdCaXGYVYRPUr9S","question_id":5,"timestamp":"2023-05-09 13:21:00","answer_description":"","topic":"1","discussion":[{"comment_id":"950044","poster":"Just_Ninja","content":"Selected Answer: A\nDear Admin, Please Fix the Wrong response here! \nIt´s A:\nThis solution meets all the requirements:\n\nDetect potentially compromised EC2 instances, suspicious network activity, and unusual API activity: Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior. It analyzes events from AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs to detect such activities.\n\nSend a notification to the operational support team: Creating an Amazon EventBridge rule that matches GuardDuty findings and then forwarding these to an SNS topic allows for the generation of notifications whenever suspicious activity is detected.\n\nCover future AWS accounts: By designating a GuardDuty administrator account in AWS Organizations, you can manage GuardDuty across all of your existing and future AWS accounts. This ensures that any new account created under the organization is automatically covered by GuardDuty.","timestamp":"1689182640.0","upvote_count":"12"},{"content":"Selected Answer: B\nB is the right answer","poster":"Mrflip","comment_id":"1338507","upvote_count":"1","timestamp":"1736455740.0"},{"comment_id":"1261086","upvote_count":"2","poster":"jamesf","timestamp":"1722867720.0","content":"Selected Answer: A\nkeywords: compromised EC2 instances, suspicious network activity, and unusual API activity\n= GuardDuty"},{"content":"Selected Answer: A\nWhen you use GuardDuty with an AWS organization, the management account of that organization can designate any account within the organization as the delegated GuardDuty administrator account. For this administrator account, GuardDuty gets enabled automatically only in the designated AWS Region. This account also has the permission to enable and manage GuardDuty for all of the accounts in the organization within that Region. The administrator account can view the members of and add members to this AWS organization.\n\nAWS GuardDuty can detect unusual API activity within existing AWS accounts in an AWS Organization. It monitors AWS CloudTrail event logs, which include records of all API calls made within your AWS environment. GuardDuty analyzes these logs to identify unusual or suspicious API activity that might indicate a potential security threat.","comment_id":"1227991","poster":"zijo","upvote_count":"2","timestamp":"1718036880.0"},{"content":"A looks like a better choice.\nWhen you use GuardDuty with an AWS organization, the management account of that organization can designate any account within the organization as the delegated GuardDuty administrator account. For this administrator account, GuardDuty gets enabled automatically only in the designated AWS Region. This account also has the permission to enable and manage GuardDuty for all of the accounts in the organization within that Region. The administrator account can view the members of and add members to this AWS organization.","poster":"zijo","upvote_count":"1","timestamp":"1718036580.0","comment_id":"1227989"},{"content":"Selected Answer: A\nanswer A","upvote_count":"2","timestamp":"1713070980.0","poster":"dkp","comment_id":"1195276"},{"comment_id":"1194808","poster":"Mordans","timestamp":"1712997840.0","content":"If GuardDuty is indeed set up at the organization level (which is supported and encouraged by AWS for simplicity and coverage), then Option A becomes a very strong choice. It provides centralized management and automatic, seamless inclusion of all organization accounts in security monitoring without requiring manual intervention for each new account.","upvote_count":"1"},{"timestamp":"1711442760.0","upvote_count":"1","poster":"stoy123","comment_id":"1183142","content":"Selected Answer: B\nDefinitely B"},{"comment_id":"1139758","upvote_count":"3","timestamp":"1707019140.0","poster":"thanhnv142","content":"Selected Answer: A\nA is correct: <detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity> means AWS GuardDuty\nB: dont have to invite other accounts because all accounts are in an org in AWS org.\nC and D: no mention of GuardDuty"},{"comment_id":"1120811","upvote_count":"3","poster":"a54b16f","timestamp":"1705067160.0","content":"Selected Answer: A\ninvitation is used to handle users OUTSIDE the organization."},{"comment_id":"1116955","upvote_count":"2","content":"Selected Answer: A\nGo For A. \nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html","timestamp":"1704743040.0","poster":"davdan99"},{"timestamp":"1699783920.0","upvote_count":"2","poster":"saysamsuf","comment_id":"1068415","content":"Selected Answer: B\nMember accounts must accept invite from the designated guard duty account before its effect. I use AWS organisation at work and quite familiar with the workings. I lean towards B"},{"timestamp":"1699189140.0","comment_id":"1062898","poster":"2pk","upvote_count":"1","content":"It true it's missing auto enabled on. but Invitation is organization is not needed as Organization get precedence with account management when you have deligated Guardduty account. \"If you have already set up a GuardDuty administrator account with associated member accounts by invitation and the member accounts are part of the same organization, their Type changes from By Invitation to Via Organizations\" https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html"},{"content":"B is the correct answer.\nA could better if not for the fact that it doesn't handle automatic enablement on new AWS account.\nB handles this case with CloudFormation stacksets : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-guardduty-master.html","timestamp":"1692105660.0","upvote_count":"2","poster":"lakescix","comment_id":"981701","comments":[{"content":"A= does handle automatic enablement. If the GD delegated account is setup properly with automatic enablement check box ticked. As soon as the AWS account is created, GD auto enablement kicks into gear. B = How does CFN accept the GD invite? New AWS Account. CFN runs on new account > accept GD invite...but when was this invite sent? I have to login to AWS Console > GD > create invite vs A = no invite = directly enabled for GB in new account. https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html","poster":"lunt","timestamp":"1692364860.0","upvote_count":"2","comment_id":"984519"}]},{"upvote_count":"3","comment_id":"980144","poster":"lunt","timestamp":"1691944680.0","content":"Selected Answer: A\nB is wrong. Newly created AWS accounts = you don't need to do this if the GD Orgz is configured properly, you can accept from delegated admin account. The point remains, you can add the accounts using option A. The misdirect here is that A does not state anything about new accounts vs B which does. Bearing in mind A + B still have to do something in GD, A is actually the better option.\nA is right."},{"comment_id":"974896","poster":"jason7","content":"Selected Answer: B\nOption A is not the best choice because although it correctly configures GuardDuty as the administrator, it does not handle the automatic addition of new AWS accounts to GuardDuty and the forwarding of events to the SNS topic","upvote_count":"1","timestamp":"1691432160.0"},{"comment_id":"974895","poster":"jason7","upvote_count":"1","content":"Option B is the most suitable solution as it combines GuardDuty, AWS CloudFormation StackSets, and Amazon EventBridge to automatically monitor all existing and future AWS accounts and send notifications to the specified SNS topic when security events are detected.","timestamp":"1691432100.0"},{"poster":"gigi_devops","comment_id":"970205","timestamp":"1690982040.0","upvote_count":"2","content":"Selected Answer: A\nCannot be A, it does not deal with future accounts at all."},{"comment_id":"947210","content":"Selected Answer: A\nA for me. \n\nB technically sounds about right (to have an invitation accepter), but it does not address allocating a delegated admin for GD, which is required for account grouped under an Organization. https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html","timestamp":"1688905320.0","poster":"habros","upvote_count":"2"},{"comments":[{"upvote_count":"1","comment_id":"947032","content":"have you taken the exam yet? is this dump valid?","poster":"johnslayer123","timestamp":"1688892960.0"}],"timestamp":"1688364720.0","poster":"pepecastr0","comment_id":"941502","upvote_count":"2","content":"Selected Answer: A\nA - Once you are administrator you can assign other accounts as members, no need to send invitation"},{"comment_id":"925167","timestamp":"1686919500.0","comments":[{"comment_id":"941501","timestamp":"1688364660.0","content":"I agree","upvote_count":"1","poster":"pepecastr0"}],"content":"A is correct.\n\"If the account that you want to specify as the GuardDuty administrator account is part of an organization in AWS Organizations, then you can specify that account as the organization's delegated administrator for GuardDuty. The account that is registered as the delegated administrator automatically becomes the GuardDuty administrator account.\n\nYou can use this administrator account to enable and manage GuardDuty for any account in the organization when you add that account as a member account.\"\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html","upvote_count":"3","poster":"rhinozD"},{"content":"I got A as my answer","comment_id":"899236","timestamp":"1684244040.0","upvote_count":"1","poster":"OrganizedChaos25"},{"upvote_count":"2","timestamp":"1684240680.0","poster":"Mail1964","comment_id":"899184","content":"Selected Answer: A\nFor me A makes the most sense. The q states they are in an AWS organisation, invitations are when you are not using AWS organisations. https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html"},{"timestamp":"1684119120.0","comments":[{"comment_id":"925166","content":"Actually, None of your answers makes sense if you don't give any explanation.","upvote_count":"5","timestamp":"1686919440.0","poster":"rhinozD"}],"poster":"ParagSanyashiv","comment_id":"898014","upvote_count":"1","content":"Selected Answer: B\nB makes more sense. Ignore the previous response."},{"timestamp":"1684057740.0","poster":"devnv","comment_id":"897447","content":"A is the right answer","upvote_count":"1"},{"upvote_count":"1","comment_id":"894908","comments":[],"content":"Selected Answer: A\nA seems more suitable.","timestamp":"1683798060.0","poster":"ParagSanyashiv"},{"comment_id":"893043","poster":"Jeanphi72","content":"Selected Answer: A\n\" In the GuardDuty administrator account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic.\" seems correct, B is really weird.","timestamp":"1683631260.0","upvote_count":"1"}],"answer_ET":"A","answer_images":[],"exam_id":23,"isMC":true,"answer":"A","choices":{"A":"In the organization’s management account, configure an AWS account as the Amazon GuardDuty administrator account. In the GuardDuty administrator account, add the company’s existing AWS accounts to GuardDuty as members. In the GuardDuty administrator account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic.","D":"In the organization’s management account, configure an AWS account as the AWS CloudTrail administrator account. In the CloudTrail administrator account, create a CloudTrail organization trail. Add the company’s existing AWS accounts to the organization trail. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.","B":"In the organization’s management account, configure Amazon GuardDuty to add newly created AWS accounts by invitation and to send invitations to the existing AWS accounts. Create an AWS CloudFormation stack set that accepts the GuardDuty invitation and creates an Amazon EventBridge rule. Configure the rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Configure the CloudFormation stack set to deploy into all AWS accounts in the organization.","C":"In the organization’s management account, create an AWS CloudTrail organization trail. Activate the organization trail in all AWS accounts in the organization. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic."},"answers_community":["A (87%)","13%"],"question_text":"A company runs applications in AWS accounts that are in an organization in AWS Organizations. The applications use Amazon EC2 instances and Amazon S3.\n\nThe company wants to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS accounts and in any AWS accounts that the company creates in the future. When the company detects one of these events, the company wants to use an existing Amazon Simple Notification Service (Amazon SNS) topic to send a notification to its operational support team for investigation and remediation.\n\nWhich solution will meet these requirements in accordance with AWS best practices?","url":"https://www.examtopics.com/discussions/amazon/view/108803-exam-aws-certified-devops-engineer-professional-dop-c02/","unix_timestamp":1683631260,"question_images":[]}],"exam":{"numberOfQuestions":355,"name":"AWS Certified DevOps Engineer - Professional DOP-C02","isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":23,"provider":"Amazon","isImplemented":true},"currentPage":1},"__N_SSP":true}