{"pageProps":{"questions":[{"id":"TXZkNyEdRRYN7kSsZ197","topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/132694-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":171,"isMC":true,"question_images":[],"exam_id":21,"choices":{"D":"Change the format of the files that are in the dataset to Apache Parquet.","C":"Add an Amazon ElastiCache cluster between the BI application and Athena.","A":"Configure an Amazon S3 Lifecycle policy to move data to the S3 Glacier Deep Archive storage class after 1 day.","B":"Use the query result reuse feature of Amazon Athena for the SQL queries."},"answer_ET":"B","timestamp":"2024-02-02 13:31:00","unix_timestamp":1706877060,"answer":"B","answer_images":[],"answers_community":["B (91%)","9%"],"question_text":"A financial company wants to use Amazon Athena to run on-demand SQL queries on a petabyte-scale dataset to support a business intelligence (BI) application. An AWS Glue job that runs during non-business hours updates the dataset once every day. The BI application has a standard data refresh frequency of 1 hour to comply with company policies.\nA data engineer wants to cost optimize the company's use of Amazon Athena without adding any additional infrastructure costs.\nWhich solution will meet these requirements with the LEAST operational overhead?","discussion":[{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html\nUse the Query Result Reuse Feature of Amazon Athena. This leverages Athena's built-in feature to reduce redundant data scans and thus lowers query costs.","upvote_count":"6","timestamp":"1706877060.0","comments":[{"content":"Yes, seems to be B: https://aws.amazon.com/de/blogs/big-data/reduce-cost-and-improve-query-performance-with-amazon-athena-query-result-reuse/","upvote_count":"1","comment_id":"1207318","timestamp":"1714995300.0","poster":"DevoteamAnalytix"}],"comment_id":"1138517","poster":"rralucard_"},{"comment_id":"1361480","timestamp":"1740496020.0","content":"Selected Answer: D\nD\nquery result reuse will benefit the same queries that are being re-run, it wont benefit new queries. parquet will benefit all queries.","upvote_count":"1","poster":"Ell89"},{"timestamp":"1729594980.0","upvote_count":"1","comment_id":"1301534","poster":"rsmf","content":"Selected Answer: B\nWhy not D? The question specifies the option with the least overhead, and it clearly states that the Glue job runs once a day. Since the data for that day will not change, thereâ€™s no need for additional overhead."},{"timestamp":"1722582420.0","comment_id":"1259744","content":"D. Because \"query reuse feature\" is reliable only when it's identical but here hourly refresh might be on data related to that hour.","upvote_count":"1","poster":"MinTheRanger"},{"comment_id":"1259418","timestamp":"1722521100.0","content":"Why not D?","poster":"MinTheRanger","upvote_count":"3"},{"comment_id":"1196840","timestamp":"1713304320.0","upvote_count":"2","content":"Selected Answer: B\nB. Use the query result reuse feature of Amazon Athena for the SQL queries.","poster":"Ousseyni"},{"poster":"FuriouZ","comment_id":"1184119","content":"Selected Answer: B\nIt's B: Glacier adds more retrieval time and the other options cost some money","upvote_count":"1","timestamp":"1711545120.0"}]},{"id":"Oor5ZbSRCetBdy92wq4f","exam_id":21,"question_text":"A company's data engineer needs to optimize the performance of table SQL queries. The company stores data in an Amazon Redshift cluster. The data engineer cannot increase the size of the cluster because of budget constraints.\nThe company stores the data in multiple tables and loads the data by using the EVEN distribution style. Some tables are hundreds of gigabytes in size. Other tables are less than 10 MB in size.\nWhich solution will meet these requirements?","choices":{"A":"Keep using the EVEN distribution style for all tables. Specify primary and foreign keys for all tables.","D":"Specify a combination of distribution, sort, and partition keys for all tables.","B":"Use the ALL distribution style for large tables. Specify primary and foreign keys for all tables.","C":"Use the ALL distribution style for rarely updated small tables. Specify primary and foreign keys for all tables."},"answers_community":["C (100%)"],"answer_images":[],"isMC":true,"answer":"C","timestamp":"2024-02-02 13:39:00","answer_description":"","question_id":172,"discussion":[{"comment_id":"1138522","poster":"rralucard_","content":"Selected Answer: C\nUse the ALL Distribution Style for Rarely Updated Small Tables. This approach optimizes the performance of joins involving these smaller tables and is a common best practice in Redshift data warehousing. For the larger tables, maintaining the EVEN distribution style or considering a KEY-based distribution (if there are common join columns) could be more appropriate.","timestamp":"1706877540.0","upvote_count":"8"},{"poster":"jk15997","content":"why not D?","upvote_count":"3","comment_id":"1310413","timestamp":"1731379260.0"},{"timestamp":"1718148180.0","comment_id":"1228701","poster":"pypelyncar","content":"Selected Answer: C\nFor small tables (less than 10 MB in size) that are rarely updated, using the ALL distribution style can provide better query performance. With the ALL distribution style, each compute node stores a copy of the entire table, eliminating the need for data redistribution or shuffling during certain queries. This can significantly improve query performance, especially for joins and aggregations involving small tables.","upvote_count":"3"},{"content":"Selected Answer: C\n\"ALL distribution is appropriate only for relatively slow moving tables; that is, tables that are not updated frequently or extensively.\" (https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html)","timestamp":"1714995840.0","poster":"DevoteamAnalytix","comment_id":"1207322","upvote_count":"2"}],"answer_ET":"C","question_images":[],"unix_timestamp":1706877540,"url":"https://www.examtopics.com/discussions/amazon/view/132695-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1"},{"id":"XhN4qorEZugGY1k6u3rS","question_id":173,"question_text":"A company receives .csv files that contain physical address data. The data is in columns that have the following names: Door_No, Street_Name, City, and Zip_Code. The company wants to create a single column to store these values in the following format:\n//IMG//\n\nWhich solution will meet this requirement with the LEAST coding effort?","discussion":[{"content":"Selected Answer: B\nNEST_TO_ARRAY would result in:\n[ {\"key\": \"key1\", \"value\": \"value1\"}, {\"key\": \"key2\", \"value\": \"value2\"}, {\"key\": \"key3\", \"value\": \"value3\"}]\n\nwhile NEST_TO_MAP results: {\n \"key1\": \"value1\",\n \"key2\": \"value2\",\n \"key3\": \"value3\"\n}\nTherefore go with B","comment_id":"1181412","timestamp":"1711272540.0","poster":"FuriouZ","upvote_count":"12"},{"comment_id":"1228703","timestamp":"1718148360.0","poster":"pypelyncar","upvote_count":"3","content":"Selected Answer: B\nThe NEST_TO_MAP transformation is specifically designed to convert data from nested structures (like rows in a CSV) into key-value pairs,\nperfectly matching the requirement of creating a new column with address components as key-value pairs"},{"timestamp":"1713304680.0","upvote_count":"4","comment_id":"1196842","content":"Selected Answer: B\nAWS Glue DataBrew is a visual data preparation tool that allows for easy transformation of data without requiring extensive coding. The NEST_TO_MAP transformation in DataBrew allows you to convert columns into a JSON map, which aligns with the desired JSON format for the address data.","poster":"Ousseyni"},{"poster":"GiorgioGss","comment_id":"1177241","content":"Selected Answer: A\nCome on guys. That's and array there so...","timestamp":"1710846960.0","upvote_count":"1","comments":[{"content":"I take that back. I will go with B because NEST_TO_ARRAY is not suitable for the desired JSON format where each attribute has its own key.","timestamp":"1710954960.0","poster":"GiorgioGss","upvote_count":"2","comment_id":"1178538"}]},{"timestamp":"1710411000.0","comment_id":"1173310","upvote_count":"2","content":"Option B:\nNEST_TO_MAP: Converts user-selected columns into key-value pairs, each with a key representing the column name and a value representing the row value. The order of the selected column is not maintained while creating the resultant map. The different column data types are typecast to a common type that supports the data types of all columns.\nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_MAP.html\n\nPIVOT: Converts all the row values in a selected column into individual columns with values.\n\nNEST_TO_ARRAY: Converts user-selected columns into array values. The order of the selected columns is maintained while creating the resultant array. The different column data types are typecast to a common type that supports the data types of all columns.","poster":"kj07"},{"content":"Ans. A\nNEST_TO_ARRAY Converts user-selected columns into array values. The order of the selected columns is maintained while creating the resultant array. \nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.NEST_TO_ARRAY.html","comment_id":"1168062","upvote_count":"1","timestamp":"1709820060.0","poster":"damaldon"}],"answer_description":"","answer":"B","unix_timestamp":1709820060,"answer_images":[],"exam_id":21,"answers_community":["B (95%)","5%"],"answer_ET":"B","timestamp":"2024-03-07 15:01:00","question_images":["https://img.examtopics.com/aws-certified-data-engineer-associate-dea-c01/image1.png"],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/135424-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"A":"Use AWS Glue DataBrew to read the files. Use the NEST_TO_ARRAY transformation to create the new column.","C":"Use AWS Glue DataBrew to read the files. Use the PIVOT transformation to create the new column.","B":"Use AWS Glue DataBrew to read the files. Use the NEST_TO_MAP transformation to create the new column.","D":"Write a Lambda function in Python to read the files. Use the Python data dictionary type to create the new column."},"isMC":true},{"id":"cZ42nzaegEGf7uUn6DBW","answers_community":["C (100%)"],"timestamp":"2024-02-02 13:47:00","unix_timestamp":1706878020,"question_id":174,"answer_ET":"C","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/132696-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"isMC":true,"choices":{"D":"Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the Amazon S3 managed keys that encrypt the objects.","C":"Use server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the KMS keys that encrypt the objects.","B":"Use server-side encryption with customer-provided keys (SSE-C) to encrypt the objects that contain customer information. Restrict access to the keys that encrypt the objects.","A":"Use an AWS CloudHSM cluster to store the encryption keys. Configure the process that writes to Amazon S3 to make calls to CloudHSM to encrypt and decrypt the objects. Deploy an IAM policy that restricts access to the CloudHSM cluster."},"answer":"C","question_text":"A company receives call logs as Amazon S3 objects that contain sensitive customer information. The company must protect the S3 objects by using encryption. The company must also use encryption keys that only specific employees can access.\nWhich solution will meet these requirements with the LEAST effort?","topic":"1","question_images":[],"answer_description":"","discussion":[{"content":"Selected Answer: C\nC. Use server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the KMS keys that encrypt the objects.\n\nServer-side encryption with AWS KMS (SSE-KMS) provides strong encryption for S3 objects while allowing fine-grained access control through AWS Key Management Service (KMS). With SSE-KMS, you can control access to encryption keys using IAM policies, ensuring that only specific employees can access them.\n\nThis solution requires minimal effort as it leverages AWS's managed encryption service (SSE-KMS) and integrates seamlessly with S3. Additionally, IAM policies can be easily configured to restrict access to the KMS keys, providing granular control over who can access the encryption keys.","timestamp":"1729116000.0","upvote_count":"3","poster":"Ousseyni","comment_id":"1196843"},{"upvote_count":"3","content":"Selected Answer: C\nEncryption at Rest: SSE-KMS provides robust encryption of the sensitive call log data while it's stored in S3.\nKey Management and Access Control: AWS KMS offers centralized key management. You can easily create and manage KMS keys (Customer Master Keys - CMKs) and use fine-grained IAM policies to restrict access to specific employees.\nMinimal Effort: SSE-KMS is a built-in S3 feature. Enabling it requires minimal configuration and no custom code for encryption/decryption.","poster":"Christina666","comment_id":"1194771","timestamp":"1728806160.0"},{"upvote_count":"1","timestamp":"1727443380.0","comment_id":"1184201","content":"Selected Answer: C\nKMS because you can restrict access and of course for pricing ;)","poster":"FuriouZ"},{"timestamp":"1726737900.0","upvote_count":"4","comment_id":"1177248","content":"Selected Answer: C\nLeast effort = C","poster":"GiorgioGss"},{"upvote_count":"4","timestamp":"1722595620.0","poster":"rralucard_","comment_id":"1138534","content":"Selected Answer: C\nOption D does not provide the ability to restrict access to the encryption keys"}]},{"id":"qTQ0caTJet4BbUPTXc1b","isMC":true,"question_images":[],"timestamp":"2024-01-18 09:35:00","answer_description":"","question_text":"A financial services company stores financial data in Amazon Redshift. A data engineer wants to run real-time queries on the financial data to support a web-based trading application. The data engineer wants to run the queries from within the trading application.\nWhich solution will meet these requirements with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/131470-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","unix_timestamp":1705566900,"answer_ET":"B","exam_id":21,"choices":{"B":"Use the Amazon Redshift Data API.","C":"Set up Java Database Connectivity (JDBC) connections to Amazon Redshift.","D":"Store frequently accessed data in Amazon S3. Use Amazon S3 Select to run the queries.","A":"Establish WebSocket connections to Amazon Redshift."},"answer":"B","answer_images":[],"answers_community":["B (100%)"],"topic":"1","question_id":175,"discussion":[{"timestamp":"1744082640.0","poster":"ninomfr64","comment_id":"1558783","upvote_count":"1","content":"Selected Answer: B\nYou can query a Redshift cluster with either JDBC/ODBC or Data API. The latter just requires you to maintain AWS SDK and IAM role, while the former needs network plumbing to access VPC, JDBC/ODBC driver, database credentials possibly stored in Secret Manager"},{"timestamp":"1742820780.0","content":"Selected Answer: B\nB is correct","comment_id":"1409665","poster":"Scotty_Nguyen","upvote_count":"1"},{"content":"Selected Answer: B\nMost efficient solution","comment_id":"1388081","upvote_count":"1","poster":"Palee","timestamp":"1741812120.0"},{"poster":"bhawna901","timestamp":"1732086480.0","comment_id":"1315087","content":"Amazon Redshift Data API:\nProvides a serverless and simple HTTP-based API to interact with Redshift.\nIdeal for web-based applications since it eliminates the need to manage persistent database connections (like JDBC or ODBC).\nAllows the trading application to send queries directly to Redshift using HTTPS requests, making it easy to integrate with modern applications.\nRemoves the complexity of managing database connection pooling in real-time, which reduces operational overhead.\nSecurely integrates with IAM roles and policies for authentication and access control.","upvote_count":"2"},{"content":"Selected Answer: B\nA) Redshift doesn't support WebSockets;\nC) It is way harder to manage DB connections than using Redshift Data API which will offer you the possibility to run SQL queries directly.\nD)","timestamp":"1726002780.0","comment_id":"1281789","upvote_count":"1","poster":"markill123"},{"poster":"04e06cb","timestamp":"1720352160.0","content":"Selected Answer: B\nB is correct","upvote_count":"1","comment_id":"1243838"},{"content":"Selected Answer: B\nInside application with minimal effort then using API would be correct","comment_id":"1209085","timestamp":"1715292840.0","upvote_count":"2","poster":"k350Secops"},{"upvote_count":"3","timestamp":"1714721940.0","poster":"DevoteamAnalytix","comment_id":"1205975","content":"Selected Answer: B\n\"The Amazon Redshift Data API enables you to painlessly access data from Amazon Redshift with all types of traditional, cloud-native, and containerized, serverless web service-based applications and event-driven applications.\"\nhttps://aws.amazon.com/de/blogs/big-data/using-the-amazon-redshift-data-api-to-interact-with-amazon-redshift-clusters/#:~:text=The%20Amazon%20Redshift%20Data%20API%20is%20not%20a%20replacement%20for,supported%20by%20the%20AWS%20SDK."},{"content":"Selected Answer: B\nEven if you don't know nothing about them, you will still choose B because it seems the \"LEAST operational overhead\" :)","poster":"GiorgioGss","comment_id":"1171034","upvote_count":"3","timestamp":"1710163140.0"},{"poster":"Alcee","comment_id":"1158039","upvote_count":"1","timestamp":"1708792380.0","content":"B. DATA API"},{"content":"B. Use the Amazon Redshift Data API.\n\nExplanation:\nThe Amazon Redshift Data API is a lightweight, HTTPS-based API that provides an alternative to using JDBC or ODBC drivers for running queries against Amazon Redshift. It allows you to execute SQL queries directly from within your application without the need for managing connections or drivers. This reduces operational overhead as there's no need to manage and maintain WebSocket or JDBC connections.","upvote_count":"4","comment_id":"1137882","timestamp":"1706817540.0","poster":"TonyStark0122"},{"content":"Selected Answer: B\nReal time queries with S3 are obviously BS. B it is:\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html","timestamp":"1705566900.0","comment_id":"1125633","upvote_count":"4","poster":"milofficial"}]}],"exam":{"isImplemented":true,"isMCOnly":true,"id":21,"lastUpdated":"11 Apr 2025","name":"AWS Certified Data Engineer - Associate DEA-C01","provider":"Amazon","isBeta":false,"numberOfQuestions":207},"currentPage":35},"__N_SSP":true}