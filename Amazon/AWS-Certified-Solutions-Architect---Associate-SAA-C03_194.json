{"pageProps":{"questions":[{"id":"7o4ixugEFjzIrE3SyeZl","answer_ET":"C","answer":"C","topic":"1","choices":{"B":"Create an IAM policy to allow access to the AWS Management Console only from a defined set of corporate IP addresses. Restrict user access based on job responsibility by using an IAM policy and roles.","C":"Configure AWS Site-to-Site VPN to connect to the VPConfigure route table entries to direct traffic from on premises to the VPConfigure instance security groups and network ACLs to allow only required traffic from on premises.","A":"Configure AWS Direct Connect to connect to the VPC. Configure the VPC route tables to allow and deny traffic between AWS and on premises as required.","D":"Configure AWS Transit Gateway to connect to the VPC. Configure route table entries to direct traffic from on premises to the VPC. Configure instance security groups and network ACLs to allow only required traffic from on premises."},"url":"https://www.examtopics.com/discussions/amazon/view/144938-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"poster":"Abbas_Abi_AWS","content":"Selected Answer: C\nAWS Direct Connect does not provide encryption by itself; it is often used in conjunction with VPN for encrypted traffic. Direct Connect primarily offers a dedicated connection and does not inherently satisfy the encryption requirement.","comment_id":"1263350","timestamp":"1723273260.0","upvote_count":"6"},{"content":"Selected Answer: C\nA - Direct Connect provides private connectivity but does not inherently include encryption.\nB - IAM policies is irrelevent.\nC - AWS Site-to-Site VPN uses IPsec (network layer) and SSL/TLS (session layer) to encrypt all traffic between the on-premises network and the AWS VPC.\nD - Just like option A.","upvote_count":"2","timestamp":"1736056380.0","poster":"LeonSauveterre","comment_id":"1336652"},{"upvote_count":"2","timestamp":"1735336860.0","poster":"Anyio","comment_id":"1332645","content":"Selected Answer: C\nThe correct answer is C. Configure AWS Site-to-Site VPN to connect to the VPC. Configure route table entries to direct traffic from on premises to the VPC. Configure instance security groups and network ACLs to allow only required traffic from on premises.\n\nExplanation:\n\nOption C: Correct. AWS Site-to-Site VPN offers encrypted network connections over the internet, providing encryption of all traffic at the network layer (IPsec) between the on-premises network and the VPC. Moreover, by using route tables, security groups, and network ACLs, you can carefully control the flow of traffic and restrict access, thereby meeting the requirement of preventing unrestricted access."},{"poster":"blehbleh","upvote_count":"3","comment_id":"1295134","timestamp":"1728475800.0","content":"Selected Answer: C\nThis is C, but not for all the reasons everyone is posting. D, also encrypts traffic and works at the network layer and also has security controls to prevent unrestricted access between AWS and on-premises systems.\n\nSo, if you thought D like I did initially you were very close. The reason it is C, is because C works at both the network and session layer while doing all the other requirements as well. Where as D only works at the network layer. \n\nHappy studying!"},{"timestamp":"1723984080.0","upvote_count":"2","content":"Selected Answer: C\nC is correct","poster":"[Removed]","comment_id":"1268051"},{"timestamp":"1722678120.0","comments":[{"content":"Inter-Region gateway peering uses the same network infrastructure as VPC peering. Therefore traffic is encrypted using AES-256 encryption at the virtual network layer as it travels between Regions. Traffic is also encrypted using AES-256 encryption at the physical layer when it traverses network links that are outside of the physical control of AWS. As a result, traffic is double encrypted on network links outside the physical control of AWS. Within the same Region, traffic is encrypted at the physical layer only when it traverses network links that are outside of the physical control of AWS.","comment_id":"1341940","timestamp":"1737078780.0","poster":"FlyingHawk","upvote_count":"1"}],"content":"Selected Answer: D\nAnswer is D","comment_id":"1260206","poster":"komorebi","upvote_count":"1"},{"content":"Selected Answer: C\nC is correct question needs to access between on prem and AWS","comment_id":"1260193","upvote_count":"4","poster":"JunsK1e","timestamp":"1722676200.0"}],"isMC":true,"exam_id":31,"answer_images":[],"unix_timestamp":1722676200,"timestamp":"2024-08-03 11:10:00","question_id":966,"answers_community":["C (95%)","5%"],"answer_description":"","question_text":"A solutions architect needs to connect a company's corporate network to its VPC to allow on-premises access to its AWS resources. The solution must provide encryption of all traffic between the corporate network and the VPC at the network layer and the session layer. The solution also must provide security controls to prevent unrestricted access between AWS and the on-premises systems.\n\nWhich solution meets these requirements?","question_images":[]},{"id":"Jw0Tr9MODTNHuP02ePP6","question_images":[],"question_text":"A company has a custom application with embedded credentials that retrieves information from a database in an Amazon RDS for MySQL DB cluster. The company needs to make the application more secure with minimal programming effort. The company has created credentials on the RDS for MySQL database for the application user.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/145017-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":967,"isMC":true,"answer_description":"","exam_id":31,"answer":"C","unix_timestamp":1722822240,"answer_ET":"C","discussion":[{"content":"Selected Answer: C\nC\nExplanation:\nAWS Secrets Manager is designed specifically for managing and automatically rotating credentials, including database credentials, API keys, and other secrets. It provides a secure and centralized place to store credentials and allows applications to retrieve them securely without hardcoding them in the application.\nSecrets Manager also offers built-in support for automatic rotation of credentials using Lambda functions, which reduces the manual effort needed for rotation and enhances security.\nThis approach requires minimal programming effort because the application only needs to be configured to retrieve the credentials from Secrets Manager instead of being embedded within the application code.","poster":"[Removed]","upvote_count":"5","comment_id":"1268054","timestamp":"1723984320.0"},{"upvote_count":"2","content":"Selected Answer: C\nA - AWS KMS is used for encryption and key management, not for storing and rotating credentials themselves.\nB - Why manually set up a cron job?\nC - Securely stores and manages sensitive information such as credentials, and provides native support for automatic rotation.\nD - Parameter Store doesn't provide automatic rotation.","comment_id":"1336653","poster":"LeonSauveterre","timestamp":"1736056620.0"},{"content":"Selected Answer: C\nThe correct answer is C. Store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule by creating an AWS Lambda function for Secrets Manager.\n\nExplanation:\n\nOption C: Correct. AWS Secrets Manager is designed to securely store, manage, and retrieve database credentials. It supports automatic credentials rotation for Amazon RDS databases with minimal programming effort. Secrets Manager can automatically rotate the Amazon RDS database credentials using a built-in Lambda function, providing an integrated and secure solution for handling credentials.","upvote_count":"1","comment_id":"1332644","timestamp":"1735336800.0","poster":"Anyio"},{"poster":"Omshanti","timestamp":"1727547000.0","upvote_count":"2","comment_id":"1290742","content":"Selected Answer: C\nAWS Secret manager securely stores data base user id and passwords"},{"upvote_count":"2","timestamp":"1722847620.0","comment_id":"1260987","content":"Selected Answer: C\nAnswer is C","poster":"komorebi"}],"choices":{"C":"Store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule by creating an AWS Lambda function for Secrets Manager.","B":"Store the credentials in encrypted local storage. Configure the application to load the database credentials from the local storage. Set up a credentials rotation schedule by creating a cron job.","D":"Store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule in the RDS for MySQL database by using Parameter Store.","A":"Store the credentials in AWS Key Management Service (AWS KMS). Create keys in AWS KMS. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation"},"answers_community":["C (100%)"],"timestamp":"2024-08-05 03:44:00","topic":"1","answer_images":[]},{"id":"Fmn4MUWchJe4tElKEzOy","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/144939-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","discussion":[{"content":"Selected Answer: A\nA wins because it gives us encryption with AWS KMS multi-Region keys","timestamp":"1724190540.0","poster":"dhewa","upvote_count":"7","comment_id":"1269710"},{"upvote_count":"1","comment_id":"1537998","content":"Selected Answer: A\nA is correct.\n\neveryone. voting for c i think isnt paying attention to the part where you need the data replicated to a different region. how are you meant to replicate it without a new bucket?\n\neveryone saying use the existing bucket.... urm what? how can you replicate into the same bucket thats in the same region and expect it to be in a different region? \n\nclearly A.","poster":"Ell89","timestamp":"1743860880.0"},{"comment_id":"1400525","upvote_count":"1","poster":"tch","content":"Selected Answer: C\nwe don't need to create a policy specifically for Amazon S3-managed keys (SSE-S3) to encrypt objects in your S3 bucket... should be more easy and LEAST operational overhead","timestamp":"1742388420.0"},{"timestamp":"1741435500.0","upvote_count":"1","comment_id":"1366547","poster":"yangbo","content":"Selected Answer: C\nThe default encryption for S3 is SSE-S3, and cross-region replication can be enabled normally."},{"content":"Selected Answer: A\nAmazon S3 managed keys is region specific, for CRR, we must use KMS mult-region keys. https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html","timestamp":"1737081960.0","poster":"FlyingHawk","comment_id":"1341963","upvote_count":"3","comments":[{"poster":"FlyingHawk","content":"I checked AI, ChatGPT, Gemini , Claudi AI and DeepSeek think A is correct, but meta AI thinks C is correct. SSE-S3 is the default encryption, it will be least operational effort for sure. but with SSE-S3 (server-side encryption with S3-managed keys), AWS manages the encryption keys, and each region will use its own region-specific encryption key.\nThere is no shared key between the source and destination buckets. AWS handles encryption and decryption seamlessly in each region. This approach is operationally simple but lacks control and consistency for encryption keys across regions.","timestamp":"1737765720.0","upvote_count":"1","comment_id":"1346269"}]},{"content":"Selected Answer: C\nA - A new bucket + KMS multi-Region keys = Too much operational oversight.\nB - RDS is not a serverless solution\nC - By using the existing S3 bucket, you eliminate the need to create a new bucket and load data into it.\nD - Athena supports querying using SQL, so that rules out RDS.","upvote_count":"4","timestamp":"1736056920.0","poster":"LeonSauveterre","comment_id":"1336656"},{"upvote_count":"1","comment_id":"1336319","content":"Selected Answer: C\nIt’s not require highly sensitive data or complexity gain permission. So it should use sse-s3 is suitable","timestamp":"1735982940.0","poster":"trinh_le"},{"timestamp":"1735240860.0","content":"Selected Answer: C\nThe correct answer is C.\n\nExplanation:\n\nOption A: Incorrect. Although using AWS KMS multi-Region keys (SSE-KMS) and Amazon Athena to query the data meet the security and SQL querying requirements, creating a new S3 bucket and handling data migration increases operational overhead unnecessarily if the data already exists.\n\nOption C: Correct. Configuring Cross-Region Replication (CRR) on the existing S3 bucket makes efficient use of existing infrastructure and leverages server-side encryption with Amazon S3 managed keys (SSE-S3) to ensure data encryption at rest with lower operational complexity and costs compared to using KMS keys. Using Amazon Athena allows querying the data directly in S3, offering serverless and flexible SQL querying capabilities with minimal setup and operational overhead.","comment_id":"1332091","poster":"Anyio","upvote_count":"1"},{"content":"Selected Answer: C\nI will chose Option C for the following reasons:\n\n#1: Least operational overhead: Choosing \"SSE-S3\" over \"SSE-KMS\" minimizes operational overhead as it automatically manages encryption keys within S3, eliminating the need for additional KMS key management.\n\n#2: Existing S3 bucket: Reusing the existing bucket avoids the extra step of creating a new one and migrating data.\n\n#3: Athena for querying: Athena is a serverless solution ideal for querying large datasets stored in S3, aligning with the requirement for a serverless architecture.","timestamp":"1733144760.0","upvote_count":"2","comment_id":"1320953","comments":[{"comment_id":"1320954","upvote_count":"2","poster":"JA2018","timestamp":"1733144820.0","content":"For Option A: While using KMS multi-region keys provides more control, it adds extra management complexity compared to SSE-S3."}],"poster":"JA2018"},{"poster":"Abdullah2004","comment_id":"1275299","upvote_count":"3","content":"Selected Answer: A\nA is correct","timestamp":"1725059820.0"},{"poster":"komorebi","comment_id":"1260207","timestamp":"1722678180.0","upvote_count":"4","content":"Selected Answer: A\nAnswer is A"},{"content":"Selected Answer: C\nC is correct because it needs to replicate to different AWS region","upvote_count":"2","poster":"JunsK1e","timestamp":"1722676380.0","comment_id":"1260194"}],"answer_description":"","unix_timestamp":1722676380,"isMC":true,"answers_community":["A (60%)","C (40%)"],"answer_ET":"A","question_images":[],"exam_id":31,"timestamp":"2024-08-03 11:13:00","question_text":"A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing data and new data by using SQL. The company stores the data in an Amazon S3 bucket. The data must be encrypted at rest and replicated to a different AWS Region.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","choices":{"D":"Configure S3 Cross-Region Replication (CRR) on the existing S3 bucket. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.","B":"Create a new S3 bucket that uses server-side encryption with Amazon S3 managed keys (SSE-S3). Configure Cross-Region Replication (CRR). Load the data into the new S3 bucket. Use Amazon RDS to query the data.","C":"Configure Cross-Region Replication (CRR) on the existing S3 bucket. Use server-side encryption with Amazon S3 managed keys (SSE-S3). Use Amazon Athena to query the data.","A":"Create a new S3 bucket that uses server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Configure Cross-Region Replication (CRR). Load the data into the new S3 bucket. Use Amazon Athena to query the data."},"answer_images":[],"question_id":968},{"id":"qiwR1dgCciZS9Qcxaxxz","unix_timestamp":1723376760,"answer_images":[],"isMC":true,"topic":"1","question_images":[],"answer_description":"","question_text":"A company has a web application that has thousands of users. The application uses 8-10 user-uploaded images to generate AI images. Users can download the generated AI images once every 6 hours. The company also has a premium user option that gives users the ability to download the generated AI images anytime.\n\nThe company uses the user-uploaded images to run AI model training twice a year. The company needs a storage solution to store the images.\n\nWhich storage solution meets these requirements MOST cost-effectively?","answer_ET":"A","timestamp":"2024-08-11 13:46:00","exam_id":31,"answer":"A","answers_community":["A (63%)","C (30%)","7%"],"choices":{"C":"Move uploaded images to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Move premium user-generated AI images to S3 Standard. Move non-premium user-generated AI images to S3 Standard-Infrequent Access (S3 Standard-IA).","D":"Move uploaded images to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Move all generated AI images to S3 Glacier Flexible Retrieval.","B":"Move uploaded images to Amazon S3 Glacier Deep Archive Move all generated AI images to S3 Glacier Flexible Retrieval.","A":"Move uploaded images to Amazon S3 Glacier Deep Archive. Move premium user-generated AI images to S3 Standard. Move non-premium user-generated AI images to S3 Standard-Infrequent Access (S3 Standard-IA)."},"question_id":969,"url":"https://www.examtopics.com/discussions/amazon/view/145540-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"comment_id":"1269712","poster":"dhewa","upvote_count":"7","comments":[{"upvote_count":"2","comment_id":"1320961","timestamp":"1733145120.0","content":"But moving uploaded images to Glacier Deep Archive is not ideal as it has a very high retrieval cost and long retrieval time, which is not suitable for the occasional access needed for AI model training.","poster":"JA2018"}],"content":"Selected Answer: A\nThe company uses the user-uploaded images to run AI model training twice a year. So for this Deep Archive will be ncessary.","timestamp":"1724191080.0"},{"timestamp":"1728478200.0","comment_id":"1295156","poster":"blehbleh","upvote_count":"5","content":"Selected Answer: A\nThis is A. We care about cost effectiveness. In regards to images being used twice a year \"For images accessed only twice a year, S3 Glacier Deep Archive would be the more cost-effective option compared to S3 One Zone Infrequent Access, as it is designed for extremely infrequent access and offers the lowest storage cost within AWS S3 storage classes; while S3 One Zone Infrequent Access is cheaper than standard Infrequent Access, it still might be slightly more expensive for data accessed as rarely as twice a year.\" In regards to premium users keep them standard. Non premium users can be in the infrequent since they have 6 hrs. \n\nCost Effective!","comments":[]},{"timestamp":"1741399860.0","poster":"tch","content":"Selected Answer: A\nS3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year\n\nB& D are out.....","upvote_count":"2","comment_id":"1366464"},{"upvote_count":"2","content":"Selected Answer: C\nA - Deep Archive is designed for long-term cold storage and can spend hours to retrieve each object. For AI model training, you'd hate it if you must spend hundreds/thousands of hours before you can actually start training it.\nB - Premium users would be so mad if they had to wait for hours to get their generated images.\nC - S3 One Zone-IA reduces costs, and it's great now that we don't need high durability across multiple availability zones. Premium users will experience low latency to get their generated images.\nD - Glacier tiers are not designed for frequent access. This is bad practice.\n\nOne more thing about option C, since training happens extremely infrequently and the images can be re-uploaded if necessary, One Zone IA's lower redundancy is an acceptable trade-off for cost savings.","comment_id":"1336925","timestamp":"1736127360.0","poster":"LeonSauveterre"},{"timestamp":"1732869060.0","upvote_count":"3","comment_id":"1319621","content":"Selected Answer: A\nI choose A. But it not best answer. for standard-ia you get data immediately but pay for request which not make sense for requirement. The best fit of non-premium user is Glacier Fleixble standard(1-5hour)","comments":[],"poster":"Cpso"},{"timestamp":"1728848340.0","comments":[],"poster":"Oghare","comment_id":"1297093","content":"this is A\nS3 Glacier Deep Archive is the most cost-effective storage for long-term infrequently accessed data, ideal for the user-uploaded images used twice a year for AI training.\nS3 Standard for premium user-generated AI images ensures fast access, which is needed for frequent downloads.\nS3 Standard-IA for non-premium user-generated AI images is cost-effective for less frequent access, as it charges lower for storage but slightly more for retrieval, which fits the 6-hour download frequency.\nNot C because, although S3 One Zone-IA is a lower-cost option as well it provides less durability because it stores data in only one Availability Zone. While it is cost-effective, it increases the risk of data loss for critical AI training data.\n\nB and D is out \nS3 Glacier Flexible Retrieval for all generated AI images would likely introduce unacceptable retrieval delays for premium users, as they require immediate access to download images","upvote_count":"3"},{"comments":[{"timestamp":"1728560280.0","upvote_count":"3","comment_id":"1295558","content":"it says \"Users can download the generated AI images once every 6 hours.\" The upload images are used for training only 2 times a year which meets the s3 glacier deep dive. The standard users are put in infrequent access and premium users are put in standard. I don't understand how people are messing this up.","poster":"blehbleh"}],"poster":"kbgsgsgs","timestamp":"1727956200.0","upvote_count":"1","comment_id":"1292772","content":"Selected Answer: C\nS3 Glacier Deep Archive does not meet the 6-hour download requirement because it takes time to access data. User can download even if you are not a premium user"},{"timestamp":"1723985040.0","comments":[{"comment_id":"1320962","content":"Option B is using Glacier Flexible Retrieval.\n\nStoring all generated AI images in Glacier Flexible Retrieval is also not cost-effective as the premium users require faster access to their images.","timestamp":"1733145180.0","poster":"JA2018","upvote_count":"1"}],"upvote_count":"2","poster":"[Removed]","content":"Selected Answer: B\nB is correct\n\nExplanation:\nS3 Glacier Deep Archive is the most cost-effective storage option for data that is rarely accessed. Since the user-uploaded images are only used twice a year for AI model training, storing them in Glacier Deep Archive is ideal for minimizing costs. The longer retrieval time (up to 12 hours) is acceptable given the infrequent access.\n\nS3 Glacier Flexible Retrieval is suitable for storing the generated AI images because it balances cost and retrieval time. Regular users can download images every 6 hours, which Glacier Flexible Retrieval can accommodate with its flexible retrieval options (ranging from minutes to hours). This solution also works for premium users, who might need more frequent access. While S3 Standard or Standard-IA could be used, Glacier Flexible Retrieval offers significant cost savings while still meeting the access requirements.","comment_id":"1268056"},{"timestamp":"1723917540.0","upvote_count":"5","poster":"officedepotadmin","comment_id":"1267805","content":"Selected Answer: C\nS3 One Zone-IA is a cost-effective storage option for images that are accessed infrequently but are still needed for AI model training twice a year. One Zone-IA stores data in a single Availability Zone, making it less expensive but still highly available within that zone. Premium users need frequent access to their AI-generated images so S3. Non-premium users access their AI-generated images less frequently (once every 6 hours) so S3 Standard-IA"},{"content":"A is correct as Glacier deep archive provides the lowest-cost storage class.","upvote_count":"1","comment_id":"1264080","timestamp":"1723376760.0","comments":[],"poster":"pujithacg8"}]},{"id":"uJEIXo7wtv3o1s1B7uog","answer_description":"","answer_ET":"D","answer":"D","topic":"1","question_id":970,"question_text":"A company is developing machine learning (ML) models on AWS. The company is developing the ML models as independent microservices. The microservices fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the ML models through an asynchronous API. Users can send a request or a batch of requests.\n\nThe company provides the ML models to hundreds of users. The usage patterns for the models are irregular. Some models are not used for days or weeks. Other models receive batches of thousands of requests at a time.\n\nWhich solution will meet these requirements?","answers_community":["D (64%)","B (18%)","C (18%)"],"choices":{"C":"Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the ML models as AWS Lambda functions that SQS events will invoke. Use auto scaling to increase the number of vCPUs for the Lambda functions based on the size of the SQS queue.","D":"Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the ML models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Use auto scaling for Amazon ECS to scale both the cluster capacity and number of the services based on the size of the SQS queue.","A":"Direct the requests from the API to a Network Load Balancer (NLB). Deploy the ML models as AWS Lambda functions that the NLB will invoke. Use auto scaling to scale the Lambda functions based on the traffic that the NLB receives.","B":"Direct the requests from the API to an Application Load Balancer (ALB). Deploy the ML models as Amazon Elastic Container Service (Amazon ECS) services that the ALB will invoke. Use auto scaling to scale the ECS cluster instances based on the traffic that the ALB receives."},"unix_timestamp":1723985340,"url":"https://www.examtopics.com/discussions/amazon/view/145943-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-08-18 14:49:00","isMC":true,"discussion":[{"upvote_count":"5","comment_id":"1268057","timestamp":"1723985340.0","content":"Selected Answer: D\nD is the answer:\nSQS Queue: Directing API requests to SQS decouples the API from ML processing, efficiently handles high traffic, and ensures reliable request processing without overloading the ML models.\n\nAmazon ECS Services: Running ML models on ECS provides effective management of containerized applications, ideal for handling ML workloads.\n\nAuto Scaling: ECS auto scales based on SQS queue size, adjusting container and cluster capacity to match demand, ensuring efficient handling of varying workloads.","poster":"[Removed]"},{"content":"Selected Answer: D\nGiven that the problem statement does not specify the processing time, Option D (ECS) is the safer choice because it does not have the same limitations as Lambda(cold start and 15 ,minutes execution). However, if the processing time is short and cold starts are acceptable, Option C (Lambda) could also be a cost-effective solution.","poster":"FlyingHawk","timestamp":"1737086700.0","upvote_count":"1","comment_id":"1341982"},{"content":"Selected Answer: D\nA - Lambda has a cold start latency. There are ways to optimize that (like the environment variable config which I actually did when I used Lambda once), but loading 1GB into memory is still too much.\nB - Using ALB makes handling sudden bursts of traffic even more challenging.\nC - Just like A if I'm correct.\nD - Each ML model needs to load 1 GB of model data at startup ==> Containers in ECS are well-suited for this because they provide persistent execution environments that avoid the overhead of reloading the model data frequently.","poster":"LeonSauveterre","comment_id":"1336929","timestamp":"1736128620.0","upvote_count":"1"},{"poster":"RamanAgarwal","upvote_count":"2","timestamp":"1734327720.0","content":"Selected Answer: C\nSince AWS Lambda supports 1GB of memory and can be scaled seamlessly, the ans should be C. Question says a lot of APIs are not used frequently so keeping them in ECS will result in higher costs. Also these are async operations means the response is not time bound hence we can wait for lambda to startup and scale up based on the size of the SQS queue.","comment_id":"1327170"},{"content":"Selected Answer: B\nWhy should I use SQS in option D? Wouldn't ALB be enough?","poster":"kbgsgsgs","upvote_count":"2","comment_id":"1292774","timestamp":"1727956500.0","comments":[{"timestamp":"1731636660.0","poster":"Sergantus","upvote_count":"1","comment_id":"1312368","content":"It's talking about accessing models through an asynchronous API, so decoupling is needed (SQS)"}]}],"answer_images":[],"exam_id":31,"question_images":[]}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":31,"isImplemented":true,"provider":"Amazon","name":"AWS Certified Solutions Architect - Associate SAA-C03","numberOfQuestions":1019},"currentPage":194},"__N_SSP":true}