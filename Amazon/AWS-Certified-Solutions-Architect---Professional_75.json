{"pageProps":{"questions":[{"id":"d2etv1vGJLC2yrvi3eLC","question_text":"A Solutions Architect is working with a company that is extremely sensitive to its IT costs and wishes to implement controls that will result in a predictable AWS spend each month.\nWhich combination of steps can help the company control and monitor its monthly AWS usage to achieve a cost that is as close as possible to the target amount?\n(Choose three.)","timestamp":"2019-10-01 09:10:00","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/5916-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"AEF","topic":"1","answers_community":["AEF (100%)"],"question_images":[],"exam_id":32,"unix_timestamp":1569913800,"discussion":[{"upvote_count":"33","poster":"donathon","content":"AEF\nB: not feasible.\nC: Not everything is applicable to RI. E.g. S3 does not have RI.\nD: If they chose a very big instance, the bill could still be big.","timestamp":"1632103860.0","comment_id":"13383"},{"comment_id":"283989","upvote_count":"8","poster":"Ebi","content":"AEF for sure","timestamp":"1635379980.0"},{"poster":"yama234","timestamp":"1681637520.0","content":"AEF\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html","upvote_count":"2","comment_id":"871630"},{"timestamp":"1665767760.0","comment_id":"694930","content":"Selected Answer: AEF\nIt's A E F","upvote_count":"1","poster":"Blair77"},{"upvote_count":"1","comment_id":"691739","poster":"dmscountera","content":"Selected Answer: AEF\nBased on comments","timestamp":"1665463560.0"},{"timestamp":"1638951720.0","comment_id":"496635","content":"A. Implement an IAM policy that requires users to specify a ג€˜workloadג€™ tag for cost allocation when launching Amazon EC2 instances.\nE. Define ג€˜workloadג€™ as a cost allocation tag in the AWS Billing and Cost Management console.\nF. Set up AWS Budgets to alert and notify when a given workload is expected to exceed a defined cost.","upvote_count":"1","poster":"cldy"},{"poster":"AzureDP900","comment_id":"493416","content":"AEF is my answer","timestamp":"1638575220.0","upvote_count":"1"},{"comment_id":"452721","poster":"student22","upvote_count":"2","content":"AEF\n---","timestamp":"1636297560.0"},{"content":"It's A E F","timestamp":"1636119420.0","comment_id":"450000","upvote_count":"1","poster":"andylogan"},{"content":"ACF\nKey word is \"predictable spend\"\nA -> will not be able to run a resource that is not taged\n\"Effect\": \"Deny\",\n\"Action\": \"whatever:youchoose\",\n\"Resource\": \"*\",\n\"Condition\": {\n\"ForAllValues:StringNotEquals\": {\n\"aws:TagKeys\": \"workload\"\n}}\nB - impossible per user\nC -> obvious - it gives you predictable cost\nD -> impossible\nE -> it only enables TAGs in Cost&Usage\nF -> obvious (Filters:Dimension in Budget) - then you can set up alerts based on workloads (tags).","comment_id":"408883","upvote_count":"2","timestamp":"1635633780.0","poster":"DerekKey"},{"poster":"WhyIronMan","comment_id":"336869","timestamp":"1635573180.0","content":"I'll go with A,E,F","upvote_count":"4"},{"content":"only AEF can be answer","comment_id":"289435","poster":"Kian1","timestamp":"1635557340.0","upvote_count":"4"},{"upvote_count":"2","comment_id":"262948","poster":"sanjaym","content":"I think answer should be CEF.\n\nA. Not because - IAM Policy can restrict access to existing EC2 based on tag but cannot enforce to create tag while creating EC2 instance.\nB. Not Because - that can be done through IAM policy. no need to got AWS support.\nC. Because - There are many other services also needs to be in consideration to reduce cost but reserved instances can also contribute to reduce cost.\nD. Not possible.","timestamp":"1634996820.0","comments":[{"comment_id":"269723","poster":"kopper2019","timestamp":"1635259620.0","upvote_count":"2","content":"I think the same about C, customers wants predictable cost so if you buy RI you will know how much you are paying for 1 or 3 years, CEF"},{"content":"you can use IAM Policy to enforce tagging on EC2 instance creation: https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/","poster":"nano2nd","comment_id":"710372","timestamp":"1667465520.0","upvote_count":"1"}]},{"timestamp":"1634988900.0","content":"AEF\nB- Not feasible\nC- RI is for years not monthly\nD- Requirement is to put check on monthly budget not on number of instances. 1 big instance = big budget","upvote_count":"2","poster":"rscloud","comment_id":"245082"},{"upvote_count":"1","comment_id":"242362","poster":"gookseang","timestamp":"1634918700.0","content":"AEF for sure"},{"poster":"T14102020","content":"AEF are correct answers.","comment_id":"241943","timestamp":"1634854920.0","upvote_count":"1"},{"comment_id":"239743","content":"I think AEF is correct, as long as EC2 is the main service being used.\nAt first I thought what's wrong with BCD.\nB: You can contact AWS Support to decrease limit.\nC: Reserved Instances is a good way to lower your cost. Of course if some may say that the company may not use EC2 for long time, but AEF are based on EC2 is the main service the company is using.\nD: same as B.\n\nBut think it again, the key of the question is \"achieve a cost that is as close as possible to the target amount\".\nAEF can alert Solutions Architect and he/she can adjust the resources being used to control cost.","upvote_count":"1","poster":"newme","timestamp":"1634800080.0"},{"comment_id":"228623","upvote_count":"2","poster":"jackdryan","content":"I'll go with A,E,F","timestamp":"1634709960.0"},{"content":"AEF is right","timestamp":"1634631300.0","poster":"Bulti","upvote_count":"1","comment_id":"227007"},{"poster":"Jack_London","content":"AEF - note the keywords in the question are CONTROL and MONITOR --> tagging, Billing & Cost Mgmt, Budgets","timestamp":"1634605020.0","upvote_count":"1","comment_id":"205411"},{"content":"AEF is correct","comment_id":"148579","poster":"fullaws","upvote_count":"1","timestamp":"1634602620.0"},{"upvote_count":"1","comment_id":"134135","content":"AEF for sure","timestamp":"1634600820.0","poster":"NikkyDicky"},{"comment_id":"133573","upvote_count":"1","timestamp":"1634558160.0","content":"go with A,E,F\nRI is in units of years and does not meet monthly units. Custom tags can be found in each resource attribute.","poster":"noisonnoiton"},{"poster":"meenu2225","content":"AEF correct answers","upvote_count":"1","timestamp":"1634214360.0","comment_id":"101914"},{"upvote_count":"1","content":"A is not Right, You can't obligate a configuration BY using IAM Policy.\nB is more logic\nB,E,F","timestamp":"1633676040.0","poster":"AShahine21","comment_id":"95243","comments":[{"comments":[{"comment_id":"121353","poster":"AShahine21","content":"Thank you for sharing the Like.\nIt's A,E and F","timestamp":"1634472120.0","comments":[{"poster":"AShahine21","content":"Sharing the link*","timestamp":"1634512260.0","comment_id":"121354","upvote_count":"2"}],"upvote_count":"2"}],"upvote_count":"3","timestamp":"1634118540.0","comment_id":"101913","poster":"meenu2225","content":"A is not wrong, you can create a policy which allows user to spin resources with the condition of tags, https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/"}]},{"poster":"Divine","comment_id":"83803","content":"Answer is AEF","upvote_count":"1","timestamp":"1633252440.0"},{"timestamp":"1632727980.0","poster":"fw","comment_id":"79268","content":"AEF\nfor A: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_iam-tags.html\nTo control access based on tags, you provide tag information in the condition element of a policy. When you create an IAM policy, you can use IAM tags and the associated tag condition key to control access","upvote_count":"1"},{"upvote_count":"1","content":"Correct link\nhttps://stackoverflow.com/questions/52755503/aws-ec2-can-we-restrictdecrease-the-max-storage-volume-limit-allocated-per-r","timestamp":"1632699060.0","poster":"Joeylee","comment_id":"75512"},{"poster":"Joeylee","content":"A is wrong. IAM is for access control won’t force certain configurations. \n\nB - contact support to decrease limits can be done\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html.\nSo BEF","timestamp":"1632488280.0","upvote_count":"1","comments":[{"poster":"meenu2225","timestamp":"1633920240.0","content":"A is not wrong, you can create a policy which allows user to spin resources with the condition of tags, https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/","comment_id":"101912","upvote_count":"2"}],"comment_id":"75511"},{"timestamp":"1632122400.0","comment_id":"58663","poster":"SCraft","upvote_count":"3","comments":[{"poster":"qaz12wsx","content":"a - https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html","timestamp":"1632442860.0","upvote_count":"3","comment_id":"68291"}],"content":"I agree to AEF because the others are wrong. But be curious how to define an iam policy forcing users to specify \"workload\" in tag. Anyone can help?"}],"isMC":true,"answer_ET":"AEF","question_id":371,"choices":{"A":"Implement an IAM policy that requires users to specify a 'workload' tag for cost allocation when launching Amazon EC2 instances.","B":"Contact AWS Support and ask that they apply limits to the account so that users are not able to launch more than a certain number of instance types.","C":"Purchase all upfront Reserved Instances that cover 100% of the account's expected Amazon EC2 usage.","D":"Place conditions in the users' IAM policies that limit the number of instances they are able to launch.","E":"Define 'workload' as a cost allocation tag in the AWS Billing and Cost Management console.","F":"Set up AWS Budgets to alert and notify when a given workload is expected to exceed a defined cost."},"answer_description":""},{"id":"Xy7iwoUuXsJP7LSRc5Jk","topic":"1","unix_timestamp":1568267820,"answer":"B","question_text":"A large global company wants to migrate a stateless mission-critical application to AWS. The application is based on IBM WebSphere (application and integration middleware), IBM MQ (messaging middleware), and IBM DB2 (database software) on a z/OS operating system.\nHow should the Solutions Architect migrate the application to AWS?","answers_community":["B (100%)"],"question_id":372,"question_images":[],"exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/5087-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2019-09-12 07:57:00","answer_images":[],"discussion":[{"comments":[{"timestamp":"1632630480.0","comment_id":"14629","upvote_count":"3","content":"great analysis.","poster":"Moon"}],"poster":"donathon","content":"B\nA\\C: RDS only Supports Aurora, PostgreSQL, MySQL, MariaDB, Oracle & MS SQL Server.\nD: SMS cannot migrate from a z/OS, it can only migrate from VMware or HyperV. It basically replaces VM Import.\nhttps://aws.amazon.com/blogs/database/aws-database-migration-service-and-aws-schema-conversion-tool-now-support-ibm-db2-as-a-source/\nhttps://aws.amazon.com/quickstart/architecture/ibm-mq/","comment_id":"13496","upvote_count":"52","timestamp":"1632461640.0"},{"content":"https://aws.amazon.com/blogs/compute/migrating-from-ibm-mq-to-amazon-mq-using-a-phased-approach/","poster":"AWSPro24","timestamp":"1633325460.0","upvote_count":"5","comment_id":"51314"},{"poster":"DarthYoda","comment_id":"716747","content":"Selected Answer: B\nB seems to be correct","upvote_count":"1","timestamp":"1668263580.0"},{"comment_id":"691738","content":"Selected Answer: B\nBased on comments","poster":"dmscountera","upvote_count":"1","timestamp":"1665463500.0"},{"timestamp":"1665214260.0","upvote_count":"1","poster":"wassb","comment_id":"689109","content":"mission critical service > HA > ASG"},{"content":"Re-platform z/ OS-based DB2 to Amazon EC2-based DB2 --- is it not Re-Host instead Re-Platform?","poster":"vbal","comment_id":"500663","timestamp":"1639406880.0","upvote_count":"2"},{"comment_id":"493417","content":"B right choice","upvote_count":"2","timestamp":"1638575400.0","poster":"AzureDP900"},{"comment_id":"450001","poster":"andylogan","upvote_count":"1","content":"It's B","timestamp":"1636248900.0"},{"timestamp":"1636105320.0","upvote_count":"3","content":"B is the only correct answer \nRDS doesn't support DB2 and z/OS can't be migrated with AWS SMS","comment_id":"362504","poster":"Radhaghosh"},{"content":"I'll go with B","poster":"WhyIronMan","comment_id":"336870","timestamp":"1635903000.0","upvote_count":"3"},{"content":"going with B","upvote_count":"2","comment_id":"289436","poster":"Kian1","timestamp":"1635684420.0"},{"poster":"Ebi","timestamp":"1635669120.0","comment_id":"283992","content":"B is my choice","upvote_count":"3"},{"comment_id":"262953","timestamp":"1635588900.0","upvote_count":"1","poster":"sanjaym","content":"B for sure."},{"timestamp":"1635575640.0","content":"B\nOnly difference in A/B is EC2 DB2. RDS does not support DB2","poster":"rscloud","comment_id":"245083","upvote_count":"1"},{"timestamp":"1635257400.0","content":"Correct answer is B. DB2 is only on EC2.","poster":"T14102020","upvote_count":"1","comment_id":"241945"},{"upvote_count":"2","content":"I'll go with B","poster":"jackdryan","comment_id":"228627","timestamp":"1635198240.0"},{"poster":"Bulti","timestamp":"1634673660.0","upvote_count":"1","content":"Answer is B because SMS cannot migrate Z/OS to AWS.","comment_id":"227740"},{"comment_id":"198367","content":"Bi is the right option.","timestamp":"1634524740.0","upvote_count":"1","poster":"Paitan"},{"content":"B is correct","timestamp":"1634516520.0","comment_id":"148582","upvote_count":"2","poster":"fullaws"},{"content":"B for sure","timestamp":"1634116200.0","upvote_count":"1","comment_id":"134139","poster":"NikkyDicky"},{"poster":"noisonnoiton","timestamp":"1633923720.0","comment_id":"133583","content":"go with B\nRDS only Supports Aurora, PostgreSQL, MySQL, MariaDB, Oracle & MS SQL Server.","upvote_count":"1"},{"comment_id":"133251","upvote_count":"1","content":"Answer B best choice","poster":"mat2020","timestamp":"1633621920.0"},{"poster":"meenu2225","upvote_count":"1","comment_id":"109861","timestamp":"1633426500.0","content":"B is the right answer."},{"content":"Answer is B","upvote_count":"3","comment_id":"44830","poster":"amog","timestamp":"1633216980.0"},{"timestamp":"1632980640.0","upvote_count":"1","comment_id":"41172","content":"b is the answer","poster":"markpark"},{"comment_id":"30433","timestamp":"1632891720.0","poster":"dojo","upvote_count":"2","content":"B\n\nRDS doesnt support DB2"},{"upvote_count":"1","comment_id":"10717","poster":"awsec2","comments":[{"comment_id":"11017","comments":[{"upvote_count":"1","comment_id":"12901","content":"b is best","poster":"awsdog","timestamp":"1632457800.0"}],"upvote_count":"2","timestamp":"1632330060.0","poster":"dpvnme","content":"B is the best choice here"}],"content":"why not \"b\"","timestamp":"1632154440.0"}],"answer_ET":"B","isMC":true,"choices":{"C":"Orchestrate and deploy the application by using AWS Elastic Beanstalk. Re-platform the IBM MQ to Amazon SQS. Re-platform z/OS-based DB2 to Amazon RDS DB2.","D":"Use the AWS Server Migration Service to migrate the IBM WebSphere and IBM DB2 to an Amazon EC2-based solution. Re-platform the IBM MQ to an Amazon MQ.","B":"Re-host WebSphere-based applications on Amazon EC2 behind a load balancer with Auto Scaling. Re-platform the IBM MQ to an Amazon MQ. Re-platform z/ OS-based DB2 to Amazon EC2-based DB2.","A":"Re-host WebSphere-based applications on Amazon EC2 behind a load balancer with Auto Scaling. Re-platform the IBM MQ to an Amazon EC2-based MQ. Re-platform the z/OS-based DB2 to Amazon RDS DB2."},"answer_description":"Reference:\nhttps://aws.amazon.com/blogs/database/aws-database-migration-service-and-aws-schema-conversion-tool-now-support-ibm-db2-as-a-source/ https://aws.amazon.com/quickstart/architecture/ibm-mq/"},{"id":"7PSX3k5cx7AVKDUZPg4g","isMC":true,"question_images":[],"answer_images":[],"unix_timestamp":1568045820,"topic":"1","answer_description":"","question_text":"A media storage application uploads user photos to Amazon S3 for processing. End users are reporting that some uploaded photos are not being processed properly. The Application Developers trace the logs and find that AWS Lambda is experiencing execution issues when thousands of users are on the system simultaneously. Issues are caused by:\n✑ Limits around concurrent executions.\n✑ The performance of Amazon DynamoDB when saving data.\nWhich actions can be taken to increase the performance and reliability of the application? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/4954-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2019-09-09 18:17:00","discussion":[{"poster":"donathon","upvote_count":"39","comment_id":"13498","timestamp":"1632485700.0","content":"BD\nA\\C: Read is not the problem here. (when saving data…)\nB: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.requests\nD: https://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq/c\nE: Does not solve the problem. Issue does not lies with ingestion, it lies with processing."},{"upvote_count":"8","comment_id":"12320","content":"my view also BD","poster":"awsec2","timestamp":"1632311760.0"},{"poster":"DarthYoda","comment_id":"716748","upvote_count":"2","timestamp":"1668263760.0","content":"Selected Answer: BD\nI'd say B&D in the exam. Reads are not an issue here"},{"timestamp":"1665463620.0","upvote_count":"1","poster":"dmscountera","content":"Selected Answer: BD\nBased on comments","comment_id":"691742"},{"timestamp":"1648592040.0","upvote_count":"1","comment_id":"577891","content":"Selected Answer: BD\nBD is right","poster":"jj22222"},{"comment_id":"523963","content":"B is fine . But D dead letter q is only for troubleshooting the error not for reprocessing the message.","timestamp":"1642228560.0","upvote_count":"2","poster":"RVivek"},{"timestamp":"1638575520.0","content":"BD is right","upvote_count":"1","comment_id":"493419","poster":"AzureDP900"},{"upvote_count":"2","poster":"andylogan","timestamp":"1636261260.0","content":"It's B D","comment_id":"450003"},{"upvote_count":"1","content":"my Ans : B, D","comment_id":"424361","timestamp":"1636227420.0","poster":"mimadour21698"},{"upvote_count":"1","poster":"Mzehk","timestamp":"1635918120.0","content":"Agree with B and D","comment_id":"423134"},{"content":"B, D my options","timestamp":"1635727020.0","upvote_count":"1","comment_id":"362506","poster":"Radhaghosh"},{"timestamp":"1635707400.0","content":"I'll go with B,D","comment_id":"336874","poster":"WhyIronMan","upvote_count":"2"},{"poster":"Kian1","content":"for write Ans BD","timestamp":"1635664140.0","upvote_count":"2","comment_id":"289440"},{"comment_id":"288393","content":"BD is correct.","poster":"AJBA","upvote_count":"2","timestamp":"1635640800.0"},{"timestamp":"1635389400.0","poster":"LoganIsh","comment_id":"284633","upvote_count":"3","content":"BD is the answer..."},{"timestamp":"1635361980.0","comment_id":"283993","upvote_count":"2","content":"BD for sure","poster":"Ebi"},{"content":"B & D for sure.","comment_id":"262954","upvote_count":"1","timestamp":"1635319560.0","poster":"sanjaym"},{"comment_id":"242477","upvote_count":"1","timestamp":"1635117660.0","content":"BD for me","poster":"gookseang"},{"comment_id":"241950","timestamp":"1634987400.0","upvote_count":"1","poster":"T14102020","content":"BD is correct answer. Question about writing data"},{"timestamp":"1634766900.0","upvote_count":"1","comment_id":"232129","content":"B AND D YES .. THIS QUESTION SHOWS THE IMPORTANCE OF FULLY REEADING THE QUESTION AS ITS EASY TO THINK C ELASTICACHE ETC .. BUT B AND D","poster":"petebear55"},{"comment_id":"228628","poster":"jackdryan","timestamp":"1634600640.0","content":"I'll go with B,D","upvote_count":"3"},{"content":"B & D is the answer","poster":"Bulti","upvote_count":"1","timestamp":"1634560500.0","comment_id":"227745"},{"content":"I will go for B & D","timestamp":"1634505060.0","poster":"Edgecrusher77","comment_id":"223571","upvote_count":"1"},{"comment_id":"148584","timestamp":"1634461020.0","poster":"fullaws","upvote_count":"1","content":"B and D, reliability by implement DLQ"},{"content":"BD for sure","upvote_count":"1","comment_id":"134140","timestamp":"1634214120.0","poster":"NikkyDicky"},{"upvote_count":"1","comment_id":"133595","timestamp":"1634091840.0","content":"go with B,D\nhttps://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq","poster":"noisonnoiton"},{"comment_id":"133248","content":"Answer B & D","upvote_count":"1","poster":"mat2020","timestamp":"1634090700.0"},{"comment_id":"109862","timestamp":"1634056500.0","upvote_count":"1","poster":"meenu2225","content":"B and D for me"},{"poster":"Jeb","content":"Choose B if the question is about the performance of Amazon DynamoDB when saving data.\nChoose D if the limit around concurrent executions","upvote_count":"1","comment_id":"97127","timestamp":"1633657860.0"},{"comment_id":"92426","content":"Never mind . I think BD is good. I did overlook \"reliability\" which is a requirement. D option would provide reliability.","upvote_count":"1","poster":"JAWS1600","timestamp":"1633654860.0"},{"comment_id":"90123","content":"B and C. B is going to suffice the Write capacity of DB. C is going to help Lambda concurrent execution. I do not agree with D. DLQ are implemented for reliability, so that missed/failed executions can be reprocessed. DLQ is not to enhance the performance.\nhttps://theburningmonk.com/2019/10/all-you-need-to-know-about-caching-for-serverless-applications/","poster":"JAWS1600","upvote_count":"2","timestamp":"1633507320.0"},{"comment_id":"44831","upvote_count":"3","timestamp":"1633273560.0","poster":"amog","content":"Answer is B&D"},{"content":"correct link for D:\nhttps://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq/\nFor the donathon comment this is the correct link. A small copy paste error a think :)","poster":"Denis_H","comment_id":"44133","upvote_count":"1","timestamp":"1632988740.0"},{"content":"Is \"dead letter queue\" used in the generic sense to mean a place where failed requests (lambda in this case) go? I always thought dead letter queue was specific to queuing services like MQ or SQS.","upvote_count":"1","timestamp":"1632899880.0","comment_id":"43024","comments":[{"poster":"Greg1234","timestamp":"1633086600.0","comment_id":"44316","upvote_count":"2","content":"https://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq/"},{"comment_id":"239786","poster":"newme","timestamp":"1634843820.0","content":"SQS feels like better solution here, but it's not in the answer.","upvote_count":"1"}],"poster":"Freddy"},{"comment_id":"41192","upvote_count":"2","poster":"markpark","content":"b,d is the answer","timestamp":"1632881520.0"},{"timestamp":"1632187740.0","upvote_count":"6","poster":"DalianYifang","comment_id":"10307","content":"Ans is BD? any thoughts?"}],"exam_id":32,"answers_community":["BD (100%)"],"question_id":373,"answer_ET":"BD","answer":"BD","choices":{"C":"Add an Amazon ElastiCache layer to increase the performance of Lambda functions.","D":"Configure a dead letter queue that will reprocess failed or timed-out Lambda functions.","B":"Evaluate and adjust the write capacity units (WCUs) for the DynamoDB tables.","A":"Evaluate and adjust the read capacity units (RCUs) for the DynamoDB tables.","E":"Use S3 Transfer Acceleration to provide lower-latency access to end users."}},{"id":"GKynut28f4IbKfsISfT0","discussion":[{"content":"A\nB: This will take too long.\nC: The users will not be able to access the data in the last 24 hours this way.\nD: Although EFS would work and it has higher performance, it’s almost 10x more expensive then S3 and hence does not meet the criteria of reducing cost.","poster":"donathon","timestamp":"1632604440.0","comment_id":"13505","upvote_count":"46"},{"comment_id":"10590","comments":[{"poster":"newme","upvote_count":"2","timestamp":"1634498160.0","content":"Good point.\nBut cost isn't said to be a problem here, so snowball is a better solution, isn't it?\nAnd using 1Gb DX, it really will take a long time, more than 100 days, to upload data...","comment_id":"239800"},{"comment_id":"427133","timestamp":"1636104180.0","upvote_count":"2","content":"Good point. However, question states, \"most likely last 24 hours\", so there is a chance of purchasing something older than 24 hours which is part of existing 2 PB data. Everything else, you have explained perfectly fine. Answer A is best option.","poster":"Shanmahi"}],"content":"I believe because only the data/images of the last 24 hours seems to be relevant. It seems to indicate that there is no urgency to migrate the old data (2PB) over to S3 in short time (which could be achieved with Snowball). Instead this can take as long as it needs. By not using Snowball, we can save money. NOTE: \"DX\" will be setup in both Answers A) and B). Therefore skipping Snowball will reduce the overall cost of Answer B) compared to A).","timestamp":"1632183720.0","poster":"cpt","upvote_count":"10"},{"comment_id":"712318","upvote_count":"1","content":"Selected Answer: A\nA looks to be the better ans. S3 a better choice to copy data to","timestamp":"1667740620.0","poster":"Ni_yot"},{"upvote_count":"1","comment_id":"693958","content":"A is the aswer. Send the exisiting data using snowflex appliances and for immediate generating data, use direct connect to upload..","poster":"sg0206","timestamp":"1665669960.0"},{"poster":"kangtamo","upvote_count":"1","content":"Selected Answer: A\nAgree with A: S3.","comment_id":"623533","timestamp":"1656372240.0"},{"poster":"AzureDP900","timestamp":"1640116560.0","upvote_count":"1","content":"I'll go with A","comment_id":"506405"},{"upvote_count":"1","comment_id":"497792","timestamp":"1639060020.0","poster":"cldy","content":"A. Use multiple AWS Snowball appliances to migrate the existing imagery to Amazon S3. Create a 1-Gb AWS Direct Connect connection from the ground station to AWS, and upload new data to Amazon S3 through the Direct Connect connection. Migrate the data distribution website to Amazon EC2 instances. By using Amazon S3 as an origin, have this website serve the data through Amazon CloudFront by creating signed URLs."},{"comment_id":"450004","timestamp":"1636137360.0","upvote_count":"1","content":"It's A","poster":"andylogan"},{"timestamp":"1636121400.0","poster":"DonSp","content":"The most reasonable solution is A. B works technically but the 1-Gb Direct Connect connection will largely be taken up by new data coming in at a rate of 5 GB/min. Furthermore, you would not want to transfer 2 PB of achives from the command line but instead use a File Storage gateway.","comment_id":"444692","upvote_count":"1"},{"content":"Copy 2TB of data via 1 Gb DX will take close to 195 days.\nSo snowball is the best option. Option A","timestamp":"1636092540.0","poster":"Radhaghosh","upvote_count":"3","comment_id":"362507"},{"upvote_count":"1","timestamp":"1635744900.0","content":"A. Data is too much to copy over network, snowball is the answer here.","poster":"blackgamer","comment_id":"344181"},{"content":"I'll go with A","upvote_count":"1","comment_id":"336879","timestamp":"1635730860.0","poster":"WhyIronMan"},{"comments":[{"timestamp":"1635603240.0","upvote_count":"4","comment_id":"315309","content":"AWS is not looking for your experience with snowball in certification exam. They are looking for ideal answer, which is A.","poster":"nitinz"}],"upvote_count":"1","timestamp":"1635520500.0","poster":"RomanTsai","comment_id":"311094","content":"Answer should be B. Using Snowball is a very very very bad choice which take a lot of time to waiting device available, delivery / transport data, upload to S3. My bad experience took 1 week to complete all if lucky to have a device available without any waiting."},{"timestamp":"1635381300.0","poster":"gpark","content":"B.\nRequirements are\n\"most likely to access images that have been captured in the last 24 hours.\"\n\nOnly 24 hours data has a priority. 2PB is not emergent.\nThere is no restriction to send 2PB within 1 month.","comments":[{"upvote_count":"1","timestamp":"1635461040.0","comment_id":"294751","content":"B is wrong.\nHave searched best practises on AWS.\nOver PB, Snowmobile is the one.\nMy apology for confusion.","comments":[{"poster":"sarah_t","comment_id":"332823","upvote_count":"1","timestamp":"1635673260.0","content":"Amazon recommends Snowmobile for migrating more than 10 PB"}],"poster":"gpark"}],"comment_id":"294694","upvote_count":"1"},{"content":"B is wrong, the 1Gb DX has 100MB/s actual network bindwidth, this will be used by migrating the existing data and transferring new generated data, it's impossible. A is correct.","poster":"wind","comment_id":"290927","upvote_count":"1","timestamp":"1635212280.0"},{"content":"going with A","upvote_count":"2","poster":"Kian1","comment_id":"289461","timestamp":"1635175320.0"},{"upvote_count":"4","timestamp":"1635167220.0","content":"A should be the answer, It's very important that sometimes we look for the users using AWS recommendation. For PB of data, it should be snowball.","comment_id":"288399","poster":"AJBA"},{"timestamp":"1635031980.0","comment_id":"283995","upvote_count":"3","poster":"Ebi","content":"A is my choice"},{"poster":"sanjaym","timestamp":"1634773440.0","content":"A for sure.","comment_id":"262956","upvote_count":"1"},{"timestamp":"1634722440.0","comment_id":"261824","upvote_count":"1","content":"Ans B- \"Usage analysis shows that customers are most likely to access images that have been captured in the last 24 hours\" - so they don't need to spend money to transfer 2PB data using snowball. customer focus on last 24 hours photos only.","poster":"jayakumarchellam","comments":[{"timestamp":"1635009300.0","poster":"Satya1405","content":"B also mentions that \"Use the AWS Command Line Interface to copy the existing data and upload new data to Amazon S3 over the Direct Connect connection\" this can't be the right one.","upvote_count":"1","comment_id":"281478"}]},{"upvote_count":"2","timestamp":"1634518500.0","content":"A is correct choice\nSnowball and S3 origin cloudfront","comment_id":"245122","poster":"rscloud"},{"upvote_count":"1","content":"Correct answer is A.","timestamp":"1634513400.0","poster":"T14102020","comment_id":"241952"},{"upvote_count":"1","poster":"petebear55","comment_id":"232136","content":"A BECAUSE OF THE LARGE AMOUNT OF DATA SO WE USE SNOWBALL .. AND ALSO I SEE WHERE THE USERS BELOW ARE COMING FROM WITH B ... HOWEVER ANSWER A ALSO MENTIONS DC FOR MOST RECENT DATA (24 HOUR SEEMS IMPORTANT) .. SO ANSWER = A","timestamp":"1634193060.0"},{"poster":"jackdryan","upvote_count":"2","timestamp":"1634179620.0","content":"I'll go with A","comment_id":"228769"},{"timestamp":"1634118180.0","upvote_count":"1","poster":"Bulti","content":"A is the answer","comment_id":"227763"},{"timestamp":"1633942800.0","content":"A is the right option.","upvote_count":"2","comment_id":"198369","poster":"Paitan"},{"timestamp":"1633925100.0","comment_id":"190250","upvote_count":"2","content":"I'll go for A.","poster":"AlwaysLearning2020"},{"poster":"fullaws","timestamp":"1633808340.0","comment_id":"148586","content":"A is correct, Migrate 2PB with snowball, upload new content with Direct Connect, and use s3 for storage scalability","upvote_count":"2"},{"comment_id":"134145","upvote_count":"2","timestamp":"1633765320.0","poster":"NikkyDicky","content":"A for sure"},{"content":"This question is never asking about the cost-effective. You need to deliver the requirement based on the network-attached storage which is 2 PB size. The best choice is C as you need to use a snowball appliance to be able to access S3 via Direct Connect.","comment_id":"97138","poster":"Jeb","upvote_count":"2","comments":[{"content":"I read it again, it says migrate the image storage and distribution to AWS to reduce cost. The correct answer is A for two scenarios:\n1. Upload new data to S3 via Direct Connect that will be able to capture the data within 24 hours.\n2. Migrate the existing images to S3 using Snowball","upvote_count":"2","timestamp":"1633217820.0","comment_id":"97144","poster":"Jeb"}],"timestamp":"1633162260.0"},{"poster":"Smartphone","upvote_count":"5","comment_id":"79527","timestamp":"1633159860.0","content":"This question has two clues 1. \"AWS to reduce costs\", and 2. \"customers are most likely to access images that have been captured in the last 24 hours\". Here, the customer will access only the current captured data. So, the DX setup may be used to cater the current user's request. And the remaining, 2 PB of data can be uploaded later. This does not matter whether it takes 2 months or a week. User is not suppose to access the old data. So, it seems that the answer is B.","comments":[{"comment_id":"237760","poster":"Cantaloupe","timestamp":"1634373420.0","upvote_count":"2","content":"Good catch on 24h. It makes the difference"},{"poster":"pmjcr","timestamp":"1633755060.0","comment_id":"111376","upvote_count":"2","content":"Indeed this is a very good point. The 24h that users will most likely access the data makes all the difference and we can save on snowball devices costs as Dx will do what we need and we can migrate the rest in the background. I am also thinking B is the answer."},{"poster":"rcher","content":"Less likely doesn't mean the user are not accessing the old data though.. Moreover, the daily data storage in the on-prem NAS are increasing, and this kinda prolonged the time of migration ?\n\nWouldn't a strategy of migrating the bulk of old data to AWS, and updating it delta via DX are more efficient process? For Option B, you probably will have that DX connection working for longer sustained period of time before all the data are fully migrated.","timestamp":"1634869920.0","upvote_count":"2","comment_id":"276550"}]},{"comments":[{"upvote_count":"1","content":"Sorry My mistake, it should be A. Purchase option has been considered in both. And we can sync data using direct connect instead of uploading by snowball.","timestamp":"1633144980.0","poster":"kilido6049","comment_id":"79205"}],"timestamp":"1632918660.0","upvote_count":"1","content":"Why not C over A?\nLooking at \"The company runs a website that allows its customers to access and <b>purchase</b> the images over the Internet\"","comment_id":"79203","poster":"kilido6049"},{"upvote_count":"3","content":"Answer is A\nToo long without snowball","timestamp":"1632833580.0","poster":"amog","comment_id":"44835"},{"poster":"dojo","comment_id":"30450","timestamp":"1632699240.0","content":"Not sure why this is wrong in the first place. Its so straight.\nAnswer is A.","upvote_count":"3"},{"timestamp":"1632678360.0","content":"A\nThe company would like to migrate the image storage and distribution system to AWS to reduce costs and increase the number of customers that can be served. So all storage has to be migrated. Aws rule is that any data transfer which will take over a week over link. Use Snowball.","comment_id":"24041","poster":"examacc","upvote_count":"4"},{"upvote_count":"5","comment_id":"18495","poster":"rajeshsanjeevi","content":"B is the answer. Cost reduction by avoiding snowball and 24 hours data is vital which can be transferred over Direct connect, rest can slowly copy in few months.","comments":[{"comment_id":"69674","poster":"Smart","timestamp":"1632892380.0","content":"Follow AWS Best Practices & Recommendations!","upvote_count":"2"},{"content":"B is not going to be correct here.The problem is the volume of data. A DX line of 1 Gbps will process 7.5 GB/min. New data is being created at 5 GB/min, which leaves you only 2.5 GB/min to process the older data in order to \"catch up\". With 2 PB of data, that will take 20.5 months which does not make sense. A is the answer.","timestamp":"1632757260.0","upvote_count":"15","poster":"LunchTime","comment_id":"34639"}],"timestamp":"1632669300.0"},{"timestamp":"1632643140.0","comment_id":"14622","poster":"Moon","content":"Answer \"A\".","upvote_count":"4"},{"comment_id":"11722","timestamp":"1632591540.0","upvote_count":"2","content":"Since it would take months to copy just the initial 2 PB to AWS (1Gbps = 324 TB/Mo), and AWS's recommended solution is to use Snowball for petabyte-scale data transport, \"A\" is the answer they're looking for.","poster":"sb333"},{"content":"You need snowball for the initial 2pb transfer. So it's A","timestamp":"1632561600.0","upvote_count":"6","comment_id":"11019","poster":"dpvnme"},{"comment_id":"10308","content":"Why not A? I think ans is A. Transfer 2PB data over 1 G Direct Connect instead of snowball? not a good choice to me. Any comments?","poster":"DalianYifang","timestamp":"1632124740.0","upvote_count":"2"}],"choices":{"A":"Use multiple AWS Snowball appliances to migrate the existing imagery to Amazon S3. Create a 1-Gb AWS Direct Connect connection from the ground station to AWS, and upload new data to Amazon S3 through the Direct Connect connection. Migrate the data distribution website to Amazon EC2 instances. By using Amazon S3 as an origin, have this website serve the data through Amazon CloudFront by creating signed URLs.","B":"Create a 1-Gb Direct Connect connection from the ground station to AWS. Use the AWS Command Line Interface to copy the existing data and upload new data to Amazon S3 over the Direct Connect connection. Migrate the data distribution website to EC2 instances. By using Amazon S3 as an origin, have this website serve the data through CloudFront by creating signed URLs.","C":"Use multiple Snowball appliances to migrate the existing images to Amazon S3. Upload new data by regularly using Snowball appliances to upload data from the network-attached storage. Migrate the data distribution website to EC2 instances. By using Amazon S3 as an origin, have this website serve the data through CloudFront by creating signed URLs.","D":"Use multiple Snowball appliances to migrate the existing images to an Amazon EFS file system. Create a 1-Gb Direct Connect connection from the ground station to AWS, and upload new data by mounting the EFS file system over the Direct Connect connection. Migrate the data distribution website to EC2 instances. By using webservers in EC2 that mount the EFS file system as the origin, have this website serve the data through CloudFront by creating signed URLs."},"answer_description":"","answer":"A","unix_timestamp":1568046360,"answer_ET":"B","isMC":true,"topic":"1","timestamp":"2019-09-09 18:26:00","exam_id":32,"answers_community":["A (100%)"],"question_text":"A company operates a group of imaging satellites. The satellites stream data to one of the company's ground stations where processing creates about 5 GB of images per minute. This data is added to network-attached storage, where 2 PB of data are already stored.\nThe company runs a website that allows its customers to access and purchase the images over the Internet. This website is also running in the ground station.\nUsage analysis shows that customers are most likely to access images that have been captured in the last 24 hours.\nThe company would like to migrate the image storage and distribution system to AWS to reduce costs and increase the number of customers that can be served.\nWhich AWS architecture and migration strategy will meet these requirements?","question_images":[],"question_id":374,"url":"https://www.examtopics.com/discussions/amazon/view/4955-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[]},{"id":"HCNIvdfS08RGpsegezrf","answers_community":["B (100%)"],"exam_id":32,"question_images":[],"answer_images":[],"isMC":true,"question_text":"A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics is run, and each execution takes about 4 hours to complete. The statistical analysis is not mission critical to the business, and previous data points are picked up on the next execution if a particular run fails.\nThe current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations running full time to ingest and store the streaming data in attached Amazon EBS volumes. On-Demand EC2 instances are launched each night to perform the nightly processing, accessing the stored data from NFS shares on the ingestion servers, and terminating the nightly processing servers when complete. The Reserved Instance reservations are expiring, and the company needs to determine whether to purchase new reservations or implement a new design.\nWhich is the most cost-effective design?","topic":"1","answer_ET":"B","answer_description":"","answer":"B","question_id":375,"url":"https://www.examtopics.com/discussions/amazon/view/4956-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2019-09-09 18:41:00","choices":{"D":"Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use an AWS Lambda function scheduled to run nightly with Amazon CloudWatch Events to query Amazon Redshift to generate the daily statistics.","B":"Update the ingestion process to use Amazon Kinesis Data Firehouse to save data to Amazon S3. Use AWS Batch to perform nightly processing with a Spot market bid of 50% of the On-Demand price.","C":"Update the ingestion process to use a fleet of EC2 Reserved Instances behind a Network Load Balancer with 3-year leases. Use Batch with Spot instances with a maximum bid of 50% of the On-Demand price for the nightly processing.","A":"Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a fleet of On-Demand EC2 instances that launches each night to perform the batch processing of the S3 data and terminates when the processing completes."},"unix_timestamp":1568047260,"discussion":[{"timestamp":"1632427560.0","upvote_count":"47","comment_id":"13508","comments":[{"comment_id":"70035","timestamp":"1633590900.0","comments":[{"timestamp":"1635012660.0","poster":"rcher","comment_id":"276556","content":"And its ok if the processing keep failing cause its Lambda? Might as well dont implement the solution if its not even working.","upvote_count":"4"}],"poster":"hammad","content":"Application on EC2 takes 4 hours and lambda does not require to take 4 hours. since it is not mission critical it can recollect even it fails. So D is correct.","upvote_count":"1"},{"comment_id":"51033","content":"With the new Spot pricing model you don't set a spot bid only a \"Maximum Bid\" as in choice C. Thoughts? https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing/","timestamp":"1633349760.0","upvote_count":"2","poster":"AWSPro24"}],"content":"B\nA: On demand instances is more expensive.\nB: Since the analysis is not mission critical and can be restarted, spot instances is cheaper.\nC: EC2 is more expensive.\nD: Compared to B Lambda should be cheaper to Batch using spot instances which ultimately still charges based on EC2 pricing. But critical thing is Lambda cannot go beyond 15 minutes of execution time. So it is more for simple processing and in this case it is not. It takes 4 hours.\nhttps://www.simform.com/aws-lambda-vs-ec2/\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/\nhttps://aws.amazon.com/batch/faqs/?nc=sn&loc=5","poster":"donathon"},{"content":"Ans is B to me","upvote_count":"9","comment_id":"10310","poster":"DalianYifang","timestamp":"1632074340.0","comments":[{"timestamp":"1632177540.0","upvote_count":"6","poster":"DalianYifang","comment_id":"10312","comments":[{"poster":"awsdog","upvote_count":"1","content":"Isn't B cheaper than d?","timestamp":"1632410940.0","comment_id":"12902"},{"poster":"IsaacTeh","content":"do you think lambda can run 4 hours?","comments":[{"comment_id":"77310","upvote_count":"1","poster":"gaukhar","timestamp":"1633981380.0","content":"D. EC2 takes 4 hours, Redshift is meant for aggregate queries like the one mentioned in the question and as a result can run much faster than 4 hours. This is where real life experience would have come in handy. Ultimately the question is badly worded."}],"comment_id":"57170","timestamp":"1633569000.0","upvote_count":"7"},{"comment_id":"444769","poster":"AWSum1","timestamp":"1636092360.0","content":"Redshift can't be cheaper than Batch on spot? I'm looking at the question and whats jumping out to me is reproducible workloads and cost. \n\nSo on that thought I'm gonna say B","upvote_count":"2"}],"content":"Never mind, Amazon Redshift is the correct ans."}]},{"timestamp":"1666378980.0","content":"I would say B as it mentions \"and previous data points are picked up on the next execution if a particular run fails.\" so if no spots instances avail one night its not a critical issue, as about cost saving this solution given in B works for me","poster":"mrgreatness","comment_id":"701113","upvote_count":"1"},{"timestamp":"1664218140.0","poster":"Ni_yot","comment_id":"680074","content":"Selected Answer: B\nB for sure.","upvote_count":"1"},{"timestamp":"1661984820.0","comment_id":"655553","upvote_count":"1","content":"Selected Answer: B\nIt's B - Firehose is infinitely cheaper than EC2 and it fits the use case.\n\nLambda cannot run for 4h, limit is 15 min.","poster":"epomatti"},{"upvote_count":"2","content":"Selected Answer: B\nAgree with B: Spot instance.","comment_id":"623507","poster":"kangtamo","timestamp":"1656366720.0"},{"poster":"cldy","content":"B. Update the ingestion process to use Amazon Kinesis Data Firehouse to save data to Amazon S3. Use AWS Batch to perform nightly processing with a Spot market bid of 50% of the On-Demand price.","upvote_count":"1","comment_id":"497779","timestamp":"1639058640.0"},{"comment_id":"493426","content":"my answer is B, Lambda is only valid for 15 min","timestamp":"1638576840.0","poster":"AzureDP900","upvote_count":"2"},{"comment_id":"467704","content":"Definitely B is correct ! Lambda can not execute 4 hours job.","poster":"andypham","timestamp":"1636233240.0","upvote_count":"1"},{"comment_id":"450006","poster":"andylogan","upvote_count":"1","timestamp":"1636209660.0","content":"It's B"},{"upvote_count":"2","content":"Naswe is B","comment_id":"378860","poster":"nisoshabangu","timestamp":"1636032840.0"},{"comment_id":"367361","poster":"zolthar_z","upvote_count":"2","timestamp":"1635958440.0","content":"The answer is B, remember that lambda has a timeout for 15minutes. Redshift could improve the process but this exam is based in the given information and they put the 4 Hours time for a reason,"},{"poster":"digimaniac","timestamp":"1635877800.0","upvote_count":"3","comment_id":"342616","content":"unfortunately, the answer is D. this implies that Redshift can do a much better job of aggregating stats than the home brew solution, which takes 4 hours. Redshift is a data warehouse with some adv. data processing horsepower."},{"timestamp":"1635466320.0","poster":"WhyIronMan","content":"I'll go with B","upvote_count":"3","comment_id":"336884"},{"upvote_count":"4","content":"I wish to negate answer B as it says Kinesis Firehouse however it is the only option with spot instance in this non-time sensitive & \"re-doable\" scenario. So my answer still stick with B","poster":"Pupu86","comment_id":"307155","timestamp":"1635386220.0"},{"comment_id":"290931","upvote_count":"1","timestamp":"1635338940.0","content":"B is correct.","poster":"wind"},{"poster":"Kian1","upvote_count":"3","comment_id":"289464","content":"only B","timestamp":"1635303780.0"},{"comment_id":"283996","upvote_count":"5","content":"my choice is B","poster":"Ebi","timestamp":"1635024840.0"},{"content":"B for sure.","comment_id":"262958","timestamp":"1634938800.0","poster":"sanjaym","upvote_count":"2"},{"comment_id":"241957","timestamp":"1634926200.0","content":"Correct answer is B. Spot and Kinesis are keys...","poster":"T14102020","upvote_count":"2"},{"content":"B ; The key to this question is ... \"The statistical analysis is not mission critical to the business, and previous data points are picked up on the next execution if a particular run\" ... thus SPOT instances are a good option here\" so yes B","poster":"petebear55","comment_id":"232152","upvote_count":"2","timestamp":"1634812740.0"},{"poster":"jackdryan","comment_id":"229361","upvote_count":"3","content":"I'll go with B","timestamp":"1634808000.0"},{"timestamp":"1634763180.0","poster":"Bulti","upvote_count":"1","comment_id":"227776","content":"Answer is B see explanation above"},{"timestamp":"1634739780.0","comment_id":"227775","content":"Is the correct answer. Serverless + S3 is going to be cheaper to manage reserved EC2 fleet instances and the cost of EFS","poster":"Bulti","upvote_count":"1"},{"comment_id":"198371","content":"I will go with option B.","poster":"Paitan","timestamp":"1634643420.0","upvote_count":"2"},{"upvote_count":"2","content":"B is correct, Redshift is more expensive as it had to consider instance + storage cost. Kinesis data stream low the administration cost compare to option C","timestamp":"1634515380.0","comment_id":"148590","poster":"fullaws"},{"content":"B for sure","upvote_count":"3","timestamp":"1634367180.0","poster":"NikkyDicky","comment_id":"134151"},{"timestamp":"1634346300.0","comment_id":"133242","poster":"mat2020","upvote_count":"1","content":"answer : B"},{"content":"B looks more viable than D due to S3 lower cost & no AWS Batch Process cost over Lambda. \nOther point to note that Lambda function can only un till 15 minutes and it's a 4 hours batch process.","upvote_count":"1","timestamp":"1634172780.0","poster":"manoj101","comment_id":"125954"},{"timestamp":"1634166180.0","comment_id":"109864","upvote_count":"1","content":"B for me.","poster":"meenu2225"},{"comment_id":"90131","poster":"JAWS1600","content":"B is the correct one. D is based on Redshift. keep in mind, Redshift can either use On-Demand or Reserved Instances. Plus it will incur the storage cost. On the other hand, S3 is just flat storage price ( plus transfer), which is still cheaper than RS. B would make more sense.","timestamp":"1634109000.0","upvote_count":"1"},{"upvote_count":"4","timestamp":"1634044620.0","content":"I think B is the most \"cost-effective\" design. Using s3 to store the data is much more \"cost-effective\" than Redshift.","poster":"jgtran","comment_id":"79578"},{"content":"@awspro24 C doesnt talks about storing data","comment_id":"64146","poster":"ghostrider8001","timestamp":"1633575180.0","upvote_count":"1"},{"poster":"AWSPro24","upvote_count":"1","comment_id":"51030","timestamp":"1633247160.0","comments":[{"timestamp":"1633372440.0","poster":"AWSPro24","upvote_count":"1","comments":[{"timestamp":"1633533540.0","content":"When you have consistent load \"data rate\" reserved capacity (buying big up front) will always be cheaper than On-Demand capacity (with Kenisis) no?","upvote_count":"1","comment_id":"51035","comments":[{"comment_id":"75099","timestamp":"1633918020.0","content":"A (Invalid) is eliminated by B's Spot instances. \nD (Invalid) - I like option although I am not entirely sure. I wonder if a scheduled Lambda function can be asynchronously invoked to execute stored procedure in Redshift. Timeout won't matter here anymore. Regardless, growing Redshift with stale data is not cost-efficient (although, you can always drop tables with old data). Also, see next point.\nC (Invalid) - I agree with AWSPro24 that Data ingestion through Kinesis Data Firehose is going to be expensive. However, EBS/EFS storage is going to be expensive. Along with it, apply AWS Best Practices, consider managed services to bring down administrative cost. \nI will go B. Not perfect solution but that's what I would choose as well.","poster":"Smart","upvote_count":"1"}],"poster":"AWSPro24"}],"content":"Sorry, I'm backing C because of the \"constant data rate\" comment above. It's not critical so who cares how long it takes, why would you want to pay by the GB with Kinesis. The new spot pricing model also only has a max price not a bid price anymore.","comment_id":"51034"}],"content":"The only thing that has me considering C is the statement \"the data rate is constant\". When you know the load is constant and predictable aren't reserved instances a good choice?"},{"comment_id":"44837","content":"Should be B","timestamp":"1633131540.0","upvote_count":"2","poster":"amog"},{"poster":"markpark","content":"B is answer","timestamp":"1633040400.0","comment_id":"41194","upvote_count":"1"},{"content":"For B, how can you make sure there are enough spots available?","poster":"simonyu","timestamp":"1632935640.0","comment_id":"37274","upvote_count":"2"},{"content":"Answer should be B\nLambda cannot be used to process 4 hour execution","timestamp":"1632682440.0","upvote_count":"1","comment_id":"30452","poster":"dojo"},{"poster":"examacc","comment_id":"24042","content":"B looks good.","timestamp":"1632619500.0","upvote_count":"1"}]}],"exam":{"name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"provider":"Amazon","id":32,"isImplemented":true,"isBeta":false,"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025"},"currentPage":75},"__N_SSP":true}