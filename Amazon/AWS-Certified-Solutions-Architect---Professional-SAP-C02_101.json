{"pageProps":{"questions":[{"id":"zeQAcFGV0jRbuoCUzIQ7","unix_timestamp":1673713140,"question_id":501,"question_text":"A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able to send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","choices":{"C":"Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.","B":"Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for the NLConnect each device to the NLB.","A":"Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to Amazon MQ.","D":"Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB. Configure a mutual TLS certificate authorizer on the HTTP API. Run an MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB."},"exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/95292-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["C (97%)","3%"],"topic":"1","timestamp":"2023-01-14 17:19:00","answer_ET":"C","discussion":[{"comments":[{"timestamp":"1673713140.0","content":"Option A, setting up Amazon MQ queues and connecting each device to a queue, would require significant operational overhead to manage the queues and ensure that each device is properly authenticated and connected.\nOption B and D, using a Network Load Balancer (NLB) with a Lambda authorizer or an Amazon API Gateway HTTP API with a mutual TLS certificate authorizer and running an MQTT broker on EC2 instances, would also introduce more operational complexity and overhead compared to using AWS IoT Core.","upvote_count":"6","poster":"masetromain","comment_id":"775699"}],"content":"Selected Answer: C\nThe correct solution is C. Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.\n\nAWS IoT Core is a fully managed service that enables secure, bi-directional communication between internet-connected devices and the AWS Cloud. It supports the MQTT protocol and includes built-in device authentication and access control. By using AWS IoT Core, the company can easily provision and manage the X.509 certificates for each device, and connect the devices to the service with minimal operational overhead.","poster":"masetromain","upvote_count":"23","comment_id":"775698","timestamp":"1673713140.0"},{"comment_id":"1264613","poster":"MAZIADI","upvote_count":"1","content":"Selected Answer: C\nAWS IoT Core: This service is specifically designed for managing IoT devices and supports the MQTT protocol natively. It provides built-in support for device authentication using X.509 certificates.","timestamp":"1723460700.0"},{"upvote_count":"1","timestamp":"1710678780.0","comment_id":"1175814","content":"Selected Answer: C\nC, use IoT Core","poster":"gofavad926"},{"timestamp":"1707586260.0","poster":"8608f25","upvote_count":"1","content":"Selected Answer: C\nOption C is the most suitable solution as AWS IoT Core is specifically designed for IoT scenarios, including device management and secure communication. AWS IoT Core natively supports MQTT, a lightweight communication protocol ideal for IoT devices. It allows devices to connect securely with an individual X.509 certificate for authentication, significantly reducing operational overhead compared to managing a custom MQTT broker or other intermediate services. AWS IoT Core also simplifies device management and scaling, making it the best choice for the described use case.","comment_id":"1146545"},{"poster":"bjexamprep","comment_id":"1127509","content":"Selected Answer: C\nI don’t like C, but C might be the preferred answer. \nThere are thousands of devices. If C is the real answer, there should be a way to automatically create IOT thing and provision certificate. The answer seems implying to create IOT thing and provision certificates manually. If IoT core doesn’t have this automation feature, this definitely is not the right answer in real life. \nIf there is this automation way and the question designer is expecting the exam taker to know this detail, that might be too specific for the exam takers.\nD is ugly, and usually is not a correct answer in most question designs. But it provides a feasible way in the real life comparing with C.","upvote_count":"3","timestamp":"1705784520.0"},{"timestamp":"1691072640.0","upvote_count":"1","poster":"waoo","comment_id":"971188","content":"答案是C\nhttps://aws.amazon.com/cn/iot-core/faqs/?nc=sn&loc=5&dn=2"},{"poster":"NikkyDicky","comment_id":"941191","content":"Selected Answer: C\nit's C","timestamp":"1688326560.0","upvote_count":"1"},{"timestamp":"1679832600.0","comment_id":"850973","upvote_count":"1","content":"Selected Answer: C\nI choose C","poster":"mfsec"},{"upvote_count":"2","poster":"zejou1","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/iot/latest/developerguide/attach-to-cert.html\n\nIt is C, - you have to do this through IOT core, for the devices you need an AWS IOT \"thing\" and then provision a certificate for the thing. from there connect the device.","comment_id":"842293","timestamp":"1679086440.0"},{"comments":[{"content":"Sorry I meant \"C\"","timestamp":"1678401000.0","upvote_count":"2","poster":"forceli","comment_id":"834485"}],"timestamp":"1678400580.0","poster":"forceli","upvote_count":"1","comment_id":"834481","content":"Selected Answer: A\n-The AWS IoT Device SDKs support device communications using the MQTT\n-Device connections to AWS IoT use X.509 client certificates \nhttps://docs.aws.amazon.com/iot/latest/developerguide/iot-connect-devices.html"},{"poster":"zozza2023","content":"Selected Answer: C\nC is correct (less op overhead than A)","comment_id":"793318","upvote_count":"2","timestamp":"1675111560.0"},{"timestamp":"1673819340.0","content":"C is correct","comment_id":"777096","poster":"zhangyu20000","upvote_count":"3"}],"isMC":true,"answer_description":"","answer_images":[],"answer":"C","question_images":[]},{"id":"tTYKoUsrtkhiCL5I560Z","timestamp":"2023-01-14 17:24:00","url":"https://www.examtopics.com/discussions/amazon/view/95293-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"C","question_images":[],"isMC":true,"unix_timestamp":1673713440,"choices":{"D":"Provision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers’ IAM role to only allow access to their own AWS CloudFormation stack.","A":"Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers’ IAM role to only allow access to Amazon S3 and AWS CloudFormation. Use AWS CloudFormation templates to provision resources.","C":"Update the IAM policy for the engineers’ IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.","B":"Update the IAM policy for the engineers’ IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources."},"answer_images":[],"discussion":[{"timestamp":"1677820260.0","poster":"God_Is_Love","comment_id":"827677","content":"Selected Answer: C\nTricky one. Question has a hint -\"to enforce the new restriction on the IAM role\" (note its not IAM policy as mentioned in option B) Creating a policy with approved resources first and assuming/applying that role to engineers will enforce. So C is correct. (B lacks enforcement, B is incorrect)","upvote_count":"18"},{"comment_id":"891681","content":"Selected Answer: C\nC is correct not B , AWS CloudFormation makes calls to create, modify, and delete those resources on their behalf. To separate permissions between a user and the AWS CloudFormation service, use a service role. AWS CloudFormation uses the service role's policy to make calls instead of the user's policy. For more information, see AWS CloudFormation service role . check this out . https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html\nOption B would allow engineers to provision resources using other methods outside of CloudFormation, which would not comply with the new company policy. This would make it difficult to enforce the new restriction on the IAM role that the engineers use for access.","upvote_count":"11","poster":"rbm2023","timestamp":"1683493740.0"},{"poster":"amministrazione","comment_id":"1275530","upvote_count":"1","content":"C. Update the IAM policy for the engineers’ IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.","timestamp":"1725095700.0"},{"comment_id":"1175822","upvote_count":"1","poster":"gofavad926","timestamp":"1710679080.0","content":"Selected Answer: C\nC, use the IAM service role to execute the stack"},{"comment_id":"1148821","timestamp":"1707786960.0","content":"Selected Answer: C\nOption C is the most effective solution. It involves updating the engineers’ IAM role to only allow actions related to AWS CloudFormation, effectively preventing direct provisioning or modification of AWS resources outside of CloudFormation. By creating a service role (with permissions to provision approved resources) that CloudFormation assumes when executing templates, you enforce the provisioning of only approved resources through CloudFormation. This setup provides a clear separation of permissions: engineers can manage CloudFormation stacks but cannot directly create resources unless defined in a CloudFormation template and permitted by the service role.\n\nOption B suggests updating the IAM policy to allow only the provisioning of approved resources and CloudFormation actions. This approach could theoretically work by explicitly listing allowed actions for specific AWS services in the IAM policy. However, it might be challenging to maintain and could inadvertently permit actions outside of CloudFormation, depending on the policy’s specificity.","poster":"8608f25","upvote_count":"2"},{"timestamp":"1705301880.0","upvote_count":"2","content":"Selected Answer: C\nA = doesn't prevent to have a CloudFromation template with non-approved resources deployed\nB = this doesn't prevent engineers to provision resources from console or cli\nC = correct\nD = doesn't prevent to provision non-approved resources or to provision only via CloudFormation","comment_id":"1123121","poster":"ninomfr64"},{"timestamp":"1701908640.0","upvote_count":"1","comment_id":"1089870","content":"B would be created generally in organization. C is fine , but more restriction , the user can only use the cloud formation stack sets only which is not good for organization level.","poster":"subbupro"},{"comment_id":"1070075","timestamp":"1699944360.0","poster":"severlight","content":"Selected Answer: C\nwith B engineer will be able to directly provision resources without using of CF","upvote_count":"1"},{"poster":"venvig","upvote_count":"1","timestamp":"1692871680.0","content":"Selected Answer: C\nThe two contenders are Option B and C.\nOption B would allow the users to provision the approved resources without using CloudFormation (as the Users’ IAM role would permission that). So, this violates the requirement.\nOption C would ensure that Only Cloudformation can provision the resources. So, that’s the correct answer.","comment_id":"989065"},{"timestamp":"1691545020.0","content":"Selected Answer: C\nI prefer C, because you need to give permission to cloud formation","comment_id":"976156","poster":"CuteRunRun","upvote_count":"1"},{"timestamp":"1688326680.0","comment_id":"941193","upvote_count":"1","poster":"NikkyDicky","content":"Selected Answer: C\nC no doubt"},{"timestamp":"1679832720.0","poster":"mfsec","content":"Selected Answer: C\nC. Update the IAM policy for the engineers’ IAM role with permissions to only allow AWS CloudFormation actions.","comment_id":"850974","upvote_count":"2"},{"comment_id":"818543","upvote_count":"3","timestamp":"1677108600.0","poster":"c73bf38","content":"Selected Answer: C\nC IAM policy is allowing to provision of approved resources."},{"upvote_count":"3","poster":"Musk","comment_id":"793139","timestamp":"1675101420.0","content":"Selected Answer: C\nB does not enfore CF, otherwise it would work."},{"timestamp":"1674920340.0","poster":"Untamables","comment_id":"790740","content":"Selected Answer: C\nC\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/security-best-practices.html#use-iam-to-control-access\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html","upvote_count":"3"},{"upvote_count":"4","poster":"Nicocacik","comment_id":"779391","timestamp":"1673997600.0","content":"Selected Answer: C\nYou have to use a service role"},{"upvote_count":"2","comment_id":"778256","comments":[{"comments":[{"timestamp":"1700727960.0","poster":"Japanese1","content":"B works but is inappropriate.\nYou fail to consider that you NEED to use CFn for resource provisioning.\nOption B does not meet the requirement to limit this.","comment_id":"1078229","upvote_count":"1"}],"upvote_count":"1","content":"Both options B and C are correct.\n\nOption B: Update the IAM policy for the engineers’ IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.\n\nOption C: Update the IAM policy for the engineers’ IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.\n\nBoth options will enforce the new restriction on the IAM role that the engineers use for access, by limiting their access only to approved resources and only allowing them to provision resources using AWS CloudFormation. The specif","poster":"masetromain","comment_id":"778258","timestamp":"1673902740.0"}],"poster":"masetromain","content":"C. Update the IAM policy for the engineers’ IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.\n\nThis option is also correct, it is a way to restrict the access of engineers to only be able to perform AWS CloudFormation actions and provision only approved resources. By giving only permissions to the IAM role used by engineers for CloudFormation and creating a separate IAM role with permissions to provision approved resources and then assigning that role to CloudFormation during stack creation, we ensure that engineers can only provision the approved resources using CloudFormation.","timestamp":"1673902620.0"},{"timestamp":"1673818800.0","comment_id":"777089","upvote_count":"3","content":"C is correct\nA: only allow CF, no approved resources\nB: role allow approved resources and CF. User can bypass CF\nD: CF only","comments":[],"poster":"zhangyu20000"},{"comments":[{"timestamp":"1673886960.0","upvote_count":"3","content":"it allow provision of approved resources and CF in same time. User can provision resources directly without CF","comment_id":"777936","poster":"zhangyu20000"},{"timestamp":"1673713440.0","poster":"masetromain","comment_id":"775710","content":"Other options are not the correct answer because:\n\nOption A only allows access to Amazon S3 and AWS CloudFormation, but it doesn't restrict the engineers from provisioning resources other than the approved ones\nOption C only allows AWS CloudFormation actions, but it doesn't restrict the engineers from provisioning resources other than the approved ones\nOption D is incomplete, it doesn't specify how to restrict the engineers from provisioning resources other than the approved ones","upvote_count":"3"}],"timestamp":"1673713440.0","upvote_count":"1","content":"Selected Answer: B\nThe correct answer is B. Update the IAM policy for the engineers’ IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.\n\nThis solution will meet the requirement of enforcing the new restriction on the IAM role that the engineers use for access by only allowing the engineers to use AWS CloudFormation to provision the approved resources. By updating the IAM policy to only allow provisioning of approved resources and AWS CloudFormation, it will restrict the engineers from provisioning any other resources. Engineers will use AWS CloudFormation templates to create stacks with approved resources, which will ensure that only the approved resources are being provisioned.","comment_id":"775709","poster":"masetromain"}],"answer_ET":"C","answers_community":["C (98%)","2%"],"answer_description":"","question_id":502,"topic":"1","question_text":"A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved resources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to enforce the new restriction on the IAM role that the engineers use for access.\n\nWhat should the solutions architect do to create the solution?","exam_id":33},{"id":"EfEJg7yldpTLGqc7ofuE","topic":"1","question_text":"A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The application is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and needs to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the data for 120 days only, after which the data can be deleted.\n\nThe solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.\n\nWhich storage strategy is the MOST cost-effective and meets the design requirements?","choices":{"D":"Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days.","C":"Design the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs a query to delete any records older than 120 days.","B":"Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.","A":"Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a lifecycle policy to delete data older than 120 days."},"unix_timestamp":1673713560,"timestamp":"2023-01-14 17:26:00","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/95294-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"question_id":503,"answer_images":[],"answers_community":["B (76%)","D (24%)"],"question_images":[],"answer":"B","exam_id":33,"discussion":[{"timestamp":"1673713560.0","content":"Selected Answer: B\nThe most cost-effective and efficient solution that meets the design requirements would be option B, Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.\n\nDynamoDB is a NoSQL key-value store designed for high scale and performance. It is fully managed by AWS and can easily handle millions of small records per minute. Additionally, with the TTL feature, you can set an expiration time for each record, so that the data can be automatically deleted after the specified time period.","comments":[{"timestamp":"1673713560.0","upvote_count":"7","poster":"masetromain","content":"Option A, storing each incoming record as a single .csv file in an Amazon S3 bucket, would not be a good option because it would be difficult to retrieve individual records from the .csv files, and will likely increase the cost of data retrieval.\n\nOption C, storing each incoming record in a single table in an Amazon RDS MySQL database, would be a more expensive option as RDS is typically more expensive than DynamoDB. Additionally, running a cron job to delete old data could lead to additional operational overhead.\n\nOption D, storing incoming records in batches in an S3 bucket, would be a less efficient option as it would require additional processing and parsing of the data to retrieve individual records.","comment_id":"775715"}],"comment_id":"775714","upvote_count":"23","poster":"masetromain"},{"upvote_count":"6","comments":[{"timestamp":"1733925900.0","content":"Actually, the limit you mentioned for point D is per prefix or path…. Not the whole bucket. With proper data distribution across prefixes it can accommodate easily for the load mentioned.","upvote_count":"2","comment_id":"1325048","poster":"ahhatem"}],"content":"A. No, because millions of writes to a single .csv file would cause read and write latency\n\nB. Yes, because DynamoDB can support peaks of more than 20 million requests per second.\n\nC. No, because creating nightly cron is unnecessary, and a relation database isn't designed to ingest millions of small records per minute\n\nD. No, because S3 supports 210,000 PUT requests per minute (3,500 requests per second * 60 seconds per min) which is far less than 1,000,000+ writes per minute","timestamp":"1689457740.0","poster":"dkx","comment_id":"952712"},{"upvote_count":"1","timestamp":"1741932420.0","content":"Selected Answer: D\nFor those who said B, how many WCU is needed for dynamoDB?\nGiven:\n1 million records per minute\n4KB per record\nThis translates to approximately 16,667 records per second (1,000,000 / 60)\nFor DynamoDB WCU calculation:\n1 WCU = 1 write per second for items up to 1KB\nFor items larger than 1KB, the WCU is rounded up to the next 1KB\nFor 4KB items, each write will consume 4 WCUs\nTherefore:\nWCUs needed = (Records per second) × (Item size in KB rounded up)\nWCUs = 16,667 × 4\nWCUs = 66,668 WCUs\nFirst, you need to increase the quotas for that table by submitting a support ticket.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html\nSecond, this is very expensive.\nObviously, combine it with kinesis data agent and firehorse that write to S3 will be much reliable options but it will increase the cost significantly. But still cheaper than the dynamo db options.\nhttps://calculator.aws/#/estimate?id=87f1df21449660b0b9d61a6c1153632b1983d2e4","comment_id":"1395528","poster":"vmia159"},{"poster":"soulation","timestamp":"1741151280.0","upvote_count":"1","content":"Selected Answer: D\nOption B is too expensive.","comment_id":"1365273"},{"comment_id":"1325687","upvote_count":"1","content":"Selected Answer: D\nIf you really think about being cost effective than Option D is the right choice","timestamp":"1734015120.0","poster":"sergza"},{"comment_id":"1324661","upvote_count":"1","content":"Selected Answer: D\nWhy Option D Might Be Cost-Effective:\nLower Storage Costs:\nS3 storage is generally cheaper than DynamoDB when dealing with large amounts of data (e.g., $0.023/GB/month for S3 Standard vs. $0.25/GB/month for DynamoDB on-demand).\nBatching Reduces API Call Costs:\nBy batching multiple records into a single object, you reduce the number of PUT requests to S3. This can lead to lower API costs compared to writing each record individually to DynamoDB.\nLifecycle Policies for Data Expiry:\nS3 lifecycle policies automatically clean up data older than 120 days, similar to DynamoDB's TTL feature.","poster":"Heman31in","timestamp":"1733848500.0"},{"timestamp":"1725095700.0","comment_id":"1275531","poster":"amministrazione","upvote_count":"1","content":"D. Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days."},{"comment_id":"1226332","content":"Selected Answer: B\nObviously it is DynamoDB. Although as a side node I would say it is probably a very bad choice as it would be astronomically expensive for millions of writes per minute…. A Kinesis Data Streams would make much more sense especially that the data is only needed for 3 months…","poster":"ahhatem","comments":[{"poster":"ahhatem","content":"After a second thought, I am not sure it is B. D would be much cheaper if it means that objects buffered and combined before write. But the word “batch” doesn’t make me comfortable, batching means writing the objects in one go… nothing implies the objects would be combined …","timestamp":"1733928360.0","comment_id":"1325064","upvote_count":"1"}],"upvote_count":"2","timestamp":"1717784400.0"},{"upvote_count":"1","content":"Selected Answer: B\nB, dynamodb is the best option","poster":"gofavad926","comment_id":"1175828","timestamp":"1710679260.0"},{"timestamp":"1707790500.0","comment_id":"1148841","poster":"8608f25","content":"Selected Answer: B\nFor small records less than 4 KB, DynamoDB can efficiently handle the ingestion of millions o records per minute from devices around the world, meeting the application's design requirements for low-latency data access. Additionally, DynamoDB's Time to Live (TTL) feature allows for automatic deletion of items after a specific period, aligning with the requirement to store data for only 120 days.","upvote_count":"1"},{"poster":"ninomfr64","content":"Selected Answer: B\nA = S3 is not great with small files and searching for data based on index (a common pattern is to store object metadata in a database like DDB, OpenSearch or RDS/Aurora). Many small files can lead to high costs for retrieval\nB = correct\nC = single-table design, high volume write/retrieval os small object and no need for complex query are better served and cost less with DDB rather than RDS\nD = more efficient than A, but still S3 metadata search feature is limited","upvote_count":"1","timestamp":"1705302600.0","comment_id":"1123127"},{"poster":"severlight","timestamp":"1699944720.0","upvote_count":"1","content":"Selected Answer: B\nsee uC6rW1aB's answer","comment_id":"1070076"},{"timestamp":"1695133800.0","comment_id":"1011419","content":"Selected Answer: B\nB is the best for cost-effective.\nD is more cost for S3 request","poster":"vjp_training","upvote_count":"1"},{"upvote_count":"3","poster":"uC6rW1aB","content":"Selected Answer: B\nRef: https://aws.amazon.com/dynamodb/pricing/on-demand/\nDynamoDB read requests can be either strongly consistent, eventually consistent, or transactional. A strongly consistent read request of up to 4 KB requires one read request unit. For items larger than 4 KB, additional read request units are required.","comments":[{"poster":"uC6rW1aB","timestamp":"1693813440.0","comment_id":"998316","content":"for a US East write object price: \nS3 Standard put object per thound cost $0.005 -> 1 million put cost $5 ( per minutes in this situation )\nDynamo DB 1 million write cost $1.25 is a lot of cheaper","upvote_count":"4"}],"comment_id":"998299","timestamp":"1693812960.0"},{"poster":"Gmail78","content":"Selected Answer: D\nDinamo DB is at least 5X more expensive than S3 for this use case. There are million of writing and each is 4K, total disk space is 10-15TB.","timestamp":"1693182840.0","upvote_count":"1","comment_id":"991720","comments":[{"timestamp":"1693532760.0","poster":"vn_thanhtung","comment_id":"995586","upvote_count":"2","content":"D - S3 metadata search feature does not exist"}]},{"upvote_count":"1","comment_id":"990997","content":"Selected Answer: D\nAlthough both B and D are correct, Option D is more cost effective.","poster":"Soweetadad","timestamp":"1693075740.0"},{"poster":"YodaMaster","comments":[{"content":"B satisfies the requirement but D is not. The keyword here is Low latency - “ a durable location where it can be retrieved with low latency”","comment_id":"1055977","upvote_count":"3","timestamp":"1698467220.0","poster":"blackgamer"}],"content":"Selected Answer: D\nGoing with D as it's more cost effective. Question didn't ask for more efficient.","comment_id":"942298","upvote_count":"1","timestamp":"1688434140.0"},{"poster":"NikkyDicky","timestamp":"1688327040.0","content":"Selected Answer: D\nD is more cost effective, evenif more complex","comment_id":"941195","upvote_count":"2"},{"poster":"[Removed]","upvote_count":"2","content":"Selected Answer: D\nWhile B is viable, it seems like it's a massively expensive option - millions of writes per minute is a lot of WCU. Similarly, C would require a beefy database to support that many writes, may or may not be cheaper than the DDB option. But in a question asking for most cost effective, scalable writes from many sources screams an S3-based solution to me, which leaves A and D. Too many small files (A) and S3's performance will degrade, and millions of objects per minute seems like it would tax S3's ability to index buckets. Nothing in D is impossible to implement; though it's not the simplest solution, it's by far the cheapest.","comment_id":"934504","timestamp":"1687786080.0"},{"poster":"geo1551","upvote_count":"1","content":"I think it is A.\nI'm not English native speaker, but I read it the way that each incoming record will be stored in separate file, thus the retrieval of a single record would be fast based on it's key. S3 is by far the cheapest option of all.","comment_id":"922820","timestamp":"1686725760.0"},{"content":"Most cost-effective will be D. and the following makes the size under 5TB , under the limits.\nThe solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.","timestamp":"1682052780.0","comments":[{"poster":"youngmanaws","upvote_count":"3","timestamp":"1682971500.0","content":"sorry, metadata is incorrect because the following: \"millions of small records per minute from devices all around the world. Each record is less than 4 KB in size \"","comment_id":"886702"}],"comment_id":"876204","poster":"youngmanaws","upvote_count":"4"},{"comment_id":"853731","content":"Selected Answer: B\nB DynamoDB","poster":"Amac1979","upvote_count":"1","timestamp":"1680039660.0"},{"content":"Selected Answer: B\nB. Design the application to store each incoming record in an Amazon DynamoDB table","timestamp":"1679832780.0","comment_id":"850975","poster":"mfsec","upvote_count":"1"},{"timestamp":"1674426840.0","poster":"DDONG","content":"B SAP01 #613","upvote_count":"3","comment_id":"784745"},{"content":"C is correct\nDynamodb support 4KB size, low latency and TTL","comment_id":"777098","poster":"zhangyu20000","upvote_count":"3","comments":[{"upvote_count":"1","poster":"Atila50","content":"do yo mean B","timestamp":"1673904180.0","comment_id":"778291"},{"poster":"masetromain","content":"https://www.examtopics.com/discussions/amazon/view/28419-exam-aws-certified-solutions-architect-professional-topic-1/\n\nOption C is using RDS MySQL which is a relational database and will not be able to handle the scale of millions of small records per minute with low latency and it is not designed for automatic deletion of records based on time and it will be more expensive as well.","upvote_count":"1","timestamp":"1673902980.0","comment_id":"778261"},{"content":"Answer is D.\nRegarding B: WCU - Each API call to write data to your table is a write request. For items up to 1 KB in size, one WCU can perform one standard write request per second. Items larger than 1 KB require additional WCUs. \nTo do millions PUT per minute, it will be highly expensive ddb table. \nBatch S3 PUT makes more sense.","upvote_count":"2","poster":"ele","timestamp":"1699096260.0","comment_id":"1062037"}],"timestamp":"1673819520.0"}],"answer_description":""},{"id":"6W9l8AmpW82jHWzUTQYV","answer_ET":"D","answer":"D","question_id":504,"choices":{"B":"Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.","D":"Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.","A":"Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.","C":"Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region."},"url":"https://www.examtopics.com/discussions/amazon/view/95297-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"answer_description":"","discussion":[{"content":"Selected Answer: D\nThis really should be multi-az but you could move to it w/ D.\nHere is the key to this one though; Highest Availability - the read replica is an asynchronous copy, while backup is a \"time\". Easier to do the read replica, and flip the switches than to reload from backup. Global Tables relate to DynomoDB https://disaster-recovery.workshop.aws/en/services/databases/dynamodb/dynamo-global-table.html\nLittle handy \"DR\" guide","upvote_count":"15","comment_id":"842392","poster":"zejou1","timestamp":"1679100900.0"},{"timestamp":"1725095760.0","poster":"amministrazione","content":"D. Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.","upvote_count":"1","comment_id":"1275532"},{"comment_id":"1123147","upvote_count":"1","content":"Selected Answer: D\nA = you cannot promote an automated backup to a standalone DB (you restore a backup into a new DB instance instead). Creating a read replica could help in this scenario in case it is cross-region. This is not specified\nB = RDS does not support global table, copying a read replicas from a region to another make no sense to me\nC = see B\nD = correct","poster":"ninomfr64","timestamp":"1705304520.0"},{"content":"Selected Answer: D\nD for sure","comment_id":"941197","poster":"NikkyDicky","upvote_count":"1","timestamp":"1688327160.0"},{"comment_id":"891703","upvote_count":"2","poster":"rbm2023","timestamp":"1683498120.0","content":"Selected Answer: D\nThere is Aurora Global Database, DynamoDB Global Tables and the question is about RDS for MySQL DB Instance. \nhttps://jayendrapatil.com/aws-aurora-global-database-vs-dynamodb-global-tables/\nSo, options B and C are not acceptable.\nOption D refers to using a cross-region replication for disaster recovery which can be found here https://disaster-recovery.workshop.aws/en/services/databases/rds/rds-cross-region.html \nFollowing article demonstrates a similar scenario using RDS for SQL Server\nhttps://aws.amazon.com/blogs/database/use-cross-region-read-replicas-with-amazon-relational-database-service-for-sql-server/\nThe design seems to be what we are looking in terms of option D.\nhttps://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2022/11/15/dbblog-2614-image001.png"},{"poster":"mfsec","timestamp":"1679832900.0","upvote_count":"1","content":"Selected Answer: D\nD makes the most sense","comment_id":"850977"},{"content":"Selected Answer: D\nNo global tables concept in RDS, B,C are eliminated. A is wrong in terms of backing up Db copy to a standalone instance ? D provides read replicas for reading and also swtiches as a failiover in times of disruption and becomes primary. this is how HA can be maintained. D is correct.","upvote_count":"3","timestamp":"1677821400.0","poster":"God_Is_Love","comment_id":"827685"},{"upvote_count":"2","comment_id":"826791","poster":"spd","content":"Selected Answer: D\nMySQL - Read Replica. In this case, this is not aurora so not the global table option and hence can not be B and C","timestamp":"1677757200.0"},{"upvote_count":"2","timestamp":"1677749280.0","content":"I haven't found any information about a \"global table\" for RDS.\nGlobal tables are for DynamoDB. For Aurora, it's called \"global databases\".\nRDS for MySQL supports cross-region read replicas https://aws.amazon.com/fr/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/, so D has a better availability than A.","comment_id":"826698","poster":"sambb"},{"comment_id":"779550","poster":"icassp","upvote_count":"4","timestamp":"1674010800.0","comments":[{"comment_id":"814077","poster":"AlanKrish","timestamp":"1676811120.0","upvote_count":"1","content":"Is Aurora not part of RDS? You can choose Aurora's compatibility with MySQL and PostreSQL)."}],"content":"Selected Answer: D\nfor B,C, Amazon RDS does not support global tables yet. Only Aurora supports."},{"poster":"zhangyu20000","comments":[{"poster":"masetromain","timestamp":"1673903160.0","upvote_count":"1","comment_id":"778270","content":"https://www.examtopics.com/discussions/amazon/view/69438-exam-aws-certified-solutions-architect-professional-topic-1/"},{"poster":"Shahul75","content":"B is not right. Only Aurora has global tables. RDS don't","comment_id":"797150","timestamp":"1675438080.0","upvote_count":"1"},{"comment_id":"809932","poster":"[Removed]","content":"Cant be B due to global tables, ReadReplicas are supported with RDS and other options of restoring from backup do not create high availability","upvote_count":"1","timestamp":"1676491620.0"}],"upvote_count":"3","comment_id":"777099","timestamp":"1673819580.0","content":"D is correct"},{"upvote_count":"2","comments":[{"comment_id":"826155","content":"If the disruption is an outage that takes the Region offline completely, how could we use Lambda to copy the read replica from the Region that is no longer available to the backup to another Region?","upvote_count":"1","poster":"Sarutobi","timestamp":"1677694860.0"},{"timestamp":"1673713800.0","comment_id":"775726","poster":"masetromain","content":"Option C involves configuring global tables and automated backups on Amazon RDS. This solution is less efficient since it does not provide automatic failover across multiple regions and requires additional steps to copy the read replicas from one Region to another Region using AWS Lambda.\n\nOption D involves configuring read replicas on Amazon RDS. In the case of disruption, promoting a cross-Region and read replica to be a standalone DB instance. This solution is less efficient than Option B since it does not provide automatic failover across multiple regions and requires manual intervention to promote the read replica to a standalone instance.","comments":[{"poster":"bcx","upvote_count":"1","timestamp":"1687173060.0","comment_id":"927402","content":"In fact global tables is a Dynamo DB thing. And RDS has Aurora Global Database. In this case Aurora is out of the question, it says RDS MySql, not Aurora (RDS) MySQL."}],"upvote_count":"1"}],"content":"Selected Answer: B\nThe correct answer is option B. Configuring global tables and read replicas on Amazon RDS with the cross-Region scope enabled provides the highest availability for the database. In case of disruption, the company can use AWS Lambda to copy the read replicas from one Region to another Region, ensuring that the website remains operational at all times. This solution provides automatic failover across multiple regions and allows for fast recovery in case of a disruption.\n\nOption A involves promoting an automated backup to be a standalone DB instance and creating a replacement read replica that has the promoted DB instance as its source. This solution is less efficient since it requires manual intervention and additional steps to promote the backup and create a replacement read replica.","comment_id":"775724","poster":"masetromain","timestamp":"1673713800.0"}],"question_images":[],"exam_id":33,"answers_community":["D (94%)","6%"],"timestamp":"2023-01-14 17:30:00","topic":"1","question_text":"A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all times for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.\n\nWhich solution will provide the HIGHEST availability for the database?","unix_timestamp":1673713800,"answer_images":[]},{"id":"r2KT3cK1faSnPi3onye2","choices":{"D":"Modify the Site-to-Site VPN’s virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private getaway between the two VPCs.","B":"Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN connection to the transit gateway. Add a route to direct traffic to the peered VPCs, and add an authorization rule to give clients access to the VPCs A and B.","C":"Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks. Wait for up to 5 minutes for BGP propagation to finish.","A":"Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks."},"answer":"A","unix_timestamp":1673714040,"topic":"1","discussion":[{"upvote_count":"10","comment_id":"891772","timestamp":"1683516060.0","poster":"rbm2023","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/pt_br/whitepapers/latest/aws-vpc-connectivity-options/aws-transit-gateway-vpn.html\nTransit gateway is an AWS managed high availability and scalability regional network transit hub used to interconnect VPCs and customer networks. AWS Transit Gateway + VPN, using the Transit Gateway VPN Attachment, provides the option of creating an IPsec VPN connection between your remote network and the Transit Gateway over the internet, as shown in the following picture.\nhttps://docs.aws.amazon.com/images/whitepapers/latest/aws-vpc-connectivity-options/images/image4.png\nOption A is the correct answer since the transit gateway will allow both VPCs to connect to the on premises network.\nOption B suggests the same feature but is using the Transit Gateway in a incorrect way. The soul purpose of the gateway is to have point for interconnectivity."},{"content":"For those that have written SAP-C02, how relevant are these questions to the real exam questions? After adequate preparation, I wanted to truly test my knowledge before dabbling into the exam and would really appreciate anyone's candid opinion.\nThanks.","timestamp":"1683000900.0","poster":"Tunstim","comment_id":"887003","upvote_count":"5","comments":[{"comment_id":"1006124","poster":"chikorita","content":"please reply to him","upvote_count":"2","timestamp":"1694569260.0"}]},{"timestamp":"1725095760.0","poster":"amministrazione","content":"A. Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.","upvote_count":"1","comment_id":"1275533"},{"upvote_count":"1","comment_id":"1272259","poster":"jceleste1","content":"After all, what is the right answer A or D ?","timestamp":"1724610120.0"},{"timestamp":"1710679800.0","poster":"gofavad926","content":"Selected Answer: A\nA, Transit Gateway","comment_id":"1175830","upvote_count":"1"},{"content":"Selected Answer: A\nOption A is the most straightforward and effective solution. A transit gateway acts as a cloud router that simplifies network topology and connectivity between on-premises networks, VPCs, and other AWS services. By attaching both VPCs (A and B) and the Site-to-Site VPN to a single transit gateway and updating the route tables accordingly, Example Corp. can enable seamless communication between its on-premises network and both VPCs. This approach minimizes operational effort by centralizing network management and eliminating the need for complex routing configurations or multiple VPN connections.\n\nOption D proposes modifying the Site-to-Site VPN’s virtual private gateway to include both VPC A and VPC B. However, a virtual private gateway cannot be directly shared or split between VPCs in the manner described. This option misunderstands the architecture of AWS networking components and their capabilities.","poster":"8608f25","timestamp":"1707791400.0","comment_id":"1148853","upvote_count":"1"},{"content":"Selected Answer: A\nA = correct\nB = if you setup a second VPN you do not need a TGW\nC = peering does not allow edge-to-edge routing (aka VPC B cannot access on-premise via VPC A and vice versa)\nD = Virtual Private Gateway is specific to a single VPC","poster":"ninomfr64","comment_id":"1123167","upvote_count":"1","timestamp":"1705305780.0"},{"poster":"Russs99","upvote_count":"2","timestamp":"1693139700.0","content":"Selected Answer: A\nreluctantly selecting option A. these answers do not take into consideration that the On-promises already has a peered connection to VPC A through the existing site to site","comment_id":"991448"},{"comment_id":"976250","upvote_count":"1","timestamp":"1691556780.0","poster":"CuteRunRun","content":"Selected Answer: A\nI think A is right, I do not know why other guys select D"},{"comment_id":"941198","timestamp":"1688327400.0","content":"Selected Answer: A\nsurely A","poster":"NikkyDicky","upvote_count":"1"},{"timestamp":"1682692680.0","content":"Selected Answer: A\nA is the best option.\n\nCreating a transit gateway and attaching Site-to-Site VPN, VPC A, and VPC B to the transit gateway would enable the on-premise servers to access VPC B with minimal operational effort. The transit gateway route tables would need to be updated with IP range routes for all the other networks to enable communication between the VPCs and the on-premises servers.","comment_id":"883550","poster":"Parsons","upvote_count":"2"},{"comments":[{"comment_id":"852881","poster":"Arnaud92","timestamp":"1679986140.0","upvote_count":"4","content":"B is impossible : When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW (it\"s a 3 entities). You can however connect a S2S VPN to a TGW (onprem to TGW) (which is solution A).\nC : Does not work, there is no transitivity on AWS. S2S VPN cannot reach VPC B through VPC A\nD is impossible : There is no magic, you cannot \"split\" router (that does not exist). VGW is attach to a single VPC. A S2S VPN cannot multiplex VPC"}],"timestamp":"1679986140.0","upvote_count":"1","poster":"Arnaud92","content":"Selected Answer: A\nSolution A is the only one possible solution","comment_id":"852880"},{"upvote_count":"1","poster":"mfsec","comment_id":"850980","timestamp":"1679833020.0","content":"Selected Answer: A\nA. Create a transit gateway. Attach the Site-to-Site VPN"},{"poster":"dev112233xx","upvote_count":"1","comment_id":"844284","content":"Selected Answer: A\nA makes sense to me","timestamp":"1679263200.0"},{"poster":"taer","upvote_count":"1","content":"Selected Answer: A\nA for me","timestamp":"1679221800.0","comment_id":"843640"},{"upvote_count":"1","comments":[{"poster":"God_Is_Love","upvote_count":"1","content":"oops this is wrong..VPN can be attached...","timestamp":"1677824280.0","comment_id":"827705","comments":[{"poster":"God_Is_Love","upvote_count":"1","content":"Moderator, please delete this comment..","timestamp":"1677824700.0","comment_id":"827708"}]},{"upvote_count":"1","content":"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\nWhen you create a virtual private gateway, you can specify the private Autonomous System Number (ASN) for the Amazon side of the gateway. If you don't specify an ASN, the virtual private gateway is created with the default ASN (64512). You cannot change the ASN after you've created the virtual private gateway. Due to this reason, So A is not possible (with least effort). Answer should be B.","comments":[{"timestamp":"1679985660.0","poster":"Arnaud92","upvote_count":"1","comment_id":"852867","content":"THe VGW for VPCA is no more needed on A because you attach the VPCA to the TGW.\nThe ASN will be on the TGW attachment with the S2S VPN.\nThis is the best solution.\nIn the meantime, B is impossible. When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW. You can however connect a S2S VPN to a TGW (onprem to TGW)."}],"poster":"God_Is_Love","timestamp":"1677824640.0","comment_id":"827707"}],"poster":"God_Is_Love","content":"Selected Answer: B\nA has this wierd wording - attaching S-S VPN ? transit gateway attaches to VPCs only not S-S vpn. A is wrong. Since VPC A and VPC B are already peered, the easiest solution to connect from the on-premises servers to VPC B would be to create another Site-to-Site VPN connection between the on-premises data center and VPC B. This would require minimal operational effort, as the existing VPN connection with VPC A can remain unchanged.","timestamp":"1677823980.0","comment_id":"827700"},{"timestamp":"1676591640.0","comment_id":"811209","content":"Selected Answer: A\nTGW is the solutions","upvote_count":"1","poster":"spd"},{"poster":"CloudFloater","upvote_count":"1","timestamp":"1676239560.0","comment_id":"806859","content":"Selected Answer: D\nD.\nA - setting up new transit gateway - more operational cost\nB - new site-to-site - vpn - more operational cost\nC - updating route tables for site to site vpn and 3 VPCs, bgp config update for 3 networks .. more operational cost\nD - because it requires the least amount of operational effort. By modifying the Site-to-Site VPN’s virtual private gateway definition to include both VPC A and VPC B and splitting the two routers of the virtual private gateway between the two VPCs, the on-premises servers can connect to both VPCs with minimal additional effort. This solution leverages the existing Site-to-Site VPN and does not add any additional layers of complexity to the network.","comments":[{"poster":"Sarutobi","upvote_count":"1","comment_id":"826170","content":"It looks like you understood D. How can you split two routers of the VGW between two VPCs? The VGW is an object that can be attached to a single VPC at a time. What are the two routers they talk about here? Are there on-prem routers?","timestamp":"1677695580.0"},{"upvote_count":"1","comment_id":"852863","timestamp":"1679985300.0","content":"D is not possible. There is no magic, you cannot \"split\" router (that does not exist). VGW is attach to a single VPC. A S2S VPN cannot multiplex VPC ;)","poster":"Arnaud92"}]},{"poster":"zozza2023","timestamp":"1675112460.0","upvote_count":"1","content":"Selected Answer: A\nsolution is A","comment_id":"793329"},{"content":"Selected Answer: A\nA. Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.\n\nThis option will allow you to connect from the on-premises servers to VPC B with the least operational effort, as it utilizes the transit gateway to connect all networks and allows for easy updates to the route tables. BGP propagation is not necessary and the use of transit gateway will simplify the traffic routing.","poster":"masetromain","timestamp":"1673903520.0","comment_id":"778278","upvote_count":"4"},{"comment_id":"777105","poster":"zhangyu20000","content":"A is correct. on-premise is connected to TGW, use TDW to talk to VPC A/B\nB: too many VPN connections\nC: VPC B cannot use VPC A to VPN\nD: one VPN gateway cannot be associated with more than one VPC","comments":[{"timestamp":"1673903580.0","content":"Is correct that option A is the correct answer. Thank for you help.","poster":"masetromain","comment_id":"778279","upvote_count":"1"}],"timestamp":"1673820360.0","upvote_count":"4"},{"comment_id":"775732","poster":"masetromain","content":"Selected Answer: B\nThe correct answer is B. This option involves creating a new Site-to-Site VPN connection between the on-premises network and VPC B, and connecting that VPN connection to the transit gateway. This allows the on-premises network to access resources in VPC B through the transit gateway, which already has a connection to VPC A and can route traffic between the two VPCs. This solution requires minimal additional configuration and minimal operational overhead.","timestamp":"1673714040.0","upvote_count":"1","comments":[{"timestamp":"1673714040.0","upvote_count":"1","comment_id":"775733","content":"Option A involves creating a transit gateway, attaching all three networks (the on-premises network, VPC A, and VPC B) to it and updating routing tables for all networks. This solution would require significant additional configuration and would be more complex to set up and maintain.\n\nOption C involves updating routing tables for all three networks and configuring BGP propagation. This solution is complex and would require additional configuration and maintenance.\n\nOption D involves modifying the definition of the Site-to-Site VPN to include both VPC A and B and splitting the two VPN routers between the two VPCs. This solution is complex and would require additional configuration and maintenance.","poster":"masetromain"}]}],"timestamp":"2023-01-14 17:34:00","exam_id":33,"answer_description":"","answer_images":[],"isMC":true,"question_images":[],"question_text":"Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to VPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which has a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.\n\nExample Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.\n\nWhich solution will meet this requirement with the LEAST operational effort?","answer_ET":"A","answers_community":["A (91%)","6%"],"url":"https://www.examtopics.com/discussions/amazon/view/95298-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":505}],"exam":{"numberOfQuestions":529,"isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true,"provider":"Amazon","id":33},"currentPage":101},"__N_SSP":true}