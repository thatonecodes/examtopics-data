{"pageProps":{"questions":[{"id":"GQGXG3CKZKCmrnz750F4","question_id":71,"answer_description":"","topic":"2","answer":"B","question_images":[],"timestamp":"2019-09-06 03:33:00","answer_images":[],"discussion":[{"comment_id":"75929","content":"B. https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html#working-with-snapshot-restore-table-from-snapshot","timestamp":"1634782560.0","upvote_count":"4","poster":"Bulti"},{"poster":"san2020","content":"my selection B","upvote_count":"3","timestamp":"1633905120.0","comment_id":"52400"},{"content":"Its B\nhttps://aws.amazon.com/about-aws/whats-new/2016/03/amazon-redshift-now-supports-table-level-restore/","comment_id":"35514","timestamp":"1633872120.0","upvote_count":"3","poster":"awsexpert"},{"timestamp":"1633753860.0","upvote_count":"1","content":"I think B is the simplest solution here","poster":"BigEv","comment_id":"20693"},{"comment_id":"9830","comments":[{"upvote_count":"3","timestamp":"1633205880.0","content":"B is correct. Truncate and load takes time which will impact user hence not A. When are you planing to take the test?","comment_id":"9894","comments":[{"timestamp":"1633232400.0","content":"using google mail","poster":"ranabhay","upvote_count":"1","comment_id":"9902"}],"poster":"ranabhay"}],"content":"I went with A. As the question states without impacting users.","poster":"mattyb123","upvote_count":"1","timestamp":"1632246840.0"}],"choices":{"D":"Use the ALTER TABLE REVERT command and specify a time stamp of immediately before the data deletion. Specify the Amazon Resource Name of the snapshot as the SOURCE and use the OVERWRITE REPLACE option.","C":"Restore the snapshot to a new Amazon Redshift cluster. Create a DBLINK between the two clusters in the original cluster, TRUNCATE the destination table, then use an INSERT command to copy the data from the new cluster.","A":"Restore the snapshot to a new Amazon Redshift cluster, then UNLOAD the table to Amazon S3. In the original cluster, TRUNCATE the table, then load the data from Amazon S3 by using a COPY command.","B":"Use the Restore Table from a Snapshot command and specify a new table name DROP the original table, then RENAME the new table to the original table name."},"url":"https://www.examtopics.com/discussions/amazon/view/4775-exam-aws-certified-big-data-specialty-topic-2-question-14/","exam_id":17,"answer_ET":"B","unix_timestamp":1567733580,"isMC":true,"answers_community":[],"question_text":"Multiple rows in an Amazon Redshift table were accidentally deleted. A System Administrator is restoring the table from the most recent snapshot. The snapshot contains all rows that were in the table before the deletion.\nWhat is the SIMPLEST solution to restore the table without impacting users?"},{"id":"e7chMZTXbrMeIPguyV4t","choices":{"B":"Encrypt the credit card number with a symmetric encryption key, and give the key only to the authorized Data Scientist.","A":"Store a cryptographic hash of the credit card number.","D":"Encrypt the credit card number with an asymmetric encryption key and give the decryption key only to the authorized Data Scientist.","C":"Mask the credit card numbers to only show the last four digits of the credit card number."},"answer_description":"","answer_ET":"C","question_id":72,"isMC":true,"answer":"C","exam_id":17,"discussion":[{"upvote_count":"10","comments":[{"upvote_count":"3","content":"Agree, name + 4 last digit might work but is not as good as using a hash function.\n- same number will produce the same result\n- can't revert\nhttps://en.wikipedia.org/wiki/Cryptographic_hash_function","timestamp":"1635369780.0","poster":"AdamSmith","comment_id":"47617"}],"poster":"awsexpert","comment_id":"35515","content":"Its A, as hash will always be same for a given number. So they can easily group by it. Moreover hashing is one way they can't decrypt.","timestamp":"1635322260.0"},{"comment_id":"338106","upvote_count":"1","poster":"DerekKey","timestamp":"1635895080.0","content":"A - is the most suitable \nB & D are wrong - no one should have access to data\nC - will not work with only 4 digits since there may be duplication between different customers"},{"timestamp":"1635697380.0","content":"A... A hash function with the same input always generates the same output, so if there are 10 records of credit card 12345 the hash will generate xyzabc for all those 10 records.","poster":"Royk2020","upvote_count":"1","comment_id":"149090"},{"upvote_count":"1","timestamp":"1635620880.0","content":"D is the right answer. Its more secure and KMS or HSM can be easily used to achieve this.","comment_id":"103818","poster":"k115"},{"timestamp":"1635489900.0","upvote_count":"2","content":"my selection A","comments":[{"poster":"G3","timestamp":"1635494760.0","content":"Hashing credit card numbers is the most unsafe application practice as per several of the blogposts. It could also be easily hacked using a simple powershell script. I will go with C.","upvote_count":"2","comment_id":"93613"}],"comment_id":"52402","poster":"san2020"},{"poster":"BigEv","upvote_count":"2","comment_id":"20699","timestamp":"1634712060.0","content":"I will go with A"},{"timestamp":"1634529480.0","comment_id":"19906","content":"A CC has 16 digits... if you have only the last 4 you will most likely group incorrect transactions that were made using different CCs. A seems to be correct.","poster":"d00ku","upvote_count":"3"},{"timestamp":"1634371920.0","content":"I choose C. By last 4 digit it is possible to group the transactions for a customer. Assuming a customer do not have more than one same last-4 digit cc numbers. The data is needed for data scientist (may be to perform ML). You need to have un-encrypted data to do the modeling. Encrypting/Decrypting for ach Data scientist may not be a viable solution.","comment_id":"19449","poster":"cybe001","upvote_count":"3","comments":[{"poster":"cybe001","upvote_count":"3","timestamp":"1635164700.0","content":"A is correct","comment_id":"24307"}]},{"comment_id":"13812","timestamp":"1634342280.0","content":"so what's the right answer , any conclusion ?","upvote_count":"1","poster":"bigdatalearner"},{"upvote_count":"4","content":"The last 4 digits are NOT unique... they may duplicate.. so, I think A is correct","comments":[{"poster":"ME2000","timestamp":"1635340140.0","comments":[{"poster":"Corram","timestamp":"1635576720.0","content":"1. You don't know from the question that the data scientist has access to the full name.\n2. 4 digits give you 10k options. The name \"James Smith\" exists more than 38k times in the world. So uniqueness would still be questionable.","upvote_count":"2","comment_id":"101879"}],"content":"Combine last four digits with CC holder name, so it will be unique to short out.\nSo Option C is correct","comment_id":"38612","upvote_count":"1"}],"timestamp":"1634074200.0","comment_id":"12682","poster":"VB"},{"comments":[{"content":"https://security.stackexchange.com/questions/19860/minimum-requirements-for-storing-last-4-digits-of-credit-card-number. Seems you can get away with storing the last 4 digits.","poster":"mattyb123","comments":[{"upvote_count":"4","poster":"s3an","comments":[{"poster":"s3an","content":"A should be correct","timestamp":"1635311100.0","upvote_count":"2","comment_id":"25850"},{"upvote_count":"1","comment_id":"184120","timestamp":"1635886860.0","content":"A credit card contains 16 digits, many customers will have the last 4 digits identical with the remaining 12 digits different, this goes against the requirement.","poster":"vicks316"}],"comment_id":"24287","timestamp":"1634844300.0","content":"I'd say C is correct. The question said nothing about \"names\", so grouping with name and last 4 digits will help even if 2 customers have same last 4 digits"}],"upvote_count":"2","comment_id":"8635","timestamp":"1633600020.0"},{"content":"maybe A, how to perform Group while masking and storing the last 4 digits?","poster":"apertus","timestamp":"1633653660.0","comment_id":"11138","upvote_count":"1"}],"content":"Thoughts on this one? I couldn't find any blog posts on this. I was thinking either A or C.","upvote_count":"1","timestamp":"1632898440.0","poster":"mattyb123","comment_id":"8293"}],"timestamp":"2019-08-26 06:11:00","topic":"2","answer_images":[],"unix_timestamp":1566792660,"answers_community":[],"question_images":[],"question_text":"An organization's data warehouse contains sales data for reporting purposes. data governance policies prohibit staff from accessing the customers' credit card numbers.\nHow can these policies be adhered to and still allow a Data Scientist to group transactions that use the same credit card number?","url":"https://www.examtopics.com/discussions/amazon/view/4080-exam-aws-certified-big-data-specialty-topic-2-question-15/"},{"id":"qrflTCR08Gjm0UXEaJ8c","discussion":[{"upvote_count":"1","content":"A wrong -> Use Amazon ElastiCache in front of the read replicas?\nC wrong -> why five read replicas?\nD wrong -> 1. Pgpool can run in an Amazon EC2 instance for dev and test and a fleet of EC2 instances with Elastic Load Balancing and Auto Scaling in production however, we strongly recommend that you test pgpool with your PostgreSQL client before making any changes to your architecture. 2. The company needs to analyze and aggregate a part of the data in near real time to feed an internal dashboard. - for Redshift it will not be near real time - For Amazon Redshift destination, Amazon Kinesis Data Firehose delivers data to your Amazon S3 bucket first and then issues Redshift COPY command to load data from your S3 bucket to your Redshift cluster. The frequency of data delivery to Amazon S3 - buffer interval 60 to 900 seconds\nB should be correct","poster":"DerekKey","timestamp":"1636270320.0","comment_id":"338127"},{"poster":"vicks316","timestamp":"1636052280.0","comment_id":"184306","content":"B for sure","upvote_count":"1"},{"timestamp":"1635926400.0","comment_id":"140568","poster":"Venky_2020","upvote_count":"1","content":"B looks to be best solution"},{"upvote_count":"1","content":"C is the correct answer","comment_id":"103821","timestamp":"1635796140.0","poster":"k115"},{"timestamp":"1635765180.0","poster":"YashBindlish","content":"Correct Answer is B","comment_id":"74776","upvote_count":"1"},{"comment_id":"52403","poster":"san2020","timestamp":"1635685320.0","content":"my selection B","upvote_count":"1"},{"upvote_count":"1","content":"“The question is the answer.”\n― Thomas Vato, Questology\nWhat is the BEST approach for serving and analyzing data, considering the constraint of the low latency on the highly demanded data?\nFirstly, Question asking for analyzing data (The company needs to analyze and aggregate a part of the data in near real time to feed an internal dashboard.), So all about OLAP\nSecondly, low latency on the highly demanded data (The company is experiencing high latency during special event spikes, with millions of concurrent users.)- Sort of Caching solution\nFinally, D is the correct answer","timestamp":"1635373800.0","comment_id":"38617","poster":"ME2000","comments":[{"content":"agregate and analyze data in real-time with Redshift?\nmillions of users and transactions with Redshift?\nnice quote, doesn't work","comments":[{"content":"AdamSmith - you are 100% right - Redshift is not intended for such workload","timestamp":"1636221900.0","comment_id":"338114","poster":"DerekKey","upvote_count":"1"}],"poster":"AdamSmith","upvote_count":"4","comment_id":"47627","timestamp":"1635385500.0"}]},{"poster":"stevenchenau","timestamp":"1635202020.0","upvote_count":"2","comment_id":"31179","content":"Support B. Real-time bidding is a perfect use case of DynamoDB accelerator."},{"poster":"mattyb123","comment_id":"8294","timestamp":"1632145800.0","upvote_count":"1","comments":[{"comment_id":"8636","poster":"mattyb123","content":"Could be as simple as B due to DynamoDB","comments":[{"comments":[{"comment_id":"8872","content":"A? selected?","comments":[{"timestamp":"1633678080.0","content":"What are your thoughts on D again? AWS blog post shows how it can be done and AWS want users to view and consult the AWS blog posts to assist in creating solutions. As the question asks 'near real time' and 'BEST approach for serving and analyzing data' redshift can do SQL queries the fastest and with the caching engine attached with pgpool this can be done.","comment_id":"9226","upvote_count":"1","poster":"mattyb123"}],"upvote_count":"1","poster":"jlpl","timestamp":"1633633680.0"}],"poster":"mattyb123","upvote_count":"1","comment_id":"8796","content":"Since the question refers to monolithic i am now assuming it is referring to architecture design tiers using Relational Databases.This link https://blog.acolyer.org/2019/03/25/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases/ discusses the improvements on using Aurora.","timestamp":"1632987000.0"},{"comments":[{"upvote_count":"10","comments":[{"timestamp":"1634420280.0","poster":"mattyb123","content":"Thanks for the correction @ranabhay","comment_id":"9664","upvote_count":"1"}],"content":"millions of concurrent users, low latency => DynamoDB and DAX seems correct.\nreal time dash board => DynamoDB streams helps also elasticsearch can do aggregation.","comment_id":"9515","poster":"ranabhay","timestamp":"1634364360.0"},{"poster":"mattyb123","content":"Apologies. Have done more research on this question. I think the answer is B not D. The reason for D being incorrect is AWS big data blog post mentions only having 6 to 10 users for the use case. B for the following reasons DynamoDB scales well for the millions of users, DynamoDB streams can be used to aggregate data, Lambda function to push to Elasticsearch. Elasticsearch can be used for application monitoring and Analyzing product usage data. Has Kibana visualisations. Lastly question doesn't mention anything about querying\n1.https://rockset.com/blog/live-dashboards-dynamodb-streams-lambda-elasticache/\n2.https://aws.amazon.com/blogs/compute/indexing-amazon-dynamodb-content-with-amazon-elasticsearch-service-using-aws-lambda/\n3. https://aws.amazon.com/blogs/startups/combining-dynamodb-and-amazon-elasticsearch-with-lambda/\n4.https://d1.awsstatic.com/whitepapers/Big_Data_Analytics_Options_on_AWS.pdf?did=wp_card&trk=wp_card","comments":[{"timestamp":"1634781960.0","upvote_count":"6","comment_id":"20730","content":"Good catch, B +1","poster":"BigEv"}],"timestamp":"1634368980.0","comment_id":"9520","upvote_count":"23"}],"comment_id":"9383","timestamp":"1633804140.0","poster":"mattyb123","upvote_count":"1","content":"I dont think Amazon Elastisearch Service is the right use case here."}],"timestamp":"1632436080.0","upvote_count":"2"}],"content":"D's similar setup explained here https://aws.amazon.com/blogs/big-data/using-pgpool-and-amazon-elasticache-for-query-caching-with-amazon-redshift/. Just keen to hear other thoughts selected this option last time and didnt score well in storage section. I thought best practice was to use Redshift for OLAP instead of OLTP. Keen to get everyone else's thoughts?"}],"answer_images":[],"answer_description":"","unix_timestamp":1566793260,"timestamp":"2019-08-26 06:21:00","exam_id":17,"answer_ET":"Explanation","url":"https://www.examtopics.com/discussions/amazon/view/4081-exam-aws-certified-big-data-specialty-topic-2-question-16/","answers_community":[],"choices":{"B":"Use Amazon DynamoDB to store real-time data with Amazon DynamoDB. Accelerator to serve content quickly. use Amazon DynamoDB Streams to replay all changes to the table, process and stream to Amazon Elasti search Service with AWS Lambda.","C":"Use Amazon RDS with Multi Availability Zone. Provisioned IOPS EBS volume for storage. Enable up to five read replicas to serve read-only content quickly. Use Amazon EMR with Sqoop to import Amazon RDS data into HDFS for analysis.","A":"Use Amazon Aurora with Multi Availability Zone and read replicas. Use Amazon ElastiCache in front of the read replicas to serve read-only content quickly. Use the same database as datasource for the dashboard.","D":"Use Amazon Redshift with a DC2 node type and a multi-mode cluster. Create an Amazon EC2 instance with pgpoo1 installed. Create an Amazon ElastiCache cluster and route read requests through pgpoo1, and use Amazon Redshift for analysis. D"},"question_images":[],"answer":"Explanation","topic":"2","question_text":"A real-time bidding company is rebuilding their monolithic application and is focusing on serving real-time data. A large number of reads and writes are generated from thousands of concurrent users who follow items and bid on the company's sale offers.\nThe company is experiencing high latency during special event spikes, with millions of concurrent users.\nThe company needs to analyze and aggregate a part of the data in near real time to feed an internal dashboard.\nWhat is the BEST approach for serving and analyzing data, considering the constraint of the row latency on the highly demanded data?","question_id":73,"isMC":true},{"id":"Ha2mF97hgwXMUVR7Vn3q","question_id":74,"answer_description":"","topic":"2","answer":"A","question_images":[],"timestamp":"2019-08-26 06:24:00","answer_images":[],"discussion":[{"content":"Its D.\n1. https://sagemaker-workshop.com/builtin/rcf.html\n2. https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html","poster":"mattyb123","comment_id":"8295","timestamp":"1632314280.0","upvote_count":"6"},{"poster":"Venky_2020","timestamp":"1636036500.0","content":"Monitor data in realtime + Random cut forect term --> Inclines to Kinesis data analytics","upvote_count":"1","comment_id":"140570"},{"poster":"san2020","upvote_count":"3","comment_id":"52405","timestamp":"1635350820.0","content":"my selection D"},{"upvote_count":"3","timestamp":"1635006840.0","comment_id":"47640","content":"whenever see anomaly, think Random cut forest","poster":"AdamSmith"},{"comment_id":"8759","content":"D for now after reading url link from below","comments":[{"content":"Also very good description in the big data exam preparation course on AWS. https://www.aws.training/Details/Curriculum?id=21332","upvote_count":"2","comment_id":"8780","poster":"mattyb123","timestamp":"1634597640.0"}],"timestamp":"1633833600.0","upvote_count":"2","poster":"jlpl"}],"choices":{"A":"Attach a Kinesis Firehose to the stream and persist the sensor data in an Amazon S3 bucket. Schedule an AWS Lambda function to run a query in Amazon Athena against the data in Amazon S3 to identify anomalies. When a change is detected, the Lambda function sends a message to the anomaly stream to open the valve.","C":"Launch a fleet of Amazon EC2 instances with a Kinesis Client Library application that consumes the stream and aggregates sensor data over time to identify anomalies. When an anomaly is detected, the application sends a message to the anomaly stream to open the valve.","D":"Create a Kinesis Analytics application by using the RANDOM_CUT_FOREST function to detect an anomaly. When the anomaly score that is returned from the function is outside of an acceptable range, a message is sent to the anomaly stream to open the valve.","B":"Launch an Amazon EMR cluster that uses Spark Streaming to connect to the Kinesis stream and Spark machine learning to detect anomalies. When a change is detected, the Spark application sends a message to the anomaly stream to open the valve."},"url":"https://www.examtopics.com/discussions/amazon/view/4082-exam-aws-certified-big-data-specialty-topic-2-question-17/","exam_id":17,"answer_ET":"A","unix_timestamp":1566793440,"isMC":true,"answers_community":[],"question_text":"A gas company needs to monitor gas pressure in their pipelines. Pressure data is streamed from sensors placed throughout the pipelines to monitor the data in real time. When an anomaly is detected, the system must send a notification to open valve. An Amazon Kinesis stream collects the data from the sensors and an anomaly Kinesis stream triggers an AWS Lambda function to open the appropriate valve.\nWhich solution is the MOST cost-effective for responding to anomalies in real time?"},{"id":"A9JH2NuMBDTwb7Fi7di8","question_id":75,"answer_description":"","topic":"2","answer":"BE","question_images":[],"timestamp":"2019-08-26 06:53:00","answer_images":[],"discussion":[{"comments":[{"timestamp":"1634681400.0","upvote_count":"1","content":"AD looks correct. D is a very clear choice based on the link that you provided.","comment_id":"102071","poster":"certish"},{"timestamp":"1635392700.0","upvote_count":"1","poster":"matthew95","comment_id":"112074","content":"A,D - exactly, for example, the game app preceding limits access in this way so that users can only access game data that is associated with their user ID.","comments":[{"upvote_count":"1","timestamp":"1636106040.0","comment_id":"184437","poster":"vicks316","comments":[{"content":"Can the same IAM policy have a read permission for website and write permission for gaming app? -> YES can have","comment_id":"338129","timestamp":"1636246440.0","poster":"DerekKey","upvote_count":"1"}],"content":"How would D work when website requires read permissions whereas the gaming application requires write access. Can the same IAM policy have a read permission for website and write permission for gaming app? Having one policy for each permission is cleaner in my opinion, going with A and E."}]}],"upvote_count":"8","timestamp":"1634506560.0","content":"Answer : A, D\nA- Mobile app integrating with an application hosted on AWS. So Cognito is a default choice allowing user to use their social media account to login and assume temporary credentials to login to the org application backing the mobile app or the website server backing the website assuming there are 2 different backend application for the mobile and web app.\nB- Not a good choice for mobile users. Works for internal users.\nC- This is wrong as the website needs the ability to do a GET as well.\nD- Refer to this link -> https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html and go down to the Limiting User access section where you will see how both the mobile app and the website users can be restricted using fine grained IAM policy using their AWS user ID or their google or facebook userID.\nE- Not a good choice because we want users to access only their statistical information and to be able to update their own profiles.","comment_id":"76061","poster":"Bulti"},{"timestamp":"1632171360.0","upvote_count":"5","comment_id":"8296","content":"Thoughts on A & D?\nhttps://docs.amazonaws.cn/en_us/IAM/latest/UserGuide/id_credentials_temp.html\nhttps://aws.amazon.com/iam/\nhttps://aws.amazon.com/blogs/security/create-fine-grained-session-permissions-using-iam-managed-policies/","poster":"mattyb123"},{"comment_id":"145496","comments":[{"upvote_count":"1","poster":"DerekKey","content":"Hailiang is 100% correct - Identity pool use cases - Give your users access to AWS resources, such as an Amazon DynamoDB table. \nBUT\nWhen you create User Pool you will be forced to create at least one Identity Poll :)\nSince a gaming organization is developing a !!new game!! the option A&D would fit best;\n- Use Amazon Cognito user pool to authenticate to both the website and the game application.\n- Create an IAM policy with fine-grained permission for both the website and the game application.","timestamp":"1636285500.0","comment_id":"338141"}],"poster":"hailiang","upvote_count":"1","timestamp":"1636044720.0","content":"It is BD, A is wrong since if you need the app or website to access to ddb, you need Cognito Identity Pool, but not only User Pool."},{"content":"A and D. I thought A and E at first sight, but fine grained is necessary, as you don't want to allow the website itself, but a user coming from that website.\nAlso, it is possible to have multiple statements in a policy, as some thought that this is not possible:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_statement.html","comment_id":"134072","timestamp":"1635896640.0","poster":"askaron","upvote_count":"1"},{"content":"A + D\n\nhttps://aws.amazon.com/blogs/aws/fine-grained-access-control-for-amazon-dynamodb/","upvote_count":"1","poster":"alopazo","comment_id":"120919","timestamp":"1635850860.0"},{"timestamp":"1634052780.0","upvote_count":"1","poster":"piemar","comment_id":"63041","content":"Why not B and E\n\nE is more finegrained as it is only allowing read for website and write for mobile"},{"timestamp":"1633974840.0","poster":"san2020","comment_id":"52406","content":"my selection AE","upvote_count":"2"},{"poster":"AdamSmith","comment_id":"47642","timestamp":"1633830180.0","upvote_count":"1","content":"A is obvious.\nD sounds right but the catch is using a single IAM policy for both the web server and the app, which is pretty bad.\nE satisfies the requirements.\nStill a hard choice but I'd go with E."},{"timestamp":"1633446480.0","comment_id":"40238","upvote_count":"1","poster":"ME2000","content":"Option B\nIdentity Providers and Federation\nIt is also useful if you are creating a mobile app or web application that requires access to AWS resources.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html"},{"timestamp":"1632856920.0","poster":"BigEv","content":"A is correct for sure.\nNot sure whether D or E should be the 2nd answer","upvote_count":"1","comment_id":"20744","comments":[{"content":"A&E; D is incorrect because we need different policies for game application role & user role from mobile devices.","upvote_count":"4","timestamp":"1633180980.0","comment_id":"21822","comments":[{"timestamp":"1633417740.0","poster":"hailiang","comment_id":"33844","upvote_count":"1","content":"in d it just says fine grained control but nothing about same or diff policies, so d is good"}],"poster":"chaudh"}]},{"content":"what's the right answer , any conclusion ?","upvote_count":"1","comment_id":"13813","poster":"bigdatalearner","timestamp":"1632668160.0","comments":[{"comments":[{"timestamp":"1633380300.0","comment_id":"26048","upvote_count":"1","content":"Option D&E both says \"an IAM policy\", so it doesn't mean same permission for both mobile and web. Answer still seems to be AD","poster":"s3an"}],"timestamp":"1632753180.0","upvote_count":"1","poster":"d00ku","comment_id":"20494","content":"I'm sure about A but not sure between D and E. My issue with D is that is states \"create one IAM policy\" - not sure if it can accommodate both user types (mobile, web) which need different permissions. E seems straightforward - PUT for mobile and GET for web..."}]},{"content":"Hi,\nNeed your input.\nAre these question really on actual exam? all of them?\nHave you scheduled/taken your exam how did you perform ?","timestamp":"1632474960.0","comments":[{"upvote_count":"5","poster":"mattyb123","content":"Yes, @ranabhay majority of these questions were on my exam. But as you have noticed some of the selected answers are incorrect which is why i have been so active to discuss the reasons why for certain answers. As you can tell with these questions they aren't worded very well on purpose to make you either over or under think the solution.","comment_id":"8935","timestamp":"1632616260.0"}],"upvote_count":"2","comment_id":"8771","poster":"ranabhay"},{"comment_id":"8760","content":"A=cognito, -> mobile device\nD= fine grain IAM\nmake sense,","comments":[{"comments":[{"upvote_count":"2","comment_id":"26047","poster":"s3an","content":"\"The game application is writing events directly to Amazon DynamoDB from the userâ€™s \"mobile device\" ...so it's a mobile app. Also fined grain IAM access doesn't mean single policy for both. AD seems right","timestamp":"1633354980.0"}],"comment_id":"24338","poster":"cybe001","content":"question doesn't say mobile app. I think BE is correct","upvote_count":"2","timestamp":"1633328700.0"}],"poster":"jlpl","timestamp":"1632371640.0","upvote_count":"3"}],"choices":{"A":"Use Amazon Cognito user pool to authenticate to both the website and the game application.","C":"Create an IAM policy with PUT permission for both the website and the game application.","B":"Use IAM identity federation to authenticate to both the website and the game application.","E":"Create an IAM policy with PUT permission for the game application and an IAM policy with GET permission for the website.","D":"Create an IAM policy with fine-grained permission for both the website and the game application."},"url":"https://www.examtopics.com/discussions/amazon/view/4083-exam-aws-certified-big-data-specialty-topic-2-question-18/","exam_id":17,"answer_ET":"BE","unix_timestamp":1566795180,"isMC":true,"answers_community":[],"question_text":"A gaming organization is developing a new game and would like to offer real-time competition to their users. The data architecture has the following characteristics:\n✑ The game application is writing events directly to Amazon DynamoDB from the user's mobile device.\n✑ Users from the website can access their statistics directly from DynamoDB.\n✑ The game servers are accessing DynamoDB to update the user's information.\n✑ The data science team extracts data from DynamoDB for various applications.\nThe engineering team has already agreed to the IAM roles and policies to use for the data science team and the application.\nWhich actions will provide the MOST security, while maintaining the necessary access to the website and game application? (Choose two.)"}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","isBeta":false,"id":17,"numberOfQuestions":85,"isMCOnly":true,"isImplemented":true,"name":"AWS Certified Big Data - Specialty"},"currentPage":15},"__N_SSP":true}