{"pageProps":{"questions":[{"id":"17msKeKP5k3j9Zns6A1v","answers_community":["D (100%)"],"answer_images":[],"question_images":[],"topic":"1","discussion":[{"poster":"SerialiDr","upvote_count":"5","content":"Selected Answer: D\nThis service is specifically designed for real-time processing of large-scale streaming data. Kinesis Data Streams allows multiple consumers to process the same stream concurrently, making it highly suitable for scenarios where you have high-volume data streams that need to be processed in real-time by various applications. It offers high throughput, scalability, and durability for streaming data, and enables multiple applications to process the same stream concurrently, making it the most cost-effective and efficient choice for this scenario.","timestamp":"1721902560.0","comment_id":"1131636"},{"timestamp":"1714286700.0","comment_id":"1056010","content":"Selected Answer: D\nReal-time data processing is KDS","upvote_count":"5","poster":"didorins"},{"upvote_count":"3","poster":"65703c1","content":"Selected Answer: D\nD is the correct answer.","comment_id":"1216921","timestamp":"1732393860.0"},{"upvote_count":"2","poster":"Claire_KMT","timestamp":"1714300980.0","comment_id":"1056124","content":"D. Amazon Kinesis Data Streams.\n\nAmazon Kinesis Data Streams is designed for real-time data streaming and allows multiple consumers to process data concurrently and in real-time. It can handle millions of events and provides a scalable and cost-effective solution for handling high-throughput data streams."}],"timestamp":"2023-10-28 08:45:00","question_id":141,"exam_id":24,"answer_description":"","answer":"D","question_text":"An application is real-time processing millions of events that are received through an API.\n\nWhat service could be used to allow multiple consumers to process the data concurrently and MOST cost-effectively?","isMC":true,"unix_timestamp":1698475500,"url":"https://www.examtopics.com/discussions/amazon/view/124775-exam-aws-certified-developer-associate-dva-c02-topic-1/","choices":{"B":"Amazon SNS with fanout to an SQS FIFO (first-in, first-out) queue for each application","A":"Amazon SNS with fanout to an SQS queue for each application","C":"Amazon Kinesis Firehose","D":"Amazon Kinesis Data Streams"},"answer_ET":"D"},{"id":"9g8XSVSDhtO2TwoaR8Xo","answer_ET":"A","exam_id":24,"unix_timestamp":1698495180,"answer":"A","answer_images":[],"discussion":[{"timestamp":"1715976240.0","content":"Selected Answer: A\nA. Add an Export declaration to the Outputs section of the original template and use ImportValue in other templates.","poster":"bhanupriya07","upvote_count":"6","comment_id":"1073665"},{"comment_id":"1216922","upvote_count":"1","poster":"65703c1","timestamp":"1732393920.0","content":"Selected Answer: A\nA is the correct answer."},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html","upvote_count":"3","poster":"KarBiswa","timestamp":"1725340740.0","comment_id":"1164561"},{"comments":[{"poster":"papason","comment_id":"1058488","timestamp":"1714451640.0","upvote_count":"3","content":"By adding an Export declaration to the Outputs section of the original CloudFormation template, you can make the bucket name available for other templates to import and use. This allows you to reference the bucket name directly in other templates without the need for additional resources or custom logic."}],"poster":"Claire_KMT","content":"A. Add an Export declaration to the Outputs section of the original template and use ImportValue in other templates.","timestamp":"1714306380.0","upvote_count":"2","comment_id":"1056175"}],"answer_description":"","question_text":"Given the following AWS CloudFormation template:\n\n//IMG//\n\n\nWhat is the MOST efficient way to reference the new Amazon S3 bucket from another AWS CloudFormation template?","url":"https://www.examtopics.com/discussions/amazon/view/124817-exam-aws-certified-developer-associate-dva-c02-topic-1/","choices":{"A":"Add an Export declaration to the Outputs section of the original template and use ImportValue in other templates.","C":"Create a custom AWS CloudFormation resource that gets the bucket name from the ContentBucket resource of the first stack.","B":"Add Exported: true to the Content.Bucket in the original template and use ImportResource in other templates.","D":"Use Fn::Include to include the existing template in other templates and use the ContentBucket resource directly."},"isMC":true,"timestamp":"2023-10-28 14:13:00","answers_community":["A (100%)"],"topic":"1","question_images":["https://img.examtopics.com/aws-certified-developer-associate-dva-c02/image14.png"],"question_id":142},{"id":"aF4n1iQ7dPgrZmCaGNY7","choices":{"A":"Move the application to a larger EC2 instance.","E":"Change the capacity mode of the DynamoDB table from provisioned to on-demand.","D":"Increase the frequency of requests to DynamoDB by decreasing the retry delay.","C":"Reduce the frequency of requests to DynamoDB by implementing exponential backoff.","B":"Increase the number of read capacity units (RCUs) that are provisioned for the DynamoDB table."},"answer_description":"","isMC":true,"topic":"1","question_id":143,"url":"https://www.examtopics.com/discussions/amazon/view/124816-exam-aws-certified-developer-associate-dva-c02-topic-1/","timestamp":"2023-10-28 14:12:00","answer_ET":"CE","answer_images":[],"unix_timestamp":1698495120,"question_text":"A developer has built an application that inserts data into an Amazon DynamoDB table. The table is configured to use provisioned capacity. The application is deployed on a burstable nano Amazon EC2 instance. The application logs show that the application has been failing because of a ProvisionedThroughputExceededException error.\n\nWhich actions should the developer take to resolve this issue? (Choose two.)","discussion":[{"upvote_count":"6","comment_id":"1073670","content":"Selected Answer: CE\nC. Reduce the frequency of requests to DynamoDB by implementing exponential backoff.\nE. Change the capacity mode of the DynamoDB table from provisioned to on-demand.","poster":"bhanupriya07","timestamp":"1700259720.0"},{"comment_id":"1164756","poster":"chikuwan","timestamp":"1709467980.0","upvote_count":"5","content":"Selected Answer: CE\nWhat the question said is insert data..so B increase read capacity is not correct.Hence C and E."},{"comment_id":"1296280","poster":"MasoudK","content":"B And C: While E could resolve the issue by automatically scaling the capacity, it may not be the most cost-effective solution if the traffic pattern is predictable and can be managed with provisioned capacity and backoff strategies.","timestamp":"1728692340.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"1290952","content":"Selected Answer: BC\nBy implementing exponential backoff, the application will automatically retry failed requests with increasing delays between attempts. This helps to spread out the requests over time, reducing the likelihood of exceeding the provisioned throughput.\n\nSince the table is configured to use provisioned capacity, increasing the number of RCUs will allow the table to handle more read requests per second. This directly addresses the ProvisionedThroughputExceededException by increasing the table's capacity to handle more concurrent requests.","timestamp":"1727591220.0","poster":"albert_kuo"},{"content":"Selected Answer: CE\nAnswer is C & E:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html\n\nProvisionedThroughputExceededException\nMessage: You exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes. To view performance metrics for provisioned throughput vs. consumed throughput, open the Amazon CloudWatch console.\n\nExample: Your request rate is too high. The AWS SDKs for DynamoDB automatically retry requests that receive this exception. Your request is eventually successful, unless your retry queue is too large to finish. Reduce the frequency of requests using Error retries and exponential backoff.","poster":"ahadh7621","comment_id":"1250791","timestamp":"1721357460.0","upvote_count":"1"},{"upvote_count":"2","poster":"65703c1","timestamp":"1716489360.0","comment_id":"1216924","content":"Selected Answer: CE\nCE is the correct answer."},{"poster":"KarBiswa","comment_id":"1166507","upvote_count":"3","timestamp":"1709647260.0","content":"Selected Answer: CE\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html"},{"timestamp":"1709451060.0","comment_id":"1164565","content":"Selected Answer: BC\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#:~:text=The%20DynamoDB%20console%20displays%20Amazon%20CloudWatch%20metrics%20for%20your%20tables%2C%20so%20you%20can%20monitor%20throttled%20read%20requests%20and%20write%20requests.%20If%20you%20encounter%20excessive%20throttling%2C%20you%20should%20consider%20increasing%20your%20table%27s%20provisioned%20throughput%20settings.","comments":[],"poster":"KarBiswa","upvote_count":"1"},{"upvote_count":"3","poster":"SerialiDr","timestamp":"1706206980.0","comment_id":"1131939","content":"Selected Answer: BC\nB. This error indicates that the application's request rate is exceeding the throughput that has been provisioned for the table. Increasing the provisioned read capacity units (RCUs) and/or write capacity units (WCUs) for the DynamoDB table will allow it to handle a higher request rate, thereby reducing the likelihood of encountering this error. However, this approach requires careful capacity planning and may increase costs.\n\nC. Exponential backoff is a standard error retry strategy that involves progressively increasing the delay between retries when there is a ProvisionedThroughputExceededException. This approach helps to smooth out the rate of requests, giving the table time to accommodate bursts of read or write requests. Implementing exponential backoff in the application will help to effectively manage request retries and reduce the chance of continually hitting the provisioned throughput limit."},{"poster":"Certified101","timestamp":"1702725960.0","comment_id":"1098109","content":"Selected Answer: CE\nC & E correct","upvote_count":"4"},{"comment_id":"1093992","poster":"tqiu654","content":"Selected Answer: BC\nBased on ChatGPT: BC","upvote_count":"1","timestamp":"1702344660.0"},{"content":"B. Increase the number of read capacity units (RCUs) that are provisioned for the DynamoDB table.\nOR\nE. Change the capacity mode of the DynamoDB table from provisioned to on-demand.\nC. Reduce the frequency of requests to DynamoDB by implementing exponential backoff.","comment_id":"1056174","timestamp":"1698495120.0","comments":[{"poster":"tapan666","timestamp":"1698549300.0","comment_id":"1056548","content":"It 'inserts' data, so it needs WCUs and not RCUs. So option B is invalid too. C and E are the correct options.","upvote_count":"10"}],"poster":"Claire_KMT","upvote_count":"1"}],"answer":"CE","answers_community":["CE (78%)","BC (22%)"],"question_images":[],"exam_id":24},{"id":"O3MRSGMINTjsryxhbGv2","choices":{"B":"Move the documents to an Amazon WorkDocs folder. Share the links of the WorkDocs folder with the external users.","D":"Create a role that has read-only access to the S3 bucket. Share the Amazon Resource Name (ARN) of this role with the external users.","A":"Use S3 presigned URLs to share the documents with the external users. Set an expiration time of 7 days.","C":"Create temporary IAM users that have read-only access to the S3 bucket. Share the access keys with the external users. Expire the credentials after 7 days."},"unix_timestamp":1698475620,"timestamp":"2023-10-28 08:47:00","question_text":"A company is hosting a workshop for external users and wants to share the reference documents with the external users for 7 days. The company stores the reference documents in an Amazon S3 bucket that the company owns.\n\nWhat is the MOST secure way to share the documents with the external users?","answer_description":"","question_id":144,"isMC":true,"topic":"1","exam_id":24,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/124778-exam-aws-certified-developer-associate-dva-c02-topic-1/","question_images":[],"answer":"A","answers_community":["A (90%)","10%"],"answer_ET":"A","discussion":[{"content":"Selected Answer: B\nB is the answear.\nPresinged URLs can be shared 12 hours max","upvote_count":"1","timestamp":"1742465520.0","poster":"0bdf3af","comment_id":"1400987"},{"content":"This appear at 17 Jun exam","timestamp":"1718598300.0","comment_id":"1231716","upvote_count":"1","poster":"tsangckl"},{"comment_id":"1216926","poster":"65703c1","timestamp":"1716489420.0","content":"Selected Answer: A\nA is the correct answer.","upvote_count":"2"},{"content":"Selected Answer: A\nPresigned URLs are a secure way to provide temporary access to specific objects in an S3 bucket. By generating a presigned URL, you grant time-limited access to the files without having to alter the underlying permissions of the S3 bucket or objects. You can set an expiration time for the URL, ensuring that access to the document is automatically revoked after 7 days. This method is straightforward and does not require the management of user identities or permissions beyond the scope of the shared objects.","timestamp":"1706207640.0","poster":"SerialiDr","comment_id":"1131950","upvote_count":"3"},{"comment_id":"1056171","poster":"Claire_KMT","timestamp":"1698495120.0","upvote_count":"2","content":"A. Use S3 presigned URLs to share the documents with the external users. Set an expiration time of 7 days."},{"content":"Selected Answer: A\nTemporary access to S3 object to external users is Pre-signed URL","timestamp":"1698475620.0","upvote_count":"4","poster":"didorins","comment_id":"1056015"}]},{"id":"vK2KtR9cCS7bfbmaIXmx","question_text":"A developer is planning to use an Amazon API Gateway and AWS Lambda to provide a REST API. The developer will have three distinct environments to manage: development, test, and production.\n\nHow should the application be deployed while minimizing the number of resources to manage?","url":"https://www.examtopics.com/discussions/amazon/view/124815-exam-aws-certified-developer-associate-dva-c02-topic-1/","unix_timestamp":1698495120,"exam_id":24,"answers_community":["C (100%)"],"answer":"C","question_images":[],"choices":{"B":"Assign a Region for each environment and deploy API Gateway and Lambda to each Region.","C":"Create one API Gateway with multiple stages with one Lambda function with multiple aliases.","A":"Create a separate API Gateway and separate Lambda function for each environment in the same Region.","D":"Create one API Gateway and one Lambda function, and use a REST parameter to identify the environment."},"isMC":true,"timestamp":"2023-10-28 14:12:00","answer_ET":"C","answer_description":"","topic":"1","discussion":[{"comment_id":"1131954","timestamp":"1721925660.0","poster":"SerialiDr","upvote_count":"6","content":"Selected Answer: C\nThis approach involves creating a single API Gateway and a single Lambda function. Within the API Gateway, you can create multiple stages, each corresponding to a different environment (development, test, production). Similarly, for the Lambda function, you can create multiple aliases, each pointing to a different version of the Lambda function that corresponds to each environment. This setup allows for clear separation of environments within the same set of resources. It simplifies deployment and management by reducing the number of resources and also provides an easy way to promote changes from one environment to another."},{"comment_id":"1216927","poster":"65703c1","timestamp":"1732394400.0","content":"Selected Answer: C\nC is the correct answer.","upvote_count":"2"},{"comment_id":"1097327","content":"Selected Answer: C\nAPI Gateway\nA stage in API Gateway represents a deployment of your API. You can have separate stages for development, test, and production.\nEach stage can have its own settings, such as stage variables, custom domains, and caching configurations.\nLambda function\nEach alias can point to a specific version of your Lambda function. This allows you to promote versions through different environments without changing the function's Amazon Resource Name (ARN) in your API Gateway.","poster":"TanTran04","upvote_count":"3","timestamp":"1718450100.0"},{"comment_id":"1073673","content":"Selected Answer: C\nC. Create one API Gateway with multiple stages with one Lambda function with multiple aliases.","poster":"bhanupriya07","upvote_count":"4","timestamp":"1715977440.0"},{"content":"C. Create one API Gateway with multiple stages with one Lambda function with multiple aliases.","comment_id":"1056170","upvote_count":"3","timestamp":"1714306320.0","poster":"Claire_KMT"}],"question_id":145,"answer_images":[]}],"exam":{"provider":"Amazon","isImplemented":true,"isMCOnly":true,"id":24,"name":"AWS Certified Developer - Associate DVA-C02","lastUpdated":"11 Apr 2025","numberOfQuestions":551,"isBeta":false},"currentPage":29},"__N_SSP":true}