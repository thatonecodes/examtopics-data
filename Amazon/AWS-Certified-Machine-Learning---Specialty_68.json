{"pageProps":{"questions":[{"id":"4p1aHk2pS1IgaI4pPS3k","unix_timestamp":1584915540,"question_images":[],"topic":"1","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/17280-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":336,"discussion":[{"poster":"HaiHN","upvote_count":"18","comment_id":"199576","timestamp":"1634690940.0","content":"B\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html\n\"...When your dataset contains hundreds of related time series, DeepAR outperforms the standard ARIMA and ETS methods. You can also use the trained model to generate forecasts for new time series that are similar to the ones it has been trained on.\""},{"content":"Selected Answer: B\n\"You can also use the trained model to generate forecasts for new time series that are similar to the ones it has been trained on\"\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html","poster":"ninomfr64","comment_id":"1245562","upvote_count":"1","timestamp":"1720625580.0"},{"timestamp":"1709797080.0","poster":"james2033","comment_id":"1167753","upvote_count":"1","content":"Selected Answer: B\n'autoregressive integrated moving average (ARIMA)' <--> DeepAR. https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html"},{"comment_id":"1007405","upvote_count":"1","timestamp":"1694682480.0","poster":"loict","content":"Selected Answer: B\nB - DeepAR is based on GluonTS, and can use multiple time series for learning"},{"timestamp":"1693238640.0","poster":"Mickey321","upvote_count":"1","comment_id":"992366","content":"Selected Answer: B\nOption B"},{"content":"Selected Answer: B\nDeepAr for new products forever!","poster":"Valcilio","timestamp":"1678278840.0","upvote_count":"4","comment_id":"832949"},{"comment_id":"804637","content":"Selected Answer: B\nThe DeepAR algorithm is a powerful time series forecasting algorithm that is designed to handle multiple time series data and can handle irregularly spaced time series data and missing values, making it a good fit for this task. \n\nAdditionally, the large amount of sales history data available in Amazon S3 makes the use of a deep learning algorithm like DeepAR more appropriate.","poster":"AjoseO","timestamp":"1676053200.0","upvote_count":"2"},{"upvote_count":"1","poster":"Shailendraa","content":"B https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html","timestamp":"1660978260.0","comment_id":"649316"},{"poster":"hans1234","timestamp":"1634125440.0","upvote_count":"3","content":"It is B","comment_id":"147330"},{"upvote_count":"1","comment_id":"67115","content":"This is the same question as Topic 2 Q4","poster":"ac427","timestamp":"1632511200.0"}],"choices":{"D":"Train a custom XGBoost model to forecast demand for the new product.","A":"Train a custom ARIMA model to forecast demand for the new product.","C":"Train an Amazon SageMaker k-means clustering algorithm to forecast demand for the new product.","B":"Train an Amazon SageMaker DeepAR algorithm to forecast demand for the new product."},"isMC":true,"answer":"B","timestamp":"2020-03-22 23:19:00","answer_images":[],"answer_description":"","question_text":"A large consumer goods manufacturer has the following products on sale:\n* 34 different toothpaste variants\n* 48 different toothbrush variants\n* 43 different mouthwash variants\nThe entire sales history of all these products is available in Amazon S3. Currently, the company is using custom-built autoregressive integrated moving average\n(ARIMA) models to forecast demand for these products. The company wants to predict the demand for a new product that will soon be launched.\nWhich solution should a Machine Learning Specialist apply?","exam_id":26,"answer_ET":"B"},{"id":"ZH6PDhZZdVwhW3dPA8b7","answer_images":[],"question_images":[],"topic":"1","exam_id":26,"discussion":[{"content":"Agreed. Ans is B","timestamp":"1632097680.0","poster":"mlyu","comment_id":"36228","upvote_count":"17"},{"timestamp":"1739654160.0","content":"Selected Answer: B\nGenerate an Amazon CloudWatch dashboard to create a single view for latency, memory utilization, and CPU utilization \nWhy?\nAmazon SageMaker automatically pushes latency and instance utilization metrics to CloudWatch.\nCloudWatch dashboards provide a single real-time view of these key metrics during load testing.\nYou can configure custom CloudWatch alarms to trigger auto scaling based on the load.","poster":"JonSno","comment_id":"1357050","upvote_count":"2"},{"comment_id":"963944","poster":"teka112233","content":"Selected Answer: B\nthe question is clear that the specialist is seeking for latency, memory utilization, and CPU utilization during the load test and the ideal answer for all of these is amazon cloud watch which give you all these metrics\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html","upvote_count":"2","timestamp":"1727163960.0"},{"poster":"Mickey321","comment_id":"973036","content":"Selected Answer: B\nThe reason for this choice is that Amazon CloudWatch is a service that monitors and manages your cloud resources and applications. It collects and tracks metrics, which are variables you can measure for your resources and applications1. Amazon SageMaker automatically reports metrics such as latency, memory utilization, and CPU utilization to CloudWatch2. You can use these metrics to monitor the performance and health of your SageMaker endpoint during the load test.","upvote_count":"1","timestamp":"1727163960.0"},{"poster":"teka112233","comment_id":"963943","content":"the question is clear that the specialist is seeking for latency, memory utilization, and CPU utilization during the load test and the ideal answer for all of these is amazon cloud watch which give you all these metrics \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html","timestamp":"1690387560.0","upvote_count":"1"},{"poster":"Venkatesh_Babu","upvote_count":"1","content":"Selected Answer: B\nI think it should be b","comment_id":"961673","timestamp":"1690207380.0"},{"timestamp":"1678262880.0","upvote_count":"3","comment_id":"832663","content":"Selected Answer: B\nIt's B, even the resources that aren't visible in a first try are visible if you use cloudwatch agent.","poster":"Valcilio"},{"comment_id":"757148","content":"Selected Answer: B\nShould be B","upvote_count":"2","poster":"DS2021","timestamp":"1672037880.0"},{"poster":"ystotest","content":"Selected Answer: B\nagreed with B","timestamp":"1669427460.0","upvote_count":"2","comment_id":"727200"},{"poster":"apprehensive_scar","comment_id":"539425","upvote_count":"1","timestamp":"1643868000.0","content":"B is the ans"},{"comment_id":"490516","comments":[{"timestamp":"1638870060.0","content":"After further research, I think answer is B. While indeed true that Cloudwatch does not have metrics for memory utilization by default, you can achieve by installing ClouldWatch agent on the EC2. The EC2 used by Sagemaker is pre-installed with Cloudwatch Agent.","poster":"anttan","upvote_count":"2","comment_id":"495840"}],"poster":"anttan","timestamp":"1638260460.0","upvote_count":"2","content":"Should be C right, as Cloudwatch does not have metrics for memory utilization."},{"content":"I do not think that CloudWatch, by default, logs memory utilization. It does log CPU utilization. If memory utilization is required, then a separate agent needs to be installed to watch for memory. Hence, in this case, we have to write an agent if the answer has to be B. Else, C looks to be a better solution.","comment_id":"477143","timestamp":"1636747440.0","poster":"[Removed]","upvote_count":"2"},{"content":"answer is B","upvote_count":"1","comment_id":"173409","poster":"Willnguyen22","timestamp":"1635564900.0"},{"poster":"syu31svc","timestamp":"1635302100.0","upvote_count":"1","content":"Answer is B 100%; very straightforward method","comment_id":"165032"},{"timestamp":"1634607240.0","comment_id":"147773","content":"B is correct. Don't need to use Kibana or QuickSight.","upvote_count":"1","poster":"scuzzy2010"},{"comment_id":"98665","content":"ans is B","poster":"roytruong","timestamp":"1633670820.0","upvote_count":"3"},{"content":"B is correct","poster":"cybe001","timestamp":"1632960720.0","upvote_count":"3","comment_id":"37749"}],"answer_description":"","answer_ET":"B","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/11560-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"D":"Send Amazon CloudWatch Logs that were generated by Amazon SageMaker to Amazon ES and use Kibana to query and visualize the log data.","C":"Build custom Amazon CloudWatch Logs and then leverage Amazon ES and Kibana to query and visualize the log data as it is generated by Amazon SageMaker.","B":"Generate an Amazon CloudWatch dashboard to create a single view for the latency, memory utilization, and CPU utilization metrics that are outputted by Amazon SageMaker.","A":"Review SageMaker logs that have been written to Amazon S3 by leveraging Amazon Athena and Amazon QuickSight to visualize logs as they are being produced."},"question_text":"A Machine Learning Specialist is building a model that will perform time series forecasting using Amazon SageMaker. The Specialist has finished training the model and is now planning to perform load testing on the endpoint so they can configure Auto Scaling for the model variant.\nWhich approach will allow the Specialist to review the latency, memory utilization, and CPU utilization during the load test?","isMC":true,"answer":"B","unix_timestamp":1578376560,"question_id":337,"timestamp":"2020-01-07 06:56:00"},{"id":"A3EbVYkHVpQQTvvqavLO","isMC":true,"answers_community":["C (93%)","7%"],"question_id":338,"discussion":[{"content":"Should be C. \n\"You donâ€™t need to specify the AWS KMS key ID when you download an SSE-KMS-encrypted object from an S3 bucket. Instead, you need the permission to decrypt the AWS KMS key.\n\nWhen a user sends a GET request, Amazon S3 checks if the AWS Identity and Access Management (IAM) user or role that sent the request is authorized to decrypt the key associated with the object. If the IAM user or role belongs to the same AWS account as the key, then the permission to decrypt must be granted on the AWS KMS keyâ€™s policy.\"\nhttps://aws.amazon.com/premiumsupport/knowledge-center/decrypt-kms-encrypted-objects-s3/?nc1=h_ls","comment_id":"296724","poster":"seanLu","upvote_count":"29","timestamp":"1649154660.0"},{"upvote_count":"6","poster":"askaron","comment_id":"290076","content":"Should be C.\nI think it is not possible to assign a key directly to a Sagemaker notebook instance like D suggests. \nNormally in AWS in general, IAM roles are used to do so. So C.","timestamp":"1648665720.0"},{"timestamp":"1725687360.0","upvote_count":"1","poster":"james2033","content":"Selected Answer: C\n'IAM role' principle of least privilege (PoLP)","comment_id":"1167750"},{"content":"Selected Answer: C\nIAM roles securely provide temporary AWS credentials that services (like SageMaker notebooks) can assume to access other resources. This avoids using long-lived access keys or directly embedding API keys into code.\nKMS Key Policy: This policy controls access to your KMS key. Granting the notebook's role permission within this policy lets SageMaker decrypt the data when reading from S3.","timestamp":"1724155500.0","poster":"VR10","upvote_count":"1","comment_id":"1154764"},{"content":"Selected Answer: C\nSeems to follow the best cloud authorization practice","poster":"endeesa","timestamp":"1716810000.0","upvote_count":"1","comment_id":"1081595"},{"content":"Selected Answer: C\nIAM role associated with the SageMaker notebook instance must be given permissions in the KMS key policy to decrypt the data using the KMS key that was used for encryption.","upvote_count":"1","poster":"sonoluminescence","timestamp":"1714423080.0","comment_id":"1057173"},{"content":"Selected Answer: C\nanswer is C","upvote_count":"1","poster":"AmeeraM","timestamp":"1713077880.0","comment_id":"1043169"},{"upvote_count":"1","poster":"Mickey321","content":"Selected Answer: C\nAssign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant permission in the KMS key policy to that role.\nTo read data from Amazon S3 that is encrypted with AWS KMS, the Amazon SageMaker notebook instance needs to have both S3 read access and KMS decrypt permissions. This can be achieved by assigning an IAM role to the notebook instance that has the necessary policies attached, and by granting permission in the KMS key policy to that role.","timestamp":"1709054880.0","comment_id":"991531"},{"comment_id":"938697","content":"C only.","timestamp":"1703907000.0","upvote_count":"1","poster":"ADVIT"},{"comment_id":"906176","timestamp":"1700872740.0","upvote_count":"1","content":"Selected Answer: C\nShould be C. The reference doc provided did not have any information about assigning keys to the notebook. Doing so become very cumbersome as you can have 100's of notebooks and its not scalable. Someone needs to moderate these answers.","poster":"earthMover"},{"timestamp":"1696521120.0","comment_id":"862254","poster":"oso0348","upvote_count":"1","content":"Selected Answer: C\nTo allow an Amazon SageMaker notebook instance to read a dataset stored in an Amazon S3 bucket that is protected with server-side encryption using AWS KMS, the ML Specialist should assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. The IAM role should have permissions to access the S3 bucket and the KMS key that was used to encrypt the data. This role should be granted permission in the KMS key policy to allow it to decrypt the data."},{"comments":[{"upvote_count":"3","comment_id":"832897","poster":"Nadia0012","timestamp":"1694165760.0","content":"I correct myself- Option C is correct: \nBackground\nAWS Key Management Service (AWS KMS) enables Server-side encryption to protect your data at rest. Amazon SageMaker training works with KMS encrypted data if the IAM role used for S3 access has permissions to encrypt and decrypt data with the KMS key. Further, a KMS key can also be used to encrypt the model artifacts at rest using Amazon S3 server-side encryption. Additionally, a KMS key can also be used to encrypt the storage volume attached to training, endpoint, and transform instances. In this notebook, we demonstrate SageMaker encryption capabilities using KMS-managed keys. \nresource: https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/handling_kms_encrypted_data/handling_kms_encrypted_data.ipynb\nOption D is correct if sagemaker does the encryption, if you are dealing with encrypted data then C is 100% correct."}],"upvote_count":"1","comment_id":"832891","content":"Selected Answer: D\nTo encrypt the machine learning (ML) storage volume that is attached to notebooks, processing jobs, training jobs, hyperparameter tuning jobs, batch transform jobs, and endpoints, you can pass a AWS KMS key to SageMaker. If you don't specify a KMS key, SageMaker encrypts storage volumes with a transient key and discards it immediately after encrypting the storage volume. For notebook instances, if you don't specify a KMS key, SageMaker encrypts both OS volumes and ML data volumes with a system-managed KMS key.","poster":"Nadia0012","timestamp":"1694165460.0"},{"comment_id":"804641","poster":"AjoseO","timestamp":"1691684640.0","upvote_count":"2","content":"Selected Answer: C\nC. Assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant permission in the KMS key policy to that role.\n\nTo access the encrypted dataset in Amazon S3, the Amazon SageMaker notebook instance must have the appropriate permissions. This can be achieved by assigning an IAM role to the notebook with read access to the dataset in Amazon S3 and granting permission in the KMS key policy to that role. This ensures that the notebook has the necessary permissions to access the encrypted data in Amazon S3, while adhering to best practices for securing sensitive data."},{"upvote_count":"3","content":"Selected Answer: C\nagreed with C","comment_id":"727817","timestamp":"1685128440.0","poster":"ystotest"},{"poster":"AmakamaxZanny","content":"Answer is C : Open the IAM console. Add a policy to the IAM user that grants the permissions to upload and download from the bucket. You can use a policy that's similar to the following:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-default-encryption/\n(number 2)","comment_id":"554078","upvote_count":"1","timestamp":"1661203380.0"},{"content":"Seems to be D \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/encryption-at-rest-nbi.html","timestamp":"1652258400.0","poster":"Deepsachin","upvote_count":"2","comment_id":"476140"},{"comment_id":"426817","upvote_count":"1","poster":"Madwyn","timestamp":"1650171060.0","content":"Not D as if you assign the key in the notebook, that's not secure, it will make the encryption ineffective. Instead, you assign the access permission by using IAM."},{"comment_id":"425661","upvote_count":"1","content":"Should be C based on these resources\nhttps://stackoverflow.com/questions/66692579/aws-sagemaker-permissionerror-access-denied-reading-data-from-s3-bucket\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-default-encryption/","timestamp":"1650167280.0","poster":"Nickname_L"},{"upvote_count":"1","timestamp":"1649706660.0","comment_id":"400491","poster":"Huy","content":"C. D is wrong because Encryption key lets you encrypt data on the ML storage volume attached to the notebook instance using an AWS Key Management Service (AWS KMS) key. If you plan to store sensitive information on the ML storage volume, consider encrypting the information."},{"timestamp":"1649273400.0","content":"should be c \nYou donâ€™t need to specify the AWS KMS key ID when you download an SSE-KMS-encrypted object from an S3 bucket. Instead, you need the permission to decrypt the AWS KMS key.\n\nWhen a user sends a GET request, Amazon S3 checks if the AWS Identity and Access Management (IAM) user or role that sent the request is authorized to decrypt the key associated with the object. If the IAM user or role belongs to the same AWS account as the key, then the permission to decrypt must be granted on the AWS KMS keyâ€™s policy.","poster":"mona_mansour","upvote_count":"3","comment_id":"355122"},{"poster":"ahquiceno","content":"C is correct go to: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html\nhttps://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role-sagemaker-notebook.html.","comment_id":"289511","timestamp":"1648483380.0","upvote_count":"4"},{"content":"D seems reasonable based on the explanation: https://docs.aws.amazon.com/sagemaker/latest/dg/encryption-at-rest.html","comment_id":"283116","upvote_count":"2","comments":[{"timestamp":"1648156320.0","upvote_count":"1","poster":"takahirokoyama","content":"I think so.","comment_id":"283430"}],"timestamp":"1647936240.0","poster":"[Removed]"}],"url":"https://www.examtopics.com/discussions/amazon/view/43708-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"C","answer_ET":"C","answer_images":[],"question_text":"A Machine Learning Specialist uploads a dataset to an Amazon S3 bucket protected with server-side encryption using AWS KMS.\nHow should the ML Specialist define the Amazon SageMaker notebook instance so it can read the same dataset from Amazon S3?","unix_timestamp":1612204680,"timestamp":"2021-02-01 19:38:00","choices":{"A":"Define security group(s) to allow all HTTP inbound/outbound traffic and assign those security group(s) to the Amazon SageMaker notebook instance.","D":"Assign the same KMS key used to encrypt data in Amazon S3 to the Amazon SageMaker notebook instance.","C":"Assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant permission in the KMS key policy to that role.","B":"×€Â¡onfigure the Amazon SageMaker notebook instance to have access to the VPC. Grant permission in the KMS key policy to the notebook's KMS role."},"answer_description":"","exam_id":26,"question_images":[],"topic":"1"},{"id":"XxRgisIkM6QOPftWeOmA","answer_images":[],"discussion":[{"upvote_count":"29","comment_id":"282152","poster":"Paul_NoName","content":"B it is .","comments":[{"content":"I agree, B is serverless and reuses Pyspark. Similar example shown here: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-samples-medicaid.html","upvote_count":"11","poster":"[Removed]","comment_id":"283119","timestamp":"1633311180.0"}],"timestamp":"1633124220.0"},{"poster":"SophieSu","upvote_count":"12","comment_id":"296986","content":"A is not correct because Minimize the number of servers that will need to be managed. EMR is not server-less.\nB is correct. AWS Glue supports an extension of the PySpark Python dialect for scripting extract, transform, and load...\nC is not correct because using Lambda for ETL you will not be able to Reuse existing PySpark logic\nD is not correct because Kinesis is not server-less. And you can not Reuse existing PySpark logic","timestamp":"1634339220.0"},{"timestamp":"1727465580.0","poster":"xicocaio","upvote_count":"1","content":"Selected Answer: B\nOption B (using AWS Glue for the ETL process) is the best solution for the described requirements.\n\nA: This solution requires managing an Amazon EMR cluster, which would involve more server management than AWS Glue, violating the requirement to minimize the number of servers to be managed.\n\nC: AWS Lambda is not ideal for this use case because it has resource limitations, including memory and execution time limits (15 minutes max), which might not be suitable for large-scale ETL operations involving PySpark logic.\n\nD: Amazon Kinesis Data Analytics is focused on real-time stream processing, which doesn't fit the described scheduled batch processing scenario.","comment_id":"1290309"},{"poster":"akgarg00","upvote_count":"2","timestamp":"1700305680.0","content":"Answer is A, as B clearly mentions that Pyspark code is written with leverage from already existing code. Also, the server architecture used currently is on-premises which will have more servers that solution A.","comment_id":"1073943"},{"content":"Selected Answer: B\nAmazon Kinesis Data Analytics is more suited for real-time processing and streaming data. The given use case does not indicate a need for real-time processing, so this might not be the best fit. Furthermore, it doesn't support PySpark natively.","comment_id":"1057179","poster":"sonoluminescence","timestamp":"1698619500.0","upvote_count":"1"},{"comment_id":"996241","timestamp":"1693584780.0","content":"Selected Answer: B\nVoted B based on the serverless (minimum servers) and https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming.html","poster":"Shenannigan","upvote_count":"1"},{"comment_id":"991533","content":"Selected Answer: B\nIndeed B using Glue","timestamp":"1693150260.0","upvote_count":"1","poster":"Mickey321"},{"content":"B is the correct. \nA you have to manage EMR, so it's wrong.\nD you don't use Spark, so it's wrong.\nC you will not be using Spark, so it's wrong.","poster":"kaike_reis","comment_id":"969410","timestamp":"1690923120.0","upvote_count":"1"},{"comment_id":"870502","timestamp":"1681509960.0","content":"Selected Answer: B\nB ticks all boxes. Minimize servers -> AWS managed services -> Glue.","poster":"Maaayaaa","upvote_count":"2"},{"poster":"bakarys","content":"Selected Answer: A\nOption A would be the best response for this scenario.\n\nThis solution allows the Data Scientist to reuse the existing PySpark logic while migrating the ETL process to the cloud. The raw data is written to Amazon S3, and a Lambda function is scheduled to trigger a Spark step on a persistent EMR cluster based on the existing schedule. The PySpark logic is used to run the ETL job on the EMR cluster, and the results are output to a processed location in Amazon S3 that is accessible for downstream use. This solution minimizes the number of servers that need to be managed, and it allows for a seamless migration of the existing ETL process to the cloud.","comment_id":"819030","upvote_count":"1","timestamp":"1677145320.0"},{"content":"Selected Answer: B\nOption D is wrong it should be B","poster":"sqavi","timestamp":"1676019300.0","comment_id":"804142","upvote_count":"1"},{"poster":"Peeking","comment_id":"739839","upvote_count":"2","timestamp":"1670566740.0","content":"D cannot be answer as there is no streaming data or Realtime processing."},{"upvote_count":"2","timestamp":"1660860240.0","content":"Selected Answer: B\nthe answer is b","poster":"salads","comment_id":"648622"},{"poster":"Nickname_L","content":"Answer should be B. Serverless, on a regular schedule (no real time requirement), reuses PySpark code in Glue ETL script.","comment_id":"425662","timestamp":"1635696960.0","upvote_count":"4"},{"timestamp":"1634781720.0","content":"Answer is B as they specifically ask about reusing existing PySpark, which can be done with Glue","comment_id":"336990","poster":"gcpwhiz","upvote_count":"3"},{"timestamp":"1634477160.0","poster":"Aashi22","upvote_count":"1","comment_id":"299593","content":"https://docs.aws.amazon.com/glue/latest/dg/creating_running_workflows.html"},{"timestamp":"1634184840.0","upvote_count":"2","comment_id":"291614","poster":"astonm13","content":"It is B. ! \"Minimize number of servers to be managed\". B is a Serverless solution which fulfils other requirements!"},{"upvote_count":"5","comment_id":"283974","poster":"cnethers","content":"I like both A & B however with B you would need to rewrite the Pyspark code to account for the ETL process you are now introducing, so it would not be using the original code. Both A & B are using managed services. Answer B would also require a notebook instance to get set up as there is no direct integration of Pysprak in Glue so there are some assumptions being made.\nOn Balance, I am leaning towards answer A","timestamp":"1634110380.0"},{"upvote_count":"2","timestamp":"1632541380.0","comment_id":"281450","content":"The answer should be A.","poster":"Joe_Zhang"}],"url":"https://www.examtopics.com/discussions/amazon/view/43716-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2021-02-01 22:36:00","exam_id":26,"answer":"B","choices":{"C":"Write the raw data to Amazon S3. Schedule an AWS Lambda function to run on the existing schedule and process the input data from Amazon S3. Write the Lambda logic in Python and implement the existing PySpark logic to perform the ETL process. Have the Lambda function output the results to a ×’â‚¬processed×’â‚¬ location in Amazon S3 that is accessible for downstream use.","D":"Use Amazon Kinesis Data Analytics to stream the input data and perform real-time SQL queries against the stream to carry out the required transformations within the stream. Deliver the output results to a ×’â‚¬processed×’â‚¬ location in Amazon S3 that is accessible for downstream use.","A":"Write the raw data to Amazon S3. Schedule an AWS Lambda function to submit a Spark step to a persistent Amazon EMR cluster based on the existing schedule. Use the existing PySpark logic to run the ETL job on the EMR cluster. Output the results to a ×’â‚¬processed×’â‚¬ location in Amazon S3 that is accessible for downstream use.","B":"Write the raw data to Amazon S3. Create an AWS Glue ETL job to perform the ETL processing against the input data. Write the ETL job in PySpark to leverage the existing logic. Create a new AWS Glue trigger to trigger the ETL job based on the existing schedule. Configure the output target of the ETL job to write to a ×’â‚¬processed×’â‚¬ location in Amazon S3 that is accessible for downstream use."},"question_id":339,"topic":"1","answers_community":["B (90%)","10%"],"question_images":[],"question_text":"A Data Scientist needs to migrate an existing on-premises ETL process to the cloud. The current process runs at regular time intervals and uses PySpark to combine and format multiple large data sources into a single consolidated output for downstream processing.\nThe Data Scientist has been given the following requirements to the cloud solution:\nâœ‘ Combine multiple data sources.\nâœ‘ Reuse existing PySpark logic.\nâœ‘ Run the solution on the existing schedule.\nâœ‘ Minimize the number of servers that will need to be managed.\nWhich architecture should the Data Scientist use to build this solution?","isMC":true,"unix_timestamp":1612215360,"answer_description":"","answer_ET":"B"},{"id":"CsLsWUBBGfAYCijHty1X","answer_images":[],"discussion":[{"upvote_count":"13","timestamp":"1682875080.0","content":"AC - correct answer","poster":"bluer1","comment_id":"595173"},{"comment_id":"1091470","poster":"lynn22","content":"Selected Answer: AE\nI think ACE are all correct","upvote_count":"1","timestamp":"1733723100.0"},{"comment_id":"1007412","timestamp":"1726305600.0","upvote_count":"4","content":"Selected Answer: AC\nA. YES - standard for overfitting\nB. NO - we have already too much overfitting\nC. YES - feature elimination can reduce model complexity and thus overfitting\nD. NO - that does dimensionnality reduction to 2D or 3D, for visualization; we want more than a few features\nE. NO - LDA is an alternative to logistic regression; it may not address overfitting","poster":"loict"},{"timestamp":"1724772900.0","upvote_count":"1","comment_id":"991536","content":"Selected Answer: AC\nA due to fitting\nC Recursive feature elimination (RFE) is a wrapper method that iteratively removes features based on their importance scores from a classifier. RFE starts with all features and then eliminates the least important ones until a desired number of features is reached. This can help to reduce the dimensionality of the dataset and improve the model performance by removing irrelevant or redundant features. The Marketing team can then interpret the model by looking at the remaining features and their importance scores.","poster":"Mickey321"},{"content":"Selected Answer: AC\nAC are the correct","timestamp":"1722545340.0","upvote_count":"1","comment_id":"969408","poster":"kaike_reis"},{"poster":"earthMover","timestamp":"1716591780.0","upvote_count":"1","content":"Selected Answer: AC\nHow can we add features to the dataset provided.... we can't make them up from thin air. Hopefully the moderators can provide some insight on this. I was thinking of paying for this site but the answers are all over the place.","comment_id":"906188"},{"poster":"bakarys","content":"Selected Answer: AC\nA. Add L1 regularization to the classifier and C. Perform recursive feature elimination are the methods that can be used to improve the model performance and satisfy the Marketing team's needs.\n\nExplanation:\nA. Adding L1 regularization to the logistic regression classifier can help to improve the model performance and reduce overfitting. This can also help to highlight the relevant features for churn prediction as L1 regularization can shrink the coefficients of irrelevant features to zero.\n\n\nC. Recursive feature elimination can be used to select the most relevant features for the model. This can help to improve the model performance and highlight the relevant features for churn prediction.","timestamp":"1708681620.0","comment_id":"819036","upvote_count":"3"},{"content":"Selected Answer: AC\nA. Adding L1 regularization can help to reduce overfitting by shrinking the coefficients of less important features towards zero, which can improve the model's generalization performance on the validation set.\n\nC. Recursive feature elimination is a feature selection technique that removes the least important feature at each iteration and trains the model on the remaining features until a desired number of features is reached. This method can be used to identify the most relevant features for the prediction task and reduce the dimensionality of the dataset, leading to improved model performance and interpretability for the Marketing team.","upvote_count":"2","poster":"AjoseO","comment_id":"804697","timestamp":"1707594660.0"},{"poster":"wisoxe8356","content":"AC - \nKey: logistic regression model = non linear in terms of Odds and Probability, however it is linear in terms of Log Odds.\nKey: Large gap between training & validation = overfitting\n=> 5 techniques to prevent overfitting:\n1. Simplifying the model | 2. Early stopping\n3. Use data argumentation | 4. Use regularization | 5. Use dropouts\n\nA - yes to avoid overfitting (although i am thinking it is talking about regressor)\nNot B - add feature will lead to overfitting\nC - feature elimination - prevent overfitting\nNot D - t-SNE is a nonlinear dimensionality reduction technique\nNot E - find feature correlation only - Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events.","upvote_count":"4","comment_id":"735145","timestamp":"1701700320.0"},{"content":"L1 won't do naturally the feature elimination?\nI guess AB","timestamp":"1700576220.0","poster":"itallomd","comment_id":"723587","upvote_count":"1"},{"timestamp":"1693562040.0","content":"why not A & D? or C & D? \ndoes not t-SNE grant the marketing team's wish for visualization of relationships? or are we to presume that A&C are best as C (recursive feature elimination) grants us some visualization of feature importance.","poster":"Atreides457","comment_id":"656095","upvote_count":"2"},{"poster":"tgaos","timestamp":"1686208440.0","comment_id":"613127","content":"Selected Answer: AC\nAC is correct","upvote_count":"3"},{"upvote_count":"4","timestamp":"1686166560.0","content":"Selected Answer: AC\noverfitting: add regularization, remove features","comment_id":"612905","poster":"NeverMinda"}],"url":"https://www.examtopics.com/discussions/amazon/view/74983-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2022-04-30 19:18:00","exam_id":26,"answer":"AC","question_id":340,"choices":{"C":"Perform recursive feature elimination","A":"Add L1 regularization to the classifier","E":"Perform linear discriminant analysis","D":"Perform t-distributed stochastic neighbor embedding (t-SNE)","B":"Add features to the dataset"},"topic":"1","answers_community":["AC (95%)","5%"],"question_images":[],"isMC":true,"question_text":"A Data Scientist is building a model to predict customer churn using a dataset of 100 continuous numerical features. The Marketing team has not provided any insight about which features are relevant for churn prediction. The Marketing team wants to interpret the model and see the direct impact of relevant features on the model outcome. While training a logistic regression model, the Data Scientist observes that there is a wide gap between the training and validation set accuracy.\nWhich methods can the Data Scientist use to improve the model performance and satisfy the Marketing team's needs? (Choose two.)","unix_timestamp":1651339080,"answer_description":"","answer_ET":"AC"}],"exam":{"isBeta":false,"isMCOnly":false,"id":26,"numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty","isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025"},"currentPage":68},"__N_SSP":true}