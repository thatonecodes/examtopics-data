{"pageProps":{"questions":[{"id":"3oVzXZ0ITayIi6t7cWxE","choices":{"B":"Configure the application to prompt the user to provide the password to the Lambda function on the first run.","C":"Store the API key as a value in the application code.","D":"Use Lambda@Edge and only communicate over the HTTPS protocol.","A":"Store the API key as a Lambda environment variable by using an AWS Key Management Service (AWS KMS) customer managed key."},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/122624-exam-aws-certified-developer-associate-dva-c02-topic-1/","unix_timestamp":1696554360,"answers_community":["A (100%)"],"answer_ET":"A","exam_id":24,"timestamp":"2023-10-06 03:06:00","question_id":116,"answer_description":"","isMC":true,"answer":"A","discussion":[{"upvote_count":"8","poster":"Digo30sp","content":"Selected Answer: A\nThe correct answer is (A).\n\nStoring the API key as a Lambda environment variable using an AWS Key Management Service (AWS KMS) customer-managed key is the most secure solution. AWS KMS is a managed encryption service that provides customer-managed keys. Customer-managed keys are encrypted with an AWS KMS master key, which is stored in an AWS KMS vault.","timestamp":"1712419080.0","comment_id":"1026781"},{"comment_id":"1216718","content":"Selected Answer: A\nA is the correct answer.","timestamp":"1732385040.0","upvote_count":"1","poster":"65703c1"},{"timestamp":"1721664180.0","content":"Selected Answer: A\nLambda environment variables can be encrypted using a customer managed key in AWS KMS. This approach ensures that the API key is encrypted at rest and seamlessly integrated into the Lambda function. When the function is executed, it can access the decrypted value of the API key for authenticating with the third-party system.","comment_id":"1128918","poster":"SerialiDr","upvote_count":"3"},{"content":"AAAAAAAAAA","timestamp":"1712365560.0","poster":"fordiscussionstwo","upvote_count":"2","comment_id":"1026118"}],"question_images":[],"topic":"1","question_text":"A developer must provide an API key to an AWS Lambda function to authenticate with a third-party system. The Lambda function will run on a schedule. The developer needs to ensure that the API key remains encrypted at rest.\n\nWhich solution will meet these requirements?"},{"id":"dMra6B9YRj7kdhYt2foV","answers_community":["A (50%)","C (25%)","B (25%)"],"question_text":"An IT department uses Amazon S3 to store sensitive images. After more than 1 year, the company moves the images into archival storage. The company rarely accesses the images, but the company wants a storage solution that maximizes resiliency. The IT department needs access to the images that have been moved to archival storage within 24 hours.\n\nWhich solution will meet these requirements MOST cost-effectively?","topic":"1","answer_images":[],"unix_timestamp":1696554360,"exam_id":24,"question_id":117,"answer":"A","timestamp":"2023-10-06 03:06:00","choices":{"D":"Use S3 One Zone-Infrequent Access (S3 One Zone-IA) to store the images. Use S3 Glacier Deep Archive with bulk retrieval to store and retrieve archived images.","C":"Use S3 Intelligent-Tiering to store the images. Use S3 Glacier Deep Archive with standard retrieval to store and retrieve archived images.","A":"Use S3 Standard-Infrequent Access (S3 Standard-IA) to store the images. Use S3 Glacier Deep Archive with standard retrieval to store and retrieve archived images.","B":"Use S3 Standard-Infrequent Access (S3 Standard-IA) to store the images. Use S3 Glacier Deep Archive with bulk retrieval to store and retrieve archived images."},"discussion":[{"timestamp":"1697382240.0","content":"A is correct. The requirement of maximizing resiliency rules out One Zone. Standard recover is within 12 hours, which fits the requirement of within 24 hours. https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects-retrieval-options.html","poster":"Learning4life","upvote_count":"8","comment_id":"1044252"},{"content":"Selected Answer: A\nI had incorrectly selected B but the right option is A. the main reason is the retrieval time in the standard way is within 12 hours which meets the requirement","comment_id":"1324852","timestamp":"1733890440.0","poster":"f271c23","upvote_count":"1"},{"timestamp":"1728370200.0","upvote_count":"1","poster":"MasoudK","comment_id":"1294607","content":"Option A is not most Cost effective the standard approach maximize resiliency but is more expensive than Option B. Option B handles both goals."},{"timestamp":"1727501460.0","comment_id":"1290516","poster":"albert_kuo","comments":[{"timestamp":"1728526920.0","comment_id":"1295368","upvote_count":"1","content":"bulk retrievals take 48 hours\nstandard retrievals take 24hours\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects-retrieval-options.html\nRefer the table 2 row","poster":"lambdaFun"}],"content":"Selected Answer: B\nThe average retrieval time for bulk retrievals typically ranges from 5 to 12 hours, but it can take up to 48 hours. This method is suitable for cold storage data, making it a highly cost-effective option when retrieval frequency is low, and there are no stringent requirements on retrieval time.","upvote_count":"2"},{"upvote_count":"2","timestamp":"1723643280.0","content":"Selected Answer: B\nMOST cost-effective solution is:\n\nB. Use S3 Standard-Infrequent Access (S3 Standard-IA) to store the images. Use S3 Glacier Deep Archive with bulk retrieval to store and retrieve archived images.\n\nHereâ€™s why:\n\nS3 Standard-Infrequent Access (S3 Standard-IA):\nProvides a balance between cost and retrieval speed.\nSuitable for long-lived, less frequently accessed data.\nAccessible within hours.","poster":"Saurabh04","comment_id":"1265802"},{"timestamp":"1721538780.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/glacier-storage-classes.html#archival-storage","comment_id":"1252235","poster":"Anandesh","upvote_count":"1"},{"content":"Selected Answer: A\nA is the correct answer.","timestamp":"1716482280.0","upvote_count":"1","comment_id":"1216748","poster":"65703c1"},{"comments":[{"comment_id":"1182290","poster":"DeaconStJohn","upvote_count":"1","timestamp":"1711352940.0","content":"Initially I thought C also. However, lifecycle policies seem to be better for this use case. S3-IT will start at standard pricing, after 30 days > IA, after 90 days > archive instant retrieval. None of these are the most cost effective. S3-IT works well for use cases were there is no defined policy in place, i.e. after 1 year move to archive. reqs state archive after 365 days. s3-IT will action this after 90 days depending on access patterns."}],"comment_id":"1164084","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects-retrieval-options.html#:~:text=Deep%20Archive%20or-,S3%20Intelligent%2DTiering%20Deep%20Archive%20Access,-Not%20available","poster":"KarBiswa","timestamp":"1709379300.0","upvote_count":"1"},{"comment_id":"1128923","upvote_count":"3","content":"Selected Answer: A\nS3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. It offers a lower storage cost while still providing high durability, availability, and performance.\nS3 Glacier Deep Archive is the most cost-effective option for archival storage in AWS and is designed for data that is accessed very rarely. The standard retrieval option in Glacier Deep Archive typically returns data within 12 hours, meeting the requirement of access within 24 hours.","poster":"SerialiDr","timestamp":"1705946940.0"},{"content":"ChatGPT goes with B","comment_id":"1128534","comments":[{"content":"As a society we need to learn to challenge AI models.\n\nhttps://aws.amazon.com/s3/faqs/#Amazon_S3_Glacier_Deep_Archive\nWhen restoring an archived object, you can specify one of the following options in the Tier element of the request body: Standard is the default tier and lets you access any of your archived objects within 12 hours, with retrievals typically starting within 9 hours when initiated using S3 Batch Operations. Bulk lets you retrieve large amounts of data, even petabytes of data, inexpensively and typically completes within 48 hours.","poster":"DeaconStJohn","timestamp":"1711352220.0","comment_id":"1182282","upvote_count":"2"}],"poster":"_YaWeb","upvote_count":"2","timestamp":"1705921080.0"},{"comment_id":"1122602","poster":"dostonbekabdullaev","content":"Selected Answer: A\nAAAAAAAAA","upvote_count":"1","timestamp":"1705242840.0"},{"comment_id":"1098080","poster":"Certified101","timestamp":"1702723500.0","upvote_count":"1","content":"Selected Answer: A\nA is correct -Bulk retrival is 48hours"},{"poster":"TanTran04","content":"Selected Answer: B\nWith Option A: Standard retrieval would provide faster access to the archived images (typically within 3-5 hours), it is more expensive than Bulk retrieval. Since the company has indicated they can wait up to 24 hours for access, the slower but cheaper\n\n=> Option B is the best choice.","timestamp":"1702544940.0","upvote_count":"1","comment_id":"1096260"},{"poster":"Hanny","content":"Selected Answer: C\nC. Use S3 Intelligent-Tiering to store the images. Use S3 Glacier Deep Archive with standard retrieval to store and retrieve archived images.","timestamp":"1702388100.0","upvote_count":"2","comment_id":"1094568"},{"timestamp":"1701764580.0","poster":"tqiu654","content":"Selected Answer: B\nGPT: B. Use S3 Standard-Infrequent Access (S3 Standard-IA) to store the images. Use S3 Glacier Deep Archive and select Batch Retrieval to store and retrieve archived images.","upvote_count":"2","comment_id":"1088275"},{"upvote_count":"4","comment_id":"1053578","comments":[{"poster":"ut18","upvote_count":"1","timestamp":"1698845340.0","content":"Check the requirement : \n The IT department needs access to the images that have been moved to archival storage within 24 hours.","comment_id":"1059730"}],"poster":"hcsaba1982","timestamp":"1698223200.0","content":"Selected Answer: C\nA : Glacier Deep Archive is cheaper than Standard-IA. \nC : Standard archival is 12h.\nB : bulk retrieval is 48h\nD : S3 One Zone-IA - cross-out due to \"maximizes resiliency\""},{"content":"Selected Answer: A\nIt is A","poster":"Cerakoted","comment_id":"1040667","upvote_count":"2","timestamp":"1697029260.0"},{"timestamp":"1696598280.0","content":"Selected Answer: A\nA) Correct A) because the standard recovery is carried out within 12 hours and the requirement says that it must be recovered within 24 hours.\nBulk recovery takes up to 48 hours","upvote_count":"4","poster":"Digo30sp","comment_id":"1026653"},{"upvote_count":"2","content":"BBBBBBBBBB","timestamp":"1696554360.0","comment_id":"1026119","poster":"fordiscussionstwo"}],"answer_ET":"A","answer_description":"","isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/122625-exam-aws-certified-developer-associate-dva-c02-topic-1/"},{"id":"stCzWu1euIqVDC6mwV4b","timestamp":"2023-10-06 03:07:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/122626-exam-aws-certified-developer-associate-dva-c02-topic-1/","exam_id":24,"choices":{"D":"Use the existing AWS SAM template. Add additional parameters to configure specific attributes for the serverless function and database table resources that are in each environment. Deploy updates to the testing and staging environments by using the sam deploy command.","A":"Add a configuration file in TOML format to group configuration entries to every environment. Add a table for each testing and staging environment. Deploy updates to the environments by using the sam deploy command and the --config-env flag that corresponds to each environment.","B":"Create additional AWS SAM templates for each testing and staging environment. Write a custom shell script that uses the sam deploy command and the --template-file flag to deploy updates to the environments.","C":"Create one AWS SAM configuration file that has default parameters. Perform updates to the testing and staging environments by using the --parameter-overrides flag in the AWS SAM CLI and the parameters that the updates will override."},"discussion":[{"comment_id":"1129401","content":"Selected Answer: A\nTo set up deployments to multiple environments with the least development effort in a serverless application using the AWS Serverless Application Model (AWS SAM), the developer can utilize a configuration file in TOML format with grouped configuration entries for each environment. This approach allows for easy management of different environment configurations and streamlines the deployment process. The specific steps would include:\n\nCreating a configuration file in TOML format: This file will include a table for each testing and staging environment, where each table contains the specific configuration for that environment.\nUsing the sam deploy command with the --config-env flag: This flag allows specifying which environment configuration to use for the deployment, corresponding to the tables defined in the configuration file.\nThis solution aligns with Option A:","poster":"SerialiDr","timestamp":"1706003700.0","upvote_count":"9"},{"upvote_count":"8","timestamp":"1697285520.0","comment_id":"1043409","content":"Selected Answer: A\nA should be correct\nreference this stackoverflow post https://stackoverflow.com/questions/68826108/how-to-deploy-to-different-environments-with-aws-sam","poster":"Jing2023"},{"content":"Selected Answer: A\nsam deploy --config-env default\nsam deploy --config-env testing\nsam deploy --config-env production","poster":"albert_kuo","timestamp":"1727502840.0","comment_id":"1290518","upvote_count":"1"},{"poster":"KennethNg923","upvote_count":"1","timestamp":"1724298000.0","comment_id":"1270474","content":"Selected Answer: A\nAWS SAM supports configuration files in TOML format, which allows you to define multiple environments in a single file."},{"upvote_count":"1","poster":"Anandesh","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-config.html#serverless-sam-cli-config-default","timestamp":"1720496640.0","comment_id":"1244662"},{"timestamp":"1716482460.0","content":"Selected Answer: C\nC is the correct answer.","comment_id":"1216750","upvote_count":"1","poster":"65703c1"},{"upvote_count":"2","content":"Selected Answer: C\nC. Create one AWS SAM configuration file that has default parameters. Perform updates to the testing and staging environments by using the --parameter-overrides flag in the AWS SAM CLI and the parameters that the updates will override.","comment_id":"1177064","timestamp":"1710834660.0","poster":"41eb566"},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-config.html","timestamp":"1709379840.0","upvote_count":"2","comment_id":"1164088","poster":"KarBiswa"},{"comment_id":"1098083","poster":"Certified101","upvote_count":"1","content":"Selected Answer: C\nC with least development overhead","timestamp":"1702723680.0"},{"comment_id":"1094597","poster":"TanTran04","upvote_count":"3","timestamp":"1702390140.0","content":"Selected Answer: C\nWith at LEAST development effort, Option C is better than A\n\nWhile this approach may work, it introduces additional complexity with the need for a separate configuration file, and it may not be as straightforward as using parameter overrides, as suggested in option C. The use of TOML format might be more suited for certain scenarios, but in the context of AWS SAM, which commonly relies on YAML or JSON configurations, it might be an extra layer of complexity that isn't necessary.\n\nOption C, on the other hand, recommends using a single AWS SAM configuration file with default parameters and updating testing and staging environments using the --parameter-overrides flag. This approach is more aligned with typical AWS SAM practices and is simpler and more straightforward than managing multiple configuration files."},{"content":"With at LEAST development effort, Option C is better than A\n\nWhile this approach may work, it introduces additional complexity with the need for a separate configuration file, and it may not be as straightforward as using parameter overrides, as suggested in option C. The use of TOML format might be more suited for certain scenarios, but in the context of AWS SAM, which commonly relies on YAML or JSON configurations, it might be an extra layer of complexity that isn't necessary.\n\nOption C, on the other hand, recommends using a single AWS SAM configuration file with default parameters and updating testing and staging environments using the --parameter-overrides flag. This approach is more aligned with typical AWS SAM practices and is simpler and more straightforward than managing multiple configuration files.","timestamp":"1702390080.0","comment_id":"1094596","poster":"TanTran04","upvote_count":"1"},{"timestamp":"1702388160.0","comment_id":"1094571","content":"C. Create one AWS SAM configuration file that has default parameters. Perform updates to the testing and staging environments by using the --parameter-overrides flag in the AWS SAM CLI and the parameters that the updates will override.","poster":"Hanny","upvote_count":"2"},{"poster":"NinjaCloud","content":"Correct Answer: C, \nYou can create a single AWS SAM configuration file with default parameters and then use the --parameter-overrides flag with the AWS SAM CLI to specify parameters that override the defaults for each testing and staging environment. This approach keeps the AWS SAM template file (the infrastructure-as-code) consistent and minimizes duplication. It's a clean and simple way to manage multiple environments without having to create separate templates or custom scripts.","comment_id":"1059960","upvote_count":"8","timestamp":"1698860700.0"},{"comment_id":"1049073","upvote_count":"2","content":"Selected Answer: C\nHere all the options can do the Job but option C does it with least effort.","poster":"Rameez1","timestamp":"1697830380.0"},{"poster":"PrakashM14","upvote_count":"2","timestamp":"1697796060.0","content":"Selected Answer: C\nOptions A and B introduce additional complexities such as configuration files in TOML format or writing custom shell scripts. These might require more effort and maintenance.\n\nOption D involves adding additional parameters to the existing AWS SAM template, which can work but may lead to a more complex and less maintainable template as the number of environments grows.\n\nTherefore, option C is a straightforward and efficient solution for deploying to multiple environments with AWS SAM.","comment_id":"1048622"},{"content":"Selected Answer: A\nA is correct","poster":"dilleman","comment_id":"1040494","timestamp":"1697019360.0","upvote_count":"4"},{"timestamp":"1696598520.0","comment_id":"1026660","upvote_count":"3","content":"Selected Answer: D\nThe correct answer is (D).\n\nUsing the existing AWS SAM template is the option that requires the LEAST development effort. To configure deployments across multiple environments, you can add additional parameters to your AWS SAM template to configure specific attributes for the serverless function and database table resources that are in each environment.","poster":"Digo30sp"},{"timestamp":"1696554420.0","poster":"fordiscussionstwo","comment_id":"1026121","upvote_count":"3","content":"AAAAAAAAAA"}],"unix_timestamp":1696554420,"answer":"A","question_text":"A developer is building a serverless application by using the AWS Serverless Application Model (AWS SAM). The developer is currently testing the application in a development environment. When the application is nearly finished, the developer will need to set up additional testing and staging environments for a quality assurance team.\n\nThe developer wants to use a feature of the AWS SAM to set up deployments to multiple environments.\n\nWhich solution will meet these requirements with the LEAST development effort?","isMC":true,"question_id":118,"answer_ET":"A","answer_images":[],"topic":"1","answers_community":["A (63%)","C (30%)","8%"],"answer_description":""},{"id":"k7kVufSUDFYxLTwfyoE1","exam_id":24,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/122627-exam-aws-certified-developer-associate-dva-c02-topic-1/","timestamp":"2023-10-06 03:09:00","topic":"1","choices":{"C":"Add a trigger to the Lambda function. Select the S3 bucket as the source.","B":"Add an Amazon EventBridge event to the Lambda function. Select the S3 bucket as the source.","A":"Add an asynchronous invocation to the Lambda function. Select the S3 bucket as the source.","D":"Add a layer to the Lambda function. Select the S3 bucket as the source."},"question_id":119,"isMC":true,"answer":"C","discussion":[{"comment_id":"1026661","upvote_count":"6","content":"Selected Answer: C\nThe correct answer is (C).\n\nAdding a trigger to your Lambda function is the solution that will meet these requirements. A trigger is an event that can invoke a Lambda function. In the case of this issue, the trigger must be an Amazon S3 event that fires when a new file is uploaded to the bucket.","poster":"Digo30sp","timestamp":"1696598580.0"},{"content":"Selected Answer: B\nOption B is quicker than Option C, because S3 bucket trigger does not guarantee immediate invocation. It relies on event notification from S3.","poster":"Saurabh04","timestamp":"1723644120.0","upvote_count":"1","comment_id":"1265810"},{"timestamp":"1721955360.0","poster":"BrainFried","content":"Selected Answer: B\nYou cannot add a Trigger directly to Lambda. If you want to choose C, then the answer should state: \"Add a trigger to S3, select Lambda as the destination\"\n\nSince C states \"Add trigger to Lambda\" (which isn't possible), I will select answer B.\n\nSee https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html\n\"The trigger is actually stored and managed by the service that generates the events, not by Lambda.\"","comment_id":"1255315","upvote_count":"1"},{"upvote_count":"2","poster":"Anandesh","timestamp":"1721539140.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html","comment_id":"1252237"},{"content":"Selected Answer: C\nC is the correct answer.","upvote_count":"1","comment_id":"1216754","poster":"65703c1","timestamp":"1716482700.0"},{"comments":[{"comments":[{"poster":"BrainFried","upvote_count":"1","timestamp":"1721955120.0","content":"From what I've read, you do not add a trigger to a lambda, you add it else-where (in this case, you add the trigger to S3). The answer says \"Add a trigger to Lambda\" - this isn't possible!\n\nRead: https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html\n\"The trigger is actually stored and managed by the service that generates the events, not by Lambda.\"\n\nThe answer should be B then, since EventBridge can monitor S3 bucket and invoke Lambda with the new data.","comment_id":"1255313"}],"poster":"tsangckl","timestamp":"1718024520.0","comment_id":"1227876","upvote_count":"1","content":"https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html"}],"timestamp":"1714459200.0","comment_id":"1204375","poster":"1dfed2b","content":"Selected Answer: B\nsure that B, give me a link why everyone want C.","upvote_count":"1"},{"content":"Selected Answer: C\nTo meet the requirement of processing data files immediately after they are uploaded to an Amazon S3 bucket, the best solution is to add a trigger to the AWS Lambda function with the S3 bucket as the source. This will configure the Lambda function to be automatically invoked when a new file is uploaded to the specified S3 bucket.","timestamp":"1706003940.0","upvote_count":"3","comment_id":"1129407","poster":"SerialiDr"},{"upvote_count":"1","poster":"Certified101","timestamp":"1702723860.0","content":"Selected Answer: C\nC using S3 Events, no need for EventBridge here.","comment_id":"1098090"},{"timestamp":"1702222740.0","comments":[{"content":"You can use Amazon EventBridge to monitor an S3 bucket for new image uploads. When a new image is detected, EventBridge triggers a Lambda function that processes the image, applies filters, and generates thumbnails, all without manual intervention","poster":"LR2023","comment_id":"1092605","upvote_count":"1","timestamp":"1702222800.0"}],"upvote_count":"1","poster":"LR2023","comment_id":"1092603","content":"Selected Answer: B\nEventBridge can be employed to collect real-time data streams from various sources like IoT devices, mobile apps, or web applications. Lambda functions can then process this data to perform analytics, generate alerts, or update dashboards."},{"upvote_count":"3","comment_id":"1040497","poster":"dilleman","timestamp":"1697019540.0","content":"Selected Answer: C\nC is correct"},{"upvote_count":"4","comment_id":"1026124","content":"CCCCCCCCCCCCCC","poster":"fordiscussionstwo","timestamp":"1696554540.0"}],"answer_ET":"C","answer_description":"","answer_images":[],"question_text":"A developer is working on an application that processes operating data from IoT devices. Each IoT device uploads a data file once every hour to an Amazon S3 bucket. The developer wants to immediately process each data file when the data file is uploaded to Amazon S3.\n\nThe developer will use an AWS Lambda function to process the data files from Amazon S3. The Lambda function is configured with the S3 bucket information where the files are uploaded. The developer wants to configure the Lambda function to immediately invoke after each data file is uploaded.\n\nWhich solution will meet these requirements?","unix_timestamp":1696554540,"answers_community":["C (80%)","B (20%)"]},{"id":"3aS9H3ybAD4oF3z0AgZm","exam_id":24,"choices":{"A":"Add an --enable-termination-protection command line option to the create-stack command and the update-stack command.","C":"Add a --parameters ParameterKey=PreserveResources,ParameterValue=True command line option to the create-stack command and the update-stack command.","D":"Add a --tags Key=PreserveResources,Value=True command line option to the create-stack command and the update-stack command.","B":"Add a --disable-rollback command line option to the create-stack command and the update-stack command."},"question_images":[],"discussion":[{"timestamp":"1696598640.0","content":"Selected Answer: B\nThe correct answer is (B).\n\nThe --disable-rollback command-line option will prevent CloudFormation from rolling back the stack to the previous state if an error occurs. This will ensure that successfully provisioned resources are preserved.","comment_id":"1026662","upvote_count":"7","poster":"Digo30sp"},{"content":"Selected Answer: B\naws cloudformation create-stack \\\n --stack-name my-app-stack \\\n --template-body file://my-template.yaml \\\n --parameters ParameterKey=InstanceType,ParameterValue=t2.micro \\\n --disable-rollback\n\n\naws cloudformation update-stack \\\n --stack-name my-app-stack \\\n --template-body file://my-template-updated.yaml \\\n --parameters ParameterKey=InstanceType,ParameterValue=t2.medium \\\n --disable-rollback","comment_id":"1290524","timestamp":"1727503320.0","poster":"albert_kuo","upvote_count":"1"},{"timestamp":"1716482820.0","content":"Selected Answer: B\nB is the correct answer.","poster":"65703c1","comment_id":"1216757","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stack-failure-options.html","timestamp":"1709446800.0","upvote_count":"1","poster":"KarBiswa","comment_id":"1164531"},{"poster":"joshnort","content":"Selected Answer: B\nIt should look like this:\n\naws cloudformation create-stack --stack-name myteststack --template-body file://DOC-EXAMPLE-BUCKET.json --disable-rollback\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stack-failure-options.html#stack-failure-options-cli","comment_id":"1139695","upvote_count":"3","timestamp":"1707008100.0"},{"comment_id":"1080630","content":"Selected Answer: B\n\"Specify the disable-rollback option or on-failure DO_NOTHING enumeration during a create-stack operation\"\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stack-failure-options.html","poster":"kaes","timestamp":"1701001620.0","upvote_count":"3"},{"poster":"dilleman","upvote_count":"3","content":"Selected Answer: B\nB is correct","comment_id":"1040506","timestamp":"1697019840.0"},{"content":"Selected Answer: B\nhttps://www.cloudhesive.com/blog-posts/cloudformation-disable-rollback/","upvote_count":"4","comment_id":"1039852","timestamp":"1696969320.0","poster":"kashtelyan"},{"timestamp":"1696554600.0","poster":"fordiscussionstwo","upvote_count":"3","comment_id":"1026125","content":"BBBBBBBBBBBBBBBBB"}],"unix_timestamp":1696554600,"answers_community":["B (100%)"],"topic":"1","answer_ET":"B","answer_description":"","isMC":true,"question_text":"A developer is setting up infrastructure by using AWS CloudFormation. If an error occurs when the resources described in the Cloud Formation template are provisioned, successfully provisioned resources must be preserved. The developer must provision and update the CloudFormation stack by using the AWS CLI.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/122628-exam-aws-certified-developer-associate-dva-c02-topic-1/","answer_images":[],"timestamp":"2023-10-06 03:10:00","question_id":120,"answer":"B"}],"exam":{"numberOfQuestions":551,"isImplemented":true,"lastUpdated":"11 Apr 2025","provider":"Amazon","id":24,"isBeta":false,"isMCOnly":true,"name":"AWS Certified Developer - Associate DVA-C02"},"currentPage":24},"__N_SSP":true}