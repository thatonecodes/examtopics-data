{"pageProps":{"questions":[{"id":"IpYS6HXAXqr3ccrhoy4I","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/110331-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"exam_id":33,"answer_ET":"B","isMC":true,"question_id":116,"answer_description":"","discussion":[{"upvote_count":"7","comment_id":"1101197","timestamp":"1703045160.0","content":"Selected Answer: B\nBad question design. EC2 is in ASG, which means the application part is stateless, so no need to backup or replicate. Only database need replication.","poster":"bjexamprep"},{"content":"Selected Answer: B\nExplanation:\nOption B leverages infrastructure as code (IaC) to provision the necessary infrastructure in the DR Region, which allows for automated and repeatable deployments.\nCreating a cross-Region read replica for the Amazon RDS DB instance ensures that the database is replicated and available in the DR Region.\nAWS Elastic Disaster Recovery can be used to continuously replicate the EC2 instances from the primary Region to the DR Region, ensuring up-to-date copies of the application.\nRunning the EC2 instances at the minimum capacity in the DR Region helps reduce costs, as resources are only utilized when failover occurs.\nUsing an Amazon Route 53 failover routing policy allows for automatic failover to the DR Region in the event of a disaster, minimizing downtime.\nIncreasing the desired capacity of the Auto Scaling group ensures that sufficient resources are available in the DR Region to handle the workload during failover.","timestamp":"1686565380.0","poster":"Jonalb","upvote_count":"5","comment_id":"921334"},{"poster":"FZA24","upvote_count":"1","content":"Selected Answer: B\nRPO seconds, RTO minutes => warm standby \nwarm standby => always running but smaller\nalways running but smaller => B. Run the EC2 instances at the minimum capacity in the DR Region","timestamp":"1731601920.0","comment_id":"1312159"},{"poster":"career360guru","content":"Selected Answer: B\nOption B most cost effective for RTO=10 min and RPO=30 min.","upvote_count":"3","timestamp":"1700720280.0","comment_id":"1078120","comments":[{"comment_id":"1078121","timestamp":"1700720340.0","upvote_count":"2","content":"RPO=30 sec","poster":"career360guru"}]},{"timestamp":"1700574660.0","content":"Selected Answer: B\nRPO of 30 seconds can be achieved with Elastic disaster recovery for continuous EC2 instance replication, while DB read replica can be promoted to primary within 30 seconds","poster":"Pupu86","comment_id":"1076328","upvote_count":"3"},{"upvote_count":"3","comment_id":"986028","content":"Selected Answer: B\nClose between B & D but Max out ASG is tie-breaker","timestamp":"1692559620.0","poster":"SK_Tyagi"},{"content":"Selected Answer: D\nI think (D) only aurora global database can meet RPO 30 seconds? although B is cost-effective","poster":"softarts","upvote_count":"2","timestamp":"1691454540.0","comment_id":"975080"},{"timestamp":"1688596140.0","content":"Selected Answer: B\nB for sure","upvote_count":"1","comment_id":"944149","poster":"NikkyDicky"},{"comment_id":"935963","timestamp":"1687915800.0","content":"Selected Answer: B\nA) Not seems for my , posible backup\nB) Active Pasive\nC) Backup \nD ) Active Active\nThen B is correct in this case","upvote_count":"3","poster":"SkyZeroZx"},{"poster":"Jackhemo","content":"olabiba.ai said B.","comment_id":"924847","upvote_count":"1","timestamp":"1686890280.0"},{"poster":"Moallal","timestamp":"1686420960.0","comment_id":"920230","content":"Selected Answer: A\nDo the math, option A is 5.55 days.","upvote_count":"1"},{"upvote_count":"5","comment_id":"910651","poster":"Snape","content":"Selected Answer: B\nA Wrong - I have stopped reading after 'create cron' , Same goes with C.\nD Wrong - Running ASG at full capacity in the DR is not cost efficient","timestamp":"1685490240.0"},{"content":"i think i agree with option B, initially chosen D\nthe problem is that we need a cost effective solution and based on the following the global database might be more expensive and the fact the RDS cross region replication may cover the RTO of 10 minutes.\nquick compare on global database and cross region replication\nRDS Cross Region Replication - You will accrue charges for data transfer between Amazon EC2 and Amazon RDS across Regions, charged on both sides of the transfer ($0.02/GB out)\nAurora Global Database - you pay for replicated write I/O operations between the primary Region and each secondary Region. The number of replicated write I/O operations to each secondary Region is the same as the number of in-Region write I/O operations performed by the primary Region Replicated Write I/Os $0.20 per million replicated write I/Os","upvote_count":"2","timestamp":"1685487420.0","comment_id":"910631","poster":"rbm2023"},{"upvote_count":"1","timestamp":"1685431560.0","comment_id":"910008","content":"Selected Answer: B\nI would go with B as 10minutes RTO allows for scale up the ASG size. Also read replica is cheaper and can be promoted to primary. Also aurora replication to read replica is usually much less than 100 milliseconds after the primary writes operation which will be enough fot the RPO of 30 seconds.","poster":"andreitugui"},{"comment_id":"909594","upvote_count":"2","timestamp":"1685381940.0","poster":"dbaroger","content":"Selected Answer: B\nCost efective = B"},{"timestamp":"1685302980.0","content":"Selected Answer: B\nAgree with B","poster":"AMEJack","comment_id":"908839","upvote_count":"1"},{"comment_id":"907729","content":"Selected Answer: C\nsave the running EC2 cost. Only bring up when needed","comments":[{"upvote_count":"3","timestamp":"1685190780.0","poster":"Roontha","content":"but the question is saying \"web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes\"\nHow RPO/RTO can be achieved with bare minimum EC2 is up and running in DR site. \n\nCan you paste the link/reading to justify your answer.\nThanks","comment_id":"907979"}],"poster":"Masonyeoh","upvote_count":"1","timestamp":"1685158800.0"},{"poster":"Roontha","comments":[{"upvote_count":"1","timestamp":"1685191200.0","content":"https://aws.amazon.com/disaster-recovery/","comment_id":"907988","poster":"Roontha"},{"timestamp":"1685311500.0","upvote_count":"1","poster":"ShinLi","content":"me too. B looks better.","comment_id":"908887"}],"timestamp":"1685143680.0","comment_id":"907621","upvote_count":"3","content":"I agree with Answer B"}],"choices":{"B":"Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. Increase the desired capacity of the Auto Scaling group.","C":"Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster.","A":"Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances from the latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster.","D":"Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster."},"answer":"B","unix_timestamp":1685143680,"question_text":"A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fleet of t3.large Amazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multiple Availability Zones.\n\nIn the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes.\n\nWhich solution will meet these requirements MOST cost-effectively?","topic":"1","timestamp":"2023-05-27 01:28:00","answers_community":["B (90%)","5%"]},{"id":"Iun7voIPxabJ7laopebI","unix_timestamp":1685104920,"question_id":117,"isMC":true,"topic":"1","answer":"C","question_text":"A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's current internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a month to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database more quickly.\n\nWhich solution will migrate the database in the LEAST amount of time?","answers_community":["C (97%)","3%"],"url":"https://www.examtopics.com/discussions/amazon/view/110304-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"C":"Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.","B":"Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use AWS Application Migration Service to migrate the on-premises MySQL database to Aurora MySQL.","A":"Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises MySQL database to Aurora MySQL.","D":"Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL."},"discussion":[{"content":"Selected Answer: C\nWhy Not D:\n1- C=SnowBall Edge, D=SnowBall Device. \nThe basic difference between Snowball and Snowball Edge is the capacity they provide. Snowball provides a total of 50 TB or 80 TB, out of which 42 TB or 72 TB is available, while Amazon Snowball Edge provides 100 TB, out of which 83 TB is available.\n\n2- C=AWS Database Migration . D=Application Migration Service, \nApplication Migration Service simplifies, expedites, and reduces the cost of migrating and modernizing applications. Not for Database","timestamp":"1701375480.0","poster":"F_Eldin","comment_id":"910519","upvote_count":"27"},{"poster":"TonytheTiger","content":"Selected Answer: C\nOption C : How To \n\nhttps://aws.amazon.com/blogs/storage/enable-large-scale-database-migrations-with-aws-dms-and-aws-snowball/","timestamp":"1728487260.0","comment_id":"1192367","upvote_count":"2"},{"content":"Selected Answer: C\nAWS Snowball and Snowball Edge refers the same thing. From the Snowball FAQ \"AWS Snowball is a service that provides secure, rugged devices, so you can bring AWS computing and storage capabilities to your edge environments, and transfer data into and out of AWS. Those rugged devices are commonly referred to as AWS Snowball or AWS Snowball Edge devices. \". Between C and D, it's C using Snowball edge with AWS DMS.","timestamp":"1720612620.0","upvote_count":"1","comment_id":"1118604","poster":"Maygam"},{"content":"Selected Answer: C\nOption C - Direct connection would take 1 month","upvote_count":"1","timestamp":"1716438240.0","poster":"career360guru","comment_id":"1078127"},{"upvote_count":"1","content":"Selected Answer: C\nBasic Snowball edge / DMS use case","timestamp":"1704501060.0","comment_id":"944150","poster":"NikkyDicky"},{"comment_id":"920231","upvote_count":"2","comments":[{"poster":"breadops","upvote_count":"2","timestamp":"1706398140.0","comment_id":"965054","content":"It can take months to provision a DX connection, its not A."},{"upvote_count":"2","comment_id":"924849","content":"it takes ages to order a 1G circuit.","poster":"Jackhemo","timestamp":"1702709040.0"},{"poster":"covabix879","comment_id":"1023832","content":"Keyword is one-time migration. In addition to time it takes to deliver, it will be huge waste for one-time task.","timestamp":"1712140200.0","upvote_count":"2"}],"timestamp":"1702239480.0","poster":"Moallal","content":"Do the math, option A is 5.55 days. It's A"},{"comment_id":"910011","upvote_count":"3","content":"Selected Answer: C\nFirst of all a snowball solution is required for one time migration will focus in C & D.\nNow since we are looking o migrate a database, DMS is needed also Snowball edge can accommodate the 60TB of data as the capacity limit it 80TB. \nD is wrong by mentioning Application Migration service to migrate a database. \n\nSo correct answer is C). Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.","poster":"andreitugui","timestamp":"1701336720.0"},{"poster":"rbm2023","content":"Selected Answer: C\nI agree with option C.\nOption D does not seem ideal because mentions Application Migration Service, also the snowball is more required for petabyte scale data migration while edge seems to be a better fit.","comment_id":"910621","upvote_count":"1","timestamp":"1701304680.0"},{"poster":"dbaroger","timestamp":"1701287220.0","content":"Selected Answer: D\nD better cost than C and it does the same for S3. Need adpter too","comment_id":"909603","upvote_count":"1"},{"content":"Answer : C ( Key words : Limited bandwidth + DB migration should be done quickly)\n\nif there no DB migration, we can go with B","comment_id":"907358","upvote_count":"2","timestamp":"1701009720.0","poster":"Roontha"}],"question_images":[],"answer_images":[],"exam_id":33,"answer_description":"","answer_ET":"C","timestamp":"2023-05-26 14:42:00"},{"id":"YaX2PF9os4w6q6BMBGzE","url":"https://www.examtopics.com/discussions/amazon/view/110302-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"timestamp":"1685432220.0","content":"Selected Answer: C\nCorrect is C. For those voting with B, you missed the Instance configuration part. DLM will only backup the EBS volume not the instance settings also. AWS backup will backup ebs & instance settings. \n\nOption C, using AWS Backup, provides a centralized and cost-effective solution for managing backups across multiple services, including EC2 instances. By creating a scheduled daily backup plan for the EC2 instances, AWS Backup ensures regular backups are taken. The backups can be configured to be stored in a vault in the secondary Region, fulfilling the requirement of maintaining backups in a separate Region.\nThe EC2 instance volumes and configurations can then be restored from the backup vault using AWS Backup's restore capabilities. This allows for the recovery of EC2 instances and their configurations within the required timeframe of 1 business day, with a maximum data loss of 1 day's worth.","upvote_count":"20","comments":[{"content":"Answer is B.\n\nhttps://aws.amazon.com/ebs/data-lifecycle-manager/\n\nIt has aws sponsored video which stated clearly can take EBS backed AMIs with AWS DLM","comment_id":"914778","comments":[{"content":"just C","timestamp":"1724293080.0","upvote_count":"1","poster":"helloworldabc","comment_id":"1270459"},{"upvote_count":"1","poster":"Just_Ninja","comment_id":"957292","timestamp":"1689840540.0","content":"B is Wrong!\nWhy? They must!! So that means Compliance is important. AWS Backup is a service for Compliance and Goverment Targets. C Match"}],"poster":"Roontha","timestamp":"1685892780.0","upvote_count":"1"}],"poster":"andreitugui","comment_id":"910016"},{"comment_id":"1359435","poster":"sergza888","timestamp":"1740079740.0","upvote_count":"1","content":"Selected Answer: A\nWith these RTO/RPO WE don't need to backup entire EC2 especially for cost efficiency. We Only need to maintain CF In another region as well as EBS Backups. System Manager allows you to script and execute backup and copy it to another region instead of DL"},{"content":"Selected Answer: C\nC because of this one. \"The company has limited staff and needs a backup solution that optimizes operational efficiency and cost.\" AWS Backup really optimizes your backup solution. We backup everthing now with AWS Backup. B works too but it more complicated. The restore from AWS Backup is nearly a no brainer","poster":"chris_spencer","upvote_count":"1","timestamp":"1728755820.0","comment_id":"1296603"},{"poster":"gfhbox0083","content":"Selected Answer: C\nC, for sure.\nUse AWS Backup.\nDLM itself does not directly support restore operations.","timestamp":"1720853940.0","upvote_count":"1","comment_id":"1247156"},{"comments":[{"content":"I meant DLM cannot restore so the option B is wrong.","upvote_count":"1","timestamp":"1707046260.0","comment_id":"1140043","poster":"saggy4"}],"comment_id":"1140041","upvote_count":"1","timestamp":"1707046200.0","content":"Correct Answer is C.\nWhy not B, DLM can only take backup on restore. The options says using DLM restore the volumes.","poster":"saggy4"},{"poster":"career360guru","content":"Selected Answer: C\nOption C","timestamp":"1700721060.0","upvote_count":"2","comment_id":"1078136"},{"comment_id":"1073843","poster":"severlight","upvote_count":"1","content":"Selected Answer: C\nBecause AWS Back ups supports restore and DLM doesn't","timestamp":"1700292360.0"},{"content":"Selected Answer: B\nB\nThe explanation here fits the use-case\nhttps://aws.amazon.com/blogs/storage/automating-amazon-ebs-snapshot-and-ami-management-using-amazon-dlm/","comment_id":"986043","timestamp":"1692560880.0","upvote_count":"1","poster":"SK_Tyagi"},{"timestamp":"1688597220.0","poster":"NikkyDicky","content":"Selected Answer: C\nC\nB would be ok, if DLM supported restore. it doesn't","upvote_count":"2","comment_id":"944156"},{"upvote_count":"1","content":"Selected Answer: C\nI think correct is C. AWS Backup is easier and perfectly fits the scenario","timestamp":"1688047020.0","comment_id":"938206","poster":"javitech83"},{"upvote_count":"2","comment_id":"932475","timestamp":"1687604760.0","content":"Selected Answer: C\nB says \"Use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region\" - just tested it and could not find any option for DLM to restore volumes, think the snapshots are managed the usual way.","poster":"Maria2023"},{"timestamp":"1687376400.0","content":"c-c-c-c-c-c-c-c-c-c","upvote_count":"1","poster":"easytoo","comment_id":"929863"},{"timestamp":"1686564240.0","upvote_count":"1","comment_id":"921320","content":"Selected Answer: B\nIts B!!!!!!!!!!!!!!!!","poster":"Jonalb"},{"content":"Why not A?","comment_id":"920086","timestamp":"1686403260.0","poster":"clownfishman","upvote_count":"3"},{"content":"Selected Answer: B\nI prefer B to C as this sentence \"The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes\", in this question, there is no database mentioned, I assume all persistent data is in EBS, so no need to backup ec2 instances, you can directly startup ec2 instance by cloudformation and load backuped ebs.","poster":"Jesuisleon","comment_id":"913856","timestamp":"1685815020.0","upvote_count":"2"},{"comment_id":"910620","upvote_count":"2","comments":[{"upvote_count":"1","poster":"Jesuisleon","comment_id":"913854","content":"DLM can copy snapshots to another region, see https://aws.amazon.com/about-aws/whats-new/2019/12/amazon-data-lifecycle-manager-enables-automation-snapshot-copy-via-policies/","timestamp":"1685814780.0"}],"timestamp":"1685485560.0","poster":"rbm2023","content":"Selected Answer: C\nAWS Backup is more cost effective so I would chose C as well. The DLM option B, does not contemplate the back up in another region as far as I could see."},{"upvote_count":"1","timestamp":"1685473560.0","comments":[{"content":"But B mentions only EBS snapshots (Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes)! Does not say anything about AMI's.\nSo IMO the answer is C","timestamp":"1685518800.0","poster":"andreitugui","comment_id":"910972","upvote_count":"1"}],"content":"Selected Answer: B\nAWS Backup is a latter service which tries to simplify the challenge of administering a backup in each service individually.\n\nHowever AWS Lifecycle Manager originally only made EBS snapshots but has been expanded to create AMIs. I don't believe AWS Backup can trigger AMI creation.","comment_id":"910549","poster":"F_Eldin"},{"comment_id":"908299","timestamp":"1685244180.0","comments":[{"upvote_count":"1","content":"https://aws.amazon.com/blogs/storage/automating-amazon-ebs-snapshots-management-using-data-lifecycle-manager/","timestamp":"1685244300.0","poster":"deegadaze1","comment_id":"908300"}],"content":"The answer is B","upvote_count":"2","poster":"deegadaze1"},{"comments":[{"content":"@deegadaze1: Agreed with answer B\nhttps://aws.amazon.com/blogs/storage/automating-amazon-ebs-snapshot-and-ami-management-using-amazon-dlm/\n\nIn this blog post, we examine how you can use Amazon Data Lifecycle Manager (Amazon DLM) lifecycle policies to automate the creation, retention, and deletion of Amazon EBS snapshots. With Amazon DLM, the need for complicated and custom scripts to manage EBS snapshots is eliminated. Amazon DLM enables you to create, manage, and delete EBS snapshots in a simple, automated way based on resource tags for EBS volumes or Amazon EC2 instances. This reduces the operational complexity of managing EBS snapshots, thereby saving time and money. Also, let’s not forget the best part: Amazon DLM is free to use and is available in all AWS Regions.","comment_id":"908663","timestamp":"1685281020.0","upvote_count":"1","poster":"Roontha"}],"timestamp":"1685103660.0","content":"Answer : C\nhttps://aws.amazon.com/getting-started/hands-on/amazon-ec2-backup-and-restore-using-aws-backup/\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/integrate-cloudformation-with-aws-backup.html","poster":"Roontha","upvote_count":"3","comment_id":"907346"}],"question_id":118,"question_text":"A company has an application in the AWS Cloud. The application runs on a fleet of 20 Amazon EC2 instances. The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.\n\nThe company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration within 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes operational efficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network configuration in a secondary Region.\n\nWhich solution will meet these requirements?","topic":"1","unix_timestamp":1685103660,"question_images":[],"choices":{"A":"Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy the snapshots to the secondary Region. In the event of a failure launch the CloudFormation templates, restore the EBS volumes from snapshots, and transfer usage to the secondary Region.","B":"Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure, launch the CloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region.","C":"Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region. In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region.","D":"Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure, launch the CloudFormation template and transfer usage to the secondary Region."},"answers_community":["C (84%)","Other"],"exam_id":33,"isMC":true,"answer_description":"","timestamp":"2023-05-26 14:21:00","answer_ET":"C","answer":"C","answer_images":[]},{"id":"njLM9V0dLIJGCYaqLTjf","question_images":[],"discussion":[{"poster":"SkyZeroZx","content":"Selected Answer: ACE\nAnswer : ACE\nA) SSE S3 sounds good encript in rest data\nB) sounds good until say in ACLs is incorrect \nC) Bucket Policy avoid upload unencrypted is correct sounds good\nD) CloudFront with KMS ? why ? not seems\nE) HTTP redirect to HTTPS sounds good is clasic this case \nF) why ? not seems in this case","comment_id":"941957","upvote_count":"18","timestamp":"1688396040.0"},{"poster":"Just_Ninja","upvote_count":"6","content":"Selected Answer: ACE\nACE.\nBut A is deprecated :)\nbecause since the 05.01.2023 S3 use automatical atRest encryption for new objekts.","comments":[{"comment_id":"1149668","poster":"dankositzke","timestamp":"1707868260.0","upvote_count":"2","content":"Right I would go with CEF for 2024 onwards"}],"timestamp":"1689840720.0","comment_id":"957296"},{"comment_id":"1180160","poster":"khchan123","content":"Selected Answer: BCE\nBCE\nYou need B to enforce encryption in transit with S3. Other options cannot do that.","upvote_count":"1","timestamp":"1711124460.0","comments":[{"timestamp":"1724293140.0","comment_id":"1270460","content":"just ACE","poster":"helloworldabc","upvote_count":"1"}]},{"content":"Selected Answer: ACE\nThis question was obviously formulated before S3 buckets were encrypted by default.","comment_id":"1172368","poster":"Dgix","upvote_count":"1","timestamp":"1710317700.0"},{"upvote_count":"2","poster":"duriselvan","timestamp":"1702110180.0","content":"A. Turn on S3 server-side encryption for the S3 bucket that the web application uses.\nD. Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).\nE. Configure redirection of HTTP requests to HTTPS requests in CloudFront.\n\nHere's why these steps are necessary:\n\nA. S3 server-side encryption: This encrypts data in the S3 bucket at rest, ensuring data confidentiality even if someone gains unauthorized access to the bucket.\nD. CloudFront SSE-KMS: This encrypts data in transit between CloudFront and the client, ensuring data confidentiality when users upload and download files.\nE. HTTP to HTTPS redirect: This ensures all communication between the client and CloudFront occurs over HTTPS, encrypting data in transit and preventing eavesdropping.","comment_id":"1091525"},{"poster":"career360guru","comment_id":"1078141","content":"Selected Answer: ACE\nOptions A, C , E","timestamp":"1700721360.0","upvote_count":"1"},{"content":"Selected Answer: ADE\nA. Turn on S3 server-side encryption for the S3 bucket that the web application uses.\nD. Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).\nE. Configure redirection of HTTP requests to HTTPS requests in CloudFront.\nData at rest encrypted for Both S3 and Cloudfront\nE for data in transit","comment_id":"1020376","upvote_count":"1","poster":"task_7","timestamp":"1695957240.0"},{"poster":"Simon523","upvote_count":"2","timestamp":"1692691260.0","comment_id":"987205","content":"Selected Answer: ACE\nHow to Prevent Uploads of Unencrypted Objects to Amazon S3\nhttps://aws.amazon.com/tw/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/"},{"content":"ACE but why not F?","comment_id":"983505","timestamp":"1692268680.0","upvote_count":"1","comments":[{"poster":"chikorita","comments":[{"comment_id":"1262710","poster":"kgpoj","content":"When you have pre-signed urls, you don't even necessarily need cloudFront","timestamp":"1723163760.0","upvote_count":"1"}],"content":"question nowhere mentions the use of pre-signed URLs\nif it was used in this scenario then it could potentially be one of the right answers","timestamp":"1693757460.0","upvote_count":"3","comment_id":"997776"}],"poster":"RotterDam"},{"content":"Selected Answer: ACE\nwe don't have a \"encrytion at rest\" for cloudfront in the console","comment_id":"946034","upvote_count":"1","timestamp":"1688769360.0","poster":"Christina666"},{"upvote_count":"1","content":"Selected Answer: ACE\nA and C are a bit redundant. I'd pick D instead of C, but for ACL reference","poster":"NikkyDicky","comment_id":"944159","timestamp":"1688598000.0"},{"timestamp":"1687376580.0","content":"a-d-e a-d-e a-d-e","upvote_count":"2","comment_id":"929865","poster":"easytoo"},{"comment_id":"913838","content":"Selected Answer: ACE\nSource: https://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule\n\nB is wrong as \"aws:SecureTransport\": \"true\" does not deny 'http' traffic","upvote_count":"1","timestamp":"1685813100.0","poster":"chathur"},{"comments":[{"comment_id":"913861","timestamp":"1685815680.0","poster":"Jesuisleon","upvote_count":"6","content":"you should add \"aws:SecureTransport\": \"true\" in the S3 bucket policy not S3 ACL.\nsee https://stackoverflow.com/questions/47815526/s3-bucket-policy-vs-access-control-list\n\nand \" We recommend allowing only encrypted connections over HTTPS (TLS) by using the aws:SecureTransport condition in your Amazon S3 bucket policies\" from https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html"},{"upvote_count":"1","timestamp":"1685884740.0","content":"Because C does just that","poster":"BabaP","comment_id":"914646"},{"upvote_count":"1","timestamp":"1685813040.0","comment_id":"913836","poster":"chathur","content":"https://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule\nit is not enough"}],"comment_id":"911669","poster":"consultornetwork","timestamp":"1685587680.0","content":"Why not B?","upvote_count":"2"},{"comment_id":"910030","upvote_count":"2","timestamp":"1685432940.0","poster":"andreitugui","content":"Selected Answer: ACE\nI will go with ACE"},{"poster":"Roontha","upvote_count":"4","content":"Answer : ACE","timestamp":"1685102640.0","comment_id":"907337"}],"topic":"1","unix_timestamp":1685102640,"exam_id":33,"answers_community":["ACE (94%)","3%"],"question_text":"A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files. According to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront.\n\nWhich combination of steps will meet the encryption requirements? (Choose three.)","question_id":119,"answer_ET":"ACE","url":"https://www.examtopics.com/discussions/amazon/view/110300-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_images":[],"answer":"ACE","choices":{"B":"Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs.","E":"Configure redirection of HTTP requests to HTTPS requests in CloudFront.","C":"Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.","F":"Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses.","A":"Turn on S3 server-side encryption for the S3 bucket that the web application uses.","D":"Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS)."},"timestamp":"2023-05-26 14:04:00","isMC":true,"answer_description":""},{"id":"aZ5UT0mkWvVAZCGqC5Pb","answer_images":[],"question_images":[],"exam_id":33,"timestamp":"2023-05-26 13:53:00","answer":"D","unix_timestamp":1685101980,"topic":"1","question_id":120,"choices":{"C":"Store the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS Key Management Service (AWS KMS) customer managed key. Restrict access to the customer managed key so that only the IT security team can access the key.","D":"Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the secret. Restrict access to the secret and the customer managed key so that only the IT security team can access the secret and the key.","B":"Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the environment variables of each Lambda function. Load the credentials from the environment variables in the Lambda code. Restrict access to the KMS key so that only the IT security team can access the key.","A":"Store the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the SecureString parameter. Restrict access to the SecureString parameter and the customer managed key so that only the IT security team can access the parameter and the key."},"discussion":[{"poster":"Snape","content":"Selected Answer: D\nAnswer : D\nRotation = Secret Manager (and Not Parameter store)","comment_id":"910668","upvote_count":"13","timestamp":"1685492760.0"},{"poster":"_Jassybanga_","comment_id":"1274437","content":"Answer should be A , As we are talking of encryption Key rotation by customer IT key responisble person and not the database credential rotation","upvote_count":"2","timestamp":"1724923680.0"},{"upvote_count":"1","content":"Selected Answer: D\nTo use parameters from Parameter Store in AWS Lambda functions without using an SDK, you can use the AWS Parameters and Secrets Lambda Extension.\nTo use parameters in a Lambda function without the Lambda extension, you must configure your Lambda function to receive configuration updates by integrating with the GetParameter API action for Parameter Store.","comment_id":"1269445","poster":"AA001","timestamp":"1724156580.0"},{"comment_id":"1078146","poster":"career360guru","timestamp":"1700721540.0","content":"Selected Answer: D\nOption D","upvote_count":"1"},{"timestamp":"1688598120.0","poster":"NikkyDicky","content":"Selected Answer: D\nits a D","upvote_count":"1","comment_id":"944161"},{"upvote_count":"2","timestamp":"1688047440.0","content":"Selected Answer: D\nKeys is DB credentials rotation","comment_id":"938216","poster":"javitech83"},{"poster":"easytoo","upvote_count":"1","content":"d-d-d-d-dd-d-dd-d-d-d","comment_id":"929867","timestamp":"1687376760.0"},{"comments":[{"comment_id":"957315","content":"Nice description, but A is Wrong. Parameter Store is not the best practice for Secrets based on AWS Well Architecting Framework","timestamp":"1689842400.0","poster":"Just_Ninja","upvote_count":"2"},{"poster":"Jackhemo","content":"Answer is D. This is for the next question.","timestamp":"1686956520.0","comment_id":"925599","upvote_count":"2"}],"content":"Selected Answer: A\nFrom olabiba.ai \n\"Based on the requirements of resolving scaling issues and minimizing licensing costs, the most cost-effective solution would be option A: Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.\"","upvote_count":"1","poster":"Jackhemo","comment_id":"925598","timestamp":"1686956400.0"},{"comments":[{"content":"A does not satisfy the requirement \"This key must be rotated on a regular basis.\"","poster":"F_Eldin","upvote_count":"3","comment_id":"911291","timestamp":"1685537760.0"},{"poster":"kejam","content":"Agreed. Requirement is to rotate the Key. KMS CMKs can be rotated:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html","timestamp":"1705033380.0","comment_id":"1120429","upvote_count":"1"}],"comment_id":"910611","timestamp":"1685483880.0","poster":"rbm2023","upvote_count":"4","content":"Selected Answer: A\nI think the answer is A the requirement is to rotate the KEY and not the password, looks like this question was created to make us chose option D. \nOption A stores the password in the Param Store encrypting it with KMS which is the requirement “the credentials for the production database must be encrypted with a key that only members of the IT security team's IAM user group can access.”\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/ps-integration-lambda-extensions.html \nCheck the Authentication section."},{"content":"Selected Answer: D\nAnswering D","upvote_count":"1","comment_id":"910035","poster":"andreitugui","timestamp":"1685433120.0"},{"comment_id":"907731","content":"Selected Answer: D\nD, Secret Manager is the accurate solution","poster":"Masonyeoh","upvote_count":"1","timestamp":"1685159160.0"},{"comment_id":"907328","poster":"Roontha","content":"Answer : D \nKeys is DB credentials rotation","upvote_count":"1","timestamp":"1685101980.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/110299-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"answer_ET":"D","answer_description":"","answers_community":["D (80%)","A (20%)"],"question_text":"A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on Amazon RDS. The company has separate environments for development and production, including a clone of the database system.\n\nThe company's developers are allowed to access the credentials for the development database. However, the credentials for the production database must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a regular basis.\n\nWhat should a solutions architect do in the production environment to meet these requirements?"}],"exam":{"name":"AWS Certified Solutions Architect - Professional SAP-C02","provider":"Amazon","id":33,"isMCOnly":true,"isBeta":false,"numberOfQuestions":529,"isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":24},"__N_SSP":true}