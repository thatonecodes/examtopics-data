{"pageProps":{"questions":[{"id":"VU4NQUYBHEHPJ8OE46DL","discussion":[{"comment_id":"1123489","timestamp":"1705332840.0","upvote_count":"1","poster":"MultiAZ","content":"Selected Answer: C\nAnswer is C\nYou can now mix and match Serverless and normal Auroras in the same cluster."},{"comment_id":"962594","upvote_count":"1","content":"The Answer is B for Serverless V2.","timestamp":"1690281600.0","poster":"Windy"},{"poster":"SamDDD","upvote_count":"3","timestamp":"1688851260.0","comment_id":"946750","content":"By the way, in Serverless V2 it is possible to fail over: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.upgrade.html#aurora-serverless-v2.switch-from-provisioned\nThis question is a bit old."},{"timestamp":"1684523040.0","content":"Creating serverless readreplica and promoting whenever lag is minimal is the best option I can go with - C\nDMS is the longest route","comment_id":"902192","poster":"manig","upvote_count":"2"},{"poster":"teo2157","content":"Selected Answer: D\nYou can´t create an Aurora Serverless replica from the existing Aurora DB cluster. With no downtime, the answer is definitely D","timestamp":"1675864860.0","upvote_count":"1","comment_id":"802088"},{"content":"Answer is B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/aurora-migrate-provisioned-serverless/","poster":"parle101","upvote_count":"1","comment_id":"758069","timestamp":"1672106280.0"},{"content":"Selected Answer: C\nI am really confused here. Pls refer to https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html - Which clearly says that \"Aurora Replicas connect to the same storage volume as the primary DB instance, but support read operations only\". Which means there is no lag at all given Read replicas connect the same storage. Hence i will go for \"C\" which is the simplest from operations perspective","comment_id":"754827","upvote_count":"2","timestamp":"1671880860.0","poster":"RBSK","comments":[{"content":"You really need to study again, very carefully. Replica is a basic concept in RDS and Aurora. Replicas and primary are the type of nodes in the *SAME* cluster, be it a RDS cluster or Aurora cluster. So when you are creating an \"Aurora Serverless Replica\", there has to be an Aurora *Serverless* Primary in the first place. That doesn't exist in the question - there is only an \"Aurora MySQL DB cluster\"\nIn other words, \"You can´t create an Aurora Serverless replica from the existing Aurora DB cluster\"","comments":[{"upvote_count":"1","timestamp":"1709719140.0","content":"English is way harder than databases apparently","poster":"confusedyeti69","comment_id":"1167064"}],"comment_id":"908129","upvote_count":"1","poster":"lehoang15tuoi","timestamp":"1685208300.0"}]},{"poster":"Jiang_aws1","comment_id":"692419","content":"C vs D : looks take same amount of to do finial data sync & C is much simple than D\nC: Just auto sync by Aurora Replica\nD: Sync data by CDC","timestamp":"1665523320.0","upvote_count":"1"},{"timestamp":"1653831360.0","poster":"rlnd2000","comment_id":"608768","upvote_count":"1","content":"Both C and D provides near Zero down time but the option C says \"...when the replica lag is minimal\" for me this is unacceptable for a \"business's mission-critical production workload\", B is out, impossible without downtime. I will go with D"},{"poster":"novice_expert","content":"Selected Answer: D\nD. Replicate the data between the existing DB cluster and a new Aurora Serverless DB cluster by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) enabled.\n\nfor DMS: Aurora Serverless can not be source but can be target","comment_id":"595408","timestamp":"1651372500.0","upvote_count":"4"},{"upvote_count":"2","comment_id":"585077","poster":"kret","content":"Selected Answer: D\nbecause of the limited downtime requirement I'll go with DMS and D","timestamp":"1649834220.0"},{"poster":"RotterDam","upvote_count":"3","timestamp":"1646582460.0","comment_id":"562110","content":"Selected Answer: D\nB looks tempting but it will result in downtime. D wont have any downtime"},{"comment_id":"533263","upvote_count":"1","poster":"awsmonster","content":"Option B","comments":[{"poster":"awsmonster","upvote_count":"1","comments":[{"content":"Sorry, D is correct. The doc state it does not support as Source, but the Aurora Serverless in this question is target.","poster":"awsmonster","timestamp":"1643229900.0","upvote_count":"4","comment_id":"533268"},{"timestamp":"1696094040.0","comment_id":"1021739","content":"That applies only to Serverless V1.\nYou can use AWS Database Migration Service (DMS) and Change Data Capture (CDC) with Aurora Serverless v2 DB clusters though. \nImplementing Change Data Capute (CDC) with Aurora Serverless v2. https://dev.to/omarrosadio/implementing-change-data-capute-cdc-with-aurora-serverless-v2-4i1l","upvote_count":"1","poster":"Germaneli"}],"timestamp":"1643229780.0","comment_id":"533267","content":"You can't use AWS Database Migration Service and Change Data Capture (CDC) with Aurora Serverless DB clusters. Only provisioned Aurora DB clusters support CDC with AWS DMS as a source.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations"}],"timestamp":"1643229540.0"},{"poster":"Shunpin","upvote_count":"2","timestamp":"1640689920.0","content":"Selected Answer: D\nThe requirement to Minimum Downtime and from Provisioned to Serverless. The best choice is DMS.","comment_id":"511021"},{"comment_id":"508793","upvote_count":"1","content":"Option D","timestamp":"1640374860.0","poster":"jove"},{"content":"B. https://aws.amazon.com/rds/aurora/faqs/\nAmazon Aurora is drop-in compatible with existing MySQL open-source databases and adds support for new releases regularly. This means you can easily migrate MySQL databases to and from Aurora using standard import/export tools or snapshots. It also means that most of the code, applications, drivers, and tools you already use with MySQL databases today can be used with Aurora with little or no change.","comments":[{"poster":"RotterDam","content":"yes but snapshot restore has downtime","comment_id":"562109","upvote_count":"1","timestamp":"1646582400.0"},{"content":"Since you would need to stop the writes on the source for this to work > Take snapshot and restore. then repoint the app - depending on the size of the database this could be substantial. With DMS its zero and the question is asking about LEAST DOWNTIME and LEAST APPLICATION IMPACT","upvote_count":"2","comment_id":"562112","poster":"RotterDam","timestamp":"1646582880.0"}],"poster":"[Removed]","comment_id":"496267","upvote_count":"2","timestamp":"1638903780.0"},{"upvote_count":"4","content":"D.\nhttps://medium.com/@souri29/how-to-migrate-from-amazon-rds-aurora-or-mysql-to-amazon-aurora-serverless-55f9a4a74078","poster":"cynthiacy","timestamp":"1638443400.0","comment_id":"492379"}],"question_text":"A company is running its critical production workload on a 500 GB Amazon Aurora MySQL DB cluster. A database engineer must move the workload to a new\nAmazon Aurora Serverless MySQL DB cluster without data loss.\nWhich solution will accomplish the move with the LEAST downtime and the LEAST application impact?","isMC":true,"topic":"1","answer_images":[],"answer":"D","question_images":[],"unix_timestamp":1638443400,"choices":{"D":"Replicate the data between the existing DB cluster and a new Aurora Serverless DB cluster by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) enabled.","A":"Modify the existing DB cluster and update the Aurora configuration to ג€Serverless.ג€","B":"Create a snapshot of the existing DB cluster and restore it to a new Aurora Serverless DB cluster.","C":"Create an Aurora Serverless replica from the existing DB cluster and promote it to primary when the replica lag is minimal."},"answer_description":"","timestamp":"2021-12-02 12:10:00","answer_ET":"D","exam_id":22,"answers_community":["D (80%)","C (20%)"],"question_id":86,"url":"https://www.examtopics.com/discussions/amazon/view/67082-exam-aws-certified-database-specialty-topic-1-question-176/"},{"id":"ZhVFWqNHoZ1WloJyXcS4","question_images":[],"answer_images":[],"answer":"A","timestamp":"2021-12-10 15:44:00","choices":{"B":"Amazon DynamoDB streams with AWS Lambda to replicate the data","D":"An Amazon Aurora global database","C":"An Amazon ElastiCache for Redis cluster with cluster mode enabled and multiple shards","A":"Amazon DynamoDB global tables"},"question_text":"A company is building a web application on AWS. The application requires the database to support read and write operations in multiple AWS Regions simultaneously. The database also needs to propagate data changes between Regions as the changes occur. The application must be highly available and must provide latency of single-digit milliseconds.\nWhich solution meets these requirements?","answer_ET":"A","isMC":true,"exam_id":22,"question_id":87,"url":"https://www.examtopics.com/discussions/amazon/view/67569-exam-aws-certified-database-specialty-topic-1-question-177/","answer_description":"","discussion":[{"upvote_count":"5","content":"A: DynamoDB Global tables\n\nAurora Global Databases provides a writer and a reader endpoints in the primary region but only a reader endpoints in other region. Although strongly consistent, it does not fulfill the requirements that \"there are plenty of read / write activities\" in all regions.","timestamp":"1641975060.0","comment_id":"521985","poster":"awsmonster"},{"content":"Selected Answer: A\nA: DynamoDB Global tables\n\nbecause of below\nA: DynamoDB providing single-digit millisecond latency \nD: Aurora Global Database with typical cross-Region replication latencies below 1 second.","timestamp":"1694555880.0","poster":"Pranava_GCP","comment_id":"1006054","upvote_count":"1"},{"comment_id":"595347","upvote_count":"1","content":"Selected Answer: A\nA. Amazon DynamoDB global tables","poster":"novice_expert","timestamp":"1651361580.0"},{"comment_id":"585158","timestamp":"1649847960.0","content":"Selected Answer: A\nwe need to write in several regions -> Aurora Global is single master, so DynamoDB Global is an answer","upvote_count":"3","poster":"kret"},{"upvote_count":"3","comment_id":"502299","content":"The questions states that the database needs to \"support concurrent read and write activities in several AWS Regions.\" Aurora Global databases only allow writes to the single master in a single region. DynamoDB Global tables allow read and write to all instances in all regions. \nAnswer is A","poster":"grekh001","timestamp":"1639583280.0","comments":[{"poster":"jove","upvote_count":"1","timestamp":"1640318760.0","content":"I agree, the answer is A","comment_id":"508300"}]},{"comments":[{"upvote_count":"2","comment_id":"498712","comments":[{"poster":"Radhaghosh","upvote_count":"1","comment_id":"605660","timestamp":"1653241800.0","content":"Did you read this line? \"the database supports concurrent read and write activities in several AWS Regions\". Does Aurora Global Database support this feature?"}],"timestamp":"1639147560.0","poster":"2025flakyt","content":"The question states that - Additionally, the database must communicate data changes across Regions as they occur. \nDynamoDB is eventual consistency\nAurora is immediate consistency"}],"upvote_count":"1","comment_id":"498711","poster":"2025flakyt","content":"D is the answer","timestamp":"1639147440.0"}],"answers_community":["A (100%)"],"unix_timestamp":1639147440,"topic":"1"},{"id":"y9Jw7IR1FQFRhOBpq74i","topic":"1","timestamp":"2021-11-11 23:43:00","url":"https://www.examtopics.com/discussions/amazon/view/65858-exam-aws-certified-database-specialty-topic-1-question-178/","answer_ET":"C","unix_timestamp":1636670580,"discussion":[{"upvote_count":"11","comments":[{"comment_id":"477715","poster":"johnconnor","content":"you are actually right, taking from AWS documentation \"The only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster. See Neptune's data export service and utility for an easy way to export data from a DB cluster, and Neptune's bulk loader for an easy way to import data back into Neptune.\"","upvote_count":"3","timestamp":"1636833660.0"},{"poster":"user0001","timestamp":"1645976040.0","content":"Neptune storage allocation\nEven though a Neptune cluster volume can grow to 64 TiB, you are only charged for the space actually allocated. The total space allocated is determined by the storage high water mark, which is the maximum amount allocated to the cluster volume at any time during its existence.\n\nThis means that even if user data is removed from a cluster volume, such as by a drop query like g.V().drop(), the total allocated space remains the same. Neptune does automatically optimize the unused allocated space for reuse in the future.","comment_id":"557408","upvote_count":"1"}],"comment_id":"476542","poster":"leunamE","timestamp":"1636670580.0","content":"Option C. The only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster. \nCreating and restoring a snapshot does not reduce the amount of storage allocated for your DB cluster, because a snapshot retains the original image of the cluster's underlying storage."},{"upvote_count":"2","timestamp":"1651261740.0","comment_id":"594672","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-storage.html#feature-overview-storage-best-practices\n\nThe only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster.","poster":"novice_expert"},{"comment_id":"566951","timestamp":"1647185340.0","content":"Selected Answer: C\nC\n\n\"The only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster. \"\n\n\"Creating and restoring a snapshot does not reduce the amount of storage allocated for your DB cluster, because a snapshot retains the original image of the cluster's underlying storage.\"\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-storage.html","upvote_count":"3","poster":"Dantas"},{"comment_id":"561242","poster":"RotterDam","timestamp":"1646458140.0","content":"Selected Answer: C\nAnswer is C. Its very easy to get tricked into \"Restoring from a snapshot\" but remember - restoring from a snapshot WONT change the storage capacity (in console RDS Snapshot > Restore > under Allocate Capacity its greyed out).","upvote_count":"3"},{"poster":"jove","comment_id":"505718","upvote_count":"4","timestamp":"1640040900.0","content":"Selected Answer: C\nOption C"},{"poster":"mnzsql365","content":"Ans is C\nThe only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster.","upvote_count":"2","timestamp":"1639710780.0","comment_id":"503328"},{"comment_id":"485898","timestamp":"1637753760.0","content":"Selected Answer: C\nOption C. As for every provisioned storage in AWS, you cant reduce the capacity choosen, you need to create a new one and transfer the data.","poster":"GMartinelli","upvote_count":"2"}],"isMC":true,"answers_community":["C (100%)"],"exam_id":22,"answer_description":"","question_id":88,"answer_images":[],"choices":{"A":"Take a snapshot of the cluster volume. Restore the snapshot in another cluster with a smaller volume size.","B":"Use the AWS CLI to turn on automatic resizing of the cluster volume.","C":"Export the cluster data into a new Neptune DB cluster.","D":"Add a Neptune read replica to the cluster. Promote this replica as a new primary DB instance. Reset the storage space of the cluster."},"question_text":"A company is using Amazon Neptune as the graph database for one of its products. The company's data science team accidentally created large amounts of temporary information during an ETL process. The Neptune DB cluster automatically increased the storage space to accommodate the new data, but the data science team deleted the unused information.\nWhat should a database specialist do to avoid unnecessary charges for the unused cluster volume space?","answer":"C","question_images":[]},{"id":"o2nO9fkrCYbXtMkeycfm","isMC":true,"timestamp":"2021-11-21 18:43:00","question_id":89,"answer_images":[],"exam_id":22,"topic":"1","discussion":[{"poster":"novice_expert","content":"Selected Answer: D\nD. Use Multi-AZ and deploy a read replica in a secondary Region.\n\nHigh availability = Multi AZ\nDisaster recovery = Read replica in a secondary region (or standby instance)","upvote_count":"3","timestamp":"1651368960.0","comment_id":"595389"},{"poster":"kret","content":"Selected Answer: D\nHA -> Multi AZ, cross region DR -> read replica in a secondary region","comment_id":"585087","timestamp":"1649835120.0","upvote_count":"3"},{"timestamp":"1644327420.0","content":"I guess there must be a 6 hours limit somehow","upvote_count":"1","poster":"Hariru","comment_id":"543070"},{"content":"Answer is D","upvote_count":"1","poster":"SMAZ","comment_id":"509322","timestamp":"1640472720.0"},{"poster":"jove","upvote_count":"4","content":"Selected Answer: D\nHigh availability = Multi AZ\nDisaster recovery = Read replica in a secondary region","timestamp":"1640369580.0","comment_id":"508715"},{"comment_id":"490228","timestamp":"1638219480.0","poster":"IshtarSQL","content":"Answer: A Multi-AZ is for High Availability, Read Replicas are for Scalability, which the \n question does not mention. A is the only choice that does not include a \n needing a Read Replica.","comments":[{"timestamp":"1639593240.0","content":"The question asks for High Availability AND Disaster Recovery. So you need both Multi-AZ for the High Availabilty and a Read Replica for the DR.\nAnswer is D","poster":"grekh001","comment_id":"502413","upvote_count":"3"}],"upvote_count":"3"},{"comment_id":"483517","content":"Shouldn't it be D?","upvote_count":"3","timestamp":"1637516580.0","poster":"johnconnor"}],"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/66480-exam-aws-certified-database-specialty-topic-1-question-179/","choices":{"D":"Use Multi-AZ and deploy a read replica in a secondary Region.","A":"Use a Multi-AZ deployment in each Region.","C":"Use Multi-AZ and read replica deployments within a Region.","B":"Use read replica deployments in all Availability Zones of the secondary Region."},"answer_ET":"D","answers_community":["D (100%)"],"answer_description":"","question_images":[],"question_text":"A database specialist is responsible for designing a highly available solution for online transaction processing (OLTP) using Amazon RDS for MySQL production databases. Disaster recovery requirements include a cross-Region deployment along with an RPO of 5 minutes and RTO of 30 minutes.\nWhat should the database specialist do to align to the high availability and disaster recovery requirements?","unix_timestamp":1637516580},{"id":"gwWbVqGtY2T5CWK6igND","answer_images":[],"question_images":[],"answer":"C","timestamp":"2020-07-18 04:50:00","choices":{"C":"Create an AWS CloudFormation template and use a stack set to deploy the template to all the Regions.","B":"Create an AWS CloudFormation template and deploy the template to all the Regions.","A":"Create an AWS CLI command to deploy the DynamoDB table to all the Regions and save it for future deployments.","D":"Create DynamoDB tables using the AWS Management Console in all the Regions and create a step-by-step guide for future deployments."},"question_text":"A gaming company wants to deploy a game in multiple Regions. The company plans to save local high scores in Amazon DynamoDB tables in each Region. A\nDatabase Specialist needs to design a solution to automate the deployment of the database with identical configurations in additional Regions, as needed. The solution should also automate configuration changes across all Regions.\nWhich solution would meet these requirements and deploy the DynamoDB tables?","answer_ET":"C","isMC":true,"question_id":90,"exam_id":22,"url":"https://www.examtopics.com/discussions/amazon/view/26011-exam-aws-certified-database-specialty-topic-1-question-18/","answer_description":"","discussion":[{"upvote_count":"11","timestamp":"1633198020.0","comments":[{"upvote_count":"1","comment_id":"557628","timestamp":"1645996380.0","poster":"user0001","content":"true but wording is wrong as well"}],"poster":"helpaws","content":"C here","comment_id":"137941"},{"timestamp":"1635665820.0","upvote_count":"5","content":"C : Use CloudFormation StackSets to Provision Resources Across Multiple AWS Accounts and Regions\nhttps://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/","poster":"Anuragdba","comment_id":"439245"},{"upvote_count":"2","content":"Selected Answer: C\nC. Create an AWS CloudFormation template and use a stack set to deploy the template to all the Regions.\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\n\n\"AWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions.\"","timestamp":"1694751180.0","poster":"Pranava_GCP","comment_id":"1008109"},{"poster":"adelcold","comment_id":"930389","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html","upvote_count":"2","timestamp":"1687429680.0"},{"poster":"f___16","comment_id":"829826","upvote_count":"3","content":"Selected Answer: C\nStack sets are used to deploy to multiple regions.","timestamp":"1678011540.0"},{"comment_id":"826002","timestamp":"1677683400.0","content":"Selected Answer: C\nC is the best choice\nA & D so manual","upvote_count":"1","poster":"Dean791"},{"timestamp":"1671069060.0","poster":"lollyj","comment_id":"745611","content":"Selected Answer: C\nStack sets to deploy to regions","upvote_count":"1"},{"poster":"sayed","comment_id":"698931","upvote_count":"1","content":"Selected Answer: C\nC\n\nautomate the deployment of the database with identical configurations in additional Regions (leads us to cloud formation), as needed. The solution should also automate configuration changes across all Regions (leads us to stack set)","timestamp":"1666176300.0"},{"content":"Selected Answer: C\nCreate an AWS CloudFormation template \n-> use a stack set \n-> deploy the template to all the Regions.","upvote_count":"1","poster":"novice_expert","comment_id":"595094","timestamp":"1651330560.0"},{"content":"Selected Answer: C\nOption C","upvote_count":"2","comment_id":"496022","timestamp":"1638882300.0","poster":"GMartinelli"},{"upvote_count":"1","comment_id":"441070","timestamp":"1635902280.0","content":"C >>> Absolutely agreed","poster":"astood"},{"timestamp":"1635562500.0","comment_id":"434509","poster":"aws4myself","upvote_count":"1","content":"C => stack set to deploy in multiple regions"},{"poster":"guru_ji","content":"Correct Answer ==>> C","timestamp":"1635479820.0","upvote_count":"1","comment_id":"430114"},{"content":"C. \nA stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template","comment_id":"423461","poster":"ChauPhan","timestamp":"1634734980.0","upvote_count":"2"},{"upvote_count":"1","content":"Yup C it is.","timestamp":"1634555160.0","poster":"manan728","comment_id":"341829"},{"poster":"LMax","timestamp":"1634419260.0","comment_id":"314765","upvote_count":"2","content":"Answer C"},{"upvote_count":"1","timestamp":"1634280180.0","comment_id":"297853","content":"Ans: C","poster":"myutran"},{"timestamp":"1634243640.0","poster":"JobinAkaJoe","content":"C seems to to be the right answer.\nAlso good use case for global table","comment_id":"253115","upvote_count":"1"},{"content":"yes, C should be","timestamp":"1634177700.0","poster":"Ashoks","comment_id":"211623","upvote_count":"1"},{"poster":"szmulder","comments":[{"comment_id":"150690","upvote_count":"1","poster":"szmulder","content":"Sorry, C is correct","timestamp":"1634121780.0"}],"comment_id":"150125","content":"Ans is B, I think the stack set is for different AWS account and regions.","upvote_count":"1","timestamp":"1633753680.0"},{"upvote_count":"2","comment_id":"145729","timestamp":"1633581600.0","poster":"BillyC","content":"Ans C is correct"},{"upvote_count":"4","content":"Think C as well.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html","comment_id":"139716","timestamp":"1633379460.0","poster":"BillyMadison"}],"answers_community":["C (100%)"],"unix_timestamp":1595040600,"topic":"1"}],"exam":{"provider":"Amazon","isImplemented":true,"id":22,"lastUpdated":"11 Apr 2025","numberOfQuestions":359,"isBeta":false,"isMCOnly":false,"name":"AWS Certified Database - Specialty"},"currentPage":18},"__N_SSP":true}