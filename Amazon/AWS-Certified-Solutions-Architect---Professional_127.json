{"pageProps":{"questions":[{"id":"bgAYyh0P52hbrWYOfT6O","unix_timestamp":1604503740,"question_images":[],"discussion":[{"poster":"asldavid","timestamp":"1632116940.0","content":"B & D \nhttps://aws.amazon.com/getting-started/hands-on/migrate-oracle-to-amazon-redshift/","upvote_count":"19","comment_id":"212836"},{"comment_id":"258987","upvote_count":"5","timestamp":"1633474020.0","poster":"Ebi","content":"Answer is BD"},{"comment_id":"760977","timestamp":"1672316040.0","content":"Selected Answer: BD\nI'll go with B and D","poster":"evargasbrz","upvote_count":"1"},{"upvote_count":"1","comment_id":"525113","timestamp":"1642355100.0","content":"Selected Answer: BD\ni vote b and d","poster":"pititcu667"},{"content":"B and D correct","timestamp":"1638403920.0","comment_id":"492026","poster":"AzureDP900","upvote_count":"1"},{"content":"B & D is correct answer, looks like they intentionally updating wrong answers. Read the question and understand why it is wrong vs right.","timestamp":"1638049260.0","comment_id":"488519","upvote_count":"1","poster":"AzureDP900"},{"poster":"andylogan","comment_id":"443434","timestamp":"1636144860.0","upvote_count":"1","content":"It's B D - Redshift and RDS PostgreSQL"},{"timestamp":"1635856740.0","upvote_count":"2","content":"B,D\n---\nOracle Dara Warehouse --> Redshift\nPostgreSQL --> RDS PostgreSQL","comment_id":"442275","poster":"student22"},{"comment_id":"434886","poster":"tgv","timestamp":"1635297480.0","upvote_count":"1","content":"BBB DDD\n---"},{"timestamp":"1635068700.0","upvote_count":"1","content":"B and D","poster":"blackgamer","comment_id":"433732"},{"timestamp":"1634785740.0","poster":"WhyIronMan","comment_id":"413349","upvote_count":"4","content":"I'll go with B,D"},{"timestamp":"1634675580.0","comment_id":"365385","upvote_count":"1","content":"BD, SAA level.","poster":"mustpassla"},{"upvote_count":"4","poster":"Waiweng","comment_id":"356385","timestamp":"1634028900.0","content":"it's B&D"},{"comment_id":"293819","timestamp":"1633697820.0","poster":"Kian1","upvote_count":"3","content":"going with BD"},{"poster":"kopper2019","comment_id":"271704","content":"B and D for sure","upvote_count":"2","timestamp":"1633609200.0"},{"content":"B & D is the answer.","comment_id":"256386","timestamp":"1633129500.0","poster":"Bulti","upvote_count":"1"},{"content":"B,D for sure","timestamp":"1633100100.0","upvote_count":"1","comment_id":"244972","poster":"rscloud"},{"poster":"T14102020","timestamp":"1632844740.0","content":"For sure B & D","comment_id":"240975","upvote_count":"1"},{"poster":"jackdryan","timestamp":"1632359760.0","upvote_count":"3","comment_id":"232663","content":"I'll go with B,D"},{"poster":"liono","comment_id":"213318","content":"B & D \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-postgresql-database-to-amazon-rds-for-postgresql.html","upvote_count":"5","timestamp":"1632140880.0"}],"question_text":"A company plans to migrate to AWS. A solutions architect uses AWS Application Discovery Service over the fleet and discovers that there is an Oracle data warehouse and several PostgreSQL databases.\nWhich combination of migration patterns will reduce licensing costs and operational overhead? (Choose two.)","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/36061-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["BD (100%)"],"timestamp":"2020-11-04 16:29:00","choices":{"A":"Lift and shift the Oracle data warehouse to Amazon EC2 using AWS DMS.","E":"Migrate the Oracle data warehouse to an Amazon EMR managed cluster using AWS DMS.","C":"Lift and shift the PostgreSQL databases to Amazon EC2 using AWS DMS.","B":"Migrate the Oracle data warehouse to Amazon Redshift using AWS SCT and AWS DMS","D":"Migrate the PostgreSQL databases to Amazon RDS for PostgreSQL using AWS DMS."},"question_id":631,"answer_description":"","exam_id":32,"answer_ET":"DE","topic":"1","isMC":true,"answer":"BD"},{"id":"nVD0M066SrNhlHIGteqM","timestamp":"2020-01-18 20:05:00","answer":"BE","question_text":"A customer has established an AWS Direct Connect connection to AWS. The link is up and routes are being advertised from the customer's end, however the customer is unable to connect from EC2 instances inside its VPC to servers residing in its datacenter.\nWhich of the following options provide a viable solution to remedy this situation? (Choose two.)","discussion":[{"upvote_count":"9","comment_id":"50726","content":"B and E correct: https://aws.amazon.com/premiumsupport/knowledge-center/routing-dx-private-virtual-interface/","timestamp":"1633315020.0","poster":"Gorha"},{"poster":"amministrazione","timestamp":"1723754100.0","comment_id":"1266678","upvote_count":"1","content":"B. Enable route propagation to the virtual pinnate gateway (VGW).\nE. Modify the Instances VPC subnet route table by adding a route back to the customer's on-premises environment."},{"content":"Is pinnate gateway a typo?","poster":"KiraguJohn","upvote_count":"3","timestamp":"1656625440.0","comment_id":"625432","comments":[{"comment_id":"637734","content":"yes typo.","poster":"hilft","timestamp":"1658886060.0","upvote_count":"1"}]},{"content":"Selected Answer: BE\nWhy NOT C:\n“routes are listed on the client's end”, VGW successfully propagate routes to CGW.","upvote_count":"1","comment_id":"563843","timestamp":"1646812800.0","poster":"Alexey79"},{"content":"E is fine\nBetween B and C ,\nC is correct because the question says \"routes are being advertised from the customers's end\" which means Customer end router is probagating routes to VGW\nNow VGW should be configured to probagate routes \"TO\" customer end router","poster":"RVivek","comments":[{"upvote_count":"1","content":"My thought too.","comment_id":"538607","timestamp":"1643800140.0","poster":"CGJoon"}],"timestamp":"1641036360.0","upvote_count":"4","comment_id":"514489"},{"timestamp":"1640929140.0","poster":"cldy","comment_id":"513899","content":"B and E.","upvote_count":"1"},{"timestamp":"1639178400.0","comment_id":"499002","upvote_count":"1","content":"My Answer: B & E","poster":"challenger1"},{"upvote_count":"1","poster":"01037","comment_id":"351987","content":"Yes\nB and E","timestamp":"1636259460.0"},{"upvote_count":"1","poster":"cldy","comment_id":"325728","content":"B. E.\nRoute propagation on VGW + route back to on-prem","timestamp":"1635819120.0"},{"upvote_count":"3","poster":"fullaws","content":"B and E is correct","comment_id":"143937","timestamp":"1635499020.0"},{"poster":"fullaws","comment_id":"143929","timestamp":"1635427200.0","content":"B and E is correct","upvote_count":"3"},{"comment_id":"40423","timestamp":"1632458040.0","poster":"CloudFloater","comments":[{"content":"I think you have a typo. you said 'a' is an answer, then in your explanation you said it is not because it refers to VPN and not DX (which is true)","timestamp":"1632523260.0","comment_id":"43851","comments":[{"content":"Right ... BE is correct","upvote_count":"1","poster":"virtual","timestamp":"1635219720.0","comment_id":"51644"}],"upvote_count":"5","poster":"sarah1"}],"content":"AE\nhttp://jayendrapatil.com/aws-direct-connect-dx/\n\na.(deals with VPN)\nb.CORRECT (VGW)\nc.(route propagation is enabled on VGW)\nd.(no route command available)\ne.CORRECT","upvote_count":"3"}],"choices":{"E":"Modify the Instances VPC subnet route table by adding a route back to the customer's on-premises environment.","B":"Enable route propagation to the virtual pinnate gateway (VGW).","D":"Modify the route table of all Instances using the 'route' command.","C":"Enable route propagation to the customer gateway (CGW).","A":"Add a route to the route table with an iPsec VPN connection as the target."},"isMC":true,"answer_ET":"BE","answer_images":[],"unix_timestamp":1579374300,"answers_community":["BE (100%)"],"answer_description":"","question_id":632,"topic":"1","exam_id":32,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/12292-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"IipLqSHy5bcnNrAnSdml","isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/46609-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","exam_id":32,"answer":"BCE","discussion":[{"comment_id":"308495","content":"Ans: BCE","poster":"kalyan_krishna742020","upvote_count":"23","timestamp":"1632212220.0"},{"timestamp":"1730533980.0","comment_id":"1306083","poster":"nimbus_00","upvote_count":"1","content":"Selected Answer: BCE\nLooks right"},{"timestamp":"1656570480.0","content":"Selected Answer: BCE\nNo-brain, it's BCE","poster":"TechX","upvote_count":"1","comment_id":"625069"},{"poster":"jj22222","comment_id":"577673","upvote_count":"1","content":"Selected Answer: BCE\nbce - look right","timestamp":"1648564020.0"},{"content":"Selected Answer: BCE\nit is BCE","upvote_count":"1","timestamp":"1643133240.0","poster":"shotty1","comment_id":"532311"},{"comment_id":"521510","content":"Selected Answer: BCE\nA will only move the dns after 1 hour so ..","poster":"pititcu667","timestamp":"1641902160.0","upvote_count":"1"},{"content":"B,C,E is perfect answer.\nThe requirements can be achieved by using an Amazon DynamoDB database with a global table. DynamoDB is a\nNoSQL database so it fits the requirements. A global table also allows both reads and writes to occur in both Regions.\nFor the web and application tiers Auto Scaling groups should be configured. Due to the 1-minute RTO these must be\nconfigured in an active/passive state. The best pricing model to lower price but ensure resources are available when\nneeded is to use a combination of zonal reserved instances and on-demand instances.\nTo failover between the Regions, a Route 53 failover routing policy can be configured with a TTL configured on the\nrecord of 30 seconds. This will mean clients must resolve against Route 53 every 30 seconds to get the latest record.\nIn a failover scenario the clients would be redirected to the secondary site if the primary site is unhealthy.","upvote_count":"4","timestamp":"1638404100.0","comment_id":"492029","poster":"AzureDP900"},{"timestamp":"1636192380.0","comment_id":"469986","upvote_count":"2","content":"Yes, BBB CCC EEE","poster":"andypham"},{"comment_id":"443444","content":"It's B C E","upvote_count":"1","poster":"andylogan","timestamp":"1635443640.0"},{"timestamp":"1634523120.0","content":"BBB CCC EEE\n---","upvote_count":"1","comment_id":"436373","poster":"tgv"},{"content":"BCE is the answer","timestamp":"1634441700.0","comment_id":"433738","upvote_count":"1","poster":"blackgamer"},{"timestamp":"1634025660.0","content":"Agree B,C,E","comment_id":"418377","upvote_count":"1","poster":"DanShone"},{"comment_id":"413350","upvote_count":"3","poster":"WhyIronMan","timestamp":"1633671480.0","content":"I'll go with B,C,E"},{"upvote_count":"1","timestamp":"1633635300.0","content":"B,C,E for sure","comment_id":"407716","poster":"Kopa"},{"upvote_count":"4","poster":"victordun","timestamp":"1633471560.0","comment_id":"362577","content":"BCE should be most optimal choices that meets requirements"},{"timestamp":"1632629580.0","content":"BCE is correct","upvote_count":"4","comment_id":"356388","poster":"Waiweng"},{"comment_id":"334794","poster":"CarisB","content":"Yes, BCE","upvote_count":"1","timestamp":"1632549180.0"},{"poster":"wasabidev","content":"BCE for me","timestamp":"1632426360.0","comment_id":"313903","upvote_count":"1"},{"timestamp":"1632302700.0","comment_id":"313658","content":"Agreed BCE is correct","upvote_count":"1","poster":"nitinz"}],"answer_ET":"BCE","question_images":[],"answers_community":["BCE (100%)"],"answer_images":[],"timestamp":"2021-03-12 02:59:00","unix_timestamp":1615514340,"choices":{"E":"Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.","C":"Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.","D":"Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.","B":"Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.","A":"Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.","F":"Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources."},"question_id":633,"question_text":"A solutions architect needs to define a reference architecture for a solution for three-tier applications with web, application, and NoSQL data layers. The reference architecture must meet the following requirements:\n✑ High availability within an AWS Region\n✑ Able to fail over in 1 minute to another AWS Region for disaster recovery\n✑ Provide the most efficient solution while minimizing the impact on the user experience\nWhich combination of steps will meet these requirements? (Choose three.)"},{"id":"ltpoCPmkaA2A7ptxtL2O","question_images":[],"answers_community":["B (100%)"],"exam_id":32,"answer_images":[],"answer_ET":"B","timestamp":"2020-11-05 11:21:00","discussion":[{"timestamp":"1632892020.0","poster":"XRiddlerX","content":"Answer is B\nA is incorrect because shutting down the application over the weekend will cause downtime to the application.\nC is incorrect because you can't restore a SQL Server snapshot to Aurora MySQL. They are two very different DBS engines\nD is incorrect cause you can restore a native MSSQL backup to a Aurora MySQL because they are two different DBS engines and I'm not aware of restoring backup file functionality in DMS.\n\nB is correct because since your have used the AWS SCT all you need to do for this migration is migrate the existing data and keep replication going until cutover.","comments":[{"upvote_count":"3","comment_id":"237800","content":"https://docs.aws.amazon.com/dms/latest/sbs/CHAP_SQLServer2Aurora.Steps.html","poster":"jackdryan","timestamp":"1633470720.0"},{"upvote_count":"1","content":"To transfer 1TB data over 1 Gbps ; it will take only 2 and 26 mins, is that not acceptable as this statement in the question? \nThe company would like to migrate data to Aurora MySQL and perform reconfigurations with minimal downtime to the applications.","timestamp":"1633508400.0","comments":[{"upvote_count":"2","poster":"StelSen","content":"Read this in Option-B -> \"migrate existing data and ONGOING Replication....\". So, initially it will take 2.5 hrs and afterwards it will be almost realtime sync. So, literally no downtime or may be 5 mins to just to finish last replication.","timestamp":"1636259700.0","comment_id":"458683"}],"comment_id":"244321","poster":"GopiSivanathan"}],"upvote_count":"36","comment_id":"218475"},{"content":"Selected Answer: B\nB. Create an AWS DMS replication instance and task to migrate existing data and ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.","timestamp":"1688235420.0","upvote_count":"1","poster":"SkyZeroZx","comment_id":"940259"},{"comment_id":"495829","content":"B. Create an AWS DMS replication instance and task to migrate existing data and ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.","timestamp":"1638869400.0","upvote_count":"1","poster":"cldy"},{"timestamp":"1638404340.0","upvote_count":"1","content":"B is right","poster":"AzureDP900","comment_id":"492030"},{"upvote_count":"1","content":"It's B","timestamp":"1636042440.0","comment_id":"444012","poster":"andylogan"},{"content":"BBBBBBBBBBBB\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-sqlserver2aurora.steps.html","comment_id":"435880","poster":"Suresh108","upvote_count":"2","timestamp":"1635782940.0"},{"content":"BBB\n---","comment_id":"434888","upvote_count":"1","poster":"tgv","timestamp":"1635581700.0"},{"timestamp":"1635340980.0","comment_id":"413363","poster":"WhyIronMan","upvote_count":"1","content":"I'll go with B"},{"upvote_count":"2","timestamp":"1635191580.0","comment_id":"356854","content":"it's B","poster":"Waiweng"},{"upvote_count":"1","content":"should be B","timestamp":"1634752140.0","comment_id":"321718","poster":"alisyech"},{"poster":"Kian1","timestamp":"1634733420.0","comment_id":"293834","upvote_count":"2","content":"going with B"},{"upvote_count":"1","content":"B\nLOL you can do B with 1-2 hours of total outage time MAX (if things go very VERY poorly), and schedule that time during extremely low usage periods or scheduled outage (if you have an uptime requirement it won't affect your SLA). \"A\" requires a full weekend of downtime. Terrible, terrible solution. See XRiddlerX's answer for why the other two options are garbage.","timestamp":"1634572260.0","poster":"Trap_D0_r","comment_id":"283589"},{"upvote_count":"1","timestamp":"1634556420.0","content":"I've changed my idea to B. A is not wrong because it's doable but reading carefully this article https://docs.aws.amazon.com/dms/latest/sbs/CHAP_SQLServer2Aurora.Steps.htm, it seems that B should be the right answer.","poster":"Superomam","comment_id":"272694"},{"comment_id":"263762","upvote_count":"1","poster":"Superomam","timestamp":"1633911840.0","comments":[{"upvote_count":"1","timestamp":"1634905200.0","poster":"sarah_t","comment_id":"334463","content":"you can use DMS for ongoing replication until you cut over"},{"comment_id":"427632","timestamp":"1635361140.0","upvote_count":"4","poster":"pablobairat","content":"Your company is lucky for having you ;) It is B"}],"content":"A. I'm currently working into the \"migration arena\" and every time a DB must be migrated, you've to stop the application to avoid writing to the DB."},{"poster":"Ebi","comment_id":"258990","upvote_count":"4","timestamp":"1633703760.0","content":"B for sure,"},{"poster":"Bulti","content":"B is the right answer. Not C because you cannot use DMS to import a SQL Server snapshot into the Aurora SQL DB.","comment_id":"256400","timestamp":"1633683480.0","upvote_count":"2"},{"poster":"petebear55","comment_id":"255902","content":"Ive seen these questions before and the answer is always A ... don't think the question is written very well. if it mentions the app needs to run 24/7 then of course i would not choose A. but given experience with similar ? i will choose A","timestamp":"1633566720.0","upvote_count":"1"},{"timestamp":"1633485960.0","poster":"T14102020","comment_id":"241085","content":"For sure B","upvote_count":"1"},{"poster":"jackdryan","upvote_count":"2","comment_id":"232671","content":"I'll go with B","timestamp":"1633158060.0"},{"content":"D seems to be correct to me","upvote_count":"1","poster":"AK2020","timestamp":"1632652260.0","comment_id":"218295"},{"timestamp":"1632594120.0","comment_id":"213362","upvote_count":"1","poster":"liono","content":"Typo, it is D"},{"content":"After rethinking, I choose C to be more accurate","poster":"liono","comment_id":"213360","timestamp":"1632362160.0","upvote_count":"1"},{"comment_id":"213334","upvote_count":"2","timestamp":"1632292920.0","content":"B\nhttps://docs.aws.amazon.com/dms/latest/sbs/CHAP_SQLServer2Aurora.Steps.CreateReplicationInstance.html","poster":"liono"}],"question_text":"A company has a Microsoft SQL Server database in its data center and plans to migrate data to Amazon Aurora MySQL. The company has already used the AWS\nSchema Conversion Tool to migrate triggers, stored procedures and other schema objects to Aurora MySQL. The database contains 1 TB of data and grows less than 1 MB per day. The company's data center is connected to AWS through a dedicated 1Gbps AWS Direct Connect connection.\nThe company would like to migrate data to Aurora MySQL and perform reconfigurations with minimal downtime to the applications.\nWhich solution meets the company's requirements?","question_id":634,"choices":{"A":"Shut down applications over the weekend. Create an AWS DMS replication instance and task to migrate existing data from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.","C":"Create a database snapshot of SQL Server on Amazon S3. Restore the database snapshot from Amazon S3 to Aurora MySQL. Create an AWS DMS replication instance and task for ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.","D":"Create a SQL Server native backup file on Amazon S3. Create an AWS DMS replication instance and task to restore the SQL Server backup file to Aurora MySQL. Create another AWS DMS task for ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.","B":"Create an AWS DMS replication instance and task to migrate existing data and ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint."},"answer":"B","isMC":true,"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/36140-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1604571660},{"id":"AiyDhgj1gSkRdSXMvDaS","timestamp":"2020-11-08 20:31:00","question_id":635,"answer_ET":"C","answer_images":[],"discussion":[{"upvote_count":"16","content":"Definitely C https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html","comments":[{"upvote_count":"1","poster":"Kelvin1477","timestamp":"1632582480.0","comment_id":"225428","content":"Looks like it: 100 GiB gp2 volume has a baseline performance of 300 IOPS."},{"content":"agree it is C","upvote_count":"1","timestamp":"1653407760.0","poster":"user0001","comment_id":"606793"},{"comment_id":"407723","timestamp":"1634994720.0","upvote_count":"6","poster":"Kopa","content":"key word \"A Solutions Architect is tasked with lowering costs without impacting performance and durability\" so it's B, cheaper and it fullfills the perfomance requirements."},{"upvote_count":"5","comment_id":"225436","poster":"Kelvin1477","content":"But need to consider also ec2 fleet instances, 1 ec2 can tied to only 1 ebs volume","timestamp":"1632897180.0"},{"content":"Answer is B as per tutorialdojo","poster":"Appukk","comment_id":"700105","timestamp":"1666282200.0","upvote_count":"2"}],"timestamp":"1632125700.0","comment_id":"215489","poster":"Gmail78"},{"timestamp":"1632328860.0","comment_id":"215603","comments":[{"upvote_count":"4","poster":"keos","comment_id":"224937","timestamp":"1632492240.0","content":"should be C, cheaper than provisioned\n\nA,B is misleading"}],"upvote_count":"15","poster":"keos","content":"B likely"},{"timestamp":"1669771020.0","poster":"SureNot","content":"Selected Answer: C\nC the cheapest","comment_id":"730978","upvote_count":"2"},{"timestamp":"1669409580.0","poster":"timmysixstrings","comment_id":"727076","content":"Selected Answer: A\nLaunch templates are immutable, so C and D are out. Question never mentions throughput, so B is out. The remaining answer is A. \n\nThe question also never mentions if the storage can be shared or not. I chose A because I eliminated the other answers but overall I think this question is poorly written.","upvote_count":"2"},{"content":"gp2 is designed to offer single-digit millisecond latency, deliver a consistent baseline performance of 3 IOPS/GB (minimum 100 IOPS) to a maximum of 16,000 IOPS, and provide up to 250 MB/s of throughput per volume. gp2 volumes smaller than 1 TB can also burst up to 3,000 IOPS.\n\nThe right ans is C","upvote_count":"1","timestamp":"1668079200.0","poster":"Heer","comment_id":"715174"},{"content":"Lets say there are 1000 EC2 instances and we want to save cost.\nThe choice is between B and C:\n1000 instances with 1TB of GP2 data is: 10K a month\n1000 instances with 100GB of io1 with 3000 iops is: 20.7K a month\nSo C is valid.\nNow regarding EFS its not clear by the question if the data can be shared between the instances or its unique per instance - if it can be shared 100 GB of EFS is cheaper\nif we need 100gb*1000 = 100tb C is cheaper.\nSince the questions does not state that data can be shared between instance i will have to choose C","upvote_count":"4","poster":"Enigmaaaaaa","timestamp":"1659269280.0","comment_id":"640098"},{"poster":"kangtamo","content":"Selected Answer: C\nGo with C.","upvote_count":"1","timestamp":"1654649400.0","comment_id":"612993"},{"poster":"AzureDP900","timestamp":"1638404520.0","comment_id":"492031","upvote_count":"2","content":"General Purpose SSD, It is typo in question. I will go with C."},{"upvote_count":"4","content":"The key consideration here is that the company is paying for individual disks for every EC2 instance in their fleet. The question translates to: Is it cheaper to provision that on an individual disk basis for every EC2 instance in the fleet using a different disk approach, or is it cheaper to provision a shared EFS volume and mount it on every instance in the fleet?\n\nUltimately this question comes down to:\n- how much EFS data throughput is your app going to need (which you would need to multiply by the number of servers accessing the EFS filesystem)\n- how many servers are in your fleet?\n\nAnd we are not told how many servers are in the fleet, nor the throughput needed based on the application's average block size per operation. Both are critical factors in making this decision. I'm not a fan of this question due to that missing info.\n\nWithout that crucial info, I'm just going to default to keeping things the way they're doing today with individual disks on each instance, and save on cost by going with gp2, but that's really answering the question at all.","timestamp":"1636120080.0","comment_id":"463257","poster":"kirrim"},{"poster":"andylogan","content":"It's C - 1-TB EBS General Purpose SSO (gp2)","timestamp":"1636015140.0","comment_id":"444024","upvote_count":"3"},{"timestamp":"1635985860.0","poster":"tgv","content":"CCC\n---","comment_id":"434894","upvote_count":"2"},{"upvote_count":"1","timestamp":"1635658320.0","comment_id":"433740","content":"B is the answer. It can’t be A because of the unnecessary cost for max io.","poster":"blackgamer"},{"upvote_count":"2","comment_id":"422929","timestamp":"1635584760.0","poster":"saggarwal4114","content":"It is B"},{"content":"\"Launch templates are immutable. To modify a launch template, you must create a new version of the launch template.\" - there is no option to update the launch templates (C&D). Provisioned for EFS is referring to the throughput (MiB/s), for which we do not have any info. The remaining option is A - MaxIO","comments":[{"poster":"tkanmani76","comment_id":"508913","timestamp":"1640392620.0","upvote_count":"2","content":"Agree on the launch templates - C and D are incorrect. The below passage sounds A is not the right choice. \nSome latency-sensitive workloads require the higher I/O levels provided by Max I/O performance mode and the lower latency provided by General Purpose performance mode. For this type of workload, we recommend creating multiple General Purpose performance mode file systems. \nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\nHence going with B."}],"poster":"mericov","timestamp":"1635287040.0","upvote_count":"5","comment_id":"414500"},{"comment_id":"413365","timestamp":"1635275520.0","poster":"WhyIronMan","comments":[{"content":"C- because of this exact reason","timestamp":"1636187520.0","poster":"AWSum1","upvote_count":"2","comment_id":"470863"},{"timestamp":"1635827820.0","content":"https://aws.amazon.com/ebs/general-purpose/\nI think GP might do the required 3000 iops","upvote_count":"3","comment_id":"434518","poster":"somebodyelse"}],"upvote_count":"9","content":"I'll go with C\n\nGuys, please READ the question!!!!!\n\nThe questions never said that's a single volume mounted/shared across instances!!!!\n\nSo, instead of having\n- 1 x 100 Gb 3000 PIOPs PER INSTANCE is cheaper to have\n- 1 x 1000 Gb (1000 x 3 iops) PER INSTANCE\n\nIf you guys don't read the question at least twice it'll be difficult to go well in the exam"},{"comment_id":"402332","timestamp":"1634973240.0","content":"It is C\n\nEFS is not correct for this random access requirement, so rule out A/B","upvote_count":"1","poster":"qurren"},{"timestamp":"1634906940.0","poster":"XAvenger","comment_id":"380635","upvote_count":"1","content":"There are concerns related to the EBS volume attached to multiple EC2 instances (how are they going to use single volume for multiple EC2 instances??) If they are going to user multiple EBS volumes then EFS looks cheaper.\nBUT I tried to find any information related to EFS IOPS. About EFS throughput - there is much information, but not about EFS IOPS.\n\nI would choose C despite the fact the option is weird."},{"comment_id":"376682","content":"The Answer is C. with A (File systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second. This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode.)","poster":"zolthar_z","timestamp":"1634734860.0","upvote_count":"1"},{"poster":"tonywang806","comment_id":"368565","content":"B is incorrect.\nBecause IOPS is a unit about performance mode, not about throughput mode.","upvote_count":"4","timestamp":"1634694660.0"},{"content":"it's B","poster":"Waiweng","comment_id":"356859","upvote_count":"8","timestamp":"1634668380.0"},{"content":"B is correct EFS file system with the throughput mode set to Provisioned\n\nA File systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second. However, this scaling is done with a tradeoff of slightly higher latencies for file metadata operations.\n\nB Use the Provisioned Throughput mode on the EFS volume to ensure that the application can reach the required IOPS.\n\nC Although this may look cheaper at first, creating several 1TB volumes for each EC2 instance entails higher costs. The Amazon EFS volume solution will be cheaper for sharing storage across all EC2 instances. Although you can use EBS Multi-Attach to attach EBS volumes to multiple EC2 instances, this is limited only to Provisioned IOPS SSD (io1 or io2) volumes that are attached to Nitro-based EC2 instances in the same Availability Zone.\n\nD Instance local instance storage is ephemeral which means that you will lose all data in the volume when you stop/start the instance. This is not recommended for this mission-critical application.","poster":"ExtHo","timestamp":"1634634180.0","upvote_count":"8","comment_id":"328096"},{"upvote_count":"1","comment_id":"321720","content":"im going with C","poster":"alisyech","timestamp":"1634613240.0"},{"upvote_count":"2","comment_id":"298215","poster":"gpark","timestamp":"1634234880.0","content":"B\n---\nRule out C&D -> Durability and for supporting multiple instances.\nRule out A -> Max I/O has relatively higher latency than general perpose Performance Mode or Throughput Mode."},{"poster":"lechuk","upvote_count":"3","content":"A:\nEBS can not be mounted to a fleet of instances","comment_id":"290397","timestamp":"1633812180.0"},{"poster":"Ebi","comment_id":"284564","content":"C for sure","upvote_count":"3","timestamp":"1633789740.0"},{"upvote_count":"5","timestamp":"1633777080.0","content":"Definitely B\nEFS Supports up to 3500 Read actions or 3000 write actions (IOPS) out of the box (https://blog.famzah.net/2018/04/29/amazon-efs-benchmarks/), therefore the answer MUST be EFS. The question also deliberately doesn't tell you the SIZE of the fleet you're supporting--is it 5 instances or is it 5000? Doesn't really matter, EFS supports THOUSANDS of systems at 3000 IOPS and you can assume from the question that the fleet size is highly variable. Provisioned mode makes sure you never hit a throughput ceiling when your fleet spikes, and Max i/o is NOT NEEDED since max i/o increases your IOPS not your throughput (https://aws.amazon.com/premiumsupport/knowledge-center/linux-efs-performance-modes/).\nCommon sense says the answer is B.","comment_id":"283595","poster":"Trap_D0_r"},{"upvote_count":"5","comment_id":"270285","poster":"01037","content":"I prefer b.\n\nUse \nhttps://calculator.aws/#/\nto do some calculations.\nEBS 1TB = 100$/month\nEFS 100GB with provisioned throughput of 50MiB/month = 270$/month.\nI assume size per I/O operation of EFS is 16KiB, though I can't find any reference about it.\n\nThen with three instances EBS will be expensive than EFS.","comments":[{"upvote_count":"1","timestamp":"1633707600.0","comment_id":"270288","content":"50MiB/s per month","poster":"01037"},{"upvote_count":"1","content":"3000 * 16KiB / 1024 is about 5MiB/s","comment_id":"270292","poster":"01037","timestamp":"1633744320.0"},{"upvote_count":"1","poster":"certainly","comment_id":"304919","content":"Agree with the calculation. B is easily a cheaper option. https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system","timestamp":"1634422560.0"}],"timestamp":"1633688880.0"},{"timestamp":"1633543620.0","poster":"Bulti","content":"I think the answer is C. The choice is between A and C because the dominant factor is IOPS and not Throughput. However I think A will not be as cheap as C and C will satisfy the requirement of 3000 IOPS where its max IOPS is 16000 and is .025 per GB/month cheaper than the Provisioned IOPS SSD.","comment_id":"256426","upvote_count":"2"},{"poster":"darthvoodoo","comment_id":"245066","comments":[{"poster":"Bulti","content":"See my cost comparison below between general purposed SSD and provisioned IOPS SSD. It is .25 per GB/month cheaper","comment_id":"263472","timestamp":"1633610820.0","upvote_count":"1"},{"comment_id":"380631","upvote_count":"1","poster":"XAvenger","content":"Interesting question. On the other hand you can read that similar approach is already implemented: \"A Development team has configured the EC2 launch template to provision a 100-GB Provisioned IOPS\n(PIOPS) Amazon EBS volume with 3 000 IOPS provisioned.\"\nSo developers have already implemented a solution (I don't know how) to use EBS volume across a fleet of EC2 instances.","timestamp":"1634873940.0"}],"timestamp":"1633452180.0","content":"Explain to me how you plan to mount a 1TB EBS volume across a fleet of EC2 instances. You can of course create one for each and somehow make sure the data is synced across the nodes but it would defeat the purpose of lowering the cost. \"A\" would be the way to go as the instance would need \"random\" access to data.","upvote_count":"3"},{"content":"Correct is C. 1TB * 3 IOPS per GB = 3000 IOPS","upvote_count":"2","comment_id":"241103","poster":"T14102020","timestamp":"1633350540.0"},{"timestamp":"1633320180.0","poster":"PAUGURU","comment_id":"238560","content":"C - EFS does never state IOPS parameters but goes for MB/sec being a NAS. For IOPS you need EBS","upvote_count":"1"},{"comment_id":"232798","upvote_count":"4","content":"A - https://docs.aws.amazon.com/efs/latest/ug/performance.html\nEFS Max I/O Performance Mode\nFile systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second.","poster":"taoteching1","comments":[{"upvote_count":"1","comment_id":"298212","timestamp":"1634115720.0","content":"Max I/O has higher latency than provisioned gp or throughput Mode.","poster":"gpark"}],"timestamp":"1633276440.0"},{"poster":"jackdryan","upvote_count":"3","timestamp":"1633059600.0","comment_id":"232674","content":"I'll go with C"}],"answers_community":["C (60%)","A (40%)"],"exam_id":32,"question_text":"A company runs an application on a fleet of Amazon EC2 instances. The application requires low latency and random access to 100 GB of data. The application must be able to access the data at up to 3.000 IOPS. A Development team has configured the EC2 launch template to provision a 100-GB Provisioned IOPS\n(PIOPS) Amazon EBS volume with 3 000 IOPS provisioned. A Solutions Architect is tasked with lowering costs without impacting performance and durability.\nWhich action should be taken?","topic":"1","question_images":[],"answer_description":"","unix_timestamp":1604863860,"choices":{"D":"Update the EC2 launch template to exclude the PIOPS volume. Configure the application to use local instance storage.","C":"Update the EC2 launch template to allocate a new 1-TB EBS General Purpose SSO (gp2) volume.","B":"Create an Amazon EFS file system with the throughput mode set to Provisioned. Configure the EC2 operating system to mount the EFS file system.","A":"Create an Amazon EFS file system with the performance mode set to Max I/O. Configure the EC2 operating system to mount the EFS file system."},"isMC":true,"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/36496-exam-aws-certified-solutions-architect-professional-topic-1/"}],"exam":{"id":32,"provider":"Amazon","lastUpdated":"11 Apr 2025","isMCOnly":false,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Professional","isBeta":false,"isImplemented":true},"currentPage":127},"__N_SSP":true}