{"pageProps":{"questions":[{"id":"NsHbS8CmdzfHP6bXkhr7","answers_community":["C (100%)"],"discussion":[{"comment_id":"346536","poster":"Jaypdv","timestamp":"1633775460.0","content":"C.\nAdding the organization as a principal ensure that current and future accounts will have access to the share. The question mentions that there will be many new accounts, that's the clue","upvote_count":"22"},{"comment_id":"714949","upvote_count":"2","poster":"janvandermerwer","content":"Selected Answer: C\nC appears to be the most operationally efficient.","timestamp":"1668059460.0"},{"content":"Selected Answer: C\nWe have to add organization as principle due to \"swift increase in account num\".: So it's C or D.\nThere's no reason to share a VPC, so it's C.","poster":"bobsmith2000","comments":[{"timestamp":"1665264540.0","upvote_count":"1","comment_id":"689687","content":"You can't share VPC anyways in RAM..only subnets","poster":"[Removed]"}],"timestamp":"1652856480.0","comment_id":"603183","upvote_count":"3"},{"upvote_count":"1","timestamp":"1639030020.0","comment_id":"497374","poster":"cldy","content":"C. Add all subnets within the VPC to the resource share. Add the organization as a principal"},{"content":"I will go with C as mentioned @https://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html#getting-started-sharing-create To restrict resource sharing to only principals in your organization, choose Allow sharing with principals in your organization only.","upvote_count":"1","comment_id":"497211","poster":"AzureDP900","timestamp":"1639002300.0"},{"timestamp":"1637460660.0","comment_id":"482981","content":"How to get access to questions from 390 onwards. I can only see questions upto 389. Please suggest","poster":"pcops","upvote_count":"2","comments":[{"upvote_count":"4","timestamp":"1637629440.0","poster":"acloudguru","content":"use incognito mode directly with the url","comment_id":"484672","comments":[{"comment_id":"695378","timestamp":"1665836340.0","upvote_count":"2","content":"and a Cloudfront distribution with OAI configured","poster":"wassb"}]}]},{"upvote_count":"1","content":"It's C","timestamp":"1636061100.0","poster":"andylogan","comment_id":"448665"},{"upvote_count":"1","poster":"tgv","comment_id":"434837","timestamp":"1635906840.0","content":"CCC\n---"},{"poster":"WhyIronMan","timestamp":"1635873420.0","content":"I'll go with C","upvote_count":"1","comment_id":"414218"},{"comments":[{"timestamp":"1635874380.0","poster":"sergioandreslq","comment_id":"434051","content":"This questing is the 760, Answer is ACF.","upvote_count":"1"},{"timestamp":"1635574800.0","content":"Answer is A C F","poster":"Chibuzo1","upvote_count":"3","comment_id":"405479"},{"content":"looks like ACE","timestamp":"1635497340.0","comment_id":"401607","upvote_count":"2","poster":"pradhyumna"}],"comment_id":"398542","content":"New Solutions Architect Pro question. \n\nA company runs an application in the cloud that consists of a database and a website. Users can post data to the website, have the data processed, and have the data sent back to them in an email, Data is stored in a MySQL database running on an Amazon EC2 instance. The database is running with two private subnets. The website is running on Apache Tomcat in a single EC2 instance in a different VPC with one public subnet. There is a single VPC peering connection between the database and website VPC. \n\nThe website has suffered several outages during the last month due to high traffic. \n\nWhich actions should a solutions architect take to increase the reliability of the application? (select three)\n\nA – Place the Tomcat server in an Autoscaling group with multiple EC2 instances behind an Application Load Balancer\n\nB – Provision an additional VPC peering connection \n\nC – Migrate the MySQL database to Amazon Aurora with one Aurora Replica \n\nD – Provision two NAT gateways in the database VPC \n\nE – Move the tomcat server to the database VPC\n\nF – Create an additional public subnet in a different Availability Zone in the website VPC","timestamp":"1635454800.0","upvote_count":"3","poster":"Akbarali"},{"content":"it's C\nhttps://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html#getting-started-sharing-create","upvote_count":"2","timestamp":"1635229200.0","comment_id":"384605","poster":"Waiweng"},{"poster":"Chibuzo1","timestamp":"1634685180.0","upvote_count":"2","comment_id":"384369","content":"To specify a principal from the list, for each principal, select the principal type, enter the ID or ARN, \nAWS account: To add an AWS account, enter the 12-digit account ID. For example:\n123456789012\nOrganization: To add your entire organization, enter the ID of the organization. For example:\no-abcd1234efgh5678\nThe right answer is B. To add organization, you add the ID, and to add an Account you add the ID."},{"poster":"beebatov","content":"Answer: C\n\nYou share the resources of the VPC which are Subnets in this case + add Organization as the principal as the number of accounts will grow in future.\n\nhttps://docs.aws.amazon.com/ram/latest/userguide/ram-ug.pdf","timestamp":"1634365440.0","upvote_count":"4","comment_id":"352322"},{"comment_id":"346229","comments":[{"comment_id":"347722","upvote_count":"7","poster":"Jaypdv","timestamp":"1634020980.0","content":"C. Is better, you can add the entire organization as principal. This covers all existing and future accounts as per the question. See https://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html#getting-started-sharing-create"}],"content":"i think B is correct see the link","timestamp":"1632716460.0","poster":"gsw","upvote_count":"1"}],"isMC":true,"answer_ET":"C","exam_id":32,"question_text":"A company has a policy that all Amazon EC2 instances that are running a database must exist within the same subnets in a shared VPC. Administrators must follow security compliance requirements and are not allowed to directly log in to the shared account. All company accounts are members of the same organization in AWS Organizations. The number of accounts will rapidly increase as the company grows.\nA solutions architect uses AWS Resource Access Manager to create a resource share in the shared account.\nWhat is the MOST operationally efficient configuration to meet these requirements?","question_images":[],"choices":{"B":"Add all subnets within the VPC to the resource share. Add the account IDs as principals","C":"Add all subnets within the VPC to the resource share. Add the organization as a principal","A":"Add the VPC to the resource share. Add the account IDs as principals","D":"Add the VPC to the resource share. Add the organization as a principal"},"answer_images":[],"answer_description":"","unix_timestamp":1619789940,"question_id":741,"topic":"1","timestamp":"2021-04-30 15:39:00","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/51264-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"0hoTUNsxJbfXopBqLMrg","answer_description":"","unix_timestamp":1619790180,"isMC":true,"answer_images":[],"question_images":[],"exam_id":32,"discussion":[{"comment_id":"346537","content":"BD.\nSeems natural to me","upvote_count":"17","timestamp":"1632190620.0","poster":"Jaypdv"},{"upvote_count":"7","content":"I'll go with B,D\n\nhttps://aws.amazon.com/documentdb/?nc1=h_ls\n\nhttps://aws.amazon.com/blogs/containers/using-alb-ingress-controller-with-amazon-eks-on-fargate/","comment_id":"414220","timestamp":"1634294640.0","poster":"WhyIronMan"},{"timestamp":"1642693200.0","comment_id":"528554","content":"Selected Answer: BD\nBD for sure","poster":"kubala","upvote_count":"2"},{"content":"B,D is my choice. MongoDB is compatible with DocumentDB and Containers can be hosted on Fargate","poster":"AzureDP900","timestamp":"1639002660.0","upvote_count":"2","comment_id":"497213"},{"upvote_count":"1","comment_id":"448668","content":"It's B D","timestamp":"1635710940.0","poster":"andylogan"},{"timestamp":"1635275040.0","content":"B,D\nMinimum code changes","comment_id":"440181","poster":"student22","upvote_count":"1"},{"poster":"tgv","content":"BBB DDD\n----","upvote_count":"1","timestamp":"1634977680.0","comment_id":"434838"},{"upvote_count":"2","content":"B and D","comment_id":"434524","poster":"blackgamer","timestamp":"1634977380.0"},{"comment_id":"400003","upvote_count":"1","timestamp":"1634150880.0","content":"It’s BD","poster":"vimgoru24"},{"comment_id":"390737","poster":"nik_aws","timestamp":"1633452660.0","content":"Given that Lambda now supports containers and it specifically says the containers are stateless, option A also seems good.","comments":[{"comment_id":"400002","content":"On paper - yes. But if you’d really try to convert a regular web app to a lambda compatible image - you’d see that this option still far away from “little code changes” :)","poster":"vimgoru24","upvote_count":"1","timestamp":"1633560240.0"}],"upvote_count":"1"},{"upvote_count":"1","comment_id":"385788","poster":"hk436","timestamp":"1633280640.0","content":"BD is my answer!!"},{"poster":"glahitette","upvote_count":"1","content":"BD for me too","comment_id":"384910","timestamp":"1633056900.0"},{"timestamp":"1633041060.0","content":"BD for sure","poster":"mustpassla","upvote_count":"1","comment_id":"368527"},{"content":"it's B,D","upvote_count":"4","timestamp":"1632582540.0","poster":"Waiweng","comment_id":"360399"},{"timestamp":"1632374880.0","upvote_count":"2","content":"Answer: BD","comment_id":"352323","poster":"beebatov"},{"timestamp":"1632095880.0","poster":"gsw","upvote_count":"1","comment_id":"346234","content":"if they meant lambda layers then its poorly expressed"}],"answer_ET":"AE","topic":"1","timestamp":"2021-04-30 15:43:00","choices":{"A":"Create a REST API in Amazon API Gateway and use AWS Lambda functions as the application layer","C":"Migrate the storage layer to Amazon DynamoDB","E":"Create an Application Load Balancer and move the storage layer to an EC2 Auto Scaling group","B":"Create an Application Load Balancer and migrate the Docker container to AWS Fargate","D":"Migrate the storage layer to Amazon DocumentDB (with MongoDB compatibility)"},"url":"https://www.examtopics.com/discussions/amazon/view/51265-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A solutions architect is evaluating the reliability of a recently migrated application running on AWS. The front end is hosted on Amazon S3 and accelerated by\nAmazon CloudFront. The application layer is running in a stateless Docker container on an Amazon EC2 On-Demand Instance with an Elastic IP address. The storage layer is a MongoDB database running on an EC2 Reserved Instance in the same Availability Zone as the application layer.\nWhich combination of steps should the solutions architect take to eliminate single points of failure with minimal application code changes? (Choose two.)","answers_community":["BD (100%)"],"question_id":742,"answer":"BD"},{"id":"BnwKBURtuDQYkZHR3FHH","choices":{"B":"Mange encryption keys in a Hardware Security Module (HSM) appliance on-premises serve r with sufficient storage to temporarily store, encrypt, and upload files directly into Amazon Glacier.","D":"Manage encryption keys in an AWS CloudHSM appliance. Encrypt files prior to uploading on the employee desktop, and then upload directly into Amazon Glacier.","A":"Manage encryption keys on-premises in an encrypted relational database. Set up an on-premises server with sufficient storage to temporarily store files, and then upload them to Amazon S3, providing a client-side master key.","C":"Manage encryption keys in Amazon Key Management Service (KMS), upload to Amazon Simple Storage Service (S3) with client-side encryption using a KMS customer master key ID, and configure Amazon S3 lifecycle policies to store each object using the Amazon Glacier storage tier."},"exam_id":32,"discussion":[{"content":"C.\n Amazon S3 Encryption Client + KMS\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html","comment_id":"327340","timestamp":"1634240280.0","upvote_count":"7","poster":"cldy"},{"upvote_count":"1","poster":"amministrazione","content":"C. Manage encryption keys in Amazon Key Management Service (KMS), upload to Amazon Simple Storage Service (S3) with client-side encryption using a KMS customer master key ID, and configure Amazon S3 lifecycle policies to store each object using the Amazon Glacier storage tier.","comment_id":"1266695","timestamp":"1723755720.0"},{"comment_id":"967707","upvote_count":"1","poster":"TravelKo","timestamp":"1690772040.0","content":"Selected Answer: B\nI will go with B, encrypt the file before sending those to cloud."},{"comment_id":"754356","upvote_count":"2","content":"Selected Answer: C\nKey words: \"thousands of employees\", \"archiving\", \"encrypted before being uploaded\", \"highly available\", and \"cost-efficient\".\nA. not good, encryption in the cloud.\nB. Doable by on-premises HSM is not cheap.\nC. Beter than B, reduce the key management cost. The encryption is on client side with the key managed in KMS (https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html)\nD. ClouldHSM is more expensive than KMS. Managing load to Glacier directly is not more complex than utilizing the S3 life-cycle to the work. Also granting a user access to his/her own document is easy with S3.","timestamp":"1671813480.0","poster":"TigerInTheCloud"},{"poster":"evgeng","comments":[{"content":"C is Client side encryption, which means files are encrypted in client side before uploaded to S3. Your link to ServerSideEncryptionCustomerKeys is different thing.","timestamp":"1692569640.0","upvote_count":"1","comment_id":"986093","poster":"jack_melvin"}],"content":"How could it be C?\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html\nThe encryption is happening on the Cloud... \nAnd it does not matter that it is encrypted in transit as it is encrypted by TLS over HTTP. However, it appear unencrypted on S3 before it is encrypted with client provided key...\nhttps://awsinfographics.s3.amazonaws.com/S3_Encryption_Infographic.png","comment_id":"717461","timestamp":"1668365220.0","upvote_count":"1"},{"timestamp":"1643230020.0","poster":"AMKazi","content":"C: client side encryption will ensure data is encrypted in transit as well","upvote_count":"2","comment_id":"533269"},{"upvote_count":"1","timestamp":"1640943240.0","content":"C: Amazon S3 Encryption Client + KMS","poster":"cldy","comment_id":"513991"},{"upvote_count":"1","timestamp":"1640173140.0","poster":"Ni_yot","content":"C for me. AWS KMS and Client side Enc","comment_id":"507038"},{"timestamp":"1635590640.0","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html","upvote_count":"1","poster":"nwk","comment_id":"429784"},{"comment_id":"361691","content":"Yes C\nGlacier isn't necessary.","comments":[{"timestamp":"1648558500.0","upvote_count":"1","content":"C is the answer but not because Glacier isn't necessary.\n\nThe objects need to go through S3 first for client-side encryption before they are moved the Glacier. That's why C is the answer.","comment_id":"577606","poster":"Jonfernz"}],"poster":"01037","timestamp":"1634569740.0","upvote_count":"1"},{"comment_id":"314420","timestamp":"1632609960.0","poster":"TaherShaker","content":"You can use the Amazon S3 Encryption Client in the AWS SDK in your own application to encrypt objects and upload them to Amazon S3. This method allows you to encrypt your data locally to ensure its security as it passes to the Amazon S3 service. The Amazon S3 service receives your encrypted data; it does not play a role in encrypting or decrypting it.\n\nThe Amazon S3 Encryption Client encrypts the object by using envelope encryption. The client calls AWS KMS as a part of the encryption call you make when you pass your data to the client. AWS KMS verifies that you are authorized to use the customer master key (CMK) that you specify and, if so, returns a new plaintext data key and the data key encrypted under the CMK. The Amazon S3 Encryption Client encrypts the data by using the plaintext key and then deletes the key from memory. The encrypted data key is sent to Amazon S3 to store alongside your encrypted data.\n\nReferences:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html","upvote_count":"4"}],"question_images":[],"answer":"C","question_id":743,"answers_community":["C (67%)","B (33%)"],"answer_description":"","question_text":"You are designing a personal document-archiving solution for your global enterprise with thousands of employees. Each employee has potentially gigabytes of data to be backed up in this archiving solution. The solution will be exposed to the employees as an application, where they can just drag and drop their files to the archiving system. Employees can retrieve their archives through a web interface. The corporate network has high bandwidth AWS Direct Connect connectivity to\nAWS.\nYou have a regulatory requirement that all data needs to be encrypted before being uploaded to the cloud.\nHow do you implement this in a highly available and cost-efficient way?","timestamp":"2021-03-18 23:02:00","isMC":true,"topic":"1","unix_timestamp":1616104920,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/47688-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"C"},{"id":"wTlhT9nbxX87W8aRPmZ9","choices":{"A":"Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3.","D":"Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP support on the S3 bucket.","C":"Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the NLB. Store the files in Amazon S3.","B":"Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the ALB. Store the files in attached Amazon Elastic Block Store (Amazon EBS) volumes."},"url":"https://www.examtopics.com/discussions/amazon/view/51690-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","answers_community":["A (100%)"],"topic":"1","unix_timestamp":1620049200,"answer_images":[],"answer":"A","question_id":744,"question_images":[],"exam_id":32,"isMC":true,"question_text":"A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public SFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for outbound traffic. Changes to the\nSFTP endpoint IP addresses are not permitted.\nThe company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.\nWhich solution meets these requirements?","discussion":[{"comment_id":"350976","poster":"miniso8153","comments":[{"content":"A:\nBring your own IP addresses (BYOIP)\nYou can bring part or all of your publicly routable IPv4 or IPv6 address range from your on-premises network to your AWS account. You continue to own the address range, but AWS advertises it on the internet by default. After you bring the address range to AWS, it appears in your AWS account as an address pool.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-byoip.html\n\nAWS Transfer for SFTP enables you to easily move your file transfer workloads that use the Secure Shell File Transfer Protocol (SFTP) to AWS without needing to modify your applications or manage any SFTP servers.\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/aws-transfer-for-sftp-fully-managed-sftp-for-s3/","upvote_count":"5","comment_id":"433880","poster":"sergioandreslq","timestamp":"1635885660.0"}],"upvote_count":"21","content":"I vote for A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/sftp-enable-elastic-ip-addresses/","timestamp":"1632727200.0"},{"comment_id":"947802","timestamp":"1688969820.0","upvote_count":"1","poster":"BasselBuzz","content":"Selected Answer: A\nAWS Transfer for SFTP endpoint"},{"content":"I will go with A after reading the AWS docs mentioned by miniso8153","comment_id":"497214","upvote_count":"1","timestamp":"1639003020.0","poster":"AzureDP900"},{"upvote_count":"1","comment_id":"495975","poster":"cldy","timestamp":"1638880260.0","content":"A. Register the customer-owned block of IP addresses in the companyג€™s AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3."},{"comment_id":"448670","upvote_count":"2","timestamp":"1636024380.0","poster":"andylogan","content":"It's A with AWS Transfer for SFTP"},{"timestamp":"1635901320.0","upvote_count":"1","content":"AAA\n---\nB: You cannot assign an Elastic IP to an Application Load Balancer\nC: You cannot register the customer-owned block of IP addresses with Amazon Route 53\nD: Not sure if you can assign an IP to S3 VPC endpoint","comment_id":"434842","poster":"tgv"},{"timestamp":"1635161640.0","content":"A for sure.\nThe AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3 or Amazon EFS. With support for Secure File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP), the AWS Transfer Family helps you seamlessly migrate your file transfer workflows to AWS by integrating with existing authentication systems, and providing DNS routing with Amazon Route 53 so nothing changes for your customers and partners, or their applications. With your data in Amazon S3 or Amazon EFS, you can use it with AWS services for processing, analytics, machine learning, archiving, as well as home directories and developer tools.","comments":[{"upvote_count":"1","content":"A\nhttps://aws.amazon.com/aws-transfer-family/","poster":"student22","comment_id":"440188","timestamp":"1635989760.0"}],"upvote_count":"4","comment_id":"416173","poster":"Kopa"},{"poster":"WhyIronMan","upvote_count":"1","comment_id":"414221","timestamp":"1634483640.0","content":"I'll go for A"},{"upvote_count":"1","content":"It’s A","poster":"vimgoru24","comment_id":"400006","timestamp":"1634404560.0"},{"comment_id":"388870","timestamp":"1634229060.0","content":"A for sure","poster":"mustpassla","upvote_count":"1"},{"upvote_count":"3","timestamp":"1633051680.0","comment_id":"360402","poster":"Waiweng","content":"it's A"},{"poster":"beebatov","comment_id":"352340","content":"Answer: A\n\nCOIP is registered with an AWS Account (Not Route 53)\n\nhttps://aws.amazon.com/about-aws/whats-new/2020/01/aws-transfer-for-sftp-supports-vpc-security-groups-and-elastic-ip-addresses/","upvote_count":"1","timestamp":"1632994020.0"},{"comment_id":"350455","poster":"ExtHo","content":"C no due to NLB A,D can be valid in first view but D can be ruled out due to Amazon S3 VPC endpoint as the main usage is access to S3 without internet (AWS resources in Private Subnets) that leaves A is only valid option what i think.","upvote_count":"3","timestamp":"1632682440.0"},{"comment_id":"348663","comments":[{"upvote_count":"1","comment_id":"362565","timestamp":"1633603020.0","poster":"digimaniac","content":"watch out for operational overhead cue in the question. host FTP will increase op. overhead"}],"upvote_count":"1","timestamp":"1632396240.0","content":"could be C?","poster":"gsw"}],"timestamp":"2021-05-03 15:40:00","answer_ET":"A"},{"id":"S1Acv1CsNcMEHwnNCDb5","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/59925-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2021-08-19 15:32:00","exam_id":32,"choices":{"C":"Export the VM images to an AWS Snowball Edge Storage Optimized device.","A":"Use an AWS Server Migration Service (AWS SMS) replication job to migrate the database server VM to AWS.","D":"Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS.","E":"Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance.","B":"Use VM Import/Export to import the application server VM."},"question_text":"A company is migrating a legacy application from an on-premises data center to AWS. The application consists of a single application server and a Microsoft SQL\nServer database server. Each server is deployed on a VMware VM that consumes 500 TB of data across multiple attached volumes.\nThe company has established a 10 Gbps AWS Direct Connect connection from the closest AWS Region to its on-premises data center. The Direct Connect connection is not currently in use by other services.\nWhich combination of steps should a solutions architect take to migrate the application with the LEAST amount of downtime? (Choose two.)","answers_community":["DE (60%)","BE (20%)","AD (20%)"],"answer":"DE","answer_ET":"DE","unix_timestamp":1629379920,"answer_description":"","discussion":[{"content":"D & E\n\nA Not correct, database need to be migrated to RDS, not to EC2\nB - AWS Server Migration Service is a significant enhancement of EC2 VM Import. The AWS Server Migration Service provides automated, live incremental server replication and AWS Console support. For customers using EC2 VM Import for migration, we recommend using AWS Server Migration Service.\nC- Lease amount of downtime will not work\nD- is correct SMS for application server\nE - Is correct. DMS for data base","poster":"Jupi","comment_id":"431883","timestamp":"1633825140.0","upvote_count":"11"},{"upvote_count":"1","poster":"mnsait","content":"Selected Answer: AD\nAs Anupamgoyal_aws has pointed out, RDS SQL Server does not support this amount of storage. Max is 64 TB (earlier it was 16 TB). Therefore, the next best option is A.","comment_id":"1327204","timestamp":"1734335580.0"},{"content":"RDS SQL server supports only 16TB max. Cannot be E.","upvote_count":"2","timestamp":"1732250460.0","comment_id":"1316160","poster":"Anupamgoyal_aws"},{"upvote_count":"1","content":"Selected Answer: BE\nConsiderations\nUsually usage case of AWS SMS is for migration Virtual Machine ( VM Ware ) \nUsually usage case of AWS DMS is for migration of Database\n\n A) AWS SMS for Database is incorrect in this case\nB) more overhead\nC) more time than use Direct Connect in this case \nD) Sounds Good SMS for VM to AWS , OK\nE) Sounds Good DMS for Database to AWS in RDS , OK","comment_id":"939279","poster":"SkyZeroZx","timestamp":"1688145360.0","comments":[{"content":"missed take option is DE","upvote_count":"1","poster":"SkyZeroZx","comment_id":"939280","timestamp":"1688145420.0"}]},{"timestamp":"1667656260.0","upvote_count":"1","comment_id":"711790","content":"The question doest mention time of project, but only LEAST amount of downtime. Excluding Snowball, and instead SMS/DMS replication jobs , D/E","poster":"resnef"},{"poster":"examaws","content":"A & D. \nthis is for legacy apps, it may not compatble with RDS, plus RDS MS SQL has limited capacity up to 16TB.","upvote_count":"1","comment_id":"711594","timestamp":"1667632020.0"},{"comment_id":"691309","poster":"redipa","upvote_count":"2","content":"The question says EACH server has 500TB of attached storage. \n\nMaximum storage for RDS SQL Server is 16TB. RDS cannot be part of the solution.","timestamp":"1665416040.0"},{"poster":"kangtamo","content":"Selected Answer: DE\nGo with DE: SMS + DMS","timestamp":"1656359160.0","comment_id":"623475","upvote_count":"3"},{"comment_id":"536241","poster":"timlow84","content":"Product Update: As of March 31, 2022, AWS will discontinue AWS Server Migration Service (AWS SMS). You can initiate new migration jobs using AWS SMS until January 1, 2022. Please complete your active migration projects using AWS SMS by March 31, 2022. Going forward, we recommend AWS Application Migration Service (AWS MGN) as the primary migration service for lift-and-shift migrations.","timestamp":"1643554080.0","upvote_count":"3"},{"upvote_count":"1","timestamp":"1639051980.0","poster":"cldy","content":"D. Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS.\nE. Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance.","comment_id":"497701"},{"poster":"AzureDP900","comment_id":"497218","content":"D, E sounds good to me","upvote_count":"1","timestamp":"1639003200.0"},{"timestamp":"1635715200.0","comment_id":"448682","poster":"andylogan","content":"It's D E","upvote_count":"1"},{"timestamp":"1635201240.0","comment_id":"435407","content":"DDD EEE\n---","upvote_count":"1","poster":"tgv"},{"timestamp":"1633921860.0","content":"D@ and E","upvote_count":"1","poster":"blackgamer","comment_id":"434562"},{"comment_id":"429128","upvote_count":"1","poster":"denccc","content":"D and E","timestamp":"1633517340.0"},{"timestamp":"1632781020.0","comment_id":"427503","poster":"fukuyama","upvote_count":"2","content":"It's DE"},{"content":"It is D and E","upvote_count":"2","poster":"pkboy78","timestamp":"1632540480.0","comment_id":"427488"}],"question_images":[],"question_id":745,"topic":"1","isMC":true}],"exam":{"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isImplemented":true,"isMCOnly":false,"id":32,"provider":"Amazon","isBeta":false},"currentPage":149},"__N_SSP":true}