{"pageProps":{"questions":[{"id":"KyKHVEOqn6mUl8EtzQfu","question_images":[],"timestamp":"2024-11-27 05:12:00","discussion":[{"poster":"GiorgioGss","comment_id":"1318639","timestamp":"1732711560.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-models.html\n\"Each model package in a Model Group corresponds to a trained model. The version of each model package is a numerical value that starts at 1 and is incremented with each new model package added to a Model Group. For example, if 5 model packages are added to a Model Group, the model package versions will be 1, 2, 3, 4, and 5.\"","upvote_count":"8"},{"upvote_count":"1","timestamp":"1743617220.0","comment_id":"1423474","content":"Selected Answer: C\nSageMaker Model Registry is specifically designed for managing ML models within the SageMaker ecosystem.\nIt provides built-in versioning and cataloging capabilities for ML models.\nModel groups in the Model Registry allow for logical organization of related models.\nIt integrates seamlessly with other SageMaker components like training, deployment, and monitoring.\nThis solution offers the least operational overhead as it's a native SageMaker feature designed for this exact purpose.\nIt provides features like model approval workflows, which are crucial for managing models in production environments.","poster":"Laxma99"},{"poster":"khchan123","comment_id":"1335152","timestamp":"1735722600.0","content":"Selected Answer: C\nThe correct answer is C. Use the SageMaker Model Registry and model groups to catalog the models.\n\nSageMaker Model Registry is specifically designed for managing ML models within the SageMaker ecosystem.\nIt provides built-in versioning and cataloging capabilities for ML models.\nModel groups in the Model Registry allow for logical organization of related models.\nIt integrates seamlessly with other SageMaker components like training, deployment, and monitoring.\nThis solution offers the least operational overhead as it's a native SageMaker feature designed for this exact purpose.\nIt provides features like model approval workflows, which are crucial for managing models in production environments.","upvote_count":"3"},{"poster":"prabirg","comment_id":"1332958","content":"Selected Answer: C\nAmazon SageMaker Model Registry creates Catalog models for production and Manage model versions.","upvote_count":"1","timestamp":"1735390860.0"},{"poster":"S_201996","upvote_count":"1","content":"Selected Answer: C\nAmazon SageMaker Model Registry is specifically designed to manage and catalog models in a centralized way, including versioning, approval workflows, and deployment history. It simplifies the process of managing different versions of models, which aligns with the company's requirement to use a central model registry.","comment_id":"1331331","timestamp":"1735096980.0"},{"comment_id":"1330082","content":"Selected Answer: C\nA. No, ECR is used to store container images\nB. No, ECR is used to store container images\nC. Yes\nD. No, Each model package in a Model Group corresponds to a trained model. The version of each model package is a numerical value that starts at 1 and is incremented with each new model package added to a Model Group - https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-models.html","upvote_count":"1","poster":"ninomfr64","timestamp":"1734791760.0"},{"timestamp":"1733769900.0","comment_id":"1324174","poster":"motk123","content":"Selected Answer: C\nC: The SageMaker Model Registry organizes models into Model Package Groups, tracks versions as Model Packages, and optionally aggregates groups into Collections. This structure ensures robust versioning and manageability for trained models.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-models.html","upvote_count":"2"},{"content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/machine-learning/centralize-model-governance-with-sagemaker-model-registry-resource-access-manager-sharing/","comment_id":"1318418","timestamp":"1732680720.0","upvote_count":"4","poster":"Neo_2022"}],"topic":"1","question_id":1,"answer_ET":"C","exam_id":27,"answer":"C","isMC":true,"question_text":"Case Study -\nA company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring.\nThe application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3.\nThe company needs to use the central model registry to manage different versions of models in the application.\nWhich action will meet this requirement with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/152098-exam-aws-certified-machine-learning-engineer-associate-mla/","answer_description":"","choices":{"D":"Use the SageMaker Model Registry and unique tags for each model version.","A":"Create a separate Amazon Elastic Container Registry (Amazon ECR) repository for each model.","C":"Use the SageMaker Model Registry and model groups to catalog the models.","B":"Use Amazon Elastic Container Registry (Amazon ECR) and unique tags for each model version."},"unix_timestamp":1732680720,"answers_community":["C (100%)"],"answer_images":[]},{"id":"j2cGaBw2QwqfBwdeuXWp","answer_description":"","answers_community":["D (71%)","A (29%)"],"answer_ET":"D","timestamp":"2024-11-27 01:48:00","question_images":[],"unix_timestamp":1732668480,"topic":"1","question_text":"Case study -\nAn ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3.\nThe dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data.\nWhich AWS service or feature can aggregate the data from the various data sources?","url":"https://www.examtopics.com/discussions/amazon/view/152088-exam-aws-certified-machine-learning-engineer-associate-mla/","question_id":2,"answer":"D","discussion":[{"content":"Selected Answer: A\nAmazon EMR with Spark is an excellent choice for aggregating, processing, and transforming large datasets from multiple sources (e.g., Amazon S3 and on-premises MySQL database). Spark jobs can handle both structured and unstructured. While Lake Formation is great for managing data lakes, it doesnâ€™t provide the ETL and data processing capabilities required to aggregate and transform datasets from multiple sources.","comment_id":"1318762","upvote_count":"11","timestamp":"1732721820.0","poster":"tigrex73"},{"upvote_count":"7","timestamp":"1732668480.0","poster":"a4002bd","content":"Selected Answer: D\nIs it D? AWS Lake Formation ? EMR Spark jobs is more manual.","comment_id":"1318356"},{"timestamp":"1742938200.0","content":"Selected Answer: A\nEMR with Spark can aggregate large datasets from multiple sources, including S3 and on-premises MySQL.","upvote_count":"1","poster":"Sadrik","comment_id":"1410168"},{"upvote_count":"1","poster":"chris_spencer","comment_id":"1381937","timestamp":"1741614480.0","content":"Selected Answer: A\nA. Amazon EMR Spark jobs"},{"poster":"djeong95","upvote_count":"2","timestamp":"1741038480.0","comment_id":"1364605","content":"Selected Answer: D\nThe answer is D (Lake Formation). This is because EMR Spark does not natively support getting data from on-prem DB as its data source. You would need DataSync or something else for that. On the other hand, Lake Formation fulfills all use cases documented clearly as links shown below. \n\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html#lake-formation-features\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-get-data-in.html"},{"upvote_count":"2","comment_id":"1352346","timestamp":"1738842780.0","content":"Selected Answer: D\nLake Formation ain't only the storage service - it actually an umbrella over most AWS Glue Offerings. And cause it also provides fully serverless Spark, it seems to be better option than EMR. This point is quite tricky as often when someone refers to LF, they mean the governance part only.","poster":"gorloff"},{"poster":"Udyan","comment_id":"1351898","upvote_count":"2","timestamp":"1738764060.0","content":"Selected Answer: D\nThe correct answer is D. AWS Lake Formation.\n\nExplanation:\nAWS Lake Formation is designed for aggregating, organizing, and securing large datasets from multiple sources (e.g., S3, on-premises databases). It simplifies the creation of a centralized data lake, enabling seamless integration and analysis of diverse data formats. This is particularly useful for tasks like fraud detection, where data comes from different sources.\n\nAmazon EMR Spark jobs (Option A) is more suited for large-scale data processing and analytics. While it can process and transform data, it requires more operational effort to configure and manage compared to AWS Lake Formation.\n\nWhy AWS Lake Formation?\nAggregates and organizes data from S3 and MySQL easily.\nOffers integrated data cataloging for better feature engineering.\nReduces operational overhead compared to setting up EMR."},{"content":"Selected Answer: D\nLake Formation is the correct answer","timestamp":"1738618020.0","poster":"shabak","comment_id":"1351115","upvote_count":"3"},{"upvote_count":"5","timestamp":"1737854280.0","content":"Selected Answer: D\nAWS Lake Formation is a service designed to aggregate, catalog, and manage data from multiple data sources, including on-premises databases and Amazon S3, making it an ideal choice for this scenario.\n\nWhile Amazon EMR with Apache Spark is powerful for processing and analyzing large datasets, it focuses more on data processing than on data aggregation and cataloging. It doesn't inherently manage interdependencies or schema enforcement","comment_id":"1346754","poster":"dbcert87"},{"upvote_count":"1","comment_id":"1346752","poster":"dbcert87","timestamp":"1737854160.0","content":"Selected Answer: A\nAmazon EMR is correct answer for aggregation"},{"content":"Selected Answer: D\nData Lake is used for data discovery","upvote_count":"3","poster":"fnuuu","timestamp":"1737611040.0","comment_id":"1345124"},{"upvote_count":"3","poster":"xukun","content":"Selected Answer: D\nOnce you specify where your existing databases are and provide your access credentials, Lake Formation reads the data and its metadata (schema) to understand the contents of the data source. It then imports the data to your new data lake and records the metadata in a central catalog. With Lake Formation, you can import data from MySQL, PostgreSQL, SQL Server, MariaDB, and Oracle databases running in Amazon RDS or hosted in Amazon EC2. Both bulk and incremental data loading are supported.\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html","comment_id":"1339769","timestamp":"1736733180.0"},{"timestamp":"1736156460.0","upvote_count":"2","comment_id":"1337058","content":"Selected Answer: A\nWhile AWS Lake Formation could potentially be used in conjunction with other services for data lake management, Amazon EMR with Spark jobs is the most direct and powerful solution for aggregating and processing data from the various sources mentioned in this scenario. It provides the necessary tools to handle the data integration, address the class imbalance, and perform the complex feature engineering that may be required for the fraud detection model.","poster":"Makendran"},{"content":"Selected Answer: D\nMy first choice was Lake Formation","comment_id":"1335109","timestamp":"1735708200.0","poster":"CloudHandsOn","upvote_count":"3"},{"timestamp":"1735489980.0","comment_id":"1333600","poster":"ninomfr64","upvote_count":"5","content":"Selected Answer: D\nYet another poorly worded AWS certification question. Here is my reasoning, the question is about \"aggregate the data from S3 and on-premise mysql\" and I do intend \"aggregate\" as put in the same place, therefore:\nA. No, while EMR spark job can connect to S3 and MySQL (spark can connect to mysql database), but it is a better tool to process data and then sore them in S3\nB. No, KDS it is for delivering streaming data sources to specific destinations (S3, OpenSearch ...)\nC. No, DynamoDB is a nosql db that is not a great fit here\nD. Yes, Lake Formation \"combine different types of structured and unstructured data into a centralized repository\" https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html and \"with Lake Formation, you can import your data using workflows\" and as it is based on AWS Glue it supports both S3 and mysql"},{"comment_id":"1331884","poster":"AsankaIshara","timestamp":"1735210380.0","upvote_count":"3","content":"Selected Answer: D\nQuestion is which AWS service or feature can aggregate the data from the various data sources? So lake formation"},{"comment_id":"1330977","timestamp":"1734999180.0","content":"Selected Answer: A\nI think it is A, it is more aligned with machine learning model","poster":"breathingcloud","upvote_count":"1"},{"content":"Selected Answer: A\nLake formation can catalog data from various sources, it doesn't provide the data processing capabilities needed for this scenario. EMR is more appropriate in this case.","poster":"AbhayD","upvote_count":"1","comment_id":"1328567","timestamp":"1734533460.0"},{"upvote_count":"4","content":"Selected Answer: D\nData Aggregation: Lake Formation is designed to create a data lake, a centralized repository that stores and manages data from various sources, including S3, relational databases (like MySQL), and other data sources.\n\nData Transformation: It can transform and clean data, making it suitable for analysis and machine learning. This includes handling class imbalance and feature interdependencies.\n\nData Access: It provides a unified interface to access data, simplifying the process of integrating data from different sources into the ML model.\n\nWhile other options like Amazon EMR Spark jobs and Amazon Kinesis Data Streams could be used for data processing and streaming, they are not the most efficient and straightforward solutions for this specific use case. Amazon DynamoDB is a NoSQL database, not designed for batch data processing and aggregation.\n\nTherefore, AWS Lake Formation is the best choice to aggregate and prepare the data for the ML model.\nref:https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html","poster":"TonyKean888","comment_id":"1324377","timestamp":"1733804940.0"},{"poster":"LR2023","comment_id":"1322908","timestamp":"1733517360.0","content":"Selected Answer: D\nLake formation would be a better choice over EMR as it involes complexity setting up . For data aggregation and ETL processes, especially involving multiple data sources and ensuring data quality and security, AWS Lake Formation or Amazon Glue are more specialized and suitable option","upvote_count":"4"},{"poster":"GiorgioGss","upvote_count":"1","comment_id":"1318704","timestamp":"1732716600.0","content":"Selected Answer: A\nI would go with EMR Spark jobs just because I think Lake Formation is not designed for feature engineering. Spark is."}],"isMC":true,"answer_images":[],"exam_id":27,"choices":{"B":"Amazon Kinesis Data Streams","A":"Amazon EMR Spark jobs","D":"AWS Lake Formation","C":"Amazon DynamoDB"}},{"id":"2IhhD5HGHK1UV85WGIh5","question_text":"A company is using Amazon SageMaker to develop ML models. The company stores sensitive training data in an Amazon S3 bucket. The model training must have network isolation from the internet.\n\nWhich solution will meet this requirement?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/168860-exam-aws-certified-machine-learning-engineer-associate-mla/","answer_description":"","answer":"B","unix_timestamp":1741700760,"timestamp":"2025-03-11 14:46:00","question_id":3,"answers_community":["B (100%)"],"question_images":[],"choices":{"A":"Run the SageMaker training jobs in private subnets. Create a NAT gateway. Route traffic for training through the NAT gateway.","D":"Encrypt traffic to Amazon S3 by using a bucket policy that includes a value of True for the aws:SecureTransport condition key. Use default at-rest encryption for Amazon S3. Encrypt SageMaker instance storage by using server-side encryption with AWS KMS keys (SSE-KMS).","B":"Run the SageMaker training jobs in private subnets. Create an S3 gateway VPC endpoint. Route traffic for training through the S3 gateway VPC endpoint.","C":"Run the SageMaker training jobs in public subnets that have an attached security group. In the security group, use inbound rules to limit traffic from the internet. Encrypt SageMaker instance storage by using server-side encryption with AWS KMS keys (SSE-KMS)."},"isMC":true,"discussion":[{"upvote_count":"1","poster":"chris_spencer","content":"Selected Answer: B\nUse private subnets and S3 gateway VPC endpoint to bypass public Internet.","timestamp":"1741700760.0","comment_id":"1387447"}],"exam_id":27,"answer_images":[],"answer_ET":"B"},{"id":"ju2gKi53YfG5ryTTSfer","question_id":4,"isMC":true,"discussion":[{"timestamp":"1741701240.0","content":"Selected Answer: D\nD is the correct answer.\n\nA The csv and docx files has to be vectorized first. Beside this option does not mention anything about the data\nB and C are not applicable in this case.","upvote_count":"1","poster":"chris_spencer","comment_id":"1387450"}],"choices":{"A":"Create a pipeline in Amazon SageMaker Pipelines to generate a new model. Call the new model from Amazon Bedrock to perform RAG queries.","B":"Convert the data into vectors. Store the data in an Amazon Neptune database. Connect the database to Amazon Bedrock. Call the Amazon Bedrock API to perform RAG queries.","C":"Fine-tune an existing LLM by using an AutoML job in Amazon SageMaker. Configure the S3 bucket as a data source for the AutoML job. Deploy the LLM to a SageMaker endpoint. Use the endpoint to perform RAG queries.","D":"Create a knowledge base for Amazon Bedrock. Configure a data source that references the S3 bucket. Use the Amazon Bedrock API to perform RAG queries."},"question_text":"A company needs to use Retrieval Augmented Generation (RAG) to supplement an open source large language model (LLM) that runs on Amazon Bedrock. The company's data for RAG is a set of documents in an Amazon S3 bucket. The documents consist of .csv files and .docx files.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answers_community":["D (100%)"],"timestamp":"2025-03-11 14:54:00","question_images":[],"answer":"D","topic":"1","answer_ET":"D","exam_id":27,"answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/168861-exam-aws-certified-machine-learning-engineer-associate-mla/","unix_timestamp":1741701240},{"id":"vjTDgbP07ru2g7k249Wc","discussion":[{"comment_id":"1387464","upvote_count":"1","poster":"chris_spencer","content":"Selected Answer: B\nAgree with B.\n\nIn general, real-time inference supports payloads up to 5 MB for synchronous requests, while asynchronous inference can support larger payloads, often up to 5 GB. \n\nThe use case in this questions involves inference payloads of 100 MB to 300 MB and needs to be processed in under 60 minutes, Asynchronous Inference is the best choice for handling large payloads without strict real-time requirements.","timestamp":"1741702080.0"}],"question_text":"A company plans to deploy an ML model for production inference on an Amazon SageMaker endpoint. The average inference payload size will vary from 100 MB to 300 MB. Inference requests must be processed in 60 minutes or less.\n\nWhich SageMaker inference option will meet these requirements?","timestamp":"2025-03-11 15:08:00","choices":{"A":"Serverless inference","B":"Asynchronous inference","D":"Batch transform","C":"Real-time inference"},"answer_description":"","question_images":[],"answers_community":["B (100%)"],"isMC":true,"unix_timestamp":1741702080,"exam_id":27,"answer_images":[],"answer_ET":"B","question_id":5,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/168862-exam-aws-certified-machine-learning-engineer-associate-mla/","answer":"B"}],"exam":{"name":"AWS Certified Machine Learning Engineer - Associate MLA-C01","isBeta":false,"isMCOnly":false,"id":27,"isImplemented":true,"provider":"Amazon","numberOfQuestions":106,"lastUpdated":"11 Apr 2025"},"currentPage":1},"__N_SSP":true}