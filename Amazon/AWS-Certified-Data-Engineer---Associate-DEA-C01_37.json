{"pageProps":{"questions":[{"id":"LmO77wQjLuGBHtrjdhIX","answers_community":["CD (83%)","CE (17%)"],"question_text":"A data engineer needs to use an Amazon QuickSight dashboard that is based on Amazon Athena queries on data that is stored in an Amazon S3 bucket. When the data engineer connects to the QuickSight dashboard, the data engineer receives an error message that indicates insufficient permissions.\nWhich factors could cause to the permissions-related errors? (Choose two.)","exam_id":21,"question_id":181,"isMC":true,"unix_timestamp":1706879760,"topic":"1","timestamp":"2024-02-02 14:16:00","answer_description":"","discussion":[{"poster":"fceb2c1","timestamp":"1711279920.0","comment_id":"1181484","content":"Selected Answer: CD\nC and D\nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html\n\nE is incorrect because it will result in authentication/authorization error, not insufficient permission error.","upvote_count":"8"},{"comment_id":"1138557","poster":"rralucard_","content":"Selected Answer: CD\nC. QuickSight does not have access to the S3 bucket: Amazon QuickSight needs to have the necessary permissions to access the S3 bucket where the data resides. If QuickSight lacks the permissions to read the data from the S3 bucket, it would result in an error indicating insufficient permissions.\n\nD. QuickSight does not have access to decrypt S3 data: If the data in S3 is encrypted, QuickSight needs permissions to use the necessary keys to decrypt the data. Without access to the decryption keys, typically managed by AWS Key Management Service (KMS), QuickSight cannot read the encrypted data and would give an error.","upvote_count":"6","timestamp":"1706879760.0"},{"upvote_count":"2","content":"Selected Answer: CE\nC. QuickSight does not have access to the S3 bucket. Amazon QuickSight needs to have the necessary permissions to access the Amazon S3 bucket where the data is stored. If these permissions are not correctly configured, QuickSight will not be able to access the data, resulting in an error.\n\nE. There is no IAM role assigned to QuickSight. Amazon QuickSight uses AWS Identity and Access Management (IAM) roles to access AWS resources. If QuickSight is not assigned an IAM role, or if the assigned role does not have the necessary permissions, QuickSight will not be able to access the resources it needs, leading to an error.","timestamp":"1719907140.0","comment_id":"1240628","poster":"bakarys"},{"content":"Selected Answer: CD\nC and D","upvote_count":"1","comment_id":"1197194","poster":"Ousseyni","timestamp":"1713353640.0"},{"poster":"Christina666","content":"Selected Answer: CD\nThe two most likely factors causing the permissions-related errors are:\n\nC. QuickSight does not have access to the S3 bucket. To access data from an S3 bucket, QuickSight needs explicit S3 permissions. This is typically handled through an IAM role associated with the QuickSight service.\nD. QuickSight does not have access to decrypt S3 data. If the data in S3 is encrypted (e.g., using KMS), QuickSight must have the necessary permissions to decrypt the data using the relevant KMS key.\nLet's analyze why the other options are less likely the primary culprits:\n\n\nE. There is no IAM role assigned to QuickSight. QuickSight needs an IAM role for overall functionality. A missing role would likely cause broader service failures, not specific data access errors.","upvote_count":"4","comment_id":"1194786","timestamp":"1712995860.0"},{"content":"Selected Answer: CE\nI think the assumptions in the problem are insufficient. If the data is encrypted, then D can be the correct answer, but if not, then E is the correct answer.","timestamp":"1710713760.0","comment_id":"1176085","poster":"taka5094","upvote_count":"2"},{"comment_id":"1168329","upvote_count":"2","content":"Ans. CD\nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html","poster":"damaldon","timestamp":"1709842200.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/132701-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"answer_ET":"CD","answer":"CD","choices":{"E":"There is no IAM role assigned to QuickSight.","D":"QuickSight does not have access to decrypt S3 data.","B":"The Athena tables are not cataloged.","C":"QuickSight does not have access to the S3 bucket.","A":"There is no connection between QuickSight and Athena."},"question_images":[]},{"id":"rK4gny3N266EtRyByDWy","timestamp":"2024-02-02 14:20:00","question_id":182,"answer":"A","unix_timestamp":1706880000,"exam_id":21,"question_images":[],"answers_community":["A (67%)","C (17%)","B (17%)"],"discussion":[{"comment_id":"1177346","timestamp":"1710854340.0","upvote_count":"7","content":"Selected Answer: A\nLEAST operational overhead? query straight with Athena without any intermediate actions or services","poster":"GiorgioGss"},{"upvote_count":"1","timestamp":"1718150040.0","content":"Selected Answer: A\nthena natively supports querying JSON data stored in S3 using standard SQL functions.\nThis eliminates the need for additional data transformation steps using Glue jobs (as required in Option C or D).","comment_id":"1228717","poster":"pypelyncar"},{"upvote_count":"1","poster":"tgv","comment_id":"1220133","timestamp":"1716891240.0","content":"As chris_spencer mentioned below, now Athena supports querying with PartiQL which technically makes the answer A correct."},{"poster":"VerRi","upvote_count":"1","content":"Selected Answer: A\nB requires Redshift Spectrum, so A","timestamp":"1716387000.0","comment_id":"1215791"},{"content":"Selected Answer: C\nAnswer should be C.\n\nAmazon Athena does not support querying with PartiQL until 16.04.2024, https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-athena-federated-query-pass-through/\n\nThe DEA01 exam should not have include the latest feature","upvote_count":"2","comment_id":"1197931","timestamp":"1713442440.0","poster":"chris_spencer"},{"poster":"Christina666","content":"Selected Answer: A\nA. Unified Querying with Athena: Athena provides a SQL-like interface for querying various data sources, including JSON and CSV in S3, as well as traditional databases.\nPartiQL Support: Athena's PartiQL extension allows querying semi-structured JSON data directly, eliminating the need for a separate query engine.\nServerless and Managed: Both AWS Glue and Athena are serverless, minimizing infrastructure management for the data engineers.\nNo Unnecessary Transformations: Avoiding transformations for JSON data simplifies the pipeline and reduces operational overhead.\nB. Redshift Spectrum: While Spectrum can query external data, it's primarily intended for Redshift data warehouse extensions. It adds complexity for the RDS and DynamoDB data sources.","upvote_count":"4","timestamp":"1712996160.0","comment_id":"1194793"},{"timestamp":"1712173800.0","content":"Selected Answer: B\nI will go with B","poster":"lucas_rfsb","comment_id":"1188875","comments":[{"timestamp":"1712403420.0","comment_id":"1190362","content":"B is the best choice:\nAWS Glue Data Catalog: AWS Glue can crawl and catalog the data sources (S3 buckets, RDS databases, DynamoDB tables) and store the metadata in the AWS Glue Data Catalog. This provides a centralized metadata repository for all data sources.\nAmazon Redshift Spectrum: Redshift Spectrum is a feature of Amazon Redshift that allows you to query data directly from various data sources, including S3 buckets, without loading the data into Redshift tables. This means you can query the JSON and CSV files in S3, as well as the RDS and DynamoDB data sources, using standard SQL syntax.\nSQL and PartiQL Support: Redshift Spectrum supports querying structured data sources (like RDS and CSV files) using SQL, and querying semi-structured data sources (like JSON files) using PartiQL, which is a SQL-compatible query language for JSON data.","poster":"nyaopoko","upvote_count":"1"}],"upvote_count":"4"},{"upvote_count":"4","timestamp":"1711836720.0","poster":"Luke97","comment_id":"1186419","content":"The answer should be B.\nA is incorrect because Athena does NOT support PartiQL.\nC is NOT the least operational (has the additional step to convert JSON to Parquet or csv)\nD is incorrect because DynamoDB export data to S3 in DynamoDB JSON or Amzone Ion format only (https://aws.amazon.com/blogs/aws/new-export-amazon-dynamodb-table-data-to-data-lake-amazon-s3/)."},{"content":"Selected Answer: C\nAWS Athena can only query in SQL, not PartiQL, so both A and B are incorrect. LakeFormation can not work directly with DynamoDB, so D is incorrect. \nThe only acceptable answer is C","comments":[{"timestamp":"1712505420.0","comment_id":"1191043","content":"similar to SQL, so A","poster":"andrevus","upvote_count":"1"}],"upvote_count":"2","poster":"halogi","comment_id":"1184525","timestamp":"1711595340.0"},{"upvote_count":"3","timestamp":"1706880000.0","comment_id":"1138558","poster":"rralucard_","content":"Selected Answer: A\nOption A, using AWS Glue and Amazon Athena, would meet the requirements with the least operational overhead. This solution allows data scientists to directly query data in its original format without the need for additional data transformation steps, making it easier to implement and manage."}],"choices":{"A":"Use AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use Amazon Athena to query the data. Use SQL for structured data sources. Use PartiQL for data that is stored in JSON format.","B":"Use AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use Redshift Spectrum to query the data. Use SQL for structured data sources. Use PartiQL for data that is stored in JSON format.","D":"Use AWS Lake Formation to create a data lake. Use Lake Formation jobs to transform the data from all data sources to Apache Parquet format. Store the transformed data in an S3 bucket. Use Amazon Athena or Redshift Spectrum to query the data.","C":"Use AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use AWS Glue jobs to transform data that is in JSON format to Apache Parquet or .csv format. Store the transformed data in an S3 bucket. Use Amazon Athena to query the original and transformed data from the S3 bucket."},"answer_ET":"A","question_text":"A company stores datasets in JSON format and .csv format in an Amazon S3 bucket. The company has Amazon RDS for Microsoft SQL Server databases, Amazon DynamoDB tables that are in provisioned capacity mode, and an Amazon Redshift cluster. A data engineering team must develop a solution that will give data scientists the ability to query all data sources by using syntax similar to SQL.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/132702-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","isMC":true,"answer_description":""},{"id":"gIzFbiSZbzKrsajvabGn","answer_description":"","answer_images":[],"answer_ET":"B","question_id":183,"exam_id":21,"question_text":"A data engineer is configuring Amazon SageMaker Studio to use AWS Glue interactive sessions to prepare data for machine learning (ML) models.\nThe data engineer receives an access denied error when the data engineer tries to prepare the data by using SageMaker Studio.\nWhich change should the engineer make to gain access to SageMaker Studio?","answers_community":["B (61%)","C (39%)"],"discussion":[{"upvote_count":"6","comment_id":"1220137","timestamp":"1716891840.0","content":"Selected Answer: B\nI don't believe you're supposed to assign a FullAccess policy, so I will go with B.","poster":"tgv"},{"timestamp":"1710855240.0","poster":"GiorgioGss","content":"Selected Answer: B\nI will go with B since you can get access denied even with the AmazonSageMakerFullAccess.\n See here: https://stackoverflow.com/questions/64709871/aws-sagemaker-studio-createdomain-access-error","upvote_count":"5","comment_id":"1177365"},{"comment_id":"1294134","content":"Selected Answer: B\nB. the engineer needs to assume specific roles to allow interaction between these services. The sts:AssumeRole action is necessary for this purpose","upvote_count":"1","timestamp":"1728288480.0","poster":"mohamedTR"},{"upvote_count":"2","poster":"junrun3","timestamp":"1724532960.0","comment_id":"1271855","content":"Selected Answer: C\nB, this approach involves setting up the trust relationship for roles. It is not a typical requirement for resolving access issues with SageMaker Studio directly."},{"poster":"LR2023","comments":[{"comment_id":"1248617","timestamp":"1721091780.0","upvote_count":"1","content":"and You can attach AWSGlueServiceRole to your users, groups, and roles.","poster":"LR2023","comments":[{"poster":"LR2023","content":"Sorry changed my mind option B makes most sense","upvote_count":"1","comment_id":"1290086","timestamp":"1727446560.0"}]}],"comment_id":"1248616","timestamp":"1721091720.0","content":"OPtion A\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-is-security.html","upvote_count":"1"},{"comment_id":"1194798","poster":"Christina666","content":"Selected Answer: C\nSageMaker Permissions: The AmazonSageMakerFullAccess managed policy provides broad permissions for using Amazon SageMaker features, including SageMaker Studio and the ability to interact with other AWS services like AWS Glue.\nLeast Privilege: While this policy is quite permissive, it's the most direct solution to the immediate access issue. After resolving the error, you can refine permissions for a more granular approach.","upvote_count":"1","timestamp":"1712996520.0"},{"poster":"lucas_rfsb","content":"Selected Answer: C\nI will go with C","comment_id":"1188878","timestamp":"1712174160.0","upvote_count":"3","comments":[{"upvote_count":"2","comment_id":"1190364","timestamp":"1712403540.0","content":"Option A (AWSGlueServiceRole managed policy) is not relevant, as this policy is intended for the AWS Glue service itself, not for users accessing SageMaker Studio.\nOption B (adding a policy with sts:AssumeRole action) is not necessary, as SageMaker handles the role assumption process internally.\nOption D (sts:AddAssociation action) is not a valid action and is not required for accessing SageMaker Studio or using AWS Glue interactive sessions.","poster":"nyaopoko"}]},{"comment_id":"1181492","poster":"fceb2c1","timestamp":"1711281000.0","content":"https://repost.aws/knowledge-center/sagemaker-featuregroup-troubleshooting","upvote_count":"1"},{"upvote_count":"2","comment_id":"1168338","timestamp":"1709843280.0","content":"Ans. C\nhttps://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonSageMakerFullAccess.html","poster":"damaldon"},{"poster":"atu1789","upvote_count":"2","timestamp":"1708148580.0","content":"Selected Answer: B\nB. Add a policy to the data engineer’s IAM user that includes the sts:AssumeRole action for the AWS Glue and SageMaker service principals in the trust policy.\n\n • This is the most appropriate solution. The sts:AssumeRole action allows the data engineer’s IAM user to assume a role that has the necessary permissions for both AWS Glue and SageMaker. This is a common approach for granting cross-service access in AWS.","comment_id":"1152387"},{"comment_id":"1138563","poster":"rralucard_","content":"Selected Answer: C\nAmazon SageMaker requires permissions to perform actions on your behalf. By attaching the AmazonSageMakerFullAccess managed policy to the data engineer’s IAM user, you grant the necessary permissions for SageMaker Studio to access AWS Glue and other related services.","upvote_count":"3","timestamp":"1706880180.0"}],"isMC":true,"answer":"B","unix_timestamp":1706880180,"topic":"1","question_images":[],"timestamp":"2024-02-02 14:23:00","url":"https://www.examtopics.com/discussions/amazon/view/132703-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"A":"Add the AWSGlueServiceRole managed policy to the data engineer's IAM user.","C":"Add the AmazonSageMakerFullAccess managed policy to the data engineer's IAM user.","D":"Add a policy to the data engineer's IAM user that allows the sts:AddAssociation action for the AWS Glue and SageMaker service principals in the trust policy.","B":"Add a policy to the data engineer's IAM user that includes the sts:AssumeRole action for the AWS Glue and SageMaker service principals in the trust policy."}},{"id":"8np86vXgZkg0o7yhevKn","discussion":[{"timestamp":"1726745700.0","comment_id":"1177366","upvote_count":"6","content":"Selected Answer: B\nLeast effort = B","poster":"GiorgioGss"},{"content":"Selected Answer: B\nB. Use AWS Glue to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark.","poster":"rralucard_","comment_id":"1138567","timestamp":"1722598080.0","upvote_count":"5"},{"poster":"Christina666","comment_id":"1194800","content":"Selected Answer: B\nGlue ETL","timestamp":"1728807960.0","upvote_count":"1"},{"poster":"kj07","timestamp":"1726305540.0","comment_id":"1173328","upvote_count":"3","content":"The option with the least operational overhead is B."}],"question_id":184,"choices":{"B":"Use AWS Glue to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark.","C":"Create a PySpark program in AWS Lambda to extract, transform, and load the data into the S3 bucket.","A":"Use Amazon EMR to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark.","D":"Create a stored procedure in Amazon Redshift to detect the schema and to extract, transform, and load the data into a Redshift Spectrum table. Access the table from Amazon S3."},"answers_community":["B (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/132706-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"B","question_text":"A company extracts approximately 1 TB of data every day from data sources such as SAP HANA, Microsoft SQL Server, MongoDB, Apache Kafka, and Amazon DynamoDB. Some of the data sources have undefined data schemas or data schemas that change.\nA data engineer must implement a solution that can detect the schema for these data sources. The solution must extract, transform, and load the data to an Amazon S3 bucket. The company has a service level agreement (SLA) to load the data into the S3 bucket within 15 minutes of data creation.\nWhich solution will meet these requirements with the LEAST operational overhead?","isMC":true,"timestamp":"2024-02-02 14:28:00","exam_id":21,"topic":"1","answer_ET":"B","unix_timestamp":1706880480,"answer_images":[],"answer_description":""},{"id":"aEXuTStHXOfOPr6jLSfB","exam_id":21,"answers_community":["B (100%)"],"question_text":"A company has multiple applications that use datasets that are stored in an Amazon S3 bucket. The company has an ecommerce application that generates a dataset that contains personally identifiable information (PII). The company has an internal analytics application that does not require access to the PII.\nTo comply with regulations, the company must not share PII unnecessarily. A data engineer needs to implement a solution that with redact PII dynamically, based on the needs of each application that accesses the dataset.\nWhich solution will meet the requirements with the LEAST operational overhead?","question_id":185,"isMC":true,"unix_timestamp":1706880660,"topic":"1","timestamp":"2024-02-02 14:31:00","answer_description":"","discussion":[{"upvote_count":"1","timestamp":"1724042160.0","poster":"teo2157","content":"Selected Answer: B\nIt's B based on AWS documentation\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transforming-objects.html","comment_id":"1268342"},{"comment_id":"1228730","poster":"pypelyncar","upvote_count":"3","content":"Selected Answer: B\nS3 Object Lambda automatically triggers the Lambda function only when there's a request to access data in the S3 bucket. This eliminates the need for pre-processing or creating multiple data copies with varying levels of redaction (Options A and C).","timestamp":"1718150940.0"},{"upvote_count":"2","timestamp":"1716811320.0","poster":"4c78df0","content":"Selected Answer: B\nB is correct","comment_id":"1219529"},{"upvote_count":"4","timestamp":"1709844000.0","poster":"damaldon","comment_id":"1168344","content":"Ans. B\nYou can use an Amazon S3 Object Lambda Access Point to control access to documents with personally identifiable information (PII).\nhttps://docs.aws.amazon.com/comprehend/latest/dg/using-access-points.html"},{"poster":"atu1789","upvote_count":"1","timestamp":"1708149360.0","comment_id":"1152391","content":"Selected Answer: B\nS3 Object Lambda allows you to add custom processing, such as redaction of PII, to data retrieved from S3. This is done dynamically, meaning you don’t need to store multiple copies of the data. It’s a more efficient and operationally simpler approach compared to managing multiple dataset versions."},{"timestamp":"1706880660.0","poster":"rralucard_","upvote_count":"3","comment_id":"1138568","content":"Selected Answer: B\nAmazon S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data as it is returned to an application. For example, you could use an S3 Object Lambda to dynamically redact personally identifiable information (PII) from data retrieved from S3. This would allow you to control access to sensitive information based on the needs of different applications, without having to create and manage multiple copies of your data."}],"url":"https://www.examtopics.com/discussions/amazon/view/132707-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"answer_ET":"B","answer":"B","choices":{"C":"Use AWS Glue to transform the data for each application. Create multiple copies of the dataset. Give each dataset copy the appropriate level of redaction for the needs of the application that accesses the copy.","D":"Create an API Gateway endpoint that has custom authorizers. Use the API Gateway endpoint to read data from the S3 bucket. Initiate a REST API call to dynamically redact PII based on the needs of each application that accesses the data.","A":"Create an S3 bucket policy to limit the access each application has. Create multiple copies of the dataset. Give each dataset copy the appropriate level of redaction for the needs of the application that accesses the copy.","B":"Create an S3 Object Lambda endpoint. Use the S3 Object Lambda endpoint to read data from the S3 bucket. Implement redaction logic within an S3 Object Lambda function to dynamically redact PII based on the needs of each application that accesses the data."},"question_images":[]}],"exam":{"name":"AWS Certified Data Engineer - Associate DEA-C01","numberOfQuestions":207,"id":21,"isImplemented":true,"isMCOnly":true,"provider":"Amazon","isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":37},"__N_SSP":true}