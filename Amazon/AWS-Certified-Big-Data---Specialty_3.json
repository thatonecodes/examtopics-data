{"pageProps":{"questions":[{"id":"BRz8DTnkZlUgajua1ni6","discussion":[{"upvote_count":"1","poster":"Mandy_007","timestamp":"1702985160.0","comment_id":"1100562","content":"A is correct for this use case."},{"content":"Yes, it is Presto.","upvote_count":"1","timestamp":"1635919980.0","poster":"esalas0691","comment_id":"113516"},{"comment_id":"93297","timestamp":"1635719400.0","poster":"Josh1981","upvote_count":"2","content":"Like everyone, I went with option A"},{"content":"Correct Answer is A","comments":[{"content":"Presto would be most appropriate answer due to following phrase in the question “interactive joins and then display results quickly”\nPig is more suitable for batch processing and Presto for interactive queries.","timestamp":"1635481020.0","poster":"YashBindlish","upvote_count":"2","comment_id":"74498"}],"timestamp":"1635195420.0","upvote_count":"1","poster":"YashBindlish","comment_id":"74497"},{"content":"my selection A","timestamp":"1634635800.0","poster":"san2020","comment_id":"52332","upvote_count":"1"},{"poster":"kalpanareddy","content":"Answer is A\nhttps://aws.amazon.com/emr/features/presto/","comment_id":"42578","upvote_count":"2","timestamp":"1634553840.0"},{"comment_id":"14977","poster":"M2","timestamp":"1634200920.0","upvote_count":"1","content":"Presto is the right answer here"},{"poster":"bigdatalearner","comment_id":"13740","content":"Presto is good for Peta bytes of data and for interactive queries while pig is mostly used for etl processing so correct answer is A i.e. Presto","upvote_count":"2","timestamp":"1633943520.0"},{"comments":[{"comment_id":"11519","upvote_count":"2","content":"A .. Presto is used for fast interactive joins","timestamp":"1633764420.0","poster":"exams"}],"content":"A is correct. Presto is more suited for interactive joins whereas pig is for batch processing","poster":"pra276","comment_id":"7523","timestamp":"1633753320.0","upvote_count":"3"},{"timestamp":"1633450680.0","comment_id":"6793","poster":"Jialu","upvote_count":"2","content":"A is the correct answer"},{"content":"A is the answer","upvote_count":"3","poster":"jlpl","comment_id":"6446","timestamp":"1633046760.0"},{"timestamp":"1632548100.0","upvote_count":"4","content":"It's A. Supports PBs https://dzone.com/articles/getting-introduced-with-presto","poster":"mattyb123","comment_id":"6407"}],"answer_images":[],"isMC":true,"answer":"C","choices":{"D":"R Studio","A":"Presto","B":"MicroStrategy","C":"Pig"},"answers_community":[],"answer_ET":"C","question_text":"An Amazon EMR cluster using EMRFS has access to petabytes of data on Amazon S3, originating from multiple unique data sources. The customer needs to query common fields across some of the data sets to be able to perform interactive joins and then display results quickly.\nWhich technology is most appropriate to enable this capability?","question_id":11,"answer_description":"","topic":"1","unix_timestamp":1565387880,"url":"https://www.examtopics.com/discussions/amazon/view/3407-exam-aws-certified-big-data-specialty-topic-1-question-19/","exam_id":17,"question_images":[],"timestamp":"2019-08-09 23:58:00"},{"id":"U4vBDmTHMrcDFniJZESL","exam_id":17,"answers_community":[],"choices":{"A":"Copy the data into Amazon ElastiCache to perform text analysis on the in-memory data and export the results of the model into Amazon Machine Learning.","C":"Use Amazon Elasticsearch Service to store the text and then use the Python Elasticsearch Client to run analysis against the text index.","D":"Initiate a Python job from AWS Data Pipeline to run directly against the Amazon S3 text files.","B":"Use Amazon EMR to parallelize the text analysis tasks across the cluster using a streaming program step."},"answer":"C","discussion":[{"poster":"Bulti","comment_id":"76624","timestamp":"1635836700.0","content":"Answer is B: Hadoop Streaming is a utility that comes with Hadoop that enables you to develop MapReduce executables in languages other than Java. Streaming is implemented in the form of a JAR file, so you can run it from the Amazon EMR API or command line just like a standard JAR file. \n https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UseCase_Streaming.html","upvote_count":"6"},{"comment_id":"1117273","upvote_count":"1","timestamp":"1704786720.0","content":"Option B (Use Amazon EMR) is the best service strategy for this use case. EMR can handle the scale of the data and provides the necessary computational resources to run complex text analysis algorithms in parallel across a large cluster, making it a suitable choice for processing 5 PB of data.","poster":"fagilom"},{"upvote_count":"1","content":"Option B.\nEMR supports on S3 supports 5PB. Hadoop or spark steaming supports Python algo. \nElastisearch has limit of 30TB for a single domain. Even after request to Amazon to increase, it can go max 300TB, so not possible to store 3TB.\nSecondly, using lambda in this case might not be good choice with such huge volume of data flowing in.","comment_id":"148362","timestamp":"1636266720.0","poster":"Abhi09"},{"timestamp":"1635999600.0","poster":"alopazo","comment_id":"120001","content":"B\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/CLI_CreateStreaming.html","upvote_count":"2"},{"timestamp":"1635608520.0","content":"C cannot be a right answer considering that the question does not mention about a Lambda function to copy from S3, secondly the question is talking about 5PB of data. Elasticsearch can not support 5 PB. Option B is the correct answer as the data is huge EMR can be used to parallelly analyse the data using Streaming program which supports python.","poster":"YashBindlish","comment_id":"74389","upvote_count":"2"},{"upvote_count":"1","comment_id":"73896","poster":"jxj","content":"C is reasonable","timestamp":"1635567060.0"},{"poster":"yuvaraj228","upvote_count":"1","timestamp":"1635535680.0","content":"C is right","comment_id":"61326"},{"upvote_count":"1","content":"Selected B","comment_id":"52139","timestamp":"1635319800.0","poster":"san2020"},{"poster":"ME2000","content":"The question is lying here:\nThe algorithm analyzes the free text contained within a sample set of 1 million e-mails stored on Amazon S3.\nThis secondary information is for overthinking:\nThe algorithm must be scaled across a production dataset of 5 PB, which also resides in Amazon S3 storage. (Must notice...also resides in Amazon S3 storage.)\nSo clearly the correct answer is C","comment_id":"38660","upvote_count":"1","timestamp":"1634305080.0"},{"upvote_count":"2","comment_id":"30379","content":"It can NOT be B, cause it says that 'using a streaming program step', how can a streaming program analysis text content and figure out if the content is spam then give feedback the content belongs to which piece of mail?","timestamp":"1633829460.0","poster":"shwang","comments":[{"upvote_count":"1","poster":"Vlad511","timestamp":"1633833720.0","content":"For me, C is the right one","comment_id":"30456"},{"content":"AWS ES cant support more than 3 PB data","upvote_count":"2","timestamp":"1634057640.0","poster":"cert_learner","comment_id":"34752"},{"comment_id":"81057","timestamp":"1635853260.0","poster":"srirampc","upvote_count":"1","content":"streaming could mean using hadoop streaming/spark streaming to process files in S3. Each row in csv is sent over to the streaming python function to be process. B is could be a good option to parallelize a function thats working good on a sample."}]},{"content":"I dont understand, I would have said also that answer is B but it seems it is not, is it possible that in the exam not only one answer is allowed?","poster":"antoneti","comments":[{"content":"thing is, the answers provided by examtopics are often wrong for some reason (at least for this exam)","poster":"Corram","comment_id":"100283","upvote_count":"1","timestamp":"1635884820.0"}],"timestamp":"1633759080.0","comment_id":"24941","upvote_count":"1"},{"comment_id":"23949","content":"for data size in PB, EMR and answer B should be","upvote_count":"2","poster":"WWODIN","timestamp":"1633357860.0"},{"content":"Answer is B","timestamp":"1633190220.0","upvote_count":"2","comment_id":"14950","poster":"M2"},{"timestamp":"1632943740.0","content":"Spam filtering is a machine learning algorithm. It works with EMR and S3 which are most suitable scenario. b is the correct answer","comment_id":"6879","upvote_count":"3","poster":"muhsin"},{"comment_id":"6775","poster":"Jialu","comments":[{"upvote_count":"1","timestamp":"1634577960.0","poster":"practicioner","content":"https://aws.amazon.com/ru/elasticsearch-service/faqs/\n\nQ: Is there a limit on the amount of EBS storage that can be allocated to an Amazon Elasticsearch Service domain?\n\nYes. Amazon Elasticsearch Service supports one EBS volume (max size of 1.5 TB) per instance associated with a domain. With the default maximum of 20 data nodes allowed per Amazon Elasticsearch Service domain, you can allocate about 30 TB of EBS storage to a single domain. You can request a service limit increase up to 200 instances per domain by creating a case with the AWS Support Center. With 200 instances, you can allocate about 300 TB of EBS storage to a single domain.","comment_id":"43872"}],"content":"B is the correct answer , since Amazon Elasticsearch Servic can not suppot 5 PB","upvote_count":"3","timestamp":"1632669840.0"},{"comment_id":"6571","poster":"mattyb123","comments":[{"upvote_count":"1","comments":[{"upvote_count":"3","comments":[{"comments":[{"content":"Hi, if B is correct, what does this URL you pasted, explains?","upvote_count":"1","poster":"shouvanik","timestamp":"1634616660.0","comment_id":"50729"}],"timestamp":"1632965340.0","content":"B is correct","poster":"exams","comment_id":"11392","upvote_count":"2"}],"poster":"mattyb123","timestamp":"1632511320.0","comment_id":"6637","content":"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UseCase_Streaming.html)"}],"timestamp":"1632306960.0","content":"https://aws.amazon.com/blogs/database/indexing-metadata-in-amazon-elasticsearch-service- using-aws-lambda-and-python/","comment_id":"6628","poster":"mattyb123"}],"upvote_count":"3","content":"B is the correct answer","timestamp":"1632264720.0"}],"question_id":12,"timestamp":"2019-08-12 05:49:00","topic":"1","answer_ET":"C","isMC":true,"question_images":[],"unix_timestamp":1565581740,"answer_description":"Reference: https://aws.amazon.com/blogs/database/indexing-metadata-in-amazon-elasticsearch-service- using-aws-lambda-and-python/","question_text":"A new algorithm has been written in Python to identify SPAM e-mails. The algorithm analyzes the free text contained within a sample set of 1 million e-mails stored on Amazon S3. The algorithm must be scaled across a production dataset of 5 PB, which also resides in Amazon S3 storage.\nWhich AWS service strategy is best for this use case?","url":"https://www.examtopics.com/discussions/amazon/view/3484-exam-aws-certified-big-data-specialty-topic-1-question-2/","answer_images":[]},{"id":"KPGuvmlDo8T4gK3GHm25","isMC":true,"answer_images":[],"choices":{"B":"Feed the data into Spark Mlib and build a random forest modest.","C":"Feed the data into Apache Mahout and build a multi-classification model.","D":"Feed the data into Amazon Machine Learning and build a binary classification model.","A":"Feed the data into Amazon Machine Learning and build a regression model."},"question_id":13,"topic":"1","timestamp":"2019-08-10 14:12:00","exam_id":17,"question_text":"A game company needs to properly scale its game application, which is backed by DynamoDB. Amazon\nRedshift has the past two years of historical data. Game traffic varies throughout the year based on various factors such as season, movie release, and holiday season. An administrator needs to calculate how much read and write throughput should be provisioned for DynamoDB table for each week in advance.\nHow should the administrator accomplish this task?","question_images":[],"answer_ET":"B","answers_community":[],"discussion":[{"comment_id":"75115","content":"Answer is A. Key is \"Redshift has the past two years of historical data\". This means we have labelled data that we can use to train a linear regression model to predict RCU and WCU.","poster":"Bulti","upvote_count":"4","timestamp":"1636041600.0"},{"timestamp":"1635535560.0","upvote_count":"3","poster":"san2020","comment_id":"52333","content":"my selection A"},{"comment_id":"46412","poster":"AdamSmith","timestamp":"1635176580.0","upvote_count":"4","content":"\"needs to calculate how much read and write throughput should be provisioned for DynamoDB table for each week in advance\"\n\nA regression model is more suitable for this job, you can just run 2 models, one for WCU, one for RCU.\nB is for detecting anomalies."},{"comment_id":"42583","upvote_count":"2","timestamp":"1634928600.0","poster":"kalpanareddy","content":"Normally, when predict some numbers like salary, price we use the regression model based on the histrory data. So personally, i agree with the B as it is used to predicted the number of the read and write throughput"},{"upvote_count":"3","comment_id":"39537","timestamp":"1634722740.0","poster":"ME2000","content":"The correct answer is B\n... An anomaly score with low values indicates that the data point is considered “normal” whereas high values indicate the presence of an anomaly. The definitions of “low” and “high” depend on the application, but common practice suggests that scores beyond three standard deviations from the mean score are considered anomalous.\nhttps://aws.amazon.com/blogs/machine-learning/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection/"},{"upvote_count":"2","content":"Not sure about A, mainly because regression is to precit just one numeric value, not two (RCU/WCU). Regarding Random forest (B) is also used for reggresion tasks as suggest here: https://www.newgenapps.com/blog/random-forest-analysis-in-ml-and-when-to-use-it","poster":"antoneti","timestamp":"1633984320.0","comment_id":"28097"},{"poster":"bigdatalearner","comment_id":"13741","content":"A. Feed the data into Amazon Machine Learning and build a regression model is the right answer because regression model is used for numeric value and here we are looking for RCU/WCU which is numeric as well. though B can be an option but it's not cost effective as it needs EMR cluster","timestamp":"1633569480.0","upvote_count":"4"},{"timestamp":"1633476180.0","upvote_count":"2","comment_id":"11892","comments":[{"comment_id":"31389","poster":"shwang","upvote_count":"2","timestamp":"1634463360.0","content":"Random forest modest can do regression"}],"content":"it should be A .. Random forest in B is for classification model.","poster":"VB"},{"comment_id":"11520","upvote_count":"2","poster":"exams","content":"A I think","timestamp":"1633471200.0"},{"comment_id":"7562","poster":"jlpl","timestamp":"1633030380.0","upvote_count":"2","content":"A, because historiccal using regression ml model"},{"comment_id":"7524","upvote_count":"1","poster":"pra276","timestamp":"1632892500.0","comments":[{"poster":"Corram","comment_id":"98808","content":"because a random forest decides between two options and is therefore not suited for choosing WCU and RCU, which could be any positive number.","timestamp":"1636293540.0","upvote_count":"1"}],"content":"Why not B?"},{"timestamp":"1632883140.0","content":"A is correct one","poster":"Jialu","upvote_count":"2","comment_id":"6794"},{"upvote_count":"3","poster":"mattyb123","comment_id":"6471","content":"Correct. A","timestamp":"1632709020.0"},{"content":"a? anyone?","comment_id":"6449","upvote_count":"3","poster":"jlpl","timestamp":"1632628620.0"}],"answer":"B","unix_timestamp":1565439120,"url":"https://www.examtopics.com/discussions/amazon/view/3424-exam-aws-certified-big-data-specialty-topic-1-question-20/","answer_description":""},{"id":"vgwBt1TTXwfivjMD6k02","isMC":true,"discussion":[{"poster":"san2020","comment_id":"52334","upvote_count":"5","timestamp":"1634278140.0","content":"my selection B"},{"poster":"Abhi09","content":"The FIRST one with most straightforward answer, without any controversy, confusion, doubt or anxiety ;-) !!","comment_id":"150299","timestamp":"1636224960.0","upvote_count":"1"},{"poster":"kalpanareddy","upvote_count":"2","timestamp":"1633057140.0","content":"B\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#snapshot-restore","comment_id":"42586"},{"upvote_count":"2","comment_id":"13748","timestamp":"1632776760.0","content":"B is the right answer","poster":"bigdatalearner"},{"poster":"exams","comment_id":"11521","timestamp":"1632453900.0","content":"B.. Manual snapshot","upvote_count":"2"}],"answers_community":[],"question_text":"A data engineer is about to perform a major upgrade to the DDL contained within an Amazon Redshift cluster to support a new data warehouse application. The upgrade scripts will include user permission updates, view and table structure changes as well as additional loading and data manipulation tasks.\nThe data engineer must be able to restore the database to its existing state in the event of issues.\nWhich action should be taken prior to performing this upgrade task?","exam_id":17,"timestamp":"2019-09-18 06:02:00","topic":"1","answer":"B","question_id":14,"choices":{"D":"Call the waitForSnapshotAvailable command from either the AWS CLI or an AWS SDK.","A":"Run an UNLOAD command for all data in the warehouse and save it to S3.","B":"Create a manual snapshot of the Amazon Redshift cluster.","C":"Make a copy of the automated snapshot on the Amazon Redshift cluster."},"unix_timestamp":1568779320,"answer_description":"Reference: https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html#working-with- snapshot-restore-table-from-snapshot","answer_images":[],"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/5344-exam-aws-certified-big-data-specialty-topic-1-question-21/","question_images":[]},{"id":"MmHY9z7SoNl9nCnJSbHi","answer":"C","unix_timestamp":1565388000,"discussion":[{"upvote_count":"5","content":"It's A . why so many questions have wrong answer ? who provide these answers?","timestamp":"1635897420.0","comment_id":"80107","poster":"kkyong"},{"comment_id":"52335","upvote_count":"5","content":"my selection A","poster":"san2020","timestamp":"1635831180.0"},{"comments":[{"timestamp":"1635589800.0","comment_id":"51838","poster":"JonSno","content":"https://docs.aws.amazon.com/iot/latest/developerguide/iot-create-rule.html","upvote_count":"2"}],"poster":"JonSno","content":"It's \"A\" as IOT also includes IOT Analytics..https://d1.awsstatic.com/IoT/AWS%20Industrial%20-%20Predictive%20Maintenance%20Reference%20Architecture.pdf","comment_id":"51837","upvote_count":"1","timestamp":"1635284400.0"},{"upvote_count":"1","timestamp":"1635210780.0","comment_id":"42594","content":"It's A . IOT rules engine have capability to send the messages to the target topic","poster":"kalpanareddy"},{"timestamp":"1634673540.0","comments":[{"upvote_count":"4","timestamp":"1636058760.0","content":"thresholds of pipeline system are defined when building . It is a physical problem . predicting peak thresholds of gas system would cause disasert .","comment_id":"80108","poster":"kkyong"}],"comment_id":"39561","upvote_count":"1","content":"To determine \"peak thresholds\" need the option C. Create an Amazon Machine Learning model and invoke it with AWS Lambda.\n\nFor each ML model, you need to determine the threshold for inference confidence that equates to a predicted failure condition. For example, if an inference for a machine you are monitoring indicates with high confidence (let’s say a level of 90%), then you would take appropriate action. \nhttps://aws.amazon.com/blogs/iot/using-aws-iot-for-predictive-maintenance/","poster":"ME2000"},{"timestamp":"1634261340.0","comment_id":"39090","upvote_count":"2","poster":"bc5468521","content":"actually, I know a lot of people choose A, but C still makes sense. In this case, the AWS IoT rule still uses machine learning + lambda step function to trigger SNS. the real question is what kind of IoT rule you are about to create."},{"timestamp":"1633726680.0","upvote_count":"1","comment_id":"13749","poster":"bigdatalearner","content":"A. IOT rule is the right answer for this question"},{"comment_id":"11894","upvote_count":"2","poster":"VB","content":"it is a .. but why some of the answers are wrong? :(","comments":[{"upvote_count":"1","content":"Near real-time alerts is a requirement, thus the SNS part","comment_id":"24174","poster":"jrsm","timestamp":"1633806420.0"}],"timestamp":"1633630320.0"},{"content":"IOT rule is fast and cost-effective","poster":"exams","upvote_count":"2","comment_id":"11522","timestamp":"1633284540.0"},{"upvote_count":"1","comment_id":"7561","timestamp":"1633273140.0","content":"A, itis","poster":"jlpl"},{"content":"it is A","poster":"Jialu","timestamp":"1632583380.0","comment_id":"6795","upvote_count":"2"},{"content":"It's A","comment_id":"6408","timestamp":"1632559020.0","upvote_count":"2","poster":"mattyb123"}],"isMC":true,"timestamp":"2019-08-10 00:00:00","question_text":"A large oil and gas company needs to provide near real-time alerts when peak thresholds are exceeded in its pipeline system. The company has developed a system to capture pipeline metrics such as flow rate, pressure, and temperature using millions of sensors. The sensors deliver to AWS IoT.\nWhat is a cost-effective way to provide near real-time alerts on the pipeline metrics?","topic":"1","choices":{"C":"Create an Amazon Machine Learning model and invoke it with AWS Lambda.","D":"Use Amazon Kinesis Streams and a KCL-based application deployed on AWS Elastic Beanstalk.","A":"Create an AWS IoT rule to generate an Amazon SNS notification.","B":"Store the data points in an Amazon DynamoDB table and poll if for peak metrics data from an Amazon EC2 application."},"exam_id":17,"question_id":15,"url":"https://www.examtopics.com/discussions/amazon/view/3408-exam-aws-certified-big-data-specialty-topic-1-question-22/","question_images":[],"answers_community":[],"answer_description":"","answer_images":[],"answer_ET":"C"}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","name":"AWS Certified Big Data - Specialty","numberOfQuestions":85,"provider":"Amazon","id":17,"isMCOnly":true,"isImplemented":true},"currentPage":3},"__N_SSP":true}