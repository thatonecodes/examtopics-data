{"pageProps":{"questions":[{"id":"9imwD30WvAvyW9J2Jjp9","url":"https://www.examtopics.com/discussions/amazon/view/95385-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"timestamp":"1678640580.0","content":"Selected Answer: A\nAnswer is A\nKeyword is \"The S3 buckets have millions of objects\"\nIf there are million of objects then you should use Batch operations. \nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/","upvote_count":"27","poster":"testingaws123","comments":[{"poster":"forceli","upvote_count":"1","comment_id":"844307","content":"good point, changing my answer to A","timestamp":"1679265960.0"}],"comment_id":"837247"},{"comment_id":"1317953","poster":"mnsait","timestamp":"1732605900.0","content":"This is outdated now. \n\"Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 are automatically encrypted at no additional cost and with no impact on performance.\"\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html","upvote_count":"1"},{"comment_id":"1295020","timestamp":"1728459720.0","content":"Selected Answer: A\nS3 Batch Operations can be used to efficiently apply changes to a large number of objects in a bucket, including copying and encrypting them in place. This is ideal for retroactively encrypting millions of existing objects without needing to manually handle them one by one.","upvote_count":"1","poster":"nimbus_00"},{"content":"I understand S3 Batch operations is required. But why no one is choosing SSE-KMS?","comments":[{"upvote_count":"2","timestamp":"1710718920.0","poster":"StevePace","content":"Because the question states the company wants to use SSE-S3, nowhere does it mention SSE-KMS","comment_id":"1176113"}],"comment_id":"1170997","timestamp":"1710159660.0","upvote_count":"1","poster":"ajeeshb"},{"upvote_count":"2","content":"To encrypt your existing unencrypted Amazon S3 objects, you can use Amazon S3 Batch Operations. You provide S3 Batch Operations with a list of objects to operate on, and Batch Operations calls the respective API to perform the specified operation. You can use the Batch Operations Copy operation to copy existing unencrypted objects and write them back to the same bucket as encrypted objects. A single Batch Operations job can perform the specified operation on billions of objects. https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html","poster":"TonytheTiger","timestamp":"1709911620.0","comment_id":"1168915"},{"upvote_count":"4","poster":"ninomfr64","content":"Selected Answer: A\nA = correct (see https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/)\nB = KMS is for SSE-KMS not for the requested SSE-S3\nC = CLI is less efficient than S3 Batch\nD = see answer B","timestamp":"1705660020.0","comment_id":"1126603"},{"poster":"career360guru","comment_id":"1103671","upvote_count":"1","timestamp":"1703275140.0","content":"Selected Answer: A\nA is the right answer"},{"content":"Selected Answer: A\nCorrect answer should be A. But this question seem too old to be true now since SSE-S3 based encryption is by default enabled and can't be disabled (you can change however) since Jan 2023.","poster":"jainparag1","upvote_count":"4","timestamp":"1700900940.0","comment_id":"1079843"},{"upvote_count":"1","poster":"covabix879","content":"Selected Answer: D\nSince SSE-S3 does not support cross-account replication, answer should be D","comment_id":"1022023","timestamp":"1696138440.0"},{"timestamp":"1694619300.0","poster":"deivid83","comment_id":"1006758","content":"In a cross-account scenario, where the source and destination buckets are owned by different AWS accounts, you can use a KMS key to encrypt object replicas. However, the KMS key owner must grant the source bucket owner permission to use the KMS key.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html#replication-kms-cross-acct-scenario\n\nS3 Batch operation:\nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/","upvote_count":"3"},{"timestamp":"1693916280.0","content":"Selected Answer: A\nS3 Batch operation is the MOST operationally efficient way for millions objects","upvote_count":"1","comment_id":"999476","poster":"uC6rW1aB"},{"upvote_count":"1","comment_id":"952295","poster":"sachstarinfoaws","content":"Selected Answer: A\nAnswer is A","timestamp":"1689418740.0"},{"comment_id":"941243","poster":"NikkyDicky","content":"Selected Answer: A\nA more efficient","upvote_count":"1","timestamp":"1688332620.0"},{"content":"Selected Answer: A\nI vote for A. Batch operations is better for such a high number of objects","comment_id":"929500","poster":"Maria2023","timestamp":"1687350960.0","upvote_count":"1"},{"timestamp":"1683601860.0","upvote_count":"2","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/\nThe launch of S3 default encryption feature automate the wok of encrypting new objects, and you asked for similar, straightforward ways to encrypt existing objects in your buckets. While tools and scripts exist to do this work, each one requires some development work to set up. S3 batch operations gives you a solution for encrypting large number of archived files. \nThis can also be done by CLI, Option C, however, the same article refers to Batch Operations in case you have a large bucket with millions of objects. \nhttps://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/ \nOption A should be the most efficient, even though it has more operational cost to implement but the question is the about efficiency, it would take to much time to complete this using CLI (Option C).","poster":"rbm2023","comment_id":"892636"},{"comment_id":"851035","upvote_count":"1","poster":"mfsec","timestamp":"1679836200.0","content":"Selected Answer: A\nA is much more efficient"},{"timestamp":"1678407660.0","upvote_count":"1","content":"Selected Answer: C\nA and C seems to be correct but using batch requires more steps.\nhttps://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/","poster":"forceli","comment_id":"834525"},{"poster":"God_Is_Love","timestamp":"1677995700.0","upvote_count":"2","comment_id":"829668","content":"Selected Answer: A\nC is wrong. How can S3 copy encrypt ? A is correct. Refer how S3 batch operations are used to encrypt here -https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/"},{"comment_id":"826260","content":"I guess A and/or C can be because they are pretty close; after reading everything here, they are a lot of good points.","timestamp":"1677703920.0","upvote_count":"1","poster":"Sarutobi"},{"timestamp":"1676560560.0","poster":"c73bf38","comment_id":"810813","upvote_count":"3","content":"Encrypting existing objects\nTo encrypt your existing Amazon S3 objects, you can use Amazon S3 Batch Operations. You provide S3 Batch Operations with a list of objects to operate on, and Batch Operations calls the respective API to perform the specified operation. You can use the Batch Operations Copy operation to copy existing unencrypted objects and write them back to the same bucket as encrypted objects. A single Batch Operations job can perform the specified operation on billions of objects. For more information, see Performing large-scale batch operations on Amazon S3 objects and the AWS Storage Blog post Encrypting objects with Amazon S3 Batch Operations.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html?icmpid=docs_s3_hp_create_bucket_default_encryption"},{"upvote_count":"2","poster":"mmendozaf","timestamp":"1675437060.0","content":"Selected Answer: A\nTo encrypt you need to Re-copy the file and batch is more efficient.","comment_id":"797131"},{"content":"Selected Answer: A\nAs per Romidan's link, it is clear.","timestamp":"1675276020.0","poster":"Musk","upvote_count":"1","comment_id":"795402"},{"comment_id":"793830","poster":"mikeshop","timestamp":"1675153200.0","content":"Selected Answer: A\nBatch operations are more efficient for millions of objects than running the CLI command.","upvote_count":"2"},{"timestamp":"1674870480.0","upvote_count":"4","comment_id":"790161","content":"Selected Answer: A\nOption A seems efficient as per the blog - \nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/","poster":"romidan"},{"comment_id":"786641","timestamp":"1674573840.0","upvote_count":"1","poster":"vsk12","content":"Option A is correct since manual copying (Option C) for millions of objects is time-consuming."},{"poster":"masssa","content":"Selected Answer: C\nI vote C\nhttps://aws.amazon.com/jp/blogs/news/encrypting-existing-amazon-s3-objects-with-the-aws-cli/","comments":[{"poster":"mikeshop","comment_id":"793828","content":"If you read further in that post, it says that for large object stores, batch operations are more efficient.","timestamp":"1675153140.0","upvote_count":"1"},{"poster":"Musk","comment_id":"795401","upvote_count":"2","timestamp":"1675275960.0","content":"As mikeshop said: For S3 buckets with a large number of objects, in the order of millions or billions of objects, using Amazon S3 inventory or Amazon S3 Batch Operations can be a better option than using the AWS CLI instructions in this post."}],"comment_id":"785616","timestamp":"1674492240.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1674476520.0","poster":"Untamables","comment_id":"785332","content":"Selected Answer: A\nI thought option A might be correct after reading the below blog article because there were millions of objects in the S3 buckets in this scenario.\nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/"},{"timestamp":"1673772600.0","content":"Selected Answer: C\nThe correct answer is option C. Turn on SSE-S3 on both S3 buckets and encrypt the existing objects by using an S3 copy command in the AWS CLI.\nThis option is the most operationally efficient solution because it uses the built-in SSE-S3 feature of S3, which eliminates the need to create and manage additional KMS keys and encrypting existing objects using S3 copy command is a straight forward process.\n\nOption A is not the most operationally efficient solution because it requires additional steps to encrypt the objects which might take time as there are millions of objects.\n\nOption B and D are not the most operationally efficient solution because they require additional steps to create and manage KMS keys. Additionally, they also require additional steps to encrypt the existing objects.","comment_id":"776290","upvote_count":"3","poster":"masetromain"}],"answer":"A","isMC":true,"timestamp":"2023-01-15 09:50:00","question_id":521,"answer_images":[],"answer_description":"","question_text":"A company consists or two separate business units. Each business unit has its own AWS account within a single organization in AWS Organizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3 bucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.\n\nRecently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be stored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).\n\nWhat is the MOST operationally efficient solution that meets these requirements?","topic":"1","exam_id":33,"answer_ET":"A","question_images":[],"unix_timestamp":1673772600,"choices":{"C":"Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI.","B":"Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI.","A":"Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.","D":"Create an AWS Key Management Service, (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Use S3 Batch Operations to copy the objects into the same location."},"answers_community":["A (91%)","8%"]},{"id":"hn3agOUEmGOLOtR7IbAl","answer_ET":"C","answer_images":[],"timestamp":"2023-01-15 09:54:00","question_text":"A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes every day.\n\nThe company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must retain all the data indefinitely for compliance reasons.\n\nWhich solution will meet these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/95386-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"C","topic":"1","answer_description":"","question_images":[],"answers_community":["C (93%)","6%"],"discussion":[{"timestamp":"1673974800.0","content":"Selected Answer: C\nThe correct answer is C. Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.\n\nThis solution allows you to use Amazon Athena and the AWS Glue Data Catalog to query and analyze the data in an S3 bucket. Amazon Athena is a serverless, interactive query service that allows you to analyze data in S3 using SQL. The AWS Glue Data Catalog is a managed metadata repository that can be used to store and retrieve table definitions for data stored in S3. Together, these services can provide a cost-effective way to query and analyze large amounts of unstructured data. Additionally, by using an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive, you can retain the data indefinitely for compliance reasons while also reducing storage costs.","upvote_count":"21","comment_id":"779093","comments":[{"poster":"masetromain","comment_id":"779094","upvote_count":"9","comments":[{"comments":[{"upvote_count":"2","comment_id":"1147560","content":"Amazon Redshift is designed for structured data. However, Amazon Redshift Spectrum enables you to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required.","poster":"dankositzke","timestamp":"1707673920.0"}],"comment_id":"1082426","upvote_count":"4","timestamp":"1701167280.0","poster":"Japanese1","content":"This is a nonsense explanation.\nIn the first place, Redshift cannot handle unstructured data."}],"timestamp":"1673974860.0","content":"The other options are not correct because:\nA. Using S3 Select is good for filtering data in S3, but it may not be a suitable solution for querying and analyzing large amounts of data.\n\nB. Amazon Redshift Spectrum can be used to query data stored in S3, but it may not be as cost-effective as using Amazon Athena for querying unstructured data\n\nD. Using Amazon Redshift Spectrum with S3 Intelligent-Tiering could be a good solution, but S3 Intelligent-Tiering is designed to optimize storage costs based on access patterns and it would not be the best solution for compliance reasons as S3 Intelligent-Tiering will move data to other storage classes according to access patterns."}],"poster":"masetromain"},{"content":"Selected Answer: C\nGenerally, unstructured data should be converted structured data before querying them. AWS Glue can do that.\nhttps://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.html\nhttps://docs.aws.amazon.com/athena/latest/ug/glue-athena.html","upvote_count":"7","comment_id":"790789","poster":"Untamables","timestamp":"1674922500.0"},{"content":"Selected Answer: C\nB, C seem both acceptable. The reason C is selected is because redshift spectrum need Glue Data Catalog as well which is not mentioned there.","poster":"GabrielShiao","upvote_count":"1","timestamp":"1728211980.0","comment_id":"1293817"},{"timestamp":"1710682020.0","poster":"gofavad926","comment_id":"1175849","upvote_count":"1","content":"Selected Answer: C\nC, aws glue + amazon athena"},{"comments":[{"content":"This question is actually pretty difficult since both Redshift Spectrum and AWS Glue + Athena could query unstructured data. Redshift Spectrum and Athena actually cost about the same per TB. However, with Athena, you could lower the cost by compressing the data. Glue doesn't seem to cost that much either. \n\nhttps://aws.amazon.com/redshift/pricing/\nhttps://aws.amazon.com/athena/pricing/\nhttps://aws.amazon.com/glue/pricing/","poster":"djeong95","upvote_count":"1","comment_id":"1165756","timestamp":"1709566800.0"}],"comment_id":"1136555","upvote_count":"1","poster":"AimarLeo","content":"Many comments were not convincing of not using Redshift Spectrum.. the only reason I see it to exclude that option is a Redshift Spectrum MUST have a Redshift Cluster available to start the query to S3..","timestamp":"1706691600.0"},{"content":"Selected Answer: C\nA = S3 Select good for filtering an retrieve subset of data, not enough to analyze\nB = need a Redshift instance that is expensive\nC = correct (Glue Data Catalog can help putting some structure to data and Athena is good for both query and analytics, transition to Deep Archive after 1 year)\nD = see answer B + Intelligent-Tiering not the best option here","upvote_count":"2","poster":"ninomfr64","timestamp":"1705658280.0","comment_id":"1126584"},{"upvote_count":"1","poster":"nzin4x","content":"redshift spectrum vs athena: https://www.upsolver.com/blog/aws-serverless-redshift-spectrum-athena\n\nBoth are good solutions to query s3 data. However, redshift spectrum is useful for joining S3 data with other data in Redshift, and if the data is only in S3, it would be preferable to choose athena.","comment_id":"1117364","timestamp":"1704794100.0"},{"upvote_count":"2","poster":"career360guru","content":"Selected Answer: C\nC is the right answer as Data needs to be queried and Analyzed.","timestamp":"1703275320.0","comment_id":"1103673"},{"comment_id":"1089974","timestamp":"1701928380.0","content":"Athena and aws glue is more cost , so better go with A . and what is the purpose for aws glue here. AWS glue is for ETL purpose unnecessary","upvote_count":"1","poster":"subbupro"},{"comment_id":"1079860","upvote_count":"1","timestamp":"1700902920.0","content":"C correct: S3 copy command in AWS CLI is less operational processes than the batch operation.","poster":"Andy16240"},{"upvote_count":"2","content":"Selected Answer: C\nIn this particular scenario, using Amazon Athena and AWS Glue Data Catalog might be a better fit due to the large amount of data stored in S3 buckets and growing every day. Athena can query data across an entire S3 bucket or across multiple buckets, which is useful when parsing multiple files and large amounts of data.","poster":"uC6rW1aB","timestamp":"1693917300.0","comment_id":"999495"},{"comment_id":"974133","comments":[{"content":"Using an Olabiba to explain the differences between the two:\n\n1. Query Capability: Amazon Athena is a fully managed interactive query service that allows you to run SQL queries directly on your data in S3. It supports complex queries, joins, aggregations, and even nested data structures. Athena is designed for ad-hoc querying and analysis of large datasets.\n\nOn the other hand, S3 Select is a feature of Amazon S3 that allows you to retrieve a subset of data from an object using SQL expressions. It is primarily used for selective retrieval of specific data within an object, rather than running complex queries across multiple objects.","comments":[{"upvote_count":"2","timestamp":"1691345700.0","poster":"chico2023","content":"2. Data Format: Amazon Athena supports various data formats such as CSV, JSON, Parquet, Avro, and more. It can automatically infer the schema of your data or you can provide a schema explicitly. Athena can handle structured, semi-structured, and unstructured data.\n\nS3 Select, on the other hand, is limited to querying CSV, JSON, and Parquet files. It requires the data to be in a specific format and does not support nested data structures.","comment_id":"974135","comments":[{"upvote_count":"3","poster":"chico2023","comment_id":"974136","content":"3. Performance: Amazon Athena is optimized for running queries on large datasets and can parallelize the query execution across multiple nodes. It automatically scales resources based on the query complexity and data size, providing fast and efficient query performance.\n\nS3 Select, on the other hand, is designed for retrieving a subset of data from an object. It can significantly reduce the amount of data transferred over the network and improve query performance by only retrieving the necessary data.\n\n4. Cost: Both Amazon Athena and S3 Select have different pricing models. Amazon Athena charges based on the amount of data scanned by your queries, while S3 Select charges based on the amount of data selected and returned by your queries. The cost will depend on the size of your data and the complexity of your queries.","timestamp":"1691345700.0"}]}],"poster":"chico2023","comment_id":"974134","upvote_count":"2","timestamp":"1691345640.0"}],"content":"Selected Answer: C\nAnswer: C\n\nCriminally tricky question. S3 Select does the same thing as Athena but there are some differences. The key here is \"...a large amount of unstructured data...\"\nIf wasn't this, S3 Select hands down.","timestamp":"1691345580.0","upvote_count":"3","poster":"chico2023"},{"comment_id":"949865","content":"Selected Answer: C\nits a C , true question!","upvote_count":"1","poster":"Jonalb","timestamp":"1689170520.0"},{"upvote_count":"1","content":"C for sure","poster":"NikkyDicky","timestamp":"1688332800.0","comment_id":"941245"},{"comments":[{"comment_id":"938601","content":"Not the best for cost.","timestamp":"1688079300.0","poster":"rxhan","upvote_count":"1"}],"content":"Selected Answer: B\nredshift spectrum can run sql queries directly on s3","comment_id":"910344","upvote_count":"1","timestamp":"1685458080.0","poster":"johnballs221"},{"poster":"mfsec","content":"Selected Answer: C\nC is the best choice for unstructured data","upvote_count":"3","timestamp":"1679836260.0","comment_id":"851039"},{"upvote_count":"4","timestamp":"1677996960.0","comment_id":"829679","content":"Selected Answer: C\nS3 select only to select few parts of the data and here its lot of unstructured data. So A is wrong. Use Athena console to create Glue crawler as referred here - \nhttps://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html","poster":"God_Is_Love"},{"content":"I think \"semi-structured\" is the right word here, because unstructured can be videos, images or text that has no schema.\nAssuming that we want to query semi-structured data :\nI don't understand why everyone is voting Athena.\nAthena is fast in certain cases and has more features for aggregation, but we are just asking querying here (and analyzing is very vague).\nIn terms of cost, S3 select is around 2$ by TB scanned, and Athena is 5$.\nGlue data catalog brings ease of use, but is not required for querying with athena.\nS3 select is not limited in the amount of scanned data, only in the row size (1MB)\nCan someone explain ?","upvote_count":"3","poster":"sambb","comment_id":"827819","timestamp":"1677835020.0"},{"content":"Selected Answer: C\nAWS Glue Data Catalog to convert data to be structured before querying them \nAmazon Athena to query the data. \nCreate an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.","comment_id":"793411","poster":"zozza2023","upvote_count":"3","timestamp":"1675117260.0"},{"timestamp":"1673823600.0","comment_id":"777154","upvote_count":"2","poster":"zhangyu20000","content":"C is correct because it is unstructured data. You cannot use S3 select and must use Glue Crawler to generate catalg."},{"upvote_count":"3","content":"Selected Answer: A\nA is the correct answer. S3 Select allows you to query the data stored in an S3 bucket, which can be useful when you need to retrieve specific subsets of data from a large amount of data. By creating an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive, you can save cost as it is a low-cost storage class for archival data that is infrequently accessed and for which retrieval times of several hours are acceptable. This solution is most cost-effective as it allows you to keep all the data indefinitely for compliance reasons while also reducing storage costs for older data that is not frequently accessed.\n\nThe other options are not as cost-effective as they would require additional costs for data transfer, storage and query in other services.","timestamp":"1673772840.0","comment_id":"776293","poster":"masetromain"}],"choices":{"A":"Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.","C":"Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.","B":"Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old 10 S3 Glacier Deep Archive.","D":"Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering."},"isMC":true,"unix_timestamp":1673772840,"question_id":522,"exam_id":33},{"id":"kgTggKf36OtIjQBx0SeC","unix_timestamp":1673772960,"isMC":true,"exam_id":33,"question_text":"A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.\n\nThe company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the connection.\n\nWhich solution will meet these requirements MOST cost-effectively?","timestamp":"2023-01-15 09:56:00","topic":"1","answers_community":["A (100%)"],"answer_ET":"A","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95387-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","discussion":[{"comments":[{"content":"Option B is not a cost-effective solution, as setting up and maintaining a 10 Gbps Direct Connect connection can be quite expensive, especially if it's only needed for a one-time data transfer.\n\nOption C is not a cost-effective solution, as creating a VPN connection between the on-premises storage and the nearest AWS region would require significant networking configuration and maintenance, and would likely be more expensive than using Snowball Edge devices.\n\nOption D is not a cost-effective solution, as deploying an AWS Storage Gateway file gateway on premises would require additional hardware and ongoing maintenance costs, and may not be necessary for a one-time data transfer.","timestamp":"1689404160.0","upvote_count":"3","poster":"masetromain","comment_id":"776296"}],"content":"Selected Answer: A\nThe correct answer is A. Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.\n\nThis option will meet the requirements to complete the data transfer within 3 weeks, as the Snowball Edge devices can transfer large amounts of data quickly and securely. The data will be encrypted in transit and at rest. The company's internet connection speed is not a bottleneck as the data transfer will happen on the devices and not over the internet.","comment_id":"776295","poster":"masetromain","timestamp":"1689404160.0","upvote_count":"11"},{"content":"Selected Answer: A\nA = correct\nB = takes a month or more to setup DX\nC = this would take more than 3 weeks for transferring data\nD = this would take more than 3 weeks for transferring data","poster":"ninomfr64","comment_id":"1126637","upvote_count":"2","timestamp":"1721382780.0"},{"upvote_count":"1","timestamp":"1719079440.0","content":"Selected Answer: A\nOption A","poster":"career360guru","comment_id":"1103676"},{"timestamp":"1714551600.0","comment_id":"1059561","upvote_count":"1","content":"Selected Answer: A\nwish all the questions were like this. happy days :)","poster":"yorkicurke"},{"timestamp":"1707598800.0","content":"Selected Answer: A\nas we know snowball storage optimized NVMe up to 210 TB <3 A is the best and easy answer","upvote_count":"4","comments":[{"comments":[{"upvote_count":"1","timestamp":"1709220780.0","content":"several thanks too :)","poster":"chikorita","comment_id":"995143"}],"upvote_count":"1","comment_id":"978008","poster":"xplusfb","content":"like several sorry for any confision :)","timestamp":"1707598860.0"}],"comment_id":"978007","poster":"xplusfb"},{"comment_id":"941246","content":"Selected Answer: A\nA - basic snowball use case","timestamp":"1704237660.0","upvote_count":"1","poster":"NikkyDicky"},{"upvote_count":"1","timestamp":"1703170080.0","poster":"Maria2023","comment_id":"929516","content":"Selected Answer: A\nGiven the deadline (3 weeks) and the amount of data I would use Snowball Edge"},{"upvote_count":"3","poster":"mfsec","content":"Selected Answer: A\nA obviously","comment_id":"851041","timestamp":"1695733980.0"},{"timestamp":"1693927920.0","poster":"God_Is_Love","content":"Selected Answer: A\nAround 8 devices and snowball (actually a Rectangular box)\nSnowball Edge Storage Optimized device is equipped with up to 80 terabytes (TB) of storage capacity, as well as 40 vCPUs and 80 GB of memory for running compute-intensive applications. It also includes an optional GPU for accelerated computing workloads.\n\nBuilt-in security features such as tamper-resistant enclosures, an E Ink shipping label, and 256-bit encryption for data at rest and in transit.","comment_id":"830135","upvote_count":"4"},{"timestamp":"1690748520.0","poster":"zozza2023","content":"Selected Answer: A\n3 weeks + cost effective ==> Snowball Edge Storage","upvote_count":"1","comment_id":"793413"}],"question_id":523,"answer":"A","choices":{"A":"Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.","C":"Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection.","D":"Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway.","B":"Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3."},"answer_images":[]},{"id":"ayadRdMNhnktNgaDeodE","discussion":[{"comment_id":"776302","comments":[{"poster":"masetromain","comment_id":"776303","content":"Option A:\nThis option would require significant development and maintenance effort and would not take advantage of fully managed services, resulting in increased operational overhead.\n\nOption B:\nThis option is similar to option A in that it would require significant development and maintenance effort to train and host the models, and would not take advantage of fully managed services resulting in increased operational overhead.\n\nOption C:\nThis option is similar to option B in that it would require significant development and maintenance effort to train and host the models, and would not take advantage of fully managed services resulting in increased operational overhead.","timestamp":"1689404580.0","upvote_count":"3"}],"upvote_count":"14","timestamp":"1689404580.0","poster":"masetromain","content":"Selected Answer: D\nThe correct answer is D. Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.\n\nThis solution meets the requirements of accurate form extraction, minimal time to market, and minimal long-term operational overhead. Amazon Textract and Amazon Comprehend are fully managed and serverless services that can perform OCR and extract relevant data from the forms, which eliminates the need to develop custom libraries or train and host models. Using AWS Step Functions and Lambda allows for easy automation of the process and the ability to scale as needed."},{"poster":"gofavad926","upvote_count":"1","comment_id":"1175852","content":"Selected Answer: D\nD. This solution meets the requirements of accurate form extraction, minimal time to market, and minimal long-term operational overhead","timestamp":"1726572780.0"},{"timestamp":"1719079800.0","poster":"career360guru","comment_id":"1103680","upvote_count":"1","content":"Selected Answer: D\nOption D"},{"poster":"NikkyDicky","content":"Selected Answer: D\nD - basic use case for textract","timestamp":"1704237840.0","comment_id":"941247","upvote_count":"1"},{"content":"Selected Answer: D\nAn easy one - if AWS has a service for something - do not reinvent the wheel - use Textract and Comprehend","comment_id":"929522","upvote_count":"2","timestamp":"1703170260.0","poster":"Maria2023"},{"upvote_count":"1","comment_id":"920840","timestamp":"1702319280.0","poster":"SkyZeroZx","content":"Selected Answer: D\nD : Managed AWS Services"},{"poster":"mfsec","comment_id":"851045","upvote_count":"1","timestamp":"1695734040.0","content":"Selected Answer: D\nAmazon Textract.."},{"comment_id":"830341","upvote_count":"1","content":"Selected Answer: D\nTextract can analyze different types of documents such as forms, invoices, receipts, and tables, and can extract information such as text, tables, and key-value pairs.\n\nComprehend provides a set of APIs that can be used to analyze text data in real-time. The service can identify the language of the text, extract entities such as people, organizations, and locations, and detect the sentiment expressed in the text. It can also extract key phrases that summarize the meaning of the text, and can classify the text into predefined categories.","timestamp":"1693941780.0","poster":"God_Is_Love"},{"timestamp":"1693726920.0","poster":"sambb","content":"Selected Answer: D\nD : Managed AWS Services","comment_id":"827839","upvote_count":"1"}],"answer_ET":"D","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95390-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"choices":{"B":"Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.","C":"Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine teaming (AI/ML) models that are trained and hosted in Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the output in Amazon ElastiCache. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.","A":"Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tier to process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. Submit the data to the target system's APL. Host the new application tier on EC2 instances.","D":"Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API."},"answers_community":["D (100%)"],"timestamp":"2023-01-15 10:03:00","answer":"D","isMC":true,"question_text":"A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files through a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances and an Amazon RDS for PostgreSQL database.\n\nWhen forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team member then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering the information into another system that uses an API.\n\nA solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction. minimize time to market, and minimize tong-term operational overhead.\n\nWhich solution will meet these requirements?","answer_description":"","question_id":524,"topic":"1","unix_timestamp":1673773380,"question_images":[]},{"id":"9V3zHbPsqjzJVmdxD98G","discussion":[{"timestamp":"1673773800.0","content":"Selected Answer: A\nOption A is the correct answer. In this solution, the company creates an Amazon Machine Image (AMI) of the web server VM, which can be used to launch EC2 instances that are identical to the on-premises web servers. The company then creates an EC2 Auto Scaling group that uses the AMI and an Application Load Balancer (ALB) to provide automatic scaling and high availability for the web front end. The company also replaces the on-premises messaging queue (RabbitMQ) with Amazon MQ, which is a managed message broker service that is fully compatible with RabbitMQ. Finally, the company uses Amazon Elastic Kubernetes Service (EKS) to host the order-processing backend, which allows them to run their existing Kubernetes cluster in the AWS cloud without making any major changes to the application. This approach allows the company to lift and shift their existing platform with minimal operational overhead.","poster":"masetromain","upvote_count":"20","comments":[{"comment_id":"1328787","poster":"pk0619","timestamp":"1734567900.0","content":"AMI is an AWS EC2 specific, I am confused on how to create an AMI of an on-premise VM and launch instance from it ?","comments":[{"timestamp":"1734568080.0","content":"Looking back, it seems the intent might have been to use VM Import/Export or the AWS Application Migration Service (MGN) to create an AMI. However, it is a significant oversimplification to skip those critical steps and simply state \"create an AMI,\" as this assumes the process is straightforward without addressing the necessary prerequisites and tools.","upvote_count":"1","comment_id":"1328788","poster":"pk0619"}],"upvote_count":"1"},{"comments":[{"poster":"sambb","comment_id":"827846","content":"Your justification for option C is wrong.\nOption C is valid, as Kubernetes on EC2 is very similar as the existing Kubernetes environment on-premises. But EKS is a safe bet and reduces operational overhead, while keeping the same API as previously. Hence, A is a better choice.","upvote_count":"10","timestamp":"1677836820.0"}],"timestamp":"1673773800.0","poster":"masetromain","comment_id":"776314","content":"Option B, using a custom AWS Lambda runtime and Amazon API Gateway, would require significant changes to the application and may not be compatible with the current codebase. \n\nOption C, installing Kubernetes on a fleet of different EC2 instances, would also require significant changes to the application and may not be compatible with the current codebase. \n\nOption D, using Amazon Simple Queue Service (Amazon SQS) instead of Amazon MQ, would not provide the same level of messaging capabilities as Amazon MQ and may not be sufficient for the needs of the order-processing platform.","upvote_count":"4"}],"comment_id":"776312"},{"content":"AWS exams got more 'sarcastic' with the ways of formulating questions.. E.g here: _'A company is refactoring its on-premises order-processing platform in the AWS Cloud' \nBUT '\nThe company does not want to make any major changes to the application.\n\nReplatforming and Rehosting is not real refactoring.. but the closest answer as an architect with least operational overhead is A obvisouly.. aws questions sometimes can be ultra vague","upvote_count":"6","comment_id":"1136586","timestamp":"1706693160.0","poster":"AimarLeo"},{"upvote_count":"1","poster":"madeesha","comment_id":"1228792","content":"Selected Answer: A\nanswer is A","timestamp":"1718160780.0"},{"comment_id":"1175854","timestamp":"1710682440.0","poster":"gofavad926","content":"Selected Answer: A\nA, is the only option to don't involve a rearchitectured solution","upvote_count":"1"},{"content":"Selected Answer: A\nA better explanation to choose between option A and D is that Amazon MQ respondes to the requirement of not changing the app, because it accepts the same protocol as RabbitMQ (Supports AMQP, MQTT, STOMP, OpenWire, and JMS) while SQS has its own API, so it would need more changes to the app.","upvote_count":"3","timestamp":"1704307440.0","comment_id":"1113030","poster":"jpa8300"},{"timestamp":"1703276220.0","poster":"career360guru","content":"Selected Answer: A\nOption A","upvote_count":"1","comment_id":"1103683"},{"upvote_count":"2","comment_id":"1068850","poster":"Mikado211","content":"Selected Answer: A\na bunch of keywords for this migration here : \nKubernetes == EKS\nRabbitMQ == Amazon MQ\nA fleet of VM == AMI + ec2 instances\n\nThe answer A proposes all thoses points, so it's perfect here.","timestamp":"1699819440.0"},{"poster":"NikkyDicky","content":"Selected Answer: A\nA no doubt","timestamp":"1688333160.0","upvote_count":"1","comment_id":"941249"},{"content":"Selected Answer: A\nA is the best choice.","timestamp":"1679836560.0","poster":"mfsec","upvote_count":"1","comment_id":"851051"},{"poster":"Musk","comments":[{"content":"It says the company does not want to make changes to the application in the problem statement. B would require significant code changes to the application.","comment_id":"817400","poster":"c73bf38","timestamp":"1677033420.0","upvote_count":"6"}],"timestamp":"1676887980.0","upvote_count":"2","comment_id":"815096","content":"Selected Answer: B\nOption A is re-hosting or mybe re-platforming. The question says the purpose is re-factoring, then it's B."}],"exam_id":33,"answer_images":[],"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/95391-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"B":"Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.","C":"Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend.","D":"Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.","A":"Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend."},"timestamp":"2023-01-15 10:10:00","topic":"1","question_images":[],"question_text":"A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_description":"","answer":"A","question_id":525,"unix_timestamp":1673773800,"isMC":true,"answers_community":["A (94%)","6%"]}],"exam":{"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true,"lastUpdated":"11 Apr 2025","id":33,"isMCOnly":true,"provider":"Amazon","numberOfQuestions":529},"currentPage":105},"__N_SSP":true}