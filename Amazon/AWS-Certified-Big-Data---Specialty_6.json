{"pageProps":{"questions":[{"id":"Atpk3Uva5YWOTQF3N39b","url":"https://www.examtopics.com/discussions/amazon/view/5348-exam-aws-certified-big-data-specialty-topic-1-question-32/","unix_timestamp":1568780880,"answers_community":[],"question_images":[],"answer_description":"","answer_images":[],"question_text":"A travel website needs to present a graphical quantitative summary of its daily bookings to website visitors for marketing purposes. The website has millions of visitors per day, but wants to control costs by implementing the least-expensive solution for this visualization.\nWhat is the most cost-effective solution?","question_id":26,"topic":"1","answer":"A","answer_ET":"A","timestamp":"2019-09-18 06:28:00","discussion":[{"upvote_count":"6","comment_id":"11534","content":"A. transient cluster is cost-effective solution","poster":"exams","timestamp":"1632987480.0"},{"poster":"san2020","upvote_count":"3","timestamp":"1634475900.0","content":"my selection A","comment_id":"52346"},{"timestamp":"1633427220.0","poster":"kalpanareddy","comment_id":"42661","content":"OPTION C and D are ruled out . AS it is for marketing purpose only they can just generate graph daily once and store it in S3. webiste can access the graph using S3 static web hosting. So Option A is best answer","upvote_count":"1"}],"isMC":true,"choices":{"A":"Generate a static graph with a transient EMR cluster daily, and store it an Amazon S3.","C":"Implement a Jupyter front-end provided by a continuously running EMR cluster leveraging spot instances for task nodes.","B":"Generate a graph using MicroStrategy backed by a transient EMR cluster.","D":"Implement a Zeppelin application that runs on a long-running EMR cluster."},"exam_id":17},{"id":"lNx88W0nUgEXqlRBShQ5","discussion":[{"upvote_count":"5","timestamp":"1633173900.0","poster":"apertus","content":"Should be A. Vault access policy can be modified, so it mean the data can be tampered when someone change the vault access policy. Vault lock policy cannot be modified, so it can say 'never be tampered'","comment_id":"11209"},{"content":"A for sure.\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html\n\"As an example of a Vault Lock policy, suppose that you are required to retain archives for one year before you can delete them. To implement this requirement, you can create a Vault Lock policy that denies users permissions to delete an archive until the archive has existed for one year. You can test this policy before locking it down.\"\nYou can also find a policy with a Deny effect on \"glacier:DeleteArchive\" action on the same link.","upvote_count":"2","poster":"vicks316","comment_id":"183278","timestamp":"1635751080.0"},{"poster":"guruguru","upvote_count":"1","timestamp":"1635630120.0","comment_id":"117102","content":"A. Because the data requires NEVER be tampered, after uploaded."},{"poster":"awane","timestamp":"1634997060.0","comment_id":"103868","content":"Should be A :\nAWS Docs : https://d1.awsstatic.com/Projects/P4113791/aws-project_set-up-compliant-archive.pdf\nPage 4 : A vault lock policy is different than a vault access policy. Both policies govern access controls to your vault. \nHowever, a vault lock policy can be locked to prevent future changes, providing strong enforcement for your compliance controls. \nYou can use the vault lock policy to deploy regulatory and compliance\ncontrols, which typically require tight controls on data access. \nIn contrast, you use a vault access policy to implement access controls that are not compliance related, temporary, and subject to frequent modification. Vault lock and vault access policies can be used together. For example, you can implement time-based data retention rules in the vault lock policy (deny deletes), and grant read access to designated third parties or your business partners (allow reads).","upvote_count":"1"},{"comment_id":"67959","poster":"Jayraam","content":"Answer is A based on AWS Documentation below.\n\nAn Amazon S3 Glacier (S3 Glacier) vault can have one resource-based vault access policy and one Vault Lock policy attached to it. A Vault Lock policy is a vault access policy that you can lock. Using a Vault Lock policy can help you enforce regulatory and compliance requirements. Amazon S3 Glacier provides a set of API operations for you to manage the Vault Lock policies, see Locking a Vault by Using the Amazon S3 Glacier API.\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html\nAs an example of a Vault Lock policy, suppose that you are required to retain archives for one year before you can delete them. To implement this requirement, you can create a Vault Lock policy that denies users permissions to delete an archive until the archive has existed for one year. You can test this policy before locking it down. After you lock the policy, the policy becomes immutable. For more information about the locking process, see Amazon S3 Glacier Vault Lock. If you want to manage other user permissions that can be changed, you can use the vault access policy (see Amazon S3 Glacier Access Control with Vault Access Policies).","timestamp":"1634883060.0","upvote_count":"2"},{"poster":"AdamSmith","comment_id":"59188","content":"A\n\"A vault lock policy is different than a vault access policy. Both policies govern access controls to your vault. However, a vault lock policy can be locked to prevent future changes, providing strong enforcement for your compliance controls. You can use the vault lock policy to deploy regulatory and compliance controls, which typically require tight controls on data access. In contrast, you use a vault access policy to implement access controls that are not compliance related, temporary, and subject to frequent modification\"\n\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html","upvote_count":"3","timestamp":"1634731620.0","comments":[{"comment_id":"62468","poster":"piemar","content":"Agree\n\nSuppose that you have a regulatory requirement to retain archives for up to one year before you can delete them. You can enforce that requirement by implementing the following Vault Lock policy. The policy denies the glacier:DeleteArchive action on the examplevault vault if the archive being deleted is less than one year old. The policy uses the S3 Glacier-specific condition key ArchiveAgeInDays to enforce the one-year retention requirement.","upvote_count":"1","timestamp":"1634848560.0"}]},{"comment_id":"55603","timestamp":"1634607300.0","content":"i think it should be A. \"You can use the Vault Lock policy to deploy regulatory and compliance controls that are typically restrictive and are “set and forget” in nature.\" Here also they use the word 'Never","poster":"sam3787","upvote_count":"2"},{"upvote_count":"1","content":"my selection A","comment_id":"52347","timestamp":"1634588100.0","poster":"san2020"},{"comment_id":"40303","upvote_count":"1","content":"Answer: A\nhttps://aws.amazon.com/glacier/faqs/ -- refer Vault Lock section","poster":"shan75","timestamp":"1634383680.0"},{"poster":"Tomo","content":"You can now create a Vault Lock policy on a vault, and after it is locked, the policy cannot be overwritten or deleted. For SaaS, one should have chance to delete customers' data. Any suggestions?","timestamp":"1634359620.0","upvote_count":"1","comment_id":"30898"},{"upvote_count":"1","comment_id":"22258","poster":"Raju_k","timestamp":"1634204340.0","content":"It would be A over C since Vault Lock policy is immutable and it satisfies the requirement\nthat data will never be tampered once uploaded."},{"comment_id":"21414","timestamp":"1634138100.0","poster":"viduvivek","content":"Answer is A\n\nA Glacier \"vault access policy\" is a resource based policy that you can use to manage permissions to your vault.You can modify permissions in a Vault access policy at any time. \n\nA Glacier \"vault lock policy\" is vault access policy that can be locked. After you lock a vault lock policy, the policy cannot be changed. You can use a vault lock policy to enforce compliance controls. \n\nYou can enforce the requirement by implementing the following vault lock policy: \"glacier:DeleteArchieve\" action on the vault.","upvote_count":"4"},{"content":"I will go with A. As there is no \"glacier:DeleteArchive\" option in Vault Access Policy","upvote_count":"2","timestamp":"1634068440.0","poster":"BigEv","comment_id":"19818"},{"poster":"WWODIN","comment_id":"18448","comments":[{"timestamp":"1633887720.0","upvote_count":"1","poster":"cybe001","comment_id":"19241","content":"It is A, read FAQ for the difference between Vault Lock vs Vault Access"}],"timestamp":"1633689960.0","upvote_count":"3","content":"Should be A\nhttps://aws.amazon.com/glacier/faqs/\na Vault Lock policy can be made immutable ...."},{"timestamp":"1633623540.0","poster":"asadao","upvote_count":"1","comment_id":"18257","content":"It is C"},{"content":"I think it is \"A\" (vault lock) ..because the question says \"..makes sure that data will never be tampered with once it has been uploaded...\" . If it is vault-access, you can change it after anytime and this is not permitted according the question.","upvote_count":"1","poster":"VB","comment_id":"11955","timestamp":"1633233780.0"},{"poster":"muhsin","upvote_count":"3","comments":[{"comment_id":"8784","poster":"mattyb123","content":"Doesn't the question ask never be tampered with, meaning no user access it at all? From the link it mentions the below: A vault lock policy is different than a vault access policy. Both policies govern access controls to your vault. However, a vault lock policy can be locked to prevent future changes, providing strong enforcement for your compliance controls. You can use the vault lock policy to deploy regulatory and compliance controls, which typically require tight controls on data access. In contrast, you use a vault access policy to implement access controls that are not compliance related, temporary, and subject to frequent modification. Vault lock and vault access policies can be used together. For example, you can implement time-based data retention rules in the vault lock policy (deny deletes), and grant read access to designated third parties or your business partners (allow reads).","timestamp":"1632619800.0","upvote_count":"3"},{"timestamp":"1632739920.0","content":"Apologies it is C. \nhttps://aws.amazon.com/glacier/faqs/\nVault access policies can make certain use cases simpler. For example, to protect information in a business-critical vault from unintended deletion, you can create a vault access policy that denies delete attempts from all users.","comment_id":"8943","upvote_count":"1","poster":"mattyb123","comments":[{"upvote_count":"1","comment_id":"110933","content":"It should be A. If someone was able to remove the vault access policy then the data can be tampered with. The keyword is \"never\". Therefore the vault lock policy will 100% gaurantee the data will not be tampered as you cannot change the vault lock policy once it is created","timestamp":"1635263040.0","poster":"Nik225"}]}],"content":"policy name is vault lock policy. but the configuration is being done with vaul-access-policy\nc is the answer.","comment_id":"8647","timestamp":"1632605400.0"},{"comment_id":"7560","content":"A itis","timestamp":"1632530340.0","upvote_count":"1","poster":"jlpl"},{"comment_id":"7532","poster":"pra276","upvote_count":"1","comments":[{"upvote_count":"2","timestamp":"1632675480.0","comment_id":"8942","content":"Apologies it is C. \nhttps://aws.amazon.com/glacier/faqs/\nVault access policies can make certain use cases simpler. For example, to protect information in a business-critical vault from unintended deletion, you can create a vault access policy that denies delete attempts from all users.","poster":"mattyb123"}],"content":"Answer is C: \"Deny\" Access vault policy","timestamp":"1632503280.0"},{"upvote_count":"3","comments":[{"poster":"mattyb123","upvote_count":"1","comment_id":"6657","timestamp":"1632486000.0","content":"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-access-policy.html states: Glacier also supports a Vault Lock policy on each vault that, after you lock it, cannot be altered."}],"content":"reference link even refers to vault-lock-policy","poster":"mattyb123","timestamp":"1632436500.0","comment_id":"6417"},{"timestamp":"1632435600.0","content":"I think this answer should be A","comment_id":"6416","poster":"mattyb123","upvote_count":"3"}],"exam_id":17,"url":"https://www.examtopics.com/discussions/amazon/view/3413-exam-aws-certified-big-data-specialty-topic-1-question-33/","timestamp":"2019-08-10 03:35:00","answer_description":"Reference: https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock-policy.html","choices":{"B":"Create an Amazon S3 bucket. Specify a \"Deny\" bucket policy on this bucket to block \"s3:DeleteObject\".","A":"Create an Amazon Glacier Vault. Specify a \"Deny\" Vault Lock policy on this Vault to block \"glacier:DeleteArchive\".","C":"Create an Amazon Glacier Vault. Specify a \"Deny\" vault access policy on this Vault to block \"glacier:DeleteArchive\".","D":"Create secondary AWS Account containing an Amazon S3 bucket. Grant \"s3:PutObject\" to the primary account."},"unix_timestamp":1565400900,"topic":"1","question_text":"A system engineer for a company proposes digitalization and backup of large archives for customers. The systems engineer needs to provide users with a secure storage that makes sure that data will never be tampered with once it has been uploaded.\nHow should this be accomplished?","question_id":27,"answer":"C","answer_ET":"C","answers_community":[],"answer_images":[],"isMC":true,"question_images":[]},{"id":"TNbGksm1g9WnmNBKi2zB","answer_images":[],"answer_description":"","isMC":true,"discussion":[{"content":"A is obvious but when you say Reveal solution why it is showing C. very misleading answers","comment_id":"137355","poster":"srikrish","upvote_count":"2","timestamp":"1636246800.0"},{"upvote_count":"4","poster":"san2020","timestamp":"1636188720.0","comment_id":"52348","content":"my selection A"},{"timestamp":"1635876180.0","poster":"michelleY","comment_id":"35027","content":"A is correct.","upvote_count":"1"},{"poster":"antoneti","comment_id":"25876","content":"same over here, mine would be A for sure","upvote_count":"1","timestamp":"1635831660.0"},{"content":"Agreed.A would be cost effective solution.","poster":"Raju_k","upvote_count":"1","comment_id":"22259","timestamp":"1634543100.0"},{"comment_id":"19242","poster":"cybe001","content":"A is cost-effective","timestamp":"1634189040.0","upvote_count":"2"},{"poster":"mattyb123","upvote_count":"4","content":"Thoughts on A. Highly flexible storage system makes me think s3","timestamp":"1632202560.0","comment_id":"6900","comments":[{"comment_id":"7033","poster":"mattyb123","content":"Anyone agree or disagree?","timestamp":"1632478140.0","upvote_count":"1","comments":[{"comments":[{"comment_id":"9564","content":"I also agree with A from Cost-effective","upvote_count":"1","poster":"Jialu","timestamp":"1632916500.0"}],"upvote_count":"1","content":"Agreed: Answer should be A","poster":"pra276","timestamp":"1632750180.0","comment_id":"7533"}]}]}],"url":"https://www.examtopics.com/discussions/amazon/view/3598-exam-aws-certified-big-data-specialty-topic-1-question-34/","question_text":"An organization needs to design and deploy a large-scale data storage solution that will be highly durable and highly flexible with respect to the type and structure of data being stored. The data to be stored will be sent or generated from a variety of sources and must be persistently available for access and processing by multiple applications.\nWhat is the most cost-effective technique to meet these requirements?","answers_community":[],"question_images":[],"timestamp":"2019-08-14 23:23:00","answer":"C","exam_id":17,"answer_ET":"C","choices":{"A":"Use Amazon Simple Storage Service (S3) as the actual data storage system, coupled with appropriate tools for ingestion/acquisition of data and for subsequent processing and querying.","D":"Launch an Amazon Relational Database Service (RDS), and use the enterprise grade and capacity of the Amazon Aurora engine for storage, processing, and querying.","B":"Deploy a long-running Amazon Elastic MapReduce (EMR) cluster with Amazon Elastic Block Store (EBS) volumes for persistent HDFS storage and appropriate Hadoop ecosystem tools for processing and querying.","C":"Use Amazon Redshift with data replication to Amazon Simple Storage Service (S3) for comprehensive durable data storage, processing, and querying."},"unix_timestamp":1565817780,"question_id":28,"topic":"1"},{"id":"bxbS53icANNxV1nbKlxV","discussion":[{"poster":"Corram","comment_id":"98907","upvote_count":"3","content":"EMRFS consistent view is made for this, so A.","timestamp":"1632862560.0"},{"poster":"san2020","comment_id":"52351","content":"my selection A","upvote_count":"2","timestamp":"1632840660.0"},{"upvote_count":"3","timestamp":"1632102840.0","content":"I agree with A","poster":"exams","comment_id":"11536"}],"timestamp":"2019-09-18 06:33:00","url":"https://www.examtopics.com/discussions/amazon/view/5349-exam-aws-certified-big-data-specialty-topic-1-question-35/","answer_ET":"A","answers_community":[],"choices":{"A":"Turn on EMRFS consistent view when configuring the EMR cluster.","C":"Set hadoop.data.consistency = true in the core-site.xml file.","D":"Set hadoop.s3.consistency = true in the core-site.xml file.","B":"Use AWS Data Pipeline to orchestrate the data processing cycles."},"topic":"1","answer_description":"","isMC":true,"question_id":29,"answer":"A","question_images":[],"answer_images":[],"question_text":"A customer has a machine learning workflow that consists of multiple quick cycles of reads-writes-reads on\nAmazon S3. The customer needs to run the workflow on EMR but is concerned that the reads in subsequent cycles will miss new data critical to the machine learning from the prior cycles.\nHow should the customer accomplish this?","unix_timestamp":1568781180,"exam_id":17},{"id":"ayUOiXVkK7n8lrQt5U6B","url":"https://www.examtopics.com/discussions/amazon/view/3362-exam-aws-certified-big-data-specialty-topic-1-question-36/","choices":{"D":"In the source region, enable cross-region replication and specify the name of the copy grant created.","E":"In the destination region, enable cross-region replication and specify the name of the copy grant created.","A":"Create a new KMS key in the destination region.","B":"Copy the existing KMS key to the destination region.","C":"Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the source region."},"answer":"ABD","isMC":true,"answer_images":[],"discussion":[{"upvote_count":"1","content":"Question 66 is the same","comment_id":"275175","timestamp":"1635801420.0","poster":"Magicroko"},{"content":"ACD is correct. Direct question from \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#working-with-aws-kms","comment_id":"123340","timestamp":"1635775860.0","poster":"yogesh88","upvote_count":"1"},{"comment_id":"52352","content":"my selection ACD","poster":"san2020","upvote_count":"2","timestamp":"1635343260.0"},{"upvote_count":"2","poster":"ME2000","timestamp":"1635241320.0","comment_id":"39855","content":"Here one option is missing\nC. Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the destination region. (Correct)\nhttps://chercher.tech/aws-certification/aws-certified-big-data-speciality-practice-exams-set-1\n\nfind here step by step AWS Redshift cross-region copy snapshot through the console (Question ask for CLI but both has same steps)\nhttps://www.youtube.com/watch?v=9DepoiBOe6o"},{"upvote_count":"1","content":"i think ACD.","poster":"michelleY","comment_id":"35029","timestamp":"1635103500.0"},{"upvote_count":"3","content":"Before the snapshot is copied to the destination AWS Region, Amazon Redshift decrypts the snapshot using the master key in the source AWS Region and re-encrypts it temporarily using a randomly generated RSA key that Amazon Redshift manages internally. Amazon Redshift then copies the snapshot over a secure channel to the destination AWS Region, decrypts the snapshot using the internally managed RSA key, and then re-encrypts the snapshot using the master key in the destination AWS Region.","poster":"cert_learner","timestamp":"1634680200.0","comment_id":"33425"},{"timestamp":"1634118720.0","upvote_count":"2","content":"I would choose ACD though C is not accurate answer as suggested by Mattyb123","poster":"Raju_k","comment_id":"22283"},{"content":"I agree with ACD.there was one more option as below. though that is wrong option. \nUse CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the source region.","timestamp":"1633930800.0","comment_id":"19450","poster":"samiraninside","upvote_count":"2"},{"content":"ACD is correct","timestamp":"1633125420.0","poster":"cybe001","upvote_count":"1","comment_id":"19243"},{"comment_id":"7687","timestamp":"1632992760.0","poster":"pra276","comments":[{"comment_id":"25877","timestamp":"1634557920.0","poster":"antoneti","content":"why destination? the cross-region replica is made from the source so the source is the one to have access to the KMS created","comments":[{"timestamp":"1635757620.0","upvote_count":"1","content":"If you want to enable cross-Region snapshot copy for an AWS KMS–encrypted cluster, you must configure a snapshot copy grant for a master key in the destination AWS Region. By doing this, you enable Amazon Redshift to perform encryption operations in the destination AWS Region\n\nLink: https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#xregioncopy-kms-encrypted-snapshot","comment_id":"113352","poster":"Soona_Paana"}],"upvote_count":"1"},{"timestamp":"1633018080.0","comment_id":"8640","upvote_count":"2","poster":"mattyb123","content":"Same question is on page 14. https://www.examtopics.com/exams/amazon/aws-certified-big-data-specialty/view/14/ ADF is correct answer. as F includes Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key created in the destination region"}],"upvote_count":"3","content":"C option should be like Use CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key created in the destination region NOT source region"},{"content":"ACD is the correct answer","poster":"Jialu","comments":[{"content":"ACD is correct","upvote_count":"1","poster":"pra276","timestamp":"1632831420.0","comment_id":"7534"}],"upvote_count":"2","comment_id":"6929","timestamp":"1632817620.0"},{"upvote_count":"2","comments":[{"timestamp":"1632780060.0","upvote_count":"1","poster":"mattyb123","comment_id":"6664","content":"https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"}],"poster":"mattyb123","comment_id":"6418","content":"Agreed ACD.","timestamp":"1632297060.0"},{"content":"acd ? anyone?","upvote_count":"2","timestamp":"1632268440.0","poster":"jlpl","comment_id":"6307"}],"answer_ET":"ABD","exam_id":17,"unix_timestamp":1565263260,"answers_community":[],"answer_description":"Reference: https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#working-with- aws-kms","topic":"1","question_text":"An Amazon Redshift Database is encrypted using KMS. A data engineer needs to use the AWS CLI to create a KMS encrypted snapshot of the database in another AWS region.\nWhich three steps should the data engineer take to accomplish this task? (Choose three.)","question_images":[],"timestamp":"2019-08-08 13:21:00","question_id":30}],"exam":{"id":17,"name":"AWS Certified Big Data - Specialty","isImplemented":true,"isMCOnly":true,"numberOfQuestions":85,"provider":"Amazon","isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":6},"__N_SSP":true}