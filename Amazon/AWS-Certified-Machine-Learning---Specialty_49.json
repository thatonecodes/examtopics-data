{"pageProps":{"questions":[{"id":"NISRMPLrsENSvdooDOb8","choices":{"C":"Use Amazon SageMaker automatic model tuning (AMT). Specify a range of values for each hyperparameter.","A":"Use a bootstrap script to install scikit-learn on an Amazon EMR cluster. Deploy the EMR cluster. Apply k-fold cross-validation methods to the algorithm.","D":"Subscribe to an AUC algorithm that is on AWS Marketplace. Specify a range of values for each hyperparameter.","B":"Deploy Amazon SageMaker prebuilt Docker images that have scikit-learn installed. Apply k-fold cross-validation methods to the algorithm."},"topic":"1","answer":"C","timestamp":"2024-03-10 16:46:00","answer_images":[],"discussion":[{"upvote_count":"2","comment_id":"1204524","timestamp":"1730293920.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html","poster":"Peter_Hsieh"},{"poster":"vkbajoria","upvote_count":"1","content":"Selected Answer: C\nautomated model Tuning will be the best solution here","comment_id":"1178859","timestamp":"1726869480.0"},{"poster":"rav009","comment_id":"1176284","content":"Selected Answer: C\nAmazon SageMaker automatic model tuning (AMT) for sure","timestamp":"1726631460.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1726147740.0","content":"Selected Answer: C\nAutomated model tuning minimizes operational overhead because it automates the entire process of hyperparameter tuning, including setting up and managing the training jobs, tracking performance metrics, and selecting the best model configuration","comment_id":"1171795","poster":"AIWave"},{"timestamp":"1725975960.0","poster":"F1Fan","upvote_count":"1","content":"C. Use Amazon SageMaker automatic model tuning (AMT). Specify a range of values for each hyperparameter.","comment_id":"1170422"}],"question_id":241,"url":"https://www.examtopics.com/discussions/amazon/view/135663-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"C","answers_community":["C (100%)"],"question_text":"A machine learning (ML) specialist needs to solve a binary classification problem for a marketing dataset. The ML specialist must maximize the Area Under the ROC Curve (AUC) of the algorithm by training an XGBoost algorithm. The ML specialist must find values for the eta, alpha, min_child_weight, and max_depth hyperparameters that will generate the most accurate model.\n\nWhich approach will meet these requirements with the LEAST operational overhead?","exam_id":26,"answer_description":"","isMC":true,"unix_timestamp":1710085560,"question_images":[]},{"id":"YJ3lF37Yp9fKeGUMaosd","answer_ET":"A","question_id":242,"answer":"A","answers_community":["A (100%)"],"choices":{"C":"Use a SageMaker notebook instance to perform a singular value decomposition analysis.","D":"Use the multicollinearity feature to perform a lasso feature selection to perform an importance scores analysis.","B":"Use a SageMaker notebook instance to perform principal component analysis (PCA).","A":"Use SageMaker Data Wrangler to perform a Gini importance score analysis."},"discussion":[{"comment_id":"1204519","timestamp":"1730293500.0","upvote_count":"2","content":"Selected Answer: A\nThe Quick model of Data Wrangler calculates feature importance for each feature using the Gini importance method.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html","poster":"Peter_Hsieh"},{"upvote_count":"1","timestamp":"1726257000.0","comment_id":"1172926","poster":"AIWave","content":"Selected Answer: A\nWrangler allows to calculate feature importance scores using Gini importance"},{"comment_id":"1170423","timestamp":"1725976020.0","content":"A. Use SageMaker Data Wrangler to perform a Gini importance score analysis.\n\nBy using the Gini importance score analysis in Data Wrangler, the ML developer can obtain importance scores for each feature of the sales dataset with minimal development effort, as it is a built-in functionality with a visual interface. This approach requires no coding or additional setup, making it the least effort-intensive solution compared to the other options involving custom coding or separate analyses.","upvote_count":"1","poster":"F1Fan"}],"answer_images":[],"isMC":true,"question_text":"A machine learning (ML) developer for an online retailer recently uploaded a sales dataset into Amazon SageMaker Studio. The ML developer wants to obtain importance scores for each feature of the dataset. The ML developer will use the importance scores to feature engineer the dataset.\n\nWhich solution will meet this requirement with the LEAST development effort?","exam_id":26,"timestamp":"2024-03-10 16:47:00","answer_description":"","topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/135664-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1710085620},{"id":"wBz3sx27fak0lOOLr2QC","answers_community":["A (100%)"],"answer_ET":"A","isMC":true,"answer":"A","answer_description":"","question_text":"A company is setting up a mechanism for data scientists and engineers from different departments to access an Amazon SageMaker Studio domain. Each department has a unique SageMaker Studio domain.\n\nThe company wants to build a central proxy application that data scientists and engineers can log in to by using their corporate credentials. The proxy application will authenticate users by using the company's existing Identity provider (IdP). The application will then route users to the appropriate SageMaker Studio domain.\n\nThe company plans to maintain a table in Amazon DynamoDB that contains SageMaker domains for each department.\n\nHow should the company meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/135665-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"question_id":243,"discussion":[{"upvote_count":"2","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreatePresignedDomainUrl.html","poster":"Peter_Hsieh","comment_id":"1204501","timestamp":"1730292540.0"},{"timestamp":"1726280160.0","upvote_count":"2","comment_id":"1173091","poster":"AIWave","content":"Selected Answer: A\n- Domain specific presigned URL's can be generated through dynamodb to route users to correct domain\n- central proxy app can authenticate existing users with company IDP -> retrieve presigned url for users dept from dynamodb -> redirect user to sagemake domain without needing direct auth on AWS"},{"comment_id":"1170424","timestamp":"1725976260.0","poster":"F1Fan","content":"A. Use the SageMaker CreatePresignedDomainUrl API to generate a presigned URL for each domain according to the DynamoDB table. Pass the presigned URL to the proxy application.\n\nThe AWS documentation mentions the CreatePresignedDomainUrl API, which generates a presigned URL that authenticates a user to a specified Amazon SageMaker Domain. By using this API, the company can generate presigned URLs for each department's SageMaker Studio domain based on the information stored in the DynamoDB table. These presigned URLs can then be passed to the central proxy application, which can authenticate users using the company's existing Identity Provider (IdP) and provide them with the appropriate presigned URL for their department's SageMaker Studio domain. When the user accesses the presigned URL, they will be automatically authenticated and routed to the corresponding SageMaker Studio domain.","upvote_count":"1"}],"choices":{"B":"Use the SageMaker CreateHumanTaskUi API to generate a UI URL. Pass the URL to the proxy application.","C":"Use the Amazon SageMaker ListHumanTaskUis API to list all UI URLs. Pass the appropriate URL to the DynamoDB table so that the proxy application can use the URL.","D":"Use the SageMaker CreatePresignedNotebooklnstanceUrl API to generate a presigned URL. Pass the presigned URL to the proxy application.","A":"Use the SageMaker CreatePresignedDomainUrl API to generate a presigned URL for each domain according to the DynamoDB table. Pass the presigned URL to the proxy application."},"answer_images":[],"question_images":[],"timestamp":"2024-03-10 16:51:00","unix_timestamp":1710085860,"topic":"1"},{"id":"eVA2Zj3ZMiQg4daxgjoy","question_text":"An insurance company is creating an application to automate car insurance claims. A machine learning (ML) specialist used an Amazon SageMaker Object Detection - TensorFlow built-in algorithm to train a model to detect scratches and dents in images of cars. After the model was trained, the ML specialist noticed that the model performed better on the training dataset than on the testing dataset.\n\nWhich approach should the ML specialist use to improve the performance of the model on the testing data?","topic":"1","choices":{"B":"Reduce the value of the dropout_rate hyperparameter.","A":"Increase the value of the momentum hyperparameter.","D":"Increase the value of the L2 hyperparameter.","C":"Reduce the value of the learning_rate hyperparameter"},"question_images":[],"unix_timestamp":1710390420,"answer_images":[],"timestamp":"2024-03-14 05:27:00","answer":"D","question_id":244,"exam_id":26,"answers_community":["D (100%)"],"discussion":[{"content":"Selected Answer: D\nD\nIf your model is overfitting the training data, it makes sense to take actions that reduce model flexibility. To reduce model flexibility, try the following:\n\nFeature selection: consider using fewer feature combinations, decrease n-grams size, and decrease the number of numeric attribute bins.\nIncrease the amount of regularization used.\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html","upvote_count":"1","comment_id":"1204490","timestamp":"1730291460.0","poster":"Peter_Hsieh"},{"poster":"vkbajoria","comment_id":"1186072","timestamp":"1727690100.0","content":"Selected Answer: D\nIncrease the value of L2 regularization","upvote_count":"1"},{"comment_id":"1173099","timestamp":"1726280820.0","poster":"AIWave","upvote_count":"1","content":"Selected Answer: D\nA- No - momentum is for SGD with momentum, N/A in this case\nB: No - reducing dropout may or may not help \nC: No - reducing learning rate may increase overfitting even further\nD: Yes - L2 regularization penalizes large weights in the model. Increasing can help prevent overfitting by encouraging smaller weights."}],"url":"https://www.examtopics.com/discussions/amazon/view/135988-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","isMC":true,"answer_ET":"D"},{"id":"sn3YX4J0p7qc3YplTTdG","answer":"C","answer_description":"","question_text":"A developer at a retail company is creating a daily demand forecasting model. The company stores the historical hourly demand data in an Amazon S3 bucket. However, the historical data does not include demand data for some hours.\n\nThe developer wants to verify that an autoregressive integrated moving average (ARIMA) approach will be a suitable model for the use case.\n\nHow should the developer verify the suitability of an ARIMA approach?","question_id":245,"url":"https://www.examtopics.com/discussions/amazon/view/135990-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer_images":[],"choices":{"D":"Use Amazon SageMaker Autopilot. Create a new experiment that specifies the S3 data location. Impute missing hourly values. Choose ARIMA as the machine learning (ML) problem. Check the model performance.","A":"Use Amazon SageMaker Data Wrangler. Import the data from Amazon S3. Impute hourly missing data. Perform a Seasonal Trend decomposition.","C":"Use Amazon SageMaker Data Wrangler. Import the data from Amazon S3. Resample data by using the aggregate daily total. Perform a Seasonal Trend decomposition.","B":"Use Amazon SageMaker Autopilot. Create a new experiment that specifies the S3 data location. Choose ARIMA as the machine learning (ML) problem. Check the model performance."},"unix_timestamp":1710390900,"exam_id":26,"timestamp":"2024-03-14 05:35:00","isMC":true,"answer_ET":"C","answers_community":["C (57%)","A (43%)"],"discussion":[{"upvote_count":"2","timestamp":"1743168600.0","comment_id":"1411311","poster":"ef12052","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-handle-missing-time-series"},{"comments":[{"comment_id":"1304073","timestamp":"1730137560.0","poster":"MultiCloudIronMan","upvote_count":"2","content":"I think its 'C'"}],"poster":"MultiCloudIronMan","timestamp":"1729409040.0","comment_id":"1300325","content":"Selected Answer: A\nImputing Missing Data: ARIMA models require a complete time series without missing values. Imputing the missing hourly data ensures the dataset is suitable for ARIMA modeling1.\nSeasonal Trend Decomposition: This step helps in understanding the underlying patterns in the data, such as seasonality and trends, which are crucial for verifying the suitability of ARIMA2.","upvote_count":"1"},{"content":"Selected Answer: C\nResample is needed. \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-resample-time-series","comments":[{"upvote_count":"1","timestamp":"1735104360.0","poster":"LeoD","comment_id":"1331362","content":"https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-handle-missing-time-series\nOption A"}],"timestamp":"1714463880.0","poster":"Peter_Hsieh","comment_id":"1204406","upvote_count":"2"},{"poster":"vkbajoria","content":"Selected Answer: C\nidentify if data has any underlying patterns or trends should be the first step","upvote_count":"1","comment_id":"1186075","timestamp":"1711799940.0"},{"timestamp":"1710390900.0","comment_id":"1173106","content":"Selected Answer: C\n- Daily aggregation is needed to forecast daily demand and also takes care of missing hourly values. \n- Seasonal Trend decomposition on the daily aggregated data helps in understanding the underlying patterns, trends, and seasonality, which is essential for determining whether an ARIMA model would be appropriate.","poster":"AIWave","comments":[{"poster":"MultiCloudIronMan","comment_id":"1306538","upvote_count":"1","timestamp":"1730642640.0","content":"You make a valid point about the importance of daily aggregation and understanding seasonal trends. However, the key issue here is the missing hourly data. By aggregating the data daily, you might lose valuable information that could impact the accuracy of the ARIMA model."}],"upvote_count":"1"}],"question_images":[]}],"exam":{"isImplemented":true,"numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty","lastUpdated":"11 Apr 2025","id":26,"isBeta":false,"isMCOnly":false,"provider":"Amazon"},"currentPage":49},"__N_SSP":true}