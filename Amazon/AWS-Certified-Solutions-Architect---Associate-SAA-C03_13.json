{"pageProps":{"questions":[{"id":"7l8bSYjidFUjt7QcYrm2","exam_id":31,"answer_description":"","question_id":61,"timestamp":"2022-10-20 09:26:00","url":"https://www.examtopics.com/discussions/amazon/view/85994-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"discussion":[{"timestamp":"1666250760.0","content":"Selected Answer: D\n**AWS PrivateLink provides private connectivity between VPCs, AWS services, and your on-premises networks, without exposing your traffic to the public internet**. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify your network architecture.\nInterface **VPC endpoints**, powered by AWS PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace. \nhttps://aws.amazon.com/privatelink/","comment_id":"699637","poster":"123jhl0","upvote_count":"33"},{"poster":"remand","comment_id":"779285","upvote_count":"11","timestamp":"1673984760.0","content":"Selected Answer: D\nThe solution that meets these requirements best is option D.\n\nBy asking the provider to create a VPC endpoint for the target service, the company can use AWS PrivateLink to connect to the target service. This enables the company to access the service privately and securely over an Amazon VPC endpoint, without requiring a NAT gateway, VPN, or AWS Direct Connect. Additionally, this will restrict the connectivity only to the target service, as required by the company's security team.\n\nOption A VPC peering connection may not meet security requirement as it can allow communication between all resources in both VPCs.\nOption B, asking the provider to create a virtual private gateway in its VPC and use AWS PrivateLink to connect to the target service is not the optimal solution because it may require the provider to make changes and also you may face security issues.\nOption C, creating a NAT gateway in a public subnet of the company’s VPC can expose the target service to the internet, which would not meet the security requirements."},{"timestamp":"1726665540.0","comment_id":"1285748","upvote_count":"2","content":"Selected Answer: D\nAns D - create a unique, private only link: \"Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the target service\"","poster":"PaulGa"},{"content":"Selected Answer: D\nno split decisions on this answer eh? not like the last one. lol","upvote_count":"2","poster":"lofzee","comment_id":"1220054","timestamp":"1716880560.0"},{"upvote_count":"3","poster":"RNess","comment_id":"1047019","content":"Selected Answer: D\nAWS PrivateLink / VPC Endpoint Services:\n• Connect services privately from your service VPC to customers VPC\n• Doesn’t need VPC Peering, public Internet, NAT Gateway, Route Tables\n• Must be used with Network Load Balancer & ENI","timestamp":"1697641140.0"},{"comment_id":"998219","upvote_count":"1","content":"Selected Answer: D\noption D is correct","timestamp":"1693803240.0","poster":"TariqKipkemei"},{"comment_id":"982876","poster":"Guru4Cloud","timestamp":"1692210720.0","upvote_count":"3","content":"Selected Answer: D\nThe best solution to meet the requirements is option D:\n\nAsk the provider to create a VPC endpoint for the target service\nUse AWS PrivateLink to connect to the target service\nThe reasons are:\n\nPrivateLink provides private connectivity between VPCs without using public internet.\nThe provider creates a VPC endpoint in their VPC for the target service.\nThe company uses PrivateLink to securely access the endpoint from their VPC.\nConnectivity is restricted only to the target service.\nThe connection is initiated only from the company's VPC.\nOptions A, B, C would expose the connection to the public internet or require infrastructure changes in the provider's VPC.\n\nPrivateLink enables private, restricted connectivity to the target service without VPC peering or public exposure."},{"poster":"cookieMr","comment_id":"931364","upvote_count":"4","content":"Selected Answer: D\nOption C meets the requirements of establishing a private and restricted connection to the service hosted in the provider's VPC. By asking the provider to create a VPC endpoint for the target service, you can establish a direct and private connection from your company's VPC to the target service. AWS PrivateLink ensures that the connectivity remains within the AWS network and does not require internet access. This ensures both privacy and restriction to the target service, as the connection can only be initiated from your company's VPC.\n\nA. VPC peering does not restrict access only to the target service.\nB. PrivateLink is typically used for accessing AWS services, not external services in a provider's VPC.\nC. NAT gateway does not provide a private and restricted connection to the target service.\n\nOption D is the correct choice as it uses AWS PrivateLink and VPC endpoint to establish a private and restricted connection from the company's VPC to the target service in the provider's VPC.","timestamp":"1687508460.0"},{"content":"VPC Endpoint (Target Service) - for specific services (not accessing whole vpc)\nVPC Peering - (accessing whole VPC)","timestamp":"1685514660.0","poster":"Abrar2022","upvote_count":"4","comment_id":"910912"},{"upvote_count":"2","content":"VPC Peering Connection:\nAll resources in a VPC, such as ECSs and load balancers, can be accessed.\n\nVPC Endpoint:\nAllows access to a specific service or application. Only the ECSs and load balancers in the VPC for which VPC endpoint services are created can be accessed.","poster":"Abrar2022","timestamp":"1685249940.0","comment_id":"908340"},{"upvote_count":"2","poster":"eugene_stalker","comment_id":"906118","timestamp":"1684956120.0","content":"Selected Answer: D\nOption D, but seems that it is vise versa. Customer needs to create Privatelink and and you VPC endpoint to connect to Privatelink"},{"timestamp":"1683668460.0","comment_id":"893452","poster":"studynoplay","upvote_count":"3","content":"AWS PrivateLink / VPC Endpoint Services:\n• Connect services privately from your service VPC to customers VPC\n• Doesn’t need VPC Peering, public Internet, NAT Gateway, Route Tables\n• Must be used with Network Load Balancer & ENI"},{"timestamp":"1676835120.0","content":"Selected Answer: D\nD. Here you are the one initiating the connection","comment_id":"814436","poster":"Help2023","upvote_count":"2"},{"timestamp":"1675221120.0","comment_id":"794837","poster":"devonwho","content":"Selected Answer: D\nPrivateLink is a more generalized technology for linking VPCs to other services. This can include multiple potential endpoints: AWS services, such as Lambda or EC2; Services hosted in other VPCs; Application endpoints hosted on-premises.\n\nhttps://www.tinystacks.com/blog-post/aws-vpc-peering-vs-privatelink-which-to-use-and-when/","upvote_count":"2"},{"upvote_count":"3","timestamp":"1675205460.0","comment_id":"794694","content":"Selected Answer: D\nWhile VPC peering enables you to privately connect VPCs, AWS PrivateLink enables you to configure applications or services in VPCs as endpoints that your VPC peering connections can connect to.","poster":"devonwho"},{"timestamp":"1672214340.0","comments":[{"upvote_count":"2","content":"AWS PrivateLink documentation: https://docs.aws.amazon.com/privatelink/latest/userguide/what-is-privatelink.html","comment_id":"759542","poster":"Buruguduystunstugudunstuy","timestamp":"1672214400.0"}],"upvote_count":"5","content":"Selected Answer: D\nThe solution that meets these requirements is Option D:\n\n* Ask the provider to create a VPC endpoint for the target service.\n* Use AWS PrivateLink to connect to the target service.\n\nOption D involves asking the provider to create a VPC endpoint for the target service, which is a private connection to the service that is hosted in the provider's VPC. This ensures that the connection is private and restricted to the target service, as required by the company's security team. The company can then use AWS PrivateLink to connect to the target service over the VPC endpoint. AWS PrivateLink is a fully managed service that enables you to privately access services hosted on AWS, on-premises, or in other VPCs. It provides secure and private connectivity to services by using private IP addresses, which ensures that traffic stays within the Amazon network and does not traverse the public internet. \n\nTherefore, Option D is the solution that meets the requirements.","poster":"Buruguduystunstugudunstuy","comment_id":"759539"},{"timestamp":"1672084320.0","content":"D is right,if requirement was to be ok with public internet then option C was ok.","upvote_count":"1","poster":"techhb","comment_id":"757795"},{"timestamp":"1671952980.0","comment_id":"755480","poster":"k1kavi1","content":"Selected Answer: D\nD (VPC endpoint) looks correct. Below are the differences between VPC Peering & VPC endpoints.\n\nhttps://support.huaweicloud.com/intl/en-us/vpcep_faq/vpcep_04_0004.html#:~:text=You%20can%20create%20a%20VPC%20endpoint%20to%20connect%20your%20local,connection%20over%20an%20internal%20network.&text=VPC%20Peering%20supports%20only%20communications%20between%20two%20VPCs%20in%20the%20same%20region.&text=You%20can%20use%20Cloud%20Connect,between%20VPCs%20in%20different%20regions.","upvote_count":"2"},{"content":"Selected Answer: D\nD is the right answer","comment_id":"748646","upvote_count":"1","timestamp":"1671346740.0","poster":"career360guru"},{"poster":"Sahilbhai","comment_id":"741940","upvote_count":"1","timestamp":"1670780580.0","content":"answer is D"}],"answer":"D","choices":{"B":"Ask the provider to create a virtual private gateway in its VPC. Use AWS PrivateLink to connect to the target service.","C":"Create a NAT gateway in a public subnet of the company’s VPUpdate the route table to connect to the target service.","D":"Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the target service.","A":"Create a VPC peering connection between the company's VPC and the provider's VPC. Update the route table to connect to the target service."},"topic":"1","question_text":"A company runs workloads on AWS. The company needs to connect to a service from an external provider. The service is hosted in the provider's VPC. According to the company’s security team, the connectivity must be private and must be restricted to the target service. The connection must be initiated only from the company’s VPC.\nWhich solution will mast these requirements?","answers_community":["D (100%)"],"answer_ET":"D","isMC":true,"unix_timestamp":1666250760,"answer_images":[]},{"id":"SzV3SPtedDvOVe0eKdv3","exam_id":31,"unix_timestamp":1665720600,"timestamp":"2022-10-14 06:10:00","answer_description":"","question_images":[],"answer":"AC","answer_images":[],"answer_ET":"AC","url":"https://www.examtopics.com/discussions/amazon/view/85438-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The on-premises database must remain online and accessible during the migration. The Aurora database must remain synchronized with the on-premises database.\nWhich combination of actions must a solutions architect take to meet these requirements? (Choose two.)","answers_community":["AC (92%)","7%"],"discussion":[{"comments":[{"poster":"pentium75","content":"PostgreSQL -> Aurora PostgreSQL requires schema conversion per https://aws.amazon.com/dms/schema-conversion-tool/","comment_id":"1105856","timestamp":"1703582700.0","comments":[{"timestamp":"1704965160.0","upvote_count":"3","content":"SCT is compatible with PostgreSQL as source and Aurora PostgreSQL as destination, but not required.","comment_id":"1119542","poster":"LoXoL"}],"upvote_count":"3"}],"poster":"123jhl0","upvote_count":"33","comment_id":"699673","timestamp":"1666252980.0","content":"Selected Answer: AC\nAWS Database Migration Service (AWS DMS) helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.\n... With AWS Database Migration Service, you can also continuously replicate data with low latency from any supported source to any supported target.\nhttps://aws.amazon.com/dms/"},{"timestamp":"1672674180.0","poster":"gustavtd","upvote_count":"11","content":"Selected Answer: AC\nAC, here it is clearly shown https://docs.aws.amazon.com/zh_cn/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql.html","comments":[{"content":"You nailed it !","upvote_count":"1","comment_id":"777138","poster":"LuckyAro","timestamp":"1673822400.0"}],"comment_id":"763816"},{"upvote_count":"1","timestamp":"1732236840.0","poster":"0de7d1b","content":"Selected Answer: AC\nDMS helps to migrate the data from onpremise to aws and require replication task","comment_id":"1316085"},{"comments":[{"comment_id":"1287606","upvote_count":"3","content":"No, since you're using the same schema for Postgresql.","timestamp":"1726990380.0","poster":"MatAlves"}],"poster":"PaulGa","timestamp":"1726665960.0","content":"Selected Answer: CE\nAns C, E - \nC: AWS Database Migration Service to migrate databases to AWS, source database remains fully operational during the migration, avoiding application downtime. \nhttps://aws.amazon.com/dms/\nE: monitor with CloudWatch\nAs for C - not convinced: its PostgreSQL to PostgreSQL migration... no SCT is needed?","comment_id":"1285754","upvote_count":"1"},{"timestamp":"1721739300.0","comment_id":"1253668","upvote_count":"1","poster":"jaradat02","content":"Selected Answer: AC\nA and c obviously"},{"poster":"jatric","comment_id":"1243041","upvote_count":"2","timestamp":"1720209840.0","content":"Selected Answer: AC\nAC is more accurate as DMS will help to migrate the on-premises data base to cloud with ease and for ongoing replication to synchronized the datbase \"ongiong replication task\" will be helpfull.\n\nAnd yes its PostgreSQL to PostgreSQL migration so no SCT is needed ehre"},{"content":"Selected Answer: AC\nWeLL CHATGPT says SCT is not required so , AC makes sense","comment_id":"1238201","timestamp":"1719496200.0","upvote_count":"2","poster":"BombArat"},{"comment_id":"1228761","content":"Selected Answer: AC\nKeywords:\n- migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL\n- The Aurora database must remain synchronized with the on-premises database\nAnalysis:\nOption A satisfy the requirement of “synchronized with the on-premises database”\nOption C suits for the homogeneous database migration.\nOption D is not needed in this scenario, it suits for the heterogeneous database.\n\nHomogeneous database migration tools: https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-oracle-database/homogeneous-migration-tools.html\n\nHeterogeneous database migration tools: https://aws.amazon.com/dms/schema-conversion-tool/","poster":"CCCat","upvote_count":"2","timestamp":"1718154600.0"},{"content":"CD:\n Migrating a schema from PostgreSQL to Amazon Aurora (PostgreSQL) usually requires using the AWS Schema Conversion Tool (SCT) and the AWS Database Migration Service (DMS)","poster":"[Removed]","upvote_count":"1","timestamp":"1717451040.0","comment_id":"1223811"},{"poster":"Hopeyemi","comment_id":"1201627","upvote_count":"4","timestamp":"1713990960.0","content":"To meet the requirements of migrating an on-premises PostgreSQL database to Amazon Aurora PostgreSQL while keeping the on-premises database online and ensuring synchronization with the Aurora database, the following actions need to be taken:\n\nCreate an ongoing replication task (Option A): This action involves setting up continuous replication between the on-premises PostgreSQL database and the Aurora PostgreSQL database. This ensures that changes made to the on-premises database are replicated to the Aurora database in real-time, keeping them synchronized.\nCreate an AWS Database Migration Service (AWS DMS) replication server (Option C): AWS DMS provides a reliable and efficient way to migrate databases to AWS while minimizing downtime. By creating an AWS DMS replication server, you can configure and manage the replication tasks between the on-premises database and the Aurora database."},{"timestamp":"1710035760.0","content":"answer is CD : postgresql and aurora postgresql have different schemes, you need sct for conversion and dms for the migration (replication)","upvote_count":"1","poster":"Alphateccc","comment_id":"1169991"},{"comment_id":"1151868","timestamp":"1708076340.0","content":"Selected Answer: AC\nAC\nperform ongoing replication using AWS DMS to keep the source and target databases in sync","upvote_count":"2","poster":"vip2"},{"comment_id":"1107544","upvote_count":"3","poster":"farnamjam","content":"Selected Answer: AC\nDMS has Continuous Data Replication using CDC","timestamp":"1703750220.0"},{"upvote_count":"2","poster":"Michael_Li","comments":[{"upvote_count":"2","timestamp":"1703583000.0","comment_id":"1105859","content":"https://docs.aws.amazon.com/zh_cn/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-ongoing-replication.html literally says that you must \"configure the ongoing replication task\"","poster":"pentium75"}],"comment_id":"1089901","content":"CD\nA is out because it does not specify what is the service to perform the replication task, clearly what needed here is DMS\nB is out because backup is solution to keep 2 DB in sync, backup and restore takes long time\nC is correct as DMS takes care both full load and ongoing replication, see this youtube video https://www.youtube.com/watch?v=VhXDa9SPDLw \nD is right as from to PostgreSQL to Amazon Aurora PostgreSQL you need AWS Schema Conversion Tool, see https://aws.amazon.com/dms/schema-conversion-tool/\nE is out monitor itself doen't perform the replication work, if we have to choose 3 options then we can have E selected","timestamp":"1701914040.0"},{"content":"Well technically when you operate such task, you must create a database on the cloud, then operate a migration using DMS and none of the propositions give you those two tasks separately. Sometimes those questions can be really frustrating.","comment_id":"1088834","poster":"Mikado211","timestamp":"1701814020.0","upvote_count":"1"},{"content":"C. Create an AWS Database Migration Service (AWS DMS) replication server.\nE. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database synchronization.\n\nAWS DMS can replicate data from on-premises databases to Aurora PostgreSQL in real time, so the on-premises database will remain online and accessible during the migration. AWS DMS can also automatically convert the database schema, so there is no need to use AWS SCT.\n\nAn Amazon EventBridge rule can be used to monitor the database synchronization and send notifications if any errors occur. This is important because it allows the solutions architect to quickly identify and resolve any issues that may arise during the migration.\n\nA database backup of the on-premises database is not necessary because AWS DMS will replicate the data in real time. Creating an ongoing replication task is not necessary because AWS DMS will automatically create an ongoing replication task when the replication server is created.","upvote_count":"2","comment_id":"1025209","comments":[{"upvote_count":"4","poster":"David_Ang","timestamp":"1697561580.0","comment_id":"1046216","content":"Mate you can monitor everything you want but it is not going to make sure the synchronization is working, an alert is not going to help."}],"poster":"Amitabha09","timestamp":"1696462500.0"},{"upvote_count":"4","timestamp":"1693974840.0","comment_id":"1000169","poster":"TariqKipkemei","content":"Selected Answer: AC\nCreate an AWS Database Migration Service (AWS DMS) replication server then create an ongoing replication task"},{"content":"Selected Answer: AC\nA) Create an ongoing replication task\n\nC) Create an AWS Database Migration Service (AWS DMS) replication server\n\nThe key reasons are:\n\nAn ongoing DMS replication task keeps the source and target databases synchronized during the migration.\nThe DMS replication server manages and executes the replication tasks.\nTogether, these will continuously replicate changes from on-prem to Aurora to keep them in sync.\nA database backup alone wouldn't maintain synchronization.","timestamp":"1692211140.0","poster":"Guru4Cloud","upvote_count":"2","comment_id":"982883"},{"timestamp":"1689602160.0","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql.html\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_GettingStarted.Replication.html","poster":"MutiverseAgent","upvote_count":"2","comment_id":"954266"},{"upvote_count":"4","comment_id":"931370","content":"Selected Answer: AC\nThese two actions (AC) will help meet the requirements of migrating the on-premises PostgreSQL database to Amazon Aurora PostgreSQL while keeping the on-premises database accessible and synchronized with the Aurora database. The ongoing replication task will ensure continuous data replication between the on-premises database and Aurora. The AWS DMS replication server will facilitate the migration process and handle the data replication.\n\nB. Creating a database backup does not ensure ongoing synchronization.\nD. Converting the database schema does not address the requirement of synchronization.\nE. Creating an EventBridge rule only monitors synchronization, but doesn't handle migration.\nThe correct combination is A and C.","poster":"cookieMr","timestamp":"1687508880.0"},{"content":"Answer is CD. Postgresql to Aurora Postgresql needed SCT.\nhttps://aws.amazon.com/ko/dms/schema-conversion-tool/","comment_id":"918852","timestamp":"1686280260.0","poster":"Nandha707","upvote_count":"1"},{"poster":"Bmarodi","upvote_count":"1","content":"Selected Answer: AC\nOption A & C are the right answer.","timestamp":"1686240960.0","comment_id":"918467"},{"comment_id":"880866","upvote_count":"2","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-postgresql-database-to-aurora-postgresql.html","poster":"kruasan","timestamp":"1682457300.0"},{"comment_id":"850573","content":"A->https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.oracle2rds.replication.html\nC->https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html","poster":"osmk","upvote_count":"2","timestamp":"1679789820.0"},{"content":"Selected Answer: AC\nThis question is giving us two conditions to solve it. One of them is on-premise database must remain online and accessible during the migration and the second one is Aurora database must remain synchronized with the on-premises database. So to meet them all A and C will be the correct options for us.\n\nPS: if the question was just asking us something related to the DB migration process alone, all options would be correct.","poster":"Erbug","timestamp":"1679564280.0","comment_id":"848021","upvote_count":"3"},{"content":"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-postgresql-database-to-aurora-postgresql.html \n\nThis link talks about using DMS . I saw the other link pointing to SCT - not sure which one is correct","upvote_count":"1","poster":"G3","comment_id":"792050","timestamp":"1675025100.0"},{"upvote_count":"3","poster":"aba2s","content":"Selected Answer: CD\nDMS for database migration\nSCT for having the same scheme","timestamp":"1672759740.0","comments":[{"comment_id":"810479","content":"The source and destination are both MySQL so schema is not needed.","upvote_count":"4","poster":"Help2023","timestamp":"1676537880.0"}],"comment_id":"764812"},{"poster":"SilentMilli","content":"Selected Answer: AC\nAWS Database Migration Service (AWS DMS)","comment_id":"764428","upvote_count":"2","timestamp":"1672741800.0"},{"poster":"bamishr","content":"A. Create an ongoing replication task: An ongoing replication task can be used to continuously replicate data from the on-premises database to the Aurora database. This will ensure that the Aurora database remains in sync with the on-premises database.\n\nD. Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT): The AWS SCT can be used to convert the schema of the on-premises database to a format that is compatible with Aurora. This will ensure that the data can be properly migrated and that the Aurora database can be used with the same applications and queries as the on-premises database.","timestamp":"1672263960.0","comments":[],"upvote_count":"2","comment_id":"760363"},{"timestamp":"1672254480.0","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: AC\nTo meet the requirements of maintaining an online and accessible on-premises database while migrating to Amazon Aurora PostgreSQL and keeping the databases synchronized, a solutions architect should take the following actions:\n\nOption A. Create an ongoing replication task. This will allow the architect to continuously replicate data from the on-premises database to the Aurora database.\n\nOption C. Create an AWS Database Migration Service (AWS DMS) replication server. This will allow the architect to use AWS DMS to migrate data from the on-premises database to the Aurora database. AWS DMS can also be used to continuously replicate data between the two databases to keep them synchronized.","comment_id":"760234","upvote_count":"5"},{"comment_id":"757800","timestamp":"1672084800.0","poster":"techhb","content":"Selected Answer: CD\nC&D ,SCT is required,its a mandate not an option.","upvote_count":"2"},{"comment_id":"755353","timestamp":"1671929580.0","content":"Selected Answer: CD\nAnswer is CD. Postgresql to Aurora Postgresql needed SCT.\nhttps://aws.amazon.com/ko/dms/schema-conversion-tool/","poster":"berks","upvote_count":"1"},{"content":"Answer is CD. Postgresql to Aurora Postgresql needed SCT.\nhttps://aws.amazon.com/ko/dms/schema-conversion-tool/","timestamp":"1671929520.0","poster":"berks","comments":[{"comment_id":"978810","content":"not needed if you are migrating same engine like PostgreSQL to Postgre SQL.","poster":"flywithmustafa","upvote_count":"2","timestamp":"1691771520.0"}],"upvote_count":"2","comment_id":"755351"},{"comment_id":"752528","content":"Selected Answer: AC\nYou do not need to use SCT if you are migrating the same DB engine\n• Ex: On-Premise PostgreSQL => RDS PostgreSQL\n• The DB engine is still PostgreSQL (RDS is the platform)","timestamp":"1671637800.0","poster":"Silvestr","upvote_count":"5"},{"comment_id":"748649","upvote_count":"1","content":"Selected Answer: AC\nA and C","poster":"career360guru","timestamp":"1671347040.0"},{"upvote_count":"3","comments":[{"timestamp":"1674611640.0","poster":"jwu413","comment_id":"787177","content":"You're going from Postgres to Postgres. What schema are you converting??","upvote_count":"2"}],"timestamp":"1671090120.0","content":"A & C\nSCT is not needed here.","comment_id":"745824","poster":"andreiushu"},{"upvote_count":"3","timestamp":"1670995140.0","content":"Selected Answer: AC\nboth source and target is PostgreSQL so SCT is not needed.","comment_id":"744740","poster":"wly_al"},{"comment_id":"744588","timestamp":"1670977680.0","content":"Selected Answer: CD\ni voted CD","upvote_count":"2","poster":"333666999"},{"poster":"Bazooka123","content":"All, I researched other websites also and most of places answer is given as CD similar to the answer provided here . But here voting is for AC. I am just confused. So what usually is correct answer and how it works? Should I go with Actual answer provided here or with voting? I reviewed options and for me AC seems right but still not sure if we need OptionA ? And do we need schema conversion when you migrate postgress SQL to Aurora?","comments":[{"comment_id":"753261","upvote_count":"2","poster":"JayBee65","timestamp":"1671711300.0","content":"You should read the explanations and do the research if you're not clear"}],"timestamp":"1670974560.0","upvote_count":"2","comment_id":"744547"},{"timestamp":"1670709420.0","poster":"drake2020","comment_id":"741261","content":"Ans is CD : reason is detailed in this link : https://aws.amazon.com/dms/schema-conversion-tool/ On-prem PostgreSQL to AWS Aurora PostgreSQL needs SCT","comments":[{"content":"Yes, this is a nice link.","upvote_count":"1","poster":"berks","comment_id":"755350","timestamp":"1671929460.0"}],"upvote_count":"4"},{"comment_id":"733372","content":"Anyone can explain why A ? Because DMS take care the replication","upvote_count":"2","poster":"sunny1984","timestamp":"1669949820.0"},{"poster":"JayanKuruwita","timestamp":"1669540320.0","upvote_count":"2","content":"Highly cofused why we ned SCT in here, because we don't have to use SCT for homogeneous migration. If someone knows please explain.","comment_id":"728079"},{"poster":"Wpcorgan","timestamp":"1669119840.0","upvote_count":"1","content":"Selected Answer: AC\nA and C","comment_id":"724313"},{"upvote_count":"4","poster":"mabotega","comment_id":"715401","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html","timestamp":"1668099240.0"},{"upvote_count":"1","poster":"SimonPark","timestamp":"1667109780.0","content":"Selected Answer: AC\nA and C","comment_id":"707635"},{"content":"Selected Answer: AC\nThey are migrating Postgres to Postgres,\nthus D does not make sense and A and C seems to be the right answer to me","comment_id":"704866","timestamp":"1666802460.0","upvote_count":"3","poster":"Six_Fingered_Jose"},{"upvote_count":"2","content":"Selected Answer: AC\nA & C are the correct combinations.","poster":"dave9994","comment_id":"703423","timestamp":"1666655040.0"},{"upvote_count":"1","comment_id":"694631","timestamp":"1665739680.0","poster":"trancex","content":"I think A and C"},{"content":"Selected Answer: AC\nSame as this https://www.examtopics.com/discussions/amazon/view/81317-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"huiy","upvote_count":"4","comment_id":"694479","timestamp":"1665720600.0"}],"topic":"1","question_id":62,"choices":{"B":"Create a database backup of the on-premises database.","E":"Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database synchronization.","A":"Create an ongoing replication task.","C":"Create an AWS Database Migration Service (AWS DMS) replication server.","D":"Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT)."},"isMC":true},{"id":"Nk3cInL3zwMSMdVZvbwV","exam_id":31,"answer_description":"","question_id":63,"timestamp":"2022-10-20 10:12:00","url":"https://www.examtopics.com/discussions/amazon/view/85997-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"discussion":[{"timestamp":"1666253520.0","comment_id":"699683","content":"Selected Answer: B\nUse a group email address for the management account's root user\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html#best-practices_mgmt-acct_email-address","poster":"123jhl0","upvote_count":"28"},{"upvote_count":"14","comment_id":"931377","timestamp":"1687509060.0","content":"Selected Answer: B\nOption B ensures that all future notifications are not missed by configuring the AWS account root user email addresses as distribution lists that are monitored by a few administrators. By setting up alternate contacts in the AWS Organizations console or programmatically, the notifications can be sent to the appropriate administrators responsible for monitoring and responding to alerts. This solution allows for centralized management of notifications and ensures they are limited to account administrators.\n\nA. Floods all users with notifications, lacks granularity.\nC. Manual forwarding introduces delays, centralizes responsibility.\nD. No flexibility for specific account administrators, limits customization.","poster":"cookieMr"},{"timestamp":"1741049640.0","upvote_count":"1","comment_id":"1364650","poster":"15df3d0","content":"Selected Answer: D\n//docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html#best-practices_mgmt-acct_email-address"},{"comment_id":"1285757","timestamp":"1726666200.0","upvote_count":"2","poster":"PaulGa","content":"Selected Answer: B\nAns B - as opposed to option D, because the organisation account structure implies there is more than one root account: \"The root email recipient missed a notification that was sent to the root user email address of one account.\""},{"content":"Selected Answer: B\n\"Use a group email address for root user credentials:\n\nUse an email address that is managed by your business and forwards received messages directly to a group of users. If AWS must contact the owner of the account, this approach reduces the risk of delays in responding, even if individuals are on vacation, out sick, or have left the business. The email address used for the root user should not be used for other purposes.\"\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/root-user-best-practices.html#ru-bp-group","timestamp":"1724741880.0","upvote_count":"2","comment_id":"1273234","poster":"MatAlves"},{"poster":"MandAsh","timestamp":"1716946020.0","comment_id":"1220643","content":"Lol.. How is this AWS related question. Isnt it general knowledge.","upvote_count":"3"},{"poster":"awsgeek75","comment_id":"1110786","content":"Selected Answer: B\nNo idea why \"D\" would be correct answer unless there is some missing context in the question or the answer. \"B\" is best practice as pointed out in other links.","upvote_count":"1","timestamp":"1704051540.0"},{"content":"Selected Answer: B\nthe only answer with sense is \"B\", because \"A\" is not exclusive, \"C\" is exactly the case the want to avoid, and \"D\" just don't make sense","timestamp":"1697562000.0","upvote_count":"2","poster":"David_Ang","comment_id":"1046220"},{"upvote_count":"2","poster":"tom_cruise","content":"Selected Answer: B\ndistribution list is the way to go","timestamp":"1697059440.0","comment_id":"1041119"},{"timestamp":"1692211320.0","content":"Selected Answer: B\nThe reasons are:\n\nAlternate contacts allow defining other users to receive root emails.\nDistribution lists ensure multiple admins get notified.\nLimits notifications to account admins rather than all users.\nUsing the same root email address for all accounts (Option D) is not recommended.\nRelying on one admin or external forwarding (Options A, C) introduces delays or single points of failure.","comment_id":"982888","upvote_count":"2","poster":"Guru4Cloud"},{"upvote_count":"1","poster":"Itsume","comment_id":"926570","timestamp":"1687078860.0","content":"all admins need access or else some wont get the right mails and cant do their job,\nsending it only to a few would disrupt the workflowso it is D"},{"timestamp":"1686200640.0","content":"Selected Answer: D\nFrom the links provided below there are no mention of having a distribution list capability within AWS:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html#best-practices_mgmt-acct_email-address\n\nAs per link for best practices:\nUse a group email address for the management account's root user!","upvote_count":"2","poster":"fishy_resolver","comment_id":"917806"},{"content":"The clue is in the pudding!!\n\nQuestion: account \"administrators\"\nAnswer: Configure all AWS account root user email addresses as distribution lists that go to a few \"administrators\"","timestamp":"1685514900.0","comment_id":"910914","poster":"Abrar2022","upvote_count":"3"},{"poster":"Rainchild","timestamp":"1682563500.0","comment_id":"882176","content":"Selected Answer: B\nOption A: wrong - sends email to everybody\nOption B: correct (but sub-optimal because distribution lists aren't all that secure)\nOption C: wrong - single point of failure on the new administrator\nOption D: wrong - each root email address must be unique, you can't change them all to the same one","upvote_count":"3"},{"poster":"jdr75","timestamp":"1680853440.0","comment_id":"863620","upvote_count":"3","content":"Selected Answer: B\nThe more aligned answer to this article:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html#best-practices_mgmt-acct_email-address\n\nis B.\n\nD would be best if it'd said that the email you configure as \"root user email address\" will be a distribution list.\nThe phrase \"all future notifications are not missed\" points to D, cos' it said:\n\".. and all newly created accounts to use the same root user email address\"\nso the future account that will be created will be covered with the business policy.\n\nIt's not 100% clear, but I'll choose B."},{"comments":[{"timestamp":"1680851640.0","content":"El administrador de \"examtopics\" pasa olímpicamente de marcar la respuesta correcta y es evidente que muchas respuestas que indica como \"correctas\" no lo son. Dice muy poco del servicio que dan.","comment_id":"863588","upvote_count":"1","poster":"jdr75"}],"upvote_count":"1","content":"Una pregunta si la gente va votando las preguntas por que los administradores no cambian la respuesta correcta. Es a interpretación y ya?","comment_id":"859066","timestamp":"1680453120.0","poster":"TheAbsoluteTruth"},{"content":"Using the method of crossing out the option that does not fit....\nOption A: address to all users of organization (wrong)\nOption B: go to a few administration who can respond to alerts (question says to send notification to administrators not a selected few )\nOption C: send to one administrator and giving him responsibility (wrong)\nOption D: correct (as this is the one option left after checking all others).","timestamp":"1679578140.0","comment_id":"848254","poster":"jaswantn","upvote_count":"1"},{"poster":"Zerotn3","content":"Selected Answer: D\nOption B does not meet the requirements because it would require configuring all AWS account root user email addresses as distribution lists, which is not necessary to meet the requirements.","upvote_count":"3","comment_id":"763118","timestamp":"1672563840.0"},{"upvote_count":"2","comment_id":"761179","content":"Unless I am reading this wrong from AWS, it seems D is proper as it says to use a single account and then set to forward to other emails.\n \n\nUse an email address that forwards received messages directly to a list of senior business managers. In the event that AWS needs to contact the owner of the account, for example, to confirm access, the email is distributed to multiple parties. This approach helps to reduce the risk of delays in responding, even if individuals are on vacation, out sick, or leave the business.","timestamp":"1672326840.0","poster":"mp165"},{"upvote_count":"4","comment_id":"760285","poster":"Buruguduystunstugudunstuy","timestamp":"1672257720.0","comments":[{"poster":"bullrem","upvote_count":"6","comment_id":"784473","timestamp":"1674404280.0","content":"Option D would not meet the requirement of limiting the notifications to account administrators. Instead, it is better to use option B, which is to configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. This way, the company can ensure that the notifications are received by the appropriate people and that they are not missed. Additionally, AWS account alternate contacts can be configured in the AWS Organizations console or programmatically, which allows the company to have more granular control over who receives the notifications."}],"content":"Selected Answer: D\nTo meet the requirements of ensuring that all future notifications are not missed and are limited to account administrators, the company should take the following action:\n\nOption D. Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.\n\nBy configuring all AWS accounts to use the same root user email address and setting up AWS account alternate contacts, the company can ensure that all notifications are sent to a single email address that is monitored by one or more administrators. This will allow the company to ensure that all notifications are received and responded to promptly, without the risk of notifications being missed."},{"content":"B makes more sense","timestamp":"1672085040.0","poster":"techhb","comment_id":"757803","upvote_count":"1"},{"content":"answer b is makes more sense","poster":"Sahilbhai","timestamp":"1671336780.0","upvote_count":"1","comment_id":"748598"},{"upvote_count":"1","timestamp":"1667809380.0","comment_id":"712922","poster":"PS_R","content":"Selected Answer: B\nB makes more sense and is a best practise"},{"upvote_count":"3","timestamp":"1666293180.0","comment_id":"700227","poster":"Chunsli","content":"Selected Answer: B\nB makes better sense in the context"}],"answer":"B","choices":{"D":"Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.","B":"Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.","A":"Configure the company’s email server to forward notification email messages that are sent to the AWS account root user email address to all users in the organization.","C":"Configure all AWS account root user email messages to be sent to one administrator who is responsible for monitoring alerts and forwarding those alerts to the appropriate groups."},"topic":"1","question_text":"A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each business unit's account independently upon request. The root email recipient missed a notification that was sent to the root user email address of one account. The company wants to ensure that all future notifications are not missed. Future notifications must be limited to account administrators.\nWhich solution will meet these requirements?","answers_community":["B (86%)","14%"],"answer_ET":"B","isMC":true,"unix_timestamp":1666253520,"answer_images":[]},{"id":"Y09Ef9jtZ70cLnq3Pfi7","unix_timestamp":1666254420,"discussion":[{"timestamp":"1666254420.0","comments":[{"comment_id":"732395","content":"This also helps anyone in doubt; https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html","poster":"EKA_CloudGod","upvote_count":"2","timestamp":"1669882920.0"},{"timestamp":"1666886280.0","upvote_count":"4","poster":"UWSFish","comment_id":"705714","content":"Yes but active/standby is fault tolerance, not HA. I would concede after thinking about it that B is probably the answer that will be marked correct but its not a great question."}],"upvote_count":"29","comment_id":"699693","content":"Selected Answer: B\nMigrating to Amazon MQ reduces the overhead on the queue management. C and D are dismissed.\nDeciding between A and B means deciding to go for an AutoScaling group for EC2 or an RDS for Postgress (both multi- AZ). The RDS option has less operational impact, as provide as a service the tools and software required. Consider for instance, the effort to add an additional node like a read replica, to the DB.\nhttps://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html\nhttps://aws.amazon.com/rds/postgresql/","poster":"123jhl0"},{"content":"Selected Answer: B\nTo meet the requirements of providing the highest availability with the least operational overhead, the solutions architect should take the following actions:\n\n* By migrating the queue to Amazon MQ, the architect can take advantage of the built-in high availability and failover capabilities of the service, which will help ensure that messages are delivered reliably and without interruption.\n\n* By creating a Multi-AZ Auto Scaling group for the EC2 instances that host the application, the architect can ensure that the application is highly available and able to handle increased traffic without the need for manual intervention.\n\n* By migrating the database to a Multi-AZ deployment of Amazon RDS for PostgreSQL, the architect can take advantage of the built-in high availability and failover capabilities of the service, which will help ensure that the database is always available and able to handle increased traffic.\n\nTherefore, the correct answer is Option B.","poster":"Buruguduystunstugudunstuy","comment_id":"760497","timestamp":"1672278900.0","upvote_count":"8"},{"poster":"zx9r","timestamp":"1732795920.0","comment_id":"1319212","content":"Selected Answer: B\nB makes the most sense","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nB is the correct answer","timestamp":"1721753160.0","poster":"jaradat02","comment_id":"1253784"},{"upvote_count":"3","comments":[{"upvote_count":"1","comment_id":"1316471","timestamp":"1732304460.0","poster":"Gizmo2022","content":"Thank you for explaining the answer"}],"comment_id":"1124403","content":"Selected Answer: B\nCD, you cannot have EC2 scaling work with RabbitMQ as only once instance can be active\nA: Is good but B is better\nB: Correct due to usage of RDS for PG so less overhead","poster":"awsgeek75","timestamp":"1705428180.0"},{"upvote_count":"1","content":"Agree with B","comment_id":"1012995","timestamp":"1695292980.0","poster":"chandu7024"},{"poster":"TariqKipkemei","content":"Selected Answer: B\nB offers high availability and low operational overheads.","timestamp":"1693975620.0","upvote_count":"2","comment_id":"1000175"},{"content":"Selected Answer: B\nOption B is the best solution to meet the high availability and low overhead requirements:\n\nMigrate the queue to redundant Amazon MQ\nUse Auto Scaling groups across AZs for the application\nMigrate the database to Multi-AZ RDS PostgreSQL\nThe reasons are:\n\nAmazon MQ provides a managed, highly available RabbitMQ cluster\nMulti-AZ Auto Scaling distributes the application across AZs\nRDS PostgreSQL is managed, multi-AZ capable database\nTogether this architecture removes single points of failure\nRDS and MQ reduce operational overhead over self-managed","comment_id":"982898","upvote_count":"6","poster":"Guru4Cloud","timestamp":"1692211800.0"},{"poster":"MNotABot","upvote_count":"2","timestamp":"1689260520.0","content":"B\nleast operational overhead (Amazon RDS for PostgreSQL --> hence AD out / C says EC2 so out --> Hence B)","comment_id":"950777"},{"poster":"cookieMr","content":"Selected Answer: B\nOption B provides the highest availability with the least operational overhead. By migrating the queue to a redundant pair of RabbitMQ instances on Amazon MQ, the messaging system becomes highly available. Creating a Multi-AZ Auto Scaling group for EC2 instances hosting the application ensures that it can automatically scale and maintain availability across multiple Availability Zones. Migrating the database to a Multi-AZ deployment of Amazon RDS for PostgreSQL provides automatic failover and data replication across multiple Availability Zones, enhancing availability and reducing operational overhead.\n\nA. Incorrect because it does not address the high availability requirement for the RabbitMQ queue and the PostgreSQL database.\n\nC. Incorrect because it does not provide redundancy for the RabbitMQ queue and does not address the high availability requirement for the PostgreSQL database.\n\nD. Incorrect because it does not address the high availability requirement for the RabbitMQ queue and does not provide redundancy for the application instances.","upvote_count":"4","timestamp":"1687509180.0","comment_id":"931379"},{"timestamp":"1677688380.0","poster":"Gary_Phillips_2007","content":"Selected Answer: B\nB for me.","upvote_count":"1","comment_id":"826070"},{"comment_id":"757806","upvote_count":"1","content":"Selected Answer: B\nB is right all explanations below are correct","timestamp":"1672085340.0","poster":"techhb"},{"upvote_count":"1","timestamp":"1671347640.0","content":"Selected Answer: B\nOption B is right answer","poster":"career360guru","comment_id":"748652"},{"timestamp":"1669120020.0","content":"B for me","upvote_count":"1","poster":"Wpcorgan","comment_id":"724316"}],"url":"https://www.examtopics.com/discussions/amazon/view/85999-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"timestamp":"2022-10-20 10:27:00","answers_community":["B (100%)"],"exam_id":31,"answer":"B","answer_images":[],"isMC":true,"answer_description":"","topic":"1","question_text":"A company runs its ecommerce application on AWS. Every new order is published as a massage in a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are processed by a different application that runs on a separate EC2 instance. This application stores the details in a PostgreSQL database on another EC2 instance. All the EC2 instances are in the same Availability Zone.\nThe company needs to redesign its architecture to provide the highest availability with the least operational overhead.\nWhat should a solutions architect do to meet these requirements?","choices":{"A":"Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Create another Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database.","C":"Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.","D":"Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Create a third Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database","B":"Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL."},"answer_ET":"B","question_id":64},{"id":"kdCuFV4SgtiyDzLYmcLJ","answers_community":["D (77%)","14%","8%"],"answer_ET":"D","choices":{"A":"Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.","D":"Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.","B":"Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.","C":"Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type."},"question_text":"A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.\nThe reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.\nWhat should a solutions architect do to meet these requirements with the LEAST operational overhead?","question_id":65,"topic":"1","isMC":true,"timestamp":"2022-10-19 09:02:00","exam_id":31,"answer_images":[],"discussion":[{"content":"Selected Answer: D\ni go for D here\nA and B says you are copying the file to another bucket using lambda,\nC an D just uses S3 replication to copy the files,\n\nThey are doing exactly the same thing while C and D do not require setting up of lambda, which should be more efficient\n\nThe question says the team is manually copying the files, automatically replicating the files should be the most efficient method vs manually copying or copying with lambda.","timestamp":"1666802940.0","poster":"Six_Fingered_Jose","comment_id":"704868","upvote_count":"31","comments":[{"timestamp":"1687148820.0","content":"yes d because of least operational overhead and also s3 event notification can only send to sns.sqs.and lambda , not to sagemaker.eventbridge can send to sagemaker","upvote_count":"22","poster":"vipyodha","comment_id":"927110"},{"timestamp":"1729299300.0","poster":"Tsige","comment_id":"1299842","content":"S3 Replication: Configuring S3 replication between the initial and analysis S3 buckets automates the process of moving files between the buckets without the need to manually copy files or run a Lambda function for this purpose. This reduces operational overhead.\nS3 Event Notifications: Once files are replicated to the analysis bucket, you can configure S3 event notifications for the s3:ObjectCreated\nevent. This event triggers actions (such as invoking Lambda functions and sending data to SageMaker Pipelines) when new files are placed in the analysis bucket.\nThe answer is C","upvote_count":"3"},{"comment_id":"1025364","upvote_count":"2","poster":"Abdou1604","comments":[{"poster":"pentium75","upvote_count":"5","timestamp":"1703583960.0","content":"The Lambda functions should run \"on the copied data\", so first copy, THEN run Lambda function, which is achieved by D.","comment_id":"1105875"}],"timestamp":"1696491420.0","content":"but the reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied , S3 replication cons is copying everything"}]},{"poster":"123jhl0","timestamp":"1666256100.0","comment_id":"699712","comments":[{"poster":"JayBee65","upvote_count":"9","content":"I think you are mis-interpreting the question. I think you need to use all files, including the ones provided by other teams, otherwise how can you tell what files to copy? I think the point of this statement is to show that more files are in use, and being copied at different times, rather than suggesting you need to differentiate between the two sources of files.","comment_id":"753278","timestamp":"1671712740.0"},{"comment_id":"715773","comments":[{"comment_id":"1098019","timestamp":"1702717440.0","poster":"byteb","content":"\"The reporting team wants to move the files automatically to analysis S3 bucket as the files enter the initial S3 bucket.\" Replication is asynchronous, with lambda the data will be available faster. So I think A is the answer.","upvote_count":"1"}],"content":"Not sure how far lambda will cope up with larger files with the timelimit in place.","timestamp":"1668143040.0","poster":"KADSM","upvote_count":"4"},{"comment_id":"927111","content":"but B is not least operational overhead , D is least operational overhead","upvote_count":"2","poster":"vipyodha","timestamp":"1687148880.0"},{"poster":"jdr75","content":"You misinterpret it ... the reporting team is overload, cos' more teams request their services uploading more data to the bucket. That's the reason reporting team need to automate the process. So ALL the bucket objects need to be copied to other bucket, and the replication is better an cheaper than use Lambda. So the answer is D.","comment_id":"863646","timestamp":"1680854820.0","upvote_count":"3"},{"upvote_count":"7","content":"Nowhere in the question did they mention that other files were unrelated to reporting ....\n\"The reporting team wants to move the files automatically to analysis S3 bucket as the files enter the initial S3 bucket\" where did it say they were unrelated files ? except for conjecture.","poster":"LuckyAro","comment_id":"777191","timestamp":"1673827260.0"}],"content":"Selected Answer: B\nC and D aren't answers as replicating the S3 bucket isn't efficient, as other teams are starting to use it to store larger docs not related to the reporting, making replication not useful.\nAs Amazon SageMaker Pipelines, ..., is now supported as a target for routing events in Amazon EventBridge, means the answer is B\nhttps://aws.amazon.com/about-aws/whats-new/2021/04/new-options-trigger-amazon-sagemaker-pipeline-executions/","upvote_count":"18"},{"comment_id":"1401497","upvote_count":"1","timestamp":"1742550780.0","content":"Selected Answer: C\nWhy do we need to add EventBridge (additional layer/operational overhead) when S3 Event Notification does the job?","poster":"SirDNS"},{"timestamp":"1741117380.0","upvote_count":"2","poster":"AwsAbhiKumar","content":"Selected Answer: C\nThe question is little confusing between C and D. \nOption C suggest to use S3 events notification in combination with AWS lambda and SageMaker. But S3 events notification dont't have native integration to directly trigger Amazon SageMaker Pipelines. But it can work around this limitation by having the S3 event trigger a Lambda function, and then that Lambda function can call the SageMaker Pipelines API to start a pipeline execution.(Option doesn't directly suggests this combo)\n\nOption D is fine but it uses S3 replication combined with EventBridge for notifications, which introduces an extra layer (EventBridge rules) that is unnecessary since S3 event notifications can directly trigger Lambda and SageMaker Pipelines. as suggested above.","comment_id":"1365088"},{"content":"Selected Answer: C\nc is the answer , S3 event notification can directly invoke a Lambda function, which can then in turn trigger a SageMaker Pipeline execution, effectively allowing an S3 event to initiate a SageMaker pipeline through a Lambda intermediary. we dont need to invoke EventBridge","poster":"hpirnaj","upvote_count":"3","comment_id":"1336860","timestamp":"1736106720.0"},{"upvote_count":"5","poster":"salman7540","comment_id":"1329441","timestamp":"1734699120.0","content":"Selected Answer: D\nS3 replication can use to copy files in different buckets so we don't need lambda.\nS3 events can't be sent directly to sagemaker so we have to utilise eventbridge who supports many targets including sagemaker."},{"upvote_count":"3","comment_id":"1328001","content":"Selected Answer: D\nIn the case of S3 event notification only one destination type can be specified for each event notification.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-destinations","poster":"rmanuraj","timestamp":"1734449940.0"},{"upvote_count":"1","poster":"PSH123","comment_id":"1319527","content":"Selected Answer: C\ngpt said 'C' is solution","timestamp":"1732851780.0"},{"comment_id":"1287258","upvote_count":"4","content":"Selected Answer: D\nAns D - least operational overhead using replication; I was initially going for Ans C until I spotted S3 event notification can only send to SQS, SNS, Lambda - not directly to Sagemaker; but Eventbridge can send to Sagemaker. Not sure why author prefers A...?","timestamp":"1726910280.0","poster":"PaulGa"},{"content":"Selected Answer: D\nS3 event can't be use to notify sagemaker, So C can't be right option. AB required lambda which is not unnecessary","upvote_count":"4","poster":"jatric","comment_id":"1243050","timestamp":"1720212120.0"},{"comment_id":"1220060","poster":"lofzee","upvote_count":"4","comments":[{"comment_id":"1220061","content":"Sorry i meant Sagemaker is not supported as an S3 Event Notification. Lambda is though. Still doesn't change what the answer is.... D","upvote_count":"4","timestamp":"1716881760.0","poster":"lofzee"}],"content":"Answer is D because it requires least operational overhead and S3 replication does the copying for you.\nAlso read this https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\nLambda and Sagemaker are not supported destinations for S3 Event Notifications","timestamp":"1716881700.0"},{"upvote_count":"1","content":"I go for C, because option C no need to configure event notifications, but D need to extra work to configure the event notification, for the least operation, option C is best choice","poster":"andyngkh86","timestamp":"1705719360.0","comment_id":"1127034"},{"timestamp":"1700772420.0","comment_id":"1078787","poster":"Marco_St","upvote_count":"2","content":"Selected Answer: D\nB is the first option I denied. Since it makes the event happens inside the analysis bucket to trigger the lambda function. so if the lambda function is running code to copy files from initial bucket to analysis bucket. Then this lambda function should be triggered by the event in initial bucket like once the data reaches in the initial bucket then lambda is triggered. D is the answer."},{"comment_id":"1028752","poster":"AntonioMinolfi","timestamp":"1696847760.0","content":"Selected Answer: D\nUtilizing a lambda function would introduce additional operational overhead, eliminating options A and B. S3 replication offers a simpler setup and efficiently accomplishes the task. S3 notifications cannot use SageMaker as a destination; the permissible destinations include SQS, SNS, Lambda, and Eventbridge, so C is out.","upvote_count":"11"},{"timestamp":"1695814140.0","comments":[{"content":"You are right. This is the key point - Sagemaker cannot be the destination of S3 event notification.","poster":"fantastique007","timestamp":"1713381120.0","comment_id":"1197407","upvote_count":"1"}],"poster":"vijaykamal","comment_id":"1018748","upvote_count":"3","content":"Selected Answer: D\nCreate lambda for replication is overhead. This dismisses A and B\nS3 event notification cannot be directed to Sagemaker directly. This dismisses C\nCorrect Answer: D"},{"upvote_count":"2","poster":"TariqKipkemei","comment_id":"1000178","timestamp":"1693976040.0","content":"Selected Answer: D\nD provide the least operational overhead"},{"timestamp":"1692212160.0","comment_id":"982906","upvote_count":"4","content":"Selected Answer: D\nOption D is the solution with the least operational overhead:\n\nUse S3 replication between buckets\nSend S3 events to EventBridge\nAdd Lambda and SageMaker as EventBridge rule targets\nThe reasons this has the least overhead:\n\nS3 replication automatically copies new objects to analysis bucket\nEventBridge allows easily adding multiple targets for events\nNo custom Lambda function needed for copying objects\nLeverages managed services for event processing","poster":"Guru4Cloud"},{"timestamp":"1689611460.0","upvote_count":"4","comment_id":"954375","poster":"MutiverseAgent","content":"Selected Answer: D\nCorrect: D\nB & D the only possible as Sagemaker is not supported as target for S3 events. Using bucket replication as D mention is more efficient than using a lambda as B mention."},{"timestamp":"1687509480.0","content":"Selected Answer: D\nOption D is correct because it combines S3 replication, event notifications, and Amazon EventBridge to automate the copying of files from the initial S3 bucket to the analysis S3 bucket. It also allows for the execution of Lambda functions and integration with SageMaker Pipelines.\nOption A is incorrect because it suggests manually copying the files using a Lambda function and event notifications, but it does not utilize S3 replication or EventBridge for automation.\nOption B is incorrect because it suggests using S3 event notifications directly with EventBridge, but it does not involve S3 replication or utilize Lambda for copying the files.\nOption C is incorrect because it only involves S3 replication and event notifications without utilizing EventBridge or Lambda functions for further processing.","comment_id":"931381","poster":"cookieMr","upvote_count":"3"},{"poster":"studynoplay","timestamp":"1683671220.0","comment_id":"893465","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-destinations\nS3 can NOT send event notification to SageMaker. This rules out C. you have to send to • Amazon EventBridge 1st then to SageMaker","upvote_count":"7"},{"timestamp":"1681107360.0","upvote_count":"6","comment_id":"866037","content":"Selected Answer: D\nWhy I believe it is not C? The key here is in the s3:ObjectCreated:\"Put\". The replication will not fire the s3:ObjectCreated:Put. event. See link here: https://aws.amazon.com/blogs/aws/s3-event-notification/","poster":"eendee"},{"content":"Selected Answer: D\nD takes care of automated moving and lambda for pattern matching are covered efficiently in D.","poster":"kraken21","upvote_count":"2","timestamp":"1680118740.0","comment_id":"854879"},{"content":"only one destination type can be specified for each event notification in S3 event notifications","timestamp":"1679297220.0","comment_id":"844614","upvote_count":"1","poster":"SuketuKohli"},{"comment_id":"835545","poster":"gmehra","timestamp":"1678496940.0","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"858832","poster":"Kaireny54","content":"A and B says : create a lambda function to COPY also. Then, folowing your idea, A and B are out too... ;)\nobviously move argument isn't accute in this question","timestamp":"1680438780.0"},{"upvote_count":"1","content":"I searched S3 documentation and couldn't find where s3 event notification can trigger sagemaker pipelines. It can SNS,SQS and lambda. I am not sure A is the right choice.","comment_id":"925657","poster":"markw92","timestamp":"1686960600.0"}],"content":"Selected Answer: A\nAnswer is A\nThe statement says move the file. Replication won't move the file it will just create a copy. so Obviously C and D are out. When you Event notification and Lambda why we need EVent bridge as more service. So answer is A"},{"content":"Selected Answer: B\nUsing lambda is one of the requirements. Sns, sqs, lambda, and event bridge are the only s3 notification destinations\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html.","comment_id":"829461","upvote_count":"2","timestamp":"1677968880.0","poster":"Steve_4542636"},{"poster":"bullrem","comment_id":"783819","timestamp":"1674344340.0","content":"both A and D options can meet the requirements with the least operational overhead as they both use automatic event-driven mechanisms (S3 event notifications and EventBridge rules) to trigger the Lambda function and copy the files to the analysis S3 bucket. The Lambda function can then run the pattern-matching code, and the files can be sent to the SageMaker pipeline. \nOption A, directly copying the files to the analysis S3 bucket using a Lambda function, is more straight forward, option D using S3 replication and EventBridge rules is more flexible and can be more powerful as it allows you to use more complex event-driven flows.","upvote_count":"2"},{"content":"Ans : D\n\nS3 event notification can only send notifications to SQS. SNS and Lambda, BUT not Sagamaker\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\n\nS3 event notification can send notification to SNS, SQS and Lambda, but not SageMaker","upvote_count":"8","comment_id":"773912","timestamp":"1673558880.0","poster":"AHUI"},{"comment_id":"771607","content":"Selected Answer: D\nA and B are ruled out as it requires an extra Lambda job to do the copy while S3 replication will take care of it with little to no overhead.\nC is incorrect because, S3 notifcations are not supported on Sagemake pipeline (https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-destinations)","poster":"RBKumaran","timestamp":"1673367480.0","upvote_count":"6"},{"poster":"Mahadeva","comment_id":"768150","timestamp":"1673049840.0","content":"Selected Answer: C\nSince we are working already on S3 buckets, configuring S3 event notification (with evet type: s3:ObjectCreated:Put) is much easier than doing the same through EventBridge (which is an additional service in this case). Less operational overhead.","upvote_count":"5"},{"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/zh_cn/AmazonS3/latest/userguide/NotificationHowTo.html","upvote_count":"1","poster":"gustavtd","comment_id":"763832","timestamp":"1672675440.0"},{"poster":"Zerotn3","comment_id":"763125","timestamp":"1672565640.0","content":"Selected Answer: D\nI would recommend option D as it is the most efficient way to meet the requirements with the least operational overhead.\n\nOption D involves configuring S3 replication between the two S3 buckets, which will automatically copy the files from the initial S3 bucket to the analysis S3 bucket as they are added. This eliminates the need to manually copy the files every day and will ensure that the analysis S3 bucket always has the most recent data.","upvote_count":"2","comments":[{"content":"In addition, configuring the analysis S3 bucket to send event notifications to Amazon EventBridge (CloudWatch Events) and creating an ObjectCreated rule allows you to trigger Lambda functions and SageMaker Pipelines when new objects are created in the analysis S3 bucket. This allows you to perform pattern-matching and data processing on the copied data automatically as it is added to the analysis S3 bucket.\n\nOption A and option C involve manually copying the files to the analysis S3 bucket, which is not an efficient solution given the increased volume of data that the reporting team is expecting. Option B does not involve S3 replication, so it does not address the requirement to automatically copy the data to the analysis S3 bucket.","comment_id":"763126","upvote_count":"1","timestamp":"1672565700.0","poster":"Zerotn3"}]},{"upvote_count":"6","timestamp":"1672289280.0","comment_id":"760585","content":"Selected Answer: D\nOptions A and B are incorrect because it involves creating a Lambda function to copy the files to the analysis S3 bucket, which is unnecessary. The requirement is to move the files automatically to the analysis S3 bucket as soon as they are added to the initial S3 bucket. This can be achieved more efficiently using S3 replication, which is not mentioned in Options A and B.\n\nOption C is incorrect because it involves configuring S3 replication between the S3 buckets, which is correct. However, it does not involve configuring the analysis S3 bucket to send event notifications to Amazon EventBridge (CloudWatch Events). This is necessary to trigger the subsequent actions (i.e., running pattern-matching code using Lambda functions and sending data files to a pipeline in SageMaker Pipelines).\n\nTherefore, the correct answer is Option D.","poster":"Buruguduystunstugudunstuy"},{"content":"Selected Answer: D\nGoing with D","timestamp":"1672085820.0","poster":"techhb","comment_id":"757811","upvote_count":"1"},{"timestamp":"1670999220.0","poster":"wly_al","upvote_count":"2","comment_id":"744782","content":"Selected Answer: D\nlambda function for copy the data between S3 bucket was overuse and produce some cost when we can just use S3 replication"},{"content":"B. To review is the same as to analyze, that requires Lamba, and Lamba can be configure to copy to S3 after analysis. And it's serverless hence removes overhead.","comment_id":"743372","poster":"Qjb8m9h","upvote_count":"1","comments":[{"timestamp":"1670993040.0","upvote_count":"1","poster":"Qjb8m9h","content":"\"The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket\" Based on this line i think the believe the answer is D. They aren't willing to analysis the files before copying so Lamba is not required.. \nIT's D","comment_id":"744718"}],"timestamp":"1670882940.0"},{"timestamp":"1670808000.0","comments":[{"poster":"JayBee65","content":"It might add cost but does not add operational overhead.","timestamp":"1671712800.0","comment_id":"753281","upvote_count":"2"}],"comment_id":"742281","content":"I will go with B since enabling replication also requires versioning on the bucket to be enabled which adds more operational overhead eventually and cost structure","poster":"tz1","upvote_count":"1"},{"content":"Selected Answer: B\nB. Team is reviewing then copying to the analysis bucket. Review implies the need for lambda before copying.","comment_id":"740783","upvote_count":"1","timestamp":"1670661300.0","comments":[{"comment_id":"753286","content":"\"The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket.\" implies they want the files copied immediately, i.e. before being reviewed","upvote_count":"1","timestamp":"1671712920.0","poster":"JayBee65"}],"poster":"lapaki"},{"comment_id":"740781","timestamp":"1670661240.0","content":"B. Team is reviewing then copying to the analysis bucket. Review implies the need for lambda before copying.","upvote_count":"1","poster":"lapaki"},{"timestamp":"1668226140.0","poster":"nhlegend","comment_id":"716459","content":"Should be D\nThey manually copy data, uses S3 replication to copy the files (eliminate A and B)\nC is incorrect since S3 event notification can only send to SNS, SQS and Lambda\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\nSo D is correct","upvote_count":"7"},{"upvote_count":"6","comment_id":"715774","content":"Answer D - The files are getting large, less operational overhead - so will choose S3 replication. Event bridge is far more advanced than S3 event notification and they support multiple targets. S3 Event notification may not support Sagemaker. Also filtering and pattern matching available in Event bridge. So answer D","timestamp":"1668143880.0","poster":"KADSM"},{"poster":"backbencher2022","content":"Selected Answer: D\nOption D has the least overhead and should be the correct answer in my opinion.","comment_id":"712090","upvote_count":"3","timestamp":"1667694300.0"},{"poster":"LeGloupier","comment_id":"698745","content":"Selected Answer: D\nD make more sense","comments":[{"timestamp":"1666256280.0","content":"Replicating docs not related to the reporting team, you are consuming more resources (duplicating storage of docs not needed), and very probably introducing discarding overhead, noise or breaking the reporting result, as these additional docs are introduced in the analytic process.","upvote_count":"1","comments":[{"poster":"Onimole","content":"arent they already duplicating it and need it to be in another bucket?","comment_id":"711721","upvote_count":"1","timestamp":"1667647560.0"},{"comment_id":"711723","content":"someone creating a function is additional overhead. CRR replication is an aws feature","timestamp":"1667647620.0","poster":"Onimole","upvote_count":"3"}],"comment_id":"699717","poster":"123jhl0"}],"timestamp":"1666162920.0","upvote_count":"6"}],"answer":"D","unix_timestamp":1666162920,"url":"https://www.examtopics.com/discussions/amazon/view/85872-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"answer_description":""}],"exam":{"isMCOnly":true,"provider":"Amazon","id":31,"isBeta":false,"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","isImplemented":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03"},"currentPage":13},"__N_SSP":true}