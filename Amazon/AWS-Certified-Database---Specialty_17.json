{"pageProps":{"questions":[{"id":"5nOZ5YQlvRQPxHtY21kK","timestamp":"2021-11-10 22:33:00","discussion":[{"upvote_count":"1","timestamp":"1684417920.0","comment_id":"901262","poster":"cloudbusting","content":"BEF https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html"},{"poster":"Jiang_aws1","timestamp":"1664235900.0","content":"Selected Answer: BCF\nhttps://docs.aws.amazon.com/neptune/latest/userguide/dms-neptune.html\nAWS Database Migration Service (AWS DMS) can load data into Neptune from supported source databases quickly and securely.","upvote_count":"1","comment_id":"680253"},{"upvote_count":"1","timestamp":"1651249620.0","poster":"novice_expert","comment_id":"594589","content":"Selected Answer: BEF\nB. specification in separate csv\nE. access from Neptune to S3\nF. S3 vpc endpoint to run Loader command"},{"comment_id":"566191","content":"Selected Answer: BEF\nAmazon Neptune provides a Loader command for loading data from external files directly into a Neptune DB instance. You can use this command instead of executing a large number of INSERT statements, addV and addE steps, or other API calls.\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html","timestamp":"1647093720.0","upvote_count":"2","poster":"Dantas"},{"comment_id":"561143","poster":"RotterDam","upvote_count":"4","timestamp":"1646448840.0","content":"Selected Answer: BEF\nThe question is saying the professional has already uploaded the data. What was needed to complete the upload?\n\nQue: \"The database professional uploaded the data to the Neptune DB instance through a series of API calls. Which sequence of actions enables the database professional to upload the data most quickly?\"\nAns: Setup S3 VPC endpoint. Setup the IAM roles with GET and LIST permissions to S3. Make sure the contents of CSV are accurate. These are (B) (E) (F)!!"},{"poster":"harshagc180","content":"Selected Answer: BEF\nCorrect order of steps","comment_id":"559224","upvote_count":"1","timestamp":"1646204340.0"},{"content":"Selected Answer: BEF\nCorrect order of steps","timestamp":"1645727160.0","poster":"tugboat","upvote_count":"1","comment_id":"555476"},{"comment_id":"554948","content":"B,E,F: Correct","upvote_count":"1","poster":"kped21","timestamp":"1645659420.0"},{"upvote_count":"2","comments":[{"timestamp":"1642145940.0","upvote_count":"1","comment_id":"523412","content":"Sorry, I meant disagree with D","poster":"awsmonster"}],"timestamp":"1642145880.0","poster":"awsmonster","comment_id":"523411","content":"BEF,\n\nDisagree with C:\ncurl is issued from CLI to load the data from S3 to Neptune, not \"log in\" to to the database and run those commands:\n\ncurl -X POST https://your-neptune-endpoint:port/loader \\ \n -H 'Content-Type: application/json' \\ \n -d ' \n { \n \"source\" : \"s3://bucket-name/object-key-name\", \n \"format\" : \"opencypher\",\n \"userProvidedEdgeIds\": \"TRUE\", \n \"iamRoleArn\" : \"arn:aws:iam::account-id:role/role-name\", \n \"region\" : \"region\", \n \"failOnError\" : \"FALSE\", \n \"parallelism\" : \"MEDIUM\", \n }'"},{"content":"I think BEF \nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-optimize.html","comment_id":"499779","timestamp":"1639283700.0","poster":"mnzsql365","upvote_count":"3"},{"upvote_count":"1","comment_id":"497823","poster":"nood","timestamp":"1639062420.0","content":"DEF for me\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html"},{"content":"BEF is the correct answer. Look out for the \"got you\"\nAmazon Neptune provides a Loader command for loading data from external files directly into a Neptune DB instance. You can use this command instead of executing a large number of INSERT statements, addV and addE steps, or other API calls.\nThis makes D incorrect","upvote_count":"4","timestamp":"1638984180.0","comments":[{"comment_id":"497751","upvote_count":"1","timestamp":"1639056660.0","poster":"grekh001","content":"Agree with BEF\n\"To load Apache TinkerPop Gremlin data using the CSV format, you must specify the vertices and the edges in separate files.\"\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-format-gremlin.html"}],"comment_id":"496994","poster":"2025flakyt"},{"timestamp":"1638265920.0","poster":"Justu","comment_id":"490558","content":"CEF is the correct one!","upvote_count":"2"},{"upvote_count":"1","poster":"Sp230","comment_id":"486640","timestamp":"1637844060.0","content":"Bcd Loader does not need separate a addvertex addedge"},{"timestamp":"1636579980.0","comment_id":"475816","poster":"leunamE","content":"DEF\nFrom https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html\n1. Copy the data files to an Amazon Simple Storage Service (Amazon S3) bucket.\n2. Create an IAM role with Read and List access to the bucket.\n3. Create an Amazon S3 VPC endpoint.\n4. Start the Neptune loader by sending a request via HTTP to the Neptune DB instance.\n5. The Neptune DB instance assumes the IAM role to load the data from the bucket.","comments":[{"upvote_count":"2","content":"that makes sense but BCF also seem necessary for quick upload, I imagine the full question is missing","poster":"johnconnor","timestamp":"1637560020.0","comment_id":"483884","comments":[{"poster":"jove","upvote_count":"2","content":"I'm not a Neptune expert but option C, moving data to the Neptune \"Loader\" doesn't make sense. Neptune Loader is just an API which helps to load data to a NeptuneDB, right?","timestamp":"1637784180.0","comment_id":"486222"}]}],"upvote_count":"4"}],"choices":{"A":"Ensure Amazon Cognito returns the proper AWS STS tokens to authenticate the Neptune DB instance to the S3 bucket hosting the CSV file.","C":"Use AWS DMS to move data from Amazon S3 to the Neptune Loader.","E":"Ensure an IAM role for the Neptune DB instance is configured with the appropriate permissions to allow access to the file in the S3 bucket.","F":"Create an S3 VPC endpoint and issue an HTTP POST to the database's loader endpoint.","B":"Ensure the vertices and edges are specified in different .csv files with proper header column formatting.","D":"Curl the S3 URI while inside the Neptune DB instance and then run the addVertex or addEdge commands."},"exam_id":22,"url":"https://www.examtopics.com/discussions/amazon/view/65814-exam-aws-certified-database-specialty-topic-1-question-171/","question_text":"A database specialist is launching a test graph database using Amazon Neptune for the first time. The database specialist needs to insert millions of rows of test observations from a .csv file that is stored in Amazon S3. The database specialist has been using a series of API calls to upload the data to the Neptune DB instance.\nWhich combination of steps would allow the database specialist to upload the data faster? (Choose three.)","answers_community":["BEF (90%)","10%"],"isMC":true,"answer":"BEF","answer_images":[],"question_images":[],"unix_timestamp":1636579980,"answer_description":"","topic":"1","answer_ET":"BEF","question_id":81},{"id":"fSwrnxmYhHli5eZ0Vr1z","question_images":[],"question_id":82,"topic":"1","answer_images":[],"answers_community":["A (100%)"],"answer":"A","timestamp":"2021-11-10 23:20:00","exam_id":22,"choices":{"A":"Configure all replica tables to use DynamoDB auto scaling.","D":"Configure the table-level write throughput limit service quota to a higher value.","C":"Configure the primary table to use DynamoDB auto scaling and the replica tables to use manually provisioned capacity.","B":"Configure a DynamoDB Accelerator (DAX) cluster on each of the replicas."},"question_text":"A company is using Amazon DynamoDB global tables for an online gaming application. The game has players around the world. As the game has become more popular, the volume of requests to DynamoDB has increased significantly. Recently, players have reported that the game state is inconsistent between players in different countries. A database specialist observes that the ReplicationLatency metric for some of the replica tables is too high.\nWhich approach will alleviate the problem?","unix_timestamp":1636582800,"answer_ET":"A","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/65815-exam-aws-certified-database-specialty-topic-1-question-172/","discussion":[{"timestamp":"1682307660.0","comment_id":"878996","content":"Selected Answer: A\nMake sure that you have enough provisioned capacity to perform replicated writes to all global table Regions. To verify, use DynamoDB auto scaling or on-demand capacity mode.\nhttps://repost.aws/knowledge-center/replication-delay-global-table-dynamodb","upvote_count":"1","poster":"SeemaDataReader"},{"comment_id":"869248","content":"B\nbecause network latency caused the inconsistent.","poster":"Mintwater","upvote_count":"1","timestamp":"1681376760.0"},{"poster":"novice_expert","timestamp":"1651253640.0","comment_id":"594622","upvote_count":"1","content":"Selected Answer: A\nIt is important that the replica tables and secondary indexes in your global table have identical write capacity settings to ensure proper replication of data."},{"upvote_count":"1","timestamp":"1645736280.0","comment_id":"555551","content":"Selected Answer: A\nautoscaling with fix it","poster":"tugboat"},{"comment_id":"497831","content":"A for me\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_reqs_bestpractices.html","timestamp":"1639062840.0","poster":"nood","upvote_count":"1"},{"content":"Option A. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_reqs_bestpractices.html","timestamp":"1636582800.0","upvote_count":"2","poster":"leunamE","comment_id":"475830"}],"answer_description":""},{"id":"9tI1nLNDlg6CLCQ49XUs","question_images":[],"answer_ET":"C","question_id":83,"answer":"C","isMC":true,"topic":"1","question_text":"A company runs a MySQL database for its ecommerce application on a single Amazon RDS DB instance. Application purchases are automatically saved to the database, which causes intensive writes. Company employees frequently generate purchase reports. The company needs to improve database performance and reduce downtime due to patching for upgrades.\nWhich approach will meet these requirements with the LEAST amount of operational overhead?","answers_community":["C (100%)"],"choices":{"A":"Enable a Multi-AZ deployment of the RDS for MySQL DB instance, and enable Memcached in the MySQL option group.","D":"Add a read replica and promote it to an Amazon Aurora MySQL DB cluster master. Then enable Amazon Aurora Serverless.","C":"Enable a Multi-AZ deployment of the RDS for MySQL DB instance, and add a read replica.","B":"Enable a Multi-AZ deployment of the RDS for MySQL DB instance, and set up replication to a MySQL DB instance running on Amazon EC2."},"timestamp":"2021-11-24 12:33:00","exam_id":22,"answer_images":[],"unix_timestamp":1637753580,"url":"https://www.examtopics.com/discussions/amazon/view/66628-exam-aws-certified-database-specialty-topic-1-question-173/","discussion":[{"content":"Selected Answer: C\nEnable a Multi-AZ deployment of the RDS for MySQL DB instance, and add a read replica.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html","poster":"novice_expert","upvote_count":"2","comment_id":"594671","timestamp":"1651261260.0"},{"comment_id":"565483","content":"Answer is C. \nPer - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html\n\nUsing a read replica to reduce downtime when upgrading a MySQL database. If your MySQL DB instance is currently in use with a production application, you can use the following procedure to upgrade the database version for your DB instance. This procedure can reduce the amount of downtime for your application.\n\nBy using a read replica, you can perform most of the maintenance steps ahead of time and minimize the necessary changes during the actual outage. With this technique, you can test and prepare the new DB instance without making any changes to your existing DB instance.","upvote_count":"2","timestamp":"1647002280.0","poster":"ak9830"},{"timestamp":"1646457300.0","comment_id":"561230","content":"Selected Answer: C\nC is the correct answer. The question mentions LEAST operational overhead and the employees frequently create purchase reports (Read heavy). Moving queries to RR will open up the Primary to take on additional loads wrt writes.","poster":"RotterDam","upvote_count":"1"},{"timestamp":"1637802300.0","comment_id":"486358","upvote_count":"2","poster":"jove","content":"Answer : C"},{"content":"Selected Answer: C\nThe best option here is C. But I would choose Aurora Multi-Master if it was an option","upvote_count":"1","comments":[{"upvote_count":"4","comment_id":"555835","poster":"Hariru","content":"it has a high volume writes, since when can you write on a read replica?","timestamp":"1645775160.0"}],"poster":"GMartinelli","comment_id":"485895","timestamp":"1637753580.0"}],"answer_description":""},{"id":"GT9OlAooycZB39nvLMZm","answer_description":"","question_id":84,"question_text":"An ecommerce company is migrating its core application database to Amazon Aurora MySQL. The company is currently performing online transaction processing\n(OLTP) stress testing with concurrent database sessions. During the first round of tests, a database specialist noticed slow performance for some specific write operations.\nReviewing Amazon CloudWatch metrics for the Aurora DB cluster showed 90% CPU utilization.\nWhich steps should the database specialist take to MOST effectively identify the root cause of high CPU utilization and slow performance? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/66252-exam-aws-certified-database-specialty-topic-1-question-174/","question_images":[],"unix_timestamp":1637176080,"answers_community":["AC (75%)","AE (25%)"],"isMC":true,"exam_id":22,"answer_images":[],"topic":"1","timestamp":"2021-11-17 20:08:00","choices":{"E":"Enable Advance Auditing to log QUERY events in Amazon CloudWatch before the next round of tests.","B":"Review the VolumeBytesUsed metric in CloudWatch to see if there is a spike in write I/O.","A":"Enable Enhanced Monitoring at less than 30 seconds of granularity to review the operating system metrics before the next round of tests.","C":"Review Amazon RDS Performance Insights to identify the top SQL statements and wait events.","D":"Review Amazon RDS API calls in AWS CloudTrail to identify long-running queries."},"answer_ET":"AC","answer":"AC","discussion":[{"comment_id":"480227","upvote_count":"15","poster":"jelongpark","content":"A,C i smy pick.\nB: VolumeBytesUsed metric is for total used storage size, not for CPU","timestamp":"1637176080.0"},{"content":"https://aws.amazon.com/premiumsupport/knowledge-center/rds-instance-high-cpu/\n\nA, C is the answer","poster":"rags1482","upvote_count":"1","timestamp":"1667060100.0","comment_id":"707318"},{"upvote_count":"1","timestamp":"1665837060.0","comment_id":"695384","poster":"awsjjj","content":"Selected Answer: AC\nE is for auditing including query events. For performance I go with A and C"},{"timestamp":"1663833780.0","poster":"venimus_vidimus_vicimus","content":"Selected Answer: AE\nGuys.. why are you all Posting RDS Docs Links? This Question is about Aurora which is why I go with A and E","upvote_count":"2","comment_id":"675867","comments":[{"poster":"ixdb","content":"Performance Insights is currently available for Amazon Aurora PostgreSQL compatible Edition, MySQL compatible Edition, PostgreSQL, MySQL, Oracle, SQL Server, and MariaDB.","comment_id":"873716","upvote_count":"1","timestamp":"1681827780.0"},{"comment_id":"908111","timestamp":"1685206320.0","upvote_count":"2","poster":"lehoang15tuoi","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_PerfInsights.Overview.html\n\nyou can use Perf Insight in *Aurora* to identify the most heavy-hitting queries. Nobody uses Advance Auditing to solve performance problems..."}]},{"poster":"venimus_vidimus_vicimus","upvote_count":"1","content":"Guys.. why are you all Posting RDS Docs Links? This Question is about Aurora which is why I go with A and E","timestamp":"1663833720.0","comment_id":"675866"},{"comment_id":"646214","content":"Selected Answer: AC\nA,C are related to performance. B is related to storage. D is related to security (API calls tracking) and E (Advanced Auditing) does not exist.","timestamp":"1660378320.0","upvote_count":"1","poster":"shammous","comments":[{"comment_id":"686518","upvote_count":"1","content":"Using Advanced Auditing with an Amazon Aurora MySQL DB cluster\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Auditing.html","timestamp":"1664935320.0","poster":"Jiang_aws1"}]},{"comment_id":"594500","poster":"novice_expert","timestamp":"1651239960.0","upvote_count":"2","content":"Selected Answer: AC\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-instance-high-cpu/\n\nA. Enhanced Monitoring checks OS\nxB. VolumeBytesUsed is the amount of storage used \nC. Performance Insights"},{"content":"Selected Answer: AC\nPer - https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-slow-query/\n\nTo analyze the workload contributing to resource consumption, use Performance Insights. Performance Insights will provide a graphic analysis of all your queries and any waits that are contributing to increased resource consumption.\n\nYou can also use Enhanced Monitoring to retrieve the list of operating systems involved in your workload and the underlying system metrics. By default, the monitoring interval for Enhanced Monitoring is 60 seconds. It's a best practice to set this to 1-5 second intervals for more granular data points.","upvote_count":"2","timestamp":"1645659720.0","comment_id":"554950","poster":"tugboat"},{"upvote_count":"1","comment_id":"554915","poster":"kped21","content":"A,C : PI and Enhanced Monitoring","timestamp":"1645654380.0"},{"timestamp":"1639249200.0","poster":"123xjuanperez","content":"A,C is the solution","upvote_count":"1","comment_id":"499615"},{"content":"A,C for me\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-instance-high-cpu/","upvote_count":"1","poster":"nood","comment_id":"497795","timestamp":"1639060140.0"},{"poster":"grekh001","upvote_count":"1","comment_id":"497691","timestamp":"1639051260.0","content":"A and C is correct.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-slow-query/"},{"comment_id":"490218","poster":"IshtarSQL","timestamp":"1638216780.0","upvote_count":"1","content":"C,D is my choice \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/logging-using-cloudtrail.html\n:\"enableCloudwatchLogsExports\": [\n \"audit\",\n \"error\",\n \"general\",\n \"slowquery\""}]},{"id":"41Nr27SoPI1CpYUxpW0K","answer_images":[],"question_id":85,"answer":"C","isMC":true,"answer_ET":"C","timestamp":"2021-11-12 00:51:00","question_images":[],"question_text":"An online advertising company is implementing an application that displays advertisements to its users. The application uses an Amazon DynamoDB table as a data store. The application also uses a DynamoDB Accelerator (DAX) cluster to cache its reads. Most of the reads are from the GetItem query and the\nBatchGetItem query. Consistency of reads is not a requirement for this application.\nUpon deployment, the application cache is not performing as expected. Specific strongly consistent queries that run against the DAX cluster are taking many milliseconds to respond instead of microseconds.\nHow can the company improve the cache behavior to increase application performance?","exam_id":22,"answers_community":["C (100%)"],"choices":{"D":"Create a new DAX cluster with a higher TTL for the item cache.","C":"Use eventually consistent reads instead of strongly consistent reads.","B":"Configure DAX to be an item cache with no query cache","A":"Increase the size of the DAX cluster."},"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/65861-exam-aws-certified-database-specialty-topic-1-question-175/","discussion":[{"content":"Selected Answer: C\nAs strong consistency is not a requirement","upvote_count":"1","comment_id":"878976","poster":"SeemaDataReader","timestamp":"1682305080.0"},{"content":"Selected Answer: C\nextremely consistent => bypass DAX","poster":"novice_expert","comment_id":"594740","upvote_count":"2","timestamp":"1651277520.0"},{"upvote_count":"2","poster":"tugboat","comment_id":"555610","content":"Selected Answer: C\nEventually consistent reads are good","timestamp":"1645741620.0"},{"upvote_count":"4","comment_id":"528393","content":"I vote for C","poster":"awsmonster","timestamp":"1642681260.0"},{"poster":"jove","content":"Selected Answer: C\nC is correct","upvote_count":"1","timestamp":"1638044160.0","comment_id":"488488"},{"upvote_count":"1","timestamp":"1637941440.0","content":"Option C","comment_id":"487489","poster":"Sp230"},{"timestamp":"1636674660.0","content":"Option B. If you specify zero as the query cache TTL setting, the query response will not be cached.","poster":"leunamE","comments":[{"comment_id":"478277","upvote_count":"2","poster":"leunamE","timestamp":"1636914600.0","content":"I was wrong. The correct answer is C. Consistent queries are sent directly to dynamodb so the response time is longer."},{"upvote_count":"1","comment_id":"478005","poster":"johnconnor","content":"I am not sure about that one \"Strongly consistent reads may have higher latency than eventually consistent reads\", hence eventually consistent reads have lower latency, we are trying to lower the latency, C may be right","timestamp":"1636878000.0"}],"comment_id":"476565","upvote_count":"1"}],"unix_timestamp":1636674660,"answer_description":""}],"exam":{"numberOfQuestions":359,"isImplemented":true,"name":"AWS Certified Database - Specialty","id":22,"provider":"Amazon","isMCOnly":false,"lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":17},"__N_SSP":true}