{"pageProps":{"questions":[{"id":"O88INwgKBBZBcYTWX3y8","question_images":[],"answers_community":["C (60%)","B (40%)"],"discussion":[{"upvote_count":"28","comment_id":"28403","timestamp":"1632222240.0","content":"Ans C is reasonable","poster":"rsimham"},{"upvote_count":"6","timestamp":"1636279800.0","comment_id":"278713","poster":"cloud_trail","content":"Agree with C. Quicksight cannot handle 100TB each day."},{"comment_id":"1303006","content":"Selected Answer: C\nAmazon QuickSight, particularly when using its SPICE (Super-fast, Parallel, In-memory Calculation Engine) feature, has specific data capacity limits. For the Enterprise Edition, SPICE can handle up to 1 billion rows or 1 TB per dataset1. This means that while QuickSight is highly capable, handling 100 TB of data per day would exceed its current capacity limits.","poster":"MultiCloudIronMan","upvote_count":"1","timestamp":"1729888200.0"},{"timestamp":"1728396960.0","content":"Selected Answer: B\nThe limit of QuickSight for 1TB is soft limit which can be increased to unlimited number of TBs.","poster":"AMEJack","comment_id":"1294743","upvote_count":"1"},{"poster":"Ali_Redha","comment_id":"1176972","upvote_count":"2","timestamp":"1710820440.0","content":"Ans C Because Quicksight Can't handle \n\n100 TB even in Entiripise \n\n\n\n\nQuotas for SPICE are as follows:\n\n2,047 Unicode characters for each field\n\n127 Unicode characters for each column name\n\n2,000 columns for each file\n\n1,000 files for each manifest\n\nFor Standard edition, 25 million (25,000,000) rows or 25 GB for each dataset\n\nFor Enterprise edition, 1 billion (1,000,000,000) rows or 1 TB for each dataset\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html"},{"content":"QuickSight can handle large volumes of data for analytics and visualizations. Some key points:\n\nQuickSight scales seamlessly from hundreds of megabytes to many terabytes of data without needing to manage infrastructure.\n\nIt uses an in-memory engine called SPICE to enable high performance analytics on large datasets.\nso the choice is B","poster":"VR10","upvote_count":"1","timestamp":"1708420740.0","comment_id":"1154595"},{"upvote_count":"1","timestamp":"1707352020.0","comment_id":"1143920","content":"Selected Answer: B\nB. Generate daily precision-recall data in Amazon QuickSight, and publish the results in a dashboard shared with the Business team.\n\nThis solution leverages QuickSight's managed service capabilities for both data processing and visualization, which should minimize the coding effort required to provide the Business team with the necessary insights. However, it's important to note that QuickSight's ability to calculate the precision-recall data depends on its support for the necessary statistical functions or the availability of such calculations in the dataset. If QuickSight cannot perform these calculations directly, option C might be necessary, despite the increased effort.","poster":"kyuhuck"},{"comment_id":"1139406","upvote_count":"2","poster":"Topg4u","timestamp":"1706975640.0","content":"The question does not ask for processing of 1Tb data. it asks for visuals/predications of that data. So B"},{"poster":"phdykd","content":"C.\nConsidering the large volume of data (100 TB daily), Option C seems to be the most appropriate solution","timestamp":"1704641640.0","upvote_count":"1","comment_id":"1115970"},{"upvote_count":"2","timestamp":"1700226780.0","content":"Selected Answer: C\nB it's not correct because of 100tb data size.\nC is the answer: https://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html","poster":"iskorini","comment_id":"1073334"},{"upvote_count":"1","poster":"Snape","content":"Selected Answer: C\nANs c is correct","comment_id":"1054996","timestamp":"1698365700.0"},{"content":"Selected Answer: C\nA. NO - we want a dashboard for business\nB. NO - 100TB is very large, it will not fit in memory (1TB max for SPICE dataset) or return within the 2min limit if delegated to a DB (https://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html)\nC. YES - best combination; EMR can distribute the computation of precision-recall for each slice of data\nD. NO - ES cannot help to generate precision-recall","poster":"loict","timestamp":"1694673840.0","comment_id":"1007300","upvote_count":"1"},{"content":"Selected Answer: B\nalthough C is tempting but goes with B due to less effort","comment_id":"992244","poster":"Mickey321","upvote_count":"1","timestamp":"1693230240.0","comments":[{"upvote_count":"1","poster":"teka112233","content":"it is not about the least effort only, since the least effort solution here will not get your job done, look at the quick sight max data it can deal with when it compared to EMR which is built to deal with Big data.","comment_id":"993546","timestamp":"1693348200.0"}]},{"comment_id":"987003","upvote_count":"2","content":"Selected Answer: C\nusing quick sight for creation of the precision recall with 100 TB every day cann't be done since the max size for quick sight to deal with is :\nFor Standard edition, 25 million (25,000,000) rows or 25 GB for each dataset\nFor Enterprise edition, 1 billion (1,000,000,000) rows or 1 TB for each dataset\nacc to AWS documentation :\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html\nbut we can do it with EMR and latterly use quick sight to visualize the results","poster":"teka112233","timestamp":"1692669540.0"},{"content":"Selected Answer: C\nLooking at the QuickSight documentation: it has a limit of 1 TB per dataset. So it's necessary a previous layer. Letter C is the correct one.","upvote_count":"1","comment_id":"968428","poster":"kaike_reis","timestamp":"1690829700.0"},{"upvote_count":"1","poster":"ADVIT","comment_id":"938664","content":"It's 100TB daily, need EMR to reduce, option C is correct.","timestamp":"1688085660.0"},{"poster":"petervu","timestamp":"1686987180.0","content":"Selected Answer: C\nQuicksight can handle maximum 1TB data set only. We have 100TB data set so we need EMR.\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html","upvote_count":"3","comment_id":"925832"},{"upvote_count":"2","comment_id":"870938","content":"Selected Answer: C\nc is correct answer","poster":"Ahmedhadi_","timestamp":"1681564020.0"},{"comments":[{"comment_id":"966547","timestamp":"1690651980.0","upvote_count":"2","content":"so? chatgpt is a parrot that says whatever you want to listen","poster":"ccpmad"}],"content":"Selected Answer: B\nB is the correct answer according to this resource and even according to ChatGPT, GPT-4 and Claude+\nhttps://aws.amazon.com/blogs/big-data/diligent-enhances-customer-governance-with-automated-data-driven-insights-using-amazon-quicksight/","poster":"Ahmedhadi_","timestamp":"1681505940.0","comment_id":"870474","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: B\nQuicksight can do it now with customazable methods!","comment_id":"832781","comments":[{"poster":"Valcilio","comment_id":"832783","timestamp":"1678268400.0","upvote_count":"1","content":"I don't know if in the past quicksight wasn't serveless, but today it's and can analyze as much data as is needed!"}],"poster":"Valcilio","timestamp":"1678268340.0"},{"poster":"drcok87","content":"Seems like quicksight can do all of this through built-in capabilities https://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html\n\nEarlier I would go with C but now I'm leaning towards B","comment_id":"792919","timestamp":"1675089000.0","upvote_count":"1"},{"timestamp":"1674461460.0","content":"Q: How much data can I analyze with Amazon QuickSight?\nWith Amazon QuickSight you don’t need to worry about scale. You can seamlessly grow your data from a few hundred megabytes to many terabytes of data without managing any infrastructure. \nhttps://aws.amazon.com/quicksight/resources/faqs/\nQuicksight indeed can handle 100 TB data and i think answer should be B.","comment_id":"785117","upvote_count":"2","poster":"muralee_xo"},{"poster":"eussou","upvote_count":"2","comment_id":"781999","timestamp":"1674203040.0","content":"Selected Answer: C\nC is what works as Quick Sight alone could not do it with B"},{"content":"Selected Answer: B\nThe question specifically states: Which solution requires the LEAST coding effort?\nWith that being the real question the answer will be B. \nC requires a lot of coding and it is possible for Quicksight to handle 100 TB of predictions, but it would require a high performance configuration of the underlying resources ( a lot). That is my thought but could be wrong.","upvote_count":"3","comment_id":"777770","poster":"Tomatoteacher","timestamp":"1673879220.0"},{"poster":"bitsplease","upvote_count":"1","comment_id":"529223","content":"why not D?","timestamp":"1642780140.0"},{"comment_id":"121747","content":"can someone explain why we need EMR here? Quicksight is capable of running calculations..","upvote_count":"4","poster":"Wira","comments":[{"upvote_count":"6","comment_id":"126790","content":"100TB data to much four quick sight alone","timestamp":"1634785260.0","poster":"oief2oi3fj23ogi23g"},{"comment_id":"127126","upvote_count":"5","content":"Large volume.......that's why EMR is good. you can't even proceed with EC2 instance, however, Quicksight is ligh weight BI tool. From my own project experience, we move away from Quicksight to Tableau in middle of the deployment.","poster":"Urban_Life","timestamp":"1635322620.0"}],"timestamp":"1634110740.0"},{"timestamp":"1632835080.0","content":"C is correct...100 TB daily can be handled by EMR and Quicksight (no coding) is the right solution for providing read only access to business","upvote_count":"4","comment_id":"65484","poster":"PRC"},{"comments":[{"comment_id":"131132","timestamp":"1636084260.0","poster":"qwerty456","content":"B won't solve the \"daily scheduled\" requirement","upvote_count":"4"}],"content":"Agree with C \n\nBUT\n\nB would have been more appropriate if CURVE was specified instead of data\n\nGenerate daily precision-recall CURVE (instead of data) in Amazon QuickSight, and publish the results in a dashboard shared with the Business team","comment_id":"57560","timestamp":"1632224520.0","upvote_count":"2","poster":"rajs"}],"unix_timestamp":1575944760,"answer_ET":"C","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/10083-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A company is running a machine learning prediction service that generates 100 TB of predictions every day. A Machine Learning Specialist must generate a visualization of the daily precision-recall curve from the predictions, and forward a read-only version to the Business team.\nWhich solution requires the LEAST coding effort?","timestamp":"2019-12-10 03:26:00","answer":"C","isMC":true,"topic":"1","choices":{"A":"Run a daily Amazon EMR workflow to generate precision-recall data, and save the results in Amazon S3. Give the Business team read-only access to S3.","D":"Generate daily precision-recall data in Amazon ES, and publish the results in a dashboard shared with the Business team.","B":"Generate daily precision-recall data in Amazon QuickSight, and publish the results in a dashboard shared with the Business team.","C":"Run a daily Amazon EMR workflow to generate precision-recall data, and save the results in Amazon S3. Visualize the arrays in Amazon QuickSight, and publish them in a dashboard shared with the Business team."},"question_id":321,"answer_description":"","exam_id":26},{"id":"9NDGSDzMrm3jSflkj3zf","unix_timestamp":1575944880,"isMC":true,"answers_community":["C (100%)"],"answer_images":[],"answer_description":"","question_images":[],"exam_id":26,"question_text":"A Machine Learning Specialist is preparing data for training on Amazon SageMaker. The Specialist is using one of the SageMaker built-in algorithms for the training. The dataset is stored in .CSV format and is transformed into a numpy.array, which appears to be negatively affecting the speed of the training.\nWhat should the Specialist do to optimize the data for training on SageMaker?","answer_ET":"C","choices":{"D":"Use the SageMaker hyperparameter optimization feature to automatically optimize the data.","B":"Use AWS Glue to compress the data into the Apache Parquet format.","A":"Use the SageMaker batch transform feature to transform the training data into a DataFrame.","C":"Transform the dataset into the RecordIO protobuf format."},"timestamp":"2019-12-10 03:28:00","url":"https://www.examtopics.com/discussions/amazon/view/10084-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer":"C","discussion":[{"timestamp":"1663744500.0","content":"C is okay","upvote_count":"19","comment_id":"28405","poster":"rsimham"},{"timestamp":"1664829300.0","comment_id":"57062","upvote_count":"16","content":"Anwer is C.\nMost Amazon SageMaker algorithms work best when you use the optimized protobuf recordIO format for the training data.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html","poster":"stamarpadar"},{"comment_id":"992245","poster":"Mickey321","content":"Selected Answer: C\noption C","timestamp":"1724852700.0","upvote_count":"1"},{"poster":"AjoseO","content":"Selected Answer: C\nThe Specialist should transform the dataset into the RecordIO protobuf format. This format is optimized for use with SageMaker and has been shown to improve the speed and efficiency of training algorithms. \n\nUsing the RecordIO protobuf format is a best practice for preparing data for use with Amazon SageMaker, and it is specifically recommended for use with the built-in algorithms.","upvote_count":"1","timestamp":"1707576600.0","comment_id":"804444"},{"content":"Selected Answer: C\nI would assume the issue is the transformation. It can be nasty slow between pandas / csv / numpy. Go to protobuf.","poster":"Jeremy1","upvote_count":"1","timestamp":"1700206560.0","comment_id":"720300"},{"content":"C is the best","poster":"C10ud9","upvote_count":"5","timestamp":"1667512260.0","comment_id":"102676"},{"poster":"PRC","upvote_count":"6","timestamp":"1666258980.0","content":"Agree with C","comment_id":"65485"}],"question_id":322},{"id":"xYAc62Ogaf91j6NJRL17","topic":"1","unix_timestamp":1573961880,"choices":{"A":"Increase the training data by adding variation in rotation for training images.","B":"Increase the number of epochs for model training","C":"Increase the number of layers for the neural network.","D":"Increase the dropout rate for the second-to-last layer."},"question_id":323,"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/8386-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"comments":[{"poster":"Jeremy1","comment_id":"720301","timestamp":"1684301820.0","content":"Donald, your caps lock is on.","upvote_count":"13","comments":[{"poster":"kaike_reis","timestamp":"1706823600.0","comment_id":"969339","content":"Okay, was funny","upvote_count":"1"},{"poster":"Nadia0012","content":"LOL :D","comment_id":"832767","upvote_count":"1","timestamp":"1694158020.0"}]},{"upvote_count":"1","timestamp":"1706556900.0","content":"is it possible no using MAYUS? it is annoying","poster":"ccpmad","comment_id":"966551"},{"content":"The key phrase might be \"constant test set\", so you can't increase training set by shrinking the size of test set. Thus the only feasible choice is to increase training time by increasing the number of epochs => answer B.","upvote_count":"2","comment_id":"47668","timestamp":"1649165460.0","comments":[{"comments":[{"poster":"Urban_Life","upvote_count":"1","timestamp":"1650736500.0","comment_id":"127155","content":"What's your answer B?"}],"comment_id":"93623","upvote_count":"3","content":"The problem is images are upside down and misclassified. If right side up then the model would classify correctly. This can only be fixed ba rotating not by trying to recognise upside down cat more times.","timestamp":"1650041040.0","poster":"mawsman"},{"timestamp":"1649543040.0","comment_id":"60887","upvote_count":"1","content":"A . Increase the training data by adding variation in rotation for training images.\n\nIt never says to move the images from Test data set (because it is constant)... only variations are added to the images..so, A is correct.","poster":"VB"}],"poster":"tap123"},{"upvote_count":"9","content":"agree with A","timestamp":"1648638960.0","comment_id":"28410","poster":"rsimham"}],"poster":"DonaldCMLIN","upvote_count":"63","timestamp":"1647957540.0","content":"NO CORRECT TRAINING DATA, MORE WORKS JUST WASTE TIME.\n\nONE OF THE REASONS FOR POOR ACCURACY COULD BE INSUFFICIENT DATA. THIS CAN BE OVERCOME BY IMAGE AUGMENTATION.\nIMAGE AUGMENTATION IS A TECHNIQUE OF INCREASING THE DATASET SIZE BY PROCESSING (MIRRORING, FLIPPING, ROTATING, INCREASING/DECREASING BRIGHTNESS, CONTRAST, COLOR) THE IMAGES.\n\nHTTPS://MEDIUM.COM/DATADRIVENINVESTOR/AUTO-MODEL-TUNING-FOR-KERAS-ON-AMAZON-SAGEMAKER-PLANT-SEEDLING-DATASET-7B591334501E\n\nANSWER A. ADD MORE TRAINING DATA FOR ROTATION IMAGES COULD BE A WAY TO DEAL WITH ISSUE","comment_id":"22102"},{"upvote_count":"1","comment_id":"1117129","timestamp":"1720478760.0","content":"A is answer","poster":"phdykd"},{"upvote_count":"2","poster":"Kensev","comment_id":"1108188","timestamp":"1719611220.0","content":"Selected Answer: A\nData Augmentation would fix the missing conditional data"},{"content":"Selected Answer: A\nChatGPT says the answer is A. Trust a model to answer an ML question correctly! ;)","poster":"cgsoft","timestamp":"1715747340.0","comment_id":"1071137","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nhow come more epochs it better than augmentation?","timestamp":"1712918940.0","poster":"AmeeraM","comment_id":"1041672"},{"poster":"Mickey321","content":"Selected Answer: A\noption A","timestamp":"1709135160.0","comment_id":"992247","upvote_count":"1"},{"content":"Selected Answer: A\nshould be A","poster":"nilmans","comment_id":"930514","upvote_count":"1","timestamp":"1703256180.0"},{"content":"Selected Answer: A\nMore epochs is not a good approach to fundamental data issues","poster":"earthMover","upvote_count":"2","comment_id":"906165","timestamp":"1700868840.0"},{"content":"Selected Answer: A\nthe Specialist can apply data augmentation techniques to increase the training data by adding variation in rotation for training images. This technique will allow the model to learn to recognize cats in various orientations, including upside down.","poster":"oso0348","upvote_count":"1","comment_id":"834184","timestamp":"1694270220.0"},{"comment_id":"804445","content":"Selected Answer: A\nAdding more variation in rotation to the training data can help the model to learn how to classify cats in different orientations, including when they are held upside down. This can improve the model's ability to identify cats in this position and reduce the misclassification rate for images in which the cats are upside down.\n\nBy adding more rotation to the training data, the model can be trained to generalize better to new images, including those with cats in different orientations. This can help to reduce overfitting and improve the model's overall performance.","timestamp":"1691671860.0","upvote_count":"1","poster":"AjoseO"},{"timestamp":"1689510600.0","content":"Selected Answer: A\nOnly logical answer 100% A.","poster":"Tomatoteacher","comment_id":"777778","upvote_count":"1"},{"upvote_count":"1","timestamp":"1684301880.0","poster":"Jeremy1","comment_id":"720302","content":"Selected Answer: A\nMore data is a good answer. A"},{"comment_id":"674087","poster":"ryuhei","timestamp":"1679319000.0","upvote_count":"1","content":"Selected Answer: A\nAnswer is ”A””"},{"timestamp":"1673727720.0","content":"Answer is A","poster":"Morsa","comment_id":"631489","upvote_count":"1"},{"upvote_count":"1","timestamp":"1671837360.0","comment_id":"621321","poster":"ovokpus","content":"Selected Answer: A\nThis is a clear case of Data Augumentation solution."},{"content":"Selected Answer: A\nCommon step in CNN, Image augmentation. A.","poster":"yc1005","upvote_count":"1","timestamp":"1670595240.0","comment_id":"613971"},{"timestamp":"1661481240.0","poster":"AmakamaxZanny","upvote_count":"1","comment_id":"556508","content":"Selected Answer: A\nimage data should be augmented which includes rotation (A)"},{"comment_id":"539422","timestamp":"1659499020.0","poster":"apprehensive_scar","upvote_count":"2","content":"Selected Answer: A\nA! How come all the answers this site gives out are incorrect?"},{"timestamp":"1655332260.0","comments":[{"poster":"joe3232","timestamp":"1690725900.0","content":"or just rotate the images and get a better model","comment_id":"793016","upvote_count":"2"}],"comment_id":"502542","content":"B is the answer. The model always goes from underfitting to Optimal to overfitting when you increase the epochs. The problem statement says that data is misclassified (not correctly classified) by the model. The question then arise how can you make the model correctly classify the data.....and the answer is by passing it through the model more numbers of the time (epochs) so that it can see the correct orientation. No where it says the data is not correct. Another way to say this - by adding more data (choice A) we are not making model classify it correctly. But by increasing number of epochs you are.","poster":"Asrivastava3","upvote_count":"2"},{"timestamp":"1654746600.0","content":"it should be A","comment_id":"497361","poster":"pp13","upvote_count":"1"},{"timestamp":"1651635900.0","upvote_count":"3","poster":"eganilovic","content":"The answer is A!","comment_id":"343517"},{"poster":"tmld","timestamp":"1651493100.0","upvote_count":"1","comment_id":"335991","content":"My answer is A"},{"poster":"ahquiceno","timestamp":"1651334880.0","upvote_count":"1","content":"Answer is A, model need variation of data that CNN need learn and then predict, in this case data augmentation is the recommended step.","comment_id":"281292"},{"content":"Since the test set is constant, to modify your training data based on what you have saw on your test set will overfitting your model. Answer is probably B.","comment_id":"201135","timestamp":"1651287600.0","poster":"weslleylc","upvote_count":"2"},{"content":"The ML Specialist notices that, in over 75% of the misclassified images, the cats were held upside down by their owners.\nOption A would solve this problem","poster":"syu31svc","timestamp":"1651195200.0","upvote_count":"2","comment_id":"169137"},{"comment_id":"127160","timestamp":"1651088760.0","upvote_count":"1","poster":"Urban_Life","content":"I think the answer is A. If you add more training data with rotated image, it will mitigate the problem. I think B comes next, after answer A is not solving the problem. Agree?"},{"content":"correct answer is A","upvote_count":"1","poster":"Antriksh","comment_id":"108606","timestamp":"1650633120.0"},{"timestamp":"1650143280.0","comment_id":"103443","upvote_count":"2","poster":"qururu","content":"The answer is A. This is a no-brainer if you have DL experience with image recognition. Data augmentation is a must have."},{"comment_id":"102678","content":"A it is","poster":"C10ud9","upvote_count":"2","timestamp":"1650097740.0"},{"upvote_count":"2","poster":"deep_n","timestamp":"1649828460.0","comment_id":"89562","content":"by far A is correct"},{"poster":"AKT","content":"Answer is A. you need to do data augmentation of training images to get better results.","comment_id":"59548","timestamp":"1649280360.0","upvote_count":"1"},{"comment_id":"45349","content":"It can't be A because \"the cats were held upside down by their owners\". The question wants to says that the owners did the data augmentation manually.\nIt doesn't say anything about the train accuracy so B or C is still ok.\nIf the train accuracy is low too, B is okay.\nIf B can't help, I will do C.\nB came first so B is my choice.","poster":"Phong","timestamp":"1648976700.0","upvote_count":"1"},{"content":"It can't be A because \"the cats were held upside down by their owners\". The question wants to says that the owners did the data augmentation manually. \nIt doesn't say anything about the train accuracy so B or C is still ok.\nIf the train accuracy is low too, B is okay.\nIf B can't help, I will do C.\nB came first so C is my choice.","comment_id":"45348","comments":[{"upvote_count":"6","content":"Look more carefully. The upside down cats were in the test set not the training set. So you still need rotated examples to train on...","comment_id":"47871","timestamp":"1649224680.0","poster":"Walz"}],"poster":"Phong","timestamp":"1648652280.0","upvote_count":"1"}],"answer":"A","answer_ET":"A","timestamp":"2019-11-17 04:38:00","answer_images":[],"answer_description":"","question_text":"A Machine Learning Specialist is required to build a supervised image-recognition model to identify a cat. The ML Specialist performs some tests and records the following results for a neural network-based image classifier:\nTotal number of images available = 1,000\nTest set images = 100 (constant test set)\nThe ML Specialist notices that, in over 75% of the misclassified images, the cats were held upside down by their owners.\nWhich techniques can be used by the ML Specialist to improve this specific test error?","exam_id":26,"isMC":true,"question_images":[]},{"id":"eGo2SXG9dr4IQdMvQ95I","answer_ET":"C","topic":"1","exam_id":26,"question_text":"A Machine Learning Specialist needs to be able to ingest streaming data and store it in Apache Parquet files for exploration and analysis.\nWhich of the following services would both ingest and store this data in the correct format?","answers_community":["C (100%)"],"answer_images":[],"answer_description":"","question_id":324,"url":"https://www.examtopics.com/discussions/amazon/view/10085-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"B":"Amazon Kinesis Data Streams","C":"Amazon Kinesis Data Firehose","A":"AWS DMS","D":"Amazon Kinesis Data Analytics"},"answer":"C","question_images":[],"isMC":true,"timestamp":"2019-12-10 03:30:00","discussion":[{"comments":[{"poster":"shammous","content":"The storage part will be taken care of by S3 anyway. Firehose would just transform to Parquet on the fly.","timestamp":"1723802760.0","upvote_count":"1","comment_id":"1266953"}],"poster":"JayK","content":"the answer is C. as the main point of the question is data transformation to Parquet format which is done by Kinesis Data Firehose not Data Stream. Coming to the data store the data store in Kinesis Data Stream is only for couple of days so it does not serve the purpose here","comment_id":"35301","timestamp":"1632415680.0","upvote_count":"52"},{"timestamp":"1636290120.0","content":"Firehose","comment_id":"343519","upvote_count":"5","poster":"eganilovic"},{"content":"Not sure Firehose can store the data .... Data Stream can store the data. Someone please explain the answer","upvote_count":"1","poster":"earthMover","comments":[{"timestamp":"1690829880.0","upvote_count":"1","content":"Firehose is to Store the data. Stream requires other service to do that.","comment_id":"968432","poster":"kaike_reis"}],"comment_id":"906167","timestamp":"1684964340.0"},{"comment_id":"889877","content":"Kinesis Data Streams can Store for up to 365 days, While Firehouse sends it to S3. Which is correct?","timestamp":"1683273180.0","poster":"GOSD","upvote_count":"1"},{"poster":"Valcilio","upvote_count":"2","content":"Selected Answer: C\nFirehose can do it if the data is in JSON or ORC format initially!","comment_id":"832785","timestamp":"1678268580.0"},{"comment_id":"824577","content":"It should be KDS","timestamp":"1677575220.0","poster":"DS2021","upvote_count":"1"},{"poster":"AjoseO","content":"Selected Answer: C\nAmazon Kinesis Data Firehose is a fully managed service that can automatically load streaming data into data stores and analytics tools. \n\nIt can ingest real-time streaming data such as application logs, website clickstreams, and IoT telemetry data, and then store it in the correct format, such as Apache Parquet files, for exploration and analysis. \n\nThis makes it a suitable option for the requirement described in the question.","upvote_count":"1","comment_id":"804599","timestamp":"1676049360.0"},{"upvote_count":"2","poster":"Thai_Xuan","comment_id":"202472","content":"B\nhttps://github.com/ravsau/aws-exam-prep/issues/10","timestamp":"1635990540.0"},{"upvote_count":"3","content":"B) Only Amazon Kinesis Data Streams can store and Ingest data. We don't need to apply any transformation; the question asks to ingest and store data in Apache Parquet format, There is no assumption that the data coming in a different format than parquet.","poster":"weslleylc","comments":[{"upvote_count":"1","content":"KDS cant store to s3\nhttps://stackoverflow.com/questions/66097886/writing-to-s3-via-kinesis-stream-or-firehose","comment_id":"793093","poster":"joe3232","timestamp":"1675099320.0"}],"timestamp":"1635805500.0","comment_id":"202078"},{"content":"It is C with no doubt\nhttps://aws.amazon.com/about-aws/whats-new/2018/05/stream_real_time_data_in_apache_parquet_or_orc_format_using_firehose/","poster":"In","timestamp":"1635647760.0","comment_id":"183701","upvote_count":"5"},{"comments":[{"poster":"GeeBeeEl","content":"It appears all agree that the answer is between Firehose and Analytics. Data Streams handle stuff like event data, clickstream etc. Its not interested in special format, the focus is speed. The question did not talk of transformation, only ingestion. Kinesis Firehose is used for ingestion. Both firehose and analytics can store, only firehose can ingest. https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html shows firehose can store parquet to S3","timestamp":"1635544440.0","upvote_count":"1","comment_id":"148923"}],"upvote_count":"3","content":"It appears all agree that the answer is between Firehose and Analytics. Kinesis Firehose is used for ingestion. Both firehose and analytics can store, only firehose can ingest. https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html shows firehose can store parquet to S3","timestamp":"1634266440.0","comment_id":"148921","poster":"GeeBeeEl"},{"poster":"Urban_Life","comment_id":"127165","content":"Think just like this -- batch process Glue ETL and Streaming process Firehose ETL ......covert to parquet or any other format.","timestamp":"1633735260.0","upvote_count":"1"},{"comment_id":"109311","upvote_count":"2","poster":"CMMC","content":"C for Firehose","timestamp":"1633587900.0"},{"upvote_count":"2","poster":"Erso","content":"Just in case https://acloud.guru/forums/aws-certified-big-data-specialty/discussion/-KhI3MgPEo-FY5rfgl3J/what_is_difference_between_kin","timestamp":"1632925020.0","comment_id":"81587"},{"upvote_count":"3","content":"Amazon Kinesis Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3.\nhttps://github.com/awsdocs/amazon-kinesis-data-firehose-developer-guide/blob/master/doc_source/record-format-conversion.md","poster":"BigEv","timestamp":"1632857220.0","comment_id":"43962"},{"timestamp":"1632403260.0","comments":[{"content":"It's the other way around. Firehouses stores data; data streams does not.","upvote_count":"1","poster":"cloud_trail","comment_id":"278720","timestamp":"1636166940.0"}],"upvote_count":"3","content":"I would go with B. Kinesis data streams stores data, while Firehose not.","poster":"rsimham","comment_id":"28407"}],"unix_timestamp":1575945000},{"id":"VkyCWasOgjYfBHS6rJXw","timestamp":"2022-04-24 01:44:00","question_text":"A data scientist has explored and sanitized a dataset in preparation for the modeling phase of a supervised learning task. The statistical dispersion can vary widely between features, sometimes by several orders of magnitude. Before moving on to the modeling phase, the data scientist wants to ensure that the prediction performance on the production data is as accurate as possible.\nWhich sequence of steps should the data scientist take to meet these requirements?","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/74274-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"content":"Selected Answer: C\nC would be my answer here. Rescaling each set independently could lead to strange skews. Training set, Test set and Evaluation set should be on the same scale","comment_id":"590830","poster":"cron0001","comments":[{"content":"You're right. test set and val set should be rescaled on the same scale.\nBut the scale value should be extracted by only statistical value from training data.\nI think C means that the rescaling stage is affected by the values from the whole data (with val, test set)\nSo, I think B is correct","upvote_count":"9","poster":"GiyeonShin","timestamp":"1671669420.0","comment_id":"752899"}],"upvote_count":"17","timestamp":"1650757440.0"},{"poster":"masoa3b","content":"Selected Answer: B\nhttps://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data\n\nC also leads to data leakage. You are using the test data to scale everything. So part of the data in the test set is used to scale for when you build the model on the training and check against the validation set.","comment_id":"704953","upvote_count":"17","timestamp":"1666810740.0"},{"timestamp":"1723562940.0","content":"Selected Answer: B\nIf you Rescale all the data first you are going to do data leakage by showing all the variance of data with in training. The rescaling needs to be after splitting the data and not before it","comment_id":"1265226","upvote_count":"1","poster":"ML_2"},{"comment_id":"1154691","upvote_count":"5","timestamp":"1708431240.0","poster":"Denise123","content":"Selected Answer: B\nThe best practice is --> to split the dataset into training, validation, and test sets first, and then rescale the training set and apply the SAME scaling to the validation and test sets. This ensures that the scaling parameters (e.g., mean and standard deviation for standardization or min and max values for min-max scaling) are calculated only based on the training set to prevent data leakage and maintain the integrity of the evaluation process.\n\nBy following this approach, you prevent information from the validation and test sets from influencing the scaling parameters, which could lead to data leakage and overestimation of model performance. Keeping the scaling consistent across all subsets ensures a fair evaluation of the model's generalization performance on new, unseen data."},{"timestamp":"1704761640.0","comment_id":"1117131","content":"Answer is B.\nThe other options have shortcomings:\n\nA: Random sampling is a good practice, but it doesn't address the issue of feature scaling. Also, rescaling should occur after splitting the data.\nC: Rescaling the entire dataset before splitting could lead to data leakage, where information from the validation/test sets inadvertently influences the training process.\nD: Rescaling the sets independently would lead to inconsistencies in scale across the training, validation, and test sets, which could negatively impact model performance and evaluation.","poster":"phdykd","upvote_count":"2"},{"poster":"Sukhi4fornet","content":"OPTION C. Rescale the dataset. Then split the dataset into training, validation, and test sets.\n\nExplanation:\n\nRescaling the dataset:\n\nThis is the first step to address the varying statistical dispersion among features. By rescaling, you ensure that all features are on a similar scale, which is important for many machine learning algorithms.\nSplitting into training, validation, and test sets:\n\nAfter rescaling, the dataset is split into training, validation, and test sets. This ensures that the model is trained on one set, validated on another set, and tested on a third set. This separation helps evaluate the model's performance on unseen data.\nOption C ensures that the rescaling is applied before splitting the data, ensuring consistency in the scaling across different sets. This approach prevents data leakage and provides a more accurate representation of how the model will perform on new, unseen data.","comment_id":"1099302","upvote_count":"1","timestamp":"1702853220.0"},{"content":"Selected Answer: B\nValidation and test set should be scaled as per parameters used for scaling of training set. Independent scaling of test set would mean that drift of model in production will be way quicker and is not recommended in data science","comment_id":"1087020","upvote_count":"1","timestamp":"1701627240.0","poster":"akgarg00"},{"comment_id":"1067031","poster":"elvin_ml_qayiran25091992razor","content":"Selected Answer: B\nB is correct, scale on train and apply the others. prevent to data leakage","upvote_count":"1","timestamp":"1699602600.0"},{"timestamp":"1699378620.0","comment_id":"1065036","poster":"akgarg00","upvote_count":"1","content":"Selected Answer: B\nAnswer B, C is not a good data science practise."},{"upvote_count":"1","poster":"DimLam","content":"Selected Answer: B\nWe need firstly split the data to avoid data leakage from test/eval sets, then rescale data in all sets using statistics from training set","comment_id":"1054308","timestamp":"1698298980.0"},{"comment_id":"1005687","poster":"DavidRou","timestamp":"1694518260.0","upvote_count":"2","content":"Selected Answer: B\nI think the right answer here is B. We need to split the dataset into Training, Validation and Test set. Then we can only scale (by using some technique) data contained in the Training set. Data that belong to Validation and Test set must be scaled by using the parameters used on the training.\n\nFor example, if we want to apply a standardization, we can do that only on the Training set as we should not be allowed to use mean and standard deviation computed on Validation/Test set. We must act as we don't own those data!"},{"upvote_count":"1","poster":"Mickey321","comment_id":"992249","content":"Selected Answer: B\noption B","timestamp":"1693230480.0"},{"comment_id":"969345","content":"Data Science 101:\n(A) Given the question, doesn't solve the magnitude problem.\n(B) Correct\n(C) Data Leakage\n(D) It's not correct, still data leakage.","poster":"kaike_reis","upvote_count":"1","timestamp":"1690919040.0"},{"upvote_count":"1","poster":"gusta_dantas","content":"Tricky question, but, D, definitely! \n\nB: You can't apply the same scaling to the validation and test sets 'cause you may suffer data leakage! \nC: You shouldn't rescale the whole dataset then split into training, validation and test, it's not a good practice and may suffer data leakage as well. \n\nD: You're first splitting the whole dataset and applying rescaling individually, preventing any data leakage and each set is rescaled based in your own statistics.","comments":[{"content":"Theoretically, you should not have Test set data at Training time (when you're doing the scaling), so how do you think to do that?\nWhat if you will not have an entire Test set, but you will receive each new row at a time?","poster":"DavidRou","timestamp":"1694518380.0","comment_id":"1005688","upvote_count":"1"},{"upvote_count":"1","comment_id":"969346","timestamp":"1690919160.0","content":"but you are leaking information from validation samples between themselves.","poster":"kaike_reis"}],"timestamp":"1690106580.0","comment_id":"960316"},{"comment_id":"908454","timestamp":"1685264160.0","upvote_count":"3","content":"Selected Answer: B\nFrom Bing chat (and it makes complete sense)\n\"Based on the search results, I think the best sequence of steps for the data scientist to take is B. Split the dataset into training, validation, and test sets. Then rescale the training set and apply the same scaling to the validation and test sets.\n\nThis sequence of steps ensures that the data scientist can evaluate the model performance on different subsets of data that have not been used for training or tuning. It also ensures that the data scientist can rescale the features to have a common scale without introducing any data leakage from the validation or test sets. Rescaling the features can help improve the accuracy of some machine learning algorithms that are sensitive to the magnitude or distribution of the data, such as distance-based methods or gradient-based methods 1.","poster":"JK1977"},{"content":"Selected Answer: B\nYou want to measure how the model performs on new data. Scaling with the test set is a no-no.","timestamp":"1684940580.0","upvote_count":"1","comment_id":"905990","poster":"tommct"},{"upvote_count":"1","comment_id":"889890","poster":"GOSD","content":"B or D, I dont understand the semantics of \"independently\" and the effect it would have. It's most def not done before because of data leakage. https://www.linkedin.com/pulse/feature-scaling-dataset-spliting-arnab-mukherjee/","timestamp":"1683274200.0"},{"comment_id":"882842","upvote_count":"1","content":"C definitely, as rescaling after split leads to data snooping.","poster":"kukreti18","timestamp":"1682614440.0"},{"content":"for the sake of consistency, it makes most sense to perform rescaling, e.g. min-max scaling or standardisation, _before_ splitting the data into training, validation and test sets","poster":"tbomez","comment_id":"849363","upvote_count":"1","timestamp":"1679667120.0"},{"content":"Selected Answer: C\nIt is essential to rescale the data before splitting it into training, validation, and test sets to avoid data leakage. Rescaling the data after splitting can lead to the validation and test sets being influenced by information from the training set. Therefore, it is important to rescale the data before splitting it into different sets.","poster":"oso0348","timestamp":"1678379700.0","comment_id":"834181","upvote_count":"1"},{"timestamp":"1677879420.0","comment_id":"828437","poster":"AjoseO","upvote_count":"4","content":"Selected Answer: B\nThis is because rescaling the entire dataset before splitting it can lead to data leakage, where information from the validation or test sets can influence the scaling of the training set. Rescaling only the training set and then applying the same scaling to the validation and test sets ensures that the model is trained on data that is scaled in the same way as the production data."},{"upvote_count":"7","content":"Selected Answer: B\nIt is 100% B here, scaling should only be based on the training data ALONE. The testing data should be transformed by the same scaler based on the training data. \nC would introduce information from the testing data in the process of scaling the training data, therefore introducing information leakage, and that can lead to the model picking up information from the test data.","timestamp":"1677036600.0","poster":"Siyuan_Zhu","comment_id":"817433"},{"content":"Selected Answer: C\nAnswer is C. What have people been doing? I have always been instructed, and seen scaling preformed before splitting the data. You must do that, if you scale the 3 groups independently then you allow some bias to skew the data (especially if the validation and test sets are small). Always normalize/ scale data before any splitting!","comments":[{"poster":"Tomatoteacher","comment_id":"777802","content":"As even though Option B would prevent data leakage, but it's not the best option to ensure the best performance on unseen data.","upvote_count":"1","timestamp":"1673880780.0"}],"comment_id":"777794","poster":"Tomatoteacher","upvote_count":"3","timestamp":"1673880420.0"},{"upvote_count":"3","comment_id":"752897","poster":"GiyeonShin","timestamp":"1671669120.0","content":"Selected Answer: B\nB\nWhen a machine learns features of data, the machine should consider only \"training data\", not val and test. So, It should be eliminated the mean, var or some statistical values from val, and test data.\nBecause we don't know the real statistical value of real data."},{"upvote_count":"1","comment_id":"749310","timestamp":"1671408240.0","poster":"hamimelon","content":"You should rescale the data first, so that all the data are on the same scale, and then split them."},{"content":"Selected Answer: B\nI was going to suggest D, but ... you should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables.\n\nThis would make sense as you are making a model and then in the real world it works on \"something\"... you only have the trained model and have to apply what you have learned to the real word.","upvote_count":"1","timestamp":"1668670980.0","comment_id":"720304","poster":"Jeremy1"},{"comment_id":"694461","upvote_count":"3","poster":"shailendra1508","content":"B is the correct Answer.","timestamp":"1665716760.0"},{"timestamp":"1663802580.0","poster":"31Rishab","comment_id":"675578","content":"Selected Answer: B\nhttps://datascience.stackexchange.com/questions/39932/feature-scaling-both-training-and-test-data","upvote_count":"2"},{"timestamp":"1656119580.0","poster":"ovokpus","comment_id":"621920","content":"Selected Answer: B\nhttps://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data\n\nAND\n\nhttps://stats.stackexchange.com/questions/375165/scaling-separately-in-train-and-test-set\n\ntell the same story.\n\nSplit the data\nFit the scaler on the train set and use the same fit to transform train, val and test sets.","upvote_count":"5"},{"poster":"[Removed]","timestamp":"1655315700.0","content":"C is not correct. B is correct","comment_id":"616892","upvote_count":"4"},{"comment_id":"615659","timestamp":"1655101140.0","content":"Selected Answer: B\nB, details see here\nhttps://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data","poster":"yc1005","upvote_count":"5"},{"comment_id":"611297","content":"D is correct","timestamp":"1654306980.0","poster":"tgaos","upvote_count":"1"},{"comment_id":"597383","poster":"edvardo","content":"Selected Answer: B\nB. With C you introduce data leakage","timestamp":"1651767120.0","upvote_count":"6","comments":[{"upvote_count":"1","poster":"zaige123","comment_id":"600682","content":"should not include test data set","timestamp":"1652367420.0"}]},{"upvote_count":"2","poster":"bluer1","content":"B for me","comment_id":"593391","timestamp":"1651091880.0"}],"choices":{"C":"Rescale the dataset. Then split the dataset into training, validation, and test sets.","A":"Apply random sampling to the dataset. Then split the dataset into training, validation, and test sets.","B":"Split the dataset into training, validation, and test sets. Then rescale the training set and apply the same scaling to the validation and test sets.","D":"Split the dataset into training, validation, and test sets. Then rescale the training set, the validation set, and the test set independently."},"isMC":true,"answer_description":"","answer_images":[],"answers_community":["B (76%)","C (24%)"],"question_images":[],"unix_timestamp":1650757440,"exam_id":26,"question_id":325,"answer_ET":"B","topic":"1"}],"exam":{"isBeta":false,"id":26,"provider":"Amazon","lastUpdated":"11 Apr 2025","numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty","isImplemented":true,"isMCOnly":false},"currentPage":65},"__N_SSP":true}