{"pageProps":{"questions":[{"id":"r1hNzpBxZEc1ibOcJpU7","topic":"1","timestamp":"2020-01-12 14:54:00","question_text":"A Machine Learning Specialist is creating a new natural language processing application that processes a dataset comprised of 1 million sentences. The aim is to then run Word2Vec to generate embeddings of the sentences and enable different types of predictions.\nHere is an example from the dataset:\n\"The quck BROWN FOX jumps over the lazy dog.`\nWhich of the following are the operations the Specialist needs to perform to correctly sanitize and prepare the data in a repeatable manner? (Choose three.)","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/11820-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"choices":{"C":"Remove stop words using an English stopword dictionary.","B":"Normalize all words by making the sentence lowercase.","D":"Correct the typography on \"quck\" to \"quick.ג€","E":"One-hot encode all words in the sentence.","A":"Perform part-of-speech tagging and keep the action verb and the nouns only.","F":"Tokenize the sentence into words."},"exam_id":26,"answer":"BCF","answer_ET":"BCF","answers_community":["BCF (71%)","BDF (29%)"],"question_id":186,"discussion":[{"timestamp":"1663922160.0","upvote_count":"35","poster":"ozan11","comment_id":"40990","content":"B C F should be correct."},{"poster":"BigEv","comment_id":"42153","timestamp":"1664248020.0","content":"I will select B, C, F\n1- Apply words stemming and lemmatization\n2- Remove Stop words\n3- Tokensize the sentences\n\nhttps://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925","upvote_count":"26"},{"upvote_count":"1","comment_id":"1388213","timestamp":"1741850220.0","poster":"Togy","content":"Selected Answer: BDF\nB. Normalize all words by making the sentence lowercase:\n\nWord2Vec treats words as distinct entities. If you don't convert everything to lowercase, \"The\" and \"the\" will be considered different words, which is generally not what you want. Lowercasing ensures consistency.\nD. Correct the typography on \"quck\" to \"quick\":\n\nMisspellings need to be corrected. Word2Vec learns embeddings based on the words it encounters. If \"quck\" remains, it will be treated as a separate word from \"quick,\" and you'll lose the relationship between them. Correcting typos is crucial for data quality.\nF. Tokenize the sentence into words:\n\nTokenization is the process of breaking down the sentence into individual words (or tokens). Word2Vec operates on individual words, so you need to split the sentence into its constituent parts. This is a fundamental step in NLP."},{"upvote_count":"1","comment_id":"1358030","timestamp":"1739835120.0","content":"Selected Answer: BDF\nWhile C - is debatable - not always necessary to remove stop words in Word2Vec - as sometimes the stop words do provide context\n====================\n\nFor Word2Vec training, data preprocessing is essential to ensure that words are correctly represented, consistent, and free from unnecessary noise. The key steps are:\n\nLowercasing the text (B)\n\nWord embeddings treat \"FOX\" and \"fox\" as different words. To avoid redundancy, lowercasing the text ensures consistency.\nCorrecting typos (D)\n\n\"quck\" should be corrected to \"quick\" to prevent incorrect word representations in Word2Vec. Misspelled words can create meaningless embeddings.\nTokenizing the sentence into words (F)\n\nWord2Vec operates at the word level, so breaking the sentence into individual tokens (words) is necessary.","poster":"JonSno"},{"upvote_count":"1","timestamp":"1726221360.0","poster":"loict","comment_id":"1006446","content":"Selected Answer: BCF\nA. NO - word2vec works on raw data\nB. YES - case here is not significant\nC. YES - will help reduce dimensionality\nD. NO - word2vec will do it by itself\nE. NO - One-hot encoding is for classification\nF. YES - word2vec takes tokens as input"},{"comment_id":"832700","upvote_count":"2","content":"Selected Answer: BCF\nData need to be tokenized and cleaned!","poster":"Valcilio","timestamp":"1709886720.0"},{"upvote_count":"2","content":"Selected Answer: BCF\nB, C F is the correct","timestamp":"1703859780.0","comment_id":"761112","poster":"Aninina"},{"timestamp":"1667391480.0","upvote_count":"2","comment_id":"296949","content":"BCF correct. D is not correct (Pay attention to “in a repeatable manner” in the question.)","poster":"SophieSu"},{"upvote_count":"2","timestamp":"1667376660.0","comment_id":"278191","content":"B/C/F. D should not be performed because spell check is a subjective thing. You don't know for sure what the word was supposed to be if you have a typo.","poster":"cloud_trail"},{"content":"I saw this exact question on \"whizlabs\" practice exam and correct options were B/C/F","comment_id":"260718","poster":"harmanbirstudy","upvote_count":"1","timestamp":"1666743960.0"},{"comment_id":"149708","content":"https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281\nData Preparation — Define corpus, clean, normalise and tokenise words\nTo begin, we start with the following corpus:\n “natural language processing and machine learning is fun and exciting”\nFor simplicity, we have chosen a sentence without punctuation and capitalization. Also, we did not remove stop words “and” and “is”.\nIn reality, text data are unstructured and can be “dirty”. Cleaning them will involve steps such as \no removing stop words, \no removing punctuations, \no convert text to lowercase (actually depends on your use-case), \no replacing digits, etc.\no After preprocessing, we then move on to tokenising the corpus\nAnswer: B, C, F","timestamp":"1666519620.0","poster":"GeeBeeEl","upvote_count":"8","comments":[{"timestamp":"1666771080.0","poster":"cnethers","content":"BCF is 100% correct","comment_id":"273827","upvote_count":"2"}]},{"comment_id":"108382","upvote_count":"2","content":"Correct answers are B, C and F","timestamp":"1666081500.0","poster":"Antriksh"},{"content":"The correct answer is B, C and F\nA: POS tagging has nothing to do with word2vec\nD: fixing \"quck\" to \"quick\" only works for that specific word\nF: word2vec can use CBOW or skipgram, so no need to have one-hot decoding here","comments":[{"comment_id":"85217","content":"sorry E: word2vec can use CBOW or skipgram, so no need to have one-hot decoding here","timestamp":"1665130320.0","poster":"TuanAnh","upvote_count":"4"}],"poster":"TuanAnh","upvote_count":"4","comment_id":"85215","timestamp":"1665090900.0"},{"poster":"PRC","upvote_count":"2","comment_id":"65583","timestamp":"1664972460.0","content":"BCF is correct"},{"comment_id":"58386","content":"B, C F correct","upvote_count":"2","timestamp":"1664723640.0","poster":"AKT"},{"content":"B, C, and F are correct answers. I have done this question many times in many practice tests.","comment_id":"50953","timestamp":"1664333220.0","poster":"Phong","upvote_count":"12"},{"upvote_count":"3","poster":"tap123","content":"B, C, F are my choice. D is also possible but not as widely used as others.","comment_id":"43744","timestamp":"1664324040.0"},{"timestamp":"1663783260.0","poster":"cybe001","comment_id":"38098","content":"Why C is not included in the answer? ABCD, all are correct answers","upvote_count":"1","comments":[{"content":"\"choose three\"","comment_id":"104491","timestamp":"1665833400.0","upvote_count":"1","poster":"halfway"}]}],"answer_images":[],"unix_timestamp":1578837240,"answer_description":""},{"id":"iSwYElsuysSchnKYEoqI","topic":"1","question_id":187,"exam_id":26,"choices":{"A":"Use an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster to ingest event data. Use Amazon Kinesis Data Analytics to transform the most recent 10 minutes of data before inference.","B":"Use Amazon Kinesis Data Streams to ingest event data. Store the data in Amazon S3 by using Amazon Kinesis Data Firehose. Use AWS Lambda to transform the most recent 10 minutes of data before inference.","D":"Use an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster to ingest event data. Use AWS Lambda to transform the most recent 10 minutes of data before inference.","C":"Use Amazon Kinesis Data Streams to ingest event data. Use Amazon Kinesis Data Analytics to transform the most recent 10 minutes of data before inference."},"answer":"C","answers_community":["C (100%)"],"question_text":"A company has a podcast platform that has thousands of users. The company implemented an algorithm to detect low podcast engagement based on a 10-minute running window of user events such as listening to, pausing, and closing the podcast. A machine learning (ML) specialist is designing the ingestion process for these events. The ML specialist needs to transform the data to prepare the data for inference.\n\nHow should the ML specialist design the transformation step to meet these requirements with the LEAST operational effort?","answer_ET":"C","answer_images":[],"question_images":[],"isMC":true,"answer_description":"","unix_timestamp":1688790060,"discussion":[{"comment_id":"1082993","timestamp":"1732832580.0","poster":"endeesa","upvote_count":"1","content":"Selected Answer: C\nEasiest option would be C"},{"comment_id":"1005858","poster":"loict","upvote_count":"1","timestamp":"1726152120.0","content":"Selected Answer: C\nA. NO - Kinesis Data Analytics can use only Firehose or Amazon Kinesis Data Streams as input (https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works.html)\nB. NO - no need to save in S3, can do on-the-fly\nC. YES\nD. NO - AWS Lambda would be invoked on a per-record basis"},{"content":"why not B?","comment_id":"994441","poster":"Pt8442","upvote_count":"2","timestamp":"1725047640.0"},{"upvote_count":"2","content":"Selected Answer: C\nOption C also allows the ML specialist to use Amazon Kinesis Data Analytics to transform the most recent 10 minutes of data before inference. Kinesis Data Analytics is a fully managed service that enables users to analyze streaming data using SQL or Apache Flink. Kinesis Data Analytics can process streaming data in real time and generate insights, metrics, and alerts. Kinesis Data Analytics can also integrate with other AWS services, such as Lambda, S3, or SageMaker. The ML specialist can use Kinesis Data Analytics to apply SQL queries or Flink applications to transform the event data based on the 10-minute running window and prepare it for inference.","timestamp":"1722799260.0","poster":"Mickey321","comment_id":"972423"},{"content":"Selected Answer: C\nIt’s real-time and less operational overhead","timestamp":"1721874780.0","upvote_count":"1","poster":"awsarchitect5","comment_id":"962254"},{"upvote_count":"1","comment_id":"946158","timestamp":"1720412460.0","content":"I think it's C as we are using only 2 services and it's less operational effort.","poster":"ADVIT"}],"timestamp":"2023-07-08 06:21:00","url":"https://www.examtopics.com/discussions/amazon/view/114482-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"hC3q4kg7R9XeOTBoGdbx","question_images":[],"timestamp":"2023-07-08 06:24:00","unix_timestamp":1688790240,"question_id":188,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/114483-exam-aws-certified-machine-learning-specialty-topic-1/","answers_community":["A (92%)","8%"],"discussion":[{"content":"Selected Answer: A\nOption A allows the ML specialist to add class weights to the MLP’s loss function, and then retrain. Class weights are a way of assigning different importance or penalties to different classes in a classification problem. Class weights can help balance the data distribution and reduce the bias towards the majority classes. Class weights can also help improve the recall metric, which is the ratio of true positives to the sum of true positives and false negatives. Recall measures how well the model can identify the relevant instances of a class, especially when the class is rare or unique. The ML specialist can use class weights to increase the importance or penalty of the target class of interest, and then retrain the MLP to improve its recall.","poster":"Mickey321","upvote_count":"6","timestamp":"1707081840.0","comment_id":"972425"},{"content":"Option A - Add class weights to MLP's loss function - improve recall with the least amount of time and effort by making the model more sensitive to the underrepresented target class during training.","upvote_count":"1","comment_id":"1206769","poster":"JonSno","timestamp":"1730785140.0"},{"content":"Selected Answer: A\nApologies for the confusion but on second thoughts, A is the right answer as unique doesn't mean unknown and this is still a supervised learning problem. Adding weights to classes would even out the bias caused by unique class and improve recall as mentioned by other experts in this forum. Please ignore my previous comment. A is the correct option indeed.","poster":"backbencher2022","comment_id":"1022397","timestamp":"1711984080.0","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: C\nLeaning towards C as the target class of interest is unique as compared to dataset (as given in this question). If the target class is unique / non-existing in data set then we are talking about unsupervised learning and k-means is a right fit so, option C seems to be more appropriate than option A. Adding weights may still not be able to solve the purpose as target class is not present in data set. It is almost an unlabeled data set if target class is unknown / unique as compared to existing classes in data set. Unlabeled data sets are better solved using unsupervised learning.","comment_id":"1022366","poster":"backbencher2022","timestamp":"1711982640.0"},{"upvote_count":"1","content":"Selected Answer: A\nAgreed A","comment_id":"962255","timestamp":"1706157300.0","poster":"awsarchitect5"},{"timestamp":"1704695040.0","poster":"ADVIT","content":"Selected Answer: A\nA as this is Faster solution.","upvote_count":"2","comment_id":"946159"}],"answer":"A","choices":{"A":"Add class weights to the MLP's loss function, and then retrain.","D":"Train an anomaly detection model instead of an MLP.","B":"Gather more data by using Amazon Mechanical Turk, and then retrain.","C":"Train a k-means algorithm instead of an MLP."},"isMC":true,"topic":"1","answer_ET":"A","question_text":"A machine learning (ML) specialist is training a multilayer perceptron (MLP) on a dataset with multiple classes. The target class of interest is unique compared to the other classes in the dataset, but it does not achieve an acceptable recall metric. The ML specialist varies the number and size of the MLP's hidden layers, but the results do not improve significantly.\n\nWhich solution will improve recall in the LEAST amount of time?","answer_images":[],"exam_id":26},{"id":"IX3qu17hjt8bz3ufzXWo","question_id":189,"timestamp":"2023-07-07 12:22:00","answer_images":[],"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/114426-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1688725320,"discussion":[{"content":"Selected Answer: AD\nThe key here is before training the model. If it is before training then we can do that using SageMaker Data Wrangler. \n\nAfter training and model is deployed for inference, we can use model monitor","poster":"vkbajoria","comment_id":"1186481","timestamp":"1727653740.0","upvote_count":"2"},{"comment_id":"1010126","upvote_count":"1","poster":"jopaca1216","content":"A and C are correct.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-reports-ui.html","timestamp":"1710726000.0"},{"comment_id":"1005871","timestamp":"1710262740.0","poster":"loict","content":"Selected Answer: AD\nA. YES - SageMaker Clarify can be used to detect bias during prep (https://aws.amazon.com/sagemaker/clarify/)\nB. NO - SageMaker Ground Truth is not used in the solution\nC. NO - drift report to during inference\nD. YES - Configure SageMaker Data Wrangler to generate a bias report.\nE. NO - SageMaker Experiments is to compare model outputs","upvote_count":"1"},{"comments":[{"poster":"Mickey321","upvote_count":"1","content":"I think the combination of actions that will meet the requirements with the least operational overhead are A and D. Use SageMaker Clarify to automatically detect data bias and configure SageMaker Data Wrangler to generate a bias report.","timestamp":"1708698180.0","comment_id":"988298"}],"content":"Selected Answer: AD\nThis combination meets all the requirements with the least operational overhead. You can use SageMaker Data Wrangler to ingest and clean your data in Amazon SageMaker Studio without writing any code. You can also use SageMaker Clarify to automatically detect potential bias in your data using predefined or custom metrics. You can then configure SageMaker Data Wrangler to generate a bias report that shows the results of the bias analysis in a visual and interactive way2.","timestamp":"1706985780.0","poster":"Mickey321","upvote_count":"1","comment_id":"971319"},{"upvote_count":"1","content":"Selected Answer: AD\nA: AWS Clarify used to generate Bias report. \nD: AWS Data Wrangler to generate Bias report","comment_id":"962259","timestamp":"1706157660.0","poster":"awsarchitect5"},{"timestamp":"1704757920.0","poster":"Richaqua","content":"AD: https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-reports-ui.html","upvote_count":"2","comment_id":"946758","comments":[{"comment_id":"950394","timestamp":"1705133220.0","content":"SageMaker Clarify is integrated with Amazon SageMaker Data Wrangler, which can help you identify bias during data preparation without having to write your own code. Data Wrangler provides an end-to-end solution to import, prepare, transform, featurize, and analyze data with Amazon SageMaker Studio.","poster":"Peng001","upvote_count":"1"}]},{"upvote_count":"1","poster":"ADVIT","timestamp":"1704695220.0","content":"I think it's A+E","comment_id":"946161"},{"comment_id":"945559","poster":"kukreti18","timestamp":"1704630120.0","content":"AC is correct.\nA: AWS Clarify used to generate Bias report.\nC: AWS Data Wrangler to generate Bias report and is operationally efficient. \nhttps://catalog.us-east-1.prod.workshops.aws/workshops/1e224d5a-4273-444a-acec-28d44a5bfb28/en-US/data-preparation/amazon-sagemaker/data-wrangler","upvote_count":"1"}],"answer":"AD","answers_community":["AD (100%)"],"answer_ET":"AD","isMC":true,"question_text":"A machine learning (ML) specialist uploads 5 TB of data to an Amazon SageMaker Studio environment. The ML specialist performs initial data cleansing. Before the ML specialist begins to train a model, the ML specialist needs to create and view an analysis report that details potential bias in the uploaded data.\n\nWhich combination of actions will meet these requirements with the LEAST operational overhead? (Choose two.)","exam_id":26,"question_images":[],"choices":{"C":"Use SageMaker Model Monitor to generate a bias drift report.","E":"Use SageMaker Experiments to perform a data check","B":"Turn on the bias detection option in SageMaker Ground Truth to automatically analyze data features.","D":"Configure SageMaker Data Wrangler to generate a bias report.","A":"Use SageMaker Clarify to automatically detect data bias"}},{"id":"XdqviR4PzfMdAfG7k4uJ","question_id":190,"answer_ET":"C","answer_description":"","choices":{"D":"Use Amazon Kinesis Data Firehose to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using AWS Lambda.","B":"Use Amazon Kinesis Data Firehose to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using a short-lived Amazon EMR cluster.","A":"Use AWS Lambda to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using Amazon Kinesis Data Firehose.","C":"Use Amazon Kinesis Data Analytics to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using Amazon Kinesis Data Firehose."},"discussion":[{"content":"Selected Answer: C\nNot D because Firehose is not best to aggregate, and lambda is not necessary.","poster":"sukye","upvote_count":"1","timestamp":"1732372980.0","comment_id":"1078494"},{"content":"Selected Answer: C\nA. NO - AWS Lambda is not per to aggregate, it works on a per-row basis \nB. NO - Kinesis Data Firehose is not best to aggregate\nC. YES - Amazon Kinesis Data Analytics can aggregate and stream to Firehose\nD. NO - Kinesis Data Firehose is not best to aggregate","timestamp":"1726156920.0","comment_id":"1005894","upvote_count":"3","poster":"loict"},{"comment_id":"1005182","poster":"jyrajan69","upvote_count":"1","timestamp":"1726091580.0","content":"Kinesis Data Firehose can invoke your Lambda function to transform incoming source data and deliver the transformed data to destinations. So the answer cannot be C, has to be D"},{"content":"C it is","timestamp":"1725409920.0","poster":"chet100","upvote_count":"1","comment_id":"998050"},{"upvote_count":"1","content":"Selected Answer: C\nchanging back to C. very confusing","timestamp":"1724935080.0","poster":"Mickey321","comment_id":"993074"},{"upvote_count":"1","content":"Selected Answer: D\nChanging to option D\nOption D is the best option because it allows the network security vendor to use Amazon Kinesis Data Firehose to read and aggregate the data hourly from Amazon Kinesis Data Streams, and use AWS Lambda to transform the data and store it in Amazon S3. This way, the network security vendor can leverage the benefits of both services: Amazon Kinesis Data Firehose can provide a simple and scalable way to ingest, buffer, compress, and batch the streaming data; AWS Lambda can provide a flexible and cost-effective way to perform custom logic on the data, such as selecting only 7 to 12 fields for Athena queries. This option meets the requirements with the least amount of customization to transform and store the ingested data","poster":"Mickey321","comment_id":"992760","timestamp":"1724904600.0"},{"poster":"kaike_reis","content":"Selected Answer: C\nLetter B is wrong, as it brings the addition of yet another new service (EMR). Letter D is wrong, as we cannot directly use KDF to perform transformations (it's a load service only). Letter C is the most correct and fastest, as it uses the Kinesis family. Letter A is functional, as we can call Lambda via KDS, but it would involve more customization given the Lambda code to be built.","comment_id":"986843","timestamp":"1724265480.0","upvote_count":"1"},{"comment_id":"972437","content":"Selected Answer: C\nChanging my answer to C","poster":"Mickey321","upvote_count":"1","timestamp":"1722800340.0"},{"timestamp":"1722702900.0","content":"Selected Answer: D\nThis option meets all the requirements with the least amount of customization. You can use Amazon Kinesis Data Firehose to ingest streaming data from thousands of endpoints and configure it to buffer the data by size or time interval (such as 1 hour). You can use AWS Lambda to transform the data and select only the relevant fields before delivering it to Amazon S3. You can also configure Amazon Kinesis Data Firehose to convert the data to a columnar format such as Apache Parquet or Apache ORC, which are optimized for querying with Amazon Athena3.","upvote_count":"1","comment_id":"971313","poster":"Mickey321","comments":[]},{"timestamp":"1721875500.0","comment_id":"962261","poster":"awsarchitect5","upvote_count":"1","content":"Selected Answer: C\nC it is"},{"timestamp":"1721768160.0","upvote_count":"1","content":"C is correct!","comment_id":"960824","poster":"gusta_dantas"},{"timestamp":"1720413120.0","upvote_count":"1","poster":"ADVIT","comment_id":"946164","content":"Vote for C."},{"poster":"kukreti18","comment_id":"945551","timestamp":"1720346820.0","upvote_count":"2","content":"C is correct."}],"timestamp":"2023-07-07 12:07:00","answers_community":["C (80%)","D (20%)"],"exam_id":26,"answer_images":[],"answer":"C","question_text":"A network security vendor needs to ingest telemetry data from thousands of endpoints that run all over the world. The data is transmitted every 30 seconds in the form of records that contain 50 fields. Each record is up to 1 KB in size. The security vendor uses Amazon Kinesis Data Streams to ingest the data. The vendor requires hourly summaries of the records that Kinesis Data Streams ingests. The vendor will use Amazon Athena to query the records and to generate the summaries. The Athena queries will target 7 to 12 of the available data fields.\n\nWhich solution will meet these requirements with the LEAST amount of customization to transform and store the ingested data?","unix_timestamp":1688724420,"url":"https://www.examtopics.com/discussions/amazon/view/114425-exam-aws-certified-machine-learning-specialty-topic-1/","question_images":[],"isMC":true,"topic":"1"}],"exam":{"isMCOnly":false,"name":"AWS Certified Machine Learning - Specialty","isBeta":false,"id":26,"provider":"Amazon","numberOfQuestions":369,"lastUpdated":"11 Apr 2025","isImplemented":true},"currentPage":38},"__N_SSP":true}