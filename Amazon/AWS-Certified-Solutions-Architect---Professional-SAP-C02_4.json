{"pageProps":{"questions":[{"id":"35GBUzZjIW6dBi0ewD4A","question_text":"A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.\n\nWhile reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several large instance types account for a high proportion of the costs. The solutions architect finds out that the company’s developers are launching new Amazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.\n\nThe solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.\n\nWhich solution will meet these requirements?","discussion":[{"timestamp":"1673796060.0","comments":[{"poster":"masetromain","timestamp":"1673796060.0","upvote_count":"12","content":"A. Creating a desired-instance-type managed rule in AWS Config is not a sufficient solution, as it only identifies when an instance is launched with an unauthorized type, it does not prevent it.\n\nB. Creating a launch template that specifies the instance types that are allowed is not a sufficient solution, because it limits the instances types that can be launched in the EC2 console, but it does not prevent the launch of instances through the AWS SDK, AWS CLI, or other AWS services.\n\nD. Using EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image is not a direct solution to the problem of limiting the instance types that only the developers can launch. It can be useful for creating standardize images for the developers, but it does not provide the necessary control mechanism to limit the instance types.","comment_id":"776730"}],"comment_id":"776729","poster":"masetromain","upvote_count":"15","content":"Selected Answer: C\nThe correct answer is C.\n\nIn this solution, a new IAM policy is created that specifies the allowed instance types. This policy is then attached to an IAM group that contains the IAM accounts for the developers. This will ensure that the developers can only launch instances of the specified types, thus limiting the costs associated with the creation and termination of large instances."},{"poster":"gagol14","upvote_count":"6","comment_id":"1132445","content":"Selected Answer: C\n{\n \"Sid\": \"limitedSize\",\n \"Effect\": \"Deny\",\n \"Action\": \"ec2:RunInstances\",\n \"Resource\": \"arn:aws:ec2:*:*:instance/*\",\n \"Condition\": {\n \"ForAnyValue:StringNotLike\": {\n \"ec2:InstanceType\": [\n \"*.nano\",\n \"*.small\",\n \"*.micro\",\n \"*.medium\"\n ]\n }\n }\n }","timestamp":"1706264940.0"},{"comment_id":"1276471","poster":"amministrazione","upvote_count":"1","content":"C. Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers","timestamp":"1725257820.0"},{"comments":[{"comment_id":"1130032","timestamp":"1706041800.0","poster":"igor12ghsj577","comments":[{"content":"You have IAM users...Not IAM \"accounts\". Bad wording here...","poster":"sse69","comment_id":"1225122","timestamp":"1717644600.0","upvote_count":"2"}],"upvote_count":"1","content":"yes, in IAM group you have user IAM accounts."}],"content":"\"an IAM group that contains the IAM accounts\" ???","timestamp":"1705565460.0","upvote_count":"1","poster":"cox1960","comment_id":"1125618"},{"upvote_count":"1","comment_id":"1103788","content":"Selected Answer: C\nOption C","poster":"career360guru","timestamp":"1703289360.0"},{"poster":"NikkyDicky","comment_id":"941362","timestamp":"1688348820.0","content":"Selected Answer: C\nIts a C","upvote_count":"1"},{"timestamp":"1687398720.0","content":"Selected Answer: C\nThe only technical achievable choices are A and C. However A will only identify the issue and will not prevent it. Even if we set up a remediation rule to terminate the instances immediately - that will cause more issues for the developers and unclear signals that something is wrong with the testing. So A remains the only possible option.","comments":[{"comment_id":"935590","upvote_count":"1","timestamp":"1687883160.0","content":"C is the correct solution remained. Typo mistake in the comments.","poster":"Parimal1983"}],"poster":"Maria2023","comment_id":"930063","upvote_count":"2"},{"upvote_count":"1","comment_id":"926232","content":"c-c-c-c-c-cc-c-c-cc-c-c-c-c-cc-","timestamp":"1687032240.0","poster":"easytoo"},{"comment_id":"851712","poster":"mfsec","content":"Selected Answer: C\nIAM policy..","upvote_count":"1","timestamp":"1679892540.0"},{"timestamp":"1675120800.0","comment_id":"793443","upvote_count":"3","content":"Selected Answer: C\nanswer is C","poster":"zozza2023"}],"isMC":true,"question_id":16,"question_images":[],"answers_community":["C (100%)"],"topic":"1","choices":{"A":"Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched.","B":"In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers’ IAM accounts.","D":"Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image.","C":"Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers"},"answer_ET":"C","answer":"C","timestamp":"2023-01-15 16:21:00","unix_timestamp":1673796060,"url":"https://www.examtopics.com/discussions/amazon/view/95448-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","exam_id":33,"answer_images":[]},{"id":"SWzZGgZRbTX2MV6kR7Af","answer_ET":"ABE","question_id":17,"exam_id":33,"timestamp":"2023-01-15 16:33:00","isMC":true,"topic":"1","discussion":[{"comments":[{"content":"If there are no organizations used, D can be used to prevent EC2 run instances too,\nC is for vulnerabilities checking..F for all security issues consolidated..","upvote_count":"4","comment_id":"831431","poster":"God_Is_Love","timestamp":"1678149240.0"}],"poster":"God_Is_Love","content":"Selected Answer: ABE\nIf config rule is added (A) it can be seen in AWS Config aggregator (E) Using SCP in as aws organization is used here in question. So, A,B,E","comment_id":"831430","upvote_count":"7","timestamp":"1678149180.0"},{"comment_id":"863594","upvote_count":"5","poster":"OCHT","timestamp":"1680851820.0","content":"Selected Answer: ABE\nA. Create an AWS Config rule in each account to find resources with missing tags.\nBy creating an AWS Config rule in each account, you can check if resources are missing tags or have tags that are not conforming to your organization's standards. You can also use AWS Config to automatically remediate non-compliant resources by applying tags. This can help ensure that resources are properly tagged for cost allocation purposes. Here is the AWS Config documentation for creating rules:\nhttps://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html","comments":[{"comment_id":"863596","comments":[{"upvote_count":"2","content":"So what is the point of having A if you have E at an Org level?","comment_id":"1133528","timestamp":"1706376480.0","comments":[{"upvote_count":"3","content":"AWS Config aggregator does not run any rules on its own. Instead, it collects the data from the \"source accounts\" where AWS Config is enabled. \nA to get the list of EC2 instances in each account.\nE to aggregate the lists from all accounts in one place.\nB to disallow creating non-compliant EC2 instances.\nSee https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html.","comment_id":"1218975","timestamp":"1716726360.0","poster":"fartosh"}],"poster":"AWSLord32"}],"upvote_count":"7","poster":"OCHT","timestamp":"1680851880.0","content":"E. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.\nBy creating an AWS Config aggregator, you can collect a list of EC2 instances across multiple accounts in the organization that are missing the required Project tag. This can help you identify instances that need to be tagged properly for cost allocation. Here is the AWS Config documentation for creating aggregators:\nhttps://docs.aws.amazon.com/config/latest/developerguide/config-aggregator.html"},{"content":"B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.\nBy creating a Service Control Policy (SCP) in the organization, you can enforce a deny action for EC2 instances that do not have the required Project tag. This can prevent users from launching instances that are not tagged correctly and ensure that new instances are tagged properly for cost allocation. Here is the AWS Organizations documentation for creating SCPs:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html","timestamp":"1680851820.0","upvote_count":"5","poster":"OCHT","comment_id":"863595"}]},{"comment_id":"1276474","upvote_count":"1","content":"A. Create an AWS Config rule in each account to find resources with missing tags.\nB. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.\nE. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.","poster":"amministrazione","timestamp":"1725257880.0"},{"comment_id":"1175903","upvote_count":"1","content":"Selected Answer: ABE\nABE, SCP + Config + Config Aggregator","poster":"gofavad926","timestamp":"1710687720.0"},{"upvote_count":"3","content":"Selected Answer: BE\nB and E handle the requirements in a centralised manner, giving least operational overhead, without anything needing to be added. The question is plainly wrongly stated. If three options have to be selected, then A is the least absurd one.","comment_id":"1165685","timestamp":"1709562720.0","poster":"Dgix"},{"timestamp":"1708014180.0","poster":"8608f25","comment_id":"1151116","upvote_count":"2","content":"Selected Answer: ABE\nA. Create an AWS Config rule in each account to find resources with missing tags.AWS Config can evaluate the configuration of your AWS resources and identify resources that do not comply with specified requirements, such as missing specific tags. This helps in identifying existing resources with the issue.\n B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.Service Control Policies (SCPs) can enforce permissions across all accounts in an organization. By creating an SCP that denies launching EC2 instances without the required Project tag, you can prevent the problem from occurring in the future at the organization level.\nE. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.An AWS Config aggregator can aggregate compliance data from multiple accounts and regions. This allows for centralized visibility of instances lacking the required tags, making it easier to address and resolve the issue across the entire organization."},{"upvote_count":"1","poster":"AWSLord32","comments":[{"comments":[{"upvote_count":"1","poster":"8608f25","content":"It is not D. Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing. \nIAM policies do not directly support conditional denies based on tag presence during the resource creation process in the same way SCPs do. This enforcement is better handled at the organization level with SCPs.","timestamp":"1708014300.0","comment_id":"1151117"}],"upvote_count":"1","poster":"AWSLord32","content":"I meant E, not D","comment_id":"1133527","timestamp":"1706376420.0"}],"comment_id":"1133526","content":"Selected Answer: BDE\nA is not needed if you have D. Correct answer is BDE.","timestamp":"1706376300.0"},{"upvote_count":"1","comment_id":"1103790","timestamp":"1703289720.0","poster":"career360guru","content":"Selected Answer: ABE\nOption A, B and E"},{"content":"Selected Answer: ABE\nInspector checks for Vulnerabilities but not the tags.","comment_id":"1054281","upvote_count":"3","timestamp":"1698295560.0","poster":"Sandeep_B"},{"content":"Selected Answer: ABE\nits ABE","timestamp":"1688349120.0","poster":"NikkyDicky","upvote_count":"2","comment_id":"941364"},{"timestamp":"1682231520.0","comment_id":"877944","content":"A. AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents. These documents define the actions to be performed on noncompliant AWS resources evaluated by AWS Config Rules. You can associate SSM documents by using AWS Management Console or by using APIs.\n\nAWS Config provides a set of managed automation documents with remediation actions. You can also create and associate custom automation documents with AWS Config rules.\n\nTo apply remediation on noncompliant resources, you can either choose the remediation action you want to associate from a prepopulated list or create your own custom remediation actions using SSM documents. AWS Config provides a recommended list of remediation action in the AWS Management Console.\n\nIn the AWS Management Console, you can either choose to manually or automatically remediate noncompliant resources by associating remediation actions with AWS Config rules. With all remediation actions, you can either choose manual or automatic remediation.","poster":"youngmanaws","upvote_count":"3"},{"upvote_count":"1","poster":"mfsec","content":"Selected Answer: ABE\nABE is the better choice","comment_id":"851715","timestamp":"1679892660.0"},{"comment_id":"842117","timestamp":"1679069640.0","poster":"Damijo","comments":[{"content":"Fully agree, BDE","timestamp":"1706376540.0","poster":"AWSLord32","upvote_count":"1","comments":[{"comment_id":"1133535","timestamp":"1706376780.0","poster":"AWSLord32","content":"Did some research..\n\nAggregators provide a read-only view into the source accounts and regions that the aggregator is authorized to view. Aggregators do not provide mutating access into the source account or region. For example, this means that you cannot deploy rules through an aggregator or pull snapshot files from the source account or region through an aggregator.\nhttps://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html#multi-account-multi-region-data-aggregation\n\nSo ABE seems correct","upvote_count":"2"}],"comment_id":"1133529"}],"upvote_count":"4","content":"what's the value of A and E together- it's either or ? the outcome is the same - thoughts?"},{"upvote_count":"1","timestamp":"1676996400.0","content":"ABE makes sense","comment_id":"816824","poster":"jaysparky"},{"timestamp":"1676288460.0","upvote_count":"1","content":"Selected Answer: ABE\nConfig, SCP and IAM policy may not require in each account but it says to select three options so going with ABE","poster":"spd","comment_id":"807334"},{"timestamp":"1675504620.0","upvote_count":"1","poster":"Musk","content":"Selected Answer: AE\nBE makes sense","comment_id":"797795"},{"poster":"zozza2023","upvote_count":"2","content":"Selected Answer: ABE\nthe best way to deploy config rules accross accounts= SCP","comment_id":"790538","timestamp":"1674903600.0"},{"timestamp":"1674744300.0","content":"Selected Answer: ABE\nIn adding tag, the keywords are config, scp, aggreagator.","upvote_count":"2","poster":"masssa","comment_id":"788814"},{"content":"Selected Answer: ABE\nA and E are correct. But the below is the best way to deploy config rules accross accounts.\nhttps://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html\nhttps://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\nB is correct.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html","timestamp":"1674628140.0","upvote_count":"2","poster":"Untamables","comment_id":"787324"},{"content":"Selected Answer: ABE\nABE for me\n\nD I am sure it is not, it would be too much trouble putting the policy in each account","upvote_count":"1","poster":"ccort","timestamp":"1674592620.0","comment_id":"786939"},{"comment_id":"777204","poster":"zhangyu20000","upvote_count":"1","content":"BDE are correct","timestamp":"1673828580.0"},{"upvote_count":"2","poster":"masetromain","comments":[{"timestamp":"1673796840.0","comment_id":"776744","upvote_count":"1","content":"The other options, A and C are not appropriate for this scenario, because they would only identify the instances that are missing the tag, but not prevent the problem from happening again. Using option F is also not appropriate for this scenario, because AWS Security Hub is not used for cost allocation.","poster":"masetromain"},{"content":"stop asking to ChatGPT. The right answer is ABE. ChatGPT forgot to list the missing tags on the existing instances","timestamp":"1694306220.0","poster":"Arnaud92","comment_id":"1003575","upvote_count":"1"}],"content":"Selected Answer: BDE\nThe correct answer is BDE.\n\nB: Creating an SCP (Service Control Policy) in the organization with a deny action for ec2:RunInstances if the Project tag is missing will prevent developers from launching instances without the necessary tag. This is a good option because it will prevent the problem from happening again in the future.\n\nD: Creating an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing will also prevent developers from launching instances without the necessary tag. This is a good option because it will prevent the problem from happening again in the future.\n\nE: Creating an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag will help the team identify which instances are missing the tag, so they can take action to add the tag. This is a good option because it will help resolve the problem that has already happened and also help the team identify any instances that are not compliant with the company's tagging policy.","comment_id":"776742","timestamp":"1673796780.0"}],"answer":"ABE","url":"https://www.examtopics.com/discussions/amazon/view/95451-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1673796780,"choices":{"C":"Use Amazon Inspector in the organization to find resources with missing tags.","B":"Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.","A":"Create an AWS Config rule in each account to find resources with missing tags.","E":"Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.","F":"Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag.","D":"Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing."},"answer_description":"","question_images":[],"answer_images":[],"answers_community":["ABE (81%)","Other"],"question_text":"A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.\n\nWhich actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)"},{"id":"ZxdadwTVkW9sxrnfvDSY","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95454-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage.\n\nThe company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:\n• Managed AWS services to minimize operational complexity.\n• A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\n• A visualization tool to create dashboards to observe events in near-real time.\n• Support for semi-structured JSON data and dynamic schemas.\n\nWhich combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)","exam_id":33,"answer_images":[],"choices":{"D":"Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.","B":"Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.","C":"Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.","E":"Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.","A":"Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events."},"unix_timestamp":1673797080,"discussion":[{"poster":"God_Is_Love","content":"Selected Answer: AD\nAmazon Kinesis Data Firehose (A) allows you to buffer events in two ways: through buffering size or buffering time. With buffering size, you can configure the maximum size of the buffer in MB or the maximum number of records in the buffer. Once the buffer is full, it will automatically deliver the data to the destination\n\nAmazon ES (D) has its ability to receive events from various sources in real-time. Amazon ES can ingest data from a variety of sources, such as Amazon Kinesis Data Firehose, Amazon CloudWatch Logs, and Amazon S3, making it a powerful tool for organizations looking to analyze and visualize real-time streaming data. (Kibana dashboards)","timestamp":"1678159560.0","upvote_count":"14","comment_id":"831526"},{"upvote_count":"12","timestamp":"1680852420.0","content":"Selected Answer: AD\nOption B includes using an Amazon Kinesis data stream to buffer events, which is a valid solution for a streaming data use case. However, it requires more ongoing administration compared to using Amazon Kinesis Data Firehose, which is a fully managed service. Additionally, the use of Amazon Kinesis Data Firehose allows the company to take advantage of built-in data transformation and processing capabilities, which can reduce the amount of code required to implement the solution. Therefore, I selected option A over option B as it better meets the requirement of minimizing operational complexity.","comment_id":"863606","poster":"OCHT"},{"upvote_count":"1","poster":"Paul123456789","timestamp":"1743522960.0","comment_id":"1418566","content":"Selected Answer: BD\nAWS Lambda is a source for Amazon Kinesis Data Firehose not a destination\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-name.html\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-destination.html\nalso, Firehose encountered timeout errors when calling AWS Lambda. The maximum supported function timeout is 5 minutes\nhttps://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\ncorrect answer B and D"},{"poster":"albert_kuo","timestamp":"1741066800.0","upvote_count":"2","comment_id":"1364741","content":"Selected Answer: BD\nOption B (Kinesis Data Streams + Lambda) + Option D (Amazon ES + Kibana):\nBuffer: Kinesis Data Streams automatically scales and buffers events.\nProcessing: Lambda transforms JSON events and sends them to Amazon ES.\nStorage: Amazon ES stores semi-structured JSON with dynamic schemas.\nVisualization: Kibana provides near-real-time dashboards.\nManaged: All services (Kinesis, Lambda, ES, Kibana) are fully managed.\nWorkflow: Events flow from on-premises via VPN to Kinesis → Lambda → ES → Kibana.\nResult: Meets all requirements seamlessly."},{"comment_id":"1341612","content":"Selected Answer: BD\nWhile most voted AD, I vote BD. I picked B instead of A because you can not use lambda to access the kinesis firehose directly.","timestamp":"1737021120.0","upvote_count":"2","poster":"GabrielShiao","comments":[{"timestamp":"1741066620.0","content":"Agreeed","comment_id":"1364740","upvote_count":"1","poster":"albert_kuo"}]},{"upvote_count":"1","timestamp":"1725257940.0","comment_id":"1276476","content":"A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.\nD. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.","poster":"amministrazione"},{"content":"Selected Answer: AD\n\"A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\" \nI think buffer, here, means a solution that will reliably hold information for further successful processing. I don't think it means to buffer and batch process the events so I don't agree with other people's comments in regards to buffer. \n\nThat said, my concern is with \"automatically scales to match the throughput of data\". Firehose does it automatically. Kinesis can also do automatically if on-demand mode is chosen. \n\nAlso, \"Support for semi-structured JSON data and dynamic schemas.\" Dynamic Schemas? Firehose or Data stream don't do that. Firehose does do dynamic partitioning and JSON deserializing. I guess that's what the question meant?","timestamp":"1714938300.0","comment_id":"1207051","upvote_count":"1","poster":"Smart"},{"content":"Selected Answer: AD\nOption A NOT Option B - Amazon Data Firehose buffers incoming streaming data in memory to a certain size (buffering size) and for a certain period of time (buffering interval) before delivering it to the specified destinations. \n\nhttps://docs.aws.amazon.com/firehose/latest/dev/buffering-hints.html","upvote_count":"1","timestamp":"1712241180.0","comment_id":"1189402","poster":"TonytheTiger"},{"upvote_count":"1","poster":"Dgix","comment_id":"1171522","timestamp":"1710230940.0","content":"Selected Answer: AD\nOn second thought: A because B requires manual shard configuration."},{"timestamp":"1710230760.0","content":"Selected Answer: BE\nAlso, Streams is more real-time.","comment_id":"1171520","poster":"Dgix","comments":[{"upvote_count":"1","content":"It says \"Near Real Time\" not \"Real Time\" so Firehouse is the better option between the 2","timestamp":"1728111960.0","poster":"AWSum1","comment_id":"1293374"}],"upvote_count":"1"},{"upvote_count":"2","poster":"Dgix","comment_id":"1171519","content":"Selected Answer: BD\nB rather than A because B integrates the lambda functionality for transformation of the data, which must be done as an added step in A, thereby increasing operational overhead.","timestamp":"1710230640.0"},{"poster":"8608f25","upvote_count":"2","comments":[{"timestamp":"1708017900.0","comment_id":"1151153","content":"D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards. Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, secure, operate, and scale Elasticsearch to search, analyze, and visualize data in real-time. Kibana is an open-source visualization tool designed to work with Elasticsearch, providing powerful and easy-to-use features to create dashboards that can visualize data in near-real-time.","poster":"8608f25","upvote_count":"1"}],"content":"Selected Answer: AD\nA. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.Amazon Kinesis Data Firehose provides a fully managed service for effortlessly loading streaming data into AWS services such as Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk. It scales automatically to match the throughput of data and requires no ongoing administration. AWS Lambda can be used in conjunction with Kinesis Data Firehose to process and transform the data before it’s loaded into the destination, supporting dynamic schemas and semi-structured JSON data. Additionally, Amazon Kinesis Data Firehose has built-in buffering capabilities and can be used to observe events in near-real time, making it a more appropriate choice for the given scenario.","comment_id":"1151152","timestamp":"1708017900.0"},{"content":"ElasticSearch is the ex name of new OpenSearch","upvote_count":"2","comment_id":"1136806","poster":"AimarLeo","timestamp":"1706709060.0"},{"poster":"ninomfr64","timestamp":"1705673520.0","upvote_count":"1","comment_id":"1126726","content":"Selected Answer: BD\nI choose Data Stream (KDS) over Data Firehose (KDF) in this scenario:\n- KDS allows to you store events up to 1 year, allowing to achieve buffering with no constraints on size and with a very large time limit. KDS support on-demand capacity mode\n- KDF transport mechanism is based on buffering, but here buffering is limited on size (max 128MiB) and time (up to 900 sec)"},{"poster":"career360guru","content":"Selected Answer: AD\nA and D","comment_id":"1103793","upvote_count":"1","timestamp":"1703289960.0"},{"poster":"AMohanty","comment_id":"999055","comments":[{"content":"but about \"• Managed AWS services to minimize operational complexity.\"\ni believe Kinesis Firehose is managed solution whereas DataStream required operational overhead","comment_id":"1005702","upvote_count":"1","timestamp":"1694519460.0","poster":"chikorita"}],"upvote_count":"2","content":"BD\nQuestion states near-Real time \nThats the differentiating factor between Kinesis data stream and Firehose\n\nI would go for B and D","timestamp":"1693889100.0"},{"content":"Selected Answer: AD\nAD for unstructured data","comment_id":"941368","poster":"NikkyDicky","timestamp":"1688349300.0","upvote_count":"1"},{"content":"Selected Answer: AD\nAD is my vote","timestamp":"1679892780.0","comment_id":"851718","poster":"mfsec","upvote_count":"1"},{"comment_id":"829807","upvote_count":"1","content":"A,D seem correct. https://www.examtopics.com/discussions/amazon/view/47625-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"1678009860.0","poster":"Zek"},{"comment_id":"777207","timestamp":"1673828640.0","poster":"zhangyu20000","upvote_count":"1","content":"AD are correct"},{"comment_id":"776749","timestamp":"1673797080.0","comments":[{"content":"Option B: Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events. is incorrect because Kinesis Data Stream is a different service than Kinesis Data Firehose and does not have the buffer feature.\n\nOption C: Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards. is incorrect because Amazon Aurora is a relational database service and does not support JSON data or dynamic schemas.\n\nOption E: Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards. is incorrect because Amazon Neptune is a graph database service and does not support JSON data or dynamic schemas.","poster":"masetromain","upvote_count":"4","comments":[{"comments":[{"comment_id":"1115372","content":"What does it mean? You choose Kinesis data stream over Forehose?","poster":"jpa8300","upvote_count":"1","timestamp":"1704565740.0"}],"poster":"Sarutobi","upvote_count":"4","comment_id":"826986","timestamp":"1677766860.0","content":"We use the Kinesis data stream specifically for its capability to store data \"aka buffer events\". Firehouse also has some resemblance of this feature but is more of a transportation service."}],"timestamp":"1673797080.0","comment_id":"776750"}],"content":"Selected Answer: AD\nThe combination of components that will enable the company to create a monitoring solution that will satisfy these requirements is:\n\nA. Use Amazon Kinesis Data Firehose to buffer events. This service can automatically scale to match the throughput of data, and it requires no ongoing administration. With Firehose, it's possible to use a Lambda function to process and transform events as well as to store them in other services like S3 or Redshift.\n\nD. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. With Amazon Elasticsearch Service, it's possible to create an index for the events, making them searchable and queryable. This service is a fully managed service so it minimizes operational complexity. Also, it's possible to use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.","upvote_count":"7","poster":"masetromain"}],"answer_ET":"AD","question_id":18,"isMC":true,"answer":"AD","topic":"1","answers_community":["AD (82%)","Other"],"timestamp":"2023-01-15 16:38:00","answer_description":""},{"id":"cJcfdqeYti17O5RfEm41","url":"https://www.examtopics.com/discussions/amazon/view/95455-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"D","exam_id":33,"timestamp":"2023-01-15 16:42:00","question_text":"A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company’s applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets.\n\nA solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category.\n\nWhat should the solutions architect do to meet these requirements?","choices":{"C":"Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic.","A":"Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs.","B":"Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.","D":"Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications."},"answer_images":[],"question_images":[],"topic":"1","answers_community":["D (86%)","14%"],"answer_description":"","discussion":[{"upvote_count":"14","poster":"God_Is_Love","content":"Selected Answer: D\nVPC endpoints to mitigate NAT gateway huge data transfer costs especially in Kinesis usecase where large data is passed thru\n\nWith a VPC endpoint policy, you can define rules to control access to the VPC endpoint. You can specify the source IP address or IP address range that is allowed to access the endpoint, as well as the type of traffic that is allowed, such as HTTP, HTTPS, or custom TCP ports. You can also specify the resources that can be accessed through the VPC endpoint, such as an Amazon S3 bucket or an Amazon DynamoDB table.","timestamp":"1678161480.0","comment_id":"831540"},{"content":"Selected Answer: D\nB is a distractor. You don't need IAM permissions to use a service via an endpoint. You only need to set up proper routing to that endpoint","comment_id":"930077","upvote_count":"10","timestamp":"1687400040.0","poster":"Maria2023"},{"comment_id":"1346944","content":"Selected Answer: B\nThe correct answer is B.\nRationale:\n\nInterface VPC endpoints for Kinesis eliminate NAT gateway traffic costs\nApplications in private subnets can access Kinesis through the VPC endpoint\nIAM permissions are the proper security control\nMaintains functionality while reducing costs\n\nOther options issues:\nA/C: Flow logs analysis won't reduce NAT costs\nD: Similar to B but focuses on endpoint policy instead of IAM permissions","upvote_count":"2","poster":"kylix75","timestamp":"1737900720.0"},{"content":"Selected Answer: D\nWhy Option D is a Better Fit Here\nCost Reduction Goal: The scenario is primarily about reducing NAT gateway costs by using a VPC endpoint. A properly configured VPC endpoint policy ensures applications can connect to Kinesis through the private endpoint without hitting NAT gateways.\nIAM Permissions Likely Already Exist: If the applications are already interacting with Kinesis, their IAM permissions should already be in place. The focus, therefore, shifts to configuring the new VPC endpoint properly.\nEndpoint Policy Completeness: VPC endpoint policies act as a resource-based policy at the network level, which is critical for ensuring that applications can route their traffic correctly through the VPC endpoint.","comment_id":"1325430","timestamp":"1733984640.0","upvote_count":"1","poster":"Heman31in"},{"timestamp":"1731964800.0","upvote_count":"1","poster":"youonebe","comment_id":"1314233","content":"Answer is B.\n\nOption D is incorrect.\nWhile similar to B, focuses on endpoint policy instead of IAM permissions\nVPC endpoint policies alone are insufficient\nIAM permissions are crucial for application access"},{"content":"Selected Answer: B\nAccess Permissions are still required for most AWS services, including Kinesis Data Streams, even when accessed via a VPC endpoint. The endpoint allows traffic to the service, but your application or users still need IAM permissions to interact with the service. Without proper IAM permissions, even if the routing is set up correctly, the service will not authorize actions like reading from or writing to a Kinesis stream.","upvote_count":"2","comment_id":"1287205","timestamp":"1726904760.0","poster":"Syre"},{"content":"D. Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications.","poster":"amministrazione","timestamp":"1725258000.0","upvote_count":"1","comment_id":"1276479"},{"comment_id":"1202567","timestamp":"1714131780.0","poster":"red_panda","content":"Selected Answer: D\nD without any doubt.","upvote_count":"1"},{"timestamp":"1710687900.0","poster":"gofavad926","content":"Selected Answer: D\nD, VPC endpoint","upvote_count":"2","comment_id":"1175905"},{"poster":"gofavad926","upvote_count":"1","timestamp":"1710687840.0","comment_id":"1175904","content":"Selected Answer: D\nD, VPC endpoint"},{"poster":"career360guru","upvote_count":"1","comment_id":"1103794","content":"Selected Answer: D\nOption D","timestamp":"1703290200.0"},{"timestamp":"1697858280.0","content":"Answer is D. \nAn endpoint policy is a resource-based policy that you attach to a VPC endpoint to control which AWS principals can use the endpoint to access an AWS service.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html","comment_id":"1049234","upvote_count":"1","poster":"rlf"},{"timestamp":"1689214560.0","content":"Selected Answer: D\nIt's a d","comment_id":"950274","upvote_count":"1","poster":"NikkyDicky"},{"poster":"SkyZeroZx","upvote_count":"3","content":"Selected Answer: D\nreduce cost == interface VPC endpoint","comments":[{"upvote_count":"1","timestamp":"1687114080.0","content":"A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category.","poster":"SkyZeroZx","comment_id":"926861"}],"timestamp":"1687114080.0","comment_id":"926860"},{"upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1684453800.0","poster":"romiao106","content":"No. in your document it says \"By default, users and roles don't have permission to create or modify AWS PrivateLink resources\". Users and roles don't have permissions so they do need permissions to use an interface endpoint","comment_id":"901565"}],"content":"Selected Answer: D\nD is the answer.\n\nIt's not B because user's/applications doesn't need permissions to use an endpoint: https://docs.aws.amazon.com/vpc/latest/privatelink/security_iam_id-based-policy-examples.html","timestamp":"1681826820.0","comment_id":"873700","poster":"Anonymous9999"},{"timestamp":"1679892840.0","poster":"mfsec","comment_id":"851721","upvote_count":"1","content":"Selected Answer: D\nD is the best choice."},{"poster":"Sarutobi","timestamp":"1677767580.0","content":"If this is a cost-saving question is very hard to answer, you pay for both, and depending on the region one can be cheaper than the other. There is a cost for a NAT GW and also for a VPCendpoint per AZ plus the traffic you generate over them. In my experience, because you need a VPCendpoint for each service NAT-GW is cheaper.","comment_id":"827001","comments":[{"poster":"fartosh","upvote_count":"1","content":"I agree that both NAT GW and interface VPC endpoints can become expensive. I believe that's why the question mentioned that most applications use KDS. I assume that it's the biggest middleware service and you will not need VPC endpoints for other services.\n\nPricing (based on Ohio):\nNAT GW: 0.045 $/h + 0.045 $/GB\nInterface VPC Endpoint: 0.01 $/h + 0.01 $/GB (lowered if more data transferred)\n\nIn the final setup the company will still pay for NAT GW (hourly fee) but the transfer cost (most of it) will be moved to VPCE, which gives:\nfor 1 GB per month\nNAT GW: (24*30)h*0.045$/h + 1GB*0.045$/GB = 32.445$ > (24*30)h*0.01$/h + 1GB*0.01$/GB = 7.21$\nfor 1000GB per month\nNAT GW: (24*30)h*0.045$/h + 1000GB*0.045$/GB = 77.4$ > (24*30)h*0.01$/h + 1000GB*0.01$/GB = 17.2$","timestamp":"1716728220.0","comment_id":"1218986"}],"upvote_count":"1"},{"upvote_count":"3","poster":"c73bf38","content":"Selected Answer: D\nAllowing traffic from the application using the VPC endpoint is key to bypassing NAT Gateway.","comment_id":"818741","timestamp":"1677122820.0"},{"comment_id":"805702","timestamp":"1676152200.0","upvote_count":"1","content":"Selected Answer: D\nWhich is which?\n\nA VPC endpoint policy is an IAM resource policy that you attach to a VPC endpoint. It determines which principals can use the VPC endpoint to access the endpoint service. The default VPC endpoint policy allows all actions by all principals on all resources over the VPC endpoint. https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html#vpc-endpoints-policies","poster":"moota"},{"content":"Selected Answer: B\nB seems correct too.","upvote_count":"3","timestamp":"1675507740.0","poster":"Musk","comment_id":"797843"},{"comment_id":"777209","poster":"zhangyu20000","upvote_count":"1","timestamp":"1673828760.0","content":"D: by pass internet to save cost on NAT GW"},{"timestamp":"1673797320.0","comment_id":"776753","poster":"masetromain","content":"Selected Answer: D\nThe correct answer is D. Adding an interface VPC endpoint for Kinesis Data Streams to the VPC will allow the applications to access the service without the need for a NAT gateway. This will reduce the cost associated with NatGateway-Bytes charges, which are increasing the cost in the EC2-Other category.\n\nOption A is not correct because enabling VPC Flow Logs and reviewing the logs for traffic that can be removed is not a direct solution for reducing NatGateway-Bytes charges. Additionally, security groups are used to control access to resources, not to optimize network traffic.","upvote_count":"3","comments":[{"poster":"masetromain","content":"Option B is not correct because it does not address the specific issue of high NatGateway-Bytes charges. Additionally, ensuring that applications have the correct IAM permissions is a best practice but it is not directly related to reducing costs.\n\nOption C is not correct because while reviewing Detective findings for traffic that is not related to Kinesis Data Streams can help identify potential issues, it does not directly address the issue of high NatGateway-Bytes charges. Additionally, Configuring security groups to block that traffic is not a solution for reducing costs associated with NatGateway-Bytes charges.","timestamp":"1673797320.0","upvote_count":"3","comment_id":"776754"}]}],"question_id":19,"answer_ET":"D","unix_timestamp":1673797320,"isMC":true},{"id":"5xsIFn1MLOwrsDJAKyt8","answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/95457-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.","A":"Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.","C":"Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connect gateway to route traffic between the transit gateways.","B":"Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-8 connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing."},"discussion":[{"poster":"God_Is_Love","comment_id":"831563","comments":[{"content":"Whichever option has this text is correct - \"Peer the transit gateways with each other to support cross-Region routing\"","poster":"God_Is_Love","timestamp":"1678165200.0","comment_id":"831566","upvote_count":"5"}],"timestamp":"1678164720.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png\nB is wrong as it says, two DX Gateways contradictory\nC is wrong as it says to configure DXG to route traffic. infact Transit gateway peering need to be done between two transit gateways of each reigon.\nA is wrong because Private VIF is not apt in mentioned config of the question. Public VIF is correct (Transit public VIF)\nIf you are using a single DX Gateway","upvote_count":"16"},{"poster":"Syre","comment_id":"1287207","content":"Selected Answer: C\nWhile transit gateway peering can enable cross-Region VPC communication, it is not necessary when you are using a Direct Connect gateway. A Direct Connect gateway already provides the capability to route traffic across multiple Regions without needing to peer the transit gateways directly.","upvote_count":"2","timestamp":"1726905120.0"},{"content":"D. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.","comment_id":"1276481","timestamp":"1725258240.0","poster":"amministrazione","upvote_count":"1"},{"comment_id":"1201726","poster":"teo2157","timestamp":"1714015140.0","upvote_count":"1","content":"It can be both A or D based on AWS documentation: https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/hybrid-network-connections.html"},{"upvote_count":"1","content":"Selected Answer: D\nDon't let \"No single points of failure can exist on the network\" mislead you into thinking that you need two DCGWs. DCGWs are not part of the region they connect to. Therefore, no SPOF translates to a double DC connection to a single DCGW. Hence, D.","timestamp":"1711042440.0","comment_id":"1179468","poster":"Dgix"},{"timestamp":"1710688080.0","upvote_count":"1","content":"Selected Answer: D\nD, this approach ensures high availability and robust network connectivity across the specified AWS regions and the on-premises data center.","comment_id":"1175908","poster":"gofavad926"},{"upvote_count":"3","content":"Answer D - As per from AWS \nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-more-than-3.html","poster":"_Jassybanga_","comment_id":"1159278","timestamp":"1708908600.0"},{"poster":"career360guru","comment_id":"1103798","timestamp":"1703292180.0","upvote_count":"1","content":"Selected Answer: D\nChoice is between C and D. Better the two D is the right option."},{"poster":"subbupro","comments":[{"timestamp":"1732698720.0","upvote_count":"1","comment_id":"1318536","poster":"mnsait","content":"This is the best answer I found for this question. Thank you @subbupro for the reference. It explains exactly what is needed to understand here."}],"content":"D is correct ref architecture https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html","comment_id":"1090030","upvote_count":"4","timestamp":"1701934740.0"},{"poster":"shaaam80","upvote_count":"1","comment_id":"1088102","timestamp":"1701736980.0","content":"Selected Answer: D\nAnswer D. Peer the transit gateways for cross-region routing."},{"upvote_count":"1","poster":"severlight","comment_id":"1071225","content":"Selected Answer: D\nto connect to transit gateways through the dx gateway you should use transit VIF","timestamp":"1700037000.0"},{"content":"I agree 'D' is a good answer to the problem, but isn't the DXGW a single point of failure?\n\nQuestion says \"No single points of failure can exist on the network.\"","upvote_count":"2","timestamp":"1693832280.0","comment_id":"998556","poster":"frfavoreto"},{"timestamp":"1688350080.0","content":"Selected Answer: D\nit's D","poster":"NikkyDicky","upvote_count":"1","comment_id":"941371"},{"poster":"happystrawberry","timestamp":"1684882020.0","content":"Would it be C for the answer? A Direct Connect gateway supports communication between attached transit virtual interfaces and associated transit gateways only and may enable a virtual private gateway to another virtual private gateway. https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-transit-gateways.html","comment_id":"905294","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"905305","content":"Actually, D is a proper answer.","poster":"happystrawberry","timestamp":"1684882320.0"}]},{"comment_id":"899721","comments":[{"timestamp":"1684295700.0","content":"Refer to the following article\nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html","comment_id":"899722","poster":"rbm2023","upvote_count":"3"}],"upvote_count":"4","timestamp":"1684295640.0","content":"Selected Answer: D\nI agree with option D \nRefer to the diagram below which explains in detail the use of Transit VIF and Public VIF. Also demonstrates the necessity for peering the transit gateways to allow the cross-region routing. \nhttps://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png\nThe only options that are using the cross-region routing are A and D. Option A mentions the use of Private VIF and not the Transit VIF. Hence A is incorrect.","poster":"rbm2023"},{"comment_id":"865414","upvote_count":"3","timestamp":"1681035600.0","poster":"dev112233xx","content":"Selected Answer: D\nTransit VIF required to connect to Transit Gateway, and Transit peering is required to connect multi regions... \nHere is the full diagram:\nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html"},{"content":"Selected Answer: D\nD is the answer","upvote_count":"2","comment_id":"851722","timestamp":"1679893020.0","poster":"mfsec"},{"upvote_count":"2","poster":"zejou1","timestamp":"1679207040.0","content":"Selected Answer: D\nThis model is constructed of the following:\n• Multi AWS Regions\n• Dual Direct Connect connections to independent DX locations\n• Single on-premises data center with dual connections to AWS\n• AWS DXGW with AWS Transit Gateway\n• High scale of VPCs per Region\n\nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html","comment_id":"843478"},{"upvote_count":"2","timestamp":"1677768120.0","content":"Selected Answer: D\nYeah, a single DX-GW tied to TGW on different regions that further connect to the VPCs on those regions.","comment_id":"827010","poster":"Sarutobi"},{"poster":"Yowie351","timestamp":"1677036420.0","comment_id":"817429","upvote_count":"1","content":"Selected Answer: B\nMultiple dynamically routed AWS Direct Connect connections are necessary to support high availability.\nRefer to the second diagram:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html"},{"timestamp":"1676417700.0","poster":"spd","comment_id":"808944","upvote_count":"1","content":"Selected Answer: D\nD Seems Correct"},{"timestamp":"1675324680.0","content":"Selected Answer: D\nD: \nhttps://aws.amazon.com/blogs/networking-and-content-delivery/aws-transit-gateway-now-supports-intra-region-peering/","upvote_count":"1","comment_id":"795867","poster":"jojom19980","comments":[{"content":"and this for connect two transit gateways with one direct connect gateway :\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html","comment_id":"795869","timestamp":"1675324860.0","upvote_count":"1","poster":"jojom19980"},{"upvote_count":"1","poster":"Musk","comment_id":"798178","timestamp":"1675529640.0","content":"Maybe you meant this one: https://aws.amazon.com/blogs/aws/new-for-aws-transit-gateway-build-global-networks-and-centralize-monitoring-using-network-manager/"}]},{"poster":"zozza2023","comment_id":"793448","content":"Selected Answer: D\nanswer is D","timestamp":"1675121580.0","upvote_count":"1"},{"timestamp":"1674633600.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/virtualgateways.html\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/high_resiliency.html\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html","comment_id":"787378","upvote_count":"1","poster":"Untamables"},{"content":"D use DX GW for multi region to on-premise, direct TGW peer for cross regions","poster":"zhangyu20000","timestamp":"1673828880.0","comment_id":"777212","upvote_count":"1"},{"content":"Selected Answer: D\nhttps://www.examtopics.com/discussions/amazon/view/91771-exam-aws-certified-solutions-architect-professional-topic-1/\n\nThe correct answer is D.\n\nIn this solution, two transit VIFs are created - one from the DX-A connection and one from the DX-B connection - into the same Direct Connect gateway for high availability. Both the eu-west-1 and us-east-1 transit gateways are then associated with this Direct Connect gateway. The transit gateways are then peered with each other to support cross-Region routing.\n\nThis solution meets the requirements of the company by creating a highly available connection between the on-premises data center and the VPCs in both the eu-west-1 and us-east-1 regions, and by enabling direct traffic routing between VPCs in those regions.","comments":[{"upvote_count":"4","poster":"masetromain","timestamp":"1673797980.0","content":"Option A is incorrect because a private VIF does not support inter-VPC traffic and cross-Region routing.\n\nOption B is incorrect because it separates the two Direct Connect connections into separate Direct Connect gateways, which would not provide high availability.\n\nOption C is incorrect because it does not mention how to peer the transit gateways to support cross-Region routing.","comments":[{"content":"I think the reason why B is wrong is because there is no need to have 2 Direct Connect Gateways. DX-GW is a global object, separating the regions with 2 DX-GW only creates fragmentation of the routing, which can be good in some cases.","comments":[{"comment_id":"878019","content":"Seems 1 DX-GW can connect to up to 3 Transit-GW, if you have more than 3 Transit-GW, you can use 2 DX-GW.","upvote_count":"2","timestamp":"1682236020.0","poster":"youngmanaws"}],"comment_id":"827009","timestamp":"1677768000.0","upvote_count":"2","poster":"Sarutobi"}],"comment_id":"776761"}],"timestamp":"1673797920.0","comment_id":"776760","poster":"masetromain","upvote_count":"2"}],"exam_id":33,"answer":"D","isMC":true,"unix_timestamp":1673797920,"answer_images":[],"question_images":[],"timestamp":"2023-01-15 16:52:00","question_id":20,"answer_ET":"D","answers_community":["D (91%)","7%"],"question_text":"A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.\n\nThe company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is configured to route all inter-VPC traffic within that Region.\n\nWhich solution will meet these requirements?"}],"exam":{"provider":"Amazon","isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isMCOnly":true,"id":33,"lastUpdated":"11 Apr 2025","isImplemented":true,"numberOfQuestions":529},"currentPage":4},"__N_SSP":true}