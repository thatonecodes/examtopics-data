{"pageProps":{"questions":[{"id":"rjq9JZKc4AGN19hS4dH2","question_images":[],"isMC":true,"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/16836-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"answer_images":[],"answer_ET":"B","question_id":531,"question_text":"A company has an application that uses Amazon EC2 instances in an Auto Scaling group. The Quality Assurance (QA) department needs to launch a large number of short-lived environments to test the application. The application environments are currently launched by the Manager of the department using an AWS\nCloudFormation template. To launch the stack, the Manager uses a role with permission to use CloudFormation, EC2, and Auto Scaling APIs. The Manager wants to allow testers to launch their own environments, but does not want to grant broad permissions to each user.\nWhich set up would achieve these goals?","topic":"1","answer_description":"Reference:\nhttps://aws.amazon.com/ru/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/","choices":{"A":"Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to assume the Manager's role and add a policy that restricts the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console.","C":"Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to use CloudFormation and S3 APIs, with conditions that restrict the permission to the template and the resources it creates. Train users to launch the template from the CloudFormation console.","B":"Create an AWS Service Catalog product from the environment template. Add a launch constraint to the product with the existing role. Give users in the QA department permission to use AWS Service Catalog APIs only. Train users to launch the templates from the AWS Service Catalog console.","D":"Create an AWS Elastic Beanstalk application from the environment template. Give users in the QA department permission to use Elastic Beanstalk permissions only. Train users to launch Elastic Beanstalk environment with the Elastic Beanstalk CLI, passing the existing role to the environment as a service role."},"answers_community":["B (100%)"],"timestamp":"2020-03-17 08:36:00","discussion":[{"comment_id":"65077","timestamp":"1632096540.0","poster":"jay1ram2","content":"A? Grant QA users access to Manager Role is a blatant violation of Security.\n\nB makes more sense to me as it restricts users to create services through the catalog.","upvote_count":"26"},{"content":"Selected Answer: B\nA? Grant QA users access to Manager Role is a blatant violation of Security.\n\nB makes more sense to me as it restricts users to create services through the catalog.","upvote_count":"1","comment_id":"931102","poster":"SkyZeroZx","timestamp":"1687474140.0"},{"upvote_count":"1","timestamp":"1685907060.0","content":"At first I chose C. \nI guess C is wrong because tester can still modify CloudFormation template to access more resources. \nSo B is right.","comment_id":"914895","poster":"Jesuisleon"},{"timestamp":"1665767940.0","content":"Selected Answer: B\nI'll go with B!!","upvote_count":"1","poster":"Blair77","comment_id":"694931"},{"comment_id":"494751","content":"Service Catalog is right option. B for sure right answer.","timestamp":"1638749820.0","upvote_count":"4","poster":"AzureDP900"},{"poster":"denccc","content":"It's B, not sure what other discussion is going on below.","timestamp":"1636260060.0","comments":[{"poster":"AzureDP900","content":"Below discussions doesn't seems related to this question, I am not sure what they are taking :)","comment_id":"494752","upvote_count":"5","timestamp":"1638749880.0"}],"upvote_count":"3","comment_id":"430104"},{"timestamp":"1636254900.0","poster":"WhyIronMan","content":"I'll go with B","comment_id":"411688","upvote_count":"3"},{"timestamp":"1636247520.0","poster":"chuck_lee","comment_id":"336220","comments":[{"comment_id":"455159","content":"D.\nLambdas are stateless and can't rely on connection pool. To get over this problem, AWS provide RDS proxy for connection pool management.","upvote_count":"1","poster":"joe16","timestamp":"1636279980.0"}],"upvote_count":"1","content":"needs to improve the scalable performance and availability of the database.\nWhich solution meets these requirements?\n\nA. Create an Amazon CloudWatch alarm action that triggers a Lambda function to add an Amazon RDS for MySQL read replica when resource utilization hits a threshold\nB. Migrate the database to Amazon Aurora, and add a read replica Add a database connection pool outside of the Lambda handler function\nC. Migrate the database to Amazon Aurora, and add a read replica Use Amazon Route 53 weighted records\nD. Migrate the database to Amazon Aurora, and add an Aurora Replica Configure Amazon RDS Proxy to manage database connection pools"},{"poster":"chuck_lee","comment_id":"336219","timestamp":"1636239780.0","upvote_count":"1","content":"A company runs a software-as-a-service (SaaS) application on AWS. The application consists of AWS Lambda functions and an Amazon RDS for MySQL Multi-AZ database. During market events the application has a much higher workload than normal Users notice slow response times during the peak periods because of many database connections. The company needs"},{"content":"Answer is B.","timestamp":"1636182900.0","upvote_count":"2","poster":"Bulti","comment_id":"253044"},{"poster":"T14102020","comment_id":"243988","upvote_count":"1","content":"Correct is B. Service Catalog product","timestamp":"1636177020.0"},{"comment_id":"231607","content":"I'll go with B","timestamp":"1636133760.0","upvote_count":"4","poster":"jackdryan"},{"upvote_count":"1","comment_id":"228546","content":"B makes sense as AWS Service Catalog is created for a use-case like this.","timestamp":"1636098720.0","poster":"bbnbnuyh"},{"poster":"CYL","content":"B, using service catalog to show what are the allowed services will be the easiest way to approach the restrictions.","timestamp":"1636056120.0","upvote_count":"1","comment_id":"209722"},{"content":"A. Verify the AWS IoT Device Shadow service is subscribed to the appropriate topic and is executing the AWS Lambda function.\nB. Verify that AWS IoT monitoring shows that the appropriate AWS IoT rules are being executed, and that the AWS IoT rules are enabled with the correct rule actions.\nC. Check the AWS IoT Fleet indexing service and verify that the thing group has the appropriate IAM role to update DynamoDB.\nD. Verify that AWS IoT things are using MQTT instead of MQTT over WebScocket, then check that the provisioning has the appropriate policy attached.","upvote_count":"1","timestamp":"1635677460.0","comments":[{"timestamp":"1635841020.0","upvote_count":"1","content":"D is correct","comments":[{"upvote_count":"1","comment_id":"181384","comments":[{"content":"Answer is B: IoT Rules. https://docs.aws.amazon.com/iot/latest/developerguide/iot-rules.html","upvote_count":"1","timestamp":"1646480520.0","poster":"lifebegins","comment_id":"561402"}],"poster":"Fua","content":"explain please","timestamp":"1635869520.0"}],"poster":"Phat","comment_id":"179346"}],"comment_id":"160580","poster":"JBRIAN"},{"comment_id":"160579","timestamp":"1635673140.0","poster":"JBRIAN","content":"NO.80 An IoT company has rolled out a fleet of sensors for monitoring temperatures in remote locations. Each device connect to AWS IoT Core and sends a message 30 seconds, updating an Amazon DynamoDB table. A System Administrator users AWS IoT to verify the devices are still sending messages to AWS IoT Core: the database is not updating.\nWhat should a Solution Architect check to determine why the database is not being updated?","upvote_count":"1"},{"upvote_count":"2","comment_id":"160576","comments":[{"upvote_count":"1","poster":"SadioMane","timestamp":"1635814140.0","content":"Answer is ABD","comment_id":"171376","comments":[{"comment_id":"171377","upvote_count":"1","content":"Sorry. The answer is meant for Q #63","poster":"SadioMane","timestamp":"1635817500.0"}]},{"timestamp":"1636047600.0","poster":"Nit_1","content":"What is the ans for Q77","comment_id":"206629","upvote_count":"1"}],"timestamp":"1635661800.0","content":"A. Store the data in Amazon DocumentDB Create a single global Amazon CloudFront distribution with a custom origin built on edge-optimized Amazon API Gateway and AWS Lambda Assign the company's domain as an alternate domain for the distribution. and configure Amazon Route 53 with an alias to the CloudFront distribution\nB. Store the data in replicated Amazon S3 buckets in two Regions Create an Amazon CloudFront distribution in each Region, with custom origins built on Amazon API Gateway and AWS Lambda launched in each Region Assign the company's domain as an alternate domain for both distributions and configure Amazon Route 53 with a failover routing policy between them\nC. Store the data in an Amazon DynamoDB global table in two Regions using on-demand capacity mode In both Regions, run the web service as Amazon ECS Fargate tasks in an Auto Scaling ECS service behind an Application Load Balancer (ALB) In Amazon Route 53, configure an alias record in the company's domain and a Route 53 latency-based routing policy with health checks to distribute traffic between the two ALBs","poster":"JBRIAN"}],"unix_timestamp":1584430560},{"id":"IkRnRWoZscJAhrJKDXwZ","topic":"1","answer":"AD","isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/13998-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"answer_ET":"AD","exam_id":32,"discussion":[{"comment_id":"49910","content":"Yes. A and D","poster":"BillyC","timestamp":"1632184560.0","upvote_count":"16"},{"timestamp":"1632953580.0","poster":"noisonnoiton","content":"go with A,D\nImplement sharding to distribute load to multiple RDS MySQL instances (this is only a read contention, the writes work fine)\nIncrease the RDS MySQL Instance size and Implement provisioned IOPS (not scalable, this is only a read contention, the writes work fine)","comment_id":"131147","upvote_count":"6"},{"timestamp":"1723751760.0","upvote_count":"1","poster":"amministrazione","content":"A. Deploy ElastiCache in-memory cache running in each availability zone\nD. Add an RDS MySQL read replica in each availability zone","comment_id":"1266652"},{"poster":"SkyZeroZx","timestamp":"1687254480.0","comment_id":"927762","content":"AD\nbest option for read intensive or read problems in app with use RDS \nkeyword in question = After comprehensive tests you discover that there is read contention","upvote_count":"1"},{"content":"Selected Answer: AD\nYes. A and D","timestamp":"1687195800.0","poster":"SkyZeroZx","upvote_count":"1","comment_id":"927763"},{"upvote_count":"1","timestamp":"1684505760.0","content":"AD is correct answer. Both read replica and cache will decrease load on primary MySQL","comment_id":"902005","poster":"rtguru"},{"upvote_count":"1","content":"Dont understand why not B and C?\nthe question is about high writes and reads, how could D meet the requiement with only read replica?","comment_id":"711687","poster":"RRRichard","timestamp":"1667645040.0","comments":[{"comment_id":"879976","upvote_count":"1","content":"Because the issue to solve is \"read contention on RDS MySQL\". There is no mention to DB write issues.","poster":"fdpv","timestamp":"1682400840.0"}]},{"comment_id":"527872","upvote_count":"1","timestamp":"1642622400.0","poster":"Ni_yot","content":"Yes A and D."},{"upvote_count":"1","poster":"challenger1","timestamp":"1639193580.0","comment_id":"499069","content":"My Answer: A&D"},{"timestamp":"1638335760.0","poster":"acloudguru","comment_id":"491290","upvote_count":"1","content":"very easy one, hope I can have it in my exam"},{"poster":"01037","content":"A and D","comment_id":"349109","upvote_count":"1","timestamp":"1635727860.0"},{"comment_id":"340760","upvote_count":"1","timestamp":"1635119220.0","poster":"Malcnorth59","content":"A and D is the best answer"},{"upvote_count":"1","content":"A. D.\nto improve read performance.","timestamp":"1634912100.0","poster":"cldy","comment_id":"325518"},{"comment_id":"293838","upvote_count":"1","content":"AD is the right answer","timestamp":"1634022900.0","poster":"bustedd"}],"question_id":532,"unix_timestamp":1581585780,"answer_images":[],"question_text":"Your company is getting ready to do a major public announcement of a social media site on AWS. The website is running on EC2 instances deployed across multiple Availability Zones with a Multi-AZ RDS MySQL Extra Large DB Instance. The site performs a high number of small reads and writes per second and relies on an eventual consistency model. After comprehensive tests you discover that there is read contention on RDS MySQL.\nWhich are the best approaches to meet these requirements? (Choose two.)","answers_community":["AD (100%)"],"timestamp":"2020-02-13 10:23:00","choices":{"D":"Add an RDS MySQL read replica in each availability zone","C":"Increase the RDS MySQL Instance size and Implement provisioned IOPS","A":"Deploy ElastiCache in-memory cache running in each availability zone","B":"Implement sharding to distribute load to multiple RDS MySQL instances"}},{"id":"W4W6Wb9MJ4Rv6ROmoBsj","answers_community":["D (100%)"],"answer_description":"","answer_images":[],"timestamp":"2020-08-10 07:38:00","choices":{"C":"Use an AWS Glue ETL job to copy all the RDS databases to a single Amazon Aurora PostgreSQL database. Run SQL queries on the Aurora PostgreSQL database.","B":"Create an Amazon EMR cluster with enough core nodes. Run an Apache Spark job to copy data from the RDS databases to a Hadoop Distributed File System (HDFS). Use a local Apache Hive metastore to maintain the table definition. Use Spark SQL to run the query.","A":"Create a new Amazon Redshift cluster. Create an AWS Glue ETL job to copy data from the RDS databases to the Amazon Redshift cluster. Use Amazon Redshift to run the query.","D":"Use an AWS Glue crawler to crawl all the databases and create tables in the AWS Glue Data Catalog. Use an AWS Glue ETL job to load data from the RDS databases to Amazon S3, and use Amazon Athena to run the queries."},"isMC":true,"answer":"D","answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/27853-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":533,"question_images":[],"question_text":"A company has several teams, and each team has their own Amazon RDS database that totals 100 TB. The company is building a data query platform for\nBusiness Intelligence Analysts to generate a weekly business report. The new system must run ad-hoc SQL queries.\nWhat is the MOST cost-effective solution?","topic":"1","exam_id":32,"discussion":[{"poster":"Nemer","timestamp":"1632295860.0","content":"D. ad-hoc queries + cost advantage over Redshift -> Athena.","upvote_count":"16","comment_id":"154150"},{"timestamp":"1635496080.0","comment_id":"276797","poster":"Ebi","content":"Cheapest option is D","upvote_count":"6"},{"comment_id":"931101","poster":"SkyZeroZx","upvote_count":"1","content":"Selected Answer: D\nD. ad-hoc queries + cost advantage over Redshift -> Athena.","timestamp":"1687474080.0"},{"comment_id":"636845","upvote_count":"3","poster":"hilft","content":"Badly formed question. each team got 100tb of data set and you still not using Redshift? The right answer is D. because it is asking for the most cost-efficient way of querying.","timestamp":"1658769060.0"},{"poster":"aandc","upvote_count":"1","content":"Selected Answer: D\nkeyword: ad-hoc SQL queries, cost -> Athena","comment_id":"626436","timestamp":"1656827580.0"},{"timestamp":"1643186220.0","content":"Selected Answer: D\ncorrect answer is D","comment_id":"532704","poster":"shotty1","upvote_count":"1"},{"comment_id":"529347","timestamp":"1642795260.0","poster":"CloudChef","content":"Selected Answer: D\nD via Digital Cloud Training","upvote_count":"3"},{"upvote_count":"2","comment_id":"527454","timestamp":"1642591860.0","poster":"pititcu667","content":"Selected Answer: D\nd because cheap + ad-hoc"},{"comment_id":"494753","upvote_count":"1","content":"I will go with D, This is most cost -effective solution.","poster":"AzureDP900","timestamp":"1638750060.0"},{"upvote_count":"1","timestamp":"1636303980.0","comment_id":"411690","content":"I'll go with D","poster":"WhyIronMan"},{"timestamp":"1635879540.0","poster":"Waiweng","upvote_count":"3","comment_id":"349362","content":"it's D"},{"timestamp":"1635631680.0","comment_id":"347310","poster":"digimaniac","content":"D. Redshift, EMR, and Anthena can all do the job. Read this article.\nhttps://aws.amazon.com/athena/faqs/#When_to_use_Athena_vs_other_big_data_services","upvote_count":"4"},{"poster":"Kian1","timestamp":"1635569880.0","content":"going with D","upvote_count":"2","comment_id":"292212"},{"upvote_count":"1","content":"D. Athena is cheap over Redshift","comment_id":"266158","poster":"rkbala","timestamp":"1635467220.0"},{"timestamp":"1635347220.0","comment_id":"253049","upvote_count":"1","poster":"Bulti","content":"Correct answer is D."},{"poster":"Britts","upvote_count":"3","comments":[{"timestamp":"1635420420.0","comment_id":"253445","content":"\"QUERIES\" !!!! IS THE KEY HERE!! ... This indicates simple things like .. how many blue jumpers were sold etc. Redshift would be more appropriate for DEEP statistical analysis ... such as plotting flight routes .. answer is D","poster":"petebear55","upvote_count":"3"}],"comment_id":"250760","timestamp":"1634832360.0","content":"The question clearly states, that the purpose is to have Business analytics kind of queries. pushes this towards Redshift. i.e. A. Otherwise why would somebody ever need a redshift cluster, if S3 and Athena could have been used"},{"content":"Correct is D. Athena + Glue Crawler","comment_id":"243990","timestamp":"1634831580.0","poster":"T14102020","upvote_count":"1"},{"comment_id":"231610","content":"I'll go with D","timestamp":"1634812620.0","poster":"jackdryan","upvote_count":"3"},{"timestamp":"1634460660.0","upvote_count":"1","comment_id":"227676","content":"When it mentions ad hoc queries and most cost effective look for Athena and s3 .. so answer is D","poster":"petebear55"},{"upvote_count":"1","content":"D. S3 storage cost is lower than the rest of the options. Adhoc SQL are also often associated with Athena usage.","comment_id":"209317","poster":"CYL","timestamp":"1634218860.0"},{"comment_id":"203582","timestamp":"1634183220.0","content":"\"In case any ad-hoc queries need to be run, Athena seems the better choice as it provides ease of accessibility that is absent in Redshift.\"\nhttps://blog.panoply.io/an-amazonian-battle-comparing-athena-and-redshift#:~:text=Athena%20works%20hand%20in%20hand,%240.250%20per%20hour%20is%20charged.","upvote_count":"1","poster":"hyemi"},{"timestamp":"1634058360.0","poster":"Paitan","content":"I will go with option D","upvote_count":"1","comment_id":"199180"},{"comment_id":"179352","content":"B is very expensive, D is cost effective.\nBy the way: https://ujjwalbhardwaj.me/post/migrate-relational-databases-to-amazon-s3-using-aws-glue","upvote_count":"1","poster":"mgat","timestamp":"1633971960.0"},{"timestamp":"1633251360.0","poster":"Kibana01","content":"D, Glue crawler is a key component that can scan data in all kinds of repo, classify it, extract schema info from it & store the metadata automatically in the glu data catalog","comments":[{"timestamp":"1633442880.0","upvote_count":"3","poster":"Carupano","comments":[{"content":"The question talks about ad-hoc queries and cost effective solution. EMR is out of question :-)","upvote_count":"3","poster":"Paitan","comment_id":"199182","timestamp":"1634068560.0"}],"content":"Incorrect, you do not have any clue what you are responding bunch of locos\nhttps://aws.amazon.com/emr/features/spark/ Correct B.","comment_id":"174189"}],"upvote_count":"2","comment_id":"159153"},{"upvote_count":"2","timestamp":"1633001580.0","poster":"Anila_Dhharisi","content":"D is right answer.","comment_id":"156977"},{"content":"My answer is D - Latest AWS tools for this task","timestamp":"1632432420.0","upvote_count":"2","comment_id":"154472","poster":"zeronine"}],"unix_timestamp":1597037880},{"id":"5CJBKlPATcnnULcA4oNl","answer_ET":"D","exam_id":32,"answers_community":["D (80%)","C (20%)"],"answer":"D","answer_description":"","choices":{"D":"Allow users to download solution code artifacts. Use AWS CodeCommit and AWS CodePipeline for the CI/CD pipeline. Use the AWS Cloud Development Kit constructs for different solution features, and use the manifest file to turn features on and off. Use AWS CodeBuild to run unit tests and security scans, and for deploying and updating a solution with changes.","B":"Allow users to download solution code artifacts. Use AWS CodeCommit and AWS CodePipeline for the CI/CD pipeline. Use AWS Amplify plugins for different solution features and user prompts to turn features on and off. Use AWS Lambda to run unit tests and security scans, and AWS CodeBuild for deploying and updating a solution with changes.","C":"Allow users to download solution code artifacts in their Amazon S3 buckets. Use Amazon S3 and AWS CodePipeline for the CI/CD pipelines. Use CloudFormation StackSets for different solution features and to turn features on and off. Use AWS Lambda to run unit tests and security scans, and CloudFormation for deploying and updating a solution with changes.","A":"Allow users to download solution code as Docker images. Use AWS CodeBuild and AWS CodePipeline for the CI/CD pipeline. Use Docker images for different solution features and the AWS CLI to turn features on and off. Use AWS CodeDeploy to run unit tests and security scans, and for deploying and updating a solution with changes."},"discussion":[{"content":"D. AWS CDK enables you to define your infrastructure with code and provision it through AWS CloudFormation. You get all the benefits of CloudFormation, including repeatable deployment, easy rollback, and drift detection.","poster":"bbnbnuyh","comment_id":"211007","timestamp":"1632191940.0","upvote_count":"25"},{"upvote_count":"7","poster":"cloudgc","timestamp":"1632264780.0","comment_id":"231594","comments":[{"poster":"Kelvin1477","timestamp":"1633167540.0","upvote_count":"1","comment_id":"236543","content":"somewhat agree with stacksets use case for providing various flavor of the solution template"},{"poster":"cloudgc","comments":[{"comment_id":"1324969","content":"yes, if they had mentioned CodeDeploy to deploy the solution, D would have been right. Without this, it appears like CodeBuild will deploy making this wrong.","poster":"mnsait","upvote_count":"1","timestamp":"1733912700.0"},{"timestamp":"1634906520.0","poster":"dijesim222","upvote_count":"2","comment_id":"341167","content":"CDK as in answer D can EITHER output cloudformation tempaltes OR deploy the stack immediately (which is totally feasible to do with codebuild). IF the output was a cloudformation template (which was NOT in answer D) it is totally feasible to deploy cloudformation templates with codepipeline alone, no codedeploy etc. is needed. -> anwser D is perfect"}],"timestamp":"1633557540.0","comment_id":"237818","upvote_count":"3","content":"looks like a keyword is missing in Answer-D.\n\nand XXXX for deploying and updating a solution with changes.\n\nif this is true then the answer can be D."}],"content":"Answer-C.\nA - codedeploy - not used for unit tests and security scans\nB - codebuild - not used for deploying and updating\nD - codebuild - not used for deploying and updating"},{"comment_id":"869642","upvote_count":"1","timestamp":"1681406820.0","content":"Selected Answer: C\nB,D AWS CodeBuild can't be used to deploy\nA CodeDeploy can't be used to run tests\nC makes sense\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-stackset-deployment.html","poster":"dev112233xx"},{"comment_id":"632406","poster":"rahulseth","timestamp":"1658015220.0","upvote_count":"1","content":"A. CodeBuild can't be use for AWS run unit tests and security scans, and for deploying and updating a solution with changes."},{"content":"D is correct.\nhttps://docs.aws.amazon.com/solutions/latest/smart-product-solution/components.html","upvote_count":"1","poster":"bobokyo","comment_id":"559958","timestamp":"1646299440.0"},{"timestamp":"1639026780.0","poster":"cldy","upvote_count":"2","comment_id":"497344","content":"D. Allow users to download solution code artifacts. Use AWS CodeCommit and AWS CodePipeline for the CI/CD pipeline. Use the AWS Cloud Development Kit constructs for different solution features, and use the manifest file to turn features on and off. Use AWS CodeBuild to run unit tests and security scans, and for deploying and updating a solution with changes."},{"poster":"AzureDP900","content":"I will go with D.","comment_id":"494754","timestamp":"1638750240.0","upvote_count":"1"},{"upvote_count":"4","comment_id":"484644","timestamp":"1637626620.0","content":"Selected Answer: D\nCorrect is D. CodePipeline + Cloud Development Kits for turn features on and off + CodeBuild to run unit tests","poster":"acloudguru"},{"poster":"WhyIronMan","content":"I'll go with D","comment_id":"411692","timestamp":"1636180920.0","upvote_count":"2"},{"upvote_count":"5","timestamp":"1635921360.0","poster":"Waiweng","content":"it's D","comment_id":"350032"},{"comment_id":"348171","poster":"blackgamer","upvote_count":"3","content":"D seems to be correct.","timestamp":"1635728100.0"},{"timestamp":"1634586540.0","content":"D is correct","upvote_count":"2","poster":"alisyech","comment_id":"321758"},{"content":"going with D","comment_id":"293092","timestamp":"1634526720.0","poster":"Kian1","upvote_count":"3"},{"upvote_count":"1","timestamp":"1634278620.0","comment_id":"290123","content":"Seems there is somthing missing in D. CodeBuild is not intended to deploy","poster":"lechuk"},{"content":"Answer is D","comment_id":"265324","timestamp":"1634107020.0","poster":"Ebi","upvote_count":"6"},{"comment_id":"255265","timestamp":"1633950960.0","poster":"Bulti","content":"D is the correct answer. No other options allows the developers to modify the solution code and deploy it using CodePipeline the way D does.","upvote_count":"2"},{"upvote_count":"4","content":"Correct is D. CodePipeline + Cloud Development Kits for turn features on and off + CodeBuild to run unit tests","timestamp":"1633678620.0","comment_id":"244843","poster":"T14102020"},{"timestamp":"1632485220.0","content":"CodeBuild can build, test, and ofc run scanning job. CDK is a CloudFormation for developers.\nD is correct","comment_id":"234391","upvote_count":"3","poster":"ting_66"},{"comment_id":"233554","content":"seems D","poster":"gookseang","timestamp":"1632389880.0","upvote_count":"1"},{"comment_id":"232556","content":"I'll go with C","poster":"jackdryan","upvote_count":"2","timestamp":"1632291360.0"},{"poster":"pinox1","upvote_count":"3","timestamp":"1632242040.0","content":"D. Use AWS CodeBuild to run unit tests","comment_id":"217934"}],"question_images":[],"isMC":true,"topic":"1","timestamp":"2020-11-02 08:20:00","question_id":534,"question_text":"A company provides AWS solutions to its users with AWS CloudFormation templates. Users launch the templates in their accounts to have different solutions provisioned for them. The users want to improve the deployment strategy for solutions while retaining the ability to do the following:\n✑ Add their own features to a solution for their specific deployments.\n✑ Run unit tests on their changes.\n✑ Turn features on and off for their deployments.\n✑ Automatically update with code changes.\n✑ Run security scanning tools for their deployments.\nWhich strategies should the Solutions Architect use to meet the requirements?","unix_timestamp":1604301600,"url":"https://www.examtopics.com/discussions/amazon/view/35738-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[]},{"id":"mv82OlwHH3R9DqTxU6OL","answers_community":["B (100%)"],"answer_ET":"B","exam_id":32,"answer":"B","answer_description":"","choices":{"D":"In the build account, modify the continuous integration process to perform a lookup of the IAM user credentials from AWS Secrets Manager. In the web account, create a new IAM user. Store the access key and secret access key in Secrets Manager. Attach the PowerUserAccess IAM policy to the IAM user.","B":"In the build account, create a new IAM role, which can be assumed by Amazon EC2 only. Attach the role to the EC2 instance running the continuous integration process. Create an IAM policy to allow s3: PutObject calls on the S3 bucket in the web account. In the web account, create an S3 bucket policy attached to the S3 bucket that allows the newly created IAM role to use s3:PutObject calls.","C":"In the build account, create a new IAM user. Store the access key and secret access key in AWS Secrets Manager. Modify the continuous integration process to perform a lookup of the IAM user credentials from Secrets Manager. Create an IAM policy to allow s3: PutObject calls on the S3 bucket in the web account, and attack it to the user. In the web account, create an S3 bucket policy attached to the S3 bucket that allows the newly created IAM user to use s3:PutObject calls.","A":"In the build account, create a new IAM role, which can be assumed by Amazon EC2 only. Attach the role to the EC2 instance running the continuous integration process. Create an IAM policy to allow s3: PutObject calls on the S3 bucket in the web account. In the web account, create an S3 bucket policy attached to the S3 bucket that allows the build account to use s3:PutObject calls."},"discussion":[{"comments":[{"poster":"joe16","comment_id":"455766","upvote_count":"4","content":"B.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-cross-account-upload-access/","timestamp":"1636056780.0"}],"timestamp":"1632209820.0","poster":"Nemer","comment_id":"154191","content":"B. No long term credentials -> use roles. Bucket policies to grant permissions to the role, not the account itself.","upvote_count":"20"},{"timestamp":"1688313600.0","content":"Selected Answer: B\nB. No long term credentials -> use roles. Bucket policies to grant permissions to the role, not the account itself.\n\nusually cross acount role strategy\n\nA is incorrect by this text \"In the web account, create an S3 bucket policy attached to the S3 bucket that allows the build account to use s3:PutObject calls.\"\n\n*allows the build account to use* role is not for account is for role between accounts","poster":"SkyZeroZx","comment_id":"941039","upvote_count":"1"},{"comment_id":"652938","poster":"gnic","timestamp":"1661691300.0","content":"Selected Answer: B\nIt's B. \"allow new role to use the API putObject\"","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nB BBBBB","poster":"aandc","comment_id":"626464","timestamp":"1656833100.0"},{"timestamp":"1649094180.0","content":"Selected Answer: B\nVote B","poster":"roka_ua","comment_id":"580868","upvote_count":"1"},{"timestamp":"1645297620.0","comment_id":"551260","content":"When DEV account assumes role in PROD account, the s3 only has to allow the role of PROD account to make any changes. 2 way trust is not required in S3 policy - it is at IAM level which should already be taken care.\n\nBBB","upvote_count":"1","poster":"cannottellname"},{"upvote_count":"2","poster":"shotty1","comment_id":"532761","timestamp":"1643190780.0","content":"I am pretty sure it is A. Using a role as a trusted Principal for cross account access has never worked for me, even though the documentation is sometimes a bit vague on that topic."},{"timestamp":"1642618860.0","comment_id":"527835","content":"Selected Answer: B\nroles should be used no?","upvote_count":"1","poster":"pititcu667"},{"content":"It's A. B is just creating a policy, but not a role which can be used by anything. Additionally there should be a two way trust established, but isn't. \nIt's not good to enable the complete build-account to write into the bucket by the bucket policy, but at least this scenario will work and fullfills the requirements. A","comment_id":"507386","poster":"bwestpha","timestamp":"1640207580.0","upvote_count":"1"},{"timestamp":"1638750660.0","content":"I am going with B. Initially I thought of D , however that doesn't make any sense.","upvote_count":"2","poster":"AzureDP900","comment_id":"494758"},{"poster":"moon2351","comment_id":"448502","content":"Answer is B","timestamp":"1635878040.0","upvote_count":"1"},{"upvote_count":"1","content":"I would think it's A, no? Can you allow a remote role in your bucket policy?","poster":"denccc","timestamp":"1635253200.0","comment_id":"422842","comments":[{"timestamp":"1635572460.0","poster":"denccc","comment_id":"422844","content":"Okay it's B: https://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/","upvote_count":"5"}]},{"poster":"WhyIronMan","upvote_count":"3","comment_id":"411747","content":"I'll go with B","timestamp":"1635215160.0"},{"content":"it's B","comment_id":"350034","timestamp":"1635138660.0","poster":"Waiweng","upvote_count":"4"},{"upvote_count":"1","comment_id":"348183","poster":"blackgamer","content":"Going with B","timestamp":"1634971500.0"},{"comments":[{"poster":"sarah_t","upvote_count":"2","comment_id":"333883","content":"Your link says B: \n\"At the end of this tutorial, you have the following:\n- Users in the Development account (the trusted account) that are allowed to assume a specific role in the Production account.\n- A role in the Production account (the trusting account) that is allowed to access a specific Amazon S3 bucket.\n- The productionapp bucket in the Production account.\"","timestamp":"1634511180.0"}],"upvote_count":"1","comment_id":"295537","poster":"certainly","timestamp":"1634462340.0","content":"A. is correct. https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html"},{"comment_id":"292470","upvote_count":"2","content":"going with B","timestamp":"1633482480.0","poster":"Kian1"},{"upvote_count":"2","poster":"Firststack","timestamp":"1633185360.0","comment_id":"281180","content":"B is correct"},{"content":"I go with B","poster":"Ebi","timestamp":"1633160400.0","upvote_count":"4","comment_id":"276802"},{"comment_id":"253058","content":"Answer is B","upvote_count":"2","poster":"Bulti","timestamp":"1632910200.0"},{"timestamp":"1632870720.0","content":"Why would I create a role if I am going to grant access to the account. Easy, breezy this one - B.","comment_id":"250757","upvote_count":"2","poster":"Britts"},{"content":"Correct is B. S3 bucket that allows the newly created IAM role","timestamp":"1632865260.0","poster":"T14102020","upvote_count":"1","comment_id":"244006"},{"timestamp":"1632747360.0","comment_id":"231611","upvote_count":"4","content":"I'll go with B","poster":"jackdryan"},{"poster":"CYL","comment_id":"209323","upvote_count":"1","timestamp":"1632471900.0","content":"B. Permissions are granted to IAM roles."},{"timestamp":"1632380160.0","comment_id":"156978","upvote_count":"2","content":"B is right answer","poster":"Anila_Dhharisi"},{"content":"My answer is B","comment_id":"154473","poster":"zeronine","timestamp":"1632363000.0","upvote_count":"2"}],"question_images":[],"topic":"1","isMC":true,"timestamp":"2020-08-10 08:38:00","question_id":535,"unix_timestamp":1597041480,"question_text":"A company uses Amazon S3 to host a web application. Currently, the company uses a continuous integration tool running on an Amazon EC2 instance that builds and deploys the application by uploading it to an S3 bucket. A Solutions Architect needs to enhance the security of the company's platform with the following requirements:\n✑ A build process should be run in a separate account from the account hosting the web application.\n✑ A build process should have minimal access in the account it operates in.\n✑ Long-lived credentials should not be used.\nAs a start, the Development team created two AWS accounts: one for the application named web account process; other is a named build account.\nWhich solution should the Solutions Architect use to meet the security requirements?","url":"https://www.examtopics.com/discussions/amazon/view/27858-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[]}],"exam":{"name":"AWS Certified Solutions Architect - Professional","isImplemented":true,"provider":"Amazon","isMCOnly":false,"id":32,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019},"currentPage":107},"__N_SSP":true}