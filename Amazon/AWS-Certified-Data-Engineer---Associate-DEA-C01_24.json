{"pageProps":{"questions":[{"id":"2xBHrM8mcQSpoeX2vlOi","answer":"A","discussion":[{"upvote_count":"1","comment_id":"1358488","timestamp":"1739917920.0","content":"Selected Answer: A\nA: Sí, porque Amazon Redshift permite habilitar el registro de actividades y conexiones, y almacenar esos logs automáticamente en un bucket de S3, lo que cumple con las regulaciones y minimiza la sobrecarga operativa.","poster":"italiancloud2025"},{"comment_id":"1328395","upvote_count":"1","poster":"7a1d491","timestamp":"1734518940.0","content":"Selected Answer: A\nS3 Bucket to store logs"}],"question_text":"A company has a data warehouse in Amazon Redshift. To comply with security regulations, the company needs to log and store all user activities and connection activities for the data warehouse.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/153155-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"D":"Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable logging for the Amazon Redshift cluster. Write the logs to the EBS volume.","C":"Create an Amazon Aurora MySQL database. Enable logging for the Amazon Redshift cluster. Write the logs to a table in the Aurora MySQL database.","B":"Create an Amazon Elastic File System (Amazon EFS) file system. Enable logging for the Amazon Redshift cluster. Write logs to the EFS file system.","A":"Create an Amazon S3 bucket. Enable logging for the Amazon Redshift cluster. Specify the S3 bucket in the logging configuration to store the logs."},"question_images":[],"answer_ET":"A","timestamp":"2024-12-18 11:49:00","answers_community":["A (100%)"],"answer_description":"","answer_images":[],"isMC":true,"unix_timestamp":1734518940,"question_id":116,"exam_id":21,"topic":"1"},{"id":"YkZVeKNgXgYbakqx5Ogr","question_id":117,"answers_community":["B (100%)"],"question_text":"A company wants to migrate a data warehouse from Teradata to Amazon Redshift.\n\nWhich solution will meet this requirement with the LEAST operational effort?","isMC":true,"question_images":[],"choices":{"C":"Use AWS Database Migration Service (AWS DMS) to migrate the data. Use automatic schema conversion.","A":"Use AWS Database Migration Service (AWS DMS) Schema Conversion to migrate the schema. Use AWS DMS to migrate the data.","B":"Use the AWS Schema Conversion Tool (AWS SCT) to migrate the schema. Use AWS Database Migration Service (AWS DMS) to migrate the data.","D":"Manually export the schema definition from Teradata. Apply the schema to the Amazon Redshift database. Use AWS Database Migration Service (AWS DMS) to migrate the data."},"url":"https://www.examtopics.com/discussions/amazon/view/153154-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","timestamp":"2024-12-18 11:48:00","unix_timestamp":1734518880,"answer_images":[],"discussion":[{"poster":"HagarTheHorrible","timestamp":"1734943020.0","upvote_count":"1","comment_id":"1330731","content":"Selected Answer: B\nB, \nA, seems a lot like it but AWS DMS has limited schema conversion capabilities. It is better paired with AWS SCT for schema migration."},{"upvote_count":"1","timestamp":"1734518880.0","content":"Selected Answer: B\nB for least operational overhead","poster":"7a1d491","comment_id":"1328394"}],"answer_description":"","answer_ET":"B","answer":"B","exam_id":21},{"id":"pAnjHJOVQ4tQX43oiNhS","answer_description":"","unix_timestamp":1734254400,"topic":"1","isMC":true,"answer_images":[],"answer_ET":"A","timestamp":"2024-12-15 10:20:00","exam_id":21,"answer":"A","discussion":[{"upvote_count":"1","content":"Selected Answer: A\nA so you don't have to bother with manual provisioning.","poster":"MerryLew","comment_id":"1341213","timestamp":"1736967360.0"},{"comment_id":"1339270","upvote_count":"1","content":"Selected Answer: A\nRedshift Serverless automatically scales resources up or down based on query workload. This eliminates the need for manual capacity provisioning and scaling, significantly reducing operational overhead.","timestamp":"1736624520.0","poster":"ceaser221"},{"upvote_count":"1","poster":"7a1d491","content":"Selected Answer: A\nServerless is for unpredictable loads","comment_id":"1326764","timestamp":"1734254400.0"}],"question_images":[],"answers_community":["A (100%)"],"question_id":118,"url":"https://www.examtopics.com/discussions/amazon/view/152992-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A company uses a variety of AWS and third-party data stores. The company wants to consolidate all the data into a central data warehouse to perform analytics. Users need fast response times for analytics queries.\n\nThe company uses Amazon QuickSight in direct query mode to visualize the data. Users normally run queries during a few hours each day with unpredictable spikes.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","choices":{"A":"Use Amazon Redshift Serverless to load all the data into Amazon Redshift managed storage (RMS).","C":"Use Amazon Redshift provisioned clusters to load all the data into Amazon Redshift managed storage (RMS).","B":"Use Amazon Athena to load all the data into Amazon S3 in Apache Parquet format.","D":"Use Amazon Aurora PostgreSQL to load all the data into Aurora."}},{"id":"habnnfQqkZHi1CGZn1D2","discussion":[{"poster":"Faye15599","content":"Selected Answer: A\nA is the best solution because the issue of hot shards is typically caused by an uneven distribution of records across shards due to poorly chosen partition keys. Using a random partition key ensures that records are distributed more evenly across all shards, reducing the likelihood of any single shard becoming \"hot\" and experiencing throttling.\n\nB is incorrect because while increasing the number of shards can help handle more data, it does not resolve the root cause of hot shards, which is uneven distribution due to poor partition key selection. Without addressing the partition key issue, adding shards may still result in some shards being overloaded.","timestamp":"1742056380.0","upvote_count":"3","comment_id":"1398929"},{"upvote_count":"2","timestamp":"1740207540.0","comment_id":"1360036","poster":"JekChong","content":"Selected Answer: B\nAmazon Kinesis Data Streams uses shards to distribute data, and each shard has a fixed throughput limit. If certain shards receive significantly more data than others (hot shards), they will experience throttling. To resolve this issue:\n\nIncrease the number of shards – This increases the overall capacity of the stream.\nDistribute records more evenly across shards – This can be done by modifying the partition key strategy so that data is spread more evenly."},{"timestamp":"1739917980.0","poster":"italiancloud2025","upvote_count":"2","content":"Selected Answer: A\nA: Sí, usar una clave de partición aleatoria distribuirá uniformemente los registros entre los shards, reduciendo cuellos de botella en shards \"calientes\".\nB: No, aumentar shards no soluciona la desproporción si la clave sigue causando concentración.","comment_id":"1358489"}],"exam_id":21,"answer_description":"","answers_community":["A (71%)","B (29%)"],"timestamp":"2025-02-18 23:33:00","unix_timestamp":1739917980,"question_images":[],"question_text":"A data engineer uses Amazon Kinesis Data Streams to ingest and process records that contain user behavior data from an application every day.\n\nThe data engineer notices that the data stream is experiencing throttling because hot shards receive much more data than other shards in the data stream.\n\nHow should the data engineer resolve the throttling issue?","answer_images":[],"topic":"1","question_id":119,"choices":{"C":"Limit the number of records that are sent each second by the producer to match the capacity of the stream.","D":"Decrease the size of the records that the producer sends to match the capacity of the stream.","A":"Use a random partition key to distribute the ingested records.","B":"Increase the number of shards in the data stream. Distribute the records across the shards."},"answer_ET":"A","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/156778-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true},{"id":"9jePS18rrXAiyxlA4QxR","question_text":"A company has a data processing pipeline that includes several dozen steps. The data processing pipeline needs to send alerts in real time when a step fails or succeeds. The data processing pipeline uses a combination of Amazon S3 buckets, AWS Lambda functions, and AWS Step Functions state machines.\n\nA data engineer needs to create a solution to monitor the entire pipeline.\n\nWhich solution will meet these requirements?","timestamp":"2025-02-18 23:35:00","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/156779-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"answer_description":"","isMC":true,"answer":"D","answer_ET":"D","question_id":120,"answers_community":["D (100%)"],"unix_timestamp":1739918100,"choices":{"C":"Use AWS CloudTrail to send a message to an Amazon Simple Notification Service (Amazon SNS) topic that sends notifications when a state machine fails to run or succeeds to run.","D":"Configure an Amazon EventBridge rule to react when the execution status of a state machine changes. Configure the rule to send a message to an Amazon Simple Notification Service (Amazon SNS) topic that sends notifications.","A":"Configure the Step Functions state machines to store notifications in an Amazon S3 bucket when the state machines finish running. Enable S3 event notifications on the S3 bucket.","B":"Configure the AWS Lambda functions to store notifications in an Amazon S3 bucket when the state machines finish running. Enable S3 event notifications on the S3 bucket."},"answer_images":[],"discussion":[{"content":"Selected Answer: D\nD: Sí, porque EventBridge puede reaccionar en tiempo real a los cambios de estado de las state machines y enviar notificaciones a través de SNS.","comment_id":"1358491","timestamp":"1739918100.0","poster":"italiancloud2025","upvote_count":"1"}],"exam_id":21}],"exam":{"numberOfQuestions":207,"isBeta":false,"id":21,"lastUpdated":"11 Apr 2025","provider":"Amazon","name":"AWS Certified Data Engineer - Associate DEA-C01","isMCOnly":true,"isImplemented":true},"currentPage":24},"__N_SSP":true}