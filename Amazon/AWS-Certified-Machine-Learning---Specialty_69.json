{"pageProps":{"questions":[{"id":"ahTAYrohq7V5YkDcwXUL","url":"https://www.examtopics.com/discussions/amazon/view/43717-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":341,"answers_community":["D (92%)","8%"],"question_images":[],"discussion":[{"content":"D near-real time","comment_id":"281451","upvote_count":"43","comments":[{"content":"The main problem with D is that Amazon Kinesis Data Firehose can not be a source service for Amazon Kinesis Data Analytics.\n\nThe answer would be correct if it said \n\"Using Amazon Kinesis Data Stream to ingest data, using Amazon Kinesis Data Analytics for defect detection and using Amazon Kinesis Data Firehose for storing data for further Analysis\"\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-name.html","upvote_count":"2","comment_id":"1054437","poster":"DimLam","timestamp":"1698314100.0","comments":[{"timestamp":"1708440240.0","upvote_count":"2","poster":"VR10","content":"Actually Kinesis Data Firehose can be used for Data Ingestion.\nSo the correct option is still D","comment_id":"1154787"}]}],"timestamp":"1632206700.0","poster":"Joe_Zhang"},{"timestamp":"1633172760.0","upvote_count":"17","comment_id":"284051","content":"Glad we are all in agreement D is the correct answer","poster":"cnethers"},{"comment_id":"1290328","upvote_count":"1","timestamp":"1727466540.0","poster":"xicocaio","content":"Selected Answer: D\nAmazon Kinesis Data Firehose is a fully managed service for real-time data ingestion, which fits the requirement for near-real-time defect detection. It can ingest large volumes of data from various sources and reliably load the data into other AWS services like Amazon S3 for storage.\nAmazon Kinesis Data Analytics with Random Cut Forest (RCF) is highly efficient for detecting anomalies in streaming data in near real time, which is what the engineers need to catch manufacturing defects during testing.\nAfter detecting anomalies, the data can be stored in Amazon S3 via Kinesis Data Firehose for offline analysis."},{"poster":"SandyHenshaw","comment_id":"1254430","content":"Selected Answer: D\nD - firehose for near realtime","timestamp":"1721827560.0","upvote_count":"1"},{"poster":"VR10","comment_id":"1154788","content":"Selected Answer: D\nKinesis Data Firehose is a fully managed service that can ingest streaming data and load it into destinations like S3, Redshift, Elasticsearch.\nand with Kinesis Data Analytics and RCF and then Data Firehose again to store on S3.\nD is the best choice.","timestamp":"1708440360.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"1093460","timestamp":"1702298880.0","content":"https://docs.aws.amazon.com/managed-flink/latest/java/get-started-exercise-fh.html","poster":"fa0d8b7"},{"timestamp":"1701092760.0","comment_id":"1081606","upvote_count":"1","poster":"endeesa","content":"Selected Answer: D\nKinesis seems like the only viable option"},{"timestamp":"1700318640.0","content":"The answer is D. Since, data is continuously coming in Kinesis datafirehose is our streaming application (also we need near Real time defect detection and storage in S3) and anomaly detection can be done by kinesis data application (RCF algorithm).","upvote_count":"1","comment_id":"1074075","poster":"akgarg00"},{"poster":"AmeeraM","upvote_count":"1","timestamp":"1697269620.0","content":"Selected Answer: D\nD, near real-time ingestion is the key","comment_id":"1043219"},{"poster":"loict","comment_id":"1007427","content":"Selected Answer: D\nA. NO - AWS IoT will first store the data, then make it available for Analytics/Jupyter (https://docs.aws.amazon.com/iotanalytics/latest/userguide/welcome.html); so not real-time\nB. NO - not realtime to store the data before analytics\nC. NO - not realtime to store the data before analytics\nD. YES - real-time pipe, RCF best for anomalities","timestamp":"1694683860.0","upvote_count":"1"},{"comment_id":"1006686","timestamp":"1694613000.0","upvote_count":"1","content":"Selected Answer: D\nHow can someone use S3 for ingestion? Firehose is the right answer","poster":"DavidRou"},{"content":"Selected Answer: D\nThis option meets the requirements of performing near-real time defect detection, storing all the data for offline analysis, and handling 200 performance metrics in a time-series. Amazon Kinesis Data Firehose is a fully managed service that can ingest streaming data from various sources and deliver it to destinations such as Amazon S3, Amazon OpenSearch Service, and Amazon Redshift. Amazon Kinesis Data Analytics is a service that can process streaming data using SQL or Apache Flink applications. Amazon Kinesis Data Analytics provides a built-in RANDOM_CUT_FOREST function, a machine learning algorithm that can detect anomalies in streaming data1. This function can handle high-dimensional data and assign an anomaly score to each record based on how distant it is from other records1. The anomaly scores can then be delivered to another destination using Kinesis Data Firehose or consumed by other applications using Kinesis Data Streams.","comment_id":"991540","timestamp":"1693150680.0","upvote_count":"1","poster":"Mickey321"},{"content":"D is the correct\nIf the question says \"data streaming\", \"real time data\" or \"near real time\" you should look for kinesis services.\nB and C are totally wrong: It's not possible to use S3 to ingestion, only storage.","upvote_count":"2","comment_id":"969399","timestamp":"1690922580.0","poster":"kaike_reis"},{"upvote_count":"1","poster":"ADVIT","content":"D,\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html","timestamp":"1688156940.0","comment_id":"939370"},{"poster":"earthMover","timestamp":"1684969260.0","comment_id":"906187","upvote_count":"4","content":"Selected Answer: D\nAt a minimum the moderators should put some explanation when the community vote overwhelmingly for a different option."},{"content":"Selected Answer: C\nOption D is not necessarily incorrect, but it may not be the most effective approach to perform near-real time defect detection in this scenario. Here are some potential drawbacks of this approach:\n\nAmazon Kinesis Data Firehose is primarily used for data ingestion and delivery to other services, and may not be the best choice for real-time analysis.\nUsing Amazon Kinesis Data Analytics for anomaly detection may be less flexible than using Amazon SageMaker, which provides a wide range of algorithms and models for anomaly detection.\nRandom Cut Forest (RCF) is a popular anomaly detection algorithm used for time-series data, and Amazon SageMaker provides an RCF implementation that can be used for anomaly detection in real-time or offline. While Amazon Kinesis Data Analytics also provides RCF, using Amazon SageMaker may be a better choice for scalability and flexibility.","comment_id":"862262","timestamp":"1680710220.0","upvote_count":"1","poster":"oso0348"},{"content":"Selected Answer: C\nYes, option C can provide near real-time defect detection. Amazon SageMaker's Random Cut Forest (RCF) algorithm is designed to work with streaming data and can detect anomalies in near real-time. It can process data in batches as small as a single data point, making it well-suited for real-time anomaly detection.\n\nIn this scenario, if the manufacturing process is generating data in real-time, it can be ingested into Amazon S3 and processed by Amazon SageMaker's RCF algorithm, allowing for near real-time detection of critical manufacturing defects during testing.","upvote_count":"1","comments":[{"content":"this is ridiculous. How can you store in s3 and then conduct real-time analysis?","timestamp":"1682014620.0","upvote_count":"2","comment_id":"875879","poster":"ZSun"}],"comment_id":"862260","timestamp":"1680710160.0","poster":"oso0348"},{"upvote_count":"2","comment_id":"850225","poster":"Paolo991","content":"Selected Answer: D\nFirehose to ingest in near real time and RCF to do Anomaly is the best approach","timestamp":"1679756220.0"},{"poster":"fez_2312","comment_id":"839719","upvote_count":"2","timestamp":"1678870920.0","content":"This question confused me because A is the only solution which can be implemented offline, S3 is an online, so how can the data be analysed if it has to be onlone?"},{"poster":"sqavi","content":"Selected Answer: D\nD is correct answer. When solution is available in AWS as a service, no need to build it with anything else","comment_id":"804144","upvote_count":"3","timestamp":"1676019660.0"},{"comment_id":"621205","timestamp":"1656008460.0","content":"Selected Answer: D\nKey word is \"near real time\", which will require streams, and that is what Kinesis provides","poster":"ovokpus","upvote_count":"3"},{"timestamp":"1643448600.0","comment_id":"535355","poster":"clawo","upvote_count":"2","content":"Selected Answer: D\nD of course"},{"content":"Selected Answer: D\nD is the answer. it's near real time so Kinesis and also anomaly detection RCF.","poster":"geekgirl007","comment_id":"518942","timestamp":"1641558120.0","upvote_count":"2"},{"upvote_count":"2","timestamp":"1636647660.0","comment_id":"476357","poster":"[Removed]","content":"Whenever there is a requirement for real-time data ingestion (time series data and otherwise) and perform data transformation and real-time analytics on the fly, then the combination of Firehose and Kinesis Analytics is the de-facto architecture pattern."},{"timestamp":"1635872040.0","upvote_count":"2","content":"Should be D (near real time, auto anomaly detection using RCF, uses Firehose later to store data for offline analysis)","comment_id":"425667","poster":"Nickname_L"},{"upvote_count":"1","poster":"kezzzzz","content":"Should be A. Ingest, store, do real time analytics and AI\nhttps://aws.amazon.com/iot-analytics/?c=i&sec=srv","timestamp":"1635682080.0","comment_id":"404236"},{"timestamp":"1635321240.0","comments":[{"upvote_count":"2","poster":"dunhill","timestamp":"1640625720.0","comment_id":"510506","content":"I believe that the reason is \"need near-real-time detection\". IoT needs jupyter for detect and analyse for later. In the other way, Kinesis Analytics can handle IoT data."}],"content":"Why not A?","comment_id":"404235","upvote_count":"1","poster":"kezzzzz"},{"poster":"tmld","upvote_count":"3","comment_id":"336124","content":"S3 is an object storage, not for data ingestion. B&C are wrong. I go with D","timestamp":"1634956080.0"},{"poster":"Vita_Rasta84444","timestamp":"1633976700.0","content":"Answer is D","upvote_count":"2","comment_id":"324873"},{"content":"D is correct!","poster":"astonm13","comment_id":"291623","timestamp":"1633324140.0","upvote_count":"2"},{"upvote_count":"4","content":"Ans. is D!","comment_id":"283445","poster":"takahirokoyama","timestamp":"1632853740.0"},{"content":"should be D","upvote_count":"4","poster":"wolf90","timestamp":"1632831240.0","comment_id":"282920"},{"upvote_count":"3","content":"D is correct.","comment_id":"282834","poster":"takahirokoyama","timestamp":"1632310500.0"}],"answer":"D","timestamp":"2021-02-01 22:42:00","answer_ET":"D","topic":"1","question_text":"An aircraft engine manufacturing company is measuring 200 performance metrics in a time-series. Engineers want to detect critical manufacturing defects in near- real time during testing. All of the data needs to be stored for offline analysis.\nWhat approach would be the MOST effective to perform near-real time defect detection?","choices":{"B":"Use Amazon S3 for ingestion, storage, and further analysis. Use an Amazon EMR cluster to carry out Apache Spark ML k-means clustering to determine anomalies.","D":"Use Amazon Kinesis Data Firehose for ingestion and Amazon Kinesis Data Analytics Random Cut Forest (RCF) to perform anomaly detection. Use Kinesis Data Firehose to store data in Amazon S3 for further analysis.","C":"Use Amazon S3 for ingestion, storage, and further analysis. Use the Amazon SageMaker Random Cut Forest (RCF) algorithm to determine anomalies.","A":"Use AWS IoT Analytics for ingestion, storage, and further analysis. Use Jupyter notebooks from within AWS IoT Analytics to carry out analysis for anomalies."},"unix_timestamp":1612215720,"answer_images":[],"exam_id":26,"isMC":true,"answer_description":""},{"id":"S6HtUPlnxR2yiqDQZpa4","url":"https://www.examtopics.com/discussions/amazon/view/43916-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":342,"question_text":"A Machine Learning team runs its own training algorithm on Amazon SageMaker. The training algorithm requires external assets. The team needs to submit both its own algorithm code and algorithm-specific parameters to Amazon SageMaker.\nWhat combination of services should the team use to build a custom algorithm in Amazon SageMaker? (Choose two.)","answers_community":["CE (100%)"],"discussion":[{"timestamp":"1633216920.0","content":"CE is the right answer. ECR uses ECS internally while using SGM.","upvote_count":"11","poster":"Paul_NoName","comment_id":"282896","comments":[{"timestamp":"1633939500.0","comment_id":"283140","poster":"[Removed]","upvote_count":"6","content":"CE based on criteria and this documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-mkt-create-model-package.html\n\n\"For Location of inference image, type the path to the image that contains your inference code. The image must be stored as a Docker container in Amazon ECR.\nFor Location of model data artifacts, type the location in S3 where your model artifacts are stored.\"","comments":[{"upvote_count":"3","timestamp":"1682269920.0","content":"the answer is correct but the explanation is completely wrong. \nThe question is about how to create your own algorithm using container, not \"put the inference in market\" (which is your resource link).\nthe right citation should be \"Adapting your own training container\": create s3 to store model articraft, and push code to ECR.","poster":"ZSun","comment_id":"878660"}]}]},{"timestamp":"1635064620.0","content":"CE IS THE CORRECT ANSWER 100%","upvote_count":"5","poster":"SophieSu","comment_id":"298720"},{"poster":"MultiCloudIronMan","comment_id":"1288546","timestamp":"1727175000.0","upvote_count":"1","content":"Selected Answer: CE\nAmazon ECR (Option C): Amazon Elastic Container Registry (ECR) is used to store, manage, and deploy Docker container images. The team can package their custom algorithm code into a Docker container and store it in Amazon ECR1.\nAmazon S3 (Option E): Amazon Simple Storage Service (S3) is used to store external assets and data. The team can store the algorithm-specific parameters and any other required data in Amazon S3"},{"timestamp":"1693150920.0","content":"Selected Answer: CE\nAmazon ECR is a fully managed container registry service that allows users to store, manage, and deploy Docker container images. Amazon SageMaker supports using custom Docker images for training and inference, which can contain the user’s own training algorithm and any external assets or dependenciesAd1. The user can push their Docker image to Amazon ECR and then reference it in their Amazon SageMaker training job configurationAd1.","comment_id":"991544","poster":"Mickey321","upvote_count":"1"},{"upvote_count":"1","poster":"jackzhao","content":"CE is correct!","comment_id":"840616","timestamp":"1678950420.0"},{"timestamp":"1678279560.0","upvote_count":"1","content":"ECR for the code, S3 for the parameters!","poster":"Valcilio","comment_id":"832953"},{"content":"Selected Answer: CE\nC contain the algorithm's image and E contain algorithm's parameters.","timestamp":"1678041180.0","poster":"Valcilio","comment_id":"830184","upvote_count":"2"},{"poster":"randomnamer","timestamp":"1635856620.0","content":"The location of the model artifacts. Model artifacts can either be packaged in the same Docker container as the inference code or stored in Amazon S3. Not so sure.","comment_id":"324946","upvote_count":"1"},{"timestamp":"1634231820.0","upvote_count":"3","comment_id":"284057","poster":"cnethers","content":"https://aws.amazon.com/blogs/machine-learning/bringing-your-own-custom-container-image-to-amazon-sagemaker-studio-notebooks/\nIf you wish to use your private VPC to securely bring your custom container, you also need the following:\n\n A VPC with a private subnet\n VPC endpoints for the following services:\n Amazon Simple Storage Service (Amazon S3)\n Amazon SageMaker\n Amazon ECR\n AWS Security Token Service (AWS STS)\n CodeBuild for building Docker containers\n\nAnswer C+E"},{"upvote_count":"1","timestamp":"1633004400.0","comment_id":"282708","poster":"ahquiceno","comments":[{"upvote_count":"1","content":"Sorry, CE is correct.","comments":[{"content":"Sagemaker will spin up the instances needed with the right image. No need to use ECS. CE is right","timestamp":"1635966660.0","poster":"gcpwhiz","upvote_count":"1","comment_id":"337019"}],"timestamp":"1635230400.0","comment_id":"307254","poster":"ahquiceno"}],"content":"For me CD. needs storage and create a custom docker using ECR to store it."}],"answer_images":[],"topic":"1","isMC":true,"answer_description":"","unix_timestamp":1612355520,"answer_ET":"CE","exam_id":26,"choices":{"B":"AWS CodeStar","A":"AWS Secrets Manager","D":"Amazon ECS","E":"Amazon S3","C":"Amazon ECR"},"question_images":[],"answer":"CE","timestamp":"2021-02-03 13:32:00"},{"id":"IexUDhLqHkybBwgsliuL","question_text":"A Machine Learning Specialist wants to determine the appropriate SageMakerVariantInvocationsPerInstance setting for an endpoint automatic scaling configuration. The Specialist has performed a load test on a single instance and determined that peak requests per second (RPS) without service degradation is about 20 RPS. As this is the first deployment, the Specialist intends to set the invocation safety factor to 0.5.\nBased on the stated parameters and given that the invocations per instance setting is measured on a per-minute basis, what should the Specialist set as the\nSageMakerVariantInvocationsPerInstance setting?","answers_community":["C (100%)"],"question_images":[],"choices":{"A":"10","C":"600","D":"2,400","B":"30"},"url":"https://www.examtopics.com/discussions/amazon/view/43915-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":343,"exam_id":26,"topic":"1","timestamp":"2021-02-03 13:23:00","answer_description":"","answer_images":[],"answer_ET":"C","discussion":[{"timestamp":"1667206980.0","comment_id":"282897","upvote_count":"14","poster":"Paul_NoName","content":"C is correct .\nSageMakerVariantInvocationsPerInstance = (MAX_RPS * SAFETY_FACTOR) * 60\nAWS recommended Saf_fac =0 .5"},{"timestamp":"1666976340.0","upvote_count":"6","comment_id":"282700","content":"Answer C: SageMakerVariantInvocationsPerInstance = (MAX_RPS * SAFETY_FACTOR) * 60\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html","poster":"ahquiceno"},{"poster":"Mickey321","timestamp":"1724773560.0","upvote_count":"1","comment_id":"991546","content":"Selected Answer: C\nTo calculate the SageMakerVariantInvocationsPerInstance setting, we can use the following equation from the web search results1:\nSageMakerVariantInvocationsPerInstance = (MAX_RPS * SAFETY_FACTOR) * 60\nWhere MAX_RPS is the maximum RPS that the variant can handle, SAFETY_FACTOR is the safety factor that we choose to ensure that we don’t exceed the maximum RPS, and 60 is to convert from RPS to invocations-per-minute.\nPlugging in the given values, we get:\nSageMakerVariantInvocationsPerInstance = (20 * 0.5) * 60 SageMakerVariantInvocationsPerInstance = 10 * 60 SageMakerVariantInvocationsPerInstance = 600\nTherefore, the Specialist should set the SageMakerVariantInvocationsPerInstance setting to 600."},{"content":"SageMakerVariantInvocationsPerInstance = (MAX_RPS * SAFETY_FACTOR) * 60","comment_id":"840625","poster":"jackzhao","upvote_count":"1","timestamp":"1710573480.0"},{"content":"SageMakerVariantInvocationsPerInstance = (MAX_RPS * SAFETY_FACTOR) * 60\n(20RPS * 0.5Safety Factor) * 60\n(10)*60 = 600\nAnswer C","comment_id":"789277","timestamp":"1706323260.0","upvote_count":"1","poster":"King_Chess1"},{"poster":"Peeking","content":"Selected Answer: C\nMaximum request at peak time = 20 RPS = 20x60 = 1200RPM\nSafety factor of 0.5 = 1200*0.5 = 600\nBasic setting of parameter = 600 (requests per minutes)","comment_id":"739856","upvote_count":"1","timestamp":"1702104780.0"}],"answer":"C","unix_timestamp":1612354980,"isMC":true},{"id":"Tua4TqpXGAew3BQbxdbe","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/43874-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","timestamp":"2021-02-03 04:58:00","discussion":[{"poster":"jiadong","upvote_count":"24","comment_id":"282405","content":"I think the right answer is D","timestamp":"1632727200.0"},{"content":"D is correct.\nC is not the best the answer because the question states that tuning parameters doesn't help a lot. Transfer learning would be better solution!","poster":"SophieSu","comment_id":"297009","timestamp":"1632900780.0","upvote_count":"11"},{"comment_id":"1290335","content":"Selected Answer: D\nUsing word2vec embeddings would give the model more accurate representations of words at the start, potentially leading to a significant performance boost for text classification tasks.","timestamp":"1727467620.0","upvote_count":"1","poster":"xicocaio"},{"content":"Selected Answer: D\nA. NO, transfer learning helps, word2vec > TD-ITF as the first keeps into account part of the word context (there is a hyperparameter for this)\nB. LTSM delivers better results wrt GRU which is in turn a compromise architecture to balance accuracy with training time/cost\nC. Heperparameters tuning has been already applied, this will not help\nD. YEs, transfer learning will help and word3vec is better option in this scenario","poster":"ninomfr64","timestamp":"1719130800.0","upvote_count":"2","comment_id":"1235740"},{"poster":"3eb0542","comment_id":"1142598","content":"Selected Answer: D\nHow are the 'correct' answers being provided? I'm seeing so many answers that seem to be wrong and usually, the community vote seems to be correct. This is kind of frustrating.","timestamp":"1707253620.0","upvote_count":"3"},{"comment_id":"991551","timestamp":"1693151400.0","poster":"Mickey321","upvote_count":"3","content":"Selected Answer: D\nWord2vec is a technique that can learn distributed representations of words, also known as word embeddings, from large amounts of text data. Word embeddings can capture the semantic and syntactic similarities and relationships between words, and can be used as input features for neural network models. Word2vec can be trained on domain-specific corpora to obtain more relevant and accurate word embeddings for a particular task."},{"comment_id":"970444","content":"Selected Answer: D\nFrom my perspective, B and C are wrong because the DS already tried something close to this. D is correct.","timestamp":"1691000040.0","poster":"kaike_reis","upvote_count":"1"},{"content":"I don't think High Dimensionality is take care by C2V; TF-IDF is required. A.","upvote_count":"1","comment_id":"907525","timestamp":"1685122440.0","poster":"vbal"},{"comment_id":"740331","timestamp":"1670609460.0","upvote_count":"2","poster":"Peeking","content":"Selected Answer: D\nTransfer learning, in my experience, has been a good way to boost performance when hyperparameter tuning did not work."},{"poster":"Sidekick","comment_id":"607475","timestamp":"1653541680.0","content":"The case ask for predicting labels for sentences, the appropriate algo should be \"Text Classification\" Which, just as \"wrod2vec,i part of Blazing Text.","upvote_count":"1"},{"timestamp":"1649913120.0","poster":"julpeg","comment_id":"585544","content":"Selected Answer: D\nThe answer should be D. My reasoning is that by using a word embedding which is trained on domain specific material, the embeddings between two words are more domain specific. This means that relations (good or bad) are represented in a better way, which also means that the model should be able to predict the results in a more accurate way.","upvote_count":"3"},{"upvote_count":"6","poster":"bitsplease","content":"both A & D \"seem\" correct, but word2vec takes ORDER of words into acc (to some extent)--while TF-IDF does not. Thus max boost is from D.\n\nB,C are wrong because the DS has tried several network architectures (aka LSTM) and hyperparameter tuning (aka option C)","comment_id":"527756","timestamp":"1642611480.0"},{"comments":[{"content":"I think that the general tf-idf vectors cannot be directly adapted to the deep learning model, because of the large dimension in vector values","timestamp":"1671684900.0","comment_id":"752969","upvote_count":"1","poster":"GiyeonShin"}],"upvote_count":"1","poster":"ahmedelbhy","content":"i think answer is A as The model reviews multi-page text documents","timestamp":"1635144540.0","comment_id":"430928"},{"timestamp":"1633490700.0","poster":"puffpuff","content":"I think it should be B\nA/D are false flags because the question doesn't specify what kind of data engineering is currently done on the inputs, as a baseline\nPer wikipedia, for GRUs, \"GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets\", which fits the context of a particular energy sector","upvote_count":"2","comment_id":"397336"},{"upvote_count":"1","comment_id":"382293","poster":"ChanduPatil","content":"why not B??","timestamp":"1633441020.0","comments":[{"upvote_count":"1","timestamp":"1671684600.0","content":"Generally, LSTM has the better performance then GRU in large datasets such as multi-page documents. GRU has advantages of memory allocation and training time.","poster":"GiyeonShin","comment_id":"752967"},{"content":"Early stopping can give the model better performance, but I think that the model needs more condition like patience value for early stopping. This is because the model doesn't always show the performance at its maximum when the validation loss stops decreasing.","upvote_count":"1","timestamp":"1671685260.0","comment_id":"752971","poster":"GiyeonShin"}]},{"comment_id":"343895","poster":"jkreddy","comments":[{"content":"but they need to classify the whole sentence i think for such a case we use object2vec not word2vec, but since it's not available in the answers, B is the only answer left.","comment_id":"391126","poster":"YJ4219","upvote_count":"2","timestamp":"1633470540.0"}],"timestamp":"1633320240.0","content":"It cannot be C, because hyper parameter tuning didnt work as given in question. Also, A and D are same, however, word2vec model internally implements tf-idf much more efficiently. So answer got to be D","upvote_count":"4"},{"upvote_count":"2","content":"I go for C","comment_id":"336207","timestamp":"1633286880.0","poster":"tmld"},{"upvote_count":"3","timestamp":"1632818640.0","comment_id":"291631","poster":"astonm13","content":"Agree. D seems more reasonable, as word2vec provides content of the sentences which is very important for evaluation of the risk."}],"exam_id":26,"answers_community":["D (100%)"],"topic":"1","answer_images":[],"question_id":344,"question_text":"A company uses a long short-term memory (LSTM) model to evaluate the risk factors of a particular energy sector. The model reviews multi-page text documents to analyze each sentence of the text and categorize it as either a potential risk or no risk. The model is not performing well, even though the Data Scientist has experimented with many different network structures and tuned the corresponding hyperparameters.\nWhich approach will provide the MAXIMUM performance boost?","choices":{"B":"Use gated recurrent units (GRUs) instead of LSTM and run the training process until the validation loss stops decreasing.","D":"Initialize the words by word2vec embeddings pretrained on a large collection of news articles related to the energy sector.","A":"Initialize the words by term frequency-inverse document frequency (TF-IDF) vectors pretrained on a large collection of news articles related to the energy sector.","C":"Reduce the learning rate and run the training process until the training loss stops decreasing."},"answer_ET":"D","isMC":true,"question_images":[],"unix_timestamp":1612324680},{"id":"O63QaN73dhLO9rucjBPW","question_id":345,"unix_timestamp":1612219380,"answer_images":[],"answer":"BC","discussion":[{"upvote_count":"32","timestamp":"1647717060.0","content":"should be BC","comment_id":"281476","comments":[{"content":"Agreed, AWS Example: https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html","upvote_count":"6","poster":"[Removed]","timestamp":"1649628180.0","comment_id":"283143"}],"poster":"Joe_Zhang"},{"comment_id":"1154842","timestamp":"1724163900.0","upvote_count":"2","content":"It is obviously B and C, I am frustrated with the number of wrong answers. Why the moderator's answers keep being super weird?","poster":"Denise123"},{"content":"Selected Answer: BC\nB for near real-time \nC for hourly","comment_id":"991553","upvote_count":"2","poster":"Mickey321","timestamp":"1709056440.0"},{"comment_id":"860840","timestamp":"1696410660.0","content":"The right answer is BC","upvote_count":"2","poster":"Khalil11"},{"timestamp":"1691693460.0","comment_id":"804782","upvote_count":"4","poster":"AjoseO","content":"Selected Answer: BC\nAWS Data Pipeline (Option C) can be used to move the hourly data, as it provides a way to move data from various sources to Amazon EMR for processing.\n\nAmazon Kinesis (Option B) can be used to process data in near-real time, as it is a real-time data streaming service that can handle large amounts of incoming data from multiple sources. The data can be fed to Amazon EMR MapReduce jobs for processing."},{"upvote_count":"2","timestamp":"1687949040.0","comment_id":"759820","content":"Selected Answer: BC\nshould be BC","poster":"DS2021"},{"poster":"Peeking","content":"Selected Answer: BC\nKinesis for near realtime data and pipeline for the other data moved hourly.","comment_id":"740335","upvote_count":"3","timestamp":"1686327240.0"},{"poster":"John_Pongthorn","content":"Selected Answer: BC\nAWS ES is an elastic search , it is nothing to do with this question.","upvote_count":"3","timestamp":"1660977360.0","comment_id":"551644"},{"content":"Kinesis data into EMR: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-kinesis.html","timestamp":"1660830120.0","poster":"scsas","comment_id":"550315","upvote_count":"1"},{"comment_id":"540168","timestamp":"1659576360.0","poster":"apprehensive_scar","content":"BC. easy","upvote_count":"2"},{"content":"I believe the answer is BC","upvote_count":"1","comment_id":"516011","timestamp":"1656871260.0","poster":"KM226"},{"poster":"Madwyn","content":"BD.\nData Pipeline is to orchestrate the workflow, how can that feed data to the MR jobs?","timestamp":"1651321920.0","comment_id":"426872","upvote_count":"2"},{"comment_id":"324888","poster":"Vita_Rasta84444","content":"Answer is B and C","upvote_count":"1","timestamp":"1650212460.0"},{"poster":"astonm13","comment_id":"291637","content":"Answer is for sure BC","timestamp":"1650078540.0","upvote_count":"3"},{"upvote_count":"2","timestamp":"1649098740.0","poster":"takahirokoyama","content":"Ans is BC.\n(https://aws.amazon.com/jp/emr/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc)","comment_id":"282843"}],"question_images":[],"answers_community":["BC (100%)"],"choices":{"E":"Amazon ES","D":"Amazon Athena","B":"Amazon Kinesis","A":"AWS DMS","C":"AWS Data Pipeline"},"question_text":"A Machine Learning Specialist needs to move and transform data in preparation for training. Some of the data needs to be processed in near-real time, and other data can be moved hourly. There are existing Amazon EMR MapReduce jobs to clean and feature engineering to perform on the data.\nWhich of the following services can feed data to the MapReduce jobs? (Choose two.)","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/43721-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"answer_ET":"BC","answer_description":"","timestamp":"2021-02-01 23:43:00","topic":"1"}],"exam":{"isBeta":false,"name":"AWS Certified Machine Learning - Specialty","isMCOnly":false,"isImplemented":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":369,"provider":"Amazon","id":26},"currentPage":69},"__N_SSP":true}