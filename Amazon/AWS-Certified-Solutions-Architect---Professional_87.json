{"pageProps":{"questions":[{"id":"2fXoi80KsgIbor8fgdal","url":"https://www.examtopics.com/discussions/amazon/view/5673-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1569388560,"question_id":431,"answers_community":["B (100%)"],"topic":"1","answer_ET":"B","discussion":[{"comments":[{"content":"Very well explained.","timestamp":"1636117980.0","poster":"StelSen","comment_id":"456072","upvote_count":"1"}],"timestamp":"1632302280.0","upvote_count":"41","content":"Host Affinity is configured at the instance level. It establishes a launch relationship between an instance and a Dedicated Host. (This set which host the instance can run on)\nAuto-placement allows you to manage whether instances that you launch are launched onto a specific host, or onto any available host that has matching configurations. Auto-placement must be configured at the host level. (This sets which instance the host can run.)\nWhen affinity is set to Host, an instance launched onto a specific host always restarts on the same host if stopped. This applies to both targeted and untargeted launches.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-dedicated-hosts-work.html","poster":"donathon","comment_id":"12553"},{"upvote_count":"1","comment_id":"716963","timestamp":"1668293280.0","content":"Selected Answer: B\nDedicated host, so B","poster":"DarthYoda"},{"poster":"superuser784","timestamp":"1667421300.0","upvote_count":"1","content":"Selected Answer: B\nI hope to have several of these straightforward questions on the real test!","comment_id":"710061"},{"timestamp":"1664818020.0","upvote_count":"1","comment_id":"685707","poster":"dmscountera","content":"Selected Answer: B\nBased on all comments"},{"content":"Why is it C instead of A?","upvote_count":"1","comment_id":"562665","poster":"Fuccon","timestamp":"1646661840.0"},{"upvote_count":"1","timestamp":"1638710640.0","comment_id":"494356","content":"B. Run the instance on a dedicated host with Host Affinity set to Host.","poster":"cldy"},{"upvote_count":"1","timestamp":"1638648660.0","poster":"AzureDP900","content":"B is right answer based on requirement.","comment_id":"493937"},{"comment_id":"447684","timestamp":"1635838380.0","upvote_count":"1","content":"Answer is B","poster":"moon2351"},{"timestamp":"1635555300.0","comment_id":"409745","content":"I'll go with B","poster":"WhyIronMan","upvote_count":"2"},{"upvote_count":"2","poster":"Waiweng","content":"it's B","comment_id":"345043","timestamp":"1635410580.0"},{"timestamp":"1635338460.0","content":"Full House B","upvote_count":"3","comment_id":"293717","poster":"kiev"},{"comment_id":"290689","timestamp":"1635090840.0","poster":"Kian1","upvote_count":"2","content":"B host affinity"},{"poster":"Ebi","upvote_count":"3","content":"I go with B","comment_id":"282481","timestamp":"1634391120.0"},{"timestamp":"1633937760.0","content":"B for sure\n\nWhen affinity is set to Host, an instance launched onto a specific host always restarts on the same host if stopped. This applies to both targeted and untargeted launches.\n\nWhen affinity is set to Off, and you stop and restart the instance, it can be restarted on any available host. However, it tries to launch back onto the last Dedicated Host on which it ran (on a best-effort basis).","comment_id":"269361","poster":"kopper2019","upvote_count":"3"},{"comment_id":"268112","poster":"sanjaym","upvote_count":"1","timestamp":"1633885440.0","content":"B is correct."},{"poster":"T14102020","content":"Correct answer is B. Host Affinity","comment_id":"242550","timestamp":"1633244700.0","upvote_count":"1"},{"poster":"jackdryan","upvote_count":"2","comment_id":"230054","content":"I'll go with B","timestamp":"1633126320.0"},{"upvote_count":"2","poster":"Bulti","content":"B is the right answer","comment_id":"229857","timestamp":"1633024320.0"},{"timestamp":"1632733740.0","poster":"Puran","content":"B for sure","upvote_count":"1","comment_id":"209752"},{"upvote_count":"3","poster":"fullaws","comment_id":"149666","content":"B is correct","timestamp":"1632564120.0"},{"comment_id":"134443","poster":"NikkyDicky","content":"must be B","timestamp":"1632499620.0","upvote_count":"2"}],"exam_id":32,"answer":"B","isMC":true,"choices":{"A":"Run a dedicated instance with auto-placement disabled.","D":"Run the instance on a licensed host with termination set for 90 days.","B":"Run the instance on a dedicated host with Host Affinity set to Host.","C":"Run an On-Demand Instance with a Reserved Instance to ensure consistent placement."},"answer_images":[],"timestamp":"2019-09-25 07:16:00","question_text":"A company needs to run a software package that has a license that must be run on the same physical host for the duration of its use. The software package is only going to be used for 90 days. The company requires patching and restarting of all instances every 30 days.\nHow can these requirements be met using AWS?","answer_description":"","question_images":[]},{"id":"SHn9z6obuIQ6kqHiWIKA","discussion":[{"comment_id":"32580","poster":"CSharpPro","upvote_count":"10","content":"A - Default Bucket Limit of 100 per account invalidates B","timestamp":"1633447440.0"},{"timestamp":"1633612740.0","comment_id":"37901","content":"Answer is A\nB: Limit 100 buckets\nC: Too expensive\nD: Limit 100 buckets","poster":"amog","upvote_count":"7","comments":[{"poster":"mnsait","comment_id":"1310440","timestamp":"1731389160.0","upvote_count":"1","content":"Also, even if the limit of 100 buckets is increased to 1000 by requesting AWS, the solution is not scalable as the number of clients can increase in future. Rules out the 'create a bucket per customer' approach."}]},{"upvote_count":"1","content":"A. ASK their customers to use an S3 client instead of an FTP client. Create a single S3 bucket Create an IAM user for each customer Put the IAM Users in a Group that has an IAM policy that permits access to sub-directories within the bucket via use of the 'username' Policy variable.","poster":"amministrazione","comment_id":"1266515","timestamp":"1723735080.0"},{"upvote_count":"1","content":"Selected Answer: A\nCost: \"Requester Pays\" means the customer pays for their own requests and data transfer, but having a separate bucket for each customer can increase costs.\nThis option might not be the most cost-effective due to multiple buckets and might not be preferred by customers since they have to pay for their own requests.\nGiven the options and the requirements, the best solution is: A","comment_id":"1016277","timestamp":"1695605160.0","poster":"Takshashila"},{"comment_id":"950677","timestamp":"1689253200.0","poster":"kondratyevmn","upvote_count":"1","content":"Selected Answer: B\nB - looks like a viable option.\n\nS3 RRS - costs to a minimum\ns3 provides scalability and security via (server side encryption or kms)"},{"timestamp":"1671416040.0","content":"Selected Answer: A\nC and D are obviously wrong.\nB sounds good for the cost. However, RRS is deprecated and is more expensive than the standard class for quite a while.","comment_id":"749370","poster":"TigerInTheCloud","upvote_count":"1"},{"upvote_count":"1","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html 100 buckets per account is default limit which can be increased to max 1000. So if we go with B its not scalable beyond limit.","comment_id":"671223","timestamp":"1663384500.0","poster":"kaushik9845"},{"poster":"hilft","comment_id":"637718","upvote_count":"1","timestamp":"1658884800.0","content":"why not b?"},{"content":"A. ASK their customers to use an S3 client instead of an FTP client. Create a single S3 bucket Create an IAM user for each customer Put the IAM Users in a Group that has an IAM policy that permits access to sub-directories within the bucket via use of the 'username' Policy variable.","upvote_count":"1","poster":"cldy","timestamp":"1639219080.0","comment_id":"499335"},{"content":"A is the solution.\nBut in reality, customers won't like to change their tools unless a really good reason.","upvote_count":"2","comment_id":"346713","poster":"01037","timestamp":"1635974160.0"},{"content":"A.\nBut 250 IAM users is a little too many.\nI think Identity federation may be a better option.","poster":"newme","upvote_count":"2","comment_id":"211006","timestamp":"1635930540.0"},{"poster":"TerrenceC","comment_id":"203280","upvote_count":"1","timestamp":"1635634200.0","content":"Here is another two inputs that RRS might not be an ideal option.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html\n\"We recommend that you not use this storage class. The S3 Standard storage class is more cost effective\"\n\nAlso, when you look at the pricing between Standard and RRS then you would observe that RRS is a bit more expensive than Standard."},{"upvote_count":"1","comment_id":"201838","content":"This question has not mentioned about reduced redundancy storage requirements then perhaps B can't be the best fit.","timestamp":"1635166620.0","poster":"Amitv2706"},{"timestamp":"1635105660.0","poster":"smartassX","comment_id":"188547","upvote_count":"1","content":"100 by Default; limit increase up to 1000. \n\"By default, you can create up to 100 buckets in each of your AWS accounts. If you need additional buckets, you can increase your account bucket limit to a maximum of 1,000 buckets by submitting a service limit increase. There is no difference in performance whether you use many buckets or just a few. \" \nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html"},{"comment_id":"170454","content":"A is Correct - B is not scalable when no of customers exceed the limit on buckets per account.","timestamp":"1634819760.0","poster":"Bulti","upvote_count":"2"},{"poster":"kratnesh","timestamp":"1634548020.0","content":"Answer B. The default now is 1000 buckets per account","upvote_count":"3","comment_id":"149830"},{"poster":"fullaws","timestamp":"1634069760.0","content":"A is correct","upvote_count":"2","comment_id":"143862"},{"content":"go with A","upvote_count":"3","timestamp":"1633921260.0","comment_id":"131102","poster":"noisonnoiton"},{"comment_id":"49678","content":"A is Correct!","timestamp":"1633699860.0","upvote_count":"2","poster":"BillyC"},{"poster":"pra276","timestamp":"1633428360.0","content":"Correct answer is A","upvote_count":"2","comment_id":"19812"},{"content":"Why? I think B is more cost and less durability","comment_id":"17060","timestamp":"1632923220.0","poster":"Pupina","upvote_count":"2"},{"content":"Answer is B","poster":"SivaG","upvote_count":"3","timestamp":"1632133020.0","comment_id":"13018"}],"topic":"1","answers_community":["A (67%)","B (33%)"],"url":"https://www.examtopics.com/discussions/amazon/view/5799-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"D":"Create a single S3 bucket with Requester Pays turned on and ask their customers to use an S3 client instead of an FTP client Create a bucket tor each customer with a Bucket Policy that permits access only to that one customer.","C":"Create an auto-scaling group of FTP servers with a scaling policy to automatically scale-in when minimum network traffic on the auto-scaling group is below a given threshold. Load a central list of ftp users from S3 as part of the user Data startup script on each Instance.","B":"Create a single S3 bucket with Reduced Redundancy Storage turned on and ask their customers to use an S3 client instead of an FTP client Create a bucket for each customer with a Bucket Policy that permits access only to that one customer.","A":"ASK their customers to use an S3 client instead of an FTP client. Create a single S3 bucket Create an IAM user for each customer Put the IAM Users in a Group that has an IAM policy that permits access to sub-directories within the bucket via use of the 'username' Policy variable."},"answer_images":[],"isMC":true,"exam_id":32,"unix_timestamp":1569661980,"answer_description":"Reference:\nhttps://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders- in-an-amazon-s3-bucket/","answer_ET":"A","question_text":"A web design company currently runs several FTP servers that their 250 customers use to upload and download large graphic files They wish to move this system to AWS to make it more scalable, but they wish to maintain customer privacy and Keep costs to a minimum.\nWhat AWS architecture would you recommend?","timestamp":"2019-09-28 11:13:00","question_images":[],"question_id":432,"answer":"A"},{"id":"MjbQUJaIsL3IwVF8hFM8","url":"https://www.examtopics.com/discussions/amazon/view/5175-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"poster":"Smart","timestamp":"1632703140.0","comment_id":"76547","content":"A & D (Invalid): Cross-Region Replication won't support 15-min RTO.\nB (Invalid): KMS Keys are region-specific. \nC (Valid): Issue about data delivery failure, Cloudwatch Logs itself is durable storage that can retain logs indefinitely.","comments":[{"timestamp":"1632874740.0","comment_id":"87539","poster":"sam422","upvote_count":"1","content":"do we have integration of cloud watch logs to kinesis?"},{"timestamp":"1635236160.0","comment_id":"280477","poster":"shammous","upvote_count":"2","content":"Indeed, CloudWatch logs can be retained for up to 10 years and one day!"},{"upvote_count":"2","poster":"DerekKey","comments":[{"timestamp":"1704867060.0","poster":"shammous","upvote_count":"1","content":"You can export logs into an Amazon Glacier from CloudWatch directly","comment_id":"1118224"}],"content":"correction to your B - KMS customer managed keys can be replicated between regions.","timestamp":"1636118640.0","comment_id":"413153"},{"comments":[{"content":"It supports encryption https://docs.aws.amazon.com/firehose/latest/dev/encryption.html","comment_id":"544588","timestamp":"1644500400.0","upvote_count":"2","poster":"RVivek"}],"timestamp":"1637546280.0","comment_id":"483805","upvote_count":"1","poster":"acloudguru","content":"For C, kinese data fire hose does not support encryption."},{"poster":"heany","upvote_count":"2","content":"cloudwatch log store (archive) is 0.03 usd /gb, glacier instant retrieval is 0.004 usd/gb. I don't believe people will use cloudwatch logs for long term ( 7 years ) storage. the cost difference is order of magnitude","comment_id":"644323","timestamp":"1660015080.0"}],"upvote_count":"22"},{"poster":"donathon","timestamp":"1632322440.0","comment_id":"12555","content":"A\nA: Creates an export task, which allows you to efficiently export data from a log group to an Amazon S3 bucket. Separate KMS keys are needed because only 1 key (used by S3) will be shared across to the other region.\nB: The application is in one region, how would it be able to export the logs to another cloudwatch in other region? S3 should be used.\nC: If data delivery to your Amazon S3 bucket fails, Amazon Kinesis Data Firehose retries to deliver data every 5 seconds for up to a maximum period of 24 hours. If the issue continues beyond the 24-hour maximum retention period, it discards the data.\nD: Glacier cannot achieve the 15minute RTO.","comments":[{"comment_id":"13781","upvote_count":"4","timestamp":"1632356640.0","comments":[{"poster":"Shawn1","comment_id":"138634","upvote_count":"3","timestamp":"1633929240.0","content":"the first link says \"Exporting to S3 buckets that are encrypted with AES-256 is supported. Exporting to S3 buckets encrypted with SSE-KMS is not supported.\" Will this make answer A invalid?"}],"content":"https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateExportTask.html\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/\nhttps://aws.amazon.com/blogs/devops/ensuring-security-of-your-code-in-a-cross-regioncross-account-deployment-solution/","poster":"donathon"},{"content":"Exporting to S3 buckets that are encrypted with AES-256 is supported. Exporting to S3 buckets encrypted with SSE-KMS is not supported.","timestamp":"1632391980.0","comments":[{"poster":"AWSPro24","comment_id":"47456","content":"To be clear I think you are pointing out that the CRR bucket will be the one encrypted with KMS because the same-region bucket can't support the export.","upvote_count":"1","timestamp":"1632499560.0"}],"comment_id":"16189","poster":"chaudh","upvote_count":"9"},{"content":"Glacier expedited retrieval can restore within 5 minutes. Did I miss anything here?","comments":[{"timestamp":"1632427020.0","comments":[{"content":"https://docs.aws.amazon.com/amazonglacier/latest/dev/DataEncryption.html\nData at rest stored in S3 Glacier is automatically server-side encrypted using 256-bit Advanced Encryption Standard (AES-256) with keys maintained by AWS. If you prefer to manage your own keys, you can also use client-side encryption before storing data in S3 Glacier.","timestamp":"1632445380.0","poster":"PacoDerek","upvote_count":"5","comment_id":"42744"}],"upvote_count":"2","comment_id":"24542","poster":"examacc","content":"for glacier you will need to do client side encryption as server side encryption in glacier is only with aws managed keys"}],"upvote_count":"4","poster":"Frank1","timestamp":"1632414420.0","comment_id":"22542"},{"comment_id":"98939","content":"Yes it can with expedited option","poster":"JAWS1600","upvote_count":"3","timestamp":"1633083960.0"},{"upvote_count":"5","comment_id":"279827","content":"15 minute RTO can be achieved with Glacier Expedited retrievals (access data in 1 – 5 minutes)","timestamp":"1635221160.0","poster":"shammous"},{"timestamp":"1635489660.0","content":"How 7 years retention clause will be satisfied with A ?\n\nGlacier expedite retrieval can achieve 15 min RTO\nSeems D is better option","poster":"Amitv2706","upvote_count":"4","comment_id":"333920"},{"content":"I think D is correct.\nA: It cannot meet the 15 minutes RPO requirement.\nBecause CreateExportTask costs 5 minutes and cross-region replication costs up to 15 minutes.\nSo A costs up to 5 + 15 = 20 minutes.\n\n\"Replicate objects within 15 minutes — You can use S3 Replication Time Control (S3 RTC) to replicate your data in the same AWS Region or across different Regions in a predictable time frame. S3 RTC replicates 99.99 percent of new objects stored in Amazon S3 within 15 minutes (backed by a service level agreement). \"\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\n\nD: By using expedited retrieval, S3 glacier can achieve the 15 minute RTO and RPO","comments":[{"upvote_count":"1","poster":"sam422","timestamp":"1632774960.0","comment_id":"87503","content":"what is RTO on galcier to restore data, 15 mins??no way"},{"upvote_count":"1","timestamp":"1634466180.0","poster":"deejiw","content":"Even though CRR takes up 15 minutes but that is for a pile of HUGE files. This scenario is just simple logs so A should be fine","comment_id":"196465"},{"upvote_count":"1","content":"The Data Loss Prevention team requires that data at rest must be encrypted using a key that the team controls, rotates, and revokes.\nGlacier Vault cannot modify files, therefore, keys cannot be rotated.","poster":"bad_syntax","comment_id":"414219","timestamp":"1636220340.0"}],"upvote_count":"4","comment_id":"56837","timestamp":"1632624360.0","poster":"exam2019"},{"comment_id":"567108","poster":"BlueGreen","upvote_count":"2","content":"Amazon CloudWatch Log retention – By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group.","timestamp":"1647193980.0"}],"upvote_count":"21"},{"poster":"SkyZeroZx","comment_id":"926306","content":"Selected Answer: C\nBased on the provided information, option C seems to be the most appropriate for meeting the requirements of backing up chat transcripts to a backup region with separate AWS KMS keys for encryption. Here's a breakdown of the options:\n\nA. This option exports chat transcripts from CloudWatch Logs to an S3 bucket using CreateExportTask and enables cross-region replication for backup. However, it does not specify any separate AWS KMS keys for encryption.\n\nB. This option logs chat messages into two different CloudWatch Logs groups in different regions, and both groups export logs to an Amazon Glacier vault with a long-term vault lock policy. While this approach provides redundancy, it does not involve a separate AWS KMS key for the S3 backup.","upvote_count":"1","comments":[{"comment_id":"926307","upvote_count":"1","poster":"SkyZeroZx","timestamp":"1687039980.0","content":"C. This option logs chat messages into CloudWatch Logs and uses a subscription filter to feed the logs into an Amazon Kinesis Data Firehose, which streams them into an S3 bucket in the backup region. Separate AWS KMS keys are specified for both the CloudWatch Logs group and the Kinesis Data Firehose, ensuring encryption in transit and at rest.\n\nD. This option exports chat transcripts from CloudWatch Logs to an Amazon Glacier vault with a 7-year vault lock policy. Glacier cross-region replication is used to mirror chat archives to the backup region. While this option provides backup and long-term retention, it does not involve separate AWS KMS keys for encryption.\n\nTherefore, option C is the most suitable choice as it ensures chat transcripts are logged, encrypted, and backed up to an S3 bucket in the backup region using separate AWS KMS keys for encryption."}],"timestamp":"1687039920.0"},{"comment_id":"685874","comments":[{"upvote_count":"4","timestamp":"1668241860.0","content":"CloudWatch logs CAN be exported to S3 buckets encrypted by AWS KMS.\nIn the link you provided, it literally says in the first paragraph: \"Exporting log data to S3 buckets that are encrypted by AWS KMS is supported.\"\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html#:~:text=Exporting%20log%20data%20to%20S3%20buckets%20that%20are%20encrypted%20by%20AWS%20KMS%20is%20supported.","comment_id":"716561","poster":"et22s"}],"upvote_count":"2","content":"Selected Answer: C\nA is not correct because CW Logs cannot export log data to Amazon S3 buckets that are encrypted by AWS KMS\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html","timestamp":"1664849940.0","poster":"[Removed]"},{"timestamp":"1646580960.0","comment_id":"562100","upvote_count":"2","content":"Selected Answer: C\nI will go with C, using elimination:\nD - in order to create an export will take about 12 hours - \"Log data can take up to 12 hours to become available for export. For near real-time analysis of log data, see Analyzing log data with CloudWatch Logs Insights or Real-time processing of log data with subscriptions instead.\"\nfor A - Lambda time is 5 Minutes + CRR can take up to 15 minutes (or more) \nFor B - KMS keys are regional - so its invalid.\nso the only valid answer is C","poster":"asfsdfsdf"},{"content":"C is correct. https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html Log data can take up to 12 hours to become available for export. For near real-time analysis of log data, see Analyzing log data with CloudWatch Logs Insights or Real-time processing of log data with subscriptions instead. Only C meets the RPO/RTO requirements.","upvote_count":"3","timestamp":"1645897140.0","comment_id":"556875","poster":"johnnsmith"},{"timestamp":"1644715860.0","upvote_count":"2","content":"A is the answer.\nB &D are wrong since The Data Loss Prevention team requires that data at rest must be encrypted using a key that the team controls, rotates, and revokes.\nGlacier Vault cannot modify files, therefore, keys cannot be rotated.","poster":"RVivek","comment_id":"546181"},{"timestamp":"1644504360.0","upvote_count":"1","comment_id":"544625","poster":"RVivek","content":"Answer is A\nB & D Glacier Vault will not allow encryption using KMS amanged keys\nC: If data delivery to your Amazon S3 bucket fails, Amazon Kinesis Data Firehose retries to deliver data every 5 seconds for up to a maximum period of 24 hours. If the issue continues beyond the 24-hour maximum retention period, it discards the data"},{"content":"To Support C\nhttps://docs.aws.amazon.com/firehose/latest/dev/security-best-practices.html\nData at rest and data in transit can be encrypted in Kinesis Data Firehose\nhttps://docs.aws.amazon.com/firehose/latest/dev/encryption.html","timestamp":"1640909820.0","poster":"peddyua","upvote_count":"1","comment_id":"513786"},{"upvote_count":"1","comment_id":"496591","content":"C. The chat application logs each chat message into Amazon CloudWatch Logs. A subscription filter on the CloudWatch Logs group feeds into an Amazon Kinesis Data Firehose which streams the chat messages into an Amazon S3 bucket in the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the Kinesis Data Firehose.","timestamp":"1638945240.0","poster":"cldy"},{"upvote_count":"1","poster":"acloudguru","content":"For C, kinese data fire hose does not support encryption. So C is wrong.","timestamp":"1637546220.0","comment_id":"483804"},{"poster":"student22","timestamp":"1636284240.0","upvote_count":"1","content":"C \n---","comment_id":"455674"},{"comment_id":"449620","timestamp":"1636274820.0","poster":"Bigbearcn","upvote_count":"3","content":"B and D are wrong because CloudWatch logs cannot be exported to glacier directly. It can be exported to S3 and then move to glacier afterwards.\nA is wrong because exporting log data to Amazon S3 buckets that are encrypted by AWS KMS is not supported.\n\nOnly option is C."},{"upvote_count":"1","poster":"Kopa","content":"Im going for C","comment_id":"448853","timestamp":"1636254720.0"},{"timestamp":"1636247160.0","upvote_count":"1","poster":"tgv","content":"CCC\n---","comment_id":"437858"},{"poster":"DerekKey","comment_id":"413177","timestamp":"1636210980.0","content":"A wrong - CreateExportTask can not export to S3 buckets encrypted with SSE-KMS\nB & D wrong - you can not specify a key for Glacier\nC correct","upvote_count":"2"},{"timestamp":"1635967320.0","comment_id":"409748","upvote_count":"2","poster":"WhyIronMan","content":"I'll go with C"},{"timestamp":"1635937380.0","comment_id":"406844","poster":"Akhil254","upvote_count":"1","content":"C COrrect"},{"timestamp":"1635931260.0","content":"C is correct\nA- Does not meet RPO/RTO https://aws.amazon.com/premiumsupport/knowledge-center/s3-\ncrr-replication-time/ \nhttps://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-get-logs-to-export-to-s3/ (also suggest C as the proper approach)\nB, D - Cloudwatch does not support export directly to Glaxier","upvote_count":"2","comment_id":"392165","poster":"Desailly"},{"comment_id":"383690","poster":"tekkart","timestamp":"1635859200.0","content":"D\nThe solution must meet all the requirements \nA: does not satisfy \"durable storage for at least 7 years\"\nB: Not possible to use the same KMS key over 2 Logs groups in 2 regions. Encryption in-flight is respected (KMS key for Logs group), encryption at-rest too (KMS key for S3), plus the RTO and RPO 15 min are probably met but how is not descriptive enough (2 independent regions ? with Route 53 geo-location to direct the users ?)\nC : the real-time of Kinesis satisfies the RPO, looks better than the Lambda, but the 7 year retention criteria is not met. Plus there is a suspiscion that Kinesis cannot copy directly to another region, it would copy to S3 and then S3 would replicate \nD : the 7 year retention criteria is met. The RPO of 15 minutes is met through S3 RTC \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-time-control.html\nThe RTO of 15 minutes is met through S3 Glacier Expedite Retrieval (for small volumes 150MB+). The encryption in-flight of transcript is satisfied with KMS Key for Logs Groups and at rest is performed through KMS Keys for S3 bucket. Therefore D seems to meet all the requirements","upvote_count":"3"},{"comment_id":"381313","poster":"JonSno","content":"https://aws.amazon.com/about-aws/whats-new/2016/11/access-your-amazon-glacier-data-in-minutes-with-new-retrieval-options/\nGlacier expedited retrieval takes between 1-5 mins plus the retention for 7 years does demand glacier","timestamp":"1635846900.0","upvote_count":"2"},{"comment_id":"368934","poster":"zolthar_z","timestamp":"1635764700.0","upvote_count":"1","content":"The answer is C ... \"Note: It may take up to 12 hours for the logs to be available for exporting and the export task itself can take some time. For real-time processing or continuously archiving new data to S3, use subscription filters. You can stream to Amazon Kinesis Data Firehose and set Amazon S3 as the target. For archiving historical data to S3, export your data to Amazon S3.\" source: https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-get-logs-to-export-to-s3/"},{"timestamp":"1635735000.0","content":"A Exporting log data to Amazon S3 buckets that are encrypted by AWS KMS is not supported\nB KMS are region specific\nD Cannot upload Cloudwatch log directly to cannot upload directly to glacier\nC is correct\nCloudwatch logs are retained for 10years\nKinesis exports to another bucket, kept until we delete it.\nExporting to S3 buckets that are encrypted with AES-256 is supported","upvote_count":"5","poster":"Waiweng","comment_id":"345071"},{"timestamp":"1635616020.0","poster":"ExtHo","upvote_count":"2","content":"Very simple in question do we see anything for cost? \ndata must be preserved on durable storage for at least 7 years does S3 not support 7 year storage?\nDoes question mentioned will data automatically deleted after 7 years NO right?\nthen no point to go for Glacier A is correct","comment_id":"335372"},{"content":"will go with C","comment_id":"290699","timestamp":"1635420120.0","poster":"Kian1","upvote_count":"1"},{"upvote_count":"1","content":"I go with D. https://aws.amazon.com/about-aws/whats-new/2016/11/access-your-amazon-glacier-data-in-minutes-with-new-retrieval-options/","timestamp":"1635306780.0","poster":"certainly","comment_id":"289952"},{"timestamp":"1635252120.0","poster":"bnagaraja9099","content":"A is incorrect - Cloud watch S3 export does not support KMS https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html\n\"Exporting log data to Amazon S3 buckets that are encrypted by AWS KMS is not supported.\"\nB is incorrect - KMS is region specific\nD is incorrect - No direct integration between cloud watch and glacier. \nIt leaves C as the best choice available. Data can be in S3 for 7 years","comments":[{"upvote_count":"1","timestamp":"1635703800.0","poster":"01037","content":"Agreed.\n\nRPO and RTO are just distraction.","comment_id":"342483"}],"upvote_count":"1","comment_id":"285669"},{"poster":"Ebi","content":"I go with C","comments":[{"upvote_count":"1","timestamp":"1635460800.0","comment_id":"315333","comments":[{"poster":"DerekKey","content":"you are wrong\n\"an Amazon Kinesis Data Firehose which streams the chat messages into an Amazon S3 bucket in the backup region\"","comment_id":"413176","timestamp":"1636163340.0","upvote_count":"1"}],"content":"C is incorrect as DR region is missing.","poster":"ItsmeP"}],"timestamp":"1635239820.0","upvote_count":"5","comment_id":"282599"},{"upvote_count":"1","timestamp":"1635145140.0","poster":"elf78","content":"C. \n\nThis link gives some ideas that why A and D cannot be the answer. Not sure though: https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-get-logs-to-export-to-s3/ \n\nI was on impression that A is the right answer but considering 12 hours delay that may not be the right candidate.","comment_id":"278256"},{"poster":"rkbala","timestamp":"1635125400.0","content":"D is the correct ans. KMS is global service but keys are specific to region","upvote_count":"1","comment_id":"266190"},{"poster":"cox1960","timestamp":"1635055440.0","content":"A is wrong because of the polling which requires from/to time attributes. So, unless you use a DynamoDB table to keep track of timing and analyse what logs you got, this solution is tricky to implement. This is wrong when you also think that CW logs subscription filters support streaming to Lambda (push) which would be much simpler.","comment_id":"262828","upvote_count":"2"},{"upvote_count":"3","comment_id":"242558","timestamp":"1634870640.0","poster":"T14102020","comments":[{"poster":"arulrajjayaraj","timestamp":"1634986500.0","content":"Ans - C --> Log data can take up to 12 hours to become available for export\n• The API call is CreateExportTask • Not near-real time or real-time… use Logs Subscriptions instead ..","upvote_count":"1","comment_id":"248155"}],"content":"Correct answer is C. Firehouse supports cross region copy logs"},{"timestamp":"1634845620.0","upvote_count":"2","comment_id":"230447","poster":"jackdryan","content":"I'll go with C"},{"poster":"YouYouYou","timestamp":"1634811420.0","content":"B and D are out for sure you can't use the same key across regions and you can export cloud watch logs directly to glacier(tested it doesn't work at least out of the box)\n\nthat leaves A and C and both can work however A doesn't meet the RTO and RPO export every 5 minutes is not added to the replication time expected it has no effect so eventually there is a 15 minutes expected for replication that can go up to two hours which means we won't be able to meet the design requirements with if things go south\n\nthe last option that i checked practically and it works even though i don't love it is.\nC\neverything in that sentence works practically forget about documentations it just works\nyou can use a subscription filter to deliver logs to firehose\nand firehose can deliver to a secondary region bucket you can even create the bucket from firehose interface if doesn't exist. (all tested)\nC is the for me the valid answer \ndisregard everything else it's the only solution that works straight from the interface and have piece to piece integration.","upvote_count":"2","comment_id":"230368"},{"comment_id":"229883","content":"The correct answer is C. Firehouse supports writing streaming data to the S3 bucket you own ( doesn't matter which region the bucket is in). It also support generating a KMS key in another region to encrypt the data before writing to the destination S3 bucket. As regards failure to write the streaming data to S3 from Kinesis Firehose, the data is not lost. Its in CloudWatch logs and you can reprocess it using a new subscription filter against the Log group.","timestamp":"1634801220.0","upvote_count":"4","poster":"Bulti"},{"poster":"Chris_1990","timestamp":"1634757840.0","upvote_count":"3","content":"I would vote for C.\n\nA: Based on the AWS documentation exporting from Cloud Watch Logs to S3 is not possible for S3 buckets, that are encrypted with KMS (\"Exporting log data to Amazon S3 buckets that are encrypted by AWS KMS is not supported.\"): https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\nB & D: I can't find any documentation on directly exporting from CloudWatch Logs to Glacier. It's possible to export to S3 and use lifecycle policies to move the data to Glacier after 30 days, but since lifecycle polies are not mentioned, B & D are wrong for me.\nC: Correct","comment_id":"226823"},{"content":"B and D are invalid, cannot upload directly glacier.\n\nB.w A and C:\nA does not support KMS on S3 while exporting\n\nC is correct\nCloudwatch logs are retained for 10years\nKinesis exports to another bucket, kept until we delete it.\n\nExporting to S3 buckets that are encrypted with AES-256 is supported\n\nLog Retention – By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group, keeping the indefinite retention, or choosing a retention period between 10 years and one day.","poster":"cpd","upvote_count":"1","comment_id":"219415","comments":[{"timestamp":"1634727540.0","poster":"kj07","content":"CloudWatch logs are retained for 15 months.","upvote_count":"2","comment_id":"222865"}],"timestamp":"1634675340.0"},{"poster":"SamAWSExam99","upvote_count":"3","content":"C is correct. \nC. The chat application logs each chat message into Amazon CloudWatch Logs. A subscription filter on the CloudWatch Logs group feeds into an Amazon Kinesis Data Firehose which streams the chat messages into an Amazon S3 bucket in the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the Kinesis Data Firehose.","timestamp":"1634595420.0","comment_id":"208791"},{"upvote_count":"1","content":"Who would like to keep and retain the chats in S3 for 7 years? Glacier is the best solution with expedited retrieval.","comment_id":"203597","poster":"liono","timestamp":"1634577060.0"},{"timestamp":"1634350620.0","upvote_count":"1","poster":"ipindado2020","comment_id":"182701","content":"I go for D"},{"comment_id":"167543","poster":"nameisreqd","content":"I think its C\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\nhttps://aws.amazon.com/premiumsupport/knowledge-center/streaming-cloudwatch-logs/\n\nFirst link confirms kenisis firehose is one of possible tergets for cloudwatch subscription filters\nSecond confirms Cloudwatch subscription filtered data can be streamed cross account\n\nExport has a lag of upto 12 hours so none of the other options meet 15 min RTO in case of regional disaster","upvote_count":"3","comments":[{"poster":"nameisreqd","comment_id":"167546","content":"*Cross account and cross region","upvote_count":"1","timestamp":"1634234520.0"}],"timestamp":"1634204940.0"},{"poster":"TK2019","comment_id":"156588","upvote_count":"2","content":"C seems more correct and appropriate","timestamp":"1634144280.0"},{"comment_id":"149672","timestamp":"1634143980.0","content":"D is correct","poster":"fullaws","upvote_count":"1"},{"comments":[{"comment_id":"138155","upvote_count":"2","content":"D is not right because the CloudWatch Logs group can't be exported directly into an Amazon Glacier, it has to go from S3 to Glacier. But, here in D S3 is not mentioned.","poster":"MKM","timestamp":"1633823160.0"}],"content":"It is D \nThe way the question is worded, is a little bit tricky:\nFirst of all, this is about finding a solution for storing chat transcripts.\nThe RPO/RTO sentence is irrelevant in this scenario. (\"By the way, how can \"chat logs\" help to repair your failed application?).","comment_id":"135897","timestamp":"1633777320.0","upvote_count":"1","poster":"Hypermasterd"},{"poster":"inf","comment_id":"132929","timestamp":"1633739580.0","comments":[{"comments":[{"upvote_count":"1","content":"CRR: \nhttps://aws.amazon.com/about-aws/whats-new/2018/11/s3-glacier-api-simplification/#:~:text=You%20can%20now%20set%20S3,or%20other%20data%20protection%20purposes.&text=This%20includes%20the%20ability%20to,AWS%20GovCloud%20(US)%20Region.\n\nCloudWatch Logs to S3:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html","timestamp":"1634072880.0","poster":"IAmNotLambda","comment_id":"139453"}],"comment_id":"134447","upvote_count":"2","content":"And how do you export cloudwatch logs to Glacier? also how do you configure cross-region replication (Glacier to glacier)? I don't think it's possible ATM","poster":"NikkyDicky","timestamp":"1633765800.0"}],"upvote_count":"1","content":"Answer: D\nNote: Assumption for ALL answers must be that the \"Portal\" has its own recovery process - we have no idea how the portal is built, an no answer discusses the portal recovery or RTO. This question is about RPO.\nA - incorrect - \"must be preserved for 7 years\" - how is that possible without a vault lock? e.g. Glacier? S3 on its own does not meet this requirement. Also, if exports from Cloudwatch are every 5 minutes, a message sent (ie. received by Cloudwatch) it could take up to 20 minutes to replicate - 5 minute delay + 15 minute to replicate. RPO is not met \nB - incorrect - Cloudwatch logs are stored in the region they were generated, plus can't use the same KMS key across regions as they are constrained to a region\nC - incorrect - same as A, no Glacier vault lock, no 7 year retention.\nD - correct - makes sense. Glacier is S3 storage and conforms to the same replication principles - that is use of RTC for sub 15 minute replication which covers RPO. Vault lock for retention, and KMS strategy is viable."},{"comment_id":"132331","content":"C - most likely\nA - cloudwatch export can take up to 12 hours (https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-logs-retrieve-data/#:~:text=Log%20data%20can%20take%20up,PENDING%2C%20PENDING_CANCEL%2C%20or%20RUNNING.)\nother optoions - no export to Glacier","timestamp":"1633730340.0","poster":"NikkyDicky","upvote_count":"5"},{"upvote_count":"1","comment_id":"123778","poster":"chicagomassageseeker","timestamp":"1633698720.0","content":"A is answer. \nEliminate C as Kinesis Firehose is region bound and cannot stream to a bucket in different region.\nEliminate D as Log data can take up to 12 hours to become available for export. Does not meet RTO in this case. (https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html)"},{"comment_id":"121256","content":"D is the best answer, with one caveat I mention at the of my analysis.\nA: Works but does not ensure the 7-year retention requirement. Also, if there is a problem with the Lambda function, it’s possible that some of the chats are not backed up and hence lost. (Obviously depends how this is designed and coded. I’m just pointing out it’s not a slam dunk with this approach.)\nB: Wrong as KMS keys cannot cross regions.\nC: Wrong as Kinesis only works in the same region and also not ensure 7-year retention.\nD Works as long as Glacier expedited retrieval is used (1 - 5 minutes vs 3 -5 hours vs 5 -12 hours). It also addresses the 7-year retention requirement. In addition, It provides the lowest cost option although that is not a requirement for this question.\n\nGiven all that, I would say D is the best answer.","comments":[{"content":"However, there is one caveat – Glacier supporting cross region replication only started on Nov 26, 2018 (https://aws.amazon.com/about-aws/whats-new/2018/11/s3-glacier-api-simplification/). It’s possible that this question is OLDER than that and hence might assume that's not possible, which would then make A the best answer. \n\nI do understand from others that the real exam questions can be OLD and will not alway take into account new features / capabilities.\n\nStill, I would go with D.","poster":"LunchTime","upvote_count":"3","comment_id":"121262","timestamp":"1633664640.0","comments":[{"upvote_count":"1","comments":[{"poster":"LunchTime","content":"Sorry, the answer is \"A\".\nI overlooked my prior point that Kinesis cannot stream data across regions. As per https://aws.amazon.com/kinesis/data-firehose/faqs/:\n\"Q: Can I use a Firehose delivery stream in one region to deliver my data into an Amazon Elasticsearch Service domain VPC destination in a different region?\n\nNo, your Firehose delivery stream and destination Amazon Elasticsearch Service domain need to be in the same region.\"\nThat leaves A as the only possible answer that covers all of the bases.","timestamp":"1634183820.0","comment_id":"163616","upvote_count":"2"}],"content":"I've changed my mind based on the feedback of others.\nC is definitely correct.\nAs indicated by chicagomassageseeker and NikkyDicky with their references, it can take up to 12 hours for CloudWatch logs to be exported to S3 (and hence Glacier with is also really S3). That means B and D are out. The key reference for C being correct is provided by NikkyDicky: https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-logs-retrieve-data/#:~:text=Log%20data%20can%20take%20up,PENDING%2C%20PENDING_CANCEL%2C%20or%20RUNNING.\nIn that link it states that \"for real-time analysis and processing, use subscription filters.\"\nC is the only option that uses subscription filters and hence is the correct answer. Option \"A\" may work but \"C\" is a better solution.","poster":"LunchTime","timestamp":"1634164980.0","comment_id":"163598"}]}],"upvote_count":"1","poster":"LunchTime","timestamp":"1633578720.0"},{"content":"In my understanding, 15min RTO is not the requirements for the log persist solution.\nIt's a requirement for the portal. recover a portal from disaster doesn't need to recover all the logs it generated, right?","poster":"94xychen","upvote_count":"1","comment_id":"112072","timestamp":"1633577220.0"},{"content":"I vote for D\nThere is CRR for glacier as well ..\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/s3-glacier-api-simplification/#:~:text=You%20can%20now%20set%20S3,or%20other%20data%20protection%20purposes.&text=This%20includes%20the%20ability%20to,AWS%20GovCloud%20(US)%20Region.\n\nYou can now set S3 Cross-Region Replication (CRR) policies to directly replicate data into the S3 Glacier storage class in a different AWS Region for backup or other data protection purposes.","comments":[{"poster":"mychiv","timestamp":"1633456560.0","upvote_count":"2","content":"Yes, D is correct\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/s3-glacier-api-simplification/","comment_id":"109399","comments":[{"poster":"pmjcr","upvote_count":"1","timestamp":"1633490460.0","comment_id":"111692","content":"Glacier cannot user Customer managed keys using KMS and server side encryption https://docs.aws.amazon.com/amazonglacier/latest/dev/key-management.html so D cannot be unless the data is encrypted before is stored"}]}],"comment_id":"107389","upvote_count":"3","timestamp":"1633418520.0","poster":"VrushaliD"},{"poster":"ripntear","content":"\"chat conversations must be encrypted in-flight\" - A doesn't mention this. S3 CRR can replicate unencrypted objects. Answer C https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-what-is-isnot-replicated.html","timestamp":"1633281120.0","comment_id":"102209","upvote_count":"2"},{"poster":"NKnab","comments":[{"poster":"Krishna2637","comment_id":"103395","upvote_count":"2","content":"Exporting to S3 buckets that are encrypted with AES-256 is supported. Exporting to S3 buckets encrypted with SSE-KMS is not supported.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html","timestamp":"1633416120.0"}],"comment_id":"102196","upvote_count":"2","content":"A is correct. stores data in both regions. use diff kms keys per region. Looks like some crappy english used when it talks about lambda otherwise makes sense.","timestamp":"1633247400.0"},{"comment_id":"99323","poster":"JohnyGaddar","comments":[{"poster":"IAmNotLambda","timestamp":"1633952820.0","upvote_count":"1","content":"https://aws.amazon.com/about-aws/whats-new/2018/11/s3-glacier-api-simplification/#:~:text=You%20can%20now%20set%20S3,or%20other%20data%20protection%20purposes.&text=This%20includes%20the%20ability%20to,AWS%20GovCloud%20(US)%20Region.\n\nCRR supports Glacier too.","comment_id":"139449"}],"content":"a - seems correct\nb - cannot have same key for different regions\nc - solution does not have a local storage.. only regional storage.\nd - RPO of 15 min not met also there is nothing like glacier cross region replication","timestamp":"1633160880.0","upvote_count":"1"},{"poster":"sam422","comment_id":"87504","timestamp":"1632784080.0","upvote_count":"1","content":"A best suits for this situation"},{"upvote_count":"2","content":"For D; there is nothing such as 'Glacier Cross region replication'. Also, D doesn't export chat messages every 15 minutes or more frequent; which means RPO cannot be met.\nI'm going with A.","timestamp":"1632756660.0","poster":"[Removed]","comment_id":"87442"},{"upvote_count":"2","poster":"jgtran","timestamp":"1632716340.0","content":"I vote D.","comment_id":"82674"},{"timestamp":"1632544620.0","upvote_count":"1","comment_id":"50050","content":"Look like A","poster":"amog"},{"comments":[{"upvote_count":"1","comments":[{"comment_id":"335369","upvote_count":"1","timestamp":"1635564660.0","poster":"ExtHo","content":"\" chat transcripts must be preserved on durable storage for at least\n7 years\" but not mention after 7 year data will deleted so A is good option"}],"comment_id":"180490","content":"i support this. do not get confused with 7year to set answers to glacier. A is the answer.","timestamp":"1634283420.0","poster":"proxyolism"}],"content":"I support answer \"A\".\nB/D: uses Glacier, and it does not meet RTO.\nC: solution does not have a local storage! only regional storage. what if other region fail, then Kinesis can not store more than 24 hours.","upvote_count":"6","poster":"Moon","comment_id":"13871","timestamp":"1632370500.0"},{"upvote_count":"3","timestamp":"1632164700.0","comment_id":"11725","content":"I would go with C","poster":"dpvnme","comments":[{"poster":"awsec2","comments":[{"content":"aws glacier","comment_id":"243814","timestamp":"1634968500.0","upvote_count":"1","poster":"petebear55"}],"comment_id":"12224","timestamp":"1632226140.0","content":"how you will preserve the logs for 7 years ?","upvote_count":"2"}]},{"comments":[{"timestamp":"1632154080.0","poster":"dpvnme","comment_id":"11724","upvote_count":"10","content":"you can't share kms keys across region"},{"timestamp":"1634096880.0","comment_id":"143611","upvote_count":"1","poster":"MultiAZ","content":"Glacier does not support customer keys"}],"content":"b is my choice","timestamp":"1632100800.0","comment_id":"11094","poster":"awsec2","upvote_count":"2"}],"topic":"1","answer_images":[],"answer_description":"","exam_id":32,"timestamp":"2019-09-14 19:24:00","question_text":"A bank is designing an online customer service portal where customers can chat with customer service agents. The portal is required to maintain a 15-minute\nRPO or RTO in case of a regional disaster. Banking regulations require that all customer service chat transcripts must be preserved on durable storage for at least\n7 years, chat conversations must be encrypted in-flight, and transcripts must be encrypted at rest. The Data Loss Prevention team requires that data at rest must be encrypted using a key that the team controls, rotates, and revokes.\nWhich design meets these requirements?","unix_timestamp":1568481840,"question_images":[],"choices":{"B":"The chat application logs each chat message into two different Amazon CloudWatch Logs groups in two different regions, with the same AWS KMS key applied. Both CloudWatch Logs groups are configured to export logs into an Amazon Glacier vault with a 7-year vault lock policy with a KMS key specified.","D":"The chat application logs each chat message into Amazon CloudWatch Logs. The CloudWatch Logs group is configured to export logs into an Amazon Glacier vault with a 7-year vault lock policy. Glacier cross-region replication mirrors chat archives to the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the Amazon Glacier vault.","C":"The chat application logs each chat message into Amazon CloudWatch Logs. A subscription filter on the CloudWatch Logs group feeds into an Amazon Kinesis Data Firehose which streams the chat messages into an Amazon S3 bucket in the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the Kinesis Data Firehose.","A":"The chat application logs each chat message into Amazon CloudWatch Logs. A scheduled AWS Lambda function invokes a CloudWatch Logs CreateExportTask every 5 minutes to export chat transcripts to Amazon S3. The S3 bucket is configured for cross-region replication to the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the S3 bucket."},"answer":"C","answer_ET":"C","answers_community":["C (100%)"],"isMC":true,"question_id":433},{"id":"ntrMdkNGZm5YDfO6IjNu","discussion":[{"comment_id":"926308","poster":"SkyZeroZx","timestamp":"1687040220.0","content":"Selected Answer: D\nD - Via process of elimination. And allows for more security than other options.","upvote_count":"1"},{"poster":"mimadour21698","upvote_count":"1","timestamp":"1684096980.0","content":"Selected Answer: D\nD for sure","comment_id":"897882"},{"comment_id":"715027","poster":"janvandermerwer","upvote_count":"1","timestamp":"1668067620.0","content":"Selected Answer: D\nD - Via process of elimination. And allows for more security than other options.\n\nA , C- whitelist S3 ip range -- seems a bit overkill\nB - Allows access from all IPs in the VPC - Potentially overly permissive."},{"upvote_count":"1","poster":"resnef","content":"Agreed with D too, We cannot use aws:SourceIp for VPCE","comment_id":"712548","timestamp":"1667758320.0"},{"timestamp":"1656555240.0","poster":"TechX","upvote_count":"2","comment_id":"624987","content":"Selected Answer: D\nIt's D, no doubt"},{"timestamp":"1642048920.0","upvote_count":"4","poster":"m0h3n","comments":[{"upvote_count":"1","timestamp":"1643284920.0","poster":"wahlbergusa","content":"Agreed.","comment_id":"533707"}],"comment_id":"522630","content":"Ans D."}],"answer_ET":"D","question_text":"A company currently runs a secure application on Amazon EC2 that takes files from on-premises locations through AWS Direct Connect, processes them, and uploads them to a single Amazon S3 bucket. The application uses HTTPS for encryption in transit to Amazon S3, and S3 server-side encryption to encrypt at rest.\nWhich of the following changes should the Solutions Architect recommend to make this solution more secure without impeding application's performance?","exam_id":32,"choices":{"C":"Add a NAT gateway. Update the security groups on the EC2 instance to allow access to and from the S3 IP range only. Configure an S3 bucket policy that allows communication from the source public IP address of the on-premises network only.","B":"Add a VPC endpoint. Configure endpoint policies on the VPC endpoint to allow access to the required Amazon S3 buckets only. Implement an S3 bucket policy that allows communication from the VPC's source IP range only.","D":"Add a VPC endpoint. Configure endpoint policies on the VPC endpoint to allow access to the required S3 buckets only. Implement an S3 bucket policy that allows communication from the VPC endpoint only.","A":"Add a NAT gateway. Update the security groups on the EC2 instance to allow access to and from the S3 IP range only. Configure an S3 bucket policy that allows communication from the NAT gateway's Elastic IP address only."},"answer":"D","answers_community":["D (100%)"],"topic":"1","answer_images":[],"question_id":434,"question_images":[],"timestamp":"2022-01-13 05:42:00","isMC":true,"unix_timestamp":1642048920,"url":"https://www.examtopics.com/discussions/amazon/view/69943-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":""},{"id":"ZvUsvki4HATAeq8wHHNW","unix_timestamp":1568801700,"answers_community":["C (100%)"],"isMC":true,"question_text":"As a part of building large applications in the AWS Cloud, the Solutions Architect is required to implement the perimeter security protection. Applications running on AWS have the following endpoints:\n✑ Application Load Balancer\n✑ Amazon API Gateway regional endpoint\n✑ Elastic IP address-based EC2 instances.\n✑ Amazon S3 hosted websites.\n✑ Classic Load Balancer\nThe Solutions Architect must design a solution to protect all of the listed web front ends and provide the following security capabilities:\n✑ DDoS protection\n✑ SQL injection protection\n✑ IP address whitelist/blacklist\n✑ HTTP flood protection\n✑ Bad bot scraper protection\nHow should the Solutions Architect design the solution?","discussion":[{"content":"C\nAll AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.","comment_id":"13783","poster":"donathon","timestamp":"1632712740.0","upvote_count":"26"},{"poster":"Lee","content":"C is the correct answer. CloudFront also can solve a partial DDoS attack.","upvote_count":"13","comment_id":"11591","timestamp":"1632122400.0","comments":[{"comment_id":"11730","timestamp":"1632366300.0","upvote_count":"4","poster":"dpvnme","content":"yep, cloudfront helps"}]},{"timestamp":"1687040340.0","poster":"SkyZeroZx","content":"Selected Answer: C\nDDoS protection == AWS Shield \nSQL injection protection == AWS WAF\nIP address whitelist/blacklist == AWS WAF\nHTTP flood protection == AWS Shield","upvote_count":"1","comment_id":"926309"},{"timestamp":"1658793780.0","content":"A vs. C\nC","poster":"hilft","comment_id":"637004","upvote_count":"1"},{"comment_id":"596143","content":"Selected Answer: C\nC. Services such as Cloudfront, WAF and Shield Advanced address the security and anti-DDoS protection required for the architecture specifications of the given company.","upvote_count":"1","timestamp":"1651512300.0","poster":"tartarus23"},{"timestamp":"1641550200.0","content":"It is true that a CLB is not supported with WAF but it does indeed support a CloudFront distribution. Hence C is the correct answer.","poster":"lucesarano","comment_id":"518882","upvote_count":"2"},{"timestamp":"1638649560.0","comment_id":"493944","content":"C is right and this is simple question","upvote_count":"2","poster":"AzureDP900"},{"upvote_count":"2","timestamp":"1635980220.0","content":"I'll go with C","poster":"WhyIronMan","comment_id":"409778"},{"upvote_count":"2","comment_id":"345076","content":"It's C","poster":"Waiweng","timestamp":"1635787860.0"},{"content":"going with C","comment_id":"290735","upvote_count":"2","poster":"Kian1","timestamp":"1635751440.0"},{"upvote_count":"4","comment_id":"287481","content":"Answer is C","poster":"Ebi","timestamp":"1635685560.0"},{"timestamp":"1635682560.0","comment_id":"268204","upvote_count":"2","poster":"sanjaym","content":"I'll go with C."},{"content":"Correct answer is C. All of features","timestamp":"1634968440.0","upvote_count":"1","poster":"T14102020","comment_id":"242568"},{"upvote_count":"3","poster":"jackdryan","timestamp":"1634821440.0","content":"I'll go with C","comment_id":"230057"},{"content":"C is correct. Although A seems right, not protecting the S3 hosted website using CloudFront and AWS WAF with it is not a good security posture for that web endpoint. So C is the correct answer.","timestamp":"1634269560.0","poster":"Bulti","comment_id":"229889","upvote_count":"4"},{"upvote_count":"1","comment_id":"226825","poster":"lostri","timestamp":"1633982460.0","content":"Answer is C because CLB does not support WAF"},{"comment_id":"193426","upvote_count":"4","timestamp":"1633895460.0","comments":[{"comment_id":"212544","timestamp":"1633956060.0","content":"AWS Lambda can check the third-party IP reputation lists hourly for new ranges to block.","poster":"iamgk","upvote_count":"3"},{"upvote_count":"2","poster":"wassb","content":"There are several benefits to using Lambda (Lambda@Edge) for authorization operations like filtering out unauthorized requests before they reach your origin infrastructure.","timestamp":"1665250320.0","comment_id":"689545"}],"content":"What exactly are you going to use the Lambda for in answer C? Answer seems OK, but Lambda does not fit...","poster":"pddddd"},{"comment_id":"181990","upvote_count":"1","timestamp":"1633752780.0","poster":"df1228","content":"I support answer \"C\"."},{"timestamp":"1633544880.0","comment_id":"179285","content":"C it is","poster":"friendswades","upvote_count":"2"},{"timestamp":"1633520280.0","upvote_count":"2","content":"C is correct, CLB need cloudfront for WAF","comment_id":"149678","poster":"fullaws"},{"timestamp":"1633400760.0","poster":"NikkyDicky","comment_id":"134452","content":"C make sense","upvote_count":"2"},{"upvote_count":"1","comment_id":"102219","comments":[{"content":"There is no such limitation I think. https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-connection-fails/","poster":"Krishna2637","comment_id":"103596","timestamp":"1633223700.0","upvote_count":"2"},{"upvote_count":"2","content":"Classic Load Balancers can absolutely sit behind CloudFront. The following is an example of that: https://blog.webnersolutions.com/configuring-web-distribution-and-load-balancer-in-aws/","timestamp":"1633230000.0","comment_id":"121284","poster":"LunchTime"},{"comment_id":"439897","timestamp":"1636172520.0","poster":"tonikus","content":"that statement is wrong. It can.","upvote_count":"1"}],"content":"A - Cloudfront cannot use Classic LB as origin","poster":"ripntear","timestamp":"1633179180.0"},{"poster":"kinetic1g","content":"C. Deploy Amazon CloudFront in front of all the endpoints. Deploy AWS WAF and AWS Shield Advanced. Add AWS WAF rules to enforce the company's requirements. Use AWS Lambda to automate and enhance the security posture.","comment_id":"75230","timestamp":"1633105320.0","upvote_count":"2"},{"content":"I don't think you can put Cloudfront directly in front of an EIP. \nTherefore I would go with A. C seems to be a 'gotcha' answer.","poster":"nil12","upvote_count":"3","timestamp":"1632783900.0","comment_id":"51561","comments":[{"timestamp":"1632815700.0","content":"https://aws.amazon.com/blogs/networking-and-content-delivery/dynamic-whole-site-delivery-with-amazon-cloudfront/","poster":"Asds","comment_id":"61899","upvote_count":"1"},{"comment_id":"121290","poster":"LunchTime","upvote_count":"1","content":"Yes you can. Classic Load Balancers can absolutely sit behind CloudFront. The following is an example of that: https://blog.webnersolutions.com/configuring-web-distribution-and-load-balancer-in-aws/","timestamp":"1633370040.0"}]},{"poster":"amog","comment_id":"51064","timestamp":"1632778260.0","upvote_count":"3","content":"C is better"},{"upvote_count":"4","content":"WAF is not supported in Claasic load balancer. So We need to put CloudFront in front of the classic LB. So the answer is C.","timestamp":"1632765480.0","comment_id":"34956","poster":"arunkumar"},{"comment_id":"13870","timestamp":"1632756660.0","content":"I support answer \"C\".","upvote_count":"5","poster":"Moon"}],"topic":"1","answer_images":[],"question_id":435,"url":"https://www.examtopics.com/discussions/amazon/view/5371-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"choices":{"C":"Deploy Amazon CloudFront in front of all the endpoints. Deploy AWS WAF and AWS Shield Advanced. Add AWS WAF rules to enforce the company's requirements. Use AWS Lambda to automate and enhance the security posture.","A":"Deploy AWS WAF and AWS Shield Advanced on all web endpoints. Add AWS WAF rules to enforce the company's requirements.","B":"Deploy Amazon CloudFront in front of all the endpoints. The CloudFront distribution provides perimeter protection. Add AWS Lambda-based automation to provide additional security.","D":"Secure the endpoints by using network ACLs and security groups and adding rules to enforce the company's requirements. Use AWS Lambda to automatically update the rules."},"answer":"C","answer_description":"","answer_ET":"C","exam_id":32,"timestamp":"2019-09-18 12:15:00"}],"exam":{"name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"provider":"Amazon","isImplemented":true,"id":32,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019},"currentPage":87},"__N_SSP":true}