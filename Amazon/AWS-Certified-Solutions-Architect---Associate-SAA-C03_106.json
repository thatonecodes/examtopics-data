{"pageProps":{"questions":[{"id":"PeWF2NRZ3IHEjSAbndDw","answers_community":["C (100%)"],"choices":{"C":"Use the memory optimized instance family for both the application and the database.","D":"Use the high performance computing (HPC) optimized instance family for the application. Use the memory optimized instance family for the database.","A":"Use the compute optimized instance family for the application. Use the memory optimized instance family for the database.","B":"Use the storage optimized instance family for both the application and the database."},"answer":"C","question_text":"A company's SAP application has a backend SQL Server database in an on-premises environment. The company wants to migrate its on-premises application and database server to AWS. The company needs an instance type that meets the high demands of its SAP database. On-premises performance data shows that both the SAP application and the database have high memory utilization.\n\nWhich solution will meet these requirements?","exam_id":31,"answer_description":"","question_id":526,"unix_timestamp":1691277900,"isMC":true,"answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/117442-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"C","timestamp":"2023-08-06 01:25:00","topic":"1","discussion":[{"poster":"Guru4Cloud","upvote_count":"16","timestamp":"1724261700.0","content":"Selected Answer: C\nSince both the app and database have high memory needs, the memory optimized family like R5 instances meet those requirements well.\nUsing the same instance family simplifies management and operations, rather than mixing instance types.\nCompute optimized instances may not provide enough memory for the SAP app's needs.\nStorage optimized is overkill for the database's compute and memory needs.\nHPC is overprovisioned for the SAP app.","comment_id":"986790"},{"upvote_count":"5","poster":"manOfThePeople","comment_id":"995367","timestamp":"1725126420.0","content":"High memory utilization = memory optimized.\nC is the answer"},{"poster":"LeonSauveterre","comment_id":"1323372","content":"Selected Answer: C\nWhy D is incorrect: HPC family is geared toward compute-heavy workloads such as simulations or scientific calculations, not appropriate for a memory-intensive application like SAP.","timestamp":"1733635380.0","upvote_count":"1"},{"timestamp":"1732086600.0","upvote_count":"3","comment_id":"1075222","content":"Selected Answer: C\nUse the memory optimized instance family for both the application and the database","poster":"TariqKipkemei"},{"content":"Selected Answer: C\nI thyink its C","poster":"mrsoa","timestamp":"1722900300.0","upvote_count":"3","comment_id":"973425"}]},{"id":"uULjxO6PsNsYElJxcNsk","url":"https://www.examtopics.com/discussions/amazon/view/116983-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["A (100%)"],"answer_ET":"A","choices":{"A":"Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private subnets. Add to the endpoint a security group that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets.","D":"Implement a gateway endpoint for Amazon SQS. Add a NAT gateway to the private subnets. Attach an IAM role to the EC2 instances that allows access to the SQS queue.","C":"Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach an Amazon SQS access policy to the interface VPC endpoint that allows requests from only a specified VPC endpoint.","B":"Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach to the interface endpoint a VPC endpoint policy that allows access from the EC2 instances that are in the private subnets."},"question_text":"A company runs an application in a VPC with public and private subnets. The VPC extends across multiple Availability Zones. The application runs on Amazon EC2 instances in private subnets. The application uses an Amazon Simple Queue Service (Amazon SQS) queue.\n\nA solutions architect needs to design a secure solution to establish a connection between the EC2 instances and the SQS queue.\n\nWhich solution will meet these requirements?","answer_description":"","discussion":[{"poster":"Guru4Cloud","content":"Selected Answer: A\nAn interface VPC endpoint is a private way to connect to AWS services without having to expose your VPC to the public internet. This is the most secure way to connect to Amazon SQS from the private subnets.\nConfiguring the endpoint to use the private subnets ensures that the traffic between the EC2 instances and the SQS queue is only within the VPC. This helps to protect the traffic from being intercepted by a malicious actor.\nAdding a security group to the endpoint that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets further restricts the traffic to only the authorized sources. This helps to prevent unauthorized access to the SQS queue.","timestamp":"1708543680.0","comment_id":"986785","upvote_count":"11"},{"upvote_count":"6","poster":"Bmaster","comment_id":"969049","content":"A is correct.\n\nB,C: 'Configuring endpoints to use public subnets' --> Invalid\nD: No Gateway Endpoint for SQS.","timestamp":"1706796600.0"},{"upvote_count":"2","comments":[{"poster":"awsgeek75","upvote_count":"2","content":"Sorry, the link is wrong for A. Please ignore it!","comment_id":"1126998","timestamp":"1721421120.0"}],"timestamp":"1720715760.0","poster":"awsgeek75","content":"Selected Answer: A\nBC are using public subnets so not useful for security\nD uses gateway endpoint which is not useful to connect to SQS\nA: https://docs.aws.amazon.com/vpc/latest/privatelink/aws-services-privatelink-support.html","comment_id":"1120068"},{"content":"A seems the most suitable,\nbut security group can't add to the endpoint derectly, right?","upvote_count":"2","comment_id":"1095209","poster":"ShawnTang","timestamp":"1718254020.0"},{"comment_id":"1075229","timestamp":"1716182640.0","upvote_count":"2","poster":"TariqKipkemei","content":"Selected Answer: A\nAnswer is A"},{"upvote_count":"2","poster":"TariqKipkemei","timestamp":"1716182580.0","comment_id":"1075228","content":"Interface endpoints enable connectivity to services over AWS PrivateLink. It is a collection of one or more elastic network interfaces with a private IP address that serves as an entry point for traffic destined to a supported service. \nImplement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private subnets. Add to the endpoint a security group that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets."},{"poster":"potomac","upvote_count":"2","timestamp":"1714252200.0","content":"A is correct","comment_id":"1055839"},{"timestamp":"1706975460.0","poster":"mrsoa","comment_id":"971168","content":"Selected Answer: A\nI think its A","upvote_count":"2"}],"answer":"A","question_images":[],"question_id":527,"exam_id":31,"timestamp":"2023-08-01 14:10:00","isMC":true,"unix_timestamp":1690891800,"answer_images":[],"topic":"1"},{"id":"K6J4D4WiWNQoLLlsOOM2","question_id":528,"url":"https://www.examtopics.com/discussions/amazon/view/117434-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"D":"Create an IAM user in the AWS CloudFormation template that has the required permissions to read and write from the DynamoDB tables. Use the GetAtt function to retrieve the access and secret keys, and pass them to the application instances through the user data.","A":"Create an IAM role to read the DynamoDB tables. Associate the role with the application instances by referencing an instance profile.","B":"Create an IAM role that has the required permissions to read and write from the DynamoDB tables. Add the role to the EC2 instance profile, and associate the instance profile with the application instances.","C":"Use the parameter section in the AWS CloudFormation template to have the user input access and secret keys from an already-created IAM user that has the required permissions to read and write from the DynamoDB tables."},"discussion":[{"timestamp":"1721826240.0","poster":"upliftinghut","upvote_count":"3","comment_id":"1130762","content":"Selected Answer: B\nbest practice is using IAM role for database access. From app to DB => need both read & write, only B meets these 2"},{"comment_id":"1111626","content":"Selected Answer: B\nApplication \"stores and retrieves\" data in DynamoDB while A grants only access \"to read\".","upvote_count":"3","poster":"pentium75","timestamp":"1719892380.0"},{"comment_id":"1047790","content":"Selected Answer: B\nB is correct, A total wrong because \"read the DynamoDB tables\", so what about write in database.","timestamp":"1713521520.0","upvote_count":"4","poster":"Nisarg2121"},{"upvote_count":"2","content":"question says: ...application tier stores and retrieves user data in Amazon DynamoDB tables... so it needs read and write access\nA) is only read access \nB) seems to be the right answer","comment_id":"987628","timestamp":"1708628580.0","poster":"darekw"},{"upvote_count":"3","content":"Selected Answer: B\nOption B is the correct approach to meet the requirements:\n\nCreate an IAM role with permissions to access DynamoDB\nAdd the IAM role to an EC2 Instance Profile\nAssociate the Instance Profile with the application EC2 instances\nThis allows the instances to assume the IAM role to obtain temporary credentials to access DynamoDB.","comment_id":"986782","timestamp":"1708543200.0","poster":"Guru4Cloud"},{"comment_id":"982659","upvote_count":"3","poster":"anibinaadi","content":"Explanation. Both A and B seems suitable. But Option A is incorrect because it says “Associate the role with the application instances by referencing an instance profile”. Which just only a Part of the solution.\nIn API/AWS CLI following steps are required to complete the Role-> instance profile association-> to instance.\n1. Create an IAM Role\n2. add-role-to-instance-profile (aws iam add-role-to-instance-profile --role-name S3Access --instance-profile-name Webserver)\n3. associate-iam-instance-profile (aws ec2 associate-iam-instance-profile --instance-id i-123456789abcde123 --iam-instance-profile Name=admin-role)\nhence Option B is correct.","timestamp":"1708100040.0"},{"timestamp":"1707630480.0","comment_id":"978256","content":"Selected Answer: B\nWhy \"No read and write\" ? The question clearly states that application tier STORE and RETRIEVE the data from DynamoDB. Which means write and read... I think answer should be B","poster":"DannyKang5649","upvote_count":"3"},{"comment_id":"976993","timestamp":"1707516240.0","upvote_count":"2","content":"Selected Answer: B\nhttps://www.examtopics.com/discussions/amazon/view/80755-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"xyb"},{"poster":"Ale1973","upvote_count":"2","comment_id":"976651","content":"Selected Answer: B\nMy rationl: Option A is wrong because the scenario says \"stores and retrieves user data in Amazon DynamoDB tables\", STORES and RETRIVE, if you set a role to READ, you can write on DinamoDB database","timestamp":"1707489660.0"},{"content":"Selected Answer: A\nAAAAAAAAA","upvote_count":"2","timestamp":"1707241740.0","poster":"mrsoa","comment_id":"973987","comments":[]},{"timestamp":"1707158460.0","poster":"kangho","comments":[],"content":"Selected Answer: A\nA is correct","upvote_count":"1","comment_id":"973179"}],"isMC":true,"topic":"1","question_images":[],"answer":"B","unix_timestamp":1691253660,"timestamp":"2023-08-05 18:41:00","answers_community":["B (87%)","13%"],"question_text":"A solutions architect is using an AWS CloudFormation template to deploy a three-tier web application. The web application consists of a web tier and an application tier that stores and retrieves user data in Amazon DynamoDB tables. The web and application tiers are hosted on Amazon EC2 instances, and the database tier is not publicly accessible. The application EC2 instances need to access the DynamoDB tables without exposing API credentials in the template.\n\nWhat should the solutions architect do to meet these requirements?","exam_id":31,"answer_ET":"B","answer_description":"","answer_images":[]},{"id":"VKiNSvL3OGtDf5Q2JxeF","url":"https://www.examtopics.com/discussions/amazon/view/117344-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1691155800,"answers_community":["B (79%)","A (21%)"],"topic":"1","question_images":[],"answer":"B","answer_ET":"B","isMC":true,"discussion":[{"poster":"Guru4Cloud","comment_id":"986771","upvote_count":"18","content":"Selected Answer: B\nOption B is the correct solution that meets the requirements:\n\nUse Amazon EMR to process the semi-structured data in Amazon S3. EMR provides a managed Hadoop framework optimized for processing large datasets in S3.\nEMR supports parallel data processing across multiple nodes to speed up the processing.\nEMR can integrate directly with Amazon Redshift using the EMR-Redshift integration. This allows querying the Redshift data from EMR and joining it with the S3 data.\nThis enables enriching the semi-structured S3 data with the information stored in Redshift","timestamp":"1708542780.0"},{"poster":"zjcorpuz","comment_id":"972197","content":"By combining AWS Glue and Amazon Redshift, you can process the semistructured data in parallel using Glue ETL jobs and then store the processed and enriched data in a structured format in Amazon Redshift. This approach allows you to perform complex analytics efficiently and at scale.","upvote_count":"9","timestamp":"1707060600.0"},{"timestamp":"1721829600.0","content":"Selected Answer: B\nD: not relevant, data is semistructured and Glue is more batch than stream data\nA: not correct, Athena is for querying data\nB & C look ok but C is out => redundant with Kinesis data stream; EMR already processed data as input into Redshift for parallel processing\n\nOnly B is most logical","upvote_count":"4","comment_id":"1130837","poster":"upliftinghut"},{"timestamp":"1721421600.0","upvote_count":"3","comment_id":"1127000","poster":"awsgeek75","content":"Selected Answer: B\nKey requirement: parallel data processing\nparallel data processing is EMR (Kind of Apache Hadoop) so it only leave B and C\nC is Kinesis to Redshift which is pointless logic here\nB EMR for S3 and EMR for Redshift gives maximum parallel processing here"},{"comment_id":"1111629","timestamp":"1719893160.0","upvote_count":"7","content":"Selected Answer: B\nA has a pitfall, \"use Amazon Athena to PROCESS the data\". With Athena you can query, not process, data. \nC is wrong because Kinesis has no place here.\nD is wrong because it does not process the Redshift data, and Glue does ETL, not analyze\n\nThus it's B. EMR can use semi-structured data from from S3 and structured data from Redshift and is ideal for \"parallel data processing\" of \"large amounts\" of data.","poster":"pentium75"},{"upvote_count":"3","timestamp":"1718272440.0","content":"Selected Answer: B\nlarge amount of data + parallel data processing = EMR","comment_id":"1095428","poster":"aws94"},{"poster":"[Removed]","timestamp":"1718065920.0","upvote_count":"1","comments":[{"poster":"pentium75","comment_id":"1111630","content":"Y, but A says \"process\", not \"query\" data with Athena.","timestamp":"1719893220.0","upvote_count":"2"}],"content":"Selected Answer: A\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL.","comment_id":"1093016"},{"timestamp":"1717588680.0","poster":"SHAAHIBHUSHANAWS","upvote_count":"1","content":"Selected Answer: D\n\nGlue use apache pyspark cluster for parallel processing. EMR or Glue are possible options. Glue is serverless so better use this plus pyspark is in memory parallel processing.","comment_id":"1088543"},{"content":"i think a is correct\nsemistructured data ==> Athena","comment_id":"1081975","timestamp":"1716840180.0","poster":"aragornfsm","upvote_count":"1","comments":[{"comment_id":"1111631","upvote_count":"2","content":"\"Hadoop [as used by EMR] helps you turn petabytes of un-structured or semi-structured data into useful insights\"\n\nhttps://aws.amazon.com/emr/features/hadoop/","timestamp":"1719893280.0","poster":"pentium75"}]},{"timestamp":"1716660300.0","poster":"riyasara","comment_id":"1080301","upvote_count":"3","content":"Athena is not designed for parallel data processing. So it's B"},{"upvote_count":"1","timestamp":"1716183660.0","content":"Selected Answer: A\nAnswer is A","poster":"TariqKipkemei","comment_id":"1075234"},{"content":"Of course EMR can access S3\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html","comment_id":"1111632","poster":"pentium75","upvote_count":"2","timestamp":"1719893340.0"},{"upvote_count":"2","comment_id":"1072275","poster":"bogobob","timestamp":"1715841840.0","content":"Selected Answer: B\nFor those answering A, AWS Glue can directly query S3, it can't use Athena as a source of data. The questions say the Redshift data should be user to \"enrich\" which means thats the redshift data needs to be \"added\" to the s3 data. A doesn't allow that."},{"content":"Selected Answer: B\nChoose option B.\nOption A is not correct. Amazon Athena is suitable for querying data directly from S3 using SQL and allows parallel processing of S3 data.\nAWS Glue can be used for data preparation and enrichment but might not directly integrate with Amazon Redshift for enrichment.","comment_id":"1072271","poster":"hungta","upvote_count":"2","timestamp":"1715840940.0"},{"comment_id":"1064009","content":"Selected Answer: A\nAthena and Redshift both do SQL query","timestamp":"1715003100.0","upvote_count":"1","poster":"potomac"},{"upvote_count":"4","comments":[{"content":"\"Hadoop helps you turn petabytes of un-structured or semi-structured data into useful insights about your applications or users.\"\n\nhttps://aws.amazon.com/emr/features/hadoop/?nc1=h_ls","poster":"pentium75","timestamp":"1719892740.0","upvote_count":"2","comment_id":"1111628"}],"timestamp":"1712489580.0","poster":"Sab123","content":"Selected Answer: A\nsemi-structure supported by Athena not by EMR","comment_id":"1027283"},{"poster":"JKevin778","upvote_count":"1","content":"Selected Answer: A\nathena for s3","comment_id":"1019355","timestamp":"1711602780.0"},{"comment_id":"991637","timestamp":"1709070420.0","upvote_count":"3","content":"Selected Answer: B\nEMR Works best for Analytics based solutions.","poster":"BrijMohan08"},{"upvote_count":"4","comment_id":"978286","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/architecture/reduce-archive-cost-with-serverless-data-archiving/","timestamp":"1707635880.0","poster":"ukivanlamlpi"}],"exam_id":31,"answer_images":[],"question_id":529,"timestamp":"2023-08-04 15:30:00","answer_description":"","question_text":"A solutions architect manages an analytics application. The application stores large amounts of semistructured data in an Amazon S3 bucket. The solutions architect wants to use parallel data processing to process the data more quickly. The solutions architect also wants to use information that is stored in an Amazon Redshift database to enrich the data.\n\nWhich solution will meet these requirements?","choices":{"B":"Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to enrich the S3 data.","C":"Use Amazon EMR to process the S3 data. Use Amazon Kinesis Data Streams to move the S3 data into Amazon Redshift so that the data can be enriched.","A":"Use Amazon Athena to process the S3 data. Use AWS Glue with the Amazon Redshift data to enrich the S3 data.","D":"Use AWS Glue to process the S3 data. Use AWS Lake Formation with the Amazon Redshift data to enrich the S3 data."}},{"id":"73gpIZVU68lXFHO2Cgur","timestamp":"2023-08-02 07:17:00","choices":{"C":"Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter-VPC communication.","B":"Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.","A":"Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.","D":"Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication."},"discussion":[{"timestamp":"1722575820.0","comment_id":"969709","upvote_count":"5","content":"Selected Answer: C\nC is the correct answer. \n\nVPC peering is the most cost-effective way to connect two VPCs within the same region and AWS account. There are no additional charges for VPC peering beyond standard data transfer rates.\n\nTransit Gateway and VPN add additional hourly and data processing charges that are not necessary for simple VPC peering.\n\nDirect Connect provides dedicated network connectivity, but is overkill for the relatively low inter-VPC data transfer needs described here. It has high fixed costs plus data transfer rates.\n\nFor occasional inter-VPC communication of moderate data volumes within the same region and account, VPC peering is the most cost-effective solution. It provides simple private connectivity without transfer charges or network appliances.","poster":"luiscc"},{"upvote_count":"5","content":"Selected Answer: C\nThe key reasons are:\n\nVPC peering provides private connectivity between VPCs without using public IP space.\nData transferred between peered VPCs is free as long as they are in the same region.\n500 GB/month inter-VPC data transfer fits within peering free tier.\nTransit Gateway (Option A) incurs hourly charges plus data transfer fees. More costly than peering.\nSite-to-Site VPN (Option B) incurs hourly charges and data transfer fees. More expensive than peering.\nDirect Connect (Option D) has high hourly charges and would be overkill for this use case.","timestamp":"1724259840.0","poster":"Guru4Cloud","comment_id":"986756"},{"comment_id":"1075237","content":"Selected Answer: C\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different Regions (also known as an inter-Region VPC peering connection).\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html#:~:text=A-,VPC%20peering,-connection%20is%20a","poster":"TariqKipkemei","upvote_count":"3","timestamp":"1732088760.0"},{"comment_id":"991640","poster":"BrijMohan08","content":"Selected Answer: C\nTransit Gateway network peering.\nVPC Peering to peer 2 or more VPC in the same region.","timestamp":"1724788260.0","upvote_count":"4"},{"comment_id":"971276","upvote_count":"2","poster":"mrsoa","timestamp":"1722700620.0","content":"Selected Answer: C\nVPC peering is the most cost-effective solution"},{"poster":"Deepakin96","upvote_count":"2","timestamp":"1722676800.0","content":"Selected Answer: C\nCommunicating with two VPC in same account = VPC Peering","comment_id":"970935"}],"question_text":"A company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network traffic between these VPCs. Approximately 500 GB of data transfer will occur between the VPCs each month.\n\nWhat is the MOST cost-effective solution to connect these VPCs?","question_id":530,"answer":"C","answer_ET":"C","topic":"1","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/117053-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["C (100%)"],"isMC":true,"question_images":[],"answer_description":"","answer_images":[],"unix_timestamp":1690953420}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isImplemented":true,"isBeta":false,"isMCOnly":true,"provider":"Amazon","id":31,"name":"AWS Certified Solutions Architect - Associate SAA-C03"},"currentPage":106},"__N_SSP":true}