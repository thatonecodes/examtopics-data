{"pageProps":{"questions":[{"id":"CLys2GKa3AiINnJwIBkw","topic":"1","timestamp":"2019-09-06 03:20:00","answer_images":[],"question_id":6,"question_text":"A Redshift data warehouse has different user teams that need to query the same table with very different query types. These user teams are experiencing poor performance.\nWhich action improves performance for the user teams in this situation?","answer":"D","isMC":true,"question_images":[],"unix_timestamp":1567732800,"discussion":[{"content":"D is correct","timestamp":"1632552300.0","comment_id":"12072","poster":"gonda","upvote_count":"5"},{"comment_id":"116257","upvote_count":"1","timestamp":"1635992700.0","poster":"guruguru","content":"D is my answer. Different user have different query types, could be long or short. WLM should be use to allocate different type of queue to run the jobs, either automatically or manually. Hopping queue is one of the manual option to tune the queue."},{"upvote_count":"2","comment_id":"100351","poster":"Corram","timestamp":"1635982380.0","content":"C in my opinion.\nNot A - Non-materialized views won't magically make your query faster.\nNot B - \"per team\" doesn't make sense, there could be only one interleaved sort key that all use. Also, this is only feasible in case there are few teams as adding many columns to an interleaved sort key degrades performance.\nNot D - query hopping only affects execution order of querys, not their performance.\nC - admittedly, this is brute force and I wouldn't recommend it, but you can't argue that it actually improves performance for the user teams."},{"comments":[{"comment_id":"82376","content":"Also, too many interleaved sort keys will be a drag on performance. They need frequent VACCUM. Adding too many interleaved keys is an anti pattern.","upvote_count":"1","timestamp":"1635848040.0","poster":"srirampc"}],"comment_id":"82371","upvote_count":"3","content":"answer is D. It cannot be B because, adding interleaved sort key per team is not scalable with operational overhead. A new team comes with a new query type, interleaved sort key cannot be altered once created. https://docs.aws.amazon.com/redshift/latest/dg/r_ALTER_TABLE.html\nWLM does not have such an issue.","timestamp":"1635776460.0","poster":"srirampc"},{"content":"Answer : B\nNot A – Its impractical to create as many view as the query types.\nNot C – Its impractical to create as many copies of the table as the user teams\nNot D – Unless the table is tuned for performance, hoping queues will become more frequent resulting in degraded performance. Associating one queue per user group also won’t guarantee improved performance if the table is filtered on a column that it is cannot be sorted on\nAnswer is B- An interleaved sort gives equal weight to each column, or subset of columns, in the sort key. If multiple queries use different columns for filters, then you can often improve performance for those queries by using an interleaved sort style. When a query uses restrictive predicates on secondary sort columns, interleaved sorting significantly improves query performance as compared to compound sorting.","poster":"Bulti","upvote_count":"3","timestamp":"1635701520.0","comment_id":"76474"},{"upvote_count":"2","timestamp":"1635364920.0","comment_id":"74094","poster":"jxj","content":"D is more correct. \nB is correct if different queries have their WHERE clause on different columns that are being defined on global index. However, the problem statement did not mention that."},{"comment_id":"70357","content":"It is D. see https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html. The question is trying to reinforce 'different users'","upvote_count":"1","comments":[{"comments":[{"timestamp":"1635552480.0","poster":"G3","comment_id":"76159","upvote_count":"1","content":"I changed my opinion, it is B."}],"comment_id":"72747","upvote_count":"2","content":"Correct, it has to be D. \n\nThe question states, that \"user teams are experiencing poor performance.\nWhich action improves performance for the user teams in this situation? \". \n\nWLM is the one that can be used to route queries based on user groups (user teams) and alloted priorities.","poster":"G3","timestamp":"1635095280.0"}],"timestamp":"1635072600.0","poster":"jiedee"},{"comment_id":"52327","timestamp":"1634965200.0","content":"my selection B","poster":"san2020","upvote_count":"1"},{"upvote_count":"4","comment_id":"42114","timestamp":"1634866200.0","poster":"kalpanareddy","content":"D is correct \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html"},{"content":"On the first side, it seems B. Add interleaved sort keys per team. is correct. But how it can be \"per team\", this made it a wrong answer.\nThe nearest alternative option is D. Add support for workload management queue hopping.\nit can share resources between the parallel query. And in that way, it improves performance.","comment_id":"39481","upvote_count":"1","comments":[{"poster":"practicioner","upvote_count":"2","comments":[{"upvote_count":"1","poster":"practicioner","timestamp":"1634894220.0","comment_id":"43891","content":"I changed my opinion. B is the right answer.\n\n\"But how it can be \"per team\", this made it a wrong answer.\"\nEasy. interleaved sort keys could include different columns for different team. As example you can create key with (n1,n2,n3) columns where different team can use their filter conditions."}],"comment_id":"40581","timestamp":"1633713780.0","content":"This is confusing a lot \"the same table with very different query types\". I think it means a lot of types of queries (short living time,long living, etc). It doesn't mention about key inside tables and there're no any ways to fix it with keys (all teams share the same data)"}],"timestamp":"1633660380.0","poster":"ME2000"},{"upvote_count":"1","timestamp":"1633577940.0","content":"i think it is B, it mentions, the same table different types of queries.","comment_id":"35011","poster":"michelleY"},{"upvote_count":"2","comment_id":"19685","timestamp":"1633070400.0","content":"Ans is B\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html#t_Sorting_data-interleaved","poster":"BigEv"},{"comment_id":"14966","poster":"M2","timestamp":"1633035060.0","content":"answer is B","upvote_count":"2"},{"upvote_count":"2","content":"I support B as well","comment_id":"11404","timestamp":"1632408180.0","poster":"exams"},{"comment_id":"9824","content":"Answer is B. \nWLM primary use case is for queuing long or short queries.","poster":"mattyb123","timestamp":"1632209100.0","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/amazon/view/4773-exam-aws-certified-big-data-specialty-topic-1-question-14/","exam_id":17,"answer_ET":"D","answer_description":"Reference: https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html","answers_community":[],"choices":{"A":"Create custom table views.","D":"Add support for workload management queue hopping.","C":"Maintain team-specific copies of the table.","B":"Add interleaved sort keys per team."}},{"id":"sbcjBOM0XGuDha4epfwv","question_id":7,"unix_timestamp":1567732860,"answers_community":[],"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/4774-exam-aws-certified-big-data-specialty-topic-1-question-15/","answer":"B","question_text":"A company operates an international business served from a single AWS region. The company wants to expand into a new country. The regulator for that country requires the Data Architect to maintain a log of financial transactions in the country within 24 hours of the product transaction. The production application is latency insensitive. The new country contains another AWS region.\nWhat is the most cost-effective way to meet this requirement?","discussion":[{"timestamp":"1633702140.0","content":"D is the right answer. S3 is very cost effective. CRR happens from few seconds to few minutes and will satisfy within 24 hrs requirement. \n\nB is an absurd - CloudFront is for content serving and as such logs content access. It does not store transaction logs.\n\nC is NOT the right answer. Kinesis stores data only for a period of time (24 Hrs by default) and then the data will expire. Any data regulatory requirement would also expect the data to be durable along with being available within a period of time.","poster":"jay1ram2","upvote_count":"5","comment_id":"38366","comments":[{"content":"agree, \nEven with Kinesis Firehose, you can't directly stream data into S3 bucket in another region.\nSo S3 CRR would solve the problem and is also cost-effective.","comment_id":"46076","poster":"AdamSmith","timestamp":"1633792200.0","upvote_count":"2"}]},{"comment_id":"1100556","upvote_count":"1","poster":"Mandy_007","content":"I will go with D. CRR seems correct to me.","timestamp":"1702984680.0"},{"upvote_count":"1","timestamp":"1636073640.0","poster":"thuffman","content":"The consensus seems do be C or D but the posted answer is B, Cloudfront. Does anyone have an explanation or is the answer wrong?","comment_id":"359444"},{"comment_id":"122351","poster":"kein22190","content":"I will probably choose D","timestamp":"1636031280.0","upvote_count":"1"},{"comment_id":"113507","content":"For me the key in this questions is \"The new country contains another AWS region\" so hinting at CRR","upvote_count":"1","poster":"esalas0691","timestamp":"1635734220.0"},{"poster":"Debi_mishra","comment_id":"95396","upvote_count":"1","timestamp":"1635267840.0","content":"Answer should be D, as its cost effective."},{"content":"I think D is the right answer coz of cost effectiveness. C seems to be a dis-tractor to me.","poster":"Josh1981","comment_id":"93281","upvote_count":"1","timestamp":"1635110460.0"},{"comment_id":"82380","poster":"srirampc","upvote_count":"1","content":"answer id D. Enabling S3 cross region replication is cheaper. the sentence latency insensitive means no Kinesis and Cloudfront, both can improve performance but at additional service cost.","timestamp":"1634926200.0"},{"timestamp":"1634743500.0","content":"B\n'Cost efficiency'-A is out\n‘latency intensive ’ C&D are out","upvote_count":"1","comment_id":"70360","comments":[{"timestamp":"1635152460.0","poster":"iamsajal","upvote_count":"1","content":"It's latency insensitive. That's why it's D.","comment_id":"95104"},{"upvote_count":"1","comment_id":"100354","content":"Who says anything about latency intensity? D is correct. >Also, s3 cross region replication may take quite a few hours in rare cases.","poster":"Corram","timestamp":"1635523200.0"}],"poster":"jiedee"},{"poster":"Rajash","upvote_count":"2","timestamp":"1634121600.0","content":"Data Architect to maintain a log of financial transactions in the country within 24 hours of the product transaction - If C, Kinesis records expire after 24 hours, if the logs are to be maintained, then we need a persistent storage in S3. D - Seems correct.","comment_id":"55561"},{"content":"my selection D","comment_id":"52328","upvote_count":"1","poster":"san2020","timestamp":"1634087580.0"},{"content":"I chose D. As only the logs need to be maintained in the new country, S3 cross region replication can be used to copy the data to the AWS region within the new Country.","poster":"kalpanareddy","timestamp":"1633722780.0","upvote_count":"1","comment_id":"42565"},{"poster":"rusu","content":"D -- based on this link... \nIt says \"CRR is a bucket-level configuration, and it can help you meet compliance requirements and minimize latency by keeping copies of your data in different Regions.\"\n\nhttps://aws.amazon.com/blogs/big-data/trigger-cross-region-replication-of-pre-existing-objects-using-amazon-s3-inventory-amazon-emr-and-amazon-athena/\n\nPlease let me know if you agree","timestamp":"1633529700.0","upvote_count":"4","comment_id":"27852"},{"content":"what's the final answer here ... C or D?","timestamp":"1633153620.0","comment_id":"24833","poster":"s3an","upvote_count":"1"},{"poster":"viduvivek","comment_id":"19952","content":"C looks reasonable.\n\n1. The primary requirement in this scenario is to maintain a log of the financial transaction in the country (existing) within 24 hours of the product transaction.\n2. Given that the product application is latency insenstive. \nBased on #2 above, they can continue to serve from the existing region.\nAnd use Kinesis to stream transactions to the Regulator.","timestamp":"1633013220.0","upvote_count":"2"},{"upvote_count":"3","poster":"BigEv","timestamp":"1632881520.0","comment_id":"19688","comments":[{"content":"kinesis maintains data for 24 hours, not logs","upvote_count":"4","comment_id":"105167","timestamp":"1635549180.0","poster":"abhineet"}],"content":"Ans is C. By default, Records of a Kinesis stream are accessible for up to 24 hours from the time they are added to the stream"},{"timestamp":"1632875940.0","comment_id":"18266","poster":"Zire","content":"The regulator for that country requires the Data Architect to maintain a log of financial transactions in the country \"within 24 hours of the product transaction\" : Option C.\nSending each product transaction to Kinesis will make each of them persist there for 24 hours(default) before they expire.","comments":[{"upvote_count":"4","content":"\"within 24 hours of the product transaction\" means the logs must be in the country in 24hrs or less of the transaction, and doesn't imply keeping the logs for only 24hrs. D seems the best solution here","comment_id":"25797","timestamp":"1633527660.0","poster":"s3an"}],"upvote_count":"1"},{"upvote_count":"3","timestamp":"1632828720.0","comment_id":"14969","poster":"M2","content":"the correct is D"},{"timestamp":"1632755280.0","content":"I think it is D because the question says \"..The production application is latency insensitive...\", so they don't care about latency and they need \"cost-effective\" solution.","poster":"VB","comment_id":"13089","upvote_count":"3"},{"content":"B is correct because data transfer between original region and cloud front will be free if needed and is important to notice that within new country AWS have a region.","timestamp":"1632730320.0","upvote_count":"2","poster":"gonda","comment_id":"12073"},{"upvote_count":"2","poster":"exams","timestamp":"1632234840.0","comment_id":"11405","content":"I think its D, because of cost effective solution. Cloudfront doesn't have transaction logs, they can only be in DB I think"},{"timestamp":"1632109620.0","upvote_count":"1","poster":"Hitu","comment_id":"11187","content":"I think answer should be D - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html"},{"upvote_count":"1","comments":[{"comment_id":"11038","content":"Agree, also CloudFront is located in Edge location, having region does not mean having CloudFront","poster":"apertus","timestamp":"1632078120.0","upvote_count":"1"}],"content":"I chose C. As my thinking was the cloudfront use case doesnt really make sense here.","timestamp":"1632075240.0","comment_id":"9825","poster":"mattyb123"}],"topic":"1","choices":{"D":"Use Amazon S3 cross-region replication to copy and persist production transaction logs to a bucket in the new countrys region.","B":"Use Amazon CloudFront to serve application content locally in the country; Amazon CloudFront logs will satisfy the requirement.","A":"Use CloudFormation to replicate the production application to the new region.","C":"Continue to serve customers from the existing region while using Amazon Kinesis to stream transaction data to the regulator."},"timestamp":"2019-09-06 03:21:00","exam_id":17,"answer_description":"","isMC":true,"question_images":[],"answer_images":[]},{"id":"RgrMHnextsPHXVU73vAW","question_text":"An administrator needs to design the event log storage architecture for events from mobile devices. The event data will be processed by an Amazon EMR cluster daily for aggregated reporting and analytics before being archived.\nHow should the administrator recommend storing the log data?","topic":"1","question_id":8,"unix_timestamp":1565588040,"answer_description":"","isMC":true,"answer":"A","discussion":[{"content":"Thoughts on C?","comment_id":"6576","timestamp":"1632102060.0","poster":"mattyb123","upvote_count":"6"},{"content":"I think C is correct. We are running EMR daily, so partitioning by day will give data for all devices for each day.","upvote_count":"1","poster":"Mandy_007","comment_id":"1100559","timestamp":"1702984860.0"},{"timestamp":"1635377340.0","content":"I think it is A because the EMR job is run daily, so partitioning by day will lead to only one partition. Partitioning by device, on the other hand, allows for parallel jobs to be run for each device or group of devices.","poster":"KartV","comment_id":"96621","comments":[{"upvote_count":"1","timestamp":"1635398400.0","poster":"KartV","content":"On second thought, it could still be C because one can sub-partition the logs by device under date. E.g. date/device which still allows for parallelization. Whereas device/date will be the read query more complex as the job needs to be run daily.","comments":[{"upvote_count":"1","content":"Actually, you don't need supartitioning. Partitioning isn't even a thing in S3. Instead, EMR is quite capable of reading many objects from one s3 folder in parallel. Also, if possible, it splits up the objects into 64MB chunks when reading so it can even read one object in parallel.\nBottom line, C is true and thinking about partitioning does not make sense in this context. hope it helps :)","comment_id":"100352","poster":"Corram","timestamp":"1635730920.0"}],"comment_id":"96626"}],"upvote_count":"2"},{"timestamp":"1635301260.0","content":"Answer should be C. It's a daily aggregation not by device.","comment_id":"95397","upvote_count":"1","poster":"Debi_mishra"},{"poster":"jiedee","content":"of course is C\nA is clearly wrong because the api call fee is TOOOOOOOOO expensive","upvote_count":"2","comment_id":"70361","timestamp":"1635251340.0"},{"comment_id":"52329","upvote_count":"1","poster":"san2020","timestamp":"1635250860.0","content":"my selection C"},{"comments":[{"content":"Because of the partition scheme. We should imagine about simple access method. In this case \"yyyy/mm/dd\" or similar are the best choice for partitioning.","upvote_count":"1","timestamp":"1635011160.0","poster":"practicioner","comment_id":"43895"}],"content":"Why not A","upvote_count":"1","timestamp":"1634664300.0","comment_id":"42567","poster":"kalpanareddy"},{"poster":"practicioner","comment_id":"40582","upvote_count":"1","timestamp":"1634653980.0","content":"C is the right choice. Nothing else"},{"comment_id":"36395","upvote_count":"4","poster":"PK1234","timestamp":"1634456160.0","content":"This is not for real time analysis, but intends to process log data in batch, hence S3 is better than dynamo db."},{"comment_id":"19689","upvote_count":"2","poster":"BigEv","timestamp":"1634419140.0","content":"Why can't we use DynamoDB?"},{"comment_id":"14970","upvote_count":"1","poster":"M2","timestamp":"1634033640.0","content":"C looks correct to me"},{"content":"Yeah C looks correct. day/time mechanism is always better for storing logs","upvote_count":"3","poster":"exams","comment_id":"11406","timestamp":"1633356720.0"},{"poster":"pra276","comment_id":"7519","upvote_count":"1","content":"Answer is C:","timestamp":"1633086660.0"},{"content":"It is C. Daily EMR job and based on time not device.","poster":"muhsin","comment_id":"6884","timestamp":"1632515700.0","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/3489-exam-aws-certified-big-data-specialty-topic-1-question-16/","answer_ET":"A","timestamp":"2019-08-12 07:34:00","choices":{"A":"Create an Amazon S3 bucket and write log data into folders by device. Execute the EMR job on the device folders.","D":"Create an Amazon DynamoDB table partitioned on EventID, write log data to table. Execute the EMR job on the table.","B":"Create an Amazon DynamoDB table partitioned on the device and sorted on date, write log data to table. Execute the EMR job on the Amazon DynamoDB table.","C":"Create an Amazon S3 bucket and write data into folders by day. Execute the EMR job on the daily folder."},"answer_images":[],"exam_id":17,"question_images":[],"answers_community":[]},{"id":"zhyZ1ZxsIl1ErtzwX5wV","question_images":[],"answer_description":"","question_text":"A data engineer wants to use an Amazon Elastic Map Reduce for an application. The data engineer needs to make sure it complies with regulatory requirements. The auditor must be able to confirm at any point which servers are running and which network access controls are deployed.\nWhich action should the data engineer take to meet this requirement?","unix_timestamp":1565801700,"exam_id":17,"timestamp":"2019-08-14 18:55:00","isMC":true,"answers_community":[],"topic":"1","answer_images":[],"question_id":9,"answer":"C","choices":{"D":"Provide the auditor with access to AWS DirectConnect to use their existing tools.","A":"Provide the auditor IAM accounts with the SecurityAudit policy attached to their group.","C":"Provide the auditor with CloudFormation templates.","B":"Provide the auditor with SSH keys for access to the Amazon EMR cluster."},"url":"https://www.examtopics.com/discussions/amazon/view/3594-exam-aws-certified-big-data-specialty-topic-1-question-17/","answer_ET":"C","discussion":[{"upvote_count":"6","comment_id":"6885","content":"It is A. you can check it from https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html#jf_security-auditor \nat the option C, there is not any information about cloudformation templates.","timestamp":"1632260040.0","poster":"muhsin"},{"upvote_count":"5","poster":"pra276","comment_id":"7521","comments":[{"content":"A look most appropriate","timestamp":"1632567180.0","comment_id":"11516","poster":"exams","upvote_count":"2"}],"timestamp":"1632556860.0","content":"cloudformation templates only provide the information about what was deployed not about what is currently running, so best answer is A"},{"poster":"exam_da","comment_id":"135443","upvote_count":"1","timestamp":"1635817140.0","content":"answer is A"},{"upvote_count":"1","timestamp":"1635622560.0","poster":"Debi_mishra","content":"C is wrong, template doesn't provide whats implemented. Its A","comment_id":"95400"},{"content":"A , obviously","comment_id":"88863","timestamp":"1635466140.0","upvote_count":"1","poster":"menthlo"},{"upvote_count":"1","poster":"srirampc","comment_id":"82389","content":"answer is C. auditor wants to know about the servers and (roles) associated with them, not how people in the group have their security policies like in \"IAM accounts with the SecurityAudit policy attached to their group\". If you have to know how servers are deployed cloudformation is the way.","timestamp":"1635424680.0"},{"timestamp":"1634706000.0","comment_id":"74135","poster":"jxj","content":"Between A and C, A is more correct way to get a security auditor to start the process. \nhttps://kevinslin.com/aws/aws_account_access_policies/#","upvote_count":"1"},{"content":"It is A.","comment_id":"70364","poster":"jiedee","upvote_count":"1","timestamp":"1634025240.0"},{"poster":"drneon","upvote_count":"3","comment_id":"61334","timestamp":"1633673400.0","content":"The SecurityAudit policy has a permission about checking cloudformation documents already.\nSo, answer is A ^^"},{"timestamp":"1633467000.0","poster":"san2020","content":"my selection A","upvote_count":"2","comment_id":"52330"},{"upvote_count":"1","poster":"ME2000","comment_id":"39527","content":"Option C is valid\nThe template is a blueprint that provides intend servers and network access controls.\nAnd by checking resource drift status can find current status against intend status.","timestamp":"1633395900.0"},{"poster":"M2","upvote_count":"1","comment_id":"14972","content":"A looks right bcoz c will not tell you about running servers.","timestamp":"1633126380.0"}]},{"id":"wUuBQUqVyzsehw6u1r1J","answer_ET":"B","answer_description":"","timestamp":"2019-09-18 06:01:00","question_id":10,"unix_timestamp":1568779260,"question_text":"A social media customer has data from different data sources including RDS running MySQL, Redshift, and\nHive on EMR. To support better analysis, the customer needs to be able to analyze data from different data sources and to combine the results.\nWhat is the most cost-effective solution to meet these requirements?","question_images":[],"isMC":true,"answer":"B","choices":{"D":"Write a program running on a separate EC2 instance to run queries to three different systems. Aggregate the results after getting the responses from all three systems.","B":"Install Presto on the EMR cluster where Hive sits. Configure MySQL and PostgreSQL connector to select from different data sources in a single query.","A":"Load all data from a different database/warehouse to S3. Use Redshift COPY command to copy data to Redshift for analysis.","C":"Spin up an Elasticsearch cluster. Load data from all three data sources and use Kibana to analyze."},"answers_community":[],"exam_id":17,"discussion":[{"comment_id":"52331","upvote_count":"5","timestamp":"1634540640.0","content":"my selection B","poster":"san2020"},{"upvote_count":"1","comment_id":"95405","content":"Technically B is wrong, as it mentions nothing about redshift.","timestamp":"1635751440.0","comments":[{"comment_id":"106817","timestamp":"1636241220.0","poster":"tubadc","upvote_count":"2","content":"It's be, Redshift connector is the sames as Postrges connector, you only need to set one\nconnector.name=redshift\nconnection-url=jdbc:postgresql://example.net:5439/database\nconnection-user=root\nconnection-password=secret"}],"poster":"Debi_mishra"},{"poster":"Josh1981","content":"I got with B","timestamp":"1634880360.0","comment_id":"93293","upvote_count":"1"},{"comment_id":"74495","timestamp":"1634687400.0","poster":"YashBindlish","content":"Correct Answer is B as Presto (or PrestoDB) is an open source, distributed SQL query engine, designed from the ground up for fast analytic queries against data of any size. It supports both non-relational sources, such as the Hadoop Distributed File System (HDFS), Amazon S3, Cassandra, MongoDB, and HBase, and relational data sources such as MySQL, PostgreSQL, Amazon Redshift, Microsoft SQL Server, and Teradata.\nhttps://aws.amazon.com/big-data/what-is-presto/","upvote_count":"3"},{"timestamp":"1632960180.0","content":"Why not C? It seems a lot more easier to ingest data to Elasticsearch","comment_id":"19692","poster":"BigEv","upvote_count":"1","comments":[{"timestamp":"1633233300.0","comment_id":"25948","content":"The question says most cost-effective way, and Elasticsearch is an expensive option. I feel its B.","poster":"G3","upvote_count":"2"},{"comments":[{"poster":"antoneti","upvote_count":"2","timestamp":"1634176860.0","comment_id":"26476","content":"I would agree if only mention \"Presto\" but why configure MySQL and PostgreSQL? this makes me go with C","comments":[{"comment_id":"30473","comments":[{"comment_id":"35019","content":"i agree, i think answer is B.","poster":"michelleY","upvote_count":"1","timestamp":"1634359800.0"}],"upvote_count":"5","timestamp":"1634200260.0","content":"because PostgreSQL is used for the redshift and MySQL for the RDS, Answer is B","poster":"marwan"}]}],"poster":"kamikazestar","upvote_count":"2","content":"Because 'Presto is a fast SQL query engine designed for interactive analytic queries over large datasets from multiple sources' (source: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-presto.html), and EMR had been already provisioned.","comment_id":"26335","timestamp":"1634098020.0"}]},{"upvote_count":"1","content":"A & D are for sure not the answer. question is only between b & c","comment_id":"14978","poster":"M2","timestamp":"1632426600.0"},{"timestamp":"1632370680.0","upvote_count":"3","content":"B is correct I think","poster":"exams","comment_id":"11518"}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/5343-exam-aws-certified-big-data-specialty-topic-1-question-18/","answer_images":[]}],"exam":{"isImplemented":true,"isBeta":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","name":"AWS Certified Big Data - Specialty","id":17,"isMCOnly":true,"numberOfQuestions":85},"currentPage":2},"__N_SSP":true}