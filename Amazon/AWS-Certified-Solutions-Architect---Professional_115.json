{"pageProps":{"questions":[{"id":"rfW3uj6oK7QTinV8ydBF","question_text":"A company is migrating its three-tier web application from on-premises to the AWS Cloud. The company has the following requirements for the migration process:\n✑ Ingest machine images from the on-premises environment.\n✑ Synchronize changes from the on-premises environment to the AWS environment until the production cutover.\n✑ Minimize downtime when executing the production cutover.\n✑ Migrate the virtual machines' root volumes and data volumes.\nWhich solution will satisfy these requirements with minimal operational overhead?","answer_images":[],"answer_description":"","exam_id":32,"question_images":[],"question_id":571,"answer":"A","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/28246-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1597225020,"discussion":[{"comment_id":"156313","upvote_count":"17","poster":"Nemer","timestamp":"1632233760.0","content":"A. SMS for automated, live incremental replication of live server volumes."},{"poster":"LunchTime","upvote_count":"6","comment_id":"160371","content":"A is correct. \nSMS can handle migrating the data volumes: https://aws.amazon.com/about-aws/whats-new/2018/09/aws-server-migration-service-adds-support-for-migrating-larger-data-volumes/","timestamp":"1632486540.0"},{"poster":"Heer","upvote_count":"1","content":"Server Migration Service solely can do the following :\nMigrate Virtual machine i.e the volumes are also getting migrated ,\nAWS SMS incrementally replicates your server VMs as cloud-hosted Amazon Machine Images (AMIs) ready for deployment on Amazon EC2.\n\nRight option is A","timestamp":"1667445120.0","comment_id":"710211"},{"comment_id":"580874","poster":"roka_ua","timestamp":"1649095080.0","content":"Vote A","upvote_count":"1"},{"timestamp":"1643191080.0","upvote_count":"1","poster":"shotty1","content":"Selected Answer: A\nIt is A","comment_id":"532767"},{"poster":"AzureDP900","content":"A is good","comment_id":"494896","timestamp":"1638766080.0","upvote_count":"1"},{"content":"I'll go with A","upvote_count":"2","poster":"WhyIronMan","comment_id":"412567","timestamp":"1635836700.0"},{"comment_id":"368738","timestamp":"1635580440.0","poster":"syc1205","upvote_count":"1","content":"Ingest machine images from the on-premises environment. So B"},{"upvote_count":"1","poster":"blackgamer","content":"A is the answer.","timestamp":"1635427860.0","comment_id":"352429"},{"content":"it's A","timestamp":"1635135540.0","upvote_count":"3","comment_id":"350940","poster":"Waiweng"},{"timestamp":"1635011220.0","content":"A for sure","upvote_count":"1","comment_id":"321750","poster":"alisyech"},{"timestamp":"1634614680.0","content":"going with A","comment_id":"292581","poster":"Kian1","upvote_count":"1"},{"comments":[{"content":"First of all snapshot import is a feature of VM Export/Import not a feature of SMS:\nhttps://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-import-snapshot.html\nSecondly, if you import snapshot you need a create a volume from snapshot,","poster":"Ebi","comment_id":"285420","upvote_count":"4","timestamp":"1634302080.0"},{"timestamp":"1634367720.0","content":"Thirdly how you manage incremental changes on data volume?\nAnswer is NOT C","poster":"Ebi","comment_id":"285422","upvote_count":"6"}],"comment_id":"283371","timestamp":"1634249460.0","poster":"Trap_D0_r","upvote_count":"1","content":"C\nI wouldn't wrap up every drive for every vm into one giant ami and the phrasing of A \"launch a replication job for each tier of the application\" is a little vague and sounds fishy to me. Personally C looks better--use SMS to create ami's from root volumes, snapshot and import data drives, create EC2s from amis and reattach data drives. Test and deploy. That makes more sense to me."},{"upvote_count":"4","timestamp":"1634137320.0","poster":"Ebi","content":"A is the answer","comment_id":"268619"},{"poster":"T14102020","comment_id":"244645","timestamp":"1633872300.0","comments":[{"comment_id":"244646","content":"without AWS CLI","upvote_count":"1","timestamp":"1634127720.0","poster":"T14102020"}],"content":"Correct is A. SMS with minimum steps opposite C","upvote_count":"1"},{"upvote_count":"3","comment_id":"232321","timestamp":"1633550580.0","content":"I'll go with A","poster":"jackdryan"},{"timestamp":"1632887280.0","comment_id":"208598","upvote_count":"1","content":"A. SMS for migration of VMs.","poster":"CYL"},{"comment_id":"207956","poster":"SamAWSExam99","content":"A. Just use AMI created by SMS","timestamp":"1632753180.0","upvote_count":"1"},{"poster":"Bulti","comment_id":"206764","content":"A is better than C because of less operational overhead.","timestamp":"1632618780.0","upvote_count":"1"},{"timestamp":"1632589620.0","poster":"SanjeevB","comment_id":"196214","content":"There is no information provided on Data volume. SMS can handle only upto 16 TB of data. C seems more appropriate answer.","upvote_count":"1"},{"timestamp":"1632422640.0","poster":"Anila_Dhharisi","upvote_count":"5","comment_id":"157845","content":"A works out."}],"answer_ET":"B","choices":{"A":"Use AWS Server Migration Service (SMS) to create and launch a replication job for each tier of the application. Launch instances from the AMIs created by AWS SMS. After initial testing, perform a final replication and create new instances from the updated AMIs.","C":"Use AWS Server Migration Service (SMS) to upload the operating system volumes. Use the AWS CLI import-snapshot command for the data volumes. Launch instances from the AMIs created by AWS SMS and attach the data volumes to the instances. After initial testing, perform a final replication, launch new instances from the replicated AMIs, and attach the data volumes to the instances.","D":"Use AWS Application Discovery Service and AWS Migration Hub to group the virtual machines as an application. Use the AWS CLI VM Import/Export script to import the virtual machines as AMIs. Schedule the script to run incrementally to maintain changes in the application. Launch instances from the AMIs. After initial testing, perform a final virtual machine import and launch new instances from the AMIs.","B":"Create an AWS CLI VM Import/Export script to migrate each virtual machine. Schedule the script to run incrementally to maintain changes in the application. Launch instances from the AMIs created by VM Import/Export. Once testing is done, rerun the script to do a final import and launch the instances from the AMIs."},"answers_community":["A (100%)"],"timestamp":"2020-08-12 11:37:00","topic":"1"},{"id":"IPmmUMmGy0hqaYSvL1Yu","question_images":[],"question_id":572,"topic":"1","question_text":"An enterprise company's data science team wants to provide a safe, cost-effective way to provide easy access to Amazon SageMaker. The data scientists have limited AWS knowledge and need to be able to launch a Jupyter notebook instance. The notebook instance needs to have a preconfigured AWS KMS key to encrypt data at rest on the machine learning storage volume without exposing the complex setup requirements.\nWhich approach will allow the company to set up a self-service mechanism for the data scientists to launch Jupyter notebooks in its AWS accounts with the\nLEAST amount of operational overhead?","answer_images":[],"answer_ET":"C","discussion":[{"poster":"Nemer","content":"C. Service Catalog\nhttps://aws.amazon.com/blogs/mt/enable-self-service-secured-data-science-using-amazon-sagemaker-notebooks-and-aws-service-catalog/","upvote_count":"20","timestamp":"1632520260.0","comment_id":"156315"},{"upvote_count":"1","content":"A seems to fit the simplicity","poster":"spdracr713","timestamp":"1663333800.0","comment_id":"670858"},{"comment_id":"636904","upvote_count":"1","poster":"hilft","timestamp":"1658774280.0","content":"Why it's not B?"},{"comment_id":"626450","poster":"aandc","timestamp":"1656829260.0","content":"Selected Answer: C\nkeyword \"The data scientists are unfamiliar with AWS\" -> service catalog","upvote_count":"3"},{"timestamp":"1651494720.0","upvote_count":"1","poster":"tartarus23","comment_id":"596034","content":"Selected Answer: C\nC. Using service catalog is easier than S3 bucket for the data scientists"},{"upvote_count":"1","comment_id":"532733","content":"Selected Answer: C\nI think it is C","timestamp":"1643188500.0","poster":"shotty1"},{"timestamp":"1641990720.0","content":"Selected Answer: C\nC just because service catalog simplifies it.","comment_id":"522146","poster":"pititcu667","upvote_count":"2"},{"poster":"AzureDP900","content":"I will go with C","comment_id":"494898","upvote_count":"1","timestamp":"1638766140.0"},{"comment_id":"412596","poster":"WhyIronMan","upvote_count":"3","timestamp":"1636189380.0","content":"I'll go with C"},{"poster":"blackgamer","timestamp":"1635777300.0","comment_id":"352439","content":"The answer is C for sure, service catalog.","upvote_count":"2"},{"comment_id":"350944","upvote_count":"3","poster":"Waiweng","timestamp":"1635527040.0","content":"it's C"},{"content":"Service catalog is the key. C","upvote_count":"2","timestamp":"1635358140.0","poster":"kiev","comment_id":"296087"},{"poster":"Kian1","timestamp":"1634675160.0","content":"going with C","upvote_count":"2","comment_id":"292585"},{"comment_id":"269306","poster":"Ebi","timestamp":"1634602380.0","upvote_count":"4","content":"I will go with C"},{"comment_id":"244652","content":"Correct is C. CloudFormat + ServiceCatalog","poster":"T14102020","upvote_count":"3","timestamp":"1634140380.0"},{"content":"I'll go with C","timestamp":"1634010060.0","upvote_count":"3","comment_id":"232322","poster":"jackdryan"},{"timestamp":"1633954080.0","poster":"oopsy","content":"seems C","upvote_count":"1","comment_id":"229467"},{"content":"C is correct","comment_id":"205981","upvote_count":"2","poster":"Bulti","timestamp":"1633896720.0"},{"comment_id":"173879","content":"C. is correct. LEAST amount of operational overhead makes A. a bad choice.","upvote_count":"3","timestamp":"1633542180.0","poster":"wsw"},{"poster":"[Removed]","timestamp":"1633242660.0","upvote_count":"2","content":"C is correct answer","comment_id":"167136"},{"comment_id":"159325","poster":"directconnect","upvote_count":"3","content":"On second thought I will go for \"C\". Service Catalog lets you create your interface without having to login to the AWS console. That is easier to manage than creating your own S3 website.","timestamp":"1633109700.0","comments":[{"content":"Every option needs the scientists to login AWS console.\nC is better than A, because in the question, obviously there are multiple AWS accounts that the scientists are using.\nA needs privilege to call AWS API, which means the scientists have to provides AWS account info (like login credentials). That's difficult to implement.","timestamp":"1634598420.0","comment_id":"264673","upvote_count":"2","poster":"01037"}]},{"poster":"directconnect","upvote_count":"2","content":"A. \nThe Data Scientist have limited AWS knowledge. \"C\" s wrong because even if you share the CloudFormation template with them in the Service Catalog, they have to build the stack in order to get the instance running. \"A\" simply lets them fill out a form to create a request.","comment_id":"159312","timestamp":"1632801660.0"},{"poster":"Kibana01","comment_id":"157957","upvote_count":"2","content":"Why B? C is right:The data scientists have limited AWS knowledge","timestamp":"1632742560.0"}],"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/28247-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","timestamp":"2020-08-12 11:47:00","unix_timestamp":1597225620,"answers_community":["C (100%)"],"isMC":true,"exam_id":32,"choices":{"B":"Create an AWS CloudFormation template to launch a Jupyter notebook instance using the AWS::SageMaker::NotebookInstance resource type with a preconfigured KMS key. Add a user-friendly name to the CloudFormation template. Display the URL to the notebook using the Outputs section. Distribute the CloudFormation template to the data scientists using a shared Amazon S3 bucket.","A":"Create a serverless front end using a static Amazon S3 website to allow the data scientists to request a Jupyter notebook instance by filling out a form. Use Amazon API Gateway to receive requests from the S3 website and trigger a central AWS Lambda function to make an API call to Amazon SageMaker that will launch a notebook instance with a preconfigured KMS key for the data scientists. Then call back to the front-end website to display the URL to the notebook instance.","C":"Create an AWS CloudFormation template to launch a Jupyter notebook instance using the AWS::SageMaker::NotebookInstance resource type with a preconfigured KMS key. Simplify the parameter names, such as the instance size, by mapping them to Small, Large, and X-Large using the Mappings section in CloudFormation. Display the URL to the notebook using the Outputs section, then upload the template into an AWS Service Catalog product in the data scientist's portfolio, and share it with the data scientist's IAM role.","D":"Create an AWS CLI script that the data scientists can run locally. Provide step-by-step instructions about the parameters to be provided while executing the AWS CLI script to launch a Jupyter notebook with a preconfigured KMS key. Distribute the CLI script to the data scientists using a shared Amazon S3 bucket."}},{"id":"JMFKk3hwtV6Etz2PsMso","unix_timestamp":1597387380,"topic":"1","answer_description":"","choices":{"D":"Using CloudFormation, create an IAM role for each developer, and attach policies that allow interaction with CloudFormation. Use CloudFormation StackSets to deploy this template to each AWS account.","E":"In a central AWS account, create an IAM role that can be assumed by CloudFormation that has permissions to create the resources the company requires. Create a CloudFormation stack policy that allows the IAM role to manage resources. Use CloudFormation StackSets to deploy the CloudFormation stack policy to each AWS account.","C":"Using CloudFormation, create an IAM role that can be assumed by developers, and attach policies that allow interaction with and passing a role to CloudFormation. Attach an inline policy to deny access to all other AWS services. Use CloudFormation StackSets to deploy this template to each AWS account.","A":"Using CloudFormation, create an IAM role that can be assumed by CloudFormation that has permissions to create all the resources the company needs. Use CloudFormation StackSets to deploy this template to each AWS account.","B":"In a central account, create an IAM role that can be assumed by developers, and attach a policy that allows interaction with CloudFormation. Modify the AssumeRolePolicyDocument action to allow the IAM role to be passed to CloudFormation."},"question_text":"A company is migrating its applications to AWS. The applications will be deployed to AWS accounts owned by business units. The company has several teams of developers who are responsible for the development and maintenance of all applications. The company is expecting rapid growth in the number of users.\nThe company's chief technology officer has the following requirements:\n✑ Developers must launch the AWS infrastructure using AWS CloudFormation.\nDevelopers must not be able to create resources outside of CloudFormation.\n//IMG//\n\n✑ The solution must be able to scale to hundreds of AWS accounts.\nWhich of the following would meet these requirements? (Choose two.)","answer_images":[],"answers_community":["AC (100%)"],"isMC":true,"exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/28545-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":573,"answer":"AC","discussion":[{"timestamp":"1632325080.0","poster":"Kibana01","upvote_count":"13","comment_id":"157966","content":"A&C seems a better combination."},{"timestamp":"1632278760.0","poster":"Anila_Dhharisi","upvote_count":"12","comments":[{"content":"A&C\nB is wrong.\n\"Modify the AssumeRolePolicyDocument action to allow the IAM role to be passed to CloudFormation.\" => this sentence is wrong.\n\n\"AssumeRolePolicyDocument\nThe trust policy that is associated with this role. Trust policies define which entities can assume the role.\" \n\nWe need to use iam:Passrole to pass the role from developer to cloudformation.\nAssumeRolePolicyDocument is used for assume the role only.","comment_id":"377690","timestamp":"1635588780.0","upvote_count":"4","poster":"viet1991"}],"comment_id":"157864","content":"Between A & E, its better to go with A. In E, they gave option of stack policy. We use stack policy only for updates and as well to avoid any unintentional updates. In this scenario, they had not discussed the requirement of updates on the resources of CloudFormation stack.\nBetween B,C,D - its better to go with B. \nIn option C, they mentioned about inline policy which is not appropriate as we need to embed the policy not attach it and better to use managed policies than inline policies. Inline policies are assigned to service linked roles which is inherited from the parent or user . \nIn option D, its saying to create role to each of the developers which is not the right way in assigning the permissions. A role can be used by multiple developers instead of creating each role to each developer."},{"poster":"Blair77","upvote_count":"1","comment_id":"694921","timestamp":"1665767040.0","content":"Selected Answer: AC\nAAA CCC 110% sure!"},{"poster":"tomosabc1","comment_id":"687601","timestamp":"1665043020.0","upvote_count":"1","content":"Selected Answer: AC\nThe answer is AC.\n\nB(wrong):\"Modify the AssumeRolePolicyDocument action to allow the IAM role to be passed to CloudFormation.\" => this sentence is wrong.\n\"AssumeRolePolicyDocument\nThe trust policy that is associated with this role. Trust policies define which entities can assume the role.\"\nWe need to use iam:Passrole to pass the role from developer to cloudformation.\n\nD(wrong): \"create an IAM role for each developer\". This sentence is wrong.\nE(wrong): The newly created role in central account cannot be directly used by CloudFormation to create resources in other account. In addition, similar to S3 bucket policy, CloudFormation stack policy is used to control who can update the stack, rather than allowing the stack to create/manage AWS resource."},{"content":"C and E","timestamp":"1658703480.0","poster":"hilft","upvote_count":"1","comment_id":"636281"},{"poster":"aandc","upvote_count":"1","comment_id":"626419","content":"Selected Answer: AC\nagreed with AC","timestamp":"1656825480.0"},{"upvote_count":"4","comment_id":"553381","content":"First we start by looking at either B or E, here E is more detailed and complete answer, so will go with E. Then is between A,C and D. D is not compliant with AWS not best practice to create role for each developer, so then between A and C. My answer would be C as this has an inline policy that prevents the developer from accessing the services directly. So answer is C and E","poster":"jyrajan69","timestamp":"1645496580.0"},{"timestamp":"1638766380.0","poster":"AzureDP900","upvote_count":"1","comment_id":"494901","content":"I'll go with A,C"},{"content":"I'll go with A,C","comment_id":"412598","upvote_count":"3","timestamp":"1636039620.0","poster":"WhyIronMan"},{"content":"Here is a proposition of reasoning. \nFirst you must start from an account. Between B and E, you choose E because B is tempting (the statement about AssumeRolePolicyDocument looks right if I look this example : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html#aws-resource-iam-role--examples) but B does not allow you to export your configuration to other accounts. With E, CloudFormation gives itself the rights needed to auto-assign IAM.\nThen you use CloudFormation (options A, C, D). Remains to give rights to developers (C, D), but between these options, to respect the statement that \"developers must not be able to create resources outside of CloudFormation\", the only option is C \"attach an inline policy to deny access to all other AWS services\" not very fine grained, but the only present. Plus D has the keywords \"for each developer\" which as said by Anila is tedious.\nTherefore CE would be the right answers (as in the autocorrection, looking at the comments 90% of the answers are supposed to be good and I see discussion about the proposed answers on 50% !)","poster":"tekkart","comments":[{"content":"Agree with Tekkart, C&E are the right choices.","comments":[{"comment_id":"548384","timestamp":"1644997440.0","upvote_count":"2","poster":"tkanmani76","content":"Correcting to A and C. C is the only choice which limits access to use of other services. And A deploys the template. (E deploys only stack set policy which is not correct)."}],"upvote_count":"2","timestamp":"1641600780.0","poster":"tkanmani76","comment_id":"519255"}],"upvote_count":"6","comment_id":"389906","timestamp":"1635599400.0"},{"poster":"Waiweng","timestamp":"1635461100.0","upvote_count":"2","content":"it's A&C","comment_id":"350950"},{"content":"I would go with A & B. Because C is kinda duplicated with A and developers can manually amend policy by itself if required.","upvote_count":"1","poster":"ppshein","comment_id":"346663","timestamp":"1635296400.0"},{"comment_id":"292588","timestamp":"1635245700.0","poster":"Kian1","content":"going with AC","upvote_count":"1"},{"content":"I will go with AC","comment_id":"269311","upvote_count":"4","poster":"Ebi","timestamp":"1634817000.0"},{"content":"A AND C","poster":"petebear55","timestamp":"1634211300.0","upvote_count":"2","comment_id":"254303"},{"poster":"Bulti","upvote_count":"4","comment_id":"253745","timestamp":"1634157780.0","content":"A& C is the right answer. E is a misleading option. You need to deploy the CloudFormation template and not just the Stack policy. Moreover, the purpose of the stack policy is to prevent accidental changes to the resources being created by the CloudFormation template which is not the requirement. So A&C is correct."},{"upvote_count":"4","comment_id":"232349","poster":"jackdryan","timestamp":"1633819980.0","content":"I'll go with A,C"},{"upvote_count":"1","content":"Combination of A and C will achieve the requirements needed. Limited developer resource creation outside of CF. Having stacksets to push out restrictions to all AWS account is the easiest way to enforce organization wide standardization.","timestamp":"1633594680.0","comment_id":"208610","poster":"CYL"},{"content":"B does not work, cuz you cannot allow role from one account to perform actions in another account. You need role in each acc.","upvote_count":"2","timestamp":"1633454160.0","comment_id":"192788","poster":"pddddd"},{"comment_id":"192713","upvote_count":"1","timestamp":"1633425000.0","poster":"Alan76","content":"Between B and C, not sure how B will fulfill the requirement of \"Developers must not be able to create resources outside of CloudFormation\". Clunky as it sounds, Option C will fulfill this?"},{"timestamp":"1632798480.0","poster":"mgat","upvote_count":"3","comment_id":"179113","content":"A&C.\nB is wrong: the IAM role that can be assumed by developers is not the role to be passed to CF"},{"content":"AB.\nA is obvious, and B provides a central, cross-account dev role with a trust relationship (AssumeRolePolicyDocument). \n\nWhat would be needed here is a Permissions boundary managed policy, to interact with CF. \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html","poster":"Nemer","timestamp":"1632742500.0","upvote_count":"4","comment_id":"158204"}],"timestamp":"2020-08-14 08:43:00","answer_ET":"AC","question_images":["https://www.examtopics.com/assets/media/exam-media/04241/0039500002.png"]},{"id":"yZ5TQOdiLwH3BTPC8qUm","answer_images":[],"answer_ET":"B","answers_community":["A (67%)","B (33%)"],"timestamp":"2020-08-12 15:14:00","isMC":true,"discussion":[{"upvote_count":"23","comments":[{"timestamp":"1635687780.0","poster":"viet1991","content":"A is right.\nBy default, an S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. To get access to the object, the object owner must explicitly grant you (the bucket owner) access.\n\naws s3 cp BuildAccountFile s3://DistributionAccountS3/ --acl bucket-owner-full-control","upvote_count":"5","comment_id":"378387"},{"timestamp":"1667820900.0","content":"B: This exact scenario is detailed here: \nhttps://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-artifacts-s3/\n\nThe cross account role sets the owner as the distribution account.","upvote_count":"2","comment_id":"713006","poster":"Byrney"}],"comment_id":"156437","timestamp":"1632072000.0","poster":"Nemer","content":"A. \nhttps://aws.amazon.com/tw/premiumsupport/knowledge-center/s3-bucket-owner-access/"},{"comment_id":"241915","poster":"darthvoodoo","comments":[{"comments":[{"timestamp":"1635744300.0","content":"At first I thought it was B, then I changed my mind to A.\nIt is not the Pipeline in Build Account which cannot access the object (answer B).\nIt is CloudFront, together with S3 bucket in Distribution Account, activated by OAI, which cannot access the object. Because, by giving cross-account permission, it lost its bucket full control : https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-8\nI go with A","upvote_count":"1","poster":"tekkart","comment_id":"389914"}],"comment_id":"254305","timestamp":"1634362380.0","content":"WELL DONE","poster":"petebear55","upvote_count":"1"},{"upvote_count":"1","timestamp":"1634434440.0","content":"Good point","comment_id":"264831","poster":"01037"},{"timestamp":"1635678240.0","content":"It's not A because it means S3 bucket owner asks permission of object. In this case, S3 belongs to CloudFront but accessor is app. So app won't have same permission as Cloud Front. You have to chose B.","poster":"oscargee","comment_id":"363180","upvote_count":"1"}],"timestamp":"1633961160.0","upvote_count":"11","content":"The answer is definitely A...this is one of the questions that always pops up in the security specialty exam https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/ \nIf B was the issue, you would have been getting a 404 error instead."},{"poster":"dev112233xx","comment_id":"871722","upvote_count":"1","content":"Selected Answer: A\nA is the correct answer.\nB is almost correct, cross account can solve the ACL issue but only when adding this condition in the role: \"s3:x-amz-acl\": \"bucket-owner-full-control\" \n\nhttps://repost.aws/knowledge-center/s3-bucket-owner-full-control-acl","timestamp":"1681646520.0"},{"upvote_count":"1","comment_id":"824113","poster":"[Removed]","content":"Selected Answer: B\nIf the bucket policy grants public read access, confirm that AWS account that owns the bucket also owns the object\nFor a bucket policy to allow public read access to objects, the AWS account that owns the bucket must also own the objects. For existing Amazon S3 buckets with the default object ownership settings, the object owner is the AWS account of the AWS Identity and Access Management (IAM) identity which uploaded the object to the bucket.","timestamp":"1677528540.0","comments":[{"comment_id":"831222","upvote_count":"1","poster":"[Removed]","timestamp":"1678133460.0","content":"Selected B by mistake, i meant to vote A"}]},{"content":"Selected Answer: A\nAnswer is A. No brainer","poster":"Dionenonly","timestamp":"1665900900.0","upvote_count":"1","comment_id":"696032"},{"poster":"CloudHell","content":"My initial instinct was B, but after reading the comments A sounds like a better choice.","comment_id":"615453","upvote_count":"1","timestamp":"1655056560.0"},{"comment_id":"554932","upvote_count":"2","content":"B is correct. There are three possible Object Ownership settings: (1)Bucket owner enforced: bucket owner always owns the object. That is not the case we have here. (2) Bucket owner preferred. If an object upload includes the bucket-owner-full-control canned ACL, the bucket owner owns the object. Objects uploaded with other ACLs are owned by the writing account. Answer A would only work with this setting. But we don't if the bucket used this setting. (3) Object writer: Object writer owns the object. Answer B works regardless of the Object Ownership setting of the bucket. Details at https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html","timestamp":"1645657320.0","poster":"johnnsmith"},{"upvote_count":"1","comment_id":"497329","content":"A. Modify the S3 upload process in the Build Account to add the bucket-owner-full-control ACL to the objects at upload.","timestamp":"1639025100.0","poster":"cldy"},{"content":"A is fine","poster":"AzureDP900","timestamp":"1638766500.0","comment_id":"494903","upvote_count":"1"},{"comment_id":"437016","timestamp":"1636111800.0","upvote_count":"2","poster":"student22","content":"A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/"},{"comment_id":"433142","timestamp":"1636105740.0","poster":"blackgamer","upvote_count":"1","content":"It definitely is A. This document explains it - \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html#object-ownership-replication"},{"comment_id":"412600","timestamp":"1636019400.0","upvote_count":"1","poster":"WhyIronMan","content":"I'll go with A"},{"comment_id":"403166","poster":"SJain50","timestamp":"1635828540.0","upvote_count":"2","content":"B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-artifacts-s3/"},{"poster":"Waiweng","upvote_count":"1","comment_id":"350957","content":"it's A","timestamp":"1635574800.0"},{"timestamp":"1635352320.0","content":"Neal Davis went for B. I am going for my exam next week. I am lost now. I know both both A and B works","poster":"kiev","upvote_count":"4","comment_id":"308309"},{"comment_id":"296802","content":"A is correct. By assuming cross-account role, the pipeline would give up any permissions in Build account, that it might need to complete build actions. So it must keep with its own role and the answer is A.","poster":"ele","upvote_count":"3","timestamp":"1635324360.0"},{"timestamp":"1635291960.0","upvote_count":"1","comment_id":"292592","poster":"Kian1","content":"going with A"},{"timestamp":"1634726460.0","content":"I will go with A","upvote_count":"3","comment_id":"269319","poster":"Ebi","comments":[{"upvote_count":"1","timestamp":"1635026100.0","comment_id":"285438","poster":"Ebi","content":"Actually A and B both are correct answer, but A is more straight forward\nOne of those questions from AWS which evaluates ability to pick the BEST answer not only the right one"}]},{"content":"A.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/","timestamp":"1634576760.0","upvote_count":"2","comment_id":"265525","poster":"01037"},{"timestamp":"1634269680.0","content":"A is the correct answer. https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example4.html. This article shows a scenario where the bucket owned by Account A has a bucket policy that allows users in Account B to upload objects. But unless Account B doesn't give Account A full access control to the objects it created in the bucket owned by Account A, user/applications cannot access the objects in that bucket via Cloudfront for example. So A is correct. B is incorrect because by assuming a role in the bucket owner account, you don't give up ownership of the objects you have uploaded from another account automatically. You need to explicitly give full control to the bucket owner.","upvote_count":"2","comments":[{"comment_id":"279962","upvote_count":"2","poster":"cox1960","timestamp":"1634973180.0","content":"Sorry, this is wrong Bulti: \"B is incorrect because by assuming a role in the bucket owner account, you don't give up ownership of the objects you have uploaded from another account automatically. You need to explicitly give full control to the bucket owner.\"\nBy assuming a role in Account D, all created resources belong to D. \"If you upload an object using AWS Identity and Access Management (IAM) user or role credentials, the AWS account that the user or role belongs to owns the object.\" https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-overview.html\nI prefer B so all objects are owned by the Distribution account."}],"comment_id":"253751","poster":"Bulti"},{"upvote_count":"2","timestamp":"1634229480.0","comments":[{"upvote_count":"1","comment_id":"277748","timestamp":"1634750520.0","poster":"shammous","content":"That would break the least privilege principle"}],"content":"Correct is A. bucket-owner-full-control","poster":"T14102020","comment_id":"244667"},{"comment_id":"244117","content":"A is the correct answer.\n\nDistribution account owns the S3 bucket and Build account is uploading the objects. Cross account role is needed for this operation. Questions does not says any problem with operation means this role already exists.\n\nProblem is: objects uploaded by Build account are not accessible by Distribution account even though Distribution account is the bucket owner. We need canned ACL concept here. Build account need to grant either bucket-owner-read or bucket-owner-full-control ACL. So answer A correct.","timestamp":"1634213880.0","upvote_count":"4","comments":[{"timestamp":"1634368500.0","content":"Hi!\nIf the cross-account role was used for upload from build account. Then the object belongs to Distribution Account. Why we will still need the canned ACL Concept. I'm also confused because, i have read this article too https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/","poster":"vivektiwari","upvote_count":"1","comment_id":"257476"}],"poster":"exam_prep"},{"poster":"jackdryan","timestamp":"1633881780.0","comment_id":"232360","upvote_count":"3","content":"I'll go with A"},{"poster":"cloudgc","timestamp":"1633836960.0","upvote_count":"3","content":"Answer - B.\nCame across this wonderful video. Start watching from 42:20 secs for the answer.","comment_id":"231040"},{"comment_id":"229544","poster":"oopsy","upvote_count":"2","content":"must be B","timestamp":"1633582980.0"},{"poster":"liono","comment_id":"211434","timestamp":"1633582620.0","upvote_count":"2","content":"https://n2ws.com/blog/aws-cloud/managing-aws-accounts-cross-account-iam-roles\nB"},{"comment_id":"205957","content":"Answer is A. B and D are misleading. C is not possible.","timestamp":"1633528740.0","upvote_count":"1","poster":"Bulti"},{"timestamp":"1633322760.0","content":"No such thing as cross-account role.","comments":[{"upvote_count":"1","comment_id":"277745","poster":"shammous","timestamp":"1634729880.0","content":"There is. It's mentioned twice here: https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/"}],"comment_id":"192794","poster":"pddddd","upvote_count":"3"},{"comment_id":"176161","poster":"Ganfeng","upvote_count":"5","content":"I would go with A. my reason is, if the pipeline assume the role, that it will loose its permission in its own account","timestamp":"1633311360.0"},{"poster":"wsw","timestamp":"1633253940.0","upvote_count":"1","content":"Yes B. seems correct. The question is a little confusing and clearly state an issue with Read from CloudFront over an issue with the Write from the build pipeline. But we can assume B. is correct because the IAM role assumed by the pipeline should be a cross-account role created in the Distribution account, instead of the Build account","comment_id":"173906"},{"poster":"ansaDev","content":"B seems correct","comment_id":"171521","timestamp":"1633227000.0","upvote_count":"1"},{"timestamp":"1632836100.0","comment_id":"163919","poster":"balisongjam","comments":[{"upvote_count":"2","comment_id":"171325","timestamp":"1632930420.0","comments":[{"comment_id":"429079","upvote_count":"1","timestamp":"1636029180.0","content":"Build account will not get full permissions","poster":"DerekKey"}],"poster":"SadioMane","content":"Why do we need to grand full access??? Build account needs just write access. So, I guess B is more suitable answer"}],"upvote_count":"1","content":"A and B both work but i'm going to go with A. It's more direct."},{"content":"B is correct","timestamp":"1632662340.0","upvote_count":"1","comment_id":"163007","poster":"shakthi000005"},{"comments":[{"upvote_count":"1","content":"The error us caused by CloudFront to not being able to access S3 objects","poster":"bobsmith2000","timestamp":"1652767020.0","comment_id":"602799"}],"poster":"directconnect","content":"B.\nErrors are caused by the fact that the pipeline in the build account cannot access the S3 bucket in the distribution account. A IAM role providing cross-account access is needed.","upvote_count":"3","timestamp":"1632591480.0","comment_id":"159344"},{"content":"A is good option as per the link given by Nemer.","timestamp":"1632379200.0","comment_id":"157869","poster":"Anila_Dhharisi","upvote_count":"2"}],"question_images":[],"choices":{"D":"Create a new IAM role in the Distribution Account with read access to the S3 bucket. Configure CloudFront to use this new role as its OAI. Modify the build pipeline to assume this role when uploading files from the Build Account.","C":"Modify the S3 upload process in the Build Account to set the object owner to the Distribution Account.","A":"Modify the S3 upload process in the Build Account to add the bucket-owner-full-control ACL to the objects at upload.","B":"Create a new cross-account IAM role in the Distribution Account with write access to the S3 bucket. Modify the build pipeline to assume this role to upload the files to the Distribution Account."},"exam_id":32,"unix_timestamp":1597238040,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/28273-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":574,"question_text":"A media company has a static web application that is generated programmatically. The company has a build pipeline that generates HTML content that is uploaded to an Amazon S3 bucket served by Amazon CloudFront. The build pipeline runs inside a Build Account. The S3 bucket and CloudFront distribution are in a Distribution Account. The build pipeline uploads the files to Amazon S3 using an IAM role in the Build Account. The S3 bucket has a bucket policy that only allows CloudFront to read objects using an origin access identity (OAI). During testing all attempts to access the application using the CloudFront URL result in an\nHTTP 403 Access Denied response.\nWhat should a solutions architect suggest to the company to allow access the objects in Amazon S3 through CloudFront?","answer":"A","answer_description":""},{"id":"UQlWSHvNRsa39usv6sxU","unix_timestamp":1597239120,"answer_description":"","question_text":"A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in\nAmazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1,000 EC2 instances, overall performance was well below expectations.\nWhich collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/28275-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2020-08-12 15:32:00","answer_images":[],"answers_community":["ACF (100%)"],"question_images":[],"question_id":575,"discussion":[{"comments":[{"poster":"blackgamer","timestamp":"1635576180.0","upvote_count":"2","content":"Yes, definitely ACF. Well explained.","comment_id":"433144"}],"comment_id":"156754","timestamp":"1633306920.0","poster":"easytoo","upvote_count":"42","content":"A. High performance computing (HPC) workload cluster should be in a single AZ.\nC. Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instances to accelerate High Performance Computing (HPC)\nF. Amazon FSx for Lustre - Use it for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling.\n\nCluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"},{"timestamp":"1634973420.0","upvote_count":"6","comment_id":"269329","poster":"Ebi","content":"I will go with ACF"},{"poster":"SkyZeroZx","timestamp":"1688261040.0","upvote_count":"1","content":"Selected Answer: ACF\nI will go with ACF","comment_id":"940394"},{"poster":"maxh8086","timestamp":"1673013780.0","comments":[{"upvote_count":"1","content":"https://aws.amazon.com/blogs/hpc/highly-available-hpc-infrastructure-on-aws/\n\n- The two clusters are created using AWS ParallelCluster and, the main front end used to access the environment, is managed by EnginFrame in the Enterprise configuration. Amazon Aurora is used by EnginFrame as RDBMS to manage Triggers, Job-Cache, and Applications and Views users’ groups. Amazon Elastic File System is used as shared file system between the two clusters environments. Two Amazon FSx file systems, one per each Availability Zone (AZ), are used as high-performance file system for the HPC jobs.","comment_id":"767776","poster":"maxh8086","timestamp":"1673027400.0"}],"upvote_count":"1","content":"https://aws.amazon.com/hpc/faqs/\n- There is no built-in limit to the size of the cluster you can build with AWS ParallelCluster. \n- AWS ParallelCluster also integrates with Elastic Fabric Adapter (EFA) for applications that require low-latency networking between nodes of HPC clusters. AWS ParallelCluster is also integrated with Amazon FSx for Lustre, a high-performance file system with scalable storage for compute workloads, and Amazon Elastic File System.\n- AWS ParallelCluster is also compatible with Amazon Elastic File System (EFS), RAID, and Amazon FSx for Lustre file systems.","comment_id":"767775"},{"comment_id":"760269","upvote_count":"1","timestamp":"1672256820.0","poster":"evargasbrz","content":"Selected Answer: ACF\nI'll go with ACF"},{"content":"Selected Answer: ACF\nACF\nA - for network perfomance (single AZ is better than multiple AZ because the latency)\nC- EFA no brain for HPC\nF - EFS Lustre is for HPC, no brain too","timestamp":"1661758680.0","comment_id":"653346","poster":"gnic","upvote_count":"1"},{"poster":"ka1tw","upvote_count":"1","timestamp":"1648261860.0","comment_id":"575341","content":"Why NOT B with multiple ENI?"},{"content":"Yes ACF.","comment_id":"521758","poster":"Ni_yot","upvote_count":"1","timestamp":"1641932940.0"},{"upvote_count":"1","timestamp":"1641015300.0","poster":"cldy","comment_id":"514385","content":"A.C.F. \nEFA + FSx Lustre + single AZ."},{"poster":"Tan0k","upvote_count":"1","content":"Selected Answer: ACF\nACF got to be","timestamp":"1639578960.0","comment_id":"502248"},{"content":"ACF is correct","poster":"AzureDP900","upvote_count":"1","comment_id":"494904","timestamp":"1638766560.0"},{"content":"Selected Answer: ACF\nACF is good.","comment_id":"491088","timestamp":"1638318780.0","upvote_count":"1","poster":"acloudguru"},{"upvote_count":"2","timestamp":"1635556140.0","comment_id":"412602","content":"I'll go with A,C,F","poster":"WhyIronMan"},{"comment_id":"350959","poster":"Waiweng","content":"it's A,C,F","timestamp":"1635447840.0","upvote_count":"3"},{"poster":"Kian1","upvote_count":"3","content":"going with ACF","comment_id":"292593","timestamp":"1635074880.0"},{"content":"Correct ACF. Elastic Fabric Adapter + FSx Lustre for HPC + single AZ","timestamp":"1634773620.0","comment_id":"244672","upvote_count":"4","poster":"T14102020"},{"content":"I'll go with A,C,F","comment_id":"232368","upvote_count":"5","poster":"jackdryan","timestamp":"1634612040.0"},{"content":"seems ACF","comment_id":"229549","timestamp":"1634533380.0","poster":"oopsy","upvote_count":"2"},{"poster":"CYL","comment_id":"208620","content":"ACF to enhance on networking and file system level optimization.","timestamp":"1634517840.0","upvote_count":"2"},{"content":"ACF is the right answer","timestamp":"1634303100.0","upvote_count":"2","poster":"Bulti","comment_id":"205940"},{"comment_id":"176166","poster":"Ganfeng","timestamp":"1634169840.0","upvote_count":"4","content":"agree ACF"},{"poster":"Kibana01","upvote_count":"1","content":"Thanks easytoo, your explanation has done justice to the question. ACF","timestamp":"1633441920.0","comment_id":"157969"},{"content":"Yes its ACF. its not required to replace EFS to EBS.","comment_id":"157872","timestamp":"1633396920.0","poster":"Anila_Dhharisi","upvote_count":"3"},{"comment_id":"156451","timestamp":"1633015740.0","poster":"Nemer","upvote_count":"1","content":"CEF. Elastic Fabric Adapter & FSx Lustre for HPC, and EBS faster than EFS.","comments":[{"poster":"Nemer","timestamp":"1634131140.0","upvote_count":"3","content":"Err. Change to ACF. EBS not good for sharing files. Should double- check that its is indeed a cluster pl group.","comment_id":"158216"}]}],"exam_id":32,"choices":{"D":"Ensure the clusters is launched across multiple Availability Zones.","E":"Replace Amazon EFS win multiple Amazon EBS volumes in a RAID array.","A":"Ensure the HPC cluster is launched within a single Availability Zone.","B":"Launch the EC2 instances and attach elastic network interfaces in multiples of four.","F":"Replace Amazon EFS with Amazon FSx for Lustre.","C":"Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled."},"answer_ET":"ACF","topic":"1","answer":"ACF"}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":false,"provider":"Amazon","id":32,"numberOfQuestions":1019,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional","isImplemented":true},"currentPage":115},"__N_SSP":true}