{"pageProps":{"questions":[{"id":"BTwIBDtWfudQyHzKRpLw","question_id":326,"answer_description":"","question_images":[],"choices":{"A":"Review the Incomplete Multipart Upload Bytes metric in the S3 Storage Lens dashboard. Create an S3 Lifecycle policy to automatically delete any incomplete multipart uploads after 7 days.","C":"Access the S3 console. Review the Metrics tab to check the storage that incomplete multipart uploads are consuming. Create an AWS Lambda function to delete any incomplete multipart uploads after 7 days.","D":"Use the S3 analytics storage class analysis tool to identify and measure incomplete multipart uploads. Configure an S3 bucket policy to enforce restrictions on multipart uploads to delete incomplete multipart uploads after 7 days.","B":"Implement S3 Intelligent-Tiering to move data into lower-cost storage classes after 7 days. Create an S3 Storage Lens policy to automatically delete any incomplete multipart uploads after 7 days."},"isMC":true,"answer_ET":"A","unix_timestamp":1704135240,"answers_community":["A (100%)"],"discussion":[{"poster":"nharaz","content":"Selected Answer: A\nAmazon S3 supports a bucket lifecycle rule that you can use to direct Amazon S3 to stop multipart uploads that aren't completed within a specified number of days after being initiated. When a multipart upload isn't completed within the specified time frame, it becomes eligible for an abort operation. Amazon S3 then stops the multipart upload and deletes the parts associated with the multipart upload.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mpu-abort-incomplete-mpu-lifecycle-config.html#","comment_id":"1111396","upvote_count":"10","timestamp":"1719852840.0"},{"poster":"Kipalom","content":"Selected Answer: A\nIn my opinion the bucket policy is just a rule, it doesn't do the deleting. So for me its the answer A. As the Storage Lens is having metrics for uncompleted multipart uploads and the lifecycle rule actually really deletes old uncompleted uploads.","comment_id":"1111438","timestamp":"1719861720.0","upvote_count":"8"}],"topic":"1","answer":"A","timestamp":"2024-01-01 19:54:00","question_text":"A company has scientists who upload large data objects to an Amazon S3 bucket. The scientists upload the objects as multipart uploads. The multipart uploads often fail because of poor end-client connectivity.\n\nThe company wants to optimize storage costs that are associated with the data. A SysOps administrator must implement a solution that presents metrics for incomplete uploads. The solution also must automatically delete any incomplete uploads after 7 days.\n\nWhich solution will meet these requirements?","exam_id":34,"url":"https://www.examtopics.com/discussions/amazon/view/130103-exam-aws-certified-sysops-administrator-associate-topic-1/","answer_images":[]},{"id":"gv9XmFcpVFoHOX4ULZza","unix_timestamp":1704135420,"answer_ET":"D","answers_community":["D (47%)","C (47%)","3%"],"timestamp":"2024-01-01 19:57:00","question_text":"A company stores data in Amazon S3 buckets that are provisioned in three separate AWS Regions. The data is copied from the S3 buckets to the data center over the public internet using a VPN. The SysOps administrator notices that, occasionally, the transfers take longer than usual, and determines the issue is congestion within the company's ISP network.\n\nWhat is the MOST cost-effective approach the administrator can take to ensure consistent transfer times from S3 to the data center?","exam_id":34,"question_id":327,"url":"https://www.examtopics.com/discussions/amazon/view/130104-exam-aws-certified-sysops-administrator-associate-topic-1/","answer_images":[],"answer_description":"","discussion":[{"timestamp":"1708121760.0","comment_id":"1152248","content":"Selected Answer: C\nI would also say C because 1. the ISP is a problem so a private wired connection would solve this and 2. the question mentions \"most cost-effective\" => 1 DC connection is cheaper than 3. You can replicate the buckets cross region within the aws network or w/e, there are solutions","comments":[{"content":"DX* not DC","comment_id":"1152250","upvote_count":"1","poster":"dinuts","timestamp":"1708121880.0"}],"upvote_count":"9","poster":"dinuts"},{"comment_id":"1145984","upvote_count":"6","content":"why is no one talking about option C. To me that is the correct option. With public virtual interface, you will still pay for data transfer charges and it is not even as fast as private virtual interface. Also while creating multiple direct link would be faster, it is not the most cost effective option. I will choose option CCCCCC","poster":"henro4niger","timestamp":"1707554040.0"},{"timestamp":"1734450420.0","poster":"Aresius","upvote_count":"2","comment_id":"1328007","content":"Selected Answer: D\nAWS Direct Connect with a public virtual interface allows access to public AWS services like Amazon S3 across all AWS Regions over a dedicated private connection.\nWith a single Direct Connect link and a public virtual interface, the data center can access S3 buckets across all Regions without needing separate links to each Region.\nThis approach avoids ISP congestion and provides consistent transfer performance at a lower cost compared to multiple Direct Connect links."},{"poster":"numark","timestamp":"1733402880.0","comment_id":"1322362","upvote_count":"1","content":"Selected Answer: D\nBy establishing a single Direct Connect link and creating a public virtual interface, the company can directly connect to public AWS services like Amazon S3 across all regions without incurring the costs of inter-region data transfer over the AWS network. This could result in consistent and possibly enhanced transfer speeds compared to using the public internet without the additional cost and complexity.Establishing a Direct Connect link to just one AWS Region and creating a PRIVATE virtual interface allows the company to use AWSâ€™s internal network for data transfer to the data center. HOWEVER, for accessing S3 buckets in multiple regions securely and privately, traffic would need to be routed over inter-region VPC peering or transit gateways, which may incur additional costs and complexity."},{"comments":[{"upvote_count":"1","comment_id":"1301251","content":"With public virtual interface, you will still pay for data transfer charges and it is not even as fast as private virtual interface. Would you still go for D considering this factor?..","poster":"Aamee","timestamp":"1729543080.0"}],"comment_id":"1300041","poster":"Slays","content":"Selected Answer: D\nExplanation:\n\nDirect Connect provides a dedicated network connection between your data center and AWS, which helps bypass the public internet and reduces issues caused by ISP congestion.\nA public virtual interface over Direct Connect allows you to access public AWS resources like Amazon S3 without routing traffic over the public internet, improving performance and consistency.\nEstablishing a Direct Connect link to only one Region is more cost-effective compared to setting up Direct Connect links to all three Regions, and you can still access S3 buckets in other Regions over the AWS backbone, ensuring consistent performance.","timestamp":"1729348200.0","upvote_count":"2"},{"content":"Answer is D. We use Private Gateway with respect to VPC.","upvote_count":"2","comment_id":"1283650","timestamp":"1726321320.0","poster":"nss373"},{"poster":"VerRi","content":"Selected Answer: D\nOne region is more cost-effective.\nPrivate VIF is used to integrate resources within VPC, but S3 is a global service and does not reside in VPC.","comment_id":"1255808","upvote_count":"4","timestamp":"1722013260.0"},{"content":"Selected Answer: D\nPubliv vif for sure","timestamp":"1717308660.0","comment_id":"1223037","upvote_count":"2","poster":"tsangckl"},{"content":"Selected Answer: D\nPublic Virtual Interface: By creating a public virtual interface, you can access public AWS services, such as Amazon S3, over the Direct Connect link. This setup allows you to transfer data from S3 buckets in any region to your data center using the consistent, dedicated bandwidth of Direct Connect.","poster":"nyalpellymkar07","upvote_count":"2","timestamp":"1716838800.0","comment_id":"1219817"},{"timestamp":"1712477700.0","poster":"seetpt","comment_id":"1190846","content":"Selected Answer: C\nC should be it","upvote_count":"1"},{"poster":"Manoel","timestamp":"1712050680.0","comment_id":"1187954","content":"Correct is D\nQ: What is a virtual interface (VIF)?\n\nA virtual interface (VIF) is necessary to access AWS services, and is either public or private. A public virtual interface enables access to public services, such as Amazon S3. A private virtual interface enables access to your VPC. For more information, see AWS Direct Connect virtual interfaces.\n\nhttps://aws.amazon.com/directconnect/faqs/","upvote_count":"2"},{"content":"Selected Answer: D\nFirst lets make some things clear:\n1. Private VIF provides connection to VPC in just 1 specific region.\nso: Since we can reach 1 bucket in region, what is going to happen to other region buckets?\nEventually even if we manage to set up connections via that one VPC with other regions it would incur inter-region data transfer charges!\n2.Public VIF allows you to access all AWS public services - means S3 regardless of S3 bucket's region. Transfer cost is the same for all regions. We are not charged data transfer rates for transferring data out of S3 to our Direct Connect location.\n\nI hope this makes it clear that we will be using 1 DX connection and public VIF(virtual interface) to access those 3 different region buckets bypassing internet.","timestamp":"1709299620.0","poster":"awsamar","comment_id":"1163515","upvote_count":"3"},{"comment_id":"1162574","timestamp":"1709212260.0","content":"Selected Answer: C\nshould be C","upvote_count":"3","poster":"March2023"},{"comment_id":"1156230","timestamp":"1708589340.0","content":"Selected Answer: C\nTo setup a Direct Connect to one or more VPC in many different\nregions (same account), you must use a Direct Connect Gateway\n1) create one AWS Direct Connect in one region\n2) create one Direct Connect Gateway\n3) create Private virtual interface to VPCs in regions","upvote_count":"5","poster":"nakuaadam"},{"timestamp":"1705951140.0","content":"Selected Answer: B\nSee response below","comment_id":"1128960","poster":"Learning4life","upvote_count":"1"},{"poster":"Learning4life","comments":[],"comment_id":"1128956","upvote_count":"2","timestamp":"1705950780.0","content":"Selected Answer: D\nEach region needs a Direct Connect established. Once established, you will setup a private virtual interface to your VPC, but outside of your VPC is where you need a public virtual interface to communicate with services such as S3, Amazon Glacier, etc. This is diagramed out in Stephane Marek's course."},{"timestamp":"1704135420.0","upvote_count":"1","poster":"nharaz","content":"Selected Answer: A\nA - provides dedicated, private connections to each AWS Region using AWS Direct Connect, ensuring a more consistent and reliable data transfer mechanism compared to relying on the public internet.","comment_id":"1111399"}],"topic":"1","question_images":[],"answer":"D","isMC":true,"choices":{"B":"Establish an AWS Direct Connect link to each Region. Create a public virtual interface over each link.","C":"Establish an AWS Direct Connect link to one of the Regions. Create a private virtual interface over that link.","D":"Establish an AWS Direct Connect link to one of the Regions. Create a public virtual interface over that link.","A":"Establish an AWS Direct Connect link to each Region. Create a private virtual interface over each link."}},{"id":"3RCkt0G7TYvGT6Y5xtEf","choices":{"B":"Create a snapshot of the existing EBS volume. When the snapshot is complete, create an EBS volume of a larger size from the snapshot in the same Availability Zone as the EC2 instance. Attach the new EBS volume to the EC2 instance. Mount the file system.","A":"Modify the EBS volume by adding additional drive space. Log on to the EC2 instance. Use the file system-specific commands to extend the file system.","D":"Stop the EC2 instance. Change the EC2 instance to a larger instance size that includes additional drive space. Start the EC2 instance.","C":"Create a new EBS volume of a larger size in the same Availability Zone as the EC2 instance. Attach the EBS volume to the EC2 instance. Copy the data from the existing EBS volume to the new EBS volume."},"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/129820-exam-aws-certified-sysops-administrator-associate-topic-1/","unix_timestamp":1703897280,"answer_images":[],"answers_community":["A (94%)","6%"],"timestamp":"2023-12-30 01:48:00","topic":"1","question_id":328,"question_text":"A company has an Amazon EC2 instance that supports a production system. The EC2 instance is backed by an Amazon Elastic Block Store (Amazon EBS) volume. The EBS volume's drive has filled to 100% capacity, which is causing the application on the EC2 instance to experience errors.\n\nWhich solution will remediate these errors in the LEAST amount of time?","isMC":true,"answer_description":"","discussion":[{"content":"Selected Answer: A\nModify the EBS volume by adding additional drive space. Log on to the EC2 instance. Use the file system-specific commands to extend the file system.","timestamp":"1704063960.0","upvote_count":"5","comment_id":"1110864","poster":"LemonGremlin"},{"comments":[{"comment_id":"1316601","content":"Good explanation\n\nAdding to point 2: \nFor Windows you would go to Disk Management and right-click the used portion of your disk and then choosing Extend Volume.","timestamp":"1732350240.0","upvote_count":"1","poster":"Albanki"}],"comment_id":"1225018","upvote_count":"3","content":"Selected Answer: A\nWith Amazon EBS Elastic Volumes, you can increase the volume size without needing to detach the volume or restart the instance. This is the fastest and most efficient solution, as it allows you to expand the volume capacity and extend the file system without any downtime.\n\nThe steps would be:\n\n1. Log in to the EC2 instance.\n2. Use the appropriate file system commands (e.g., `resize2fs` for ext4, `xfs_growfs` for XFS) to extend the file system to utilize the additional space.\n\nThis is the least disruptive and quickest solution, as it does not require creating a snapshot, provisioning a new volume, or stopping the instance. The Elastic Volumes feature allows you to make the necessary changes to the EBS volume directly, with the changes taking effect immediately.","timestamp":"1717625280.0","poster":"Student013657"},{"comment_id":"1190135","content":"Selected Answer: A\nA: Modify the EBS volume by adding additional drive space. Log on to the EC2 instance. Use the file system-specific commands to extend the file system.\n\nThis allows you to extend the existing EBS volume's capacity directly without having to create new volumes or copy data. It's the quickest because it directly addresses the issue of the filled EBS volume, extending its capacity to resolve the errors without having to create new volumes or copy data. You can resize the EBS volume while it is attached to the running EC2 instance, and then extend the file system within the EC2 instance's operating system. This method does not require downtime for the EC2 instance or data migration.\n\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-modify-volume.html","timestamp":"1712358300.0","upvote_count":"3","poster":"joshnort"},{"comment_id":"1162571","comments":[{"upvote_count":"1","content":"LEAST amount of time it asks for so read it again..","comment_id":"1301299","timestamp":"1729551180.0","poster":"Aamee"}],"poster":"March2023","timestamp":"1709212080.0","upvote_count":"1","content":"Selected Answer: B\nanswer is B"},{"poster":"WinAndWin","content":"Selected Answer: A\nafter extend disk. it still needs to do some more steps from OS level.","comment_id":"1109312","upvote_count":"4","timestamp":"1703897280.0"}],"answer_ET":"A","exam_id":34,"answer":"A"},{"id":"bIX7UggmAf40nvt8ODsC","discussion":[{"comment_id":"1111182","poster":"WinAndWin","timestamp":"1719832620.0","upvote_count":"4","content":"Selected Answer: B\nB is the best choice. as document https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"},{"comment_id":"1110865","upvote_count":"2","poster":"LemonGremlin","content":"Selected Answer: B\nI think this is B","timestamp":"1719781620.0"}],"answer":"B","isMC":true,"question_images":[],"answer_images":[],"choices":{"B":"In the organization's management account, create a service control policy (SCP) to deny actions on EC2 instances by the root user in all member accounts.","A":"Create an identity-based IAM policy in each member account to deny actions on EC2 instances by the root user.","D":"Use Amazon Inspector in each member account to scan for root user logins and to prevent any actions on EC2 instances by the root user.","C":"Use AWS Config to prevent any actions on EC2 instances by the root user."},"timestamp":"2024-01-01 00:07:00","question_text":"A company has several member accounts that are in an organization in AWS Organizations. The company recently discovered that administrators have been using account root user credentials. The company must prevent the administrators from using root user credentials to perform any actions on Amazon EC2 instances.\n\nWhat should a SysOps administrator do to meet this requirement?","exam_id":34,"question_id":329,"answers_community":["B (100%)"],"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/130033-exam-aws-certified-sysops-administrator-associate-topic-1/","unix_timestamp":1704064020,"answer_ET":"B"},{"id":"xkNfwZ1OwlRu5IEKIrUU","answer_description":"","question_images":[],"question_text":"A company is transitioning away from applications that are hosted on Amazon EC2 instances. The company wants to implement a serverless architecture that uses Amazon S3, Amazon API Gateway, AWS Lambda, and Amazon CloudFront. As part of this transition, the company has Elastic IP addresses that are unassociated with any EC2 instances after the EC2 instances are terminated.\n\nA SysOps administrator needs to automate the process of releasing all unassociated Elastic IP addresses that remain after the EC2 instances are terminated.\n\nWhich solution will meet this requirement in the MOST operationally efficient way?","isMC":true,"answer_images":[],"timestamp":"2024-01-02 20:40:00","topic":"1","discussion":[{"poster":"nharaz","comment_id":"1112195","timestamp":"1719942000.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-aws-releaseelasticip.html","upvote_count":"5"},{"upvote_count":"3","timestamp":"1719957540.0","content":"Selected Answer: A\nOption A is operationally efficient as it leverages AWS Config, which is designed for resource configuration tracking and compliance, along with Systems Manager Automation for remediation tasks. This approach is native to AWS and simplifies the process of releasing unassociated Elastic IP addresses.","comment_id":"1112362","poster":"LemonGremlin"}],"exam_id":34,"answers_community":["A (100%)"],"choices":{"C":"Create an Amazon EventBridge rule. Specify AWS services as the event source, Instance State-change Notification as the event type, and Amazon EC2 as the service. Invoke a Lambda function that extracts the Elastic IP address from the notification. Use AWS CloudFormation to release the address by specifying the AllocationId as an input parameter.","A":"Activate the eip-attached AWS Config managed rule to run automatically when resource changes occur in the AWS account. Configure automatic remediation for the rule. Specify the AWS-ReleaseElasticIP AWS Systems Manager Automation runbook for remediation. Specify an appropriate role that has permission for the remediation.","D":"Create a custom Lambda function that calls the EC2 ReleaseAddress API operation and specifies the Elastic IP address AllocationId. Invoke the Lambda function by using an Amazon EventBridge rule. Specify AWS services as the event source, Instance State-change Notification as the event type, and Amazon EC2 as the service.","B":"Create a custom Lambda function that calls the EC2 ReleaseAddress API operation and specifies the Elastic IP address AllocationId. Invoke the Lambda function by using an Amazon EventBridge rule. Specify AWS services as the event source, All Events as the event type, and AWS Trusted Advisor as the target."},"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/130138-exam-aws-certified-sysops-administrator-associate-topic-1/","answer_ET":"A","question_id":330,"unix_timestamp":1704224400}],"exam":{"isImplemented":true,"id":34,"isMCOnly":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":477,"name":"AWS Certified SysOps Administrator - Associate"},"currentPage":66},"__N_SSP":true}