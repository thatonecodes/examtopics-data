{"pageProps":{"questions":[{"id":"LyeovyfNsMS5uG6ThAdB","exam_id":31,"answer":"B","answer_description":"","answer_ET":"B","unix_timestamp":1669651320,"question_id":131,"url":"https://www.examtopics.com/discussions/amazon/view/89141-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","question_text":"A telemarketing company is designing its customer call center functionality on AWS. The company needs a solution that provides multiple speaker recognition and generates transcript files. The company wants to query the transcript files to analyze the business patterns. The transcript files must be stored for 7 years for auditing purposes.\n\nWhich solution will meet these requirements?","choices":{"A":"Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.","D":"Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis.","B":"Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.","C":"Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis."},"discussion":[{"timestamp":"1687543800.0","comments":[{"upvote_count":"8","content":"What bothers me is the 7 years of storage.","timestamp":"1696320840.0","poster":"TheAbsoluteTruth","comment_id":"859672"},{"poster":"enzomv","comments":[{"content":"Disregard. I meant B","upvote_count":"2","comment_id":"788548","poster":"enzomv","timestamp":"1690354380.0"},{"content":"https://aws.amazon.com/about-aws/whats-new/2022/06/amazon-transcribe-supports-automatic-language-identification-multi-lingual-audio/\nAmazon Translate is a service for multi-language identification, which identifies all languages spoken in the audio file and creates transcript using each identified language.","comment_id":"788541","comments":[{"comment_id":"788552","content":"Disregard. I meant Amazon Transcribe","upvote_count":"1","poster":"enzomv","timestamp":"1690354440.0"}],"poster":"enzomv","timestamp":"1690353960.0","upvote_count":"2"}],"upvote_count":"4","content":"The correct answer is C.\nhttps://docs.aws.amazon.com/transcribe/latest/dg/what-is.html\nYou can transcribe streaming media in real time or you can upload and transcribe media files. To see which languages are supported for each type of transcription, refer to the Supported languages and language-specific features table.","comment_id":"786646","timestamp":"1690205280.0"}],"poster":"Buruguduystunstugudunstuy","content":"Selected Answer: B\nThe correct answer is B: Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.\n\nAmazon Transcribe is a service that automatically transcribes spoken language into written text. It can handle multiple speakers and can generate transcript files in real-time or asynchronously. These transcript files can be stored in Amazon S3 for long-term storage.\n\nAmazon Athena is a query service that allows you to analyze data stored in Amazon S3 using SQL. You can use it to analyze the transcript files and identify patterns in the data.\n\nOption A is incorrect because Amazon Rekognition is a service for analyzing images and videos, not transcribing spoken language.\n\nOption C is incorrect because Amazon Translate is a service for translating text from one language to another, not transcribing spoken language.\n\nOption D is incorrect because Amazon Textract is a service for extracting text and data from documents and images, not transcribing spoken language.","comment_id":"754495","upvote_count":"26"},{"poster":"awsgeek75","comment_id":"1110903","content":"Selected Answer: B\nThis is a poorly worded question with poorly worded options. Rekognition and Translate cannot convert speech to text so those options A, C & D are gone. B is the closes option but it does not mention S3 or retention policy of 7 years. Just a best guess on massive assumptions.","upvote_count":"6","timestamp":"1719786720.0"},{"poster":"satyaammm","upvote_count":"1","comment_id":"1356926","timestamp":"1739635680.0","content":"Selected Answer: B\nTranscribe for speech to text conversion and Athena for analysis."},{"content":"Option B is the most relevant one, but it doesn't mention how to retain data in 7 years...","upvote_count":"2","comment_id":"1187448","poster":"james_3005","timestamp":"1727788320.0"},{"comment_id":"1108370","content":"Selected Answer: B\ncheck out this blog here: https://aws.amazon.com/de/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena/","timestamp":"1719633000.0","poster":"SinghJagdeep","upvote_count":"3"},{"upvote_count":"4","comment_id":"1106640","content":"Selected Answer: B\nPerfectly explained here: https://aws.amazon.com/de/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena/","poster":"pentium75","timestamp":"1719465840.0"},{"content":"really hope I could have this kind of question during the exam, 4 different techs in the first 5 words of the answer! Just go with the correct one and ignore the rest of the text XDDD","upvote_count":"3","comment_id":"1038987","poster":"youdelin","timestamp":"1712697000.0"},{"upvote_count":"2","poster":"paniya93","comment_id":"1023012","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena/","timestamp":"1712054580.0"},{"poster":"vijaykamal","upvote_count":"3","timestamp":"1711605840.0","content":"Selected Answer: B\nAmazon Rekognition is primarily designed for image and video analysis, not for transcribing audio or recognizing multiple speakers. -> Option A and D are ruled out\nAmazon Translate is used for language translation -> Option C is ruled out","comment_id":"1019397"},{"poster":"TariqKipkemei","upvote_count":"2","timestamp":"1710743220.0","comment_id":"1010245","content":"Selected Answer: B\nProvide multiple speaker recognition and generate transcript files = Amazon Transcribe\n Query the transcript files = Amazon Athena"},{"poster":"Guru4Cloud","comment_id":"984735","content":"Selected Answer: B\nThe correct answer is B: Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.","timestamp":"1708289100.0","upvote_count":"3"},{"upvote_count":"3","comments":[{"content":"\"The company wants to query the transcript files\" is the requirement. How they will be using the query results \"to analyze the business patterns\" is not our issue.\n\nThe \"7 years\" are not mentioned in any of the options, but Transcribe stores results in S3.","comment_id":"1106639","poster":"pentium75","upvote_count":"2","timestamp":"1719465780.0"}],"poster":"Thornessen","comment_id":"958067","content":"Selected Answer: B\nTricky or incomplete question..\n\nB is the answer because Transcribe is the right service for processing voice calls.\n\nBut 7 years of storage is not covered (should add S3 storage)\n\nAnd Athena querying is just SQL querying, it cannot help you much to recognize business patterns, for that I would think some text analysis service like Comprehend would be needed. \n\nUnless... We use Transcribe not only to transcribe, but also to recognize some key words, and then create a DB/S3 record with multiple fields, e.g. if it is a telemarketing questionnaire, record answer for each question. Then SQL querying might be useful.","timestamp":"1705820820.0"},{"comment_id":"941574","upvote_count":"3","timestamp":"1704273300.0","poster":"sickcow","content":"Selected Answer: C\nTranscribe and (s3) + Athena is the way to go here.\nRedshift sounds like an overkill"},{"timestamp":"1703603760.0","poster":"cookieMr","content":"Amazon Transcribe provides accurate transcription of audio recordings with multiple speakers, generating transcript files. These files can be stored in Amazon S3. To analyze the transcripts and extract insights, Amazon Athena allows SQL-based querying of the stored files.\n\nA. Amazon Rekognition is for image and video analysis, not audio transcription.\n\nC. Amazon Translate is for language translation, not speaker recognition or transcript analysis. Amazon Redshift may not be the best choice for storing and querying transcript files.\n\nD. Amazon Rekognition is for image and video analysis, and Amazon Textract is for document extraction, not suitable for audio transcription or analysis. Storing the transcript files in S3 is appropriate, but the analysis requires a different service like Amazon Athena.","comment_id":"934495","upvote_count":"2"},{"timestamp":"1700925480.0","poster":"Bmarodi","comment_id":"906681","content":"Selected Answer: B\nthe solution that meets these requirements is option B.","upvote_count":"2"},{"timestamp":"1699327920.0","upvote_count":"2","poster":"cheese929","comment_id":"891106","content":"Selected Answer: B\nB is correct"},{"content":"Amazon Transcribe is a service that convert speech into text, so B is the answer","comment_id":"889453","timestamp":"1699116720.0","upvote_count":"2","poster":"Rahulbit34"},{"poster":"k33","comment_id":"850688","content":"Selected Answer: B\nAnswer : B","upvote_count":"3","timestamp":"1695696900.0"},{"upvote_count":"1","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/transcribe/latest/dg/what-is.html","poster":"enzomv","comment_id":"786651","timestamp":"1690205460.0"},{"poster":"master1004","upvote_count":"2","comment_id":"768288","timestamp":"1688706240.0","content":"The correct answer is C.\nWouldn't it be the right answer to save and analyze using Amazon Redshift, which can be used to analyze big data on data warhousing?"},{"content":"B\n\nhttps://aws.amazon.com/transcribe/\nAmazon Transcribe\nAutomatically convert speech to text","upvote_count":"2","comment_id":"755195","timestamp":"1687637700.0","poster":"Chirantan"},{"timestamp":"1687477500.0","upvote_count":"4","poster":"techhb","content":"Selected Answer: B\nOnly B\nashttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c03/view/7/#\nRekognition - Image and Video Analysis\nTranscribe - Text to speech\nTranslate - Translate a text-based file from a language to another language","comment_id":"753795"},{"upvote_count":"3","content":"Selected Answer: B\nRekognition - Image and Video Analysis\nTranscribe - Text to speech \nTranslate - Translate a text based file from a language to another language\nSo by logical deduction is it B","timestamp":"1687441860.0","poster":"kvenikoduru","comment_id":"753446"},{"content":"Selected Answer: B\nB is the right answer. You can specify the S3 bucket with transcribe to store the data for 7 years and use Athena for Analytics later. Transcribe also supports Multiple speaker recognition.","comment_id":"748242","upvote_count":"4","poster":"career360guru","timestamp":"1687016100.0"},{"content":"Selected Answer: B\nAnswer is B - pretty straightforward.","upvote_count":"2","comment_id":"733483","timestamp":"1685678880.0","poster":"justsaysid"},{"poster":"dcyberguy","comment_id":"732194","content":"Selected Answer: B\nAnswer is B.","timestamp":"1685580900.0","upvote_count":"2"},{"content":"Why is it not C?\n\"Amazon Translate is a text translation service that uses advanced machine learning technologies to provide high-quality translation on demand. You can use Amazon Translate to translate unstructured text documents or to build applications that work in multiple languages.\"","poster":"TelaO","comment_id":"732082","upvote_count":"2","comments":[{"timestamp":"1685567880.0","comment_id":"732083","poster":"TelaO","content":"Disregard. I meant B","upvote_count":"1"}],"timestamp":"1685567880.0"},{"timestamp":"1685409720.0","content":"Why it is B instead of C? The question didn't mention to use S3 to store the data, so it cannot be athena ?","comment_id":"731045","upvote_count":"2","poster":"kmaneith","comments":[{"timestamp":"1688963760.0","comment_id":"771117","poster":"JayBee65","content":"\"The transcript files must be stored for 7 years for auditing purposes\" which implied S3 storage. C is text translation (text from language 1 to language 2), you are asked for audio transcription (audio to text), which are completely different things.","upvote_count":"3"}]},{"timestamp":"1685403480.0","upvote_count":"1","content":"B Transcribe","comment_id":"730999","poster":"TonyghostR05"},{"upvote_count":"2","poster":"[Removed]","timestamp":"1685380860.0","content":"Selected Answer: B\nAmazon Transcribe now supports speaker labeling for streaming transcription. Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy for you to convert speech-to-text. In live audio transcription, each stream of audio may contain multiple speakers. Now you can conveniently turn on the ability to label speakers, thus helping to identify who is saying what in the output transcript.\n\nhttps://aws.amazon.com/about-aws/whats-new/2020/08/amazon-transcribe-supports-speaker-labeling-streaming-transcription/","comment_id":"730774"},{"comments":[{"timestamp":"1688963820.0","upvote_count":"3","comment_id":"771118","poster":"JayBee65","content":"D identifies images and video, so completely irrelevant"},{"upvote_count":"4","content":"\"Use Amazon Athena for transcript file analysis\" -> this implies that the data has to reside on S3 so it does take care of the storage question as well.","timestamp":"1685473200.0","comment_id":"731983","poster":"tdkcumberland"}],"comment_id":"730711","poster":"Manlikeleke","upvote_count":"1","content":"Selected Answer: D\nIt cannot be B because it leaves out the storage part of the question.","timestamp":"1685376900.0"},{"content":"Selected Answer: B\nAmazon transcribe convert speech to text and Athena for query","timestamp":"1685358780.0","poster":"Heyang","upvote_count":"2","comment_id":"730419"},{"timestamp":"1685358300.0","content":"Selected Answer: B\nCannot be Rekognition, because it's for:\n • Find objects, people, text, scenes in images and videos using ML\n • Facial analysis and facial search to do user verification, people counting\n • Create a database of \"familiar faces\" or compare against celebrities\n\nTranscribe is for:\n • Automatically convert speech to text\n • Uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately\n • Automatically remove PII using reduction\n • Use cases:\n ○ Transcribe customer service calls\n ○ Automate closed captioning and subtitling\n ○ Generate metadata for media assets to create a fully searchable archive","upvote_count":"3","poster":"Gil80","comment_id":"730411"},{"upvote_count":"2","poster":"JCH760310","content":"Selected Answer: B\nTranscribe - https://aws.amazon.com/about-aws/whats-new/2020/08/amazon-transcribe-supports-speaker-labeling-streaming-transcription/","comment_id":"730279","timestamp":"1685350980.0"},{"timestamp":"1685346060.0","comment_id":"730172","poster":"PS_R","content":"D is not correct one. Amazon Rekognition cannot be used for speech detection.","upvote_count":"2"},{"upvote_count":"2","timestamp":"1685330100.0","comment_id":"729933","poster":"KADSM","content":"B should be the correct answer - transcribe can do speech to text."},{"upvote_count":"2","comments":[{"comment_id":"730575","content":"Amazon Rekognition is used for object detection. transcribe is used for converting speech to text.","timestamp":"1685367840.0","poster":"leonnnn","upvote_count":"2"}],"comment_id":"729692","content":"Selected Answer: D\nD - Amazon Textract for transcript file analysis.","poster":"TMM369","timestamp":"1685307720.0"},{"comment_id":"729354","upvote_count":"2","content":"Selected Answer: B\nAmazon transcribe convert speech to text.","timestamp":"1685282520.0","poster":"leonnnn"}],"answer_images":[],"answers_community":["B (92%)","4%"],"isMC":true,"timestamp":"2022-11-28 17:02:00","question_images":[]},{"id":"dLXgcqfXdhforaSQFSyF","discussion":[{"upvote_count":"69","timestamp":"1665318060.0","poster":"airraid2010","comment_id":"690161","comments":[{"comment_id":"692564","upvote_count":"2","poster":"BoboChow","timestamp":"1665540840.0","content":"I agree C is the answer"},{"comment_id":"690542","poster":"tt79","timestamp":"1665349440.0","upvote_count":"1","content":"C is right."}],"content":"Answer: C\n\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds."},{"content":"Selected Answer: C\nKeyword:\n- Queries will be simple and will run on-demand.\n- Minimal changes to the existing architecture.\nA: Incorrect - We have to do 2 step. load all content to Redshift and run SQL query (This is simple query so we can you Athena, for complex query we will apply Redshit)\nB: Incorrect - Our query will be run on-demand so we don't need to use CloudWatch Logs to store the logs.\nC: Correct - This is simple query we can apply Athena directly on S3\nD: Incorrect - This take 2 step: use AWS Glue to catalog the logs and use Spark to run SQL query","timestamp":"1680488040.0","upvote_count":"45","poster":"PhucVuu","comment_id":"859496"},{"comment_id":"1359293","timestamp":"1740062220.0","poster":"Bl_12","upvote_count":"1","content":"Selected Answer: A\nRedshift is query"},{"timestamp":"1738760880.0","poster":"sumanl75","upvote_count":"1","content":"Selected Answer: C\nC Correct answer. Athena integrates seamlessly with S3 and allows you to run simple SQL queries in no time. When working with Apache Spark or with SQL in S3, using this service is the best option","comment_id":"1351874"},{"content":"Selected Answer: C\nAmazon Athena meets the requirements for on-demand log analysis with the least operational overhead. It integrates seamlessly with data in Amazon S3, uses SQL for querying, and does not require managing servers or clusters","upvote_count":"1","comment_id":"1335415","timestamp":"1735808100.0","poster":"liamezr"},{"timestamp":"1735694460.0","comment_id":"1335094","content":"Selected Answer: C\nAmazon Athena meets the requirements for on-demand log analysis with the least operational overhead. It integrates seamlessly with data in Amazon S3, uses SQL for querying, and does not require managing servers or clusters.","poster":"ANDREWKIM1","upvote_count":"1"},{"content":"Selected Answer: C\nAWS Athena is a serverless, interactive query service that allows you to analyze data directly in Amazon S3 using standard SQL. \nIt is commonly used for querying logs, performing ad hoc analysis, and building dashboards. Key aspects of AWS Athena include:\nKey Features:\n1)Serverless:\nNo infrastructure to manage; pay only for the queries you run.\n3)Query S3 Data:\nWorks directly with structured, semi-structured, and unstructured data stored in S3 (e.g., CSV, JSON, ORC, Parquet).\n3)Integration:\nWorks seamlessly with AWS Glue for data cataloging, enabling you to query datasets more efficiently.\n4)SQL Support:\nBuilt on Presto, Athena supports ANSI SQL.\n5)Pay-Per-Query:\nPricing is based on the amount of data scanned per query.","timestamp":"1735136820.0","poster":"MGKYAING","comment_id":"1331585","upvote_count":"1"},{"poster":"vietqtran","content":"Selected Answer: C\nAWS Athena is an interactive data analytics service from Amazon Web Services (AWS) that allows you to run SQL queries directly on data stored in Amazon S3 without having to manage infrastructure. Athena provides a quick and easy way to analyze data in formats such as CSV, JSON, ORC, Parquet, and Avro without having to move data outside of S3.","timestamp":"1732679400.0","comment_id":"1318415","upvote_count":"1"},{"timestamp":"1731702840.0","content":"not related to this question, but practically, using Redshift Spectrum would be an option here?","comments":[{"timestamp":"1731976980.0","comment_id":"1314312","content":"spectrum best for complex query that need parallel/multi processing.","poster":"zoftdev","upvote_count":"1"}],"poster":"yyfabc","comment_id":"1312806","upvote_count":"1"},{"timestamp":"1728992340.0","content":"Selected Answer: C\nAmazon Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena. It uses\nAmazon QuickSight for data visualization\nAWS Glue Data Catalog allows you to create tables and query data in Athena based on a central metadata store available","poster":"whileloops","upvote_count":"1","comment_id":"1298206"},{"content":"Selected Answer: C\nTo meet the requirements of analyzing log files stored in JSON format in an Amazon S3 bucket with minimal changes to the existing architecture and minimal operational overhead, the most suitable option would be Option C: Use Amazon Athena directly with Amazon S3 to run the queries as needed.\n\nAmazon Athena is a serverless interactive query service that allows you to analyze data directly from Amazon S3 using standard SQL queries. It eliminates the need for infrastructure provisioning or data loading, making it a low-overhead solution.\n\nOverall, Amazon Athena offers a straightforward and efficient solution for analyzing log files stored in JSON format, ensuring minimal operational overhead and compatibility with simple on-demand queries.","poster":"cookieMr","timestamp":"1726903020.0","upvote_count":"3","comment_id":"926012"},{"poster":"Andreshere","content":"A. Despite this option can be valid, it implies a bit of operational overhead compared with other options. Additionally, there is no need to aggregate that change to the existing architecture because we are already working in S3, and using other storage services incurs unnecessary costs. \nB. To collect the logs, we use CloudTrail over CloudWatch. Running SQL queries from the Amazon CloudWatch console is not recommended for this use case, since it is more used for filtering. \nC. Correct answer. Athena integrates seamlessly with S3 and allows you to run simple SQL queries in no time. When working with Apache Spark or with SQL in S3, using this service is the best option.\nD. This option incurs elevated operational overhead. Glue is not used to catalog the logs. Analyzing logs with Spark on an EMR cluster is very common, but you can do it faster with the Athena service integrated with S3 directly.","timestamp":"1726902960.0","upvote_count":"4","comment_id":"1116811"},{"comment_id":"1263376","upvote_count":"1","poster":"PaulGa","timestamp":"1723277160.0","content":"C. Athena lets you analyse S3 data using standard SQL. No other steps needed"},{"comment_id":"1223965","poster":"ChymKuBoy","content":"C for sure","upvote_count":"1","timestamp":"1717480920.0"},{"content":"Selected Answer: C\nUsing Amazon Athena with Amazon S3 is direct and efficient way for querying JSON log files with minimum operational overhead.","timestamp":"1716982740.0","poster":"Ishu_","comment_id":"1220872","upvote_count":"1"},{"timestamp":"1713411360.0","comment_id":"1197667","upvote_count":"1","content":"Selected Answer: C\nA, B, D operational overhead. C accept all requirement","poster":"nanadaime"},{"upvote_count":"1","comment_id":"1197080","poster":"Muavia","timestamp":"1713340500.0","content":"Option A is correct\nAmazon Athena is an interactive query service provided by Amazon Web Services (AWS) that enables you to analyze data stored in Amazon S3 (Simple Storage Service) using standard SQL queries."},{"timestamp":"1710386340.0","upvote_count":"1","content":"Answer should be C, Simple approach, Store logs in S3 and use Athena to query. Redshift will be costly approach.Cloudwatch does not store any data. So A and B ruled out.","comment_id":"1173057","poster":"48cd959"},{"comment_id":"1121946","timestamp":"1705170120.0","poster":"awsgeek75","content":"Selected Answer: C\nS3 + Athena is simples approach","upvote_count":"1"},{"content":"Selected Answer: C\nC seems right.","timestamp":"1705148280.0","upvote_count":"1","comment_id":"1121600","poster":"A_jaa"},{"timestamp":"1704728940.0","content":"Selected Answer: C\nAmazon Athena, because it provides the easiest way to run simple SQL service on a on-demand basis on an S3 bucket. The data is not complex so Redshift and EMR are a overhead or simply not suitable. CloudWatch does not have a console where you can run queries.","upvote_count":"3","poster":"andreadhelpra","comment_id":"1116737"},{"comment_id":"1091103","content":"Selected Answer: C\nC seems to be fine","poster":"karolmrozik","upvote_count":"1","timestamp":"1702041600.0"},{"comment_id":"1078622","content":"Selected Answer: C\nNo need to build a server and it is on the fly","poster":"Genlor","timestamp":"1700762580.0","upvote_count":"1"},{"poster":"Ruffyit","upvote_count":"1","timestamp":"1700211660.0","comment_id":"1073124","content":"Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds."},{"content":"C. \nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. No operational overhead","upvote_count":"1","comment_id":"1053595","poster":"Ruffyit","timestamp":"1698225480.0"},{"upvote_count":"1","content":"Selected Answer: C\nSimple and fast","comment_id":"1042784","timestamp":"1697211840.0","poster":"pedrogaf"},{"comment_id":"1021346","content":"serverless operation simply","poster":"Abitek007","timestamp":"1696062240.0","upvote_count":"1"},{"content":"Selected Answer: C\nKeyword:\n- needs to perform the analysis with minimal changes to the existing architecture.\n- LEAST amount of operational overhead.\nC","upvote_count":"1","comment_id":"989751","poster":"thanhnv","timestamp":"1692947100.0"},{"upvote_count":"1","poster":"Theocode","timestamp":"1691406540.0","comment_id":"974615","content":"Selected Answer: C\nA no-brainer, Athena can be used to query data directly from s3"},{"content":"Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.","poster":"sandhyaejji","upvote_count":"1","comment_id":"973238","timestamp":"1691256000.0"},{"upvote_count":"1","content":"Selected Answer: C\nAmazon Athena is a serverless, interactive analytics service used to query data in relational, nonrelational, object, and custom data sources running on S3.","poster":"TariqKipkemei","timestamp":"1690430700.0","comment_id":"964313"},{"comment_id":"954257","upvote_count":"4","content":"Selected Answer: C\nExplanation:\nOption C is the most suitable choice for this scenario. Amazon Athena is a serverless query service that allows you to analyze data directly from Amazon S3 using standard SQL queries. Since the log files are already stored in JSON format in an S3 bucket, there is no need for data transformation or loading into another service. Athena can directly query the JSON logs without the need for any additional infrastructure.","poster":"Guru4Cloud","timestamp":"1689600840.0"},{"content":"Best option is CCCC","comment_id":"949943","timestamp":"1689175800.0","upvote_count":"1","poster":"miki111"},{"upvote_count":"1","content":"Selected Answer: C\nathena is great for querying data in s3","timestamp":"1688376780.0","poster":"animefan1","comment_id":"941667"},{"upvote_count":"1","content":"Selected Answer: C\nC is right","poster":"oaidv","comment_id":"934046","timestamp":"1687755960.0"},{"content":"Selected Answer: C\nI agree C","upvote_count":"1","poster":"KAMERO","timestamp":"1687192860.0","comment_id":"927698"},{"timestamp":"1687190700.0","comment_id":"927670","poster":"Gullashekar","content":"Selected Answer: C\nC","upvote_count":"1"},{"comment_id":"912070","content":"Selected Answer: C\nC is answer.","upvote_count":"1","poster":"Bmarodi","timestamp":"1685621940.0"},{"content":"C is correct","poster":"cheese929","upvote_count":"1","comment_id":"897307","timestamp":"1684045200.0"},{"poster":"rag300","content":"Selected Answer: C\nserverless to avoid operational overhead c is the answer","timestamp":"1682524380.0","upvote_count":"2","comment_id":"881789"},{"upvote_count":"1","content":"Selected Answer: C\nEs el C \nsin dudas, las opciones A y D requieren una infraestructura de base de datos separada, lo que puede aumentar los costos operativos. La opción B no es adecuada para este escenario ya que Amazon CloudWatch Logs no admite consultas SQL directamente y puede requerir una transformación adicional de los datos antes de que se puedan analizar.","poster":"juanvaliente","timestamp":"1681328340.0","comment_id":"868744"},{"comment_id":"855966","timestamp":"1680194400.0","poster":"linux_admin","content":"Selected Answer: C\nOption C proposes using Amazon Athena directly with Amazon S3 to run queries as needed. This would allow for simple on-demand queries without any additional infrastructure setup or maintenance. Athena is designed for querying data stored in S3 using SQL statements and can handle a variety of file formats, including JSON. Athena also provides a serverless solution with no infrastructure to manage, allowing the solutions architect to focus on the data analysis instead of the infrastructure.","upvote_count":"3"},{"content":"Selected Answer: C\nOption C is the simplest and most efficient solution for analyzing log files stored in JSON format in an Amazon S3 bucket with minimal changes to the existing architecture.","timestamp":"1679922780.0","poster":"elearningtakai","comment_id":"852134","upvote_count":"1"},{"upvote_count":"1","poster":"channn","comment_id":"850964","timestamp":"1679831820.0","content":"Selected Answer: C\ni choose C"},{"timestamp":"1679553840.0","poster":"tienhoboss","comment_id":"847881","upvote_count":"1","content":"Selected Answer: C\nAthena is a good choice."},{"content":"Selected Answer: C\nAnswer is C.","timestamp":"1679372880.0","poster":"iamRohanKaushik","upvote_count":"1","comment_id":"845513"},{"timestamp":"1679339220.0","upvote_count":"2","comment_id":"845166","poster":"Mehkay","content":"C is the correct option."},{"content":"Answer: C Use Amazon Athena directly with Amazon S3 to run the queries as needed.","upvote_count":"1","timestamp":"1678775220.0","comment_id":"838599","poster":"lalithamurthy"},{"comment_id":"828792","upvote_count":"1","poster":"Ruhi02","timestamp":"1677922800.0","content":"Answer c"},{"upvote_count":"1","poster":"bilel500","comment_id":"817947","content":"Selected Answer: C\nAnswer is C.","timestamp":"1677077400.0"},{"poster":"Victorn","comment_id":"811926","upvote_count":"5","timestamp":"1676641200.0","content":"Amazon Athena\n\nAthena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena."},{"poster":"buiducvu","upvote_count":"1","comment_id":"807017","content":"Selected Answer: C\nAnswer: C","timestamp":"1676258760.0"},{"timestamp":"1676258700.0","upvote_count":"1","content":"Selected Answer: C\nC is right","comment_id":"807016","poster":"buiducvu"},{"timestamp":"1672898400.0","upvote_count":"1","comments":[{"timestamp":"1675871280.0","content":"You missed this part \"minimal changes to the existing architecture.\" There is a lot you have to implement for D.","upvote_count":"1","comment_id":"802197","poster":"Ello2023"}],"poster":"envest","content":"IMO: on-demand with least overhead would mean automated serverless (e.g. schedule). Answer A lacks Spectrum, Answer C lacks Glue, but D has all necessary components & services (Glue, Spark, EMR serverless). But for simple log queries it takes a lot of serverless know how thought for big data and not logs. Considering this, I go with D.","comment_id":"766277"},{"timestamp":"1671840600.0","content":"It's C","comment_id":"754610","upvote_count":"1","poster":"Zerotn3"},{"comments":[{"poster":"Buruguduystunstugudunstuy","upvote_count":"1","content":"Overall, Option C is the most straightforward and least operationally complex solution for analyzing the log files using SQL.","comment_id":"750247","timestamp":"1671484800.0"}],"comment_id":"750246","content":"Selected Answer: C\nAnswered by ChatGPT\n\nThe correct solution that the solutions architect should do to meet these requirements with the least amount of operational overhead is Option C: Use Amazon Athena directly with Amazon S3 to run the queries as needed.\n\nOption C involves using Amazon Athena, which is a fully managed, serverless query service that allows you to analyze data stored in Amazon S3 using SQL. Athena is particularly well suited for analyzing JSON-formatted data, such as the log files in this case. You can use Athena to run on-demand queries against the log data in S3, without the need to set up any infrastructure or perform any data ingestion or transformation tasks.","poster":"Buruguduystunstugudunstuy","timestamp":"1671484740.0","upvote_count":"2"},{"content":"Selected Answer: C\nAthena analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena.","upvote_count":"1","poster":"psr83","timestamp":"1671429420.0","comment_id":"749506"},{"upvote_count":"1","timestamp":"1671209100.0","poster":"NikaCZ","comment_id":"747424","content":"Selected Answer: C\nAmazon Athena is an interactive query service that makes it easy to analyze data directly."},{"poster":"Myxa","comment_id":"747116","timestamp":"1671191220.0","upvote_count":"1","content":"Selected Answer: C\nIts C."},{"comment_id":"739819","content":"C is correct","timestamp":"1670564400.0","poster":"benaws","upvote_count":"1"},{"comment_id":"735296","upvote_count":"1","content":"Selected Answer: C\nC is correct","poster":"AlaN652","timestamp":"1670175180.0"},{"poster":"Drekorig","timestamp":"1669042620.0","content":"Selected Answer: C\nAthena allows query data stored in S3 with SQL","comment_id":"723653","upvote_count":"1"},{"comment_id":"723457","upvote_count":"1","timestamp":"1669033140.0","poster":"Wpcorgan","content":"C is correct"},{"timestamp":"1667672280.0","poster":"pm2229","upvote_count":"1","content":"Serverless query service to perform analytics on S3.","comment_id":"711943"},{"poster":"17Master","upvote_count":"3","timestamp":"1666982640.0","comment_id":"706706","content":"Selected Answer: C\nAmazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run."},{"content":"Selected Answer: C\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL.","poster":"cheese929","timestamp":"1666937640.0","comment_id":"706185","upvote_count":"1"},{"timestamp":"1665946020.0","poster":"queen101","comment_id":"696439","content":"CCCCCCCCCC","upvote_count":"1"},{"comment_id":"695055","timestamp":"1665791940.0","content":"Selected Answer: C\nAnswer is C.","upvote_count":"1","poster":"bilel500"},{"upvote_count":"2","poster":"sba21","content":"Selected Answer: C\nAthena allows directly query data stored in S3","timestamp":"1665469440.0","comment_id":"691820"},{"comment_id":"691041","upvote_count":"1","content":"Answer: C\nAthena is only service that allows directly query data stored in S3","poster":"sba21","timestamp":"1665400260.0"},{"poster":"D2w","comment_id":"690914","content":"Selected Answer: C\nAmazon Athena is Serverless query service to perform analytics against S3 objects. And cz it wants minimal changes I'll definitely go with it.","upvote_count":"3","timestamp":"1665392700.0"}],"timestamp":"2022-10-09 14:21:00","answers_community":["C (99%)","1%"],"answer_description":"","answer_images":[],"unix_timestamp":1665318060,"isMC":true,"answer":"C","exam_id":31,"question_images":[],"choices":{"D":"Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed.","A":"Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.","C":"Use Amazon Athena directly with Amazon S3 to run the queries as needed.","B":"Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console."},"question_text":"A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture.\nWhat should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?","question_id":132,"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/84848-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1"},{"id":"5rgtMybsAx3d2MKU1mk0","exam_id":31,"answer":"D","answer_description":"","answer_ET":"D","unix_timestamp":1665548520,"url":"https://www.examtopics.com/discussions/amazon/view/85226-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":133,"topic":"1","question_text":"A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance.\nA solutions architect needs to minimize the time that is required to clone the production data into the test environment.\nWhich solution will meet these requirements?","choices":{"B":"Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.","A":"Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.","D":"Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.","C":"Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots."},"answer_images":[],"discussion":[{"comment_id":"704695","upvote_count":"54","poster":"UWSFish","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html\n\nAmazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.","timestamp":"1666789380.0"},{"upvote_count":"40","comment_id":"863573","poster":"PhucVuu","timestamp":"1726905300.0","content":"Selected Answer: D\nKeywords:\n- Modifications to the cloned data must not affect the production environment.\n- Minimize the time that is required to clone the production data into the test environment.\n\nA: Incorrect - we can do this But it is not minimize the time as requirement.\nB: Incorrect - This approach use same EBS volumes for produciton and test. If we modify test then it will be affected prodution environment.\nC: Incorrect - EBS snapshot will create new EBS volumes. It can not restore from existing volumes.\nD: Correct - Turn on the EBS fast snapshot restore feature on the EBS snapshots -> no latency on first use"},{"upvote_count":"1","content":"Selected Answer: D\nAns D - as per PhucVuu response... what's to debate...","timestamp":"1723381380.0","poster":"PaulGa","comment_id":"1264161"},{"upvote_count":"1","timestamp":"1722522600.0","poster":"1e22522","content":"Selected Answer: D\nYe its d cuh","comment_id":"1259438"},{"comment_id":"1168717","poster":"1dfed2b","timestamp":"1709891700.0","upvote_count":"2","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-restoring-volume.html\nIts C. reate a new volume from the snapshot. Use the create-volume command. For --snapshot-id, specify the ID of the snapshot to use. For --availability-zone, specify the same Availability Zone as the instance. Configure the remaining parameters as needed."},{"poster":"lsomas","comment_id":"1150774","timestamp":"1707977880.0","content":"Answer is D because volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.\nVolumes created from normal snapshots will take time to initialize","upvote_count":"2"},{"upvote_count":"3","timestamp":"1705177320.0","poster":"awsgeek75","content":"Selected Answer: D\nA: Can work but long cloning time\nB: Wrong as multi attach will mean changes by test will affect production\nC: Slow\nD: Fast restore makes this a quicker option","comment_id":"1122024"},{"content":"Selected Answer: D\nAnswer-D","upvote_count":"1","timestamp":"1705149300.0","comment_id":"1121629","poster":"A_jaa"},{"timestamp":"1698309480.0","comment_id":"1054399","poster":"Ruffyit","content":"Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.","upvote_count":"1"},{"content":"Selected Answer: A\nwhy not A? high I/O, no need durability","comment_id":"980746","comments":[{"comment_id":"1001257","content":"Although it is test environment, it's data should be durable","timestamp":"1694069820.0","poster":"JackLo","upvote_count":"3"}],"timestamp":"1692013860.0","poster":"ukivanlamlpi","upvote_count":"2"},{"comment_id":"968716","upvote_count":"1","content":"Selected Answer: D\nNeeds to minimize the time that is required to clone the production data into the test environment = EBS fast snapshot restore feature","timestamp":"1690864980.0","poster":"TariqKipkemei"},{"comments":[{"upvote_count":"2","timestamp":"1703424480.0","content":"But you don't need a new, empty volume, you need a restore of the PROD snapshot. Thus D.","comment_id":"1104616","poster":"pentium75"}],"poster":"Anil_Awasthi","comment_id":"965206","timestamp":"1690507800.0","content":"Selected Answer: C\nOption C provides an effective solution for cloning large amounts of production data into a test environment with minimized time, high I/O performance, and without affecting the production environment.","upvote_count":"1"},{"comment_id":"957722","content":"Selected Answer: D\nThe correct answer is D.\nHere is a step-by-step explanation of how to clone production data into a test environment using EBS snapshots:\n Take EBS snapshots of the production EBS volumes.\n Turn on the EBS fast snapshot restore feature on the EBS snapshots.\n Restore the snapshots into new EBS volumes.\n Attach the new EBS volumes to EC2 instances in the test environment.\nThe EBS fast snapshot restore feature allows you to restore snapshots more quickly than the default method. This is because the feature uses a process called parallel restore, which allows multiple EBS volumes to be restored at the same time.\nThe EBS fast snapshot restore feature is only available for EBS snapshots that are created in the same AWS Region as the EC2 instances that you are using to restore the snapshots.","poster":"Guru4Cloud","upvote_count":"6","timestamp":"1689865440.0"},{"comments":[{"content":"Its not, D its the solution. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html","upvote_count":"1","poster":"idanr391","timestamp":"1689410400.0","comment_id":"952185"}],"comment_id":"950311","upvote_count":"1","poster":"Thornessen","timestamp":"1689220980.0","content":"For consistently high IO, option A is the solution. Instance store has the highest IO"},{"content":"Option D is the ideal answer.","upvote_count":"1","poster":"miki111","timestamp":"1689181560.0","comment_id":"950032"},{"timestamp":"1687078260.0","comment_id":"926562","upvote_count":"2","content":"Selected Answer: D\nTake EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.\n\nEnabling the EBS fast snapshot restore feature allows you to restore EBS snapshots into new EBS volumes almost instantly, without needing to wait for the data to be fully copied from the snapshot. This significantly reduces the time required to clone the production data.\n\nBy taking EBS snapshots of the production EBS volumes and restoring them into new EBS volumes in the test environment, you can ensure that the cloned data is separate and does not affect the production environment. Attaching the new EBS volumes to the EC2 instances in the test environment allows you to access the cloned data.","poster":"cookieMr"},{"content":"Selected Answer: D\nAmazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.","poster":"TienHuynh","upvote_count":"1","timestamp":"1686849420.0","comment_id":"924422"},{"comment_id":"897367","poster":"cheese929","timestamp":"1684050240.0","content":"Selected Answer: D\nD is correct","upvote_count":"1"},{"content":"You can use EBS Fast Snapshot restore feature to restore EBS snapshots to a new EBS volume with minimal downtime.","poster":"Abrar2022","upvote_count":"1","timestamp":"1683955860.0","comment_id":"896414"},{"comment_id":"893621","upvote_count":"1","content":"ANSWER - C","poster":"EA100","timestamp":"1683695100.0"},{"comment_id":"858589","timestamp":"1680419580.0","upvote_count":"1","poster":"channn","content":"Selected Answer: D\nKey words: minimize the time"},{"comment_id":"823774","timestamp":"1677506520.0","content":"Selected Answer: D\nThe EBS fast snapshot restore feature allows you to restore EBS snapshots to new EBS volumes with minimal downtime. This is particularly useful when you need to restore large volumes or when you need to restore a volume to an EC2 instance in a different Availability Zone. When you enable the fast snapshot restore feature, the EBS volume is restored from the snapshot in the shortest amount of time possible, typically within a few minutes.","upvote_count":"1","poster":"bilel500"},{"timestamp":"1676094000.0","upvote_count":"5","poster":"Bofi","content":"Selected Answer: A\nOption A is correct because the question stated that the software that will access the test environment needs High I/O performance which is the core feature of instance store. The only risk for instance store its lost when the EC2 that it is attached to is terminated, however, this is a test environment, long term durability may not be required. Option C is not correct because it mentioned creating a new EBS and restoring the snapshot. The snap shot can be restored without creating a new EBS. It did not satisfy the minimum overhead requirement","comment_id":"805012"},{"comment_id":"803530","timestamp":"1675963680.0","poster":"Ello2023","content":"Selected Answer: D\nD. They are all viable solutions, however EBS fast snapshot will increase the speed as the question does ask for minimal time and not about cost, automation, minimum overheads etc.","upvote_count":"1"},{"content":"C is correct\nOption A, restoring EBS snapshots onto EC2 instance store volumes is not correct, because EC2 Instance store volumes are not as durable as EBS volumes, it may not guarantee the data durability and availability.\nOption B, using the EBS Multi-Attach feature is not correct, because it would still need to detach and reattach the volumes, and it will cause the data unavailability.\nOption D, using the EBS fast snapshot restore feature is not correct, it would still require to create new volumes and attach them to the instances, and it does not guarantee the data ready for use as soon as the restore process completes.","timestamp":"1673582820.0","comments":[{"poster":"BlueVolcano1","upvote_count":"1","comment_id":"778949","timestamp":"1673961660.0","content":"Option B is wrong because Multi-Attach (which isn't available for all instance types) allows attaching the SAME EBS volume to multiple EC2 instances, which would mean that modifications in the test environment would also modify production data.\n\nOption D is correct, the data IS ready for use as soon as the restore process completes. It ensures that the I/O performance remains consistent even when reading blocks for the first time.\n\nOption C is incorrect as it's saying you're creating new instances with completely new volumes and THEN restoring the EBS snapshots. Creating new, empty volumes is unnecessary. Just restore them from the EBS snapshot."}],"comment_id":"774085","poster":"jannymacna","upvote_count":"2"},{"timestamp":"1673582520.0","poster":"jannymacna","comments":[{"content":"The EBS fast snapshot restore feature is the one that gives you consistently high I/O performance.\n\nFrom the AWS docs:\n\"Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.\"\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html","upvote_count":"1","timestamp":"1673961840.0","poster":"BlueVolcano1","comment_id":"778953"}],"comment_id":"774081","content":"C. Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.\nTake EBS snapshots of the production EBS volumes, which are point-in-time copies of the data.\nCreate and initialize new EBS volumes in the test environment.\nAttach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots. This will allow the data to be ready for use as soon as the restore process completes, and it ensures that the software that accesses the data will have consistently high I/O performance.","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nThe EBS fast snapshot restore feature allows you to restore EBS snapshots to new EBS volumes with minimal downtime. This is particularly useful when you need to restore large volumes or when you need to restore a volume to an EC2 instance in a different Availability Zone. When you enable the fast snapshot restore feature, the EBS volume is restored from the snapshot in the shortest amount of time possible, typically within a few minutes.","comment_id":"768049","poster":"SilentMilli","timestamp":"1673035740.0"},{"content":"Selected Answer: D\nTake EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes.","timestamp":"1672633440.0","comment_id":"763488","poster":"techhb","upvote_count":"2"},{"timestamp":"1672172580.0","comment_id":"759027","comments":[{"timestamp":"1672172580.0","upvote_count":"5","poster":"Buruguduystunstugudunstuy","comment_id":"759029","content":"Option A is incorrect because restoring EBS snapshots onto EC2 instance store volumes will not provide consistently high I/O performance.\n\nOption B is incorrect because using the EBS Multi-Attach feature to attach the production EBS volumes to the EC2 instances in the test environment could potentially affect the production environment and is not a recommended practice.\n\nOption C is incorrect because creating and initializing new EBS volumes and restoring the production data onto them can take longer than restoring the data from an EBS snapshot with the EBS fast snapshot restore feature."}],"upvote_count":"2","content":"Selected Answer: D\nThe solution that will meet these requirements is D: Take EBS snapshots of the production EBS volumes, turn on the EBS fast snapshot restore feature on the EBS snapshots, and restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.\n\nEBS fast snapshot restore is a feature that enables you to restore an EBS snapshot to a new EBS volume within seconds, providing consistently high I/O performance. By taking EBS snapshots of the production EBS volumes, turning on the EBS fast snapshot restore feature, and restoring the snapshots into new EBS volumes, you can quickly clone the production data into the test environment and minimize the time required to do so. The new EBS volumes can be attached to EC2 instances in the test environment to provide access to the cloned data.","poster":"Buruguduystunstugudunstuy"},{"timestamp":"1671987660.0","upvote_count":"1","comment_id":"755842","content":"Selected Answer: D\n\nAmazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html","poster":"Chirantan"},{"upvote_count":"2","comment_id":"754921","timestamp":"1671890760.0","poster":"psr83","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/aws/new-amazon-ebs-fast-snapshot-restore-fsr/"},{"poster":"NikaCZ","timestamp":"1671213120.0","comment_id":"747470","content":"Selected Answer: D\nD. Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.","upvote_count":"2"},{"comment_id":"743229","poster":"Shasha1","timestamp":"1670872200.0","upvote_count":"1","content":"Answer :C \ntake EBS snapshots of the production EBS volumes and create new EBS volumes in the test environment. The new EBS volumes should be initialized and attached to the EC2 instances in the test environment before restoring the production data from the EBS snapshots. This will minimize the time that is required to clone the production data, as the new EBS volumes will be ready to accept the data from the EBS snapshots as soon as the snapshots are restored. Option D, using the EBS fast snapshot restore feature, would not provide a solution for minimizing the time that is required to clone the data."},{"comment_id":"723491","content":"D is correct","poster":"Wpcorgan","upvote_count":"1","timestamp":"1669034820.0"},{"timestamp":"1668246900.0","upvote_count":"2","comment_id":"716625","poster":"Wajif","content":"Selected Answer: D\nMinimize the time is a key requirement. So D."},{"content":"Selected Answer: D\nD seems to be the correct one.","upvote_count":"1","timestamp":"1666037160.0","poster":"GameDad09","comment_id":"697669"},{"content":"Selected Answer: D\nD: Fast snapshots\n\nAmazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.","timestamp":"1665958500.0","poster":"ninjawrz","comment_id":"696611","upvote_count":"2"},{"comment_id":"696465","timestamp":"1665948180.0","upvote_count":"1","content":"DDDDDDDDDDDDD","poster":"queen101"},{"comment_id":"696257","upvote_count":"3","poster":"123jhl0","timestamp":"1665926880.0","content":"Selected Answer: D\nThe \"fast snapshot restore\" on EBS eliminates the initialisation time required for the EBS volumes, providing both requests: 1) reduces time to clone data from production to test and 2) provide consistently high performance (as volumes are already initialised)\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html"},{"poster":"[Removed]","timestamp":"1665575580.0","comment_id":"693026","content":"C is the answer\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/restore.html","upvote_count":"1"},{"comment_id":"692623","timestamp":"1665548520.0","upvote_count":"5","poster":"BoboChow","content":"Selected Answer: D\nhttps://aws.amazon.com/cn/about-aws/whats-new/2020/11/amazon-ebs-fast-snapshot-restore-now-available-us-govcloud-regions/"}],"answers_community":["D (93%)","5%"],"isMC":true,"timestamp":"2022-10-12 06:22:00","question_images":[]},{"id":"nrj9E29MNBXMbDvufVzt","question_text":"A company hosts its application on AWS. The company uses Amazon Cognito to manage users. When users log in to the application, the application fetches required data from Amazon DynamoDB by using a REST API that is hosted in Amazon API Gateway. The company wants an AWS managed solution that will control access to the REST API to reduce development efforts.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","timestamp":"2022-11-28 17:05:00","question_images":[],"discussion":[{"poster":"Buruguduystunstugudunstuy","timestamp":"1687543320.0","content":"Selected Answer: D\nKEYWORD: LEAST operational overhead\n\nTo control access to the REST API and reduce development efforts, the company can use an Amazon Cognito user pool authorizer in API Gateway. This will allow Amazon Cognito to validate each request and ensure that only authenticated users can access the API. This solution has the LEAST operational overhead, as it does not require the company to develop and maintain any additional infrastructure or code.\n\nTherefore, Option D is the correct answer.\n\nOption D. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.","upvote_count":"15","comment_id":"754489"},{"content":"Selected Answer: D\nA is possible if the authorisation logic makes sense and does not require operational overhead.\nB is too much overhead for each new user.\nC is lol\nD Company already has Cognito for it's users so just integrate it with the API gateway\n\nThis question and options are poorly worded an A could be a reasonable choice if more information is provided. Just keep that in mind for the exam!","upvote_count":"5","comment_id":"1110904","poster":"awsgeek75","timestamp":"1719787020.0"},{"poster":"ad2dj28","timestamp":"1739078760.0","upvote_count":"1","comment_id":"1353723","content":"Selected Answer: D\nyou can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway"},{"comment_id":"1213897","content":"Control access to a REST API using Amazon Cognito user pools as authorizer\n\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html","upvote_count":"3","poster":"drich22","timestamp":"1732044000.0"},{"timestamp":"1728846540.0","comment_id":"1195104","content":"Selected Answer: D\nAnswer D\n\nBy integrating Amazon Cognito User Pools with API Gateway, you can secure your APIs and control access based on user authentication and authorization, allowing you to build secure and scalable web and mobile applications.","poster":"MehulKapadia","upvote_count":"2"},{"comment_id":"1187127","poster":"Adi312100","content":"Selected Answer: D\nOption D. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.","upvote_count":"2","timestamp":"1727747880.0"},{"timestamp":"1718373780.0","content":"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html","upvote_count":"3","comment_id":"1096620","poster":"osmk"},{"poster":"Tom123456ac","content":"The description of this question is really bad. Company is using Cognito to manage users already, but still verifying user info from dynamodb, very wired situation. But just select Cognito when you see Api gateway + cognito + authentication + least efforts","comment_id":"1025207","upvote_count":"4","timestamp":"1712273640.0"},{"timestamp":"1710743460.0","upvote_count":"2","comment_id":"1010247","content":"Selected Answer: D\nuse Amazon Cognito to authorize user requests.","poster":"TariqKipkemei"},{"poster":"Guru4Cloud","comment_id":"1005987","content":"Selected Answer: D\nD. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request","timestamp":"1710275280.0","upvote_count":"3"},{"comment_id":"984736","poster":"Guru4Cloud","upvote_count":"3","timestamp":"1708289160.0","content":"Selected Answer: D\nOption D is the best solution with the least operational overhead:\n\nConfigure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.\n\nThe key reasons are:\n\nº Cognito user pool authorizers allow seamless integration between Cognito and API Gateway for access control.\nº API Gateway handles validating the access tokens from Cognito automatically without any custom code.\nº This is a fully managed solution with minimal ops overhead."},{"content":"By configuring an Amazon Cognito user pool authorizer in API Gateway, you can leverage the built-in functionality of Amazon Cognito to authenticate and authorize users. This eliminates the need for custom development or managing access keys. Amazon Cognito handles user authentication, securely manages user identities, and provides seamless integration with API Gateway for controlling access to the REST API.\n\nA. Configuring an AWS Lambda function as an authorizer in API Gateway would require custom implementation and management of the authorization logic.\n\nB. Creating and assigning an API key for each user would require additional management and validation logic in an AWS Lambda function.\n\nC. Sending the user's email address in the header and validating it with an AWS Lambda function would also require custom implementation and management of the authorization logic.\n\nOption D, using an Amazon Cognito user pool authorizer, provides a streamlined and managed solution for controlling access to the REST API with minimal operational overhead.","poster":"cookieMr","upvote_count":"3","timestamp":"1703603820.0","comment_id":"934497"},{"comment_id":"906686","content":"Selected Answer: D\nsolution will meet these requirements with the LEAST operational overhead is option D.","upvote_count":"2","timestamp":"1700925900.0","poster":"Bmarodi"},{"comment_id":"898353","poster":"studynoplay","timestamp":"1700063820.0","upvote_count":"2","content":"Selected Answer: D\nLEAST operational overhead = Serverless = Cognito user pool"},{"upvote_count":"2","timestamp":"1699327800.0","content":"Selected Answer: D\nD is correct.","comment_id":"891104","poster":"cheese929"},{"poster":"k33","content":"Selected Answer: D\nAnswer : D","comment_id":"850689","upvote_count":"2","timestamp":"1695696900.0"},{"comment_id":"847131","poster":"Hello4me","content":"D is correct","timestamp":"1695384180.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"770673","timestamp":"1688915220.0","content":"Selected Answer: A\nThere is a difference between \"Grant Access\" (Authentication done by Cognito user pool), and \"Control Access\" to APIs (Authorization using IAM policy, custom Authorizer, Federated Identity Pool). The question very specifically asks about *Control access to REST APIs* which is a clear case of Authorization and not Authentication. So custom Authorizer using Lambda in API Gateway is the solution. \n\nPls refer to this blog: https://aws.amazon.com/blogs/security/building-fine-grained-authorization-using-amazon-cognito-api-gateway-and-iam/","poster":"Mahadeva","comments":[{"content":"Control access to a REST API using Amazon Cognito user pools as authorizer\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html","comment_id":"832036","upvote_count":"4","poster":"TungPham","timestamp":"1694091600.0"},{"timestamp":"1688915580.0","upvote_count":"1","comment_id":"770678","poster":"Mahadeva","comments":[{"content":"Oh yes there is :)","upvote_count":"3","comment_id":"771126","timestamp":"1688964300.0","poster":"JayBee65"}],"content":"Option D: there is nothing called Cognito user pool authorizer. We only have custom Authorizer function through Lambda."}]},{"poster":"k1kavi1","comment_id":"755661","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html","timestamp":"1687691640.0","upvote_count":"5","comments":[{"content":"up this","comment_id":"961579","upvote_count":"1","timestamp":"1706105220.0","poster":"MutiverseAgent"}]},{"content":"Selected Answer: D\nOption D - As company already has all the users authentication information in Cognito","poster":"career360guru","comment_id":"747745","upvote_count":"2","timestamp":"1686966600.0"},{"upvote_count":"3","poster":"k1kavi1","timestamp":"1686833460.0","content":"Selected Answer: D\nD is correct","comment_id":"746213"},{"poster":"mj98","comment_id":"731990","upvote_count":"3","timestamp":"1685474400.0","content":"API + Cognito integration - Answer D"},{"poster":"Nigma","upvote_count":"2","comment_id":"729877","timestamp":"1685323020.0","content":"Selected Answer: D\nAnswer : D\nCheck Gabs90 link\n\nUse the Amazon Cognito console, CLI/SDK, or API to create a user pool—or use one that's owned by another AWS account"},{"upvote_count":"2","comment_id":"729694","poster":"TMM369","timestamp":"1685307900.0","content":"Selected Answer: D\nD - https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-cognito-user-pool-authorizer/"},{"content":"Selected Answer: D\nseems to be D to me: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html","timestamp":"1685285100.0","poster":"Gabs90","upvote_count":"5","comment_id":"729435"},{"upvote_count":"2","poster":"leonnnn","comment_id":"729355","content":"Selected Answer: D\nD is correct","timestamp":"1685282700.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/89142-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"D","question_id":134,"unix_timestamp":1669651500,"answer_description":"","answer_images":[],"answer_ET":"D","choices":{"A":"Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the request.","C":"Send the user’s email address in the header with every request. Invoke an AWS Lambda function to validate that the user with that email address has proper access.","B":"For each user, create and assign an API key that must be sent with each request. Validate the key by using an AWS Lambda function.","D":"Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request."},"isMC":true,"topic":"1","exam_id":31,"answers_community":["D (98%)","2%"]},{"id":"AT6nPHFWoYUT0804geK0","choices":{"A":"Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.","B":"Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.","C":"Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.","D":"Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving."},"timestamp":"2022-11-28 15:23:00","exam_id":31,"answer_images":[],"discussion":[{"comment_id":"1010256","poster":"TariqKipkemei","timestamp":"1710744480.0","content":"Selected Answer: B\nMarketing communications = Amazon Pinpoint","upvote_count":"16"},{"upvote_count":"10","timestamp":"1703604540.0","comment_id":"934505","content":"Selected Answer: B\nBy using Pinpoint, the company can effectively send SMS messages to its mobile app users. Additionally, Pinpoint allows the configuration of journeys, which enable the tracking and management of user interactions. The events generated during the journey, including user responses to SMS, can be captured and sent to an Kinesis data stream. This data stream can then be used for analysis and archiving purposes.\n\nA. Creating an Amazon Connect contact flow is primarily focused on customer support and engagement, and it lacks the capability to store and process SMS responses for analysis.\n\nC. Using SQS is a message queuing service and is not specifically designed for handling SMS responses or capturing them for analysis.\n\nD. Creating an SNS FIFO topic and subscribing a Kinesis data stream is not the most appropriate solution for capturing and storing SMS responses, as SNS is primarily used for message publishing and distribution.\n\nIn summary, option B is the best choice as it leverages Pinpoint to send SMS messages and captures user responses for analysis and archiving using an Kinesis data stream.","poster":"cookieMr"},{"poster":"scar0909","comment_id":"1170009","comments":[{"content":"Amazon Connect is primarily a contact center service, not designed for sending bulk SMS messages or handling two-way SMS communication for marketing purposes. It is overkill for this use case.","upvote_count":"1","timestamp":"1738391400.0","comment_id":"1349770","poster":"FlyingHawk"}],"timestamp":"1725928920.0","upvote_count":"1","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/connect/latest/adminguide/setup-sms-messaging.html"},{"upvote_count":"1","content":"Why not A. Amazon connect has this option.","poster":"[Removed]","comment_id":"1156759","timestamp":"1724355480.0"},{"poster":"awsgeek75","upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/pinpoint/latest/userguide/welcome.html\nAmazon Pinpoint is the easiest solution.\nAmazon Connect is Contact Centre as a Service so this option is not relevant to the requirement.\nSQS and SNS options are overengineered or under engineered for the requirements so natural choice is \"B\"","timestamp":"1719833220.0","comment_id":"1111192"},{"poster":"whoob","content":"base function of AWS Pinpoint","comment_id":"1020093","timestamp":"1711649820.0","upvote_count":"2"},{"content":"Selected Answer: B\nB. AWS Pinpoint is for Marketing communications.","comment_id":"1005986","upvote_count":"4","poster":"Guru4Cloud","timestamp":"1710275160.0"},{"content":"Selected Answer: B\nOption B is correct answer: link: https://aws.amazon.com/pinpoint/, and video under the link.","comment_id":"921239","timestamp":"1702375920.0","poster":"Bmarodi","upvote_count":"3"},{"timestamp":"1700066400.0","content":"Selected Answer: B\nTwo-Way Messaging\nReceive SMS messages from your customers and reply back to them in a chat-like interactive experience. With Amazon Pinpoint, you can create automatic responses when customers send you messages that contain certain keywords.","comment_id":"898403","poster":"studynoplay","upvote_count":"2"},{"upvote_count":"1","content":"Based on my research Kinesis stream is real time data ingestion, and also stores only event data and not the actual people responses, furthermore there is no requirement to have real time data streaming. That is probably why I am hesitating agree here with everyone on B and rather choose A.","timestamp":"1698857580.0","comments":[{"comment_id":"1106642","poster":"pentium75","timestamp":"1719466200.0","upvote_count":"2","content":"\"A Kinesis data stream stores records for 24 hours by default, up to 365 days (8,760 hours).\"\n\nhttps://aws.amazon.com/de/blogs/big-data/retaining-data-streams-up-to-one-year-with-amazon-kinesis-data-streams/#:~:text=A%20Kinesis%20data%20stream%20stores,parallel%20and%20at%20low%2Dlatency."}],"comment_id":"886406","poster":"CLOUDUMASTER"},{"poster":"jayce5","timestamp":"1698508440.0","upvote_count":"3","comment_id":"883697","content":"Selected Answer: B\nThe answer is B. AWS Pinpoint is for Marketing communications.\nAWS Connect is for Contact center."},{"comment_id":"882175","poster":"jaswantn","timestamp":"1698374520.0","comments":[{"upvote_count":"3","content":"no no, there is no SMS, note the question stated all activities through SMS, also Amazon connect flow most likely working on web application UI, but if you see question clearly, this is receiving and sending SMS not through application UI (Web/Mobile App). So for those reason we choose B","comment_id":"915932","timestamp":"1701848640.0","poster":"smartegnine"}],"upvote_count":"1","content":"Selected Answer: A\nAccording to the following link I would choose Option A. \nhttps://docs.aws.amazon.com/connect/latest/adminguide/web-and-mobile-chat.html"},{"content":"Selected Answer: B\nAmazon Pinpoint is a flexible, scalable and fully managed push notification and SMS service for mobile apps.","poster":"ProfXsamson","comment_id":"792557","timestamp":"1690698720.0","upvote_count":"4"},{"timestamp":"1689770160.0","comment_id":"781229","content":"It's B, see following link https://docs.aws.amazon.com/pinpoint/latest/developerguide/event-streams.html","poster":"Foucault","upvote_count":"3"},{"timestamp":"1689620100.0","upvote_count":"3","content":"Selected Answer: B\nhttps://aws.amazon.com/pinpoint/product-details/sms/\nTwo-Way Messaging:\nReceive SMS messages from your customers and reply back to them in a chat-like interactive experience. With Amazon Pinpoint, you can create automatic responses when customers send you messages that contain certain keywords. You can even use Amazon Lex to create conversational bots.\nA majority of mobile phone users read incoming SMS messages almost immediately after receiving them. If you need to be able to provide your customers with urgent or important information, SMS messaging may be the right solution for you.\n\nYou can use Amazon Pinpoint to create targeted groups of customers, and then send them campaign-based messages. You can also use Amazon Pinpoint to send direct messages, such as appointment confirmations, order updates, and one-time passwords.","comment_id":"779329","poster":"LuckyAro"},{"upvote_count":"3","poster":"DavidNamy","content":"Selected Answer: D\nD:\nAmazon Simple Notification Service (SNS) is a fully managed messaging service that enables you to send and receive SMS messages in a cost-effective and highly scalable way. By creating an SNS FIFO topic, you can ensure that the SMS messages are delivered to your users in the order they were sent and that the SMS responses are processed and stored in the same order. You can also configure your SNS FIFO topic to publish SMS responses to an Amazon Kinesis data stream, which will allow you to store and analyze the responses for a year.\n\nAmazon Pinpoint ?¿?¿? NO! \n\nis not correct solution because while Amazon Pinpoint allows you to send SMS and Email campaigns, as well as handle push notifications to a user base, it doesn't provide SMS sending feature by itself. Furthermore, it's a service mainly focused on sending and tracking marketing campaigns, not for managing two-way SMS communication and the reception of reply.","comments":[{"poster":"Omok","content":"What do think about https://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-two-way.html?","upvote_count":"2","comment_id":"797629","timestamp":"1691120820.0"}],"comment_id":"772280","timestamp":"1689060240.0"},{"upvote_count":"5","comment_id":"754484","timestamp":"1687542960.0","content":"Selected Answer: B\nTo send SMS messages and store the responses for a year for analysis, the company can use Amazon Pinpoint. Amazon Pinpoint is a fully-managed service that allows you to send targeted and personalized SMS messages to your users and track the results.\n\nTo meet the requirements of the company, a solutions architect can build an Amazon Pinpoint journey and configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving. The Kinesis data stream can be configured to store the data for a year, allowing the company to analyze the responses over time.\n\nSo, Option B is the correct answer.\n\nOption B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.","poster":"Buruguduystunstugudunstuy"},{"poster":"techhb","content":"Selected Answer: B\nWe need to analyze and archiving A doesnt help with it.","upvote_count":"2","timestamp":"1687384560.0","comment_id":"752881"},{"upvote_count":"2","poster":"BENICE","content":"B is correct answer","timestamp":"1687226220.0","comment_id":"750468"},{"poster":"career360guru","timestamp":"1686967200.0","upvote_count":"2","content":"Selected Answer: B\nAnswer B, This is Pinpoint usecase","comment_id":"747746"},{"upvote_count":"5","poster":"Marge_Simpson","content":"Selected Answer: B\nAnytime you see marketing or campaign, just pick AWS Pinpoint.","comment_id":"743699","timestamp":"1686633480.0"},{"poster":"Rameez1","timestamp":"1685900580.0","content":"Selected Answer: B\nAmazon Pinpoint is perfect choice for this scenario, as it provides 2 way communication and can stream events to kinesis Data stream for further analysis.","comment_id":"735400","upvote_count":"5"},{"upvote_count":"2","comment_id":"734170","timestamp":"1685752740.0","content":"Selected Answer: B\nThe diagram on the link shows \"Campaign and journeys\" with the arrow directing to Channels which includes SMS, emails etc. So the correct choice is B.\nhttps://aws.amazon.com/pinpoint/","poster":"icaniwill"},{"timestamp":"1685589900.0","upvote_count":"2","poster":"Wilson_S","content":"https://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-two-way.html","comment_id":"732279"},{"content":"Amazon Pinpoint +Kinesis can store for upto a year - answer B","timestamp":"1685474520.0","poster":"mj98","comment_id":"731993","upvote_count":"2"},{"upvote_count":"4","comment_id":"729697","timestamp":"1685308200.0","content":"Selected Answer: A\nA - https://aws.amazon.com/blogs/contact-center/building-personalized-customer-experiences-over-sms-through-amazon-connect/#:~:text=Get%20Amazon%20Connect%20instance%20details%201%20Navigate%20to,in%20and%20note%20down%20the%20Contact%20Flow%20ID","poster":"TMM369"},{"poster":"jambajuice","comment_id":"729609","timestamp":"1685299380.0","upvote_count":"2","content":"Selected Answer: B\nAmazon Pinpoint for two marketing"},{"content":"Selected Answer: B\nPinpoint is the correct one https://aws.amazon.com/it/pinpoint/","comment_id":"729437","timestamp":"1685285280.0","poster":"Gabs90","upvote_count":"2"},{"timestamp":"1685282940.0","content":"Selected Answer: B\nAmazon Connect is more like a custom service. However, amazon pinpoint can send sms to customers for confirmation.","poster":"leonnnn","comment_id":"729360","upvote_count":"2"},{"upvote_count":"3","poster":"Nigma","timestamp":"1685276580.0","comment_id":"729209","content":"Selected Answer: B\nAnswer : B\nhttps://aws.amazon.com/pinpoint/"}],"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/89080-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis.\n\nWhat should a solutions architect do to meet these requirements?","answer":"B","answer_ET":"B","question_images":[],"question_id":135,"answers_community":["B (90%)","7%"],"answer_description":"","unix_timestamp":1669645380}],"exam":{"id":31,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false,"numberOfQuestions":1019,"provider":"Amazon","lastUpdated":"11 Apr 2025","isMCOnly":true,"isImplemented":true},"currentPage":27},"__N_SSP":true}