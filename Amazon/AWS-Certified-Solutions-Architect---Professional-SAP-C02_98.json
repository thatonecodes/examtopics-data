{"pageProps":{"questions":[{"id":"BZWlbbzn27khKr5wxVob","unix_timestamp":1670692860,"choices":{"C":"In Account A, set the S3 bucket policy to the following:","A":"Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A.","D":"In Account B, set the permissions of User_DataProcessor to the following:","B":"In Account A, set the S3 bucket policy to the following:","E":"In Account B, set the permissions of User_DataProcessor to the following:"},"answer_description":"","question_id":486,"url":"https://www.examtopics.com/discussions/amazon/view/90940-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"comment_id":"741606","poster":"robertohyena","content":"Answer: C & D\n\nSource: \nhttps://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example4.html","upvote_count":"34","timestamp":"1727017560.0"},{"content":"C & D\n\nTo allow User_DataProcessor to access the S3 bucket from Account B, the following steps need to be taken:\n\n In Account A, set the S3 bucket policy to allow access to the bucket from the IAM user in Account B. This is done by adding a statement to the bucket policy that allows the IAM user in Account B to perform the necessary actions (GetObject and ListBucket) on the bucket and its contents.\n\n In Account B, create an IAM policy that allows the IAM user (User_DataProcessor) to perform the necessary actions (GetObject and ListBucket) on the S3 bucket and its contents. The policy should reference the ARN of the S3 bucket and the actions that the user is allowed to perform.\n\nNote: turning on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A is not necessary for this scenario as it is typically used for allowing web browsers to access resources from different domains.","poster":"higashikumi","comment_id":"827622","timestamp":"1727061420.0","upvote_count":"19"},{"upvote_count":"1","content":"Selected Answer: C\nThis policy will give access to User in B account for Bucket in A account.","comment_id":"1317010","timestamp":"1732447560.0","poster":"DhirajBansal"},{"poster":"Jorkaef","upvote_count":"2","timestamp":"1731639480.0","content":"The correct combination of steps for this scenario are:\n\nC. In Account A, set the S3 bucket policy to the following:\n\nE. In Account B, set the permissions of User_DataProcessor to the following:\n\nHere's why these are the correct steps:\n\nStep C: The bucket policy in Account A (the retail company) needs to explicitly allow access to the IAM user from Account B (the business partner). This policy grants the necessary permissions to User_DataProcessor from Account B to access the S3 bucket in Account A.\n\nStep E: In Account B (the business partner's account), the IAM user User_DataProcessor needs to be granted permissions to access S3 resources. This IAM policy allows the user to perform the necessary S3 actions.","comment_id":"1312387"},{"upvote_count":"1","content":"Selected Answer: C\nC & D.\nIn Account A, set the S3 bucket policy to allow only 'User_DataProcessor' from Account B access. \nIn Account B, set the permissions of User_DataProcessor to allow access to S3 bucket in Account A.","timestamp":"1730265120.0","poster":"TariqKipkemei","comment_id":"1304862"},{"timestamp":"1729877040.0","comment_id":"1302952","poster":"85b5b55","content":"Answer: C & D","upvote_count":"1"},{"comment_id":"1100124","timestamp":"1727061420.0","poster":"atirado","upvote_count":"1","content":"Selected Answer: C\nOption A - CORS does not address cross-account access to S3 buckets\n\nOption B - This option would not work because the bucket policy is missing the Principal\n\nOption C - This option provides a valid S3 bucket policy that grants access to User_DataProcessor\n\nOption D - These permissions allow User_DataProcessor to get objects out of the bucket\n\nOption E - This option would not work because it is not a valid IAM policy"},{"timestamp":"1725090240.0","content":"C. In Account A, set the S3 bucket policy to the following:\nD. In Account B, set the permissions of User_DataProcessor to the following:","poster":"amministrazione","comment_id":"1275433","upvote_count":"1"},{"upvote_count":"1","comment_id":"1251484","poster":"dEgYnIDA","content":"Selected Answer: D\nThe question says Choose two. The answer is C & D.","timestamp":"1721438760.0"},{"timestamp":"1718249760.0","upvote_count":"1","poster":"kpcert","content":"Selected Answer: C\nAns C and D\n2 Options have to be selected","comment_id":"1229603"},{"upvote_count":"1","comment_id":"1229601","poster":"kpcert","timestamp":"1718249700.0","content":"Ans - C and D\n2 Options have to be selected"},{"poster":"MoT0ne","upvote_count":"1","content":"Selected Answer: C\nCross-Origin Resource Sharing (CORS) is a security feature in Amazon S3 that allows you to control access to your S3 resources from a different domain (origin) than the one serving the resources. CORS defines a way for client web applications running in one origin to interact with resources in a different origin, which is otherwise restricted by the same-origin policy enforced by web browsers.","timestamp":"1710248520.0","comment_id":"1171702"},{"comment_id":"1163474","poster":"Dgix","upvote_count":"1","content":"C and D.","timestamp":"1709295180.0"},{"comment_id":"1159017","timestamp":"1708885740.0","poster":"awsylum","upvote_count":"2","content":"The answer is C and D. You need to give the IAM User in Account B an IAM Policy and you need to give a Bucket Policy in Account A.\n\nWho is maintaining this database of questions? Someone needs to seriously set the correct answers before making a lot of people confused and potentially screw up their exam."},{"comment_id":"1140846","poster":"chelbsik","upvote_count":"2","content":"Selected Answer: D\nCorrect answer: C and D\nAdding my vote for D to balance the result\n\nModerator, please fix the vote in this ticket.","timestamp":"1707124320.0"},{"content":"why we need two steps? I think that we get only one from resource-based policy or identity-based policy.","upvote_count":"1","timestamp":"1706792040.0","poster":"ftaws","comment_id":"1137626"},{"upvote_count":"1","comment_id":"1131135","poster":"Vaibs099","content":"Answer C & D","timestamp":"1706130240.0"},{"timestamp":"1701652920.0","content":"Selected Answer: C\nAnswer - C & D","comment_id":"1087260","poster":"shaaam80","upvote_count":"2"},{"timestamp":"1699698360.0","poster":"severlight","comment_id":"1067761","upvote_count":"4","content":"Selected Answer: D\nC, D. D and not E, because it is an identity-based inline policy already attached to the specific principal."},{"content":"A,C\nAccess setting need to be done only on Account A as it's an owner. So Enabling Cross origin access and access to the bucket for account B IAM user.","poster":"alonis2201","timestamp":"1699287900.0","upvote_count":"2","comment_id":"1064047"},{"poster":"rlf","comment_id":"1047513","upvote_count":"2","timestamp":"1697690220.0","content":"Answer : C&D."},{"poster":"puffetor","comment_id":"1021345","upvote_count":"4","timestamp":"1696062120.0","content":"Hello I've just tested it on my AWS account to be 100% sure.\nCorrect answer in C & D. Only C is enough only for same account access, but for cross-account like in this case D is needed too, otherwise it does not work."},{"content":"Selected Answer: C\nAnswer: C","poster":"ansgohar","comment_id":"1019658","timestamp":"1695893040.0","upvote_count":"2"},{"upvote_count":"3","timestamp":"1693758480.0","poster":"career360guru","content":"A & C are the right answer","comment_id":"997796"},{"comment_id":"934815","upvote_count":"2","poster":"NikkyDicky","content":"C&D. can only vote for one? lol","timestamp":"1687816800.0"},{"upvote_count":"2","poster":"BasselBuzz","comment_id":"927192","content":"Selected Answer: D\nC and D for sure","timestamp":"1687153800.0"},{"poster":"SkyZeroZx","content":"Selected Answer: D\nAnswer: C & D\n\nSource:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example4.html","upvote_count":"2","comment_id":"920219","timestamp":"1686418740.0"},{"timestamp":"1684869840.0","content":"Selected Answer: C\nC AND D\nC. In Account A, set the S3 bucket policy to the following: \n\"Effect\": \"Allow\", \"Principal\" : { \"AWS\" : \"arn:aws:iam::AccountB:user/User_DataProcessor\" ) , \"Action\": [ \"s3 : GetObject\", \"s3 : ListBucket\" \n], \"Resource\": ( \"arn:aws:s3:::AccountABucketName/*\" \nD. In Account B, set the permissions of User_DataProcessor to the following: \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": \"arn:aws:s3:::AccountABucketName/*\" \nThese steps allow the IAM user User_DataProcessor from Account B to access the S3 bucket in Account A by granting the appropriate permissions.","comment_id":"905177","poster":"rbm2023","upvote_count":"2"},{"upvote_count":"2","timestamp":"1684843680.0","poster":"rtguru","content":"C&D is the correct answer","comment_id":"904869"},{"content":"Answer is C& D\n\nRef \n\nhttps://repost.aws/knowledge-center/cross-account-access-s3","timestamp":"1683831420.0","comment_id":"895328","poster":"AmitB","upvote_count":"2"},{"upvote_count":"2","content":"Answer: C & D are correct","timestamp":"1682693280.0","poster":"iamunstopable","comment_id":"883557"},{"comment_id":"877642","timestamp":"1682196720.0","comments":[{"timestamp":"1682717400.0","comment_id":"883915","upvote_count":"1","poster":"momo3321","content":"Nope, this is the multiple answers question and in this case, it's required the performing from both way (Account A & Account B), doesn't work if only Account B open the policy to the bucket which belong to Account A"}],"poster":"EthicalBond","content":"Selected Answer: C\nDoesn't make sense for account B to control access to resources in account A. So D is NOT the answer.\nAccount A owns the bucket and sets the bucket policy to allow access to a principal/user in Account B","upvote_count":"2"},{"timestamp":"1681831980.0","upvote_count":"2","content":"C & D - 100%","poster":"Don2021","comment_id":"873799"},{"comment_id":"862747","timestamp":"1680766440.0","poster":"elad18","upvote_count":"2","content":"Selected Answer: C\nC & D.\nBut the ListBucket action won't work as you need to mention the arn of the bucket itself as well (without the /*)"},{"comment_id":"861751","content":"Selected Answer: C\nC & D.","poster":"OCHT","upvote_count":"1","timestamp":"1680668640.0"},{"content":"Selected Answer: C\nC and D, 100%","timestamp":"1680164340.0","comment_id":"855471","upvote_count":"1","poster":"hpipit"},{"poster":"dev112233xx","content":"Selected Answer: C\nC+D no doubts","timestamp":"1680033360.0","comment_id":"853650","upvote_count":"1"},{"poster":"mfsec","timestamp":"1679977260.0","content":"Selected Answer: C\nC + D are right","upvote_count":"1","comment_id":"852760"},{"content":"Selected Answer: C\nI would select C as Account A need to grant access","timestamp":"1678462320.0","upvote_count":"1","comment_id":"835163","poster":"gameoflove"},{"timestamp":"1678182240.0","content":"Selected Answer: C\ngoing with C and D","comment_id":"831715","poster":"kiran15789","upvote_count":"1"},{"comment_id":"829430","content":"Selected Answer: D\nC & D are the correct answers ✅","poster":"dev112233xx","upvote_count":"2","timestamp":"1677965880.0"},{"content":"Two ways for Cross account permissions is either through bucket policies or using IAM role.\nWith Bucket Policy you need; and for this question , a user policy is required to delegate access to the user in the partner account. A bucket policy and a userpolicy. and bucket policy will include an arn\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example4.html#access-policies-walkthrough-example4-overview\n\nC: Bucket Policy in account A\nD: User Policy in Account B","poster":"Ajani","comment_id":"827180","upvote_count":"2","timestamp":"1677779460.0"},{"upvote_count":"1","timestamp":"1677303420.0","comment_id":"821217","poster":"vandergun","content":"Selected Answer: C\nc&D for sure"},{"timestamp":"1675275660.0","upvote_count":"2","poster":"DWsk","content":"Selected Answer: D\nI think the answer is C & D.\nBut what's with E? You don't need the principal, but it would still work, right?","comment_id":"795394"},{"timestamp":"1674076680.0","poster":"skashanali","content":"Selected Answer: C\nAllow specific user and specific actions on the mentioned S3 bucket is the right way. We always think of fine grain access.","upvote_count":"1","comment_id":"780438"},{"comment_id":"770967","poster":"Teknoklutz","upvote_count":"1","content":"Selected Answer: C\nC and E","timestamp":"1673312400.0"},{"content":"Selected Answer: C\nPermissions is required to provide on the source component, at least.","timestamp":"1672317120.0","comment_id":"760996","upvote_count":"1","poster":"mmendozaf"},{"upvote_count":"1","timestamp":"1672304820.0","poster":"hobokabobo","comment_id":"760806","content":"Selected Answer: A\nIt says choose two. \nC&A \nC grants access and A whitelists the different domain.","comments":[{"upvote_count":"1","timestamp":"1680418200.0","content":"Stupid me, if only I could read: C and D are the necessary policies.","comment_id":"858584","poster":"hobokabobo"}]},{"poster":"skashanali","content":"Selected Answer: C\nAns C, is for the S3 CORS bucket policy and\nAns D, for the User permission set to allow S3 bucket","timestamp":"1672253220.0","upvote_count":"1","comment_id":"760211"},{"upvote_count":"2","poster":"Arun_Bala","content":"Selected Answer: C\nAns C & D","timestamp":"1671281220.0","comment_id":"748053"}],"answer":"C","question_text":"A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3 bucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to access the files from its own AWS account (Account B).\nWhich combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)","timestamp":"2022-12-10 18:21:00","exam_id":33,"answer_images":[],"answer_ET":"C","answers_community":["C (64%)","D (34%)","2%"],"isMC":true,"question_images":[],"topic":"1"},{"id":"8SSK4wrKNZ9IX1Q6006N","url":"https://www.examtopics.com/discussions/amazon/view/95268-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"B":"Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load.","C":"Create a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config parameter to distribute the load.","A":"Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.","D":"Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load."},"answers_community":["A (97%)","3%"],"answer_images":[],"question_images":[],"discussion":[{"comment_id":"780169","content":"Selected Answer: A\nA. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load is the correct answer as it meets the requirement of supporting a canary release.\n\nOption B is not correct because while it would allow for a canary release, it would involve deploying the new version of the application into a separate CloudFormation stack, which would be a more complex and time-consuming process compared to creating an alias for a new version of the Lambda function.\n\nOption C is not correct because while it would allow for a canary release, it would involve creating a version for every new deployed Lambda function, which would be more complex and time-consuming process compared to creating an alias for a new version of the Lambda function.","timestamp":"1674057120.0","upvote_count":"21","comments":[{"timestamp":"1674057180.0","poster":"masetromain","content":"Option D is not correct because AWS CodeDeploy is a deployment service that allows you to automate code deployments to a variety of compute services like EC2 and on-premises servers, but it does not support routing configuration for a canary release on AWS Lambda.","comments":[{"comment_id":"896727","comments":[{"upvote_count":"9","timestamp":"1684184940.0","comment_id":"898724","poster":"Jesuisleon","content":"He copied from chatgpt, you didn't find it ?"}],"content":"Thank you masetromain, you have been really helpful for taking the time and providing explanation.","timestamp":"1683986940.0","upvote_count":"1","poster":"karma4moksha"},{"upvote_count":"3","comment_id":"1114591","poster":"ninomfr64","content":"This is not 100% correct. Actually CodeDeploy support deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application. You can shift traffic using a canary, linear, or all-at-once deployment configuration. The following lists the predefined configurations available for AWS Lambda canary deployments:\n- CodeDeployDefault.LambdaCanary10Percent5Minutes \n- CodeDeployDefault.LambdaCanary10Percent10Minutes \n- CodeDeployDefault.LambdaCanary10Percent15Minutes\n- CodeDeployDefault.LambdaCanary10Percent30Minutes","timestamp":"1704467760.0","comments":[{"poster":"Jason666888","comment_id":"1260551","content":"Yeah the reason D is wrong is not because CodeDeploy doesn't support lambda canary deployment, it's because `OneAtATime` deployment strategy is only for EC2 instances but not for lambdas","upvote_count":"2","timestamp":"1722756300.0"}]}],"upvote_count":"6","comment_id":"780171"}],"poster":"masetromain"},{"poster":"Atila50","comment_id":"777275","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/28312-exam-aws-certified-solutions-architect-professional-topic-1/","upvote_count":"10","timestamp":"1673838120.0"},{"timestamp":"1741352940.0","poster":"29fb203","upvote_count":"1","content":"Selected Answer: D\nAWS CodeDeploy supports canary releases for Lambda functions, which is what the requirement is aiming for.","comment_id":"1366267"},{"timestamp":"1737777720.0","comment_id":"1346321","upvote_count":"1","poster":"d401c0d","content":"Selected Answer: A\nA. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load"},{"timestamp":"1733817960.0","comment_id":"1324433","upvote_count":"1","poster":"Heman31in","content":"Selected Answer: A\nNot D because, the CodeDeployDefault.OneAtATime deployment configuration is primarily designed for EC2 and on-premises instances. For AWS Lambda functions, AWS CodeDeploy provides deployment strategies specific to Lambda, such as Canary, Linear, and All-at-Once."},{"timestamp":"1733515320.0","poster":"Aritra88","upvote_count":"2","content":"Selected Answer: A\nUsing Lambda Aliases for Canary Releases:\n* Lambda aliases are pointers to specific versions of a Lambda function.\n* You can use the update-alias command with the routing-config parameter to configure traffic shifting between the current version and the newly deployed version.\n* This allows a gradual shift of traffic to the new version while maintaining traffic to the current version.\nAWS CLI Example:\n* Create a new alias or update an existing alias to shift a portion of the traffic\n\n\nTesting and Monitoring:\n\nGradually increase the percentage of traffic to the new version.\nUse Amazon CloudWatch metrics to monitor errors, latency, or other performance issues.\nRoll back traffic to the previous version if any issues are detected.\n\naws lambda update-alias \\\n --function-name MyFunction \\\n --name MyAlias \\\n --routing-config '{\"AdditionalVersionWeights\": {\"2\": 0.10}}'","comment_id":"1322901"},{"poster":"amministrazione","content":"A. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.","comment_id":"1275514","upvote_count":"1","timestamp":"1725095220.0"},{"upvote_count":"2","content":"Selected Answer: A\nA. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.   \n\nExplanation:\nThis option provides a granular level of control for canary deployments:\n\nVersioning: Creating a new version for each deployment ensures that you have a clear record of changes.\nAlias: An alias acts as a stable endpoint, allowing you to gradually shift traffic to the new version.\nRouting configuration: The routing-config parameter provides fine-grained control over traffic distribution between versions.\nBy using this approach, you can gradually increase the percentage of traffic to the new version, monitor its performance, and roll back if necessary, minimizing the impact of potential issues.","poster":"Chakanetsa","comment_id":"1256232","comments":[{"comment_id":"1256233","content":"Breakdown of other options:\nB: While Route 53 weighted routing can distribute traffic, it's less granular than using Lambda aliases and doesn't provide the same level of control.\nC: Using the update-function-configuration command doesn't provide the flexibility to gradually shift traffic.\nD: CodeDeploy is primarily for deploying code to EC2 instances, not for managing Lambda function traffic.\nBy using Lambda aliases and the routing-config parameter, you can effectively implement a canary release strategy for your Lambda functions.","timestamp":"1722072420.0","poster":"Chakanetsa","upvote_count":"1"}],"timestamp":"1722072420.0"},{"content":"Selected Answer: A\nNot B. This introduces R53 in the scenario, but we are not sure if R53 fits in the scenario. To combine R53 and Lambda we should use function URL that is not mentioned and we don't know if the app is public. A lot of uncertainty here\nNot C. routing-config is an Alias specific configuration aka Weighted Alias and it is not available for the update-function-configuration command https://docs.aws.amazon.com/cli/latest/reference/lambda/update-function-configuration.html\nNot D. CodeDeployDefault.OneAtATime is a CodeDeploy option for EC2/on-premise, while in this scenario we need a canary option for Lambda such as CodeDeployDefault.LambdaCanary10Percent5Minutes\n\nA does the job https://docs.aws.amazon.com/cli/latest/reference/lambda/update-alias.html and https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html#configuring-alias-routing","poster":"ninomfr64","comment_id":"1114589","timestamp":"1704467520.0","upvote_count":"1"},{"poster":"JMAN1","content":"100% A is correct. :) I was confused between D and A. But, this url says Codedeployee.AllatOnce deploy option is not for 'canary release'.\nhttps://docs.aws.amazon.com/ko_kr/codedeploy/latest/userguide/deployment-configurations.html","timestamp":"1703736060.0","comment_id":"1107379","upvote_count":"2"},{"poster":"totten","comment_id":"1022610","timestamp":"1696185120.0","upvote_count":"3","content":"Selected Answer: A\nHere's why Option A is suitable:\n\nCreate an alias: For every new version of your Lambda function, create an alias. Aliases allow you to associate a user-friendly name with a specific version of the function.\n\nRouting configuration: AWS Lambda supports routing configurations that allow you to gradually shift traffic from one alias to another. Using the \"routing-config\" parameter with the AWS CLI \"update-alias\" command, you can specify how much traffic each alias should receive.\n\nGradual release: By configuring the routing, you can control the percentage of traffic directed to the new version (canary). You can gradually increase the traffic percentage as you gain confidence in the new release. If issues arise, you can quickly roll back by adjusting the routing configuration."},{"content":"Selected Answer: A\nnew release-> lambda alias-> update-alias: aws lambda update-alias --function-name my-function --name alias-name --function-version version-number","timestamp":"1688680560.0","poster":"Christina666","upvote_count":"2","comment_id":"945032"},{"timestamp":"1688313600.0","comment_id":"941037","upvote_count":"2","poster":"NikkyDicky","content":"Selected Answer: A\nD would be an optionn if used Lambda-specific config"},{"content":"Selected Answer: A\nkeyword = alias for every new deployed version \nis a classic usage for deployment canary for lambdas other option usually is codeDeploy but in this options AllAtOnce \nthen A","poster":"SkyZeroZx","upvote_count":"3","comment_id":"926748","timestamp":"1687099920.0"},{"comment_id":"889866","upvote_count":"1","poster":"AMEJack","content":"Sorry OneAtTime","timestamp":"1683272040.0"},{"timestamp":"1677609900.0","upvote_count":"5","content":"Selected Answer: A\naws update-alias command has routing-config option to route the weighted % traffic\nAs is correct\nhttps://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/\n# Point alias to new version, weighted at 5% (original version at 95% of traffic)\naws lambda update-alias --function-name myfunction --name myalias --routing-config '{\"AdditionalVersionWeights\" : {\"2\" : 0.05} }'","poster":"God_Is_Love","comment_id":"825167"},{"timestamp":"1676081760.0","upvote_count":"4","poster":"moota","comments":[{"comment_id":"923983","content":"or you know, you could start thinking yourself rather than use glorified rubbish google","comments":[{"content":"Dont get mad, get Glad.","timestamp":"1687219860.0","poster":"aliasdoe110","upvote_count":"1","comment_id":"928047"}],"timestamp":"1686821820.0","upvote_count":"1","poster":"Perkuns"}],"content":"Selected Answer: A\nAccording to ChatGPT, The \"update-alias\" command is a feature of AWS Lambda service. It is used to update the configuration of a Lambda alias, including the routing configuration which can be used for canary releases, blue/green deployments, and other deployment strategies.","comment_id":"804922"},{"upvote_count":"1","timestamp":"1673801040.0","comment_id":"776824","poster":"zhangyu20000","content":"A is correct.\nD does not have routing to distribute load"},{"content":"Selected Answer: D\nAWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and on-premises instances. CodeDeploy allows to perform a canary release, which is a technique that releases new versions of software to a small subset of users or systems before releasing it to the entire infrastructure. This makes it possible to test the new version of the software before releasing it to the entire population.\n\nOption A creates an alias for every new deployed version of the Lambda function, but it doesn't include the ability to perform a canary release.\nOption B Deploy the application into a new CloudFormation stack, and use an Amazon Route 53 weighted routing policy to distribute the load, this option can be used for canary release, but it is not the best solution for it.\nOption C creates a version for every new deployed Lambda function, but it does not include the ability to perform a canary release.","comment_id":"775459","poster":"masetromain","upvote_count":"1","comments":[{"upvote_count":"5","poster":"jaysparky","content":"You have 2 different answers.....I think it is better you delete this.","timestamp":"1676913480.0","comment_id":"815588","comments":[{"poster":"chikorita","upvote_count":"2","comment_id":"911801","content":"he can't.....nobody can delete once posted","timestamp":"1685601060.0"}]}],"timestamp":"1673704440.0"}],"question_text":"A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.\n\nWhich solution will meet these requirements?","exam_id":33,"unix_timestamp":1673704440,"answer":"A","answer_description":"","question_id":487,"timestamp":"2023-01-14 14:54:00","isMC":true,"answer_ET":"A","topic":"1"},{"id":"10mtV6FiE4ui0ocBDPfI","question_images":[],"isMC":true,"answer_ET":"B","choices":{"B":"Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.","C":"Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file gateway endpoint.","D":"Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB.","A":"Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB."},"exam_id":33,"question_text":"A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties. The company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files are uploaded, they are moved to the data lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route 53.\n\nWhat should a solutions architect do to improve the reliability and scalability of the SFTP solution?","url":"https://www.examtopics.com/discussions/amazon/view/95272-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-01-14 14:59:00","unix_timestamp":1673704740,"discussion":[{"poster":"tinyflame","upvote_count":"31","comment_id":"802654","timestamp":"1675899120.0","content":"Selected Answer: B\nA=ALB cannot be used with SFTP\nB = Correct\nC=Storage Gateway is not an SFTP Server\nD=NLB can be used with SFTP, but EC2 is single"},{"content":"Selected Answer: B\nOption B is the correct answer. Migrating the SFTP server to AWS Transfer for SFTP will improve the reliability and scalability of the SFTP solution. AWS Transfer for SFTP is a fully managed SFTP service that enables the company to transfer files directly into and out of Amazon S3 using the SFTP protocol. By using this service, the company can offload the management of the SFTP server to AWS, which will provide high availability, scalability, and security. The company can then update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname, which will ensure that the SFTP server is reachable on the DNS.","timestamp":"1673704740.0","upvote_count":"14","comments":[{"comment_id":"775469","poster":"masetromain","timestamp":"1673704740.0","content":"Option A, C and D do not provide the same level of scalability and reliability as AWS Transfer for SFTP. While placing the EC2 instance behind a load balancer can help improve availability, it will not necessarily improve scalability, and it would still require the company to manage the SFTP server. Option C , migrating the SFTP server to a file gateway in AWS Storage Gateway, would not necessarily improve the scalability and reliability of the SFTP solution, as it would still require the company to manage the SFTP server.","upvote_count":"4"},{"comment_id":"1104430","content":"How about the cron job?","timestamp":"1703387100.0","poster":"rioisverycute","upvote_count":"1"}],"comment_id":"775468","poster":"masetromain"},{"poster":"amministrazione","upvote_count":"1","comment_id":"1275515","timestamp":"1725095220.0","content":"B. Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname."},{"poster":"NikkyDicky","comment_id":"941046","content":"Selected Answer: B\nB of course","timestamp":"1688314200.0","upvote_count":"1"},{"poster":"SkyZeroZx","timestamp":"1687101480.0","upvote_count":"2","content":"Selected Answer: B\nkeyword = AWS Transfer for SFTP\nthen B","comment_id":"926762"},{"content":"Selected Answer: B\nB is the way to go..","poster":"mfsec","upvote_count":"3","timestamp":"1679830740.0","comment_id":"850952"}],"question_id":488,"answers_community":["B (100%)"],"answer":"B","answer_images":[],"topic":"1","answer_description":""},{"id":"LbIbU3vwEY0hmLNwoVd1","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/95273-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"unix_timestamp":1673704920,"choices":{"D":"Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on the on-premises VM. Register the VM with Systems Manager to be a managed instance. Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2 instance that is based on the AMI.","B":"Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.","C":"Configure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create an AMI from the backup copy. Launch an EC2 instance that is based on the AMI.","A":"Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs to Amazon EC2."},"discussion":[{"upvote_count":"15","content":"Selected Answer: B\nThe correct answer is B. Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command. This approach allows the solutions architect to export the application as an image in OVF format, which preserves the software and configuration settings, and then import it into Amazon EC2 using the EC2 import command.","comments":[{"content":"https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html","poster":"sammyhaj","timestamp":"1704577080.0","comment_id":"1115452","upvote_count":"3"},{"poster":"masetromain","content":"Option A is incorrect because it uses AWS DataSync and FSx for Windows File Server to replicate the data store, but it doesn't preserve the software and configuration settings of the application.\n\nOption C is incorrect because it uses AWS Storage Gateway to export a CIFS share, but it doesn't preserve the software and configuration settings of the application.\n\nOption D is incorrect because it uses AWS Systems Manager and AWS Backup to create a snapshot of the VM, but it doesn't preserve the software and configuration settings of the application.","upvote_count":"9","timestamp":"1673704920.0","comment_id":"775473"}],"poster":"masetromain","comment_id":"775472","timestamp":"1673704920.0"},{"poster":"amministrazione","timestamp":"1725095220.0","content":"B. Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.","upvote_count":"1","comment_id":"1275516"},{"content":"Selected Answer: B\nA = SMB share cannot host VMware datastore. Also, installing agent modify configuration settings\nB = correct\nC = not clear how the backup copy is created and what format is used to allow then creating an AMI from it\nD = hybrid activation allows SSM to manage on-premise / other cloud VM but doesn't enable AWS Backup. This instead requires a backup gateway to backup VMware environment https://aws.amazon.com/blogs/storage/backup-and-restore-on-premises-vmware-virtual-machines-using-aws-backup/","timestamp":"1704468960.0","poster":"ninomfr64","upvote_count":"2","comment_id":"1114605"},{"comment_id":"982259","timestamp":"1692167820.0","content":"Selected Answer: B\nThe only thing that is missing from the B answer is that the OVF file has to be transformed to a OVA file : https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html.","upvote_count":"3","poster":"SorenBendixen"},{"comment_id":"976421","poster":"Brightalw","upvote_count":"2","timestamp":"1691573040.0","content":"what the B is wrong is that the VM format, should be OVA or VMDK or VHD, not OVF"},{"poster":"CuteRunRun","timestamp":"1691482740.0","content":"Selected Answer: B\nI prefer B I do not know why the correct is D.","comment_id":"975331","upvote_count":"1"},{"timestamp":"1688319360.0","upvote_count":"1","comment_id":"941124","poster":"NikkyDicky","content":"Selected Answer: B\nit's a B"},{"poster":"rbm2023","timestamp":"1683345000.0","content":"Selected Answer: B\nhttps://www.learnitguide.net/2023/01/how-to-migrate-vmware-vm-to-aws-ec2.html","comments":[{"upvote_count":"1","comment_id":"976419","poster":"Brightalw","timestamp":"1691572980.0","content":"It said the VM fomat is OVA or VMDK, not OVF"}],"upvote_count":"3","comment_id":"890452"},{"upvote_count":"1","content":"I vote to B. Why the admin has selected D as Answer.","poster":"asifjanjua88","timestamp":"1681169760.0","comment_id":"866692"},{"poster":"mfsec","upvote_count":"2","comment_id":"850954","timestamp":"1679830860.0","content":"Selected Answer: B\nB is the answer - OVF."},{"timestamp":"1677624960.0","poster":"God_Is_Love","comments":[{"timestamp":"1677625080.0","upvote_count":"2","content":"https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html\nPrerequisites\n Create an Amazon S3 bucket for storing the exported images or choose an existing bucket. The bucket must be in the Region where you want to import your VMs. For more information about S3 buckets, see the Amazon Simple Storage Service User Guide.\n\n Create an IAM role named vmimport. For more information, see Required service role.\n\nIf you have not already installed the AWS CLI on the computer you'll use to run the import commands, see the AWS Command Line Interface User Guide.","comment_id":"825367","poster":"God_Is_Love"}],"comment_id":"825363","content":"Selected Answer: B\nUse VM Import/Export. B is correct . https://aws.amazon.com/ec2/vm-import/","upvote_count":"4"},{"timestamp":"1675159860.0","poster":"Signup_Nickname","upvote_count":"1","comment_id":"793948","content":"Selected Answer: B\nI vote B\nhttps://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html"}],"answer_ET":"B","topic":"1","question_id":489,"question_text":"A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions architect must preserve the software and configuration settings during the migration.\n\nWhat should the solutions architect do to meet these requirements?","answer":"B","question_images":[],"timestamp":"2023-01-14 15:02:00","isMC":true,"answer_description":"","answer_images":[]},{"id":"mEHXyZVNTKGyg1JCkRf0","question_images":[],"answers_community":["AB (87%)","8%"],"discussion":[{"poster":"zhangyu20000","comments":[{"upvote_count":"1","content":"While voting A and B, B doesn't really work because Lambda has reached the 15-minute timeout limitation.","poster":"GabrielShiao","comment_id":"1342881","timestamp":"1737266760.0"},{"timestamp":"1673901000.0","poster":"masetromain","content":"You are correct, both options A and B involve creating a Docker image of the application code and running it on Amazon Elastic Container Service (ECS) using either Fargate or EC2 as the launch type. These options would allow for more control over the resources allocated to the application and potentially prevent timeout errors. Option A is necessary to create the image and store it in a registry, and option B is necessary to run the image on Fargate which is a managed container orchestration service that eliminates the need for provisioning and scaling of the underlying infrastructure.","upvote_count":"9","comment_id":"778216"}],"comment_id":"776832","upvote_count":"27","timestamp":"1673801520.0","content":"A: create Docker image and save it to ECR\nB: run this image on Fargate\n\nNo answer should have Lambda the will be time out"},{"timestamp":"1725095280.0","upvote_count":"1","content":"A. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).\nB. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.","poster":"amministrazione","comment_id":"1275517"},{"timestamp":"1723451280.0","content":"Selected Answer: AB\nTo be honest, I don't trust the examtopic answers anymore, we should only rely on most voted ones","comment_id":"1264552","poster":"MAZIADI","upvote_count":"1"},{"comment_id":"1175662","upvote_count":"1","poster":"gofavad926","content":"Selected Answer: AB\nAB, ECR + ECS Margate","timestamp":"1710662460.0"},{"comment_id":"1108221","content":"Selected Answer: AB\nA: create Docker image and save it to ECR\nB: run this image on Fargate","upvote_count":"1","timestamp":"1703811180.0","poster":"Ak47g"},{"comment_id":"1097522","content":"Selected Answer: AB\nA: create docker image and store in on ECR\nB: run it on a AWS-managed infrastructure (as required)","timestamp":"1702662600.0","poster":"Nicoben","upvote_count":"1"},{"content":"The correct answer is A and B. But Lambda function should be replaced with EventBridge.","timestamp":"1698454560.0","upvote_count":"1","poster":"blackgamer","comment_id":"1055929"},{"content":"Selected Answer: BC\nB - 100%\nC OR E ??","poster":"ggrodskiy","upvote_count":"1","comment_id":"1045060","timestamp":"1697466600.0"},{"comment_id":"975339","poster":"CuteRunRun","upvote_count":"1","timestamp":"1691483220.0","content":"Selected Answer: AB\nI think is AB"},{"timestamp":"1688319540.0","upvote_count":"1","comment_id":"941127","poster":"NikkyDicky","content":"Selected Answer: AB\nit's AB"},{"comment_id":"935629","timestamp":"1687885800.0","poster":"Jonalb","content":"Selected Answer: AB\nAB\nits correct!","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: AB\nA + B \nA , basic dockerized the aplication and use Elastic Container Register \nB , deploy how serverless with fargate without overhead managament infrastructure","comment_id":"926766","poster":"SkyZeroZx","timestamp":"1687101840.0"},{"content":"Selected Answer: B\nA + B.","timestamp":"1679830980.0","poster":"mfsec","upvote_count":"2","comment_id":"850956"},{"timestamp":"1679095860.0","upvote_count":"2","content":"Selected Answer: AB\nA+B makes sense to me","comment_id":"842363","poster":"dev112233xx"},{"content":"Selected Answer: AB\nBased on Serverless solutions used, need to go with Fargate in combination with either ECS/EC2.As company does not want to manage infra, we go for because Fargate-ECS combo as Fargate-EC2 needs more maintenance .That means D is out. E is obviously out EFS does not contribute to lambda invocation timeouts.\nC is wrong because, increased concurrency (more lambda versions) won't solve timeouts.\nThat leaves A and B as right answers.","timestamp":"1677637800.0","poster":"God_Is_Love","upvote_count":"4","comment_id":"825481"},{"content":"Selected Answer: AB\nC is not right, question clearly said no involve infrastructure, EC2 is a infrastructure, Lamda time out 15 mins.","poster":"klog","timestamp":"1676471160.0","comment_id":"809640","upvote_count":"2"},{"content":"Selected Answer: AB\nlamda will time out\nA: create Docker image and save it to ECR\nB: run this image on Fargate","comment_id":"793269","timestamp":"1675108740.0","upvote_count":"2","poster":"zozza2023"},{"poster":"Musk","upvote_count":"2","comment_id":"792049","timestamp":"1675024920.0","content":"Selected Answer: AB\nAB makes most sense"},{"timestamp":"1673705460.0","upvote_count":"2","content":"Selected Answer: BC\nB and C are correct choices for this question.\n\nB: Creating a new Amazon Elastic Container Service (ECS) task definition with a compatibility type of AWS Fargate and adjusting the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3 can help to prevent invocation failures by breaking up the image processing work into smaller tasks that can be processed concurrently.\n\nC: Creating an AWS Step Functions state machine with a Parallel state to invoke the Lambda function and increasing the provisioned concurrency of the Lambda function can also help to prevent invocation failures by allowing the Lambda function to handle more requests in parallel.","comments":[{"content":"Option A is not a correct answer because it does not address the issue of the Lambda function timing out.\n\nOption D is not a correct answer because it is similar to option B, but it uses Amazon EC2 instead of AWS Fargate which is a more modern and serverless way to run containerized applications.\n\nOption E is not a correct answer because it does not address the issue of the Lambda function timing out.","comment_id":"775485","upvote_count":"1","timestamp":"1673705460.0","poster":"masetromain"}],"poster":"masetromain","comment_id":"775484"}],"question_text":"A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed image in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.\n\nThe application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing frequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application’s architecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)","isMC":true,"timestamp":"2023-01-14 15:11:00","answer":"AB","topic":"1","unix_timestamp":1673705460,"answer_description":"","exam_id":33,"question_id":490,"url":"https://www.examtopics.com/discussions/amazon/view/95275-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"AB","choices":{"B":"Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.","E":"Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share.","C":"Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function.","D":"Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.","A":"Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR)."},"answer_images":[]}],"exam":{"numberOfQuestions":529,"name":"AWS Certified Solutions Architect - Professional SAP-C02","lastUpdated":"11 Apr 2025","isMCOnly":true,"id":33,"isImplemented":true,"isBeta":false,"provider":"Amazon"},"currentPage":98},"__N_SSP":true}