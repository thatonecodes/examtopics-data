{"pageProps":{"questions":[{"id":"zQI1a50Svb5I4jw84Fob","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/115104-exam-aws-certified-database-specialty-topic-1-question-324/","answers_community":["C (100%)"],"question_id":251,"unix_timestamp":1689290760,"answer_images":[],"choices":{"B":"Use RDS automated snapshots every 6 hours. Use Amazon S3 Cross-Region Replication to copy the snapshot to a second Region.","C":"Use AWS Backup to take an RDS snapshot every 6 hours and to copy the snapshot to a second Region.","A":"Use RDS automated snapshots. Create an AWS Lambda function to copy the snapshot to a second Region.","D":"Create an RDS cross-Region read replica in a second Region. Use AWS Backup to take an automated snapshot of the read replica every 6 hours."},"topic":"1","question_text":"A company has a 250 GB Amazon RDS Multi-AZ DB instance. The company’s disaster recovery policy requires an RPO of 6 hours in a second AWS Region.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_ET":"C","answer_description":"","question_images":[],"timestamp":"2023-07-14 01:26:00","exam_id":22,"answer":"C","discussion":[{"poster":"Germaneli","comment_id":"1045128","content":"Selected Answer: C\n\"As part of your backup plan, you can optionally create a backup copy in another AWS Region.\"\n\nhttps://aws.amazon.com/getting-started/hands-on/amazon-rds-backup-restore-using-aws-backup/","upvote_count":"2","timestamp":"1697472720.0"},{"upvote_count":"1","comment_id":"954551","poster":"Windy","timestamp":"1689620580.0","content":"The answer is C."},{"poster":"TQM__9MD","comment_id":"951060","timestamp":"1689290760.0","upvote_count":"1","content":"Selected Answer: C\nC is answer"}]},{"id":"B7xlMC3J3mjb9490TaPE","unix_timestamp":1689290940,"answers_community":["CE (75%)","AB (25%)"],"question_text":"A database administrator is reviewing the deployment of an application that uses Amazon DynamoDB. A fleet of Amazon EC2 application instances accesses the database.\n\nThe database administrator notices that EC2 instances are using public IP addresses to access the database and that the database is available to the internet. Company policy requires that all corporate data must be accessed privately and that external access from the internet is not allowed.\n\nWhich combination of steps will ensure that the DynamoDB database meets these requirements? (Choose two.)","answer":"CE","topic":"1","question_images":[],"exam_id":22,"answer_images":[],"question_id":252,"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/115105-exam-aws-certified-database-specialty-topic-1-question-325/","answer_ET":"CE","discussion":[{"comment_id":"1000116","content":"Selected Answer: CE\nC. Create a gateway VPC endpoint \nE. Use the aws:sourceVpce condition \n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ddb.html","poster":"Pranava_GCP","upvote_count":"3","timestamp":"1693969020.0"},{"poster":"zanhsieh","upvote_count":"1","comment_id":"983766","timestamp":"1692285660.0","content":"Selected Answer: CE\nCE.\nA: DDB can only assign security group during cluster creation, and binding with VPC. It seems no document stated DDB can change VPC later on.\nB: No. DDB doesn't have privatelink vpc endpoint.\nC: Yes. DDB only have gateway vpc endpoint.\nD: No. Not relevant.\nE: Yes.\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ddb.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices-security-preventative.html"},{"timestamp":"1690586760.0","upvote_count":"2","poster":"Windy","comment_id":"965917","content":"Selected Answer: CE\nAfter the second thought. I change my anwer to CE. There is no security group and ACL for a DynamoDB table. So A is not correct."},{"content":"The answer is AC","poster":"Windy","comment_id":"954541","timestamp":"1689619440.0","upvote_count":"1"},{"content":"Selected Answer: AB\nIn Option A, configure the DynamoDB security group and network ACL to block external access. This restricts access to the database from the internet, ensuring that all company data is accessed privately.\n\nIn Option B, create an AWS PrivateLink VPC endpoint for DynamoDB and update the VPC route table. Using AWS PrivateLink allows accessing DynamoDB through a private network, completely blocking access from the internet and keeping database access private.","comments":[{"comment_id":"1045131","poster":"Germaneli","timestamp":"1697473140.0","content":"No. You would use a VPC Endpoint to access DynamoDB, not a PrivateLink endpoint. SO it would be C.\n\nTo access the Virtual Gateway, you can restrict client usage with the aws:sourceVpce condition = E.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html","upvote_count":"1"}],"timestamp":"1689290940.0","poster":"TQM__9MD","comment_id":"951062","upvote_count":"2"}],"timestamp":"2023-07-14 01:29:00","choices":{"C":"Create a gateway VPC endpoint for DynamoDB. Update the VPC route table.","E":"Use the aws:sourceVpce condition for all the IAM roles that provision access to the table.","B":"Create an AWS PrivateLink VPC endpoint for DynamoDUpdate the VPC route table.","A":"Configure the DynamoDB security group and network ACLs to block external access.","D":"Provision a NAT gateway to access DynamoDB. Update the VPC route table."}},{"id":"9fCVAQQtQWnH2zp0tuJL","topic":"1","answer_ET":"D","exam_id":22,"question_id":253,"unix_timestamp":1689291000,"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/115106-exam-aws-certified-database-specialty-topic-1-question-326/","answers_community":["D (71%)","14%","14%"],"question_images":[],"question_text":"A company recently created a snapshot of an Amazon RDS for PostgreSQL DB instance that hosts a production database. The company created a new DB instance from the snapshot to test a new application feature while providing isolation from the production database.\n\nDuring testing of the new application feature, the company noticed that read latency on the new database was higher than normal. A database specialist needs to resolve the latency issue.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","answer_images":[],"isMC":true,"choices":{"A":"Log in to the database by using the PostgreSQL administration tool. Issue a SELECT * command against each table in the database.","B":"Create a new parameter group and set the max_connections parameter to 100. Assign the parameter group to the new database. Apply the changes immediately.","D":"Login to the database by using the PostgreSQL administration tool. Issue the VACUUM (ANALYZE, DISABLE_PAGE_SKIPPING) command.","C":"Edit the default parameter group for the matching PostgreSQL engine. Set the max_connections parameter to 100. Reboot the new database to pick up the changes to the parameter group."},"timestamp":"2023-07-14 01:30:00","discussion":[{"timestamp":"1709983920.0","content":"A to mitigate the lazy loading impact to the performance.","comment_id":"1169428","upvote_count":"1","poster":"HSong"},{"timestamp":"1705158480.0","upvote_count":"1","poster":"MultiAZ","content":"Selected Answer: D\nA will work, but D is more operationally efficient","comment_id":"1121777"},{"comment_id":"1028738","upvote_count":"1","poster":"Sathish_dbs","content":"Selected Answer: A\nto avoid first touch penalty we can do select * for all the tables when the read replica is created, VACCUM can be done during the DB running not so effective for first time running.","timestamp":"1696846680.0"},{"content":"Selected Answer: D\nD. run VACUUM (ANALYZE, DISABLE_PAGE_SKIPPING) command\n\nhttps://www.enterprisedb.com/blog/postgresql-vacuum-and-analyze-best-practice-tips","comment_id":"1005115","timestamp":"1694458440.0","poster":"Pranava_GCP","upvote_count":"1"},{"comment_id":"998348","timestamp":"1693814760.0","content":"Selected Answer: D\nI THINK D","poster":"chen0305_099","upvote_count":"1"},{"timestamp":"1693129140.0","poster":"kerl","content":"https://stackoverflow.com/questions/47545414/aws-rds-instance-created-from-snapshot-very-slow","upvote_count":"4","comment_id":"991349"},{"content":"it is D.","timestamp":"1689609900.0","comment_id":"954351","poster":"Windy","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nVACUUM is effective to improve the query performance and to reduce the latency.","poster":"saikarthikeya777","comment_id":"952361","timestamp":"1689423720.0"},{"upvote_count":"1","poster":"TQM__9MD","timestamp":"1689291000.0","comment_id":"951063","content":"Selected Answer: C\nIn option C, editing the default parameter group of the PostgreSQL engine and setting the max_connections parameter to 100 is a way to increase the limit on simultaneous connections. By restarting the new database to reflect the changes in the parameter group, it is possible to increase the number of concurrently executed queries and potentially alleviate read latency."}],"answer_description":""},{"id":"YKzukobIRfk3mQVSmvbh","answer_ET":"A","answer_description":"","question_images":[],"answer":"A","answers_community":["A (85%)","D (15%)"],"answer_images":[],"unix_timestamp":1689291120,"question_text":"A legal research company wants to build a recommendation engine on AWS that connects datasets to help lawyers create legal arguments. The recommendation engine will collect millions of unstructured text documents from third-party sources to identify connections between documents without users needing to manually compare the documents.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","exam_id":22,"topic":"1","timestamp":"2023-07-14 01:32:00","choices":{"A":"Build a graph-based recommendation engine by using Amazon Neptune. Search the documents for vertices with relationships among the different sources to connect.","B":"Create an AWS Lambda application in which the documents are uploaded into Amazon S3. Populate Amazon DynamoDB tables with the metadata of the documents for users to search.","C":"Develop a serverless document scanner by using Amazon Textract to analyze the text from the various sources. Store the detected text in an Amazon Aurora database for analysis.","D":"Define the data sources in an Amazon S3 data lake. Analyze the documents by using AWS Glue. Query the documents for relationships by using Amazon Athena."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/115107-exam-aws-certified-database-specialty-topic-1-question-327/","question_id":254,"discussion":[{"timestamp":"1705177920.0","content":"Selected Answer: A\nThis can be implemented using an RDF graph in Neptune","upvote_count":"2","poster":"MultiAZ","comment_id":"1122032"},{"upvote_count":"2","comment_id":"1013055","poster":"zyjp","timestamp":"1695298740.0","content":"this is the last question 327 ? not 335 ?"},{"content":"Selected Answer: A\nA. Amazon Neptune.\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/intro.html","poster":"Pranava_GCP","upvote_count":"2","timestamp":"1693960920.0","comment_id":"1000032"},{"timestamp":"1693814880.0","comment_id":"998350","content":"Selected Answer: A\nAAAAAA","upvote_count":"2","poster":"chen0305_099"},{"poster":"marcelodba","content":"Selected Answer: A\nGraphDB","upvote_count":"2","timestamp":"1693410660.0","comment_id":"994262"},{"poster":"EueChan","content":"Selected Answer: A\nIts A. Graph database for relationships. also Amazon Athena doesn't work with unstructured documents !","timestamp":"1692909420.0","comment_id":"989482","upvote_count":"3"},{"upvote_count":"3","comment_id":"954327","content":"It's A.","poster":"Windy","timestamp":"1689608160.0"},{"comment_id":"951065","timestamp":"1689291120.0","poster":"TQM__9MD","content":"Selected Answer: D\nOption D, defining data sources in an Amazon S3 data lake, analyzing the documents using AWS Glue, and querying the relationships between documents using Amazon Athena, is the recommended solution. It allows for the storage and analysis of unstructured text documents in the data lake, while leveraging AWS Glue and Amazon Athena for efficient querying of document relationships. This approach minimizes operational overhead while meeting the stated requirements.","upvote_count":"2"}]},{"id":"UxOAd5t1bYSUJWu7XITS","answer_description":"","answers_community":["D (100%)"],"isMC":true,"question_images":[],"timestamp":"2023-11-26 02:05:00","url":"https://www.examtopics.com/discussions/amazon/view/127215-exam-aws-certified-database-specialty-topic-1-question-328/","topic":"1","answer_images":[],"answer_ET":"D","discussion":[{"timestamp":"1700960700.0","comment_id":"1080413","content":"I think D.\nInline LOB mode – In inline LOB mode, you set the maximum LOB size that DMS transfers inline. LOBs smaller than the specified size are transferred inline. LOBs larger than the specified size are replicated using full LOB mode. You can select this option to replicate both small and large LOBs when most of the LOBs are small. DMS doesn’t support inline LOB mode for endpoints that don’t support Full LOB mode, like S3 and Redshift.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html","poster":"marll88","upvote_count":"5"},{"timestamp":"1705178280.0","upvote_count":"2","content":"Selected Answer: D\nAnswer is D. \nSet InlineLobMaxSize to 32KB to migrate the small documents fast.","comment_id":"1122036","poster":"MultiAZ"},{"timestamp":"1702957680.0","upvote_count":"3","comment_id":"1100255","content":"Selected Answer: D\nFull LOB Mode - Full LOB mode can be quite slow, \nLimited LOB Mode- LOBs that exceed the maximum LOB size are truncated, and a warning is issued to the log file\nInline LOB mode - LOBs larger than the specified size are replicated using full LOB mode.","poster":"KikiNoviandi"},{"upvote_count":"1","poster":"silvaa360","comment_id":"1093339","timestamp":"1702288380.0","content":"The fastest will always be LIMITED, but we loose data. Also, INLINE is only supported during full load (not CDC), so if the ongoing replication means only CDC can be an indicator to choose LIMITED.","comments":[{"poster":"silvaa360","timestamp":"1702288620.0","upvote_count":"1","content":"Actually I'm chaning to inline. The question says all data, so cannot be limited. Also, as marll88 shows, when the most of the LOBs are small it makes sense to use INLINE.","comment_id":"1093342"}]}],"unix_timestamp":1700960700,"question_id":255,"exam_id":22,"answer":"D","question_text":"A database specialist needs to move a table from a database that is running on an Amazon Aurora PostgreSQL DB cluster into a new and distinct database cluster. The new table in the new database must be updated with any changes to the original table that happen while the migration is in progress.\n\nThe original table contains a column to store data as large as 2 GB in the form of large binary objects (LOBs). A few records are large in size, but most of the LOB data is smaller than 32 KB.\n\nWhat is the FASTEST way to replicate all the data from the original table?","choices":{"C":"Use AWS Database Migration Service (AWS DMS) with ongoing replication in limited LOB mode.","A":"Use AWS Database Migration Service (AWS DMS) with ongoing replication in full LOB mode.","B":"Take a snapshot of the database. Create a new DB instance by using the snapshot.","D":"Use AWS Database Migration Service (AWS DMS) with ongoing replication in inline LOB mode."}}],"exam":{"isBeta":false,"provider":"Amazon","numberOfQuestions":359,"isImplemented":true,"lastUpdated":"11 Apr 2025","id":22,"isMCOnly":false,"name":"AWS Certified Database - Specialty"},"currentPage":51},"__N_SSP":true}