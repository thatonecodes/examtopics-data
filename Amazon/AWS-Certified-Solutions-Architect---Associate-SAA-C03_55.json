{"pageProps":{"questions":[{"id":"gWiafCMcM3C9RLNrV0Ln","question_text":"A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data.\n\nThe company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency.\n\nWhich solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/99711-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"discussion":[{"content":"D is the correct answer \n\nVolume Gateway CACHED Vs STORED \nCached = stores a subset of frequently accessed data locally \nStored = Retains the ENTIRE (\"all file types\") in on prem data centre","timestamp":"1679354040.0","comment_id":"845337","upvote_count":"27","poster":"Grace83"},{"poster":"netcj","timestamp":"1694266920.0","upvote_count":"6","comment_id":"1003263","content":"Selected Answer: D\n\"users retain immediate access to all file types\"\nimmediate cannot be cached -> D"},{"poster":"dkw2342","upvote_count":"2","comment_id":"1164123","timestamp":"1709385480.0","content":"Bad question. No RTO/RPO, so impossible to properly answer. They probably want to hear option D. \n\nDepending on RPO, option B is also an adequate solution (data remains immediately accessible without experiencing latency via existing infrastructure, backup to cloud for DR). Also, this option requires LESS changes to existing infra than A. Only argument against B is that VTLs are usually used for legacy DR solutions, not for new ones, where object storage such as S3 is usually supported natively."},{"upvote_count":"2","poster":"MrPCarrot","timestamp":"1709103060.0","content":"Answer is C go argue somewhere.","comment_id":"1161322"},{"upvote_count":"2","content":"Selected Answer: D\nA,B are wrong types of gateways for hundreds of TB of data that needs immediate access on-prem. C limits to 10TB. D provides access to all the files.","comment_id":"1112960","timestamp":"1704302340.0","poster":"awsgeek75"},{"timestamp":"1703861580.0","upvote_count":"3","comment_id":"1108800","poster":"pentium75","content":"Selected Answer: D\n\"Immediate access to all file types from the on-premises systems without experiencing latency\" requirement is not met by C. Also the solution is meant for DR purposes, the primary storage for the data should remain on premises."},{"poster":"daniel1","content":"Selected Answer: C\nFrom chatGPT4\nConsidering the requirements of minimal infrastructure change, immediate file access, and low-latency, Option C: Provisioning an AWS Storage Gateway Volume Gateway (cached volume) with a 10 TB local cache, seems to be the most fitting solution. This setup aligns with the existing iSCSI setup and provides a local cache for low-latency access, while also configuring scheduled snapshots for disaster recovery. In the event of a disaster, restoring a snapshot to an Amazon EBS volume and attaching it to an Amazon EC2 instance as described in this option would align with the recovery objective.","comment_id":"1050309","upvote_count":"1","timestamp":"1697960340.0","comments":[{"upvote_count":"9","timestamp":"1703861400.0","comments":[{"comment_id":"1146022","upvote_count":"2","poster":"LoXoL","content":"pentium75 is right.","timestamp":"1707558600.0"}],"poster":"pentium75","comment_id":"1108797","content":"ChatGPT is wrong. \"Immediate access to all file types from the on-premises systems without experiencing latency\" needs \"stored volume\" type. With \"cached volume\" not all data will be available locally."}]},{"poster":"TariqKipkemei","upvote_count":"3","comment_id":"1040193","content":"Selected Answer: D\nEnd users retain immediate access to all file types = Volume Gateway stored volume","timestamp":"1696998180.0"},{"upvote_count":"3","timestamp":"1693920900.0","content":"Selected Answer: D\ndddddddd","comment_id":"999559","poster":"Guru4Cloud"},{"upvote_count":"3","timestamp":"1685295420.0","poster":"alexandercamachop","comment_id":"908773","content":"Selected Answer: D\nCorrect answer is Volume Gateway Stored which keeps all data on premises.\nTo have immediate access to the data. Cached is for frequently accessed data only."},{"poster":"omoakin","upvote_count":"1","content":"CCCCCCCCCCCCCCCC","timestamp":"1685236380.0","comments":[{"poster":"24b2e9e","upvote_count":"2","timestamp":"1718771520.0","comment_id":"1232704","content":"The stored volume configuration stores the entire data set on-premises and asynchronously backs up the data to AWS. The cached volume configuration stores recently accessed data on-premises, and the remaining data is stored in Amazon S3\n-that is why D is right"}],"comment_id":"908274"},{"comment_id":"903837","content":"Selected Answer: D\nD is the correct answer\nVolume Gateway CACHED Vs STORED\nCached = stores a data recentlly at local\nStored = Retains the ENTIRE (\"all file types\") in on prem data centre","poster":"lucdt4","upvote_count":"2","timestamp":"1684742340.0"},{"poster":"rushi0611","timestamp":"1683358440.0","upvote_count":"3","content":"Selected Answer: D\nIn the cached mode, your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access.\nIn the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.\nReference: https://aws.amazon.com/storagegateway/faqs/\nGood luck.","comment_id":"890564"},{"upvote_count":"2","timestamp":"1682837400.0","content":"Selected Answer: D\nIt is stated the company wants to keep the data locally and have DR plan in cloud. It points directly to the volume gateway","comment_id":"884934","poster":"kruasan"},{"upvote_count":"3","timestamp":"1679151900.0","comment_id":"842853","poster":"UnluckyDucky","content":"Selected Answer: D\n\"The company wants to ensure that end users retain immediate access to all file types from the on-premises systems \"\n\nD is the correct answer."},{"upvote_count":"3","poster":"CapJackSparrow","comments":[{"upvote_count":"2","timestamp":"1683377460.0","content":"https://docs.aws.amazon.com/storagegateway/latest/vgw/StorageGatewayConcepts.html\n\nStored volumes can range from 1 GiB to 16 TiB in size and must be rounded to the nearest GiB. Each gateway configured for stored volumes can support up to 32 volumes and a total volume storage of 512 TiB (0.5 PiB).","comment_id":"890742","poster":"eddie5049"},{"poster":"MssP","upvote_count":"4","content":"all file types. Cached only save the most frecuently or lastest accesed. If you didn´t access any type for a long time, you will not cache it -> No immediate access","comment_id":"850046","timestamp":"1679741700.0"},{"comment_id":"1108799","upvote_count":"2","timestamp":"1703861460.0","content":"Also the solution is meant for DR purposes, it's not like they need more storage or so.","poster":"pentium75"}],"timestamp":"1678980780.0","content":"Selected Answer: C\nall file types, NOT all files. Volume mode can not cache 100TBs.","comment_id":"841112"},{"content":"Selected Answer: D\n\"The company wants to ensure that end users retain immediate access to all file types from the on-premises systems \"\n\nThis points to stored volumes..","upvote_count":"2","poster":"WherecanIstart","timestamp":"1678941360.0","comment_id":"840549"},{"upvote_count":"4","content":"Selected Answer: D\nOption D is the right choice for this question . \"The company wants to ensure that end users retain immediate access to all file types from the on-premises systems \"\n- Cached volumes: low latency access to most recent data\n- Stored volumes: entire dataset is on premise, scheduled backups to S3\nHence Volume Gateway stored volume is the apt choice.","poster":"KAUS2","comment_id":"838625","timestamp":"1678779840.0"},{"comment_id":"835253","content":"Answer is C.\n\nOption D is not the best solution because a Volume Gateway stored volume does not provide immediate access to all file types and would require additional steps to retrieve data from Amazon S3, which can result in latency for end-users.","timestamp":"1678468140.0","comments":[{"poster":"UnluckyDucky","timestamp":"1678561080.0","content":"You're confusing cached mode with stored volume mode.","upvote_count":"2","comment_id":"836415"}],"poster":"bangfire","upvote_count":"2"},{"comment_id":"834488","content":"Selected Answer: C\nAnswer is C.\nwhy?\nhttps://docs.aws.amazon.com/storagegateway/latest/vgw/StorageGatewayConcepts.html#storage-gateway-stored-volume-concepts\n\n\"Stored volumes can range from 1 GiB to 16 TiB in size and must be rounded to the nearest GiB. Each gateway configured for stored volumes can support up to 32 volumes and a total volume storage of 512 TiB\"\n\nOption D states: \"Provision an AWS Storage Gateway Volume Gateway stored *volume* with the same amount of disk space as the existing file storage volume.\". \nNotice that it states volume and not volumes, which would be the only way to match the information that the question provides.\nInitial question states that on-premise volume is 100s of TB in size.\nTherefore, only logical and viable answer can be C.\nFeel free to prove me wrong","poster":"un1x","upvote_count":"3","timestamp":"1678401540.0","comments":[{"content":"Stored volumes can range from 1 GiB to 16 TiB in size and must be rounded to the nearest GiB. Each gateway configured for stored volumes can support up to 32 volumes and a total volume storage of 512 TiB (0.5 PiB).\n\nwhy not configure multiple gateway to achieve the hundreds of TB?","timestamp":"1683377580.0","poster":"eddie5049","comment_id":"890743","upvote_count":"2"}]},{"upvote_count":"4","timestamp":"1677873960.0","poster":"Steve_4542636","content":"Selected Answer: D\nStored Volume Gateway will retain ALL data locally whereas Cached Volume Gateway retains frequently accessed data locally","comment_id":"828353"},{"poster":"KZM","upvote_count":"2","content":"As per the given information, option 'C' can support the Company's requirements with the LEAST amount of change to the existing infrastructure, I think.\nhttps://aws.amazon.com/storagegateway/volume/","comment_id":"819887","timestamp":"1677194520.0"},{"poster":"bdp123","timestamp":"1677089880.0","content":"Selected Answer: D\nthe \" all file types\" is confusing - does not say \"all files\" - also, hundreds of Terabytes is enormously large to maintain all files on-prem. Cache volume is also low latency","upvote_count":"3","comment_id":"818198"},{"poster":"LuckyAro","comment_id":"817574","content":"Selected Answer: D\nAnswer is D","upvote_count":"2","timestamp":"1677050820.0"},{"timestamp":"1676768640.0","comment_id":"813622","content":"Answer is D - Retain Immediate Access","upvote_count":"4","poster":"rrharris"},{"poster":"zTopic","upvote_count":"5","timestamp":"1676702460.0","content":"Selected Answer: D\nKeyword: Retain access to ALL data on-premise. \nProvision an AWS Storage Gateway Volume Gateway stored volume","comment_id":"812727"},{"timestamp":"1676665320.0","upvote_count":"3","content":"Selected Answer: C\nhttps://aws.amazon.com/storagegateway/volume/","poster":"bdp123","comments":[{"upvote_count":"2","poster":"Rehan33","timestamp":"1676932860.0","content":"access to all file types not upto 10 tb. thats mean we will use store one not cached . D is correct","comment_id":"815981"}],"comment_id":"812335"}],"exam_id":31,"answer_description":"","question_images":[],"answers_community":["D (83%)","C (17%)"],"unix_timestamp":1676665320,"answer_ET":"D","choices":{"D":"Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.","B":"Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.","C":"Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.","A":"Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files."},"topic":"1","question_id":271,"timestamp":"2023-02-17 21:22:00","isMC":true},{"id":"y9Cpr1XKHBAynvB0D1KK","question_id":272,"answers_community":["A (95%)","5%"],"topic":"1","discussion":[{"comments":[{"content":"Option B is incorrect because updating the S3 ACL (Access Control List) will only affect the permissions of the application, not the users accessing the content.\n\nOption C is incorrect because redeploying the application to Amazon S3 will not resolve the issue related to user access permissions.\n\nOption D is incorrect because updating custom attribute mappings in Amazon Cognito will not directly grant users the proper permissions to access the protected content.","upvote_count":"12","timestamp":"1716918720.0","comment_id":"908781","poster":"alexandercamachop"}],"comment_id":"908780","timestamp":"1716918660.0","upvote_count":"16","content":"Selected Answer: A\nTo resolve the issue and provide proper permissions for users to access the protected content, the recommended solution is:\n\nA. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.\n\nExplanation:\n\nAmazon Cognito provides authentication and user management services for web and mobile applications.\nIn this scenario, the application is using Amazon Cognito as an identity provider to authenticate users and obtain JSON Web Tokens (JWTs).\nThe JWTs are used to access protected resources stored in another S3 bucket.\nTo grant users access to the protected content, the proper IAM role needs to be assumed by the identity pool in Amazon Cognito.\nBy updating the Amazon Cognito identity pool with the appropriate IAM role, users will be authorized to access the protected content in the S3 bucket.","poster":"alexandercamachop"},{"timestamp":"1708587120.0","upvote_count":"6","poster":"LuckyAro","comment_id":"817575","content":"Selected Answer: A\nA is the best solution as it directly addresses the issue of permissions and grants authenticated users the necessary IAM role to access the protected content.\n\nA suggests updating the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content. This is a valid solution, as it would grant authenticated users the necessary permissions to access the protected content."},{"poster":"Marco_St","content":"Selected Answer: A\nIAM role is assinged to IAM users or groups or assumed by AWS service. So IAM role is given to AWS Cognito service which provides temporary AWS credentials to authenticated users. so technically When a user is authenticated by Cognito, they receive temporary credentials based on the IAM role tied to the Cognito identity pool. If this IAM role has permissions to access certain S3 buckets or objects, the authenticated user will be able to access those resources as allowed by the role. This service is used under the hood by Cognito to provide these temporary credentials. The credentials are limited in time and scope based on the permissions defined in the IAM role.","timestamp":"1733762400.0","upvote_count":"2","comment_id":"1091913"},{"comment_id":"999548","timestamp":"1725542760.0","upvote_count":"3","content":"Selected Answer: A\nA. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.","poster":"Guru4Cloud"},{"comment_id":"920442","timestamp":"1718084460.0","upvote_count":"2","content":"Selected Answer: A\nServices access other services via IAM Roles. Hence why updating AWS Cognito identity pool to assume proper IAM Role is the right solution.","poster":"Abrar2022"},{"timestamp":"1712633700.0","poster":"shanwford","content":"Selected Answer: A\nAmazon Cognito identity pools assign your authenticated users a set of temporary, limited-privilege credentials to access your AWS resources. The permissions for each user are controlled through IAM roles that you create. https://docs.aws.amazon.com/cognito/latest/developerguide/role-based-access-control.html","upvote_count":"3","comment_id":"865259"},{"poster":"Brak","upvote_count":"2","comment_id":"831636","content":"Selected Answer: D\nA makes no sense - Cognito is not accessing the S3 resource. It just returns the JWT token that will be attached to the S3 request.\n\nD is the right answer, using custom attributes that are added to the JWT and used to grant permissions in S3. See https://docs.aws.amazon.com/cognito/latest/developerguide/using-attributes-for-access-control-policy-example.html for an example.","timestamp":"1709797380.0","comments":[{"poster":"Abhineet9148232","content":"But even D requires setting up the permissions as bucket policy (as show in the shared example) which includes higher overhead than managing permissions attached to specific roles.","upvote_count":"3","comment_id":"833669","timestamp":"1709967900.0"},{"content":"A says \"Identity Pool\"\nAccording to AWS: \"With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB.\"\nSo, answer is A","upvote_count":"4","comment_id":"842383","poster":"asoli","timestamp":"1710721920.0"}]},{"content":"Selected Answer: A\nServices access other services via IAM Roles.","timestamp":"1709496540.0","poster":"Steve_4542636","upvote_count":"2","comment_id":"828359"},{"poster":"jennyka76","timestamp":"1708348560.0","comment_id":"814106","content":"ANSWER - A\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/tutorial-create-identity-pool.html\nYou have to create an custom role such as read-only","upvote_count":"5"},{"timestamp":"1708238760.0","poster":"zTopic","content":"Selected Answer: A\nAnswer is A","upvote_count":"3","comment_id":"812733"}],"choices":{"B":"Update the S3 ACL to allow the application to access the protected content.","A":"Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.","D":"Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and grant users the proper permissions to access the protected content.","C":"Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content."},"exam_id":31,"timestamp":"2023-02-18 07:46:00","answer":"A","answer_description":"","answer_ET":"A","unix_timestamp":1676702760,"question_text":"A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket.\n\nUpon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content.\n\nWhich solution meets these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/99754-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"answer_images":[],"question_images":[]},{"id":"mBShQ0G8ep038400Vx7b","question_id":273,"url":"https://www.examtopics.com/discussions/amazon/view/99755-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","isMC":true,"answers_community":["AB (65%)","BD (25%)","9%"],"question_text":"An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets.\n\nWhich combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)","answer":"AB","choices":{"B":"Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.","E":"Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.","D":"Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.","C":"Configure an S3 Lifecycle policy to clean up expired object delete markers.","A":"Move assets to S3 Intelligent-Tiering after 30 days."},"timestamp":"2023-02-18 07:49:00","answer_ET":"AB","unix_timestamp":1676702940,"exam_id":31,"topic":"1","discussion":[{"timestamp":"1676726040.0","poster":"Neha999","comment_id":"812990","content":"AB\nA : Access Pattern for each object inconsistent, Infrequent Access\nB : Deleting Incomplete Multipart Uploads to Lower Amazon S3 Costs","upvote_count":"25"},{"poster":"TungPham","upvote_count":"16","comment_id":"816082","timestamp":"1676942580.0","content":"Selected Answer: AB\nB because Abort Incomplete Multipart Uploads Using S3 Lifecycle => https://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/\nA because The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent => random access => S3 Intelligent-Tiering"},{"comment_id":"1319593","upvote_count":"1","content":"Selected Answer: AB\nInconsistent access patterns: A is correct, and D is out (could work but less ideal and costs more).\n\nC: \"object delete markers\" only apply to versioned buckets.","poster":"LeonSauveterre","timestamp":"1732863960.0"},{"timestamp":"1721549340.0","comment_id":"1252312","upvote_count":"3","poster":"ChymKuBoy","content":"Selected Answer: AB\nAB for sure"},{"content":"Selected Answer: BD\nIf we consider these statements:\n1. For the first 30 days after upload, the objects will be accessed frequently\n2.The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent\n3.The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets.\n4.The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again.\nStatements 1 and 2 cloudl be completed with option D and not A because datas are infrequently accessed only after 30 days.\nDue to usage of multipart upload, to meet requirement regarding cost optimization, option B will be used to clean up buckets uncompleted file parts(statements 3 & 4).","comment_id":"1171808","poster":"bujuman","timestamp":"1710258840.0","upvote_count":"4"},{"content":"Selected Answer: AD\nBecause A & D address the main ask, there's no mention of cost optimization.","upvote_count":"2","comment_id":"1152558","comments":[{"poster":"NayeraB","content":"*Facepalm* It does ask for reducing the cost, A&B it is!","comment_id":"1152562","timestamp":"1708176780.0","upvote_count":"3"}],"timestamp":"1708176540.0","poster":"NayeraB"},{"poster":"NayeraB","comments":[{"poster":"NayeraB","timestamp":"1708176600.0","upvote_count":"2","comment_id":"1152559","content":"Not C ':D, I meant to say A&D. Added another vote for that one."}],"upvote_count":"1","content":"Selected Answer: AC\nBecause A & C address the main ask, there's no mention of cost optimization.","comment_id":"1152557","timestamp":"1708176480.0"},{"timestamp":"1704302760.0","comment_id":"1112967","upvote_count":"3","content":"Selected Answer: AB\nA as the access pattern for each object is inconsistent so let AWS AWS do the handling.\nB deals with multi-part duplication issues and saves money by deleting incomplete uploads\nC No mention of deleted object so this is a distractor\nD The objects will be accessed in unpredictable pattern so can't use this\nE Not HA compliant","comments":[{"upvote_count":"4","poster":"awsgeek75","timestamp":"1704302940.0","content":"Also, don't be confused by 30 days. The question has tricky wording: \" The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent\"\nIt does NOT say that objects will be accessed less frequently after 30 days. It says the access is unpredictable which means it could go up or down. Don't make assumptions.","comment_id":"1112970"}],"poster":"awsgeek75"},{"timestamp":"1703862060.0","upvote_count":"4","content":"Selected Answer: AB\nC is nonsense\nE does not meet the \"high availability and resiliency\" requirement\nB is obvious (incomplete multipart uploads consume space -> cost money)\n\nThe tricky part is A vs. D. However, 'inconsistent access patterns' are the primary use case for Intelligent-Tiering. There are probably objects that will never be accessed and that would be moved to Glacier Instant Retrieval by Intelligent-Tiering, thus the overall cost would be lower than with D.","comment_id":"1108812","poster":"pentium75"},{"timestamp":"1703698860.0","content":"bd https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-infreq-data-access =>S3 Standard-IA objects\nare resilient to the loss of an Availability Zone. This storage class offers greater availability and\nresiliency than the S3 One Zone-IA class","poster":"osmk","upvote_count":"2","comment_id":"1107038"},{"content":"Selected Answer: AB\nI wouldnt go with D since \" the access patterns for each object will be inconsistent.\", so we cannot move all assets to IA","timestamp":"1703249460.0","poster":"raymondfekry","comment_id":"1103368","upvote_count":"2"},{"content":"Selected Answer: AB\nincosistent access pattern brings more sense to use Intelligent-Tiering after 30 days which also covers infrequent access.","timestamp":"1702140420.0","upvote_count":"2","poster":"Marco_St","comment_id":"1091918"},{"poster":"Guru4Cloud","upvote_count":"2","content":"Selected Answer: AB\nA. Move assets to S3 Intelligent-Tiering after 30 days.\nB. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.","comment_id":"999543","timestamp":"1693920000.0"},{"upvote_count":"2","comment_id":"962831","timestamp":"1690296840.0","poster":"vini15","content":"should be A and B"},{"poster":"MrAWSAssociate","upvote_count":"2","comment_id":"929660","content":"Selected Answer: BD\nOption A has not been mentioned for resiliency in S3, check the page: https://docs.aws.amazon.com/AmazonS3/latest/userguide/disaster-recovery-resiliency.html\nTherefore, I am with B & D choices.","comments":[{"upvote_count":"2","comment_id":"1108808","timestamp":"1703861820.0","poster":"pentium75","content":"Intelligent-Tiering just moves to Standard-IA or Glacier Instant Access based on access patterns. This does not affect resiliency."}],"timestamp":"1687360080.0"},{"poster":"alexandercamachop","content":"Selected Answer: AB\nA. Move assets to S3 Intelligent-Tiering after 30 days.\nB. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.\n\nExplanation:\n\nA. Moving assets to S3 Intelligent-Tiering after 30 days: This storage class automatically analyzes the access patterns of objects and moves them between frequent access and infrequent access tiers. Since the objects will be accessed frequently for the first 30 days, storing them in the frequent access tier during that period optimizes performance. After 30 days, when the access patterns become inconsistent, S3 Intelligent-Tiering will automatically move the objects to the infrequent access tier, reducing storage costs.\n\nB. Configuring an S3 Lifecycle policy to clean up incomplete multipart uploads: Multipart uploads are used for large objects, and incomplete multipart uploads can consume storage space if not cleaned up. By configuring an S3 Lifecycle policy to clean up incomplete multipart uploads, unnecessary storage costs can be avoided.","timestamp":"1685296620.0","upvote_count":"2","comment_id":"908782"},{"content":"Selected Answer: AD\nAD.\n\nB makes no sense because multipart uploads overwrite objects that are already uploaded. The question never says this is a problem.","upvote_count":"2","comments":[{"comment_id":"945556","upvote_count":"3","timestamp":"1688725140.0","poster":"VellaDevil","content":"Questions says to optimize cost and if incomplete multiparts are not aborted it will still use capacity on S3 Bucket thus increase unnecessary cost."}],"comment_id":"907219","timestamp":"1685091240.0","poster":"antropaws"},{"timestamp":"1679951820.0","content":"Selected Answer: AB\nthe following two actions to optimize S3 storage costs while maintaining high availability and resiliency of stored assets:\n\nA. Move assets to S3 Intelligent-Tiering after 30 days. This will automatically move objects between two access tiers based on changing access patterns and save costs by reducing the number of objects stored in the expensive tier.\n\nB. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads. This will help to reduce storage costs by removing incomplete multipart uploads that are no longer needed.","upvote_count":"3","poster":"klayytech","comment_id":"852498"},{"comment_id":"849642","poster":"datz","comments":[{"timestamp":"1679696880.0","comments":[{"poster":"datz","upvote_count":"2","content":"sorry remove the above comment, as we are setting solution which will be needed after 30 Days\n\nthis should be : Amazon S3 Standard-Infrequent Access (S3 Standard-IA)","comment_id":"849647","timestamp":"1679697060.0"}],"comment_id":"849645","poster":"datz","content":"Apologies D is wrong for sure lol \n\n\"S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed.\" and for the first 30 days data is frequently accessed lol. \n\nSo best solution will be A - Amazon S3 Intelligent-Tiering","upvote_count":"2"}],"content":"Selected Answer: BD\nB = Deleting incomplete uploads will lower S3 cost.\n\nand D: as \"For the first 30 days after upload, the objects will be accessed frequently\"\n\nIntelligent checks and if file haven't been access for 30 consecutive days and send infrequent access.So if somebody accessed the file 20 days after the upload with the intelligent process, file will be moved to Infrequent Access tier after 50 days. Which will reflect against the COST.\n\n\"S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier and after 90 days of no access to the Archive Instant Access tier. For data that does not require immediate retrieval, you can set up S3 Intelligent-Tiering to monitor and automatically move objects that aren’t accessed for 180 days or more to the Deep Archive Access tier to realize up to 95% in storage cost savings.\"\n\nhttps://aws.amazon.com/s3/storage-classes/#Unknown_or_changing_access","upvote_count":"4","timestamp":"1679696520.0"},{"timestamp":"1679258520.0","comment_id":"844234","poster":"MLCL","content":"Selected Answer: BD\nInfrequent access is written in the question so it's BD","comments":[{"content":"It is not infrequent... it is LESS frequent. It can be few less or too much less (infrequent) but it is clear that pattern is inconsistent -> A","poster":"MssP","timestamp":"1679935740.0","upvote_count":"3","comment_id":"852290"}],"upvote_count":"1"},{"content":"Selected Answer: AB\nThe answer is AB\nA: \"the access patterns for each object will be inconsistent\" so Intelligent-Tiering works well for this assumption (even better than D. It may put it in lower tiers based on access patterns that Standard-IA)\nD: incomplete multipart is just a waste of resources","comment_id":"842386","comments":[{"comment_id":"842387","upvote_count":"1","content":"I meant B: incomplete multipart is just a waste of resources","timestamp":"1679099940.0","poster":"asoli"}],"poster":"asoli","upvote_count":"3","timestamp":"1679099880.0"},{"timestamp":"1678953000.0","comment_id":"840654","content":"Selected Answer: AB\nhttps://www.examtopics.com/discussions/amazon/view/84533-exam-aws-certified-solutions-architect-associate-saa-c02/","upvote_count":"3","poster":"AlessandraSAA"},{"upvote_count":"2","poster":"cenil","timestamp":"1678931940.0","content":"AB, Unknown of changing access pattern\nhttps://aws.amazon.com/s3/storage-classes/","comment_id":"840492"},{"content":"Selected Answer: AB\nI think B is obvious, and I chose A because the pattern is unpredictable","comment_id":"839604","poster":"houzuun","timestamp":"1678863780.0","upvote_count":"3"},{"upvote_count":"2","timestamp":"1678818180.0","poster":"Maximus007","comment_id":"839120","content":"B is clear\nthe choice might be between A and D\nI vote for A - S3 Intelligent-Tiering will analyze patterns and decide properly"},{"comment_id":"833900","timestamp":"1678361220.0","poster":"[Removed]","upvote_count":"2","content":"Selected Answer: BD\ni think b , d make more sense\nit is no matter where each object is moved, \nwe only know object is not accessed frequently after 30days\nso i go with D"},{"upvote_count":"1","comment_id":"833672","poster":"Abhineet9148232","timestamp":"1678346040.0","content":"Selected Answer: BD\nS3-IA provides same low latency and high throughput performance of S3 Standard. Ideal for infrequent but high throughput access.\n\nhttps://aws.amazon.com/s3/storage-classes/#Unknown_or_changing_access"},{"timestamp":"1677874620.0","content":"Selected Answer: AB\nFor A vs D, this comment is \"but the access patterns for each object will be inconsistent.\" That means some object will be accessed, others will not. This will give the Inteligent tier the opportunity to move the S3 object to Glacier Instant Retireval which still has very low latency. This is a confusing question though since Inteligent tiering does add additional costs per object.","poster":"Steve_4542636","upvote_count":"3","comment_id":"828371"},{"comment_id":"827409","upvote_count":"1","timestamp":"1677789240.0","poster":"HaineHess","content":"Selected Answer: BD\nb d for cost saving & high availability"},{"upvote_count":"2","poster":"KZM","timestamp":"1677508800.0","comment_id":"823822","content":"Selected Answer: BD\nB is sure\nHere is why D is correct for the storage solution with less frequent access. See the below link for detail about that.\nhttps://aws.amazon.com/s3/storage-classes/#Infrequent_access"},{"upvote_count":"2","comment_id":"823817","content":"It is sure that the correct answer are option B and D.\nS3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval charge.\nhttps://aws.amazon.com/s3/storage-classes/#Infrequent_access","poster":"KZM","timestamp":"1677508380.0"},{"timestamp":"1677194400.0","content":"Selected Answer: AB\nAs it says \"inconsistent patterns\" intelligent tiering is best","upvote_count":"4","comment_id":"819884","poster":"Ja13"},{"timestamp":"1677171300.0","comment_id":"819426","poster":"bdp123","content":"Selected Answer: AB\nS3 Intelligent-Tiering - Data with unknown, changing, or unpredictable access patterns and moves objects that have not been accessed in 30 consecutive days to the Infrequent Access tier.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html","upvote_count":"3"},{"upvote_count":"2","timestamp":"1677051900.0","comment_id":"817582","poster":"LuckyAro","content":"Selected Answer: BD\nMakes more sense to me"},{"comment_id":"816016","timestamp":"1676935980.0","upvote_count":"3","content":"AB, \nDelete failed multi part uploads https://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/","poster":"geekgirl22"},{"comment_id":"815656","content":"Selected Answer: BD\nhttps://www.examtopics.com/discussions/amazon/view/84533-exam-aws-certified-solutions-architect-associate-saa-c02/","timestamp":"1676917500.0","poster":"Virgilio1t","upvote_count":"1"},{"comment_id":"814618","content":"Selected Answer: BD\nB and D","timestamp":"1676844900.0","poster":"skiwili","upvote_count":"1"},{"poster":"NolaHOla","comment_id":"814296","upvote_count":"4","timestamp":"1676826480.0","content":"AB:\nThe company can optimize its S3 storage costs while maintaining high availability and resiliency of stored assets by taking the following actions:\n\nA. Move assets to S3 Intelligent-Tiering after 30 days to automatically move infrequently accessed objects to the infrequent access tier and minimize storage costs. This storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier without any performance impact or operational overhead. It provides automatic cost savings by moving data between two access tiers (frequent and infrequent) when access patterns change, without any performance impact or operational overhead.\n\nB. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads. This will help prevent storage costs from increasing due to incomplete multipart uploads and minimize storage costs.\n\nTherefore, the recommended actions are A and B."},{"upvote_count":"2","poster":"jennyka76","comment_id":"814089","content":"Answer A & D\nhttps://aws.amazon.com/s3/storage-classes/","timestamp":"1676811720.0"},{"upvote_count":"4","poster":"kpato87","content":"Selected Answer: AD\nThe access patterns for each object are inconsistent after 30 days, so moving the assets to S3 Intelligent-Tiering will optimize storage costs while maintaining high availability and resiliency.\n\nMoving assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days will also optimize storage costs, as S3 Standard-IA is designed for infrequently accessed data that needs to be stored for longer durations, while still maintaining high availability and durability.","comment_id":"813075","timestamp":"1676729760.0"},{"comment_id":"812993","upvote_count":"2","timestamp":"1676726160.0","poster":"Neha999","content":"A: Intelligent-Tiering (not Infrequent Access )"},{"timestamp":"1676702940.0","upvote_count":"1","poster":"zTopic","content":"Selected Answer: BD\nShould be B & D","comment_id":"812736"}],"answer_images":[],"question_images":[]},{"id":"AzPNi9phj4mGKXfEpoEo","exam_id":31,"answers_community":["A (91%)","9%"],"question_id":274,"timestamp":"2023-02-18 14:03:00","unix_timestamp":1676725380,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/99795-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"poster":"Bhawesh","comment_id":"819594","comments":[{"timestamp":"1709651820.0","poster":"Guru4Cloud","comment_id":"999538","upvote_count":"2","content":"Option A uses a network firewall which is overkill for instance-level rules."}],"timestamp":"1692810060.0","content":"Selected Answer: A\nCorrect Answer A. Send the outbound connection from EC2 to Network Firewall. In Network Firewall, create stateful outbound rules to allow certain domains for software patch download and deny all other domains. \n\nhttps://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata-examples.html#suricata-example-domain-filtering","upvote_count":"15"},{"upvote_count":"11","content":"Selected Answer: A\nCan't use URLs in outbound rule of security groups. URL Filtering screams Firewall.","comment_id":"836432","timestamp":"1694452260.0","poster":"UnluckyDucky"},{"timestamp":"1724698860.0","content":"Selected Answer: A\nSecurity Groups operate at the transport layer (Layer 4) of the OSI model and are primarily concerned with controlling traffic based on IP addresses, ports, and protocols. They do not have the capability to inspect or filter traffic based on URLs.\nThe solution to restrict outbound internet traffic based on specific URLs typically involves using a proxy or firewall that can inspect the application layer (Layer 7) of the OSI model, where URL information is available.\nAWS Network Firewall operates at the network and application layers, allowing for more granular control, including the ability to inspect and filter traffic based on domain names or URLs.\nBy configuring domain list rule groups in AWS Network Firewall, you can specify which URLs are allowed for outbound traffic.\nThis option is more aligned with the requirement of allowing access to approved third-party software repositories based on their URLs.","upvote_count":"5","poster":"TheFivePips","comment_id":"1160075"},{"poster":"awsgeek75","upvote_count":"4","content":"Selected Answer: A\nhttps://aws.amazon.com/network-firewall/features/\n\"Web filtering:\nAWS Network Firewall supports inbound and outbound web filtering for unencrypted web traffic. For encrypted web traffic, Server Name Indication (SNI) is used for blocking access to specific sites. SNI is an extension to Transport Layer Security (TLS) that remains unencrypted in the traffic flow and indicates the destination hostname a client is attempting to access over HTTPS. In addition, **AWS Network Firewall can filter fully qualified domain names (FQDN).**\"\nAlways use an AWS product if the advertisement meets the use case.","comment_id":"1112976","timestamp":"1720020840.0"},{"content":"Selected Answer: A\nAWS Network Firewall\n• Protect your entire Amazon VPC\n• From Layer 3 to Layer 7 protection\n• Any direction, you can inspect\nTraffic filtering: Allow, drop, or alert for the traffic that matches the rules, • Active flow inspection to intrusion prevention","poster":"farnamjam","comment_id":"1109430","timestamp":"1719718500.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1718598960.0","content":"D not possible?","poster":"Subhrangsu","comments":[{"poster":"awsgeek75","content":"ALB is for inbound traffic. D is not possible as it is suggesting to direct OUTBOUND traffic.","upvote_count":"4","timestamp":"1720021020.0","comment_id":"1112981"}],"comment_id":"1098701"},{"upvote_count":"2","poster":"Cyberkayu","content":"Selected Answer: A\nAWS network firewall is stateful, providing control and visibility to Layer 3-7 network traffic, thus cover the application too","timestamp":"1718450340.0","comment_id":"1097332"},{"timestamp":"1712810520.0","upvote_count":"2","content":"Selected Answer: A\nJust tried on the console to set up an outbound rule, and URLs cannot be used as a destination. I will opt for A.","poster":"TariqKipkemei","comment_id":"1040202"},{"comment_id":"999536","timestamp":"1709651760.0","upvote_count":"2","comments":[{"content":"Security Groups work with CIDR ranges, not URLs.","poster":"pentium75","comment_id":"1108818","upvote_count":"3","timestamp":"1719666240.0"}],"poster":"Guru4Cloud","content":"Selected Answer: C\nImplement strict inbound security group rules\nConfigure an outbound security group rule to allow traffic only to the approved software repository URLs\nThe key points:\n\nHighly sensitive EC2 instances in private subnet that can access only approved URLs\nOther internet access must be blocked\nSecurity groups act as a firewall at the instance level and can control both inbound and outbound traffic."},{"comment_id":"924865","upvote_count":"4","content":"Isnt private subnet not connectible to internet at all, unless with a NAT gateway?","timestamp":"1702710960.0","poster":"kelvintoys93"},{"content":"Selected Answer: A\nWe can't specifu URL in outbound rule of security group. Create free tier AWS account and test it.","poster":"VeseljkoD","timestamp":"1694177580.0","comment_id":"833075","upvote_count":"2"},{"comment_id":"831762","timestamp":"1694075580.0","content":"Selected Answer: C\nCCCCCCCCCCC","poster":"Leo301","comments":[{"poster":"pentium75","comment_id":"1108819","timestamp":"1719666300.0","content":"Security Groups with IP ranges, not URLs","upvote_count":"2"}],"upvote_count":"1"},{"upvote_count":"3","poster":"Brak","content":"It can't be C. You cannot use URLs in the outbound rules of a security group.","timestamp":"1694065920.0","comment_id":"831645"},{"comments":[],"timestamp":"1693879380.0","poster":"johnmcclane78","comment_id":"829613","content":"Option C is the best solution to meet the requirements of this scenario. Implementing strict inbound security group rules that only allow traffic from approved sources can help secure the VPC network that hosts Amazon EC2 instances. Additionally, configuring an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs will ensure that only approved third-party software repositories can be accessed from the EC2 instances. This solution does not require any additional AWS services and can be implemented using VPC security groups.\n\nOption A is not the best solution as it involves the use of AWS Network Firewall, which may introduce additional operational overhead. While domain list rule groups can be used to block all internet traffic except for the approved third-party software repositories, this solution is more complex than necessary for this scenario.","upvote_count":"2"},{"comments":[{"comment_id":"829867","content":"Is Security Group able to allow a specific URL? According to https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html, I cannot find such description.","poster":"Theodorz","upvote_count":"2","timestamp":"1693906020.0"}],"timestamp":"1693765800.0","poster":"Steve_4542636","comment_id":"828392","upvote_count":"1","content":"Selected Answer: C\nIn the security group, only allow inbound traffic originating from the VPC. Then only allow outbound traffic with a whitelisted IP address. The question asks about blocking EC2 instances, which is best for security groups since those are at the EC2 instance level. A network firewall is at the VPC level, which is not what the question is asking to protect."},{"upvote_count":"3","comment_id":"820068","poster":"KZM","comments":[{"comment_id":"946253","content":"I think C is in private subnet. Even with security group, it could not go public to download the software.","upvote_count":"1","timestamp":"1704704100.0","poster":"Mia2009687"},{"comments":[{"content":"Because you want to filter based on URLs, not IP ranges.","comment_id":"1108823","upvote_count":"2","timestamp":"1719666360.0","poster":"pentium75"},{"comment_id":"827761","poster":"Karlos99","content":"And it is easier to do it at the VPC level","upvote_count":"2","timestamp":"1693721940.0"},{"content":"And it is easier to do it at the level","upvote_count":"2","poster":"Karlos99","comment_id":"827760","timestamp":"1693721880.0"},{"poster":"Karlos99","content":"Because in this case, the session is initialized from inside","timestamp":"1693721580.0","comment_id":"827756","upvote_count":"2"}],"content":"Same here - why is C not a valid option?","upvote_count":"2","comment_id":"825796","poster":"Zohx","timestamp":"1693563180.0"},{"upvote_count":"3","poster":"ruqui","content":"C is not valid. Security groups can allow only traffic from specific ports and/or IPs, you can't use an URL. Correct answer is A","comment_id":"909394","timestamp":"1701268260.0"}],"content":"I am confused that It seems both options A and C are valid solutions.","timestamp":"1692839820.0"},{"comments":[{"upvote_count":"4","poster":"asoli","content":"Although the answer is A, the link you provided here is not related to this question.\nThe information about \"Network Firewall\" and how it can help this issue is here: \nhttps://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata-examples.html#suricata-example-domain-filtering\n\n(thanks to \"@Bhawesh\" to provide the link in their answer)","comment_id":"842390","timestamp":"1694991000.0"}],"timestamp":"1692441960.0","poster":"jennyka76","comment_id":"814074","upvote_count":"6","content":"Answer - A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/ec2-al1-al2-update-yum-without-internet/"},{"timestamp":"1692356580.0","poster":"Neha999","content":"A as other options are controlling inbound traffic","upvote_count":"5","comment_id":"812973"}],"topic":"1","question_text":"A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet traffic must be blocked.\n\nWhich solution meets these requirements?","answer_ET":"A","choices":{"A":"Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.","B":"Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.","D":"Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound traffic to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.","C":"Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs."},"answer_images":[],"isMC":true,"question_images":[],"answer":"A"},{"id":"vqot9vFa6YHAvkgxKdwI","unix_timestamp":1676663220,"url":"https://www.examtopics.com/discussions/amazon/view/99704-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","answer_ET":"D","timestamp":"2023-02-17 20:47:00","discussion":[{"upvote_count":"22","timestamp":"1677875760.0","comments":[{"comment_id":"1356443","content":"they want to process it sucessfully not faster (SQS) is the way...","upvote_count":"1","timestamp":"1739538360.0","poster":"AdamVigas"},{"comment_id":"1064559","poster":"Chef_couincouin","upvote_count":"4","timestamp":"1699339440.0","content":"ensure that all the requests are processed successfully? doesn't mean more quickly"},{"comment_id":"850047","content":"Hell true: I'd rather combine the both options: a SQS + auto-scaled bound to the length of the queue.","upvote_count":"10","poster":"lizzard812","timestamp":"1679741760.0"},{"poster":"joechen2023","comment_id":"925127","upvote_count":"2","timestamp":"1686917280.0","content":"As an architecture, it is not possible to add more backend workers (it is part of the HR and boss's job, not for architecture design the solution). So when the demand surge, the only correct choice is to buffer them using SQS so that workers can take their time to process it successfully"},{"poster":"rushi0611","comment_id":"890548","timestamp":"1683356280.0","upvote_count":"18","content":"\"ensure that all the requests are processed successfully?\"\nwe want to ensure success not the speed, even in the auto-scaling, there is the chance for the failure of the request but not in SQS- if it is failed in sqs it is sent back to the queue again and new consumer will pick the request."},{"content":"eeeee... In the restaurant also here, the request in the line will be dropped in case of high-sudden volume flooding in so it will bring risk of missing request. while with SQS, it can ensure no lost but still slow processing with only 3 processor though so it is based on what is the question's concern. So I still voted for D.","upvote_count":"2","poster":"Marco_St","timestamp":"1702141860.0","comment_id":"1091931"}],"content":"Selected Answer: B\nThe auto-scaling would increase the rate at which sales requests are \"processed\", whereas a SQS will ensure messages don't get lost. If you were at a fast food restaurant with a long line with 3 cash registers, would you want more cash registers or longer ropes to handle longer lines? Same concept here.","poster":"Steve_4542636","comment_id":"828400"},{"poster":"Abhineet9148232","comment_id":"890395","timestamp":"1683334680.0","upvote_count":"17","content":"Selected Answer: D\nB doesn't fit because Auto Scaling alone does not guarantee that all requests will be processed successfully, which the question clearly asks for. \n\nD ensures that all messages are processed."},{"content":"Selected Answer: D\nThere is no scaling based on network traffic. since the backend worker works asynchronously SQS should be right here","comment_id":"1559698","poster":"ChhatwaniB","upvote_count":"1","timestamp":"1744315920.0"},{"upvote_count":"3","comment_id":"1293744","poster":"samadal","content":"The problem states that the application consists of \"static and dynamic front-end content.\" Static content typically includes cacheable resources such as HTML, CSS, and image files. Therefore, from this statement, one can infer that caching static content using CloudFront would improve performance. In other words, the mention of \"static content\" in the problem itself leads to the conclusion that CloudFront should be added for static content.\n\nAdditionally, the problem mentions \"asynchronously processed backend workers.\" Asynchronous processing is well-suited for services like SQS, which can improve efficiency by handling dynamic requests that do not require immediate processing. The mention of \"successfully processing all requests\" also suggests that SQS is needed to ensure that all requests are handled properly.\n\nTherefore, the correct answer is D.","timestamp":"1728193860.0"},{"upvote_count":"2","content":"Selected Answer: B\nImportant question to answer D. Can you connect the website with SQS directly? How do you control access to who can put messages to SQS? I have never seen such a situation it has to be at least behind API gateway. So that conclusion brings me to answer B, application also can process async everything without SQS.","comment_id":"1167011","timestamp":"1709714100.0","poster":"Adinas_"},{"timestamp":"1705587720.0","upvote_count":"4","poster":"awsgeek75","comment_id":"1125951","content":"Selected Answer: D\nI chose D because I love SQS! These questions are hammering SQS in every solution as a \"protagonist\" that saves the day. \nAC are clearly useless\nB can work but D is better because of SQS being better than EC2 scaling. The other part is that backend workers process the request asynchronously therefore a queue is better."},{"poster":"awsgeek75","content":"Selected Answer: D\nA and C don't solve anything so ignore them.\nBetween B and D, D guarantees the scaling via SQS and order processing. B can also do that but it is not guaranteed that EC2 scaling will work to process the order. \nAs usual, I suspect that this \"brain dump\" may be missing critical wording to differentiate between the options so read carefully in the exam.","timestamp":"1704304440.0","comment_id":"1112994","upvote_count":"4"},{"poster":"pentium75","upvote_count":"8","content":"Selected Answer: D\nThere are two components that we need\n* Frontend: Hosted on S3, performance can be increased with CloudFront\n* Backend: There's no reason to process all the orders instantly, so we should decouple the processing from the API which we do with SQS\n\nThus D, CloudFront + SQS","comment_id":"1108828","comments":[{"upvote_count":"3","timestamp":"1703862660.0","content":"And as others said, B might speed up the processing or reduce the number of lost orders, but we need to make sure that \"ALL requests are processed successfully\", NOT that \"less requests are lost\".","comment_id":"1108831","poster":"pentium75"}],"timestamp":"1703862600.0"},{"timestamp":"1702141380.0","comment_id":"1091928","upvote_count":"4","content":"Selected Answer: D\nI picked B before I read D option. Read the question again, it concerns:asynchronous processing of sales requests, Option D seems to align more closely with the requirements. So the requirement is ensuring all requests are processed successfully which means no request would be missed. So D is better option","poster":"Marco_St"},{"upvote_count":"4","timestamp":"1697529360.0","comment_id":"1045681","poster":"wsdasdasdqwdaw","content":"Amazon SQS will make sure that the requests are stored and didn't get lost. After that the workers asynchronously will process the requests. I would go for D"},{"comment_id":"1040204","poster":"TariqKipkemei","timestamp":"1696999800.0","upvote_count":"3","content":"Technically both option B and D would work. But, there's a need to process requests asynchronously, hence decoupling, hence Amazon SQS. I will settle with option D."},{"timestamp":"1693919400.0","poster":"Guru4Cloud","comment_id":"999532","upvote_count":"3","content":"Selected Answer: D\nD is correct."},{"timestamp":"1685106060.0","upvote_count":"2","comment_id":"907373","poster":"antropaws","content":"Selected Answer: D\nD is correct."},{"comment_id":"884948","timestamp":"1682838960.0","poster":"kruasan","upvote_count":"3","content":"Selected Answer: D\nAn SQS queue acts as a buffer between the frontend (website) and backend (API). Web requests can dump messages into the queue at a high throughput, then the queue handles delivering those messages to the API at a controlled rate that it can sustain. This prevents the API from being overwhelmed.","comments":[{"poster":"kruasan","upvote_count":"1","comment_id":"884949","timestamp":"1682838960.0","content":"Options A and B would help by scaling out more instances, however, this may not scale quickly enough and still risks overwhelming the API. Caching parts of the dynamic content (option C) may help but does not provide the buffering mechanism that a queue does."}]},{"comment_id":"880071","timestamp":"1682407200.0","upvote_count":"2","content":"Selected Answer: D\nD make sens","poster":"seifshendy99"},{"timestamp":"1680388680.0","content":"Selected Answer: D\nD makes more sense","comment_id":"858332","upvote_count":"2","poster":"kraken21"},{"upvote_count":"1","comment_id":"858331","content":"There is no clarity on what the asynchronous process is but D makes more sense if we want to process all requests successfully. The way the question is worded it looks like the msgs->SQS>ELB/Ec2. This ensures that the messages are processed but may be delayed as the load increases.","timestamp":"1680388620.0","poster":"kraken21"},{"timestamp":"1680325500.0","content":"Selected Answer: D\nalthough i agree with B for better performance. but i choose 'D' as question request to ensure that all the requests are processed successfully.","poster":"channn","upvote_count":"3","comment_id":"857588"},{"timestamp":"1680264540.0","comment_id":"857031","upvote_count":"2","content":"To ensure that all the requests are processed successfully, I would recommend adding an Amazon CloudFront distribution for the static content and an Amazon CloudFront distribution for the dynamic content. This will help to reduce the load on the API and improve its performance. You can also place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic. This will help to ensure that you have enough capacity to handle the increase in traffic during events for the launch of new products.","poster":"klayytech"},{"timestamp":"1680085440.0","comment_id":"854257","upvote_count":"2","poster":"AravindG","content":"Selected Answer: D\nThe company is expecting a significant and sudden increase in the number of sales requests and keyword async. So I feel option D suits here."},{"comment_id":"850062","timestamp":"1679742840.0","upvote_count":"5","poster":"MssP","content":"Selected Answer: D\nCritical here is \"to ensure that all the requests\". ALL REQUESTS, so it is only possible with a SQS. ASG can spend time to launch new instances so any request can be lost."},{"upvote_count":"3","content":"Selected Answer: D\nI vote for D. \"The company is expecting a significant and sudden increase in the number of sales requests\". Sudden increase means ASG might not be able to deploy more EC2 instances when requests rocket and some of request will get lost.","poster":"andyto","timestamp":"1679576580.0","comment_id":"848227"},{"content":"Selected Answer: D\nThe keyword here about the orders is \"asynchronously\". Orders are supposed to process asynchronously. So, it can be published in an SQS and processed after that. Also, it ensures in a spike, there is no lost order.\n\nIn contrast, if you think the answer is B, the issue is the sudden spike. Maybe the auto-scaling is not acting fast enough and some orders are lost. So, B i snot correct.","poster":"asoli","comment_id":"842394","timestamp":"1679101080.0","upvote_count":"4"},{"poster":"harirkmusa","content":"Selected D","comment_id":"831137","timestamp":"1678127520.0","upvote_count":"2"},{"upvote_count":"2","comment_id":"830611","poster":"[Removed]","content":"Selected Answer: D\nanwer d","timestamp":"1678087740.0"},{"poster":"KZM","timestamp":"1677212460.0","content":"I think D.\nIt may be SQS as per the points, \n>workers process sales requests asynchronously and \n?the requests are processed successfully,","comment_id":"820099","upvote_count":"4"},{"upvote_count":"3","timestamp":"1677122280.0","comments":[{"content":"D maybe inappropriate for this scenario because by adding an Amazon CloudFront distribution for the static content and adding an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances, is not the best option as it adds unnecessary complexity to the system. It would be better to add an Auto Scaling group to handle the increased traffic.","timestamp":"1677122400.0","comments":[{"content":"SQS also doesn't ensure real-time processing since the EC2s would be the bottleneck.","poster":"Steve_4542636","timestamp":"1677875640.0","comment_id":"828398","upvote_count":"1","comments":[{"poster":"MssP","timestamp":"1679758620.0","comment_id":"850251","upvote_count":"2","content":"Where you see real-time processing?? Here the question is ensure to process ALL requests, not real-time."}]},{"content":"No, because you must ensure the requests are processed successfully. If there is a sudden spike in usage some messages might be missed whereas with SQS the messages must be processed before being removed from the queue. Answer D is correct","upvote_count":"1","poster":"nder","comment_id":"821420","timestamp":"1677324300.0"}],"poster":"LuckyAro","comment_id":"818733","upvote_count":"1"}],"poster":"LuckyAro","content":"Selected Answer: B\nBased on the provided information, the best option is B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.\n\nThis option addresses the need for scaling the infrastructure to handle the increase in traffic by adding an Auto Scaling group to the existing EC2 instances, which allows for automatic scaling based on network traffic. Additionally, adding an Amazon CloudFront distribution for the static content will improve the performance of the website by caching content closer to the end-users.","comment_id":"818732"},{"poster":"Neha999","upvote_count":"3","timestamp":"1676725020.0","comment_id":"812964","content":"D\nhttps://www.examtopics.com/discussions/amazon/view/67936-exam-aws-certified-solutions-architect-associate-saa-c02/"},{"timestamp":"1676663220.0","upvote_count":"5","comment_id":"812296","content":"Selected Answer: D\nStatic content can include images and style sheets that are the same across all users and are best cached at the edges of the content distribution network (CDN). Dynamic content includes information that changes frequently or is personalized based on user preferences, behavior, location or other factors - all content is sales requests","poster":"bdp123"}],"answer_images":[],"question_text":"A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.\n\nThe company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products.\n\nWhat should a solutions architect recommend to ensure that all the requests are processed successfully?","answer":"D","exam_id":31,"question_images":[],"question_id":275,"answers_community":["D (73%)","B (27%)"],"isMC":true,"choices":{"C":"Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.","B":"Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.","A":"Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.","D":"Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances."},"topic":"1"}],"exam":{"lastUpdated":"11 Apr 2025","id":31,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false,"isMCOnly":true,"provider":"Amazon","isImplemented":true},"currentPage":55},"__N_SSP":true}