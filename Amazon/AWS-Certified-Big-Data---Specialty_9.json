{"pageProps":{"questions":[{"id":"7WyHceWmv7sHrq8sL4Fv","question_text":"An enterprise customer is migrating to Redshift and is considering using dense storage nodes in its Redshift cluster. The customer wants to migrate 50 TB of data. The customers query patterns involve performing many joins with thousands of rows.\nThe customer needs to know how many nodes are needed in its target Redshift cluster. The customer has a limited budget and needs to avoid performing tests unless absolutely needed.\nWhich approach should this customer use?","exam_id":17,"discussion":[{"upvote_count":"17","comment_id":"9362","content":"A is correct.\nhttps://d1.awsstatic.com/whitepapers/Size-Cloud-Data-Warehouse-on-AWS.pdf\nUsing compression ratio of 3 as per the link. The 50TB/3= 16TB. \nThe calculation 50TB/3=16.66 * (1.25) =20.83 ~21TB. 21TB/2 =10.5 ~11 ds2.xlarge nodes\nThe calculation 50TB/3=16.66 * (1.25) =20.83 ~21TB. 21TB/16 =1.3125 ~2 ds2.8xlarge nodes.","timestamp":"1632179220.0","poster":"mattyb123"},{"timestamp":"1632447600.0","comment_id":"78114","poster":"Bulti","content":"With the required storage size you can either go with RA2 or DS2 node types and not with DC2. If we go with DS2.xlarge type nodes, we will need 11 of them to hold 50 TB of data giving us a total of 44 CPU cores for compute. if we go with DS2.8xlarge type nodes, we will need 2 of them giving us a total of 72 CPU ores for compute. Starting with many small nodes will help distribute the data and query processing across 11 nodes with a sizable compute power considering there are many joins in the query and will cost way less than using 2 Ds2.8xlarge nodes. It will be about $2500 /month cheaper to operate 11 xlarge nodes than operating 2 8xlarge nodes.","upvote_count":"9"},{"timestamp":"1635245880.0","content":"B. best practice is fewer large nodes. Reason is changing Node types for scale in/out will result in classic/manual resize","upvote_count":"1","poster":"Royk2020","comment_id":"150172"},{"timestamp":"1634929080.0","poster":"MichRox","comments":[{"content":"\"many joins with thousands of rows\", joins are on the small tables, you can dist all those tables to avoid traffic between nodes","poster":"jove","timestamp":"1635825720.0","comment_id":"210147","upvote_count":"1"}],"comment_id":"133856","upvote_count":"1","content":"is the paragraph about \"many large joins\" a confounder? Because one best practice that's often mentioned is to avoid traffic between nodes as much as possible (often mentioned with resources about distribution styles). That would point to B (few large nodes)."},{"comment_id":"111288","upvote_count":"2","timestamp":"1633503060.0","poster":"freedomeox","content":"I think it should be B.\nif they are performing many joins, many data will be passing back and forth between nodes during the query. For better performance, it's better to use larger nodes. The question states they don't have budget for testing, but not saying they cann't afford larger nodes..."},{"timestamp":"1632403020.0","content":"my selection A","poster":"san2020","upvote_count":"3","comment_id":"52365"}],"choices":{"A":"Start with many small nodes.","B":"Start with fewer large nodes.","C":"Have two separate clusters with a mix of a small and large nodes.","D":"Insist on performing multiple tests to determine the optimal configuration."},"timestamp":"2019-09-02 23:23:00","answers_community":[],"question_id":41,"answer_ET":"A","unix_timestamp":1567459380,"answer":"A","question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/4587-exam-aws-certified-big-data-specialty-topic-1-question-46/","answer_images":[],"answer_description":"","isMC":true},{"id":"SnhfTjGkeDDydZruKqW2","exam_id":17,"discussion":[{"timestamp":"1633414260.0","poster":"exams","comment_id":"11549","content":"A. https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html","upvote_count":"13"},{"timestamp":"1633872480.0","comment_id":"52366","poster":"san2020","upvote_count":"5","content":"my selection A"},{"content":"A:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/data-integrity-s3/","upvote_count":"1","comment_id":"337454","timestamp":"1636010820.0","poster":"DerekKey"}],"question_text":"A company is centralizing a large number of unencrypted small files from multiple Amazon S3 buckets. The company needs to verify that the files contain the same data after centralization.\nWhich method meets the requirements?","choices":{"C":"Place a HEAD request against the source and destination objects comparing SIG v4.","D":"Compare the size of the source and destination objects.","A":"Compare the S3 Etags from the source and destination objects.","B":"Call the S3 CompareObjects API for the source and destination objects."},"timestamp":"2019-09-18 07:19:00","question_id":42,"answers_community":[],"answer_ET":"A","question_images":[],"unix_timestamp":1568783940,"answer":"A","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/5352-exam-aws-certified-big-data-specialty-topic-1-question-47/","isMC":true,"answer_images":[],"answer_description":""},{"id":"FfYDR4tfvm8Rgc9rQaeM","exam_id":17,"question_text":"An online gaming company uses DynamoDB to store user activity logs and is experiencing throttled writes on the companys DynamoDB table. The company is NOT consuming close to the provisioned capacity. The table contains a large number of items and is partitioned on user and sorted by date. The table is 200GB and is currently provisioned at 10K WCU and 20K RCU.\nWhich two additional pieces of information are required to determine the cause of the throttling? (Choose two.)","discussion":[{"timestamp":"1634851920.0","comment_id":"105677","comments":[{"timestamp":"1636025280.0","content":"Spot on, this is exactly my reasoning as well, 100% agree with this.","poster":"vicks316","upvote_count":"1","comment_id":"183395"}],"poster":"freedomeox","content":"AC in my opinion. The key of this question is to figure out the cause of throttling under capacity. I take the \"capacity\" as the wcu and rcu you defined when creating a new dynamodb table. the capacity is the TOTAL wcu and rcu in ALL partitions together. the max wcu per partition is 1000, the max size per partition is 10G. However, we have 200G in total and the total wcu requested is 10k. This means, dynamoDB has to give us 20 partitions, with 500 wcu for each partition. therefore, a throttling will occur if for a single partiton the wcu goes over 500. option c gives us information of the most frequently updated attributes and the size, thus can help us calculate whether a wcu for a single partition is over 500. On the other hand, since GSI uses its own capacity, so even the traffic is under \"general capacity\", it might exceed the GSI capacity, thus we need to know if there is any GSI, and the capacity corresponding to it.","upvote_count":"9"},{"upvote_count":"1","content":"A & B are the right answers","timestamp":"1636300980.0","comment_id":"248683","poster":"hoty"},{"content":"Hot partition issue, A & D, GSI has partition key & LSI will have the table partition key as it leftmost key","timestamp":"1635319200.0","comment_id":"150173","upvote_count":"1","poster":"Royk2020"},{"upvote_count":"2","timestamp":"1635268020.0","comment_id":"137430","content":"A and C in my opinion:\nA - GSI definitely impacts base table writes if GSI WCU is not provisioned correct. \nC - for understanding if the columns in GSI get updated frequently and compute required WCUs. best practice is to set the WCU same as base table.\n\nThrottling on a GSI affects the base table in different ways, depending on whether the throttling is for read or write activity:\n\nWhen a GSI has insufficient read capacity, the base table isn't affected.\nWhen a GSI has insufficient write capacity, write operations won't succeed on the base table or any of its GSIs.","poster":"mmk1"},{"upvote_count":"2","comment_id":"122438","poster":"kein22190","content":"My guess is A and C. After reading all comments.","timestamp":"1634979240.0","comments":[{"poster":"kein22190","comment_id":"122444","content":"Changed my mind to A and E. \n\nPrevious comments:\nA is right because:\n' If your table uses a global secondary index, then any write to the table also writes to the index. If the many writes are occuring on a single partition key for the index, regardless of how well the table partition key is distributed, the write to the table will be throttled too.'\nhttps://www.bluematador.com/docs/troubleshooting/aws-dynamo-throttling\n\nE: Consider the following scenario:\n\nyou dial up the throughput for a table because thereâ€™s a sudden spike in traffic or you need the extra throughput to run an expensive scan\nthe extra throughputs cause DynamoDB to increase the no. of partitions\nyou dial down the throughput to previous levels, but now you notice that some requests are throttled even when you have not exceeded the provisioned throughput on the table\nThis happens because there are less read and write throughput units per partition than before due to the increased no. of partitions.","timestamp":"1635121920.0","upvote_count":"1"}]},{"content":"B and D. This is likely to be a hot partition issue, need to find out how to confirm it is due to hot partition.\nA. GSI has its own RCU and WCU, will NOT impact the table's RCU and WCU.\nC. Average item couldn't tell whether hit hot partition or not.\nE. Historical doesn't mean you won't have hot partition now.","upvote_count":"2","poster":"guruguru","comment_id":"117376","timestamp":"1634922600.0"},{"upvote_count":"1","content":"Excessive throttling is caused by:\n\nHot partitions: throttles are caused by a few partitions in the table that receive more requests than the average partition\nNot enough capacity: throttles are caused by the table itself not having enough capacity to service requests on many partitions\n the question ask we to forcus on finding hot partitions . \n\nA: GSI has different partition key . \nD: update rates for the key","comments":[{"comment_id":"105110","timestamp":"1634668620.0","poster":"kkyong","content":"GSI has different capcity","upvote_count":"1"},{"content":"Update rates is C, not D. I agree with AC","timestamp":"1635946560.0","comment_id":"183394","upvote_count":"1","poster":"vicks316"}],"timestamp":"1634258760.0","comment_id":"105108","poster":"kkyong"},{"poster":"certish","content":"I think A is incorrect. GSI will have its own capacity units for both Read & Write. Also, the question is to determine the root cause for WRITE throttling and NOT about under-used provisioned capacity. Which means, E is incorrect as well. In my view, it has to be between B, C & D. I will go with B & C.","comment_id":"101349","timestamp":"1634256240.0","comments":[{"upvote_count":"1","poster":"vicks316","comment_id":"183393","timestamp":"1635560100.0","content":"Exactly, because GSI has its own WCU, it could be throttled, don't you agree?"}],"upvote_count":"1"},{"content":"It is given that the company is not consuming close to the provisioned capacity. Therefore, option B and E is not the choice. This requires to know the assess pattern of table, which can determine hot partition issue. Being said that, option C gives clue about assess pattern and option A gives clue if GSI is there and how it is partitioned. D can give some information but it is more aligned with table partition which we know is user with sorted by date.","timestamp":"1633580520.0","comments":[{"timestamp":"1635481920.0","comment_id":"183392","poster":"vicks316","content":"So are you agreeing with A&C ? :)","upvote_count":"1"}],"comment_id":"92843","upvote_count":"1","poster":"ub19"},{"comments":[{"comment_id":"101348","content":"@Bulit - I like most of your explanation. However for this one, I just thought of sharing my inputs. \nI think A is incorrect. GSI will have its own capacity units for both Read & Write. Also, the question is to determine the root cause for WRITE throttling and NOT about under-used provisioned capacity. Which means, E is incorrect as well. In my view, it has to be between B, C & D. I will go with B & C.","comments":[{"timestamp":"1635325560.0","upvote_count":"1","comment_id":"183391","content":"A is correct because GSI uses its own WCU/RCU independent of the table's WCU/RCU which can be throttled since the question says that the table's provisioned throughput is not execeeded (at least it is what I understood from it). C is my 2nd option.","poster":"vicks316"}],"upvote_count":"1","poster":"certish","timestamp":"1634252520.0"}],"comment_id":"75454","upvote_count":"4","poster":"Bulti","content":"Answer is A and E- For A read this link- > https://www.bluematador.com/docs/troubleshooting/aws-dynamo-throttling and for E -> https://theburningmonk.com/2017/05/beware-of-dilution-of-dynamodb-throughput-due-to-excessive-scaling/","timestamp":"1633183740.0"},{"comments":[{"poster":"piemar","upvote_count":"1","content":"why would you go for D?","timestamp":"1633147980.0","comment_id":"62482"}],"comment_id":"58065","timestamp":"1632875400.0","poster":"aewis","content":"I would go for B and D","upvote_count":"3"},{"poster":"san2020","comment_id":"52367","content":"my selection AD","timestamp":"1632776880.0","upvote_count":"2"},{"comment_id":"49818","content":"A is right because: \n' If your table uses a global secondary index, then any write to the table also writes to the index. If the many writes are occuring on a single partition key for the index, regardless of how well the table partition key is distributed, the write to the table will be throttled too.'\nhttps://www.bluematador.com/docs/troubleshooting/aws-dynamo-throttling","upvote_count":"4","timestamp":"1632776280.0","poster":"zhengtoronto"},{"timestamp":"1632554820.0","poster":"ME2000","content":"Answer D and E\nWhy option E ? find the explanation\nDynamoDB provisioned capacity up and down induces less read and write throughput units per partition than before due to the increased no. of partitions after scale down.\nhttps://theburningmonk.com/2017/05/beware-of-dilution-of-dynamodb-throughput-due-to-excessive-scaling/","upvote_count":"2","comment_id":"38100"},{"comment_id":"36143","timestamp":"1632551820.0","upvote_count":"1","poster":"PK1234","content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/monitoring-cloudwatch.html\n\nYou do need cloudwatch logs...."},{"upvote_count":"1","poster":"am7","comments":[{"comment_id":"101001","poster":"Corram","timestamp":"1633633020.0","content":"Application level metrics are not useful here since we care about summed reads and writes, hence C is wrong.\nLSI is only about sort keys not partition keys, therefore D is wrong.","upvote_count":"1"}],"comment_id":"34153","timestamp":"1632328920.0","content":"It should be C and D. C because if we get information if a specific attribute only is getting updated which is causing throttling and D because of hot key partition."},{"timestamp":"1632317700.0","content":"It should be caused by Hot Key (Partition), D should be one of the answers","comment_id":"18456","upvote_count":"1","poster":"WWODIN"},{"comments":[{"upvote_count":"1","poster":"Corram","comment_id":"101003","timestamp":"1633812960.0","content":"LSI is only about sort keys not partition keys, therefore D is wrong. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html"}],"content":"I think AD.","upvote_count":"2","poster":"exams","comment_id":"11550","timestamp":"1632244560.0"},{"comments":[{"poster":"Corram","timestamp":"1633858620.0","upvote_count":"2","comment_id":"101006","content":"The exercise spefically states \"The company is NOT consuming close to the provisioned capacity\" therefore B cannot be the solution."}],"poster":"Hitu","comment_id":"11220","timestamp":"1632170700.0","upvote_count":"1","content":"I think its B and C\n\nThrottlingException\nMessage: Rate of requests exceeds the allowed throughput.\nThis exception might be returned if you perform any of the following operations too rapidly: CreateTable, UpdateTable, DeleteTable.\n\nReference - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html"}],"choices":{"C":"Application-level metrics showing the average item size and peak update rates for each attribute","B":"CloudWatch data showing consumed and provisioned write capacity when writes are being throttled","A":"The structure of any GSIs that have been defined on the table","E":"The maximum historical WCU and RCU for the table","D":"The structure of any LSIs that have been defined on the table"},"timestamp":"2019-09-16 02:59:00","answers_community":[],"question_id":43,"answer_ET":"AD","question_images":[],"answer":"AD","unix_timestamp":1568595540,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/5216-exam-aws-certified-big-data-specialty-topic-1-question-48/","answer_description":"","answer_images":[],"isMC":true},{"id":"hTgLWBJsLdYsUza1JiDs","url":"https://www.examtopics.com/discussions/amazon/view/3564-exam-aws-certified-big-data-specialty-topic-1-question-49/","question_images":[],"isMC":true,"question_id":44,"exam_id":17,"answer_description":"","answer_images":[],"choices":{"B":"Use the Amazon Redshift COPY command to move the data from Amazon S3 into Redshift and perform a SQL query that outputs the most popular bicycle stations.","C":"Persist the data on Amazon S3 and use a transient EMR cluster with spot instances to run a Spark streaming job that will move the data into Amazon Kinesis.","A":"Move the data from Amazon S3 into Amazon EBS-backed volumes and use an EC-2 based Hadoop cluster with spot instances to run a Spark job that performs a stochastic gradient descent optimization.","D":"Keep the data on Amazon S3 and use an Amazon EMR-based Hadoop cluster with spot instances to run a Spark job that performs a stochastic gradient descent optimization over EMRFS."},"answers_community":[],"discussion":[{"content":"my selection D","timestamp":"1634063040.0","upvote_count":"6","poster":"san2020","comment_id":"52368"},{"upvote_count":"5","content":"Answer : D because you would want to use the EMR cluster with EMRFS as opposed to Ec2 based Hadoop cluster using HDFS to run the Spark Job without moving the data from S3","comment_id":"75464","timestamp":"1635745920.0","poster":"Bulti"},{"comments":[{"timestamp":"1633948920.0","poster":"d00ku","comments":[{"comment_id":"253759","timestamp":"1635853140.0","poster":"MihirB","content":"I am confused between B and D, as I am still not completely convinced for a use case of employing ML as stated in option D.\nThe question states that the information of \"Number of slots available and taken at a given time\" is present, which indicates how busy/popular the station is. As more the number of taken slots --> more popular the station --> which becomes the ideal location for increasing the Bicycle station. \nAny thoughts ?","upvote_count":"1"}],"comment_id":"19893","upvote_count":"11","content":"How would you identify new spots for bicycle stations from the current data? You only have current stations, their usage and the trips of the people that use the bikes. You need ML here to identify ideal locations for new stations. Answer D should be correct."}],"timestamp":"1633661760.0","comment_id":"19321","poster":"cybe001","upvote_count":"2","content":"I go with B, gradient descent is an optimization technique used in many ML algorithms. The answer didn't tell what ML algorithm is used. Moreover the problem doesn't require machine learning predictive model to solve it. They've data and need to be analyzed to find the answer which can be done using SQL queries using Redshift."},{"comment_id":"11551","upvote_count":"2","timestamp":"1633506240.0","poster":"exams","content":"I'll go with D"},{"comments":[{"comments":[{"upvote_count":"1","content":"Any further ideas anyone?","timestamp":"1632669720.0","comments":[{"comment_id":"7954","timestamp":"1632721200.0","poster":"jlpl","upvote_count":"1","content":"tricky, i chose B and failed, so D is making sense for now?","comments":[{"poster":"mattyb123","timestamp":"1632770340.0","upvote_count":"1","comment_id":"8311","content":"That's my logic as well","comments":[{"content":"I think , for B , the query result is the existed bicycle stations who are most popular. But this question is to add more bicycle stations which is no way to build the same place . So it is a prediction problem , D is the right answer,","comment_id":"9689","timestamp":"1632830040.0","upvote_count":"6","poster":"Jialu"}]}]},{"poster":"axlrose","upvote_count":"4","content":"This question is not about can \"Redshift SQL query on the current information\" v/s \"ML stochastic gradient descent algorithm\". \nData is in S3. What is the easiest way tand most cost effecctive way to analyze data one time? Use EMR spot instance (cheap) Use EMRFS to read data directly from S3. No additional charges for spinning up a redshift cluster","comment_id":"66154","timestamp":"1635043500.0"}],"comment_id":"7859","poster":"mattyb123"}],"content":"Just thinking can a Redshift SQL query on the current information even outputs the most popular bicycle stations? Compared to the ML stochastic gradient descent algorithm?","comment_id":"7035","upvote_count":"2","poster":"mattyb123","timestamp":"1632308580.0"}],"comment_id":"6771","upvote_count":"3","poster":"mattyb123","timestamp":"1632074400.0","content":"thoughts on D?"}],"answer":"B","unix_timestamp":1565740140,"question_text":"A city has been collecting data on its public bicycle share program for the past three years. The 5PB dataset currently resides on Amazon S3. The data contains the following datapoints:\nâœ‘ Bicycle origination points\nâœ‘ Bicycle destination points\nâœ‘ Mileage between the points\nâœ‘ Number of bicycle slots available at the station (which is variable based on the station location)\nâœ‘ Number of slots available and taken at a given time\nThe program has received additional funds to increase the number of bicycle stations available. All data is regularly archived to Amazon Glacier.\nThe new bicycle stations must be located to provide the most riders access to bicycles.\nHow should this task be performed?","topic":"1","timestamp":"2019-08-14 01:49:00","answer_ET":"B"},{"id":"gDsVRPU011wZmuhnJXkP","unix_timestamp":1565659620,"question_text":"Company A operates in Country X. Company A maintains a large dataset of historical purchase orders that contains personal data of their customers in the form of full names and telephone numbers. The dataset consists of 5 text files, 1TB each. Currently the dataset resides on-premises due to legal requirements of storing personal data in-country. The research and development department needs to run a clustering algorithm on the dataset and wants to use Elastic Map Reduce service in the closest AWS region. Due to geographic distance, the minimum latency between the on-premises system and the closet AWS region is 200 ms.\nWhich option allows Company A to do clustering in the AWS Cloud and meet the legal requirement of maintaining personal data in-country?","url":"https://www.examtopics.com/discussions/amazon/view/3512-exam-aws-certified-big-data-specialty-topic-1-question-5/","isMC":true,"topic":"1","answer_description":"","answer_images":[],"timestamp":"2019-08-13 03:27:00","choices":{"D":"Use AWS Import/Export Snowball device to securely transfer the data to the AWS region and copy the files onto an EBS volume. Have the EMR cluster read the dataset using EMRFS.","B":"Establish a Direct Connect link between the on-premises system and the AWS region to reduce latency. Have the EMR cluster read the data directly from the on-premises storage system over Direct Connect.","A":"Anonymize the personal data portions of the dataset and transfer the data files into Amazon S3 in the AWS region. Have the EMR cluster read the dataset using EMRFS.","C":"Encrypt the data files according to encryption standards of Country X and store them on AWS region in Amazon S3. Have the EMR cluster read the dataset using EMRFS."},"exam_id":17,"discussion":[{"poster":"mattyb123","upvote_count":"7","content":"A. EMR cant ingest data from on on premise and also there would be alot of latency.","comments":[{"content":"I think A is not an option. When the data is being anonymous, you can not categorize the users because there is no user info at the dataset.","upvote_count":"2","timestamp":"1632156780.0","comment_id":"8554","comments":[{"comment_id":"8634","timestamp":"1632160320.0","upvote_count":"6","comments":[{"timestamp":"1632166260.0","comment_id":"11395","content":"I go with A","upvote_count":"2","poster":"exams"}],"content":"What about with the key statement being \"legal requirements of storing personal data in-country.\" If the AWS region is out of country (which is implied by the distance but not directly stated), then isnt A is the answer. All other options put customer data in AWS (either through encryption or copying), which does not meet the legal requirement. \n\nData anonymization before analysis is the demand for a number of compliance standards like HIPAA, GDPR etc. In the question it's underlined - \"meet the legal requirement of maintaining personal data in-country\". Data should be anonymized , transferred to S3. After use EMR with EMRFS to directly process data from S3.","poster":"mattyb123"},{"timestamp":"1632585060.0","poster":"cybe001","upvote_count":"1","content":"I also choose A. When you anonymous data, you can still main the actual-anonymous map onpremis, run ML in aws and map it back to onpremis actual.","comments":[{"upvote_count":"2","comment_id":"22238","content":"You can anonymous data and still run cluster algorithm.","poster":"cybe001","timestamp":"1633168140.0"}],"comment_id":"19173"},{"content":"you do not need personal information like name and telephone numbers for clustering... I would go with option A","upvote_count":"3","timestamp":"1634815380.0","poster":"sck","comment_id":"47191"}],"poster":"muhsin"},{"timestamp":"1632846900.0","comment_id":"19634","content":"Amazon EMR provides several ways to get data onto a cluster. If you have large amounts of on-premises data to process, you may find the AWS Direct Connect service useful\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-get-data-in.html","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"38818","content":"Exactly, Option B is the correct answer.","poster":"ME2000","timestamp":"1634650200.0"}],"poster":"BigEv"}],"timestamp":"1632153060.0","comment_id":"6638"},{"poster":"jay1ram2","comment_id":"37898","upvote_count":"6","comments":[{"timestamp":"1636213800.0","upvote_count":"1","comment_id":"209642","content":"agree with option A","poster":"vanireddy"}],"timestamp":"1634599560.0","content":"Option A is the right answer: I have worked with PII and PHI data in US. Anonymization/de-identification/tokenization of personalized data is how most of the companies deal with moving and operating on sensitive data without violating laws and regulations around privacy. Once anonymized, it is does not matter where the data lives. \n\nOption B is not right because 1) The problem statement does not clearly say whether the nearest AWS region is within country X. Even if, EMR can only operate on files that are either in S3 or HDFS (instance store/EBS). It is not possible to work with remote data directly with EMR (SAN/FTP/etc). Option A will work much better and cost effective (direct connect still costs you money to move data. However S3 to EMR is free)"},{"content":"B is clearly wrong. EMR can't use on premise data directly using connect direct.","comment_id":"95269","timestamp":"1635091920.0","poster":"Debi_mishra","upvote_count":"1","comments":[{"comment_id":"106323","upvote_count":"3","poster":"tubadc","content":"You can, https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-get-data-in.html, and another thing, people are confusing the fact that the data CAN'T LEAVE the onpremise, so doest not matter if it's obfuscated, anonymized, encrypted, whatever... it CAN'T, so the only option is to build an interconnection and extend your network.","timestamp":"1635216180.0"},{"comment_id":"147437","timestamp":"1635344700.0","poster":"abhineet","content":"read the links ppl have posted emr can be used with on prem","upvote_count":"1"}]},{"comment_id":"93237","upvote_count":"1","content":"IMO it should be A. Main emphasize is on maintaining legal requirements for handling PII data. So option A is most accurate.","poster":"Josh1981","timestamp":"1635090840.0"},{"timestamp":"1635074160.0","comment_id":"74391","upvote_count":"1","poster":"YashBindlish","content":"Answer B - Beginning with Amazon EMR version 5.28.0, you can create and run EMR clusters on AWS Outposts. AWS Outposts enables native AWS services, infrastructure, and operating models in on-premises facilities https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-outposts.html"},{"upvote_count":"2","content":"Option B.\nIf you have large amounts of on-premises data to process, you may find the AWS Direct Connect service useful. (Read Paragraph 1)\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-get-data-in.html","poster":"IDvarma","comment_id":"65437","timestamp":"1634946960.0"},{"timestamp":"1634915880.0","upvote_count":"5","comment_id":"60175","poster":"drneon","content":"Absolutely B,\nScenario 2. Use AWS Direct Connect to connect your data center with AWS resources. Once connected, you can use Amazon EMR to process your data stored in your own data center and store the results on AWS or back in your data center. This approach gives you 1 or 10 gigabit-per-second link connectivity to AWS at all time. And directconnect outbound bandwidth costs less than public Internet outbound cost. So in cases where you expect great amount of traffic exported to your own data center, having direct connect in place can reduce your bandwidth charges.\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf"},{"content":"Selected A","poster":"san2020","timestamp":"1634837220.0","comment_id":"52144","upvote_count":"2"},{"poster":"kalpanareddy","upvote_count":"1","comment_id":"41803","timestamp":"1634676840.0","content":"answer B.\nhttps://aws.amazon.com/getting-started/projects/connect-data-center-to-aws/"},{"content":"i will go with B.\nhe most common way is to upload the data to Amazon S3 and use the built-in features of Amazon EMR to load the data onto your cluster. You can also use the Distributed Cache feature of Hadoop to transfer files from a distributed file system to the local file system. The implementation of Hive provided by Amazon EMR (Hive version 0.7.1.1 and later) includes functionality that you can use to import and export data between DynamoDB and an Amazon EMR cluster. If you have large amounts of on-premises data to process, you may find the AWS Direct Connect service useful.\n https://aws.amazon.com/emr/features/outposts/","upvote_count":"1","poster":"kalpanareddy","timestamp":"1634656800.0","comment_id":"41792"},{"content":"Correct answer is A as the latency is high it would be ideal to transfer the data to AWS and process using EMR and EMRFS. Also, anonymizing the data would help meet the legal requirement.","comment_id":"34998","upvote_count":"1","timestamp":"1634221380.0","poster":"hka"},{"upvote_count":"4","content":"Ladies and gentlemen I have found the answer I believe and it is B. Since we all know a direct connection will reduce latency the next question is if EMR can handle data directly from on prem and here is a quote that will clear this up.\n\nAWS Outposts bring AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility. Amazon EMR is available on AWS Outposts, allowing you to set up, deploy, manage, and scale Apache Hadoop, Apache Hive, Apache Spark, and Presto clusters in your on-premises environments, just as you would in the cloud\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-input-directconnect.html\nhttps://aws.amazon.com/emr/features/outposts/","comment_id":"34090","timestamp":"1633896780.0","poster":"I_heart_shuffle_girls","comments":[{"comment_id":"150142","timestamp":"1636175220.0","poster":"Abhi09","content":"AWS Outposts is different from DirectConnect. It does not mention anything about Outposts in the question","upvote_count":"1"}]},{"timestamp":"1633171980.0","comment_id":"30283","upvote_count":"2","comments":[{"comment_id":"34089","upvote_count":"3","poster":"I_heart_shuffle_girls","content":"I must disagree with you, B would not be an absolute violation of data-in-country laws. The United States has multiple regions inside of it. A direct connection from a business located in the U.S. to a region in the U.S. would absolutely be allowed. Processing the data in and EMR cluster would also not be a violation as you can configure your settings to ensure the data remains within the same region. B is the answer.","timestamp":"1633851120.0"}],"content":"I work with data redisency use cases (China Cybersecurity Law, EUs GDPR etc.) - option B would be absolute violation of data-in-country requirements - of course reding data from EMR cluster would move data personal data our of country even for temporary processing! I don't know who chose the answer, event if it's from AWS it's just not right.","poster":"yuriy_ber"},{"comments":[{"upvote_count":"2","poster":"BigEv","content":"Agree, i will go with B","comment_id":"19635","timestamp":"1632924540.0"},{"poster":"hailiang","timestamp":"1634219040.0","upvote_count":"2","comment_id":"34415","content":"emr cannot \"read data from direct connect\""}],"content":"Answer B looks reasonable.\n\nJustification points :\na) Options A,C and E talk about moving data out of the premises. The mentioned dataset resides on-premises due to legal requirements of storing personal data in-country.\nb) Given that the minimum latency between the on-premises system and the closet AWS region is 200 ms. Time that would be taken to move 5 Text Files of 1TB should also be considered.","comment_id":"19481","poster":"viduvivek","upvote_count":"3","timestamp":"1632818760.0"},{"timestamp":"1632383760.0","comment_id":"14954","poster":"M2","upvote_count":"1","content":"these question is related to data security. A looks more accurate bcoz in options B it says direct connection to reduce latency, which will not be the case as it is reading from onpremise it will be slow and does not address the confidentiality issue of maintaining personal data in country"},{"upvote_count":"3","comment_id":"14282","content":"looks B but not sure. anyone can explain further ?","poster":"bigdatalearner","timestamp":"1632332460.0"},{"poster":"061","upvote_count":"2","timestamp":"1632288840.0","comment_id":"13142","content":"Why not B?\nData can be transfer on direct connect\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-input-directconnect.html"}],"question_images":[],"answers_community":[],"answer_ET":"B","question_id":45,"answer":"B"}],"exam":{"isMCOnly":true,"provider":"Amazon","id":17,"name":"AWS Certified Big Data - Specialty","numberOfQuestions":85,"isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true},"currentPage":9},"__N_SSP":true}