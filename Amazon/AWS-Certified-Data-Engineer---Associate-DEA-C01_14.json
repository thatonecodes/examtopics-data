{"pageProps":{"questions":[{"id":"Z0A01jDf4r42IOn8UQMF","answer_description":"","answer_ET":"B","question_id":66,"isMC":true,"question_text":"A company has a data lake in Amazon S3. The company uses AWS Glue to catalog data and AWS Glue Studio to implement data extract, transform, and load (ETL) pipelines.\n\nThe company needs to ensure that data quality issues are checked every time the pipelines run. A data engineer must enhance the existing pipelines to evaluate data quality rules based on predefined thresholds.\n\nWhich solution will meet these requirements with the LEAST implementation effort?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151925-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"B","question_images":[],"choices":{"D":"Add a new custom transform to each Glue ETL job. Use the Great Expectations library to implement a ruleset that includes the data quality rules that need to be evaluated.","B":"Add a new Evaluate Data Quality transform to each Glue ETL job. Use Data Quality Definition Language (DQDL) to implement a ruleset that includes the data quality rules that need to be evaluated.","A":"Add a new transform that is defined by a SQL query to each Glue ETL job. Use the SQL query to implement a ruleset that includes the data quality rules that need to be evaluated.","C":"Add a new custom transform to each Glue ETL job. Use the PyDeequ library to implement a ruleset that includes the data quality rules that need to be evaluated."},"exam_id":21,"topic":"1","timestamp":"2024-11-25 01:56:00","answers_community":["B (100%)"],"unix_timestamp":1732496160,"discussion":[{"timestamp":"1736963100.0","poster":"MerryLew","comment_id":"1341164","upvote_count":"1","content":"Selected Answer: B\nAWS Glue Data Quality works with Data Quality Definition Language (DQDL) to define data quality rules."},{"timestamp":"1732496160.0","upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/tutorial-data-quality.html","poster":"emupsx1","comment_id":"1317263"}]},{"id":"35clTfbd9Ssjg5a5kpxI","url":"https://www.examtopics.com/discussions/amazon/view/151926-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"AD","answer_description":"","exam_id":21,"answer_images":[],"answer":"AD","isMC":true,"topic":"1","timestamp":"2024-11-25 02:11:00","question_text":"A company has an application that uses a microservice architecture. The company hosts the application on an Amazon Elastic Kubernetes Services (Amazon EKS) cluster.\n\nThe company wants to set up a robust monitoring system for the application. The company needs to analyze the logs from the EKS cluster and the application. The company needs to correlate the cluster's logs with the application's traces to identify points of failure in the whole application request flow.\n\nWhich combination of steps will meet these requirements with the LEAST development effort? (Choose two.)","question_images":[],"question_id":67,"answers_community":["AD (100%)"],"choices":{"C":"Use Amazon CloudWatch to collect logs. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to collect traces.","B":"Use Amazon CloudWatch to collect logs. Use Amazon Kinesis to collect traces.","D":"Use Amazon OpenSearch to correlate the logs and traces.","A":"Use FluentBit to collect logs. Use OpenTelemetry to collect traces.","E":"Use AWS Glue to correlate the logs and traces."},"discussion":[{"poster":"emupsx1","upvote_count":"2","content":"Selected Answer: AD\nhttps://aws.amazon.com/blogs/big-data/part-1-microservice-observability-with-amazon-opensearch-service-trace-and-log-correlation/","comment_id":"1317264","timestamp":"1732497060.0"}],"unix_timestamp":1732497060},{"id":"jdTXQhxwg7OaYT0XidaW","exam_id":21,"topic":"1","answers_community":["B (100%)"],"discussion":[{"content":"Selected Answer: B\nWhoever is the admin that pre-marks the answers, it's time to go","comment_id":"1127214","timestamp":"1705748040.0","poster":"milofficial","upvote_count":"21"},{"content":"B. AWS Database Migration Service (AWS DMS)\n\nExplanation:\nAWS Database Migration Service (DMS) is specifically designed for migrating data from various sources, including on-premises databases, to AWS with minimal downtime and disruption to applications. It supports homogeneous migrations (e.g., SQL Server to SQL Server) as well as heterogeneous migrations (e.g., SQL Server to Amazon RDS for SQL Server).","upvote_count":"13","comment_id":"1137929","timestamp":"1706821200.0","poster":"TonyStark0122"},{"upvote_count":"1","comment_id":"1279864","timestamp":"1725672960.0","poster":"shammous","content":"Hahaha, I loved the \"downtown\" typo in the question. I always say the same instead of \"downtime\".."},{"comment_id":"1269298","upvote_count":"2","poster":"San_Juan","comments":[{"poster":"shammous","upvote_count":"3","comment_id":"1279863","timestamp":"1725672840.0","content":"Right, but Snowball isn't mentioned in Option D. Thus, you can't consider it as OK."}],"timestamp":"1724145960.0","content":"D could be OK.\nI mean, it is talking about migrating to a cloud database and switching-off the current on-premise database. So you could use AWS Snowball Edge Storage to move the backup of the on-premises database, and when it is in the edge storage, copy the data to a new cloud-based SQL server instance using AWS DataSync\n\nhttps://aws.amazon.com/es/blogs/storage/seamlessly-migrate-large-sql-databases-using-aws-snowball-and-aws-datasync/"},{"timestamp":"1717893840.0","upvote_count":"2","comment_id":"1226986","poster":"pypelyncar","content":"Selected Answer: B\nAWS DMS offers a cost-effective solution for database migrations compared to replicating data to a fully managed RDS instance.\nYou only pay for the resources used during the migration, making it ideal for infrequent, monthly transfers"},{"upvote_count":"1","comment_id":"1204495","timestamp":"1714473660.0","content":"AWS Database Migration Service (DMS)","poster":"xaocho"},{"upvote_count":"2","content":"Selected Answer: B\nB Since it's for Migration porpouse, typical for DMS","comment_id":"1186460","timestamp":"1711841820.0","poster":"lucas_rfsb"}],"answer":"B","question_id":68,"question_images":[],"timestamp":"2024-01-20 11:54:00","unix_timestamp":1705748040,"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/131677-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"C":"AWS Direct Connect","D":"AWS DataSync","A":"AWS Lambda","B":"AWS Database Migration Service (AWS DMS)"},"isMC":true,"question_text":"A company uses an on-premises Microsoft SQL Server database to store financial transaction data. The company migrates the transaction data from the on-premises database to AWS at the end of each month. The company has noticed that the cost to migrate data from the on-premises database to an Amazon RDS for SQL Server database has increased recently.\nThe company requires a cost-effective solution to migrate the data to AWS. The solution must cause minimal downtown for the applications that access the database.\nWhich AWS service should the company use to meet these requirements?","answer_ET":"B"},{"id":"XQSR4LhH1jh69gNTTyeK","timestamp":"2024-11-25 02:14:00","answer_description":"","question_images":[],"choices":{"B":"Configure an AWS Glue job to have a source of Amazon DynamoDB and a destination of Amazon OpenSearch Service to transfer data in near real time.","D":"Use a custom OpenSearch plugin to sync data from the Amazon DynamoDB tables.","C":"Use Amazon DynamoDB Streams to capture table changes. Use an AWS Lambda function to process and update the data in Amazon OpenSearch Service.","A":"Use AWS Step Functions to periodically export data from the Amazon DynamoDB tables to an Amazon S3 bucket. Use an AWS Lambda function to load the data into Amazon OpenSearch Service."},"discussion":[{"upvote_count":"1","timestamp":"1736963340.0","comment_id":"1341169","content":"Selected Answer: C\nDynamoDB supports streaming of item-level change data capture records in *near-real time*","poster":"MerryLew"},{"poster":"emupsx1","comment_id":"1317265","upvote_count":"1","timestamp":"1732497240.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/configure-client-ddb.html"}],"exam_id":21,"isMC":true,"topic":"1","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/151927-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":69,"question_text":"A company has a gaming application that stores data in Amazon DynamoDB tables. A data engineer needs to ingest the game data into an Amazon OpenSearch Service cluster. Data updates must occur in near real time.\n\nWhich solution will meet these requirements?","answer_ET":"C","answer_images":[],"unix_timestamp":1732497240,"answer":"C"},{"id":"rvmCQhbTEEENGDHeIXeR","answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/151853-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"topic":"1","answer":"C","timestamp":"2024-11-22 23:06:00","question_images":[],"answers_community":["C (67%)","B (33%)"],"isMC":true,"question_id":70,"question_text":"A company uses Amazon Redshift as its data warehouse service. A data engineer needs to design a physical data model.\n\nThe data engineer encounters a de-normalized table that is growing in size. The table does not have a suitable column to use as the distribution key.\n\nWhich distribution style should the data engineer use to meet these requirements with the LEAST maintenance overhead?","discussion":[{"poster":"idenrai","comment_id":"1324173","upvote_count":"2","content":"Selected Answer: C\nthe LEAST maintenance overhead = C\n\nWith AUTO distribution, Amazon Redshift assigns an optimal distribution style based on the size of the table data. For example, if AUTO distribution style is specified, Amazon Redshift initially assigns the ALL distribution style to a small table. When the table grows larger, Amazon Redshift might change the distribution style to KEY, choosing the primary key (or a column of the composite primary key) as the distribution key. If the table grows larger and none of the columns are suitable to be the distribution key, Amazon Redshift changes the distribution style to EVEN. The change in distribution style occurs in the background with minimal impact to user queries.","timestamp":"1733769660.0"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html","timestamp":"1732499100.0","comment_id":"1317289","upvote_count":"2","poster":"emupsx1"},{"comment_id":"1316509","content":"Selected Answer: B\nIf the table grows larger and none of the columns are suitable to be the distribution key, Amazon Redshift changes the distribution style to EVEN.","poster":"jacob_nz","upvote_count":"2","timestamp":"1732313160.0"}],"choices":{"A":"ALL distribution","D":"KEY distribution","C":"AUTO distribution","B":"EVEN distribution"},"unix_timestamp":1732313160,"answer_ET":"C"}],"exam":{"provider":"Amazon","lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","numberOfQuestions":207,"id":21,"isBeta":false,"isImplemented":true},"currentPage":14},"__N_SSP":true}