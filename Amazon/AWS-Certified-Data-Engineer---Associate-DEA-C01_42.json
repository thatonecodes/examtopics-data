{"pageProps":{"questions":[{"id":"jw5igyacjyPIFGDEjzXe","answer_description":"","answer_ET":"AD","url":"https://www.examtopics.com/discussions/amazon/view/142573-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":206,"question_text":"A company wants to use machine learning (ML) to perform analytics on data that is in an Amazon S3 data lake. The company has two data transformation requirements that will give consumers within the company the ability to create reports.\n\nThe company must perform daily transformations on 300 GB of data that is in a variety format that must arrive in Amazon S3 at a scheduled time. The company must perform one-time transformations of terabytes of archived data that is in the S3 data lake. The company uses Amazon Managed Workflows for Apache Airflow (Amazon MWAA) Directed Acyclic Graphs (DAGs) to orchestrate processing.\n\nWhich combination of tasks should the company schedule in the Amazon MWAA DAGs to meet these requirements MOST cost-effectively? (Choose two.)","answer":"AD","timestamp":"2024-06-15 10:46:00","exam_id":21,"unix_timestamp":1718441160,"discussion":[{"timestamp":"1720014900.0","content":"A. For daily incoming data, use AWS Glue crawlers to scan and identify the schema.\nD. For daily and archived data, use Amazon EMR to perform data transformations.\n\nHere's why:\n\nA. AWS Glue crawlers are well-suited for scanning and identifying the schema of data in S3. They are cost-effective and efficient for daily incoming data.\nD. Amazon EMR is a cost-effective solution for performing large-scale data transformations. It can handle both the daily transformations of 300 GB of data and the one-time transformations of terabytes of archived data efficiently.","upvote_count":"5","comment_id":"1241451","poster":"Ja13"},{"comment_id":"1254086","content":"Selected Answer: AD\nGlue crawlers for identifying the schema, EMR to run batch processing on the data","upvote_count":"2","poster":"andrologin","timestamp":"1721792700.0"},{"poster":"HunkyBunky","timestamp":"1720066500.0","content":"A / D - Looks good for me","comment_id":"1241804","upvote_count":"1"},{"comment_id":"1241454","poster":"Ja13","upvote_count":"2","timestamp":"1720014960.0","content":"Selected Answer: AD\nAccording to ChatGPT"},{"content":"Selected Answer: AD\nA. For daily incoming data, use AWS Glue crawlers to scan and identify the schema. This is cost-effective and simplifies the process of managing metadata.\n\nD. For daily and archived data, use Amazon EMR to perform data transformations. EMR is suitable for both large-scale and regular transformations, offering flexibility and cost efficiency.","poster":"tgv","timestamp":"1718441160.0","comment_id":"1230846","upvote_count":"3"}],"answer_images":[],"topic":"1","choices":{"B":"For daily incoming data, use Amazon Athena to scan and identify the schema.","A":"For daily incoming data, use AWS Glue crawlers to scan and identify the schema.","C":"For daily incoming data, use Amazon Redshift to perform transformations.","E":"For archived data, use Amazon SageMaker to perform data transformations.","D":"For daily and archived data, use Amazon EMR to perform data transformations."},"question_images":[],"isMC":true,"answers_community":["AD (100%)"]},{"id":"POeLfaoMm1IkmxZNH7ll","question_text":"A retail company uses AWS Glue for extract, transform, and load (ETL) operations on a dataset that contains information about customer orders. The company wants to implement specific validation rules to ensure data accuracy and consistency.\n\nWhich solution will meet these requirements?","answer":"B","answer_images":[],"answers_community":["B (100%)"],"answer_description":"","isMC":true,"exam_id":21,"discussion":[{"timestamp":"1720066680.0","comment_id":"1241805","upvote_count":"2","content":"Selected Answer: B\nOnly B - makes sense","poster":"HunkyBunky"},{"content":"Selected Answer: B\nB. Create custom AWS Glue Data Quality rulesets to define specific data quality checks.\n\nCustom AWS Glue Data Quality rulesets allow you to define precise data quality checks tailored to your specific needs, ensuring that the data meets the required standards of accuracy and consistency. This approach provides flexibility to implement a wide range of validation rules based on your business requirements.","poster":"Ja13","timestamp":"1720015080.0","comment_id":"1241457","upvote_count":"4"},{"poster":"tgv","content":"Selected Answer: B\nThis option provides the necessary flexibility to define and implement custom validation rules tailored to the company's specific requirements for data accuracy and consistency.","timestamp":"1718441340.0","comment_id":"1230850","upvote_count":"3"}],"topic":"1","question_id":207,"answer_ET":"B","question_images":[],"choices":{"D":"Use AWS Glue Data Catalog to maintain a centralized data schema and metadata repository.","C":"Use the built-in AWS Glue Data Quality transforms for standard data quality validations.","B":"Create custom AWS Glue Data Quality rulesets to define specific data quality checks.","A":"Use AWS Glue job bookmarks to track the data for accuracy and consistency."},"unix_timestamp":1718441340,"timestamp":"2024-06-15 10:49:00","url":"https://www.examtopics.com/discussions/amazon/view/142574-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"}],"exam":{"isImplemented":true,"id":21,"lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Amazon","isMCOnly":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","numberOfQuestions":207},"currentPage":42},"__N_SSP":true}