{"pageProps":{"questions":[{"id":"3dtkQ8opJht0gPGoGO4P","question_text":"A company is migrating its SAP S/4HANA landscape from on premises to AWS. An SAP solutions architect is designing a backup solution for the SAP S/4HANA landscape on AWS. The backup solution will use AWS Backint Agent for SAP HANA (AWS Backint agent) to store backups in Amazon S3.\n\nThe company's backup policy for source systems requires a retention period of 150 days for weekly full online backups. The backup policy requires a retention period of 30 days for daily transaction log backups. The company must keep the same backup policy on AWS while maximizing data resiliency. The company needs the ability to retrieve the backup data one or two times each year within 10 hours of the retrieval request.\n\nThe SAP solutions architect must configure AWS Backint agent and S3 Lifecycle rules according to these parameters.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_ET":"D","answer_images":[],"unix_timestamp":1690494840,"discussion":[{"comment_id":"1010200","timestamp":"1695005460.0","upvote_count":"2","content":"Selected Answer: D\nD. A is most cost effective but the requirement is retrieval within 10 hours. Glacier Deep archive takes longer so not A.","poster":"G4Exams"},{"timestamp":"1691408400.0","comment_id":"974644","content":"D. Standard-IA provides good durability by storing data across multiple AZs. Transitioning data to S3 Glacier Flexible Retrieval (if it provides retrieval within 10 hours) after 30 days can be cost-effective.\n B is expensive.","upvote_count":"3","poster":"zzw890827"},{"timestamp":"1690949820.0","comment_id":"969652","poster":"kaishin0527","upvote_count":"4","content":"Selected Answer: D\nD: You should configure the target S3 bucket to use S3 Standard-Infrequent Access (S3 Standard-IA) for the backup files. This storage class is designed for data that is accessed less frequently, but requires rapid access when needed, which matches your backup requirements.\n\nYou then need to create S3 Lifecycle rules on the S3 bucket to move full online backup files that are older than 30 days to S3 Glacier Flexible Retrieval. This storage class is designed for long-term archiving with retrieval times ranging from minutes to hours, matching your requirement of retrieving the backup data within 10 hours of the retrieval request.\n\nThen, you should set up rules to delete log backup files that are older than 30 days, and full online backup files that are older than 150 days.\n\nThis solution meets your backup policy requirements and maximizes data resiliency, while being cost-effective."},{"content":"Selected Answer: B\nB to Maximize cost","comment_id":"965084","upvote_count":"2","timestamp":"1690494840.0","poster":"tonatiuhop"}],"question_id":111,"url":"https://www.examtopics.com/discussions/amazon/view/116642-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","answers_community":["D (75%)","B (25%)"],"timestamp":"2023-07-27 23:54:00","answer":"D","question_images":[],"topic":"1","isMC":true,"exam_id":28,"choices":{"A":"Configure the target S3 bucket to use S3 Glacier Deep Archive for the backup files. Create S3 Lifecycle rules on the S3 bucket to delete full online backup files that are older than 150 days and to delete log backup files that are older than 30 days.","D":"Configure the target S3 bucket to use S3 Standard-Infrequent Access (S3 Standard-IA) for the backup files. Create S3 Lifecycle rules on the S3 bucket to move full online backup files that are older than 30 days to S3 Glacier Flexible Retrieval and to delete log backup files that are older than 30 days. Create an additional S3 Lifecycle rule to delete full online backup files that are older than 150 days.","B":"Configure the target S3 bucket to use S3 Standard storage for the backup files. Create an S3 Lifecycle rule on the S3 bucket to move all the backup files to S3 Glacier Instant Retrieval. Create additional S3 Lifecycle rules to delete full online backup files that are older than 150 days and to delete log backup files that are older than 30 days.","C":"Configure the target S3 bucket to use S3 One Zone-Infrequent Access (S3 One Zone-IA) for the backup files. Create S3 Lifecycle rules on the S3 bucket to move full online backup files that are older than 30 days to S3 Glacier Flexible Retrieval and to delete log backup files that are older than 30 days. Create an additional S3 Lifecycle rule to delete full online backup files that are older than 150 days."},"answer_description":""},{"id":"hS5xoNHNJGEBBjCUGO69","answer_images":[],"choices":{"A":"Use AWS AppSync to extract the data from SAP S/4HANA and to store the data in Amazon S3. Use AWS Glue to perform analytics. Use Amazon Forecast for sales forecasts.","D":"Integrate AWS Glue and AWS Lambda with the SAP Operational Data Provisioning (ODP) Framework to extract the data from SAP S/4HANA and to store the data in Amazon S3. Use Amazon QuickSight to perform analytics. Use Amazon Forecast for sales forecasts.","C":"Use Amazon AppFlow to extract the data from SAP S/4HANA and to store the data in Amazon S3. Use Amazon QuickSight to perform analytics. Use Amazon Forecast for sales forecasts.","B":"Use the SAP Landscape Transformation (LT) Replication Server SDK to extract the data, to integrate the data with SAP Data Services, and to store the data in Amazon S3. Use Amazon Athena to perform analytics. Use Amazon Forecast for sales forecasts."},"url":"https://www.examtopics.com/discussions/amazon/view/116643-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","answer":"C","question_text":"A global retail company is running its SAP S/4HANA workload on AWS. The company's business has grown in the past few years, and user activity has generated a significant amount or data in the SAP S/4HANA system.\n\nThe company wants to expand into new geographies. Before the company finalizes the expansion plan, the company wants to perform analytics on the historical data from the past few years. The company also wants to generate sales forecasts for potential expansion locations.\n\nAn SAP solutions architect must implement a solution to extract the data from SAP S/4HANA into Amazon S3. The solution also must perform the required analytics and forecasting tasks.\n\nWhich solution will meet these requirements with the LEAST custom development effort?","unix_timestamp":1690494900,"answer_ET":"C","question_images":[],"timestamp":"2023-07-27 23:55:00","answer_description":"","answers_community":["C (60%)","B (30%)","10%"],"isMC":true,"discussion":[{"content":"Selected Answer: C\nC - https://youtu.be/lFh16QxL60M?si=NKQoUoYRd4EZmFa2","comment_id":"1109387","timestamp":"1703908800.0","upvote_count":"1","poster":"Demianwholovesjudo"},{"upvote_count":"1","poster":"thuyeinaung","timestamp":"1703418660.0","content":"Selected Answer: C\nvote for C","comment_id":"1104563"},{"timestamp":"1695005760.0","upvote_count":"1","comment_id":"1010203","content":"Selected Answer: B\nIt is most likely B but C also possible. I did not see the scenario in prod so not 100% sure but B should be least effort like the requiremnt says.","poster":"G4Exams"},{"content":"Selected Answer: C\nB is a valid choice but might involve a significant setup for both SAP LT Replication and SAP Data Services.\nD is also require development.\n\nI think it is C.","timestamp":"1691409540.0","comment_id":"974665","upvote_count":"4","poster":"zzw890827"},{"upvote_count":"1","content":"Selected Answer: D\nD: AWS Glue and AWS Lambda can be used to extract data from SAP S/4HANA by leveraging the SAP ODP Framework. AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. AWS Lambda lets you run code without provisioning or managing servers. The data can then be stored in Amazon S3, which is an object storage service that offers industry-leading scalability, data availability, security, and performance.\n\nAmazon QuickSight can be used to perform the analytics. QuickSight is a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization.\n\nFinally, Amazon Forecast can be used for the sales forecasts. Amazon Forecast is a fully managed service that uses machine learning to deliver highly accurate forecasts, based on the same technology used by Amazon.com.","timestamp":"1690949340.0","comment_id":"969646","poster":"kaishin0527"},{"content":"Selected Answer: B\nAnswer is B. \nReplicate data from SAP applications, such as SAP S/4HANA or SAP Business Suite, to Amazon Simple Storage Service (Amazon S3) using SAP Landscape Transformation (LT) Replication Server and SAP Data Services (DS)\nhttps://aws.amazon.com/blogs/awsforsap/sap-data-services-and-sap-lt-server-for-near-real-time-replication-to-aws-data-lakes/","comment_id":"967199","timestamp":"1690726260.0","upvote_count":"1","poster":"karanbhasin"},{"comment_id":"965085","upvote_count":"1","timestamp":"1690494900.0","content":"Selected Answer: B\nAgree with B","poster":"tonatiuhop"}],"question_id":112,"topic":"1","exam_id":28},{"id":"QTsR1qpWbHCP4z9lBsnh","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/116644-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","isMC":true,"choices":{"D":"Three overlay IP addresses: 192.168.0.50 for SAP ASCS, 192.168.0.52 for SAP ERS, and 192.168.0.54 for SAP HANA","B":"Two overlay IP addresses: 192.168.0.50 for SAP ASCS and 192.168.0.54 for SAP HANA","C":"Three overlay IP addresses: 10.0.0.50 for SAP ASCS, 10.0.0.52 for SAP ERS, and 10.0.0.54 for SAP HANA","A":"Two overlay IP addresses: 10.0.0.50 for SAP ASCS and 10.0.0.54 for SAP HANA"},"discussion":[{"comment_id":"976618","content":"Selected Answer: D\nD.. Overlay IP address should remain outside VPC CIDR block..\nhttps://docs.aws.amazon.com/sap/latest/sap-hana/sap-oip-sap-on-aws-high-availability-setup.html","timestamp":"1691582580.0","poster":"ArtanisAM91","upvote_count":"9"},{"upvote_count":"1","content":"Every ChatGPT is wrong.\nC is wrong.","poster":"LocalHero","timestamp":"1705494600.0","comment_id":"1124950"},{"content":"Selected Answer: D\nhttps://access.redhat.com/articles/4175371#create-overlayip-addresses","comment_id":"1114720","poster":"acethetest1000","upvote_count":"1","timestamp":"1704478980.0"},{"poster":"thuyeinaung","comment_id":"1104996","timestamp":"1703475660.0","upvote_count":"1","content":"Selected Answer: C\nadd for C"},{"timestamp":"1700038620.0","content":"https://documentation.suse.com/sbp/all/html/SLES4SAP-hana-sr-guide-perfopt-15-aws/index.html\n\n\"The Overlay IP address must be an IP outside the Virtual Private Cloud (VPC) CIDR.\"\nIt already cannot be an answer A and C because their IP's for Overlay are within the VPC CIDR. \n\nAbout B and D, I'm not fully confident. As per my understanding only the ASCS should have an overlay IP because it's the one that actually is being high-available.","upvote_count":"2","comment_id":"1071242","poster":"hapvlz"},{"comment_id":"1070576","content":"D is definitely more right than C, but does anyone know why ERS is included as a third overlay IP when it isn't mentioned in the question?","timestamp":"1699976160.0","comments":[{"poster":"odre90","upvote_count":"2","comment_id":"1088640","timestamp":"1701794340.0","content":"When setting HA setup for ASCS, you have to make sure that the ERS (as the name suggests it replicates enqueue server) has its own installation."}],"poster":"[Removed]","upvote_count":"1"},{"poster":"zzw890827","content":"Selected Answer: C\nRegarding the IP address ranges, the overlay IPs should fall within the VPC's CIDR range. The given VPC CIDR block is 10.0.0.0/24, which means any IP within the 10.0.0.1 to 10.0.0.254 range can be used (excluding the network and broadcast addresses).\nC is correct.","upvote_count":"4","comment_id":"974675","timestamp":"1691410320.0"},{"poster":"tonatiuhop","comment_id":"965087","content":"Agree with C","timestamp":"1690495080.0","upvote_count":"3"}],"answer":"D","topic":"1","timestamp":"2023-07-27 23:58:00","answers_community":["D (67%)","C (33%)"],"question_id":113,"question_images":[],"exam_id":28,"unix_timestamp":1690495080,"answer_description":"","answer_ET":"D","question_text":"A company's SAP solutions architect needs to design an architecture to deploy a highly available SAP S/4HANA application on AWS. The company requires the SAP NetWeaver ASAP ASCS components and the SAP HANA database components or the application to be highly available. The company will operate the SAP NetWeaver ASCS, SAP NetWeaver PAS, and SAP HANA database components on separate Amazon EC2 instances. Each EC2 instance will run the Red Hat Enterprise Linux operating system.\n\nThe company's AWS account has a VPC with a CIDR block that uses the 10.0.0.0/24 address block. The VPC contains two subnets. Each subnet is assigned to a different Availability Zone. The company has no other VPCs in this account, and the company has no other AWS accounts.\n\nWhich set of overlay IP addresses can the SAP solutions architect use to provide the required highly available architecture?"},{"id":"fwAli3v2ee4qH87w4wVv","answer_ET":"B","unix_timestamp":1690495860,"url":"https://www.examtopics.com/discussions/amazon/view/116647-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","isMC":true,"question_text":"A company is running its on-premises SAP ERP Central Component (SAP ECC) system on an Oracle database on Oracle Enterprise Linux. The database is 1 TB in size and uses 27,000 IOPS for its peak performance Multiple SSD volumes are striped to store Oracle data files in separate sapdata directories to gain the required IOPS.\n\nThe company is planning to move this workload to AWS. The company chooses high I/O bandwidth instances with a Nitro hypervisor to host the target database instance. Downtime is not a constraint for the migration. The company needs an Amazon Elastic Block Store (Amazon EBS) storage layout that optimizes cost for the migration.\n\nHow should the company reorganize the Oracle data files to meet these requirements?","question_id":114,"exam_id":28,"timestamp":"2023-07-28 00:11:00","question_images":[],"answer":"B","choices":{"C":"Reorganize the Oracle data files into one 1 TB General Purpose SSD (gp3) EBS volume with 27,000 provisioned IOPS.","D":"Reorganize the Oracle data files into ten 100 GB General Purpose SSD (gp3) EBS volumes.","B":"Reorganize the Oracle data files into a striped volume of three 3 TB General Purpose SSD (gp2) EBS volumes.","A":"Reorganize the Oracle data files into one 9 TB General Purpose SSD (gp2) EBS volume."},"discussion":[{"comment_id":"974683","poster":"zzw890827","content":"Selected Answer: B\nC is not correct as the max IOPS of gp3 is 16000. I prefer B since aach 3TB gp2 volume can provide a maximum of 9,000 IOPS (3 IOPS * 3,000 GB). Three of them combined can provide 27,000 IOPS. So, this meets the requirement.","upvote_count":"8","timestamp":"1691410860.0"},{"content":"D.\nPlz Use this.\nhttps://calculator.aws/#/createCalculator/EBS","timestamp":"1705495080.0","poster":"LocalHero","comment_id":"1124965","upvote_count":"1"},{"upvote_count":"2","comment_id":"1104997","timestamp":"1703475780.0","poster":"thuyeinaung","content":"Selected Answer: B\nadd for B"},{"content":"Selected Answer: D\nGP3 costs 20% lower than GP2. GP3 provides first 3000 iops free. 10 GP3 with 3000 iops each will be sufficient for 27000 IOPS requirement.","timestamp":"1700741160.0","poster":"rrshah83","comment_id":"1078397","upvote_count":"3"},{"poster":"Jeanz501","timestamp":"1698865260.0","content":"D: As this is cost optimized, a single volume of GP3 has an average IOPS of 3000. \n When 10 are in an array you reach 30000, enough to meet the 27000 IOPS requirement. Answer C would require additional costs and max OPS for a signle GP3 volume is 16000. \n https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","comments":[{"poster":"[Removed]","upvote_count":"1","content":"Option D does not state that the volumes are in an array/striped. All of these answers are poor choices.","timestamp":"1699976700.0","comment_id":"1070586"}],"comment_id":"1060003","upvote_count":"2"},{"content":"Selected Answer: D\nYou guys here talk about IOPS but A and B provide way too much storage if the database is just 1 TB so its too expensive. D should be right.","timestamp":"1695006000.0","poster":"G4Exams","upvote_count":"4","comment_id":"1010205"},{"poster":"kaishin0527","timestamp":"1690949160.0","upvote_count":"1","comment_id":"969643","content":"Selected Answer: C\nC: The general purpose SSD (gp3) volumes allow you to provision performance independent of storage capacity and provides the flexibility to adjust IOPS and throughput as needed. For this workload, having a single 1TB gp3 volume with 27,000 provisioned IOPS would meet the company's requirements most cost-effectively."},{"poster":"tonatiuhop","timestamp":"1690495860.0","comment_id":"965095","content":"Selected Answer: D\nD\nhttps://aws.amazon.com/es/ebs/general-purpose/","upvote_count":"1"}],"topic":"1","answer_images":[],"answer_description":"","answers_community":["B (53%)","D (42%)","5%"]},{"id":"KJNGmcHLQ2DkNDOxSciG","answer_description":"","answers_community":["B (100%)"],"unix_timestamp":1690495980,"answer_images":[],"choices":{"B":"The change in EC2 instance type requires SAP system downtime, but the change in EBS volume type does not require SAP system downtime.","D":"Both the change in EC2 instance type and the change in EBS volume type require SAP system downtime.","C":"Neither the change in EC2 instance type nor the change in EBS volume type requires SAP system downtime.","A":"The change in EC2 instance type does not require SAP system downtime, but the change in EBS volume type requires SAP system downtime."},"timestamp":"2023-07-28 00:13:00","question_text":"A company has grown rapidly in a short period of time. This growth has led to an increase in the volume of data, the performance requirements for storage, and the memory and vCPU requirements for the company's SAP HANA database that runs on AWS. The SAP HANA database is a scale-up installation. Because of the increased requirements, the company plans to change the Amazon EC2 instance type to a virtual EC2 High Memory instance and plans to change the Amazon Elastic Block Store (Amazon EBS) volume type to a higher performance volume type for the SAP HANA database.\n\nThe EC2 instance is a current-generation instance, both before and after the change. Additionally, the EC2 instance and the EBS volume meet all the prerequisites for instance type change and EBS volume type change. An SAP basis administrator must advise the company about whether these changes will require downtime for the SAP system.\n\nWhich guidance should the SAP basis administrator provide to the company?","question_id":115,"exam_id":28,"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/116648-exam-aws-certified-sap-on-aws-specialty-pas-c01-topic-1/","discussion":[{"upvote_count":"3","poster":"kaishin0527","content":"Selected Answer: B\nB: Resizing the Amazon EC2 instance type requires the instance to be stopped before the instance type can be changed, thus causing SAP system downtime. However, changing the Amazon EBS volume type can be done without causing any downtime as it can be performed while the volume is in use.","comment_id":"969636","timestamp":"1690948800.0"},{"comment_id":"967241","poster":"karanbhasin","upvote_count":"3","timestamp":"1690730700.0","content":"Selected Answer: B\nThe change in EC2 instance type requires SAP system downtime, but the change in EBS volume type does not require SAP system downtime\nhttps://docs.aws.amazon.com/sap/latest/sap-hana/migrating-hana-to-hm.html\nStop the source instance in the Amazon EC2 console or by using the AWS CLI.\nhttps://docs.aws.amazon.com/sap/latest/sap-netweaver/storage-1.html\nYou can increase EBS volume size or change the type of volume (for example, gp2 to io1) without requiring downtime"},{"content":"Selected Answer: B\nB seems correct","upvote_count":"2","comment_id":"965100","poster":"tonatiuhop","timestamp":"1690495980.0"}],"topic":"1","isMC":true,"answer_ET":"B","question_images":[]}],"exam":{"isBeta":false,"numberOfQuestions":130,"id":28,"name":"AWS Certified SAP on AWS - Specialty PAS-C01","lastUpdated":"11 Apr 2025","provider":"Amazon","isMCOnly":true,"isImplemented":true},"currentPage":23},"__N_SSP":true}