{"pageProps":{"questions":[{"id":"llaL1Z2kBDwHK4C93Cbb","answer_ET":"C","exam_id":33,"answer_description":"","unix_timestamp":1675505460,"answer_images":[],"choices":{"B":"Associate the existing web ACL with the ALB.","C":"Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.","D":"Add a security group rule to the ALB to allow only the various CloudFront IP address ranges.","A":"Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB."},"topic":"1","question_images":[],"question_id":71,"discussion":[{"upvote_count":"12","timestamp":"1691152440.0","poster":"masssa","content":"Selected Answer: C\nhttps://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/LocationsOfEdgeServers.html\nAWS managed prefix list is more recommended.","comment_id":"798038"},{"content":"Selected Answer: C\nhttps://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/LocationsOfEdgeServers.html\nIf your origin is hosted on Amazon and protected by an Amazon VPC security group, you can use the CloudFront managed prefix list to allow inbound traffic to your origin only from CloudFront's origin-facing servers, preventing any non-CloudFront traffic from reaching your origin\n, imagine that your origin is an Amazon EC2 instance in the Europe (London) Region (eu-west-2). If the instance is in a VPC, you can create a security group rule that allows inbound HTTPS access from the CloudFront managed prefix list. This allows all of CloudFront's global origin-facing servers to reach the instance. If you remove all other inbound rules from the security group, you prevent any non-CloudFront traffic from reaching the instance","timestamp":"1700633700.0","upvote_count":"5","poster":"rbm2023","comment_id":"903705"},{"upvote_count":"1","timestamp":"1719258240.0","comment_id":"1104859","poster":"career360guru","content":"Selected Answer: C\nOption C"},{"timestamp":"1716523560.0","content":"Selected Answer: C\nOption C","upvote_count":"1","comment_id":"1079059","poster":"career360guru"},{"timestamp":"1704390720.0","content":"Selected Answer: C\nC for sure","comment_id":"942953","upvote_count":"1","poster":"NikkyDicky"},{"content":"C. Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.","poster":"mfsec","upvote_count":"2","comment_id":"851905","timestamp":"1695801900.0"},{"upvote_count":"2","content":"C https://aws.amazon.com/blogs/news/limit-access-to-your-origins-using-the-aws-managed-prefix-list-for-amazon-cloudfront/","poster":"ExamTopix01","timestamp":"1691138820.0","comment_id":"797839"},{"comment_id":"797807","upvote_count":"3","poster":"jojom19980","content":"Selected Answer: C\nhttps://aws.amazon.com/about-aws/whats-new/2022/02/amazon-cloudfront-managed-prefix-list/","timestamp":"1691136660.0"}],"answers_community":["C (100%)"],"answer":"C","timestamp":"2023-02-04 11:11:00","url":"https://www.examtopics.com/discussions/amazon/view/97934-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"question_text":"A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is associated with the CloudFront distribution.\n\nThe company needs a solution that will prevent internet traffic from directly accessing the ALB.\n\nWhich solution will meet these requirements with the LEAST operational overhead?"},{"id":"fg2dWGkDb5uTpYmWCSiH","answer_ET":"B","answer_images":[],"exam_id":33,"answers_community":["B (74%)","A (26%)"],"isMC":true,"answer":"B","timestamp":"2023-02-02 19:28:00","answer_description":"","choices":{"A":"Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication.","D":"Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication.","C":"Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication.","B":"Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication."},"question_images":[],"unix_timestamp":1675362480,"topic":"1","discussion":[{"content":"Selected Answer: A\nEncryption in transit cannot be enabled on an existing ElastiCache cluster. A new cluster must be created.","poster":"zhen234","timestamp":"1738714620.0","upvote_count":"1","comment_id":"1351618"},{"poster":"d401c0d","content":"Selected Answer: B\nAmazon ElastiCache for Redis now supports updates to encryption in transit on existing cluster resources. You can change the TLS configuration of your Redis clusters without re-building or re-provisioning them or impacting application availability. When enabling encryption in transit, your overall solution can remain connected to Redis clusters.\n\nTo get started, upgrade your Redis cluster to version 7 or above. You can then modify the encryption-in-transit property for your cluster using the Elasticache Console, API or CLI. This feature is available in all regions at no additional cost. To learn more, see the ElastiCache user guide.","comment_id":"1350045","upvote_count":"1","timestamp":"1738438560.0"},{"comment_id":"1344844","timestamp":"1737560400.0","poster":"kylix75","upvote_count":"1","content":"Selected Answer: A\nThe correct answer is A - Create an AUTH token, store it in Parameter Store, and create a new cluster with AUTH and in-transit encryption.\nKey reasons:\n\nElastiCache doesn't allow enabling AUTH on existing clusters\nSSL certificates aren't used for Redis authentication\nParameter Store is more cost-effective than Secrets Manager for this case\nSolution meets both requirements: AUTH authentication and end-to-end encryption"},{"comment_id":"1302029","timestamp":"1729683600.0","poster":"TewatiaAmit","content":"Selected Answer: A\nA or B? Option B is suggesting to update the cluster which is not feasible. Once a cluster is created without encryption in transit, it cannot be modified to enable encryption in transit.","upvote_count":"1"},{"upvote_count":"3","poster":"Sin_Dan","content":"Selected Answer: A\nEnabling encryption in transit on an existing ElastiCache cluster that wasn’t originally configured with this feature is not possible. Encryption in transit, as well as encryption at rest, can only be specified at the time the cluster is created.\n\nAWS Documentation on Encryption in Transit:\n\nAccording to AWS ElastiCache documentation, if you want to enable encryption in transit, you must set this option when creating the ElastiCache cluster. Once a cluster is created without encryption in transit, it cannot be modified to enable this feature later. The same applies to Redis AUTH.\n\nThus, if a Redis cluster was deployed without encryption in transit, the only way to enable it is to create a new ElastiCache cluster with this setting enabled. Then, the data would need to be migrated from the existing cluster to the new one.","comment_id":"1300186","timestamp":"1729375680.0"},{"content":"Selected Answer: B\nB=Better :-)","comment_id":"1293628","poster":"JoeTromundo","upvote_count":"1","timestamp":"1728168720.0"},{"comments":[{"upvote_count":"1","content":"https://docs.aws.amazon.com/AmazonElastiCache/latest/dg/in-transit-encryption.html#in-transit-encryption-constraints\n\"Modifying the in-transit encryption setting, for an existing cluster, is supported on replication groups running Valkey 7.2 and later, and Redis OSS version 7 and later.\" => modifying is possible => so B","poster":"attila9778","comment_id":"1325016","timestamp":"1733919660.0"},{"timestamp":"1724648700.0","comment_id":"1272419","poster":"helloworldabc","content":"just B","upvote_count":"3"}],"upvote_count":"1","content":"Selected Answer: A\nIt seems to configure in-transit encryption in both new cluster and existing cluster, but updating is supported on Redis version 7 and later. So I will choose option A.\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html#in-transit-encryption-constraints","comment_id":"1220636","timestamp":"1716944100.0","poster":"ke1dy"},{"poster":"gofavad926","timestamp":"1710743880.0","comment_id":"1176305","upvote_count":"3","content":"Selected Answer: B\nA or B? I didn't read any comparison between these 2 options... For sure we need an auth token. Both, using SSM Parameter Store or Secrets Manager will work. Both, create a new cluster or update the current one will work. I will choose B because this approach avoids the need to set up a new cluster, potentially reducing effort and costs associated with migration or duplication of resources..."},{"comment_id":"1104866","content":"Selected Answer: B\nOption B","timestamp":"1703454600.0","upvote_count":"2","poster":"career360guru"},{"upvote_count":"2","comment_id":"1079065","poster":"career360guru","content":"Selected Answer: B\nOption B","timestamp":"1700807100.0"},{"content":"Selected Answer: B\nB, per redis docs. \nEC encr in transit is a config option","upvote_count":"2","poster":"NikkyDicky","comment_id":"942959","timestamp":"1688486160.0"},{"timestamp":"1687278900.0","upvote_count":"4","comment_id":"928600","content":"b-b-b-b-b-b-b\n\nCreating an AUTH token provides a form of authentication for accessing the ElastiCache cluster.\nStoring the AUTH token in AWS Secrets Manager ensures secure and centralized management of the token.\nConfiguring the existing ElastiCache cluster to use the AUTH token enables authentication for accessing the cache.\nEnabling encryption in transit ensures that data is encrypted when it is transferred between the client and the ElastiCache cluster.\nUpdating the application to retrieve the AUTH token from Secrets Manager and use it for authentication ensures that only authorized users can access the cache.","poster":"easytoo"},{"upvote_count":"1","content":"Selected Answer: B\nCreate an AUTH token. Store the token in AWS Secrets Manager.","comment_id":"851916","poster":"mfsec","timestamp":"1679904960.0"},{"content":"Selected Answer: B\nRedis CLI has AUTH command as a feature to SET/ROTATE strategies\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html","upvote_count":"4","poster":"God_Is_Love","comment_id":"838581","timestamp":"1678771740.0"},{"poster":"Zek","comment_id":"832841","timestamp":"1678272660.0","upvote_count":"3","content":"B seems right. \nTo enable authentication on an existing Redis server, call the ModifyReplicationGroup API operation. Call ModifyReplicationGroup with the --auth-token parameter as the new token and the --auth-token-update-strategy with the value ROTATE.\n\nAfter the modification is complete, the cluster supports the AUTH token specified in the auth-token parameter in addition to supporting connecting without authentication. Enabling authentication is only supported on Redis servers with encryption in transit (TLS) enabled.\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html"},{"upvote_count":"2","poster":"spd","timestamp":"1676375460.0","content":"Selected Answer: B\nAs per https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html","comment_id":"808330"},{"content":"You have to create a new cluster, otherwise the the cluster supports the AUTH token specified and supports connecting without authentication.","comment_id":"799120","upvote_count":"1","timestamp":"1675625880.0","poster":"harleydog"},{"poster":"jojom19980","upvote_count":"1","timestamp":"1675506180.0","content":"Selected Answer: B\nPreviously, you needed to set up authentication for ElastiCache for Redis clusters using Redis user passwords or store the password in AWS Secrets Manager or on a third-party secrets management tool. However, in large organizations that host many applications, passwords can often become out of sync when it comes time to rotate the password. IAM authentication provides a streamlined security posture by allowing access management from a centralized service. With IAM authentication, ElastiCache users can use their IAM identities when connecting to their Redis clusters","comment_id":"797822"},{"comment_id":"796326","upvote_count":"1","timestamp":"1675362480.0","poster":"bititan","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html"}],"url":"https://www.examtopics.com/discussions/amazon/view/97689-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":72,"question_text":"A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that the company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit. Additionally, users can access the cache without authentication.\n\nA solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.\n\nWhich solution will meet these requirements?"},{"id":"1jTZrHLU14nSfV2p1jtb","answer_description":"","answer_ET":"B","question_images":[],"answers_community":["B (100%)"],"answer":"B","question_id":73,"topic":"1","answer_images":[],"question_text":"A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement groups and a single instance type.\n\nRecently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to improve the overall reliability of the workload.\n\nWhich solution will meet this requirement?","choices":{"A":"Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.","D":"Update the launch template to use a larger instance type.","B":"Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.","C":"Update the launch template Auto Scaling group to increase the number of placement groups."},"discussion":[{"timestamp":"1675365840.0","content":"Selected Answer: B\nlaunch config is replaced by launch template hence is not advisible, option A rulled out. C is wrong because launch template cannot be updated. D is also wrong for the same reason","comment_id":"796364","upvote_count":"14","poster":"bititan"},{"upvote_count":"6","timestamp":"1692688800.0","content":"Selected Answer: B\nAs an alternative to manually specifying the instance types, you can specify the attributes that an instance must have, and Amazon EC2 will identify all the instance types with those attributes. \nThis is known as attribute-based instance type selection. \nFor example, you can specify the minimum and maximum number of vCPUs required for your instances, and EC2 Fleet will launch the instances using any available instance types that meet those vCPU requirements.","poster":"Simon523","comment_id":"987182"},{"comment_id":"1293629","upvote_count":"1","poster":"JoeTromundo","timestamp":"1728168900.0","content":"Selected Answer: B\nCorrect answer: B"},{"poster":"career360guru","content":"Selected Answer: B\nOption B","timestamp":"1703455140.0","upvote_count":"1","comment_id":"1104870"},{"poster":"career360guru","upvote_count":"1","content":"Selected Answer: B\nOption B","comment_id":"1079597","timestamp":"1700857980.0"},{"comment_id":"1028119","upvote_count":"5","content":"Selected Answer: B\nWhen you use attribute-based instance type selection, you allow AWS to diversify the instances across different instance types within a specified instance family or similar characteristics. This helps in reducing the risk of Spot Instance termination due to capacity issues or price fluctuations.","timestamp":"1696779960.0","poster":"totten"},{"content":"B\nAmazon EC2 Auto Scaling can select from a wide range of instance types for launching Spot Instances. This meets the Spot best practice of being flexible about instance types, which gives the Amazon EC2 Spot service a better chance of finding and allocating your required amount of compute capacity.","comment_id":"964194","upvote_count":"1","poster":"rl97","timestamp":"1690408800.0"},{"poster":"Christina666","upvote_count":"2","comment_id":"945989","timestamp":"1688762400.0","content":"Selected Answer: B\nkey word \"spot instance launch failure\"-> attribute based selection"},{"poster":"NikkyDicky","content":"Selected Answer: B\nits a b","comment_id":"942960","timestamp":"1688486220.0","upvote_count":"1"},{"comment_id":"928604","poster":"easytoo","timestamp":"1687279320.0","content":"b-b-b-b-bb-b-\n\nCreating a new launch template version allows for making changes to the template without disrupting the existing instances.\nUsing attribute-based instance type selection enables the Auto Scaling group to automatically select the most suitable instance type based on the defined attributes, such as availability zone, instance family, or instance size.\nBy leveraging attribute-based instance type selection, the Auto Scaling group can adapt to changing Spot Instance availability and launch instances in zones with higher availability, reducing launch failures.\nUpdating the launch template with this new version ensures that new instances launched by the Auto Scaling group utilize the improved instance selection process, thereby enhancing reliability.","upvote_count":"5"},{"comment_id":"851919","poster":"mfsec","comments":[{"comment_id":"908829","poster":"Roontha","upvote_count":"1","timestamp":"1685300340.0","content":"Agreed with B"}],"content":"Selected Answer: B\nB. Create a new launch template version that uses attribute-based instance type selection.","upvote_count":"2","timestamp":"1679905020.0"},{"upvote_count":"2","timestamp":"1678772640.0","comment_id":"838587","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-instance-type-requirements.html#use-attribute-based-instance-type-selection-prerequisites","poster":"God_Is_Love"},{"content":"Selected Answer: B\nConfused between B and D , will choose B","upvote_count":"1","timestamp":"1677442320.0","poster":"kiran15789","comment_id":"822918"},{"poster":"saurabh1805","comment_id":"821867","content":"Selected Answer: B\nb is correct \n\nhttps://aws.amazon.com/blogs/aws/new-attribute-based-instance-type-selection-for-ec2-auto-scaling-and-ec2-fleet/","upvote_count":"2","timestamp":"1677363780.0"},{"content":"Selected Answer: B\nB is correct","comment_id":"804018","poster":"etechsystem_ts","upvote_count":"1","timestamp":"1676008920.0"}],"isMC":true,"timestamp":"2023-02-02 20:24:00","url":"https://www.examtopics.com/discussions/amazon/view/97695-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"unix_timestamp":1675365840},{"id":"4zrrsZYeiNYrZrcCwIv9","question_text":"A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the document processing is finished, customers can download the documents directly from Amazon S3.\n\nDuring the migration, the company discovered that it could not immediately update the processing server that generates many documents to support the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server finishes processing, the files must be available to the public for download within 30 minutes.\n\nWhich solution will meet these requirements with the LEAST amount of effort?","exam_id":33,"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/97690-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"answer_ET":"B","answers_community":["B (70%)","C (26%)","4%"],"choices":{"C":"Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS.","B":"Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway.","A":"Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3.","D":"Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3."},"unix_timestamp":1675363380,"answer":"B","answer_images":[],"discussion":[{"comment_id":"866150","content":"Selected Answer: B\nB is correct imo\nC is incorrect, FSx for Luster doesn't support NFS protocol\nIt actually support only POSIX protocol:\nCustom (POSIX-compliant) protocol optimized for performance","upvote_count":"26","poster":"dev112233xx","timestamp":"1681119300.0"},{"timestamp":"1675363380.0","upvote_count":"23","content":"Selected Answer: C\nC: \nAmazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Powered by Lustre, the world’s most popular high-performance file system, FSx for Lustre offers shared storage with sub-ms latencies, up to terabytes per second of throughput, and millions of IOPS. FSx for Lustre file systems can also be linked to Amazon Simple Storage Service (S3) buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API.","comment_id":"796336","poster":"schalke04","comments":[{"poster":"lxrdm","comment_id":"943554","timestamp":"1688551080.0","content":"I wouldnt choose Lustre.. would only pick it if its related to HPC (high performance computing), the amount of files generated here is nothing..","upvote_count":"5"},{"content":"I disagree with option C. This is an example of how to mount a Lustre from an EC2 Linux system. it does not use NFS \nsudo mount -t lustre <fsx-dns-name>@tcp:/<mount-point>\nAmazon FSx for Lustre provides its own Lustre-specific mount command and protocol for mounting the file system on Linux instances.\nThe lustre file system type in the mount command indicates that it is specifically for mounting Lustre-based file systems, such as Amazon FSx for Lustre.\nI would still go for option B","upvote_count":"9","timestamp":"1684760160.0","poster":"rbm2023","comment_id":"904047"}]},{"content":"Selected Answer: B\nLuster does not support NFS.","poster":"d401c0d","upvote_count":"1","timestamp":"1738439280.0","comment_id":"1350055"},{"content":"Note that it keeps saying \"The Server\" implying 1 server and not a fleet or multiple. NFS is from 1 client to 1 server. \n\nSo C is incorrect","upvote_count":"1","poster":"AWSum1","timestamp":"1728324360.0","comment_id":"1294435"},{"poster":"JoeTromundo","timestamp":"1728169260.0","comment_id":"1293632","upvote_count":"1","content":"Selected Answer: B\nOption C is not possible: how will you mount the document store on the EC2 instance through the Lustre client using NFS? Lustre is not compatible with NFS!"},{"comment_id":"1261926","timestamp":"1723003980.0","poster":"xktm","upvote_count":"5","content":"The English in this question is very confusing, what is it trying do? what is the problem? where is the processing server?"},{"timestamp":"1708823520.0","content":"https://repost.aws/knowledge-center/storage-gateway-automate-refreshcache","comment_id":"1158256","upvote_count":"1","poster":"duriselvan"},{"timestamp":"1706285760.0","content":"Selected Answer: B\nA = migrating to lambda requires a lot of work and doesn't solve the need to have fast access to files\nB = correct\nC = FSx for Lustre doesn't support NFS\nD = DataSynch can schedule transfer hourly, daily or weekly, cannot meet 30 minutes requirement","upvote_count":"7","poster":"ninomfr64","comment_id":"1132645"},{"poster":"career360guru","timestamp":"1703455680.0","upvote_count":"3","content":"Selected Answer: B\nOption B as Fsx Luster though supports Linux, it does not support NFS.","comment_id":"1104878"},{"content":"Selected Answer: B\nB is right. Though it is meant to be used to with on-premise in Hybrid environment, it is possible to use it on EC2.","poster":"career360guru","upvote_count":"3","comment_id":"1079612","timestamp":"1700862300.0","comments":[{"content":"B is right. If it wasn't possible to update that Linux server at that moment it implies they would have to remain it on premises for a while, in this case Amazon S3 File Gateway is the way to go.","comment_id":"1306263","upvote_count":"1","poster":"Dougmaster","timestamp":"1730568480.0"}]},{"upvote_count":"2","timestamp":"1700203260.0","comment_id":"1073062","poster":"severlight","content":"Selected Answer: B\njust because NFS mentioned with Lustre, but everything else is pointing to the Lustre: Linux, fast, read/writes to S3"},{"content":"Selected Answer: C\nB. Extra effort due to refreshCache API\nD. DataSync runs in task schedule, which can't run faster than once per hour.\nSo remaining answer is C","poster":"covabix879","upvote_count":"1","timestamp":"1696085280.0","comment_id":"1021582"},{"content":"Selected Answer: D\nThe core of the problem is make the file available in S3\n for When the server finishes processing, the files must be available to the public for download within 30 minutes.\nWhich solution will meet these requirements with the LEAST amount of effort?\nI think Option D (AWS DataSync) is a more straightforward and efficient choice.","timestamp":"1695876780.0","upvote_count":"1","poster":"task_7","comment_id":"1019427","comments":[{"comment_id":"1021580","poster":"covabix879","content":"DataSync task cannot run faster than 1 hour. \"Even with a cron expression, you can't schedule a task to run at an interval faster than 1 hour.\" https://docs.aws.amazon.com/datasync/latest/userguide/task-scheduling.html","upvote_count":"5","timestamp":"1696085100.0"}]},{"comments":[{"comment_id":"997371","timestamp":"1693722120.0","content":"FSX for Lustre is for Linux and does not support Windows","upvote_count":"3","poster":"chikorita"}],"upvote_count":"4","content":"Selected Answer: B\nThe server is running Linux, How can we use Fsx?","comment_id":"996121","poster":"Gabehcoud","timestamp":"1693576800.0"},{"comment_id":"993888","upvote_count":"3","timestamp":"1693386900.0","poster":"CloudHandsOn","content":"Selected Answer: B\nI believe that B is correct, given that Lustre does not support NFS (it supports POSIX)"},{"poster":"xav1er","upvote_count":"5","content":"Selected Answer: B\nB as file gateway seems simple working solution for this. Lustre does not support NFS and might be an overkill for this solution - its primary used for HPC clusters. DataSync is rather for batch daad migrations and periodic data migration jobs, isn't it?","timestamp":"1692353760.0","comment_id":"984364"},{"comment_id":"973635","content":"Selected Answer: B\ndon't understand the question and answer, include B&C. how does it mount to EC2 by using NFS? I think the processing server is running on Premise??","poster":"softarts","timestamp":"1691309280.0","upvote_count":"3"},{"poster":"ggrodskiy","timestamp":"1690329120.0","upvote_count":"1","comment_id":"963200","content":"Correct D."},{"content":"Selected Answer: B\nB works\nC would be better if not for NFS mention","comment_id":"942978","upvote_count":"2","poster":"NikkyDicky","timestamp":"1688487240.0"},{"content":"Selected Answer: C\nC\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/mount-fs-auto-mount-onreboot.html","comments":[{"timestamp":"1687618740.0","upvote_count":"1","poster":"PhuocT","content":"change to B, Lustre does not support NFS mount.","comment_id":"932689"}],"comment_id":"932686","poster":"PhuocT","timestamp":"1687618620.0","upvote_count":"1"},{"poster":"easytoo","comment_id":"928608","upvote_count":"2","content":"d-d-d-d-d-d-d-d-d-d-d\n\nAWS DataSync is a fully-managed service that can be used to synchronize data between on-premises storage and AWS storage services. In this case, AWS DataSync could be used to synchronize the files that the processing server generates and modifies to Amazon S3. Once the files are in Amazon S3, they can be made available to the public for download within 30 minutes.\n\nTherefore, the best solution is to configure AWS DataSync to connect to an Amazon EC2 instance and configure a task to synchronize the generated files to and from Amazon S3. This option would require the least amount of effort and would still meet the company's requirements.","timestamp":"1687279560.0"},{"comment_id":"926920","poster":"SkyZeroZx","timestamp":"1687123620.0","content":"Selected Answer: B\nB. Least amount of effort and it supports NFS which Lustre does not.","upvote_count":"1"},{"upvote_count":"2","timestamp":"1685146080.0","content":"he correct answer would be D. Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3.\n\nThis is because AWS DataSync automates and accelerates moving and synchronizing data between on-premises storage and AWS, including S3. The use case described involves a situation where the server generating the documents can't yet support the S3 API directly, and DataSync can bridge that gap by synchronizing the files between the EC2 instance (where the server can have fast local access) and S3.\n\nThe other solutions require more changes and more setup than option D, which simply requires setting up the DataSync task. The requirement is to fulfill the needs with the least amount of effort, making option D the best answer.","comment_id":"907653","poster":"Bwitch"},{"content":"Selected Answer: B\nAlthough the solution described in C is possible, there is an error at the end mentioning that you would mount the FSX using NFS which is not supported.\nhttps://aws.amazon.com/blogs/storage/persistent-storage-for-high-performance-workloads-using-amazon-fsx-for-lustre/\nhttps://d2908q01vomqb2.cloudfront.net/e1822db470e60d090affd0956d743cb0e7cdf113/2020/04/24/FSx-for-Lustre-persistent-storage-diagram.png\nMount the file system you created as shown below:\n$ sudo mount -t lustre -o noatime,flock file_system_dns_name@tcp:/mountname /fsx","poster":"rbm2023","comment_id":"904056","timestamp":"1684760520.0","upvote_count":"3"},{"timestamp":"1684535520.0","poster":"Jesuisleon","upvote_count":"4","comment_id":"902249","content":"Selected Answer: B\nC is wrong, amazon FSx for luster does NOT suport nfs file system!"},{"upvote_count":"3","timestamp":"1684398960.0","comment_id":"900938","content":"Selected Answer: B\nAmazon FSx for Lustre cannot be mounted using NFS so the only choice is B","poster":"intp75"},{"poster":"SVGoogle89","timestamp":"1684252380.0","content":"C\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/supported-clients-fsx.html","upvote_count":"1","comment_id":"899321"},{"upvote_count":"3","content":"Selected Answer: D\nGuys, the question is \"with the LEAST amount of effort\" and the AWS DataSync is designed for data transfer and synchronization between different storage services, including EC2 instances and S3, and would provide more efficient and automated data movement for this use case, ensuring the files are available on S3 within the required 30 minutes with less operational overhead.\nIs it doesn't least effort than the others?\nI'll go with D","timestamp":"1683943260.0","poster":"momo3321","comment_id":"896364"},{"poster":"2aldous","content":"Selected Answer: B\nB is the answer:\nFSx Lustre support not support NFS. Both are different protocols.","timestamp":"1683574980.0","upvote_count":"2","comment_id":"892477"},{"timestamp":"1679905500.0","upvote_count":"2","content":"Selected Answer: B\nI think b is a better choice its far easier to implement.","comment_id":"851924","poster":"mfsec"},{"poster":"Arnaud92","upvote_count":"2","content":"I think it's B.\nC can be a good solution but at the end of the answer, there is a mention to mount with NFS. Lustre is not using NFS. So I'll go for B","comment_id":"848553","timestamp":"1679596380.0","comments":[{"poster":"marcoforexam","content":"I agree with this.\nFSx is going to be mounted as ”Lusture\"\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/mount-fs-auto-mount-onreboot.html","comment_id":"850192","timestamp":"1679754120.0","upvote_count":"1"}]},{"poster":"God_Is_Love","content":"Selected Answer: C\nI choose C between B and C after reading the feature of seamless linking the S3 file system with Lustre - https://aws.amazon.com/fsx/lustre/","timestamp":"1678848900.0","upvote_count":"1","comment_id":"839517"},{"comments":[{"timestamp":"1678848120.0","content":"Above may be wrong-\nOption C could be the solution that will meet the requirements with the least amount of effort. By linking the new file system to an S3 bucket, the company can store, retrieve, and modify documents with fast local access. The Lustre client can be installed on an Amazon EC2 instance, which will provide the processing server with the fast local access it requires. This solution requires minimal changes to the processing server and will enable the files to be available to the public for download within 30 minutes.\n\nOption B may not be a good solution because it involves setting up an S3 File Gateway and configuring a file share, which would require additional infrastructure and configuration. This solution also involves using NFS, which may not provide the level of performance required for the processing server.","comment_id":"839506","poster":"God_Is_Love","upvote_count":"5"}],"upvote_count":"2","content":"Selected Answer: B\nOption C (Configure Amazon FSx for Lustre with an import and export policy) may be overkill for the given workload and may require additional management overhead, resulting in additional effort and cost.On the other hand, Option B (Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store) provides a straightforward solution that requires minimal changes to the processing server. By using the S3 File Gateway, the processing server can continue to generate and modify files on the local file system, and changes can be automatically synced to S3 via the gateway. Customers can then access the files directly from S3 within 30 minutes.","timestamp":"1678847400.0","comment_id":"839497","poster":"God_Is_Love"},{"comment_id":"834453","timestamp":"1678398240.0","content":"Selected Answer: C\nhttps://aws.amazon.com/fsx/lustre/","poster":"kiran15789","upvote_count":"2","comments":[{"upvote_count":"1","poster":"PSPaul","content":"I support C\nFor this scenario, question say Server (Linux) require fast local storage.\nThis Server (Linux) as a Linux Client, can work with Lystre","timestamp":"1683259020.0","comment_id":"889770"}]},{"upvote_count":"1","comment_id":"829422","poster":"Sarutobi","content":"Selected Answer: B\nIf I understand the question correctly, the processing server needs direct access to the files and is located on-premises. In this case, you would need to use a file storage solution that provides local access to the files from the processing server.","timestamp":"1677964860.0"},{"upvote_count":"1","timestamp":"1677120720.0","comments":[{"comment_id":"877802","timestamp":"1682218620.0","content":"Change to B. Least amount of effort and it supports NFS which Lustre does not.","upvote_count":"1","poster":"Yowie351"}],"poster":"Yowie351","comment_id":"818708","content":"Selected Answer: C\nFast access and linking to S3. Answer is C."},{"timestamp":"1676902800.0","poster":"Musk","comment_id":"815357","upvote_count":"5","content":"D might be leasat amount of effort..."}],"timestamp":"2023-02-02 19:43:00","question_id":74,"topic":"1"},{"id":"1219hWgNnkrctAfs0yNu","choices":{"C":"Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.","D":"Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table.","B":"Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.","A":"Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table."},"discussion":[{"content":"Selected Answer: A\nThe trigger is that the central user service deletes a user in the DynamoDB table. The DynamoDB Streams meets the requirement.\nhttps://aws.amazon.com/blogs/database/how-to-perform-ordered-data-replication-between-applications-by-using-amazon-dynamodb-streams/\nOption B is wrong. There is no feature named DynamoDB event notifications.","timestamp":"1676017140.0","upvote_count":"17","comments":[{"poster":"Amac1979","content":"Correct, the point they want to make is central user service is system of record. You should not be deleting from other services until you delete from DynamoDB.","upvote_count":"1","timestamp":"1680012600.0","comment_id":"853331"},{"timestamp":"1693328520.0","comments":[{"timestamp":"1701165720.0","content":"You can have many consumers which means any of the consumers can receive and process the message.","comment_id":"1082389","poster":"jainparag1","upvote_count":"5"}],"upvote_count":"3","poster":"kjcncjek","comment_id":"993321","content":"how can you use 1 sqs queue for all microservises?"}],"poster":"Untamables","comment_id":"804114"},{"upvote_count":"17","content":"Selected Answer: C\nC seems correct; SQS is one queue to one microservice, could not find anything on dynamodb event notifications.","timestamp":"1676458380.0","poster":"CloudFloater","comment_id":"809388"},{"content":"it is MUCH MORE faster to send the event through eventbridge to microservices once the event of deletion needs to happen. I first read option A and thought it was the right one but have the microservices polling SQS QUEUE is less performant than the other one. \nAND it's impossible for SQS to have multiple consumers as this is not the main purpose of this service, this is not a fan-out architecture with SQS and SNS.\nTotally sure that C is the correct answer, I repeat, I thought it was A but it's not.","upvote_count":"1","comment_id":"1356556","poster":"juanife","timestamp":"1739557980.0"},{"content":"Selected Answer: C\nC, The problem with A the SQS solution ist that the \"other microservices which stores data chunks seperatly\". We do not know how many services are storing the userdata, and with SQS we would have one message on the queue which is processed by one of these microservices. how could the other microservices know that they have to delete the data when the message is allready consumed and processed?","poster":"chris_spencer","timestamp":"1728484320.0","comment_id":"1295195","upvote_count":"1"},{"poster":"ry1999","upvote_count":"3","timestamp":"1725757560.0","content":"Selected Answer: C\nSQS does not have a fan-out capability. You need SNS --> SQS to achieve the microservices to be notified. Hence A is incorrect and C is correct.","comment_id":"1280161"},{"comment_id":"1167924","upvote_count":"4","content":"Selected Answer: C\nA is not viable since SQS is not used in a fan-out situation.\nB is not viable since there's no such thing as \"DynamoDB event notifications\".\nC is viable.\nD is not viable, again due to the fact that SQS is not used for fan-out.","poster":"Dgix","timestamp":"1709809920.0"},{"content":"Selected Answer: C\nThis is tricky question. C seems to be best and feasible. Rest options are not correct as they are using SQS where messages can be delivered only to one reader while in this scenario there are multiple microservices that needs to read the same message and delete the user information.","poster":"career360guru","upvote_count":"4","timestamp":"1703457180.0","comment_id":"1104890"},{"comment_id":"1103477","content":"Lets Ignore the insanity of \nSeveral other microservices store in ---- different storage services. ------\ncentral user service deletes a user, every other microservice must\nalso delete its copy of the data immediately. \nYET ALL the options attempt a delete in the OG DynamoDB\nYeah OK Whatever Blue is green and Red is Orange these days.\nBTW ans. == C , A will work but why poll SQS when Evt Brdg can invoke Microservice.\nPersonally I'd invoke a lambda to delete related records from the disparate data sources per KeyId and not bother the services but I'm not Architecting this mess maybe they want a clean log trail of the delete process as invoked by central user service whatever","poster":"CProgrammer","comments":[{"content":"Agreed. If this is an actual exam question, I am concerned about the intellect of the exam writers.","poster":"dankositzke","comment_id":"1148605","timestamp":"1707769620.0","upvote_count":"4"}],"upvote_count":"4","timestamp":"1703261040.0"},{"timestamp":"1702575960.0","comment_id":"1096700","poster":"Bad_Mat","upvote_count":"3","content":"I vote for C because the question says: Delete the user IMMEDIATELY\nA and D use SQS and messages in SQS can stay a pretty long time"},{"comments":[{"comment_id":"1082383","content":"this is for Q165,","poster":"jainparag1","upvote_count":"2","timestamp":"1701165540.0"}],"comment_id":"1082364","upvote_count":"1","poster":"jainparag1","content":"Selected Answer: C\nAmazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Powered by Lustre, the world's most popular high-performance file system, FSx for Lustre offers shared storage with sub-ms latencies, up to terabytes per second of throughput, and millions of IOPS. FSx for Lustre file systems can also be linked to Amazon Simple Storage Service (S3) buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API.","timestamp":"1701164820.0"},{"timestamp":"1700890320.0","comment_id":"1079753","poster":"career360guru","content":"Selected Answer: C\nOption C","upvote_count":"1"},{"comment_id":"1009163","content":"Selected Answer: A\nhttps://aws.amazon.com/vi/getting-started/hands-on/send-fanout-event-notifications/?nc1=f_ls","poster":"vjp_training","timestamp":"1694876640.0","upvote_count":"2"},{"comment_id":"990578","timestamp":"1693033500.0","upvote_count":"4","content":"A real-world use case utterly destroyed with some of the worst possible options for solutions.\nSimplest solution is to have the interested parties consume events off the DynamoDB streams and delete the user information in their respective datastores. Too many red herrings in the options given, and the only relatively sane one of the lot is Option C.\nThe bar for coming up with questions with SA professional keeps getting lowered.","poster":"Ganshank"},{"content":"Selected Answer: A\nEvent trigger from DynamoDb -- Choose DynamoDb Streams","upvote_count":"2","timestamp":"1692469920.0","poster":"SK_Tyagi","comment_id":"985369"},{"timestamp":"1692354960.0","comment_id":"984383","upvote_count":"3","poster":"xav1er","content":"Where the hell is fan-out pattern? stupid answers ..."},{"upvote_count":"1","content":"* The central user service stores sensitive data in an Amazon DynamoDB table. \n* Several of the other microservices store a copy of parts of the sensitive data in different storage services.\n\nApparently only the central user service stores user data in DynamoDB. The others use \"different storage services\". Yet, all of the answers focus on DynamoDB...","poster":"aviathor","timestamp":"1692252360.0","comment_id":"983274"},{"upvote_count":"2","comments":[{"upvote_count":"2","comment_id":"983260","comments":[{"poster":"kjcncjek","comment_id":"993322","upvote_count":"1","timestamp":"1693328580.0","content":"it should be SQSs but all answers indicates only 1 queue"},{"poster":"aviathor","comment_id":"983272","upvote_count":"1","content":"Instead of configuring multiple EventBridge rules, there could be multiple SQS streams :)","timestamp":"1692252180.0"}],"content":"Can you seriously mean one should \"Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.\"???\n\nWhy not SQS?","poster":"aviathor","timestamp":"1692251700.0"}],"comment_id":"942985","content":"Selected Answer: C\nC\nA would be preferable with SNS instead of SQS","poster":"NikkyDicky","timestamp":"1688487540.0"},{"timestamp":"1687544400.0","upvote_count":"3","content":"Selected Answer: C\nNo matter how I would like to use the native DynamoDB services, option A and B have some major issues - A and D expects SQS to be used by several microservices, which is not really what the service is supposed to do. B seems like a nice scenario, however, there isn't something like \"DynamoDB event notifications\". So we leave with option C","poster":"Maria2023","comment_id":"931857"},{"comment_id":"931071","content":"Selected Answer: C\nThe solution that will meet the requirements of immediately deleting user information across all microservices is:\n\nC. Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.\nIn this case, you can create an EventBridge rule for each microservice to match the user deletion event pattern and invoke the logic in the microservice to delete the corresponding user data from their respective storage services, including the DynamoDB table.","poster":"Alabi","timestamp":"1687470000.0","upvote_count":"2"},{"poster":"EricZhang","comment_id":"929284","upvote_count":"1","content":"does DynamoDB event notifications exist?","timestamp":"1687339740.0"},{"comment_id":"908800","timestamp":"1685298000.0","content":"Answer : C","poster":"Roontha","upvote_count":"1"},{"timestamp":"1684762860.0","poster":"rbm2023","upvote_count":"2","comments":[{"timestamp":"1692252120.0","upvote_count":"2","poster":"aviathor","comment_id":"983269","content":"It also does not talk about deleting the sensitive information stored elsewhere than in DynamoDB, which I assume from the question also pertains to users"}],"content":"Selected Answer: C\nI would work on this solution using streams but option A is incorrect since the stream would only occur after the deletion of the user and the option states at the end that the user would be deleted from Dynamo table, so this is in an incorrect order.\nThis is also a nice article using event bridge with a real world case.\nhttps://medium.com/aws-serverless-microservices-with-patterns-best/aws-event-driven-serverless-microservices-using-aws-lambda-api-gateway-eventbridge-sqs-dynamodb-a7f46220b738","comment_id":"904096"},{"content":"Selected Answer: C\nThis is a tricky one because you could definitely do this using DynamoDB Streams, as that would be the right tool for the job. But the question is trying to trick you because the second half of that answer is to use SQS which would not work when you have multiple consumers, each message can only be consumed once.\nTherefore the answer has to be C","comments":[{"upvote_count":"2","timestamp":"1684762620.0","poster":"rbm2023","content":"I agree, with the addition to the fact that every single option states that the final step is to delete the record from Dynamo, by using streams, the deletion would already occur, making option A incorrect.","comment_id":"904090"}],"comment_id":"875706","timestamp":"1682000940.0","poster":"DWsk","upvote_count":"8"},{"upvote_count":"3","content":"Selected Answer: C\nThe recommended solution is to configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Each microservice can then create an EventBridge rule that matches the user deletion event pattern and invokes logic in the microservice to delete the user from the DynamoDB table. This solution provides an easy way to coordinate between microservices and ensures that every microservice deletes its copy of the data immediately upon user deletion from the central user service. It also requires minimal effort as there is no need to set up additional services or infrastructure.","poster":"birbyne","comment_id":"859004","timestamp":"1680451140.0"},{"comment_id":"857900","poster":"Asagumo","upvote_count":"2","content":"Selected Answer: C\nSensitive user information must be deleted immediately. There is no assumption that the time required to poll the SQS queue is acceptable for this mechanism.","timestamp":"1680352860.0"},{"upvote_count":"1","timestamp":"1679906040.0","poster":"mfsec","comment_id":"851930","content":"Selected Answer: A\nI think A is the best fit here due to the phrasing of the question around who is deleting users."},{"poster":"taer","upvote_count":"1","timestamp":"1679122500.0","comment_id":"842531","content":"Selected Answer: C\nC. Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table."},{"content":"Selected Answer: A\nC,D are wrong. Flaw is \"Company deletes user? \" Nope, Its central service does deletion.\nB is wrong because, there is no such thing as DynamoDB events feature, Only Streams can enable CRUD operations and eventually could trigger lambda, Refer here : https://stackoverflow.com/questions/53857304/how-to-get-notified-when-a-aws-dynamo-db-entry-are-updated\nhttps://dynobase.dev/dynamodb-triggers/","comment_id":"839557","timestamp":"1678857240.0","poster":"God_Is_Love","upvote_count":"2"},{"upvote_count":"2","poster":"kiran15789","comment_id":"834458","timestamp":"1678398480.0","content":"Selected Answer: C\nA and B require changes to all the existing microservices"},{"poster":"Sarutobi","timestamp":"1678066980.0","content":"Selected Answer: B\nI believe the question is looking for the fan-out pattern. Basically is one event that notify multiple endpoints. SQS cannot be used for that because only one micro-service in this case will get the message and the rest will never delete the user. EventBridge could potentially do it too but you need to targets all micro services. The method I see used commonly for this is SNS. I don't like how B is written, in theory we can do DynamoDB Streams=>Lambda=>SNS=>micro-services.","comment_id":"830485","upvote_count":"4","comments":[{"content":"I changed my mind here to C after carefully reading this question again. It also made me think about a few apps we have using this model, the route trough using SNS is the \"old way\" now that event bridge exists. Also because how the exam work there is no direct way to trigger SNS from DynamoDB, we need to use Lambda for it so B is not technically correct.","comment_id":"899160","timestamp":"1684238700.0","poster":"Sarutobi","upvote_count":"1"}]},{"content":"Selected Answer: C\nThis solution doesnt require continous polling of the queue","timestamp":"1677442740.0","upvote_count":"3","comment_id":"822926","poster":"kiran15789"},{"timestamp":"1676530800.0","upvote_count":"2","content":"Selected Answer: A\nDynamoDB stream can trigger the change of specific item, so there is no noise.","poster":"masssa","comment_id":"810340"},{"comment_id":"808336","timestamp":"1676376000.0","poster":"spd","content":"Selected Answer: C\nC looks correct - EventBridge rule has the ability to inform multiple services","upvote_count":"4"},{"content":"Selected Answer: C\nC seems correct, the others don't. The ones that refer to SQS would not work because the SQS notification can be consumed by a single microservice. With A, you would be getting all changes through DynamoDB Streams, which is too much noise.","comment_id":"805487","timestamp":"1676138220.0","upvote_count":"4","poster":"Musk"},{"timestamp":"1676136480.0","content":"SQS delivers the queue item to only one service. Only SNS can do it.","upvote_count":"2","poster":"Ilk","comment_id":"805462"},{"comment_id":"799122","poster":"harleydog","content":"We can't use a queue, the message will only get processed once. I don't think we can generate event notifications on DynamoDB table updates.","timestamp":"1675626300.0","comments":[{"comment_id":"841838","poster":"aqiao","timestamp":"1679044620.0","content":"So the only option is C","upvote_count":"1"}],"upvote_count":"4"}],"topic":"1","exam_id":33,"question_id":75,"timestamp":"2023-02-05 20:45:00","url":"https://www.examtopics.com/discussions/amazon/view/98107-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","answers_community":["C (69%)","A (27%)","4%"],"answer":"C","isMC":true,"question_text":"A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.\n\nThe company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.\n\nWhich solution will meet these requirements?","unix_timestamp":1675626300,"answer_ET":"C","answer_images":[],"question_images":[]}],"exam":{"numberOfQuestions":529,"isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"AWS Certified Solutions Architect - Professional SAP-C02","id":33,"isBeta":false},"currentPage":15},"__N_SSP":true}