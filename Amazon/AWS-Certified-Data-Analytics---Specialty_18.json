{"pageProps":{"questions":[{"id":"uE1OmicpswScXI8KhqQp","answer":"B","answer_description":"","question_images":[],"timestamp":"2020-08-16 07:07:00","discussion":[{"content":"It is B based on this link: \nhttps://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html","comment_id":"158993","upvote_count":"26","poster":"jersyl","comments":[{"comment_id":"478507","poster":"attaraya","timestamp":"1636960860.0","upvote_count":"1","content":"Agreed: more reference \nhttps://docs.aws.amazon.com/quicksight/latest/user/how-does-rcf-generate-forecasts.html"}],"timestamp":"1632549660.0"},{"poster":"Ali_Hussein","timestamp":"1692042600.0","upvote_count":"1","content":"Selected Answer: C\nThe correct answer is C.\n\nHere is the explanation:\n\n Use a pre-build ML AMI from the AWS Marketplace to create forecasts and then use Amazon QuickSight to visualize the data. This is the most out-of-the-box solution and will require the least amount of management overhead.\n AWS Glue ML transforms are a great way to automate ML tasks, but they require some technical expertise to set up and use.\n Amazon QuickSight is a great visualization tool, but it does not have built-in ML capabilities.\n Calculated fields are a way to create new fields in a data set, but they cannot be used to create forecasts.","comment_id":"981070"},{"timestamp":"1691056980.0","content":"Selected Answer: B\nQuicksight supports RCF and can connect easily to multiple data sources (S3, JDBC ..etc)","upvote_count":"1","comment_id":"970969","poster":"MLCL"},{"timestamp":"1684765980.0","poster":"Espa","comment_id":"904137","upvote_count":"1","content":"Selected Answer: B\nQuicksight uses a built-in version of RCF"},{"comments":[{"timestamp":"1715568660.0","poster":"okrasheno","upvote_count":"1","comment_id":"1210681","content":"Dude is a legend now"},{"comment_id":"1091927","content":"It's very nice to know!! No one noticed","upvote_count":"1","timestamp":"1702141320.0","poster":"yazquez"}],"timestamp":"1682947800.0","content":"B: I passed the test","comment_id":"886301","poster":"pk349","upvote_count":"1"},{"timestamp":"1682611200.0","comment_id":"882789","poster":"uk_dataguy","content":"this scenario is shocking to see because the team is all non-technical ....","upvote_count":"2"},{"timestamp":"1678637760.0","upvote_count":"1","poster":"AwsNewPeople","comment_id":"837202","content":"Selected Answer: B\nBoth options B and C involve using Amazon QuickSight to visualize the data. However, option C involves using a pre-built machine learning Amazon Machine Image (AMI) from the AWS Marketplace to create forecasts, which may require more technical expertise to set up and manage than option B, which simply involves using ML-powered forecasting within Amazon QuickSight. Therefore, option B may be more suitable for a non-technical team looking for an out-of-the-box solution with minimal management overhead."},{"comment_id":"822011","content":"Amazon QuickSight uses a built-in version of the Random Cut Forest (RCF) algorithm. \n\nB is correct","poster":"rags140882","upvote_count":"1","timestamp":"1677378600.0"},{"content":"Option C is the best solution for this scenario. The company wants an out-of-the-box solution that requires the least amount of management overhead, and using a pre-built ML AMI from the AWS Marketplace to create forecasts and then using Amazon QuickSight to visualize the data is the most straightforward approach. The pre-built ML AMI will provide the Random Cut Forest algorithm for the team to use, and Amazon QuickSight provides an easy-to-use interface for data visualization. This solution will require minimal technical expertise and management overhead from the non-technical team.\n\nOption B is not the best solution as using ML-powered forecasting in Amazon QuickSight does not provide the Random Cut Forest algorithm that the company wants to use.","comments":[],"comment_id":"822009","timestamp":"1677378480.0","upvote_count":"1","poster":"rags140882"},{"poster":"renfdo","content":"Selected Answer: B\nB, Quicksigth has many ML tools.","upvote_count":"2","timestamp":"1670755920.0","comment_id":"741611"},{"comment_id":"711802","upvote_count":"2","poster":"cloudlearnerhere","content":"Correct answer is B as QuickSight ML provides an out-of-the-box ML Random Cut Forest (RCF) algorithm to help visualize complex real-world scenarios, such as detecting seasonality and trends, excluding outliers, and imputing missing values.\n\nOptions A, C & D are wrong as they do not come with the least operational overhead.","timestamp":"1667657820.0"},{"poster":"nharaz","timestamp":"1667204040.0","content":"B is correct.\nAmazon QuickSight uses a built-in version of the Random Cut Forest (RCF) algorithm. The following sections explain what that means and how it is used in Amazon QuickSight.\n\nFirst, let's look at some of the terminology involved:\n\n Anomaly – Something that is characterized by its difference from the majority of the other things in the same sample. Also known as an outlier, an exception, a deviation, and so on.\n\n Data point – A discrete unit—or simply put, a row—in a dataset. However, a row can have multiple data points if you use a measure over different dimensions.\n\n Decision Tree – A way of visualizing the decision process of the algorithm that evaluates patterns in the data.\n\n Forecast – A prediction of future behavior based on current and past behavior.\n\n Model – A mathematical representation of the algorithm or what the algorithm learns.\n\n Seasonality – The repeating patterns of behavior that occur cyclically in time series data.\n\n Time series – An ordered set of date or time data in one field or column.","comment_id":"708284","upvote_count":"3"},{"comment_id":"704411","timestamp":"1666762020.0","content":"Selected Answer: B\nB. Amazon QuickSight enables nontechnical users to confidently forecast their key business metrics. The built-in ML Random Cut Forest algorithm automatically handles complex real-world scenarios such as detecting seasonality and trends, excluding outliers, and imputing missing values. You can interact with the data with point-and-click simplicity.","poster":"thirukudil","upvote_count":"2"},{"poster":"muhsin","comment_id":"645643","content":"it is B\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/concept-of-ml-algorithms.html","upvote_count":"1","timestamp":"1660261440.0"},{"comment_id":"634926","upvote_count":"1","timestamp":"1658452500.0","content":"Selected Answer: B\nB is the answer.","poster":"rocky48"},{"comment_id":"605246","timestamp":"1653205200.0","content":"Selected Answer: B\nAnswer should be B","upvote_count":"1","poster":"Bik000"},{"upvote_count":"1","comment_id":"597604","timestamp":"1651821000.0","comments":[{"upvote_count":"1","timestamp":"1666086180.0","comment_id":"698091","poster":"JoellaLi","content":"Yes of course. \"The built-in ML Random Cut Forest algorithm automatically handles complex real-world scenarios such as detecting seasonality and trends, excluding outliers, and imputing missing values. You can interact with the data with point-and-click simplicity.\"\n\nLink: https://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html"}],"content":"Selected Answer: B\nAggree with B.\nBut one problem is, the question mentions about \"inputing missing data\". Can quicksight handle missing data for ML related visualization?","poster":"MWL"},{"timestamp":"1651354380.0","poster":"jrheen","comment_id":"595303","upvote_count":"1","content":"Answer - B"},{"timestamp":"1639102980.0","poster":"MjRiddle","content":"Selected Answer: B\nB is the answer with no overheard as the team is non-tech","upvote_count":"2","comment_id":"498228"},{"timestamp":"1637382420.0","poster":"aws2019","comment_id":"482260","content":"B is the right answer","upvote_count":"1"},{"poster":"Donell","content":"Answer B. Use Amazon QuickSight to visualize the data and then use ML-powered forecasting to forecast the key business metrics.","upvote_count":"3","comment_id":"386299","timestamp":"1635971520.0"},{"poster":"AjithkumarSL","content":"Looks like B is the correct Option for Out of the Box Solution..","upvote_count":"2","comment_id":"348348","timestamp":"1634582640.0"},{"upvote_count":"4","timestamp":"1634182380.0","poster":"lostsoul07","comment_id":"274282","content":"B is the right answer"},{"poster":"BillyC","comment_id":"216779","timestamp":"1634113680.0","content":"B is correct!","upvote_count":"2"},{"upvote_count":"4","content":"Answer is B; QuickSight has RCF integrated into it\nhttps://docs.aws.amazon.com/quicksight/latest/user/what-is-random-cut-forest.html","poster":"syu31svc","timestamp":"1633820340.0","comment_id":"191547"},{"upvote_count":"4","content":"I will go with option B.","comment_id":"175526","timestamp":"1633496520.0","poster":"Paitan"},{"comment_id":"159235","content":"B. See:\nhttps://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html","upvote_count":"2","timestamp":"1632741600.0","poster":"zanhsieh"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/28696-exam-aws-certified-data-analytics-specialty-topic-1-question/","unix_timestamp":1597554420,"answer_ET":"B","question_id":86,"exam_id":20,"topic":"1","question_text":"A company wants to use an automatic machine learning (ML) Random Cut Forest (RCF) algorithm to visualize complex real-world scenarios, such as detecting seasonality and trends, excluding outers, and imputing missing values.\nThe team working on this project is non-technical and is looking for an out-of-the-box solution that will require the LEAST amount of management overhead.\nWhich solution will meet these requirements?","answers_community":["B (92%)","8%"],"isMC":true,"choices":{"B":"Use Amazon QuickSight to visualize the data and then use ML-powered forecasting to forecast the key business metrics.","A":"Use an AWS Glue ML transform to create a forecast and then use Amazon QuickSight to visualize the data.","C":"Use a pre-build ML AMI from the AWS Marketplace to create forecasts and then use Amazon QuickSight to visualize the data.","D":"Use calculated fields to create a new forecast and then use Amazon QuickSight to visualize the data."}},{"id":"0moBISLMcexrsQ8AwYeC","isMC":true,"answer_description":"","timestamp":"2020-08-16 01:40:00","answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/28683-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1","discussion":[{"upvote_count":"37","timestamp":"1632552780.0","poster":"Priyanka_01","content":"D any thoughts?\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","comments":[{"content":"Agree\nQuick tip - row level security only available in Enterprise Edition","upvote_count":"3","comment_id":"505877","poster":"lakediver","timestamp":"1640070180.0"},{"upvote_count":"2","comment_id":"1119165","timestamp":"1704931680.0","content":"agree, this question appearedin my test","poster":"GCPereira"}],"comment_id":"158893"},{"content":"Ans D\nThis is a textbook question.\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","timestamp":"1635889620.0","comment_id":"383913","upvote_count":"7","poster":"Shraddha"},{"content":"Selected Answer: D\nOptions A and B involve managing access at the S3 level, which does not directly apply to controlling visibility within QuickSight dashboards. \nOption C, creating a manifest file, is part of how you might structure data for QuickSight, but it does not directly address row-level security within QuickSight itself.\n\nOption D is correct - creating dataset rules with row-level security within Amazon QuickSight is the most effective and governance-compliant method to ensure that product owners have access only to their respective product analysis in the dashboard reports.","comment_id":"1155457","upvote_count":"1","poster":"NarenKA","timestamp":"1708513980.0"},{"comment_id":"886306","timestamp":"1682947920.0","content":"D: I passed the test","poster":"pk349","upvote_count":"2","comments":[{"poster":"kondi2309","timestamp":"1707809100.0","comment_id":"1148952","content":"agree with no doubt","upvote_count":"1"}]},{"poster":"uk_dataguy","timestamp":"1682611320.0","content":"Selected Answer: D\nD without any doubt. \nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","upvote_count":"1","comment_id":"882794"},{"poster":"AwsNewPeople","upvote_count":"4","comments":[{"poster":"[Removed]","comment_id":"868381","upvote_count":"1","timestamp":"1681304400.0","content":"Hey man thanks for you answers across threads really helped me a lot, could you let us know if you have taken the exam or any tips & tricks"}],"timestamp":"1678637940.0","comment_id":"837207","content":"Selected Answer: D\nD. Create dataset rules with row-level security would be the best approach for this use case. Row-level security (RLS) allows you to define filters at the row level, which can be used to control access to specific data based on user attributes or permissions. This would enable the data analytics team to ensure that each product owner can only see the data for their respective products, as the filters would be applied to the dashboard data before it is displayed to the user. Option A and B would not provide the necessary granularity to restrict access to specific data based on the product owners, and option C would not be applicable in this case as it is used for securing data in Amazon Redshift clusters."},{"upvote_count":"1","content":"i think D","comment_id":"729907","timestamp":"1669697100.0","poster":"dhyuk"},{"poster":"cloudlearnerhere","upvote_count":"3","comment_id":"711805","content":"Selected Answer: D\nCorrect answer is D as dataset rules with row-level security can be used to restrict the data the product owners would see, which is based on the product.\n\nOptions A & B are wrong as they would not provide fine-grained access control and would need extra effort.\n\n\nOption C is wrong as the row-level security rules need to be defined in the dataset and not in the manifest file.","timestamp":"1667658060.0"},{"upvote_count":"3","poster":"nharaz","content":"D is correct\nIn the Enterprise edition of Amazon QuickSight, you can restrict access to a dataset by configuring row-level security (RLS) on it. You can do this before or after you have shared the dataset. When you share a dataset with RLS with dataset owners, they can still see all the data. When you share it with readers, however, they can only see the data restricted by the permission dataset rules. By adding row-level security, you can further control their access.","timestamp":"1667204220.0","comment_id":"708286"},{"comment_id":"704422","timestamp":"1666762860.0","poster":"thirukudil","upvote_count":"2","content":"Selected Answer: D\nIn the Enterprise edition of Amazon QuickSight, you can restrict access to a dataset by configuring row-level security (RLS) on it. You can do this before or after you have shared the dataset. When you share a dataset with RLS with dataset owners, they can still see all the data. When you share it with readers, however, they can only see the data restricted by the permission dataset rules. By adding row-level security, you can further control their access."},{"content":"Selected Answer: D\nIt should be done through dataset rules","comment_id":"678441","poster":"Arka_01","upvote_count":"1","timestamp":"1664082240.0"},{"comment_id":"634942","upvote_count":"2","timestamp":"1658455020.0","poster":"rocky48","comments":[{"content":"https://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","poster":"rocky48","upvote_count":"1","comment_id":"668259","timestamp":"1663088220.0"}],"content":"Selected Answer: D\nAnswer-D"},{"poster":"Ahamedkabir","comment_id":"623844","upvote_count":"1","content":"Selected Answer: B\nFor sure it is right answer","timestamp":"1656405180.0"},{"timestamp":"1653205020.0","poster":"Bik000","comment_id":"605241","content":"Selected Answer: D\nAnswer should be D","upvote_count":"1"},{"timestamp":"1653182460.0","content":"A is correct answer. Note that there are various dashboards for different products. And access needs to be provisioned to the product owners for respective products. Hence, ideal way is to create an IAM user for each product owner and use that user to access quickSight dashboard.","upvote_count":"1","comment_id":"605069","poster":"certificationJunkie"},{"upvote_count":"2","comment_id":"595317","timestamp":"1651356540.0","poster":"jrheen","content":"Answer-D"},{"poster":"Teraxs","timestamp":"1651216860.0","comment_id":"594282","content":"Selected Answer: D\nas mentions by others https://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","upvote_count":"2"},{"timestamp":"1637446980.0","comment_id":"482862","poster":"aws2019","upvote_count":"2","content":"D is the correct answer."},{"poster":"SuperSundra","comment_id":"337340","timestamp":"1635781320.0","upvote_count":"3","content":"This is D for sure. https://www.youtube.com/watch?v=W50Xr10hwbs"},{"timestamp":"1635671760.0","comment_id":"288204","upvote_count":"4","content":"Does D not assume we are using Enterprise Quicksight edition?\n\nB would work regardless....","poster":"brfc"},{"comment_id":"274283","timestamp":"1635264540.0","content":"D is the right answer","poster":"lostsoul07","upvote_count":"1"},{"poster":"BillyC","timestamp":"1634853300.0","content":"D is correct for me","upvote_count":"1","comment_id":"216776"},{"upvote_count":"1","content":"D is the correct answer.","comment_id":"207419","timestamp":"1634561040.0","poster":"jove"},{"upvote_count":"1","poster":"sanjaym","comment_id":"204850","content":"Answer should be B","timestamp":"1634227920.0"},{"comment_id":"195150","poster":"jack42","timestamp":"1633732380.0","upvote_count":"1","content":"The anaswer seems B to me, because row-level security only support with textual data types, here we have given average selling price which should have other data types as well. You can see why B is right choice-https://docs.aws.amazon.com/quicksight/latest/user/iam-policy-examples.html#security_iam_id-based-policy-examples-dashboards","comments":[{"upvote_count":"1","comment_id":"198804","timestamp":"1634188440.0","content":"It seems you are right:\n\"Row-level security only works for fields containing textual data (string, char, varchar, and so on). It doesn't currently work for dates or numeric fields\". They work on average price.\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","poster":"metin"},{"content":"You need to filter the data by product which can be a textual data. Clearly the correct answer is D.","comments":[{"timestamp":"1635092340.0","comments":[{"content":"it was not mentioned, i mean","comment_id":"225835","poster":"Othmanoof","upvote_count":"1","timestamp":"1635105900.0"}],"comment_id":"225833","content":"this is an assumption, it was mentioned in the question that the product is textual","upvote_count":"1","poster":"Othmanoof"},{"comment_id":"284765","content":"per product = textual data.","poster":"Magicroko","timestamp":"1635356100.0","upvote_count":"1"}],"comment_id":"207423","upvote_count":"5","timestamp":"1634692980.0","poster":"jove"},{"timestamp":"1661329680.0","poster":"Dun6","content":"That same link says \n\n“Or you can upload dataset rules from a text file or spreadsheet. If you are using a comma-separated value (CSV) file, don't include any spaces on the given line. Terms with spaces inside them need to be delimited with quotation marks. If you use dataset rules that are file-based, apply any changes by overwriting the existing rules in the dataset's permissions settings’","comment_id":"651152","upvote_count":"1"}]},{"timestamp":"1632760020.0","comment_id":"175527","poster":"Paitan","content":"D for sure.","upvote_count":"2"},{"comment_id":"159902","poster":"zeronine","timestamp":"1632698940.0","content":"I think D is the answer. https://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","upvote_count":"3"}],"question_images":[],"exam_id":20,"answer":"D","choices":{"B":"Separate the data by product and use IAM policies for authorization.","C":"Create a manifest file with row-level security.","A":"Separate the data by product and use S3 bucket policies for authorization.","D":"Create dataset rules with row-level security."},"unix_timestamp":1597534800,"answer_images":[],"question_id":87,"answers_community":["D (94%)","6%"],"question_text":"A retail company's data analytics team recently created multiple product sales analysis dashboards for the average selling price per product using Amazon\nQuickSight. The dashboards were created from .csv files uploaded to Amazon S3. The team is now planning to share the dashboards with the respective external product owners by creating individual users in Amazon QuickSight. For compliance and governance reasons, restricting access is a key requirement. The product owners should view only their respective product analysis in the dashboard reports.\nWhich approach should the data analytics team take to allow product owners to view only their products in the dashboard?"},{"id":"D3ODE76FybcoJXUdAUt1","topic":"1","question_images":[],"answer_ET":"D","answer":"D","answers_community":["D (70%)","B (30%)"],"timestamp":"2022-04-22 15:25:00","question_id":88,"url":"https://www.examtopics.com/discussions/amazon/view/74136-exam-aws-certified-data-analytics-specialty-topic-1-question/","choices":{"A":"Use Spot Instances for core and task nodes and a Reserved Instance for the EMR master node. Configure the EMR cluster with multiple master nodes. Schedule automated snapshots using Amazon EventBridge.","B":"Store the data on an EMR File System (EMRFS) instead of HDFS. Enable EMRFS consistent view. Create an EMR HBase cluster with multiple master nodes. Point the HBase root directory to an Amazon S3 bucket.","D":"Store the data on an EMR File System (EMRFS) instead of HDFS and enable EMRFS consistent view. Create a primary EMR HBase cluster with multiple master nodes. Create a secondary EMR HBase read-replica cluster in a separate Availability Zone. Point both clusters to the same HBase root directory in the same Amazon S3 bucket.","C":"Store the data on an EMR File System (EMRFS) instead of HDFS and enable EMRFS consistent view. Run two separate EMR clusters in two different Availability Zones. Point both clusters to the same HBase root directory in the same Amazon S3 bucket."},"isMC":true,"exam_id":20,"answer_images":[],"question_text":"A real estate company has a mission-critical application using Apache HBase in Amazon EMR. Amazon EMR is configured with a single master node. The company has over 5 TB of data stored on an Hadoop Distributed File System (HDFS). The company wants a cost-effective solution to make its HBase data highly available.\nWhich architectural pattern meets company's requirements?","unix_timestamp":1650633900,"answer_description":"","discussion":[{"content":"Selected Answer: D\nD is correct as Amazon EMR version 5.7.0 or later, you can set up a read-replica cluster, which allows you to maintain read-only copies of data in Amazon S3. In the event that the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations simultaneously.\n\nA is incorrect because using Spot EC2 instances for both of your core and task nodes could potentially cause downtime. Although this solution is the most cost-effective, it certainly doesn’t provide the highest availability for Amazon EMR.\n\nB is incorrect. While an EMR cluster with multiple master nodes can survive scenarios in which a primary master node fails, it is not, however, tolerant of Availability Zone failures.\n\nC is wrong as It's not possible for two primary clusters to be linked to the same root directory at the same time. Take note that only one active cluster at a time can use the same HBase root directory in Amazon S3. The best way to implement this is to launch a primary EMR cluster and a secondary (read-replica) EMR cluster, since using two primary clusters is not supported.","poster":"cloudlearnerhere","timestamp":"1667562480.0","upvote_count":"36","comments":[{"poster":"henom","upvote_count":"5","comment_id":"758078","timestamp":"1672107360.0","content":"The answer is D.\nUdemy course by Bonso has the same Logic."}],"comment_id":"711093"},{"comment_id":"625898","upvote_count":"14","timestamp":"1656727740.0","poster":"dushmantha","content":"Selected Answer: B\nIf we strictly want high availability then answer should be \"D\". But to be cost effective it only needs to go from current HDFS to S3, to make the data more available than before. Read replica is the next step if we want availability over master node crashes, etc. And it comes with additional cost. So I also suggest ans \"B\""},{"content":"Option D provides a robust and cost-effective solution that meets the company's requirements for making its HBase data highly available while leveraging Amazon EMR's capabilities effectively.","timestamp":"1725606360.0","upvote_count":"10","poster":"[Removed]","comment_id":"1279442"},{"comment_id":"1154776","upvote_count":"3","content":"Selected Answer: D\nOption D provides a robust and cost-effective solution that meets the company's requirements for making its HBase data highly available while leveraging Amazon EMR's capabilities effectively.","poster":"NarenKA","timestamp":"1708438620.0"},{"poster":"kondi2309","comment_id":"1150828","timestamp":"1707986940.0","content":"Selected Answer: D\nthe answer here is D.","upvote_count":"3"},{"content":"Selected Answer: D\nD because it requires HA deploying two different AZ","comment_id":"1149679","poster":"joselopezjm","upvote_count":"2","timestamp":"1707869580.0"},{"comment_id":"1043494","content":"Selected Answer: D\nas cloudlearnerhere explains \"\"D is correct as Amazon EMR version 5.7.0 or later, you can set up a read-replica cluster, which allows you to maintain read-only copies of data in Amazon S3. In the event that the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations simultaneously\"","timestamp":"1697291040.0","upvote_count":"3","poster":"gofavad926"},{"comment_id":"968512","upvote_count":"1","poster":"NikkyDicky","timestamp":"1690839420.0","content":"Selected Answer: B\nB for cost\nHbase data availability is satisfied by EMRFS"},{"comment_id":"886250","content":"D: I passed the test","comments":[{"comment_id":"1109013","poster":"Shaggy_98","content":"Did you clear using this dumps ?","upvote_count":"1","timestamp":"1703875920.0"}],"timestamp":"1682945880.0","upvote_count":"3","poster":"pk349"},{"content":"EMR is Single availability zone cluster which means we need to setup cluster in different avz for high availability. Two primary cluster is not an option. So answer is D","upvote_count":"2","poster":"anjuvinayan","comment_id":"876262","timestamp":"1682062740.0"},{"poster":"kozer","comment_id":"876020","upvote_count":"2","timestamp":"1682027400.0","content":"this recent aws documentation stateshttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-plan-consistent-view.html indicates consistent views are not supported and is not needed since 2020 . So yes D seems accurate or best answer but these questions are outdated and given how fast features change in AWS , this question certainly would be worded differently ."},{"poster":"bjmailbox","timestamp":"1680885780.0","upvote_count":"1","content":"Has to be option B , because it says HBASE data to be highly available which is already satisfied by EMRFS. It doesn't talk about cluster availability directly anywhere also considering the costs option D can be eliminated compared to B.","comment_id":"864082"},{"timestamp":"1680571260.0","poster":"rit25","content":"Just to tell you why not B.\n\nEnabling EMRFS consistent view and pointing the HBase root directory to an Amazon S3 bucket are two different concepts, but they are related in this scenario.\n\nEMRFS (EMR File System) is a file system interface that allows EMR clusters to access data stored in Amazon S3 in the same way as data stored on HDFS. By enabling EMRFS consistent view, EMR ensures that all nodes in the cluster see a consistent view of data stored in S3, which is important for applications like HBase that require strong consistency.\n\nOn the other hand, pointing the HBase root directory to an S3 bucket means that HBase tables and metadata are stored in S3, rather than on HDFS. This allows HBase to take advantage of the durability and scalability of S3, while still providing low-latency access to data.\n\nSo, in option B, the company is using both EMRFS and S3. EMRFS is used to provide a consistent view of data stored in S3, while HBase is configured to store its tables and metadata in S3.","upvote_count":"1","comment_id":"860523"},{"timestamp":"1678534560.0","poster":"AwsNewPeople","content":"D. Store the data on an EMR File System (EMRFS) instead of HDFS and enable EMRFS consistent view. Create a primary EMR HBase cluster with multiple master nodes. Create a secondary EMR HBase read-replica cluster in a separate Availability Zone. Point both clusters to the same HBase root directory in the same Amazon S3 bucket.\n\nThis solution provides a cost-effective way to make HBase data highly available by creating a primary EMR HBase cluster with multiple master nodes and a secondary EMR HBase read-replica cluster in a separate Availability Zone. By storing data on EMRFS and enabling EMRFS consistent view, both clusters can access the same data stored on an Amazon S3 bucket. This eliminates the need to store data redundantly and reduces costs. The use of multiple master nodes improves HBase availability and reliability. If the primary cluster fails, the secondary read-replica cluster can continue to serve read traffic.","comment_id":"835904","upvote_count":"3"},{"comment_id":"794706","timestamp":"1675206720.0","upvote_count":"3","content":"Selected Answer: D\nD based on Bonso Udemy Course","poster":"Matheus_Sampaio"},{"comment_id":"743619","content":"Selected Answer: B\nData highly available.\nD is not cost effective.","upvote_count":"1","timestamp":"1670909040.0","poster":"Vskvar"},{"poster":"Kako","comment_id":"696634","upvote_count":"2","content":"Selected Answer: D","timestamp":"1665963720.0"},{"upvote_count":"2","comment_id":"689777","comments":[{"timestamp":"1669965420.0","content":"Please share the link provided in your dumps. Just that its paid, does not mean it will provide correct answer. No offense.","upvote_count":"4","poster":"shubhary25","comment_id":"733530"}],"poster":"SandeepGun","content":"Selected Answer: D Confirmed from Paid dumps","timestamp":"1665274680.0"},{"timestamp":"1664522280.0","upvote_count":"1","poster":"rav009","comment_id":"683307","content":"Selected Answer: B\nAccoring to AWS doc, emr hbase high availablity=multi master nodes"},{"content":"-----------------------------------------------------------------------------------\nAnswer : C \n-----------------------------------------------------------------------------------","poster":"JHJHJHJHJ","comment_id":"679807","upvote_count":"1","timestamp":"1664200080.0"},{"comment_id":"678406","poster":"Arka_01","timestamp":"1664079480.0","upvote_count":"4","content":"Selected Answer: D\nTwo primary clusters cannot point to same S3, as root directory. One of them should be a read replica."},{"poster":"rocky48","content":"Selected Answer: B\nTo be cost effective, it only needs to go from current HDFS to S3. Read replica is required for availability over master node crashes, etc. And it comes with additional cost. So I also suggest Option \"B\"","timestamp":"1658212740.0","comment_id":"633400","upvote_count":"3"},{"poster":"arboles","timestamp":"1658042880.0","comment_id":"632489","upvote_count":"2","comments":[{"poster":"rocky48","comment_id":"633397","timestamp":"1658212260.0","content":"@arboles \nHow is option D more expensive solution when compared to option B ? \nIs it because of the secondary EMR HBase read-replica cluster in a separate AZ ??","upvote_count":"2"}],"content":"Selected Answer: B\ncost-efective one , so b, D is much more expensive and for hingh availanility"},{"comments":[{"poster":"rocky48","comment_id":"633399","content":"@Wesley27 The question is asking for a cost-effective solution, how is adding secondary EMR HBase read-replica cluster helping with staying cost-effective ?","upvote_count":"2","timestamp":"1658212560.0"}],"poster":"Wesley27","content":"Selected Answer: D\nThe HBase root directory is stored in Amazon S3, including HBase store files and table metadata. This data is persistent outside of the cluster, available across Amazon EC2 Availability Zones, and you don't need to recover using snapshots or other methods.\nSo B and C are out.\n\nYou can set up a read-replica cluster, which allows you to maintain read-only copies of data in Amazon S3. You can access the data from the read-replica cluster to perform read operations simultaneously, and in the event that the primary cluster becomes unavailable.\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hbase-s3.html\nSo I suggest answer D.","upvote_count":"5","comment_id":"629655","timestamp":"1657473480.0"},{"poster":"Dardiry","content":"Selected Answer: D\nUsing the Amazon EMR version 5.7.0 or later, you can set up a read-replica cluster, which allows you to maintain read-only copies of data in Amazon S3. In the event that the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations simultaneously.","upvote_count":"4","timestamp":"1654357680.0","comment_id":"611486"},{"timestamp":"1654250940.0","poster":"Ramshizzle","content":"I do see the allure of answer D as well. Adding a read replica does increase uninterrupted access to the cluster if the primary cluster becomes unavailable, thus increasing the availability. What still pushes me towards answer B is that this was also mentioned to be the correct answer in a very similar question I answered on ACloudGuru practice exam.","upvote_count":"1","comment_id":"611018"},{"upvote_count":"2","comment_id":"611014","content":"Selected Answer: B\nB is right answer. It specifically mentions to increase the availability of the HBase data. When the HBase data is in an S3 bucket it will be more available. Adding more clusters like answer C and D will not increase availability of the HBase data that is in S3.","timestamp":"1654250700.0","poster":"Ramshizzle"},{"upvote_count":"2","content":"B is right answer. We don't need D as there is no use of read replica whose main purpose is high read throughput. EMR clusters are always in same availability zones. So B should be it.","comment_id":"607946","timestamp":"1653633120.0","poster":"certificationJunkie"},{"content":"Selected Answer: B\nAvailability solved by Multi Master Node\nCost Effective, Hence Can Not create multiple cluster\nB allows data to be persistent outside of the cluster (in S3), available across Amazon EC2 Availability Zones.","timestamp":"1651474380.0","poster":"SengarAshu","upvote_count":"3","comment_id":"595923"},{"timestamp":"1651427460.0","poster":"rrshah83","upvote_count":"4","content":"Selected Answer: B\nScenario questions asks for cost-effective solution. B allows data to be persistent outside of the cluster (in S3), available across Amazon EC2 Availability Zones. Read replicas and multi-AZ cluster adds to cost.","comment_id":"595690"},{"upvote_count":"4","content":"Selected Answer: D\nD, see https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hbase-s3.html","timestamp":"1651042080.0","poster":"Teraxs","comment_id":"592921"},{"content":"Ans is C","timestamp":"1650932040.0","poster":"AWSRanger","comment_id":"592018","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: D\nD - create a replica that can act as primary if it fails","timestamp":"1650633900.0","comment_id":"590013","poster":"rb39"}]},{"id":"YNbmIWNoRzAfySBKRfdW","isMC":true,"answer_description":"","question_text":"A company has developed an Apache Hive script to batch process data stared in Amazon S3. The script needs to run once every day and store the output in\nAmazon S3. The company tested the script, and it completes within 30 minutes on a small local three-node cluster.\nWhich solution is the MOST cost-effective for scheduling and executing the script?","question_images":[],"discussion":[{"timestamp":"1633072680.0","poster":"carol1522","comments":[{"comment_id":"164791","content":"Agree with A","timestamp":"1633190940.0","poster":"awssp12345","upvote_count":"2"},{"poster":"Prodip","content":"Perfect Explanation ; I wanted to write something but your text covers everything.","timestamp":"1633254120.0","upvote_count":"2","comment_id":"167822"},{"upvote_count":"9","comment_id":"343965","poster":"chengxu32","content":"https://docs.aws.amazon.com/emr/latest/APIReference/API_RunJobFlow.html\nWith KeepJobFlowAliveWhenNoSteps parameter is set to False, the cluster will be shutdown once the steps are completed, thus the cost effective requirement is met","timestamp":"1635998880.0"}],"content":"For me it is A. Not B because we are not supposed to run core nodes in spot instances, just task nodes and it is more expensive because to schedule with oozie, our cluster have to be up all the time. It is not C because glue cannot run hive script, and it is not c because lambda cannot run hive scripts also. https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html","comment_id":"161721","upvote_count":"44"},{"content":"+ A is the correct answer. \n- B : Spot Instances are not a good option to run a 30-min-script\n- C: Glue cannot run Hive scripts\n- D: Lambda can run for 15 minutes maximum. Not enough time to run that script.","comments":[{"upvote_count":"4","content":"C: Glue cannot run Hive scripts---> Clue can run hive scripts. But the problem is that C keep all the Glue setting and does not terminate it.\nA By default, an Amazon EMR cluster will be terminated automatically when all steps have completed and there are no pending steps or other applications running on the cluster.","timestamp":"1680566100.0","comment_id":"860473","poster":"Bob888"}],"upvote_count":"8","comment_id":"207434","poster":"jove","timestamp":"1635633840.0"},{"content":"Selected Answer: C\nBing\nOption C is correct because AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. You can create a job in AWS Glue that incorporates your Hive script, and you can schedule this job to run once a day. This approach does not require the provisioning or management of servers, making it a cost-effective solution.\nOther options involve using Amazon EMR or AWS Lambda, which could incur higher costs due to the need for server provisioning and potential for idle resources.","upvote_count":"1","timestamp":"1711429800.0","comment_id":"1183044","poster":"tsangckl"},{"content":"Selected Answer: A\nA. As carol1522 explains in her comment","comment_id":"1043697","timestamp":"1697313300.0","poster":"gofavad926","upvote_count":"1"},{"comment_id":"1028213","content":"Selected Answer: A\nA satisfies all the requirements","poster":"Hamza98","timestamp":"1696786800.0","upvote_count":"1"},{"timestamp":"1695472740.0","comment_id":"1014901","poster":"rlnd2000","comments":[{"timestamp":"1697313240.0","comment_id":"1043696","upvote_count":"2","content":"The lambda function only create and initialise the EMR...","poster":"gofavad926"}],"upvote_count":"1","content":"Selected Answer: C\n\"Could anyone who chose \"A\" as the correct answer please explain how to make a Lambda function run for 30 minutes?\""},{"content":"Selected Answer: C\nSince AWS Glue can run Hive script. So C will be cheaper than A.","timestamp":"1687679040.0","upvote_count":"3","poster":"petervu","comment_id":"933350"},{"poster":"Venkkat","timestamp":"1686108420.0","upvote_count":"1","content":"A for sure","comment_id":"916826"},{"poster":"pk349","comment_id":"886307","timestamp":"1682947980.0","content":"A: I passed the test","upvote_count":"1","comments":[{"content":"You passed, OK but this question was wrong in your test I think, how can you make lambda run for 30 minutes?","poster":"rlnd2000","upvote_count":"1","comments":[{"poster":"uyendo123","comment_id":"1018570","upvote_count":"1","content":"I guess that the A means Lambda function would just spin up the EMR Cluster, when Cluster has started, the Lambda function would stop. Then the Hive script run on EMR Cluster, and terminated when script running done.","timestamp":"1695799140.0"}],"timestamp":"1695472860.0","comment_id":"1014905"}]},{"upvote_count":"4","comments":[{"timestamp":"1691431560.0","upvote_count":"3","comment_id":"974889","content":"I think you misunderstand this blog. hive can use the catalog generated by glue, but glue running hive script.\nSo, C is still wrong, A is the correct answer","poster":"he11ow0rId"}],"content":"aws glue can run hive scripts - https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html","timestamp":"1676662320.0","poster":"Arjun777","comment_id":"812278"},{"comment_id":"711808","timestamp":"1667658300.0","content":"Selected Answer: A\nCorrect answer is A as the EMR cluster can be used to execute the Hive scripts. KeepJobFlowAliveWhenNoSteps set to false and disabling the termination protection flag would help destroy the cluster once no running jobs. CloudWatch Events with Lambda can be used to trigger the scheduled activity.\n\nOption B is wrong as Oozie requires the EMR cluster always running, else the job cannot be scheduled and executed. Using Spot instances for core nodes is not recommended.\n\nOption C is wrong as Glue does not support running Hive scripts.\n\nOption D is wrong as Lambda would not be able to meet the 30 minutes job runtime requirement.","upvote_count":"7","poster":"cloudlearnerhere"},{"content":"Selected Answer: A\nThis one is a classic scenario of Transient cluster. So A is the answer here.","upvote_count":"2","poster":"Arka_01","comment_id":"678443","timestamp":"1664082300.0"},{"upvote_count":"1","poster":"rocky48","timestamp":"1658986740.0","comment_id":"638493","content":"Selected Answer: A\nAnswer is A"},{"poster":"Pradhan","upvote_count":"3","content":"I will go with A.","comment_id":"388620","timestamp":"1636283520.0"},{"timestamp":"1636253700.0","upvote_count":"4","content":"Ans A\nB = wrong, termination flag should be off, spot instances not good for core nodes. C = Glue runs on Spark, cannot run Hive scripts. D = wrong, Lambda maximum running time 15 minutes.","poster":"Shraddha","comment_id":"383981"},{"content":"A is the right answer","comment_id":"274285","timestamp":"1635924240.0","upvote_count":"2","poster":"lostsoul07"},{"comment_id":"247219","poster":"Sai12","timestamp":"1635714960.0","upvote_count":"2","content":"A based on its similarity to this article https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs-process-sample-data.html"},{"timestamp":"1635712380.0","poster":"BillyC","comment_id":"216765","upvote_count":"1","content":"A is correct!"},{"content":"Would got for A as the most cost effective","comment_id":"214952","upvote_count":"1","poster":"LMax","timestamp":"1635658680.0"},{"poster":"sanjaym","upvote_count":"1","comment_id":"204899","content":"Answer should be D. step functions automatically manages error handling, retry logic, and state. With its built-in operational controls, Step Functions manages sequencing, error handling, retry logic, and state, removing a significant operational burden from your team.","timestamp":"1635075660.0"},{"upvote_count":"1","content":"Glue jobs can run hive scripts ?","comment_id":"198119","timestamp":"1634898360.0","poster":"Warrior001"},{"comment_id":"194543","poster":"jack42","timestamp":"1634294340.0","upvote_count":"1","content":"Its B (Because its most cost effective and using spot for core can give better benefit), There is no issue to using spot instance for core nodes as the termination protection is enabled. While using other option we are incurring cost like lambda and step functions. (Glue is never used to submit hive jobs)","comments":[{"poster":"metin","upvote_count":"1","comment_id":"198808","timestamp":"1635031980.0","content":"what do you think about Phoenyx89's comment: \n\"if you use oozie you must have the cluster always running\""},{"content":"\"Core nodes process data and store information using HDFS. Terminating a core instance risks data loss. For this reason, you should only run core nodes on Spot Instances when partial HDFS data loss is tolerable.\"\n\nSince raw data stored in S3, (for example, they don't stream data from a destination like Kinesis) can we assume that loss of data is tolerable here? If something goes wrong, they can re-process the data, right?","timestamp":"1634920560.0","upvote_count":"1","poster":"metin","comment_id":"198807"}]},{"upvote_count":"1","content":"I would go for B\n\"Use the Apache Oozie Workflow Scheduler to manage and coordinate Hadoop jobs\" from link: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-oozie.html\nYes spot instances are not recommended for core nodes but since the question asks for cost-effective solution and that the job completes in 30 minutes, using spot instances should not be an issue\nJust my thoughts","comment_id":"191557","timestamp":"1634100720.0","poster":"syu31svc"},{"comment_id":"176302","upvote_count":"2","poster":"Phoenyx89","content":"A. B can be correct but is not the most cost-effective solution because if you use oozie you must have the cluster always running","timestamp":"1633671540.0"},{"upvote_count":"1","comment_id":"175529","comments":[{"content":"In option A, Lambda would just trigger the EMR launch and its job is over.Does not matter with 15 minutes limitation. Do you see some issue with A?","comments":[{"comment_id":"176486","content":"Also in the case of B, EMR needs to keep running which is not going to be cost effective option.","upvote_count":"2","comments":[{"timestamp":"1633931820.0","poster":"GauravM17","content":"Also clearly B is not the anwser \"core nodes for spot instances\" is something you would not want to go with. Spot instances are good with Task nodes","upvote_count":"3","comment_id":"176487"}],"timestamp":"1633878900.0","poster":"GauravM17"},{"poster":"Paitan","comment_id":"177118","content":"You are right. I missed that point. A makes more sense now.","upvote_count":"1","timestamp":"1634045520.0"}],"comment_id":"176483","upvote_count":"5","poster":"GauravM17","timestamp":"1633836060.0"}],"timestamp":"1633312320.0","poster":"Paitan","content":"Lambda will timeout if execution time is more than 15 minutes and here it talks about job execution of upto 30 minutes. So I will go with B."},{"content":"My answer is B","timestamp":"1632518340.0","poster":"zeronine","upvote_count":"1","comment_id":"159910"},{"comment_id":"159350","upvote_count":"1","timestamp":"1632360300.0","poster":"singh100","content":"B. EMR for Hive, Oozie for scheduling."},{"comment_id":"159241","upvote_count":"2","content":"D. See:\nhttps://docs.aws.amazon.com/step-functions/latest/dg/connect-glue.html","poster":"zanhsieh","timestamp":"1632258000.0"},{"content":"Glue jobs would not able to run hive scripts","upvote_count":"1","comment_id":"159089","poster":"paul0099","timestamp":"1632148500.0"}],"unix_timestamp":1597566600,"answer_ET":"A","topic":"1","timestamp":"2020-08-16 10:30:00","url":"https://www.examtopics.com/discussions/amazon/view/28708-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"answer_images":[],"answer":"A","answers_community":["A (71%)","C (29%)"],"question_id":89,"choices":{"C":"Create an AWS Glue job with the Hive script to perform the batch operation. Configure the job to run once a day using a time-based schedule.","B":"Use the AWS Management Console to spin up an Amazon EMR cluster with Python Hue. Hive, and Apache Oozie. Set the termination protection flag to true and use Spot Instances for the core nodes of the cluster. Configure an Oozie workflow in the cluster to invoke the Hive script daily.","A":"Create an AWS Lambda function to spin up an Amazon EMR cluster with a Hive execution step. Set KeepJobFlowAliveWhenNoSteps to false and disable the termination protection flag. Use Amazon CloudWatch Events to schedule the Lambda function to run daily.","D":"Use AWS Lambda layers and load the Hive runtime to AWS Lambda and copy the Hive script. Schedule the Lambda function to run daily by creating a workflow using AWS Step Functions."}},{"id":"2TpGRsOxXHnIo4T7040M","answer_description":"","answer":"B","question_images":[],"timestamp":"2020-08-17 07:55:00","discussion":[{"upvote_count":"44","comment_id":"175530","timestamp":"1632950160.0","poster":"Paitan","content":"B for sure.\nThe COPY command loads the data in parallel from multiple files, dividing the workload among the nodes in your cluster. When you load all the data from a single large file, Amazon Redshift is forced to perform a serialized load, which is much slower. Split your load data files so that the files are about equal size, between 1 MB and 1 GB after compression. For optimum parallelism, the ideal size is between 1 MB and 125 MB after compression. The number of files should be a multiple of the number of slices in your cluster"},{"content":"Ans B\nA = wrong, compression means download file from S3 then compress, time-consuming, also you use COPY for files not INSERT. C = wrong, will not improve performance. D = wrong, vacuum frees up storage. This is a question about parallel loading.","poster":"Shraddha","comment_id":"383991","upvote_count":"11","timestamp":"1636298880.0"},{"timestamp":"1690134180.0","content":"Selected Answer: B\nB for sure","poster":"Mayank7g","comment_id":"960690","upvote_count":"1"},{"content":"B: I passed the test","poster":"pk349","upvote_count":"2","comment_id":"886309","timestamp":"1682948040.0"},{"comment_id":"837215","upvote_count":"3","content":"Selected Answer: B\nOption B is the most appropriate solution for improving data loading performance. Splitting large .csv files and using a COPY command can parallelize the load process and reduce the data load time. The data partitioning by date can help further optimize the load process by reducing the data scanned for each load. Compressing the .csv files may help reduce the storage cost, but it may not improve the data load time. Using an INSERT statement to ingest data into Amazon Redshift can be slow and does not take advantage of Redshift's parallel processing capability. Amazon Kinesis Data Firehose can be used to ingest streaming data in real-time, but may not be the best choice for large batch loads. Loading the .csv files in an unsorted key order and vacuuming the table can help optimize the table for query performance but may not improve the data loading performance.","timestamp":"1678638420.0","poster":"AwsNewPeople"},{"upvote_count":"5","content":"Selected Answer: B\nCorrect answer is B as splitting the large file into multiple files can help improve the data loading performance using the COPY command.\n\nOption A is wrong as the COPY command would provide the best benefit.\n\nOption C is wrong as Kinesis Data Firehose cannot move data from S3 to Redshift. Kinesis Data Firehose delivers your data to your S3 bucket first and then issues an Amazon Redshift COPY command to load the data into your Amazon Redshift cluster. So it doesn't still improve the load performance.\n\nOption D is wrong as vacuuming will free up space but does not improve the load performance.","timestamp":"1667658420.0","comment_id":"711812","poster":"cloudlearnerhere"},{"content":"Selected Answer: B\nS3 to Redshift upload can be done through copy command. To utilize parallelism, large files are recommended to split into small chunk of files.","comment_id":"678446","upvote_count":"1","timestamp":"1664082420.0","poster":"Arka_01"},{"content":"Don't know why B? here is uncompressed csv file, so no need to split file (Redshift will do automatically?\nIn contrast, when you load delimited data from a large, uncompressed file, Amazon Redshift makes use of multiple slices. These slices work in parallel, automatically. This provides fast load performance.\nIn https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-use-multiple-files.html","timestamp":"1658580840.0","comments":[{"timestamp":"1669534620.0","poster":"Ryo0w0o","content":"Agreed. It seems like no correct answer among the choices.","comment_id":"728007","upvote_count":"1"}],"comment_id":"635557","upvote_count":"2","poster":"Binh12"},{"upvote_count":"1","comment_id":"633857","timestamp":"1658294940.0","content":"Selected Answer: B\nAnswer = B","poster":"rocky48"},{"upvote_count":"1","comment_id":"605268","timestamp":"1653206040.0","content":"Selected Answer: B\nAnswer is B","poster":"Bik000"},{"poster":"moon2351","comment_id":"570145","upvote_count":"2","timestamp":"1647565020.0","content":"Selected Answer: B\nAnswer is B"},{"upvote_count":"1","comment_id":"490743","poster":"awsmani","timestamp":"1638281220.0","content":"Ans:B split large files will help loading in performance. Having one large file will load in serialized manner which lowers performance"},{"upvote_count":"3","content":"B is the right answer","poster":"lostsoul07","comment_id":"274286","timestamp":"1633974060.0"},{"poster":"BillyC","content":"B is correct for me","upvote_count":"2","timestamp":"1633973640.0","comment_id":"216764"},{"poster":"sanjaym","content":"B for sure.","timestamp":"1633446360.0","upvote_count":"2","comment_id":"204903"},{"comment_id":"191565","poster":"syu31svc","content":"It's already in S3 so answer is B 100%","upvote_count":"1","timestamp":"1633110240.0"},{"poster":"ali_baba_acs","content":"Answer is B, A compress is a good practice but then copy command not insert, Kinesis Firehose will not improve performance, the vacuum will help freeing space not improve perf too.","comment_id":"166892","upvote_count":"3","timestamp":"1632866340.0"},{"poster":"awssp12345","content":"Answer is B - https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html\nAlso, Vaccum command will help with clearing up space on the cluster but not improve data loading time.","comment_id":"164804","upvote_count":"3","timestamp":"1632783420.0"},{"comment_id":"159915","content":"My answer is Bhttps://www.examtopics.com/exams/amazon/aws-certified-data-analytics-specialty/view/#","poster":"zeronine","upvote_count":"1","timestamp":"1632635580.0"},{"poster":"cloud4gr8","timestamp":"1632244380.0","content":"is it B?","upvote_count":"3","comment_id":"159655"}],"answer_images":[],"answer_ET":"B","unix_timestamp":1597643700,"url":"https://www.examtopics.com/discussions/amazon/view/28797-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"question_id":90,"topic":"1","question_text":"A company wants to improve the data load time of a sales data dashboard. Data has been collected as .csv files and stored within an Amazon S3 bucket that is partitioned by date. The data is then loaded to an Amazon Redshift data warehouse for frequent analysis. The data volume is up to 500 GB per day.\nWhich solution will improve the data loading performance?","answers_community":["B (100%)"],"isMC":true,"choices":{"B":"Split large .csv files, then use a COPY command to load data into Amazon Redshift.","C":"Use Amazon Kinesis Data Firehose to ingest data into Amazon Redshift.","D":"Load the .csv files in an unsorted key order and vacuum the table in Amazon Redshift.","A":"Compress .csv files and use an INSERT statement to ingest data into Amazon Redshift."}}],"exam":{"numberOfQuestions":164,"name":"AWS Certified Data Analytics - Specialty","lastUpdated":"11 Apr 2025","provider":"Amazon","id":20,"isImplemented":true,"isBeta":false,"isMCOnly":true},"currentPage":18},"__N_SSP":true}