{"pageProps":{"questions":[{"id":"TROtiOP4breBT73MS6Vb","answer_images":[],"choices":{"C":"Create a composite of the plant identifier and sensor identifier as the partition key. Use the measurement time as the sort key. Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key.","D":"Use the plant identifier as the partition key and the sensor identifier as the sort key. Create a local secondary index (LSI) on the fault attribute.","B":"Create a composite of the plant identifier and sensor identifier as the partition key. Use the measurement time as the sort key. Create a local secondary index (LSI) on the fault attribute.","A":"Use the plant identifier as the partition key and the measurement time as the sort key. Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key."},"isMC":true,"exam_id":22,"timestamp":"2021-04-04 06:20:00","question_id":6,"answer_description":"","discussion":[{"timestamp":"1632675900.0","upvote_count":"18","poster":"[Removed]","comment_id":"340468","content":"I am going with D. As I understand it, you can have an item as a partition key and an item as a sort key to make a composite key. However, you cannot have two items as a partition key and a third item as sort key to make a composite key.","comments":[{"upvote_count":"3","comment_id":"696760","content":"you can combine more than one attribute to create a partition key .\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\n\"Use composite attributes. Try to combine more than one attribute to form a unique key, if that meets your access pattern. For example, consider an orders table with customerid#productid#countrycode as the partition key and order_date as the sort key, where the symbol # is used to split different field.","poster":"awsjjj","timestamp":"1665976020.0","comments":[{"content":"The access pattern for this question is: finding all faulty sensors within a given power plant.\n\nPartition key DOESN'T support using condition expressions - you can only use them in sort key and in filter expressions. If you combine plantID and sensorID in a partition key, how do you find ALL faulty sensors within a single power plant? You simply cannot. With such a partition key, you always have to query for a specific sensor located in a specific power plant. \n\nIf 1 of the answers would be to place pantID and sensorID in sort key, then we could consider this option.\n\nI will go with D.","timestamp":"1682673180.0","comment_id":"883368","upvote_count":"2","poster":"FooBarBazBazinga"},{"upvote_count":"2","poster":"Jiang_aws1","comment_id":"722388","content":"DynamoDB don't allow us do that way. we have to merge / combine into as 1 attribute \nSo B, C are wrong","timestamp":"1668918960.0"}]}]},{"poster":"Jaypdv","upvote_count":"8","comments":[{"upvote_count":"1","comment_id":"331911","content":"Agreed C. Answer","poster":"shantest1","timestamp":"1632625620.0"}],"content":"C. Putting the Fault attribute in a sparse index, based on https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general-sparse-indexes.html","comment_id":"327727","timestamp":"1632535440.0"},{"poster":"Hisayuki","comment_id":"1111682","content":"Selected Answer: D\nIf the answer is B, you need 200 partitions for each sensor.","timestamp":"1704180960.0","upvote_count":"1"},{"poster":"Sathish_dbs","content":"Selected Answer: B\nD is not having option to filter measurement time as an sensor could have been faulty last month but might have been fixed all the recent entries no faulty, though the question explicitly asked \"based on the recent measurement\" we need to assume it, and option D don't have this field in its solution, hence it will become a problem","comment_id":"1040776","upvote_count":"1","timestamp":"1697035680.0"},{"poster":"omle2","upvote_count":"2","content":"C is correct Answer I think.\n\nDetails are below.\n\n\"Power plants are identified by a globally unique identifier. Sensors are identified by a unique identifier within each power plant. \"\n\nIt means that 'sensor_id' is not globally unique.\n\n\"A database specialist needs to design the table to support an efficient method of finding all faulty sensors within a given power plant.\"\n\nThe database specialist knows the 'plant_id' before executing the query.\n\nAs a prerequisite knowledge\n- Secondary indexes like LSI and GSI in DynamoDB are not unique.\n- The data in a LSI is organized by the same partition key as the base table, but with a different sort key.","comment_id":"1014789","timestamp":"1695462360.0","comments":[{"comments":[{"comment_id":"1014791","timestamp":"1695462420.0","content":"- C : o\n\nKey ( pk : plant_id#sensor_id - sk : measurement_time )\nGSI ( pk : plant_id - sk : fault ) \n\nAs shown in A, fault sensor_id can be obtained using GSI.\n\nThe measurement_time is unique with the combination of plant_id and sensor_id.\n\n- D : x\n\nKey ( pk : plant_id - sk : sensor_id )\nLSI ( pk : plant_id - sk : fault ) \n\nThe combination of the partition key value and sort key value for each item must be unique.\n\nOnly one item per 'plant_id' and 'sensor_id' can be stored.","poster":"omle2","upvote_count":"1"}],"upvote_count":"1","content":"- A : x\n\nKey ( pk : plant_id - sk : measurement_time )\nGSI ( pk : plant_id - sk : fault ) \n\nIf the plant_id is known, it is easy to use 'fault' to find the failed sensor.\n\n1. Query for items with a specific 'plant_id' and 'fault' set to true.\n2. The 'sensor_id' can be obtained from the acquired item.\n\nIf the measurement_time is exactly the same time in plant, the data could be overwritten.\n\n- B : x\n\nKey ( pk : plant_id#sensor_id - sk : measurement_time )\nLSI ( pk : plant_id#sensor_id - sk : fault ) \n\nIf you don't know the 'sensor_id', you can't query it.","timestamp":"1695462420.0","poster":"omle2","comment_id":"1014790"}]},{"timestamp":"1695233040.0","comments":[{"timestamp":"1695233760.0","comment_id":"1012554","content":"Sorry, wrong reasoning: An LSI has the same partition key as the base table, but a different sort key. A GSi can have an altogether different partition key...\nThat means, B has a wrong LSI, C is not unique, D has a wrong LSI.\nRemains A...","upvote_count":"1","poster":"Germaneli"}],"content":"Selected Answer: B\nx A is out: a GSI must have the same partition key as the base table.\nB. The PK consisting of partition key+sort key is unique. An LSI on the fault attribute is possible. That would satisfy the query.\nx C. a GSI must have the same partition key as the base table.\nx D. The PK consisting of partition key+sort key is not unique.","comment_id":"1012551","upvote_count":"1","poster":"Germaneli"},{"timestamp":"1694897100.0","content":"Selected Answer: D\nD is correct answer.\n\nComposite primary key: This is a combination of partition key and sort key. So, B & C are ruled out. Partition key composes of one attribute only. \"Partition key: A simple primary key, composed of one attribute known as the partition key. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems.\"\n\nA is wrong because no info about sensors identifier.\n\n\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/","upvote_count":"1","comment_id":"1009311","poster":"Pranava_GCP"},{"upvote_count":"2","poster":"IhorK","comment_id":"974424","content":"Selected Answer: D\nComposite primary key: This is a combination of partition key and sort key. So, B & C are wrong. \nA - no info about sensors identifier.","timestamp":"1691387280.0"},{"poster":"Zdujgfr567783ff","content":"Selected Answer: D\nlooks like D\nthere is nothing about the timestamp in the task","timestamp":"1686519840.0","upvote_count":"1","comment_id":"921002"},{"upvote_count":"4","comment_id":"900061","poster":"aviathor","content":"Selected Answer: B\nA: The GSI key is not unique\nB: The Primary key is unique because measurement time is the sort key, and the LSI allows us to narrow down to the required data\nC: The primary key is not unique\nD: The primary key is not unique.\n\nThe combination of plant id and sensor id is not unique since there will be many measurements for that key. Measurement time must therefore be part of the key to make it unique.","timestamp":"1684323660.0"},{"upvote_count":"2","comment_id":"868876","content":"Selected Answer: D\nYou can't use more than one attributes as a partition key","poster":"aqiao","timestamp":"1681340340.0"},{"upvote_count":"1","timestamp":"1680095100.0","comment_id":"854442","poster":"ken_test1234","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"},{"content":"Selected Answer: B\nB is the only option where an index would have sensor id as well. We need faulty sensor as query output and if sensor id is included in an index then only we are going to get faulty sensor details in the output. If we notice carefully, none of the options except B have sensor id used in index and we are assuming that we will be using an index to get data faster. So, correct choice is the one which has the query attribute in the index.","poster":"backbencher2022","upvote_count":"1","timestamp":"1679365860.0","comment_id":"845454"},{"comment_id":"802237","timestamp":"1675873080.0","poster":"im_not_robot","content":"Selected Answer: D\nA is feasible. But we can fullfill the requirement with simpler approach (which is D)\nB is completely wrong because we can not use that LSI at all for the requirement. To use the LSI, we need to specify the plant identifier as the primary key, but the primary key now contains sensor identifier so we can not use LSI.\nC is feasible simillar to A\nD is the most simpler set up.","upvote_count":"2"},{"timestamp":"1675800240.0","comment_id":"801372","content":"Selected Answer: C\nConfusing question but I think it should be C","upvote_count":"1","poster":"guau"},{"upvote_count":"2","content":"Selected Answer: D\nA,C does not make sense: there is no need to create GSI, as question says find faulty sensors by plant. So when you have plant identifier as primary key, you can find faulty sensors in that plant by using LSI.\n\nB does not make sense: Question does not mention anything finding item my measurement time. So there is no point having measurement time as sort key.\n\nD is correct.","poster":"anantarb","comment_id":"792258","timestamp":"1675042440.0"},{"poster":"BaskerS","timestamp":"1675025760.0","comment_id":"792057","content":"D is correct Answer i think\nComposite key is a Partition key and Sort Key, and then we cannot have another Sort Key\nBut \nD works which is Partition Key Plant Identifier and Sort Key as Sensor Identified, then LSI as Fault Attribute","upvote_count":"1"},{"upvote_count":"1","poster":"Sathish_dbs","content":"the access pattern of the requirement does not need measurement time hence option D provides optimal access","timestamp":"1673090220.0","comment_id":"768466"},{"poster":"lollyj","upvote_count":"1","comment_id":"752692","timestamp":"1671651240.0","comments":[{"poster":"lollyj","content":"Also the LSI indicated does not include a partition key so C makes more sense.","upvote_count":"1","timestamp":"1671657240.0","comment_id":"752759"}],"content":"Selected Answer: C\nIt seemed most logical but not I don't know smh"},{"poster":"yxyj","timestamp":"1667660280.0","content":"Answer is C. \nTo get the fastest queries with the lowest possible latency, project all the attributes that you expect those queries to return. In particular, if you query a local secondary index for attributes that are not projected, DynamoDB automatically fetches those attributes from the table, which requires reading the entire item from the table. This introduces latency and additional I/O operations that you can avoid.","comment_id":"711835","upvote_count":"1"},{"comment_id":"694211","content":"Selected Answer: B\nB seems correct","timestamp":"1665686160.0","upvote_count":"1","poster":"awsjjj"},{"timestamp":"1665108720.0","comment_id":"688223","comments":[{"content":"why do we ned to sort by time if we only look for a plant, sensor, and fault fag?","timestamp":"1686519960.0","poster":"Zdujgfr567783ff","comment_id":"921003","upvote_count":"1"}],"upvote_count":"3","content":"Plant id , Sensor id : Compsite Key\nTime : sort key\nFault : 2nd index","poster":"Jiang_aws1"},{"poster":"niteshdba","comment_id":"682794","timestamp":"1664463600.0","content":"Its should be D","upvote_count":"2"},{"timestamp":"1662177000.0","comment_id":"658038","upvote_count":"1","content":"Selected Answer: B\nB is correct","poster":"SonamDhingra"},{"comments":[{"comment_id":"694082","poster":"Jiang_aws1","content":"\"Time\" must be part of key or indexes otherwise just too many rows to search for \nSo \"B\" is the best","timestamp":"1665677460.0","upvote_count":"1"}],"comment_id":"605639","poster":"Radhaghosh","timestamp":"1653239520.0","content":"Whoever is saying any other option other than \"B\" did you read this line? \" 200 sensors that transmit data every two seconds\". Don't you think any other option (except B) will create a hot partition? And Composite of Plant ID and Sensor ID as Primary key doesn't mean two attribute separately. It mean clubbing this two attributes (may be String Join with \"_\")","upvote_count":"2"},{"timestamp":"1653222060.0","poster":"awsguys","comment_id":"605456","upvote_count":"1","content":"D right","comments":[{"poster":"Jiang_aws1","content":"D is wrong : Missing \"Time\" \"over 200 sensors that send data every 2 seconds. The sensor data includes time with milliseconds precision\"","comments":[{"content":"Almost 20% Question - Text about \"Time\" then I can't miss it","poster":"Jiang_aws1","comment_id":"694105","upvote_count":"1","timestamp":"1665678540.0"}],"upvote_count":"1","comment_id":"694104","timestamp":"1665678480.0"}]},{"upvote_count":"4","comment_id":"595268","content":"Selected Answer: B\nB. Create a composite of the plant identifier and sensor identifier as the partition key. Use the measurement time as the sort key. Create a local secondary index (LSI) on the fault attribute.\n\nI know DynamoDB does not allow 3 columns in primary key (2 columns for partition key and 1 for sort) But what if plant identifier and sensor identifier are concatenated like 'PID1-SID1' and used as partition key?","poster":"novice_expert","timestamp":"1651350540.0"},{"poster":"kret","upvote_count":"3","content":"Selected Answer: D\nyou can't create a composite key which is a primary key -> B & C are wrong; a is obviously wrong; so D is ok","timestamp":"1649848920.0","comment_id":"585163","comments":[{"upvote_count":"2","content":"I don't agree. Composite keys can be created.\n\nComposite primary key\nA composite primary key is composed of two attributes, the partition key and the sort key. All items with the same partition key are stored together, sorted by sort key value. In a table with a composite primary key, it's possible for two items to have the same partition key value. However, two items cannot have both the same partition key value and the same sort key value.","poster":"lollyj","timestamp":"1671651300.0","comment_id":"752693"}]},{"upvote_count":"1","comment_id":"548400","timestamp":"1645000020.0","content":"B&C can be ruled out at the first place; let consider how to query sensors' status at the quickest pace, from my perspective, using a measurement time as the sort key, the updated information would be sorted by the recording time, not by sensor's ID, hence provide more quicker response, because when searching for malfunctioning sensors, the most recent status is what we need. I prefer A.","poster":"pcpcpc888"},{"upvote_count":"3","comment_id":"508267","timestamp":"1640313780.0","poster":"jove","content":"How to design this DynamoDB table;\n- Keep the number of items as small as possible.\n- So, your primary key should be plant ID + sensor ID\n- Choose plant ID as the partition key, so that the query will be faster\n- Create a LSI on the fault attribute but add this attribute only for the items when fault = true (sparse index)\n \nThe answer is D"},{"upvote_count":"3","comment_id":"491783","poster":"cynthiacy","timestamp":"1638374040.0","content":"B is correct."},{"content":"I’m going with B\n• A dropped because “having the plant identifier as the partition key and the fault attribute as the sort key” will not let us know which sensor generated the fault attribute.\n• C dropped for same reason like A, “Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key” will not let us know which sensor generated the fault attribute.\n• D dropped local secondary index on fault attribute will depend on only plant identifier as primary key, so same issue like A and C, this will not let us know which sensor generated the fault attribute.\n• Only Remaining answer is B, it generates partition key which is composed of two values (PlantID+SensorID) – By the way this is not composite key -, and generate time as sort key. then Creating fault attribute as local secondary index which will depend on (PlantID+SensorID) as partition key will allow us to identify the faulty sensor","comment_id":"447688","timestamp":"1635756480.0","comments":[{"poster":"jove","timestamp":"1640313960.0","comment_id":"508272","upvote_count":"1","content":"- For option D the primary key is composed of PlantID (partition key) + SensorID (sort key)\n- Why would you need measurement time as the sort key, you won't need to keep previous measurements, just keep the latest one for each PlantID + SensorID"}],"upvote_count":"3","poster":"faramawi"},{"upvote_count":"1","comments":[{"content":"Did you pass? I failed. If you passed which dumps did you read please mention.","comments":[{"content":"did you use samedumps ? What answers did you use - dump answers or answers mentioned in discussions ?","upvote_count":"1","poster":"odba2014","timestamp":"1640373840.0","comment_id":"508777"}],"poster":"Sultana","comment_id":"451116","timestamp":"1635873720.0","upvote_count":"1"}],"comment_id":"446128","timestamp":"1634851560.0","poster":"guru_ji","content":"This Q appeared in my exam.\n60% questions came in actual exam from this 145 set. Bunch of new Questions."},{"upvote_count":"2","timestamp":"1634493240.0","content":"My choice is D\nIt can't be C, you can't have 02 items as 02 partition keys on a composite key.","poster":"ChauPhan","comment_id":"430842"},{"comment_id":"409061","upvote_count":"6","content":"D is correct. Plant id as partition key and Sensor id as a sort key. Fault can be identified quickly using the local secondary index and associated plant and sensor can be identified easily.","timestamp":"1633303560.0","comments":[{"timestamp":"1633933800.0","comment_id":"409063","poster":"Hits_23","upvote_count":"3","comments":[{"timestamp":"1644262080.0","content":"This^^^ Immediately eliminates B and C. Between A and D the choice is D","poster":"VPup","upvote_count":"1","comment_id":"542631"}],"content":"and composite primary key for dynamodb table is always combination of partition key and sort key, there is no partition key allowed with two columns"}],"poster":"Hits_23"},{"timestamp":"1632777720.0","poster":"Huy","comment_id":"360927","content":"Because the access pattern is get faulty sensors with a given plant and plantID is unique, the partition key should be plantID. Sort/range key should be sensorID. And we just need Local Secondary Index with plandID and fault attribute.","upvote_count":"5"}],"answer":"D","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/48997-exam-aws-certified-database-specialty-topic-1-question-103/","answer_ET":"D","question_text":"An electric utility company wants to store power plant sensor data in an Amazon DynamoDB table. The utility company has over 100 power plants and each power plant has over 200 sensors that send data every 2 seconds. The sensor data includes time with milliseconds precision, a value, and a fault attribute if the sensor is malfunctioning. Power plants are identified by a globally unique identifier. Sensors are identified by a unique identifier within each power plant. A database specialist needs to design the table to support an efficient method of finding all faulty sensors within a given power plant.\nWhich schema should the database specialist use when creating the DynamoDB table to achieve the fastest query time when looking for faulty sensors?","question_images":[],"answers_community":["D (50%)","B (43%)","7%"],"unix_timestamp":1617510000},{"id":"iX2ljvCbTMm9nj0SF2pO","question_images":[],"answers_community":["D (94%)","6%"],"discussion":[{"poster":"shantest1","comment_id":"328676","timestamp":"1633905240.0","upvote_count":"15","content":"D. answer"},{"timestamp":"1655619960.0","comment_id":"618550","upvote_count":"6","poster":"ryuhei","content":"Selected Answer: D\nAnswer：D\nhttps://docs.aws.amazon.com/ja_jp/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html"},{"content":"Selected Answer: D\nD. Ensure the BatchGetltem operation is called with the ConsistentRead parameter set to true.","poster":"Pranava_GCP","comment_id":"995792","timestamp":"1693553100.0","upvote_count":"2"},{"content":"Selected Answer: D\nBy default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","upvote_count":"1","comment_id":"974457","poster":"IhorK","timestamp":"1691390940.0"},{"content":"Selected Answer: D\nD is correct\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","poster":"backbencher2022","upvote_count":"2","comment_id":"845456","timestamp":"1679366040.0"},{"comment_id":"839937","timestamp":"1678888560.0","content":"D.\nBy default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","upvote_count":"2","poster":"Mintwater"},{"upvote_count":"2","poster":"sachin","content":"D is correct.","timestamp":"1655826480.0","comment_id":"619921"},{"timestamp":"1655617920.0","comment_id":"618538","poster":"elf78","upvote_count":"1","comments":[{"content":"Consistent means getting the latest data. You are talking about the \"eventual\" case. Streams is usually used to trigger actions based on events happening on the db. D is probably the answer.","upvote_count":"1","poster":"shammous","comment_id":"646247","timestamp":"1660382640.0"}],"content":"C is the answer. Questions states that \"..periodically reads\" which mean re using the BatchGetItem with consistent read (D) is not going to solve the problem since data might be already stale on the next read. However, by using Streams each device will be notified immediately."},{"poster":"dbaroger","timestamp":"1653915600.0","content":"Selected Answer: C\nC is the most effective. Streams always will show the last update of the item, so, all users will read the same item status.\nC would resolve the problem as well, but will double the read capacity unit and the cost will double too\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html","comment_id":"609207","comments":[{"timestamp":"1653915660.0","poster":"dbaroger","content":"I meant \"D\" would double the RCUs.","upvote_count":"1","comment_id":"609209"}],"upvote_count":"1"},{"content":"Selected Answer: D\ngame's status data was out of current means query got eventual consistent data","timestamp":"1651245600.0","poster":"novice_expert","upvote_count":"2","comment_id":"594546"},{"timestamp":"1647040800.0","comment_id":"565839","upvote_count":"4","content":"Selected Answer: D\nBy default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","poster":"Dantas"},{"timestamp":"1645656060.0","upvote_count":"1","comment_id":"554928","poster":"kped21","content":"D, BatchGetItem with ConsistentRead"},{"timestamp":"1645019580.0","comment_id":"548636","poster":"user0001","upvote_count":"1","content":"D\nBy default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables."},{"timestamp":"1639570560.0","content":"D\nBy default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.","comment_id":"502111","upvote_count":"2","poster":"mnzsql365"},{"comment_id":"379439","upvote_count":"1","timestamp":"1634873340.0","poster":"Suresh108","content":"DDDDDDDDD"},{"timestamp":"1634582880.0","content":"https://docs.aws.amazon.com/ja_jp/amazondynamodb/latest/developerguide/API_BatchGetItem_v20111205.html","upvote_count":"1","comment_id":"364098","poster":"gdtypk"},{"content":"D final answer","timestamp":"1634377080.0","comment_id":"360635","upvote_count":"1","poster":"Aesthet"}],"timestamp":"2021-04-05 14:51:00","answer_images":[],"answer":"D","choices":{"A":"Ensure the DynamoDB table is configured to be always consistent.","C":"Enable a stream on the DynamoDB table and subscribe each device to the stream to ensure all devices receive up-to-date status information.","B":"Ensure the BatchGetltem operation is called with the ConsistentRead parameter set to false.","D":"Ensure the BatchGetltem operation is called with the ConsistentRead parameter set to true."},"isMC":true,"topic":"1","question_text":"A company is releasing a new mobile game featuring a team play mode. As a group of mobile device users play together, an item containing their statuses is updated in an Amazon DynamoDB table. Periodically, the other users' devices read the latest statuses of their teammates from the table using the BatchGetltemn operation.\nPrior to launch, some testers submitted bug reports claiming that the status data they were seeing in the game was not up-to-date. The developers are unable to replicate this issue and have asked a database specialist for a recommendation.\nWhich recommendation would resolve this issue?","answer_ET":"D","exam_id":22,"unix_timestamp":1617627060,"url":"https://www.examtopics.com/discussions/amazon/view/49205-exam-aws-certified-database-specialty-topic-1-question-104/","answer_description":"","question_id":7},{"id":"o2nRjZUktTXXxLXQYgWX","unix_timestamp":1617369600,"answers_community":["C (100%)"],"topic":"1","question_images":[],"question_id":8,"exam_id":22,"answer_ET":"C","answer_description":"","answer":"C","discussion":[{"content":"C.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations","comment_id":"327736","upvote_count":"14","timestamp":"1634040960.0","poster":"Jaypdv"},{"content":"Selected Answer: C\nC. Create a snapshot of the unencrypted DB instance. Create an encrypted copy of the snapshot. Restore the DB instance from the encrypted snapshot. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations\n\n\"you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance. \"","upvote_count":"2","timestamp":"1693463280.0","poster":"Pranava_GCP","comment_id":"994839"},{"comment_id":"594735","timestamp":"1651276500.0","upvote_count":"3","poster":"novice_expert","content":"Selected Answer: C\nsnapshot \n-> encrypted copy of the snapshot \n-> Restore the DB instance from the encrypted snapshot."},{"content":"Selected Answer: C\nIts definitely C","poster":"RotterDam","comment_id":"561258","timestamp":"1646461320.0","upvote_count":"3"},{"upvote_count":"1","content":"Ans: C\nIn AWS console, you have to \"migrate\" unencrypted snapshot to encrypted one then performing database restoration.","poster":"Shunpin","timestamp":"1640602620.0","comment_id":"510236"},{"comment_id":"488480","poster":"jove","content":"Selected Answer: C\nIt is C","upvote_count":"3","timestamp":"1638043500.0"},{"comment_id":"487404","timestamp":"1637934240.0","upvote_count":"4","content":"Selected Answer: C\nOption C","poster":"GMartinelli"},{"upvote_count":"1","timestamp":"1636201560.0","content":"Answer: C","poster":"guru_ji","comment_id":"439706"},{"comment_id":"379444","timestamp":"1635257280.0","poster":"Suresh108","upvote_count":"1","content":"CCCCCCCCC"},{"timestamp":"1632231660.0","comment_id":"326666","poster":"shantest1","upvote_count":"2","content":"C. Answer","comments":[{"poster":"shantest1","timestamp":"1632512040.0","comment_id":"326668","upvote_count":"2","content":"A and D is incorrect\nB is for Aurora Database not for RDS Database."}]}],"isMC":true,"answer_images":[],"choices":{"C":"Create a snapshot of the unencrypted DB instance. Create an encrypted copy of the snapshot. Restore the DB instance from the encrypted snapshot.","D":"Temporarily shut down the unencrypted DB instance. Enable AWS KMS encryption in the AWS Management Console using an AWS managed CMK. Restart the DB instance in an encrypted state.","B":"Create a new RDS for MySQL DB instance with encryption enabled. Restore the unencrypted snapshot to this DB instance.","A":"Create an encrypted snapshot of the unencrypted DB instance. Copy the encrypted snapshot to Amazon S3. Restore the DB instance from the encrypted snapshot using Amazon S3."},"url":"https://www.examtopics.com/discussions/amazon/view/48811-exam-aws-certified-database-specialty-topic-1-question-105/","timestamp":"2021-04-02 15:20:00","question_text":"A company is running an Amazon RDS for MySQL Multi-AZ DB instance for a business-critical workload. RDS encryption for the DB instance is disabled. A recent security audit concluded that all business-critical applications must encrypt data at rest. The company has asked its database specialist to formulate a plan to accomplish this for the DB instance.\nWhich process should the database specialist recommend?"},{"id":"bbuDlZdrQ5nJzm0Kw9b5","question_id":9,"timestamp":"2021-04-04 06:45:00","topic":"1","answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/49000-exam-aws-certified-database-specialty-topic-1-question-106/","answer":"ACD","exam_id":22,"isMC":true,"unix_timestamp":1617511500,"answer_ET":"ACD","question_text":"A company is migrating its on-premises database workloads to the AWS Cloud. A database specialist performing the move has chosen AWS DMS to migrate an\nOracle database with a large table to Amazon RDS. The database specialist notices that AWS DMS is taking significant time to migrate the data.\nWhich actions would improve the data migration speed? (Choose three.)","choices":{"A":"Create multiple AWS DMS tasks to migrate the large table.","B":"Configure the AWS DMS replication instance with Multi-AZ.","C":"Increase the capacity of the AWS DMS replication server.","D":"Establish an AWS Direct Connect connection between the on-premises data center and AWS.","E":"Enable an Amazon RDS Multi-AZ configuration.","F":"Enable full large binary object (LOB) mode to migrate all LOB data for all large tables."},"answer_description":"","answers_community":["ACD (86%)","14%"],"discussion":[{"comment_id":"327739","upvote_count":"11","content":"ACD. Answer\nSelecting A. based on https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LargeTables \nC. and D. are the only options that have to do with increasing performance, the others are irrelevant","timestamp":"1632512760.0","poster":"Jaypdv","comments":[{"poster":"johnconnor","timestamp":"1637261100.0","comment_id":"480952","upvote_count":"3","comments":[{"upvote_count":"2","timestamp":"1638366000.0","comment_id":"491684","content":"It should be not an option, but these three (ACD) are the means which fasten the migration. Other options wouldn't do that.","poster":"Justu"}],"content":"it is going to take you days/weeks to establish a direct connection, if it is not from the start, it should not be an option"}]},{"content":"Correct Answer: ACF\n\nFull LOB mode – migrates all LOB data, piecewise in chunks (you provide LOB chunk size)","timestamp":"1633933140.0","comment_id":"439768","poster":"guru_ji","upvote_count":"6"},{"poster":"tcl08","comment_id":"1143696","content":"D can't be part of the answer. It takes about a week or more to setup a direct connect connection. I will think it ACF","timestamp":"1707332340.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: ACD\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html\n\nB & E. Multi AZ are for improving availability should a storage issue occur, not contributing to data migration speed so B&E are out. F is opposite, \"Full LOB mode provides the convenience of moving all LOB data in your tables, but the process can have a significant impact on performance.\" so F is out.","timestamp":"1694899920.0","comment_id":"1009331","poster":"Pranava_GCP"},{"poster":"cloudbusting","upvote_count":"1","content":"it should be ACF\nhttps://repost.aws/knowledge-center/dms-improve-speed-lob-data","timestamp":"1685268000.0","comment_id":"908482"},{"upvote_count":"1","timestamp":"1684907820.0","poster":"MrAliMohsan","content":"Selected Answer: ACD\nThe answer I was searching in the comments, I will post it here.\nFor those searching for a reason to select D, and how it improves migration speed.\n\nAWS Direct Connect can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections","comment_id":"905522"},{"timestamp":"1684328940.0","poster":"aviathor","comment_id":"900174","content":"Selected Answer: ACD\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.Performance","upvote_count":"1"},{"content":"F is wrong: In full LOB mode AWS DMS migrates all LOBs from source to target regardless of size. In this configuration, AWS DMS has no information about the maximum size of LOBs to expect. Thus, LOBs are migrated one at a time, piece by piece. Full LOB mode can be quite slow. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html","poster":"aqiao","upvote_count":"2","timestamp":"1681341120.0","comment_id":"868888"},{"comment_id":"821760","upvote_count":"1","timestamp":"1677351960.0","comments":[{"content":"\"To do this, split the table into segments and load the segments in parallel in the same migration task.\"\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.Performance","comment_id":"900169","timestamp":"1684328760.0","upvote_count":"1","poster":"aviathor"}],"content":"How can we break a single table into multiple DMS tasks .If the DB had multiple tables , they can be split into multiple DMS tasks . So , we cannot assume that 'large table' to mean to multiple tables. So , I will go with CDE","poster":"sk1974"},{"content":"With Amazon RDS databases, it's a good idea to turn off backups and Multi-AZ until the cutover.\n\nSo B and E eliminated\n\nF - Full LOB mode impacts performance\n\nSo ACD right answer","upvote_count":"1","poster":"rags1482","comment_id":"705071","timestamp":"1666828620.0"},{"timestamp":"1666452840.0","content":"F - Full LOB mode downgrade the performance\nFull LOB mode migrates all LOB data in your tables, regardless of size. Full LOB mode provides the convenience of moving all LOB data in your tables, but the process can have a significant impact on performance.","upvote_count":"2","comment_id":"701615","poster":"Satprave"},{"comment_id":"609912","content":"Selected Answer: ACD\n\"To improve the performance when migrating a large table, break the migration into more than one task. To break the migration into multiple tasks using row filtering, use a key or a partition key\" (A)\n\nA number of factors affect the performance of your AWS DMS migration:\n- Resource availability on the source.\n- The available network throughput. (D)\n- The resource capacity of the replication server. (C)\n- The ability of the target to ingest changes.\n- The type and distribution of source data.\n- The number of objects to be migrated.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LargeTables","poster":"Dantas","timestamp":"1654032660.0","upvote_count":"5"},{"timestamp":"1652233500.0","content":"Selected Answer: ACF\nIt takes much to deploy the direct connect, so D is not the correct answer.\nI choose ACF","comment_id":"599820","poster":"whoareyou","upvote_count":"2"},{"upvote_count":"2","comment_id":"597940","content":"Selected Answer: ACD\nB & E. Multi AZ doesn't increase speed it will add more time.\nF. LOB mode will ensure complete movement but not faster","timestamp":"1651889100.0","poster":"KaranGandhi30"},{"poster":"novice_expert","comment_id":"594024","upvote_count":"2","timestamp":"1651175520.0","content":"Selected Answer: ACD\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LargeTables"},{"poster":"awsmonster","timestamp":"1643401800.0","comment_id":"534946","upvote_count":"2","content":"ACF\n\nF: \n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dms-improve-speed-lob-data/\nIf you have LOBs that are larger than a few megabytes, then you can create a separate AWS DMS task with Full LOB mode. It's a best practice to create the separate task on a new replication instance to migrate these tables alone. Option A stated to create multiple DMS tasks. \n\nHaving an expensive Direct Connect will not resolve this issue"},{"poster":"jove","content":"Strange question. If you've already started the migration of a large table and observe that the DMS is slow it's too late for making such suggested changes. If you're planning to migrate a large table then go with A,C and D.","comment_id":"507380","upvote_count":"1","timestamp":"1640206800.0"},{"content":"Answer - ACD. \nF is wrong because \"Full LOB mode – In full LOB mode AWS DMS migrates all LOBs from source to target regardless of size. In this configuration, AWS DMS has no information about the maximum size of LOBs to expect. Thus, LOBs are migrated one at a time, piece by piece. Full LOB mode can be quite slow.\"\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html","poster":"lihze","comment_id":"487665","upvote_count":"1","timestamp":"1637963400.0"},{"content":"Ans A,C,D","upvote_count":"1","timestamp":"1636430160.0","poster":"alwaysAstudent","comment_id":"474603"},{"upvote_count":"2","poster":"Scunningham99","comment_id":"449030","timestamp":"1635713280.0","content":"A C and D - F is wrong full lob mode is slower as it migrates lobs to full size"},{"content":"This Q appeared in my exam.","timestamp":"1635016620.0","poster":"guru_ji","comment_id":"446130","upvote_count":"1"},{"comment_id":"430837","timestamp":"1633674420.0","comments":[{"comment_id":"439770","timestamp":"1634251560.0","upvote_count":"3","poster":"guru_ji","content":"I agree with you."}],"poster":"pcpcpc888","upvote_count":"2","content":"how can you establish a direct connect connection when you are in the process of migrating the data? you would need two weeks or more time to accomplish this. D should not be the right choice."},{"comment_id":"379445","upvote_count":"1","timestamp":"1633098900.0","poster":"Suresh108","content":"ACD for me as well"},{"comment_id":"360639","timestamp":"1632788340.0","content":"ACD final answer","poster":"Aesthet","upvote_count":"1"},{"upvote_count":"3","content":"A C D ...","comment_id":"331185","timestamp":"1632618660.0","poster":"shantest1"}]},{"id":"EuwrN8HwfxNorZwMqjlF","answer_images":[],"answer":"AC","question_images":[],"timestamp":"2021-04-04 06:55:00","url":"https://www.examtopics.com/discussions/amazon/view/49001-exam-aws-certified-database-specialty-topic-1-question-107/","answer_description":"","exam_id":22,"choices":{"E":"Create an AWS Glue job and related resources to migrate data from the Oracle database to the Aurora DB cluster. Once the initial load is complete, create an AWS DMS task to perform change data capture (CDC) until the company is ready to move all user traffic to the Aurora DB cluster.","C":"Create an AWS DMS task to migrate data from the Oracle database to the Aurora DB cluster. Select the migration type to replicate ongoing changes to keep the source and target databases in sync until the company is ready to move all user traffic to the Aurora DB cluster.","D":"Create an AWS DMS task to migrate data from the Oracle database to the Aurora DB cluster. Once the initial load is complete, create an AWS Kinesis Data Firehose stream to perform change data capture (CDC) until the company is ready to move all user traffic to the Aurora DB cluster.","B":"Use Oracle's Data Pump tool to export a copy of the source database schema and manually edit the schema in a text editor to make it compatible with Aurora.","A":"Use the AWS Schema Conversion Tool (AWS SCT) to convert the source database schema. Then restore the converted schema to the target Aurora DB cluster."},"topic":"1","isMC":true,"unix_timestamp":1617512100,"question_text":"A company is migrating a mission-critical 2-TB Oracle database from on premises to Amazon Aurora. The cost for the database migration must be kept to a minimum, and both the on-premises Oracle database and the Aurora DB cluster must remain open for write traffic until the company is ready to completely cut over to Aurora.\nWhich combination of actions should a database specialist take to accomplish this migration as quickly as possible? (Choose two.)","question_id":10,"answers_community":["AC (100%)"],"answer_ET":"AC","discussion":[{"timestamp":"1633601940.0","upvote_count":"13","poster":"Jaypdv","content":"AC. Answer","comment_id":"327744"},{"poster":"Suresh108","timestamp":"1634703240.0","comment_id":"379447","upvote_count":"6","comments":[{"content":"I agree with you.","comment_id":"439783","upvote_count":"1","poster":"guru_ji","timestamp":"1634763840.0"}],"content":"AC --- locked\n\nA - OK\nB - NOT OK Text editor\nC - ok though it doesn't talk about full load\nD- NOT ok firehose\nE - not ok Glue"},{"timestamp":"1694900520.0","upvote_count":"2","poster":"Pranava_GCP","comment_id":"1009333","content":"Selected Answer: AC\nAC are correct answer\n\nSCT + DMS"},{"poster":"FooBarBazBazinga","comment_id":"883370","timestamp":"1682673780.0","upvote_count":"1","content":"Selected Answer: AC\nAC. asnwer"},{"poster":"backbencher2022","upvote_count":"1","timestamp":"1679411940.0","content":"Selected Answer: AC\nA&C for sure","comment_id":"846114"},{"comment_id":"829715","timestamp":"1678002600.0","upvote_count":"1","content":"Selected Answer: AC\nAC --- locked","poster":"ninjalight25"},{"comment_id":"595226","poster":"novice_expert","content":"Selected Answer: AC\nA. Use the AWS Schema Conversion Tool (AWS SCT) to convert the source database schema. Then restore the converted schema to the target Aurora DB cluster.\nx B. edit\nC. Create an AWS DMS task to migrate data from the Oracle database to the Aurora DB cluster. Select the migration type to replicate ongoing changes to keep the source and target databases in sync until the company is ready to move all user traffic to the Aurora DB cluster.\nx D. kinesis firehose\nx E. Glue job","upvote_count":"2","timestamp":"1651346160.0"},{"content":"a and c all day long","poster":"Scunningham99","upvote_count":"2","timestamp":"1635553980.0","comment_id":"449033"},{"timestamp":"1635449820.0","poster":"guru_ji","comment_id":"446131","content":"I got this Question in exam.\n60% questions came in actual exam from this 145 set. Bunch of new Questions.","upvote_count":"1"},{"upvote_count":"3","poster":"Aesthet","timestamp":"1634567820.0","content":"AC final answer","comment_id":"360641"},{"upvote_count":"2","content":"A C is the answer","comment_id":"344235","timestamp":"1633736220.0","poster":"agrawalachin"}]}],"exam":{"isBeta":false,"provider":"Amazon","id":22,"isMCOnly":false,"lastUpdated":"11 Apr 2025","name":"AWS Certified Database - Specialty","isImplemented":true,"numberOfQuestions":359},"currentPage":2},"__N_SSP":true}