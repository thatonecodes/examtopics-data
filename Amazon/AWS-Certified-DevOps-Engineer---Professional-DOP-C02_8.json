{"pageProps":{"questions":[{"id":"mHUA9LFoeqSnljAEqER6","unix_timestamp":1687288320,"topic":"1","answers_community":["A (100%)"],"isMC":true,"answer_images":[],"exam_id":23,"url":"https://www.examtopics.com/discussions/amazon/view/112704-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"B":"Launch a new Amazon EC2 instance. Install all the dependencies for the legacy application on the EC2 instance. Use the EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.","C":"Create a custom EC2 Image Builder image. Install all the dependencies for the legacy application on the image. Launch a new Amazon EC2 instance from the image. Use the new EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.","D":"Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with an AWS Fargate profile that runs in multiple Availability Zones. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Use the custom Docker image inside the EKS cluster to build the deployable artifact and to save the artifact to the S3 bucket.","A":"Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Configure a new AWS CodeBuild project to use the custom Docker image to build the deployable artifact and to save the artifact to the S3 bucket."},"question_images":[],"answer":"A","discussion":[{"poster":"haazybanj","timestamp":"1690476240.0","upvote_count":"8","content":"Selected Answer: A\nThe most operationally efficient solution for automating the process of building the deployable artifact for the legacy application and storing it in an existing Amazon S3 bucket is:\n\nA. This solution leverages containerization with Docker, which allows for consistent and isolated builds, making it easier to manage application dependencies. The use of AWS CodeBuild allows for scalable and automated builds using the custom Docker image, making the process efficient and reliable. The deployable artifact can then be saved to the existing S3 bucket for future reference and deployments.","comment_id":"964907"},{"poster":"thanhnv142","timestamp":"1707232380.0","comment_id":"1142303","upvote_count":"6","content":"Selected Answer: A\nA is correct: <needs to automate the process of building the deployable artifact for the legacy application> means codebuild\nBCD dont mention codebuild, only A mentions"},{"poster":"jamesf","timestamp":"1722583260.0","upvote_count":"1","content":"Selected Answer: A\nkeywords: CodeBuild for Reusable artifacts","comment_id":"1259755"},{"upvote_count":"3","poster":"habros","content":"Selected Answer: A\nReusable artifacts = A.","comment_id":"947348","timestamp":"1688914320.0"},{"upvote_count":"2","poster":"FunkyFresco","comment_id":"934325","content":"Selected Answer: A\nOption A makes more sense to me.","timestamp":"1687777260.0"},{"content":"(A) This approach is the most operationally efficient because it leverages the benefits of containerization, such as isolation and reproducibility, as well as AWS managed services. AWS CodeBuild is a fully managed build service that can compile your source code, run tests, and produce deployable software packages. By using a custom Docker image that includes all dependencies, you can ensure that the environment in which your code is built is consistent. Using Amazon ECR to store Docker images lets you easily deploy the images to any environment. Also, you can directly upload the build artifacts to Amazon S3 from AWS CodeBuild, which is beneficial for version control and archival purposes.","comment_id":"928726","timestamp":"1687288320.0","poster":"tartarus23","upvote_count":"3"}],"timestamp":"2023-06-20 21:12:00","question_text":"A company has a legacy application. A DevOps engineer needs to automate the process of building the deployable artifact for the legacy application. The solution must store the deployable artifact in an existing Amazon S3 bucket for future deployments to reference.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?","question_id":36,"answer_ET":"A","answer_description":""},{"id":"d8JmQPIpl15QabLHE9Vm","unix_timestamp":1687287900,"topic":"1","isMC":true,"answers_community":["A (100%)"],"answer_images":[],"exam_id":23,"url":"https://www.examtopics.com/discussions/amazon/view/112703-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"A":"Update the buildspec.yml file to log in to the ECR repository by using the aws ecr get-login-password AWS CLI command to obtain an authentication token. Update the docker login command to use the authentication token to access the ECR repository.","B":"Add an environment variable of type SECRETS_MANAGER to the CodeBuild project. In the environment variable, include the ARN of the CodeBuild project's IAM service role. Update the buildspec.yml file to use the new environment variable to log in with the docker login command to access the ECR repository.","C":"Update the ECR repository to be a public image repository. Add an ECR repository policy that allows the IAM service role to have access.","D":"Update the buildspec.yml file to use the AWS CLI to assume the IAM service role for ECR operations. Add an ECR repository policy that allows the IAM service role to have access."},"question_images":[],"answer":"A","timestamp":"2023-06-20 21:05:00","discussion":[{"poster":"tartarus23","upvote_count":"7","comment_id":"928721","timestamp":"1703106300.0","content":"Selected Answer: A\n(A) When Docker communicates with an Amazon Elastic Container Registry (ECR) repository, it requires authentication. You can authenticate your Docker client to the Amazon ECR registry with the help of the AWS CLI (Command Line Interface). Specifically, you can use the \"aws ecr get-login-password\" command to get an authorization token and then use Docker's \"docker login\" command with that token to authenticate to the registry. You would need to perform these steps in your buildspec.yml file before attempting to push or pull images from/to the ECR repository."},{"upvote_count":"5","poster":"haazybanj","comment_id":"964910","timestamp":"1706381160.0","content":"Selected Answer: A\nA:\nWhen using Amazon ECR, you need to authenticate Docker to the ECR registry before pushing or pulling container images. The authentication token can be obtained using the aws ecr get-login-password AWS CLI command. The obtained token needs to be used with the docker login command to authenticate Docker to the ECR repository.\n\nBy following this approach, the CodeBuild project will have the necessary credentials to access the ECR repository, and the build job will be able to push the container image to the ECR repository successfully."},{"timestamp":"1722950640.0","upvote_count":"4","poster":"thanhnv142","comment_id":"1142322","content":"Selected Answer: A\nA is correct: <the job fails when the job tries to access the ECR repository.> This means there is problem when accessing the repo. <adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository> means have got sufficient permission. Need token to access with aws ecr get-login-password command\nBCD no mention of ecr get-login-password"},{"timestamp":"1708140540.0","poster":"ixdb","upvote_count":"3","comment_id":"983165","content":"Selected Answer: A\nA is right."},{"poster":"CirusD","upvote_count":"3","timestamp":"1706310960.0","content":"A..version: 0.2\n\nphases:\n pre_build:\n commands:\n - $(aws ecr get-login --no-include-email --region region-name)\n build:\n commands:\n - docker build -t repository-name .\n - docker tag repository-name:latest repository-uri:latest\n post_build:\n commands:\n - docker push repository-uri:latest","comment_id":"964172"}],"question_text":"A company builds a container image in an AWS CodeBuild project by running Docker commands. After the container image is built, the CodeBuild project uploads the container image to an Amazon S3 bucket. The CodeBuild project has an IAM service role that has permissions to access the S3 bucket.\n\nA DevOps engineer needs to replace the S3 bucket with an Amazon Elastic Container Registry (Amazon ECR) repository to store the container images. The DevOps engineer creates an ECR private image repository in the same AWS Region of the CodeBuild project. The DevOps engineer adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository. The DevOps engineer also places new repository information into the docker build command and the docker push command that are used in the buildspec.yml file.\n\nWhen the CodeBuild project runs a build job, the job fails when the job tries to access the ECR repository.\n\nWhich solution will resolve the issue of failed access to the ECR repository?","question_id":37,"answer_ET":"A","answer_description":""},{"id":"7BnkGRMbMfybXGoFMDTt","discussion":[{"upvote_count":"8","timestamp":"1687287600.0","poster":"tartarus23","comment_id":"928715","content":"Selected Answer: A\n(A) AWS SSO (Single Sign-On) integrates with external identity providers using SAML 2.0, and it can automatically synchronize users and groups from a connected directory using the SCIM (System for Cross-domain Identity Management) protocol. Thus, the DevOps engineer should configure the external IdP as an identity source and then configure automatic provisioning of users and groups by using the SCIM protocol. This will ensure the groups from the existing Active Directory system are available for permission management in AWS Identity and Access Management (IAM) and that employees can use their existing corporate credentials to access AWS."},{"comment_id":"1259777","upvote_count":"3","poster":"jamesf","content":"Selected Answer: A\nFor Note: SAML (Security Assertion Markup Language) is primarily used for authentication and authorization \nwhile SCIM (System for Cross-domain Identity Management) is a protocol used for automating user provisioning and deprovisioning across different systems and domains","timestamp":"1722584580.0"},{"timestamp":"1707233580.0","content":"Selected Answer: A\nA is correct: <The company wants employees to use their existing corporate credentials to access AWS> means we need to assign the existing IdP as an identity source\nB: <Configure AWS Directory Service as an identity source> is irrelevant\nC: < Configure an AD Connector as an identity source>: AD connector is use for connecting AWS active directory with that of on-prem. This question requires AWS identity Center\nD: <provisioning of users and groups by using the SAML protocol.>: SAML is an authenticate protocol. SCIM is the protocol for Idp connection","poster":"thanhnv142","comment_id":"1142325","upvote_count":"4"},{"upvote_count":"2","timestamp":"1701352320.0","poster":"zolthar_z","comment_id":"1084417","content":"Selected Answer: A\nA: Explanation: What is the difference between SCIM and SSO? SSO (single-sign on) is a way to authenticate (sign in), and SCIM is a way to provision (create an account)."},{"comment_id":"981964","timestamp":"1692130500.0","content":"This is quoted from aws documentationThe SAML protocol however does not provide a way to query the IdP to learn about users and groups. Therefore, you must make IAM Identity Center aware of those users and groups by provisioning them into IAM Identity Center.\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html","poster":"XP_2600","upvote_count":"1"},{"poster":"CirusD","comment_id":"964175","upvote_count":"1","timestamp":"1690406340.0","content":"Answer is A : AWS Single Sign-On (AWS SSO) can be integrated with an external SAML 2.0 identity provider (IdP). AWS SSO also supports automatic provisioning (auto-provisioning) of user and group information using the System for Cross-domain Identity Management (SCIM) protocol."},{"timestamp":"1690214400.0","poster":"sb333","comment_id":"961781","upvote_count":"2","content":"Selected Answer: A\nAnswer A is correct. It is SCIM that can provision users and groups in AWS. Of course the IdP needs to support SCIM (AWS has a list of IdPs that use SCIM). Answer D is not correct as SAML is an authentication protocol (cannot be used to provision users in AWS).\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/supported-idps.html"},{"content":"Selected Answer: A\nThe AWS IAM Identity Center (AWS Single Sign-On) has been configured initially. Now, to automate the provisioning of users and groups from the external IdP into AWS IAM, the engineer should choose the SCIM protocol. SCIM is specifically designed for automatic user provisioning, making it the appropriate choice for this scenario.\n\nOption D (Configure an external IdP as an identity source and use the SAML protocol) could work, but it does not address the requirement for automatic provisioning of users and groups. The use of SCIM (Option A) is preferred for automated user and group provisioning, as it is designed for this purpose.","timestamp":"1690019940.0","comment_id":"959401","upvote_count":"1","poster":"haazybanj"},{"comment_id":"953780","upvote_count":"1","poster":"Snape","timestamp":"1689560220.0","content":"Selected Answer: D\nThe company already has an external SAML 2.0 IdP, so the DevOps engineer should configure this IdP as an identity source in AWS Single Sign-On. Vs in option A would require to configure new identity source"},{"timestamp":"1688915040.0","comment_id":"947359","content":"Selected Answer: A\nA. SCIM is the automated way to provision users. You do it in AAD/AD and it propagates automatically into AWS SSO.","upvote_count":"1","poster":"habros"},{"comment_id":"944276","timestamp":"1688613420.0","content":"Selected Answer: A\nSCIM protocol is to sync the user and groups from the external identity source","upvote_count":"2","poster":"Blueee"},{"upvote_count":"1","timestamp":"1688547600.0","comment_id":"943504","content":"Selected Answer: D\nD is correct","poster":"Toptip"}],"unix_timestamp":1687287600,"answer_ET":"A","question_id":38,"question_text":"A company manually provisions IAM access for its employees. The company wants to replace the manual process with an automated process. The company has an existing Active Directory system configured with an external SAML 2.0 identity provider (IdP).\n\nThe company wants employees to use their existing corporate credentials to access AWS. The groups from the existing Active Directory system must be available for permission management in AWS Identity and Access Management (IAM). A DevOps engineer has completed the initial configuration of AWS IAM Identity Center (AWS Single Sign-On) in the company’s AWS account.\n\nWhat should the DevOps engineer do next to meet the requirements?","question_images":[],"choices":{"D":"Configure an external IdP as an identity source Configure automatic provisioning of users and groups by using the SAML protocol.","B":"Configure AWS Directory Service as an identity source. Configure automatic provisioning of users and groups by using the SAML protocol.","A":"Configure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.","C":"Configure an AD Connector as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol."},"answer_images":[],"answer":"A","timestamp":"2023-06-20 21:00:00","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/112702-exam-aws-certified-devops-engineer-professional-dop-c02/","exam_id":23,"answer_description":"","answers_community":["A (92%)","8%"],"isMC":true},{"id":"XMY4JLIyckzuCfZNwha6","timestamp":"2023-06-20 20:53:00","exam_id":23,"unix_timestamp":1687287180,"choices":{"A":"Use CloudFormation drift detection to identify noncompliant resources. Use drift detection events from CloudFormation to invoke an AWS Lambda function for remediation. Configure the Lambda function to publish logs to an Amazon CloudWatch Logs log group. Configure an Amazon CloudWatch dashboard to use the log group for tracking.","D":"Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon CloudWatch Logs to identify noncompliant resources. Use CloudWatch Logs filters for drift detection. Use Amazon EventBridge to invoke the Lambda function for remediation. Stream filtered CloudWatch logs to Amazon OpenSearch Service. Set up a dashboard on OpenSearch Service for tracking.","C":"Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources. Enable AWS Security Hub with the --no-enable-default-standards option in all the AWS accounts. Set up AWS Config managed rules and custom rules. Set up automatic remediation by using AWS Config conformance packs. For tracking, set up a dashboard on Security Hub in a designated Security Hub administrator account.","B":"Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon Athena to identify noncompliant resources. Use AWS Step Functions to track query results on Athena for drift detection and to invoke an AWS Lambda function for remediation. For tracking, set up an Amazon QuickSight dashboard that uses Athena as the data source."},"discussion":[{"comments":[{"timestamp":"1727114880.0","upvote_count":"1","content":"Lambda functions also have a execution limit of 15 minutes. If a remediation task were to take longer than that, it would fail.","comment_id":"1288269","poster":"heff_bezos"}],"content":"Selected Answer: C\nI don't think it is A because the question is asking the LEAST development overhead. Configuring Lambdas to remediate and send logs is development. It is much easier to use the built in features of AWS Config and SecurityHub","poster":"heff_bezos","timestamp":"1727114760.0","upvote_count":"1","comment_id":"1288268"},{"content":"Selected Answer: C\nC is the better solution. AWS CloudFormation drift detection helps identify whether the actual configuration of your AWS resources matches their expected configuration as defined in the CloudFormation stack template. While it is a powerful tool for maintaining compliance and consistency, it alone cannot fully prevent noncompliance due to security misconfigurations. Thats where you need AWS config to continuously monitor service configurations and even use aggregator to collect all aws config data from all member accounts in aws organization to Security Hub to provide a centralized dashboard.","upvote_count":"3","poster":"zijo","timestamp":"1719933720.0","comment_id":"1240866"},{"content":"Leaning towards \"A\" unless someone can convince me otherwise. Why?:\nI have a problem with this step in \"C\": \"Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources.\" \nThe fact is your not going to detect any \"drift\" by turning on the recorder AFTER the accounts are noncompliant.\nAWS Config rules (canned or custom) and Conformance Packs can do a lot, but it's definitely duplicating settings any security settings allready defined CloudFormation stacks. \nI lean towards \"A\" because \"To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation\".\nTherefore CloudFormation stacks are is where the security settings are defined, and thereby CloudFormation is implied to be part of the detection and remediation process.\nCloudFormation drift detection can be automated, and one can just \"automatically remediate the issue within 15 minutes of identification\" by just doing a stack refresh. Easy peasy.","upvote_count":"2","timestamp":"1719011040.0","poster":"Gomer","comments":[{"comment_id":"1243552","upvote_count":"1","timestamp":"1720298640.0","poster":"ajeeshb","content":"Correct, A is the answer."}],"comment_id":"1235033"},{"timestamp":"1713086460.0","comment_id":"1195414","poster":"dkp","upvote_count":"1","content":"Selected Answer: C\nanswer is C with minimal overhead"},{"poster":"tristan_07","comment_id":"1176670","content":"Selected Answer: A\nBoth Option A and C work. However, considering Option C involves a lot 'all the AWS accounts,' it undoubtedly increases development overhead","timestamp":"1710783420.0","upvote_count":"1"},{"comments":[{"timestamp":"1716256800.0","comment_id":"1214652","poster":"vn_thanhtung","upvote_count":"1","content":"A not correct because not mention how to remediate"}],"upvote_count":"1","comment_id":"1142333","poster":"thanhnv142","timestamp":"1707234540.0","content":"Selected Answer: A\nA is correct: drift detection is the best for this scenario, which utilizes AWS cloudformation\nB and D: using cloudtraid is for monitoring account activities\nC: AWS Config conformance packs cannot make remediation actions. It needs to trigger AWS SSM automation document"},{"timestamp":"1694987280.0","content":"C is right \nhttps://aws.amazon.com/blogs/security/optimize-aws-config-for-aws-security-hub-to-effectively-manage-your-cloud-security-posture/","upvote_count":"2","poster":"AzureDP900","comment_id":"1010087"},{"poster":"Snape","content":"Selected Answer: C\nCompliance usually indicates towards config","comment_id":"953784","upvote_count":"3","timestamp":"1689560460.0"},{"content":"Selected Answer: C\ncompliance means aws config,automatic remedy aws config,central dashboard security hub","timestamp":"1687313040.0","poster":"ds50421","upvote_count":"2","comment_id":"928961"},{"timestamp":"1687287180.0","poster":"tartarus23","upvote_count":"4","comment_id":"928709","content":"Selected Answer: C\n(C) This solution meets all of the requirements. AWS Config can monitor resource configurations for compliance with defined rules. The use of AWS Security Hub allows for centralized management of security alerts and compliance checks across all accounts. AWS Config conformance packs allow for automated remediation of non-compliant resources. AWS Security Hub provides a comprehensive view of high-priority security alerts and compliance status across AWS accounts. This solution is also the one with the least development overhead as it uses built-in AWS services specifically designed for configuration management and compliance tracking."}],"answer_ET":"C","answer":"C","question_images":[],"isMC":true,"topic":"1","answers_community":["C (88%)","13%"],"answer_images":[],"question_id":39,"url":"https://www.examtopics.com/discussions/amazon/view/112701-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":"","question_text":"A company is using AWS to run digital workloads. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations.\n\nThe company wants to enforce security standards across the entire organization. To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation. A production support team can modify resources in the production environment by using the AWS Management Console to troubleshoot and resolve application-related issues.\n\nA DevOps engineer must implement a solution to identify in near real time any AWS service misconfiguration that results in noncompliance. The solution must automatically remediate the issue within 15 minutes of identification. The solution also must track noncompliant resources and events in a centralized dashboard with accurate timestamps.\n\nWhich solution will meet these requirements with the LEAST development overhead?"},{"id":"7C36pghPx76YPAHTdu4j","choices":{"A":"All users in the Development OU will be allowed all API actions on all resources.","D":"All users in the Development OU will be denied all API actions on EC2 resources. All other API actions will be allowed.","B":"All users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.","C":"All users in the Development OU will be denied all API actions on all resources."},"url":"https://www.examtopics.com/discussions/amazon/view/112699-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":"","unix_timestamp":1687286880,"question_images":[],"question_text":"A company uses AWS Organizations to manage its AWS accounts. The organization root has an OU that is named Environments. The Environments OU has two child OUs that are named Development and Production, respectively.\n\nThe Environments OU and the child OUs have the default FullAWSAccess policy in place. A DevOps engineer plans to remove the FullAWSAccess policy from the Development OU and replace the policy with a policy that allows all actions on Amazon EC2 resources.\n\nWhat will be the outcome of this policy replacement?","timestamp":"2023-06-20 20:48:00","answer_ET":"B","question_id":40,"topic":"1","answer_images":[],"isMC":true,"answer":"B","answers_community":["B (80%)","A (20%)"],"discussion":[{"upvote_count":"13","comments":[{"poster":"MalonJay","timestamp":"1715116560.0","upvote_count":"1","content":"Very good link about SCPs.","comment_id":"1208033"}],"comment_id":"1110259","content":"Selected Answer: B\nThe key point is that \"SCP inheritance works differently for Allow and Deny policies\". Allowed policies are only inherited if the children don't have any Allow policy. Once they have an allow policy, only actions defined in that policy will be allowed and no \"Allow\" policy will be inherited from the parent(s) OUs. What inherits is the implicit Deny policy which is a hidden policy sitting above all.\n\nCheck the tables in this link:\nhttps://aws.amazon.com/blogs/security/get-more-out-of-service-control-policies-in-a-multi-account-environment/","timestamp":"1703999100.0","poster":"d262e67"},{"comment_id":"1193666","content":"Selected Answer: B\nI've just tested in my AWS account with the same scenario. I removed the SCP from the dev env and kept the EC2 policy, which by that I was denied access to all other operations except EC2.","timestamp":"1712829360.0","upvote_count":"7","poster":"devakram"},{"upvote_count":"1","timestamp":"1728908820.0","comment_id":"1297536","poster":"auxwww","content":"Selected Answer: B\nBest explanation I found in this forum\n\nFrom: learnwithaniket\n\"For a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself). This is why when you enable SCPs, AWS Organizations attaches an AWS managed SCP policy named FullAWSAccess which allows all services and actions. If this policy is removed and not replaced at any level of the organization, all OUs and accounts under that level would be blocked from taking any actions.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html\""},{"poster":"HayLLlHuK","upvote_count":"2","timestamp":"1712574420.0","comment_id":"1191541","content":"Selected Answer: B\nNote: Adding an SCP with full AWS access doesn’t give all the principals in an account access to everything. SCPs don’t grant permissions; they are used to filter permissions. Principals still need a policy within the account that grants them access."},{"timestamp":"1710605400.0","comments":[{"content":"no, I've just tested it in my account now, and B is the true answer. Although there were inherited SCPs coming from root and env which still showed in the SCP page for that OU, after detaching the allow all SCP, I was denied access on any other API except EC2.","comment_id":"1193664","timestamp":"1712829300.0","poster":"devakram","upvote_count":"2"}],"content":"Selected Answer: A\nA - Inherited SCPs cannot be removed so FullAWSAccess will still apply","upvote_count":"1","poster":"DanShone","comment_id":"1175084"},{"poster":"thanhnv142","timestamp":"1707235140.0","comment_id":"1142343","upvote_count":"2","content":"B is correct: SCP have allow statement and this matchs"},{"comment_id":"1130161","poster":"sarlos","timestamp":"1706065200.0","upvote_count":"1","content":"a is the answer"},{"content":"should be B, see example in here: https://aws.amazon.com/blogs/security/get-more-out-of-service-control-policies-in-a-multi-account-environment/","comment_id":"1085686","timestamp":"1701488940.0","upvote_count":"1","poster":"1123lluu"},{"upvote_count":"2","comment_id":"1084443","timestamp":"1701354000.0","content":"Selected Answer: A\nAnswer is A: You can't remove heritage policy from child OU","poster":"zolthar_z"},{"comment_id":"1073021","content":"Selected Answer: B\nB is the right answer.\nFor a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself). This is why when you enable SCPs, AWS Organizations attaches an AWS managed SCP policy named FullAWSAccess which allows all services and actions. If this policy is removed and not replaced at any level of the organization, all OUs and accounts under that level would be blocked from taking any actions.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html","upvote_count":"3","poster":"learnwithaniket","timestamp":"1700193660.0"},{"content":"Selected Answer: B\n\"SCP evaluation follows a deny-by-default model, meaning that any permissions not explicitly allowed in the SCPs are denied. If an allow statement is not present in the SCPs at any of the levels such as Root, Production OU or Account B, the access is denied.\"\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html#:~:text=SCP%20evaluation%20follows%20a%20deny%2Dby%2Ddefault%20model%2C%20meaning%20that%20any%20permissions%20not%20explicitly%20allowed%20in%20the%20SCPs%20are%20denied.%20If%20an%20allow%20statement%20is%20not%20present%20in%20the%20SCPs%20at%20any%20of%20the%20levels%20such%20as%20Root%2C%20Production%20OU%20or%20Account%20B%2C%20the%20access%20is%20denied.","timestamp":"1697763060.0","upvote_count":"4","comment_id":"1048274","poster":"tatarai1964"},{"poster":"jdx000","upvote_count":"1","comment_id":"1041616","comments":[{"poster":"zain1258","timestamp":"1700077680.0","comment_id":"1071828","upvote_count":"1","content":"This URL does not explain the SCP."}],"timestamp":"1697102940.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_mgmt.html"},{"poster":"Radeeka","upvote_count":"3","timestamp":"1692711420.0","content":"Selected Answer: A\nEven the default policy is removed, Child OU will inherit the SCP from the Environment OU, which is AWSFullAccess. So the Child OU will still have full access.","comment_id":"987472"},{"poster":"Gathix444","upvote_count":"2","timestamp":"1692489420.0","comment_id":"985479","content":"Its A, the new policy is an allow policy not deny, thus all permissions are gratned to Dev OU."},{"comment_id":"983173","poster":"ixdb","content":"Selected Answer: B\nSCP can define An allow list – actions are prohibited by default, and you specify what services and actions are allowed.","upvote_count":"3","timestamp":"1692236700.0"},{"content":"Selected Answer: A\nA is correct. \nDevelopment OU will inherit FullAccess from the Environments OU\nno explicit DENY in the new AllowAllEc2 Policy","comments":[{"comments":[],"poster":"Aja1","content":"The answer is B.\n\nWhen a policy is removed from an OU, the default policy for the parent OU is inherited. In this case, the default policy for the Environments OU is FullAWSAccess, which allows all API actions on all resources.\n\nWhen the DevOps engineer replaces the FullAWSAccess policy with a policy that allows all actions on Amazon EC2 resources, the new policy will take precedence over the default policy. This means that all users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.","comment_id":"978767","upvote_count":"3","timestamp":"1691768220.0"},{"comment_id":"1080077","poster":"yorkicurke","comments":[{"timestamp":"1700923680.0","comment_id":"1080079","poster":"yorkicurke","upvote_count":"1","content":"link;\nhttps://repost.aws/questions/QUSHz1PpiJTOqWRuguGn_Trw/resource-and-iam-policy-with-scp"}],"timestamp":"1700923560.0","content":"because SCPs define the maximum permissions for an organization or organizational unit (OU) in AWS Organizations. \nIf an SCP doesn’t explicitly grant permissions for an action, then that action is implicitly denied.","upvote_count":"1"}],"timestamp":"1691069580.0","poster":"vherman","upvote_count":"4","comment_id":"971144"},{"content":"Selected Answer: B\nB is the correct option.","upvote_count":"4","poster":"FunkyFresco","timestamp":"1687770300.0","comment_id":"934227"},{"comment_id":"928959","upvote_count":"4","content":"Selected Answer: B\nAll users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.","poster":"ds50421","timestamp":"1687312980.0"},{"comment_id":"928703","content":"Selected Answer: B\nAWS Organizations uses Service Control Policies (SCPs) to manage permissions across accounts within an organization. By removing the FullAWSAccess policy and replacing it with a policy that allows all actions on Amazon EC2 resources, the effect would be that users in the Development OU can perform all actions on EC2 resources, but will be denied all other AWS actions. This is because an SCP doesn't grant permissions, but instead acts as a guardrail that defines the maximum permissions users and roles can have.","upvote_count":"4","poster":"tartarus23","timestamp":"1687286880.0"}],"exam_id":23}],"exam":{"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon","isMCOnly":true,"id":23,"numberOfQuestions":355,"name":"AWS Certified DevOps Engineer - Professional DOP-C02"},"currentPage":8},"__N_SSP":true}