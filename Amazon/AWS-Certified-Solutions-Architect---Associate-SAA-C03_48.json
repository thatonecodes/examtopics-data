{"pageProps":{"questions":[{"id":"fR8XxLMSP2ujBPPVEzJA","answers_community":["D (100%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/99692-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"D","unix_timestamp":1676658900,"timestamp":"2023-02-17 19:35:00","choices":{"C":"Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.","D":"Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.","A":"Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.","B":"Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems with local access to the data."},"topic":"1","question_id":236,"answer_images":[],"discussion":[{"content":"Selected Answer: D\nThe question states, \"wants to maintain local access to all the data\" This is storage gateway. Cached gateway stores only the frequently accessed data locally which is not what the problem statement asks for.","upvote_count":"20","timestamp":"1709413020.0","poster":"Steve_4542636","comment_id":"827426"},{"poster":"kruasan","comment_id":"884538","upvote_count":"12","timestamp":"1714409580.0","content":"Selected Answer: D\n1. The company wants to maintain local access to all the data. Only stored volumes keep the complete dataset on-premises, providing low-latency access. Cached volumes only cache a subset locally.\n2. The company wants the data backed up on AWS. With stored volumes, periodic backups (snapshots) of the on-premises data are sent to S3, providing durable and scalable backup storage.\n3. The company wants the data transfer to AWS to be automatic and secure. Storage Gateway provides an encrypted connection between the on-premises gateway and AWS storage. Backups to S3 are sent asynchronously and automatically based on the backup schedule configured."},{"content":"Selected Answer: D\nlocal acess to all data","poster":"AdamVigas","upvote_count":"1","timestamp":"1739531340.0","comment_id":"1356423"},{"poster":"TariqKipkemei","timestamp":"1727938260.0","comment_id":"1023689","content":"Selected Answer: D\nThe Volume Gateway runs in either a cached or stored mode.\nIn the cached mode, your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access.\nIn the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.\n\nhttps://aws.amazon.com/storagegateway/faqs/#:~:text=What%20is%20Volume%20Gateway%3F","upvote_count":"9"},{"upvote_count":"3","content":"Selected Answer: D\n@kruasan well explained","timestamp":"1725558060.0","poster":"Guru4Cloud","comment_id":"999758"},{"upvote_count":"4","comment_id":"816545","timestamp":"1708518120.0","content":"Ans = D\n\nhttps://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html","poster":"ChrisG1454"},{"upvote_count":"3","timestamp":"1708269060.0","content":"D\nhttps://www.examtopics.com/discussions/amazon/view/43725-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"Neha999","comment_id":"813166"},{"poster":"bdp123","upvote_count":"3","content":"Selected Answer: D\nhttps://aws.amazon.com/storagegateway/faqs/#:~:text=In%20the%20cached%20mode%2C%20your,asynchronously%20backed%20up%20to%20AWS.\nIn the cached mode, your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access.\nIn the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.","timestamp":"1708194900.0","comment_id":"812223"}],"exam_id":31,"answer_ET":"D","question_images":[],"question_text":"A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred.\n\nWhich solution meets these requirements?","isMC":true},{"id":"Ho1GPisWY3IR37dfDUwl","discussion":[{"comment_id":"999756","upvote_count":"8","content":"Selected Answer: B\nThe correct answer is B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.\n\nA gateway VPC endpoint is a private way for Amazon EC2 instances in a VPC to access AWS services, such as Amazon S3, without having to go through the internet. This can help to improve security and performance.","timestamp":"1725557760.0","poster":"Guru4Cloud"},{"content":"Selected Answer: B\nS3 and DynamoDB are the only services with Gateway endpoint options","upvote_count":"7","poster":"Steve_4542636","comment_id":"827427","timestamp":"1709413080.0"},{"poster":"TariqKipkemei","timestamp":"1727938440.0","comment_id":"1023693","content":"Selected Answer: B\nSet up a gateway VPC endpoint for Amazon S3 in the VPC.","upvote_count":"4"},{"upvote_count":"2","content":"Agree with B","comment_id":"817308","poster":"ManOnTheMoon","timestamp":"1708562100.0"},{"upvote_count":"2","comment_id":"817248","poster":"jennyka76","timestamp":"1708555200.0","content":"ANSWER - B\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.htmlR B"},{"poster":"LuckyAro","timestamp":"1708538400.0","upvote_count":"2","content":"Selected Answer: B\nB is correct","comment_id":"816961"},{"content":"Selected Answer: B\nBbbbbbbb","comment_id":"814310","upvote_count":"4","timestamp":"1708363500.0","poster":"skiwili"}],"question_id":237,"answer":"B","exam_id":31,"choices":{"B":"Set up a gateway VPC endpoint for Amazon S3 in the VPC.","A":"Create a private hosted zone by using Amazon Route 53.","C":"Configure the EC2 instances to use a NAT gateway to access the S3 bucket.","D":"Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket."},"answers_community":["B (100%)"],"answer_description":"","isMC":true,"question_images":[],"unix_timestamp":1676827500,"url":"https://www.examtopics.com/discussions/amazon/view/99954-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"B","topic":"1","answer_images":[],"timestamp":"2023-02-19 18:25:00","question_text":"An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet.\n\nHow should a solutions architect configure access to meet these requirements?"},{"id":"7DJVtM5NLFBBWWuh3bsC","discussion":[{"timestamp":"1677790920.0","comments":[{"upvote_count":"2","content":"\"Akshually\" bro go to sleep","comment_id":"1260461","timestamp":"1722725940.0","poster":"1e22522"},{"comments":[{"comment_id":"1125429","content":"Macie is for identifying the PII data. Here it's much simpler because one of the apps need the PII data and other apps don't so you don't need to identify the PII data as you know it is already there. You just need to identify the app that needs the data which is not the best use case for Macie","upvote_count":"4","timestamp":"1705534500.0","poster":"awsgeek75"}],"comment_id":"1086770","poster":"Mikado211","upvote_count":"3","timestamp":"1701602460.0","content":"Yes. That's the problem here, Macie is the recommended tool in such case, but you do not have it in the answers."}],"comment_id":"827430","content":"Selected Answer: B\nActually this is what Macie is best used for.","poster":"Steve_4542636","upvote_count":"19"},{"poster":"fruto123","content":"Selected Answer: B\nB is the right answer and the proof is in this link.\n\nhttps://aws.amazon.com/blogs/aws/introducing-amazon-s3-object-lambda-use-your-code-to-process-data-as-it-is-being-retrieved-from-s3/","comment_id":"817832","comments":[{"comments":[{"timestamp":"1703851140.0","poster":"pentium75","comment_id":"1108649","content":"But it matches the exact use case here.","upvote_count":"2"},{"timestamp":"1705534380.0","poster":"awsgeek75","content":"Why do you think this is wrong?","comment_id":"1125426","upvote_count":"2"}],"poster":"Guru4Cloud","upvote_count":"1","content":"This is so wrong","timestamp":"1693933980.0","comment_id":"999736"}],"upvote_count":"12","timestamp":"1677070320.0"},{"timestamp":"1737395760.0","upvote_count":"1","poster":"Rcosmos","content":"Selected Answer: B\nPor que esta é a melhor solução?\nProcessamento sob demanda:\n\nO S3 Object Lambda permite que você processe e transforme objetos armazenados no S3 dinamicamente, com base na solicitação do aplicativo.\nPara os dois aplicativos que não precisam das PII, o Object Lambda pode remover essas informações no momento da solicitação, sem duplicar ou alterar os dados originais.","comment_id":"1343770"},{"upvote_count":"2","comment_id":"1273738","timestamp":"1724809320.0","poster":"AWSSURI","content":"I miss you Burugudystunstuguy....Man's been conistent with his answers"},{"comment_id":"1148866","timestamp":"1707792900.0","content":"Selected Answer: C\n[GPT4] while S3 Object Lambda is a powerful tool for real-time data transformation, it is not the best fit for processing very large datasets due to Lambda's execution limits(15 min). Instead, preprocessing the data and storing it in separate S3 buckets for each applicatin's needs is a more operationally efficient solution for the scenario describes.","upvote_count":"1","poster":"MikeJANG"},{"poster":"pentium75","content":"Selected Answer: B\nBecause this is exactly what the AWS blog says.\n\n\"When you store data in Amazon Simple Storage Service (Amazon S3), you can easily share it for use by multiple applications. However, each application has its own requirements and may need a different view of the data. For example, a dataset created by an e-commerce application may include personally identifiable information (PII) that is not needed when the same data is processed for analytics and should be redacted.\"","comment_id":"1108651","upvote_count":"7","comments":[{"comment_id":"1108652","upvote_count":"8","poster":"pentium75","timestamp":"1703851260.0","content":"\"Today, I’m very happy to announce the availability of S3 Object Lambda, a new capability that allows you to add your own code to process data retrieved from S3 before returning it to an application. S3 Object Lambda works with your existing applications and uses AWS Lambda functions to automatically process and transform your data as it is being retrieved from S3. The Lambda function is invoked inline with a standard S3 GET request, so you don’t need to change your application code.\"\n\n\nhttps://aws.amazon.com/de/blogs/aws/introducing-amazon-s3-object-lambda-use-your-code-to-process-data-as-it-is-being-retrieved-from-s3/"}],"timestamp":"1703851260.0"},{"timestamp":"1701314520.0","comment_id":"1083956","poster":"meowruki","content":"B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.\n\nThis solution allows you to use S3 Object Lambda to process and transform the data on-the-fly as it is requested by each application. S3 Object Lambda enables you to apply custom code to your data retrieval requests, allowing you to remove PII before returning the data to the requesting application. This eliminates the need to create and manage separate storage locations for each application, reducing operational overhead.","upvote_count":"3"},{"comments":[{"comment_id":"1125431","upvote_count":"2","content":"Least operational overhead. The DevOps team is throwing this problem to the developers which is why C is not best.","timestamp":"1705534560.0","poster":"awsgeek75"}],"content":"Selected Answer: C\nWhy would you reprocess the data every time you request it when you can just filter it once and be done?\nBecause of this I think A and B are highly inefficient, leaving us with C and D as options.\nSince S3 is better suited for Data Lakes, I think C is the answer.","upvote_count":"2","poster":"rvca231","timestamp":"1698213060.0","comment_id":"1053490"},{"timestamp":"1685686380.0","upvote_count":"2","content":"Selected Answer: B\nStore the data in an Amazon S3 bucket and using S3 Object Lambda to process and transform the data before returning it to the requesting application. This approach allows the PII to be removed in real-time and without the need to create separate datasets or tables for each application.","comment_id":"912576","poster":"Abrar2022"},{"upvote_count":"2","comments":[{"comments":[{"timestamp":"1690644420.0","comment_id":"966497","content":"Terabyte is just the storage. Lambda only need to process which application request. Think like removing/scratching off your social security number before sharing your doc to a third party.","upvote_count":"3","poster":"Kp88"}],"upvote_count":"2","poster":"antropaws","comment_id":"907119","timestamp":"1685083020.0","content":"Chat GPT:\n\nIsn't just 60 seconds the maximum duration for a Lambda function used by S3 Object Lambda? How can it process terabytes of data in 60 seconds?\n\nYou are correct that the maximum duration for a Lambda function used by S3 Object Lambda is 60 seconds.\n\nGiven the time constraint, it is not feasible to process terabytes of data within a single Lambda function execution. \n\nS3 Object Lambda is designed for lightweight and real-time transformations rather than extensive processing of large datasets.\n\nTo handle terabytes of data, you would typically need to implement a distributed processing solution using services like Amazon EMR, AWS Glue, or AWS Batch. These services are specifically designed to handle big data workloads and provide scalability and distributed processing capabilities.\n\nSo, while S3 Object Lambda can be useful for lightweight processing tasks, it is not the appropriate tool for processing terabytes of data within the execution time limits of a Lambda function."}],"content":"Selected Answer: A\n@fruto123 and everyone that upvoted: \n\nIs it plausible that S3 Object Lambda can process terabytes of data in 60 seconds? The same link you shared states that the maximum duration for a Lambda function used by S3 Object Lambda is 60 seconds.\n\nAnswer is A.","poster":"antropaws","timestamp":"1685082540.0","comment_id":"907114"},{"comments":[{"comment_id":"884544","upvote_count":"2","timestamp":"1682787420.0","content":"Option A requires developing and managing a proxy app layer to handle data transformation, adding overhead.\nOptions C and D require preprocessing and storing multiple copies of the transformed data, adding storage and management overhead.\nOption B using S3 Object Lambda minimizes operational overhead by handling data transformation on read transparently using the native S3 functionality. Only one raw data copy is stored in S3, with no additional applications required.","poster":"kruasan"}],"timestamp":"1682787420.0","poster":"kruasan","upvote_count":"4","comment_id":"884543","content":"Selected Answer: B\n• Storing the raw data in S3 provides a durable, scalable data lake. S3 requires little ongoing management overhead.\n• S3 Object Lambda can be used to filter and process the data on retrieval transparently. This minimizes operational overhead by avoiding the need to preprocess and store multiple transformed copies of the data.\n• Only one copy of the data needs to be stored and maintained in S3. S3 Object Lambda will transform the data on read based on the requesting application.\n• No additional applications or proxies need to be developed and managed to handle the data transformation. S3 Object Lambda provides this functionality."},{"comment_id":"817776","upvote_count":"5","timestamp":"1677065880.0","content":"Selected Answer: B\nhttps://aws.amazon.com/ko/blogs/korea/introducing-amazon-s3-object-lambda-use-your-code-to-process-data-as-it-is-being-retrieved-from-s3/","poster":"pagom"},{"comment_id":"816983","content":"Selected Answer: B\nB is the correct answer.\nAmazon S3 Object Lambda allows you to add custom code to S3 GET requests, which means that you can modify the data before it is returned to the requesting application. In this case, you can use S3 Object Lambda to remove the PII before the data is returned to the two applications that do not need to process PII. This approach has the least operational overhead because it does not require creating separate datasets or proxy application layers, and it allows you to maintain a single copy of the data in an S3 bucket.","upvote_count":"5","poster":"LuckyAro","timestamp":"1677003660.0"},{"upvote_count":"4","content":"To meet the requirement of removing the PII before processing by two of the applications, it would be most efficient to use option B, which involves storing the data in an Amazon S3 bucket and using S3 Object Lambda to process and transform the data before returning it to the requesting application. This approach allows the PII to be removed in real-time and without the need to create separate datasets or tables for each application. S3 Object Lambda can be configured to automatically remove PII from the data before it is sent to the non-PII processing applications. This solution provides a cost-effective and scalable way to meet the requirement with the least operational overhead.","timestamp":"1676907120.0","poster":"NolaHOla","comment_id":"815458"},{"content":"Selected Answer: B\nI think it is B.","upvote_count":"2","comment_id":"814746","timestamp":"1676860020.0","poster":"minglu"},{"content":"Selected Answer: C\nLooks like C is the correct answer","upvote_count":"2","poster":"skiwili","comment_id":"814314","timestamp":"1676827740.0"}],"answers_community":["B (89%)","8%"],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/99956-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"B","answer_description":"","unix_timestamp":1676827740,"timestamp":"2023-02-19 18:29:00","question_text":"An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","question_id":238,"question_images":[],"answer_ET":"B","isMC":true,"choices":{"A":"Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.","B":"Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.","D":"Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.","C":"Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket."},"answer_images":[],"exam_id":31},{"id":"BRTxl76vrSkxMjneJnsz","answer_ET":"D","discussion":[{"content":"Selected Answer: D\n10.0.1.0/32 and 192.168.1.0/32 are too small for VPC, and /32 network is only 1 host \n192.168.0.0/24 is overlapping with existing VPC","poster":"BrainOBrain","timestamp":"1692454140.0","comment_id":"814242","upvote_count":"30"},{"timestamp":"1698606060.0","content":"Selected Answer: D\n• Option A (10.0.1.0/32) is invalid - a /32 CIDR prefix is a host route, not a VPC range.\n• Option B (192.168.0.0/24) overlaps the development VPC and so cannot be used.\n• Option C (192.168.1.0/32) is invalid - a /32 CIDR prefix is a host route, not a VPC range.\n• Option D (10.0.1.0/24) satisfies the non-overlapping CIDR requirement but is a larger block than needed. Since only two VPCs need to be peered, a /24 block provides more addresses than necessary.","poster":"kruasan","comment_id":"884546","upvote_count":"13"},{"comment_id":"1160003","content":"Selected Answer: D\nIn an Amazon VPC, the first four and the last IP address in each subnet are reserved for specific purposes, and they cannot be used for customer instances. Here's how the reserved addresses are typically allocated:\n\nNetwork Address (First IP):\n\nThe first IP address (all zeros in the host portion) in a subnet is reserved as the network address. For example, if you have a subnet with a CIDR notation of 10.0.0.0/24, the network address would be 10.0.0.0.\nVPC Router (Second IP):\n\nThe second IP address in the subnet is reserved for the VPC router.\nDNS Server (Third IP):\n\nThe third IP address is reserved for the DNS server.\nReserved for Future Use (Fourth IP):\n\nThe fourth IP address is reserved for future use.\nCustomer Instances (Fifth to Second-to-Last IP):\n\nThe IP addresses from the fifth to the second-to-last IP address in the subnet are available for customer instances.\nBroadcast Address (Last IP):\n\nThe last IP address (all ones in the host portion) in a subnet is reserved as the broadcast address, even though AWS does not support broadcast.","poster":"TheFivePips","timestamp":"1724691240.0","upvote_count":"6"},{"timestamp":"1724315580.0","poster":"walter9660","content":"Selected Answer: C\n10.0.0.0 - 10.255.255.255 (10/8 prefix): Example CIDR block: 10.0.0.0/16\n172.16.0.0 - 172.31.255.255 (172.16/12 prefix): Example CIDR block: 172.31.0.0/16\n192.168.0.0 - 192.168.255.255 (192.168/16 prefix): Example CIDR block: 192.168.0.0/20\nGiven that the development VPC already uses 192.168.0.0/24, we need to choose a non-overlapping CIDR block. The smallest valid CIDR block that meets the requirements is 192.168.1.0/24 (Option C).","upvote_count":"1","comment_id":"1156300"},{"poster":"Murtadhaceit","comment_id":"1088534","timestamp":"1717588320.0","upvote_count":"2","content":"Selected Answer: D\nA and C are host IP addresses. \nB is not possible because it's using the same subnet for the other team/department. \nWe are left with D, which is the right answer."},{"content":"Selected Answer: D\n10.0.1.0/32 and 192.168.1.0/32 are too small for VPC, and /32 network is only 1 host\n192.168.0.0/24 is overlapping with existing VPC","comment_id":"999722","poster":"Guru4Cloud","timestamp":"1709664840.0","upvote_count":"2"},{"timestamp":"1701505020.0","upvote_count":"2","comment_id":"912578","content":"Definitely D. The only valid VPC CIDR block that does not overlap with the development VPC CIDR block among the options. The other 2 CIDR block options are too small.","poster":"Abrar2022"},{"content":"Selected Answer: D\nD is correct.","comment_id":"907122","poster":"antropaws","timestamp":"1700988060.0","upvote_count":"2"},{"content":"Selected Answer: D\nD is the only correct answer","comment_id":"857560","upvote_count":"2","poster":"channn","timestamp":"1696131600.0"},{"timestamp":"1695320820.0","comment_id":"846316","poster":"r04dB10ck","content":"Selected Answer: D\nonly one valid with no overlap","upvote_count":"2"},{"content":"Selected Answer: D\nA process by elimination solution here. a CIDR value is the number of bits that are lockeed so 10.0.0.0/32 means no range.","comment_id":"827438","poster":"Steve_4542636","timestamp":"1693681560.0","upvote_count":"4"},{"upvote_count":"2","timestamp":"1692638820.0","comment_id":"817060","content":"Selected Answer: D\nAnswer is D, 10.0.1.0/24.","poster":"LuckyAro"},{"upvote_count":"2","comment_id":"814320","poster":"skiwili","content":"Selected Answer: D\nYes D is the answer","timestamp":"1692459240.0"},{"upvote_count":"2","poster":"obatunde","comment_id":"814186","content":"Selected Answer: D\nDefinitely D. It is the only valid VPC CIDR block that does not overlap with the development VPC CIDR block among the options.","timestamp":"1692449340.0"},{"comment_id":"811966","poster":"bdp123","content":"Selected Answer: D\nThe allowed block size is between a /28 netmask and /16 netmask.\nThe CIDR block must not overlap with any existing CIDR block that's associated with the VPC.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/configure-your-vpc.html","timestamp":"1692274800.0","upvote_count":"6"}],"question_images":[],"answer_description":"","answers_community":["D (99%)","1%"],"question_text":"A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC.\n\nWhat is the SMALLEST CIDR block that meets these requirements?","timestamp":"2023-02-17 15:20:00","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/99651-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"isMC":true,"topic":"1","question_id":239,"choices":{"D":"10.0.1.0/24","A":"10.0.1.0/32","B":"192.168.0.0/24","C":"192.168.1.0/32"},"exam_id":31,"unix_timestamp":1676643600},{"id":"zYhgCRAv7i1vfBSiGbqr","isMC":true,"answer_description":"","answer":"B","answer_ET":"B","choices":{"D":"Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.","B":"Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.","C":"Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.","A":"Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group."},"topic":"1","exam_id":31,"question_text":"A company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%.\n\nA solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur.\n\nWhich solution will meet these requirements?","answers_community":["B (94%)","6%"],"question_id":240,"question_images":[],"discussion":[{"timestamp":"1692275400.0","poster":"bdp123","comment_id":"811978","content":"Selected Answer: B\nJust create an auto scaling policy","upvote_count":"17"},{"poster":"vilagiri","content":"I picked B.. I am not 100% sure..The application is deployed in 5 instances initially. What is the logic behind 2/3/6 ASG. Because utilization is 10%, we can set min 2? I know for sure I am not going to get this ASG question correct in the exam.","upvote_count":"4","comment_id":"1021016","timestamp":"1711746420.0"},{"timestamp":"1709664540.0","comment_id":"999718","content":"Selected Answer: B\nThe correct answer is B.\n\nThis solution will meet the requirements because it will:\n\nAutomate the scalability of the application by using EC2 Auto Scaling.\nOptimize the cost of the architecture by only scaling the number of EC2 instances up when needed.\nEnsure that the application has enough CPU resources when surges occur by setting the target value of the target tracking scaling policy to 50%.","poster":"Guru4Cloud","upvote_count":"3"},{"comment_id":"939389","content":"Wrong answers: Options A, C, and D are not the most appropriate solutions:\n\nOption A suggests creating a CloudWatch alarm to terminate an EC2 instance when CPU utilization is less than 20%. However, this approach does not ensure that the application will have enough CPU resources during surges, as it only terminates instances when CPU utilization is low, which may not meet the requirements.\nOption C suggests creating an Auto Scaling group without any specific scaling policies or configurations. This approach does not address the need for automated scaling based on CPU utilization, making it insufficient for the given requirements.\nOption D suggests using CloudWatch alarms to send notifications via Amazon SNS and manually adjusting the number of instances based on the received messages. This approach lacks automation and requires manual intervention, which does not optimize cost or meet the requirement of automated scalability.\nTherefore, Option B is the most appropriate solution in this case.","timestamp":"1704063660.0","poster":"ajchi1980","upvote_count":"3"},{"poster":"RoroJ","upvote_count":"2","comments":[{"poster":"pentium75","comment_id":"1108657","content":"\"After receiving the message, log in to decrease or increase the number of EC2 instances that are running\" does surely not \"automate the scalability\".","timestamp":"1719655620.0","upvote_count":"3"}],"comment_id":"904890","content":"Selected Answer: D\nAuto Scaling group must have an AMI for it.","timestamp":"1700749680.0"},{"content":"how can we set max to 6 since the company is using 5 ec2 instance","poster":"th3k33n","timestamp":"1700109600.0","upvote_count":"2","comments":[{"content":"In the scenario you provided, you're setting up an Auto Scaling group to manage the instances for you, and the settings (min 2, desired 3, max 6) are for the Auto Scaling group, not for your existing instances. When you integrate the instances into the Auto Scaling group, you are effectively moving from a fixed instance count to a dynamic one that can range from 2 to 6 based on the demand.\nThe existing 5 instances can be included in the Auto Scaling group, but the group can reduce the number of instances if the load is low (to the minimum specified, which is 2 in this case) and can also add more instances (up to a maximum of 6) if the load increases.","timestamp":"1700380680.0","comment_id":"901670","poster":"examtopictempacc","upvote_count":"4"}],"comment_id":"898842"},{"poster":"kruasan","timestamp":"1698606240.0","upvote_count":"3","content":"Selected Answer: B\nReasons:\n• An Auto Scaling group will automatically scale the EC2 instances to match changes in demand. This optimizes cost by only running as many instances as needed.\n• A target tracking scaling policy monitors the ASGAverageCPUUtilization metric and scales to keep the average CPU around the 50% target value. This ensures there are enough resources during CPU surges.\n• The ALB and target group are reused, so the application architecture does not change. The Auto Scaling group is associated to the existing load balancer setup.\n• A minimum of 2 and maximum of 6 instances provides the ability to scale between 3 and 6 instances as needed based on demand.\n• Costs are optimized by starting with only 3 instances (the desired capacity) and scaling up as needed. When CPU usage drops, instances are terminated to match the desired capacity.","comments":[{"timestamp":"1698606240.0","content":"Option A - terminates instances reactively based on low CPU and may not provide enough capacity during surges. Does not optimize cost.\nOption C - lacks a scaling policy so will not automatically adjust capacity based on changes in demand. Does not ensure enough resources during surges.\nOption D - requires manual intervention to scale capacity. Does not optimize cost or provide an automated solution.","poster":"kruasan","comment_id":"884550","upvote_count":"2"}],"comment_id":"884549"},{"comment_id":"881044","content":"as you dig down the question, they get more and more bogus with less and less votes","timestamp":"1698287520.0","poster":"darn","upvote_count":"1"},{"poster":"Steve_4542636","content":"Selected Answer: B\nB is my vote","upvote_count":"2","timestamp":"1693681740.0","comment_id":"827445"},{"timestamp":"1692686760.0","upvote_count":"2","comments":[{"poster":"Shrestwt","comments":[{"comment_id":"1108658","content":"\"Create an EC2 Auto Scaling group\" includes replacing your existing EC2 instances with a launch configuration that starts and stops instances automatically.","upvote_count":"2","poster":"pentium75","timestamp":"1719655680.0"}],"upvote_count":"2","content":"But the company is using only 5 EC2 Instances so how can we set maximum instance to 6.","timestamp":"1697673420.0","comment_id":"874190"}],"content":"Based on the information given, the best solution is option\"B\".\nAutoscaling group with target tracking scaling policy with min 2 instances, desired capacity to 3, and the maximum instances to 6.","poster":"KZM","comment_id":"817624"},{"upvote_count":"3","poster":"LuckyAro","timestamp":"1692637680.0","comment_id":"817040","content":"Selected Answer: B\nB is the correct solution because it allows for automatic scaling based on the average CPU utilization of the EC2 instances in the target group. With the use of a target tracking scaling policy based on the ASGAverageCPUUtilization metric, the EC2 Auto Scaling group can ensure that the target value of 50% is maintained while scaling the number of instances in the group up or down as needed. This will help ensure that the application has enough CPU resources during surges without overprovisioning, thus optimizing the cost of the architecture."},{"timestamp":"1692553800.0","upvote_count":"2","content":"Selected Answer: B\nShould be B","comment_id":"815758","poster":"Babba"}],"url":"https://www.examtopics.com/discussions/amazon/view/99652-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"timestamp":"2023-02-17 15:30:00","unix_timestamp":1676644200}],"exam":{"isImplemented":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025","id":31,"isBeta":false,"numberOfQuestions":1019,"isMCOnly":true,"provider":"Amazon"},"currentPage":48},"__N_SSP":true}