{"pageProps":{"questions":[{"id":"tAffH4MGUPDYiRCNUXn5","answer_ET":"B","question_text":"A large telecommunications company is planning to set up a data catalog and metadata management for multiple data sources running on AWS. The catalog will be used to maintain the metadata of all the objects stored in the data stores. The data stores are composed of structured sources like Amazon RDS and Amazon\nRedshift, and semistructured sources like JSON and XML files stored in Amazon S3. The catalog must be updated on a regular basis, be able to detect the changes to object metadata, and require the least possible administration.\nWhich solution meets these requirements?","question_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/64637-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"B","discussion":[{"content":"Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/crawler-data-stores.html","comment_id":"467208","upvote_count":"17","timestamp":"1634669580.0","poster":"srinivasa"},{"upvote_count":"7","poster":"cloudlearnerhere","comment_id":"705839","timestamp":"1666896060.0","comments":[{"upvote_count":"2","content":"Option D Is wrong and extracting schema is a manual work","comment_id":"705841","poster":"cloudlearnerhere","timestamp":"1666896360.0"}],"content":"Correct answer is B as AWS Glue Data Catalog can act as the central metadata repository with Glue Crawlers which can connect to multiple data stores and update the Data Catalog with metadata changes. \n\nOptions A & C are wrong they would increase the administration work. \n\nOption D is wrong as Glue Crawlers can connect to all the mentioned datastores. \n\nhttps://docs.aws.amazon.com/glue/latest/dg/crawler-data-stores.html"},{"upvote_count":"2","poster":"pk349","comment_id":"886604","timestamp":"1682966100.0","content":"B: I passed the test"},{"upvote_count":"3","comment_id":"638464","content":"Selected Answer: B\nSelected Answer: B","poster":"rocky48","timestamp":"1658984820.0"},{"comment_id":"604840","upvote_count":"2","content":"Selected Answer: B\nMy Answer is B","poster":"Bik000","timestamp":"1653131520.0"},{"timestamp":"1647737100.0","poster":"moon2351","content":"Selected Answer: B\nAnswer is B","comment_id":"571328","upvote_count":"2"},{"content":"Answer B","timestamp":"1636863000.0","upvote_count":"4","poster":"ali98","comment_id":"477930"}],"unix_timestamp":1635129480,"answers_community":["B (100%)"],"topic":"1","exam_id":20,"question_images":[],"answer_images":[],"isMC":true,"choices":{"C":"Use Amazon DynamoDB as the data catalog. Create AWS Lambda functions that will connect and gather the metadata information from multiple sources and update the DynamoDB catalog. Schedule the Lambda functions periodically.","D":"Use the AWS Glue Data Catalog as the central metadata repository. Extract the schema for RDS and Amazon Redshift sources and build the Data Catalog. Use AWS crawlers for data stored in Amazon S3 to infer the schema and automatically update the Data Catalog.","B":"Use the AWS Glue Data Catalog as the central metadata repository. Use AWS Glue crawlers to connect to multiple data stores and update the Data Catalog with metadata changes. Schedule the crawlers periodically to update the metadata catalog.","A":"Use Amazon Aurora as the data catalog. Create AWS Lambda functions that will connect and gather the metadata information from multiple sources and update the data catalog in Aurora. Schedule the Lambda functions periodically."},"timestamp":"2021-10-25 04:38:00","answer_description":""},{"id":"n6dCL17lrO9LGl1OYJIL","answer_images":[],"question_images":[],"exam_id":20,"unix_timestamp":1635130260,"choices":{"C":"Create an IAM role for QuickSight to access Amazon Redshift.","D":"Use a QuickSight admin user for creating the dataset.","A":"Grant the SELECT permission on Amazon Redshift tables.","B":"Add the QuickSight IP address range into the Amazon Redshift security group."},"discussion":[{"poster":"cloudlearnerhere","comment_id":"705859","upvote_count":"8","content":"Correct answer is B as the error is time out and not permissions denied, the most likely reason is the Redshift Security Group does not allow QuickSight IP address range. \nFor Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in that AWS Region.\n\nOptions A, C & D are wrong as the error is a timeout and not access denied.\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html","timestamp":"1666897980.0"},{"content":"B it is \nsecurity group contains an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in that AWS Region.\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html","timestamp":"1640061960.0","poster":"lakediver","upvote_count":"6","comment_id":"505821"},{"timestamp":"1688797140.0","upvote_count":"1","content":"Answer : B","poster":"ogerber","comment_id":"946217"},{"upvote_count":"2","comment_id":"886605","poster":"pk349","timestamp":"1682966220.0","content":"B: I passed the test"},{"poster":"rocky48","timestamp":"1658380140.0","upvote_count":"2","content":"Selected Answer: B\nAnswer : B","comment_id":"634366"},{"poster":"dushmantha","comment_id":"626914","upvote_count":"2","timestamp":"1656924480.0","content":"Selected Answer: B\nAnswer seems to be B due to connection timeout issue"},{"timestamp":"1653124320.0","content":"If IAM permission is not there, then it would error out immediately. Time out is usually caused by missing security group permissions. Hence B","comment_id":"604775","poster":"certificationJunkie","upvote_count":"2"},{"poster":"jrheen","comment_id":"595252","upvote_count":"1","timestamp":"1651349400.0","content":"ANswer : B"},{"poster":"MWL","upvote_count":"2","timestamp":"1651198200.0","content":"Selected Answer: B\nB. Connection timeout is always because of the network conenctivity. Check security group tp allow IP and port access.","comment_id":"594172"},{"comment_id":"594109","poster":"finnliang","content":"Selected Answer: B\nvote for B","upvote_count":"1","timestamp":"1651189860.0"},{"upvote_count":"3","poster":"umatrilok","comment_id":"488193","timestamp":"1638023100.0","content":"Error message is \"Creating a connection to your data source timed out\". Time out errors occur due to security group restrictions. Therefore, answer is B."},{"upvote_count":"3","timestamp":"1637382240.0","content":"Answer : B","poster":"aws2019","comment_id":"482257"},{"timestamp":"1636862940.0","poster":"ali98","upvote_count":"1","comment_id":"477929","content":"Answer : B"},{"content":"the answer should be B. The other options can be ruled out easily","poster":"Chints01","comment_id":"477274","upvote_count":"2","timestamp":"1636778400.0"},{"upvote_count":"1","content":"Answer : B","poster":"srinivasa","timestamp":"1635168240.0","comment_id":"467213"}],"question_text":"An ecommerce company is migrating its business intelligence environment from on premises to the AWS Cloud. The company will use Amazon Redshift in a public subnet and Amazon QuickSight. The tables already are loaded into Amazon Redshift and can be accessed by a SQL tool.\nThe company starts QuickSight for the first time. During the creation of the data source, a data analytics specialist enters all the information and tries to validate the connection. An error with the following message occurs: `Creating a connection to your data source timed out.`\nHow should the data analytics specialist resolve this error?","timestamp":"2021-10-25 04:51:00","topic":"1","answers_community":["B (100%)"],"answer_ET":"B","answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/64638-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":22,"answer":"B"},{"id":"Jgq55nwQtgapvkO6QLkC","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/64636-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":23,"timestamp":"2021-10-25 04:34:00","answer":"C","discussion":[{"upvote_count":"10","comment_id":"467207","timestamp":"1634106660.0","content":"Answer: C\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html","poster":"srinivasa"},{"content":"Option A is correct. Checked in Bing app","timestamp":"1706373180.0","comment_id":"1133485","upvote_count":"2","poster":"kalakonda"},{"timestamp":"1691113620.0","poster":"whenthan","upvote_count":"3","content":"Selected Answer: C\nKinesis Data Streams records are available to be read immediately after they are written. There are some use cases that need to take advantage of this and require consuming data from the stream as soon as it is available. You can significantly reduce the propagation delay by overriding the KCL default settings to poll more frequently","comment_id":"971606"},{"poster":"pk349","timestamp":"1682966280.0","comment_id":"886607","content":"C: I passed the test","upvote_count":"2"},{"timestamp":"1679134560.0","poster":"np2021","comment_id":"842656","upvote_count":"2","content":"I dug around the documentation, and the default is 1sec, but you can reduce to almost immediate (low millis). It's not recommended as sustainable on high traffic and you may end up with other issues on shards etc but the question doesnt care about this. Also the name of the property to change is not the same as in the question which is a little misleading, but the effect/solution is right (C)."},{"comments":[{"poster":"akashm99101001com","content":"\"The company has only one consumer application.\"","upvote_count":"5","timestamp":"1679332440.0","comment_id":"845097"}],"poster":"Arjun777","comment_id":"808841","timestamp":"1676408220.0","content":"Option A, using enhanced fan-out, can help achieve the desired reduction in latency by allowing multiple consumers to receive data from the same stream at the same time without having to share the same shard. This can help increase the overall throughput of the stream and reduce the time it takes for records to be processed and delivered to the consumer application.","upvote_count":"1"},{"upvote_count":"2","comment_id":"705861","timestamp":"1666898100.0","comments":[{"upvote_count":"1","content":"For most applications, we recommend polling each shard one time per second per application. This enables you to have multiple consumer applications processing a stream concurrently without hitting Amazon Kinesis Data Streams limits of 5 GetRecords calls per second. Additionally, processing larger batches of data tends to be more efficient at reducing network and other downstream latencies in your application. \nThe KCL defaults are set to follow the best practice of polling every 1 second. This default results in average propagation delays that are typically below 1 second.\n\nKinesis Data Streams records are available to be read immediately after they are written. There are some use cases that need to take advantage of this and require consuming data from the stream as soon as it is available. You can significantly reduce the propagation delay by overriding the KCL default settings to poll more frequently, \n\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html","poster":"cloudlearnerhere","comment_id":"705862","timestamp":"1666898160.0"}],"poster":"cloudlearnerhere","content":"Correct answer is C as the default propagation delay for the KCL consumer is 1 second and reducing the delay can help reduce the latency. \nPropagation delay is defined as the end-to-end latency from the moment a record is written to the stream until it is read by a consumer application. This delay varies depending upon a number of factors, but it is primarily affected by the polling interval of consumer applications. \nOption A is wrong as enhanced fan-out in Kinesis Data Streams enables consumers to receive records from a stream with a throughput of up to 2 MB of data per second per shard. It doesn't reduce the latency for KCL.\n\nOption B is wrong as increasing shards doesn't reduce the latency for KCL.\n\nOption D is wrong as Kinesis Data Firehose calls Kinesis Data Streams GetRecords() once every second for each Kinesis shard."},{"comment_id":"633829","poster":"rocky48","content":"Selected Answer: C\nAnswer: C\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html","upvote_count":"1","timestamp":"1658291160.0"},{"content":"Answer C\nidleTimeBetweenReadsInMillis set between 200ms and 500ms","poster":"lakeswimmer","comment_id":"492898","timestamp":"1638497700.0","upvote_count":"4"},{"timestamp":"1637981520.0","upvote_count":"2","comment_id":"487810","content":"Answer is C","poster":"Thiya"},{"upvote_count":"3","comment_id":"477928","timestamp":"1636862820.0","content":"Answer C","poster":"ali98"}],"question_images":[],"answer_description":"","exam_id":20,"unix_timestamp":1635129240,"answer_images":[],"choices":{"D":"Develop consumers by using Amazon Kinesis Data Firehose.","B":"Increase the number of shards for the Kinesis data stream.","A":"Use enhanced fan-out in Kinesis Data Streams.","C":"Reduce the propagation delay by overriding the KCL default settings."},"question_text":"A power utility company is deploying thousands of smart meters to obtain real-time updates about power consumption. The company is using Amazon Kinesis\nData Streams to collect the data streams from smart meters. The consumer application uses the Kinesis Client Library (KCL) to retrieve the stream data. The company has only one consumer application.\nThe company observes an average of 1 second of latency from the moment that a record is written to the stream until the record is read by a consumer application. The company must reduce this latency to 500 milliseconds.\nWhich solution meets these requirements?","topic":"1","answers_community":["C (100%)"],"isMC":true},{"id":"tjadzONM7akWTQlLNcAD","answers_community":["AC (100%)"],"answer_description":"","exam_id":20,"unix_timestamp":1596974100,"answer":"AC","url":"https://www.examtopics.com/discussions/amazon/view/27697-exam-aws-certified-data-analytics-specialty-topic-1-question/","choices":{"A":"Set up a trusted connection with HSM using a client and server certificate with automatic key rotation.","B":"Modify the cluster with an HSM encryption option and automatic key rotation.","E":"Enable Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) encryption in the HSM.","D":"Enable HSM with key rotation through the AWS CLI.","C":"Create a new HSM-encrypted Amazon Redshift cluster and migrate the data to the new cluster."},"question_images":[],"discussion":[{"comments":[{"poster":"GeeBeeEl","content":"I dont agree with you on c...... that site you referenced says \"When you modify your cluster to enable KMS encryption, Amazon Redshift automatically migrates your data to a new encrypted cluster. \" also see https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html","upvote_count":"2","comments":[{"content":"I see now why C is correct --- \"To migrate an unencrypted cluster to a cluster encrypted using a hardware security module (HSM), you create a new encrypted cluster and move your data to the new cluster. So I agree C is correct","timestamp":"1634265720.0","poster":"GeeBeeEl","upvote_count":"3","comment_id":"178608"}],"comment_id":"178604","timestamp":"1633727700.0"}],"comment_id":"153573","timestamp":"1632220440.0","content":"Answer should be A and C. Using HSM you have to create a new cluster (that eliminates B). See link below, it clearly states \"You can't enable hardware security module (HSM) encryption by modifying the cluster. Instead, create a new, HSM-encrypted cluster and migrate your data to the new cluster\"\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html\n\nIn the same link it says you have create certificates. \n\nMy thinking that its not D, its because it can be already configured when you are settinp up the cluster. (option C)","poster":"testtaker3434","upvote_count":"48"},{"comment_id":"162899","upvote_count":"15","content":"Answer: A, C\n\nWhen you use an HSM, you must use client and server certificates to configure a trusted connection between Amazon Redshift and your HSM.\n\nReference link:\nhttps://docs.amazonaws.cn/en_us/redshift/latest/mgmt/security-key-management.html \n\nTo migrate an unencrypted cluster to a cluster encrypted using a hardware security module (HSM), you create a new encrypted cluster and move your data to the new cluster.\n\nReference link: \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html","timestamp":"1632712020.0","poster":"Nicki1013"},{"comment_id":"1182952","content":"Bing is answering C and D. By this explanation \n\nOption A suggests setting up a trusted connection with HSM using a client and server certificate with automatic key rotation. While this is a valid method for some systems, it’s not directly applicable to Amazon Redshift. Redshift doesn’t support this method for enabling encryption.\nOption C is correct because Amazon Redshift doesn’t allow you to modify an existing cluster to use HSM encryption. You would need to create a new HSM-encrypted Redshift cluster and migrate the data to it.\nOption D is also correct. Once the new HSM-encrypted Redshift cluster is set up, you can enable HSM with key rotation through the AWS CLI.","timestamp":"1711421700.0","poster":"tsangckl","upvote_count":"1"},{"upvote_count":"1","timestamp":"1690841100.0","poster":"NikkyDicky","content":"Selected Answer: AC\nIt's AC","comment_id":"968526"},{"timestamp":"1682946600.0","poster":"pk349","upvote_count":"1","content":"AC: I passed the test","comment_id":"886270"},{"content":"Selected Answer: AC\nCorrect answer is A & C as Redshift does not allow encrypting existing cluster using HSM and there needs to be trust connection established between Redshift and HSM.\n\nOptions B & D are wrong as You can enable encryption when you launch your cluster, or you can modify an unencrypted cluster to use AWS Key Management Service (AWS KMS) encryption.\n\nOption E is wrong as it is not valid.","comment_id":"711123","upvote_count":"2","timestamp":"1667564880.0","poster":"cloudlearnerhere"},{"upvote_count":"1","comment_id":"634937","poster":"rocky48","timestamp":"1658454540.0","content":"Selected Answer: AC\nAnswer-A,C"},{"upvote_count":"1","timestamp":"1653205080.0","comment_id":"605242","poster":"Bik000","content":"Selected Answer: AC\nAnswer is A & C"},{"timestamp":"1651355580.0","content":"Answer-A,C","poster":"jrheen","comment_id":"595310","upvote_count":"1"},{"timestamp":"1637417040.0","poster":"aws2019","content":"A and C","upvote_count":"1","comment_id":"482586"},{"timestamp":"1636264380.0","upvote_count":"3","content":"A, C is correct but why Redshift with HSM is asked in 2020? Redshift only works with HSM Classic and new customer can't create HSM classic anymore.","poster":"Huy","comment_id":"387644"},{"content":"Answer: A,C (Similar question is there in Jon Bonso's practice exam).","upvote_count":"1","timestamp":"1636206720.0","comment_id":"386014","poster":"Donell"},{"timestamp":"1635863220.0","content":"B = wrong, to use HSM you have to create new clusters. D = wrong, key rotation is not done by HSM, but Redshift. E = wrong, nonsense. This is a textbook question.\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html#migrating-to-an-encrypted-cluster","comment_id":"383528","upvote_count":"1","poster":"Shraddha"},{"poster":"leliodesouza","content":"the answers are A and C.","upvote_count":"1","timestamp":"1635456420.0","comment_id":"359144"},{"timestamp":"1635250380.0","content":"Definitely A and C. First answer is from the link provided by testtaker3434, and 2nd answer from the following link https://docs.aws.amazon.com/redshift/latest/mgmt/security-key-management.html","poster":"jyrajan69","upvote_count":"3","comment_id":"324963"},{"content":"A, C is the right answer","timestamp":"1634943960.0","upvote_count":"1","poster":"lostsoul07","comment_id":"274249"},{"upvote_count":"1","timestamp":"1634849760.0","poster":"BillyC","comment_id":"216827","content":"A and C are correct"},{"comment_id":"207261","content":"Dead giveaway for A&C? \"Combination of steps\". B and D accomplish the same thing even if they were possible","poster":"[Removed]","upvote_count":"1","timestamp":"1634419800.0"},{"upvote_count":"1","comment_id":"191277","content":"The link provided indicates A & C\n\"When you opt to use an HSM for management of your cluster key, you need to configure a trusted network link between Amazon Redshift and your HSM\"\n\"When you modify your cluster to enable KMS encryption, Amazon Redshift automatically migrates your data to a new encrypted cluster\"","timestamp":"1634378640.0","poster":"syu31svc"},{"poster":"Karan_Sharma","content":"Option A & C, HSM needs client and server certificates for a trusted connection between redshift and HSM. And existing cluster can't be modified for adding HSM, it can be done for KMS though.","upvote_count":"2","comment_id":"177466","timestamp":"1633603320.0"},{"comment_id":"176565","timestamp":"1633339920.0","upvote_count":"1","content":"Answer is A, C\nvery clear in this link: \nhttps://aws.amazon.com/about-aws/whats-new/2018/10/encrypt-amazon-redshift-1-click/#:~:text=You%20can%20use%20one%2Dclick,console%20or%20the%20AWS%20CLI.","poster":"rnc21"},{"comment_id":"175260","timestamp":"1633209540.0","upvote_count":"3","poster":"Paitan","content":"I will go with A and C. I don't think we can encrypt an existing cluster."},{"upvote_count":"1","content":"\"To migrate an unencrypted cluster to a cluster encrypted using a hardware security module (HSM), you create a new encrypted cluster and move your data to the new cluster. You can't migrate to an HSM-encrypted cluster by modifying the cluster.\"\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html","poster":"kempstonjoystick","timestamp":"1632976260.0","comment_id":"173415"},{"poster":"Vipul13","comment_id":"171985","upvote_count":"2","timestamp":"1632918480.0","content":"A and B seems to be the best suited answers, as AWS gives us the feasibility to modify the cluster for encryption, whatever it may do in backend that is its internal working, for us we modified the cluster. hence A and B","comments":[{"timestamp":"1633181640.0","comment_id":"173432","upvote_count":"1","content":"A and C >> https://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html\nYou can't enable hardware security module (HSM) encryption by modifying the cluster. Instead, create a new, HSM-encrypted cluster and migrate your data to the new cluster. F","poster":"Fr3d"},{"timestamp":"1634476800.0","comment_id":"210750","poster":"jove","upvote_count":"1","content":"B is not correct.. You can not encrypt an existing cluster."}]},{"upvote_count":"1","poster":"zeronine","comment_id":"159573","timestamp":"1632570660.0","content":"A and C as testtaker3434 explained above"},{"upvote_count":"2","timestamp":"1632371340.0","content":"It is A and B, read the link provided carefully","comment_id":"159379","poster":"abhineet"},{"timestamp":"1632347700.0","content":"Agree with A&C. Explanation in the link provided by testtaker3434 made it clear.","comment_id":"159216","upvote_count":"2","poster":"singh100"},{"comment_id":"154798","upvote_count":"2","timestamp":"1632251580.0","content":"A and C","poster":"Prodip"}],"isMC":true,"question_text":"A banking company is currently using an Amazon Redshift cluster with dense storage (DS) nodes to store sensitive data. An audit found that the cluster is unencrypted. Compliance requirements state that a database with sensitive data must be encrypted through a hardware security module (HSM) with automated key rotation.\nWhich combination of steps is required to achieve compliance? (Choose two.)","answer_ET":"AC","timestamp":"2020-08-09 13:55:00","question_id":24,"topic":"1","answer_images":[]},{"id":"mqRaf5f5HCeBM5ABn118","answer_images":[],"exam_id":20,"question_images":[],"unix_timestamp":1635129060,"choices":{"B":"Use Amazon Managed Streaming for Apache Kafka to ingest the data to save it to Amazon Redshift. Enable Amazon Redshift workload management (WLM) to prioritize workloads.","D":"Use Amazon Kinesis Data Firehose to ingest the data to save it to Amazon S3. Load frequently queried data to Amazon Redshift using the COPY command. Use Amazon Redshift Spectrum for less frequently queried data.","A":"Use Amazon Managed Streaming for Apache Kafka to ingest the data to save it to Amazon S3. Use Amazon Athena to perform SQL queries over the ingested data.","C":"Use Amazon Kinesis Data Firehose to ingest the data to save it to Amazon Redshift. Enable Amazon Redshift workload management (WLM) to prioritize workloads."},"discussion":[{"content":"D is right","comment_id":"480278","upvote_count":"16","poster":"aws2019","timestamp":"1637183640.0"},{"content":"I think it should be D\n\"multiple sophisticated SQL queries with consistent performance\" means Redshift. \nWLM just prioritizes query. frequently queried can be in Redshift","comment_id":"492986","upvote_count":"6","poster":"lakeswimmer","timestamp":"1638513300.0"},{"content":"D: I passed the test","upvote_count":"2","comment_id":"886608","poster":"pk349","timestamp":"1682966280.0"},{"poster":"CleverMonkey092","content":"D is the most logical answer","comment_id":"850822","upvote_count":"2","timestamp":"1679818740.0"},{"content":"Correct answer is D as Kinesis Data Firehose can be used to push data directly to S3. Frequently queried data can be loaded into Redshift for querying. Less frequent data can be still stored in S3 and queried using Redshift Spectrum. \n\nUsing Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Multiple clusters can concurrently query the same dataset in Amazon S3 without the need to make copies of the data for each cluster.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\n\nOption A is wrong as Athena is not ideal for performing complex queries.\n\nOptions B & C are wrong as using Redshift for all the data would not be cost-effective.","upvote_count":"4","comment_id":"705864","timestamp":"1666898340.0","poster":"cloudlearnerhere"},{"timestamp":"1658379900.0","poster":"rocky48","content":"Selected Answer: D\nAnswer is D","comment_id":"634361","upvote_count":"3"},{"content":"Selected Answer: D\nAnswer is D","timestamp":"1654354380.0","upvote_count":"2","comment_id":"611464","poster":"Robot209"},{"poster":"Bik000","content":"Selected Answer: D\nMy Answer is D","comment_id":"605262","upvote_count":"2","timestamp":"1653205860.0"},{"content":"Selected Answer: D\nD is correct","upvote_count":"4","timestamp":"1648813620.0","poster":"astalavista1","comment_id":"579458"},{"upvote_count":"1","comments":[{"comment_id":"593767","content":"WLM prioritizes workloads mostly according to the run time, like long/short run. If only the data is well patitioned, and formatted, Athena on S3 will also get result in \"consistent performance\". So I think the question is mainly asking about the \"cost\".","upvote_count":"2","timestamp":"1651144620.0","poster":"MWL"}],"poster":"youonebe","timestamp":"1647326340.0","comment_id":"568172","content":"Ans: C\nThe question is focused on performance. Hence using WLM to fulfill the purpose."},{"comment_id":"557613","poster":"Agn3001","timestamp":"1645994580.0","comments":[{"poster":"yusnardo","comment_id":"564001","timestamp":"1646825940.0","upvote_count":"5","content":"Kinesis Data Firehose delivers your data to your S3 bucket first and then issues an Amazon Redshift COPY command to load the data into your Amazon Redshift cluster.\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-redshift"}],"upvote_count":"3","content":"when kinesis firehost can directly write to Redshift why write to s3 and then copy command then wlm for queries ? I think answer C"},{"content":"I think it is A. Cost-effective is the requirement, so RD cannot be the one.","poster":"goutes","upvote_count":"3","comment_id":"478421","comments":[{"upvote_count":"2","content":"But, how you are going to ingest the data from MSK to S3 ? I think this is pub-sub model, someone has to consume the Kafka Topic to put there in S3. Doest it ? \nit must be D or B. Where B might take more cost to put data on Redshift cluster.","poster":"sanpak","timestamp":"1638932280.0","comment_id":"496494"},{"content":"And you will par for Athena based on data scanning. if you query frequently on some data, you will pay much for the data scanning.","upvote_count":"1","comment_id":"593769","poster":"MWL","timestamp":"1651144740.0"},{"timestamp":"1679134740.0","content":"PERFORMANCE is the requirement not COST?","comment_id":"842659","poster":"np2021","upvote_count":"1"}],"timestamp":"1636941720.0"},{"poster":"ali98","content":"Answer : D","timestamp":"1636862760.0","upvote_count":"1","comment_id":"477927"},{"upvote_count":"2","poster":"Chints01","comments":[],"timestamp":"1636525680.0","comment_id":"475245","content":"I believe the answer should be A. thoughts?"},{"poster":"srinivasa","timestamp":"1634480460.0","comment_id":"467206","content":"Answer: D","upvote_count":"1"}],"question_text":"A company needs to collect streaming data from several sources and store the data in the AWS Cloud. The dataset is heavily structured, but analysts need to perform several complex SQL queries and need consistent performance. Some of the data is queried more frequently than the rest. The company wants a solution that meets its performance requirements in a cost-effective manner.\nWhich solution meets these requirements?","timestamp":"2021-10-25 04:31:00","topic":"1","answers_community":["D (100%)"],"answer_description":"","answer_ET":"D","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/64635-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":25,"answer":"D"}],"exam":{"id":20,"name":"AWS Certified Data Analytics - Specialty","isMCOnly":true,"isBeta":false,"numberOfQuestions":164,"lastUpdated":"11 Apr 2025","provider":"Amazon","isImplemented":true},"currentPage":5},"__N_SSP":true}