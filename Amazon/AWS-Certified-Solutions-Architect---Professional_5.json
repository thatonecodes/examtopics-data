{"pageProps":{"questions":[{"id":"mRAgOLGPunIyAur02fCb","question_text":"A company deploys workloads in multiple AWS accounts. Each account has a VPC with VPC flow logs published in text log format to a centralized Amazon S3 bucket. Each log file is compressed with gzip compression. The company must retain the log files indefinitely.\n\nA security engineer occasionally analyzes the logs by using Amazon Athena to query the VPC flow logs. The query performance is degrading over time as the number of ingested logs is growing. A solutions architect must improve the performance of the log analysis and reduce the storage space that the VPC flow logs use.\n\nWhich solution will meet these requirements with the LARGEST performance improvement?","answer_ET":"C","discussion":[{"comment_id":"746481","timestamp":"1671136080.0","content":"Answer is: C\nhttps://aws.amazon.com/about-aws/whats-new/2021/10/amazon-vpc-flow-logs-parquet-hive-prefixes-partitioned-files/\nThis the best answer based on the document and gives the best performance. it's supports gzip as per the question.","poster":"desertlotus1211","upvote_count":"5"},{"upvote_count":"1","comment_id":"948332","poster":"ggrodskiy","timestamp":"1689014640.0","content":"Correct C."},{"upvote_count":"1","content":"Selected Answer: C\nit's C","timestamp":"1672648320.0","comment_id":"763591","poster":"Appon"},{"poster":"Kende","upvote_count":"2","content":"Selected Answer: C\n\"C\" is the one.","timestamp":"1671975000.0","comment_id":"755671"},{"content":"https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-vpc-flow-logs-parquet-hive-prefixes-partitioned-files/ C","upvote_count":"1","poster":"ggrodskiy","comment_id":"732435","timestamp":"1669886340.0"},{"comment_id":"732420","timestamp":"1669884060.0","upvote_count":"2","content":"Correct C","poster":"ggrodskiy"},{"poster":"Spavanko","upvote_count":"1","timestamp":"1669743960.0","comment_id":"730689","content":"Selected Answer: D\nhttps://aws.amazon.com/about-aws/whats-new/2020/11/amazon-athena-announces-availability-of-engine-version-2/"},{"content":"Selected Answer: C\nhttps://aws.amazon.com/about-aws/whats-new/2018/09/amazon-s3-announces-new-features-for-s3-select/","upvote_count":"2","timestamp":"1669178340.0","poster":"pvrhere","comment_id":"724884"}],"answer_images":[],"exam_id":32,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/88402-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"choices":{"C":"Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files.","A":"Create an AWS Lambda function to decompress the gzip files and to compress the files with bzip2 compression. Subscribe the Lambda function to an s3:ObjectCreated:Put S3 event notification for the S3 bucket.","D":"Create a new Athena workgroup without data usage control limits. Use Athena engine version 2.","B":"Enable S3 Transfer Acceleration for the S3 bucket. Create an S3 Lifecycle configuration to move files to the S3 Intelligent-Tiering storage class as soon as the files are uploaded."},"timestamp":"2022-11-23 05:39:00","answer":"C","topic":"1","unix_timestamp":1669178340,"question_id":21,"isMC":true,"answers_community":["C (83%)","D (17%)"]},{"id":"3IUVxYGQgt5BXz4ceE1q","answer_ET":"C","exam_id":32,"answer":"C","discussion":[{"upvote_count":"1","comment_id":"948331","content":"Correct C.","timestamp":"1689014520.0","poster":"ggrodskiy"},{"content":"Selected Answer C: the statement \"RDS also supports automatic failover in 60 seconds\" is not entirely accurate. While RDS does support automatic failover, the time it takes for the failover to complete can vary depending on the specific circumstances of the failure and the configuration of the RDS instance. Some of the search results suggest that the failover process can typically be completed within 60-120 seconds, while others suggest that it can take as little as 5 seconds or up to 30 seconds for Aurora.","upvote_count":"1","timestamp":"1687066020.0","comment_id":"926442","poster":"TECHNOWARRIOR"},{"comment_id":"902101","poster":"Jesuisleon","timestamp":"1684512840.0","content":"Selected Answer: D\nI think it should be D.\nC. this sentence \"Promote the read replica to a standalone DB cluster\" should be \"Promote the read replica to primary db instance\" based on \" This means that if your primary DB instance fails, an Aurora replica is then promoted to the primary instance.\" from https://repost.aws/knowledge-center/aurora-mysql-create-read-replica.\n\nFor D I noticed okladev said automatic failover = Aurora, it's not true, see \"Support high availability for your application with automatic database failover that completes as quickly as 60 seconds with zero data loss and no manual intervention.\" from https://aws.amazon.com/rds/features/multi-az/\nRDS also support automatic failover in 60 second.","upvote_count":"1"},{"poster":"dev112233xx","timestamp":"1682960700.0","comment_id":"886538","upvote_count":"2","content":"Selected Answer: C\nautomatic failover = Aurora"},{"poster":"ggrodskiy","content":"Correct C","upvote_count":"1","timestamp":"1674065580.0","comment_id":"780296"},{"content":"Can you have Aurora as a read replica when main DB is an RDS?","poster":"syaldram","comment_id":"771316","upvote_count":"1","timestamp":"1673348580.0"},{"content":"Selected Answer: C\n\"C\" is the one.","timestamp":"1672341000.0","poster":"Kende","comment_id":"761394","upvote_count":"2"},{"timestamp":"1669744320.0","upvote_count":"3","content":"Selected Answer: C\nCorrect C:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-create-read-replica/","poster":"Spavanko","comments":[{"content":"Yes, this would be true for *aurora* database.( this one https://aws.amazon.com/rds/aurora/?nc1=h_ls)\n\nBut the company has an Amazon RDS for MySQL:\nwhich is this one https://aws.amazon.com/rds/mysql/?nc1=h_ls","poster":"hobokabobo","comment_id":"859495","upvote_count":"1","comments":[{"timestamp":"1680488220.0","upvote_count":"1","comment_id":"859497","poster":"hobokabobo","content":"Thats the link that explains it:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html"}],"timestamp":"1680487800.0"}],"comment_id":"730696"}],"topic":"1","answers_community":["C (88%)","13%"],"question_id":22,"question_images":[],"choices":{"B":"Convert the DB instance to a Multi-AZ deployment. Set the query_cache_type parameter on the database to zero. Increase the CloudFront caching TTL to reduce application server CPU utilization.","D":"Create a read replica cluster on the DB instance. Use a Multi-AZ deployment. Synchronize the read replica with the primary DB instance. Promote the read replica as the primary DB instance.","C":"Create an Amazon Aurora read replica from the DB instance. Wait until the read replica is synchronized with the source DB instance. Promote the read replica to a standalone DB cluster. Direct the application endpoint to the new Aurora DB instance.","A":"Create an incremental database backup by using Percona XtraBackup. Compress the backup files. Synchronize the backup files to Amazon S3. Restore the backup files from Amazon S3 to Amazon Aurora MySOL. Direct the application endpoint to the new Aurora DB instance."},"question_text":"A company's solutions architect is managing a learning platform that supports more than 1 million students. The company's business reporting team is experiencing slow performance while extracting large datasets from the database. The learning application is based on PHP and runs on Amazon EC2 instances that are in an Amazon EC2 Auto Scaling group behind an Application Load Balancer (ALB). Application data is stored in an Amazon S3 bucket and in an Amazon RDS for MySOL database. The ALB is the origin of an Amazon CloudFront distribution.\n\nThe solutions architect observes that slow read operations for SELECT queries are affecting the RDS for MySOL DB instance's CPU utilization. The solutions architect must find a scalable solution to improve the slow website performance with near-zero downtime. The solution also must provide automatic failover with no data loss.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/89273-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"isMC":true,"unix_timestamp":1669744320,"timestamp":"2022-11-29 18:52:00","answer_description":""},{"id":"pPaHqnUiXYAJBJvpvdtM","unix_timestamp":1670327760,"discussion":[{"comment_id":"746479","content":"Answer: C [per nyunyu link: https://docs.aws.amazon.com/firehose/latest/APIReference/API_CreateDeliveryStream.html]\nThey will need to send raw data... Not processed data which is going to S3 as the question states","upvote_count":"5","poster":"desertlotus1211","timestamp":"1671135540.0"},{"timestamp":"1708424820.0","content":"C is correct","upvote_count":"1","poster":"Ebi","comment_id":"1154631"},{"upvote_count":"1","comment_id":"948326","timestamp":"1689014220.0","content":"Correct C.\nhttps://docs.aws.amazon.com/firehose/latest/dev/writing-with-iot.html\nhttps://aws.amazon.com/blogs/big-data/stream-data-to-an-http-endpoint-with-amazon-kinesis-data-firehose/","poster":"ggrodskiy"},{"upvote_count":"2","comment_id":"862615","content":"C is the least amount of development work\nhttps://docs.aws.amazon.com/firehose/latest/dev/httpdeliveryrequestresponse.html","poster":"yama234","timestamp":"1680740640.0"},{"content":"Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Amazon OpenSearch Serverless, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.","timestamp":"1677503040.0","upvote_count":"1","comment_id":"823702","poster":"davidy2020"},{"poster":"Kende","comment_id":"755674","timestamp":"1671975360.0","upvote_count":"3","content":"Selected Answer: C\n\"C\" is the one."},{"upvote_count":"2","comment_id":"743617","timestamp":"1670908740.0","poster":"nyunyu","content":"CCC\nhttps://docs.aws.amazon.com/firehose/latest/APIReference/API_CreateDeliveryStream.html"},{"upvote_count":"2","content":"Correct B.\nhttps://aws.amazon.com/blogs/compute/using-api-destinations-with-amazon-eventbridge/","poster":"ggrodskiy","comment_id":"736768","timestamp":"1670327760.0"}],"answer_images":[],"question_images":[],"timestamp":"2022-12-06 12:56:00","answer_description":"","question_id":23,"question_text":"A company is using IoT devices on its manufacturing equipment. Data from the devices travels to the AWS Cloud through a connection to AWS IoT Core. An Amazon Kinesis data stream sends the data from AWS IoT Core to the company's processing application. The processing application stores data in Amazon S3.\n\nA new requirement states that the company also must send the raw data to a third-party system by using an HTTP API.\n\nWhich solution will meet these requirements with the LEAST amount of development work?","exam_id":32,"topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/90197-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"C","answer":"C","choices":{"B":"Create an S3 event notification with Amazon EventBridge (Amazon CloudWatch Events) as the event destination. Create an EventBridge (CloudWatch Events) API destination for the third-party HTTP API.","C":"Create an Amazon Kinesis Data Firehose delivery stream. Configure an HTTP endpoint destination that targets the third-party HTTP API. Configure the Kinesis data stream to send data to the Kinesis Data Firehose delivery stream.","D":"Create an S3 event notification with an Amazon Simple Queue Service (Amazon SQS) queue as the event destination. Configure the SOS queue to invoke a custom AWS Lambda function. Configure the Lambda function to call the third-party HTTP API.","A":"Create a custom AWS Lambda function to consume records from the Kinesis data stream. Configure the Lambda function to call the third-party HTTP API."},"answers_community":["C (100%)"]},{"id":"PsM1CmB9on6HyZ1SdMBa","question_id":24,"unix_timestamp":1669744620,"choices":{"E":"Deploy an Amazon RDS Multi-AZ DB instance. Configure the application to target the DB instance.","A":"Deploy an Application Load Balancer (ALB) that is mapped to a public subnet in each Availability Zone for the web tier. Deploy Amazon EC2 instances as web servers in each of the private subnets. Configure the web server instances as the target group for the ALB. Use Amazon EC2 Auto Scaling for the web server instances.","D":"Deploy a new Application Load Balancer (ALB) to a private subnet in each Availability Zone for the application tier. Deploy Amazon EC2 instances as application servers in each of the private subnets. Configure the web server instances to forward traffic to the application server instances. Use Amazon EC2 Auto Scaling for the application server instances.","B":"Deploy an Application Load Balancer (ALB) that is mapped to a public subnet in each Availability Zone for the web tier. Deploy Amazon EC2 instances as web servers in each of the public subnets. Configure the web server instances as the target group for the ALUse Amazon EC2 Auto Scaling for the web server instances.","C":"Deploy a new Application Load Balancer (ALB) to a private subnet in each Availability Zone for the application tier. Deploy Amazon EC2 instances as application servers in each of the private subnets. Configure the application server instances as targets for the new ALB. Configure the web server instances to forward traffic to the new ALB. Use Amazon EC2 Auto Scaling for the application server instances.","F":"Deploy an Amazon RDS Single-AZ DB instance with a read replica in another Availability Zone. Configure the application to target the primary DB instance."},"answers_community":["AE (53%)","AC (47%)"],"timestamp":"2022-11-29 18:57:00","url":"https://www.examtopics.com/discussions/amazon/view/89275-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","topic":"1","question_text":"A solutions architect is deploying a web application that consists of a web tier, an application tier, and a database tier. The infrastructure must be highly available across two Availability Zones. The solution must minimize single points of failure and must be resilient.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)","answer":"AE","isMC":true,"question_images":[],"answer_images":[],"answer_ET":"AE","exam_id":32,"discussion":[{"content":"Selected Answer: AC\nACE or not? Isn't this question for three choices? Usually when they have 6 options is for 3 choices. Also the AE only account for web and database tiers, what about application?\n\nHence I would go for ACE","poster":"ccort","comments":[{"upvote_count":"1","content":"For me sentence \"Configure the application server instances as targets for the new ALB. Configure the web server instances to forward traffic to the new ALB.\" sounds weird. Requirement is to \"minimize single points of failure\", but generally why would anybody add additional instances in front of ALB? Doesn't make sense.","timestamp":"1705606800.0","comment_id":"1126142","poster":"marszalekm"}],"comment_id":"780979","timestamp":"1674121860.0","upvote_count":"6"},{"poster":"Spavanko","upvote_count":"5","timestamp":"1669744620.0","comment_id":"730699","content":"Selected Answer: AE\nAE provide full HA!"},{"content":"AE is correct","poster":"Ebi","timestamp":"1708424760.0","comment_id":"1154628","upvote_count":"1"},{"content":"if you got far it means you are persistent, good luck on your exam","upvote_count":"1","timestamp":"1690241400.0","comment_id":"962153","poster":"SkyZeroZx"},{"timestamp":"1687726080.0","poster":"SkyZeroZx","content":"Selected Answer: AE\nAE . I agree with ccort, the answer should be three choices, can't be only AE because it's missing the \"Application tier\"","comment_id":"933885","upvote_count":"1"},{"comment_id":"886550","upvote_count":"3","timestamp":"1682961960.0","poster":"dev112233xx","content":"Selected Answer: AC\nACE... I agree with ccort, the answer should be three choices, can't be only AE because it's missing the \"Application tier\""},{"poster":"Kende","comment_id":"761397","timestamp":"1672341060.0","upvote_count":"4","content":"Selected Answer: AE\n\"A,E\" are the ones."},{"content":"Correct AE","comment_id":"732400","poster":"ggrodskiy","upvote_count":"4","timestamp":"1669883040.0"}]},{"id":"2puvvux0FVEUtfOihPBT","discussion":[{"upvote_count":"8","timestamp":"1632601380.0","content":"obvious answer: A\nbut, if u want to research more:\nhttp://jayendrapatil.com/aws-vpc-nat/","poster":"CloudFloater","comment_id":"40744"},{"upvote_count":"5","content":"Disable source/destination checks\nEach EC2 instance performs source/destination checks by default. This means that the instance must be the source or destination of any traffic it sends or receives. However, a NAT instance must be able to send and receive traffic when the source or destination is not itself. Therefore, you must disable source/destination checks on the NAT instance.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html","poster":"kuroro","comment_id":"375810","timestamp":"1634368800.0"},{"comment_id":"1267009","timestamp":"1723807200.0","content":"A. Disabling the Source/Destination Check attribute on the NAT instance","poster":"amministrazione","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nA looks right","comment_id":"543364","poster":"jj22222","timestamp":"1644357360.0"},{"timestamp":"1643341860.0","content":"A: rest are not needed.","comment_id":"534354","upvote_count":"1","poster":"AMKazi"},{"upvote_count":"1","comment_id":"524266","timestamp":"1642260660.0","content":"A is correct","poster":"MohamedSherif1"},{"comment_id":"514340","poster":"cldy","timestamp":"1641009240.0","content":"A correct.","upvote_count":"1"},{"timestamp":"1633096860.0","comments":[{"poster":"acloudguru","content":"hope I can have it in my exam","upvote_count":"1","timestamp":"1638262440.0","comment_id":"490531"}],"poster":"01037","upvote_count":"2","content":"A easy one","comment_id":"370835"},{"comment_id":"54764","content":"Answer is A.","upvote_count":"2","timestamp":"1632905340.0","poster":"miracle"}],"answer_description":"Reference:\nhttp://docs.aws.amazon.com/workspaces/latest/adminguide/gsg_create_vpc.html","answers_community":["A (100%)"],"exam_id":32,"isMC":true,"question_id":25,"timestamp":"2020-01-19 22:41:00","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/12330-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1","question_text":"After launching an instance that you intend to serve as a NAT (Network Address Translation) device in a public subnet you modify your route tables to have the\nNAT device be the target of internet bound traffic of your private subnet. When you try and make an outbound connection to the internet from an instance in the private subnet, you are not successful.\nWhich of the following steps could resolve the issue?","unix_timestamp":1579470060,"answer_images":[],"choices":{"D":"Attaching a second Elastic Network Interface (ENI) to the instance in the private subnet, and placing it in the public subnet","A":"Disabling the Source/Destination Check attribute on the NAT instance","C":"Attaching a second Elastic Network Interface (ENI) to the NAT instance, and placing it in the private subnet","B":"Attaching an Elastic IP address to the instance in the private subnet"},"answer":"A","question_images":[]}],"exam":{"isMCOnly":false,"numberOfQuestions":1019,"isBeta":false,"isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional","id":32},"currentPage":5},"__N_SSP":true}