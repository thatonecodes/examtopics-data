{"pageProps":{"questions":[{"id":"7AV5GuYY2UjXFmI9VwCc","question_images":[],"question_id":56,"exam_id":33,"unix_timestamp":1673882400,"answer_ET":"B","answers_community":["B (100%)"],"question_text":"A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS security best practices.\n\nWhich solution will meet these requirements?","discussion":[{"timestamp":"1690922820.0","poster":"tatdatpham","comment_id":"795565","content":"Selected Answer: B\nOption B is the best solution. This solution creates an IAM role that trusts the auditors' AWS account and attaches the required IAM policies to the role. This ensures that the auditors have read-only access to the company's AWS account while ensuring that the company's AWS account is secure and complies with AWS security best practices. Additionally, the unique external ID assigned to the role's trust policy adds an extra layer of security.","upvote_count":"7"},{"comment_id":"1156883","content":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html","upvote_count":"1","poster":"duriselvan","timestamp":"1724376360.0"},{"upvote_count":"1","comment_id":"1156882","content":"To create an IAM role that trusts the auditors' AWS account, you can do the following:\nSign in to the AWS Management Console and open the IAM console.\nIn the navigation pane, choose Roles, and then choose Create role.\nChoose the Custom trust policy role type.\nIn the Custom trust policy section, enter or paste the following trust policy:\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Principal\": {\n \"AWS\": \"arn:aws:iam::<auditor-account-id>:root\"\n },\n \"Action\": \"sts:AssumeRole\"\n }\n ]\n}","poster":"duriselvan","timestamp":"1724376180.0"},{"upvote_count":"1","content":"Selected Answer: B\nOption B","comment_id":"1104406","timestamp":"1719184680.0","poster":"career360guru"},{"poster":"dkcloudguru","content":"B is correct","timestamp":"1710062700.0","upvote_count":"1","comment_id":"1003770"},{"comment_id":"942907","timestamp":"1704388500.0","poster":"NikkyDicky","content":"Selected Answer: B\nits a b","upvote_count":"1"},{"upvote_count":"3","timestamp":"1695797160.0","content":"Selected Answer: B\nIn the company's AWS account, create an IAM role that trusts the auditors' AWS account.","comment_id":"851818","poster":"mfsec"},{"content":"Selected Answer: B\nB seems to be the right answer","upvote_count":"3","timestamp":"1690674300.0","poster":"zozza2023","comment_id":"792266"},{"comment_id":"778181","timestamp":"1689529980.0","upvote_count":"3","comments":[{"timestamp":"1689530040.0","poster":"masetromain","content":"Option A is incorrect because it grants access to all resources in the company's AWS account and does not provide a way to restrict the permissions that the external auditors have.\n\nOption C is incorrect because it creates an IAM user in the company's account and shares the API access keys with the external auditors, which is not secure and does not comply with AWS security best practices.\n\nOption D is incorrect because it creates an IAM user in the company's account for each auditor, which would be tedious and difficult to manage for the company. It would be more secure and efficient to use an IAM role that trusts the auditors' AWS account instead of creating individual users for each auditor.","comment_id":"778183","upvote_count":"2"}],"content":"Selected Answer: B\nThe correct answer is B. In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.\n\nThis solution meets the requirement of providing the external auditors with secure, read-only access to the company's AWS account while also complying with AWS security best practices. In this solution, an IAM role is created that trusts the auditors' AWS account and has an IAM policy with the required permissions attached to it. The role's trust policy should include a unique external ID for added security. This allows the external auditors to assume the role and access the resources with the permissions specified in the policy, without the need to share access keys or create individual IAM users for each auditor.","poster":"masetromain"},{"poster":"zhangyu20000","content":"B is correct","upvote_count":"2","comment_id":"777852","timestamp":"1689513600.0"}],"choices":{"C":"In the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors.","D":"In the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for each auditor. Add the IAM users to the IAM group.","A":"In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account. Assign a unique external ID to the resource policy.","B":"In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy."},"timestamp":"2023-01-16 16:20:00","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/95569-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","answer_description":"","isMC":true,"answer_images":[]},{"id":"DqVB8MFVg8V6vhWds9iS","exam_id":33,"answers_community":["CE (100%)"],"answer_ET":"CE","unix_timestamp":1670764320,"url":"https://www.examtopics.com/discussions/amazon/view/91008-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2022-12-11 14:12:00","question_id":57,"choices":{"A":"Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance’s private IP in the private hosted zone.","E":"Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.","C":"Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.","B":"Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.","D":"Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts."},"topic":"1","isMC":true,"answer":"CE","answer_description":"","discussion":[{"comment_id":"774714","upvote_count":"34","poster":"masetromain","content":"Selected Answer: CE\nC and E are correct.\n\nC. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.\nThis step is necessary because the VPC in Account B needs to be associated with the private hosted zone in Account A to be able to resolve the DNS records.\n\nE. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.\nThis step is necessary because the association authorization needs to be removed in Account A after the association is done in Account B.","timestamp":"1673630460.0"},{"upvote_count":"10","timestamp":"1678185180.0","poster":"kiran15789","comment_id":"831763","content":"Selected Answer: CE\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html"},{"comment_id":"1307677","content":"Selected Answer: CE\nAssociate the new VPC in Account B with the hosted zone in Account A, delete the association authorization in Account A.\nThen create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.","poster":"TariqKipkemei","upvote_count":"1","timestamp":"1730869860.0"},{"poster":"masetromain","comments":[{"timestamp":"1670865960.0","poster":"masetromain","content":"https://www.examtopics.com/discussions/amazon/view/36113-exam-aws-certified-solutions-architect-professional-topic-1/","comment_id":"743095","upvote_count":"1"}],"comment_id":"743030","timestamp":"1727062140.0","content":"Selected Answer: CE\nWith comments and links the answer is C and E. (Ty robertohyène and JosuéXu)\n\nC = 6. Run the following command to create the association between Account A's private hosted zone and Account B's VPC. Use the hosted zone's ID from step 3. B account.\nE = 7. It is recommended to remove the association permission after the association is created. This will prevent you from recreating the same association later.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/route53-private-hosted-zone/","upvote_count":"4"},{"content":"Selected Answer: CE\nC and E.\nIn order to resolve the issue, the solutions architect should create an authorization to associate the private hosted zone in Account A with the new VPC in Account B (Option C). This will allow the new VPC in Account B to access the DNS records stored in the private hosted zone in Account A.\n\nIn addition, the solutions architect should associate the new VPC in Account B with the hosted zone in Account A (Option E) and delete the association authorization in Account A. This will ensure that the new VPC in Account B is properly configured to use the private hosted zone in Account A and resolve the db.example.com CNAME record set correctly.","timestamp":"1727062140.0","upvote_count":"5","comment_id":"805463","poster":"CloudFloater"},{"content":"Selected Answer: CE\nhttps://repost.aws/knowledge-center/route53-private-hosted-zone\n\nCreate an authorization to associate the private hosted zone and as a best practice , it is recommended to delete the association authorization in account A-This step prevents you from recreating the same association later. To delete the authorization, reconnect to the EC2 instance in Account A","poster":"whenthan","upvote_count":"2","comment_id":"986977","timestamp":"1727062080.0"},{"content":"Selected Answer: CE\nA account's DNS Zone authorization is associated with B's VPC, and after B's VPC is associated with A's Priviate Zone, A's authorization permission is deleted for security reasons.","poster":"liuliangzhou","comment_id":"1285020","upvote_count":"1","timestamp":"1726540980.0"},{"upvote_count":"1","poster":"amministrazione","content":"C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.\nE. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.","comment_id":"1275446","timestamp":"1725090960.0"},{"comment_id":"1194067","poster":"7f6aef3","upvote_count":"1","content":"Selected Answer: CE\nhttps://repost.aws/knowledge-center/route53-private-hosted-zone","timestamp":"1712884500.0"},{"content":"Selected Answer: CE\nCorrect answers","comments":[{"content":"Explanation:\n * Option C is correct because, in a multi-account AWS setup, to use a Route 53 private hosted zone from one account (Account A) in another account’s VPC (Account B), you first need to create an authorization. This authorization is necessary for allowing the private hosted zone in one account to be associated with a VPC in another account. This step enables the resolution of DNS records stored in the private hosted zone across accounts.\n * Option E is correct as it follows up on the authorization created in Option C. Once the authorization is in place, you can then associate the new VPC in Account B with the private hosted zone in Account A. This association is what actually allows the EC2 instances within the VPC in Account B to resolve DNS queries using the private hosted zone in Account A, ensuring that db.example.com can be resolved as intended.","upvote_count":"4","comment_id":"1143961","poster":"8608f25","timestamp":"1727062080.0","comments":[{"comment_id":"1143962","poster":"8608f25","content":"Why the others are incorrect:\n\n * Option A is not a direct solution to the problem of DNS resolution across AWS accounts. Deploying the database on an EC2 instance does not address the issue of DNS resolution for the RDS endpoint across accounts.\n * Option B is not a scalable or AWS-recommended solution. Manually adding RDS endpoint IP addresses to the /etc/resolv.conf file on an EC2 instance is not practical for environments that require automation and could lead to issues if the RDS endpoint changes.\n * Option D involves creating a separate private hosted zone in Account B and configuring Route 53 replication between AWS accounts. This option is unnecessary and more complex than required. The direct association of VPCs across accounts to a single hosted zone is a simpler and more effective solution.\nTherefore, Options C and E are the steps that directly address the issue with the least complexity and enable the intended DNS resolution across AWS accounts.","upvote_count":"3","timestamp":"1707357540.0"}]}],"timestamp":"1707357420.0","upvote_count":"1","poster":"8608f25","comment_id":"1143958"},{"poster":"atirado","upvote_count":"1","content":"Selected Answer: CE\nOption A - This option does not work - It does not provide for solving address name resolution in the new VPC \n\nOption B - This option works but it breaks the company’s architecture where all DNS names are stored in the private zone in Account A\n\nOption C - This option contributes to the solution.\n\nOption D - Breaks the company’s architecture\n\nOption E - This option contributes to the solution","timestamp":"1702972140.0","comment_id":"1100422"},{"content":"Selected Answer: CE\nobvious","upvote_count":"1","comment_id":"1068305","timestamp":"1699764900.0","poster":"severlight"},{"comment_id":"1045507","upvote_count":"1","timestamp":"1697515800.0","content":"Selected Answer: CE\nC and E are correct.\nB is not a best solution. It's a manual setup and it may lose the configuration if we are using ASG and launching new instance.","poster":"SfQ"},{"poster":"Chainshark","upvote_count":"2","timestamp":"1696746300.0","comments":[{"poster":"SfQ","comment_id":"1045510","upvote_count":"2","timestamp":"1697515860.0","content":"B is not a best solution. It's a manual setup and it may lose the configuration if we are using ASG and launching new instance."}],"comment_id":"1027766","content":"Why is B marked as correct?"},{"poster":"NikkyDicky","timestamp":"1687914720.0","upvote_count":"1","content":"Selected Answer: CE\nit's CE","comment_id":"935950"},{"comment_id":"929198","timestamp":"1687335600.0","poster":"Jonalb","content":"Selected Answer: CE\nccccccccccccceeeeeeeeeeeeee","upvote_count":"1"},{"comment_id":"926381","content":"Selected Answer: CE\nC & E as Issue is associated with authorization","upvote_count":"1","poster":"SkyZeroZx","timestamp":"1687055520.0"},{"comment_id":"920242","content":"Selected Answer: CE\nC & E as Issue is associated with authorization","timestamp":"1686423120.0","poster":"SkyZeroZx","upvote_count":"1"},{"content":"C + E are correct","poster":"AWS_Sam","timestamp":"1683766200.0","upvote_count":"1","comment_id":"894485"},{"comment_id":"892071","content":"Selected Answer: CE\nC & E as Issue is associated with authorization","timestamp":"1683545100.0","upvote_count":"1","poster":"gameoflove"},{"comment_id":"875237","timestamp":"1681958160.0","content":"Selected Answer: CE\nC and E are correct","poster":"Maria2023","upvote_count":"2"},{"upvote_count":"2","timestamp":"1679979540.0","content":"Selected Answer: CE\nCE seme like the best choice","poster":"mfsec","comment_id":"852792"},{"poster":"mKrishna","content":"ANS: A & C\n\nB is incorrect because modifying the /etc/resolv.conf file on the EC2 instance would not resolve the issue since the issue is with the Route 53 configuration.","timestamp":"1678247040.0","comment_id":"832501","upvote_count":"1"},{"comment_id":"806146","timestamp":"1676193780.0","content":"Selected Answer: CE\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html","poster":"Musk","upvote_count":"4"},{"timestamp":"1671645000.0","upvote_count":"1","content":"C & E are correct options.","comment_id":"752618","poster":"razguru"},{"timestamp":"1670836260.0","content":"Selected Answer: CE\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html","comment_id":"742587","poster":"Raj40","upvote_count":"4"},{"content":"https://aws.amazon.com/premiumsupport/knowledge-center/route53-private-hosted-zone/","timestamp":"1670790540.0","comment_id":"742073","poster":"JoshuaXu","upvote_count":"1"},{"content":"Correct answers: C & E","comment_id":"741723","poster":"robertohyena","timestamp":"1670764320.0","upvote_count":"2"}],"question_text":"A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The company’s applications and databases are running in Account B.\nA solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53.\nDuring deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53.\nWhich combination of steps should the solutions architect take to resolve this issue? (Choose two.)","answer_images":[],"question_images":[]},{"id":"td3Oe2stxoztuHSjU9mo","discussion":[{"timestamp":"1674882360.0","comment_id":"790302","content":"Selected Answer: B\n3 nodes are required for a DAX cluster to be fault-tolerant.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.cluster.html","upvote_count":"19","poster":"Untamables"},{"upvote_count":"13","timestamp":"1692869460.0","content":"This is a poorly framed question with very little attention to how applications are architected in real life. Here's my reasoning:\nThis being a trading platform, you have a high volume of writes and reads, and stale data is essentially worse than useless. This automatically eliminates all but A, because of the way DAX performs. DAX caches data from the first query, and subsequent queries will continue to receive that cached data regardless of whether it has been updated in DynamoDB. This behavior continues till cache eviction. The only way around it is to read and write data using DAX.\nHere's the curveball - the solution must be HA, which eliminates A and D, leaving only B & C. And between B & C, you really want to use DAX for reading and DynamoDB for writing. So final answer is B - if you want to get certified.\nApplying this solution in real world however will cause you a lot of pain and grief!","poster":"Ganshank","comments":[{"content":"Cahing in DAX is always write through. Correct answer is B.","comment_id":"1081635","upvote_count":"2","timestamp":"1701094740.0","poster":"jainparag1"},{"comment_id":"1007815","upvote_count":"1","timestamp":"1694709780.0","content":"Totally agree. \n\nBut an additional issue with the question is the fact that it requires High Availability, not Fault Tolerance. These are quite different concepts and, at least up to this point, there would be no need for 3x DAX instances (in theory).","poster":"frfavoreto"}],"comment_id":"989032"},{"comment_id":"1292576","content":"Selected Answer: B\nB is Correct.\n- To achieve high availability for your application, we recommend that you provision your DAX cluster with at least three nodes. Ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html#DAX.consistency.nodes\n- If the request specifies eventually consistent reads (the default behavior), it tries to read the item from DAX. \n- With these operations, data is first written to the DynamoDB table, and then to the DAX cluster. The operation is successful only if the data is successfully written to both the table and to DAX.\nRef: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html#DAX.concepts.request-processing","timestamp":"1727916420.0","poster":"ThachNguyen","upvote_count":"1"},{"comment_id":"1147789","upvote_count":"4","poster":"saggy4","timestamp":"1707704580.0","content":"Selected Answer: B\nDAX is cache and can only be used to read so A and C are out.\nBetween B and D the question says Highly Available so we will select B (three node) instead of D (single node).\n\nSo correct answer B"},{"timestamp":"1706028060.0","upvote_count":"1","comment_id":"1129788","poster":"ninomfr64","content":"A = 2 nodes DAX is not fault-tolerant\nB = correct (write-around strategy ensure lower latency)\nC = write-through strategy can have higher latency\nD = 1 node DAX is not fault-tolerant"},{"comment_id":"1104409","upvote_count":"1","timestamp":"1703381040.0","poster":"career360guru","content":"Selected Answer: B\nOption B is the best option. Though Option A is also possible solution."},{"upvote_count":"1","timestamp":"1702315200.0","poster":"MRamos","comment_id":"1093713","content":"Selected Answer: B\nThe breakpoint is latency.\n\nYou write throught DAX, but for latency sensitive apps, AWS instruct write directly on DynamoDB instead on DAX.\n\n\"For applications that are sensitive to latency, writing through DAX incurs an extra network hop. So a write to DAX is a little slower than a write directly to DynamoDB. If your application is sensitive to write latency, you can reduce the latency by writing directly to DynamoDB instead. For more information, see Write-around.\"\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html#DAX.consistency.strategies-for-writes"},{"timestamp":"1701461220.0","poster":"amarbeg","comment_id":"1085471","upvote_count":"1","content":"Option A would be the least latency solution for this use case. Using a two node DAX cluster with the application reading and writing via DAX provides:\n\nCaching of both reads and writes within the DAX cluster nodes. This eliminates the need to go directly to DynamoDB for reads and writes, reducing latency.\n\nRedundancy with two nodes to ensure high availability of the cache.\n\nThe other options would lead to some reads or writes still going directly to DynamoDB rather than being fully served from the lower latency cached data in DAX. This could increase latency compared to option A. A single node DAX cluster would work but lacks the redundancy needed for high availability.\n\nDAX is fully managed, in-memory cache for DynamoDB that delivers low-latency data access. By caching the entire dataset in-memory across nodes, it can serve requests much faster than going to the DynamoDB tables on every request. The AWS documentation provides more details on how to configure DAX and monitor latency metrics."},{"comment_id":"1024263","upvote_count":"1","content":"Selected Answer: A\nQuestion only ask for High Availability, not Fault Tolerant. You need 3 nodes only for the latter. You must write through to keep data getting stale as mentioned by Ganshank. I would go with two-node cluster as strong consistency adds extra latency as number of clusters increase. So for this question best answer should be A.","poster":"covabix879","timestamp":"1696373040.0"},{"upvote_count":"2","poster":"dkcloudguru","comment_id":"1003782","timestamp":"1694331240.0","content":"Option B is correct: DAX is also used for caching so it improves the performance and for production 3 nodes are strongly recommended so i ll go with B."},{"timestamp":"1694050740.0","content":"https://aws.amazon.com/blogs/database/amazon-dynamodb-accelerator-dax-a-read-throughwrite-through-cache-for-dynamodb/","poster":"duriselvan","upvote_count":"1","comment_id":"1001092"},{"poster":"duriselvan","content":"sorry guys a is wrong ans: B is correct ans Important\nFor production usage, we strongly recommend using DAX with at least three nodes, where each node is placed in different Availability Zones. Three nodes are required for a DAX cluster to be fault-tolerant.\n\nA DAX cluster can be deployed with one or two nodes for development or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodes for production use. If a one- or two-node cluster encounters software or hardware errors, the cluster can become unavailable or lose cached data.","timestamp":"1694050680.0","comment_id":"1001091","upvote_count":"1"},{"upvote_count":"1","comment_id":"1001089","poster":"duriselvan","content":"A is Ans : \nRead replicas serve two additional purposes:\n\nScalability. If you have a large number of application clients that need to access DAX concurrently, you can add more replicas for read-scaling. DAX spreads the load evenly across all the nodes in the cluster. (Another way to increase throughput is to use larger cache node types.)\n\nHigh availability. In the event of a primary node failure, DAX automatically fails over to a read replica and designates it as the new primary. If a replica node fails, other nodes in the DAX cluster can still serve requests until the failed node can be recovered. For maximum fault tolerance, you should deploy read replicas in separate Availability Zones. This configuration ensures that your DAX cluster can continue to function, even if an entire Availability Zone becomes unavailable.","timestamp":"1694050200.0"},{"content":"A\nOnce u enable DAX you cant directly write onto or Read from Dynamo DB.","timestamp":"1693749300.0","poster":"AMohanty","upvote_count":"2","comment_id":"997664"},{"content":"Correct B.","poster":"ggrodskiy","comment_id":"960729","timestamp":"1690136820.0","upvote_count":"1"},{"comment_id":"951565","poster":"Just_Ninja","timestamp":"1689340680.0","content":"Selected Answer: B\nAWS recommend 3 nodes for production workloads.\nSo it must B","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nB for DAX HA","poster":"NikkyDicky","timestamp":"1688483940.0","comment_id":"942912"},{"comment_id":"930829","content":"Selected Answer: B\nDAX is used mostly to accelerate data reading, so that leaves us with B and D, Fault tolerance leaves B as the right choice","poster":"Maria2023","timestamp":"1687454400.0","upvote_count":"4"},{"poster":"rbm2023","content":"Selected Answer: B\nI initially went for option A, but I agree with B. Not only because of the 3-node option which eliminates A completely. But also due to the read and write pattern suggested on B. The application is latency sensitive and if need to reduce the latency as much as possible you need to write directly to Dynamo and read from DAX which is a Write-Around pattern.\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-accelerator-dax-a-read-throughwrite-through-cache-for-dynamodb/","timestamp":"1684679400.0","upvote_count":"2","comment_id":"903328"},{"content":"Selected Answer: B\nB is the answer","comment_id":"851820","timestamp":"1679899620.0","upvote_count":"1","poster":"mfsec"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.cluster.html\n2 node is not fault tolerant and in fact more nodes less latency. If there's an option with > 3nodes, I'd go for that instead.","upvote_count":"2","poster":"God_Is_Love","timestamp":"1678572060.0","comment_id":"836566"},{"poster":"kiran15789","upvote_count":"2","timestamp":"1677437460.0","content":"Selected Answer: B\nDynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB that can significantly improve read performance. In this scenario, since the platform is latency-sensitive, the goal is to reduce read latency.","comment_id":"822866"},{"upvote_count":"1","content":"Selected Answer: B\nAs per below link B is best option.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html#DAX.use-cases","comment_id":"821841","timestamp":"1677360300.0","poster":"saurabh1805"},{"poster":"c73bf38","upvote_count":"3","comment_id":"817990","timestamp":"1677080520.0","content":"The write-through behavior of DAX is appropriate for many application patterns. However, there are some application patterns where a write-through model might not be appropriate.\n\nFor applications that are sensitive to latency, writing through DAX incurs an extra network hop. So a write to DAX is a little slower than a write directly to DynamoDB. If your application is sensitive to write latency, you can reduce the latency by writing directly to DynamoDB instead. For more information, see Write-around.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html#DAX.consistency.strategies-for-writes.write-around"},{"poster":"spd","timestamp":"1676333460.0","comment_id":"807941","upvote_count":"1","content":"Selected Answer: B\nGoing with B due to more latency with Option A"},{"upvote_count":"1","comment_id":"795392","poster":"irene7","timestamp":"1675275480.0","content":"B, writing with DAX can have more latency and needs three nodes for HA"},{"content":"Selected Answer: A\nI\"ll go for A (enven if it is less in HA) but because it is the unique option where it it saied that read and write is done thru DAX","comments":[{"upvote_count":"2","comment_id":"794004","timestamp":"1675161360.0","poster":"mikeshop","content":"It said least latency. Writing via DAX has higher latency than writing directly to dynamodb."}],"poster":"zozza2023","timestamp":"1675043460.0","upvote_count":"1","comment_id":"792271"},{"timestamp":"1674937200.0","content":"Selected Answer: B\nA DAX cluster can be deployed with one or two nodes for development or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodes for production use. If a one- or two-node cluster encounters software or hardware errors, the cluster can become unavailable or lose cached data.A DAX cluster can be deployed with one or two nodes for development or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodes for production use. If a one- or two-node cluster encounters software or hardware errors, the cluster can become unavailable or lose cached data.","comment_id":"791055","upvote_count":"4","poster":"schalke04"},{"timestamp":"1673898900.0","comment_id":"778185","comments":[{"content":"Only primary node can write, thats mean if this node fail the second node would not have write. For that reason answer is B","comments":[{"timestamp":"1694141040.0","upvote_count":"2","poster":"k8s_Seoul","comment_id":"1002042","content":"Do you want correct answer from chatgpt bot?"}],"poster":"2aldous","comment_id":"890824","timestamp":"1683386220.0","upvote_count":"1"}],"content":"Selected Answer: A\nA. Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.\n\nDynamoDB Accelerator (DAX) is a service that provides a fully managed, in-memory cache for DynamoDB tables. By creating a two-node DAX cluster, the solution will be able to handle a large number of read and write requests with low latency. Configuring the application to read and write data by using DAX will enable the trading platform to take advantage of DAX's in-memory cache, resulting in improved performance.\n\nOption B, C and D are not correct because they all recommend using DAX only for reading data or writing data which will reduce the performance improvement, also having a single node DAX cluster will not ensure high availability for the trading platform as it has a single point of failure.","poster":"masetromain","upvote_count":"2"},{"upvote_count":"1","content":"A: two nodes with R/W through DAX","poster":"zhangyu20000","comment_id":"778016","timestamp":"1673890500.0"}],"topic":"1","answers_community":["B (92%)","8%"],"question_images":[],"question_text":"A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB table to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The new solution must ensure high availability for the trading platform.\n\nWhich solution will meet these requirements with the LEAST latency?","answer":"B","answer_images":[],"choices":{"B":"Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.","A":"Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.","C":"Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX.","D":"Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table."},"answer_description":"","answer_ET":"B","question_id":58,"isMC":true,"exam_id":33,"unix_timestamp":1673890500,"url":"https://www.examtopics.com/discussions/amazon/view/95602-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-01-16 18:35:00"},{"id":"cY1YDPiyah8bDMsALPqy","answer":"BE","topic":"1","isMC":true,"answer_ET":"BE","timestamp":"2023-01-16 16:28:00","question_text":"A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind another ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak usage of the application.\n\nThe application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives minimal traffic during the rest of the day.\n\nA solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.\n\nWhich combination of steps will meet these requirements? (Choose two.)","answer_description":"","exam_id":33,"answer_images":[],"question_images":[],"choices":{"D":"Change all the backend EC2 instances to Spot Instances.","B":"Move the application frontend to a static website that is hosted on Amazon S3.","C":"Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.","E":"Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances.","A":"Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances."},"unix_timestamp":1673882880,"discussion":[{"timestamp":"1700124360.0","upvote_count":"10","content":"Selected Answer: BE\nBurstable instances let you save costs, you pay for some baseline - say 40 percent, if the instance is utilized less - credits get accumulated. So, it is good for workloads with changing CPU loads.","comment_id":"1072278","poster":"severlight"},{"upvote_count":"5","timestamp":"1677437820.0","content":"Selected Answer: BE\nBurstable EC2 instances, also known as T instances, provide a baseline level of CPU performance with the ability to burst CPU usage when additional cycles are available. They are designed for workloads that do not require sustained high CPU performance but occasionally need more CPU power. Burstable instances can be a cost-effective option for workloads that have moderate CPU requirements but still require flexibility to handle occasional spikes in demand.","comment_id":"822874","poster":"kiran15789"},{"comments":[{"comment_id":"1271559","content":"just B,E","timestamp":"1724483640.0","upvote_count":"1","poster":"helloworldabc"}],"timestamp":"1716138240.0","comment_id":"1213889","content":"Uhm, S3 static website with a Python backend? Am I missing something? How can S3 interact with a backend?","upvote_count":"1","poster":"sse69"},{"poster":"career360guru","upvote_count":"1","content":"Selected Answer: BE\nOption B and E","comment_id":"1104453","timestamp":"1703394660.0"},{"timestamp":"1688484060.0","comment_id":"942915","content":"Selected Answer: BE\nit's BE","upvote_count":"3","poster":"NikkyDicky"},{"timestamp":"1684680120.0","poster":"rbm2023","upvote_count":"4","content":"Selected Answer: BE\nYou cannot move all backend to Spot Instances this will break the requirement for not affecting the application availability. \nYou can improve by moving the static site to S3, front end, and change the on demand instances to burst capacity.","comment_id":"903338"},{"comments":[{"comment_id":"864493","timestamp":"1680939240.0","upvote_count":"1","poster":"OCHT","content":"Option C suggests deploying the application frontend using AWS Elastic Beanstalk and using the same instance type for the nodes. Elastic Beanstalk is a fully managed service that makes it easy to deploy, run, and scale applications. It automatically handles the deployment and management of the underlying infrastructure, including capacity provisioning, load balancing, and auto-scaling. However, using Elastic Beanstalk with the same instance type as the existing EC2 instances may not necessarily reduce costs."}],"poster":"OCHT","timestamp":"1680939240.0","comment_id":"864492","content":"Selected Answer: BE\nAmazon EC2 Spot Instances allow you to take advantage of unused EC2 capacity in the AWS Cloud at a steep discount compared to On-Demand Instance prices. Spot Instances are well-suited for workloads that can be interrupted, such as batch processing, data analysis, and image or video processing. They can also be used for fault-tolerant workloads that can withstand the loss of an instance, such as web services or stateless applications.","upvote_count":"2"},{"content":"Selected Answer: BE\nBE makes the most sense here","timestamp":"1679901060.0","comment_id":"851841","poster":"mfsec","upvote_count":"1"},{"comment_id":"836583","content":"Selected Answer: BE\nBurstable because peak performance is needed at lunch time and its cost effective based on this - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html\nS3 static website hosting is cost effective","poster":"God_Is_Love","upvote_count":"5","timestamp":"1678573920.0"},{"content":"Selected Answer: BE\nThe correct answer is B, E.\nOption B of moving the frontend to a static website hosted on Amazon S3 will reduce the cost of running the frontend, as S3 is a lower cost storage option than EC2 instances.\nOption E of deploying the backend Python application to general purpose burstable EC2 instances will ensure that the backend EC2 instances have the capacity to handle spikes in usage, as burstable instances are designed to handle unpredictable workloads. This will help to optimize the cost of running the backend, as burstable instances are less expensive than On-Demand instances and more cost-effective than Spot instances.","upvote_count":"1","poster":"tatdatpham","timestamp":"1675292460.0","comment_id":"795570"},{"upvote_count":"4","content":"Selected Answer: BE\nB and E.\nOption D is wrong. A spot instance is not appropriate for a production server.\nBy the way, I would like another option that mentions changing the backend Python API Gateway and Lambda because Option B mentions changing the frontend serverless. I think this question is a typical use case of the serverless architecture.","poster":"Untamables","comment_id":"790327","timestamp":"1674884220.0"},{"timestamp":"1674612420.0","comment_id":"787192","upvote_count":"2","poster":"vsk12","content":"Selected Answer: BE\nCorrect answers are\nB & E\nOption B as S3 is a cost-effective storage solution for static websites.\nOption E as burstable general-purpose instances provides a cost-effective solution for this kind of workload."},{"poster":"masetromain","comments":[{"poster":"masetromain","timestamp":"1673899200.0","content":"Option A, C: Changing to compute optimized instances or using Elastic Beanstalk will not help reducing the cost, it will only change the instances type and not helping the cost optimization.\nOption E: Deploying the backend Python application to general purpose burstable EC2 instances will not help reducing the cost, as it still using On-Demand instances.\n\nIt is important to note that using spot instances comes with the risk of instances being terminated when the spot price goes up. To mitigate this risk, you could use the EC2 Auto Scaling group with a combination of on-demand and spot instances. This way, if a spot instance is terminated, the Auto Scaling group can automatically replace it with an on-demand instance to ensure the application is always available.","upvote_count":"1","comment_id":"778189"}],"comment_id":"778188","upvote_count":"4","timestamp":"1673899200.0","content":"Selected Answer: BD\nB. Move the application frontend to a static website that is hosted on Amazon S3.\nD. Change all the backend EC2 instances to Spot Instances.\n\nStep 1: Moving the application frontend to a static website that is hosted on Amazon S3 will reduce the cost and increase the scalability of the application. S3 is a highly scalable object storage service that can handle large amounts of data and traffic at a lower cost than running EC2 instances.\n\nStep 2: Changing the backend EC2 instances to Spot Instances can help reduce cost without negatively affecting the application availability. Spot Instances allow customers to bid on unused Amazon EC2 capacity, which can result in significant cost savings. You can also use AWS Auto Scaling to automatically increase or decrease the number of Spot Instances based on the application's traffic."},{"timestamp":"1673882880.0","content":"BE are correct\nA: Compute optimized instance is expensive than burstable instance\nB: S3 hosted static web server is cheaper\nC: Not save money\nD: Spot instance affect availibility\nE: Burstable EC2 is cheaper","upvote_count":"2","comment_id":"777864","comments":[{"content":"To mitigate this risk, you could use the EC2 Auto Scaling group with a combination of on-demand and spot instances. This way, if a spot instance is terminated, the Auto Scaling group can automatically replace it with an on-demand instance to ensure the application is always available.","poster":"masetromain","timestamp":"1673976780.0","comment_id":"779144","upvote_count":"1"}],"poster":"zhangyu20000"}],"answers_community":["BE (90%)","10%"],"question_id":59,"url":"https://www.examtopics.com/discussions/amazon/view/95570-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"AxsSYaYlhfpbof7Q3dEp","answer_ET":"B","answer":"B","timestamp":"2023-01-16 16:38:00","answer_images":[],"isMC":true,"question_text":"A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate.\n\nThe platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.\n\nWhich solution will provide the MOST cost-effective setup for the platform?","url":"https://www.examtopics.com/discussions/amazon/view/95572-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"answers_community":["B (51%)","D (42%)","4%"],"question_id":60,"choices":{"D":"Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.","B":"Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks.","C":"Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.","A":"Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year."},"unix_timestamp":1673883480,"topic":"1","discussion":[{"upvote_count":"19","content":"Selected Answer: B\nOption A, C and D are wrong. They all mention using spot instances and EKS based on EC2. A spot instance is not appropriate for a production server and the company is developing new application designed for AWS Fargate, which means we must plan the future cost improvement including AWS Fargate.\nhttps://aws.amazon.com/savingsplans/compute-pricing/","timestamp":"1674885900.0","poster":"Untamables","comment_id":"790342"},{"content":"B is correct. Compute saving plan will also cover Fargate\nA: use spot instance is not reliable\nCD: manually scale up DB","poster":"zhangyu20000","upvote_count":"12","comment_id":"777871","timestamp":"1673883480.0"},{"timestamp":"1734578340.0","poster":"bhanus","comment_id":"1328839","content":"Selected Answer: D\nCompute Savings Plans for EKS Base Load:\nSpot Instances for Peaks:\n1-Year All Upfront Reserved Instances for Database Base Load:","upvote_count":"1"},{"comment_id":"1313606","content":"B - Fargate cannot support Spot - https://docs.aws.amazon.com/eks/latest/userguide/fargate.html","poster":"nelgeozcin","timestamp":"1731856860.0","upvote_count":"1"},{"content":"Selected Answer: D\nNo brainer, it's D. C doesn't provide any cost-effectiveness!","timestamp":"1729319820.0","upvote_count":"4","comment_id":"1299919","poster":"Sin_Dan"},{"comment_id":"1299220","timestamp":"1729169700.0","content":"Selected Answer: B\nA wrong : Spot Instances to handle peaks\nB: correct\nC & D wrong : Temporarily scale up the DB instance manually during peaks.","poster":"FZA24","upvote_count":"1"},{"poster":"vip2","comments":[{"content":"just B","upvote_count":"1","poster":"helloworldabc","comment_id":"1271569","timestamp":"1724484660.0"}],"content":"Selected Answer: D\nD looks more correctable\nMainly diff. between B and D is \npredicable workload-- all upfront\nno specific for read-replication traffic\nOn-Demand Capacity Reservations ensure availability during peak times without long-term commitments. but no cost-effective","comment_id":"1256636","upvote_count":"4","timestamp":"1722152640.0"},{"timestamp":"1714978620.0","comment_id":"1207223","poster":"thotwielder","content":"Selected Answer: D\nIt's between b and d. D is more cost effective because of spot instances. And B is wrong because there is no reason to scale read replicas for RDS (the question doesn't say read only load)","upvote_count":"6"},{"upvote_count":"6","comment_id":"1167419","content":"Selected Answer: D\nI really don't understand why people are saying that Spot instances aren't suitable for production. There is a two-minute respite before they shut down, and since the application is not said to be stateful, this is plenty of time for a single request/response cycle.\n\nWith this in mind, the correct solution is D.","poster":"Dgix","timestamp":"1709751360.0","comments":[{"upvote_count":"2","content":"slightly difference betwee B and D {other than spot instances ofcourse}. Since the platform experinces peaks, might be a better idea to go for savings plan with medium load","timestamp":"1712248440.0","poster":"Keval12345","comment_id":"1189453"}]},{"upvote_count":"2","comment_id":"1147793","timestamp":"1707705180.0","poster":"saggy4","content":"Selected Answer: B\nA and C: The company will have a mix of EKS on EC2 and EKS Fargate hence reserved instance is not possible as it will cover only EKS on EC2 hence A and C are out\n\nBetween B and C:\nC seems to save the most cost, but during peak load spot instances (both EC2 or Fargate) will not provide guaranteed availability. Hence we should go ahead with B.\n\nCorrect Answer: B"},{"poster":"AWSLord32","content":"Selected Answer: C\nC is the right answer. Everything about B is wrong: Compute savings plan is more expensive than RI, on demand more expensive than spot for peaks and no upfront more exponsive than all upfront.","comment_id":"1146566","upvote_count":"1","timestamp":"1707589260.0"},{"content":"Selected Answer: D\nThe scenario ask for the most cost-effective setup. Thus:\nA = RI doesn't cover Fargate\nB = ODCR doesn't bring cost benefits, they just ensure you have capacity. Read replicas are for read only, I would expect workload peaks includes writes so this is not saving money nor fully helping with capacity needs\nC = EC2 Saving Plans do not cover Fargate\nD = correct (this is the most cost-effective setup, Compute Savings Plans apply to both EC2 and Fargate, Spot Instances applies to both EC2 and Fargate, All Upfront Reserved Instances is most cost effective option for RDS. Manually scaling RDS adds a lot of overhead, but this is not the point of the question)","comment_id":"1130282","comments":[{"comment_id":"1130289","upvote_count":"2","content":"Also, for a temporarily limited change it is easier to manually vertically scale your instance rather than adding Read replicas as adding replicas to a single instance requires to change your app to send read requests to the reader endpoint and not to the cluster (aka writer) endpoint","timestamp":"1706076840.0","poster":"ninomfr64"}],"timestamp":"1706076360.0","upvote_count":"6","poster":"ninomfr64"},{"timestamp":"1703692260.0","comment_id":"1106969","upvote_count":"1","content":"I might be leaning toward D as it does ask for the. most cost-effective solution","poster":"Jay_2pt0_1"},{"upvote_count":"8","poster":"career360guru","comment_id":"1104457","content":"Selected Answer: D\nCompute saving plans are more cost effective so B or D are right two options.\nBetween B and D - Spot instances offers better cost and Fargate supports spot instances\nhttps://aws.amazon.com/blogs/aws/aws-fargate-spot-now-generally-available/\nOption B says, scale RDS Read-Replica for based on events which may not work as workload description does not mentioned that peak load is only read traffic. So D is best and most cost effective solution.","timestamp":"1703395800.0"},{"poster":"Hyperdanny","upvote_count":"2","content":"Selected Answer: C\nI am leaning towards C, since Instance savings provide the biggest discount.\nI also couldn't find a way to scale EKS based on dates, which B suggests: \"Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks\"","comment_id":"957511","timestamp":"1689854400.0"},{"content":"Selected Answer: B\nits a b","poster":"NikkyDicky","timestamp":"1688484180.0","upvote_count":"2","comment_id":"942918"},{"comment_id":"926903","content":"Selected Answer: B\nOption A, C and D are wrong. They all mention using spot instances and EKS based on EC2. A spot instance is not appropriate for a production server and the company is developing new application designed for AWS Fargate, which means we must plan the future cost improvement including AWS Fargate.","upvote_count":"4","timestamp":"1687122300.0","poster":"SkyZeroZx"},{"poster":"Roontha","comment_id":"915741","timestamp":"1686002040.0","upvote_count":"1","content":"I go with B post reading aws portal.\n\nhttps://aws.amazon.com/savingsplans/compute-pricing/\n\nCompute Savings Plans\n\nCompute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, Region, OS or tenancy, and also apply to Fargate or Lambda usage. For example, with Compute Savings Plans, you can change from C4 to M5 instances, shift a workload from EU (Ireland) to EU (London), or move a workload from EC2 to Fargate or Lambda at any time and automatically continue to pay the Savings Plans price."},{"timestamp":"1684689900.0","poster":"y0eri","comment_id":"903409","content":"Selected Answer: D\nCompute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance [...], and also apply to Fargate or Lambda usage. For example, with Compute Savings Plans, you can [...] move a workload from EC2 to Fargate.\n\nVertical scaling is the most straightforward approach to adding more capacity in your database. [...] You can vertically scale up [or down] your RDS instance with a click of a button.\n\nSuppose that you purchase a db.t2.medium reserved DB instance, [...] if you have one db.t2.large instance running in your account in the same AWS Region, the billing benefit is applied to 50 percent of the usage of the DB instance. \n\nhttps://aws.amazon.com/savingsplans/compute-pricing/\nhttps://aws.amazon.com/blogs/database/scaling-your-amazon-rds-instance-vertically-and-horizontally/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithReservedDBInstances.html","upvote_count":"5"},{"poster":"yama234","timestamp":"1681888560.0","content":"B\nCompute Savings Plans saving EC2 and Fargate.\nproduction don't using Spot Instances","upvote_count":"1","comment_id":"874338"},{"comment_id":"865743","content":"Selected Answer: A\nA makes sense to me","timestamp":"1681065960.0","poster":"dev112233xx","upvote_count":"2"},{"poster":"Amac1979","timestamp":"1679963460.0","comment_id":"852616","upvote_count":"5","content":"Selected Answer: D\ncapacity reservations do not offer discounts. D is correct"},{"upvote_count":"3","content":"Selected Answer: B\nPurchase Compute Savings Plans for the predicted medium load of the EKS cluster.","poster":"mfsec","timestamp":"1679901240.0","comment_id":"851845"},{"content":"Selected Answer: B\nOut of some research initially against B, had to choose B because of this - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html\nOn Demand capacity reservations can be done anytime, so before events they can reserve and after events they can release to save costs\nFrom above link - \n\n Events — you can create Capacity Reservations before your business-critical events to ensure that you can scale when you need to.\n\n\"You can create Capacity Reservations at any time, without entering into a one-year or three-year term commitment. The capacity becomes available and billing starts as soon as the Capacity Reservation is provisioned in your account. When you no longer need the capacity assurance, cancel the Capacity Reservation to release the capacity and to stop incurring charges. You can also use the billing discounts offered by Savings Plans and Regional Reserved Instances to reduce the cost of a Capacity Reservation.\"","poster":"God_Is_Love","comment_id":"836653","timestamp":"1678583640.0","upvote_count":"7"},{"content":"Selected Answer: B\nspot instances never a good answer","comment_id":"834391","poster":"kiran15789","upvote_count":"3","timestamp":"1678394580.0"},{"content":"Selected Answer: B\nSurely B","upvote_count":"4","comment_id":"816237","poster":"[Removed]","timestamp":"1676957880.0"},{"poster":"spd","timestamp":"1676421360.0","upvote_count":"3","content":"Selected Answer: B\nagree with B","comment_id":"808971"},{"timestamp":"1676169660.0","content":"Selected Answer: B\nA is not good because the DB will be underutilized (1yr RI to meet the _predicted peak_). You need a reliable on-demand on event dates. There is little incentive but more downside of unreliability if you choose Spots on event dates.","poster":"moota","comment_id":"805902","upvote_count":"4"},{"comment_id":"795575","content":"Selected Answer: B\nAgree with zhangyu20000","timestamp":"1675292880.0","poster":"tatdatpham","upvote_count":"3"},{"poster":"zozza2023","timestamp":"1675044840.0","comment_id":"792302","content":"Selected Answer: B\nI agree with zhangyu20000","upvote_count":"2"},{"upvote_count":"2","comment_id":"784003","comments":[{"comment_id":"784591","upvote_count":"1","timestamp":"1674413040.0","content":"The company is developing new application features to run on Amazon EKS with AWS Fargate.\nSo any solution that deal with EC2 for EKS are not correct. ACD all deal with EC2.","poster":"zhangyu20000"},{"comment_id":"788223","timestamp":"1674691680.0","poster":"zhangyu20000","upvote_count":"1","content":"never build real time system on spot instance, spot instance is good for no-real time data processing, not good for web server"}],"content":"Selected Answer: D\nI think it's D. They use Savings Plans for the predicted base load. They scale on Spot during peaks. They manually scale up DB as they know the event dates - scaling out read replicas only won't help with writes.","poster":"pitakk","timestamp":"1674371040.0"},{"poster":"masetromain","upvote_count":"2","comments":[{"timestamp":"1673899500.0","comment_id":"778192","content":"Option B, C and D all use a combination of reserved instances, savings plans and spot instances but they don't fully utilize the cost savings of reserved instances and savings plans.\nOption B uses On-demand Capacity Reservations which are not cost-effective as they are more expensive than spot instances.\nOption C and D both use manually scaling the DB instance during peaks which is a labor-intensive process and can cause delays in handling the peak demand.","upvote_count":"1","poster":"masetromain"},{"comment_id":"795573","upvote_count":"1","content":"No, the another key point is \"The company is developing new application features to run on Amazon EKS with AWS Fargate\" -> It mean the do not want manage EC2. B is correct","poster":"tatdatpham","timestamp":"1675292820.0"},{"comment_id":"779214","poster":"zhangyu20000","timestamp":"1673980920.0","upvote_count":"2","content":"The company is developing new application features to run on Amazon EKS with AWS Fargate. This means customer should not manage EC2 at all. So ACD all incorrect"}],"timestamp":"1673899500.0","content":"Selected Answer: A\nA. Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.\n\nThis solution provides the most cost-effective setup for the platform by combining the cost savings of reserved instances with the flexibility of spot instances. By purchasing Standard Reserved Instances for the baseline load of the EKS cluster, the company can save money on the cost of running the instances. For the infrequent high peaks in demand, the company can scale the cluster with Spot Instances, which are significantly cheaper than on-demand instances. Finally, purchasing 1-year All Upfront Reserved Instances for the database will help meet the predicted peak load for the year and will help save more cost.","comment_id":"778190"}],"exam_id":33,"answer_description":""}],"exam":{"isMCOnly":true,"provider":"Amazon","numberOfQuestions":529,"name":"AWS Certified Solutions Architect - Professional SAP-C02","lastUpdated":"11 Apr 2025","isBeta":false,"isImplemented":true,"id":33},"currentPage":12},"__N_SSP":true}