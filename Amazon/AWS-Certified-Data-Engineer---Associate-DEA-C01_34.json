{"pageProps":{"questions":[{"id":"6U0NnOAX76T9TBuQwixa","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/135451-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"AD","answer_description":"","unix_timestamp":1709845020,"question_id":166,"choices":{"D":"Upgrade to a larger instance size.","E":"Implement caching to reduce the database query load.","C":"Reboot the RDS DB instance once each week.","B":"Modify the database schema to include additional tables and indexes.","A":"Use the Performance Insights feature of Amazon RDS to identify queries that have high CPU utilization. Optimize the problematic queries."},"question_text":"A company uses Amazon RDS for MySQL as the database for a critical application. The database workload is mostly writes, with a small number of reads.\nA data engineer notices that the CPU utilization of the DB instance is very high. The high CPU utilization is slowing down the application. The data engineer must reduce the CPU utilization of the DB Instance.\nWhich actions should the data engineer take to meet this requirement? (Choose two.)","answer":"AD","isMC":true,"discussion":[{"poster":"kj07","timestamp":"1710405840.0","upvote_count":"10","comment_id":"1173246","content":"Here the issue is with the writes and caching will not solve them.\n\nI will go with A and D."},{"timestamp":"1712021220.0","upvote_count":"9","comment_id":"1187784","poster":"lucas_rfsb","content":"Selected Answer: AD\nI will go for A and D, since other options are more likely to improve read performance issues."},{"poster":"michele_scar","timestamp":"1730902800.0","comment_id":"1307900","content":"Selected Answer: AD\nWith A you should understand why the CPU is in high loading. B is mentioned in the last phrase of A (optimizing). Remain valid only D","upvote_count":"2"},{"comment_id":"1207931","content":"A and D\nWith a workload that is mostly writes and a small number of reads, caching will not be as effective in reducing CPU utilization compared to read-heavy workloads. \nhttps://repost.aws/knowledge-center/rds-aurora-postgresql-high-cpu","upvote_count":"2","poster":"sdas1","timestamp":"1715096700.0"},{"poster":"fceb2c1","upvote_count":"5","comment_id":"1181455","timestamp":"1711277280.0","content":"Selected Answer: AD\nA and D.\n\nFor A it is mentioned here https://repost.aws/knowledge-center/rds-instance-high-cpu"},{"upvote_count":"5","content":"Selected Answer: BD\nSince the questions states that \"the database workload is mostly writes\" let's eliminate the options that improves the reads.","timestamp":"1710843180.0","comment_id":"1177208","poster":"GiorgioGss"},{"upvote_count":"4","comment_id":"1168355","content":"Ans. AE\nA) Use Amazon RDS Performance Insights to identify the query that's responsible for the database load. Check the SQL tab that corresponds to a particular timeframe.\nE) If there's a query that's repeatedly running, use prepared statements to lower the pressure on your CPU. Repeated running of prepared statements caches the query plan. Because the plan is already in cache for further runs, the time for planning is much less.\nhttps://repost.aws/knowledge-center/rds-aurora-postgresql-high-cpu","poster":"damaldon","timestamp":"1709845020.0"}],"answers_community":["AD (76%)","BD (24%)"],"answer_images":[],"question_images":[],"exam_id":21,"timestamp":"2024-03-07 21:57:00"},{"id":"1Olof47WGx9kxR3pAo1y","answer_images":[],"answer_description":"","question_id":167,"question_text":"A company has used an Amazon Redshift table that is named Orders for 6 months. The company performs weekly updates and deletes on the table. The table has an interleaved sort key on a column that contains AWS Regions.\nThe company wants to reclaim disk space so that the company will not run out of storage space. The company also wants to analyze the sort key column.\nWhich Amazon Redshift command will meet these requirements?","exam_id":21,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/135091-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["C (83%)","A (17%)"],"answer":"C","answer_ET":"C","question_images":[],"discussion":[{"poster":"GiorgioGss","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\n\"A full vacuum doesn't perform a reindex for interleaved tables. To reindex interleaved tables followed by a full vacuum, use the VACUUM REINDEX option.\"\nA - \"A full vacuum doesn't perform a reindex for interleaved tables.\"- from the docs above\nB- \"A DELETE ONLY vacuum operation doesn't sort table data.\" - from the docs above\nD - \"without reclaiming space freed by deleted rows. \" - from the docs above","comment_id":"1177214","timestamp":"1710843660.0","upvote_count":"12"},{"comment_id":"1292373","timestamp":"1727867400.0","poster":"Shatheesh","content":"Selected Answer: A","upvote_count":"1"},{"upvote_count":"1","timestamp":"1715177100.0","poster":"d8945a1","comment_id":"1208413","content":"Selected Answer: C\nVACUUM REINDEX makes an additional pass to analyze the interleaved sort keys. \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html#r_VACUUM_command-parameters"},{"poster":"Christina666","content":"Selected Answer: C\nReclaiming Space: After updates and deletes, Redshift tables can retain deleted data blocks, taking up space. The VACUUM REINDEX command:\n\nReclaims the space taken up by the deleted rows.\nRebuilds indexes on the sort key columns.\nAnalyzing the Sort Key: Since the sort key column contains AWS Regions, rebuilding the indexes on this column will help cluster data according to region. This clustering can improve performance for queries that filter or group by region.","comment_id":"1194731","upvote_count":"1","timestamp":"1712992560.0"},{"comment_id":"1190303","upvote_count":"1","content":"Selected Answer: C\nCorrect Answer: C\n\nRequirements:\n1. relcaim the disk space\n2. analyze the sork key column\n\nDocument: https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html#vacuum-reindex\nVACUUM FULL: A full vacuum doesn't perform a reindex for interleaved tables. To reindex interleaved tables followed by a full vacuum, use the VACUUM REINDEX option.\nVACUUM REINDEX: Analyzes the distribution of the values in interleaved sort key columns, then performs a full VACUUM operation.","timestamp":"1712393040.0","poster":"arvehisa"},{"upvote_count":"3","comment_id":"1188812","timestamp":"1712167140.0","content":"Selected Answer: A\nFULL is the only one which claims space and sorts. \nFULL\nSorts the specified table (or all tables in the current database) and reclaims disk space occupied by rows that were marked for deletion by previous UPDATE and DELETE operations. VACUUM FULL is the default.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html","poster":"lucas_rfsb"},{"timestamp":"1710513780.0","content":"B is the answer","comment_id":"1174320","poster":"damaldon","upvote_count":"1"},{"timestamp":"1710406800.0","comment_id":"1173255","poster":"kj07","content":"Option C\n\nAnalyzes the distribution of the values in interleaved sort key columns, then performs a full VACUUM operation. If REINDEX is used, a table name is required.\n\nVACUUM REINDEX takes significantly longer than VACUUM FULL because it makes an additional pass to analyze the interleaved sort keys. The sort and merge operation can take longer for interleaved tables because the interleaved sort might need to rearrange more rows than a compound sort.\n\nIf a VACUUM REINDEX operation terminates before it completes, the next VACUUM resumes the reindex operation before performing the full vacuum operation.","upvote_count":"1"},{"content":"The answer should be B. The VACUUM DELETE ONLY command is used in Amazon Redshift to remove rows that have been marked for deletion due to updates and deletes in a table.","upvote_count":"1","poster":"CalvinL4","comment_id":"1164502","timestamp":"1709442360.0"}],"choices":{"C":"VACUUM REINDEX Orders","D":"VACUUM SORT ONLY Orders","B":"VACUUM DELETE ONLY Orders","A":"VACUUM FULL Orders"},"topic":"1","timestamp":"2024-03-03 06:06:00","unix_timestamp":1709442360},{"id":"IRkHcAQpv5FkuXpJgPX4","answer":"C","unix_timestamp":1706875200,"question_text":"A manufacturing company wants to collect data from sensors. A data engineer needs to implement a solution that ingests sensor data in near real time.\nThe solution must store the data to a persistent data store. The solution must store the data in nested JSON format. The company must have the ability to query from the data store with a latency of less than 10 milliseconds.\nWhich solution will meet these requirements with the LEAST operational overhead?","answers_community":["C (100%)"],"question_id":168,"discussion":[{"upvote_count":"2","comment_id":"1228685","content":"Selected Answer: C\nAmazon Kinesis Data Streams is a fully managed service that allows for seamless integration of diverse data sources, including IoT sensors. By using Kinesis Data Streams as the ingestion mechanism, the company can avoid the overhead of setting up and managing an Apache Kafka cluster or other data ingestion pipelines.","poster":"pypelyncar","timestamp":"1718146380.0"},{"upvote_count":"3","comments":[{"content":"to be more accurate, \nKinesis Data streams = real time\nKinesis Data Firehose = near real time","comment_id":"1245465","upvote_count":"4","timestamp":"1720612380.0","poster":"GustonMari"}],"comment_id":"1205246","timestamp":"1714609200.0","poster":"Snape","content":"Selected Answer: C\nnear real time = Kinesis Data streams"},{"timestamp":"1713303420.0","poster":"Ousseyni","content":"Selected Answer: C\nOption C is the best solution to meet the requirements","upvote_count":"1","comment_id":"1196832"},{"timestamp":"1709376480.0","upvote_count":"3","comment_id":"1164063","poster":"Felix_G","content":"Option C is the best solution to meet the requirements with the least operational overhead:\n\nUse Amazon Kinesis Data Streams to ingest real-time sensor data\nStore the nested JSON data in Amazon DynamoDB for low latency queries\nThe key advantages of Option C are:\n\nKinesis Data Streams fully manages real-time data ingestion with auto-scaling and persistence\nDynamoDB provides single digit millisecond latency for queries\nDynamoDB natively supports nested JSON data models\nFully managed services minimize operational overhead\nIn contrast:\n\nOption A requires managing Kafka clusters\nOption B uses Lambda which can't provide persistent storage\nOption D requires integrating SQS, Glue, and RDS leading to complexity"},{"content":"Selected Answer: C\nOption C, using Amazon Kinesis Data Streams to capture the sensor data and storing it in Amazon DynamoDB for querying, is the best solution to meet the requirements with the least operational overhead. This solution is well-optimized for real-time data ingestion, supports the desired data format, and provides the necessary query performance.","poster":"rralucard_","upvote_count":"3","timestamp":"1706875200.0","comment_id":"1138497"}],"exam_id":21,"answer_ET":"C","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/132688-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true,"choices":{"A":"Use a self-hosted Apache Kafka cluster to capture the sensor data. Store the data in Amazon S3 for querying.","D":"Use Amazon Simple Queue Service (Amazon SQS) to buffer incoming sensor data. Use AWS Glue to store the data in Amazon RDS for querying.","B":"Use AWS Lambda to process the sensor data. Store the data in Amazon S3 for querying.","C":"Use Amazon Kinesis Data Streams to capture the sensor data. Store the data in Amazon DynamoDB for querying."},"timestamp":"2024-02-02 13:00:00","topic":"1","answer_description":"","answer_images":[]},{"id":"uMbpK5mZDDSG9UZ4Eq66","answer_ET":"A","question_id":169,"timestamp":"2024-02-02 13:03:00","choices":{"B":"Use Amazon QuickSight to access the data. Use column-level security features in QuickSight to limit the PII that users can retrieve from Amazon S3 by using Amazon Athena. Define QuickSight access levels based on the PII access requirements of the users.","D":"Create IAM roles that have different levels of granular access. Assign the IAM roles to IAM user groups. Use an identity-based policy to assign access levels to user groups at the column level.","A":"Use Amazon Athena to query the data. Set up AWS Lake Formation and create data filters to establish levels of access for the company's IAM roles. Assign each user to the IAM role that matches the user's PII access requirements.","C":"Build a custom query builder UI that will run Athena queries in the background to access the data. Create user groups in Amazon Cognito. Assign access levels to the user groups based on the PII access requirements of the users."},"exam_id":21,"answer":"A","answer_images":[],"question_images":[],"unix_timestamp":1706875380,"discussion":[{"timestamp":"1727833320.0","upvote_count":"3","comment_id":"1187788","poster":"lucas_rfsb","content":"Selected Answer: A\nAmazon Athena to query the data and setting up AWS Lake Formation with data filters, the company can ensure that user groups can access only the personally identifiable information (PII) that they require. The combination of Athena for querying and Lake Formation for access control provides a comprehensive solution for managing PII access requirements effectively and securely"},{"poster":"Felix_G","timestamp":"1725390960.0","content":"Selected Answer: A\nThe solution that will meet the requirements with the LEAST effort is:\n\nA. Use Amazon Athena to query the data. Set up AWS Lake Formation and create data filters to establish levels of access for the company’s IAM roles. Assign each user to the IAM role that matches the user’s PII access requirements.\n\nThis option leverages AWS Lake Formation to create data filters and establish access levels for IAM roles, providing a straightforward approach to managing user access based on PII requirements.","upvote_count":"2","comment_id":"1165092"},{"content":"Selected Answer: A\nOption A, using Amazon Athena with AWS Lake Formation, is the most suitable solution. Lake Formation is designed to provide fine-grained access control to data lakes stored in S3 and integrates well with Athena, thereby meeting the requirements with the least effort.\nhttps://aws.amazon.com/blogs/big-data/anonymize-and-manage-data-in-your-data-lake-with-amazon-athena-and-aws-lake-formation/","upvote_count":"4","poster":"rralucard_","comment_id":"1138501","timestamp":"1722592980.0"}],"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/132689-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","question_text":"A company stores data in a data lake that is in Amazon S3. Some data that the company stores in the data lake contains personally identifiable information (PII). Multiple user groups need to access the raw data. The company must ensure that user groups can access only the PII that they require.\nWhich solution will meet these requirements with the LEAST effort?","answers_community":["A (100%)"]},{"id":"lqOJC09SJoogEsSUssiC","unix_timestamp":1706977260,"answers_community":["BD (94%)","6%"],"timestamp":"2024-02-03 17:21:00","choices":{"E":"Configure an AWS Lambda function to invoke an AWS Glue job when a file is loaded into the S3 bucket. Configure the AWS Glue job to read the files from the S3 bucket into an Apache Spark DataFrame. Configure the AWS Glue job to also put smaller partitions of the DataFrame into an Amazon Kinesis Data Firehose delivery stream. Configure the delivery stream to load data into the Amazon Redshift tables.","B":"Use an Amazon EventBridge rule to invoke an AWS Glue workflow job every 15 minutes. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.","C":"Configure an AWS Lambda function to invoke an AWS Glue crawler when a file is loaded into the S3 bucket. Configure an AWS Glue job to process and load the data into the Amazon Redshift tables. Create a second Lambda function to run the AWS Glue job. Create an Amazon EventBridge rule to invoke the second Lambda function when the AWS Glue crawler finishes running successfully.","A":"Use an Amazon EventBridge rule to run an AWS Glue job every 15 minutes. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.","D":"Configure an AWS Lambda function to invoke an AWS Glue workflow when a file is loaded into the S3 bucket. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables."},"question_images":[],"answer_images":[],"discussion":[{"comment_id":"1140287","timestamp":"1722777360.0","comments":[{"upvote_count":"1","poster":"Felix_G","timestamp":"1725391620.0","comments":[{"upvote_count":"5","comment_id":"1185668","timestamp":"1727636340.0","poster":"Luke97","content":"A is NOT correct. The question said \"The ETL pipeline must function correctly despite changes to the data schema\", therefore run Glue crawler is necessary to handle schema changes."}],"content":"D is incorrect! Options C, D and E have issues like unnecessary complexity, latency due to triggers, or limitations in handling large file sizes. So B and A are the best and most robust options that meet all the requirements.","comment_id":"1165102"}],"poster":"rralucard_","upvote_count":"10","content":"Selected Answer: BD\nOption B: Amazon EventBridge Rule with AWS Glue Workflow Job Every 15 Minutes - for its streamlined process, automated scheduling, and ability to handle schema changes.\n\nOption D: AWS Lambda to Invoke AWS Glue Workflow When a File is Loaded - for its responsiveness to file arrival and adaptability to schema changes, though it is slightly more complex than option B."},{"comment_id":"1329072","timestamp":"1734627780.0","content":"Selected Answer: BD\nchange od schema is the key","poster":"HagarTheHorrible","upvote_count":"1"},{"comment_id":"1218346","poster":"valuedate","upvote_count":"1","content":"Selected Answer: BD\neventbridge rule or event trigger","timestamp":"1732548420.0"},{"content":"Selected Answer: AE\nChatCGT sid A and E","timestamp":"1729115220.0","upvote_count":"1","comment_id":"1196836","poster":"Ousseyni","comments":[{"timestamp":"1732548360.0","content":"ChatCGT? ahahaha. A is NOT correct and E its too complex","poster":"valuedate","upvote_count":"2","comment_id":"1218345"},{"content":"You should double check your information.","upvote_count":"2","poster":"tgv","comment_id":"1222514","timestamp":"1733037480.0"}]},{"timestamp":"1728804240.0","upvote_count":"1","comment_id":"1194738","poster":"Christina666","content":"Selected Answer: BD\neventbridge rule or event trigger"},{"timestamp":"1728205320.0","upvote_count":"1","comment_id":"1190312","poster":"arvehisa","content":"I don't think this pipeline should be triggered by an s3 file upload. However seems A cannot handle the data schema change.\n\nif s3 trigger is good, then C and E are unnessessarily complexed. so I would go with B & D (despite the s3 trigger)"},{"timestamp":"1727880540.0","upvote_count":"3","poster":"lucas_rfsb","comment_id":"1188118","content":"Selected Answer: BD\nI will go with BD"},{"upvote_count":"2","content":"Selected Answer: AB\nThe two data pipeline solutions that will meet the requirements are:\n\nA. Use an Amazon EventBridge rule to run an AWS Glue job every 15 minutes. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.\n\nB. Use an Amazon EventBridge rule to invoke an AWS Glue workflow job every 15 minutes. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables.\n\nThese solutions leverage AWS Glue to process and load the data from different file formats in the S3 bucket into the Amazon Redshift tables, while also handling changes to the data schema.","timestamp":"1725391500.0","poster":"Felix_G","comments":[{"poster":"chris_spencer","upvote_count":"1","content":"A is incorret, it doesn't take care to update the data catalog.","comment_id":"1197727","timestamp":"1729230420.0"}],"comment_id":"1165100"},{"comment_id":"1139449","upvote_count":"1","poster":"evntdrvn76","timestamp":"1722694860.0","content":"The correct answers are A. Use an Amazon EventBridge rule to run an AWS Glue job every 15 minutes. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables and B. Use an Amazon EventBridge rule to invoke an AWS Glue workflow job every 15 minutes. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables. These solutions automate the ETL pipeline with minimal operational overhead."}],"question_text":"A data engineer must build an extract, transform, and load (ETL) pipeline to process and load data from 10 source systems into 10 tables that are in an Amazon Redshift database. All the source systems generate .csv, JSON, or Apache Parquet files every 15 minutes. The source systems all deliver files into one Amazon S3 bucket. The file sizes range from 10 MB to 20 GB. The ETL pipeline must function correctly despite changes to the data schema.\nWhich data pipeline solutions will meet these requirements? (Choose two.)","answer":"BD","isMC":true,"answer_description":"","topic":"1","question_id":170,"exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/132744-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"BD"}],"exam":{"id":21,"name":"AWS Certified Data Engineer - Associate DEA-C01","lastUpdated":"11 Apr 2025","provider":"Amazon","isImplemented":true,"isMCOnly":true,"numberOfQuestions":207,"isBeta":false},"currentPage":34},"__N_SSP":true}