{"pageProps":{"questions":[{"id":"GjkwHs9aUfxd5iFdVOmm","unix_timestamp":1720256760,"timestamp":"2024-07-06 11:06:00","question_id":186,"discussion":[{"content":"Selected Answer: D\nAmazon FSx for NetApp ONTAP support Multi-protocol access to data using the Network File System (NFS), Server Message Block (SMB), Internet Small Computer Systems Interface (iSCSI), and Non-Volatile Memory Express (NVMe) protocols\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html#features-overview","timestamp":"1722414900.0","upvote_count":"4","comment_id":"1258728","poster":"jamesf"},{"content":"Selected Answer: D\n---> D","poster":"tgv","timestamp":"1721391480.0","comment_id":"1251182","upvote_count":"3"},{"timestamp":"1721008200.0","upvote_count":"4","poster":"trungtd","content":"Selected Answer: D\nNetApp ONTAP:\nMulti-Protocol Support:\n- SMB for Windows: Fully supports the SMB protocol required by your Windows applications.\n- NFS for Linux: Fully supports the NFS protocol required by your Linux applications.\n\nCross-Region Replication:\nNetApp SnapMirror: Provides efficient and reliable replication of data between ONTAP instances in different regions","comment_id":"1248017"},{"timestamp":"1720643160.0","comment_id":"1245734","poster":"TEC1","content":"Selected Answer: D\nAmazon FSx for NetApp ONTAP supports both SMB and NFS protocols, making it suitable for both Windows and Linux applications","upvote_count":"4"}],"topic":"1","exam_id":23,"question_images":[],"question_text":"A company is migrating its on-premises Windows applications and Linux applications to AWS. The company will use automation to launch Amazon EC2 instances to mirror the on-premises configurations. The migrated applications require access to shared storage that uses SMB for Windows and NFS for Linux.\n\nThe company is also creating a pilot light disaster recovery (DR) environment in another AWS Region. The company will use automation to launch and configure the EC2 instances in the DR Region. The company needs to replicate the storage to the DR Region.\n\nWhich storage solution will meet these requirements?","answer_description":"","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/143404-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"B":"Use Amazon Elastic Block Store (Amazon EBS) for the application storage. Create a backup plan in AWS Backup that creates snapshots of the EBS volumes that are in the primary Region and replicates the snapshots to the DR Region.","D":"Use Amazon FSx for NetApp ONTAP for the application storage. Create an FSx for ONTAP instance in the DR Region. Configure NetApp SnapMirror replication from the primary Region to the DR Region.","C":"Use a Volume Gateway in AWS Storage Gateway for the application storage. Configure Cross-Region Replication (CRR) of the Volume Gateway from the primary Region to the DR Region.","A":"Use Amazon S3 for the application storage. Create an S3 bucket in the primary Region and an S3 bucket in the DR Region. Configure S3 Cross-Region Replication (CRR) from the primary Region to the DR Region."},"isMC":true,"answer_images":[],"answer_ET":"D","answers_community":["D (100%)"]},{"id":"bIsGrj6oC1lIyG9wHiGT","answer_ET":"BD","question_text":"A company's application uses a fleet of Amazon EC2 On-Demand Instances to analyze and process data. The EC2 instances are in an Auto Scaling group. The Auto Scaling group is a target group for an Application Load Balancer (ALB). The application analyzes critical data that cannot tolerate interruption. The application also analyzes noncritical data that can withstand interruption.\n\nThe critical data analysis requires quick scalability in response to real-time application demand. The noncritical data analysis involves memory consumption. A DevOps engineer must implement a solution that reduces scale-out latency for the critical data. The solution also must process the noncritical data.\n\nWhich combination of steps will meet these requirements? (Choose two.)","isMC":true,"discussion":[{"upvote_count":"5","comment_id":"1248020","timestamp":"1721008800.0","poster":"trungtd","content":"Selected Answer: BD\nAWS Auto Scaling does not provide a predefined memory utilization metric type"},{"poster":"jamesf","timestamp":"1722415260.0","content":"Selected Answer: BD\nOption B (For critical data): Creates a warm pool and ensures quick scaling with On-Demand Instances, addressing the need for low latency in scaling.\nOption D (For noncritical data): Uses Spot Instances with memory-based scaling policies to handle noncritical data efficiently.","upvote_count":"4","comment_id":"1258731","comments":[{"upvote_count":"1","timestamp":"1722839100.0","comment_id":"1260951","poster":"jamesf","content":"For D, Spot Instance, Using Cloudwatch with Custom Memory Utilization Metric\nhttps://aws.amazon.com/blogs/mt/create-amazon-ec2-auto-scaling-policy-memory-utilization-metric-linux/\n\nNot E as Auto Scaling does not provide predefined memory utilization."}]},{"upvote_count":"4","comment_id":"1251184","content":"Selected Answer: BD\n---> B D","timestamp":"1721391540.0","poster":"tgv"},{"comment_id":"1245738","content":"Selected Answer: BE\nOn-Demand Instances: For critical data that cannot tolerate interruption, On-Demand Instances are reliable and provide the required stability without the risk of termination\n\nSpot Instances: Utilising Spot Instances for noncritical data processing can significantly reduce costs since these workloads can tolerate interruptions.\n\nThis combination ensures that the critical data analysis benefits from reduced scale-out latency and reliability, while noncritical data processing leverages cost-effective Spot Instances and is scaled based on memory usage.","upvote_count":"1","poster":"TEC1","timestamp":"1720643820.0"}],"choices":{"E":"For the noncritical data, create a second Auto Scaling group. Choose the predefined memory utilization metric type for the target tracking scaling policy. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.","A":"For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use Spot Instances.","D":"For the noncritical data, create a second Auto Scaling group that uses a launch template. Configure the launch template to install the unified Amazon CloudWatch agent and to configure the CloudWatch agent with a custom memory utilization metric. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.","C":"For the critical data, modify the existing Auto Scaling group. Create a lifecycle hook to ensure that bootstrap scripts are completed successfully. Ensure that the application on the instances is ready to accept traffic before the instances are registered. Create a new version of the launch template that has detailed monitoring enabled.","B":"For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use On-Demand Instances."},"timestamp":"2024-07-06 10:41:00","url":"https://www.examtopics.com/discussions/amazon/view/143403-exam-aws-certified-devops-engineer-professional-dop-c02/","answer":"BD","question_images":[],"question_id":187,"answers_community":["BD (93%)","7%"],"unix_timestamp":1720255260,"exam_id":23,"answer_images":[],"topic":"1","answer_description":""},{"id":"HVaoUGSdsObB6FoftMQc","answer_description":"","exam_id":23,"question_text":"A company recently migrated its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that uses Amazon EC2 instances. The company configured the application to automatically scale based on CPU utilization.\n\nThe application produces memory errors when it experiences heavy loads. The application also does not scale out enough to handle the increased load. The company needs to collect and analyze memory metrics for the application over time.\n\nWhich combination of steps will meet these requirements? (Choose three.)","topic":"1","answer_images":[],"answer":"ACE","question_id":188,"unix_timestamp":1720255080,"answers_community":["ACE (84%)","Other"],"choices":{"A":"Attach the CloudWatchAgentServerPolicy managed IAM policy to the IAM instance profile that the cluster uses.","D":"Collect performance logs by deploying the AWS Distro for OpenTelemetry collector as a DaemonSet.","B":"Attach the CloudWatchAgentServerPolicy managed IAM policy to a service account role for the cluster.","E":"Analyze the pod_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the Service dimension.","C":"Collect performance metrics by deploying the unified Amazon CloudWatch agent to the existing EC2 instances in the cluster. Add the agent to the AMI for any new EC2 instances that are added to the cluster.","F":"Analyze the node_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the ClusterName dimension."},"timestamp":"2024-07-06 10:38:00","isMC":true,"answer_ET":"ACE","url":"https://www.examtopics.com/discussions/amazon/view/143402-exam-aws-certified-devops-engineer-professional-dop-c02/","question_images":[],"discussion":[{"upvote_count":"9","content":"Selected Answer: ACE\nA. This policy grants the necessary permissions for the Amazon CloudWatch agent to collect and publish metrics from the EC2 instances.\nC. The unified Amazon CloudWatch agent can collect both CPU and memory utilization metrics. Deploying it ensures you capture memory metrics across all EC2 instances in the EKS cluster.\nE. pod_memory_utilization metric provides detailed insights into memory usage at the pod level\n\nB. service account role is more relevant for applications running within Kubernetes pods needing AWS permissions.\nD irrelevant\nF Node-level metrics do not provide the granularity needed to diagnose pod-level memory issues effectively","poster":"trungtd","comment_id":"1248023","timestamp":"1721009520.0"},{"comment_id":"1269600","timestamp":"1724171880.0","poster":"[Removed]","content":"Selected Answer: ACE\nvote for ACE","upvote_count":"3"},{"content":"Selected Answer: ACE\nAfter check, feel Option A better\n- provides necessary permissions at the EC2 instance level, which is where the CloudWatch agent runs.\n- is directly suitable for metrics collection because it ensures that EC2 instances can send metrics to CloudWatch. The CloudWatch agent on the EC2 instances needs the IAM policy to push metrics and logs to CloudWatch.\n\nHope someone can explain further if choose option B instead of A.","poster":"jamesf","upvote_count":"4","timestamp":"1722416460.0","comments":[{"timestamp":"1722995220.0","poster":"jamesf","upvote_count":"1","comment_id":"1261903","content":"it seen BCE better as option B \n- attached the policy to EKS service account role for better Granular Control, Scalability and Management\n- this policy targets permissions at the Kubernetes level, granting specific pods or services within the cluster the ability to collect and send metrics."}],"comment_id":"1258740"},{"upvote_count":"1","comment_id":"1250051","timestamp":"1721265480.0","poster":"noisonnoiton","content":"Selected Answer: BCE\nB - control permission with service account\nC - cloudwatch agent on k8s worker nodes\nE - monitoring with k8s service (pods)"},{"content":"Selected Answer: BCE\nI will go with B C E","comments":[{"poster":"TEC1","comment_id":"1249198","timestamp":"1721159580.0","upvote_count":"1","content":"B- Necessary permissions\nC- Cloud watch agent installed\nE - understanding performance and scaling of the application within Kubernetes Enviro"}],"upvote_count":"1","timestamp":"1721159400.0","comment_id":"1249194","poster":"TEC1"},{"comment_id":"1247042","timestamp":"1720826040.0","upvote_count":"1","content":"Selected Answer: CEF\nAnswer : C E F","poster":"komorebi"}]},{"id":"T735072GraFIkM7RHZqx","topic":"1","choices":{"C":"Enable Amazon CloudWatch Container Insights. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.","A":"Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.","B":"Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon Detective in the company's AWS account. Enable EKS audit logs from optional source packages in Detective.","D":"Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon CloudWatch Container Insights and VPC Flow Logs. Enable AWS CloudTrail logs."},"answer_images":[],"answer_ET":"B","timestamp":"2024-07-06 10:15:00","isMC":true,"answer_description":"","exam_id":23,"question_text":"A company's video streaming platform usage has increased from 10,000 users each day to 50,000 users each day in multiple countries. The company deploys the streaming platform on Amazon Elastic Kubernetes Service (Amazon EKS). The EKS workload scales up to thousands of nodes during peak viewing time.\n\nThe company's users report occurrences of unauthorized logins. Users also report sudden interruptions and logouts from the platform.\n\nThe company wants additional security measures for the entire platform. The company also needs a summarized view of the resource behaviors and interactions across the company's entire AWS environment. The summarized view must show login attempts, API calls, and network traffic. The solution must permit network traffic analysis while minimizing the overhead of managing logs. The solution must also quickly investigate any potential malicious behavior that is associated with the EKS workload.\n\nWhich solution will meet these requirements?","question_id":189,"answers_community":["B (100%)"],"unix_timestamp":1720253700,"discussion":[{"comment_id":"1258742","upvote_count":"2","timestamp":"1722416700.0","content":"Selected Answer: B\nAmazon Detective helps you quickly analyze and investigate security events across one or more AWS accounts by generating data visualizations that represent the ways your resources behave and interact over time. Detective creates visualizations of GuardDuty findings.\nhttps://docs.aws.amazon.com/guardduty/latest/ug/detective-integration.html\n\nAmazon EKS audit logs is an optional data source package that can be added to your Detective behavior graph.\nhttps://docs.aws.amazon.com/detective/latest/userguide/source-data-types-EKS.html","poster":"jamesf"},{"upvote_count":"2","content":"Selected Answer: B\nB- Guardduty any potential malicious behavior and Amazon Detective summarised view must show login attempts, API calls, and network traffic","poster":"TEC1","comment_id":"1249202","timestamp":"1721160120.0"},{"poster":"trungtd","timestamp":"1721010840.0","comment_id":"1248030","upvote_count":"3","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/guardduty/latest/ug/detective-integration.html\nhttps://docs.aws.amazon.com/detective/latest/userguide/source-data-types-EKS.html"},{"timestamp":"1720753140.0","upvote_count":"2","comment_id":"1246435","content":"Selected Answer: B\nvote B","poster":"siheom"},{"timestamp":"1720303440.0","upvote_count":"2","poster":"getadroit","comment_id":"1243575","content":"D\nhttps://aws.amazon.com/blogs/security/how-to-use-new-amazon-guardduty-eks-protection-findings/"}],"answer":"B","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/143401-exam-aws-certified-devops-engineer-professional-dop-c02/"},{"id":"qbYfVr3VVmm9AArKZT83","answer_ET":"D","unix_timestamp":1680940260,"answer_images":[],"answer_description":"","choices":{"C":"Use AWS CodeArtifact to store the application code. Use AWS CodeDeploy to deploy the application to a fleet of Amazon EC2 instances. Use Elastic Load Balancing to distribute the traffic to the EC2 instances. When making changes to the application, upload a new version to CodeArtifact and create a new CodeDeploy deployment.","D":"Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.","B":"Use Amazon Lightsail to deploy the application. Store the application in a zipped format in an Amazon S3 bucket. Use this zipped version to deploy new versions of the application to Lightsail. Use Lightsail deployment options to manage the deployment.","A":"Deploy the application on an Amazon EC2 instance, and create an AMI of the instance. Use the AMI to create an automatic scaling launch configuration that is used in an Auto Scaling group. Use Elastic Load Balancing to distribute traffic. When changes are made to the application, a new AMI will be created, which will initiate an EC2 instance refresh."},"isMC":true,"exam_id":23,"topic":"1","question_id":190,"discussion":[{"timestamp":"1718935680.0","comment_id":"1102172","content":"Selected Answer: D\nD is undoubtedly is most correct one . But they shoud mention that , we are going to deploy application in different environment . Since deployming to the same envirnmont just overide the previous deployment . In order to meed the requirement of Blue / Green deployment we need two separate enviormnent . Then we have two separate version running simulateously and we can do DNS swapping to quicky shilf traffic .","upvote_count":"5","poster":"z_inderjot"},{"timestamp":"1724959620.0","upvote_count":"3","comment_id":"1163031","poster":"zijo","content":"AWS Elastic Beanstalk deploy action can be used to deploy the application artifact from the S3 bucket to the green environment, which is the AWS cloud environment here."},{"timestamp":"1724031840.0","upvote_count":"2","poster":"dzn","content":"Selected Answer: D\nLightsail does not have built-in Blue/Green deployment capabilities like Elastic Beanstalk.","comment_id":"1153694"},{"timestamp":"1705013820.0","poster":"Kiroo","upvote_count":"3","content":"Selected Answer: D\nI was about to discard D because I was unsure if beanstalk supported GO (yes it does )\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html\n\nSo D is undoubtedly the best option to quickly move to cloud and to do blue green with an testing","comment_id":"949308"},{"content":"Selected Answer: D\nI guess D is easiest option to orchestrate blue/green deployments and A/B testing in this case.","poster":"madperro","upvote_count":"2","comment_id":"919898","timestamp":"1702202220.0"},{"timestamp":"1701353280.0","poster":"rdoty","comment_id":"911217","content":"Selected Answer: D\nD elastic beanstalk due to deployment options","upvote_count":"1"},{"timestamp":"1700068800.0","content":"Maybe D, but it looks like an old approach. we need to use codebuild and codepipelines and Elastic beanstalk, but elastic beanstalk could be changed by AWS cloudformation.","poster":"mgonblan","upvote_count":"1","comment_id":"898436"},{"content":"Selected Answer: D\nD. Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.\n\nAWS Elastic Beanstalk provides a platform for deploying web applications, which is well-suited for use cases that require blue/green deployments and A/B testing. Elastic Beanstalk can deploy applications written in a variety of programming languages and frameworks, including Go. Elastic Beanstalk supports blue/green deployments, which allow you to deploy a new version of your application to a separate environment before switching traffic to it. This enables you to perform A/B testing before fully rolling out a new version of your application. Elastic Beanstalk also allows you to manage the deployment options, including the deployment strategy, instance types, and autoscaling options.","comment_id":"886935","upvote_count":"4","poster":"haazybanj","timestamp":"1698894480.0"},{"timestamp":"1697317980.0","content":"D it is","comment_id":"870479","upvote_count":"1","poster":"alce2020"},{"poster":"ele","content":"Selected Answer: D\nElastic Beanstalk","comment_id":"864531","timestamp":"1696751460.0","upvote_count":"1"}],"timestamp":"2023-04-08 09:51:00","question_images":[],"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/105572-exam-aws-certified-devops-engineer-professional-dop-c02/","answer":"D","question_text":"A company has an on-premises application that is written in Go. A DevOps engineer must move the application to AWS. The company's development team wants to enable blue/green deployments and perform A/B testing.\nWhich solution will meet these requirements?"}],"exam":{"lastUpdated":"11 Apr 2025","id":23,"provider":"Amazon","isBeta":false,"isImplemented":true,"numberOfQuestions":355,"isMCOnly":true,"name":"AWS Certified DevOps Engineer - Professional DOP-C02"},"currentPage":38},"__N_SSP":true}