{"pageProps":{"questions":[{"id":"je5eSxVKxALYKtA4hCny","answers_community":["ADE (100%)"],"answer_images":[],"discussion":[{"timestamp":"1731811440.0","upvote_count":"1","content":"Selected Answer: ADE\nBy combining CloudFormation for infrastructure deployment, latency-based routing in Route 53, and creating a global DynamoDB table, the solutions architect can achieve the goals of disaster recovery, future growth, and improved access time while minimizing administrative overhead and ensuring data consistency across both Regions.\n\nThe other options are either not necessary or do not fully meet the requirements:\nB. Using the AWS Management Console to document and create the infrastructure manually would be time-consuming and prone to human error, increasing administrative overhead.\nC. Using weighted routing in Route 53 would distribute traffic based on predefined weights, which may not provide the best user experience or account for future growth or changes in traffic patterns.\nF. Creating a new DynamoDB table and copying data as a one-time operation would not provide continuous replication and could lead to data inconsistencies or data loss in case of a Regional outage.","poster":"0b43291","comment_id":"1313355"},{"poster":"AzureDP900","timestamp":"1731180120.0","comment_id":"1309179","content":"ADE is best option for given scenario.","upvote_count":"1"},{"poster":"NoinNothing","timestamp":"1720321260.0","comment_id":"1243663","upvote_count":"2","content":"Selected Answer: ADE\nA, D , E is correct"},{"poster":"awsaz","upvote_count":"3","timestamp":"1719602400.0","content":"Selected Answer: ADE\nA and D And E","comment_id":"1238866"},{"timestamp":"1719520200.0","upvote_count":"1","poster":"mifune","comment_id":"1238387","content":"Selected Answer: ADE\nA-D-E makes more sense to me here"}],"question_text":"A company is using a single AWS Region for its ecommerce website. The website includes a web application that runs on several Amazon EC2 instances behind an Application Load Balancer (ALB). The website also includes an Amazon DynamoDB table. A custom domain name in Amazon Route 53 is linked to the ALB. The company created an SSL/TLS certificate in AWS Certificate Manager (ACM) and attached the certificate to the ALB. The company is not using a content delivery network as part of its design.\n\nThe company wants to replicate its entire application stack in a second Region to provide disaster recovery, plan for future growth, and provide improved access time to users. A solutions architect needs to implement a solution that achieves these goals and minimizes administrative overhead.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)","url":"https://www.examtopics.com/discussions/amazon/view/143003-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":456,"exam_id":33,"timestamp":"2024-06-27 22:30:00","question_images":[],"choices":{"F":"Create a new DynamoDB table. Enable DynamoDB Streams for the new table. Add the second Region to create a global table. Copy the data from the existing DynamoDB table to the new table as a one-time operation.","A":"Create an AWS CloudFormation template for the current infrastructure design. Use parameters for important system values, including Region. Use the CloudFormation template to create the new infrastructure in the second Region.","E":"Update the configuration of the existing DynamoDB table by enabling DynamoDB Streams. Add the second Region to create a global table.","B":"Use the AWS Management Console to document the existing infrastructure design in the first Region and to create the new infrastructure in the second Region.","D":"Update the Route 53 hosted zone record for the application to use latency-based routing. Send traffic to the ALB in each Region.","C":"Update the Route 53 hosted zone record for the application to use weighted routing. Send 50% of the traffic to the ALB in each Region."},"answer_description":"","unix_timestamp":1719520200,"isMC":true,"answer":"ADE","answer_ET":"ADE","topic":"1"},{"id":"o7k9VtsiUfdFbORvAUfe","answer_ET":"B","answer_description":"","discussion":[{"poster":"masetromain","comment_id":"775242","timestamp":"1673691780.0","comments":[{"content":"Not B. \"must be encrypted by keys that the company’s security team manages\". This implies the company does not wanna use AWS KMS.","upvote_count":"5","comment_id":"998083","poster":"hamimelon","timestamp":"1693792140.0","comments":[{"upvote_count":"2","timestamp":"1707925020.0","comment_id":"1150311","poster":"hogtrough","content":"This is why they would use Customer-managed keys in AWS KMS. It is absolutely B"},{"comment_id":"1111761","poster":"jpa8300","upvote_count":"2","timestamp":"1704189420.0","content":"Hamimmelon, the Company's security Team can manage the AWS KMS service, so B is the right answer. All the others are not valid."}]},{"comments":[{"comment_id":"1111763","content":"And adding to this in option D they specify uses default AES-256, but KMS also uses the same, so this option just don't make sense.","timestamp":"1704189480.0","upvote_count":"1","poster":"jpa8300"}],"poster":"masetromain","timestamp":"1673691780.0","comment_id":"775243","upvote_count":"8","content":"Option A is not correct because it uses SSE-S3 with a customer-managed key, but it does not specify how the security team will manage the encryption keys. Additionally, it only denies unencrypted PutObject requests but does not specify how the objects will be encrypted.\n\nOption C is not correct because it does not specify how the security team will manage the encryption keys and it does not specify how the objects will be encrypted.\n\nOption D is not correct because it uses AES-256 with a customer-managed key, but it does not specify how the security team will manage the encryption keys. Additionally, it simply denies unencrypted PutObject requests, but it doesn't specify how the objects will be encrypted."},{"comment_id":"791856","upvote_count":"10","timestamp":"1675011480.0","poster":"Musk","content":"What about the requirement of customer managed keys?"},{"timestamp":"1677421320.0","comments":[{"timestamp":"1678526520.0","poster":"cherep87","upvote_count":"1","comments":[{"comments":[{"timestamp":"1707925260.0","poster":"hogtrough","upvote_count":"1","content":"No, the task was to replace SSE-SE keys which have no relation to AWS KMS. \n\n\"Amazon S3 automatically enables server-side encryption with Amazon S3 managed keys (SSE-S3) for new object uploads.\n\nUnless you specify otherwise, buckets use SSE-S3 by default to encrypt objects. However, you can choose to configure buckets to use server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) instead. \"\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html","comment_id":"1150315"}],"upvote_count":"4","content":"Task is to replace any AWS Managed keys to ones \"that the company’s security team manages\" \nSo they tell us to find a solution that does not use AWS Managed Keys.","poster":"hobokabobo","timestamp":"1680434220.0","comment_id":"858778"}],"content":"Use the AWS CLI to re-upload all objects in the S3 bucket. - \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html\nChanges to note before enabling default encryption\nAfter you enable default encryption for a bucket, the following encryption behavior applies:\n\nThere is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.\n\nWhen you upload objects after enabling default encryption:\n\nIf your PUT request headers don't include encryption information, Amazon S3 uses the bucket’s default encryption settings to encrypt the objects.","comment_id":"835789"}],"comment_id":"822512","upvote_count":"4","poster":"hobokabobo","content":"Completely ignores the task to solve: \"all current and future objects in the S3 bucket must be encrypted by keys that the company’s security team manages. \""}],"upvote_count":"42","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\n\nSo the correct answer is B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket."},{"poster":"Untamables","upvote_count":"21","timestamp":"1674215880.0","content":"Selected Answer: D\nI think D is correct.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html","comment_id":"782187","comments":[{"upvote_count":"1","content":"The issue with D is that it doesn't make it clear where the encryption is happening like all the other options do. Is it server-side (we assume that it is, but it is not what is written)? Or is it client-side?","comment_id":"1160844","timestamp":"1709053200.0","poster":"djeong95"}]},{"timestamp":"1725094680.0","poster":"amministrazione","upvote_count":"1","comment_id":"1275501","content":"B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket."},{"comment_id":"1260566","poster":"Jason666888","timestamp":"1722760140.0","content":"Selected Answer: B\nAWS KMS (Key Management Service) allows for customer-managed keys (CMKs), which can indeed be considered as \"keys that the company’s security team manages\"","upvote_count":"1"},{"timestamp":"1718625120.0","upvote_count":"1","content":"Selected Answer: B\nIn s3 option there is no option to select AES256 custom key.","poster":"Helpnosense","comment_id":"1231921"},{"poster":"higashikumi","upvote_count":"1","comment_id":"1217029","content":"Selected Answer: B\nTo meet the requirement for encrypting all current and future objects in an Amazon S3 bucket with keys managed by the company’s security team, change the S3 bucket’s default encryption to server-side encryption with AWS KMS managed keys (SSE-KMS). Implement an S3 bucket policy to deny unencrypted PutObject requests, ensuring all new uploads are encrypted with the specified KMS key. Then, use the AWS CLI to re-upload all existing objects to the S3 bucket, enforcing the new encryption policy on current data. This approach ensures compliance by applying KMS encryption to both new and existing objects without causing disruptions ￼ ￼ ￼.","timestamp":"1716499020.0"},{"upvote_count":"1","timestamp":"1716302340.0","content":"Selected Answer: B\nThe solutions need to use SSE-KMS so that the security team can manage the keys, but they also need to ensure that current and future objects are encrypted using customer-managed keys.","comment_id":"1215033","poster":"Malcnorth59"},{"comment_id":"1205530","timestamp":"1714653600.0","content":"Selected Answer: D\nNot Option D: \" Amazon S3 server-side encryption uses 256-bit Advanced Encryption Standard Galois/Counter Mode (AES-GCM) to encrypt all uploaded objects.\" AES- 256 is already the default, so you can't change it. \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html","upvote_count":"1","poster":"TonytheTiger"},{"content":"Selected Answer: B\nCorrect Approach: This option is accurate and meets all the specified requirements. By changing the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS), the company can use customer managed keys (CMKs) for encryption. This allows the security team to manage the keys, addressing the core requirement.\nSetting an S3 bucket policy to deny unencrypted PutObject requests ensures future compliance with the encryption policy.\nRe-uploading all objects using the AWS CLI ensures that existing objects are encrypted under the new policy, making sure that both current and future objects are encrypted with the keys managed by the company's security team.","poster":"mav3r1ck","timestamp":"1711150140.0","upvote_count":"2","comment_id":"1180407"},{"content":"Selected Answer: B\nB, option D confuses encryption options. AES-256 is part of the SSE-S3 encryption method and doesn't directly involve customer-managed keys","poster":"gofavad926","comment_id":"1175618","timestamp":"1710656880.0","upvote_count":"1"},{"comments":[{"comment_id":"1145753","content":"Option D is incorrect because it refers to “AES-256 with a customer managed key” in a way that mixes concepts. AES-256 is the encryption standard used by SSE-S3 and does not directly apply to the use of customer managed keys. For managing keys, the correct approach is through SSE-KMS, which allows specifying a customer managed AWS KMS key.","poster":"8608f25","upvote_count":"1","timestamp":"1707508680.0"}],"timestamp":"1707508680.0","content":"Selected Answer: B\nThe solution that meets the requirements for encrypting all current and future objects in the Amazon S3 bucket with keys that the company’s security team manages, while ensuring server-side encryption, is:\nOption B is correct because it directly addresses the new requirement by changing the default encryption method to SSE-KMS, which allows the use of AWS Key Management Service (KMS) keys managed by the company’s security team. This option ensures that all future uploads are encrypted with the specified KMS key. It also includes re-uploading existing objects to ensure they are encrypted under the new scheme. Setting an S3 bucket policy to deny unencrypted PutObject requests enforces the encryption requirement for all new uploads.","comment_id":"1145752","poster":"8608f25","upvote_count":"1"},{"comment_id":"1112588","poster":"ninomfr64","upvote_count":"2","timestamp":"1704268260.0","content":"Selected Answer: B\nNot A. SSE-S3 with a customer managed key is not an actual option as SSE-S3 uses S3 managed keys\nNot C. S3 bucket policy cannot automatically encrypt objects on GetObject and PutObject requests. With policies you can only allow/deny actions from specific principals\nNot D. AES-256 with a customer managed key is not an actual option as AES-256 is used as value for the header x-amz-server-side-encryption to set SSE-S3 on putObject and SSE-S3 uses S3 managed keys\n\nB is correct as server-side encryption with AWS KMS managed encryption keys (SSE-KMS) is an actual default encryption settings for S3 bucket and you can use S3 bucket policy to deny unencrypted PutObject. These ensure all new objects will be encrypted with customer managed keys. Then using aws cli to re-upload all object will overwrite existing objects (versioning is not enabled)"},{"upvote_count":"1","timestamp":"1702999980.0","content":"Selected Answer: D\ni think D is correct as B is mentioned KMS managed key..","poster":"ismeagain","comment_id":"1100759"},{"comment_id":"1100691","timestamp":"1702995600.0","upvote_count":"1","content":"Selected Answer: B\nA - You cannot define your own key\nB - Correct. Using SSE-KMS and your own KMS customer managed key, you adhere to the requirements\nC - Does not encrypt existing objects, and you cannot \"change\" the request to \"automatically\" encrypt\nD - You can only choose between SSE-S3 and SSE-KMS (or now DSSE-KMS as well) for default encryption. Underlying the SSE-S3 refers to AES-256 (cfr. \"s3:x-amz-server-side-encryption\": \"AES256\") but you cannot specify your customer managed key in that case.","poster":"Impromptu"},{"comment_id":"1093978","poster":"_Juwon","timestamp":"1702343640.0","upvote_count":"1","content":"Selected Answer: B\nIf use KMS-CMK , wouldn't it be possible to manage keys directly while using KMS? Does anyone have an opinion on this?"},{"timestamp":"1701270420.0","content":"Selected Answer: B\nB is correct\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html#aws-managed-customer-managed-keys\n\nWhen you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed key, or you can specify a customer managed key that you have already created.","comment_id":"1083595","poster":"eurriola10","upvote_count":"2"},{"content":"Selected Answer: D\nBetween B and D, it's D which is correct because of cust managed clause.","upvote_count":"1","poster":"jainparag1","timestamp":"1700832060.0","comment_id":"1079281"},{"comment_id":"1074929","comments":[{"timestamp":"1700831940.0","poster":"jainparag1","upvote_count":"2","comment_id":"1079280","content":"It's wrong since the key is managed by AWS but they want key to be manged by Security Team. Hence the correct answer is D."}],"content":"Selected Answer: B\nB is the best choice\n\nSSE-KMS allows the use of customer-managed keys. Setting an S3 bucket policy to deny unencrypted PutObject requests ensures that all future uploads are encrypted. Re-uploading all objects in the S3 bucket using the AWS CLI would encrypt existing objects with the new KMS key. This option meets all the requirements.","poster":"heatblur","timestamp":"1700428080.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1699861800.0","comment_id":"1069137","poster":"severlight","content":"Selected Answer: B\nsee dpatra's comment"},{"comment_id":"1054873","poster":"KCjoe","upvote_count":"2","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html\n\nSSE-C support AES-256. The question just say \"custom managed\", it didn't say KMS custom managed, of course it is SSE-C - ustomer-provided keys.","timestamp":"1698348900.0"},{"upvote_count":"3","poster":"dpatra","comment_id":"1040003","content":"Selected Answer: B\nThis aligns with the requirement to use keys managed by the company’s security team. With SSE-KMS, you can create and manage encryption keys, fulfilling the requirement.Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.","timestamp":"1696985040.0"},{"upvote_count":"1","content":"D. customer managed key not aws managed key","poster":"rlf","comment_id":"1028287","timestamp":"1696801740.0"},{"content":"Selected Answer: D\nFor who is chosing B\nKMS AWS managed key cant be managed by customer except AWS\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt","upvote_count":"4","timestamp":"1696127280.0","comment_id":"1021975","poster":"longns"},{"comment_id":"1021642","poster":"covabix879","upvote_count":"1","timestamp":"1696089180.0","content":"Selected Answer: B\nCustomer key should be managed through AWS KMS either as External (Import Key material) or External key store. Other origins are KMS - recommended or AWS CloudHSM key store"},{"poster":"Six_Fingered_Jose","upvote_count":"1","comment_id":"1017907","content":"Selected Answer: D\nanswer is D\nB states [AWS KMS managed encryption keys] , meaning they are not using the customer managed key with KMS, thus does not fulfill the requirement \"must be encrypted by keys that the company’s security team manages\"","timestamp":"1695741240.0"},{"upvote_count":"2","timestamp":"1694475960.0","poster":"awsent","comment_id":"1005248","content":"Answer: B \nCustomer could use KMS for managing the keys, they don't need \"Key material\" from the customer."},{"poster":"FunkyFresco","content":"Selected Answer: B\nOption B is the right answer. https://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/","upvote_count":"1","timestamp":"1694178600.0","comment_id":"1002497"},{"content":"Selected Answer: B\nA. SSE-S3 uses S3-managed keys which is not what we are looking for\nC. Why should one set an S3 bucket policy to automatically encrypt objects on GetObject?\nD. Although AES-256 is a cryptographic algorithm used by S3, it is not a mode of S3.\n\nB. SSE-KMS supports server-side encryption both with AWS-managed and customer-managed keys, and we do indeed ned to re-upload all objects in order to get them encrypted","comment_id":"995044","timestamp":"1693480140.0","upvote_count":"3","poster":"aviathor"},{"upvote_count":"2","timestamp":"1693202100.0","comment_id":"991829","content":"Selected Answer: D\nthe kay words is \"by keys that the company’s security team manages\", so should be A or D, and I select D cause I think should apply policy then re-upload the files.","poster":"Simon523"},{"upvote_count":"2","poster":"xflare","timestamp":"1692740160.0","content":"Selected Answer: B\nIt's B:\nIf you go to the docs, https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html, it clearly says \"The only thing that you need to do is manage the encryption keys that you provide. \" which seems to point to D.\nHOWEVER, D says to select AES256 as default encryption, but such option does not exist.\nJust go create a bucket and try to do it yourselves.\nTherefore the answer must be B.","comment_id":"987799"},{"comment_id":"984817","content":"Selected Answer: D\nI believe that it is D. the keys needs to be customer managed","poster":"CloudHandsOn","upvote_count":"1","timestamp":"1692398880.0"},{"upvote_count":"1","timestamp":"1691952780.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html","poster":"Piccaso","comment_id":"980213"},{"comment_id":"976998","content":"Selected Answer: B\n%100 B. I'm pretty sure. Beleive me","poster":"xplusfb","upvote_count":"1","timestamp":"1691611800.0"},{"content":"Selected Answer: B\nAnswer: B\nFunny that this question is VERY similar to one in a practice question you can buy at Udemy. Still, it's the best and only option that meet their requirements.\n\nI don't know why one should choose D. If you take a look at the S3 properties console, or the documentation: https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html, you won't find this option.","timestamp":"1691245800.0","upvote_count":"1","comment_id":"973093","poster":"chico2023"},{"poster":"Zox42","timestamp":"1691102040.0","upvote_count":"1","content":"Selected Answer: D\nAnswer D","comment_id":"971495"},{"timestamp":"1690714020.0","poster":"blehbleh","upvote_count":"1","content":"Selected Answer: D\nI think its D per AWS \"AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the cryptographic keys\" The question is stating that the customer manages the keys. AWS KMS is a managed service so I think that would eliminate anything that offers AWS KMS to be a viable option.","comment_id":"967035"},{"poster":"MRL110","content":"Selected Answer: D\nThe answer is spread all over this page:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html","upvote_count":"1","comment_id":"964591","timestamp":"1690453560.0"},{"timestamp":"1690437420.0","poster":"Asamara","upvote_count":"1","comment_id":"964403","content":"B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.\n\nOptions A, C, and D are incorrect:\n\nOption A suggests changing the default encryption to SSE-S3 with a customer managed key, which does not meet the requirement for the company's security team to manage the encryption keys.\n\nOption C mentions setting an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests, which is not necessary and does not align with the requirement to use customer managed keys for encryption.\n\nOption D proposes using AES-256 with a customer managed key, which does not fulfill the requirement for the security team to manage the encryption keys through AWS KMS."},{"content":"Selected Answer: D\nI do not like neither B or C because they talk about KMS MANAGED keys (instead of KMS customer-managed) whereas the question says that the customer wants to manage its own keys. Therefore, the only viable option is D","comment_id":"958144","upvote_count":"1","timestamp":"1689922500.0","poster":"aviathor"},{"poster":"Jonalb","upvote_count":"2","timestamp":"1688381640.0","comment_id":"941752","content":"why not D guys?"},{"poster":"NikkyDicky","comment_id":"940977","upvote_count":"1","timestamp":"1688309160.0","content":"Selected Answer: B\nB seems to fit"},{"comment_id":"928503","timestamp":"1687270800.0","content":"Selected Answer: B\nThe entire question is confusing. \"In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key\" Either SSE-KMS and SSE-S3 are AES-256 so it should be SSE-KMS, which is answer B:\nhttps://docs.aws.amazon.com/kms/latest/cryptographic-details/crypto-primitives.html","poster":"Maria2023","upvote_count":"1"},{"poster":"chiaseed","content":"Selected Answer: B\nI vote for Option B. S3 supports SSE-S3, SSE-KMS, and SSE-C according to https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html\n---- quoted from the link\nAmazon S3 provides these server-side encryption options:\n\nServer-side encryption with Amazon S3 managed keys (SSE-S3)\n\nServer-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)\n\nServer-side encryption with customer-provided keys (SSE-C)\n----\nGiven this, I think AES-256 with customer-managed encryption is not a valid option for S3.","comment_id":"928031","upvote_count":"2","timestamp":"1687218480.0"},{"timestamp":"1687125600.0","poster":"Jackhemo","upvote_count":"1","comment_id":"926925","content":"Selected Answer: B\nFrom olabiba.ai:\n\nThe solution that will meet the requirements is option B: In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.\n\nBy changing the default encryption to SSE-KMS, all current and future objects in the S3 bucket will be encrypted using keys managed by the company's security team. Setting an S3 bucket policy to deny unencrypted PutObject requests ensures that only encrypted objects can be uploaded to the bucket. Re-uploading all objects in the S3 bucket using the AWS CLI will ensure that all objects are encrypted with the new keys.\n\nThis solution provides the required encryption with minimal operational overhead."},{"upvote_count":"1","timestamp":"1687093620.0","poster":"ailves","comment_id":"926707","content":"Selected Answer: A\nAccording to https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html - \"Server-side encryption with customer-provided keys (SSE-C) is not supported for default encryption\", but we need a solution with the company’s security team manages, so only A has ability to encrypt with Customer-Provided Keys. I voted to A."},{"poster":"Asds","upvote_count":"1","comment_id":"921610","comments":[{"content":"This is why B is thé only option!","timestamp":"1686591300.0","comment_id":"921611","upvote_count":"1","poster":"Asds"}],"content":"Selected Answer: B\nD is correct...only when creating a bucket, not turning sse-s3 into sse-kms with cmk\n\nVerified","timestamp":"1686591240.0"},{"poster":"emiliocb4","comment_id":"905881","upvote_count":"2","timestamp":"1684931280.0","content":"Selected Answer: D\nkey managed by the team!!!"},{"upvote_count":"3","content":"This the worst worded question I have met in SAP.\nFor B SSE-KMS , it can be divided two categories, one is SSE-KMS aws managed key and the other is SSE-CMK which represent Customer Managed Key.\nFor D it mentions customer managed key but I think the it means customer provided key, that's the SSE-C means. SSE-C means customer provided key not SSE-CMK which means customer managed key.\nSo I think the people which designs this question don't understand well the SSE-CMK and SSE-C. If he states well in D that SSE-C, then we can clearly choose B.\nfor SSE-CMK and SSE-C differences. SSE-CMK let customer manage primary key, for each object in S3, S3 will generate new DEK( Data encryption key) and use this DEK to encrypt the object. Every object in S3 has different DEKs.\nWhile SSE-C, for each object customer should upload a key to encrypt object. You can use the same key for all objects in S3 or you can have different keys for different objects. But you have to save all the keys you supply to S3, because next time when you download objects from S3, you have to deliver it to S3 otherwise S3 can't decrypt.","timestamp":"1684178220.0","poster":"Jesuisleon","comment_id":"898651"},{"timestamp":"1684038960.0","poster":"intp75","upvote_count":"2","content":"Selected Answer: B\nIt is B.\n“Server-side encryption with customer-provided keys (SSE-C) is not supported for default encryption.”","comment_id":"897246"},{"upvote_count":"1","content":"Selected Answer: D\nhttps://repost.aws/knowledge-center/s3-object-encryption-keys","timestamp":"1683787800.0","poster":"daniel5cloud","comment_id":"894648"},{"timestamp":"1683656520.0","poster":"F_Eldin","upvote_count":"1","content":"Selected Answer: B\nThe 2 options in S3 encryption are:\n1- Amazon S3 managed keys (SSE-S3)\n2- AWS Key Management Service key (SSE-KMS)","comment_id":"893344"},{"poster":"scuzzy2010","comment_id":"885050","timestamp":"1682845800.0","upvote_count":"3","content":"Selected Answer: D\nAnswer is D. There's no option to select AES-256 For existing buckets, the only options are Amazon S3 managed keys (SSE-S3) and AWS Key Management Service key (SSE-KMS). SSE-KMS should be selected and a customer managed key chosen."},{"poster":"Sarutobi","comment_id":"874040","content":"I can see the question is confusing. I have never tried this, and I am not sure if it works for S3, I have used it for other services. There is an option in KMS to select an external key store. This is a widespread setup with HashiCorp Vault. Does this classify as \"by keys that the company’s security team manages\"? If it does, we can use B. If it not, I dont know what is good, according to B, we should go to the bucket properties and change the default encryption to AES-256, and give it a try; that is not an option. Available options for me are SSE-S3 or SSE-KMS.","timestamp":"1681845780.0","upvote_count":"1","comments":[{"timestamp":"1681845840.0","upvote_count":"1","poster":"Sarutobi","content":"I meant to say ** according to D** not ** according to B**.","comment_id":"874042"}]},{"comment_id":"871544","content":"Selected Answer: B\nOption D not the best:\n\nAssumes the use of \"AES-256 with a customer managed key,\" which is not a valid server-side encryption option in Amazon S3.\nSuggests the use of a key generated outside of AWS and strictly managed by the company's security team, which may be more restrictive than necessary.\nThe recommendation of using SSE-C for HIPAA compliance does not apply here, as the option does not explicitly mention SSE-C.\nThis option may not meet the requirements as written, due to the ambiguity and invalid encryption choice.","poster":"OCHT","timestamp":"1681629480.0","upvote_count":"2"},{"comment_id":"864553","content":"Selected Answer: D\n1. Usually for HIPPA for personally identifiable data, the recommendation is to use SSE-C.\n2. To deny uploading of unencrypted objects, it is the same for SSE-C and SSE-KMS. You use 's3:x-amz-server-side-encryption'. Refer to https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ for more details.\n\nIf one consider SSE-KMS as a customer managed key, then B is the answer (it is dumb but look at https://imgur.com/a/p2w0ifO). If customer managed key must be a key that is strictly generated outside of AWS and managed not within AWS, then D is the answer.\n\nFor the reason of company security team managing it and not AWS KMS, I choose D.","timestamp":"1680943680.0","poster":"OnePunchExam","upvote_count":"3"},{"comment_id":"863513","poster":"Jacky_exam","timestamp":"1680846120.0","content":"Selected Answer: B\nOption B suggests changing the default encryption to SSE-KMS, which allows for the use of customer managed keys. By setting an S3 bucket policy to deny unencrypted PutObject requests, any new objects added to the bucket will automatically be encrypted with the customer managed key. Re-uploading all objects using the AWS CLI is still necessary to ensure that all existing objects are encrypted with the customer managed key.","upvote_count":"3"},{"upvote_count":"2","content":"B is correct.\nSSE-KMS is managed by customer.\nD is not correct by key will be up by team","poster":"hankun","comments":[{"comment_id":"860848","poster":"hobokabobo","content":"\"key will be up by team\" <- which is exactly what the solution architect was asked to achieve.","upvote_count":"2","timestamp":"1680600540.0"}],"comment_id":"857865","timestamp":"1680350100.0"},{"timestamp":"1680269880.0","upvote_count":"3","poster":"ThaiNT","content":"Selected Answer: B\n\"In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. \" Is wrong, because: \"Server-side encryption with customer-provided keys (SSE-C) is not supported for default encryption.\"","comment_id":"857123"},{"upvote_count":"2","content":"Selected Answer: B\nI choose B as KMS can have customer managed keys as well. Plus as per my understanding of AWS exam question nature, they wants to promote their services which is KMS in this case.","timestamp":"1679969820.0","poster":"Cloud_noob","comment_id":"852668"},{"poster":"GioGio","upvote_count":"2","content":"Selected Answer: B\nI think it is B.\nIn the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.","timestamp":"1679302680.0","comment_id":"844669"},{"comment_id":"842929","upvote_count":"2","content":"Selected Answer: B\nI think B, because according D there is no such option in th s3 encryption propetry. It only works setting bucket policy","timestamp":"1679156100.0","poster":"Dimidrol"},{"upvote_count":"2","content":"Selected Answer: B\nSelected Answer: B","poster":"taer","comment_id":"842036","timestamp":"1679061840.0"},{"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html\n\nClearly says we need following header for SSE-C\nx-amz-server-side-encryption-customer-algorithm \nUse this header to specify the encryption algorithm. The header value must be AES256.","poster":"kiran15789","timestamp":"1678478820.0","comment_id":"835384","upvote_count":"1"},{"poster":"limjieson","content":"B is correct","comment_id":"833987","upvote_count":"1","timestamp":"1678367760.0"},{"comment_id":"832898","timestamp":"1678275360.0","upvote_count":"1","poster":"etechsystem_ts","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html"},{"timestamp":"1677959640.0","content":"For those who are voting for D, can you please walk me through the steps I need to take to replicate this?\n\nAs far as I know, AES-256 is for SSE-S3, which is the furthest thing from a customer managed key possible, so the answer is not just not suitable, but non-existent in the first place.\n\nWith SSE-KMS, you select the key you want to use by Key ARN. This can either be a key managed by AWS, a key imported by you, or a key from CloudHSM. \n\nI'd suggest you try it out in the S3 ui","comments":[{"content":"Your answer is in your last phrase. You cannot do it on GUI.\nSSE-C is AES-256, and you don't have to specify a properties on bucket to use it, when you upload an object via API, you need to specify HTTP Header : \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html\nSSE-KMS is not a key managed by customer, yes he can import his key, but then, the key will be managed by KMS","comments":[{"comment_id":"847890","content":"We can also upload S3 objects with SSE-C via AWS CLI : https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html\nTake a look at --sse-c* option","poster":"Arnaud92","timestamp":"1679554380.0","upvote_count":"1"}],"upvote_count":"1","timestamp":"1679553540.0","comment_id":"847876","poster":"Arnaud92"},{"comment_id":"829343","upvote_count":"1","timestamp":"1677959700.0","poster":"anita_student","content":"B is correct"},{"content":"read it , you will get your answer. https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html\nPerfect answer is D","timestamp":"1680719160.0","upvote_count":"1","comment_id":"862383","poster":"violet99"}],"upvote_count":"1","comment_id":"829341","poster":"anita_student"},{"timestamp":"1677782700.0","comment_id":"827253","upvote_count":"1","content":"Probably D as B says \" AWS KMS managed encryption keys (SSE-KMS)\" which kind of eludes to AWS managed but not customer manager even though SSE-KMS can do both; aws managed & customer managed","poster":"Pete697989"},{"comment_id":"822533","content":"Selected Answer: D\nA) SSE-S3 is not customer managed. \nB) SSE-KMS is not customer managed.\nC) SSE-KMS is not customer managed.\nD) Here we finally have a customer managed key to encrypt the objects. Plus a policy that enforces it.\n(The security team manages the key: we are not the security team so we are not supposed to do any managing of the key. We should not bypass the security team by do some management on our own. )","timestamp":"1677422340.0","poster":"hobokabobo","comments":[{"comment_id":"823591","content":"How would you reply to this below when you say \"SSE-KMS is not customer managed\"???\nWhen you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed key, or you can specify a customer managed key that you have already created. \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html","poster":"lkyixoayffasdrlaqd","timestamp":"1677496740.0","upvote_count":"3","comments":[{"comment_id":"824172","poster":"Sarutobi","content":"I agree with you; customer managed through KMS.","upvote_count":"2","timestamp":"1677533460.0"},{"poster":"lkyixoayffasdrlaqd","upvote_count":"1","comment_id":"825222","timestamp":"1677613980.0","content":"people don't know well, they just make comments here. correct answer is B"}]}],"upvote_count":"1"},{"content":"Tricky question. First thing, choose one with CLI because In console, only SSE-S3 and SSE-KMS are available. (Amz is promoting them I guess :-) Thats the reason CLI wording came into picture in the options. So either B or D. D clearly says AES-256 encryption that needs to be used as header value for key \"x-amz-server-side-encryption-customer-algorithm\" Check this out - https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html D is answer for sure.","poster":"God_Is_Love","comments":[{"poster":"God_Is_Love","content":"this is another request header \"x-amz-server-side-encryption-customer-key\" that needs to be used with SSE-C","comment_id":"822124","upvote_count":"1","timestamp":"1677395100.0"}],"timestamp":"1677394860.0","upvote_count":"2","comment_id":"822121"},{"upvote_count":"4","poster":"scuzzy2010","content":"Selected Answer: D\nB and C are incorrect as KMS is AWS managed. Question says it must be customer managed.","timestamp":"1677196140.0","comment_id":"819908"},{"timestamp":"1676913720.0","content":"Selected Answer: B\nD is obviously wrong. API command PutBucketEncryption has 2 types of 'SSEAlgorithm' AES256 is SSE-S3 and aws:kms = KMS. AES256 option = cannot specify a key policy for it, you can can aws:kms. Note mention of AWS256 implies API/CLI based configuration.\nB is the right option.","poster":"lunt","upvote_count":"1","comment_id":"815593"},{"comments":[{"comment_id":"819913","upvote_count":"1","poster":"c73bf38","timestamp":"1677196620.0","content":"It's a trick question\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html"}],"comment_id":"810048","content":"Selected Answer: D\ncust managed key","upvote_count":"1","timestamp":"1676500260.0","poster":"spd"},{"poster":"CloudFloater","comment_id":"806440","upvote_count":"1","content":"Selected Answer: D\nD is correct.\n(as oatif provided)\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html \n\"When you upload an object, Amazon S3 uses the encryption key that you provide to apply AES-256 encryption to your data.\"\nNot B because KMS keys are AWS managed not Customer.","timestamp":"1676212980.0"},{"comment_id":"806273","upvote_count":"1","content":"Selected Answer: B\nAt the end I vote for B, becasue KMS allows to:\nYou can create and manage your AWS KMS keys:\n\nCreate, edit, and view symmetric and asymmetric KMS keys, including HMAC keys.\n\nControl access to your KMS keys by using key policies, IAM policies, and grants. AWS KMS supports attribute-based access control (ABAC). You can also refine policies by using condition keys.\n\nCreate, delete, list, and update aliases, friendly names for your KMS keys. You can also use aliases to control access to your KMS keys.\n\nTag your KMS keys for identification, automation, and cost tracking. You can also use tags to control access to your KMS keys.\n\nEnable and disable KMS keys.\n\nEnable and disable automatic rotation of the cryptographic material in a KMS key.\n\nDelete KMS keys to complete the key lifecycle.","poster":"Musk","timestamp":"1676202060.0"},{"content":"Selected Answer: D\nD is the correct answer - https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html","upvote_count":"1","comment_id":"803835","timestamp":"1675989300.0","poster":"oatif","comments":[{"poster":"c73bf38","comment_id":"819914","upvote_count":"1","content":"KMS you have control of the keys per the docs:\n\nncryption key type\nWhen you configure default encryption for an Amazon S3 bucket, you can use either server-side encryption with Amazon S3 managed keys (SSE-S3) (the default) or server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS).\n\nWith the default option (SSE-S3), Amazon S3 uses one of the strongest block ciphers—256-bit Advanced Encryption Standard (AES-256) to encrypt each object uploaded to the bucket. With SSE-KMS, you have more control over your key. If you use SSE-KMS, you can choose an AWS KMS customer managed key or use the default AWS managed key (aws/s3).","timestamp":"1677196800.0"}]},{"timestamp":"1675911300.0","poster":"AC1984","upvote_count":"2","comment_id":"802756","content":"Selected Answer: D\nCustomer manages keys: SSE-C\nWhen you upload an object, Amazon S3 uses the encryption key that you provide to apply AES-256 encryption to your data. Amazon S3 then removes the encryption key from memory. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key that you provided matches, and then it decrypts the object before returning the object data to you."},{"comment_id":"801691","poster":"tinyflame","content":"Selected Answer: B\nDefault encryption type for S3 is SSE-S3 or SSE-KMS\nCustomer-managed keys can be used with SSE-KMS","upvote_count":"2","timestamp":"1675835280.0"},{"content":"B makes sense as KMS is better way to manage keys.","poster":"pravi1","upvote_count":"2","comment_id":"791399","timestamp":"1674967920.0"}],"choices":{"D":"In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket.","B":"In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.","A":"In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests.","C":"In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests."},"answers_community":["B (62%)","D (38%)","1%"],"topic":"1","question_images":[],"answer_images":[],"question_id":457,"unix_timestamp":1673691780,"answer":"B","question_text":"A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company’s security team manages. The S3 bucket does not have versioning enabled.\n\nWhich solution will meet these requirements?","exam_id":33,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/95171-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-01-14 11:23:00"},{"id":"bjOLt92SX2DlHDi8TtxC","answer_ET":"AC","question_id":458,"question_text":"A company wants to create a single Amazon S3 bucket for its data scientists to store work-related documents. The company uses AWS IAM Identity Center to authenticate all users. A group for the data scientists was created.\n\nThe company wants to give the data scientists access to only their own work. The company also wants to create monthly reports that show which documents each user accessed.\n\nWhich combination of steps will meet these requirements? (Choose two.)","answer_images":[],"timestamp":"2024-06-27 22:33:00","choices":{"D":"Configure AWS CloudTrail to log S3 management events to CloudWatch. Use Amazon Athena’s CloudWatch connector to query the logs and generate reports.","C":"Configure AWS CloudTrail to log S3 data events and deliver the logs to an S3 bucket. Use Amazon Athena to run queries on the CloudTrail logs in Amazon S3 and generate reports.","B":"Create an IAM Identity Center role for the data scientists group that has Amazon S3 read access and write access. Add an S3 bucket policy that allows access to the IAM Identity Center role.","A":"Create a custom IAM Identity Center permission set to grant the data scientists access to an S3 bucket prefix that matches their username tag. Use a policy to limit access to paths with the ${aws:PrincipalTag/userName}/* condition.","E":"Enable S3 access logging to EMR File System (EMRFS). Use Amazon S3 Select to query logs and generate reports."},"isMC":true,"topic":"1","answers_community":["AC (100%)"],"answer_description":"","answer":"AC","url":"https://www.examtopics.com/discussions/amazon/view/143004-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"question_images":[],"discussion":[{"poster":"0b43291","content":"Selected Answer: AC\nBy combining a custom IAM Identity Center permission set with path-based access control and CloudTrail logging with Athena querying, the company can achieve the desired access control and reporting requirements for the data scientists' work-related documents stored in the S3 bucket.\n\nThe other options are either incorrect or do not fully meet the requirements:\nB. Creating an IAM Identity Center role with S3 read and write access and adding an S3 bucket policy would not provide the granular access control required to restrict each user to their own work.\nD. Configuring CloudTrail to log S3 management events to CloudWatch and using Athena's CloudWatch connector would not capture the necessary data events for generating reports on which documents each user accessed.\nE. Enabling S3 access logging to EMRFS and using S3 Select would not provide the necessary logging and reporting capabilities for this use case.","timestamp":"1731811800.0","comment_id":"1313358","upvote_count":"2"},{"comment_id":"1238869","poster":"awsaz","content":"Selected Answer: AC\nA and C","timestamp":"1719602580.0","upvote_count":"4"},{"poster":"mifune","upvote_count":"1","content":"Selected Answer: AC\nIAM Identity Center permission + Amazon Athena to run queries on the CloudTrail logs in Amazon S3 and generate reports, answer A-C","timestamp":"1719520380.0","comment_id":"1238388"}],"unix_timestamp":1719520380},{"id":"maFrVxj1qCyAKJc7kJH8","choices":{"C":"Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an AWS Lambda function that starts the appropriate Fargate task. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the Lambda function when objects are created.","D":"Create AWS Lambda container images for the processing. Configure Lambda functions to use the container images. Extract the container selection logic to run as a decision Lambda function that invokes the appropriate Lambda processing function. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the decision Lambda function when objects are created.","B":"Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Update and containerize the container selection logic to run as a Fargate service that starts the appropriate Fargate task. Configure an EFS event notification to invoke the Fargate service when files are added to the EFS file system.","A":"Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an Amazon EventBridge rule that starts the appropriate Fargate task. Configure the EventBridge rule to run when files are added to the EFS file system."},"answer":"C","exam_id":33,"timestamp":"2024-06-29 08:28:00","isMC":true,"topic":"1","question_text":"A company hosts a data-processing application on Amazon EC2 instances. The application polls an Amazon Elastic File System (Amazon EFS) file system for newly uploaded files. When a new file is detected, the application extracts data from the file and runs logic to select a Docker container image to process the file. The application starts the appropriate container image and passes the file location as a parameter.\n\nThe data processing that the container performs can take up to 2 hours. When the processing is complete, the code that runs inside the container writes the file back to Amazon EFS and exits.\n\nThe company needs to refactor the application to eliminate the EC2 instances that are running the containers.\n\nWhich solution will meet these requirements?","question_id":459,"answer_ET":"C","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/143048-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1719642480,"answer_images":[],"question_images":[],"discussion":[{"poster":"0b43291","timestamp":"1731812400.0","upvote_count":"2","content":"Selected Answer: C\nBy combining Amazon ECS with Fargate tasks, AWS Lambda for the container selection logic, and Amazon S3 for event-driven processing, Option C provides a serverless and scalable solution that meets the requirements while minimizing the need for EC2 instances and leveraging the strengths of each AWS service.\n\nWhile this solution requires migrating the file storage from Amazon EFS to Amazon S3, it addresses the requirement of eliminating EC2 instances by leveraging AWS Fargate for the processing tasks and AWS Lambda for the container selection logic. Additionally, it utilizes the event notification capabilities of Amazon S3 to trigger the Lambda function when new files are uploaded.\n\nThe other options are either not feasible due to the limitations of Amazon EFS (Options A and B) or introduce additional constraints by using AWS Lambda for the long-running data processing tasks (Option D).","comment_id":"1313362"},{"content":"Selected Answer: C\nEFS event notification to invoke the Fargate","upvote_count":"1","poster":"mkgiz","timestamp":"1727505120.0","comment_id":"1290543"},{"comment_id":"1247856","content":"Selected Answer: C\nEFS event notification to invoke the Fargate --> EFS don't have event notification like s3","timestamp":"1720975200.0","upvote_count":"3","poster":"mark_232323"},{"poster":"vip2","content":"Selected Answer: C\nC\nEventBridge can not monitor EFS event directly, not A","comment_id":"1243192","timestamp":"1720242900.0","upvote_count":"3"},{"comment_id":"1239065","content":"C\nEFS cannot notify events and Lambda cannot do container execution for 2 hours.","poster":"kupo777","upvote_count":"3","timestamp":"1719642480.0"}],"answer_description":""},{"id":"ElYNapKI2KuGdDgqkzZG","exam_id":33,"timestamp":"2024-06-29 08:31:00","answers_community":["A (66%)","B (34%)"],"question_text":"A media company has a 30-T8 repository of digital news videos. These videos are stored on tape in an on-premises tape library and referenced by a Media Asset Management (MAM) system. The company wants to enrich the metadata for these videos in an automated fashion and put them into a searchable catalog by using a MAM feature. The company must be able to search based on information in the video, such as objects, scenery items, or people’s faces. A catalog is available that contains faces of people who have appeared in the videos that include an image of each person. The company would like to migrate these videos to AWS.\n\nThe company has a high-speed AWS Direct Connect connection with AWS and would like to move the MAM solution video content directly from its current file system.\n\nHow can these requirements be met by using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system?","question_id":460,"url":"https://www.examtopics.com/discussions/amazon/view/143049-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Set up an Amazon EC2 instance that runs the OpenCV libraries. Copy the videos, images, and face catalog from the on-premises library into an Amazon EBS volume mounted on this EC2 instance. Process the videos to retrieve the required metadata, and push the metadata into the MAM solution, while also copying the video files to an Amazon S3 bucket.","B":"Set up an AWS Storage Gateway, tape gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the tape gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video in the tape gateway, retrieve the required metadata, and push the metadata into the MAM solution.","C":"Configure a video ingestion stream by using Amazon Kinesis Video Streams. Use the catalog of faces to build a collection in Amazon Rekognition. Stream the videos from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed videos. Then, use a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Configure the stream to store the videos in Amazon S3.","A":"Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution."},"topic":"1","question_images":[],"answer_description":"","answer_images":[],"answer_ET":"A","discussion":[{"upvote_count":"5","comment_id":"1242178","timestamp":"1720109700.0","poster":"Hizumi","content":"Selected Answer: B\nAnswer is B - Use Tape Gateway, then Lambda and Rekognition can be used to process and index the data for the MAM system. \n\n\nhttps://aws.amazon.com/storagegateway/vtl/\nhttps://aws.amazon.com/blogs/aws/file-interface-to-aws-storage-gateway/"},{"timestamp":"1719744000.0","content":"Selected Answer: B\nB - Tape Gateway","comment_id":"1239606","poster":"mifune","upvote_count":"5"},{"upvote_count":"1","poster":"bdloko","content":"Selected Answer: A\nHow do people come-up with such scenarios?","timestamp":"1742844120.0","comment_id":"1409773"},{"comment_id":"1388196","timestamp":"1741844400.0","upvote_count":"1","poster":"albert_kuo","content":"Selected Answer: A\nBecause of the company wants to enrich the metadata for these videos in an automated fashion and put them into a searchable catalog by using a MAM feature, so exclude B"},{"poster":"SIJUTHOMASP","comment_id":"1332643","timestamp":"1735336800.0","content":"Selected Answer: A\nSince the need is \"move the MAM solution video content directly from its current file system.\", they want the files from tape to another storage which can be S3. Hence the solution would be A.","upvote_count":"1"},{"poster":"0b43291","comment_id":"1313370","content":"Selected Answer: A\nOption A leverages fully managed AWS services (AWS Storage Gateway, Amazon Rekognition, AWS Lambda, and Amazon S3) while minimizing disruption to the existing MAM solution and its workflow for extracting videos from the tape library. This approach strikes a balance between leveraging AWS services for metadata enrichment, face recognition, and durable video storage while causing minimal disruption to the existing system and minimizing ongoing management overhead.\n\nextract videos from onprem tapes - push through file gateway to S3","upvote_count":"1","timestamp":"1731813300.0"},{"upvote_count":"1","timestamp":"1731809460.0","content":"A \nUsing an on-premises file gateway appliance with AWS Storage Gateway has several advantages over using a tape gateway:\nLess administrative burden: Managing the file gateway appliance is still handled by AWS, reducing your administrative tasks.\nEasier to scale: With a file gateway, you can scale more easily to handle increasing amounts of video content.\nMore flexible storage options: You can store videos in Amazon S3, which provides more flexibility and scalability compared to tape storage.","poster":"AzureDP900","comment_id":"1313346"},{"comment_id":"1306258","upvote_count":"1","timestamp":"1730567340.0","content":"You can use Tape Gateway to archive data you need to preserve at another offsite location for disaster recovery or regulatory compliance needs. Tape Gateway enables you to replace magnetic tape libraries and physical tapes with AWS Cloud storage for long-term storage retention needs.","poster":"AloraCloud"},{"comment_id":"1302717","content":"I believe A should be the write answer of using file gateway, since there is processing of data required before storing. and Tape Gateway cannot be directly used, it involves complexity","upvote_count":"2","poster":"Danm86","timestamp":"1729825080.0"},{"content":"Selected Answer: A\nAWS Rekognition is capable of processing both images and videos from the following sources: Amazon S3, directly passed images, Kinesis Video Streams. Although option C mentions Kinesis Video Streams, it would be an overkill for a scenario where videos are not being streamed in real time, and it introduces unnecessary complexity. So the solution using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system is A.","poster":"JoeTromundo","comment_id":"1296972","timestamp":"1728832080.0","upvote_count":"2"},{"upvote_count":"2","poster":"mkgiz","timestamp":"1727505540.0","comment_id":"1290545","content":"Selected Answer: A\nUse the MAM solution to extract the videos from the current archive"},{"content":"Selected Answer: A\nIt is not possible to directly invoke Amazon Rekognition to process videos that are stored on an AWS Storage Gateway (whether it's a file gateway or tape gateway). Amazon Rekognition processes videos that are stored in Amazon S3.","poster":"felon124","timestamp":"1724379240.0","comment_id":"1271012","upvote_count":"3"},{"content":"Selected Answer: A\nYes the videos were stored on tape on premise, but the solution requires active processing later on, this can't be done on Tape GW; won't make sense.","timestamp":"1724225460.0","comment_id":"1269934","upvote_count":"2","poster":"asquared16","comments":[{"timestamp":"1724226240.0","comment_id":"1269944","upvote_count":"2","poster":"asquared16","content":"Okay, it's B. You cannot directly migrate videos from a tape library to an AWS Storage Gateway file gateway appliance."}]},{"comment_id":"1261764","upvote_count":"1","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/aws/file-interface-to-aws-storage-gateway/","timestamp":"1722961440.0","poster":"c22ddd8"},{"comment_id":"1253497","content":"Selected Answer: A\nUse the MAM solution to extract the videos from the current archive and push them into","poster":"[Removed]","upvote_count":"3","timestamp":"1721721780.0"},{"comment_id":"1252474","poster":"toma","timestamp":"1721568900.0","upvote_count":"2","content":"Answer is B: \"These videos are stored on tape in an on-premises tape library....\""},{"content":"Selected Answer: A\nape Gateway is designed for offline data transfer, not ideal for actively accessed videos.","poster":"Russs99","timestamp":"1720276380.0","upvote_count":"2","comment_id":"1243414"},{"comments":[{"poster":"wbedair","comment_id":"1239903","content":"so you mean B ???","timestamp":"1719789480.0","upvote_count":"1","comments":[{"upvote_count":"2","comment_id":"1241046","poster":"kupo777","content":"A - Amazon Rekognition and Tape Gateway cannot communicate directly; S3 is required.","timestamp":"1719954660.0","comments":[{"poster":"Hizumi","content":"Storage Gateway stores the tape library in S3.","timestamp":"1720109760.0","upvote_count":"2","comment_id":"1242179"}]}]}],"poster":"kupo777","timestamp":"1719642660.0","content":"A\nA tape gateway appliance is required.\nAWS Storage Gateway also requires S3 or other storage.","comment_id":"1239067","upvote_count":"2"}],"answer":"A","isMC":true,"unix_timestamp":1719642660}],"exam":{"numberOfQuestions":529,"provider":"Amazon","name":"AWS Certified Solutions Architect - Professional SAP-C02","id":33,"lastUpdated":"11 Apr 2025","isBeta":false,"isMCOnly":true,"isImplemented":true},"currentPage":92},"__N_SSP":true}