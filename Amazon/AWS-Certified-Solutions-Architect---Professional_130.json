{"pageProps":{"questions":[{"id":"TT42QDlXVZWMKlPHblrl","exam_id":32,"answer":"CE","question_text":"A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The company's applications and databases are running in Account B.\nA solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53.\nDuring deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53.\nWhich combination of steps should the solutions architect take to resolve this issue? (Choose two.)","question_images":[],"answer_ET":"CE","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/36113-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1604529960,"answers_community":["CE (100%)"],"question_id":646,"timestamp":"2020-11-04 23:46:00","topic":"1","answer_images":[],"choices":{"D":"Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts.","B":"Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.","A":"Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance's private IP in the private hosted zone.","C":"Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.","E":"Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A."},"discussion":[{"content":"C & E \nhttps://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/","poster":"ali98","comment_id":"213070","timestamp":"1632167820.0","comments":[{"comment_id":"233772","upvote_count":"4","timestamp":"1633218180.0","content":"Perfect!","poster":"cloudgc"},{"timestamp":"1636022100.0","content":"C,E\nAuthorize --> Associate --> Remove Authorization\nAs in the ali98's link.","comment_id":"439402","upvote_count":"4","poster":"student22"}],"upvote_count":"38"},{"comment_id":"940185","poster":"SkyZeroZx","upvote_count":"1","timestamp":"1688229660.0","content":"Selected Answer: CE\nC,E\nAuthorize --> Associate --> Remove Authorization"},{"content":"Selected Answer: CE\nAns: C & E","poster":"RVD","timestamp":"1647480780.0","upvote_count":"2","comment_id":"569415"},{"comment_id":"492057","timestamp":"1638405780.0","upvote_count":"1","content":"C, E is correct","poster":"AzureDP900"},{"comment_id":"445663","poster":"andylogan","timestamp":"1636053000.0","content":"It's C E \nA Authorize B --> B Associate A --> A Remove Authorization","upvote_count":"3"},{"comment_id":"436397","timestamp":"1635895500.0","poster":"tgv","content":"CCC EEE\n---","upvote_count":"1"},{"comment_id":"433854","timestamp":"1635731760.0","content":"C & E for me.","poster":"blackgamer","upvote_count":"1"},{"poster":"WhyIronMan","comment_id":"413408","content":"I'll go with C,E","upvote_count":"1","timestamp":"1634702100.0"},{"comment_id":"382750","poster":"ibrahimsow","content":"For sure, the correct answers are C & E. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html","timestamp":"1634532840.0","upvote_count":"3"},{"comment_id":"356956","upvote_count":"3","poster":"Waiweng","content":"it's C,E","timestamp":"1634478240.0"},{"content":"going for CE","timestamp":"1634407680.0","upvote_count":"3","poster":"Kian1","comment_id":"294180"},{"poster":"Ebi","timestamp":"1634343720.0","comment_id":"284534","content":"CE for sure","upvote_count":"4"},{"upvote_count":"3","timestamp":"1634169600.0","content":"C&E is the right answer","poster":"Bulti","comment_id":"263574"},{"comment_id":"245831","content":"CE for sure","poster":"rscloud","upvote_count":"2","timestamp":"1633806240.0"},{"comment_id":"241239","upvote_count":"2","timestamp":"1633647540.0","content":"For sure CE","poster":"T14102020"},{"poster":"jackdryan","timestamp":"1633208760.0","content":"I'll go with C,E\nThanks to ali98 for providing the right on link","upvote_count":"3","comment_id":"233270"}],"isMC":true},{"id":"XVJzWWBMKfQL8ePpIjgb","url":"https://www.examtopics.com/discussions/amazon/view/36333-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"topic":"1","answer_images":[],"answer_ET":"D","answer_description":"","answer":"D","question_text":"A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours.\nWhat is the MOST cost-effective migration recommendation?","discussion":[{"content":"As the length of processing the files take 1 hour, Lambda seems to be out of question, then we are left with EC2 option, D seems to be correct as we are auto-scaling EC2","upvote_count":"34","poster":"liono","timestamp":"1632110880.0","comment_id":"214505"},{"timestamp":"1634958480.0","upvote_count":"7","poster":"Ebi","comment_id":"284536","content":"Answer is D"},{"upvote_count":"1","content":"keyword here is sqs length + asg","timestamp":"1658883540.0","poster":"hilft","comment_id":"637711"},{"timestamp":"1653469320.0","comment_id":"607131","content":"Selected Answer: D\nRight by the book","upvote_count":"1","poster":"bobsmith2000"},{"comment_id":"568233","timestamp":"1647335280.0","upvote_count":"1","poster":"tyrk","content":"Selected Answer: D\nDDDDDDDDDDDD"},{"timestamp":"1644262800.0","content":"D looks right","comment_id":"542637","upvote_count":"1","poster":"jj22222"},{"upvote_count":"1","content":"https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/","timestamp":"1643110140.0","comment_id":"532069","poster":"kaush4u"},{"upvote_count":"2","content":"D is correct even though there is a typo (auto seating group) its obvious when you look at the other answers. The Lambda execution limit is 15 minutes which instantly rules out two. The remaining answer that suggests creating a new EC2 instance when something goes in the queue is not a good solution (slow).","poster":"lulz111","timestamp":"1642444320.0","comment_id":"525998"},{"timestamp":"1640867940.0","comment_id":"513356","poster":"cldy","upvote_count":"1","content":"D is correct."},{"content":"D. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Seating group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.","upvote_count":"1","comment_id":"499315","poster":"cldy","timestamp":"1639217700.0"},{"upvote_count":"1","content":"I would have chosen Lambda until i saw the processing time. Therefore i will also go with D","poster":"KiraguJohn","comment_id":"492317","timestamp":"1638436500.0"},{"comment_id":"492124","upvote_count":"1","timestamp":"1638417540.0","content":"D is correct, initially i thought A but it is not scalable.","poster":"AzureDP900"},{"content":"this is really a easy one, hope I can have it as complex one in my exam","upvote_count":"1","poster":"acloudguru","timestamp":"1638243780.0","comment_id":"490395"},{"content":"It's D","upvote_count":"1","poster":"andylogan","comment_id":"445671","timestamp":"1636285320.0"},{"upvote_count":"1","poster":"tgv","timestamp":"1636135500.0","comment_id":"436398","content":"DDD\n---"},{"content":"going for DDDDD\n\neliminate A & C for lambda (15 mins timeout)\neliminate B - solution is incomplete, it creates EC2 processes and shutdown, what triggers up new EC2??\n\nD - is correct","timestamp":"1635738900.0","upvote_count":"3","comment_id":"433286","poster":"Suresh108"},{"timestamp":"1635414240.0","upvote_count":"2","comment_id":"413411","poster":"WhyIronMan","content":"MOST cost-effective"},{"content":"it's D","poster":"Waiweng","upvote_count":"3","timestamp":"1635233940.0","comment_id":"356959"},{"timestamp":"1635125640.0","content":"going with D","upvote_count":"2","poster":"KnightVictor","comment_id":"334368"},{"timestamp":"1635045420.0","comment_id":"294183","poster":"Kian1","content":"going with D","upvote_count":"3"},{"upvote_count":"2","comment_id":"256658","content":"D is correct.","timestamp":"1634943840.0","poster":"Bulti"},{"upvote_count":"1","poster":"petebear55","content":"Is D the most cost effective ? Perhaps A ?","comment_id":"256347","timestamp":"1634906520.0"},{"comments":[{"timestamp":"1635239700.0","comment_id":"396034","upvote_count":"3","poster":"MrCarter","content":"A is more scalable actually but has a 15 mins timeout, whereas each video may take up to an hour"},{"timestamp":"1635122100.0","upvote_count":"3","poster":"certainly","content":"Each media file can take up to 1 hour to process. Lambda limit is 15 min","comment_id":"306023"}],"content":"Correct answer is D\nA also does the job but it is NOT scalable","timestamp":"1634782080.0","upvote_count":"2","comment_id":"253780","poster":"Ebi"},{"upvote_count":"1","poster":"rscloud","comment_id":"245835","timestamp":"1633520520.0","content":"D is correct"},{"upvote_count":"1","poster":"T14102020","content":"EFS and Lamda are incorrect. For sure correct answer is D.","timestamp":"1633310940.0","comment_id":"241241"},{"upvote_count":"3","content":"I'll go with D","poster":"jackdryan","comment_id":"233273","timestamp":"1633232760.0"},{"comment_id":"218394","poster":"AK2020","timestamp":"1633175520.0","content":"D is correct","upvote_count":"1"},{"upvote_count":"3","timestamp":"1632781320.0","poster":"smartassX","comment_id":"217599","content":"D : EC2 with autoscale based on the SQS queue length and S3 for storage!"},{"upvote_count":"2","timestamp":"1632646500.0","poster":"Ash1235","comment_id":"215016","content":"D is correct"}],"isMC":true,"answers_community":["D (100%)"],"unix_timestamp":1604740500,"timestamp":"2020-11-07 10:15:00","question_id":647,"question_images":[],"choices":{"D":"Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Seating group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.","B":"Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.","C":"Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.","A":"Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket."}},{"id":"cAERuflpOZb0vOwnFSiR","answer_images":[],"timestamp":"2020-11-07 10:39:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/36345-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"poster":"avland","upvote_count":"18","timestamp":"1632677220.0","comment_id":"226206","content":"A.\n\nB is nearly nonsensical with how it uses Batch.\nC wouldn't work since Step Functions operate with JSON inputs/outputs. SQS wouldn't work as an input quite like that.\nD wouldn't work because the metadata extraction Lambda functions are non-identical workers, so one function will pull a message containing the media item and process it, but then the other functions won't see that item and process their part of the metadata for it. For something like that to work, you'd actually want a separate SQS queue for each type of metadata function, and use an SNS topic to publish each item to each queue.","comments":[{"timestamp":"1632814200.0","poster":"alexmena1981","comments":[{"timestamp":"1634675220.0","poster":"MrCarter","comment_id":"396035","upvote_count":"1","content":"Ans AAAAAAAA"},{"comment_id":"236584","timestamp":"1632888360.0","poster":"gbrnq","upvote_count":"4","content":"That video shows how to trigger SQS from SF.. so it’s not relevant. Ans A"},{"upvote_count":"5","comment_id":"396036","poster":"MrCarter","timestamp":"1634683680.0","content":"No, you are incorrect, SQS cannot be the input of an AWS Step Functions workflow."}],"content":"You are incorrect,check https://www.youtube.com/watch?v=tPYa1r_cZ2E , Ans. C","upvote_count":"1","comment_id":"235281"},{"upvote_count":"1","content":"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-invoke-sfn.html\nPoints to A","comment_id":"699993","timestamp":"1666274040.0","poster":"dmscountera"},{"timestamp":"1684801320.0","comment_id":"904441","upvote_count":"1","content":"SQS can trigger aws step function, see https://serverlessland.com/patterns/eventbridge-pipes-sqs-to-step-functions-cdk-dotnet","poster":"Jesuisleon"}]},{"comment_id":"214527","upvote_count":"12","content":"C seems to be correct answer","comments":[{"upvote_count":"9","content":"SQS queue cannot trigger Step function","timestamp":"1632105600.0","comment_id":"215907","poster":"keos"}],"timestamp":"1632073380.0","poster":"liono"},{"content":"Think it's C. Step Functio Call Pack Pattern. \n https://aws.amazon.com/getting-started/hands-on/orchestrate-microservices-with-message-queues-on-step-functions/","upvote_count":"1","comments":[{"upvote_count":"1","content":"but in the questions says \"Configure the SQS queue as an input to the Step Functions workflow.\", in your example you have opposite situation","comment_id":"1143649","timestamp":"1707330240.0","poster":"marszalekm"}],"timestamp":"1683385680.0","comment_id":"890815","poster":"hwhap"},{"poster":"TonySu","comment_id":"721751","upvote_count":"1","timestamp":"1668828840.0","content":"I go with C"},{"poster":"[Removed]","timestamp":"1665215100.0","upvote_count":"1","comment_id":"689117","content":"Selected Answer: A\nWorkflow task can trigger other workflows: https://docs.aws.amazon.com/step-functions/latest/dg/concepts-nested-workflows.html"},{"timestamp":"1657087320.0","upvote_count":"3","content":"Selected Answer: A\nOnly A and C are valid but Answer is A since SQS cannot trigger SFW ... \nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-invoke-sfn.html\nOnly lambda and another SF can do it (as described in A)","comment_id":"627739","poster":"Enigmaaaaaa"},{"content":"Selected Answer: A\nC is wrong. SQS has to start a Lambda funtion first, which in turn will start Step Function.","upvote_count":"1","poster":"Dohecadi","comment_id":"562496","timestamp":"1646644440.0"},{"comment_id":"517907","timestamp":"1641434340.0","poster":"frankzeng","content":"C. when there are new items in the media catalog, a lambda function retrieve the list of media items and write the item information into SQS. The step function workflow read the item from SQS and run the LAMBDA functions in parallel.","upvote_count":"2"},{"upvote_count":"2","comments":[{"content":"this step function does broadcast to SQS ... Useless LINK","upvote_count":"1","timestamp":"1665682200.0","comment_id":"694156","poster":"wassb"}],"content":"Ans C\nhttps://www.youtube.com/watch?v=tPYa1r_cZ2E","poster":"wem","timestamp":"1638801420.0","comment_id":"495206"},{"upvote_count":"2","comment_id":"492129","timestamp":"1638418080.0","poster":"AzureDP900","content":"C \n\nThe best solution presented is to use a combination of AWS Step Functions and Amazon SQS. This results in each\nLambda function being able to run in parallel and use a queue for buffering the jobs.\nCORRECT: “Create an AWS Step Functions workflow to run the Lambda functions in parallel. Create a Lambda\nfunction to retrieve a list of files and write each item to an Amazon SQS queue. Configure the SQS queue as an input to the Step Functions workflow” is the correct answer"},{"content":"c is correct","comment_id":"492127","poster":"AzureDP900","upvote_count":"1","timestamp":"1638417900.0"},{"upvote_count":"1","comment_id":"488632","timestamp":"1638061740.0","poster":"tiana528","content":"D. It is simple, straightforward. Stepfunctions is not needed here. lambda triggered by SQS can run in parallel pretty well."},{"timestamp":"1637469180.0","poster":"acloudguru","upvote_count":"2","content":"Selected Answer: A\nSQS needs lambda to trigger step function, can not do it directly. So C is not right.","comment_id":"483008"},{"comment_id":"458733","timestamp":"1635887580.0","content":"Option-C. Taken from AWS Website. https://docs.aws.amazon.com/step-functions/latest/dg/sample-project-express-high-volume-sqs.html (We can process items from SQS).","upvote_count":"2","poster":"StelSen"},{"content":"It's A - SQS queue is not an input to the Step Functions workflow","upvote_count":"2","poster":"andylogan","comment_id":"445965","timestamp":"1635851460.0"},{"comment_id":"435250","timestamp":"1635375000.0","content":"AAA\n---","poster":"tgv","upvote_count":"2"},{"upvote_count":"1","content":"I will go with A.","comment_id":"433863","timestamp":"1635105780.0","poster":"blackgamer"},{"upvote_count":"3","poster":"mericov","timestamp":"1635028920.0","comments":[{"poster":"kirrim","upvote_count":"1","timestamp":"1636274880.0","comment_id":"463304","content":"This is absolutely the right answer. The entire question is about reducing the overall time to extract media by splitting it into multiple parallel processes, which is exactly what this blog post describes."}],"comment_id":"414561","content":"A - https://aws.amazon.com/blogs/compute/accelerating-workloads-using-parallelism-in-aws-step-functions/"},{"timestamp":"1635020820.0","content":"I'll go with A","comment_id":"413413","poster":"WhyIronMan","upvote_count":"2"},{"comment_id":"408802","poster":"Chibuzo1","timestamp":"1634983800.0","content":"The Answer is C, The best solution presented is to use a combination of AWS Step Functions and Amazon SQS. This results in each Lambda function being able to run in parallel and use a queue for buffering the jobs","upvote_count":"2"},{"comments":[{"content":"actually , after looking at it, i 'll go for A","timestamp":"1634663100.0","poster":"Waiweng","upvote_count":"6","comment_id":"357064"}],"upvote_count":"2","content":"it's C","comment_id":"356964","timestamp":"1634634060.0","poster":"Waiweng"},{"poster":"Amitv2706","comments":[{"upvote_count":"1","poster":"MrCarter","timestamp":"1634855400.0","content":"no no no","comment_id":"396037"},{"timestamp":"1634888040.0","content":"Since the max message size in an SQS queue is 256KB, how it is expected that the media items will fit in the Queue? Ans: A","upvote_count":"3","poster":"DashL","comment_id":"397177"}],"timestamp":"1634562360.0","content":"Answer C\n\nA - Incorrect. What will happen if lambda function fails while meta data extraction workflow. No mention of things like SQS or DB.\nB - Incorrect AWS Batch can't use Lambda as compute environment. It use containers (i.e. based on fargate or ec2)\nC - Correct . Parallel processing and reliable(SQS). BTW note this option is not about triggering the Step Function from SQS. Its about configuring SQS as an data input source in Step Functions.\nD- Incorrect. Not as efficient as C (where we can run lambdas in parallel to achieve the requirement of reducing extraction time).","upvote_count":"2","comment_id":"344655"},{"timestamp":"1634556780.0","comment_id":"334517","comments":[{"upvote_count":"1","timestamp":"1635035640.0","poster":"tekkart","comment_id":"424808","content":"The point in answer C is to have SQS as in input like in the example here : https://docs.aws.amazon.com/step-functions/latest/dg/sample-map-state.html\n\nWhich is Case #6 Dynamic Parallel Processing here : https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\n\nBut it is not very clear how this Map attribute is parallelizing, suggesting a Queue for parallel processing is very paradoxical, hence I would vote for A, but it is confusing that answers B, C and D mention queuing... would look too easy..."}],"poster":"sarah_t","content":"A\n\nSQS cannot trigger a step function: \nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-invoke-sfn.html","upvote_count":"4"},{"poster":"alisyech","comment_id":"321710","upvote_count":"1","content":"going with A","timestamp":"1634533560.0"},{"comment_id":"294193","upvote_count":"2","content":"going with A","poster":"Kian1","timestamp":"1634437080.0"},{"content":"A: https://aws.amazon.com/about-aws/whats-new/2019/05/aws_step_functions_enables_access_to_workflow_metadata/","upvote_count":"2","poster":"tipzzz","comment_id":"288731","timestamp":"1634119740.0"},{"content":"I go with A","upvote_count":"5","timestamp":"1634110560.0","comment_id":"284541","poster":"Ebi"},{"timestamp":"1633966860.0","comment_id":"279611","upvote_count":"1","content":"A. To execute a step function workflow another Lambda is needed to trigger it.","poster":"elf78"},{"comment_id":"271339","content":"I think A works, but why do we need the first Step Function, isn't a single Lambda Function enough?\nThe second Step Function just call the corresponding Lambda Function according the types of metadata.","poster":"01037","upvote_count":"1","timestamp":"1633827180.0"},{"comment_id":"267388","poster":"Justu","upvote_count":"1","timestamp":"1633801140.0","content":"A is the correct one."},{"timestamp":"1633631460.0","upvote_count":"3","content":"Correct Answer is A. You need to use a Lambda or a EventBridge to mediate between SQS and Step function workflow. So the best option is to use one Step function workflow to trigger another Step function workflow which is A.","poster":"Bulti","comment_id":"256659"},{"content":"I think A. SQS cannot use media item, SQS uses only messages.","comment_id":"241244","timestamp":"1633027080.0","poster":"T14102020","upvote_count":"2","comments":[{"timestamp":"1641684480.0","comment_id":"519823","poster":"balflearchen","content":"Even your answer is correct, but your point is wrong, read question carefully","upvote_count":"1"}]},{"upvote_count":"2","timestamp":"1632769440.0","poster":"jackdryan","content":"I'll go with A*","comment_id":"233281"},{"timestamp":"1632488760.0","poster":"liono","content":"https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/","upvote_count":"1","comment_id":"220434"},{"poster":"Raysquared","upvote_count":"1","content":"D sounds right","comment_id":"218839","timestamp":"1632480180.0"},{"timestamp":"1632211440.0","upvote_count":"2","comment_id":"215909","comments":[{"timestamp":"1632343620.0","poster":"Gmail78","content":"I don't believe so, it says triggered manually","comment_id":"218305","upvote_count":"1","comments":[{"content":"because you don't read questions well, as @keos mention, the solution architect split old Lambda function into serveral Lambda functions.\nThe question here is to ask what's the next step the SA should do","timestamp":"1641685140.0","comment_id":"519829","upvote_count":"2","poster":"balflearchen"}]},{"comment_id":"224302","timestamp":"1632540480.0","poster":"keos","content":"go for C after thinking from the question author's perspective.\n\"solutions architect has split the single metadata extraction Lambda function into a Lambda function for each type of metadata\". it expects the Step Function to coordinate.","upvote_count":"2"}],"poster":"keos","content":"would go for D"}],"unix_timestamp":1604741940,"question_text":"A company has a media catalog with metadata for each item in the catalog. Different types of metadata are extracted from the media items by an application running on AWS Lambda. Metadata is extracted according to a number of rules with the output stored in an Amazon ElastiCache for Redis cluster. The extraction process is done in batches and takes around 40 minutes to complete.\nThe update process is triggered manually whenever the metadata extraction rules change.\nThe company wants to reduce the amount of time it takes to extract metadata from its media catalog. To achieve this, a solutions architect has split the single metadata extraction Lambda function into a Lambda function for each type of metadata.\nWhich additional steps should the solutions architect take to meet the requirements?","answer":"A","answer_description":"","exam_id":32,"isMC":true,"answers_community":["A (100%)"],"choices":{"A":"Create an AWS Step Functions workflow to run the Lambda functions in parallel. Create another Step Functions workflow that retrieves a list of media items and executes a metadata extraction workflow for each one.","D":"Create a Lambda function to retrieve a list of media items and write each item to an Amazon SQS queue. Subscribe the metadata extraction Lambda functions to the SQS queue with a large batch size.","C":"Create an AWS Step Functions workflow to run the Lambda functions in parallel. Create a Lambda function to retrieve a list of media items and write each item to an Amazon SQS queue. Configure the SQS queue as an input to the Step Functions workflow.","B":"Create an AWS Batch compute environment for each Lambda function. Configure an AWS Batch job queue for the compute environment. Create a Lambda function to retrieve a list of media items and write each item to the job queue."},"answer_ET":"A","question_id":648,"topic":"1"},{"id":"0YERa2iSyIXJ5EScdD7c","exam_id":32,"discussion":[{"poster":"keos","comment_id":"215911","upvote_count":"23","comments":[{"content":"I DONT THINK SO. THE QUESTION DOES NOT MENTION DATA SHOULD BE PROCESSED IN REAL-TIME. \nI AM GOING WITH AB.","timestamp":"1634891700.0","poster":"tuananhngo","comment_id":"398891","upvote_count":"6"}],"content":"would go for AD","timestamp":"1632356100.0"},{"comment_id":"256683","timestamp":"1633358820.0","content":"E is out as IoT core cannot use Amazon SQS FIFO as target. C is out as increasing the payload size would not necessarily result in reduced volume over a period of time. Between B and D, I would go with D as batching the data before sending to Lambda would result in reducing the concurrency which is the reason for TooManyRequestsException. A is correct because Lambda is writing to Dynamo DB and batching wouldn't help with the rate at which Lambda is writing data to DynamoDB which is the cause of ProvisionedThroughputExceededException. So need to increase the WCU in DynamoDB. Correct answer is A and D.","upvote_count":"19","poster":"Bulti"},{"timestamp":"1696690740.0","comment_id":"1027470","upvote_count":"1","content":"Selected Answer: AD\nAD ...memory thing is not the problem here","poster":"nhorcajada"},{"upvote_count":"1","content":"Selected Answer: AB\nhttps://repost.aws/knowledge-center/lambda-troubleshoot-throttling\nI thought the only way to solve the lambda error mentioned would be to increase concurrency for lambda which lead me to choose option E. Apparently, there is reference from AWS to increase the memory as well:\n“Check for spikes in Duration metrics for your function\nConcurrency depends on function duration. If your function code is taking too long to complete, there might not be enough compute resources.\nTry increasing the function's memory setting. Then, use AWS X-Ray and CloudWatch Logs to isolate the cause of duration increases.\nNote: Changing the memory setting can affect the charges that you incur for execution time.”","timestamp":"1685398320.0","comments":[{"poster":"vn_thanhtung","content":"As more smart meters are deployed, the Engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. I don't think the problem is in the lambda","comment_id":"991231","upvote_count":"1","timestamp":"1693115280.0"}],"poster":"rbm2023","comment_id":"909755"},{"comment_id":"902845","upvote_count":"4","timestamp":"1684633800.0","comments":[{"upvote_count":"1","content":"For Bulti mentioned that E is out as IoT core cannot use Amazon SQS FIFO as target, I didn't \nsee anywhere in the question mentioned aws IoT core service , My understanding is data -> apigateway->sqs->dynamodb. So E is feasible.","comment_id":"902846","timestamp":"1684633980.0","poster":"Jesuisleon"}],"poster":"Jesuisleon","content":"Selected Answer: AE\nI think A and E.\nFor B, increase lambda memory has nothing to do with the error TooManyRequestsException which is related with lambda cocurrency limit, pls. refer to https://repost.aws/questions/QUDg8O87XnQoaITAumf8TpVg/error-lambda-toomanyrequestsexception\n\nFor E, I think data is sent to api gateway then data is queued in SQS by this lambda can handle in a limited concurrency."},{"comment_id":"872643","poster":"dev112233xx","upvote_count":"2","timestamp":"1681731180.0","content":"Selected Answer: AB\nAB are correct\nhttps://repost.aws/knowledge-center/lambda-troubleshoot-throttling\n\nAlso, can you tell me how a rest api can \"stream\" the data to Kinesis? Lol\nit's possible to do the opposite which Kinesis stream to Api Gateway but not Api Gateway to Kinesis especially when the Rest endpoint is Synchronous .."},{"content":"\"ProvisionedThroughputExceededException\". D: Kinesis can group request in batch so Lambda can handle them efficiently. B is not right. There's no mentioning that memory was bottleneck. Lambda has 100 concurrent execution per region. So batching here is more important than increase Lambda memory setting.","poster":"zWarez","comments":[{"upvote_count":"1","poster":"romiao106","comment_id":"876863","content":"If lambda takes a long time to execute it will affect the number of concurrent requests in a specific region. By allowing for more memory, it can decrease the # of time to run a lambda function thus allows for more concurrent requests.","timestamp":"1682114520.0"}],"comment_id":"821909","upvote_count":"1","timestamp":"1677369180.0"},{"comment_id":"820494","timestamp":"1677244140.0","poster":"andras","content":"Selected Answer: AB\nDoes increasing Lambda memory increase performance?\nIf a function is CPU-, network- or memory-bound, then changing the memory setting can dramatically improve its performance. Since the Lambda service charges for the total amount of gigabyte-seconds consumed by a function, increasing the memory has an impact on overall cost if the total duration stays constant.","upvote_count":"2"},{"upvote_count":"2","timestamp":"1673522160.0","content":"Selected Answer: AD\nProvisionedThroughputExceededException = DynamoDB","poster":"syaldram","comment_id":"773376"},{"poster":"evargasbrz","upvote_count":"1","timestamp":"1672320240.0","content":"Selected Answer: AB\nI will go with A and B","comment_id":"761052"},{"upvote_count":"2","poster":"gnic","timestamp":"1661796420.0","comment_id":"653588","content":"Selected Answer: AD\nAD\nD - The question talk about \"tooManyRequest\". Kinesis can batch data"},{"upvote_count":"1","comment_id":"553797","timestamp":"1645545240.0","poster":"johnnsmith","content":"D is vague. Does it use the same Lambda function? If yes, it doesn't work. If you do a batch of 10, it will take 20 minutes to finish. If a new design with EC2 is allowed, D is correct. The question says \"modification\" which implies same Lambda function. Then B is correct. Then what is currently memory size? If it is already 10GB, B is wrong. Overall, it is a badly worded question."},{"comment_id":"542325","content":"I would go for A and B.\nThe error on DynamoDb is because of the resources constraint since the requests are too high.\nA for increasing WCU\n\nB is supported by https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-throttling/\nThe blog quotes below.\nCheck for spikes in Duration metrics for your function\n\nConcurrency depends on function duration. If your function code is taking too long to complete, then there might not be enough compute resources.\n\nTry increasing the function's memory setting. Then, use AWS X-Ray and CloudWatch Logs to isolate the cause of duration increases\n\nD should not be ideal because it changes the whole architecture and will induce more latency I believe.","timestamp":"1644233460.0","poster":"Ishu_awsguy","comments":[{"content":"Basically AWS Lambda has a default safety throttle of 100 concurrent executions per account per region. Increasing the lambda size doesn't solve the root problem, so D seems to be a better option.","timestamp":"1663418880.0","comment_id":"671520","poster":"joancarles","upvote_count":"2"}],"upvote_count":"2"},{"content":"A. Increase the write capacity units to the DynamoDB table.\nD. Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.","comment_id":"497776","timestamp":"1639058400.0","poster":"cldy","upvote_count":"2"},{"upvote_count":"1","content":"A,D seems to be correct","timestamp":"1638418680.0","comment_id":"492135","poster":"AzureDP900"},{"upvote_count":"1","content":"AD. \nhttps://alienattack.workshop.aws/en/short-labs/kinesis/300-ingestion-to-dynamodb.html","poster":"Bigbearcn","timestamp":"1636216500.0","comment_id":"448478"},{"timestamp":"1636078200.0","upvote_count":"1","comment_id":"445973","content":"It's A D","poster":"andylogan"},{"poster":"student22","content":"B,D\nA good explanation is in XRiddlerX's answer below.","comment_id":"439407","timestamp":"1635912420.0","upvote_count":"3"},{"upvote_count":"1","content":"AAA DDD\n---","timestamp":"1635712500.0","poster":"tgv","comment_id":"436405"},{"upvote_count":"1","timestamp":"1635520440.0","content":"It is A and D.","comment_id":"433864","poster":"blackgamer"},{"timestamp":"1635258840.0","content":"I'll go for A,D","comment_id":"413417","upvote_count":"2","poster":"WhyIronMan"},{"poster":"Akhil254","upvote_count":"3","timestamp":"1635092580.0","content":"AD Correct","comment_id":"407373"},{"timestamp":"1634710140.0","upvote_count":"3","content":"This is straight out of Jon Bonso's exam questions. Answers are A and D","comment_id":"396038","poster":"MrCarter"},{"comment_id":"383658","poster":"kpcert","upvote_count":"9","content":"Ans : A & B\nThere are 2 issues here. \nIssue 1 - Lambda function started taking more time when the load increases. Fix : Increase Lambda CPU by increasing the memory.\nIssue 2 - TooManyRequetsException from DynamoDB. Fix: Increate WCU of DynamoDB\n\nSince this change has already passed the pilot phase and the issue is happening in the production workload, the simple fix should be considered.","timestamp":"1634635140.0","comments":[{"content":"+1\n\nTo increase CPU for a Lambda function, oddly enough, you give it more memory: https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-2/\n(This is the same kind of indirect performance increase by adjusting something seemingly unrelated like increasing an EBS disk's IOPS by increasing the disk size.)\n\nThe \"ProvisionedThroughputExceeded\" exception is in the SDK the Lambda function is using to write to DynamoDB. When DynamoDB can't keep up, it throws that error back to Lambda, and Lambda logs it. But it's indicating that you've run out of Write Capacity Units in DDB:\n\nhttps://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/ProvisionedThroughputExceededException.html","comment_id":"463307","upvote_count":"1","poster":"kirrim","timestamp":"1636296240.0"}]},{"timestamp":"1634565240.0","content":"AB may still be correct. I think the hint here is that the data collection is not real time instead every 5 minutes which is kind of queueing, so we would not really need an SQS or a KDS. So, by simply increasing the memory, lambda can process faster and since it is processing faster, an increase in the WCU should really fix the issue.","comment_id":"360296","comments":[{"upvote_count":"1","comment_id":"383660","poster":"kpcert","timestamp":"1634655960.0","content":"I Agree"}],"poster":"pradhyumna","upvote_count":"4"},{"timestamp":"1634384340.0","comment_id":"357082","content":"it's A&D","poster":"Waiweng","upvote_count":"4"},{"timestamp":"1634182200.0","content":"\"As more smart meters are deployed, the Engineers notice the Lambda functions are taking from 1 to 2 minutes to complete.\" which is resource constrain. If we increase memory eventually it will increase lambda CPU. Resource crunch also cause TooManyRequestsException - https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-throttling/ I go with AB ,","comment_id":"354473","poster":"tvs","upvote_count":"2"},{"content":"need to be careful to avoid major architectural changes - exam guidance states that which makes D a difficult choice","upvote_count":"1","comment_id":"344703","poster":"gsw","timestamp":"1634153340.0"},{"content":"AD to me. B is not relevant as the error is not relevant.","timestamp":"1634050140.0","upvote_count":"2","comment_id":"343375","poster":"blackgamer"},{"content":"should be A & D","upvote_count":"2","poster":"alisyech","timestamp":"1633957440.0","comment_id":"321712"},{"upvote_count":"3","poster":"Kian1","timestamp":"1633951980.0","content":"will go for AD","comment_id":"294195"},{"poster":"01037","comment_id":"271350","timestamp":"1633656480.0","upvote_count":"3","content":"Just feel E only could be the solution, though Standard SQS is enough, I think.\nIf it has to be two, then A & D.\nBecause we aren't sure about the reason of TooManyRequestsException erros. B may or may not solve the problem, so it can't be the answer."},{"poster":"rkbala","content":"A&D\nLamda is having 100 concurrent executions per region. So need to use data streams and batch processing","comment_id":"266073","upvote_count":"2","timestamp":"1633460400.0"},{"comment_id":"256355","content":"Answer is A & B ... Read the question properly guys !! .. D would require an architectural change .. the question is merely asking for simple changes to fix existing solution...","poster":"petebear55","comments":[{"poster":"rcher","content":"I don't think increasing lambda memory actually help much in mitigating TooManyRequestsException, which is an error due to throttling when too many requests came at once.","comment_id":"277477","timestamp":"1633820880.0","upvote_count":"2"}],"timestamp":"1633207980.0","upvote_count":"5"},{"poster":"Ebi","timestamp":"1632740460.0","upvote_count":"4","comment_id":"253786","content":"Answer is AD"},{"content":"Correct is AD\nA -> Dynamo increase write limit fix ProvisionedThroughputExceededException \nD -> Kinesis will group requests in batch so decrease requests in Lambda so fix TooManyRequestsException","timestamp":"1632655500.0","poster":"T14102020","comment_id":"241492","upvote_count":"6"},{"upvote_count":"2","comment_id":"233292","content":"I'll go for A,D","timestamp":"1632552240.0","poster":"jackdryan"},{"content":"would go for AD\nA -> Dynamo write limit\nD -> sounds like a streaming use case","upvote_count":"3","poster":"Raysquared","timestamp":"1632537900.0","comment_id":"218840"},{"comment_id":"218433","timestamp":"1632536880.0","poster":"AK2020","upvote_count":"2","content":"B & D- If your workload is unevenly distributed across partitions, or if the workload relies on short periods of time with high usage (a burst of read or write activity), the table might be throttled."},{"upvote_count":"4","content":"I would go with A & D\nThe TooManyRequestsException can be solved by \"Increasing the memory available in Lambda functions.\" The ProvisionedThroughputExceededException can be solved by writing in batches. Streaming data in Kinesis from API Gateway is a common pattern and AWS has a tutorial on how to do that.\n\nTutorial: https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html\nKB for TooManyRequestsException: https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-throttling/","comments":[{"timestamp":"1632445080.0","comments":[{"poster":"Chibuzo1","upvote_count":"2","comment_id":"363731","content":"You got it Right. The answer is B, D. \nto optimize the function execution time, the memory assigned to the Lambda functions can be increased making it B","timestamp":"1634622540.0"}],"upvote_count":"4","poster":"XRiddlerX","content":"Sorry B and D","comment_id":"218092"},{"timestamp":"1634683200.0","content":"The article does say to increase the memory setting. Thus it would be A & B","poster":"Training","upvote_count":"2","comment_id":"390036"}],"comment_id":"217974","poster":"XRiddlerX","timestamp":"1632405240.0"},{"content":"Answer is A & B","comment_id":"214338","upvote_count":"5","timestamp":"1632259020.0","poster":"bella100"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/36302-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"question_images":[],"choices":{"D":"Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.","E":"Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message.","A":"Increase the write capacity units to the DynamoDB table.","C":"Increase the payload size from the smart meters to send more data.","B":"Increase the memory available to the Lambda functions."},"unix_timestamp":1604707500,"question_text":"A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete.\nAs more smart meters are deployed, the Engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many ProvisionedThroughputExceededException errors while performing PUT operations on DynamoDB, and there are also many TooManyRequestsException errors from Lambda.\nWhich combination of changes will resolve these issues? (Choose two.)","answers_community":["AB (40%)","AD (33%)","AE (27%)"],"answer":"AB","answer_ET":"AB","answer_description":"","topic":"1","timestamp":"2020-11-07 01:05:00","question_id":649},{"id":"sUOYSFZq8TFK3sYgTq1r","url":"https://www.examtopics.com/discussions/amazon/view/36347-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"C","exam_id":32,"question_id":650,"topic":"1","answer_description":"","question_text":"An ecommerce company has an order processing application it wants to migrate to AWS. The application has inconsistent data volume patterns, but needs to be avail at all times. Orders must be processed as they occur and in the order that they are received.\nWhich set of steps should a solutions architect take to meet these requirements?","choices":{"B":"Use Amazon SNS with FIFO and send orders as they occur. Use a single large Reserved Instance for processing.","A":"Use AWS Transfer for SFTP and upload orders as they occur. Use On-Demand Instances in multiple Availability Zones for processing.","C":"Use Amazon SQS with FIFO and send orders as they occur. Use Reserved Instances in multiple Availability Zones for processing.","D":"Use Amazon SQS with FIFO and send orders as they occur. Use Spot Instances in multiple Availability Zones for processing."},"timestamp":"2020-11-07 10:49:00","answers_community":["C (100%)"],"answer_images":[],"question_images":[],"unix_timestamp":1604742540,"answer_ET":"C","discussion":[{"content":"C is correct, SQS with FIFO to process the orders as they come and reserved instances for availability at all times","timestamp":"1632200820.0","upvote_count":"20","comments":[{"timestamp":"1734094620.0","upvote_count":"1","comment_id":"1326122","poster":"mnsait","content":"Shouldn't FIFO be used with a single instance rather than process in parallel? The question says \"Orders must be processed as they occur and in the order that they are received.\"\n\nThis makes me think the answer is B."},{"comment_id":"463308","upvote_count":"2","content":"Agree, C is the best answer given\n\nBetter approach might be to:\n- start with On-Demand instances in an ASG\n- set the ASG scaling metric to SQS FIFO queue depth\n- monitor for steady-state minimum number of instances needed\n- purchase RIs for minimum number of instances needed\n- use On-Demand instances for additional bursting instances in the ASG above base","poster":"kirrim","timestamp":"1635938580.0"}],"poster":"liono","comment_id":"214533"},{"comment_id":"940183","content":"Selected Answer: C\nC is correct, SQS with FIFO to process the orders as they come and reserved instances for availability at all times","timestamp":"1688229600.0","upvote_count":"1","poster":"SkyZeroZx"},{"poster":"Anhdd","timestamp":"1654827480.0","comment_id":"614301","content":"Selected Answer: C\nC for sure","upvote_count":"1"},{"timestamp":"1653634680.0","comments":[{"comment_id":"704631","timestamp":"1666784220.0","poster":"vijay1319","content":"awesome explanation Bob !!!","upvote_count":"1"}],"upvote_count":"4","poster":"bobsmith2000","comment_id":"607954","content":"OK, everyone is on the same ground that it's between C and D.\nRI vs Spot.\n1) Data pattern is erratic.\n2) The app must be available all the time.\n3) Cost-effectiveness isn't mentioned.\n\nFirst of all, if we set up the bid price to be equal the on-demand price of a particular instance, then we are always gonna get compute power. The stop price can't be higher on-demand one and it's never gonna be interrupted.\nSecond of all, we can't predict the amount of RI to purchase due to \"1)\".\nThird of all, the Q states \"must be available all the time\".\n\nThe perfect answer would be use fleet with RI + Spot, because we can't predict how many RI to purchase.\nWithout giving it too much thoughts it's C. But if you think about it for a bit longer, it seems to be D. \nFollowing KISS principle, let's say it's C."},{"timestamp":"1651114080.0","content":"I think always means available to accept order, not necessarily for processing it","poster":"cooldeity","upvote_count":"1","comment_id":"593485"},{"poster":"AMKazi","content":"C is the right answer","upvote_count":"1","comment_id":"540647","timestamp":"1644008160.0"},{"comment_id":"514379","upvote_count":"1","timestamp":"1641014340.0","poster":"cldy","content":"C correct."},{"comment_id":"509721","comments":[{"content":"They meant RI for processing the SQS queue so answer is C","timestamp":"1640893980.0","upvote_count":"1","comment_id":"513692","poster":"kemalgoklen"},{"poster":"user0001","content":"it is C because Orders must be handled on a first-come, first-serve basis and in the order in which they are received.\n\nthey are not asking for most cost effective","comment_id":"605730","timestamp":"1653260100.0","upvote_count":"1"}],"content":"I don't see the point in using RI with SQS; https://aws.amazon.com/blogs/compute/running-cost-effective-queue-workers-with-amazon-sqs-and-amazon-ec2-spot-instances/\nAnswer: D","upvote_count":"3","poster":"vbal","timestamp":"1640539440.0"},{"poster":"AzureDP900","comment_id":"492137","upvote_count":"1","content":"C is correct","timestamp":"1638418860.0"},{"poster":"andylogan","upvote_count":"1","content":"It's C with Reserved instance","comment_id":"445974","timestamp":"1635802560.0"},{"poster":"tgv","upvote_count":"1","comment_id":"436403","content":"CCC\n---","timestamp":"1635500760.0"},{"content":"I'll go with C","comment_id":"413421","timestamp":"1635092100.0","poster":"WhyIronMan","upvote_count":"2"},{"upvote_count":"1","content":"Option C: In order (FIFO) + Reserved Instances in X AZs (Availability)","poster":"KittuCheeku","timestamp":"1634727120.0","comment_id":"404898"},{"content":"C, SAA level question","upvote_count":"1","poster":"mustpassla","timestamp":"1634124660.0","comment_id":"366186"},{"upvote_count":"3","poster":"Waiweng","content":"it's C","timestamp":"1634089080.0","comment_id":"356977"},{"timestamp":"1634025840.0","content":"Answer is C. keywords \"needs to be avail at all times\", process the orders as they come","upvote_count":"2","comment_id":"334369","poster":"KnightVictor"},{"timestamp":"1633908120.0","content":"C for sure","poster":"alisyech","upvote_count":"2","comment_id":"321713"},{"upvote_count":"2","comment_id":"294197","poster":"Kian1","timestamp":"1633899300.0","content":"going for C"},{"upvote_count":"2","poster":"Ebi","comment_id":"284542","content":"I go with C","timestamp":"1633823400.0"},{"timestamp":"1633738080.0","poster":"01037","comments":[{"poster":"sashsz","content":"spot instances. Answ: D, at all times - across AZs","timestamp":"1644765540.0","comment_id":"546531","upvote_count":"1"}],"content":"C.\nWhat does \"inconsistent data volume patterns\" imply?","comment_id":"271351","upvote_count":"1"},{"timestamp":"1633295400.0","comment_id":"256684","upvote_count":"1","content":"Answer is C.","poster":"Bulti"},{"timestamp":"1633200720.0","upvote_count":"1","content":"C\nFIFO SQS for order and process always RI","poster":"rscloud","comment_id":"245977"},{"content":"\"needs to be avail at all times\". So correct answer is C.","poster":"T14102020","upvote_count":"1","timestamp":"1633091640.0","comment_id":"241248"},{"poster":"jackdryan","content":"I'll go with C","timestamp":"1632943140.0","upvote_count":"3","comment_id":"233300"},{"comment_id":"214767","poster":"VegasDegenerate","content":"C. Simple, SQS for decoupled arch and FIFO for exactly once","timestamp":"1632745800.0","upvote_count":"1"}],"isMC":true}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","numberOfQuestions":1019,"id":32,"isMCOnly":false,"provider":"Amazon"},"currentPage":130},"__N_SSP":true}