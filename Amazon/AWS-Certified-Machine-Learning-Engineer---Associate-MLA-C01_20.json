{"pageProps":{"questions":[{"id":"7WoAt2kAWvlguR8sho3Z","isMC":true,"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/168847-exam-aws-certified-machine-learning-engineer-associate-mla/","timestamp":"2025-03-11 12:42:00","question_id":96,"question_text":"A company needs to develop an ML model. The model must identify an item in an image and must provide the location of the item.\n\nWhich Amazon SageMaker algorithm will meet these requirements?","topic":"1","choices":{"C":"Object detection","B":"XGBoost","A":"Image classification","D":"K-nearest neighbors (k-NN)"},"answer":"C","question_images":[],"exam_id":27,"answer_description":"","unix_timestamp":1741693320,"answer_images":[],"answers_community":["C (100%)"],"discussion":[{"content":"Selected Answer: C\nüîë Keyword: Identify an item in an image and provide its location\n‚úÖ Correct Answer: C. Object detection\n\nWhy?\n\nObject detection algorithms detect and localize objects within images (bounding boxes).\n\nSageMaker supports built-in object detection using SSD (Single Shot Multibox Detector) and Faster R-CNN.\n\nWhy Others Are Wrong?\n‚ùå A. Image classification only identifies the object but does not provide its location.\n‚ùå B. XGBoost is a gradient boosting algorithm for structured/tabular data, not images.\n‚ùå D. k-NN is for classification/regression, not object localization.","timestamp":"1743391380.0","poster":"aws_Tamilan","upvote_count":"1","comment_id":"1413928"},{"timestamp":"1742304420.0","comment_id":"1400159","upvote_count":"1","poster":"trongod05","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/algo-object-detection-tech-notes.html"}]},{"id":"tuHXT7xKiOhn1MBEuBsB","question_text":"A company has an Amazon S3 bucket that contains 1 –¢–í of files from different sources. The S3 bucket contains the following file types in the same S3 folder: CSV, JSON, XLSX, and Apache Parquet.\n\nAn ML engineer must implement a solution that uses AWS Glue DataBrew to process the data. The ML engineer also must store the final output in Amazon S3 so that AWS Glue can consume the output in the future.\n\nWhich solution will meet these requirements?","answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/157315-exam-aws-certified-machine-learning-engineer-associate-mla/","exam_id":27,"isMC":true,"topic":"1","choices":{"D":"Separate the data into a different folder for each file type. Use DataBrew to process each folder individually. Store the output in AWS Glue Parquet format.","A":"Use DataBrew to process the existing S3 folder. Store the output in Apache Parquet format.","B":"Use DataBrew to process the existing S3 folder. Store the output in AWS Glue Parquet format.","C":"Separate the data into a different folder for each file type. Use DataBrew to process each folder individually. Store the output in Apache Parquet format."},"answer_ET":"C","question_id":97,"answer":"A","answers_community":["A (57%)","C (43%)"],"unix_timestamp":1740824760,"timestamp":"2025-03-01 11:26:00","question_images":[],"discussion":[{"upvote_count":"1","content":"Selected Answer: A\nüîë Keyword: Process mixed file types with AWS Glue DataBrew & store for AWS Glue\n‚úÖ Correct Answer: A. Use DataBrew to process the existing S3 folder. Store the output in Apache Parquet format.\n\nWhy?\n\nAWS Glue performs best with Parquet because it is optimized for analytical queries.\n\nNo need to split data into separate folders‚ÄîDataBrew can handle mixed file types.\n\nWhy Others Are Wrong?\n‚ùå B. \"AWS Glue Parquet format\" is not a valid term. Apache Parquet is the correct format.\n‚ùå C & D. Separating files into different folders is unnecessary‚ÄîDataBrew can process multiple formats in a single folder.","poster":"aws_Tamilan","timestamp":"1743391440.0","comment_id":"1413929"},{"timestamp":"1742740560.0","poster":"michele_scar","upvote_count":"1","comment_id":"1402322","content":"Selected Answer: A\nC implies that you have to re-organize all files (1 TB is a lot). This means a lot of work. For me is A, less performance but without initial overhead of organization"},{"timestamp":"1742568900.0","content":"Selected Answer: C\n‚úÖ Explanation:\nProblem Summary:\n\n The data in S3 is mixed file formats: CSV, JSON, XLSX, and Parquet ‚Äî all in one folder.\n You need to use AWS Glue DataBrew to process the data.\n The processed data must be stored in S3 for AWS Glue to consume later.\n\nKey Considerations:\n\n DataBrew Input Requirements:\n DataBrew datasets must be in a consistent format (CSV, JSON, XLSX, or Parquet).\n DataBrew cannot process mixed formats in a single dataset. You must split the data by format.\n\n DataBrew Output Format:\n Apache Parquet is preferred for:\n Efficient storage\n Better performance with AWS Glue and other analytics tools\n Columnar storage benefits in querying and transformations\n\n \"AWS Glue Parquet format\" does not exist ‚Äî this is a distractor in the answer options.","upvote_count":"1","poster":"eesa","comment_id":"1401603"},{"poster":"chris_spencer","upvote_count":"2","comment_id":"1387388","timestamp":"1741695000.0","content":"Selected Answer: A\nShould be A.\n\nC is incorrect because it involve separating the data by file type, which is unnecessary since DataBrew can process various file types within the same folder."},{"comment_id":"1363477","content":"Selected Answer: C\nAWS Glue DataBrew can process various file formats (CSV, JSON, XLSX, Parquet)\nSince DataBrew can handle datasets with multiple file formats, there is no need to separate files into different folders by type.\nApache Parquet is an optimal format for AWS Glue\nParquet is a columnar format, which is well-suited for AWS Glue and is efficient for later analysis and ML model training.\n\"AWS Glue Parquet format\" does not exist\nOptions B and D mention \"AWS Glue Parquet format,\" which is incorrect. Parquet is a standard data format and is not exclusive to AWS Glue.\n‚úÖ Conclusion: Option A is the best solution because it allows DataBrew to process all files in the existing S3 folder and store the output in Apache Parquet format, which is efficient and compatible with AWS Glue. üöÄ","upvote_count":"2","poster":"ryuhei","timestamp":"1740824760.0"}]},{"id":"e06OieNJjkOgfOSW4Znv","answer_description":"","question_images":[],"question_text":"A manufacturing company uses an ML model to determine whether products meet a standard for quality. The model produces an output of \"Passed\" or \"Failed.\" Robots separate the products into the two categories by using the model to analyze photos on the assembly line.\n\nWhich metrics should the company use to evaluate the model's performance? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/157316-exam-aws-certified-machine-learning-engineer-associate-mla/","topic":"1","answers_community":["C (100%)"],"timestamp":"2025-03-01 11:30:00","unix_timestamp":1740825000,"answer_ET":"AC","discussion":[{"timestamp":"1742203920.0","upvote_count":"1","poster":"chris_spencer","content":"Selected Answer: C\nA. Precision and recall\nC. Accuracy and F1 score","comment_id":"1399587"}],"choices":{"E":"Perplexity","D":"Bilingual Evaluation Understudy (BLEU) score","B":"Root mean square error (RMSE) and mean absolute percentage error (MAPE)","C":"Accuracy and F1 score","A":"Precision and recall"},"isMC":true,"answer":"C","answer_images":[],"exam_id":27,"question_id":98},{"id":"FhJ2Bk9iGwv6bXmqIj2t","isMC":true,"topic":"1","unix_timestamp":1741697400,"timestamp":"2025-03-11 13:50:00","url":"https://www.examtopics.com/discussions/amazon/view/168853-exam-aws-certified-machine-learning-engineer-associate-mla/","question_images":[],"question_id":99,"answer":"B","discussion":[{"comment_id":"1413933","content":"Selected Answer: B\nüîë Keyword: Encrypt data in transit during SageMaker training\n‚úÖ Correct Answer: B. Encrypt communication between nodes in a training cluster.\n\nWhy?\n\nEncryption in transit refers to securing data as it moves between components of the SageMaker training infrastructure (e.g., between nodes in a distributed training job).\n\nSageMaker automatically encrypts communication between nodes using TLS 1.2 in multi-node training clusters.\n\nWhy Others Are Wrong?\n‚ùå A. \"Batch processing\" is not relevant to SageMaker training jobs.\n‚ùå C. AWS KMS is used for encrypting data at rest, not for securing in-transit data.\n‚ùå D. SageMaker domain encryption is for Studio environments, not training jobs.","timestamp":"1743391800.0","upvote_count":"1","poster":"aws_Tamilan"},{"timestamp":"1742480460.0","poster":"ygn4ei","upvote_count":"1","comment_id":"1401088","content":"Selected Answer: B\nthis is it"}],"choices":{"B":"Encrypt communication between nodes in a training cluster.","D":"Specify an AWS Key Management Service (AWS KMS) key during creation of the SageMaker domain.","C":"Specify an AWS Key Management Service (AWS KMS) key during creation of the training job request.","A":"Encrypt communication between nodes for batch processing."},"answer_ET":"B","exam_id":27,"question_text":"An ML engineer needs to encrypt all data in transit when an ML training job runs. The ML engineer must ensure that encryption in transit is applied to processes that Amazon SageMaker uses during the training job.\n\nWhich solution will meet these requirements?","answer_description":"","answer_images":[],"answers_community":["B (100%)"]},{"id":"3GiFflE5UeHfKNOs5JkG","url":"https://www.examtopics.com/discussions/amazon/view/155580-exam-aws-certified-machine-learning-engineer-associate-mla/","answers_community":[],"question_images":["https://img.examtopics.com/aws-certified-machine-learning-engineer-associate-mla-c01/image9.png"],"answer_ET":"","unix_timestamp":1738079520,"answer_description":"","topic":"1","answer_images":["https://img.examtopics.com/aws-certified-machine-learning-engineer-associate-mla-c01/image10.png"],"discussion":[{"content":"city = one-hot encoding; type_year = feature splitting; size of the building = standardized distribution","comment_id":"1381857","poster":"chris_spencer","timestamp":"1741614060.0","upvote_count":"1"},{"comment_id":"1361059","poster":"Corry","content":"Since the question highlight \"similarly sized homes\", which means those numbers of build size won't show skewed distribution. Thus, Size of the building should be standard distribution.","timestamp":"1740408900.0","upvote_count":"1"},{"poster":"molerowan","timestamp":"1738804680.0","upvote_count":"3","comment_id":"1352134","content":"Size of building (Square feet or Square Meters) = Logarithmic transformation \nExplanation: Building size is a numerical feature that often shows a skewed distribution and can have a non-linear relationship with price. Logarithmic transformation is suitable because:\nIt helps normalize skewed distributions\nIt can help linearize the relationship between size and price\nIt's particularly useful for features that follow exponential or multiplicative patterns\nReal estate data often shows log-normal distributions"},{"upvote_count":"2","timestamp":"1738804620.0","comment_id":"1352133","content":"Type_year (type of home and year it was built) = Feature splitting Explanation: This feature contains two different pieces of information (type and year) combined in one column. Feature splitting is appropriate because:\nIt separates the compound feature into its components\nThe type can then be one-hot encoded\nThe year can be treated as a numerical feature\nThis separation allows the model to learn from each component independently","poster":"molerowan"},{"content":"City (Name) = One-hot encoding Explanation: City names are categorical variables that don't have any numerical relationship with each other. One-hot encoding is the best technique for this type of data because:\nIt creates binary columns for each unique city\nIt avoids introducing artificial ordering between cities\nIt allows the model to treat each city as an independent feature","upvote_count":"3","comment_id":"1352132","timestamp":"1738804560.0","poster":"molerowan"},{"upvote_count":"2","comment_id":"1351699","timestamp":"1738728840.0","poster":"djeong95","content":"city = one-hot encoding; type_year = feature splitting; size of the building = standardized distribution"},{"upvote_count":"3","comment_id":"1348009","timestamp":"1738079520.0","poster":"abrarjahin","content":"Size of the building is standard distribution"}],"isMC":false,"question_id":100,"question_text":"HOTSPOT -\nAn ML engineer is working on an ML model to predict the prices of similarly sized homes. The model will base predictions on several features The ML engineer will use the following feature engineering techniques to estimate the prices of the homes:\n‚Ä¢ Feature splitting\n‚Ä¢ Logarithmic transformation\n‚Ä¢ One-hot encoding\n‚Ä¢ Standardized distribution\nSelect the correct feature engineering techniques for the following list of features. Each feature engineering technique should be selected one time or not at all (Select three.)\n//IMG//","exam_id":27,"answer":"","timestamp":"2025-01-28 16:52:00"}],"exam":{"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":106,"isMCOnly":false,"name":"AWS Certified Machine Learning Engineer - Associate MLA-C01","id":27,"provider":"Amazon"},"currentPage":20},"__N_SSP":true}