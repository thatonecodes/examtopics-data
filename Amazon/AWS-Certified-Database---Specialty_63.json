{"pageProps":{"questions":[{"id":"SydRZkioDruoKB48gjJi","url":"https://www.examtopics.com/discussions/amazon/view/25202-exam-aws-certified-database-specialty-topic-1-question-55/","question_images":[],"choices":{"A":"Dump all the tables from the Oracle database into an Amazon S3 bucket using datapump (expdp). Run data transformations in AWS Glue. Load the data from the S3 bucket to the Aurora DB cluster.","B":"Order an AWS Snowball appliance and copy the Oracle backup to the Snowball appliance. Once the Snowball data is delivered to Amazon S3, create a new Aurora DB cluster. Enable the S3 integration to migrate the data directly from Amazon S3 to Amazon RDS.","D":"Use AWS Server Migration Service (AWS SMS) to import the Oracle virtual machine image as an Amazon EC2 instance. Use the Oracle Logical Dump utility to migrate the Oracle data from Amazon EC2 to an Aurora DB cluster.","C":"Use the AWS Schema Conversion Tool (AWS SCT) to help rewrite database objects to MySQL during the schema migration. Use AWS DMS to perform the full load and change data capture (CDC) tasks."},"timestamp":"2020-07-09 07:56:00","answers_community":["C (100%)"],"isMC":true,"answer_images":[],"answer_description":"","topic":"1","question_id":311,"answer":"C","answer_ET":"C","unix_timestamp":1594274160,"exam_id":22,"discussion":[{"comment_id":"131823","poster":"chicagomassageseeker","content":"Answer C. This is heterogenous migration and requires DMS and SCT.","timestamp":"1632516180.0","upvote_count":"17"},{"comment_id":"137658","content":"Yep, C here","timestamp":"1634214900.0","upvote_count":"6","comments":[{"poster":"BillyMadison","upvote_count":"2","content":"Looks correct\nhttps://aws.amazon.com/blogs/database/migrating-oracle-databases-with-near-zero-downtime-using-aws-dms/","timestamp":"1634269740.0","comment_id":"141279"}],"poster":"learnaws"},{"timestamp":"1674843720.0","poster":"renfdo","comment_id":"789890","upvote_count":"1","content":"Selected Answer: C\nAnswer C. This is heterogenous migration and requires DMS and SCT."},{"content":"Selected Answer: C\nAnswer is C: Use AWS Schema Conversion Tool to convert Database and DMS for fast transfer.","upvote_count":"2","comment_id":"729508","poster":"examineme","timestamp":"1669659540.0"},{"upvote_count":"2","comment_id":"703460","content":"Answer: C\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-on-premoracle2aurora.html","timestamp":"1666659420.0","poster":"rags1482"},{"upvote_count":"3","comment_id":"595386","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/database/migrating-oracle-databases-with-near-zero-downtime-using-aws-dms/\n\nZero down time = First SCT and then DMS full and CDC\n\nC. Use the AWS Schema Conversion Tool (AWS SCT) to help rewrite database objects to MySQL during the schema migration. Use AWS DMS to perform the full load and change data capture (CDC) tasks.","poster":"novice_expert","timestamp":"1651368660.0"},{"content":"Answer C should be the one.","comment_id":"314952","poster":"LMax","timestamp":"1634747760.0","upvote_count":"2"},{"poster":"myutran","content":"Ans: C","comment_id":"298691","upvote_count":"1","timestamp":"1634548500.0"},{"content":"C is the right choice","comment_id":"253283","timestamp":"1634439300.0","poster":"JobinAkaJoe","upvote_count":"1"},{"upvote_count":"2","content":"Yes, it is C. Zero down time = First SCT and then DMS full and CDC","timestamp":"1634294940.0","comment_id":"212424","poster":"Ashoks"},{"upvote_count":"2","comment_id":"134923","timestamp":"1632853560.0","poster":"Mickysingh","content":"Ans D can not be right as it is not doing schema conversion and using oracle backup for restoring. Also downtime is high."}],"question_text":"A Database Specialist needs to define a database migration strategy to migrate an on-premises Oracle database to an Amazon Aurora MySQL DB cluster. The company requires near-zero downtime for the data migration. The solution must also be cost-effective.\nWhich approach should the Database Specialist take?"},{"id":"x06ts90Opn5RFqmS2Bog","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/65887-exam-aws-certified-database-specialty-topic-1-question-56/","answer_images":[],"discussion":[{"upvote_count":"13","comment_id":"476902","poster":"leunamE","content":"Option A.","timestamp":"1636718400.0"},{"upvote_count":"13","content":"Looks like option C was added as a distractor. When the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch. Answer should be A","poster":"jeyp12","comment_id":"540355","timestamp":"1643969700.0"},{"poster":"MultiAZ","comment_id":"1121707","timestamp":"1705154820.0","upvote_count":"1","content":"Selected Answer: A\nOption A\n- enable audit log\n- enable export to ClopudWatch Logs"},{"poster":"Pranava_GCP","upvote_count":"1","timestamp":"1694687040.0","content":"Selected Answer: A\nA. Enable DocumentDB to export the logs to Amazon CloudWatch Logs\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\n\n\" When auditing is enabled, Amazon DocumentDB exports your cluster’s auditing records (JSON documents) to Amazon CloudWatch Logs. You can use Amazon CloudWatch Logs to analyze, monitor, and archive your Amazon DocumentDB auditing events.\"","comment_id":"1007477"},{"upvote_count":"1","comment_id":"971869","poster":"IhorK","timestamp":"1691139840.0","content":"Selected Answer: A\nAmazon DocumentDB auditing supports the following event categories:\n - Data Definition Language (DDL)\n - Data Manipulation Language(DML)\nEnabling auditing on a cluster is a two-step process.\nStep 1. Enable the audit_logs cluster parameter (done)\nStep 2. Enable Amazon CloudWatch Logs Export\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html"},{"comment_id":"934872","upvote_count":"1","timestamp":"1687823160.0","poster":"adelcold","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html"},{"content":"The audit logs are for Document DB events.\nBut to enable it, we must enable CloudWatch Logs exports on Document DB not Document DB Events.","timestamp":"1684214940.0","poster":"f___16","upvote_count":"2","comment_id":"898892"},{"poster":"Pankaj24hrs","comment_id":"894998","upvote_count":"2","timestamp":"1683804600.0","content":"Option A\n\nBelow is from AWS Documentation - \nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\n\nWhen the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch."},{"upvote_count":"1","content":"Selected Answer: A\nA is the correct option as this is a mandatory step besides changing the value of audit_logs parameter","timestamp":"1679238780.0","poster":"backbencher2022","comment_id":"843863"},{"upvote_count":"1","content":"A is the correct answer","timestamp":"1679238660.0","comment_id":"843858","poster":"backbencher2022"},{"timestamp":"1678079820.0","content":"I don't know if this sounds like a silly question, but how about B?","comment_id":"830558","poster":"Nice_Guy","upvote_count":"1"},{"comment_id":"789894","content":"Selected Answer: A\nAnswer is A with sure. When you go to the console and hit on create a cluster, you will see an option to enable exports Profile and Audit logs to cloudwatch.","timestamp":"1674843960.0","upvote_count":"1","poster":"renfdo"},{"comment_id":"724924","content":"Option A is correct. Just validated from AWS site: \nWhen the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch.","timestamp":"1669183620.0","poster":"Arun32","upvote_count":"2"},{"timestamp":"1665955680.0","content":"Selected Answer: A\nI am leaning towards A.","comment_id":"696594","upvote_count":"1","poster":"awsjjj"},{"timestamp":"1657454340.0","poster":"Chirantan","content":"I think Answer is A \nWhen auditing is enabled, Amazon DocumentDB exports your cluster’s auditing records (JSON documents) to Amazon CloudWatch Logs. You can use Amazon CloudWatch Logs to analyze, monitor, and archive your Amazon DocumentDB auditing events.","comment_id":"629546","upvote_count":"3"},{"upvote_count":"5","content":"Selected Answer: A\nAnswer A:\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\nStep 2. Enable Amazon CloudWatch Logs Export\nWhen the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch.\n\nDo not get confused with \"Amazon DocumentDB records Data Definition Language (DDL), authentication, authorization, and user management events to Amazon CloudWatch Logs.\" The above event start to get recorded when you enable auditing via enabling audit_log parameter.","comment_id":"626523","poster":"kush_sumit","timestamp":"1656845580.0"},{"timestamp":"1656844740.0","content":"Answer A:\n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\nStep 2. Enable Amazon CloudWatch Logs Export\nWhen the value of the audit_logs cluster parameter is enabled, you must also enable Amazon DocumentDB to export logs to Amazon CloudWatch. If you omit either of these steps, audit logs will not be sent to CloudWatch.\n\nWhen creating a cluster, performing a point-in-time-restore, or restoring a snapshot, you can enable CloudWatch Logs by following these steps.\n\nDo not get confused with \"Amazon DocumentDB records Data Definition Language (DDL), authentication, authorization, and user management events to Amazon CloudWatch Logs.\" The above event start to get recorded when you enable auditing via enabling audit_log parameter.","poster":"kush_sumit","upvote_count":"1","comment_id":"626515"},{"content":"Selected Answer: A\nOption A","poster":"minhntm","timestamp":"1656818220.0","comment_id":"626395","upvote_count":"2"},{"upvote_count":"1","poster":"minhntm","content":"Option A.\nAccording to this document: https://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\nEnsure that both steps are completed, or auditing logs will not be sent to CloudWatch Logs:\n- Enable the audit_logs Cluster Parameter\n- Enable Amazon CloudWatch Logs Export\nThe question said that \"the Database Specialist has activated the audit logs option\"","timestamp":"1656818160.0","comment_id":"626394"},{"comment_id":"622698","poster":"elf78","content":"Selected Answer: C\nIn https://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html ,\nRead this line \"When auditing is enabled, Amazon DocumentDB records Data Definition Language (DDL), ....\".","timestamp":"1656263940.0","upvote_count":"1"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html","comment_id":"608892","upvote_count":"2","poster":"Dantas","timestamp":"1653849240.0"},{"timestamp":"1652606520.0","comment_id":"602010","upvote_count":"1","content":"Selected Answer: C\nI will go with C, You Must enable AUDITs to get the DDLs, for me \"Enable DocumentDB Events\" means enable the audit events that are by default disabled, check this from aws documentation: \n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\n\nWith Amazon DocumentDB (with MongoDB compatibility), you can audit events that were performed in your cluster. Examples of logged events include successful and failed authentication attempts, dropping a collection in a database, or creating an index. By default, auditing is disabled on Amazon DocumentDB and requires that you opt in to use this feature.\n\nWhen auditing is enabled, Amazon DocumentDB records Data Definition Language (DDL), ...","poster":"rlnd2000"},{"comment_id":"594746","upvote_count":"4","timestamp":"1651278780.0","content":"Selected Answer: A\nA. logs -> CloudWatch\nx B. CloudTrail is to enable auditing not file destinatiom\nx C DocumentDB Events do not support DDL\nx D.","poster":"novice_expert","comments":[{"comment_id":"990465","poster":"aqiao","timestamp":"1693013640.0","upvote_count":"1","content":"DocumentDB events definitely support DDL, see here :https://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\nBut C is still wrong, because above link said clearly: Step 2. Enable Amazon CloudWatch Logs Export"}]},{"poster":"tugboat","comment_id":"555617","upvote_count":"4","timestamp":"1645741920.0","content":"Selected Answer: A\nC is a distraction"},{"comment_id":"534658","upvote_count":"5","timestamp":"1643370360.0","content":"Option A:\n \nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\nWhen auditing is enabled, Amazon DocumentDB records Data Definition Language (DDL), authentication, authorization, and user management events to Amazon CloudWatch Logs. When auditing is enabled, Amazon DocumentDB exports your cluster’s auditing records (JSON documents) to Amazon CloudWatch Logs. You can use Amazon CloudWatch Logs to analyze, monitor, and archive your Amazon DocumentDB auditing events.\n\nDocumentDB Event does not support DDL:\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/managing-events.html\n \"EventCategories\": [\n \"notification\",\n \"failure\",\n \"creation\",\n \"maintenance\",\n \"deletion\",\n \"recovery\",\n \"restoration\",\n \"configuration change\",\n \"read replica\",\n \"backtrack\",\n \"low storage\",\n \"backup\",\n \"availability\",\n \"failover\"","poster":"awsmonster"},{"comment_id":"510338","comments":[{"content":"Events capture changes to things like instance restart, sg changes. it does NOT capture DDLs. Thats AUDITS. C is incorrect","comment_id":"561270","timestamp":"1646462160.0","poster":"RotterDam","upvote_count":"3"}],"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html","timestamp":"1640612400.0","poster":"Shunpin","upvote_count":"2"},{"comment_id":"488527","timestamp":"1638050280.0","comments":[],"upvote_count":"4","content":"Selected Answer: C\nOption C seems correct","poster":"jove"},{"upvote_count":"3","poster":"johnconnor","comment_id":"478012","content":"Shouldn't be C?\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/event-auditing.html\nAuditing Amazon DocumentDB Events\nPDF\nKindle\nRSS\nWith Amazon DocumentDB (with MongoDB compatibility), you can audit events that were performed in your cluster. Examples of logged events include successful and failed authentication attempts, dropping a collection in a database, or creating an index. By default, auditing is disabled on Amazon DocumentDB and requires that you opt in to use this feature.\n\nWhen auditing is enabled, Amazon DocumentDB records Data Definition Language (DDL), authentication, authorization, and user management events to Amazon CloudWatch Logs. When auditing is enabled, Amazon DocumentDB exports your cluster’s auditing records (JSON documents) to Amazon CloudWatch Logs. You can use Amazon CloudWatch Logs to analyze, monitor, and archive your Amazon DocumentDB auditing events.","comments":[{"poster":"SMAZ","comment_id":"509685","content":"Answer is C.","upvote_count":"1","timestamp":"1640534220.0"}],"timestamp":"1636878840.0"}],"question_id":312,"answer":"A","answers_community":["A (69%)","C (31%)"],"answer_ET":"A","choices":{"B":"Enable DocumentDB to export the logs to AWS CloudTrail","C":"Enable DocumentDB Events to export the logs to Amazon CloudWatch Logs","D":"Configure an AWS Lambda function to download the logs using the download-db-log-file-portion operation and store the logs in Amazon S3","A":"Enable DocumentDB to export the logs to Amazon CloudWatch Logs"},"isMC":true,"timestamp":"2021-11-12 13:00:00","question_text":"A marketing company is using Amazon DocumentDB and requires that database audit logs be enabled. A Database Specialist needs to configure monitoring so that all data definition language (DDL) statements performed are visible to the Administrator. The Database Specialist has set the audit_logs parameter to enabled in the cluster parameter group.\nWhat should the Database Specialist do to automatically collect the database logs for the Administrator?","exam_id":22,"unix_timestamp":1636718400,"answer_description":"","question_images":[]},{"id":"wHIfpWExFSahOLzHAKzD","answer_description":"","exam_id":22,"answer":"D","discussion":[{"comment_id":"314959","content":"Must be D","upvote_count":"9","timestamp":"1635902580.0","poster":"LMax"},{"upvote_count":"2","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-from-ibm-db2-on-amazon-ec2-to-aurora-postgresql-compatible-using-aws-dms-and-aws-sct.html","timestamp":"1687594140.0","poster":"adelcold","comment_id":"932337"},{"content":"Selected Answer: D\nrun AWS SCT from the Db2 database to an Aurora DB cluster.\n -> Create a migration assessment report to evaluate the migration compatibility.","poster":"novice_expert","upvote_count":"2","timestamp":"1651323000.0","comment_id":"595007"},{"comment_id":"555719","poster":"tugboat","upvote_count":"4","timestamp":"1645758240.0","content":"Selected Answer: D\nSCT can do the required checks"},{"upvote_count":"2","poster":"myutran","content":"Ans: D","timestamp":"1634819220.0","comment_id":"298709"},{"comment_id":"253296","timestamp":"1634799780.0","upvote_count":"1","content":"D - right answer","poster":"JobinAkaJoe"},{"upvote_count":"3","poster":"firbhat","comment_id":"153956","timestamp":"1633376880.0","content":"ANS: D\n\n• Converts DB/DW schema from source to target (including procedures / views / secondary indexes / FK and constraints)\n• Mainly for heterogeneous DB migrations and DW migrations"},{"upvote_count":"2","timestamp":"1632915180.0","content":"D - SCT","comment_id":"144765","poster":"jnassp1"}],"url":"https://www.examtopics.com/discussions/amazon/view/26760-exam-aws-certified-database-specialty-topic-1-question-57/","isMC":true,"choices":{"B":"Run AWS DMS from the Db2 database to an Aurora DB cluster. Identify the gaps and compatibility of the objects migrated by comparing the row counts from source and target tables.","A":"Perform a logical dump from the Db2 database and restore it to an Aurora DB cluster. Identify the gaps and compatibility of the objects migrated by comparing row counts from source and target tables.","C":"Run native PostgreSQL logical replication from the Db2 database to an Aurora DB cluster to evaluate the migration compatibility.","D":"Run the AWS Schema Conversion Tool (AWS SCT) from the Db2 database to an Aurora DB cluster. Create a migration assessment report to evaluate the migration compatibility."},"timestamp":"2020-07-27 11:09:00","question_id":313,"answers_community":["D (100%)"],"topic":"1","answer_ET":"D","unix_timestamp":1595840940,"question_images":[],"question_text":"A company is looking to move an on-premises IBM Db2 database running AIX on an IBM POWER7 server. Due to escalating support and maintenance costs, the company is exploring the option of moving the workload to an Amazon Aurora PostgreSQL DB cluster.\nWhat is the quickest way for the company to gather data on the migration compatibility?","answer_images":[]},{"id":"JiWzDioA9w4nJQEr5Lbp","choices":{"A":"Use AWS DMS to migrate data from DynamoDB to Amazon DocumentDB","B":"Use Amazon DynamoDB Streams and Amazon Kinesis Data Firehose to push the data into Amazon Redshift","D":"Use DynamoDB Accelerator to offload the reads","C":"Use an Amazon ElastiCache for Redis in front of DynamoDB to boost read performance"},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/25735-exam-aws-certified-database-specialty-topic-1-question-58/","unix_timestamp":1594739640,"answers_community":["D (100%)"],"exam_id":22,"answer_images":[],"answer_ET":"D","answer":"D","timestamp":"2020-07-14 17:14:00","answer_description":"","question_id":314,"discussion":[{"content":"Keywords here are \"without significant development efforts\". DAX is the answer. D","upvote_count":"17","timestamp":"1632383880.0","poster":"learnaws","comment_id":"137661"},{"upvote_count":"1","comment_id":"1007489","timestamp":"1694687940.0","poster":"Pranava_GCP","content":"Selected Answer: D\nD. Use DynamoDB Accelerator to offload the reads\n\nhttps://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html"},{"poster":"backbencher2022","comment_id":"843883","timestamp":"1679239500.0","content":"Selected Answer: D\nD is the correct answer. Per AWS document - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html\n\nFor read-heavy or bursty workloads, DAX provides increased throughput and potential operational cost savings by reducing the need to overprovision read capacity units. This is especially beneficial for applications that require repeated reads for individual keys.","upvote_count":"3"},{"content":"Option D : \nhttps://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html\n\"Applications that are read-intensive, but are also cost-sensitive. With DynamoDB, you provision the number of reads per second that your application requires. If read activity increases, you can increase your tables' provisioned read throughput (at an additional cost). Or, you can offload the activity from your application to a DAX cluster, and reduce the number of read capacity units that you need to purchase otherwise.\"","upvote_count":"2","timestamp":"1669184160.0","comment_id":"724928","poster":"Arun32"},{"timestamp":"1651261980.0","content":"Selected Answer: D\nlarge number of repeated GetItem methods => DAX","poster":"novice_expert","comment_id":"594674","upvote_count":"3"},{"poster":"user0001","timestamp":"1645976580.0","comment_id":"557412","upvote_count":"1","content":"D is the answer since they don't want developments"},{"upvote_count":"2","comment_id":"505724","poster":"jove","content":"Option D. Service cost is not a concern but development cost is.","timestamp":"1640041140.0"},{"content":"Selected Answer: D\nThe problem is various repeated operations, a in memory DB like DAX is the best aproach.","comment_id":"485900","timestamp":"1637753940.0","upvote_count":"3","poster":"GMartinelli"},{"poster":"LMax","comment_id":"314961","content":"D is the easiest option","upvote_count":"2","timestamp":"1636109700.0"},{"comment_id":"298712","upvote_count":"1","poster":"myutran","timestamp":"1636002900.0","content":"Ans: D"},{"comment_id":"253298","content":"We need caching solution to offload reads and reduce the cost.\nDAX is the best caching solution for DynamoDB API calls","upvote_count":"1","timestamp":"1635813360.0","poster":"JobinAkaJoe"},{"timestamp":"1635638640.0","comment_id":"212434","upvote_count":"1","poster":"Ashoks","content":"yes it is D"},{"poster":"AWSCert2020","comment_id":"168276","timestamp":"1635563040.0","content":"D here, but DAX is not cost effective way","upvote_count":"2"},{"content":"B was never in play for me. Redshift is not cheap and that's not what they're asking.\nI'm surprised nobody picked C, textbook case for Caching","timestamp":"1635002160.0","comments":[{"comments":[{"timestamp":"1636260120.0","comment_id":"365194","content":"C and D both caching solution. D is purpose built for DynamoDB, where in C you have to do deal with staleness manually. So D is correct answer.","upvote_count":"1","poster":"Dip11"}],"content":"C is not easy to set up....it needs significant development efforts.\nSo, its not the ideal one choose.\nD could be the best option.","timestamp":"1635256140.0","upvote_count":"1","poster":"cloud4gr8","comment_id":"164372"}],"poster":"[Removed]","upvote_count":"1","comment_id":"159859"},{"timestamp":"1634319360.0","comment_id":"158884","poster":"szmulder","upvote_count":"2","content":"Q, The company wants to control these costs\nAnswer B. I this to using Redshift and kinesis will increase the cost a lot compare to just using DAX.\nSo the answer is D"},{"upvote_count":"2","comment_id":"146605","content":"I think D as well. Dax helps with read intensiveness and cost effectiveness \"without significant development efforts\". \nhttps://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html\n\"Applications that are read-intensive, but are also cost-sensitive. With DynamoDB, you provision the number of reads per second that your application requires. If read activity increases, you can increase your tables' provisioned read throughput (at an additional cost). Or, you can offload the activity from your application to a DAX cluster, and reduce the number of read capacity units that you need to purchase otherwise.\"","poster":"BillyMadison","timestamp":"1633686300.0"},{"comment_id":"145176","content":"D here","poster":"BillyC","upvote_count":"4","timestamp":"1632528960.0"},{"poster":"Mickysingh","comment_id":"134964","comments":[{"poster":"AWSCert2020","content":"Why this ? are you thinking that RS is more cheaper than DynamoDB ? I don't think so","upvote_count":"1","comment_id":"168274","timestamp":"1635290520.0"},{"content":"You are assuming that the company has a Redshift cluster, and that is incorrect, you have to deploy a new Redshift Cluster in prod, and that is not a cheap solution.","timestamp":"1654263300.0","comment_id":"611106","poster":"rlnd2000","upvote_count":"1"}],"upvote_count":"1","content":"Ans B and its right https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/RedshiftforDynamoDB.html\nhttps://medium.com/@lewisdgavin/complex-dynamodb-data-into-redshift-made-easy-336c6329ea25\n\nIngesting DynamoDB data into Redshift\nIf you want to ingest DynamoDB data into Redshift you have a few options.\nThe Redshift Copy command\nBuild a Data Pipeline that copies the data using an EMR job to S3\nExport the DynamoDB data to a file using the AWS CLI and load the flat file into Redshift.\nYou also have the option of DynamoDB streams or a Kinesis firehouse but I’ll save those for a future article.","timestamp":"1632304800.0"}],"question_text":"An ecommerce company is using Amazon DynamoDB as the backend for its order-processing application. The steady increase in the number of orders is resulting in increased DynamoDB costs. Order verification and reporting perform many repeated GetItem functions that pull similar datasets, and this read activity is contributing to the increased costs. The company wants to control these costs without significant development efforts.\nHow should a Database Specialist address these requirements?","question_images":[],"topic":"1"},{"id":"hPDeBCf82B3py1UIcZ47","unix_timestamp":1594740120,"exam_id":22,"answer_description":"","question_id":315,"question_images":[],"isMC":true,"answer_images":[],"answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/25738-exam-aws-certified-database-specialty-topic-1-question-59/","timestamp":"2020-07-14 17:22:00","answer":"D","topic":"1","question_text":"An IT consulting company wants to reduce costs when operating its development environment databases. The company's workflow creates multiple Amazon\nAurora MySQL DB clusters for each development group. The Aurora DB clusters are only used for 8 hours a day. The DB clusters can then be deleted at the end of the development cycle, which lasts 2 weeks.\nWhich of the following provides the MOST cost-effective solution?","choices":{"C":"Use Aurora Replicas. From the master automatic pause compute capacity option, create replicas for each development group, and promote each replica to master. Delete the replicas at the end of the development cycle.","A":"Use AWS CloudFormation templates. Deploy a stack with the DB cluster for each development group. Delete the stack at the end of the development cycle.","D":"Use Aurora Serverless. Restore current Aurora snapshot and deploy to a serverless cluster for each development group. Enable the option to pause the compute capacity on the cluster and set an appropriate timeout.","B":"Use the Aurora DB cloning feature. Deploy a single development and test Aurora DB instance, and create clone instances for the development groups. Delete the clones at the end of the development cycle."},"answers_community":["D (61%)","B (39%)"],"discussion":[{"comment_id":"621702","upvote_count":"10","content":"Selected Answer: D\nIt is a close call between B and D. However, not everyone talks about the actual API call that can be used for Aurora Serverless, which is called as AutoPause. More details in the link.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_ScalingConfiguration.html\nSo it should be D","timestamp":"1656079980.0","poster":"Balki"},{"content":"A - meets the basic requirement. But its not cost effective.\nB - Good option. Saves cost on the storage layer with copy-on-write feature\nC - Meets the requirement, bit not cost effective.\nD - Most cost effective.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.how-it-works.html#aurora-serverless.how-it-works.pause-resume","comment_id":"253307","upvote_count":"6","timestamp":"1635133200.0","poster":"JobinAkaJoe","comments":[{"poster":"Dip11","comment_id":"365198","upvote_count":"1","content":"Spot on","timestamp":"1635880140.0"}]},{"timestamp":"1705155120.0","upvote_count":"1","comment_id":"1121712","content":"Selected Answer: D\nI go for D.\nB looks promising. However, the requirement does not state the data in the clusters is from the same source. Also, keeping the DBs up only 8 hours a day is great for Serverless with auto-pause.","poster":"MultiAZ"},{"content":"A is also viable solution.","upvote_count":"1","poster":"aws2023a","comment_id":"975065","timestamp":"1691452800.0"},{"comment_id":"912058","content":"Selected Answer: D\nDefinitely D since that is the only option that also allows to save on compute by stopping unneeded resources outside of the 8 hours/5 days a week they are needed","upvote_count":"1","poster":"aviathor","timestamp":"1685621580.0"},{"comment_id":"893505","upvote_count":"2","poster":"manig","timestamp":"1683678000.0","content":"- 'Most Cost effective' is the Key\n- Using clone db you can save on the storage cost but still you have to pay for Compute resources which are going to be charged more than 8Hrs.\n- Aurora server less reduces the instance cost which will be the most cost saving"},{"comment_id":"694649","upvote_count":"1","poster":"RBSK","timestamp":"1665741180.0","content":"Selected Answer: B\nMost-Cost effective solution is the key here. REfer to https://aws.plainenglish.io/aurora-database-clones-what-they-are-and-when-to-use-them-b82be9d60309 \n \nPricing\nNow let’s talk about the pricing for creating and using Amazon Aurora database clones. We get billed per hour for the database instances provisioned as part of the clone database. The pricing is based on the DB instance class that we select for the clone. We are also charged for any addition storage that we use as we make edits to the clone database.\nOption D is full copy of the DB and definitely will cost more than Option B.","comments":[{"content":"storage cost is much lower than server cost, so option D is cheaper than B","timestamp":"1675778400.0","comment_id":"800969","upvote_count":"2","poster":"im_not_robot"}]},{"timestamp":"1657454940.0","comment_id":"629550","upvote_count":"4","content":"Answer should be D","poster":"Chirantan"},{"comment_id":"619767","content":"Selected Answer: D\nAnswer:D","poster":"ryuhei","timestamp":"1655807640.0","upvote_count":"3"},{"content":"Selected Answer: B\nOption B is correct.\nQuestion talks about destroying the cluster after 2 weeks, With aurora you will not pay for the compute capacity but you will pay for the storage capacity. If Option D had the delete option, I could have happily choose Option D. The advantage of Option B, you are not paying additional Storage as clone use the same storage + delta changes.","upvote_count":"1","comment_id":"604637","poster":"Radhaghosh","timestamp":"1653082260.0"},{"upvote_count":"2","content":"Selected Answer: D\nIt is necessary to support operation for 8 hours.","comment_id":"597903","timestamp":"1651876620.0","poster":"[Removed]"},{"timestamp":"1651280160.0","content":"Selected Answer: B\nsingle dev & test instanace\n-> Aurora clone \n-> delete clones at end of dev cycle","poster":"novice_expert","comment_id":"594758","upvote_count":"2"},{"poster":"randss","content":"Even if u pause serverless cluster storage, doesnt address the destroyed after 2 weeks requirement. I go with B.","timestamp":"1650008280.0","comment_id":"586232","upvote_count":"2"},{"timestamp":"1645743000.0","content":"Selected Answer: B\nAmazon Aurora now allows you to create clones between Aurora Serverless v1 and provisioned Aurora DB clusters to enable quick sharing of data.","upvote_count":"4","comment_id":"555628","poster":"tugboat"},{"content":"I vote for B:\n\nThere isn't an \"option to pause the compute capacity on the cluster and set an appropriate timeout.\" in the RDS console for Serverless.","timestamp":"1643372340.0","upvote_count":"1","poster":"awsmonster","comment_id":"534677"},{"content":"Selected Answer: B\nI prefer B. \nAurora Serverless is not compatible to all Aurora provisioned engine version. However, you can do clone with most engine version. Meanwhile, I also consider the performance while restoring snapshot to Aurora Serverless.","upvote_count":"3","timestamp":"1640614560.0","poster":"Shunpin","comment_id":"510366"},{"timestamp":"1640112240.0","upvote_count":"1","comment_id":"506362","poster":"jove","content":"It is D"},{"upvote_count":"1","content":"Answer: D","timestamp":"1636046460.0","poster":"guru_ji","comment_id":"438599"},{"comment_id":"380818","timestamp":"1635913680.0","comments":[{"content":"I agree with you.\nAnswer: D","timestamp":"1636023900.0","upvote_count":"2","comment_id":"438598","poster":"guru_ji"}],"upvote_count":"5","poster":"Suresh108","content":"B or D???\n\nB - if you have 100TB database then it is going to be beneficial on storage cost savings. \nD - best on compute cost savings, keeping it down when not required to run the resources. \n\nI think Question gives more emphasizes on 8 hour run time of resources, I am leaning towards D."},{"comment_id":"314967","content":"The most optimal is D in the list provided","timestamp":"1635743940.0","upvote_count":"3","poster":"LMax"},{"poster":"myutran","timestamp":"1635269400.0","comment_id":"298716","content":"Ans: D","upvote_count":"2"},{"timestamp":"1634924700.0","poster":"Ashoks","content":"It is D, to use only for 8 hours a day","upvote_count":"2","comment_id":"212438"},{"upvote_count":"3","content":"D here, the AutoPause mode is described here: https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_ScalingConfiguration.html","comment_id":"168279","poster":"AWSCert2020","timestamp":"1634698860.0"},{"content":"Agreed with Mickysingh. B. \nA needs to dump and restore data.\nC needs to touch the master and promote.\nAurora Serverless does not have options to pause compute capacity, so D dropped.\nYes the link shows D is possible, but it does not explain how to 'pause compute capacity' and does not implies it's the most cost-effective choice.","comment_id":"159149","comments":[{"comment_id":"685077","content":"X D. restore to serverless from snapshot take long \nbut serverless can pause : \nAurora Serverless allows autoscaling of the resources based on the capacity (minimum and maximum) defined. It can scale up to the max capacity and scale down to zero when there \nis no activity for a 5 minute period. As described earlier, Aurora provides an option to scale down to zero capacity. You can choose an option to pause when there is no activity for a \ngiven amount of time. By default, it is 5 minutes.","timestamp":"1664745780.0","upvote_count":"1","poster":"Jiang_aws1"}],"upvote_count":"1","poster":"zanhsieh","timestamp":"1633348620.0"},{"timestamp":"1633198860.0","comment_id":"158466","upvote_count":"2","content":"Answer is D","poster":"Ebi"},{"poster":"BillyC","content":"D here","timestamp":"1632931740.0","upvote_count":"4","comment_id":"145857"},{"comments":[{"comment_id":"142346","poster":"steves","content":"I think it's D, Use Aurora Serverless. according to the above mentioned link.","upvote_count":"5","timestamp":"1632706440.0"},{"content":"Your link indicates D, and I agree with your link that the right answer is D.","upvote_count":"5","timestamp":"1633110180.0","comment_id":"150058","poster":"BillyMadison"}],"comment_id":"134974","content":"Answer B is correct https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.use-cases\n\nignore the 2 weeks requirement and focus on 8 hours requirement in question which is the key to the answer.","poster":"Mickysingh","timestamp":"1632571140.0","upvote_count":"3"}]}],"exam":{"id":22,"numberOfQuestions":359,"isMCOnly":false,"isImplemented":true,"isBeta":false,"name":"AWS Certified Database - Specialty","provider":"Amazon","lastUpdated":"11 Apr 2025"},"currentPage":63},"__N_SSP":true}