{"pageProps":{"questions":[{"id":"7VrCCo5Kb5gEgRpfhUfn","url":"https://www.examtopics.com/discussions/amazon/view/5553-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"comments":[{"timestamp":"1634792700.0","poster":"SD13","comment_id":"330640","content":"Option B don't provide a gradual deployment option. Correct Answer is D","upvote_count":"2","comments":[{"poster":"zolthar_z","content":"that is wrong, https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html","comment_id":"368799","upvote_count":"2","timestamp":"1636036080.0"}]}],"poster":"donathon","content":"B\nhttps://aws-quickstart.s3.amazonaws.com/quickstart-trek10-serverless-enterprise-cicd/doc/serverless-cicd-for-the-enterprise-on-the-aws-cloud.pdf \nhttps://aws.amazon.com/quickstart/architecture/serverless-cicd-for-enterprise/","timestamp":"1632161760.0","comment_id":"12732","upvote_count":"28"},{"comment_id":"12070","content":"I would go for D, SAM only works in CLI and the team is currently using the AWS Management Console to provision Amazon API Gateway, AWS Lambda, and Amazon DynamoDB resources.","comments":[{"comment_id":"16185","upvote_count":"11","poster":"chaudh","timestamp":"1632618780.0","content":"Because team is using Console --> need to improve for future automation deployment and not using Console any more. B is my choice."},{"timestamp":"1635122100.0","poster":"sarah_t","content":"There is no requirement to use the console in the future.","upvote_count":"3","comment_id":"333329"},{"comments":[{"comment_id":"93828","timestamp":"1632934080.0","upvote_count":"2","content":"check this out - https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/\nI will go for D","comments":[{"content":"Actually it's still sam.\nCodeDeploy only controls the traffic.","timestamp":"1635546540.0","comment_id":"342534","poster":"01037","upvote_count":"1"}],"poster":"VrushaliD"}],"comment_id":"34904","content":"Codedeploy will not deploy Lambda functions. So the answer is B.","poster":"arunkumar","upvote_count":"4","timestamp":"1632729960.0"}],"upvote_count":"6","timestamp":"1632103740.0","poster":"huhupai"},{"content":"Selected Answer: B\nSolution D uses CodeCommit, CodeBuild, and CodeDeploy to deploy the serverless application. However, it does not use SAM to define the application. This means that the development team would need to write their own YAML template, which can be complex.\nTherefore, the best solution is B.\n\nHere are some additional benefits of using SAM and CodePipeline:\n\nThey are easy to use and manage.\nThey are reliable and secure.\nThey are cost-effective.","comment_id":"926297","upvote_count":"1","timestamp":"1687037940.0","poster":"SkyZeroZx"},{"timestamp":"1673264940.0","poster":"hollie","upvote_count":"2","content":"why not C?","comment_id":"770315"},{"content":"Selected Answer: B\nBased on all comments","comment_id":"685700","timestamp":"1664817180.0","poster":"dmscountera","upvote_count":"2"},{"content":"B is the answer here is the link to end the discussion.\n\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html","comment_id":"658026","upvote_count":"1","timestamp":"1662175320.0","poster":"AYANtheGLADIATOR"},{"comment_id":"544965","upvote_count":"1","poster":"peddyua","timestamp":"1644545700.0","content":"Selected Answer: B\nB. SAM for serveles is recommended by AWS \nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"},{"upvote_count":"1","comment_id":"527663","content":"B - If you want to deploy your AWS SAM application gradually rather than all at once, you can specify deployment configurations that AWS CodeDeploy provides.","timestamp":"1642606080.0","poster":"tkanmani76"},{"comment_id":"493926","content":"B is right!","upvote_count":"2","timestamp":"1638647640.0","poster":"AzureDP900"},{"comment_id":"448753","timestamp":"1636086780.0","poster":"Kopa","upvote_count":"1","content":"going for B"},{"poster":"WhyIronMan","comment_id":"409735","timestamp":"1636067220.0","upvote_count":"3","content":"I'll go with B"},{"timestamp":"1635883380.0","content":"It's definitely B","comment_id":"345028","poster":"Waiweng","upvote_count":"3"},{"timestamp":"1635752460.0","content":"B.\nD (not correct), CodeDeploy can NOT be used to deploy lambda","comment_id":"342536","comments":[{"timestamp":"1636063080.0","content":"Your statement is just wrong. CodeDeploy can be used to deploy lambda. But the answer is still B","poster":"MrCarter","comment_id":"394910","upvote_count":"2"}],"poster":"01037","upvote_count":"1"},{"poster":"kiev","content":"My answer is B. Serverless application points to Sam.","timestamp":"1634628720.0","upvote_count":"1","comment_id":"293693"},{"timestamp":"1634423820.0","poster":"Kian1","upvote_count":"2","comment_id":"290677","content":"will go with B"},{"content":"I go with B too","timestamp":"1634068860.0","upvote_count":"4","poster":"Ebi","comment_id":"282441"},{"content":"B for sure.","poster":"sanjaym","comment_id":"268029","upvote_count":"2","timestamp":"1634060880.0"},{"timestamp":"1633958220.0","poster":"T14102020","upvote_count":"1","comment_id":"242523","content":"Answer is B. SAM"},{"timestamp":"1633448160.0","poster":"jackdryan","upvote_count":"3","comment_id":"230043","content":"I'll go with B"},{"upvote_count":"1","poster":"Bulti","timestamp":"1633231860.0","content":"Answer is B. Always use SAM for Serverless deployment","comment_id":"229352"},{"poster":"fullaws","timestamp":"1633219620.0","comment_id":"149603","upvote_count":"2","content":"B is correct"},{"content":"B - 100%","timestamp":"1633180380.0","upvote_count":"3","poster":"NikkyDicky","comment_id":"132311"},{"comment_id":"106914","timestamp":"1633048020.0","content":"After reading it again, I think the answer should be B","upvote_count":"1","poster":"VrushaliD"},{"timestamp":"1632446940.0","poster":"Moon","upvote_count":"4","content":"I would support \"B\".\nSAM is better option","comment_id":"13892"},{"upvote_count":"3","timestamp":"1632412080.0","content":"B only","poster":"SivaG","comment_id":"12795"},{"poster":"awsec2","comment_id":"12333","content":"My view is b","timestamp":"1632146160.0","upvote_count":"3"}],"answer_description":"","isMC":true,"unix_timestamp":1569114240,"timestamp":"2019-09-22 03:04:00","answer":"B","question_images":[],"exam_id":32,"answer_images":[],"answers_community":["B (100%)"],"choices":{"A":"Use AWS CloudFormation with a Lambda-backed custom resource to provision API Gateway. Use the AWS::DynamoDB::Table and AWS::Lambda::Function resources to create the Amazon DynamoDB table and Lambda functions. Write a script to automate the deployment of the CloudFormation template.","C":"Use AWS CloudFormation to define the serverless application. Implement versioning on the Lambda functions and create aliases to point to the versions. When deploying, configure weights to implement shifting traffic to the newest version, and gradually update the weights as traffic moves over.","D":"Commit the application code to the AWS CodeCommit code repository. Use AWS CodePipeline and connect to the CodeCommit code repository. Use AWS CodeBuild to build and deploy the Lambda functions using AWS CodeDeploy. Specify the deployment preference type in CodeDeploy to gradually shift traffic over to the new version.","B":"Use the AWS Serverless Application Model to define the resources. Upload a YAML template and application files to the code repository. Use AWS CodePipeline to connect to the code repository and to create an action to build using AWS CodeBuild. Use the AWS CloudFormation deployment provider in CodePipeline to deploy the solution."},"question_id":426,"answer_ET":"B","question_text":"A Development team is deploying new APIs as serverless applications within a company. The team is currently using the AWS Management Console to provision\nAmazon API Gateway, AWS Lambda, and Amazon DynamoDB resources. A Solutions Architect has been tasked with automating the future deployments of these serverless APIs.\nHow can this be accomplished?","topic":"1"},{"id":"u19vr01scblQk04QdMBP","question_text":"The company Security team requires that all data uploaded into an Amazon S3 bucket must be encrypted. The encryption keys must be highly available and the company must be able to control access on a per-user basis, with different users having access to different encryption keys.\nWhich of the following architectures will meet these requirements? (Choose two.)","question_images":[],"choices":{"C":"Use Amazon S3 server-side encryption with customer-managed keys, and use AWS CloudHSM to manage the keys. Use CloudHSM client software to control access to the keys that are generated.","D":"Use Amazon S3 server-side encryption with customer-managed keys, and use two AWS CloudHSM instances configured in high-availability mode to manage the keys. Use the CloudHSM client software to control access to the keys that are generated.","A":"Use Amazon S3 server-side encryption with Amazon S3-managed keys. Allow Amazon S3 to generate an AWS/S3 master key, and use IAM to control access to the data keys that are generated.","B":"Use Amazon S3 server-side encryption with AWS KMS-managed keys, create multiple customer master keys, and use key policies to control access to them.","E":"Use Amazon S3 server-side encryption with customer-managed keys, and use two AWS CloudHSM instances configured in high-availability mode to manage the keys. Use IAM to control access to the keys that are generated in CloudHSM."},"answer_images":[],"discussion":[{"timestamp":"1632307260.0","content":"My preference \"B\" & \"D\".\nA: customer can not control the keys!\nB: AWS-KMS managed keys, allow the user to create Master keys, and control them. It is high available as it is a managed service by AWS.\nC: CloudHSM can be high available by including a second instance in different AZ.\nD: Meet the requirement of management and high availability.\nE: Managing the keys by CloudHSM client, not IAM user!!","poster":"Moon","comments":[{"upvote_count":"10","poster":"shammous","content":"CloudHSM instance? CloudHSM is a service so D and E are ruled out.\nB and C would be good options.","timestamp":"1634170800.0","comment_id":"279817"}],"upvote_count":"30","comment_id":"13891"},{"poster":"Xiaoyao2000","comment_id":"11233","content":"I would choose b d","timestamp":"1632078960.0","upvote_count":"7"},{"poster":"SkyZeroZx","comments":[{"upvote_count":"1","comment_id":"926300","poster":"SkyZeroZx","timestamp":"1687038120.0","content":"The other solutions do not meet all of the requirements.\n\nSolution A uses Amazon S3-managed keys, which are not as highly available as AWS KMS-managed keys. Additionally, IAM cannot be used to control access to the data keys that are generated.\n\nSolution C uses customer-managed keys that are stored in AWS CloudHSM. However, it does not use IAM to control access to the keys that are generated.\n\nSolution E uses customer-managed keys that are stored in two AWS CloudHSM instances configured in high-availability mode. However, it uses IAM to control access to the keys that are generated. This is not necessary, as the CloudHSM client software can be used to control access to the keys."}],"timestamp":"1687038120.0","comment_id":"926299","upvote_count":"1","content":"Selected Answer: BD\nThe correct answers are B and D.\n\nSolution B meets the requirements because it uses AWS KMS-managed keys, which are highly available and can be controlled on a per-user basis using key policies.\n\nSolution D also meets the requirements because it uses customer-managed keys that are stored in two AWS CloudHSM instances configured in high-availability mode. The CloudHSM client software can be used to control access to the keys that are generated."},{"content":"Selected Answer: AE\nQuestion only asks to control access to the keys but not management of the keys. So keys can be managed by AWS as long as access to the keys can be restricted. \nA: seems perfectly valid and is highly available.\nB: we do not need control over the master key but access restriction \n per user.\nD: We need access restriction on a per user basis. Client does not fulfill the per user per user. \nE: highly available and one can use IAM to restrict access on a per user base. fulfills requirement.\n\nBased in that I do not see the question asking for control over key generation but only for access restriction on user base: for me its AE","timestamp":"1671164280.0","poster":"hobokabobo","comment_id":"746796","upvote_count":"1"},{"poster":"tomosabc1","timestamp":"1667389980.0","comment_id":"709776","content":"Selected Answer: BD\nI have to say that this is a really bad question with several inaccurate wordings.\n\nFirstly, the \"KMS-managed keys\" in option B actually refers to KMS keys stored in KMS(i.e. SSE-KMS), customer managed keys or AWS managed keys. Otherwise B can't be right, because the key policy of AWS managed keys cannot be changed. So to make B a valid answer, \"KMS-managed keys\" have to refer to customer managed keys, which is the same as CDE, but using a completely different wording. B have to be right answer, otherwise there aren't 2 correct answers for this question.","upvote_count":"3","comments":[{"timestamp":"1667390340.0","poster":"tomosabc1","content":"Compared with C, D looks more like a correct answer, because C makes no mention of HA, which is not enabled by default(We have to have two HSMs in CloudHSM Cluster to make it HA)","comment_id":"709788","upvote_count":"1"},{"timestamp":"1667390160.0","comment_id":"709780","poster":"tomosabc1","upvote_count":"1","content":"Secondly, \"CloudHSM instances\" and \"CloudHSM instances configured in high availability mode\" is not a real thing.\n\n\"High availability is provided automatically when you have at least two HSMs in your CloudHSM Cluster. No additional configuration is required. In the event an HSM in your cluster fails, it will be replaced automatically, and all clients will be updated to reflect the new configuration without interrupting any processing\"\n\n\"Please note it is your responsibility to architect your cluster for high availability. AWS strongly recommends that you use CloudHSM Clusters with two or more HSMs in separate Availability Zones.\"\n\nhttps://aws.amazon.com/cloudhsm/faqs/"},{"poster":"tomosabc1","upvote_count":"3","comment_id":"709777","content":"\"You have three mutually exclusive options, depending on how you choose to manage the encryption keys.\n\nServer-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\n.....\nServer-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS)\n.... Additionally, you can create and manage customer managed keys or use AWS managed keys that are unique to you, your service, and your Region.\n...\nServer-Side Encryption with Customer-Provided Keys (SSE-C)\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html","timestamp":"1667389980.0"}]},{"upvote_count":"1","content":"Selected Answer: BE\nDon't know why the community disregards E. \nYou can create KMS key with CloudHSM-backed custom-key store. The keys can then be managed by regular IAM and key policies. https://docs.aws.amazon.com/kms/latest/developerguide/manage-cmk-keystore.html\n\nNo need to use CloudHSM client, because KMS connects to CloudHSM on our behalf. https://docs.aws.amazon.com/kms/latest/developerguide/disconnect-keystore.html","comment_id":"693581","timestamp":"1665634020.0","poster":"[Removed]"},{"upvote_count":"1","poster":"dmscountera","content":"Selected Answer: BD\nBased on all comments","comment_id":"685702","timestamp":"1664817420.0"},{"comment_id":"676760","upvote_count":"1","poster":"JohnPi","timestamp":"1663907880.0","content":"Selected Answer: BD\nIn AWS CloudHSM, use any of the following to manage keys on the HSMs in your cluster:\n\n- PKCS #11 library\n- JCE provider\n- CNG and KSP providers\n- key_mgmt_util"},{"poster":"KengL","timestamp":"1654421580.0","content":"CD\nB is wrong as multiple doesn't mean each user has unique key","upvote_count":"2","comment_id":"611752"},{"poster":"bobsmith2000","content":"A doesn't fit the bill at all.\nC is wrong because we need more than 1 HSM instance in a cluster (https://docs.aws.amazon.com/cloudhsm/latest/userguide/clusters.html)\nE is wrong because CloudHSM doesn't support IAM (https://docs.aws.amazon.com/cloudhsm/latest/userguide/hsm-users.html)\nWhat we have left? B and D.\nNow referring to the \nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html,\nit becomes clear, that we can use either a \"Customer managed key\" or a \"AWS managed key\" which in turn creates \"Data Keys\".\nTaking this into account, B doesn't make sence at all!\nB) \"Use Amazon S3 server-side encryption with AWS KMS-managed keys, create multiple customer master keys\".\nIf we choose \"AWS KMS-managed key\" there's no such thing as \"customer master keys\". Moreover according to the link above: \"However, you cannot manage these AWS KMS-managed keys, rotate them, or change their key policies.\" which violates the stipulations.\nSo B is wrong as well.\nThe only answer which makes sence and fits the stipulation as D.\nCorrect me if I'm wrong.","comments":[{"upvote_count":"2","timestamp":"1654671720.0","poster":"bobsmith2000","content":"Second run.\nC doesn't mention number of instances, so it might work.\nWith B it's not possible to change key policy.\n\nSo for me it's CD","comment_id":"613123"}],"timestamp":"1653570240.0","comment_id":"607664","upvote_count":"2"},{"poster":"pititcu667","content":"Selected Answer: BD\ne is wrong because with IAM you can only do:\n \"cloudhsm:DescribeClusters\",\n \"cloudhsm:DescribeBackups\",\n \"cloudhsm:CreateCluster\",\n \"cloudhsm:CreateHsm\",\n \"cloudhsm:RestoreBackup\",\n \"cloudhsm:CopyBackupToRegion\",\n \"cloudhsm:InitializeCluster\",\n \"cloudhsm:ListTags\",\n \"cloudhsm:TagResource\",\n \"cloudhsm:UntagResource\",","upvote_count":"1","timestamp":"1642102020.0","comment_id":"523038"},{"poster":"cldy","comment_id":"513998","upvote_count":"1","timestamp":"1640943660.0","content":"B and D."},{"timestamp":"1640183880.0","upvote_count":"1","content":"B & C for me. As well as creating 2 instances for HA, you will also need to manage keys using CloudHSM software. Not IAM","comment_id":"507171","poster":"Ni_yot","comments":[{"content":"ignore that B and D","timestamp":"1640183940.0","upvote_count":"3","comment_id":"507173","poster":"Ni_yot"}]},{"timestamp":"1638647820.0","content":"Should be B,D","upvote_count":"1","comment_id":"493931","poster":"AzureDP900"},{"poster":"acloudguru","comment_id":"490507","timestamp":"1638259260.0","upvote_count":"2","content":"KMS 99.9%, HSM 99.95%, if B is ok, why need two HSM in D or E?"},{"upvote_count":"1","timestamp":"1638031500.0","comment_id":"488292","content":"B and D, not E as CloudHSM can be used only with HSM client\n\nQ: How do I set up a high availability (HA) configuration?\n\nHigh availability is provided automatically when you have at least two HSMs in your CloudHSM Cluster. No additional configuration is required. In the event an HSM in your cluster fails, it will be replaced automatically, and all clients will be updated to reflect the new configuration without interrupting any processing. Additional HSMs can be added to the cluster via the AWS API or SDK, increasing availability without interrupting your application.","poster":"Sachhi"},{"content":"Today I tried to create Cloud HSM and there is an option to choose subnet (upto 3 as I am in SGP region). But I am able to choose only one region. So I can create HSM with 1 instance. Although Cloud HSM is managed service. Creating HSM with 1 won't give HA. So, I would choose D as one of the answer over C.","upvote_count":"1","timestamp":"1636212300.0","comment_id":"456069","poster":"StelSen"},{"upvote_count":"1","comments":[{"comment_id":"476137","upvote_count":"2","timestamp":"1636627080.0","poster":"sashenka","content":"Please note it is your responsibility to architect your cluster for high availability. AWS strongly recommends that you use CloudHSM Clusters with two or more HSMs in separate Availability Zones. You can learn more about recommended best practices in our online documentation.\nhttps://aws.amazon.com/cloudhsm/faqs/"}],"timestamp":"1636026900.0","poster":"nisoshabangu","content":"B and C, changed to C because AWS Cloud HSM is a highly available managed service.","comment_id":"440731"},{"content":"Question for those who choose D as answer: did you try to create two AWS CloudHSM instances in HA mode? :-D the only option in console is to create a CloudHSM cluster :-) \"It is a fully-managed service that automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups. CloudHSM also enables you to scale quickly by adding and removing HSM capacity on-demand, with no up-front costs.\"","upvote_count":"3","poster":"mericov","comments":[{"poster":"joe16","content":"\"You can create a cluster that has from 1 to 28 HSMs (the default limit is 6 HSMs per AWS account per AWS Region). You can place the HSMs in different Availability Zones in an AWS Region. Adding more HSMs to a cluster provides higher performance. Spreading clusters across Availability Zones provides redundancy and high availability.\"\nThis means you need to configure 2 instances at least to keep the cluster highly available.\nI would go with B and D.","comment_id":"452101","upvote_count":"1","timestamp":"1636079160.0"}],"timestamp":"1635955560.0","comment_id":"413210"},{"poster":"WhyIronMan","content":"I'll go with B,D","comment_id":"409738","timestamp":"1635885360.0","upvote_count":"2"},{"comment_id":"406700","upvote_count":"1","content":"BD Correct","timestamp":"1635702360.0","poster":"Akhil254"},{"poster":"student2020","timestamp":"1635597720.0","upvote_count":"1","comment_id":"405661","comments":[{"upvote_count":"2","comment_id":"405665","poster":"student2020","timestamp":"1635612480.0","content":"But I think B and D are more accurate as per the wording of the answers."}],"content":"D an E are valid\n\nYou can create symmetric CMKs with key material generated by AWS KMS in your custom key store. Then use the same techniques to view and manage the CMKs in your custom key store that you use for CMKs in the AWS KMS key store. You can control access with IAM and key policies, create tags and aliases, enable and disable the CMKs, and schedule key deletion. You can use the CMKs for cryptographic operations and use them with AWS services that integrate with AWS KMS.\n\nIn addition, you have full control over the AWS CloudHSM cluster, including creating and deleting HSMs and managing backups. You can use the AWS CloudHSM client and supported software libraries to view, audit, and manage the key material for your CMKs. While the custom key store is disconnected, AWS KMS cannot access it, and users cannot use the CMKs in the custom key store for cryptographic operations. This added layer of control makes custom key stores a powerful solution for organizations that require it.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html"},{"content":"It's B,D for HA","comment_id":"345032","upvote_count":"2","timestamp":"1635505320.0","poster":"Waiweng"},{"poster":"sarah_t","upvote_count":"3","timestamp":"1635257460.0","content":"Everyone agrees on B.\nI think the second is C: \"CloudHSM is standards-compliant and enables you to export all of your keys to most other commercially-available HSMs, subject to your configurations. It is a fully-managed service that automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups. \"\nhttps://aws.amazon.com/cloudhsm/","comment_id":"333334"},{"content":"Technically there are 2 options, either use the (default) AWS managed CMK or the customisable customer managed CMK. In this case, Option B fulfills the requirements of \"having different users authenticated with a different set of keys\"\n\nTheoretically speaking, when you initiate a cloudHSM service (region-based), you have already initiated a cloudHSM cluster with more than 1 instance which should already fulfilled HA. the question requires 2 separate clusters in different region so option D is the closest we get.","comment_id":"312809","upvote_count":"2","poster":"Pupu86","timestamp":"1635134160.0"},{"poster":"Kian1","upvote_count":"2","comment_id":"290679","timestamp":"1634968800.0","content":"go with BD"},{"comment_id":"282447","timestamp":"1634892360.0","poster":"Ebi","upvote_count":"3","content":"I go with BD"},{"comment_id":"254520","content":"B & C\nA - S3 is managing the keys - so no\nB - we all agree becasue manageed by KMS with multipel keys\nC - CLoud HSM is a service - not to be deployed on Instance. CLient get deployed on instance \nD - refer to C - there is no HSM instance\nE - refer to C - there is no HSM instance","timestamp":"1634163480.0","comments":[{"upvote_count":"1","comment_id":"279820","poster":"shammous","content":"Good point about D and E.","timestamp":"1634473440.0"},{"comment_id":"281819","upvote_count":"1","poster":"CanBe","content":"HSM instances exist. Please read my comment below.","timestamp":"1634678280.0"},{"poster":"CanBe","content":"The CloudHSM FAQ says this at the very beginning \"The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud.\"\nhttps://aws.amazon.com/cloudhsm/faqs/","timestamp":"1634506320.0","comment_id":"281815","upvote_count":"3"}],"poster":"vipgcp","upvote_count":"5"},{"upvote_count":"2","comment_id":"242529","content":"Correct answer is BD. Client should control keys","timestamp":"1634102760.0","poster":"T14102020"},{"content":"I'll go with B,D","upvote_count":"3","poster":"jackdryan","timestamp":"1634022480.0","comment_id":"230046"},{"poster":"Bulti","comment_id":"229755","timestamp":"1633976520.0","upvote_count":"2","content":"Sorry I meant B&D."},{"timestamp":"1633898520.0","content":"A &D- Not E but D because we are talking about customer managed keys which is outside the control of AWS and therefore cannot use IAM to access those keys. We need to use the role based access control feature offered by the Cloud HSM. IAM would come into play if you are using KMS custom key store which is connected to a Cloud HSM cluster. Just like the default key store, access to keys in the custom key store can be managed using IAM.","comment_id":"229754","poster":"Bulti","upvote_count":"2"},{"content":"I will think it is B,E\nB all agree.\nNormally access to HSM keys are managed via CloudHSM client when keys are accessed by application directly, but when CloudHSM is used with KMS, CloudHSM is defined as custom keystore for KMS. Access to CloudHSM keys is granted to KMS and KMSa uses these keys when required on behalf of user. For purposes of encryption and decryption using SSE-KMS with imported keys or with custom keystore are same. Access to keys is controlled via key policies in both cases.So second option should be E\n\nIf we use SSE-C rather than SSE-KMS with custom keystore, then D makes more sense as in that case application accessing the objects in S3 will need to provide key for encryption and decryption. In this case application will need to use CloudHSM client to get keys from HSM.\n\nSSE-C is much complicated solution than using SSE-KMS with custom keystore, as application needs to crreate/maintain and manage records of encryption/data key used for encryption for each file. SO in my view E is better choice than D.","comments":[{"timestamp":"1633871700.0","content":"You can not control with IAM the access to CloudHSM keys, so E is not correct....\nAs using CloudHSM and HA it should be BD","upvote_count":"4","poster":"ipindado2020","comment_id":"182677"}],"comment_id":"167484","poster":"nameisreqd","upvote_count":"2","timestamp":"1633759800.0"},{"upvote_count":"2","comment_id":"149605","content":"B and D is correct","poster":"fullaws","timestamp":"1633574100.0"},{"upvote_count":"2","comment_id":"134438","poster":"NikkyDicky","timestamp":"1633521780.0","content":"BD more likely"},{"poster":"Shawn1","upvote_count":"1","timestamp":"1633312920.0","comment_id":"113933","comments":[{"timestamp":"1633383840.0","content":"Sorry. agree with pgarg. E is not correct but D is. so answer is B, D. because \"role-based access control is inherent in the design of CloudHSM\" (https://aws.amazon.com/cloudhsm/faqs/) so the access control is not done by IAM.","comment_id":"113942","poster":"Shawn1","upvote_count":"2"}],"content":"Answer B, E. Both addressed the HA and IAM Control."},{"comments":[{"content":"E is not correct.D is correct.AWS CloudHSM provides you access to your HSMs over a secure channel to create users and set HSM policies. The encryption keys that you generate and use with CloudHSM are accessible only by the HSM users that you specify. AWS has no visibility or access to your encryption keys.","poster":"pgarg","comment_id":"105146","timestamp":"1633108080.0","upvote_count":"1"},{"content":"Unlike most AWS services and resources, you do not use AWS Identity and Access Management (IAM) users or IAM policies to access resources within your cluster. Instead, you use HSM users directly on the hardware security module (HSM) with AWS CloudHSM. The HSM authenticates each HSM user by means of credentials that you define and manage. Each HSM user has a type that determines which operations you can perform on the HSM as that user. https://docs.aws.amazon.com/cloudhsm/latest/userguide/hsm-users.html","poster":"DashL","comment_id":"394083","timestamp":"1635530400.0","upvote_count":"2"}],"comment_id":"104667","poster":"jv1","upvote_count":"2","timestamp":"1633092240.0","content":"B E. As per link IAM policies can be used to control access to your AWS KMS custom key store and your AWS CloudHSM cluster.\nhttps://docs.amazonaws.cn/en_us/kms/latest/developerguide/authorize-key-store.html"},{"timestamp":"1633057740.0","comment_id":"53137","upvote_count":"2","poster":"Vyasc","content":"I think choice between D & E is a bit devious, E is talking about generating the keys in HSM and then uploading that to KMS and controlling access to this uploaded key using IAM. D is probably not correct because creating users in HSM is more involved, it is best to minimize that access to security team only."},{"upvote_count":"2","poster":"amog","content":"Should be B,D","timestamp":"1633016220.0","comment_id":"50046"},{"comment_id":"30510","poster":"dojo","upvote_count":"2","content":"Answer would be BD","timestamp":"1633001580.0"},{"content":"BD \nhttp://websecuritypatterns.com/blogs/2018/03/01/encryption-and-key-management-in-aws-kms-vs-cloudhsm-myths-and-realities/","timestamp":"1632758340.0","comment_id":"29135","poster":"9Ow30","upvote_count":"5"},{"timestamp":"1632653820.0","comment_id":"24246","content":"BD look more relevant","poster":"examacc","upvote_count":"5"},{"upvote_count":"2","poster":"examacc","comment_id":"24245","content":"A is wrong. you cannot control S3 Managed keys","comments":[{"upvote_count":"1","content":"I think S3 Managed keys are referring to AWS managed keys.","poster":"CKW","timestamp":"1635065520.0","comment_id":"307769"}],"timestamp":"1632559200.0"},{"poster":"manhmaluc","comment_id":"13977","upvote_count":"3","content":"how to encrypt/decrypt the data by using 2 different keys?\nbtw, I think BD is best suitable :D","timestamp":"1632418440.0"},{"timestamp":"1632131460.0","poster":"donathon","content":"AC\nI am not sure on this one. But based on the fact that B, C & E all has either 2 CloudHSM or 2 CMK, it does not make sense. How can the users access the data using 2 different keys?","comments":[{"content":"donathon - please, before answering read the docs or provision the resources","upvote_count":"1","timestamp":"1635926460.0","poster":"DerekKey","comment_id":"413114"}],"comment_id":"12828","upvote_count":"4"}],"timestamp":"2019-09-16 08:47:00","exam_id":32,"answer":"BD","question_id":427,"answer_ET":"BD","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/5222-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","unix_timestamp":1568616420,"topic":"1","answers_community":["BD (78%)","11%","11%"]},{"id":"hi0srLezCt7xBGcDSGJR","answer_images":[],"isMC":true,"answers_community":["A (100%)"],"question_text":"A company runs a public-facing application that uses a Java-based web service via a RESTful API. It is hosted on Apache Tomcat on a single server in a data center that runs consistently at 30% CPU utilization. Use of the API is expected to increase by 10 times with a new product launch. The business wants to migrate the application to AWS with no disruption, and needs it to scale to meet demand.\nThe company has already decided to use Amazon Route 53 and CNAME records to redirect traffic. How can these requirements be met with the LEAST amount of effort?","answer_ET":"A","timestamp":"2019-09-23 06:27:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/5586-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1569212820,"topic":"1","answer_description":"","question_id":428,"discussion":[{"poster":"Moon","content":"I prefer answer \"A\".\nA: using EB meets the requirement of least amount of effort.\nB: does not meet the scaling (10 times capacity) requirement\nC: alot of changes.\nD: this solution can't be considered least amount of effort. ( but it is good for no disruption though!).","upvote_count":"39","timestamp":"1632394260.0","comment_id":"13888"},{"content":"A\nWe already decide to use Route53 CNAME to redirect the traffic. Therefore, it already provides no disruption when switching to the ElastiBeanstalk.\n\nD: it does not use the Route53 CNAME to redirect at all. In addition, it's too much work, including modifying existing code to call API Gateway, converting app to Lambda.","upvote_count":"7","comment_id":"23178","poster":"uopspop","timestamp":"1632483840.0","comments":[{"timestamp":"1632543120.0","content":"Moreover, when using ElastiBeanstalk, if your domain name does not include region, you MUST use CNAME record. \n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-beanstalk-environment.html#routing-to-beanstalk-environment-create-resource-record-set","upvote_count":"2","comment_id":"23180","poster":"uopspop"}]},{"poster":"SkyZeroZx","upvote_count":"1","content":"Selected Answer: A\nOption B (Lift and shift the Apache server to the cloud using AWS SMS) involves migrating the entire Apache server to the cloud, which may require more effort and management compared to using Elastic Beanstalk. Option C (Migrate the Docker image to Amazon ECS) requires creating and managing Docker containers and Amazon ECS, which may involve additional complexity. Option D (Modify the application to call the web service via API Gateway and Lambda) requires modifying the application code and introducing additional AWS services, which may involve more effort compared to using Elastic Beanstalk.\n\nTherefore, option A (using AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling) is the option that meets the requirements with the least amount of effort.","comment_id":"926302","timestamp":"1687038720.0"},{"upvote_count":"1","comment_id":"685703","timestamp":"1664817540.0","poster":"dmscountera","content":"Selected Answer: A\nBased on all comments"},{"comment_id":"603610","content":"Selected Answer: A\nTHE LEAST work possible.\nIt's AWS Elastic Beanstalk.\nAssociate level question.","upvote_count":"3","timestamp":"1652936400.0","poster":"bobsmith2000"},{"content":"A. Use AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling. Then switch the application to use the new web service.","comment_id":"497604","timestamp":"1639045620.0","poster":"cldy","upvote_count":"1"},{"poster":"AzureDP900","upvote_count":"1","comment_id":"493933","timestamp":"1638648120.0","content":"A seems perfect."},{"comment_id":"409740","content":"I'll go with A","upvote_count":"1","poster":"WhyIronMan","timestamp":"1636217760.0"},{"upvote_count":"3","timestamp":"1636124820.0","poster":"Waiweng","content":"should be A","comment_id":"345033"},{"comments":[{"poster":"sarah_t","comment_id":"333335","content":"this belongs to the previous question. no idea how it ended up here","upvote_count":"3","timestamp":"1636072140.0"}],"content":"Everyone agrees on B. \nI think the second is C: \"CloudHSM is standards-compliant and enables you to export all of your keys to most other commercially-available HSMs, subject to your configurations. It is a fully-managed service that automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups. \"\nhttps://aws.amazon.com/cloudhsm/","comment_id":"333331","poster":"sarah_t","upvote_count":"1","timestamp":"1636016460.0"},{"content":"i will go with A","upvote_count":"1","timestamp":"1635729420.0","poster":"anandbabu","comment_id":"331170"},{"content":"A is the undisputed king.","poster":"kiev","timestamp":"1635416460.0","comment_id":"293699","upvote_count":"1"},{"content":"will go with A","poster":"Kian1","upvote_count":"2","comment_id":"290680","timestamp":"1635141180.0"},{"content":"API Gateway uses alias not CNAME\nI go with A as well although beanstalk is not my preference","comment_id":"282457","upvote_count":"2","timestamp":"1635114060.0","poster":"Ebi"},{"upvote_count":"1","comment_id":"242532","timestamp":"1634949240.0","poster":"T14102020","content":"Correct answer is A. ElastiBeanstalk and if your domain name does not include region, you MUST use CNAME record."},{"content":"it shown D in other places","upvote_count":"1","timestamp":"1634696040.0","poster":"shwanjaff","comment_id":"233166"},{"content":"I'll go with A","upvote_count":"2","comment_id":"230047","timestamp":"1634487180.0","poster":"jackdryan"},{"content":"Answer is A.- Mainly because auto scaling is included and CNAME record is being used to direct traffic to the Elastic Beanstalk env.","upvote_count":"2","poster":"Bulti","timestamp":"1634148660.0","comment_id":"229774"},{"comment_id":"223532","upvote_count":"2","content":"No disruption during migration. Change to use AWS API. Change record in Route 53. Change APC to use Lambda. So, answer be D","timestamp":"1633992540.0","poster":"PeterOr"},{"comment_id":"182680","content":"A as for detailed route 53 additional migration steps.. D seems to be incomplete for me","timestamp":"1633949040.0","poster":"ipindado2020","upvote_count":"2"},{"timestamp":"1633947240.0","comment_id":"149618","upvote_count":"2","poster":"fullaws","content":"A is correct"},{"comment_id":"132318","upvote_count":"2","timestamp":"1633443660.0","content":"A - agree with others","poster":"NikkyDicky"},{"timestamp":"1633435620.0","comment_id":"98587","upvote_count":"1","poster":"JAWS1600","comments":[{"content":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-api-gateway.html\n\nAPI Gateways can use alias records","poster":"IAmNotLambda","comment_id":"139391","timestamp":"1633918980.0","upvote_count":"1"}],"content":"A. Elastic BT uses CNAME. D is wrong-API gateway uses A records."},{"upvote_count":"4","comment_id":"50047","content":"Should be A\nD is too much effort","timestamp":"1633187220.0","poster":"amog"},{"upvote_count":"1","comment_id":"12223","poster":"awsec2","content":"d is right","timestamp":"1632070260.0"}],"answer":"A","exam_id":32,"choices":{"B":"Lift and shift the Apache server to the cloud using AWS SMS. Then switch the application to direct web service traffic to the new instance.","A":"Use AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling. Then switch the application to use the new web service.","C":"Create a Docker image and migrate the image to Amazon ECS. Then change the application code to direct web service queries to the ECS container.","D":"Modify the application to call the web service via Amazon API Gateway. Then create a new AWS Lambda Java function to run the Java web service code. After testing, change API Gateway to use the Lambda function."}},{"id":"oRE5bWGzd8l96R03rmKV","url":"https://www.examtopics.com/discussions/amazon/view/6116-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A company is using AWS for production and development workloads. Each business unit has its own AWS account for production, and a separate AWS account to develop and deploy its applications. The Information Security department has introduced new security policies that limit access for terminating certain Amazon\nEC2 instances in all accounts to a small group of individuals from the Security team.\nHow can the Solutions Architect meet these requirements?","isMC":true,"topic":"1","answer_images":[],"timestamp":"2019-10-05 12:43:00","exam_id":32,"discussion":[{"poster":"Moon","timestamp":"1632097380.0","comment_id":"13883","upvote_count":"51","comments":[{"content":"Reference:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/iam-ec2-resource-tags/","comment_id":"385392","timestamp":"1635768360.0","upvote_count":"1","poster":"NickGR"},{"comments":[{"upvote_count":"2","timestamp":"1632452880.0","comments":[{"content":"IAM user is different from aws account.\nNot using IAM role/AWS organization ,it is not possible to manage access privileges of other aws accounts.\nchoose C","timestamp":"1634628660.0","comment_id":"185274","poster":"exergeng","upvote_count":"1"}],"poster":"9Ow30","comment_id":"31565","content":"Can you explain what you mean?\nWe can do something like this, right?\nhttps://aws.amazon.com/premiumsupport/knowledge-center/restrict-ec2-iam/"}],"timestamp":"1632210660.0","poster":"examacc","comment_id":"30764","content":"this way you cannot limit root from controlling the targeted instances","upvote_count":"2"},{"comments":[{"poster":"Smart","content":"Revisiting this question. There is no mention of cross-account access. I am not sure if this should be automatically assumed. I wonder if option D can be an option (it doesn't have to have on-prem setup). Even in that, identities are authenticated & federated then authorized based on role; here it seems to be happening other way around.","upvote_count":"1","comment_id":"76534","timestamp":"1633193040.0"}],"timestamp":"1633025100.0","content":"Agreed. B vs. C: SCPs are similar to IAM permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). \n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html#scp-effects-on-permissions","comment_id":"70252","upvote_count":"7","poster":"Smart"}],"content":"I prefer answer \"B\".\nA: applying policy to master account does not mean Security Team!\nB: using tags on EC2s. Then use an IAM policy restrictions/rules on these taged instances.\nC: Organizational unit is used to limit the access, but not to provide privileges.\nD: SAML is used for federation with on premise, which is not the case here!"},{"content":"It seems the answers are not forthis question. the correct set of answers \n\n A. Modify the application to call the web service via Amazon API Gateway Then create a new AWS Lambda Java function to run the Java web service code After testing change API Gateway to use the Lambda function\n B. Lift and shift the Apache server to the cloud using AWS SMS Then switch the application to direct web service traffic to the new instance\n C. Create a Docker image and migrate the image to Amazon ECS Then change the application code to direct web service queries to the ECS container\n D. Use AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling Then switch the application to use the new web service\n\nAnswer: D","upvote_count":"37","comment_id":"522632","timestamp":"1642049400.0","poster":"RVivek"},{"timestamp":"1704385980.0","poster":"3a632a3","upvote_count":"1","comments":[{"poster":"marszalekm","upvote_count":"1","content":"This is exactly opposite of what SCP do, the deny by default, not allow.","comment_id":"1131008","timestamp":"1706121600.0"}],"comment_id":"1113891","content":"Selected Answer: C\nC is the most straight forward answer to achieve what is being asked. The question is asking to \"limit access\" which is what SCPs are designed to do. The question does not ask to grant access to the security team."},{"upvote_count":"1","timestamp":"1687039140.0","poster":"SkyZeroZx","content":"Selected Answer: C\nHere's how option C addresses the requirements:\n\nCreate an organizational unit (OU) under AWS Organizations: AWS Organizations allows you to centrally manage and govern multiple AWS accounts. By creating an OU specifically for the accounts related to the company's production and development workloads, you can organize them in a logical structure.\n\nMove all the accounts into the organizational unit: Once the OU is created, you can move the existing AWS accounts for production and development workloads into this OU. This helps in centralizing the management of these accounts.\n\nUse Service Control Policies (SCPs): SCPs allow you to set fine-grained permissions and access controls for AWS accounts within an organization. You can create an SCP that specifies a whitelist policy, allowing access to terminate specific EC2 instances only for the Security team. By applying this SCP to the OU that contains the accounts, you enforce the access restrictions across all the relevant accounts.","comment_id":"926303","comments":[{"content":"Option A (Create a new IAM policy and apply it to the AWS Organizations master account) is not the recommended approach because IAM policies are account-specific and cannot be directly applied to the AWS Organizations master account.\n\nOption B (Create a tag-based IAM policy and apply it in each account) would require manual tagging of the EC2 instances and applying the policy in each account separately, which can be cumbersome and prone to errors.\n\nOption D (Set up SAML federation and block authentication for API calls) is not the most direct solution for limiting access to terminating EC2 instances. SAML federation primarily focuses on federated access and single sign-on (SSO) and may not provide the necessary granularity for restricting access to specific EC2 instances.\n\nTherefore, option C (creating an organizational unit, moving the accounts, and applying an SCP) is the most suitable option for meeting the requirements and enforcing the access restrictions effectively.","poster":"SkyZeroZx","comment_id":"926304","timestamp":"1687039140.0","upvote_count":"1"}]},{"poster":"dev112233xx","timestamp":"1681236960.0","upvote_count":"1","comment_id":"867580","content":"Selected Answer: C\nI prefer C.. organization solution is the best for multi-accounts restrictions, and you can create SCP policy to allow only security team to perform the ec2 actions \nB can't be correct, users can easily workaround the restriction by changing the instances Tags of the instances then terminate them without any issue (the correct policy should be restricting according to the users-tags not the instances-tags)"},{"timestamp":"1668273240.0","upvote_count":"1","content":"Selected Answer: B\nIAM Policy + Tag -> ABAC model\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html","poster":"Lorrendo","comment_id":"716826"},{"timestamp":"1667505420.0","content":"Selected Answer: B\nB mentions about applying the policy to all account which make sense vs applying to master account in A","comment_id":"710768","upvote_count":"1","poster":"AjayPrajapati"},{"content":"Selected Answer: B\nB is the correct answer","comment_id":"703153","poster":"kharakbeer","timestamp":"1666624140.0","upvote_count":"1"},{"content":"Selected Answer: B\nB is correct.","timestamp":"1666596240.0","upvote_count":"1","comment_id":"702783","poster":"Yashar1691"},{"timestamp":"1664817720.0","comment_id":"685704","poster":"dmscountera","upvote_count":"2","content":"Selected Answer: B\nBased on all comments"},{"poster":"Rocketeer","timestamp":"1661720940.0","comment_id":"653158","content":"Answer seems to be C - https://aws.amazon.com/blogs/security/how-to-use-service-control-policies-to-set-permission-guardrails-across-accounts-in-your-aws-organization/#:~:text=For%20example%2C%20you%20can%20use,used%20for%20your%20central%20administrators.","upvote_count":"2"},{"content":"Selected Answer: A\nThe Answer are corresponding the incorrect qestion. Here're the ans:\n\n• A. Use AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling. Then switch the application to use the new web service.\n• B. Lift and shift the Apache server to the cloud using AWS SMS. Then switch the application to direct web service traffic to the new instance.\n• C. Create a Docker image and migrate the image to Amazon ECS. Then change the application code to direct web service queries to the ECS container.\n• D. Modify the application to call the web service via Amazon API Gateway. Then create a new AWS Lambda Java function to run the Java web service code. After testing, change API Gateway to use the Lambda function.\n\nCorrect Answer should be : A (least effort)","comments":[{"poster":"user0001","content":"i agree with you , answers are not related to this question","comment_id":"597785","upvote_count":"1","timestamp":"1651852320.0"}],"upvote_count":"6","poster":"LiamNg","comment_id":"587121","timestamp":"1650185160.0"},{"comment_id":"527311","upvote_count":"11","poster":"sTeVe86","content":"To me: this question doesn't match with the answers, didn't make any scenes.","timestamp":"1642580040.0"},{"comment_id":"523427","poster":"GeniusMikeLiu","upvote_count":"2","content":"what's the main point about this question? I am confused after read.","timestamp":"1642148460.0"},{"poster":"Duke_YU","comment_id":"520369","timestamp":"1641749220.0","content":"I don't understand this question and answers at all. The question is about to \"The organization needs a smooth transfer of the program to AWS and the ability for the application to scale in response to demand\". Why the answers are about IAM and security team? Couldn't Auto Scaling groups and Route 53 and CNAME records satisfy the requirement?","comments":[{"timestamp":"1641960480.0","poster":"Bigbearcn","content":"They made mistake. This is question 466 and the answer option is 467.","upvote_count":"3","comment_id":"521904"}],"upvote_count":"4"},{"comment_id":"455666","timestamp":"1636224300.0","poster":"student22","content":"B\n---\nNot D because we need to limit access to ec2 instances, not ec2 service.","upvote_count":"1"},{"upvote_count":"1","content":"I'll go with B","poster":"WhyIronMan","comment_id":"409741","timestamp":"1635869400.0"},{"comment_id":"406454","poster":"neta1o","timestamp":"1635818520.0","upvote_count":"1","content":"Why B, couldn't tags be easily modified to thwart the security effort?"},{"timestamp":"1635716520.0","poster":"Waiweng","content":"It's B","upvote_count":"2","comment_id":"345035"},{"comment_id":"333823","timestamp":"1635695880.0","upvote_count":"1","content":"Correct answer is B\n\nSCP alone are never sufficient to grant permissions to accounts in organization. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions.\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html","poster":"Amitv2706"},{"content":"will go with B Tag","upvote_count":"2","poster":"Kian1","comment_id":"290682","timestamp":"1635576480.0"},{"comment_id":"282467","upvote_count":"3","timestamp":"1635290640.0","poster":"Ebi","content":"My answer is B"},{"content":"I'll go with B","upvote_count":"2","comment_id":"268108","timestamp":"1635222720.0","poster":"sanjaym"},{"comment_id":"263034","content":"Answer is B not C.\n\nSCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.","upvote_count":"1","poster":"ju0n","timestamp":"1635092160.0"},{"timestamp":"1634882400.0","comment_id":"243776","upvote_count":"1","poster":"petebear55","content":"based on this ill go for B: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html"},{"poster":"T14102020","comment_id":"242540","timestamp":"1634875200.0","content":"Correct answer is B. tag-based IAM policy","upvote_count":"2"},{"upvote_count":"2","content":"https://aws.amazon.com/blogs/security/resource-level-permissions-for-ec2-controlling-management-access-on-specific-instances/\n\ni'll Go with B","comment_id":"230327","poster":"YouYouYou","timestamp":"1634779140.0"},{"upvote_count":"2","poster":"jackdryan","timestamp":"1634775540.0","content":"I'll go with B","comment_id":"230049"},{"upvote_count":"5","comment_id":"229826","timestamp":"1634763780.0","content":"Answer is B. To control access to specific EC2 instances, SCP wouldn't work even if we were use whitelisting ( i.e. providing access to EC2 service) as opposed to blacklisting access (denying access to EC2 service). This is because SCP will control access at the EC2 service level ( all instances) as opposed to specific instances. As a result B is the right answer which allows us to use AWS:ResourceTag and AWS:PrincipalTag to allow access to the specific EC2 instances by the security team to terminate those instances.","poster":"Bulti","comments":[{"content":"Exactly - for terminating !!!certain!!! Amazon EC2 instances in all accounts","timestamp":"1636121940.0","comment_id":"413119","poster":"DerekKey","upvote_count":"1"}]},{"timestamp":"1634676420.0","poster":"sam422","comment_id":"189380","content":"I go with B, SCP whitelisting or black listing is for services, any resource policies is use IAM","upvote_count":"1"},{"poster":"ipindado2020","timestamp":"1634541420.0","comment_id":"182685","upvote_count":"1","content":"B for sure"},{"upvote_count":"2","poster":"smithyt","timestamp":"1634490180.0","comment_id":"180006","content":"its B it can't be C \"SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions.\""},{"poster":"Paramg1234","timestamp":"1634423880.0","content":"C is Correct, when defining we can control and Provide access to a specific group \nThis Link will brief :\nhttps://aws.amazon.com/blogs/awsmarketplace/controlling-access-to-a-well-architected-private-marketplace-using-iam-and-aws-organizations/","comment_id":"173802","upvote_count":"1"},{"upvote_count":"3","comment_id":"149624","timestamp":"1633810380.0","poster":"fullaws","content":"B is correct, SCP cannot specific principal, if apply will affect all user."},{"poster":"MultiAZ","upvote_count":"1","timestamp":"1633749600.0","content":"D\nA, B and C speak about \"policy that allows access to those EC2 instances only for the Security team\". Te requirement is not about blocking access to the EC2 instances; the requirement is blocking termination of those.","comment_id":"143618"},{"comment_id":"134440","upvote_count":"2","timestamp":"1633673580.0","content":"B makes more sense","poster":"NikkyDicky"},{"timestamp":"1633521960.0","content":"Answer: B\nA - incorrect - can't apply permissions in this manner\nB - correct - not pretty but works\nC - incorrect - preventing \"terminate instance\" in an SCP applies to all users in the account its applied to, not just that of the security team. \"SCPs enable you set permission guardrails by defining the maximum available permissions for IAM entities in an account. If a SCP denies an action for an account, none of the entities in the account can take that action, even if their IAM permissions allow them to do so. The guardrails set in SCPs apply to all IAM entities in the account, which include all users, roles, and the account root user\"\nD - incorrect - huh?","poster":"inf","comment_id":"132762","upvote_count":"2"},{"comment_id":"97049","timestamp":"1633270380.0","upvote_count":"11","content":"Answer is B. This is why C is wrong. As question says each business unit has their own account. If you move some of those accounts under an OU and apply SCP policy to restrict access to EC2 instance. It will restrict to ALL the EC2s for all accounts under that OU. This is not desired. Question is asking for \"terminating certain EC2 instances\". This is an IAM requirement not SCP. SCP will only ALLOW or DENY usage of services under an OU. So C is not correct. B is the right one.","poster":"JAWS1600"},{"timestamp":"1633219260.0","poster":"3parusr","content":"\"In all accounts\" This leads me to C","upvote_count":"1","comment_id":"94318"},{"poster":"[Removed]","upvote_count":"2","timestamp":"1633215420.0","comment_id":"87438","content":"I believe it should be B. \nFor option C; moving all accounts under an organisation for such a small task is too much. Suppose that the other day there is another requirement. Then will you move all the accounts to another OU? It's not feasible for a single restriction."},{"timestamp":"1632842880.0","comment_id":"62266","poster":"Jshuen","upvote_count":"1","content":"i will go for C\n\nhttps://aws.amazon.com/blogs/security/how-to-use-service-control-policies-to-set-permission-guardrails-across-accounts-in-your-aws-organization/\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_example-scps.html"},{"upvote_count":"1","poster":"virtual","comment_id":"61545","timestamp":"1632763860.0","content":"As it is specified \"Each business unit has its own AWS account\", I would also go for C.\nIAM policy would have to be replicated on each account."},{"comment_id":"50048","upvote_count":"3","poster":"amog","timestamp":"1632667860.0","content":"Should be B\n\"terminating certain Amazon ECs instances\""},{"comment_id":"47517","poster":"AWSPro24","content":"The existence of this blog article makes me think the answer is likely B https://aws.amazon.com/blogs/security/resource-level-permissions-for-ec2-controlling-management-access-on-specific-instances/","timestamp":"1632560580.0","upvote_count":"3"},{"poster":"huhupai","timestamp":"1632152280.0","comments":[{"upvote_count":"2","poster":"HazemYousry","comment_id":"68193","content":"This means you can't even allow Security group to terminate those ECSs - B should be the answer","timestamp":"1632925080.0","comments":[{"comment_id":"156207","timestamp":"1634297340.0","upvote_count":"2","poster":"V007","content":"C says white list policy means to allow access to Security group."}]},{"content":"This will prevent access to EC2 instances. The goal is to prevent users from terminating the instances. B is the correct answer.","upvote_count":"1","timestamp":"1633461600.0","comment_id":"109829","poster":"purplejuice"}],"upvote_count":"14","comment_id":"22083","content":"I would go for C, AWS Organizations lets you use service control policies (SCPs) to allow or deny access to particular AWS services for individual AWS accounts, or for groups of accounts within an organizational unit (OU). The specified actions from an attached SCP affect all IAM users, groups, and roles for an account, including the root account identity."}],"answer":"A","unix_timestamp":1570272180,"answer_description":"","question_id":429,"answer_ET":"A","answers_community":["A (40%)","B (40%)","C (20%)"],"question_images":[],"choices":{"C":"Create an organizational unit under AWS Organizations. Move all the accounts into this organizational unit and use SCP to apply a whitelist policy to allow access to these EC2 instances for the Security team only.","A":"Create a new IAM policy that allows access to those EC2 instances only for the Security team. Apply this policy to the AWS Organizations master account.","D":"Set up SAML federation for all accounts in AWS. Configure SAML so that it checks for the service API call before authenticating the user. Block SAML from authenticating API calls if anyone other than the Security team accesses these instances.","B":"Create a new tag-based IAM policy that allows access to these EC2 instances only for the Security team. Tag the instances appropriately, and apply this policy in each account."}},{"id":"sBjvxhigAWqMEjTFXh4a","discussion":[{"poster":"donathon","upvote_count":"37","content":"B\nA: Workspace is expensive and a single instance of EC2 hosting the DB would not improve the availability.\nB: B is the best answer. Aurora would improve availability that can replicate to multiple AZ (6 copies). Auto scaling would improve the performance together with a ALB. AppStream is like Citrix that deliver hosted Apps to users.\nC: Fargate is managed ECS. Elasticache only improve DB access performance but not the total user experience. This does not address the slowness that users faced.\nD: Redshift is dataware house and not a SQL based database.","timestamp":"1632402720.0","comment_id":"12552","comments":[{"poster":"heany","upvote_count":"1","content":"B is wrong. should be C.\n1. B didn't mention Multi-AZ of aurora. Aurora is not Multi-AZ by default. Single AZ cannot achieve SLA required ( https://aws.amazon.com/rds/aurora/sla/ )\n2. Appstream is a good product but you need a lot of changes . you need to build the image of the app and deployed it appstream","timestamp":"1665378540.0","comment_id":"690716"}]},{"timestamp":"1632664080.0","comment_id":"13875","upvote_count":"15","poster":"Moon","content":"Answer \"B\" meets the requirements.\nA: using containers, is a big change to app. Maintaining EC2 is a challenge and could affect SLA.\nB: using RDS, is auto maintenance with high SLA. using autoscaling on EC2 allow more availability/performance. Appstream is good virtual solution.\nC: lots of re-works.\nD: there is no DB, only Datawarehouse."},{"poster":"hollie","comment_id":"770951","timestamp":"1673310240.0","content":"Selected Answer: B\nAppStream can \"lets your users start using your application immediately\"(https://aws.amazon.com/appstream2/faqs/?nc=sn&loc=7), meeting the requirements of improving slow loading","upvote_count":"2"},{"comment_id":"710770","timestamp":"1667505960.0","content":"Selected Answer: B\nB improves SLA from all layers. Desktop app can be supported by App stream\nC - elastic cache needs code changes. Does not talk about desktop app","poster":"AjayPrajapati","upvote_count":"3"},{"timestamp":"1666720440.0","comment_id":"704082","content":"Selected Answer: C\nGoing with C; B does not meet uptime SLA: https://aws.amazon.com/appstream2/sla/","upvote_count":"3","poster":"Vinafec"},{"upvote_count":"2","content":"Selected Answer: B\nDon't confuse Desktop Client Application to \"Desktop\".\nDesktop = Workspace\nDesktop Client Application = Appstream","poster":"Dionenonly","timestamp":"1665811260.0","comment_id":"695205"},{"timestamp":"1665571500.0","comments":[{"upvote_count":"1","comment_id":"692963","timestamp":"1665571980.0","content":"And by the way, AuroraDB and AppStream do not minimize costs !!","poster":"Blair77"}],"upvote_count":"2","poster":"Blair77","comment_id":"692960","content":"Selected Answer: C\n-\"A Solutions Architect must re- architect the application to ensure that it can meet or exceed the SLA.\"\nFargate SLA: 99.99%\nRDS SLA: 99.95%\nAurora SLA: 99.99\nAppStream SLA: 99.9%\n\nAppStream does not meet the 99.95% SLA. \nI'll go with C."},{"comment_id":"685705","poster":"dmscountera","upvote_count":"1","content":"Selected Answer: B\nBased on all comments","timestamp":"1664817900.0"},{"poster":"AzureDP900","timestamp":"1638648540.0","upvote_count":"2","content":"B is right, other answers doesn't make any sense as mentioned by Ebi.","comment_id":"493935"},{"timestamp":"1636151340.0","upvote_count":"1","content":"Little change... so B","comment_id":"433169","poster":"denccc"},{"timestamp":"1636027860.0","poster":"DerekKey","upvote_count":"1","content":"B ok\nC wrong - \"multiple virtual machines\" must be converted to Docker it is AGAINST \"little change to the application\"","comment_id":"413142"},{"poster":"WhyIronMan","timestamp":"1635905220.0","content":"I'll go with B","comment_id":"409744","upvote_count":"2"},{"comment_id":"345042","upvote_count":"2","content":"It's B","timestamp":"1635885600.0","poster":"Waiweng"},{"poster":"ExtHo","content":"Key of answer is desktop client applications so its AppStream =B","upvote_count":"2","comment_id":"323258","timestamp":"1635406500.0"},{"content":"Answer is B: AppStream is built for virtual desktop application","upvote_count":"1","poster":"areke","timestamp":"1635308340.0","comment_id":"298086"},{"timestamp":"1635050940.0","content":"will go with B only B makes sense","comment_id":"290686","upvote_count":"2","poster":"Kian1"},{"upvote_count":"3","timestamp":"1635009180.0","comment_id":"282470","poster":"Ebi","content":"I go with B, all other answer don't make any sense"},{"content":"The answer is definitely C -- read the question carefully, the company is having trouble meeting it's SLA REQUIREMENTS. Hence you need multi-AZ deployments. While Aurora multi-AZ + Elasticache is probably overkill, it will definitely help you meet an SLA for a HIGHLY LATENCY SENSITIVE application. Everyone saying B is better because of Appstream isn't reading the question requirements, nevermind that B does no indicated a multi-AZ deployment which will not ensure a higher SLA.\nIf you read the question, the answer is definitely C.","comment_id":"282169","poster":"Trap_D0_r","comments":[{"comments":[{"content":"SLA needed: 99.95%\nAppStream SLA: 99.9%\nC is good.","comment_id":"693373","upvote_count":"1","poster":"Blair77","timestamp":"1665607860.0"}],"poster":"wind","timestamp":"1635208680.0","upvote_count":"1","content":"C is wrong. SLA of Aurora is higher than RDS PostgreSQL.B is correct.","comment_id":"291442"},{"content":"Aurora is muti-az by default. \"Aurora stores copies of the data in a DB cluster across multiple Availability Zones in a single AWS Region. Aurora stores these copies regardless of whether the instances in the DB cluster span multiple Availability Zones.\"","upvote_count":"1","poster":"selva","comment_id":"291425","timestamp":"1635153120.0"}],"timestamp":"1634857260.0","upvote_count":"4"},{"comment_id":"269686","poster":"rasti","timestamp":"1634602860.0","upvote_count":"2","content":"The question is also about \"minimizing costs\". B with AppStream is really expensive... \nI'll go with C"},{"content":"I'll go with B","comment_id":"268111","poster":"sanjaym","timestamp":"1634468880.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"242545","poster":"T14102020","timestamp":"1634311440.0","content":"Correct answer is B. Aurora, Auto scaling and AppStream"},{"upvote_count":"3","timestamp":"1634270460.0","content":"I'll go with B","comment_id":"230051","poster":"jackdryan"},{"upvote_count":"3","comment_id":"229856","timestamp":"1634149200.0","poster":"Bulti","content":"B is the right answer. C is incorrect because it doesn't take care of the desktop client app which Appstream 2.0 in B does."},{"upvote_count":"2","poster":"fullaws","content":"B is correct","timestamp":"1633892100.0","comment_id":"149628"},{"comment_id":"134442","timestamp":"1633884420.0","upvote_count":"2","poster":"NikkyDicky","content":"B for sure"},{"timestamp":"1633520220.0","upvote_count":"2","poster":"meenu2225","comment_id":"105616","content":"B is the right one."},{"comment_id":"102193","upvote_count":"3","poster":"NKnab","timestamp":"1633432020.0","content":"Many of our customers asked for a SaaS offering for our desktop application. While we could have rewritten our application as a web app, it would have cost millions of dollars and years of development. Instead, we simply deployed our application using AppStream 2.0 without rewriting any code. Now, we can instantly deliver our applications to any customer on the computer of their choice with a responsive and fluid experience.”"},{"upvote_count":"3","comment_id":"97821","timestamp":"1633257600.0","comments":[{"comment_id":"99259","timestamp":"1633325400.0","poster":"nawfal6809","content":"B is the answer. There is a PostgreSQL-compatible edition of Aurora. https://aws.amazon.com/rds/aurora/postgresql-features/","comments":[{"content":"The problem is .. or is RDS or is Aurora. There is no such thing as \"RDS Aurora\"... what now?...","comment_id":"111438","comments":[{"content":"Aurora service is part of aws RDS service...\nhttps://aws.amazon.com/rds/?nc1=h_ls\n\nand it is build on the top of mysql and postgresql (powered version)\nhttps://aws.amazon.com/rds/aurora/\nA","comment_id":"182694","timestamp":"1634031780.0","upvote_count":"1","poster":"ipindado2020","comments":[{"comment_id":"182695","timestamp":"1634040240.0","poster":"ipindado2020","upvote_count":"1","content":"typo...then is B Aurora"}]}],"upvote_count":"1","timestamp":"1633687020.0","poster":"pmjcr"}],"upvote_count":"3"},{"upvote_count":"1","timestamp":"1634414880.0","poster":"petebear55","comment_id":"243805","content":"you are wrong it does exist https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraPostgreSQL.html"}],"content":"C. B is wrong . Read B carefully \" Amazon RDS Aurora PostgreSQL configuration\" . There is no such thing called \"Aurora PostgreSQL\" . These are two separate products. Unless this is a typo, it is not possible.","poster":"JAWS1600"},{"comments":[{"comment_id":"70270","content":"Then again, the question is asking \"little change to application\". I am not developer so I don't know what level of changes would allow this transition.","upvote_count":"1","poster":"Smart","timestamp":"1633065000.0"}],"content":"I don't get B (I don't know these sort of architectures well) - why do we have a separate autoscaled presentation layer when we are using AppStream 2.0? Also, the focus is High Availability, User Experience and Cost. Should we assume that Aurora is a Multi-AZ cluster? It is not by default - we can do automatic failover to Read Replicas and then maintain 99.99% SLA. \n\nI am not sure but there seems to be ways through which App Streaming can configured on Containers (https://www.unidata.ucar.edu/blogs/news/entry/cloudstream-an-application-streaming-docker). Fargate (based on Firecracker Tech) can start in milliseconds and it would help latency issues as well as cost. Am I thinking too much?","upvote_count":"2","comment_id":"70267","poster":"Smart","timestamp":"1632932040.0"},{"upvote_count":"4","poster":"amog","timestamp":"1632924900.0","comment_id":"50049","content":"Should be B"},{"comment_id":"12799","upvote_count":"5","poster":"SivaG","timestamp":"1632545820.0","content":"Yes Answer should be B .."},{"upvote_count":"1","timestamp":"1632352440.0","content":"why not C?","comments":[{"content":"Sorry, I prefer B. key point- desktop application.","comment_id":"11238","timestamp":"1632373200.0","upvote_count":"5","poster":"Xiaoyao2000"}],"comment_id":"11237","poster":"Xiaoyao2000"}],"unix_timestamp":1568618700,"timestamp":"2019-09-16 09:25:00","question_id":430,"answer_description":"","answers_community":["B (62%)","C (38%)"],"topic":"1","exam_id":32,"answer_images":[],"answer":"B","choices":{"A":"Migrate the database to a PostgreSQL database in Amazon EC2. Host the application and presentation layers in automatically scaled Amazon ECS containers behind an Application Load Balancer. Allocate an Amazon WorkSpaces WorkSpace for each end user to improve the user experience.","D":"Migrate the database to an Amazon Redshift cluster with at least two nodes. Combine and host the application and presentation layers in automatically scaled Amazon ECS containers behind an Application Load Balancer. Use Amazon CloudFront to improve the user experience.","B":"Migrate the database to an Amazon RDS Aurora PostgreSQL configuration. Host the application and presentation layers in an Auto Scaling configuration on Amazon EC2 instances behind an Application Load Balancer. Use Amazon AppStream 2.0 to improve the user experience.","C":"Migrate the database to an Amazon RDS PostgreSQL Multi-AZ configuration. Host the application and presentation layers in automatically scaled AWS Fargate containers behind a Network Load Balancer. Use Amazon ElastiCache to improve the user experience."},"answer_ET":"B","question_text":"A company is moving a business-critical, multi-tier application to AWS. The architecture consists of a desktop client application and server infrastructure. The server infrastructure resides in an on-premises data center that frequently fails to maintain the application uptime SLA of 99.95%. A Solutions Architect must re- architect the application to ensure that it can meet or exceed the SLA.\nThe application contains a PostgreSQL database running on a single virtual machine. The business logic and presentation layers are load balanced between multiple virtual machines. Remote users complain about slow load times while using this latency-sensitive application.\nWhich of the following will meet the availability requirements with little change to the application while improving user experience and minimizing costs?","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/5224-exam-aws-certified-solutions-architect-professional-topic-1/"}],"exam":{"provider":"Amazon","id":32,"name":"AWS Certified Solutions Architect - Professional","numberOfQuestions":1019,"isImplemented":true,"isMCOnly":false,"isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":86},"__N_SSP":true}