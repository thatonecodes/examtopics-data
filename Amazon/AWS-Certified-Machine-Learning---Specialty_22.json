{"pageProps":{"questions":[{"id":"qoqvBK6gsY4BMJ7jcimC","question_text":"A data scientist is working on a model to predict a company's required inventory stock levels. All historical data is stored in .csv files in the company's data lake on Amazon S3. The dataset consists of approximately 500 GB of data The data scientist wants to use SQL to explore the data before training the model. The company wants to minimize costs.\n\nWhich option meets these requirements with the LEAST operational overhead?","isMC":true,"answers_community":["B (100%)"],"choices":{"D":"Create an Amazon Redshift cluster. Create external tables in an external schema, referencing the S3 bucket that contains the data. Explore the data from the Amazon Redshift query editor GUI.","B":"Use AWS Glue to crawl the S3 bucket and create tables in the AWS Glue Data Catalog. Use Amazon Athena to explore the data.","C":"Create an Amazon Redshift cluster. Use the COPY command to ingest the data from Amazon S3. Explore the data from the Amazon Redshift query editor GUI.","A":"Create an Amazon EMR cluster. Create external tables in the Apache Hive metastore, referencing the data that is stored in the S3 bucket. Explore the data from the Hive console."},"url":"https://www.examtopics.com/discussions/amazon/view/89135-exam-aws-certified-machine-learning-specialty-topic-1/","question_images":[],"answer_ET":"B","answer_description":"","question_id":106,"unix_timestamp":1669649940,"timestamp":"2022-11-28 16:39:00","topic":"1","discussion":[{"comment_id":"729333","upvote_count":"6","poster":"dunhill","content":"I think the answer is B. The others are quite expensive and complicated.","timestamp":"1701185940.0"},{"poster":"loict","timestamp":"1725782460.0","comment_id":"1002277","content":"Selected Answer: B\nA. NO - B is easier\nB. YES - works natively against S3\nC. NO - no need to import S3 data to Redshift when Presto/Athena allows you to query directly\nD. NO - Redshift overkill","upvote_count":"1"},{"upvote_count":"1","timestamp":"1724501400.0","comment_id":"989171","content":"Selected Answer: B\nAWS Glue","poster":"Mickey321"},{"upvote_count":"1","content":"Selected Answer: B\nWe want to use SQL to explore the 500GB database saved in S3. We also want to minimize costs and have the least headache with the operation. Letters A - C - D mean managed services, hence: headache. Correct alternative is letter B.","timestamp":"1723386480.0","comment_id":"978718","poster":"kaike_reis"},{"upvote_count":"1","poster":"ADVIT","comment_id":"944987","content":"Selected Answer: B\nB as \"LEAST operational overhead\"","timestamp":"1720296660.0"},{"timestamp":"1713530160.0","upvote_count":"2","content":"The advantage of D is that Redshift, as a data warehouse, can handle large dataset(>1TB) and complex frequent query. In this example, 500GB dataset and infrequent query(I consider this just one-time ad-hoc query, just verify the data before training.) Athena would be a much better option.","comment_id":"874643","poster":"ZSun"},{"upvote_count":"3","poster":"oso0348","timestamp":"1711503840.0","comment_id":"851572","content":"Selected Answer: B\nThe option that meets these requirements with the LEAST operational overhead is option B: Use AWS Glue to crawl the S3 bucket and create tables in the AWS Glue Data Catalog. Use Amazon Athena to explore the data.\n\nAWS Glue is a fully managed ETL service that can automatically discover and catalog metadata about data stored in various data stores, including Amazon S3. By using AWS Glue to crawl the S3 bucket, the data scientist can easily create tables in the AWS Glue Data Catalog, without needing to create or manage any infrastructure.\n\nAmazon Athena is an interactive query service that allows querying data stored in Amazon S3 using SQL. By using Amazon Athena, the data scientist can easily explore the data using SQL, without needing to set up any infrastructure."},{"upvote_count":"3","poster":"Jerry84","comment_id":"773063","content":"Selected Answer: B\nBoth Glue and Athena are serverless hence cost effective.","timestamp":"1705026420.0"},{"content":"Selected Answer: B\nB is highly managed unlike other options.","upvote_count":"4","poster":"Peeking","timestamp":"1702269540.0","comment_id":"741409"},{"content":"Selected Answer: B\nIt seems to be B","comment_id":"734497","upvote_count":"4","timestamp":"1701612960.0","poster":"akhjhjk"}],"exam_id":26,"answer_images":[],"answer":"B"},{"id":"4Qo6E434gJBWnsE5GlqQ","isMC":true,"answer_description":"","question_images":[],"topic":"1","question_id":107,"choices":{"A":"Modify the training configuration to use two ml.p2.xlarge instances.","D":"Modify the training configuration to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 to store the input training data.","C":"Modify the training configuration to use a single ml.p3.2xlarge instance.","B":"Modify the training configuration to use Pipe input mode."},"discussion":[{"timestamp":"1701501060.0","comment_id":"733527","upvote_count":"10","content":"Selected Answer: B\nAgreed with B\nhttps://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/","poster":"tsangckl"},{"content":"Selected Answer: B\nPipe input mode.","comment_id":"989172","timestamp":"1724501520.0","upvote_count":"1","poster":"Mickey321"},{"upvote_count":"2","poster":"oso0348","timestamp":"1711504020.0","comments":[{"timestamp":"1714680360.0","poster":"ZSun","content":"the explanation of EFS is not correct.\nEFS is the default storage for sagemaker instance. That means, when you use the file mode, data is firstly copied to EFS and then fit model.\nSo the issue\" model is failing because of lack of storage\" indicates EFS is not capable to store all s3 data. We have to use pipe mode to incrementally send data from s3 to EFS.","comment_id":"887860","upvote_count":"2"}],"content":"Selected Answer: B\nThe correct solution would be to modify the training configuration to use Pipe input mode. This will allow the training data to stream directly into the training instance as it is being consumed, rather than first being downloaded from S3 into the instance's local storage. This can help reduce storage requirements and optimize performance, while also minimizing costs. Using more or larger instances may help with processing power, but it will not address the storage issue, and may even increase costs unnecessarily. Using Amazon EFS may also be an option, but it may come with additional costs and operational overhead.","comment_id":"851573"},{"content":"Selected Answer: B\nPipe mode solves the problem without incurring extra storage cost. Data is streamed directly to the training algorithm without the need to be stored in the EBS volume.","poster":"Peeking","upvote_count":"3","comment_id":"741414","timestamp":"1702269720.0"},{"content":"I think the answer is B. D is incorrect because EFS is more expensive than S3.\nIt looks like scaling up or out is no help for storage issue. Therefore A and C are not helpful.","poster":"dunhill","timestamp":"1701186300.0","comment_id":"729339","upvote_count":"3"}],"answer_ET":"B","answer":"B","question_text":"A geospatial analysis company processes thousands of new satellite images each day to produce vessel detection data for commercial shipping. The company stores the training data in Amazon S3. The training data incrementally increases in size with new images each day.\n\nThe company has configured an Amazon SageMaker training job to use a single ml.p2.xlarge instance with File input mode to train the built-in Object Detection algorithm. The training process was successful last month but is now failing because of a lack of storage. Aside from the addition of training data, nothing has changed in the model training process.\n\nA machine learning (ML) specialist needs to change the training configuration to fix the problem. The solution must optimize performance and must minimize the cost of training.\n\nWhich solution will meet these requirements?","answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/89137-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1669650300,"answer_images":[],"timestamp":"2022-11-28 16:45:00","exam_id":26},{"id":"pUp8awZciYlih9kcfrHT","exam_id":26,"answer_ET":"A","question_images":[],"discussion":[{"poster":"dunhill","content":"I think the answer is A. \nB: Snowcone has limit with 8 TB. \nC: is AWS on-premises solution, but the company wants to store these transcriptions in an Amazon S3 bucket in the AWS Cloud for model development.\nD: 100 Mbps cannot handle petabytes datasync.","upvote_count":"12","timestamp":"1701188520.0","comment_id":"729373"},{"comment_id":"1009873","poster":"jopaca1216","timestamp":"1726584420.0","content":"A: is the Right \n\nFolks, please... \n\nOUTPOST is very complicated to implement, and the question is not talking about continue to do it after.\n\nWe know that Snowball has limite of storage, but the ideia is not to send Petabytes of data to S3, just only the likelihood processed. so, the ideia is to use the Snowball as an Optimized machine to be able to process the data and send it to S3.\n\nThe Snowball Edge Compute Optimized device provides 52 vCPUs, 208 GiB of memory, and an optional NVIDIA Tesla V100 GPU. For storage, the device provides 42 TB usable HDD capacity for Amazon S3 or Amazon EBS, as well as 7.68 TB of usable NVMe SSD capacity for EBS block volumes. Snowball Edge Compute Optimized devices run Amazon EC2 sbe-c and sbe-g instances, which are equivalent to C5, M5a, G3, and P3 instances.","upvote_count":"2"},{"timestamp":"1725792540.0","content":"Selected Answer: B\nA. NO - Snowball needs to be shipped back to AWS, that does not use Datasynch\nB. YES - that is an edge computing device\nC. NO - Too big admin overhead to have your local AWS Cloud\nD. NO - Too slow over the 100Mps connection","poster":"loict","comment_id":"1002372","upvote_count":"1"},{"timestamp":"1724501640.0","comment_id":"989174","upvote_count":"1","poster":"Mickey321","content":"Selected Answer: A\nfaster"},{"content":"Selected Answer: A\nA, as this is faster option.","poster":"ADVIT","comment_id":"944994","upvote_count":"1","timestamp":"1720297260.0"},{"content":"Selected Answer: A\nA ==> Bring the compute closer to the data","poster":"dkx","timestamp":"1715682600.0","upvote_count":"2","comment_id":"897475"},{"upvote_count":"2","timestamp":"1712068380.0","poster":"Mllb","content":"Selected Answer: A\nThe key is faster\nTransfer speeds of up to 100gb per second","comment_id":"858921"},{"timestamp":"1710889560.0","upvote_count":"1","content":"Selected Answer: A\nIt either A or C. \nBut C is too complicated to setup. You order rack and AWS installs that plus you need enterprise support and the biggest reason it is not possible in this case is that it requires at least 1GB connection. \nThe question clearly asks as soon as possible so A is the best choice in my opinion","poster":"blanco750","comment_id":"844320"},{"comment_id":"837689","upvote_count":"1","timestamp":"1710311940.0","poster":"pan_b","content":"Selected Answer: A\nThe key is to deliver the transcripts to S3 as early as possible. Outpost order and provisioning takes months. I would go for A as its logical to do local inference and send transcripts to S3."},{"content":"Selected Answer: A\nThe model needs to run on-premises to don't be necessarily upload all the audio data to after run the model and this solution can use datasync to upload the results after too, then it's a good choice!","timestamp":"1709984220.0","poster":"Valcilio","upvote_count":"2","comment_id":"833910"},{"upvote_count":"2","poster":"Chelseajcole","content":"Selected Answer: A\nOutpost for use case when customer don’t want to transfer their data out","comment_id":"826163","timestamp":"1709317560.0"},{"content":"It is A.\n\nC can't be the answer, as to transfer 1PB data, it may take 1,000 days under a 100 mbps network.","upvote_count":"2","timestamp":"1709015940.0","comment_id":"823290","poster":"lmimi"},{"upvote_count":"1","content":"Selected Answer: C\nMy vote goes to C, \nA. Snowball device only stores around 80 TB of data and uploading the newly transcribed data through datasync still goes through the slow connection between on-site and AWS. Outpost seem like the only feasible solution here that can satisfy both requriements","timestamp":"1708741860.0","comment_id":"820047","poster":"Siyuan_Zhu"},{"content":"Correct Answer A.\nD is incorrect as it takes 1024 Days Approx to transfer Petabyte of data.","poster":"CertArvind","upvote_count":"3","timestamp":"1706332140.0","comment_id":"789340"},{"upvote_count":"1","timestamp":"1705873920.0","comment_id":"783759","poster":"IvanHuang","content":"Selected Answer: AD\nThe best solution would be option A and D.\nOption A: Order and use an AWS Snowball Edge Compute Optimized device with NVIDIA Tesla modules to run the transcription algorithm. Use AWS DataSync to send the generated transcriptions to a transcription S3 bucket.\nThis option allows to use a device that has the necessary GPU for running the transcription algorithm and then use the AWS DataSync to send the generated transcriptions to the S3 bucket.\nOption D: Use AWS DataSync to ingest the audio files to Amazon S3. Create an AWS Lambda function to run the transcription algorithm on the audio file as it is uploaded to Amazon S3. Configure the function to write the generated transcriptions to the transcriptions S3 bucket.\nThis option allows to automatically transcribing the audio files as they are uploaded to S3. This means that the transcriptions are ready as soon as the audio files are uploaded and eliminates the need to transcribe the audio files separately."},{"timestamp":"1705407120.0","upvote_count":"1","content":"Selected Answer: A\nws.amazon.com/about-aws/whats-new/2020/07/aws-snowball-edge-compute-optimized-now-available-additional-aws-regions/","poster":"BTRYING","comment_id":"777621"},{"content":"Selected Answer: C\nI guess C, given that \"The company wants to store these transcriptions\". \nPetabytes audio data can keep in on-prem.","timestamp":"1702574640.0","comment_id":"745323","upvote_count":"2","poster":"wjohnny"},{"upvote_count":"1","poster":"VinceCar","timestamp":"1701086640.0","comments":[{"timestamp":"1701935820.0","poster":"VinceCar","content":"should be option A. There is petabytes of records.","comment_id":"737550","upvote_count":"1"}],"content":"Selected Answer: D\nThere is high-velocity networking. So A, B, C(Snowball, Snowcone, and Outputs) are not required.","comment_id":"728175"}],"answer_description":"","answers_community":["A (68%)","C (16%)","Other"],"timestamp":"2022-11-27 13:04:00","unix_timestamp":1669550640,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/88943-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","choices":{"B":"Order and use an AWS Snowcone device with Amazon EC2 Inf1 instances to run the transcription algorithm. Use AWS DataSync to send the resulting transcriptions to the transcription S3 bucket.","A":"Order and use an AWS Snowball Edge Compute Optimized device with an NVIDIA Tesla module to run the transcription algorithm. Use AWS DataSync to send the resulting transcriptions to the transcription S3 bucket.","C":"Order and use AWS Outposts to run the transcription algorithm on GPU-based Amazon EC2 instances. Store the resulting transcriptions in the transcription S3 bucket.","D":"Use AWS DataSync to ingest the audio files to Amazon S3. Create an AWS Lambda function to run the transcription algorithm on the audio files when they are uploaded to Amazon S3. Configure the function to write the resulting transcriptions to the transcription S3 bucket."},"answer":"A","answer_images":[],"question_id":108,"question_text":"A company is using Amazon SageMaker to build a machine learning (ML) model to predict customer churn based on customer call transcripts. Audio files from customer calls are located in an on-premises VoIP system that has petabytes of recorded calls. The on-premises infrastructure has high-velocity networking and connects to the company's AWS infrastructure through a VPN connection over a 100 Mbps connection.\n\nThe company has an algorithm for transcribing customer calls that requires GPUs for inference. The company wants to store these transcriptions in an Amazon S3 bucket in the AWS Cloud for model development.\n\nWhich solution should an ML specialist use to deliver the transcriptions to the S3 bucket as quickly as possible?"},{"id":"QcbBc7gEupMQ6Hc4QGaW","timestamp":"2022-11-28 18:20:00","answer_description":"","answer":"C","choices":{"B":"Ingest event data by using Amazon Kinesis Data Streams. Store the data in Amazon S3 by using Amazon Kinesis Data Firehose. Use AWS Glue to transform the most recent 10 minutes of data before inference.","A":"Ingest event data by using a GraphQLAPI in AWS AppSync. Store the data in an Amazon DynamoDB table. Use DynamoDB Streams to call an AWS Lambda function to transform the most recent 10 minutes of data before inference.","D":"Ingest event data by using Amazon Managed Streaming for Apache Kafka (Amazon MSK). Use an AWS Lambda function to transform the most recent 10 minutes of data before inference.","C":"Ingest event data by using Amazon Kinesis Data Streams. Use an Amazon Kinesis Data Analytics for Apache Flink application to transform the most recent 10 minutes of data before inference."},"question_text":"A company has a podcast platform that has thousands of users. The company has implemented an anomaly detection algorithm to detect low podcast engagement based on a 10-minute running window of user events such as listening, pausing, and exiting the podcast. A machine learning (ML) specialist is designing the data ingestion of these events with the knowledge that the event payload needs some small transformations before inference.\n\nHow should the ML specialist design the data ingestion to meet these requirements with the LEAST operational overhead?","discussion":[{"timestamp":"1671039000.0","comment_id":"745326","poster":"wjohnny","upvote_count":"7","content":"Selected Answer: B\nB, but it was possible to use Kinesis Data Firehose directly, insted Kinesis Data Stream"},{"content":"I think the answer is B.","timestamp":"1669656000.0","poster":"dunhill","upvote_count":"6","comment_id":"729463"},{"poster":"MultiCloudIronMan","content":"Selected Answer: C\nThis has the least latency","comment_id":"1305595","timestamp":"1730411220.0","upvote_count":"1"},{"comment_id":"1257336","upvote_count":"1","poster":"72cc81d","timestamp":"1722240420.0","content":"Selected Answer: C\nMoving window, and less components"},{"timestamp":"1712334360.0","upvote_count":"2","comment_id":"1190002","poster":"Gmishra","content":"Selected Answer: B\nC: Doesn't talk how to Store the data in Amazon S3"},{"comment_id":"1160209","poster":"AIWave","timestamp":"1708999260.0","upvote_count":"2","content":"Selected Answer: B\nWith Amazon Kinesis Data Analytics for Apache Flink, the ML specialist needs to manage the scaling and resource allocation for the Flink application, including determining the appropriate number of processing units (KPUs) and handling scaling based on the incoming data volume. This requires monitoring and adjusting resources as needed, adding to the operational overhead."},{"poster":"akgarg00","content":"Selected Answer: C\nC is the correct answer. B is workable but is not good for small transformation required in question.","timestamp":"1701083280.0","comment_id":"1081444","upvote_count":"1"},{"poster":"geoan13","timestamp":"1700042940.0","content":"C\nAmazon Managed Service for Apache Flink was previously known as Amazon Kinesis Data Analytics for Apache Flink. it allows you to process and analyze streaming data providing the capability to perform transformations on the streaming data.\nB - no need of using an extra service aws glue","comment_id":"1071302","upvote_count":"1"},{"poster":"DimLam","timestamp":"1697955000.0","upvote_count":"1","content":"Selected Answer: C\nI would choose C. As we need to implement our detection on running window, and B only allows us to perform operations on the latest 10 minutes of data.\n\nIf we choose B, we also need to decide how frequently to run the Glue job and it involves some orchestrator tools. C in other way works in real-time mode, and we don't need an orchestration tool to move the window. \n\nBased on this, I would go with C as it has less overhead","comment_id":"1050247"},{"content":"Selected Answer: C\nWould choose see as the transformation required is minimal which could be easily achieved with KDA (flink job)","upvote_count":"1","comment_id":"1045056","timestamp":"1697466420.0","poster":"backbencher2022"},{"upvote_count":"2","poster":"loict","content":"Selected Answer: B\nNot sure between B & C\n\nA. NO - too many moving parts\nB. YES - clean & elegant\nC. YES - works as well in batch mode\nD. NO - MSK is outdated","comment_id":"1002377","timestamp":"1694170680.0"},{"upvote_count":"2","timestamp":"1693755900.0","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/architecture/realtime-in-stream-inference-kinesis-sagemaker-flink/","comment_id":"997749","poster":"Shenannigan"},{"comment_id":"989180","upvote_count":"2","content":"Selected Answer: C\nAmazon Kinesis Data Streams is a fully managed real-time streaming service that can be used to ingest large amounts of data from multiple sources. This makes it a good choice for ingesting the event data from the podcast platform.\nAmazon Kinesis Data Analytics for Apache Flink is a fully managed service that can be used to process streaming data using Apache Flink. Apache Flink is a popular streaming processing framework that is known for its scalability and fault tolerance. This makes it a good choice for transforming the event data before inference.","poster":"Mickey321","timestamp":"1692879540.0"},{"comments":[{"poster":"DimLam","comment_id":"1050249","upvote_count":"1","timestamp":"1697955240.0","content":"No. it's not true. for running B we need some orchestrator to run the glue job frequently. but for C it is running constantly. So C doesn't have step with orchestration"}],"upvote_count":"2","content":"Selected Answer: B\nIt's B, \"\"LEAST operational overhead\"\", C is more operations overhead.","timestamp":"1688675040.0","poster":"ADVIT","comment_id":"944997"},{"upvote_count":"2","comment_id":"897462","timestamp":"1684059120.0","content":"Selected Answer: C\nFlink distributes the data across one or more stream partitions, and user-defined operators can transform the data stream.","poster":"dkx"},{"comment_id":"844323","content":"Selected Answer: B\nB is the answer. least management overhead","timestamp":"1679267400.0","comments":[{"comments":[{"poster":"DimLam","upvote_count":"1","comment_id":"1050251","content":"And for Glue you need to write a SQL or spark script. Extra work. Ah, yes. and for B you need to create an orchestrator to run the ETL jobs frequently","timestamp":"1697955300.0"}],"poster":"blanco750","comment_id":"844326","content":"And for C you have to author and build your Apache Flink application. extra work","timestamp":"1679267460.0","upvote_count":"1"}],"poster":"blanco750","upvote_count":"4"},{"poster":"pan_b","content":"Selected Answer: C\nAnswer should be C. https://aws.amazon.com/blogs/architecture/realtime-in-stream-inference-kinesis-sagemaker-flink/","comment_id":"837685","timestamp":"1678689360.0","comments":[],"upvote_count":"3"},{"comment_id":"833911","timestamp":"1678362120.0","content":"Selected Answer: C\nIt's C, this data requires small transformations who can be did with apache flink in kinesis data analytics. If wasn't this, then could be using Glue.","upvote_count":"2","poster":"Valcilio"},{"comments":[{"poster":"ccpmad","comment_id":"971100","upvote_count":"2","timestamp":"1691065380.0","content":"yes of course, use Kinesis Data Analytics for Apache Flink is the LEAST oprational overhead..."}],"content":"Selected Answer: C\nIt's C, the Glue's not necessary here because it says that is just small transformations.","timestamp":"1678223820.0","poster":"Valcilio","upvote_count":"2","comment_id":"832300"},{"poster":"Chelseajcole","content":"Using glue with pyspark you can have tumbling window but it still require coding. Kda is more a realistic answer even though the question didn’t require real-time feature","comment_id":"826173","timestamp":"1677695700.0","upvote_count":"2"},{"timestamp":"1677046680.0","poster":"Jerry84","upvote_count":"2","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/architecture/realtime-in-stream-inference-kinesis-sagemaker-flink/\n\nFrom the link above, C makes more sense.","comment_id":"817527"},{"content":"b https://aws.amazon.com/blogs/machine-learning/build-a-predictive-maintenance-solution-with-amazon-kinesis-aws-glue-and-amazon-sagemaker/\n\nc https://aws.amazon.com/blogs/architecture/realtime-in-stream-inference-kinesis-sagemaker-flink/\n\nBoth can do what is being asked.\n\nC seems possible, we transform the incoming data and directly call inference endpoint from flink app in KDA.","comments":[{"upvote_count":"1","comments":[{"timestamp":"1697955420.0","upvote_count":"1","content":"Operational overhead is not the same as coding effort.","poster":"DimLam","comment_id":"1050253"},{"content":"transform the data has to be coded in Glue too.","upvote_count":"2","poster":"vbal","timestamp":"1690082640.0","comment_id":"960047"}],"timestamp":"1676910960.0","content":"thinking over again between c and b:\n\nKKDA requires us to write JAVA application to transform the data this is not least “operational overhead”, there could be updates to the app or library we will need to manage.\nGlue will be easier, so may be choose B ?","comment_id":"815539","poster":"drcok87"}],"comment_id":"814134","timestamp":"1676814540.0","upvote_count":"2","poster":"drcok87"},{"upvote_count":"3","comment_id":"790973","content":"I would choose C- \nhttps://docs.aws.amazon.com/kinesisanalytics/latest/java/how-it-works.html\nWe can use tumbling window to process last 10 minutes of data - \nhttps://docs.amazonaws.cn/en_us/kinesisanalytics/latest/java/examples-tumbling.html\n\nI am not sure how a tumbling window algorithm would be implemented in Glue.","timestamp":"1674932280.0","poster":"uninit"}],"exam_id":26,"question_id":109,"answer_images":[],"topic":"1","question_images":[],"answers_community":["C (51%)","B (49%)"],"url":"https://www.examtopics.com/discussions/amazon/view/89148-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"C","isMC":true,"unix_timestamp":1669656000},{"id":"a4EhkBxAamam1YdvmQgF","answer_ET":"B","question_id":110,"answer_description":"","answers_community":["B (83%)","C (17%)"],"discussion":[{"timestamp":"1685340600.0","comment_id":"730078","content":"Selected Answer: B\nyes, It seems to be 'B'","poster":"Amit11011996","upvote_count":"8","comments":[{"comment_id":"733475","upvote_count":"4","timestamp":"1685677380.0","comments":[{"poster":"Maaayaaa","content":"Not sure if the shadow testing fits into the purpose here. \"The company has developed three versions of a machine learning (ML) model within Amazon SageMaker to classify document text.\"","timestamp":"1698668040.0","upvote_count":"2","comment_id":"885092"}],"poster":"VinceCar","content":"agreed, shadow testing is supported on SageMaker. \nhttps://aws.amazon.com/cn/blogs/aws/new-for-amazon-sagemaker-perform-shadow-tests-to-compare-inference-performance-between-ml-model-variants/"}]},{"comment_id":"1187542","timestamp":"1727798580.0","poster":"vkbajoria","content":"Selected Answer: B\nB is correct, from within single endpoint, we can create multiple production variant. \nWhen lambda called, it should have been each target variant instead of production variant in the verbiage","upvote_count":"2"},{"content":"C is fine\nB is not possible as it is single sagemaker endpoint (so we won't get prediction from all models for each document)\nD is wrong as we do not need three lambda functions\nA is also wrong as time gap is 3 seconds for which we should be running batch transform jobs","poster":"akgarg00","upvote_count":"1","timestamp":"1716801420.0","comment_id":"1081451"},{"upvote_count":"1","content":"Selected Answer: B\nWill go with B","poster":"backbencher2022","timestamp":"1713275700.0","comment_id":"1045037"},{"poster":"loict","timestamp":"1709912040.0","upvote_count":"2","comment_id":"1002521","content":"Selected Answer: B\nA. NO - you don't want to create a new job for each Lambda invokation\nB. YES - best practice\nC. NO - could work but does not leverage production variants which in-turn disable some built-in model performance evaluation features\nD. NO - more operationnal overhead to have 3 endpoints"},{"upvote_count":"1","poster":"chet100","comment_id":"993317","content":"Answer is B","timestamp":"1709232960.0"},{"upvote_count":"1","timestamp":"1708785420.0","poster":"Mickey321","content":"Selected Answer: B\nAlthough C sounds like a better option but B is less operational overhead at least for short term.","comment_id":"989196"},{"upvote_count":"2","timestamp":"1704580320.0","poster":"ADVIT","comment_id":"945003","content":"Selected Answer: B\nIt's B, you can use Invoke a Multi-Model Endpoint, when you call invoke_endpoint you need to provide which model filw to use.\n\nresponse1 = runtime_sagemaker_client.invoke_endpoint(\n EndpointName = \"MAIN_ENDPOINT\",\n TargetModel = \"model1.tar.gz\",\n Body = body)\nresponse2 = runtime_sagemaker_client.invoke_endpoint(\n EndpointName = \"MAIN_ENDPOINT\",\n TargetModel = \"model2.tar.gz\",\n Body = body)\nresponse3 = runtime_sagemaker_client.invoke_endpoint(\n EndpointName = \"MAIN_ENDPOINT\",\n TargetModel = \"model3.tar.gz\",\n Body = body)\n\nRef: https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html"},{"content":"Selected Answer: B\nB - the reason is not shadow testing since it is not named and does not require client logic. The reason is that it is possible to target a model https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html","timestamp":"1698676560.0","upvote_count":"1","poster":"cox1960","comment_id":"885192"},{"timestamp":"1695158160.0","content":"Selected Answer: B\nB it is which involves using single endpoint for multiple model versions","poster":"blanco750","upvote_count":"3","comment_id":"844329"},{"content":"Selected Answer: C\nI think the answer should be C. As there is no production version of the model identified, all the 3 models need to be invoked.","poster":"pan_b","upvote_count":"2","timestamp":"1694580780.0","comment_id":"837696"},{"content":"C, prod variant is used for traffic routing. All model needs to be invoked.","upvote_count":"1","poster":"testtaker1984","comment_id":"828819","timestamp":"1693816380.0"},{"comment_id":"792898","upvote_count":"2","poster":"BTRYING","timestamp":"1690718700.0","content":"Selected Answer: C\nC is correct"},{"comment_id":"729467","poster":"dunhill","timestamp":"1685287380.0","content":"I think the answer is B.","upvote_count":"3"}],"answer":"B","topic":"1","exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/89150-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"D":"Deploy each model to its own SageMaker endpoint. Create three AWS Lambda functions. Configure each Lambda function to call a different endpoint and return the results. Configure three S3 event notifications to invoke the Lambda functions when new documents are created.","A":"Configure an S3 event notification that invokes an AWS Lambda function when new documents are created. Configure the Lambda function to create three SageMaker batch transform jobs, one batch transform job for each model for each document.","C":"Deploy each model to its own SageMaker endpoint Configure an S3 event notification that invokes an AWS Lambda function when new documents are created. Configure the Lambda function to call each endpoint and return the results of each model.","B":"Deploy all the models to a single SageMaker endpoint. Treat each model as a production variant. Configure an S3 event notification that invokes an AWS Lambda function when new documents are created. Configure the Lambda function to call each production variant and return the results of each model."},"timestamp":"2022-11-28 18:23:00","answer_images":[],"isMC":true,"unix_timestamp":1669656180,"question_text":"A company wants to predict the classification of documents that are created from an application. New documents are saved to an Amazon S3 bucket every 3 seconds. The company has developed three versions of a machine learning (ML) model within Amazon SageMaker to classify document text. The company wants to deploy these three versions to predict the classification of each document.\n\nWhich approach will meet these requirements with the LEAST operational overhead?","question_images":[]}],"exam":{"isMCOnly":false,"isImplemented":true,"provider":"Amazon","numberOfQuestions":369,"id":26,"isBeta":false,"lastUpdated":"11 Apr 2025","name":"AWS Certified Machine Learning - Specialty"},"currentPage":22},"__N_SSP":true}