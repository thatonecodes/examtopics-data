{"pageProps":{"questions":[{"id":"xy1GLOWzOP8YC0323H2D","answer_description":"","answers_community":["D (100%)"],"choices":{"B":"Create an Amazon RDS manual snapshot every day. Delete manual snapshots that are older than 90 days.","A":"Create Amazon RDS automated backups. Set the retention period to 90 days.","D":"Create a backup plan that has a retention period of 90 days by using AWS Backup for Amazon RDS.","C":"Use the Amazon Aurora Clone feature for Oracle to create a point-in-time restore. Delete clones that are older than 90 days."},"topic":"1","exam_id":31,"isMC":true,"discussion":[{"timestamp":"1723399440.0","upvote_count":"7","comment_id":"1264256","poster":"nebajp","content":"Selected Answer: D\nCorrect Answer is D - Its fulfilling the requirement of point in time.\nAutomated Backup - Default retention period is 0-35 Days - so option A is wrong."},{"upvote_count":"5","timestamp":"1727396820.0","poster":"JoeTromundo","content":"Selected Answer: D\nA: Amazon RDS automated backups support a maximum retention period of 35 days. This option does not meet the requirement to retain backups for 90 days.\nB: This approach requires manual snapshot management, including scheduling snapshots and deleting old ones. This increases operational overhead and is prone to human error.\nC: This option is not applicable as Aurora Clone is a feature specific to Amazon Aurora and not available for Amazon RDS for Oracle. Additionally, it would require manual management of clones, increasing complexity.\nD: AWS Backup supports point-in-time recovery for Amazon RDS, enabling you to restore the database to any specific point within the defined retention period, up to 35 days. For the requirement of 14 days, AWS Backup easily supports this capability.","comment_id":"1289771"},{"comment_id":"1291484","poster":"spoved","content":"Selected Answer: D\nhttps://aws.amazon.com/getting-started/hands-on/amazon-rds-backup-restore-using-aws-backup/","upvote_count":"3","timestamp":"1727685240.0"},{"timestamp":"1724089380.0","poster":"[Removed]","upvote_count":"2","content":"Selected Answer: D\nD is right","comment_id":"1268864"},{"poster":"pujithacg8","timestamp":"1723381440.0","content":"Answer is D","comment_id":"1264164","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/amazon/view/145565-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":996,"question_text":"A company is migrating its on-premises Oracle database to an Amazon RDS for Oracle database. The company needs to retain data for 90 days to meet regulatory requirements. The company must also be able to restore the database to a specific point in time for up to 14 days.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"D","unix_timestamp":1723381440,"answer_images":[],"answer":"D","question_images":[],"timestamp":"2024-08-11 15:04:00"},{"id":"4FIpIc082OimNMW1cW1D","answer_ET":"B","answer_description":"","discussion":[{"timestamp":"1736240280.0","comment_id":"1337498","poster":"LeonSauveterre","upvote_count":"2","content":"Selected Answer: B\nA - You’d pay for high performance even when you don’t need it\nB - You only pay for the database capacity you use, making it cost-efficient for unpredictable traffic.\nC - DynamoDB is not even a relational database.\nD - Magnetic disks have significantly slower IOPS compared to SSDs.\n\nP.S. In the AWS context, magnetic storage refers to standard storage volumes (previously called \"Magnetic\" volumes) in EBS. It offers lower performance compared to modern SSD storage but is cheaper per GB."},{"poster":"AMEJack","upvote_count":"2","content":"Selected Answer: B\nOption D is out: Aurora data is stored in the cluster volume, which is a single, virtual volume that uses solid state drives (SSDs). \nSo option B is correct","timestamp":"1732888860.0","comment_id":"1319745"},{"comment_id":"1269629","upvote_count":"2","poster":"[Removed]","content":"Selected Answer: B\nB looks good","timestamp":"1724176440.0"},{"comment_id":"1269042","poster":"dhewa","timestamp":"1724126220.0","content":"Selected Answer: B\nB is my choice.","upvote_count":"3"}],"isMC":true,"timestamp":"2024-08-20 05:57:00","question_text":"A company is developing a new application that uses a relational database to store user data and application configurations. The company expects the application to have steady user growth. The company expects the database usage to be variable and read-heavy, with occasional writes.\n\nThe company wants to cost-optimize the database solution. The company wants to use an AWS managed database solution that will provide the necessary performance.\n\nWhich solution will meet these requirements MOST cost-effectively?","question_id":997,"question_images":[],"unix_timestamp":1724126220,"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/146062-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"B","choices":{"B":"Deploy the database on Amazon Aurora Serverless to automatically scale the database capacity based on actual usage to accommodate the workload.","A":"Deploy the database on Amazon RDS. Use Provisioned IOPS SSD storage to ensure consistent performance for read and write operations.","D":"Deploy the database on Amazon RDS. Use magnetic storage and use read replicas to accommodate the workload.","C":"Deploy the database on Amazon DynamoDB. Use on-demand capacity mode to automatically scale throughput to accommodate the workload."},"exam_id":31,"answer_images":[],"topic":"1"},{"id":"7CxKTU7DTFKhE0jXHrLb","choices":{"D":"Modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing.","C":"Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.","B":"Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.","A":"Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds."},"answer":"C","timestamp":"2022-10-11 20:50:00","answer_ET":"C","answer_images":[],"unix_timestamp":1665514200,"answers_community":["C (79%)","B (15%)","5%"],"discussion":[{"upvote_count":"64","timestamp":"1666646340.0","poster":"Six_Fingered_Jose","comment_id":"703369","comments":[{"poster":"Ello2023","upvote_count":"9","comments":[{"content":"Increasing the visibility timeout would give time to the lambda function to finish processing the message, which would make it disappear from the queue, and therefore only one email would be send to the user. \nIf the visibility timeout ends while the lambda function is still processing the message, the message will be returned to the queue and there another lambda function would pick it up and process it again, which would result in the user receiving two or more emails about the same thing.","comment_id":"871418","comments":[{"timestamp":"1730213580.0","content":"Increasing the visibility timeout allows a message to remain hidden for a longer period after being received by a Lambda function. However, this only prevents other instances of the function from processing the same message during that time. If the function takes longer to process than expected, messages can still become visible again and be retried, potentially leading to duplicates.","comment_id":"1304500","upvote_count":"3","poster":"PaulEkwem"},{"content":"I agree with your answer explanation","timestamp":"1688020560.0","comment_id":"937621","poster":"aadityaravi8","upvote_count":"1"},{"comment_id":"979553","poster":"Abdou1604","timestamp":"1691870220.0","content":"i aggree because the issue is multiple received email for an image uploaded","upvote_count":"1"}],"upvote_count":"12","timestamp":"1681611720.0","poster":"Robrobtutu"},{"comment_id":"777299","poster":"MrAWS","timestamp":"1673841540.0","content":"I tend to agree with you. See my comments above.","upvote_count":"2"}],"comment_id":"777020","content":"I am confused. If the email has been sent many times already why would they need more time? \nI believe SQS Queue Fifo will keep in order and any duplicates with same ID will be deleted. Can you tell me where i am going wrong? Thanks","timestamp":"1673812680.0"},{"poster":"MutiverseAgent","content":"I agree it seems solution is C, as thought the SQS FIFO makes sense deduplication id would make NO sense as the system who put messages in the queue is S3 events; and as far as I know S3 do not send duplicated events. Also, the question mention that users are complaining about receiving multiple emails for each email, which is different to say they are receiving occasionally a repeated email; so my guess is SQS FIFO is not needed.","timestamp":"1689162060.0","comment_id":"949719","upvote_count":"2"},{"timestamp":"1691585460.0","upvote_count":"5","poster":"JoeGuan","comments":[{"content":"amazon s3 doesn't support fifo queues","timestamp":"1692853020.0","poster":"PLN6302","upvote_count":"3","comment_id":"988842"}],"content":"The FIFO SQS is for solving a different problem, where items in the queue require order. You cannot simply switch from a standard queue to fifo queue. Duplicate emails are a common issue with a standard queue. The documentation consistently reminds us that duplicate emails can occur, and the solution is not to create a FIFO queue, but rather adjust the configuration parameters accordingly.","comment_id":"976658"}],"content":"Selected Answer: C\nanswer should be C,\nusers get duplicated messages because -> lambda polls the message, and starts processing the message.\nHowever, before the first lambda can finish processing the message, the visibility timeout runs out on SQS, and SQS returns the message to the poll, causing another Lambda node to process that same message.\nBy increasing the visibility timeout, it should prevent SQS from returning a message back to the poll before Lambda can finish processing the message"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\n\nthis is important part:\nImmediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.","comment_id":"693607","poster":"brushek","upvote_count":"16","timestamp":"1665635820.0"},{"poster":"AwsAbhiKumar","timestamp":"1739542200.0","upvote_count":"2","comment_id":"1356452","comments":[{"timestamp":"1742658720.0","poster":"jerryl","upvote_count":"1","comment_id":"1401949","content":"I think there is another question that mention the visibility timeout concept as well\nMy understanding is for SQS, you have producer (inject data to queue) and consumer (get data from queue), the SQS itself does not know who will be the producer or consumer, this is implemented at the producer/consumer side like a Lambda function\nSo ensuring the read sequence doesnt mean no duplication at all\nInstead you need to ensure a data (element in the queue) not to be read multiple times by consumer\nAnd this is done by adjusting the visible timeout, making it long enough will make sure each consumer (Lambda in this case) can either consume the data completely (or fail) without other consumer's interruption"}],"content":"Selected Answer: B\nI believe answer is B cause FIFO queues provide exactly-once processing and guarantee that messages are delivered in order. They also support message deduplication, which prevents processing duplicate messages within a specified deduplication interval.\n\nWhereas a longer visibility timeout might prevent messages from being processed more than once if processing takes longer than expected. However, it does not eliminate the inherent at-least-once delivery behavior of standard queues, which can still cause duplicates."},{"poster":"Dharmarajan","timestamp":"1738178520.0","content":"Selected Answer: C\nSQS feature of \"Visibility timeout\" sets the time for getting acknowledgement from processor that the message has been successfully processed. I think it should have been called \"Invisibility \" rather than \"Visibility\" timeout. Anyways, I think AWS engineers have thought this Queue service through well. So if the processing time> visibility timeout, the message comes back and is picked up by another execution of lambda funtion. It is such a small thing but it can easily rake up lambda usage if not considered!","comment_id":"1348732","upvote_count":"1"},{"poster":"zdi561","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html , standard queue is delivered at lease once, the duplication can not be avoided","upvote_count":"1","comment_id":"1347725","timestamp":"1738039320.0"},{"timestamp":"1736719620.0","content":"Selected Answer: C\nC. Aumentar o tempo limite de visibilidade:\n\nUm tempo limite de visibilidade maior reduz a chance de reprocessamento prematuro, mas não resolve completamente o problema de mensagens duplicadas.\nÉ mais complexo ajustar adequadamente o tempo de visibilidade para sincronizar com o processamento da função Lambda e o tempo limite.","upvote_count":"1","poster":"Rcosmos","comment_id":"1339690"},{"comment_id":"1336776","upvote_count":"1","content":"Selected Answer: C\nUsing Visibility Timeout is the most suitable option here with the least operational overhead.","poster":"satyaammm","timestamp":"1736085420.0"},{"upvote_count":"1","timestamp":"1734580800.0","poster":"salman7540","content":"Selected Answer: C\nFIFO queues are designed to never introduce duplicate messages. However, a message producer might introduce duplicates if another producer (lambda in this case) picks up same fifo message after visibility timeout over but previous lambda was still processing it. Hence correct answer is C.","comment_id":"1328849"},{"timestamp":"1733205600.0","upvote_count":"1","poster":"JA2018","comment_id":"1321258","content":"Selected Answer: B\nit should be B\n\n- Using a FIFO (First In, First Out) queue ensures that messages are processed in the exact order they are received, preventing duplicates from being processed multiple times by the Lambda function. By setting up message deduplication IDs, you can further guarantee that even if the same message is sent multiple times, only one instance will be processed.\n\n- When dealing with potential duplicate messages in an event-driven architecture, switching to an SQS FIFO queue with message deduplication is generally the most efficient and reliable solution to prevent redundant processing."},{"content":"Selected Answer: D\nAns D - is my preferred choice because it removes the record and therefore eliminates the possibility of duplication; the only snag here is ensuring the SQS record is locked. \nAns C, which has the most votes will work but is less robust - but perhaps it is the cheapest option. \nAns A, the author's preference, won't work unless the Lambda function is guaranteed to complete within 30secs - and nowhere is that stated","comment_id":"1284270","upvote_count":"2","poster":"PaulGa","timestamp":"1726497540.0"},{"poster":"jaradat02","upvote_count":"1","content":"the correct answer is C, although B and D might also work, C is the one that achieves the least operational overhead condition.","timestamp":"1721578560.0","comment_id":"1252570"},{"content":"Selected Answer: B\nB. Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.\n\nHere’s why this option is suitable:\n\nSQS FIFO Queue:\nAmazon SQS FIFO (First-In-First-Out) queues ensure that the order of messages is preserved.\nEach message in a FIFO queue has a unique message deduplication ID.\nBy using a FIFO queue, you can prevent duplicate messages from being processed.\nMessage Deduplication ID:\nWhen sending messages to a FIFO queue, set the message deduplication ID to ensure that identical messages are treated as duplicates.\nIf a message with the same deduplication ID is sent within a 5-minute window, it is considered a duplicate and discarded.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","timestamp":"1715002020.0","comments":[{"comment_id":"1219498","timestamp":"1716809280.0","content":"This wouldn't work as expected because the issue is not the order or the queue receiving multiple times the same message. The issue is that the Lambda executes the same message multiple times.\nThe FIFO deduplication ID works when the producer sends the message, not when the consumer is receiving the message. (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html)\nFor this to work, the Lambda function would need to know/keep the last N deduplication ID to guarantee that it isn't processing the same message, which goes against Lambda's statelessness.\n\nTherefore, for this case, B is the best answer.","upvote_count":"1","poster":"Anthony_Rodrigues"}],"poster":"Solomon2001","comment_id":"1207351","upvote_count":"2"},{"content":"Selected Answer: B\nB: Directly addresses the issue of duplicates by ensuring exact-once processing and message ordering, which mitigates the risk without requiring adjustments based on anticipated processing times or additional application logic to manage potential duplicates.\n\nSo, while increasing the visibility timeout could help in managing when messages are available for processing again, it doesn't provide a structural solution to the problem of duplicate processing in the way that using an SQS FIFO queue does, nor does it ensure the operational simplicity and reliability that comes with eliminating duplicates at the source.","poster":"824c449","comment_id":"1205036","timestamp":"1714561860.0","upvote_count":"2"},{"timestamp":"1714484880.0","content":"Selected Answer: C\nto all those who are considering option B, Its INCORRECT.\nAmazon Simple Queue Service FIFO (First-In-First-Out) queues aren't supported as an Amazon S3 event notification destination.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html","poster":"ManikRoy","upvote_count":"3","comment_id":"1204594"},{"poster":"Uzbekistan","comment_id":"1179638","content":"Selected Answer: B\nGiven the requirement to resolve the issue of multiple email messages being sent to users with the least operational overhead, the most appropriate solution is:\n\nB. Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.\n\nExplanation:\n\nSQS FIFO Queue: FIFO (First-In-First-Out) queues in SQS ensure that the order in which messages are sent and received is strictly preserved and that each message is processed only once. By switching to an SQS FIFO queue, you can prevent the Lambda function from processing duplicate messages.","comments":[{"content":"Its INCORRECT\nAmazon Simple Queue Service FIFO (First-In-First-Out) queues aren't supported as an Amazon S3 event notification destination.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html","upvote_count":"1","timestamp":"1714484820.0","comment_id":"1204593","poster":"ManikRoy"}],"upvote_count":"1","timestamp":"1711054260.0"},{"timestamp":"1708066980.0","upvote_count":"1","poster":"vip2","content":"Selected Answer: C\n'Visibility Timeout' is suitable(better) solution to solve the issue.","comment_id":"1151789"},{"upvote_count":"2","comments":[],"timestamp":"1707624180.0","content":"I will stick with \"B\", the correct answer.","poster":"shwelin","comment_id":"1146981"},{"upvote_count":"2","timestamp":"1705411620.0","content":"Selected Answer: B\nAll answers are wrong, the answer B","comment_id":"1124238","poster":"0xE8D4A51000"},{"poster":"awsgeek75","timestamp":"1705271220.0","content":"Selected Answer: C\nIn this setup the only way to get multiple emails is when same image is processed multiple times. This only happens when another Lambda starts processing while the previous one hasn't finished processing. \nIncreasing the SQS queue timeout to be greater than Lambda timeout will ensure that other Lambda can't see the SQS message before previous Lambda finishes processing or times out.\nSo C is best answer\nA: Long polling won't fix anything\nB: FIFO is nice idea but how will the Lambda function know it has got a duplicate message?\nD: Wrong as in case of Lambda timeout that message is lost without being processed","comment_id":"1122926","upvote_count":"1"},{"content":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\n\nthis is important part:\nImmediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours","upvote_count":"3","comment_id":"1056663","poster":"Ruffyit","timestamp":"1698567120.0"},{"content":"Selected Answer: C\nleast operational overheads","comment_id":"1039997","poster":"lqw","upvote_count":"1","timestamp":"1696984320.0"},{"timestamp":"1696562700.0","upvote_count":"2","comment_id":"1026201","poster":"prabhjot","content":"ans B - Option A (long polling), Option C (increasing visibility timeout), and Option D (deleting messages immediately) do not address the root cause of the problem, which is the duplication of messages in the queue."},{"upvote_count":"1","timestamp":"1695737340.0","comment_id":"1017844","poster":"vijaykamal","content":"Long polling is incorrect...it just means that SQS queue is connected after specific interval instead of looking for messages in queue in very short interval...long polling saves money but does not help to remove duplicate.\n\nCorrect Answer: C"},{"content":"Selected Answer: A\nI think A is correct.\nhttps://aws.amazon.com/blogs/developer/polling-messages-from-a-amazon-sqs-queue/#:~:text=When%20disabling,more%20API%20calls.","comment_id":"1011348","timestamp":"1695129960.0","upvote_count":"1","poster":"hieulam"},{"poster":"kwang312","content":"D is an incorrect answer because Lamda automatically deletes message from the queue when finish process","comment_id":"990138","upvote_count":"1","timestamp":"1692972360.0"},{"upvote_count":"1","comment_id":"988831","content":"Selected Answer: C\nImmediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents all consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html#:~:text=SQS%20sets%20a-,visibility%20timeout,-%2C%20a%20period%20of","timestamp":"1692852000.0","poster":"TariqKipkemei"},{"poster":"Guru4Cloud","upvote_count":"1","content":"Selected Answer: C\nI would go with the C.","comment_id":"978753","timestamp":"1691767140.0"},{"upvote_count":"4","comment_id":"934864","poster":"Olaunfazed","timestamp":"1687822140.0","content":"Answer is B.\n\nB. Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.\n\nBy changing the SQS standard queue to an SQS FIFO (First-In-First-Out) queue, you can ensure that messages are processed in the order they are received and that each message is processed only once. FIFO queues provide exactly-once processing and eliminate duplicates.\n\nUsing the message deduplication ID feature of SQS FIFO queues, you can assign a unique identifier (such as the S3 object key) to each message. SQS will check the deduplication ID of incoming messages and discard duplicate messages with the same deduplication ID. This ensures that only unique messages are processed by the Lambda function.\n\nThis solution requires minimal operational overhead as it mainly involves changing the queue type and using the deduplication ID feature, without requiring modifications to the Lambda function or adjusting timeouts.","comments":[{"timestamp":"1694394240.0","content":"compare with C, SQS FIFO must take time than C, B is important when you concern about ordering","comment_id":"1004386","poster":"dangvanduc90","upvote_count":"1"}]},{"poster":"cookieMr","timestamp":"1687422060.0","upvote_count":"2","comment_id":"930287","content":"Selected Answer: C\nA. Long polling doesn't directly address the issue of multiple invocations of the Lambda for the same message. Increasing the ReceiveMessage may not completely prevent duplicate invocations.\n\nB. Changing the queue type from standard to FIFO requires additional considerations and changes to the application architecture. It may involve modifying the event configuration and handling message deduplication IDs, which can introduce operational overhead.\n\nD. Deleting messages immediately after reading them may lead to message loss if the Lambda encounters an error or fails to process the image successfully. It does not guarantee message processing and can result in data loss.\n\nC. By setting the visibility timeout to a value greater than the total time required for the Lambda to process the image and send the email, you ensure that the message is not made visible to other consumers during processing. This prevents duplicate invocations of the Lambda for the same message."},{"upvote_count":"3","poster":"Abrar2022","comment_id":"903819","timestamp":"1684740600.0","content":"FIFO - IS A SOLUTION BUT REQUIRES OPERATIONAL OVERHEAD.\nINCREASING VISIBILITY TIMEOUT - REQUIRES FAR LESS OPERATIONAL OVERHEAD."},{"comment_id":"903458","upvote_count":"2","timestamp":"1684692900.0","content":"Selected Answer: C\nI go for option C.","poster":"Bmarodi"},{"poster":"Rahulbit34","timestamp":"1683047040.0","upvote_count":"1","content":"SQS VISIBILITY TIMEOUT can help preventing the reprocessing of the message from the queue. By default the timeout is 30 secs, min 0 and max is 12 hours.","comment_id":"887715"},{"poster":"quanbui","comment_id":"881052","upvote_count":"2","timestamp":"1682477280.0","content":"Selected Answer: C\nccccccc"},{"poster":"tikytaka","timestamp":"1681888260.0","upvote_count":"1","comment_id":"874336","content":"Apologies, I meant A is wrong"},{"timestamp":"1681888200.0","comment_id":"874334","upvote_count":"1","content":"C is wrong:\n'When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.'","poster":"tikytaka"},{"timestamp":"1681642260.0","content":"I took the exam in April 2023, out of 65 Questions only 25-30 were from the dumps. I got passed (820!) but these dumps are singly not reliable. I thing is sure, going through these questions and working itself for the answers will help to pass the actual exam.","upvote_count":"2","comments":[{"timestamp":"1682863860.0","content":"Hola KUl91, cual otra fuente de test de estudio utilizaste?","poster":"lquintero","upvote_count":"1","comment_id":"885308"}],"poster":"kuls91","comment_id":"871669"},{"upvote_count":"1","content":"Selected Answer: C\nKey is minimal operational overhead.","comment_id":"854245","timestamp":"1680084360.0","poster":"kraken21"},{"poster":"PaoloRoma","comment_id":"853234","content":"Selected Answer: B\nThe only options that can rule out duplicated messages is B) as per doc \"Unlike standard queues, FIFO queues don't introduce duplicate messages. FIFO queues help you avoid sending duplicates to a queue.\"\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html\n\nAnswer C, though with less ops overheads, doesn't guarantee to rule out the event to send multiple emails related to the same image. This will avoid (minimise) processing the same message two or more times, however do not solve the problem of duplicated messages.","upvote_count":"4","timestamp":"1680006720.0","comments":[{"poster":"MinaSaied","comment_id":"948365","upvote_count":"1","content":"You can't change the queue type after you create a queue.","timestamp":"1689019320.0"}]},{"comment_id":"839313","upvote_count":"1","poster":"osmk","timestamp":"1678829220.0","content":"C In an application under heavy load or with spiky traffic patterns, it’s recommended that you:\n\nSet the queue’s visibility timeout to at least six times the function timeout value. This allows the function time to process each batch of records if the function execution is throttled while processing a previous batch.https://docs.aws.amazon.com/lambda/latest/operatorguide/sqs-retries.html"},{"upvote_count":"2","content":"Selected Answer: C\nTo address the issue of users receiving multiple email messages for every uploaded image with the least operational overhead, increasing the visibility timeout in the SQS queue is the best solution. This requires no additional configuration and thus has the least operational overhead compared to other options. However, this solution does not completely prevent duplicates, so there is still a possibility of duplicate emails being sent. While using a FIFO queue can prevent duplicates, it requires additional configuration and therefore may have higher operational overhead.","comment_id":"838612","poster":"lovelazur","timestamp":"1678777560.0"},{"comment_id":"822559","upvote_count":"1","content":"Here https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html it says this for sqs standard. For standard queues, the visibility timeout isn't a guarantee against receiving a message twice. For more information, see At-least-once delivery.","timestamp":"1677423180.0","poster":"Steve_4542636"},{"timestamp":"1673842620.0","comment_id":"777307","upvote_count":"1","content":"the only thing that addresses deduplication is using a FIFO queue OR by coding idempotency into your code. Increasing the visibility timeout only means you can delete the message you were processing, it doesn't handle the duplicates and therefore doesn't answer the question of \n\n\"What should the solutions architect do to resolve this issue \"","poster":"MrAWS","comments":[{"poster":"ces26015","timestamp":"1674660540.0","upvote_count":"2","comment_id":"787813","content":"the case is not about dups on the queue, but invoking the lambda function many times"},{"content":"I believe this is more about preventing duplicates from happening than it is with what to do with duplicates if they happen.","poster":"Robrobtutu","comment_id":"871419","timestamp":"1681611840.0","upvote_count":"1"}]},{"content":"C is right answer here\nhttps://www.examtopics.com/discussions/amazon/view/83096-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"759204","timestamp":"1672187820.0","poster":"techhb","upvote_count":"1"},{"comments":[{"content":"have a read of the page you linked too it states\n\n\"For standard queues, the visibility timeout isn't a guarantee against receiving a message twice. For more information, see At-least-once delivery.\"","timestamp":"1673842380.0","comment_id":"777306","poster":"MrAWS","upvote_count":"2"}],"timestamp":"1672098120.0","poster":"PassNow1234","comment_id":"757942","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\n\nstill valid","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://aws.amazon.com/sqs/faqs/\nSQS - LongPolling decreases the number of API calls made to SQS while increasing the efficiency and reducing latency of your application \nLong polling reduces the number of empty responses by allowing Amazon SQS to wait a specified time for a message to become available in the queue before sending a response. Also, long polling eliminates false empty responses by querying all of the servers instead of a sampling of server","comment_id":"755117","timestamp":"1671912300.0","poster":"psr83"},{"timestamp":"1671817920.0","upvote_count":"2","comment_id":"754399","poster":"naabe","content":"Selected Answer: D\nLEAST operational overhead Option D"},{"upvote_count":"2","content":"Selected Answer: D\nOption D, The solution architect should modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing. This is the least operationally overhead solution because it does not require any changes to the SQS queue or any additional configuration.","timestamp":"1671652920.0","comment_id":"752714","poster":"Buruguduystunstugudunstuy","comments":[{"upvote_count":"2","poster":"techhb","comment_id":"759203","content":"what happens if processing fails ???","timestamp":"1672187760.0"},{"comment_id":"752715","poster":"Buruguduystunstugudunstuy","content":"Option A, setting up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds, could potentially reduce the number of duplicate messages received by the Lambda function, but it would also increase the latency of message delivery and potentially increase costs.\n\nOption B, changing the SQS standard queue to an SQS FIFO queue and using the message deduplication ID to discard duplicate messages, would require changes to the queue and could potentially cause disruptions to the application if not implemented correctly. It may also require additional overhead to manage the message deduplication ID.","timestamp":"1671652980.0","upvote_count":"3","comments":[{"poster":"Buruguduystunstugudunstuy","upvote_count":"2","content":"Option C, increasing the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout, could also potentially reduce the number of duplicate messages received by the Lambda function, but it would also increase the time it takes for messages to be available for processing again if the function fails. This could result in increased latency and potentially higher costs.","timestamp":"1671652980.0","comment_id":"752716"}]}]},{"timestamp":"1671513420.0","upvote_count":"3","comment_id":"750518","poster":"Nandan747","content":"Selected Answer: C\nAt first I thought the answer should be B, since they specifically mentioned it is a Standard Queue and we know that in Std queue, we do get some duplicates. But the real catch over here is EVERY time the users are getting duplicate. So it must be the VisibilityTimeout issue which isn't long enough so EVERY time the message goes back on the queue before processing by one Lambda is completed and at the same time is being picked up by another function for processing."},{"poster":"Richaqua","comment_id":"750390","content":"Selected Answer: D\nSince SQS queue does not delete the message by default, Lambda function can be modified to delete the messages after it has been processed.","timestamp":"1671501420.0","upvote_count":"1"},{"timestamp":"1671403740.0","content":"Selected Answer: C\nOption C is the most probable case.\nThough option B can also cause some duplicates but given this is happening for every request/users C seems to be real root cuase.","comment_id":"749262","poster":"career360guru","upvote_count":"1"},{"content":"C is correct","upvote_count":"1","comment_id":"723768","timestamp":"1669049940.0","poster":"Wpcorgan"},{"timestamp":"1667351340.0","content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/83096-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"Hunkie","upvote_count":"1","comment_id":"709502"},{"upvote_count":"2","poster":"rob74","content":"I exlude Polling because-->\"The maximum long polling wait time is 20 seconds\"\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","timestamp":"1666336860.0","comment_id":"700608"},{"timestamp":"1665514200.0","upvote_count":"4","poster":"MXB05","content":"Selected Answer: C\nA can not be correct, long polling will only ensure that all images are retrieved from all SQS servers in one query. If the same message triggers the lambda function twice it is likely because the visibility timeout isn't long enough and lambda didn't repsond in time with a deletion of the message ->> C is correct","comment_id":"692345"}],"question_text":"An image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function that processes the images and sends the results to users through email.\nUsers report that they are receiving multiple email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda function more than once, resulting in multiple email messages.\nWhat should the solutions architect do to resolve this issue with the LEAST operational overhead?","question_images":[],"question_id":998,"exam_id":31,"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/85185-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1"},{"id":"KhA6w7oDHIH4Ga20L1cO","question_text":"A company hosts its application on several Amazon EC2 instances inside a VPC. The company creates a dedicated Amazon S3 bucket for each customer to store their relevant information in Amazon S3.\n\nThe company wants to ensure that the application running on EC2 instances can securely access only the S3 buckets that belong to the company’s AWS account.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/147313-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer_ET":"C","timestamp":"2024-09-11 07:13:00","unix_timestamp":1726031580,"isMC":true,"topic":"1","question_images":[],"discussion":[{"content":"Selected Answer: C\nB, C is not secure way because NAT gateway is for internet-facing outbound.\nA is not correct because company will create dedicated bucket for each customers it means number of buckets will increase dynamically. so you cant list all on profile.","poster":"toyaji","comment_id":"1281961","upvote_count":"5","timestamp":"1726031580.0"},{"comment_id":"1337501","timestamp":"1736241060.0","poster":"LeonSauveterre","upvote_count":"3","content":"Selected Answer: C\nA - It doesn’t explicitly ensure that access is limited to buckets only owned by the company. We need further adjustments to enforce this restriction (like adding a Condition key).\nB & D - NAT gateways route traffic over the internet, so it's not so secure or cost-effective as S3 Gateway Endpoint.\nC - Use an S3 Gateway Endpoint and restrict access using an IAM policy with a condition to deny access to any S3 resource outside the company’s account. Simple, secure, and easy to understand."},{"comment_id":"1335090","poster":"trinh_le","content":"Selected Answer: A\nA. Allows option fits to minimum permission policy\nC. Deny action is complexity and hard to debug","upvote_count":"1","timestamp":"1735693860.0"},{"comment_id":"1334290","poster":"Denise123","timestamp":"1735580280.0","content":"Selected Answer: C\nThe requirement in the question is to ensure that the EC2 instances can securely access only the S3 buckets that belong to (are owned by) the company's AWS account.\n\nIn that context, using the \"StringNotEquals\": {\"S3ResourceAccount\":[\"CompanyAWSAcctNumber\"]} condition key in the IAM instance profile policy is a valid approach.\n\nThis condition key restricts access to S3 buckets that are not owned by the specified AWS account number (the company's account). By setting a Deny with this condition, it effectively allows access only to the S3 buckets owned by the company's AWS account.","upvote_count":"3"},{"poster":"Denise123","upvote_count":"1","comment_id":"1334285","timestamp":"1735579980.0","content":"Selected Answer: A\nBy choosing Option A, the company can leverage the simplicity and security of VPC Gateway Endpoints for Amazon S3, combined with IAM instance profile policies to grant selective access to the required S3 buckets. This solution provides the necessary isolation and access control with minimal operational overhead, making it the most efficient and scalable approach.\nOption C and other options have limitations or introduce additional operational overhead Option C (Creating a Gateway Endpoint and using a Deny policy with S3ResourceAccount condition) is not a valid approach. The S3ResourceAccount condition key is used to restrict access based on the AWS account that owns the S3 bucket, not the AWS account that is accessing the bucket.","comments":[{"timestamp":"1738412640.0","comment_id":"1349861","poster":"GOTJ","content":"Errare humanum est :)","upvote_count":"1"}]},{"timestamp":"1735136040.0","upvote_count":"1","comment_id":"1331570","poster":"EllenLiu","content":"Selected Answer: A\nanswer C cannot restrict ec2 from accessing s3 from other accounts.\nOnly by providing access to specific buckets in IAM policy on EC2 are we able to achieve this."},{"content":"Selected Answer: A\nOption C will deny the other S3 buckets but will not Allow access to the specified Buckets, thus we should have Allow rules.\nOption A will allow the specified Buckets and implicit Deny the other buckets.","upvote_count":"2","comment_id":"1319748","timestamp":"1732889160.0","poster":"AMEJack","comments":[{"upvote_count":"1","poster":"ARV14","comment_id":"1320463","content":"You will keep updating the policy with new s3 buckets as user base grows? Operational overhead?","timestamp":"1733032260.0"}]},{"timestamp":"1732785180.0","comment_id":"1319135","upvote_count":"1","poster":"youkarthik","content":"Selected Answer: A\nA as per gen AIs"},{"content":"Selected Answer: C\nAnswer is C. Just specify only the company AWS account number, rather than listing all the Buckets","poster":"Bwhizzy","timestamp":"1728917220.0","upvote_count":"3","comment_id":"1297692"},{"upvote_count":"2","comment_id":"1297079","poster":"XXXXXlNN","content":"Vote A","timestamp":"1728845580.0"},{"timestamp":"1727743380.0","content":"VOTE A","poster":"siheom","comment_id":"1291709","upvote_count":"1"},{"upvote_count":"1","comment_id":"1284873","content":"la respuesta correcta es la A :Los buckets son servicios globales.( o sea no están en una VPC ni Subnet ), entonces no hace falta que estén en una subred publica o privada ; los Nat Gateway son para redes publicas o privadas .Entonces ahí descarta B,C y D .Cuando quieres conectar un recurso de Global Service se usa Endpoint Gateway por eso la respuesta es A .","poster":"viejito","timestamp":"1726506360.0"}],"choices":{"C":"Create a gateway endpoint for Amazon S3 that is attached to the VPUpdate the IAM instance profile policy with a Deny action and the following condition key:","B":"Create a NAT gateway in a public subnet with a security group that allows access to only Amazon S3. Update the route tables to use the NAT Gateway.","A":"Create a gateway endpoint for Amazon S3 that is attached to the VPC. Update the IAM instance profile policy to provide access to only the specific buckets that the application needs.","D":"Create a NAT Gateway in a public subnet. Update route tables to use the NAT Gateway. Assign bucket policies for all buckets with a Deny action and the following condition key:"},"question_id":999,"answer":"C","answer_images":[],"answer_description":"","answers_community":["C (70%)","A (30%)"]},{"id":"pwuwZM5U5sHMvsL1ApjL","question_id":1000,"question_images":[],"isMC":true,"question_text":"A company is building a cloud-based application on AWS that will handle sensitive customer data. The application uses Amazon RDS for the database, Amazon S3 for object storage, and S3 Event Notifications that invoke AWS Lambda for serverless processing.\n\nThe company uses AWS IAM Identity Center to manage user credentials. The development, testing, and operations teams need secure access to Amazon RDS and Amazon S3 while ensuring the confidentiality of sensitive customer data. The solution must comply with the principle of least privilege.\n\nWhich solution meets these requirements with the LEAST operational overhead?","answer_ET":"B","answer":"B","answers_community":["B (100%)"],"choices":{"D":"Use AWS Organizations to create separate accounts for each team. Implement cross-account IAM roles with least privilege. Grant specific permission for RDS and S3 access based on team roles and responsibilities.","C":"Create individual IAM users for each member in all the teams with role-based permissions. Assign the IAM roles with predefined policies for RDS and S3 access to each user based on user needs. Implement IAM Access Analyzer for periodic credential evaluation.","A":"Use IAM roles with least privilege to grant all the teams access. Assign IAM roles to each team with customized IAM policies defining specific permission for Amazon RDS and S3 object access based on team responsibilities.","B":"Enable IAM Identity Center with an Identity Center directory. Create and configure permission sets with granular access to Amazon RDS and Amazon S3. Assign all the teams to groups that have specific access with the permission sets."},"discussion":[{"poster":"kevindu","timestamp":"1723759920.0","upvote_count":"9","content":"Selected Answer: B\nIs there anyone who has recently passed the exam who can tell me approximately how many of the original questions are in the actual exam?","comment_id":"1266722"},{"poster":"dhewa","comment_id":"1269019","content":"Selected Answer: B\nIAM Identity Center: This service simplifies user management by centralizing credentials and access control.\nPermission Sets: You can create granular permission sets that align with the principle of least privilege, ensuring that each team has only the access they need.\nGroup Assignments: By assigning teams to groups with specific permission sets, you streamline access management and reduce the complexity of individual user permissions.\nThis approach minimizes operational overhead while maintaining secure and compliant access to sensitive customer data","upvote_count":"5","timestamp":"1724123040.0"},{"upvote_count":"1","timestamp":"1736242860.0","poster":"LeonSauveterre","content":"Selected Answer: B\nA - Managing and customizing IAM roles for multiple teams can become more and more complex and high-maintenance, especially if team responsibilities evolve.\nB - IAM Identity Center (formerly AWS SSO) allows you to manage/update access for multiple teams from one place.\nC - TOO MUCH WORK.\nD - Cross-account access is harder to manage and introduces additional operational overhead.","comment_id":"1337509"},{"timestamp":"1727398320.0","upvote_count":"2","comment_id":"1289778","poster":"JoeTromundo","content":"Selected Answer: B\nOption A is goo but not the best, which is option B."},{"timestamp":"1723771680.0","content":"Selected Answer: B\nAnswer is B","upvote_count":"3","poster":"aragon_saa","comment_id":"1266773"}],"answer_images":[],"unix_timestamp":1723759920,"url":"https://www.examtopics.com/discussions/amazon/view/145821-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","exam_id":31,"timestamp":"2024-08-16 00:12:00","answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","isImplemented":true,"numberOfQuestions":1019,"id":31,"isBeta":false,"isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03"},"currentPage":200},"__N_SSP":true}