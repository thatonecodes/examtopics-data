{"pageProps":{"questions":[{"id":"9kfGiBouANIXt10WLfJ6","unix_timestamp":1671167820,"topic":"1","answer_description":"","answer_ET":"B","choices":{"B":"Configure AWS DataSync. Configure the DataSync agent and deploy it to the local network. Create a transfer task and start the transfer.","D":"Configure AWS Transfer for FTP. Configure the FTP client with credentials. Script the client to connect and sync to start the transfer.","A":"Configure CloudEndure. Create a project and deploy the CloudEndure agent and token to the storage array. Run the migration plan to start the transfer.","C":"Configure the aws S3 sync command. Configure the AWS client on the client side with credentials. Run the sync command to start the transfer."},"discussion":[{"upvote_count":"1","comment_id":"948475","content":"Correct B","poster":"ggrodskiy","timestamp":"1689033720.0"},{"upvote_count":"1","comments":[{"upvote_count":"2","content":"CloudEndure now envolves into AWS MGN (application migration service) which is now used to do migration for physical server; DataSync is for migration for filesystem.","poster":"Jesuisleon","comment_id":"905344","timestamp":"1684885980.0"}],"timestamp":"1672680720.0","comment_id":"763882","poster":"BlueSpark","content":"Why not A with AWS Endure?"},{"upvote_count":"2","content":"Selected Answer: B\n\"B\" is the one.","poster":"Kende","comment_id":"755639","comments":[{"content":"I think Endure is mainly for DR.","upvote_count":"1","poster":"syaldram","timestamp":"1673277180.0","comment_id":"770558"}],"timestamp":"1671972120.0"},{"content":"Selected Answer: B\nmigration NFS storage arrays to S3,EFS,FSx with encrypted. \n= AWS DataSync + DataSync agent.","timestamp":"1671167820.0","upvote_count":"3","poster":"due","comment_id":"746827"}],"isMC":true,"exam_id":32,"timestamp":"2022-12-16 06:17:00","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/91776-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"answers_community":["B (100%)"],"question_id":1006,"question_images":[],"question_text":"A company wants to retire its Oracle Solaris NFS storage arrays. The company requires rapid data migration over its internet network connection to a combination of destinations for Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server. The company also requires a full initial copy, as well as incremental transfers of changes until the retirement of the storage arrays. All data must be encrypted and checked for integrity.\n\nWhat should a solutions architect recommend to meet these requirements?"},{"id":"seYrg9qiYDli2UyUI2Gi","unix_timestamp":1670403060,"timestamp":"2022-12-07 09:51:00","answer_images":[],"answer_description":"","topic":"1","isMC":true,"question_id":1007,"choices":{"F":"Configure Amazon Macie to scan all S3 buckets in the account on a scheduled basis. Integrate Macie with Amazon EventBridge (Amazon CloudWatch Events). Create an AWS Lambda function to validate the data classification inferred by Macie and to add the missing tag.","B":"Configure an interface VPC endpoint to securely route traffic from on premises to the S3 buckets. Configure a gateway VPC endpoint to route traffic between the S3 buckets and EC2 instances over the AWS private network.","A":"Configure a gateway VPC endpoint to securely route traffic from on premises to the S3 buckets. Configure an interface VPC endpoint to route traffic between the S3 buckets and EC2 instances over the AWS private network.","E":"Configure AWS Config to identify S3 buckets that are missing the DataClassification tag. Generate a report of all resources that AWS Config identifies as missing the tag.","D":"Configure AWS Security Hub to identify S3 buckets that are missing the DataClassification tag. Create an Amazon Simple Notification Service (Amazon SNS) topic. Deliver notifications to the topic whenever an untagged S3 bucket is identified.","C":"Configure Amazon GuardDuty to identify S3 buckets that are missing the DataClassification tag. Create an Amazon Simple Notification Service (Amazon SNS) topic. Deliver notifications to the topic whenever an untagged S3 bucket is identified."},"question_text":"A company wants to use Amazon S3 for object storage. Users must be able to access the objects from devices that are connected to their on-premises private network or Amazon EC2 instances. The company has configured AWS Direct Connect and AWS Site-to-Site VPN as a backup. The company does not want to route S3 traffic over the public Internet. The company also requires all data that is stored in S3 buckets to be appropriately classified by data type with a tag named DataClassification.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose three.)","discussion":[{"upvote_count":"1","poster":"GOTJ","timestamp":"1744033980.0","comment_id":"1558586","content":"Selected Answer: BEF\nBEF seem correct, but I couldn't identify the requirement for bucket labelling (option \"E\"). It should be rewritten to something like \"The company also requires all data that is stored in S3 buckets AND THE BUCKETS THEMSELVES to be appropriately classified by data type with a tag named DataClassification.\""},{"upvote_count":"1","comment_id":"1166305","content":"BEF is correct","timestamp":"1709627700.0","poster":"Ebi"},{"poster":"3a632a3","timestamp":"1705186920.0","upvote_count":"1","content":"Selected Answer: BEF\nAWS Config allows reporting of missing required tags. \nhttps://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/implementing-and-enforcing-tagging.html#enforcement\nGuardDuty doesn't alert on missing tags. SecurityHub integrates AWS Config so it may be able to but only because of AWS Config it can't do it stand alone.","comment_id":"1122110"},{"upvote_count":"2","content":"Selected Answer: BEF\nBEF makes sense","poster":"SkyZeroZx","comment_id":"934008","timestamp":"1687750140.0"},{"poster":"Jesuisleon","comment_id":"923414","timestamp":"1686762660.0","upvote_count":"1","content":"I prefer E to D. see the link: https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html"},{"timestamp":"1682871180.0","content":"Selected Answer: BEF\nBEF makes sense","poster":"dev112233xx","upvote_count":"2","comment_id":"885417"},{"poster":"hobokabobo","timestamp":"1679154780.0","comment_id":"842910","content":"Selected Answer: BDF\nA is wrong, we need B. Gateway for vpc and interface for remote.\nWe can use Macie to intercept and find wrongly tagged Data which is F.\nAs we can easily integrate Macy with security hub it makes sense to involve it. (D)\nE) may be possible but it lacks details and is in combination not as nice as the alternative.\nC) does not really make sense to me.","upvote_count":"2"},{"upvote_count":"1","timestamp":"1674994080.0","poster":"zozza2023","comment_id":"791609","content":"should be B\ninterface Endpoint is needed for on-prem to S3 check this https://aws.amazon.com/blogs/architecture/choosing-your-vpc-endpoint-strategy-for-amazon-s3/"},{"poster":"ccort","timestamp":"1674046680.0","upvote_count":"4","content":"Selected Answer: BEF\nInterface endpoint is needed for on-prem -> S3","comment_id":"780011"},{"content":"Selected Answer: AEF\nB - incorrect. Gateway endpoint only for local VPC. Remote access = interface endpoint.\nAEF","timestamp":"1673894160.0","poster":"lunt","upvote_count":"2","comment_id":"778105"},{"timestamp":"1672232700.0","upvote_count":"3","content":"Selected Answer: BDF\nB. on perm to S3 via interface endpoint and EC2 to S3 via gateway endpoint\nE. Macie crawal through all objects finds the untagged objects in S3.\nD. security hub display the findings from macie","poster":"Vash2303","comment_id":"759854"},{"poster":"due","timestamp":"1671169560.0","comment_id":"746853","upvote_count":"1","content":"Selected Answer: BDE\nS3 , access the objects from devices that are connected to their on-premises private network or Amazon EC2 instances , no Internet + classified by data type with a tag.\n= interface VPC endpoint for on premises + AWS Security Hub + AWS Config."},{"timestamp":"1670403060.0","comments":[{"content":"BEF\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3","upvote_count":"3","poster":"ggrodskiy","comment_id":"749847","timestamp":"1671455700.0"}],"poster":"ggrodskiy","content":"Correct AEF","upvote_count":"2","comment_id":"737612"}],"url":"https://www.examtopics.com/discussions/amazon/view/90358-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"BEF","question_images":[],"answer_ET":"BEF","exam_id":32,"answers_community":["BEF (50%)","BDF (25%)","10%","Other"]},{"id":"YfQO36VoN8S4ZHpeCFdB","answers_community":["A (40%)","B (40%)","D (20%)"],"timestamp":"2023-01-13 14:44:00","answer_images":[],"discussion":[{"poster":"YellowSky002","content":"B\nI go with B \nIf you want to authenticate for your applications, you need User pool.\nIf you want to authenticate for your AWS resources, you need Identity pool.\nIdentity pools provide AWS credentials to grant your users access to other AWS services\nYour application does not have to be compatible with OIDC or SAML because the user pool is. \nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html","timestamp":"1673617440.0","comment_id":"774537","upvote_count":"7"},{"timestamp":"1705187580.0","poster":"3a632a3","content":"Selected Answer: A\nUse developer authenticated identities with identity pools. You can't use user pools for custom auth flows. Then create two flows for free and premium users.\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/authentication-flow.html","comments":[{"poster":"3a632a3","comment_id":"1122697","upvote_count":"1","timestamp":"1705251600.0","content":"I meant custom auth backends not custom auth flows."}],"comment_id":"1122118","upvote_count":"2"},{"content":"Selected Answer: D\nEasy architecture with less complexity. Why do we need two user pools if we can get the work done with one user pool.","poster":"Pr44","comment_id":"1020972","timestamp":"1696007940.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1682875860.0","comment_id":"885477","poster":"dev112233xx","content":"Selected Answer: B\nI agree it's B ...\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html"},{"content":"Selected Answer: B\npushing AssilAbdulrahim correct answer as voting comment. \nIdentity pools for AWS Resources(\"admins\") and user pools for application(\"enduser\").","upvote_count":"3","timestamp":"1679830440.0","poster":"hobokabobo","comment_id":"850946"},{"upvote_count":"2","comment_id":"829868","timestamp":"1678015860.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\nEvery identity in your identity pool is either authenticated or unauthenticated. Authenticated identities belong to users who are authenticated by a public login provider (Amazon Cognito user pools, Login with Amazon, Sign in with Apple, Facebook, Google, SAML, or any OpenID Connect Providers) or a developer provider (your own backend authentication process). Unauthenticated identities typically belong to guest users.\nWhen Amazon Cognito receives a request, the service determines the identity type, determines the role assigned to that identity type, and uses the policy attached to that role to respond. By modifying a policy or assigning a different role to an identity type, you can control which AWS services an identity type can access.","poster":"andras"},{"timestamp":"1678015020.0","poster":"andras","content":"You can enable your users access to AWS services through an identity pool. An identity pool requires an IdP token from a user that's authenticated by a third-party identity provider (or nothing if it's an anonymous guest)\nAuthenticate with a third party and access AWS services with an identity pool\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-scenarios.html#scenario-basic-user-pool","upvote_count":"1","comment_id":"829863"}],"unix_timestamp":1673617440,"answer_description":"","question_id":1008,"isMC":true,"topic":"1","question_text":"A company is developing a new game app for mobile devices. The app has two user tiers: one tier for free-play users and another tier for premium users.\n\nThe company currently uses custom identity authentication across its apps. The company wants to continue to use custom authentication if possible. However, the company's custom identity provider (IdP) is not compatible with either the SAML or Open ID Connect (OIDC) standards. A solutions architect needs to design an authentication approach that makes it easy to transition free-play users to premium users.\n\nWhich design will meet these requirements with the LEAST development effort?","answer_ET":"B","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/95043-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"question_images":[],"choices":{"C":"Create two Amazon Cognito identity pools. Develop an authentication provider for the premium user identity pool that calls the existing custom IdP. For the second identity pool, configure an authentication flow for the free-play users.","D":"Create a single Amazon Cognito user pool. Add a user pool sign-in through the custom IdP. Set up Amazon Cognito guest access in the same user pool for the free-play users.","B":"Create an Amazon Cognito user pool for the premium users. Add a user pool sign-in through the custom IdP. Create a second Amazon Cognito user pool to provide guest access for the free-play users.","A":"Create a single Amazon Cognito identity pool. Develop an authentication provider for the pool that calls the existing custom IdP. Implement two separate authentication flows in the mobile app: one for the free-play users and one for the premium users."}},{"id":"tnHuFbCQ0mPXE0sXuBr7","topic":"1","choices":{"B":"Create one AWS OpsWorks stack create two AWS Ops Works layers, create one custom recipe","A":"Create one AWS OpsWorks stack, create one AWS Ops Works layer, create one custom recipe","D":"Create two AWS OpsWorks stacks create two AWS Ops Works layers, create two custom recipe","C":"Create two AWS OpsWorks stacks create two AWS Ops Works layers, create one custom recipe"},"isMC":true,"answers_community":[],"answer_ET":"B","answer_images":[],"timestamp":"2019-11-03 02:14:00","exam_id":32,"question_text":"A web-startup runs its very successful social news application on Amazon EC2 with an Elastic Load Balancer, an Auto-Scaling group of Java/Tomcat application- servers, and DynamoDB as data store. The main web-application best runs on m2 x large instances since it is highly memory- bound Each new deployment requires semi-automated creation and testing of a new AMI for the application servers which takes quite a while ana is therefore only done once per week.\nRecently, a new chat feature has been implemented in nodejs and wails to be integrated in the architecture. First tests show that the new component is CPU bound Because the company has some experience with using Chef, they decided to streamline the deployment process and use AWS Ops Works as an application life cycle tool to simplify management of the application and reduce the deployment cycles.\nWhat configuration in AWS Ops Works is necessary to integrate the new chat module in the most cost-efficient and flexible way?","discussion":[{"poster":"CloudFloater","timestamp":"1633406760.0","comment_id":"40733","upvote_count":"7","content":"B\nhttp://jayendrapatil.com/category/aws/opsworks/\n\nCreate one AWS Ops Works stack, create two AWS Ops Works layers create one custom recipe (Single environment stack, two layers for java and node.js application using built-in recipes and custom recipe for DynamoDB connectivity only as other configuration. Refer link)"},{"timestamp":"1723806720.0","comment_id":"1267002","content":"B. Create one AWS OpsWorks stack create two AWS Ops Works layers, create one custom recipe","upvote_count":"1","poster":"amministrazione"},{"content":"B.\nStart with one stack.\nThen two layers.\nNo idea about the recipe. I think it is irrelevant here.","poster":"hilft","timestamp":"1658616360.0","comment_id":"635776","upvote_count":"2"},{"poster":"DashL","upvote_count":"1","content":"Ans A:\nHere is explanation. I am not able to post the whole explanation in one go. Here is Part-1.\nI have seen similar question in other websites and there is question is something like \"To meet the growing number of support tickets being sent, it was decided that a new chat feature should be implemented as part of the customer support app, but should be hosted on a different set of servers\". Note the key word \"different set of servers\"\n\nThe stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving web applications. Each layer in a stack must have at least one instance and can optionally have multiple instances.","timestamp":"1635297060.0","comment_id":"406276"},{"timestamp":"1634640300.0","content":"Ans A:\nHere is explanation. I am not able to post the whole explanation in one go. Here is Part-1.\nI have seen similar question in other websites and there is question is something like \"To meet the growing number of support tickets being sent, it was decided that a new chat feature should be implemented as part of the customer support app, but should be hosted on a different set of servers\". Note the key word \"different set of servers\"\n\nThe stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving web applications. Each layer in a stack must have at least one instance and can optionally have multiple instances.","upvote_count":"1","poster":"DashL","comments":[{"poster":"DashL","content":"Ans A:\nHere is explanation. I am not able to post the whole explanation in one go. Here is Part-2.\nEvery stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each instance in a stack must be a member of at least one layer, except for registered instances. You cannot configure an instance directly, except for some basic settings such as the SSH key and hostname. You must create and configure an appropriate layer, and add the instance to the layer.\nRecipes are Ruby applications that define a system's configuration. They install packages, create configuration files from templates, execute shell commands, create files and directories, and so on. You typically have AWS OpsWorks Stacks execute recipes automatically when a lifecycle event occurs on the instance but you can also run them explicitly at any time by using the Execute Recipes stack command.","comments":[{"timestamp":"1635686760.0","content":"Ans A:\nHere is explanation. I am not able to post the whole explanation in one go. Here is Part-3.\nIn the scenario, an AMI should be deployed - so the AMI is part of the stack. it tells us that the chat feature should be implemented as part of the customer support application, but should be hosted on the same set of servers. if it is the same set of servers, it would be just one layer. But if there were two set of servers, it would have been two layers. This means that the chat feature is part of the stack, but should be in a different layer since it will be using a different set of servers. Hence, we have to use one stack and one layer and one recipe to meet the requirement.","poster":"DashL","upvote_count":"2","comment_id":"406281"}],"comment_id":"406279","upvote_count":"1","timestamp":"1635665280.0"}],"comment_id":"406274"},{"timestamp":"1633606380.0","upvote_count":"2","poster":"fullaws","content":"B is correct","comment_id":"144994"},{"upvote_count":"4","poster":"amog","comment_id":"38125","timestamp":"1633293540.0","content":"Answer is B\nSingle environment stack, two layers for java and node.js application using built-in recipes and custom recipe for DynamoDB connectivity only as other configuration"},{"comment_id":"18913","timestamp":"1632333480.0","upvote_count":"4","content":"\" a new chat feature\" is added.. thus there is no need to create an another stack under OpsWork. Instead use the same stack under OpsWorks and create an additional layer for the new feature.\n\nThus, my answer is B","poster":"skywalker"}],"question_id":1009,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/7626-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"B","unix_timestamp":1572743640,"answer_description":""},{"id":"DZnmazbjPi43Nu1AYTya","unix_timestamp":1670178780,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/90004-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true,"discussion":[{"content":"D is correct.\nB and D both make sense, but the key is \"development team has limited resources\", B needs updating application to point to new bucket.","poster":"Ebi","upvote_count":"1","timestamp":"1709628540.0","comment_id":"1166312"},{"poster":"3a632a3","timestamp":"1705252140.0","upvote_count":"1","comment_id":"1122702","content":"Selected Answer: D\n\" Many applications access objects in the S3 bucket, and the development team has limited resources.\" - every app would need to be updated for B. Even if the bucket name is a config value there are still IAM permissions and every app would need to be tested to ensure it is working properly."},{"upvote_count":"2","comment_id":"1020979","content":"Selected Answer: D\nD is better to reduce complexity. Why we go with new bucket if we can solve the issue using existing bucket itselk.","poster":"Pr44","timestamp":"1696009260.0"},{"upvote_count":"3","timestamp":"1689034260.0","poster":"ggrodskiy","content":"Correct D.","comment_id":"948481"},{"content":"Selected Answer: D\nThink while B works, the answer should be D as using Batch operations is the \"new\" AWS recommended approach for this usecase.","poster":"SkyZeroZx","upvote_count":"2","comment_id":"947693","timestamp":"1688958720.0"},{"comments":[{"content":"Besides, you also have to setup S3 accordingly","poster":"vn_thanhtung","comment_id":"985488","timestamp":"1692491940.0","upvote_count":"1"},{"timestamp":"1692490920.0","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html#:~:text=You%20can%20use,of%20your%20bucket.\nWith B require development effort change code ? One Again are you dev ?","poster":"vn_thanhtung","upvote_count":"1","comment_id":"985485"}],"content":"Selected Answer: B\nB is correct","comment_id":"885480","timestamp":"1682876160.0","upvote_count":"1","poster":"dev112233xx"},{"timestamp":"1679156040.0","content":"Selected Answer: D\nThink while B works, the answer should be D as using Batch operations is the \"new\" AWS recommended approach for this usecase.","poster":"hobokabobo","upvote_count":"3","comment_id":"842928"},{"timestamp":"1678613100.0","comment_id":"836845","content":"Selected Answer: D\nD is correct.\n\"LEAST development effort\"","poster":"Watascript","upvote_count":"1"},{"comment_id":"829883","upvote_count":"1","timestamp":"1678016940.0","poster":"andras","content":"Selected Answer: D\nno need for a new bucket I think\nhttps://spin.atomicobject.com/2020/09/15/aws-s3-encrypt-existing-objects/\nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/"},{"content":"Selected Answer: B\n\"B\" is the one.","comment_id":"755640","poster":"Kende","upvote_count":"2","timestamp":"1671972180.0"},{"timestamp":"1671171360.0","upvote_count":"4","comment_id":"746863","poster":"due","content":"Selected Answer: B\nlist of objects that contain sensitive data = Macie. , ensure that all objects are encrypted = Move to new S3 default encryption enabled.","comments":[{"content":"false to ensure all objects are encrypted you can turn on encryption in the bucket and use s3 batch to copy all unencrypted objetcs in the same bucket.\n\nhttps://aws.amazon.com/fr/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/\n\"The easiest way to encrypt this set of objects is by using the put copy operation and specifying the same destination prefix as the objects listed in the manifest. \"","poster":"MikelH93","comment_id":"901235","comments":[{"comment_id":"901514","poster":"Jesuisleon","timestamp":"1684446840.0","content":"B and D are both feasible but I think D is better based on \" the development team has limited resources\".","upvote_count":"2"}],"timestamp":"1684416000.0","upvote_count":"1"}]},{"comment_id":"735353","content":"Correct B.\nAmazon Macie generates findings when it detects potential policy violations or issues with the security or privacy of your Amazon Simple Storage Service (Amazon S3) buckets, or it discovers sensitive data in S3 objects. A finding is a detailed report of a potential issue or sensitive data that Macie found. Each finding provides a severity rating, information about the affected resource, and additional details, such as when and how Macie found the issue or data. Macie stores your policy and sensitive data findings for 90 days.","poster":"ggrodskiy","upvote_count":"2","timestamp":"1670178780.0"}],"exam_id":32,"answer_ET":"D","question_images":[],"question_text":"A company has an Amazon S3 bucket that contains millions of unencrypted objects. To comply with a recent security audit, a solutions architect needs to ensure that all objects are encrypted and needs to compile a list of objects that contain sensitive data. Many applications access objects in the S3 bucket, and the development team has limited resources.\n\nWhich solution will meet these requirements with the LEAST development effort?","answer_images":[],"choices":{"B":"Run an Amazon Macie report on the S3 bucket to identify sensitive data. Create a new S3 bucket with default encryption enabled. Transfer the unencrypted objects to the new S3 bucket. Update the applications to access the new S3 bucket.","D":"Run an Amazon Macie report on the S3 bucket to identify sensitive data. Modify the S3 bucket to enable default encryption. Use an S3 Inventory report and S3 Batch encrvnt the existing unencrypted objects in the same S3 bucket.","A":"Run an Amazon Inspector report on the S3 bucket to identify sensitive data. Create a new S3 bucket with default encryption enabled. Transfer the unencrypted objects to the new S3 bucket. Update the applications to access the new S3 bucket.","C":"Run an Amazon Inspector report against the S3 bucket to identify sensitive data. Modify the S3 bucket to enable default encryption. Use an Amazon S3 Inventory report and Amazon S3 Batch Operations to encrypt the existing unencrypted objects in the same S3 bucket."},"question_id":1010,"answer":"D","topic":"1","answers_community":["D (59%)","B (41%)"],"timestamp":"2022-12-04 19:33:00"}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","provider":"Amazon","name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"id":32,"numberOfQuestions":1019,"isImplemented":true},"currentPage":202},"__N_SSP":true}