{"pageProps":{"questions":[{"id":"pZCjbhNLVZwieOp04fol","answer_ET":"A","answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/132742-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-02-03 16:48:00","question_images":[],"answer":"A","exam_id":21,"question_id":141,"question_text":"A company is migrating its database servers from Amazon EC2 instances that run Microsoft SQL Server to Amazon RDS for Microsoft SQL Server DB instances. The company's analytics team must export large data elements every day until the migration is complete. The data elements are the result of SQL joins across multiple tables. The data must be in Apache Parquet format. The analytics team must store the data in Amazon S3.\nWhich solution will meet these requirements in the MOST operationally efficient way?","unix_timestamp":1706975280,"topic":"1","isMC":true,"discussion":[{"content":"Selected Answer: C\nChoice A) is almost the same approach, but it doesn't use the AWS Glue crawler, so have to manage the view's metadata manually.","comments":[{"comment_id":"1307317","comments":[{"timestamp":"1730803500.0","upvote_count":"2","poster":"michele_scar","content":"My fault: is correct C because during a migration process is more efficiently have a crawler that should catch eventually changes of a schema.","comment_id":"1307320"}],"content":"My fault: is correct A because during a migration process is more efficiently have a crawler that should catch eventually changes of a schema.","timestamp":"1730803440.0","upvote_count":"2","poster":"michele_scar"}],"comment_id":"1176923","poster":"taka5094","upvote_count":"7","timestamp":"1710812100.0"},{"timestamp":"1712979180.0","poster":"Christina666","content":"Selected Answer: C\nLeveraging SQL Views: Creating a view on the source database simplifies the data extraction process and keeps your SQL logic centralized.\nGlue Crawler Efficiency: Using a Glue crawler to automatically discover and catalog the view's metadata reduces manual setup.\nGlue Job for ETL: A dedicated Glue job is well-suited for the data transformation (to Parquet) and loading into S3. Glue jobs offer built-in scheduling capabilities.\nOperational Efficiency: This approach minimizes custom code and leverages native AWS services for data movement and cataloging.","comment_id":"1194630","upvote_count":"7","comments":[{"poster":"Dummy92yash","content":"Glue crawler is used to catalog and find the schema. In this requirement the data was already stored in MS SQL server which a relational database. Hence I think A is correct","timestamp":"1724600580.0","comment_id":"1272197","upvote_count":"3"}]},{"comment_id":"1409578","upvote_count":"1","content":"Selected Answer: A\nI'll go with A","timestamp":"1742802720.0","poster":"Eltanany"},{"comment_id":"1356063","timestamp":"1739441640.0","upvote_count":"1","content":"Selected Answer: A\nA is correct - no need for crawler","poster":"Certified101"},{"content":"Selected Answer: A\nthe scrawler is not necessary, use GLUE job to read data from sql server and transfert to S3 with Apache Parquet format is enough.","upvote_count":"3","timestamp":"1736690160.0","poster":"plutonash","comment_id":"1339534"},{"upvote_count":"3","comment_id":"1321942","poster":"mtrianac","timestamp":"1733322120.0","content":"Selected Answer: A\nNo, in this case, using an AWS Glue Crawler is not necessary. The schema is already defined in the SQL Server database, as the created view contains the required structure (columns and data types). AWS Glue can directly connect to the database via JDBC, extract the data, transform it into Parquet format, and store it in S3 without additional steps.\n\nA crawler is useful if you're working with data that doesn't have a predefined schema (e.g., files in S3) or if you need the data to be cataloged for services like Amazon Athena. However, for this ETL flow, using just a Glue Job simplifies the process and reduces operational complexity."},{"timestamp":"1730802720.0","content":"Selected Answer: A\nGlue crawler is useless because the schema is already in place with a SQL database","comments":[],"poster":"michele_scar","upvote_count":"1","comment_id":"1307313"},{"content":"Selected Answer: A\nUsually, views aren't true objects in a SGBD, they're just a \"nickname\" for a specific query string, different of Materialized Views. So, my questions is: can glue crawler understand their metadata?\nI'd go with A","upvote_count":"3","poster":"leonardoFelipe","timestamp":"1730666580.0","comment_id":"1306640"},{"timestamp":"1719839280.0","upvote_count":"3","comment_id":"1240163","poster":"bakarys","content":"Selected Answer: A\nOption A involves creating a view in the EC2 instance-based SQL Server databases that contains the required data elements. An AWS Glue job is then created to select the data directly from the view and transfer the data in Parquet format to an S3 bucket. This job is scheduled to run every day. This approach is operationally efficient as it leverages managed services (AWS Glue) and does not require additional transformation steps.\n\nOption D involves creating an AWS Lambda function that queries the EC2 instance-based databases using JDBC. The Lambda function is configured to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. This approach could work, but managing and scheduling Lambda functions could add operational overhead compared to using managed services like AWS Glue."},{"timestamp":"1710834960.0","content":"Selected Answer: C\nJust beacuse it decouples the whole architecture I will go with C","poster":"GiorgioGss","upvote_count":"2","comment_id":"1177068"},{"timestamp":"1709353080.0","upvote_count":"1","poster":"Felix_G","comment_id":"1163915","content":"Option C seems to be the most operationally efficient:\nIt leverages Glue for both schema discovery (via the crawler) and data transfer (via the Glue job).\nThe Glue job can directly handle the Parquet format conversion.\nScheduling the Glue job ensures regular data export without manual intervention.","comments":[{"content":"you're right: https://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/","timestamp":"1710558600.0","comments":[{"content":"Is this right?\nhttps://aws.amazon.com/jp/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/","timestamp":"1710811680.0","upvote_count":"1","poster":"taka5094","comment_id":"1176921"}],"poster":"helpaws","upvote_count":"1","comment_id":"1174703"}]},{"poster":"rralucard_","content":"Selected Answer: A\nOption A (Creating a view in the EC2 instance-based SQL Server databases and creating an AWS Glue job that selects data from the view, transfers it in Parquet format to S3, and schedules the job to run every day) seems to be the most operationally efficient solution. It leverages AWS Glueâ€™s ETL capabilities for direct data extraction and transformation, minimizes manual steps, and effectively automates the process.","timestamp":"1707052680.0","comment_id":"1140135","upvote_count":"3"},{"comment_id":"1139395","poster":"evntdrvn76","timestamp":"1706975280.0","upvote_count":"2","content":"A. Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day. This solution is operationally efficient for exporting data in the required format."}],"answers_community":["A (53%)","C (47%)"],"choices":{"C":"Use a SQL query to create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create and run an AWS Glue crawler to read the view. Create an AWS Glue job that retrieves the data and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.","A":"Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.","D":"Create an AWS Lambda function that queries the EC2 instance-based databases by using Java Database Connectivity (JDBC). Configure the Lambda function to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. Use Amazon EventBridge to schedule the Lambda function to run every day.","B":"Schedule SQL Server Agent to run a daily SQL query that selects the desired data elements from the EC2 instance-based SQL Server databases. Configure the query to direct the output .csv objects to an S3 bucket. Create an S3 event that invokes an AWS Lambda function to transform the output format from .csv to Parquet."}},{"id":"2URAied5iX7GSbkTIUW8","answer_ET":"BE","answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/131467-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-01-18 09:07:00","question_images":[],"answer":"BE","exam_id":21,"question_id":142,"unix_timestamp":1705565220,"question_text":"A financial company wants to implement a data mesh. The data mesh must support centralized data governance, data analysis, and data access control. The company has decided to use AWS Glue for data catalogs and extract, transform, and load (ETL) operations.\nWhich combination of AWS services will implement a data mesh? (Choose two.)","topic":"1","isMC":true,"discussion":[{"poster":"hsnin","timestamp":"1710747000.0","content":"Selected Answer: BE\nThe answer is B and E.\nThe data mesh implementation uses Amazon S3 and Athena for data storage and analysis, and AWS Lake Formation for centralized data governance and access control. When combined with AWS Glue, you can efficiently manage your data.","upvote_count":"7","comment_id":"1176328"},{"poster":"ninomfr64","timestamp":"1744081740.0","comment_id":"1558779","content":"Selected Answer: BE\nS3 (storage) LakeFormation (governance) Athena (analytics) can be used to implement data mesh. In real life you would use DataZone or nowadays SageMaker Unified Studio","upvote_count":"1"},{"upvote_count":"1","comment_id":"1288854","poster":"Shubham1989","content":"Selected Answer: BE\nS3 is best storage for data lake, and AWS lake formation is best for management.","timestamp":"1727233680.0"},{"comment_id":"1223304","poster":"nanaw770","timestamp":"1727080020.0","upvote_count":"2","content":"Selected Answer: BE\nBE are correct answer."},{"comment_id":"1170198","content":"Selected Answer: BE\nSometimes I think examtopics uses us to calibrate the right answers hehehe, by the goal statement and the services outlines and objectives there are no way the answer be different then B,E","timestamp":"1727080020.0","poster":"Josa2","upvote_count":"3"},{"timestamp":"1727080020.0","content":"Selected Answer: BE\nA: Cost-effective storage: Amazon S3 is a highly scalable and cost-effective object storage service perfect for storing large datasets commonly found in financial institutions.\nCentralized data lake: S3 acts as the central data lake where all data from different domains can reside in its raw or processed form.\nEasy data access: Athena provides a serverless interactive query service that allows data analysts to directly query data stored in S3 using standard SQL. This simplifies data exploration and analysis without managing servers.\nB: Data governance: Lake Formation helps establish data ownership, access control, and lineage for data products within the data mesh. It ensures data quality, security, and compliance with regulations.\nFine-grained access control: Lake Formation allows you to define granular access policies for each data domain, ensuring only authorized users can access specific data sets. This aligns with the need for centralized control in a data mesh.","comment_id":"1226760","poster":"pypelyncar","upvote_count":"1"},{"comment_id":"1254180","upvote_count":"1","timestamp":"1721803440.0","poster":"Fredrik1","content":"Must be B and E"},{"upvote_count":"1","content":"Selected Answer: BE\ni thing so","comment_id":"1182092","poster":"minhtien1707","timestamp":"1711332660.0"},{"poster":"alexua","timestamp":"1709248560.0","upvote_count":"2","comment_id":"1163084","content":"B and E . \nC - is not correct \"AWS Glue DataBrew is a visual data preparation tool that makes it easier for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning (ML)\""},{"comment_id":"1158033","timestamp":"1708792020.0","upvote_count":"1","content":"B and E","poster":"Alcee"},{"content":"BE\nGiven the requirements for implementing a data mesh architecture with centralized data governance, data analysis, and data access control, the two better choices from the options provided would be:\n\nB. Use Amazon S3 for data storage. Use Amazon Athena for data analysis.\n\nE. Use AWS Lake Formation for centralized data governance and access control.","timestamp":"1706816640.0","poster":"TonyStark0122","upvote_count":"2","comment_id":"1137870"},{"poster":"milofficial","comment_id":"1125616","content":"Selected Answer: BE\nTextbook question, the keyword data mesh means S3, the keyword data governance means LakeFormation","upvote_count":"4","timestamp":"1705565220.0"}],"answers_community":["BE (100%)"],"choices":{"C":"Use AWS Glue DataBrew for centralized data governance and access control.","D":"Use Amazon RDS for data storage. Use Amazon EMR for data analysis.","E":"Use AWS Lake Formation for centralized data governance and access control.","A":"Use Amazon Aurora for data storage. Use an Amazon Redshift provisioned cluster for data analysis.","B":"Use Amazon S3 for data storage. Use Amazon Athena for data analysis."}},{"id":"PO08YlrEEUelQ5bVBUlg","answer_description":"","unix_timestamp":1706845080,"topic":"1","isMC":true,"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/132660-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A data engineering team is using an Amazon Redshift data warehouse for operational reporting. The team wants to prevent performance issues that might result from long- running queries. A data engineer must choose a system table in Amazon Redshift to record anomalies when a query optimizer identifies conditions that might indicate performance issues.\nWhich table views should the data engineer use to meet this requirement?","discussion":[{"comment_id":"1163916","poster":"Felix_G","upvote_count":"5","timestamp":"1725243720.0","content":"B\nSTL_ALERT_EVENT_LOG records any alerts/notifications related to queries or user-defined performance thresholds. This would capture optimizer alerts about potential performance issues.\n\nSTL_PLAN_INFO provides detailed info on execution plans. The optimizer statistics and warnings provide insight into problematic query plans.\n\nSTL_USAGE_CONTROL limits user activity but does not log anomalies.\n\nSTL_QUERY_METRICS has execution stats but no plan diagnostics.\n\nBy enabling alerts and checking STL_ALERT_EVENT_LOG and STL_PLAN_INFO, the data engineer can best detect and troubleshoot queries flagged by the optimizer as problematic before they impair performance. This meets the requirement to catch potential long running queries."},{"upvote_count":"5","comment_id":"1177070","poster":"GiorgioGss","timestamp":"1726725480.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html"},{"comment_id":"1329054","upvote_count":"1","content":"Selected Answer: B\nControl table is related to usage control metrics and doesn't focus on performance issues or anomalies related to query optimization. It's more about usage limits and controls.","timestamp":"1734624960.0","poster":"HagarTheHorrible"},{"timestamp":"1733794140.0","comment_id":"1227567","content":"Selected Answer: B\nthis table records alerts that are generated by the Amazon Redshift system when it detects certain conditions that might indicate performance issues. These alerts are triggered by the query optimizer when it detects suboptimal query plans or other issues that could affect performance.","upvote_count":"1","poster":"pypelyncar"},{"comment_id":"1138084","poster":"rralucard_","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm_chap_system-tables.html\nSTL_ALERT_EVENT_LOG table view to meet this requirement. This system table in Amazon Redshift is designed to record anomalies when a query optimizer identifies conditions that might indicate performance issues","upvote_count":"1","timestamp":"1722562680.0"}],"answer":"B","exam_id":21,"choices":{"A":"STL_USAGE_CONTROL","B":"STL_ALERT_EVENT_LOG","D":"STL_PLAN_INFO","C":"STL_QUERY_METRICS"},"question_id":143,"answer_ET":"B","timestamp":"2024-02-02 04:38:00","question_images":[],"answer_images":[]},{"id":"paRzd9EgGbiHsKTNAVkC","answer_images":[],"question_images":[],"answer":"D","discussion":[{"upvote_count":"1","poster":"imymoco","timestamp":"1731462360.0","comment_id":"1311027","content":"Why not B?\nI think Athena also be able to handle json."},{"timestamp":"1717976040.0","content":"Selected Answer: D\nAthena is optimized for querying data stored in Parquet format. It can efficiently scan only the necessary columns for a specific query,\nreducing the amount of data processed. This translates to faster query execution times and lower query costs for data analysts who primarily focus on one or two columns","poster":"pypelyncar","comment_id":"1227568","upvote_count":"2"},{"content":"Selected Answer: D\nCost effectively, and they are going to use only one or two columns, columnar.","comment_id":"1218989","upvote_count":"2","poster":"FunkyFresco","timestamp":"1716728520.0"},{"comment_id":"1177071","poster":"GiorgioGss","content":"Selected Answer: D\nMOST cost-effectively = parquet","timestamp":"1710835200.0","upvote_count":"3"},{"timestamp":"1706474340.0","upvote_count":"2","comment_id":"1134424","poster":"atu1789","content":"Selected Answer: D\nGlue + Parquet for cost efectiveness"}],"answer_description":"","exam_id":21,"question_text":"A data engineer must ingest a source of structured data that is in .csv format into an Amazon S3 data lake. The .csv files contain 15 columns. Data analysts need to run Amazon Athena queries on one or two columns of the dataset. The data analysts rarely query the entire file.\nWhich solution will meet these requirements MOST cost-effectively?","unix_timestamp":1706474340,"answers_community":["D (100%)"],"isMC":true,"timestamp":"2024-01-28 21:39:00","topic":"1","question_id":144,"answer_ET":"D","choices":{"A":"Use an AWS Glue PySpark job to ingest the source data into the data lake in .csv format.","B":"Create an AWS Glue extract, transform, and load (ETL) job to read from the .csv structured data source. Configure the job to ingest the data into the data lake in JSON format.","C":"Use an AWS Glue PySpark job to ingest the source data into the data lake in Apache Avro format.","D":"Create an AWS Glue extract, transform, and load (ETL) job to read from the .csv structured data source. Configure the job to write the data into the data lake in Apache Parquet format."},"url":"https://www.examtopics.com/discussions/amazon/view/132349-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"GqVcYZ9SIxMhYSMcyP1v","answer_images":[],"answers_community":["BD (100%)"],"topic":"1","isMC":true,"choices":{"E":"Create a separate S3 bucket for each Region. Configure an IAM policy to allow S3 access. Restrict access based on Region.","D":"Enable fine-grained access control in AWS Lake Formation. Add a data filter for each Region.","B":"Register the S3 path as an AWS Lake Formation location.","A":"Use data filters for each Region to register the S3 paths as data locations.","C":"Modify the IAM roles of the HR departments to add a data filter for each department's Region."},"discussion":[{"poster":"rralucard_","timestamp":"1706846760.0","comment_id":"1138090","content":"Selected Answer: BD\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/access-control-fine-grained.html","upvote_count":"5"},{"upvote_count":"1","comment_id":"1311017","content":"B: Since its a initial step for leverage fine grain control of Lakeformation.\nD: Give granular level control and meets the requirement","timestamp":"1731460140.0","poster":"ctndba"},{"upvote_count":"3","timestamp":"1717976820.0","poster":"pypelyncar","content":"Selected Answer: BD\nRegistering the S3 path as an AWS Lake Formation location is the first step in leveraging Lake Formation's data governance and access control capabilities. This allows the data engineering team to centrally manage and govern the data stored in the S3 data lake.\nEnabling fine-grained access control in AWS Lake Formation and adding a data filter for each Region is the key step to achieve the desired access control. Data filters in Lake Formation allow you to define row-level and column-level access policies based on specific conditions or attributes, such as the Region in this case","comment_id":"1227572"},{"timestamp":"1706770500.0","upvote_count":"1","poster":"rralucard_","comment_id":"1137344","content":"If your table stored in an AWS Glue Data Catalog has tens and hundreds of thousands and millions of partitions, you can enable partition indexes on the table. With partition indexes, only the metadata for the partition value in the queryâ€™s filter is retrieved from the catalog instead of retrieving all the partitionsâ€™ metadata. The result is faster queries for such highly partitioned tables. The following table compares query runtimes between a partitioned table with no partition indexing and with partition indexing. The table contains approximately 100,000 partitions and uncompressed text data. The orders table is partitioned by the o_custkey column."},{"timestamp":"1706474280.0","comment_id":"1134423","upvote_count":"1","poster":"atu1789","content":"Selected Answer: BD\nBD makes sense"}],"exam_id":21,"question_images":[],"answer":"BD","answer_description":"","question_text":"A company has five offices in different AWS Regions. Each office has its own human resources (HR) department that uses a unique IAM role. The company stores employee records in a data lake that is based on Amazon S3 storage.\nA data engineering team needs to limit access to the records. Each HR department should be able to access records for only employees who are within the HR department's Region.\nWhich combination of steps should the data engineering team take to meet this requirement with the LEAST operational overhead? (Choose two.)","question_id":145,"url":"https://www.examtopics.com/discussions/amazon/view/132348-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"BD","unix_timestamp":1706474280,"timestamp":"2024-01-28 21:38:00"}],"exam":{"numberOfQuestions":207,"provider":"Amazon","isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true,"isImplemented":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","id":21},"currentPage":29},"__N_SSP":true}