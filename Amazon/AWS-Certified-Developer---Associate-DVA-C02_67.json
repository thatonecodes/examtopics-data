{"pageProps":{"questions":[{"id":"CY5BBVGdrwBpPjkQ4C6c","isMC":true,"choices":{"A":"Specify an Amazon S3 cache in CodeBuild. Add the S3 cache folder path to the buildspec.yaml file for the build project.","D":"Retrieve the buildspec.yaml file directly from CodeArtifact. Add the CodeArtifact repository name to the buildspec.yaml file for the build project.","B":"Specify a local cache in CodeBuild. Add the CodeArtifact repository name to the buildspec.yaml file for the build project.","C":"Specify a local cache in CodeBuild. Add the cache folder path to the buildspec.yaml file for the build project."},"answer":"C","answers_community":["C (90%)","10%"],"topic":"1","answer_description":"","question_id":331,"question_text":"A developer has a continuous integration and continuous delivery (CI/CD) pipeline that uses AWS CodeArtifact and AWS CodeBuild. The build artifacts are between 0.5 GB and 1.5 GB in size. The builds happen frequently and retrieve many dependencies from CodeArtifact each time.\n\nThe builds have been slow because of the time it takes to transfer dependencies. The developer needs to improve build performance by reducing the number of dependencies that are retrieved for each build.\n\nWhich solution will meet this requirement?","answer_images":[],"timestamp":"2024-06-29 19:41:00","url":"https://www.examtopics.com/discussions/amazon/view/143073-exam-aws-certified-developer-associate-dva-c02-topic-1/","discussion":[{"poster":"Alabi","timestamp":"1719682860.0","upvote_count":"7","comment_id":"1239368","content":"Selected Answer: C\nExplanation\n\nUsing a local cache in CodeBuild allows you to cache dependencies locally on the build host, which can significantly reduce the time it takes to retrieve dependencies during subsequent builds."},{"timestamp":"1723095000.0","upvote_count":"1","comment_id":"1262320","content":"Selected Answer: C\nversion: 0.2\n\nphases:\ninstall:\nAnyone want cheaper contributor PDF then check certificationtest[.]net\nUsing a local cache in CodeBuild allows you to cache dependencies locally on the build host, which can significantly reduce the time it takes to retrieve dependencies during subsequent builds.","poster":"Duke315"},{"content":"Selected Answer: C\nupdate buildspec.yaml\n\nversion: 0.2\n\nphases:\n install:\n commands:\n - echo Installing dependencies...\n - pip install -r requirements.txt\n build:\n commands:\n - echo Build started on `date`\n - echo Compiling the Python code...\n - python setup.py build\n\ncache:\n paths:\n - '/root/.cache/pip/**/*' # dependencies cache","upvote_count":"1","poster":"albert_kuo","comment_id":"1256595","timestamp":"1722144660.0"},{"upvote_count":"1","content":"Selected Answer: B\nC does not specify how to integrate it with CodeArtifact","poster":"cachac","comment_id":"1239407","timestamp":"1719692820.0"}],"exam_id":24,"question_images":[],"answer_ET":"C","unix_timestamp":1719682860},{"id":"xLGzZqPHIVi9YIc7BQfF","question_id":332,"isMC":true,"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/143357-exam-aws-certified-developer-associate-dva-c02-topic-1/","timestamp":"2024-07-05 14:10:00","question_images":[],"answer_description":"","exam_id":24,"unix_timestamp":1720181400,"question_text":"A company that has large online business uses an Amazon DynamoDB table to store sales data. The company enabled Amazon DynamoDB Streams on the table. The transaction status of each sale is stored in a TransactionStatus attribute in the table. The value of the TransactionStatus attribute must be either failed, pending, or completed.\n\nThe company wants to be notified of failed sales where the Price attribute is above a specific threshold. A developer needs to set up notification for the failed sales.\n\nWhich solution will meet these requirements with the LEAST development effort?","choices":{"B":"Create an event source mapping between DynamoDB Streams and an AWS Lambda function. Configure the Lambda function handler code to publish to an Amazon Simple Notification Service (Amazon SNS) topic if sales fail when price is above the specified threshold.","A":"Create an event source mapping between DynamoDB Streams and an AWS Lambda function. Use Lambda event filtering to trigger the Lambda function only if sales fail when the price is above the specified threshold. Configure the Lambda function to publish the data to an Amazon Simple Notification Service (Amazon SNS) topic.","D":"Create an Amazon CloudWatch alarm to monitor the DynamoDB Streams sales data. Configure the alarm to publish to an Amazon Simple Notification Service (Amazon SNS) topic if sales fail due when price is above the specified threshold.","C":"Create an event source mapping between DynamoDB Streams and an Amazon Simple Notification Service (Amazon SNS) topic. Use event filtering to publish to the SNS topic if sales fail when the price is above the specified threshold."},"answer":"A","answers_community":["A (77%)","C (23%)"],"answer_ET":"A","discussion":[{"comment_id":"1307034","upvote_count":"1","content":"why not D ??","poster":"Saudis","timestamp":"1730742000.0"},{"timestamp":"1728175140.0","comment_id":"1293662","content":"Selected Answer: A\nLambda event filtering\n\n{\n \"eventName\": [\"MODIFY\"],\n \"dynamodb.NewImage.TransactionStatus.S\": [\"failed\"],\n \"dynamodb.NewImage.Price.N\": [{\"numeric\": [\">\", 100]}]\n}","poster":"albert_kuo","upvote_count":"1"},{"comment_id":"1251712","timestamp":"1721474280.0","content":"Selected Answer: A\nCorrection, answer should be A\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-eventfiltering.html#filtering-ddb","upvote_count":"3","poster":"Anandesh"},{"upvote_count":"4","comment_id":"1249282","timestamp":"1721167620.0","poster":"Alagong","content":"Selected Answer: A\nDynamoDB Streams cannot be directly mapped to SNS. This option is not feasible as described. So C is wrong."},{"content":"Selected Answer: A\ndefinitely A","comment_id":"1246405","poster":"siheom","timestamp":"1720746720.0","upvote_count":"2"},{"upvote_count":"1","content":"A\n\nBy using event filtering, the Lambda function will only be triggered if the conditions (TransactionStatus is 'failed' and Price is above the specified threshold) are met. This reduces unnecessary executions and simplifies the logic within the function.","poster":"tomchandler077","timestamp":"1720366740.0","comment_id":"1243906"},{"comment_id":"1242763","upvote_count":"3","timestamp":"1720181400.0","poster":"Anandesh","content":"Selected Answer: C\ndynamodb can be mapped as an event source to SNS. While creating the table, we can turn the stream on. We can push events to SNS topic and apply filter policy"}]},{"id":"QyquDasVwZL2tsoft8iZ","answer_images":[],"topic":"1","answer_description":"","answers_community":["C (100%)"],"isMC":true,"question_id":333,"discussion":[{"timestamp":"1741852440.0","content":"Selected Answer: C\nC. SQS\nThe dead letter queue specified for the Lambda function is not compatible with the FIFO (First-In-First-Out) SNS topic. FIFO SNS topics are not supported for dead letter configuration in AWS Lambda.","upvote_count":"1","poster":"0bdf3af","comment_id":"1388220"},{"comment_id":"1294417","timestamp":"1728322860.0","upvote_count":"1","content":"Selected Answer: C\nTo collect and analyze the failed events of an AWS Lambda function invoked asynchronously, the developer can leverage Lambda's built-in Dead Letter Queue (DLQ) feature with minimal development effort.","poster":"preachr"},{"upvote_count":"3","content":"C. Add a dead-letter queue to send messages to an Amazon Simple Queue Service (Amazon SQS) standard queue.","comment_id":"1247040","timestamp":"1720825740.0","poster":"komorebi"},{"content":"Selected Answer: C\nA dlq in SQS","timestamp":"1720792140.0","upvote_count":"3","comment_id":"1246796","poster":"rdiaz"}],"url":"https://www.examtopics.com/discussions/amazon/view/143802-exam-aws-certified-developer-associate-dva-c02-topic-1/","answer":"C","timestamp":"2024-07-12 15:49:00","choices":{"A":"Add logging statements for all events in the Lambda function. Filter AWS CloudTrail logs for errors.","C":"Add a dead-letter queue to send messages to an Amazon Simple Queue Service (Amazon SQS) standard queue.","D":"Add a dead-letter queue to send messages to an Amazon Simple Notification Service (Amazon SNS) FIFO topic.","B":"Configure the Lambda function to start an AWS Step Functions workflow with retries for failed events."},"exam_id":24,"answer_ET":"C","question_images":[],"question_text":"An AWS Lambda function is invoked asynchronously to process events. Occasionally, the Lambda function falls to process events. A developer needs to collect and analyze these failed events to fix the issue.\n\nWhat should the developer do to meet these requirements with the LEAST development effort?","unix_timestamp":1720792140},{"id":"Xp6oXsRxmldiXwX5lp2P","timestamp":"2023-03-16 10:17:00","answer_description":"","unix_timestamp":1678958220,"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/102789-exam-aws-certified-developer-associate-dva-c02-topic-1/","choices":{"B":"AWS Step Functions","C":"AWS Glue","D":"AWS Lambda","A":"AWS Batch"},"question_id":334,"exam_id":24,"isMC":true,"answer_images":[],"discussion":[{"timestamp":"1683390060.0","poster":"geekdamsel","content":"Got this question in exam.Correct answer is B.","upvote_count":"14","comment_id":"890862"},{"comment_id":"1329452","timestamp":"1734700680.0","upvote_count":"1","content":"Selected Answer: B\nAWS Step Functions is an orchestration service that is designed to manage and automate workflows","comments":[{"upvote_count":"1","comment_id":"1329453","content":"A) Eliminated - AWS Batch is great for running batch processing jobs, especially for large-scale computational workloads. However, it is not designed for orchestrating complex workflows","comments":[{"poster":"sumanshu","comments":[{"timestamp":"1734700740.0","content":"Sorry I mean C for above explanation\n\nD) Eliminated - it does not provide built-in orchestration or workflow management capabilities like AWS Step Functions","upvote_count":"1","poster":"sumanshu","comment_id":"1329457"}],"upvote_count":"1","comment_id":"1329454","timestamp":"1734700740.0","content":"B) Eliminated - AWS Glue is a managed ETL (Extract, Transform, Load) service that helps you prepare and transform data. While it can handle data processing, it does not provide the orchestration and workflow management features"}],"poster":"sumanshu","timestamp":"1734700680.0"}],"poster":"sumanshu"},{"content":"Selected Answer: B\nI think those keyword when pick B: scalable, sequence, reprocessing of data if errors, least possible maintenance","poster":"trieudo","comment_id":"1324961","timestamp":"1733911860.0","upvote_count":"2"},{"content":"Selected Answer: B\nB is correct answer.","poster":"NagaoShingo","timestamp":"1717257420.0","upvote_count":"1","comment_id":"1222765"},{"upvote_count":"1","timestamp":"1716295680.0","content":"Selected Answer: B\nB is the correct answer.","poster":"65703c1","comment_id":"1214950"},{"content":"Selected Answer: B\nits clearly mention orchestration , sequence and multiple processing and transformations","comment_id":"1192315","poster":"Dikshika","timestamp":"1712670900.0","upvote_count":"2"},{"comment_id":"1190631","content":"Selected Answer: B\nStepFunctions, es el servicio recomedado para orquestar. B correcto","timestamp":"1712440320.0","poster":"vinfo","upvote_count":"1"},{"poster":"ibratoev","upvote_count":"1","comment_id":"1182550","content":"Selected Answer: B\nB is correct","timestamp":"1711376040.0"},{"content":"Selected Answer: B\nAnswer is B. Step Function is about orchestrating workflows","timestamp":"1701411900.0","comment_id":"1084986","upvote_count":"3","poster":"alven_alinan"},{"timestamp":"1699079580.0","content":"Selected Answer: B\nMy answer is B","poster":"dongocanh272","upvote_count":"1","comment_id":"1061932"},{"comment_id":"1027588","poster":"Digo30sp","upvote_count":"1","content":"Selected Answer: B\nB is correct","timestamp":"1696703760.0"},{"poster":"NinjaCloud","comment_id":"1021375","timestamp":"1696066860.0","upvote_count":"1","content":"Best option: B"},{"content":"Selected Answer: B\nb init","timestamp":"1694169480.0","poster":"panoptica","upvote_count":"1","comment_id":"1002365"},{"comment_id":"998633","upvote_count":"3","content":"The answer is B(Step Functions). For people confused with AWS Lambda, it is a compute service and can be used within Step Functions, but it alone does not provide the orchestration and error handling features required in this case.","poster":"sharma_ps93","timestamp":"1693840200.0"},{"comment_id":"995106","poster":"casharan","upvote_count":"1","content":"Selected Answer: D\ncheck the link below:\nhttps://docs.aws.amazon.com/lambda/latest/operatorguide/orchestration.html","timestamp":"1693484880.0"},{"upvote_count":"1","comment_id":"992307","poster":"hmdev","timestamp":"1693233960.0","content":"Selected Answer: B\nYou can use Step functions to create a workflow of functions that should be invoked in a sequence. You can also push output from one one-step function and use it as an input for next-step function. Also, Step functions have very useful Retry and Catch -> error-handling features."},{"timestamp":"1691547600.0","comment_id":"976173","poster":"jayvarma","upvote_count":"1","content":"Keyword: run in sequence and to handle reprocessing of data. So, answer is option B. And also each task in a step function can be handled by a different AWS Service such as AWS Lambda or AWS Glue which is used for ETL jobs."},{"upvote_count":"1","content":"Selected Answer: B\nI'm thinking B","poster":"elfinka9","timestamp":"1690871280.0","comment_id":"968778"},{"timestamp":"1688724360.0","poster":"Suvomita","upvote_count":"1","comment_id":"945549","content":"Selected Answer: D\nD is the right answer"},{"poster":"MatthewHuiii","comment_id":"930363","content":"B is correct","upvote_count":"1","timestamp":"1687428300.0"},{"timestamp":"1685706300.0","content":"Selected Answer: B\nAll the key words of the question points at Step Function, check the link below:\n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html","upvote_count":"2","comment_id":"912781","poster":"Baba_Eni"},{"poster":"ricky536","timestamp":"1685457780.0","comment_id":"910338","upvote_count":"1","content":"B is correct"},{"comment_id":"883805","poster":"ihebchorfi","content":"Selected Answer: B\nEasily B","timestamp":"1682704800.0","upvote_count":"1"},{"poster":"MrTee","content":"Selected Answer: B\nOption B is the correct choice. AWS Step Functions allows you to coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. It also provides a way to handle errors and retry failed steps, making it a good fit for the company’s requirements.","upvote_count":"2","comment_id":"879768","timestamp":"1682378460.0"},{"content":"Selected Answer: C\nQuestion talks of ingesting huge volumes of data and orchestrating data flows, keywords for aws glue. I go with C","comments":[{"content":"Glue is an ETL tool it is not for orchestration of data flows, Step Function is for orchestration I think Glue is not the best option here.","timestamp":"1682954400.0","comment_id":"886434","upvote_count":"2","poster":"rlnd2000"}],"timestamp":"1681555980.0","comment_id":"870847","poster":"MrTee","upvote_count":"1"},{"poster":"ihta_2031","timestamp":"1680353820.0","comment_id":"857915","upvote_count":"2","content":"Selected Answer: B\nB is correct"},{"poster":"March2023","comment_id":"850556","upvote_count":"2","content":"Selected Answer: B\nim going with B","timestamp":"1679787660.0"},{"upvote_count":"2","content":"Selected Answer: B\nB\nThe requirement is the orchestration of the data flows, not data.","comment_id":"845432","poster":"Untamables","timestamp":"1679362560.0"},{"upvote_count":"2","poster":"svrnvtr","content":"Selected Answer: B\nIt is B","timestamp":"1679355360.0","comment_id":"845350"},{"comment_id":"844564","upvote_count":"1","content":"The answer is C, Glue is an ETL service where data processing code is pushed then multiple crawler jobs are setup to import the data from different sources. So I go with ‘C’.","timestamp":"1679290920.0","poster":"Warlord_92"},{"content":"Selected Answer: B\nNo brainer B.\n\n`manage and automate the orchestration` is the key , it is not about the processing data jobs itself, but the management","upvote_count":"2","poster":"clarksu","comment_id":"842402","timestamp":"1679102340.0"},{"comment_id":"841481","content":"Selected Answer: C\nThe answer is clearly C. Glue is an ETL that can process large volume of data and is scalable https://docs.aws.amazon.com/glue/latest/dg/auto-scaling.html","timestamp":"1679015880.0","upvote_count":"1","poster":"m4r0ck"},{"upvote_count":"2","poster":"haaris786","timestamp":"1678958220.0","comment_id":"840743","content":"It can't be anything other than B.","comments":[{"poster":"m4r0ck","comment_id":"841483","upvote_count":"1","content":"how come B is the answer ??? Step functions do not do data transformation!","timestamp":"1679015940.0"},{"comment_id":"841487","timestamp":"1679016060.0","comments":[{"content":"I too thought it was glue, but that is because of the pre-amble. What it does.. but the question is HOW to orchestrate the job.","poster":"CapJackSparrow","timestamp":"1679446980.0","comment_id":"846545","upvote_count":"1"}],"content":"https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html","poster":"m4r0ck","upvote_count":"1"}]}],"answers_community":["B (89%)","6%"],"question_text":"A company is building a scalable data management solution by using AWS services to improve the speed and agility of development. The solution will ingest large volumes of data from various sources and will process this data through multiple business rules and transformations.\nThe solution requires business rules to run in sequence and to handle reprocessing of data if errors occur when the business rules run. The company needs the solution to be scalable and to require the least possible maintenance.\nWhich AWS service should the company use to manage and automate the orchestration of the data flows to meet these requirements?","question_images":[],"topic":"1","answer":"B"},{"id":"NwClTwtR5NeOtZkOJC7X","topic":"1","question_id":335,"question_text":"A company has deployed infrastructure on AWS. A development team wants to create an AWS Lambda function that will retrieve data from an Amazon Aurora database. The Amazon Aurora database is in a private subnet in company's VPC. The VPC is named VPC1. The data is relational in nature. The Lambda function needs to access the data securely.\nWhich solution will meet these requirements?","choices":{"D":"Export the data from the Aurora database to Amazon S3. Create and launch a Lambda function in VPC1. Configure the Lambda function query the data from Amazon S3.","C":"Create the Lambda function. Configure VPC1 access for the function. Assign a security group named SG1 to the Lambda function. Assign a second security group named SG2 to the database. Add an inbound rule to SG1 to allow TCP traffic from Port 3306.","A":"Create the Lambda function. Configure VPC1 access for the function. Attach a security group named SG1 to both the Lambda function and the database. Configure the security group inbound and outbound rules to allow TCP traffic on Port 3306.","B":"Create and launch a Lambda function in a new public subnet that is in a new VPC named VPC2. Create a peering connection between VPC1 and VPC2."},"answer_description":"","answer_ET":"A","unix_timestamp":1679435220,"question_images":[],"exam_id":24,"answer_images":[],"discussion":[{"poster":"[Removed]","timestamp":"1701927240.0","upvote_count":"10","comments":[{"poster":"9d8dd9c","upvote_count":"1","timestamp":"1729170120.0","comments":[{"timestamp":"1734446340.0","upvote_count":"3","comment_id":"1327958","poster":"Yuri_024","content":"SG1 is for the lambda, SG2 is for the database. In option C it says setting inbound traffic for the SG1 on TCP port 3306. But it should be setting inbound traffic for SG2 on TCP port 3306."}],"comment_id":"1299222","content":"But aren't the routing on SGs state-full so allowing inbound allows outbound too? or am I confusing that with something else?"}],"comment_id":"1089964","content":"ooooh this one was rough. I am going with A --> https://repost.aws/knowledge-center/connect-lambda-to-an-rds-instance\n\nI was between A and C... wording for both tricky. But the only way C would work is if the last portion of the sentence the read \"Add an inbound rule to SG2 to allow TCP traffic from port 3306\" or \"Add an outbound rule to SG1 to allow TCP traffic... \""},{"upvote_count":"7","comment_id":"854015","content":"Selected Answer: A\nCorrect Answer is Answer A\nFor B creating new VPC for lambda does not seems a suitable solution\nFor C Assigning differrent security groups to both will not work\nOption D will not be suitable for relational data and involve S3 in solution","poster":"shahs10","timestamp":"1680065580.0"},{"timestamp":"1734889080.0","content":"Selected Answer: A\nB) Eliminated - Placing the Lambda function in a public subnet compromises security\n\nC) Eliminated - The rule should allow traffic to SG2 (the database’s security group) from SG1 (the Lambda function’s security group), not the other way around.\n\nD) Eliminated - Adds significant operational complexity","upvote_count":"1","comment_id":"1330497","poster":"sumanshu"},{"upvote_count":"2","content":"this one is badly written hehe\nI would say A, but they missed to mention that this only works securely if the secgroup is listed as destination of the rules.\nB would also work, but you need to properly configure it....","comment_id":"1267243","timestamp":"1723823340.0","poster":"wh1t4k3r"},{"poster":"Saurabh04","upvote_count":"1","timestamp":"1722766140.0","comment_id":"1260610","content":"Correction answer should be option C. Lambda function, configure VPC1 access, and assign separate security groups:\nLambda Function: Associate the Lambda function with VPC1.\nSecurity Group (SG1): Assign SG1 to the Lambda function.\nSecurity Group (SG2): Assign a second security group (SG2) to the Aurora database.\nInbound Rule: Add an inbound rule to SG1 to allow TCP traffic from Port 3306 (Aurora database port).\nThis approach ensures proper separation of concerns and simplifies security group management."},{"content":"This appear at 17 Jun exam","upvote_count":"4","poster":"tsangckl","timestamp":"1718595240.0","comment_id":"1231690"},{"upvote_count":"2","poster":"65703c1","content":"Selected Answer: A\nA is the correct answer.","comment_id":"1215091","timestamp":"1716309060.0"},{"comment_id":"1183281","content":"A seems the answer, although a single SG for both the DB and Lambda is not a great practice. I would go with 2 SGs.","poster":"ibratoev","upvote_count":"3","timestamp":"1711454760.0"},{"comment_id":"1165779","upvote_count":"3","poster":"TheFivePips","timestamp":"1709568240.0","content":"Selected Answer: A\nSecurity groups are statefull so you dont need to specify both inbound and outbound rules. However, you should have security groups on both resources as a best practice, and I dont think it is enough to have an inbound rule just on the lambda security group in this case. \nThis would essentially give the DB access to send traffic to the lambda function, rather than the lambda function accessing data from the DB like we want. If the lambda function doesnt have a permission on its security group letting it access the DB, then it will never communicate with it unless the DB contacts it first.\nIf C had placed the inbound permission on the DB, or if it had placed the outbound permission on the lambda then I think it would be right.\nSo while the wording is a little confusing, I think A is correct"},{"poster":"quanghao","comment_id":"1056067","upvote_count":"2","content":"Selected Answer: B\nA Lambda function and RDS instance in different VPCs\nFirst, use VPC peering to connect the two VPCs. Then, use the networking configurations to connect the Lambda function in one VPC to the RDS instance in the other:","timestamp":"1698483120.0"},{"comments":[{"poster":"[Removed]","content":"if it were private maybe... but public so this answer definitely wrong","timestamp":"1701924060.0","comment_id":"1089949","upvote_count":"1"}],"timestamp":"1697770980.0","comment_id":"1048330","content":"Selected Answer: B\nThis is the only one where lambda can reach the Database anyway, seems to me a prerequisite if the VPC was mentioned. Lambda by default, launched outside your VPC (in an AWS-owned VPC) so it cannot access resources.","poster":"hcsaba1982","upvote_count":"1"},{"upvote_count":"1","comment_id":"1045525","content":"Selected Answer: B\nB is correct?","poster":"dexdinh91","timestamp":"1697518500.0"},{"timestamp":"1697088720.0","comments":[{"timestamp":"1701925020.0","poster":"[Removed]","upvote_count":"1","content":"C the wording throws me off... Because the inbound rule in the end of the statement should be to the database not SG1. so we want to allow lambda access to the DB... The way this option is worded is not really giving lambda access to the db... it's giving DB access to lambda but not the other way around which we need. So leaning with A","comment_id":"1089955"}],"poster":"quanbui","upvote_count":"2","comment_id":"1041389","content":"Selected Answer: C\nC, need 2 SG"},{"comment_id":"1012640","poster":"sofiatian","comments":[{"content":"nonsense\nwhy would anyone want sql application port access to lambda??\n\nA is the only naswer","upvote_count":"2","timestamp":"1706708400.0","poster":"konieczny69","comment_id":"1136794"}],"upvote_count":"1","content":"Selected Answer: C\nNeed two security groups. One is for Lambda function. The other one is for DB","timestamp":"1695248520.0"},{"comment_id":"1003533","poster":"hsinchang","upvote_count":"3","content":"A. right\nB. public, unsecure\nC. excessive connections\nD. additional cost and complexity","timestamp":"1694299680.0"},{"timestamp":"1692984780.0","content":"Selected Answer: A\nVPC Configuration:\n\nEnsure that your Lambda function is configured to run within the same VPC where your Amazon Aurora database resides (VPC1 in this case).\nConfigure the Lambda function to use the appropriate subnets within VPC1, which are associated with the private subnet where your Amazon Aurora database is located.\nSecurity Groups:\n\nAttach a security group (SG1) to both the Lambda function and the Amazon Aurora database.\nConfigure the security group inbound rules for SG1 to allow incoming TCP traffic on Port 3306, which is the default port for MySQL (used by Aurora). This will allow communication between the Lambda function and the database.\nOutbound rules should be allowed by default, so you don't need to make any changes there.","poster":"love777","upvote_count":"2","comment_id":"990274"},{"upvote_count":"5","poster":"ninomfr64","timestamp":"1692429660.0","comment_id":"985035","content":"Selected Answer: A\nThere isn't the ideal solution to the use case among the options.\n\nB) no need to create a new VPC and also you need to add route tables and configure SGs to make it works\nC) this could work if the rule on SG1 was outbound instead of inbound (the connection is initiated from Lambda to Aurora)\nD) export data to S3 is overkill and if you do that you no longer need to deploy the lambda in the VPC\n\nA) works, as SG1 is attached to both Lambda and Aurora we need outbound rule to 3306 (Lambda initiate communication to Aurora) and also inbound rule from 3306 (to allow Aurora accept connection from Lambda). I don't like to have the same SG1 for both the Lambda and the Aurora"},{"content":"Selected Answer: C\nhttps://www.youtube.com/watch?v=UgWjbSixRg4&ab_channel=DevProblems","comment_id":"958267","timestamp":"1689931500.0","upvote_count":"2","poster":"AWSdeveloper08"},{"upvote_count":"3","poster":"ancomedian","content":"Selected Answer: C\nThe correct answer is C\nhttps://www.youtube.com/watch?v=UgWjbSixRg4","comment_id":"955143","timestamp":"1689668580.0"},{"content":"It seems it is A but as I know we don’t need to create outbound rules when we return something. So why it is A ?","poster":"awsazedevsh","comments":[{"poster":"awsazedevsh","upvote_count":"2","content":"Nevermind. We need it to let Lambda to make outbound request","comment_id":"945889","timestamp":"1688750340.0"}],"timestamp":"1688039280.0","upvote_count":"1","comment_id":"938007"},{"timestamp":"1687704840.0","upvote_count":"1","comments":[{"upvote_count":"2","comment_id":"933705","poster":"umer1998","content":"For B (There is no need to create another VPC, since we can simply add a lambda to a VPC with private subnets)\nFor A (Security Group (SG) is stateless. By using NACL we can do outbound and inbound rules modification + SG is used to give access, if you keep both Lambda and DB in same same SG, if you try to give access of lambda to another resource, that another resource will automatically gets the RDS access - which is out of question)","timestamp":"1687705080.0"}],"content":"The correct answer is C\nhttps://www.youtube.com/watch?v=UgWjbSixRg4","comment_id":"933703","poster":"umer1998"},{"timestamp":"1686998160.0","poster":"rlnd2000","upvote_count":"1","comment_id":"925911","content":"Selected Answer: C\nC is correct, \nA is a wrong choice, how to config outbound rules in SG? :)"},{"timestamp":"1686727260.0","upvote_count":"2","poster":"kavi00203","comment_id":"922844","content":"I think B ,\n\nplease verify this guys,\n\nhttps://repost.aws/en/knowledge-center/connect-lambda-to-an-rds-instance#:~:text=Lambda%27s%20subnets%27%20CIDRs.-,A%20Lambda%20function%20and%20RDS%20instance%20in%20different%20VPCs,function%20in%20one%20VPC%20to%20the%20RDS%20instance%20in%20the%20other,-%3A"},{"comment_id":"898019","timestamp":"1684121640.0","content":"Ans: C\n\nTo access the Amazon Aurora database in a private subnet of VPC1, the Lambda function should be launched inside the same VPC1 and should have access to that VPC.","poster":"Nagendhar","upvote_count":"2"},{"comments":[{"poster":"yeacuz","comment_id":"900425","content":"Answer C allows inbound traffic to SG1 which is attached to the Lambda function. The Lambda function does not need inbound traffic allowed inbound on port 3306 - the database does. Also, while you do not need to configure outbound rules for *return* traffic, you DO need to configure outbound rules if the Lambda function is *initiating* the traffic - which in this case it is. Therefore, the answer is A.","upvote_count":"4","timestamp":"1684345740.0"},{"comment_id":"907006","content":"After further research on this question, I agree that Answer A is the right answer. The scenario described in this question is similar to the example explanation in this repost: https://repost.aws/knowledge-center/connect-lambda-to-an-rds-instance\n\nAn outbound rule is indeed required in this situation, therefore, answer C is wrong.","upvote_count":"2","timestamp":"1685069400.0","poster":"AgboolaKun"}],"poster":"AgboolaKun","content":"Selected Answer: C\nAnswers B and D are obviously incorrect. \n\nAnswer A is wrong because you do not need to configure outbound rule for security groups. Security groups are stateful, meaning you do not need to add rules for return.\n\nTherefore, the correct answer is C.","upvote_count":"3","timestamp":"1683916020.0","comment_id":"896159"},{"content":"Selected Answer: A\nC is incorrect, because it's missing the step where both security groups need to allow access to each other","poster":"awsdummie","comment_id":"876833","upvote_count":"3","timestamp":"1682112300.0"},{"content":"Selected Answer: A\nCorrect Answer is Answer A","poster":"Rpod","comment_id":"876491","upvote_count":"4","timestamp":"1682078520.0"},{"comments":[{"content":"I think C is incorrect. because it doesn't set the outbound rule","timestamp":"1683792540.0","comment_id":"894830","upvote_count":"2","poster":"stlim83"}],"poster":"rlnd2000","content":"Selected Answer: C\nYou must create two security groups, one for the lambda ENI and another for the RDS, the same SG for both won't work.\n\nhttps://repost.aws/knowledge-center/connect-lambda-to-an-rds-instance\n\nhttps://medium.com/@Oldmanyellingatcloud/how-to-connect-your-lambda-function-securely-to-your-private-rds-instances-in-your-vpc-29789220a33","upvote_count":"2","comment_id":"867632","timestamp":"1681242420.0"},{"poster":"Untamables","content":"Selected Answer: A\nThe correct answer is A.\nI agree with Watascript.","comment_id":"850596","timestamp":"1679792160.0","upvote_count":"4"},{"poster":"DenMaslov","content":"Based on the requirement to access the Amazon Aurora database securely from the Lambda function, the correct solution is A. The Lambda function needs to be configured to access resources in VPC1, where the database is located. A security group (SG1) should be attached to both the Lambda function and the database, and the inbound and outbound rules of SG1 should allow TCP traffic on port 3306 to enable communication between the Lambda function and the database. This approach ensures that the connection between the Lambda function and the database is secure and the data is accessed only through the allowed port.","upvote_count":"3","timestamp":"1679779920.0","comment_id":"850476"},{"timestamp":"1679748660.0","comment_id":"850132","poster":"Watascript","upvote_count":"6","content":"Selected Answer: A\nA?\nhttps://repost.aws/en/knowledge-center/connect-lambda-to-an-rds-instance"},{"content":"Selected Answer: C\nI think its C as well","poster":"March2023","comment_id":"848539","upvote_count":"1","timestamp":"1679595120.0"},{"upvote_count":"1","comments":[{"comment_id":"847591","content":"I was thinking C too but the second security group throws me off","upvote_count":"1","poster":"Dun6","timestamp":"1679526180.0"}],"timestamp":"1679435220.0","comment_id":"846396","content":"Selected Answer: C\nMay be C","poster":"svrnvtr"}],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/103523-exam-aws-certified-developer-associate-dva-c02-topic-1/","isMC":true,"timestamp":"2023-03-21 22:47:00","answers_community":["A (65%)","C (28%)","7%"]}],"exam":{"numberOfQuestions":551,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"AWS Certified Developer - Associate DVA-C02","id":24,"provider":"Amazon"},"currentPage":67},"__N_SSP":true}