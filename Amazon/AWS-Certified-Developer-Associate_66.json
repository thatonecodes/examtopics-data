{"pageProps":{"questions":[{"id":"akWi8wNhRZRHmYzyAAB4","exam_id":25,"question_images":[],"timestamp":"2023-01-22 07:44:00","answers_community":["A (78%)","C (22%)"],"question_text":"A developer is creating a new application for a pet store. The application will manage customer rewards points. The developer will use Amazon DynamoDB to store the data for the application. The developer needs to optimize query performance and limit partition overload before actual performance analysis.\n\nWhich option should the developer use for a partition key to meet these requirements?","unix_timestamp":1674369840,"discussion":[{"upvote_count":"1","content":"Selected Answer: A\nNothing was said about possible usage (I mean, how the data is queried). I also don't see any hints in the \"rewards\" domain. That's why I'd choose A... Although C is also a good option. On the other hand, imagine many users are signed up in one day. It can also impact the partitioning.","timestamp":"1710947160.0","comment_id":"1178425","poster":"a15ce96"},{"poster":"TeeTheMan","upvote_count":"1","comment_id":"964696","timestamp":"1690460700.0","content":"A is wrong because a randomly generated string isn’t useful as a partition key. How will you know what the partition key is when a customer visits the application and you need to lookup the customer’s points?\nThe only option that makes sense is B. And customer’s full names are sufficiently unique."},{"timestamp":"1675602060.0","upvote_count":"1","content":"Selected Answer: C\nThis is because using the sign-up date as the partition key distributes the data evenly across partitions, reducing the risk of hot partitioning, which can impact performance.","poster":"Drey","comment_id":"798835"},{"content":"Selected Answer: A\nA is correct","poster":"pancman","timestamp":"1675473600.0","comment_id":"797522","upvote_count":"1"},{"comment_id":"787334","upvote_count":"1","timestamp":"1674629700.0","content":"Selected Answer: A\nIts has to be unique number","poster":"breathingcloud"},{"poster":"JagpreetLM10","comment_id":"786365","upvote_count":"1","timestamp":"1674553080.0","content":"Selected Answer: C\nCan be C . A partition key is used to determine the physical partition where the item is stored. The partition key should be chosen based on the access patterns of the data and how the data will be queried. In this scenario, the developer will likely want to query the data based on the date when the customer signed up for the rewards program, so using the date as the partition key will provide the best performance. Using a date as a partition key will also help to spread the data out over multiple partitions, avoiding partition overload."},{"content":"Selected Answer: A\nThe most unique and the highest cardinality","upvote_count":"3","comment_id":"784382","timestamp":"1674397080.0","poster":"KT_Yu"},{"content":"Selected Answer: A\nA it is","comment_id":"783993","timestamp":"1674369840.0","upvote_count":"1","poster":"Phinx"}],"answer":"A","answer_ET":"A","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/96438-exam-aws-certified-developer-associate-topic-1-question-392/","choices":{"B":"The customer’s full name","C":"The date when the customer signed up for the rewards program","A":"A randomly generated universally unique identifier (UUID)","D":"The name of the customer’s pet"},"isMC":true,"answer_description":"","answer_images":[],"question_id":326},{"id":"3kD11KCvchaJXBEh8aUV","answer":"CE","question_id":327,"topic":"1","exam_id":25,"answer_images":[],"timestamp":"2023-01-20 20:02:00","url":"https://www.examtopics.com/discussions/amazon/view/96207-exam-aws-certified-developer-associate-topic-1-question-393/","isMC":true,"unix_timestamp":1674241320,"answer_description":"","answer_ET":"CE","choices":{"B":"Install the Amazon CloudWatch agent on the Apache HTTP servers. Configure the CloudWatch agent to push the logs to the Amazon Kinesis Data Firehose delivery stream.","C":"Install the Amazon Kinesis agent on the Apache HTTP servers. Configure the Kinesis agent to push the logs to the Amazon Kinesis Data Firehose delivery stream.","E":"Create an Amazon Kinesis Data Firehose delivery stream. Set the source as Direct PUT. Implement an AWS Lambda function to convert the logs to JSON format. Enable source record transformation on the Kinesis Data Firehose delivery stream for the Lambda function. Set the OpenSearch Service cluster as the destination.","D":"Create an AWS Lambda function that converts the logs to JSON format and pushes the results to the OpenSearch Service cluster. Consume the logs from the Amazon Simple Queue Service (Amazon SQS) queue by using the Lambda function.","A":"Install the Amazon CloudWatch agent on the Apache HTTP servers. Configure the CloudWatch agent to push the logs to an Amazon Simple Queue Service (Amazon SQS) queue."},"question_text":"A company needs an application that consumes logs from Apache HTTP servers at a large scale with near real-time processing. The logs will vary in size from 300 KB to 500 KB. As part of processing, the company needs to convert the logs to JSON format and then upload the logs to an Amazon OpenSearch Service cluster.\n\nWhich combination of steps will meet these requirements? (Choose two.)","question_images":[],"answers_community":["CE (58%)","BE (42%)"],"discussion":[{"upvote_count":"1","timestamp":"1686222960.0","poster":"rlnd2000","content":"Selected Answer: CE\nIn my opinion using the Amazon CloudWatch agent and CloudWatch Logs introduces an additional step of collecting the logs in CloudWatch Logs before processing them and forwarding them to the Kinesis Data Firehose delivery stream. Although CloudWatch Logs provides real-time log collection, there might be some delay in the processing and forwarding steps, which could affect the overall near real-time processing requirement.","comment_id":"918173"},{"timestamp":"1676932500.0","comment_id":"815977","upvote_count":"3","poster":"joanneli77","content":"Selected Answer: CE\n\"at a large scale\" indicates Kinesis. BE could be right, but it is not AWS \"large scale\" streaming of logs platform."},{"poster":"robotgeek","comment_id":"807270","timestamp":"1676282700.0","upvote_count":"2","content":"Selected Answer: BE\nI think it does not make sense to use a Java app to collect logs in an Apache server, cloudwatch agent is ideal for that, then you bridge CloudWatch to Kinesis in AWS"},{"comment_id":"804688","timestamp":"1676058180.0","comments":[{"timestamp":"1710947340.0","upvote_count":"1","comment_id":"1178428","poster":"a15ce96","content":"There's a Kinesis agent for these needs: https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html. Didn't know that. So CE should work."},{"comment_id":"1039587","upvote_count":"1","poster":"llw33","content":"Still can't find, it says that CloudWatch Logs Subscriptions can send data to Firehose, but not CW Agent","timestamp":"1696947480.0"}],"upvote_count":"1","content":"Selected Answer: BE\nhttps://docs.aws.amazon.com/firehose/latest/dev/writing-with-cloudwatch-logs.html download pdf, page 41 tells you how cloudwatch agent can write to kinesis firehose","poster":"Smartiup"},{"upvote_count":"2","poster":"Drey","comment_id":"801110","timestamp":"1675785960.0","content":"Selected Answer: BE\nIt's BE."},{"comments":[{"content":"cloudwatch is not realtime. There're some defaults like 5min, 1min, and high resolution custom metric in seconds.","upvote_count":"1","poster":"tieyua","comments":[{"content":"CloudWatch is near real-time.","comments":[{"content":"https://aws.amazon.com/about-aws/whats-new/2015/09/near-real-time-processing-of-amazon-cloudwatch-logs-with-aws-lambda/","timestamp":"1675603140.0","poster":"Drey","comment_id":"798851","upvote_count":"1"}],"comment_id":"798849","timestamp":"1675603140.0","upvote_count":"1","poster":"Drey"}],"comment_id":"795733","timestamp":"1675309800.0"}],"upvote_count":"3","poster":"JagpreetLM10","timestamp":"1674553200.0","content":"Selected Answer: BE\nB. Install the Amazon CloudWatch agent on the Apache HTTP servers. Configure the CloudWatch agent to push the logs to the Amazon Kinesis Data Firehose delivery stream.\nE. Create an Amazon Kinesis Data Firehose delivery stream. Set the source as Direct PUT. Implement an AWS Lambda function to convert the logs to JSON format. Enable source record transformation on the Kinesis Data Firehose delivery stream for the Lambda function. Set the OpenSearch Service cluster as the destination.\n\nBy installing the CloudWatch agent on the Apache HTTP servers, the company can push the logs to the Kinesis Data Firehose delivery stream. The delivery stream can then be configured to automatically convert the logs to JSON format by using an AWS Lambda function, and then pushes the logs to OpenSearch Service cluster. This allows the company to consume logs from Apache HTTP servers at a large scale with near real-time processing.","comment_id":"786368"},{"timestamp":"1674431100.0","content":"Selected Answer: CE\nhttps://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-destination.html","poster":"Phinx","comment_id":"784788","upvote_count":"2"},{"poster":"breathingcloud","timestamp":"1674319260.0","upvote_count":"3","comment_id":"783543","content":"Selected Answer: CE\nAgreeing for C E\nKinesis Data firehouse does the job"},{"upvote_count":"2","timestamp":"1674241320.0","comment_id":"782675","poster":"KT_Yu","content":"Selected Answer: CE\nmost likely C and E.\nSQS queue is not applicable here because it is not near-real time and the maximum message size is 256kb."}]},{"id":"qwoksf2FIuBvsZ4FNBAl","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/96492-exam-aws-certified-developer-associate-topic-1-question-394/","answers_community":["BD (100%)"],"topic":"1","question_id":328,"exam_id":25,"question_images":[],"answer_ET":"BD","unix_timestamp":1674397440,"answer":"BD","timestamp":"2023-01-22 15:24:00","discussion":[{"content":"Selected Answer: BD\nBD Simple","upvote_count":"1","poster":"breadops","comment_id":"981247","timestamp":"1692069240.0"},{"upvote_count":"3","comment_id":"822150","poster":"ka1tw","timestamp":"1677398520.0","content":"Explanation:\n\nOption B is required to upload the build artifact to Amazon S3. Since the build artifact size exceeds the maximum size allowed for Lambda deployment packages, uploading the artifact to S3 is required.\n\nOption D is used to update the Lambda function code from the S3 bucket where the build artifact was uploaded. The --s3-bucket and --s3-key parameters should be specified to specify the location of the code."},{"comment_id":"786370","content":"Selected Answer: BD\nB. Upload the build artifact to Amazon S3.\nD. Issue the aws lambda update-function-code CLI command with the --s3-bucket and --s3-key parameters.\n\nThe developer should upload the build artifact to Amazon S3, and then use the aws lambda update-function-code CLI command with the --s3-bucket and --s3-key parameters to update the function code from the build artifact that was uploaded to S3. This allows the developer to deploy the large build artifact without exceeding the 50 MB limit for the aws lambda update-function-code command, and also allows to use S3 as a deployment artifact storage.","upvote_count":"1","poster":"JagpreetLM10","timestamp":"1674553380.0"},{"timestamp":"1674437220.0","comment_id":"784844","poster":"Phinx","content":"Selected Answer: BD\nFor build packages more than 50 MB, it needs to be uploaded to S3 and specify the location of the package during deployment --s3-bucket and --s3-key\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-package.html#gettingstarted-package-awsother\nhttps://nono.ma/update-aws-lambda-function-code","upvote_count":"3"},{"upvote_count":"3","comment_id":"784387","comments":[{"comment_id":"785054","upvote_count":"1","content":"So BD?","timestamp":"1674457020.0","poster":"BobAWS23"}],"timestamp":"1674397440.0","poster":"KT_Yu","content":"build artifacts will exceed 50 MB in size => upload to S3 instead of the lambda console\n\nuse the CLI command update-function-code:\nupdate-function-code\n--function-name <value>\n[--s3-bucket <value>] <= must remember these 3 flags\n[--s3-key <value>] <= must remember these 3 flags\n[--s3-object-version <value>] <= must remember these 3 flags\n\nhttps://docs.aws.amazon.com/cli/latest/reference/lambda/update-function-code.html"}],"choices":{"A":"Issue the aws lambda update-function-code CLI command with the --zip-file fileb://my-function.zip parameter.","E":"Issue the aws lambda update-function-code CLI command with a parameter that points to the source code in AWS CodeCommit.","D":"Issue the aws lambda update-function-code CLI command with the --s3-bucket and --s3-key parameters.","C":"Issue the aws cloudformation package CLI command.","B":"Upload the build artifact to Amazon S3."},"answer_description":"","question_text":"A company is building a serverless application that uses AWS Lambda. The application includes Lambda functions that are exposed by Amazon API Gateway. The functions will use several large third-party libraries, and the build artifacts will exceed 50 MB in size.\n\nWhich combination of steps should a developer take to prepare and perform the deployment? (Choose two.)","isMC":true},{"id":"XyIsNWwmvMt0SH47AP3q","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/96490-exam-aws-certified-developer-associate-topic-1-question-395/","answers_community":["C (100%)"],"question_id":329,"exam_id":25,"topic":"1","question_images":[],"unix_timestamp":1674396660,"answer_ET":"C","answer":"C","discussion":[{"comments":[{"timestamp":"1675481400.0","upvote_count":"1","content":"Great explanation, very clear. Thanks!","comment_id":"797568","poster":"pancman"}],"timestamp":"1674553440.0","comment_id":"786373","content":"Selected Answer: C\nUse the sam local start-lambda CLI command to start Lambda. Use the sam local generate-event s3 put CLI command to create the Lambda test JSON file. Use the sam local invoke CLI command with the JSON file as the argument to invoke the Lambda function.\n\nThe AWS SAM (Serverless Application Model) provides the sam local commands which allow developers to test Lambda functions locally on their development machine, this includes the ability to simulate S3 events with sam local generate-event s3 put command. This solution has the least operational overhead as it allows the developer to test the Lambda function locally without the need to upload objects to a production S3 bucket and also it allows to test the function with similar events that will happen in production, this ensures that the function will work correctly when it is deployed in production.","poster":"JagpreetLM10","upvote_count":"6"},{"content":"Selected Answer: C\nC obviously\nsam local start-lambda & sam local generate-event\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-local-start-lambda.html\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-local-generate-event.html","timestamp":"1674397560.0","upvote_count":"3","poster":"KT_Yu","comment_id":"784388"},{"upvote_count":"2","comment_id":"784378","content":"Selected Answer: C\nUse sam local \n\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-local-start-lambda.html","timestamp":"1674396660.0","poster":"breathingcloud"}],"timestamp":"2023-01-22 15:11:00","choices":{"C":"Use the sam local start-lambda CLI command to start Lambda. Use the sam local generate-event s3 put CLI command to create the Lambda test JSON file. Use the sam local invoke CLI command with the JSON file as the argument to invoke the Lambda function.","A":"Upload an object to Amazon S3 by using the aws s3api put-object CLI command. Wait for the local Lambda invocation from the S3 event.","B":"Create a sample JSON text file for a put object S3 event. Invoke the Lambda function locally. Use the aws lambda invoke CLI command with the JSON file and Lambda function name as arguments.","D":"Create a JSON string for the put object S3 event. In the AWS Management Console, use the JSON string to create a test event for the local Lambda function. Perform the test."},"answer_description":"","question_text":"A developer is implementing an AWS Lambda function that will be invoked when an object is uploaded to Amazon S3. The developer wants to test the Lambda function in a local development machine before publishing the function to a production AWS account.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","isMC":true},{"id":"KlNnDWpiPk6kK8Lb4VAk","answer_ET":"D","answer_images":[],"answers_community":["D (50%)","A (33%)","B (17%)"],"isMC":true,"choices":{"C":"Write an AWS Lambda function that scans through all EC2 instances in the company accounts to detect EC2 instance lifecycle changes. Configure the Lambda function to write a notification message to the SQS queue in the main account if the function detects an EC2 instance lifecycle change. Add an Amazon EventBridge scheduled rule that invokes the Lambda function every minute.","D":"Configure the permissions on the main account event bus to receive events from all accounts. Create an Amazon EventBridge rule in each account to send all the EC2 instance lifecycle events to the main account event bus. Add an EventBridge rule to the main account event bus that matches all EC2 instance lifecycle events. Set the SQS queue as a target for the rule.","B":"Use the resource policies of the SQS queue in the main account to give each account permissions to write to that SQS queue. Add to the Amazon EventBridge event bus of each account an EventBridge rule that matches all EC2 instance lifecycle events. Add the SQS queue in the main account as a target of the rule.","A":"Configure Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the Amazon EventBridge event bus of the main account. Add an EventBridge rule to the event bus of the main account that matches all EC2 instance lifecycle events. Add the SQS queue as a target of the rule."},"unix_timestamp":1674242340,"question_id":330,"timestamp":"2023-01-20 20:19:00","discussion":[{"content":"Selected Answer: D\nAns: D. \nConfigure the permissions on the main account event bus to receive events from all accounts. Create an Amazon EventBridge rule in each account to send all the EC2 instance lifecycle events to the main account event bus. Add an EventBridge rule to the main account event bus that matches all EC2 instance lifecycle events. Set the SQS queue as a target for the rule.","timestamp":"1674242340.0","comments":[{"poster":"KT_Yu","comments":[{"timestamp":"1674398220.0","comment_id":"784394","comments":[{"poster":"KT_Yu","upvote_count":"1","content":"the second I mean B*\n\nhope it helps ;)","comment_id":"784397","timestamp":"1674398280.0"}],"poster":"KT_Yu","content":"I think the big difference between B and D is that D utilizes a central event bus in the main account while D sends events to SQS directly from different accounts. This is my best interpretation.","upvote_count":"1"}],"upvote_count":"1","content":"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html","timestamp":"1674242460.0","comment_id":"782692"}],"comment_id":"782689","poster":"KT_Yu","upvote_count":"5"},{"upvote_count":"1","comment_id":"852934","timestamp":"1679987820.0","poster":"shahs10","content":"Selected Answer: B\nFor Option C using lambda does not seem to be a good solution as we would have to trigger lambda on some schedule and it will has less granularity in time.\n\nFor D. Why would we be matching EC2 instance lifecycle events in Main account event bus and not in each account event bus and reducing overhead for main account"},{"poster":"DrDopey","comment_id":"823025","content":"Selected Answer: B\nThis solution uses resource policies to give permissions to all AWS accounts to write to the SQS queue in the main account. This way, EC2 instances in any account can send their lifecycle events to the SQS queue in the main account. Additionally, each account will have an EventBridge rule that matches all EC2 instance lifecycle events and sends them to the SQS queue in the main account. This solution meets the requirement of storing all the lifecycle events of EC2 instances from multiple accounts in a single Amazon SQS queue for further processing.","upvote_count":"1","timestamp":"1677452040.0"},{"upvote_count":"2","content":"Selected Answer: A\nThe best solution would be to use Amazon EventBridge and have each account deliver the EC2 instance lifecycle events to the EventBridge event bus of the main account. Then, set up a rule in the main account's event bus that matches all EC2 instance lifecycle events and set the SQS queue as a target for the rule. This solution provides a direct and efficient mechanism for collecting the EC2 instance lifecycle events and eliminates the need for a periodic scan by the Lambda function.","timestamp":"1675605300.0","comment_id":"798874","poster":"Drey"},{"content":"Selected Answer: A\nConfigure Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the Amazon EventBridge event bus of the main account. Add an EventBridge rule to the event bus of the main account that matches all EC2 instance lifecycle events. Add the SQS queue as a target of the rule.\n\nIn order to meet the requirements, the developer can use the EventBridge service. By configuring Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the EventBridge event bus of the main account, the developer can create an EventBridge rule in the main account that matches all EC2 instance lifecycle events and add the SQS queue as a target of the rule. This way, all the lifecycle events of the EC2 instances will be collected and stored in a single SQS queue in the main account for further processing.","timestamp":"1674553560.0","upvote_count":"2","comment_id":"786375","poster":"JagpreetLM10"},{"content":"Selected Answer: D\nI think it's D as well. All other options doesn't make sense.","poster":"Phinx","timestamp":"1674371400.0","upvote_count":"1","comment_id":"784004"}],"topic":"1","question_text":"A company is running Amazon EC2 instances in multiple AWS accounts. A developer needs to implement an application that collects all the lifecycle events of the EC2 instances. The application needs to store the lifecycle events in a single Amazon Simple Queue Service (Amazon SQS) queue in the company's main AWS account for further processing.\n\nWhich solution will meet these requirements?","exam_id":25,"answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/96209-exam-aws-certified-developer-associate-topic-1-question-396/","answer":"D"}],"exam":{"id":25,"name":"AWS Certified Developer Associate","isBeta":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","numberOfQuestions":443,"isMCOnly":true,"isImplemented":true},"currentPage":66},"__N_SSP":true}