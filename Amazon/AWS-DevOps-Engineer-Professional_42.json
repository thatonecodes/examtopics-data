{"pageProps":{"questions":[{"id":"4XUBKbKih5sAo5tmgFDJ","url":"https://www.examtopics.com/discussions/amazon/view/88923-exam-aws-devops-engineer-professional-topic-1-question-97/","answer_ET":"B","isMC":true,"question_text":"A company has multiple development groups working in a single shared AWS account. The senior manager of the groups wants to be alerted via a third-party API call when the creation of resources approaches the service limits for the account.\n\nWhich solution will accomplish this with the LEAST amount of development effort?","answer_images":[],"unix_timestamp":1669534440,"topic":"1","discussion":[{"comment_id":"732296","timestamp":"1669873620.0","content":"Selected Answer: B\nB. Any time there is a question about service limits, the answer is Trusted Advisor or Quota Monitor which still uses Trusted Advisor APIs.\nhttps://aws.amazon.com/solutions/implementations/quota-monitor/","upvote_count":"6","poster":"SmileyCloud"},{"content":"Selected Answer: B\nChatgpt said: B. The solution that will accomplish this with the LEAST amount of development effort is option B.\n\nOption A requires custom code to be developed to evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Option C requires custom code to be developed to refresh AWS Personal Health Dashboard checks, and option D requires custom code to be developed to add an AWS Config custom rule that streams notifications to an Amazon SNS topic.\n\nOption B leverages AWS Trusted Advisor, which provides automated checks for common AWS best practices, including service limits. A Lambda function can be deployed to refresh the Trusted Advisor checks, and a CloudWatch Events rule can be configured to run the Lambda function periodically. Another CloudWatch Events rule with an event pattern matching Trusted Advisor events can be configured to notify the senior manager via a third-party API call. This solution requires the least amount of development effort because it leverages existing AWS services and functionality, and does not require custom code to be developed.","comment_id":"860041","upvote_count":"1","timestamp":"1680533220.0","poster":"merki","comments":[{"content":"ChatGPT said answer is D","upvote_count":"1","poster":"sub_kutu","timestamp":"1696160400.0","comment_id":"1022209"}]},{"comment_id":"845424","timestamp":"1679361180.0","upvote_count":"1","content":"B-B-B-B","poster":"easytoo"},{"poster":"Piccaso","upvote_count":"2","comment_id":"803133","timestamp":"1675941720.0","content":"Selected Answer: D\nD is the simplest one"},{"poster":"Bulti","comment_id":"784893","upvote_count":"1","timestamp":"1674442500.0","content":"Selected Answer: B\nB is the correct answer."},{"timestamp":"1674263340.0","poster":"wzh5831","content":"Selected Answer: B\nTtusted Advisor allow you to check your account's service limits (quotas)\nhttps://docs.aws.amazon.com/awssupport/latest/user/service-limits.html","upvote_count":"1","comment_id":"782913"},{"timestamp":"1673301660.0","upvote_count":"1","poster":"saeidp","content":"Selected Answer: B\nB is the answer","comment_id":"770911"},{"poster":"Imstack","content":"B is the answer, best practice","timestamp":"1671790680.0","comment_id":"754120","upvote_count":"1"},{"content":"Selected Answer: B\n1. AWS Trusted Advisor can be used to access all checks in the Service Limits category.\nhttps://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html\n2. AWS Quota Monitor would be more opt but is not listed as an option. Quota Monitor leverages Trusted Advisor.","poster":"Maygam","upvote_count":"4","timestamp":"1669780800.0","comment_id":"731069"},{"poster":"adozoo","timestamp":"1669534440.0","upvote_count":"1","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/config/latest/developerguide/configlimits.html","comments":[{"upvote_count":"1","comment_id":"803135","timestamp":"1675941840.0","content":"This link shows configuration on limits. We need to detect the limits are hit. \nI though D was correct. \nI changed my mind to B.","poster":"Piccaso"}],"comment_id":"728003"}],"exam_id":35,"choices":{"D":"Add an AWS Config custom rule that runs periodically, checks the AWS service limit status, and streams notifications to an Amazon SNS topic. Deploy an AWS Lambda function that notifies the senior manager, and subscribe the Lambda function to the SNS topic.","A":"Create an Amazon CloudWatch Event rule that runs periodically and targets an AWS Lambda function. Within the Lambda function, evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Notify the senior manager if the account is approaching a service limit.","C":"Deploy an AWS Lambda function that refreshes AWS Personal Health Dashboard checks, and configure an Amazon CloudWatch Events rule to run the Lambda function periodically. Create another CloudWatch Events rule with an event pattern matching Personal Health Dashboard events and a target Lambda function. In the target Lambda function, notify the senior manager.","B":"Deploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and configure an Amazon CloudWatch Events rule to run the Lambda function periodically. Create another CloudWatch Events rule with an event pattern matching Trusted Advisor events and a target Lambda function. In the target Lambda function, notify the senior manager."},"answers_community":["B (82%)","D (18%)"],"question_images":[],"timestamp":"2022-11-27 08:34:00","answer_description":"","answer":"B","question_id":206},{"id":"SfenX0YiGUNVI9yVSXsF","answer_description":"","isMC":true,"question_text":"A DevOps engineer is architecting a continuous development strategy for a company's software as a service (SaaS) web application running on AWS. For application and security reasons, users subscribing to this application are distributed across multiple Application Load Balancers (ALBs), each of which has a dedicated Auto Scaling group and fleet of Amazon EC2 instances. The application does not require a build stage, and when it is committed to AWS CodeCommit, the application must trigger a simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets.\n\nWhich architecture will meet these requirements with the LEAST amount of configuration?","answer_ET":"C","timestamp":"2022-12-01 06:58:00","answer":"C","choices":{"A":"Create a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair.","D":"Create an AWS CodePipeline pipeline for each ALB-Auto Scaling group pair that deploys the application using an AWS CodeDeploy application and deployment group created for the same ALB-Auto Scaling group pair.","B":"Create a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.","C":"Create a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and unique deployment group for each ALB-Auto Scaling group pair."},"discussion":[{"poster":"merki","content":"Selected Answer: B\nChatgpt said: B. \nThe architecture that will meet the requirements with the LEAST amount of configuration is option B, which is to create a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.\n\nThis approach is the simplest and most straightforward way to achieve simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets. The same CodeDeploy application and deployment group can be used across all ALBs and Auto Scaling groups without any additional configuration needed.","comment_id":"860037","upvote_count":"1","timestamp":"1680533100.0"},{"upvote_count":"1","content":"c-c-c-c--c-c-c-c-","timestamp":"1679428620.0","poster":"easytoo","comment_id":"846292"},{"timestamp":"1676721720.0","content":"Selected Answer: C\nA: not correct because of \"using unique AWS CodeDeploy applications\". There is only one application.\nB: not correct because it's not complete.\nD: not correct. It must be wrong to create CodePipelines for ALB-AutoScaling group respectively.","poster":"Piccaso","upvote_count":"2","comment_id":"812921"},{"comment_id":"784896","content":"Selected Answer: C\nAnswer is C with least amount of configuration","upvote_count":"1","timestamp":"1674442740.0","poster":"Bulti"},{"upvote_count":"1","content":"Selected Answer: C\nFor me - C","comment_id":"777738","poster":"Oleg_gol","timestamp":"1673877660.0"},{"timestamp":"1672195320.0","content":"https://www.examtopics.com/discussions/amazon/view/28608-exam-aws-devops-engineer-professional-topic-1-question-177/","comment_id":"759289","poster":"apcloud","upvote_count":"1"},{"poster":"Kapello10","comment_id":"754770","timestamp":"1671870600.0","content":"Selected Answer: A\nA is the answer\n\nCreate a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair.","upvote_count":"1"},{"poster":"SmileyCloud","comment_id":"732307","content":"Selected Answer: C\nC is correct, but there is a limitation that one deployment group can deploy up to 10 ASGs. The question doesn't say how many ASGs are there, it just says multiple.","upvote_count":"2","comments":[{"comment_id":"733710","upvote_count":"2","content":"why limiation? it says clearly a different ASG per deployment group.\n\n\"\"unique deployment group for each ALB-Auto Scaling group pair.\"\"","comments":[{"timestamp":"1669984020.0","poster":"nsvijay04b1","upvote_count":"1","content":"***typo***\nwhy limitation? it says clearly a different ASG per deployment group.\n\"unique deployment group for each ALB-Auto Scaling group pair.\"","comment_id":"733711"}],"timestamp":"1669984020.0","poster":"nsvijay04b1"}],"timestamp":"1669874280.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/89512-exam-aws-devops-engineer-professional-topic-1-question-98/","exam_id":35,"unix_timestamp":1669874280,"question_images":[],"answer_images":[],"question_id":207,"topic":"1","answers_community":["C (75%)","13%","13%"]},{"id":"6a2UXH5dT6J3ZL2vyGAf","question_text":"A company's primary AWS Region contains the following infrastructure:\n\n• An Amazon S3 bucket that contains an object package that is used in instance user data to configure an application.\n• Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) with an instance profile that grants s3:Get* access on the S3 bucket.\n\nThe company has the following infrastructure in a backup Region:\n\n• An S3 bucket with the same configuration as the S3 bucket in the primary AWS Region, but without any objects.\n• EC2 instances in an Auto Scaling group behind an ALB that run with the same configuration as in the primary AWS Region.\n\nTo simulate a disaster recovery scenario, the company turns off all access to Amazon S3 and sets the Auto Scaling group's minimum, maximum, and desired instances to 0 in the primary Region. When the instances in the backup Region scale out, they do not pass Amazon Route 53 health checks.\n\nWhich combination of steps should the company take to resolve this issue? (Choose three.)","url":"https://www.examtopics.com/discussions/amazon/view/89786-exam-aws-devops-engineer-professional-topic-1-question-99/","question_images":[],"choices":{"A":"Update the Amazon EC2 Auto Scaling service-linked role to allow access to both S3 buckets.","E":"Update the EC2 instance profile to allow s3:list* actions.","F":"Update the EC2 instance profile to allow read access to both S3 buckets.","D":"Increase the timeout for the target group health check.","B":"Set up S3 Cross-Region Replication from the S3 bucket in the primary Region to the S3 bucket in the backup Region.","C":"Update the instance user data to reference the S3 bucket in the primary Region."},"discussion":[{"timestamp":"1670780700.0","comment_id":"741943","poster":"jaxsbr","content":"Selected Answer: BDF\nB - To ensure the secondary S3 contains the required files needed\nD - Provide enough time to download the S3 file and run user data\nF - Since it's a DR scenario and the primary S3 can't be read from we should read from secondary S3. Providing the instance profile read access to both allows it to be used seamlessly before and during the DR scenario","upvote_count":"17"},{"content":"Selected Answer: BCF\nChatgpt said: BCF. The three steps that the company should take to resolve this issue are:\n\nB. Set up S3 Cross-Region Replication from the S3 bucket in the primary Region to the S3 bucket in the backup Region. This will ensure that the objects in the primary Region's S3 bucket are automatically replicated to the backup Region's S3 bucket.\n\nC. Update the instance user data to reference the S3 bucket in the primary Region. Since the objects have been replicated to the backup Region, updating the instance user data to reference the S3 bucket in the primary Region will ensure that the instances in the backup Region are configured correctly.\n\nF. Update the EC2 instance profile to allow read access to both S3 buckets. This will ensure that the instances in the backup Region have the necessary permissions to read objects from both the primary and backup Regions' S3 buckets.\nThe other options do not address the issue","comment_id":"860030","timestamp":"1680532920.0","upvote_count":"1","poster":"merki"},{"content":"B - Set up S3 Cross-Region Replication from the S3 bucket in the primary Region to the S3 bucket in the backup Region to ensure the backup S3 bucket has the necessary objects needed by the instances to function properly.\n\nD - Increase the timeout for the target group health check to provide enough time for the instances to download the object package from S3 and run the user data.\n\nF - Update the EC2 instance profile to allow read access to both S3 buckets to ensure the EC2 instances have the necessary permissions to access the object package in the S3 bucket.","comment_id":"846299","timestamp":"1679429160.0","poster":"easytoo","upvote_count":"1"},{"poster":"DerekKey","comment_id":"805167","content":"Selected Answer: BDF\nBDF\nA - wrong - auto scaling will not work with S3, this is EC2 that needs to download the package\nC - wrong - S3 in the primary region will be unavailable\nE - wrong - you don't need to list the content of S3, you need to get from it","upvote_count":"2","timestamp":"1676115060.0"},{"comment_id":"803188","timestamp":"1675944540.0","content":"Selected Answer: ABF\nThe reason can be 1. The backup S3 bucket does not have objects. 2. The backup EC2 instances read the primary S3 bucket 3. Permission","poster":"Piccaso","upvote_count":"1"},{"upvote_count":"2","timestamp":"1674443160.0","content":"Selected Answer: BDF\nBDF for the reasons provided by jaxsbr","comment_id":"784903","poster":"Bulti"},{"poster":"Chaiyaporn","comment_id":"744249","timestamp":"1670947020.0","content":"BDF for me.","upvote_count":"4"},{"upvote_count":"3","content":"Selected Answer: BCE\nSet up S3 Cross-Region Replication from the S3 bucket in the primary Region to the S3 bucket in the backup Region. This will ensure that the S3 bucket in the backup Region contains the necessary object package needed to configure the application.\n\n\nUpdate the instance user data to reference the S3 bucket in the primary Region. This will ensure that the instances in the backup Region are configured to use the object package from the S3 bucket in the primary Region.\n\nOption D, increasing the timeout for the target group health check, may also be a helpful step to take, as it may give the instances in the backup Region more time to pass the health check. However, the other options are not necessary for resolving this issue.","poster":"SatenderRathee","comment_id":"741572","timestamp":"1670752140.0"},{"content":"Selected Answer: BDE\noptions need access to both s3 buckets ruled out on DR region you need access to that region bucket with CCR enabled from primary region bucket.\n\nAs we need 3rd option also to select, increasing timeout on R53 selected.","timestamp":"1669987440.0","poster":"nsvijay04b1","comment_id":"733749","upvote_count":"4"}],"exam_id":35,"unix_timestamp":1669987440,"answers_community":["BDF (70%)","13%","Other"],"timestamp":"2022-12-02 14:24:00","answer_description":"","answer":"BDF","answer_images":[],"question_id":208,"isMC":true,"answer_ET":"BDF","topic":"1"}],"exam":{"isMCOnly":false,"isBeta":false,"name":"AWS DevOps Engineer Professional","id":35,"isImplemented":true,"provider":"Amazon","numberOfQuestions":208,"lastUpdated":"11 Apr 2025"},"currentPage":42},"__N_SSP":true}