{"pageProps":{"questions":[{"id":"Qb8YcXWoVgIs56lEmus2","url":"https://www.examtopics.com/discussions/amazon/view/80855-exam-aws-certified-database-specialty-topic-1-question-239/","answer":"B","choices":{"A":"Use a proxy tier between the application and DynamoDB to regulate access to specific tables, items, and attributes.","C":"Use DynamoDB resource policies to regulate access to specific tables, items, and attributes.","B":"Use IAM policies with a combination of IAM conditions and actions to implement fine-grained access control.","D":"Configure an AWS Lambda function to extract only allowed attributes from tables based on user profiles."},"answer_description":"","question_text":"A startup company in the travel industry wants to create an application that includes a personal travel assistant to display information for nearby airports based on user location. The application will use Amazon DynamoDB and must be able to access and display attributes such as airline names, arrival times, and flight numbers. However, the application must not be able to access or display pilot names or passenger counts.\nWhich solution will meet these requirements MOST cost-effectively?","answers_community":["B (100%)"],"question_id":156,"answer_images":[],"timestamp":"2022-09-07 11:11:00","question_images":[],"isMC":true,"topic":"1","exam_id":22,"answer_ET":"B","unix_timestamp":1662541860,"discussion":[{"upvote_count":"8","poster":"mbar94","timestamp":"1662541860.0","comment_id":"662285","content":"Selected Answer: B\nIt's B - https://aws.amazon.com/blogs/aws/fine-grained-access-control-for-amazon-dynamodb/"},{"timestamp":"1663147080.0","comment_id":"668831","poster":"SonamDhingra","upvote_count":"2","content":"Selected Answer: B\nB is correct"}]},{"id":"8xL7tJa3sCIlV3Bpb2Yu","answer_images":[],"discussion":[{"comment_id":"137587","poster":"learnaws","upvote_count":"15","comments":[{"comment_id":"139732","upvote_count":"3","poster":"BillyC","content":"Yes, real time","timestamp":"1632699060.0"},{"poster":"BillyMadison","content":"Agree C, the link you posted nails it. Anytime the question want \"real time alerts or streams\", its almost always Kinesis streams. \n\"Database Activity Streams for Amazon Aurora with PostgreSQL compatibility provides a near real-time data stream of the database activity in your relational database to help you monitor activity. When integrated with third party database activity monitoring tools, Database Activity Streams can monitor and audit database activity to provide safeguards for your database and help meet compliance and regulatory requirements.\"","timestamp":"1632946080.0","comment_id":"139865","upvote_count":"4"}],"timestamp":"1632688860.0","content":"answer is C.\nhttps://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports-database-activity-streams/"},{"timestamp":"1687432440.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html","comment_id":"930418","upvote_count":"1","poster":"adelcold"},{"poster":"ken_test1234","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html","comment_id":"853078","timestamp":"1679997360.0","upvote_count":"1"},{"upvote_count":"1","poster":"SachinGoel","content":"Selected Answer: C\nAns - C","comment_id":"775331","timestamp":"1673697780.0"},{"content":"answer is C.\nhttps://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports-database-activity-streams/\nDatabase Activity Streams for Amazon Aurora with PostgreSQL compatibility provides a near real-time data stream of the database activity in your relational database to help you monitor activity. When integrated with third party database activity monitoring tools, Database Activity Streams can monitor and audit database activity to provide safeguards for your database and help meet compliance and regulatory requirements.","timestamp":"1658665320.0","comment_id":"636042","upvote_count":"1","poster":"Chirantan"},{"upvote_count":"1","content":"Database Activity Streams for Amazon Aurora with PostgreSQL compatibility provides a near real-time data stream of the database activity in your relational database to help you monitor activity. When integrated with third party database activity monitoring tools, Database Activity Streams can monitor and audit database activity to provide safeguards for your database and help meet compliance and regulatory requirements.","poster":"Chirantan","comment_id":"630528","timestamp":"1657629840.0"},{"content":"Selected Answer: C\nI chose C over B\nx B. Use AWS CloudTrail to audit the DB cluster and the Security team will get data from Amazon S3.\n- CloudTrail is automatically enabled for all accounts, and logs events, so this is good for past activities for breach\nAmazon Aurora activity is recorded in a CloudTrail event in Event history. You can use the CloudTrail console to view the last 90 days of recorded API activity and events in an AWS Region.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/logging-using-cloudtrail.html\n\nhttps://aws.amazon.com/cloudtrail/faqs/\n\nC is for future activities\nDatabase Activity Streams can monitor and audit database activity to provide real time safeguards for your database and help meet compliance and regulatory requirements.\nhttps://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports-database-activity-streams/\n\nDB->Database Activity Streams->Kinesis ->Security team\n\nC. Set up database activity streams and connect the data stream from Amazon Kinesis to consumer applications.","poster":"novice_expert","timestamp":"1651411500.0","comment_id":"595611","comments":[{"poster":"Mintwater","comment_id":"836601","content":"I like your explanation -- CloudTrail is for the historical data; Kinesis is for the future data.","timestamp":"1678577580.0","upvote_count":"1"}],"upvote_count":"2"},{"poster":"jove","content":"Real-time alerting and monitoring > Option C","timestamp":"1640377860.0","upvote_count":"1","comment_id":"508859"},{"upvote_count":"1","comment_id":"447591","timestamp":"1636045740.0","poster":"Scunningham99","content":"C https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html"},{"poster":"ChauPhan","content":"Only C meets the real-time, A, D is possible but schedule.","comment_id":"423678","timestamp":"1635695820.0","upvote_count":"1"},{"poster":"gelsm","content":"C. Set up database activity streams and connect the data stream from Amazon Kinesis to consumer applications.\n\nAurora Database activity streams provide a near real-time data stream of the database activity for an Aurora DB cluster. Database activity streams require the use of AWS KMS because the activity streams are always encrypted.","comment_id":"414349","timestamp":"1635388920.0","upvote_count":"1"},{"poster":"Dip11","content":"C for sure.","upvote_count":"1","timestamp":"1635300420.0","comment_id":"364615"},{"upvote_count":"2","poster":"LMax","timestamp":"1634762220.0","content":"Between A and C, but would go with C after reading this:\nhttps://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports-database-activity-streams/","comment_id":"314782"},{"upvote_count":"1","poster":"myutran","content":"Ans: C","comment_id":"297897","timestamp":"1634294040.0"},{"poster":"bigaws","comment_id":"260078","timestamp":"1634114460.0","content":"I agree with C. I think that Cloudtrail does not support the type of loging that would be required here, it is not the internal databsae info: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.LoggingAndMonitoring.html","upvote_count":"1"},{"upvote_count":"1","comment_id":"253132","timestamp":"1633917000.0","poster":"JobinAkaJoe","content":"Between A & C I will go with C as the logs should be used for real-time alerting and monitoring."},{"content":"C is the answer","upvote_count":"1","poster":"Ashoks","timestamp":"1633667280.0","comment_id":"212013"},{"content":"C is the answer","comment_id":"153494","poster":"Ebi","timestamp":"1633579920.0","upvote_count":"2"},{"content":"c is the correct answer","timestamp":"1633416900.0","comment_id":"152393","upvote_count":"2","poster":"halol"},{"poster":"BillyC","timestamp":"1632088080.0","comment_id":"135646","comments":[{"content":"It's using cloudwatch not cloudtrail, that's why not B\nhttps://aws.amazon.com/blogs/database/audit-amazon-aurora-database-logs-for-connections-query-patterns-and-more-using-amazon-athena-and-amazon-quicksight/","timestamp":"1633194120.0","poster":"szmulder","comment_id":"150712","upvote_count":"1"}],"content":"B... i Think","upvote_count":"1"}],"answer_description":"","answer_ET":"C","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/25814-exam-aws-certified-database-specialty-topic-1-question-24/","question_id":157,"question_text":"The Security team for a finance company was notified of an internal security breach that happened 3 weeks ago. A Database Specialist must start producing audit logs out of the production Amazon Aurora PostgreSQL cluster for the Security team to use for monitoring and alerting. The Security team is required to perform real-time alerting and monitoring outside the Aurora DB cluster and wants to have the cluster push encrypted files to the chosen solution.\nWhich approach will meet these requirements?","exam_id":22,"topic":"1","choices":{"A":"Use pg_audit to generate audit logs and send the logs to the Security team.","D":"Turn on verbose logging and set up a schedule for the logs to be dumped out for the Security team.","C":"Set up database activity streams and connect the data stream from Amazon Kinesis to consumer applications.","B":"Use AWS CloudTrail to audit the DB cluster and the Security team will get data from Amazon S3."},"isMC":true,"timestamp":"2020-07-15 13:07:00","answers_community":["C (100%)"],"answer":"C","unix_timestamp":1594811220},{"id":"LXlZ1n1GpWkYRpv58Sjc","isMC":true,"unix_timestamp":1662541980,"choices":{"C":"Leverage Amazon Aurora MySQL. Restore previous production DB instance snapshots into new test copies of Aurora MySQL DB clusters to allow them to make changes.","D":"Leverage Amazon RDS for MySQL. Use database cloning to create multiple developer copies of the production DB instance.","A":"Leverage Amazon RDS for MySQL with write-enabled replicas running on Amazon EC2. Create the test copies using a mysqidump backup from the RDS for MySQL DB instances and importing them into the new EC2 instances.","B":"Leverage Amazon Aurora MySQL. Use database cloning to create multiple test copies of the production DB clusters."},"answer_images":[],"question_text":"A large IT hardware manufacturing company wants to deploy a MySQL database solution in the AWS Cloud. The solution should quickly create copies of the company's production databases for test purposes. The solution must deploy the test databases in minutes, and the test data should match the latest production data as closely as possible. Developers must also be able to make changes in the test database and delete the instances afterward.\nWhich solution meets these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/80857-exam-aws-certified-database-specialty-topic-1-question-240/","question_images":[],"discussion":[{"comment_id":"1002022","timestamp":"1694139000.0","upvote_count":"1","content":"Selected Answer: B\nB. Amazon Aurora MySQL, database cloning","poster":"Pranava_GCP"},{"timestamp":"1694138940.0","upvote_count":"2","comment_id":"1002021","content":"B. Amazon Aurora MySQL, database cloning \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\n\n\"Aurora cloning is especially useful for quickly setting up test environments using your production data, without risking data corruption. You can use clones for many types of applications, such as the following:\nExperiment with potential changes (schema changes and parameter group changes, for example) to assess all impacts.\nRun workload-intensive operations, such as exporting data or running analytical queries on the clone.\nCreate a copy of your production DB cluster for development, testing, or other purposes.\"","poster":"Pranava_GCP"},{"upvote_count":"1","content":"Selected Answer: B\nB is correct","poster":"SonamDhingra","timestamp":"1663147080.0","comment_id":"668832"},{"upvote_count":"4","timestamp":"1662541980.0","comment_id":"662288","content":"Selected Answer: B\nQuick and easy deploy - Aurora with cloning feature. It's B.","poster":"mbar94"}],"question_id":158,"answer_description":"","exam_id":22,"answers_community":["B (100%)"],"answer_ET":"B","topic":"1","timestamp":"2022-09-07 11:13:00","answer":"B"},{"id":"2VO6jiVeqHmtk4gF10do","unix_timestamp":1662542160,"answer":"AD","answer_ET":"AD","question_id":159,"question_images":[],"answers_community":["AD (100%)"],"exam_id":22,"answer_description":"","question_text":"A company's application development team wants to share an automated snapshot of its Amazon RDS database with another team. The database is encrypted with a custom AWS Key Management Service (AWS KMS) key under the \"WeShare\" AWS account. The application development team needs to share the DB snapshot under the \"WeReceive\" AWS account.\nWhich combination of actions must the application development team take to meet these requirements? (Choose two.)","choices":{"D":"Make a copy of the DB snapshot, and set the encryption option to enable.","C":"Share the DB snapshot by setting the DB snapshot visibility option to public.","A":"Add access from the \"WeReceive\" account to the custom AWS KMS key policy of the sharing team.","E":"Share the DB snapshot by using the default AWS KMS encryption key.","B":"Make a copy of the DB snapshot, and set the encryption option to disable."},"discussion":[{"comment_id":"905567","upvote_count":"3","timestamp":"1684911120.0","poster":"aviathor","content":"Selected Answer: AD\nCannot be C because you cannot share an automated snapshot with another account\n\nhttps://repost.aws/knowledge-center/rds-snapshots-share-account\nYou can't share automated Amazon RDS snapshots with other AWS accounts. To share an automated snapshot, copy the snapshot to make a manual version, and then share that copy."},{"upvote_count":"1","content":"Selected Answer: AD\nAD without doubt","poster":"guau","timestamp":"1676135280.0","comment_id":"805440"},{"content":"Selected Answer: AD\nA & D is the best choice of answer.","comment_id":"732770","poster":"examineme","upvote_count":"1","timestamp":"1669908600.0"},{"comments":[{"comment_id":"708850","timestamp":"1667278560.0","poster":"rags1482","upvote_count":"2","content":"May be A & D is correct because we need to copy automated snapshot to share with other account"}],"comment_id":"708848","timestamp":"1667278380.0","upvote_count":"3","content":"Answer AC.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-snapshots-share-account/\n\nOpen the Amazon RDS console.\nChoose Snapshots from the left navigation pane.\nChoose the DB snapshot that you want to copy.\nChoose Actions, and then choose Share Snapshot.\nChoose the DB snapshot visibility:\nPublic allows all AWS accounts to restore a DB instance from your manual DB snapshot.\nPrivate allows only AWS accounts that you specify to restore a DB instance from your manual DB snapshot.\nIn the AWS Account ID field, enter the ID of the AWS account that you want to permit to restore a DB instance from your manual DB snapshot. Then, choose Add.","poster":"rags1482"},{"timestamp":"1662542160.0","upvote_count":"3","comment_id":"662291","comments":[{"poster":"RBSK","comment_id":"748724","timestamp":"1671357420.0","content":"URL mentioned above, in my opinion points to Ans A&E. Where do we have ref for D??","upvote_count":"1"}],"poster":"mbar94","content":"Selected Answer: AD\nIt's A&D. https://aws.amazon.com/premiumsupport/knowledge-center/rds-snapshots-share-account/"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/80858-exam-aws-certified-database-specialty-topic-1-question-241/","timestamp":"2022-09-07 11:16:00","topic":"1","answer_images":[]},{"id":"0cot1U9yGIQlxI7HMAhb","isMC":true,"exam_id":22,"url":"https://www.examtopics.com/discussions/amazon/view/80861-exam-aws-certified-database-specialty-topic-1-question-242/","answer_images":[],"topic":"1","answers_community":["BCE (76%)","ACE (24%)"],"answer":"BCE","choices":{"D":"For bulk inserts, use the parallel parameter in the COPY command to enable multi-threading.","C":"For bulk inserts, split input files on Amazon S3 into multiple files to match the number of slices on Amazon Redshift. Then use the COPY command to load data into Amazon Redshift.","B":"Stream real-time data into Redshift temporary tables before loading the data into permanent tables.","E":"Optimize analytics SQL queries to use sort keys.","A":"Modify the Kinesis Data Firehose delivery stream to stream the data to Amazon S3 with a high buffer size and to load the data into Amazon Redshift by using the COPY command.","F":"Avoid using temporary tables in analytics SQL queries."},"question_id":160,"answer_description":"","timestamp":"2022-09-07 11:19:00","question_images":[],"unix_timestamp":1662542340,"question_text":"A company is using Amazon Redshift as its data warehouse solution. The Redshift cluster handles the following types of workloads:\n✑ Real-time inserts through Amazon Kinesis Data Firehose\n✑ Bulk inserts through COPY commands from Amazon S3\n✑ Analytics through SQL queries\nRecently, the cluster has started to experience performance issues.\nWhich combination of actions should a database specialist take to improve the cluster's performance? (Choose three.)","answer_ET":"BCE","discussion":[{"comment_id":"713620","content":"Selected Answer: BCE\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/\n\nTip #6: Improving the efficiency of temporary tables\n\nTip #9: Maintaining efficient data loads\nAmazon Redshift best practices suggest using the COPY command to perform data loads of file-based data. \n\nTip #3: Sort key recommendation\nSorting a table on an appropriate sort key can accelerate query performance, especially queries with range-restricted predicates, by requiring fewer table blocks to be read from disk.\n\nHence BCE","timestamp":"1667898240.0","poster":"snehilsrcs","upvote_count":"7"},{"timestamp":"1705226940.0","content":"Selected Answer: ACE\nACE\nNo question about C and E\nFor A vs B... Using B may result in data loss. Temporary tables are better fit for storing temporary results.","comment_id":"1122447","poster":"MultiAZ","upvote_count":"1"},{"upvote_count":"1","timestamp":"1673210520.0","poster":"Sathish_dbs","content":"there is no mention how temp table is used here to conclude it optimises or not..","comment_id":"769822"},{"content":"Selected Answer: BCE\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/","comment_id":"695642","poster":"awsjjj","upvote_count":"2","timestamp":"1665861180.0"},{"comment_id":"692175","poster":"Changwha","timestamp":"1665497460.0","upvote_count":"4","content":"Selected Answer: BCE\nAx: Kinesis Data Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.\nB: The proper use of temporary tables can significantly improve performance of some ETL operations."},{"timestamp":"1664301240.0","poster":"JeanGat","comment_id":"681031","content":"C for sure - from Tip #9 here.\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/","upvote_count":"2"},{"timestamp":"1663302960.0","poster":"zoomac","upvote_count":"2","content":"A is wrong because \"Note that in circumstances where data delivery to the destination is falling behind data ingestion into the delivery stream, Kinesis Data Firehose raises the buffer size automatically to catch up and make sure that all data is delivered to the destination.\"\nReference: https://aws.amazon.com/kinesis/data-firehose/faqs/","comment_id":"670461"},{"comment_id":"668839","poster":"SonamDhingra","content":"Selected Answer: ACE\nACE is correct","timestamp":"1663148100.0","upvote_count":"2"},{"poster":"mbar94","comment_id":"662296","content":"Selected Answer: ACE\nIt's ACE.","upvote_count":"1","timestamp":"1662542340.0"}]}],"exam":{"numberOfQuestions":359,"provider":"Amazon","lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":false,"id":22,"name":"AWS Certified Database - Specialty","isBeta":false},"currentPage":32},"__N_SSP":true}