{"pageProps":{"questions":[{"id":"7widSO4XKKL2760NWfcO","answer_ET":"C","topic":"1","answer":"C","question_images":[],"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/48812-exam-aws-certified-database-specialty-topic-1-question-108/","isMC":true,"answer_images":[],"discussion":[{"content":"C. answer\n\nAurora can do cloning.","poster":"shantest1","comment_id":"326675","timestamp":"1633712640.0","upvote_count":"14"},{"comment_id":"594112","timestamp":"1651190520.0","poster":"novice_expert","upvote_count":"5","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\n\n Creating a clone is faster and more space-efficient than physically copying the data using other techniques, such as restoring a snapshot."},{"poster":"Pranava_GCP","upvote_count":"2","timestamp":"1694901300.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\n\nD. does not make sense, DMS doesn't use cloning.","comment_id":"1009338"},{"poster":"megadba","timestamp":"1650196020.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html","comment_id":"587199","upvote_count":"1"},{"poster":"Balaji_Cloud","upvote_count":"2","timestamp":"1642128300.0","content":"C - because Clone should kick-off after batch completes","comment_id":"523288"},{"upvote_count":"2","poster":"GMartinelli","comment_id":"501340","content":"Selected Answer: C\nOption C, Clone is easy and fast","timestamp":"1639484400.0"},{"comment_id":"379449","content":"CCCC - faster clones","timestamp":"1635755640.0","poster":"Suresh108","upvote_count":"1"},{"comment_id":"360642","content":"C final answer","poster":"Aesthet","upvote_count":"2","timestamp":"1635054360.0"},{"poster":"manan728","upvote_count":"1","timestamp":"1634582820.0","content":"C is correct.","comment_id":"342307"}],"answer_description":"","exam_id":22,"choices":{"D":"Set up a new daily AWS DMS task that will use cloning and change data capture (CDC) on the DB cluster to copy the data to a new DB cluster. Set up a time for the AWS DMS stream to stop when the new cluster is current.","A":"Use the AWS CLI to schedule a manual snapshot of the DB cluster. Restore the snapshot to a new DB cluster using the AWS CLI.","C":"Schedule a job to create a clone of the DB cluster at the end of the overnight batch process.","B":"Create a dump file from the DB cluster. Load the dump file into a new DB cluster."},"unix_timestamp":1617369900,"question_id":11,"question_text":"A company has a 20 TB production Amazon Aurora DB cluster. The company runs a large batch job overnight to load data into the Aurora DB cluster. To ensure the company's development team has the most up-to-date data for testing, a copy of the DB cluster must be available in the shortest possible time after the batch job completes.\nHow should this be accomplished?","timestamp":"2021-04-02 15:25:00"},{"id":"LTkZc9OqinJp3Z0No5ae","unix_timestamp":1617512280,"choices":{"B":"Configure the AWS DMS replication instance in the same account as Amazon Redshift and in the same Region as Amazon RDS.","D":"Configure the AWS DMS replication instance in the same account and Region as Amazon RDS.","A":"Configure the AWS DMS replication instance in the same account and Region as Amazon Redshift.","C":"Configure the AWS DMS replication instance in its own account and in the same Region as Amazon Redshift."},"answer_description":"","isMC":true,"answer_ET":"A","question_id":12,"url":"https://www.examtopics.com/discussions/amazon/view/49002-exam-aws-certified-database-specialty-topic-1-question-109/","answers_community":["A (100%)"],"question_text":"A company has two separate AWS accounts: one for the business unit and another for corporate analytics. The company wants to replicate the business unit data stored in Amazon RDS for MySQL in us-east-1 to its corporate analytics Amazon Redshift environment in us-west-1. The company wants to use AWS DMS with\nAmazon RDS as the source endpoint and Amazon Redshift as the target endpoint.\nWhich action will allow AVS DMS to perform the replication?","topic":"1","discussion":[{"timestamp":"1633269060.0","comment_id":"327750","upvote_count":"17","content":"Sorry I meant A. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html","poster":"Jaypdv"},{"timestamp":"1712421060.0","poster":"58a2d17","upvote_count":"1","comment_id":"1190502","content":"A. not C. Ref says it all : your Amazon Redshift cluster must be in the same account and same AWS Region as the replication instance"},{"poster":"Pranava_GCP","content":"Selected Answer: A\nA. Configure the AWS DMS replication instance in the same account and Region as Amazon Redshift.\n\n\"The Amazon Redshift cluster must be in the same AWS account and same AWS Region as the replication instance.\"\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html","comment_id":"1002855","timestamp":"1694228520.0","upvote_count":"1"},{"content":"Selected Answer: A\nuse target side","upvote_count":"2","comment_id":"921018","timestamp":"1686523800.0","poster":"Zdujgfr567783ff"},{"timestamp":"1671196260.0","upvote_count":"2","comment_id":"747204","poster":"khun","content":"Selected Answer: A\nAns A. \n\nThe Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance."},{"comment_id":"595668","poster":"novice_expert","upvote_count":"4","timestamp":"1651424280.0","content":"Selected Answer: A\nA. Configure the AWS DMS replication instance in the same account and Region as Target Database"},{"timestamp":"1643336460.0","poster":"peacegrace","comment_id":"534279","upvote_count":"2","content":"Selected Answer: A\nA is the answer. Refer : https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html"},{"content":"AAA - locked -","comment_id":"379451","poster":"Suresh108","upvote_count":"1","timestamp":"1636301040.0"},{"comment_id":"360643","timestamp":"1635433680.0","content":"A final answer\n\"The Amazon Redshift cluster must be in the same AWS account and same AWS Region as the replication instance.\"","upvote_count":"4","poster":"Aesthet"},{"comment_id":"342309","poster":"manan728","content":"A is right.","upvote_count":"1","timestamp":"1634922240.0"},{"content":"A. Answer","poster":"shantest1","comment_id":"331186","timestamp":"1633504980.0","upvote_count":"2"},{"comment_id":"327745","timestamp":"1632996360.0","content":"B. Answer\nRef https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html","poster":"Jaypdv","upvote_count":"2"}],"timestamp":"2021-04-04 06:58:00","answer":"A","exam_id":22,"answer_images":[],"question_images":[]},{"id":"Q3AluerqJRgaALVLnEn9","topic":"1","question_id":13,"unix_timestamp":1594155960,"answer_description":"","question_images":[],"answer_images":[],"answer":"B","answers_community":["B (58%)","A (27%)","D (15%)"],"exam_id":22,"question_text":"The Development team recently executed a database script containing several data definition language (DDL) and data manipulation language (DML) statements on an Amazon Aurora MySQL DB cluster. The release accidentally deleted thousands of rows from an important table and broke some application functionality.\nThis was discovered 4 hours after the release. Upon investigation, a Database Specialist tracked the issue to a DELETE command in the script with an incorrect\nWHERE clause filtering the wrong set of rows.\nThe Aurora DB cluster has Backtrack enabled with an 8-hour backtrack window. The Database Administrator also took a manual snapshot of the DB cluster before the release started. The database needs to be returned to the correct state as quickly as possible to resume full application functionality. Data loss must be minimal.\nHow can the Database Specialist accomplish this?","url":"https://www.examtopics.com/discussions/amazon/view/25052-exam-aws-certified-database-specialty-topic-1-question-11/","timestamp":"2020-07-07 23:06:00","isMC":true,"discussion":[{"poster":"edmondme","content":"D is right, you can create a clone with backtrack if the database was created with backtrack which it was in this case. It's either B (pitr) or D (backtrack). Backtrack is faster. A is wrong, because if you backtrack, you lose the data that users entered for the past 4 hours. you want to clone to another area and copy the data that was lost.","comments":[{"comment_id":"684820","upvote_count":"2","poster":"Jiang_aws1","content":"X A. May lose 4 hrs users entered\nX B. Take too long ( restored -> copy delete rows to Org-DB ) \nX C. Take too long & May lose 4 hrs users entered\nD. Better a. clone is faster than restore b. Copy delete rows to Org-DB","comments":[{"poster":"Jiang_aws1","upvote_count":"3","timestamp":"1664716200.0","content":"Only A, D using \"Rewind\" which is good\nD is correct if we can rewind \"clone with Backtrack enabled\" Anyone know ?","comments":[{"timestamp":"1692916380.0","comment_id":"989521","comments":[{"timestamp":"1693594620.0","upvote_count":"2","content":"Correct my answer, it should be B. D is wrong because \"you can't backtrack a database clone to a time before that database clone was created\" from the backtrack limitation here :https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","poster":"aqiao","comment_id":"996365"}],"poster":"aqiao","content":"Agree with you, D is the best option:https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","upvote_count":"1"}],"comment_id":"684824"}],"timestamp":"1664715960.0"},{"timestamp":"1684450320.0","upvote_count":"3","comment_id":"901544","content":"D its wrong, da backtrack only right after you create clone.","poster":"lelesp6"}],"upvote_count":"25","timestamp":"1633645440.0","comment_id":"216874"},{"timestamp":"1632392160.0","comment_id":"136938","content":"A&B correct.however, A cause loss data for last 4 hours but it's quickly will take just few minutes to rewind the databse to 4 hours before, \nhowever B will keep database and fix it by reinsert deleted records from database which created from restore point in time \nthis will take more time but without loss data as the question refer to\n\"Data loss must be minimal\"\nSo I guess B the correct answer","upvote_count":"24","poster":"halol","comments":[{"poster":"ChauPhan","upvote_count":"4","comments":[{"content":"Agree with ChauPhan. B is wrong. A is correct.","poster":"khchan123","timestamp":"1652932320.0","upvote_count":"1","comment_id":"603589","comments":[{"timestamp":"1663293840.0","comment_id":"670394","upvote_count":"3","poster":"swakan","content":"Option B suggest to restore the db to a point before the records were deleted, then copy the required records from this newly restored db (using PITR) to the original DB. So this way, all the other changes made on the DB are still there, while we copied the deleted rows from the PITR db. I think option B is correct. Please correct if wrong."}]},{"upvote_count":"1","poster":"Maze","comment_id":"695379","content":"PITR is restored another cluster with Product cluster. Executing PITR doesn't mean back-forward. we dont lose 4 hours. we can export and import from PITR restored cluster.","timestamp":"1665836520.0"}],"timestamp":"1635619080.0","content":"B is wrong because \"Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release\"\nI example: The release time was 5AM, and you found that the records are deleted at 9AM. So why we recover the database \"before 5AM\" (release time). ==> we still lose 4 hours.\nThe correct way to minimize data loss is: recover as nearest as possible time such as 8:55AM) then copy the deletion data. So only 5 mins data loss. Only this minimizes data loss.","comment_id":"422469"}]},{"timestamp":"1711373820.0","upvote_count":"1","content":"Selected Answer: B\nX D : You can't backtrack a database clone to a time before that database clone was created.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","comment_id":"1182515","poster":"missipssamarsh"},{"timestamp":"1710042840.0","poster":"Bezi","upvote_count":"2","content":"I will go for \"A\". Here's an excerpt from the below link:\nhttps://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/\n… After that regrettable moment when all seems lost, you simply pause your application, open up the Aurora Console, select the cluster, and click Backtrack DB cluster. Then you select Backtrack and choose the point in time just before your epic fail, and click Backtrack DB cluster.","comment_id":"1170042"},{"comment_id":"1120886","content":"Selected Answer: B\nThe answer is B. \nWith A you would lose all the data gathered in the last 4 hours","timestamp":"1705073640.0","poster":"MultiAZ","upvote_count":"1"},{"poster":"Hisayuki","upvote_count":"1","comment_id":"1110909","timestamp":"1704070260.0","content":"Selected Answer: A\nBackTrack - If you accidentally issue a Delete statement without a Where clause (that is, delete all items) or drop a table, you can use Backtrack to quickly revert to the previous state. You can do the same thing with PITR, but it takes time to restore from an existing backup to a different cluster/instance.","comments":[{"timestamp":"1704070440.0","content":"Sorry, I noticed the window is 8 hours. We have to use PITR not to lose the data of the 4 hours. The answer is B.","comment_id":"1110910","poster":"Hisayuki","upvote_count":"1"}]},{"poster":"Jrhp","upvote_count":"1","comment_id":"1088123","content":"Selected Answer: A\nBacktracking \"rewinds\" the DB cluster to the time you specify. Backtracking is not a replacement for backing up your DB cluster so that you can restore it to a point in time. However, backtracking provides the following advantages over traditional backup and restore:\n\nYou can easily undo mistakes. If you mistakenly perform a destructive action, such as a DELETE without a WHERE clause, you can backtrack the DB cluster to a time before the destructive action with minimal interruption of service\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html#AuroraMySQL.Managing.Backtrack.Overview","timestamp":"1701739320.0"},{"poster":"AmbrishK","comment_id":"1064860","timestamp":"1699366140.0","upvote_count":"1","content":"Selected Answer: D\nThis option combines the benefits of both Backtrack and data recovery by cloning the cluster and using Backtrack to recover lost data. After that, you can copy the missing rows back into the original database. It allows for minimal data loss.\nBased on the goal of minimizing data loss and ensuring a quick recovery, option D seems like the best choice. It leverages Backtrack, provides a way to recover the deleted data, and then copy it back into the original database. However, option B is also a valid choice if Backtrack is not an option or if the deleted data is outside the backtrack window."},{"poster":"Germaneli","content":"Selected Answer: A\nA is correct, because backtrack is the fastest way to \"[return] the database to the correct state as quickly as possible\", as the questions asks.\nThere is no mention or requirement of retaining potentially lost user input.\nAdditionally, restoring the deleted rows while keeping actual user input from the last 4 hours after the loss might infringe foreign key constraints within the database, making the data inconsistent.","upvote_count":"2","comment_id":"1003084","timestamp":"1694254140.0"},{"comment_id":"996367","content":"Selected Answer: B\nA x: It will cause 4 hours data loss after release\nB yes\nC x: It will cause 4 hours data loss after release and involve additional change\nD x: You can't backtrack a database clone to a time before that database clone was created from the backtrack limitation here :https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","poster":"aqiao","timestamp":"1693594860.0","upvote_count":"2"},{"content":"Selected Answer: A\nYou can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","comment_id":"951694","upvote_count":"1","timestamp":"1689354540.0","poster":"SamDDD"},{"comment_id":"942596","timestamp":"1688465280.0","poster":"mraronsimon","content":"Selected Answer: B\nA & C (incorrect) - dataloss!\nD (incorrect) - \"You can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created.\"\nThe correct answer is B. \n\nHowever, D could be better with a little change: \nYou should use the clone db as new production and you should rewind on the original. Copy deleted rows from original to the clone.","comments":[{"timestamp":"1688465340.0","upvote_count":"1","comment_id":"942597","poster":"mraronsimon","content":"Reference: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"}],"upvote_count":"2"},{"upvote_count":"1","poster":"MrAliMohsan","comment_id":"926722","content":"Selected Answer: B\nOption D would have been correct if it has suggested to rewind the DB cluster not the cloned cluster. Since you cannot rewind the cloned cluster before the time it was created.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html#:~:text=You%20can%27t%20backtrack%20a%20database%20clone","timestamp":"1687095540.0"},{"timestamp":"1685518020.0","upvote_count":"3","content":"Selected Answer: B\nxA. Rewinding the database means restoring the deleted data, but also undoing all other changes to the database => data loss\nB. Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release and copy the deleted rows from the restored database to the original database. => This is the best course of action.\nxC. This also entails losing all of the changes made after the deletion event.\nxD. This does not work because you cannot rewind a clone to a time before the clone was created.","comment_id":"910964","poster":"aviathor"},{"poster":"backbencher2022","content":"D is right - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","timestamp":"1679102820.0","comment_id":"842408","upvote_count":"1"},{"upvote_count":"1","content":"https://aws.amazon.com/getting-started/hands-on/aurora-cloning-backtracking/ <--- D is the answer","poster":"sk1974","timestamp":"1678620180.0","comment_id":"836939"},{"upvote_count":"1","timestamp":"1678472280.0","poster":"ixdb","content":"It's D.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\nBy using Aurora cloning, you can create a new cluster that initially shares the same data pages as the original, but is a separate and independent volume. The process is designed to be fast and cost-effective. The new cluster with its associated data volume is known as a clone. Creating a clone is faster and more space-efficient than physically copying the data using other techniques, such as restoring a snapshot.","comment_id":"835296"},{"timestamp":"1675698060.0","poster":"im_not_robot","comment_id":"799913","upvote_count":"4","content":"B is incorrect because during 4 hours after release, who knows any updates on table that has rows deleted so we will lose data integrity if we re-insert deleted data.\nC is slower than A so C is incorrect.\nD is not feasible option because we can't do backtrack on the clone\n--> A is the best choice among 4 options."},{"upvote_count":"1","timestamp":"1671969240.0","poster":"Sathish_dbs","comment_id":"755607","content":"A is correct, you don't lose 4 hours data with as you can do both back and forth with backtrack. D is so wrong as clones won't backtrack after read this - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"},{"content":"Answer is B. A clone cannot be backtracked to a point before it is created. \nYou can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created.","upvote_count":"4","timestamp":"1668732720.0","poster":"Sab","comment_id":"720987"},{"content":"B\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-pitr.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","comment_id":"701321","poster":"rags1482","timestamp":"1666412520.0","upvote_count":"2"},{"comment_id":"698923","content":"B seems to be the correct answer because they said \"Data loss must be minimal\" \n\nA. Quickly rewind the DB cluster to a point in time before the release using Backtrack.\nthis is the fastest option but we will lose 4 hours of data because we discover issue after 4 hours from running script during this period we have used the app and new data added\n\nB. Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release and copy the deleted rows from the restored database to the original database. Most Voted\nwe will restore to a new instance and copy data but anyone knows how to copy data between to clusters\n\nC. Restore the DB cluster using the manual backup snapshot created before the release and change the application configuration settings to point to the new DB cluster.\nthis will cause losing 4 hours of data as option A\n\nD. Create a clone of the DB cluster with Backtrack enabled. Rewind the cloned cluster to a point in time before the release. Copy deleted rows from the clone to the original database.\nyou can not use backtrack a cloned cluster to a time before its cloning","poster":"sayed","timestamp":"1666175340.0","upvote_count":"1","comments":[{"timestamp":"1681690140.0","poster":"Mintwater","upvote_count":"1","content":"I agree with B.\nI think the B sentence didn’t completed.\nIt says “ copy the deleted rows from the restored db to the original db “ — means there are two db: one is the db which is still running but with the db ( row truncated , another is the restored db ( from the snapshot) to a pointed in time . Then to copy the deleted row , insert back to the running original db.\nD is incorrect because what’s the point of time to make the clone db? You can not backtrack to a point of time before the row deleted because you create this clone db is AFTER the row deleted.","comment_id":"872241"}]},{"content":"Selected Answer: B\nVery tricky question. \"Data loss must be minimal\" is a key word here. Hence B","timestamp":"1665927240.0","poster":"awsjjj","comment_id":"696262","upvote_count":"1"},{"comment_id":"692040","timestamp":"1665487740.0","upvote_count":"1","poster":"awsjjj","content":"the answer is A. its in the documentation . https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html"},{"timestamp":"1664715000.0","comment_id":"684813","content":"A is correct\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\n\nYou can easily undo mistakes. If you mistakenly perform a destructive action, such as a DELETE\nBacktracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes.","poster":"Jiang_aws1","upvote_count":"1"},{"content":"Selected Answer: B\nB. You can NOT clone and THEN Backtrack to some time prior to the clone. IF we could use the source backtrack in the clone, THEN D would be correct and faster. But alas, this is not an option.\n\nhttps://aws.plainenglish.io/aurora-database-clones-what-they-are-and-when-to-use-them-b82be9d60309\n\n\"Aurora clones also support backtracking but we can backtrack an Aurora clone only up to the point when that clone was actually created from the original database and not any further (in case the original database supports backtracking up to any point further in the past).\"","timestamp":"1664544420.0","comment_id":"683583","upvote_count":"3","poster":"JeanGat"},{"content":"Selected Answer: B\nI think it's b.\nA -> will result in Data loss.\nD -> can't rewind a cloned cluster to a time before the clone was created\naccording to the comments here.","poster":"venimus_vidimus_vicimus","comment_id":"677427","timestamp":"1663964340.0","comments":[{"poster":"Jiang_aws1","content":"D. Create a clone : create \"New\" clone then rewind","upvote_count":"1","comment_id":"684826","timestamp":"1664716260.0"}],"upvote_count":"1"},{"timestamp":"1663293900.0","comment_id":"670396","upvote_count":"1","content":"Selected Answer: B\nOption B suggest to restore the db to a point before the records were deleted, then copy the required records from this newly restored db (using PITR) to the original DB. So this way, all the other changes made on the DB are still there, while we copied the deleted rows from the PITR db. I think option B is correct","poster":"swakan"},{"content":"Selected Answer: B\n\"Option A\" will result in loss of several hours of data and would not be recommended. A specialist should create a new DB instance using a backup taken at the correct recoverable point. Isn't it difficult to specify an exact time for \"D\"? So the answer would be B with point-in-time recovery.","upvote_count":"1","comment_id":"652449","poster":"kyo","timestamp":"1661567580.0"},{"timestamp":"1656355320.0","content":"Selected Answer: D\nRead carefully the question says ... just performed a database script ... ...problem was caused by a DELETE command in the script that had an improper WHERE... so the deletion happened a few seconds ago, and the release occurred four hours ago.\n\nBUT OPTION A SAYS -> Quickly rewind the DB cluster to a point in time BEFORE RELEASE using Backtrack. it was 4 hours ago, for me this is unacceptable data loss.\n\nI will go with D:","comments":[{"poster":"DevoteamAnalytix","timestamp":"1656505260.0","upvote_count":"3","comment_id":"624630","content":"I agree with D: \n=> Original stays like this\n=> Rewind clone to find the deleted data\n=> Add diffs to original DB"}],"upvote_count":"4","comment_id":"623445","poster":"rlnd2000"},{"poster":"ryuhei","upvote_count":"1","timestamp":"1655904480.0","content":"Selected Answer: B\nThe answer is B.\nIf it is A, the data for 4 hours after the release will be lost.","comment_id":"620446"},{"poster":"Radhaghosh","content":"Selected Answer: A\nOnly coping deleted rows may create data consistency issue. I will got with A","upvote_count":"1","comment_id":"604973","timestamp":"1653155580.0"},{"upvote_count":"1","timestamp":"1651327740.0","poster":"novice_expert","comment_id":"595066","content":"Selected Answer: B\nPITR table \n-> The point-in-time recovery process always restores to a new table.\n-> copy the deleted rows from the restored database to the original database."},{"comment_id":"557600","poster":"user0001","timestamp":"1645993440.0","content":"A is the best here option since it will make the database online \nD is wrong because you cant revert back to before the clone \n\nYou can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created. For more information about database cloning,","upvote_count":"2","comments":[{"upvote_count":"1","content":"the other 2 options will take time and you will still lose data","poster":"user0001","comment_id":"557601","timestamp":"1645993500.0"}]},{"content":"Answer should be B, rewinding the DB using backtrack will cause loss data for 4 hours across all tables. B should be correct, after restored have to copy the data only from actual \nrequired tables.","poster":"Sudeepshiv","timestamp":"1645929300.0","upvote_count":"2","comment_id":"557104"},{"content":"Selected Answer: A\nOption A","poster":"GMartinelli","upvote_count":"3","timestamp":"1638463560.0","comment_id":"492684"},{"comment_id":"479222","upvote_count":"2","timestamp":"1637047380.0","poster":"toppic26","comments":[{"content":"This link is not a good example. It illustrate to backtrack the database to a point before something is created.\n\nI disagree with D\nW\ne will be cloning the database in its current state, that is without those wrongly deleted rows. How are we able to backtrack it when these data doesn't exists in the clone?","upvote_count":"1","timestamp":"1641891780.0","comment_id":"521409","poster":"awsmonster"},{"poster":"whn","timestamp":"1649341800.0","comment_id":"582498","upvote_count":"2","content":"D is incorrect.\n\"You can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created. For more information about database cloning, see Cloning a volume for an Aurora DB cluster.\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","comments":[{"upvote_count":"1","comment_id":"684850","content":"You can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created. For more information about database cloning","poster":"Jiang_aws1","timestamp":"1664718660.0"}]}],"content":"D is correct. Step by step guide from aws: https://aws.amazon.com/getting-started/hands-on/aurora-cloning-backtracking/"},{"upvote_count":"1","content":"Bonus Question:- \nwhat will be the answer?\nQUESTION NO: _49\nA database specialist must load 25 GB of data files from a company’s on-premises storage to an Amazon Neptune database.\nWhich approach to load the data is FASTEST?\nA. Upload the data to Amazon S3 and use the Loader command to load the data from Amazon S3 into the Neptune database.\nB. Write a utility to read the data from the on-premises storage and run INSERT statements in a loop to load the data into the Neptune database.\nC. Use the AWS CLI to load the data directly from the on-premises storage into the Neptune database.\nD. Use AWS DataSync to load the data directly from the on-premises storage into the Neptune database.","timestamp":"1636223220.0","comment_id":"440372","comments":[{"timestamp":"1638213480.0","content":"Question no #49 - Answer - D.","comment_id":"490154","upvote_count":"1","poster":"lihze"},{"comment_id":"506623","content":"The correct answer is A. You cannot use AWS DataSync to load data into a Neptune db","poster":"jove","timestamp":"1640142120.0","comments":[{"timestamp":"1652932380.0","upvote_count":"1","poster":"khchan123","content":"Agree it's A. AWS DataSync is not possible with Neptune.","comment_id":"603590"}],"upvote_count":"2"}],"poster":"guru_ji"},{"comment_id":"440371","content":"Bonus Question:- \nwhat will be the answer?\nQUESTION NO: _48\nA company hosts an internal file-sharing application running on Amazon EC2 instances in VPC_A. This application is backed by an Amazon ElastiCache cluster, which is in VPC_B and peered with VPC_A. The company migrates its application instances from VPC_A to VPC_B. Logs indicate that the file-sharing application no longer can connect to the ElastiCache cluster.\nWhat should a database specialist do to resolve this issue?\nA. Create a second security group on the EC2 instances. Add an outbound rule to allow traffic from the ElastiCache cluster security group.\nB. Delete the ElastiCache security group. Add an interface VPC endpoint to enable the EC2 instances to connect to the ElastiCache cluster.\nC. Modify the ElastiCache security group by adding outbound rules that allow traffic to VPC_B’s CIDR blocks from the ElastiCache cluster.\nD. Modify the ElastiCache security group by adding an inbound rule that allows traffic from the EC2 instances’ security group to the ElastiCache cluster.","upvote_count":"1","comments":[{"comment_id":"506674","timestamp":"1640145600.0","poster":"jove","upvote_count":"2","content":"Correct answer is D"}],"poster":"guru_ji","timestamp":"1636214880.0"},{"timestamp":"1636011420.0","comment_id":"440368","comments":[{"comment_id":"506681","timestamp":"1640146080.0","poster":"jove","content":"Minimal replication lag = Amazon Aurora global database.. Option B","upvote_count":"1"}],"upvote_count":"1","content":"Bonus Question:- \nwhat will be the answer?\nQUESTION NO: _47\nA financial company recently launched a portfolio management solution. The backend of the application is powered by Amazon Aurora with MySQL compatibility. The company requires an RTO of 5 minutes and an RPO of 5 minutes. A database specialist must configure an efficient disaster recovery solution with minimal replication lag.\nWhich approach should the database specialist take to meet these requirements?\nA. Configure AWS Database Migration Service (AWS DMS) and create a replica in a different AWS Region.\nB. Configure an Amazon Aurora global database and add a different AWS Region.\nC. Configure a binlog and create a replica in a different AWS Region.\nD. Configure a cross-Region read replica.","poster":"guru_ji"},{"content":"Bonus Question:\n\n A software development company is using Amazon Aurora MySQL DB clusters for several use cases, including development and reporting. These use cases place unpredictable and varying demands on the Aurora DB clusters, and can cause momentary spikes in latency. System users run ad-hoc queries sporadically throughout the week. Cost is a primary concern for the company, and a solution that does not require significant rework is needed.\nWhich solution meets these requirements?\n\n A. Create new Aurora Serverless DB clusters for development and reporting, then migrate to these new DB clusters.\n B. Upgrade one of the DB clusters to a larger size, and consolidate development and reporting activities on this larger DB cluster.\n C. Use existing DB clusters and stop/start the databases on a routine basis using scheduling tools.\n D. Change the DB clusters to the burstable instance family.","comments":[{"upvote_count":"1","comment_id":"506689","content":"Here is the discussion on this question :\nhttps://www.examtopics.com/discussions/amazon/view/48684-exam-aws-certified-database-specialty-topic-1-question-93/","poster":"jove","timestamp":"1640146620.0"}],"comment_id":"439644","upvote_count":"1","timestamp":"1635992520.0","poster":"guru_ji"},{"poster":"guru_ji","content":"B ==>> Correct Answer\n\nYou can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created.","timestamp":"1635939420.0","comment_id":"437442","upvote_count":"1"},{"upvote_count":"1","content":"D is correct, CLone is faster and it is talking about rows.","comment_id":"434496","poster":"aws4myself","timestamp":"1635761400.0"},{"poster":"guru_ji","comment_id":"425854","comments":[{"content":"B is correct.\n\nYou can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created.","upvote_count":"1","poster":"guru_ji","comment_id":"437440","timestamp":"1635803940.0"}],"upvote_count":"1","content":"D ==>> Correct Answer 100%","timestamp":"1635625020.0"},{"poster":"ChauPhan","comments":[{"timestamp":"1635547080.0","comment_id":"422468","upvote_count":"1","poster":"ChauPhan","content":"Correct on above not 10:55 but 8:55AM (nearest 9AM)"}],"timestamp":"1635501660.0","comment_id":"422467","upvote_count":"1","content":"For those who chose B: B is wrong because \"Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release\"\nI example: The release time was 5AM, and you found that the records are deleted at 9AM. So why we recover the database \"before 5AM\" (release time). ==> we still lose 4 hours.\nThe correct way to minimize data loss is: recover as nearest as possible time such as 10:55AM) then copy the deletion data. So only 5 mins data loss. Only this minimizes data loss."},{"comment_id":"422187","upvote_count":"2","timestamp":"1635004140.0","poster":"ChauPhan","comments":[{"timestamp":"1635012300.0","poster":"ChauPhan","comment_id":"422188","upvote_count":"2","content":"Look at the question: \"The database needs to be returned to the correct state as quickly as possible to resume full application functionality\""},{"upvote_count":"2","poster":"ChauPhan","comment_id":"422189","content":"B is wrong because \"Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release\" => no meaning because you still lose 4 hours data. To minimize data lost, uou have to recover to NEAREST data point and copy the deleted rows.","timestamp":"1635083160.0"}],"content":"A is correct.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\n\"You can backtrack a DB cluster quickly. Restoring a DB cluster to a point in time launches a new DB cluster and restores it from backup data or a DB cluster snapshot, which can take hours. Backtracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes\"\nB and C will take hours to recover."},{"comment_id":"412667","upvote_count":"1","poster":"KevinNaik","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\n\nd:\n\nBacktracking is only available for DB clusters that were created with the Backtrack feature enabled. You can enable the Backtrack feature when you create a new DB cluster or restore a snapshot of a DB cluster. For DB clusters that were created with the Backtrack feature enabled, you can create a clone DB cluster with the Backtrack feature enabled. Currently, you can't perform backtracking on DB clusters that were created with the Backtrack feature disabled.","timestamp":"1634978520.0"},{"upvote_count":"2","timestamp":"1634837820.0","comment_id":"411925","poster":"gelsm","content":"Answer D:\n\"One of the major advantages of backtracking is that it can rewind the DB cluster much faster compared to restoring a DB cluster via point in time restore (PITR) or via a manual DB cluster snapshot, which can take hours. Backtracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes.\""},{"comment_id":"373887","poster":"Suresh108","upvote_count":"1","content":"AAAA\n\nExact word to word in the link.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","timestamp":"1634827020.0"},{"timestamp":"1634799720.0","poster":"db_interest","comment_id":"359971","content":"A. Since the question also states \"resume full application functionality\" which is only possible if the DB is taken to a point before the release (since the release caused app functionality to break). Considering multiple requirements of minimal data loss and taking app back to full functionality - Backtracking is the fastest way with some data loss.","upvote_count":"2"},{"upvote_count":"1","poster":"frankzeng","comment_id":"341594","content":"A, but the answer is not full. It should be backtrack to the release time, copy the deleted rows then, backtrack forward to the current time. I have tested and it worked.","timestamp":"1634775060.0"},{"timestamp":"1634649660.0","poster":"frankzeng","upvote_count":"2","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\nYou can explore earlier data changes. You can repeatedly backtrack a DB cluster back and forth in time to help determine when a particular data change occurred. For example, you can backtrack a DB cluster three hours and then backtrack forward in time one hour. In this case, the backtrack time is two hours before the original time.\nBacktrack the database 4 hours, copy the deleted rows, then backtrack forward 4 hours to the current status, insert the deleted rows.","comment_id":"341335"},{"content":"Answer A would lead to a data loss.\nAnswer D is not correct as you can't backtrack to point in time before cloning.\nHence, Answer B is the answer I would choose.","comment_id":"314735","upvote_count":"2","poster":"LMax","timestamp":"1634645400.0"},{"comments":[{"content":"For DB clusters that were created with the Backtrack feature enabled, you can create a clone DB cluster with the Backtrack feature enabled","timestamp":"1634527920.0","poster":"jyrajan","upvote_count":"1","comment_id":"304497"}],"comment_id":"301760","upvote_count":"2","timestamp":"1634512500.0","poster":"jyrajan","content":"The error was discovered 4 hours after the launch, which puts within the 8 hour BackTrack window, so the fastest would be to use backtrack to a point before the launch. The Snapshot will work, but that will take time to restore the snapshot. There is nothing in this question that indicates this is a cloned database"},{"upvote_count":"2","comments":[{"content":"For DB clusters that were created with the Backtrack feature enabled, you can create a clone DB cluster with the Backtrack feature enabled","comments":[{"content":"You can create a clone DB cluster with the Backtrack feature enabled but you can't backtrack a database clone to a time before that database clone was created.","timestamp":"1634558760.0","upvote_count":"1","comment_id":"305504","poster":"myutran"}],"poster":"jyrajan","comment_id":"304496","upvote_count":"2","timestamp":"1634513220.0"}],"content":"I think ans is B.\n\nYou can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","poster":"myutran","timestamp":"1634235300.0","comment_id":"296378"},{"comment_id":"272574","upvote_count":"1","comments":[{"content":"B is wrong because \"Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release\" => no meaning because you still lose 4 hours data. To minimize data lost, you have to recover to NEAREST data point and copy the deleted rows.","poster":"ChauPhan","upvote_count":"1","comment_id":"422191","timestamp":"1635408420.0"}],"content":"There are 2 requirements here, almost conflicting:\n- The database needs to be returned to the correct state as quickly as possible to resume full application functionality. \n- Data loss must be minimal\nPlease note that due to the delete the application lost SOME functionality, so it is till working and generating data. So\n- A is THE FASTEST, but you lose 4 hours of data. Also note that backtrack will work on your current cluster, so the last 4 hours' worth of data (in potentially dozens, hundreds tables) is lost indeed\n- B will take some time, but will restore the database in another instance. Then you can insert the deleted data (already identified during the investigation) without ANY data loss\nSo in real life I would always take B","poster":"MultiAZ","timestamp":"1634040300.0"},{"upvote_count":"2","timestamp":"1633990800.0","poster":"bigaws","content":"If you create a clone with backtrack enabled, you wont be able to go back past the point where the clone was created (i think. Need to check this)","comment_id":"259459"},{"poster":"bigaws","comment_id":"259457","timestamp":"1633907760.0","content":"This question seems badly worded but I think that it must be B. The answer indicated that the recovery is to a new instance which would be the best solution as you restore a backup and take the required data to restore to the main db.","upvote_count":"1"},{"content":"B seems to be best option here.\nRows are deleted accidently from a single table.\nand we want to reduce the data-loss by the recovery procedure.\nBest approach would be to restore the snapshot to a new instance, copy the records from the recovered database to original database for the table which had undergone logical corruption","comment_id":"252813","timestamp":"1633820280.0","upvote_count":"1","poster":"JobinAkaJoe"},{"comment_id":"212832","timestamp":"1632819720.0","poster":"edmondme","comments":[{"comment_id":"212835","content":"Plus if you do A you do lose the data from prod that was entered for the 4 hours since.","upvote_count":"1","timestamp":"1633068180.0","poster":"edmondme"}],"content":"If you create the Aurora DB with backtrack, and create the clone with backtrack, D is certainly possible. I think D is correct.","upvote_count":"2"},{"content":"A is for me.. simple, easy to go forward or backward closer to point of issue.","comment_id":"211593","upvote_count":"1","timestamp":"1632813840.0","poster":"Ashoks"},{"content":"A for sure","comment_id":"205100","poster":"pdboi3355","timestamp":"1632801480.0","upvote_count":"1"},{"timestamp":"1632737700.0","content":"B is correct answer.\nA although is the fastest but it causes 4 hours of data loss","comment_id":"153364","upvote_count":"7","poster":"Ebi"},{"timestamp":"1632507300.0","comment_id":"153148","content":"In this scenario A is best option. The backtrack is enabled with 8 hours.","upvote_count":"2","poster":"firbhat"},{"comment_id":"136000","timestamp":"1632371520.0","upvote_count":"2","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\n\n• You can't backtrack a database clone to a time before that database clone was created. However, you can use the original database to backtrack to a time before the clone was created. For more information about database cloning, see Cloning Databases in an Aurora DB Cluster.\n• https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html","poster":"chicagomassageseeker"},{"poster":"BillyC","comment_id":"134169","timestamp":"1632352080.0","upvote_count":"2","content":"A i think... im not complety sure..."},{"poster":"chicagomassageseeker","timestamp":"1632221040.0","comments":[{"comment_id":"139005","upvote_count":"5","timestamp":"1632414720.0","content":"Going with A as well.\n\"You can backtrack a DB cluster quickly. Restoring a DB cluster to a point in time launches a new DB cluster and restores it from backup data or a DB cluster snapshot, which can take hours. Backtracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes.\"\n\"You can easily undo mistakes. If you mistakenly perform a destructive action, such as a DELETE without a WHERE clause, you can backtrack the DB cluster to a time before the destructive action with minimal interruption of service.\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","poster":"BillyMadison"}],"content":"Answer A. D is not possible. You cant backtrack a cloned database to a point before the time cloned time. B and C take several hours.","upvote_count":"8","comment_id":"129279"}],"answer_ET":"B","choices":{"D":"Create a clone of the DB cluster with Backtrack enabled. Rewind the cloned cluster to a point in time before the release. Copy deleted rows from the clone to the original database.","A":"Quickly rewind the DB cluster to a point in time before the release using Backtrack.","C":"Restore the DB cluster using the manual backup snapshot created before the release and change the application configuration settings to point to the new DB cluster.","B":"Perform a point-in-time recovery (PITR) of the DB cluster to a time before the release and copy the deleted rows from the restored database to the original database."}},{"id":"m4vt5bCrWdqehqjLWDXW","discussion":[{"comment_id":"327746","timestamp":"1634835660.0","poster":"Jaypdv","content":"C.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html","upvote_count":"13"},{"timestamp":"1694229480.0","upvote_count":"2","poster":"Pranava_GCP","content":"Selected Answer: C\nC. Create an Aurora Global Database.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html","comment_id":"1002861"},{"comments":[{"upvote_count":"1","comment_id":"1091458","content":"How about performance impact?","timestamp":"1702097340.0","poster":"jitesh_k"}],"timestamp":"1678002780.0","comment_id":"829717","content":"Selected Answer: A\nEnable synchronous replication: Synchronous replication ensures that data is written to both the primary and standby DB clusters before the transaction is committed. This means that the secondary cluster is always in sync with the primary, with minimal lag time, and can be promoted to primary in case of a failure. This approach meets the RPO and RTO requirements and ensures there is no data loss or downtime.","upvote_count":"2","poster":"ninjalight25"},{"content":"Selected Answer: C\nC. Create an Aurora Global Database.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html\n\nManaged planned failover RPO = 0 sec, RTO = minutes","upvote_count":"4","poster":"novice_expert","timestamp":"1651367520.0","comment_id":"595379"},{"poster":"AriraAWS","comment_id":"539865","content":"Selected Answer: C\nC is the right answer.","upvote_count":"1","timestamp":"1643910060.0"},{"timestamp":"1635856200.0","comment_id":"360644","poster":"Aesthet","upvote_count":"3","content":"C final answer"},{"comments":[{"comments":[{"content":"So if we want to have it in the same region, synchronous replication would be a better solution?","comments":[{"comment_id":"603792","timestamp":"1652956020.0","poster":"khchan123","upvote_count":"1","content":"No. You can use multi-AZ read replica for single region DR."}],"comment_id":"543063","timestamp":"1644327060.0","poster":"Hariru","upvote_count":"1"}],"comment_id":"326678","content":"Sync replication not available cross region If I am not wrong.","timestamp":"1633095360.0","upvote_count":"2","poster":"shantest1"}],"comment_id":"326676","poster":"shantest1","upvote_count":"2","content":"C. I believe","timestamp":"1632190140.0"}],"timestamp":"2021-04-02 15:27:00","question_images":[],"question_id":14,"answers_community":["C (78%)","A (22%)"],"answer":"C","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/48813-exam-aws-certified-database-specialty-topic-1-question-110/","isMC":true,"unix_timestamp":1617370020,"choices":{"B":"Enable asynchronous binlog replication.","C":"Create an Aurora Global Database.","D":"Copy Aurora incremental snapshots to the us-east-1 Region.","A":"Enable synchronous replication."},"question_text":"A database specialist is managing an application in the us-west-1 Region and wants to set up disaster recovery in the us-east-1 Region. The Amazon Aurora\nMySQL DB cluster needs an RPO of 1 minute and an RTO of 2 minutes.\nWhich approach meets these requirements with no negative performance impact?","exam_id":22,"topic":"1","answer_description":"","answer_images":[]},{"id":"1zbahFIiHlIaadw6jtsT","exam_id":22,"answer_ET":"A","topic":"1","discussion":[{"timestamp":"1632365160.0","comment_id":"326680","content":"A. answer","poster":"shantest1","upvote_count":"11"},{"upvote_count":"1","poster":"chikorita","content":"right infront of my salad Amazon Cognito exists","timestamp":"1696853700.0","comment_id":"1028901"},{"timestamp":"1694230020.0","content":"Selected Answer: A\nA. Use web identity federation on the mobile app and AWS STS with an attached IAM role to get temporary credentials to access DynamoDB.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html\n\n\"If you are writing an application targeted at large numbers of users, you can optionally use web identity federation for authentication and authorization. Web identity federation removes the need for creating individual users. Instead, users can sign in to an identity provider and then obtain temporary security credentials from AWS Security Token Service (AWS STS). The app can then use these credentials to access AWS services.\n\nWeb identity federation supports the following identity providers:\nLogin with Amazon\nFacebook\nGoogle\"","upvote_count":"1","comment_id":"1002867","poster":"Pranava_GCP"},{"timestamp":"1671196560.0","comment_id":"747209","content":"Selected Answer: A\nA, Web identity Fed > STS > IAM > tempo creds","upvote_count":"1","poster":"khun"},{"poster":"elf78","upvote_count":"2","timestamp":"1655619060.0","comment_id":"618544","content":"Selected Answer: A\nIAM Role is always preferred method."},{"upvote_count":"1","comment_id":"594590","content":"Selected Answer: A\nUse web identity federation on the mobile app and AWS STS with an attached IAM role to get temporary credentials to access DynamoDB.","timestamp":"1651249740.0","poster":"novice_expert"},{"content":"Selected Answer: A\nobvious","timestamp":"1645727280.0","comment_id":"555479","upvote_count":"1","poster":"tugboat"},{"poster":"Suresh108","timestamp":"1636033020.0","upvote_count":"2","content":"AAAAAAAAA","comment_id":"379454"},{"content":"A final answer","upvote_count":"2","poster":"Aesthet","timestamp":"1635928620.0","comment_id":"360645"},{"comment_id":"344245","poster":"agrawalachin","upvote_count":"1","timestamp":"1633974720.0","content":"Answer is A - least operational"}],"url":"https://www.examtopics.com/discussions/amazon/view/48814-exam-aws-certified-database-specialty-topic-1-question-111/","answer_description":"","choices":{"C":"Use a self-developed user management system on the mobile app that lets users access the data from DynamoDB through an API.","B":"Use web identity federation on the mobile app and create individual IAM users with credentials to access DynamoDB.","D":"Use a single IAM user on the mobile app to access DynamoDB.","A":"Use web identity federation on the mobile app and AWS STS with an attached IAM role to get temporary credentials to access DynamoDB."},"question_id":15,"answer_images":[],"question_text":"A gaming company is developing a new mobile game and decides to store the data for each user in Amazon DynamoDB. To make the registration process as easy as possible, users can log in with their existing Facebook or Amazon accounts. The company expects more than 10,000 users.\nHow should a database specialist implement access control with the LEAST operational effort?","question_images":[],"unix_timestamp":1617370140,"answers_community":["A (100%)"],"isMC":true,"timestamp":"2021-04-02 15:29:00","answer":"A"}],"exam":{"provider":"Amazon","isMCOnly":false,"isImplemented":true,"numberOfQuestions":359,"id":22,"isBeta":false,"name":"AWS Certified Database - Specialty","lastUpdated":"11 Apr 2025"},"currentPage":3},"__N_SSP":true}