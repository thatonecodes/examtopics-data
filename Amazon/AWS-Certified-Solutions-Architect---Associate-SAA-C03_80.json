{"pageProps":{"questions":[{"id":"eU3YqSXOuj2KQVGows25","discussion":[{"comments":[{"upvote_count":"11","comment_id":"1028241","content":"Where's your Shield Advanced now, in your hour of need he has abandoned you","poster":"hydro143","timestamp":"1712603340.0"}],"comment_id":"928333","timestamp":"1703074080.0","upvote_count":"11","content":"Selected Answer: B\nAs no shield protect here so WAF rate limit","poster":"samehpalass"},{"upvote_count":"8","comment_id":"1110577","content":"Selected Answer: B\nBest solution Shield Advanced, not listed here, thus second-best solution, WAF with rate limiting","poster":"pentium75","timestamp":"1719746340.0"},{"poster":"awsgeek75","comment_id":"1115866","timestamp":"1720351860.0","content":"Selected Answer: B\nA. Amazon Inspector = Software vulnerabilities like OS patches etc. Not fit for purpose.\nC. Changing IP from DDoS so don't know the incoming traffic for configuration (even if it was possible)\nD. GardDuty is for workload and AWS account monitoring so it can't help with DDoS.\n\nB is correct as AWS WAF + ALB can configure rate limiting even if source IP changes.","upvote_count":"6"},{"poster":"jAtlas7","timestamp":"1719565140.0","comment_id":"1107685","content":"Selected Answer: B\naccording to some google searches... to protect against DDOS attack:\n* AWS WAF(Web Application Firewall) provides protection on the application layer (I think Application Load Balancer belongs to this level)\n * AWS Shield protects the infrastructure layers of the OSI mode (I think AWS Network Load Balancer belongs to this level)","upvote_count":"3"},{"upvote_count":"1","content":"Selected Answer: A\nThis case is A","comment_id":"990063","poster":"Guru4Cloud","timestamp":"1708871700.0","comments":[{"upvote_count":"1","timestamp":"1737295920.0","poster":"vincent2023","content":"Inspector wouldn't help in this case as it's used to detect vulnerabilities.","comment_id":"1343026"},{"upvote_count":"3","content":"Inspector is for detecting vulnerabilities, has nothing to do with the requirement.","timestamp":"1719746280.0","comment_id":"1110574","poster":"pentium75"}]},{"poster":"james2033","upvote_count":"3","content":"Selected Answer: B\nAWS Web Application Firewall (WAF) + ALB (Application Load Balancer) See image at https://aws.amazon.com/waf/ . https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html . \n\nQuestion keyword \"high request rate\", answer keyword \"rate-limiting rule\" https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-example-limit-login-page-keys.html\n\nAmazon GuardDuty for theat detection https://aws.amazon.com/guardduty/ , not for DDoS.","comment_id":"958553","timestamp":"1705851120.0"},{"upvote_count":"2","timestamp":"1702532040.0","poster":"TariqKipkemei","content":"Selected Answer: B\nB in swahili 'ba' :)\nexternal systems, incoming requests = AWS WAF","comment_id":"922723"},{"upvote_count":"2","poster":"Axeashes","timestamp":"1702525800.0","content":"Selected Answer: B\nlayer 7 DDoS protection with WAF\nhttps://docs.aws.amazon.com/waf/latest/developerguide/ddos-get-started-web-acl-rbr.html","comment_id":"922665"},{"timestamp":"1701768000.0","comment_id":"915158","upvote_count":"1","content":"Selected Answer: B\nB no doubt.","poster":"antropaws"},{"poster":"Joselucho38","comment_id":"905168","upvote_count":"5","content":"Selected Answer: B\nAWS WAF (Web Application Firewall) is a service that provides protection for web applications against common web exploits. By associating AWS WAF with the Application Load Balancer (ALB), you can inspect incoming traffic and define rules to allow or block requests based on various criteria.","timestamp":"1700773740.0"},{"timestamp":"1700314080.0","content":"B\nAWS Web Application Firewall (WAF) is a service that helps protect web applications from common web exploits and provides advanced security features. By deploying AWS WAF and associating it with the ALB, the company can set up rules to filter and block incoming requests based on specific criteria, such as IP addresses.\n\nIn this scenario, the company is facing performance issues due to a high request rate from illegitimate external systems with changing IP addresses. By configuring a rate-limiting rule in AWS WAF, the company can restrict the number of requests coming from each IP address, preventing excessive traffic from overwhelming the website. This will help mitigate the impact of potential DDoS attacks and ensure that legitimate users can access the site without interruption.","poster":"cloudenthusiast","upvote_count":"5","comment_id":"901129"},{"timestamp":"1700196240.0","content":"Selected Answer: B\nIf not AWS Shield, then WAF","upvote_count":"4","poster":"Efren","comment_id":"899678"},{"comments":[{"comment_id":"899677","timestamp":"1700196180.0","upvote_count":"2","poster":"Efren","content":"My mind slipped with AWS Shield. GuardDuty can be working along with WAF for DDOS attack, but ultimately would be WAF\n\nhttps://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/","comments":[{"upvote_count":"1","timestamp":"1705043820.0","content":"Same here, I was looking for AWS Shield","comment_id":"949492","poster":"Mia2009687"}]}],"poster":"nosense","content":"Selected Answer: B\nB obv for this","timestamp":"1700142240.0","upvote_count":"4","comment_id":"899130"},{"comments":[],"timestamp":"1700131500.0","comment_id":"899007","poster":"Efren","content":"Selected Answer: D\nD, Guard Duty for me","upvote_count":"1"}],"question_images":[],"unix_timestamp":1684226700,"answers_community":["B (96%)","2%"],"answer_description":"","timestamp":"2023-05-16 10:45:00","url":"https://www.examtopics.com/discussions/amazon/view/109378-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"answer":"B","isMC":true,"choices":{"C":"Deploy rules to the network ACLs associated with the ALB to block the incomingtraffic.","B":"Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.","D":"Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty.","A":"Deploy Amazon Inspector and associate it with the ALB."},"exam_id":31,"question_text":"A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users.\n\nWhat should a solutions architect recommend?","question_id":396,"answer_ET":"B","topic":"1"},{"id":"O3vQ93wOKeqxZ2Ax5Sm1","timestamp":"2023-05-16 13:51:00","answers_community":["D (100%)"],"answer_description":"","answer_ET":"D","question_text":"A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database.\n\nWhat is the MOST secure way for the company to share the database with the auditor?","answer_images":[],"isMC":true,"unix_timestamp":1684237860,"topic":"1","choices":{"A":"Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.","C":"Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.","D":"Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key.","B":"Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant the user access to the S3 bucket."},"exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/109398-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"upvote_count":"22","content":"Selected Answer: D\nThe most secure way for the company to share the database with the auditor is option D: Create an encrypted snapshot of the database, share the snapshot with the auditor, and allow access to the AWS Key Management Service (AWS KMS) encryption key.\n\nBy creating an encrypted snapshot, the company ensures that the database data is protected at rest. Sharing the encrypted snapshot with the auditor allows them to have their own copy of the database securely.\n\nIn addition, granting access to the AWS KMS encryption key ensures that the auditor has the necessary permissions to decrypt and access the encrypted snapshot. This allows the auditor to restore the snapshot and access the data securely.\n\nThis approach provides both data protection and access control, ensuring that the database is securely shared with the auditor while maintaining the confidentiality and integrity of the data.","timestamp":"1685888520.0","comments":[{"poster":"TariqKipkemei","content":"best explanation ever","upvote_count":"4","comment_id":"922724","timestamp":"1686713820.0"}],"comment_id":"914732","poster":"alexandercamachop"},{"timestamp":"1692966420.0","content":"Selected Answer: D\nKey word: \"Secure way\"\nThe snapshot contents are encrypted using KMS keys for data security.\nSharing the snapshot directly removes risks of extracting/transferring data.\nThe auditor can restore the snapshot into their own RDS instance.\nAccess is controlled through sharing the encrypted snapshot and KMS key.","comment_id":"990055","upvote_count":"5","poster":"Guru4Cloud"},{"content":"why not A ?","poster":"24b2e9e","comment_id":"1233534","comments":[{"poster":"LeonSauveterre","timestamp":"1733219160.0","upvote_count":"1","content":"I think it's simply because it introduces more risks than sharing just a snapshot. External entities don't need that many privileges, so it's not the MOST secure way","comment_id":"1321322"}],"upvote_count":"5","timestamp":"1718872620.0"},{"timestamp":"1718516760.0","content":"Selected Answer: D\nEncrypted snapshot must be most secure compare others","poster":"KennethNg923","comment_id":"1231216","upvote_count":"2"},{"upvote_count":"3","timestamp":"1704635520.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html\n\nWith AWS RDS, you can share snapshots across accounts so no need to go through S3 or replication. Option D allows more secure way by using encryption and sharing encryption key.","poster":"awsgeek75","comment_id":"1115881"},{"content":"Selected Answer: D\nMOST secure way","poster":"potomac","upvote_count":"3","timestamp":"1699208160.0","comment_id":"1063169"},{"timestamp":"1685949720.0","content":"Selected Answer: D\nMost likely D.","comment_id":"915163","poster":"antropaws","upvote_count":"4"},{"poster":"cloudenthusiast","content":"Option D (Creating an encrypted snapshot of the database, sharing the snapshot, and allowing access to the AWS Key Management Service encryption key) is generally considered a better option for sharing the database with the auditor in terms of security and control.","timestamp":"1684411500.0","comment_id":"901183","upvote_count":"3"},{"upvote_count":"3","poster":"nosense","content":"Selected Answer: D\nD for me","comment_id":"899146","timestamp":"1684237860.0"}],"question_images":[],"answer":"D","question_id":397},{"id":"nrXXV50VfXOujJyvW6nf","topic":"1","exam_id":31,"answer_ET":"A","question_text":"A solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads.\n\nWhich solution resolves this issue with the LEAST operational overhead?","discussion":[{"upvote_count":"8","content":"Selected Answer: A\nA is correct: You assign a single CIDR IP address range as the primary CIDR block when you create a VPC and can add up to four secondary CIDR blocks after creation of the VPC.","timestamp":"1685950020.0","poster":"antropaws","comment_id":"915171"},{"comment_id":"990046","timestamp":"1692965700.0","poster":"Guru4Cloud","upvote_count":"5","content":"Selected Answer: A\nthe architect just needs to:\n\nAdd the CIDR using the AWS console or CLI\nCreate new subnets in the VPC using the new CIDR\nLaunch resources in the new subnets"},{"comment_id":"1359536","content":"Selected Answer: A\nA - is the correct answer","upvote_count":"1","poster":"Tendu","timestamp":"1740095460.0"},{"timestamp":"1733519820.0","poster":"rosanna","upvote_count":"1","comment_id":"1322920","content":"Selected Answer: A\nThe CIDR block of a VPC can NOT be changed but can be extended.\nThus, for the LEAST operational overhead; extend the existing one."},{"poster":"f2e2419","content":"Selected Answer: A\nbest option","comment_id":"1119374","upvote_count":"3","timestamp":"1704950160.0"},{"content":"Selected Answer: A\nA: LEAST operational overhead is by creating a new CIDR block in existing VPC.\nAll other options require additional overhead of gateway or second VPC","timestamp":"1704635700.0","comment_id":"1115883","poster":"awsgeek75","upvote_count":"4"},{"content":"Selected Answer: A\nAfter you've created your VPC, you can associate additional IPv4 CIDR blocks with the VPC","comment_id":"1063171","upvote_count":"4","timestamp":"1699208340.0","poster":"potomac"},{"poster":"TariqKipkemei","upvote_count":"2","timestamp":"1686714060.0","comment_id":"922727","content":"Selected Answer: A\nA is best"},{"timestamp":"1684592400.0","content":"Selected Answer: A\nAdd additional CIDR of bigger range","upvote_count":"3","poster":"Yadav_Sanjay","comment_id":"902623"},{"timestamp":"1684291740.0","content":"Selected Answer: A\nAdd new bigger subnets","upvote_count":"3","comment_id":"899688","poster":"Efren"},{"content":"Selected Answer: A\nA valid","poster":"nosense","upvote_count":"2","timestamp":"1684237920.0","comment_id":"899148"}],"answer":"A","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/109400-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"answers_community":["A (100%)"],"choices":{"B":"Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of the second VPC.","C":"Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPUpdate the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.","D":"Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the traffic through the VPN. Create new resources in the subnets of the second VPC.","A":"Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR."},"question_id":398,"question_images":[],"answer_description":"","unix_timestamp":1684237920,"timestamp":"2023-05-16 13:52:00"},{"id":"IrCFnMn6yiR7QKJvPbzj","timestamp":"2022-10-08 18:06:00","answer_description":"","question_id":399,"answer_images":[],"question_text":"A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)","isMC":true,"question_images":[],"answer":"AB","topic":"1","unix_timestamp":1665245160,"answers_community":["AB (96%)","1%"],"choices":{"D":"Enable default encryption on the S3 bucket.","A":"Enable versioning on the S3 bucket.","B":"Enable MFA Delete on the S3 bucket.","C":"Create a bucket policy on the S3 bucket.","E":"Create a lifecycle policy for the objects in the S3 bucket."},"discussion":[{"upvote_count":"66","content":"Selected Answer: AB\nThe correct solution is AB, as you can see here:\n\nhttps://aws.amazon.com/it/premiumsupport/knowledge-center/s3-audit-deleted-missing-objects/\n\nIt states the following:\n\nTo prevent or mitigate future accidental deletions, consider the following features:\n\nEnable versioning to keep historical versions of an object.\nEnable Cross-Region Replication of objects.\nEnable MFA delete to require multi-factor authentication (MFA) when deleting an object version.","comments":[{"poster":"liams123","comment_id":"1245862","timestamp":"1720670700.0","content":"but it could be C you could use 's3:deleteobject\" permission without specific conditions or restricts only to authorized users. B does the same thing tho but B is mainly used to restrict unauthroized access not deletion. does anyone agree. I think it is A and C or A & B","upvote_count":"6"}],"timestamp":"1665388140.0","poster":"Uhrien","comment_id":"690838"},{"poster":"cookieMr","comment_id":"929221","content":"Selected Answer: AB\nEnabling versioning on S3 ensures multiple versions of object are stored in bucket. When object is updated or deleted, new version is created, preserving previous version.\n\nEnabling MFA Delete adds additional layer of protection by requiring MFA device to be present when attempting to delete objects. This helps prevent accidental or unauthorized deletions by requiring extra level of authentication.\n\nC. Creating a bucket policy on S3 is more focused on defining access control and permissions for bucket and its objects, rather than protecting against accidental deletion.\n\nD. Enabling default encryption on S3 ensures that any new objects uploaded to bucket are automatically encrypted. While encryption is important for data security, it does not directly address accidental deletion.\n\nE. Creating lifecycle policy for objects in S3 allows for automated management of objects based on predefined rules. While this can help with data retention and storage cost optimization, it does not directly protect against accidental deletion.","comments":[],"upvote_count":"13","timestamp":"1687336860.0"},{"upvote_count":"1","timestamp":"1737963780.0","comment_id":"1347283","content":"Selected Answer: AB\nc: won't be able to delete anyhting\nd: doesn't change anything\ne: baseless","poster":"Mrigraj12"},{"upvote_count":"1","content":"Selected Answer: AB\nAB is the correct solution here as we need MFA delete for making the deletion process inevitable and also we need S3 bucket versioning for MFA delete.","poster":"satyaammm","timestamp":"1735668240.0","comment_id":"1334964"},{"upvote_count":"1","timestamp":"1735287240.0","comment_id":"1332306","poster":"MGKYAING","content":"Selected Answer: AB\nTo protect critical data in an S3 bucket from accidental deletion, these two features provide robust safeguards:\n\nA. Enable versioning on the S3 bucket:\nVersioning allows the bucket to maintain multiple versions of an object. Even if an object is accidentally deleted, the previous version can be restored, ensuring data recovery.\n\nB. Enable MFA Delete on the S3 bucket:\nMFA Delete adds an additional layer of security by requiring multi-factor authentication for deletion operations. This prevents accidental or unauthorized deletions."},{"content":"Selected Answer: AB\nTo protect critical data in an Amazon S3 bucket from accidental deletion, a solutions architect should take the following steps:\n\nEnable versioning on the S3 bucket: This allows you to recover objects that are accidentally deleted or overwritten by keeping multiple versions of an object.\n\nEnable MFA Delete on the S3 bucket: This adds an extra layer of security by requiring multi-factor authentication (MFA) for delete operations, which helps prevent accidental or unauthorized deletions","poster":"Chr1s_Mrg","comment_id":"1292009","upvote_count":"2","timestamp":"1727795520.0"},{"poster":"PaulGa","upvote_count":"2","content":"Selected Answer: BD\nAns A,B - as per 'kwabsAA' 2 months ago\n\"To protect data from accidental deletion, the correct answers are B and D. Versioning does not prevent accidental deletion; it only allows for recovery after the fact. Multi-Factor Authentication (MFA) helps prevent accidental deletion by requiring an additional confirmation step before deletion, making it deliberate rather than accidental. Option D, which involves encryption, ensures that only individuals with the encryption keys can read or manipulate the data, thus preventing unauthorized access and manipulation, including deletion.\"","comment_id":"1282039","timestamp":"1726047180.0"},{"content":"Selected Answer: AB\nencryption will not prevent accidental deletions","timestamp":"1725821940.0","upvote_count":"2","comment_id":"1280558","poster":"zied007"},{"poster":"kwabsAA","content":"To protect data from accidental deletion, the correct answers are B and D. Versioning does not prevent accidental deletion; it only allows for recovery after the fact. Multi-Factor Authentication (MFA) helps prevent accidental deletion by requiring an additional confirmation step before deletion, making it deliberate rather than accidental. Option D, which involves encryption, ensures that only individuals with the encryption keys can read or manipulate the data, thus preventing unauthorized access and manipulation, including deletion.","comment_id":"1245125","comments":[{"upvote_count":"2","content":"you do realised B and D are only to stop unauthroized people from deleting it and if they did delete it would be on purpose to cause issues for a business. it says accidental id say A and C. idk the questions worded poorly dont trust half the answers on here","poster":"liams123","timestamp":"1720671240.0","comment_id":"1245873"},{"poster":"liams123","upvote_count":"2","comment_id":"1245875","content":"but it could be C you could use 's3:deleteobject\" permission without specific conditions or restricts only to authorized users. B does the same thing tho but B is mainly used to restrict unauthroized access not deletion. does anyone agree. I think it is A and C or A & B. A allows multiple versions of objects to be stored in the bucket. Even if an object is deleted, its previous versions remain intact and accessible. idk the questions weird i could see how it could be all of them except D & E","timestamp":"1720671360.0"}],"timestamp":"1720556880.0","upvote_count":"3"},{"comment_id":"1245117","content":"BD. For D, When you encrypt data, an unauthorized user (without the encryption key) cannot manipulate the data (ie. decryption, modifying, deletion).","upvote_count":"2","poster":"kwabsAA","timestamp":"1720556220.0"},{"poster":"mmrakib","content":"Selected Answer: AB\nAB will be the correct answer.","upvote_count":"3","timestamp":"1709627760.0","comment_id":"1166306"},{"upvote_count":"2","poster":"sidharthwader","comment_id":"1159762","comments":[{"poster":"liams123","content":"but it could be C you could use 's3:deleteobject\" permission without specific conditions or restricts only to authorized users. B does the same thing tho but B is mainly used to restrict unauthroized access not deletion. does anyone agree. I think it is A and C or A & B","timestamp":"1720670760.0","comment_id":"1245865","upvote_count":"1"}],"timestamp":"1708953900.0","content":"This could be done if we enable MFA delete on the bucket but in order to enable this bucket versioning must be done. Hence A and B would be the answer."},{"comment_id":"1149637","comments":[{"comment_id":"1198563","upvote_count":"1","poster":"KRC96","timestamp":"1713525600.0","content":"chatgpt will help you only if you gave correct prompt.","comments":[{"timestamp":"1720670880.0","upvote_count":"1","comment_id":"1245866","content":"yea chatgpt said A and C it does make sense. Cause B is mainly used for unauthroized access not deletion. idk this website and certlibrary give some interesting answers makes it hard to know but some questions are difficult","poster":"liams123"}]}],"poster":"Conster","content":"I am getting so confused about what answers I should study. The answers don't match here or in ChatGPT. Can anyone who just took the exam, and passed, point me in the right direction? TIA!","timestamp":"1707862920.0","upvote_count":"2"},{"timestamp":"1705232040.0","content":"Selected Answer: AB\nB: MFA to put an extra step to verify deletion and stop from accidental deletion\nA: Versioning for recovery of objects that were deleted accidentally even with MFA\n\nRemember, the solution is not required to STOP from deletion. It just wants to STOP ACCIDENTAL deletion.\n\nCDE offer nothing related to accidental deletion","comment_id":"1122489","poster":"awsgeek75","upvote_count":"3"},{"poster":"rt_7777","upvote_count":"1","content":"Not sure why Answer is BD. I am trying to rationalize it. What I guess could be to address keyword \"critical data\" where set default encryption is just enhance the security of stored data but does not prevent from deletion. This will be have 2 options A, B for that. B is make sense to ensure user know what to delete on second layer. For option A, it just help you to audit and recovered what was accidentally deleted but does not \"prevent\" accidentally delete.","comment_id":"1105687","timestamp":"1703562900.0"},{"poster":"fb4afde","upvote_count":"2","content":"Selected Answer: AB\nAgree, s3 encryption does not prevent deletion","timestamp":"1702343700.0","comment_id":"1093979"},{"comment_id":"1090578","timestamp":"1701980220.0","poster":"jjcode","content":"Yeah so.. encryption is enabled by default on S3, sooooo why is the answer D.\n\n---------\n\nStarting today, Amazon Simple Storage Service (Amazon S3) encrypts all new objects by default. Now, S3 automatically applies server-side encryption (SSE-S3) for each new object, unless you specify a different encryption option.","upvote_count":"2"},{"timestamp":"1701247200.0","content":"What's the correct answers？","comment_id":"1083284","poster":"Leo1688","upvote_count":"1","comments":[{"timestamp":"1720671120.0","comment_id":"1245872","poster":"liams123","upvote_count":"1","content":"I would say A & C"}]},{"poster":"[Removed]","upvote_count":"2","comment_id":"1075742","content":"Prevent accidental deletion - MFA, Versioning","timestamp":"1700505960.0"},{"timestamp":"1699734420.0","poster":"Marco_St","comment_id":"1068091","content":"Selected Answer: AB\nMFA will add extra security of deleting item from s3\nVersioning will make the data recovering","upvote_count":"2"},{"upvote_count":"2","timestamp":"1699031040.0","comment_id":"1061617","poster":"JustEugen","content":"Selected Answer: AB\nA) https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\nVersioning-enabled buckets can help you recover objects from accidental deletion or overwrite. For example, if you delete an object, Amazon S3 inserts a delete marker instead of removing the object permanently. The delete marker becomes the current object version. If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version\n\nB) https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html"},{"poster":"xdkonorek2","comments":[{"upvote_count":"1","poster":"liams123","timestamp":"1720671000.0","content":"thought the same thing i think the questions is pre hard especially answers i went with AC","comment_id":"1245868"},{"comment_id":"1105044","content":"You're asked to prevent ACCIDENTAL deletion, not deletion.","comments":[{"comment_id":"1245871","upvote_count":"1","timestamp":"1720671060.0","poster":"liams123","content":"but B is literally just meant for unauthorized action think about it you said ACCIDENTAL. unauthorized people would purposely delete it"}],"upvote_count":"3","timestamp":"1703484180.0","poster":"pentium75"}],"timestamp":"1698679320.0","content":"Selected Answer: AC\nA - object must be versioned, so multiple uploads won't cause data loss\nC - even though objects are version you have to specify deny policy for delete actions on bucket level to ensure they can't be deleted\n\nB - MFA helps with authentication, doesn't protect if user has permission to delete","comment_id":"1057964","upvote_count":"2"},{"content":"https://aws.amazon.com/it/premiumsupport/knowledge-center/s3-audit-deleted-missing-objects/","comment_id":"1055053","timestamp":"1698372360.0","upvote_count":"1","poster":"Ruffyit"},{"content":"Selected Answer: AB\nThe two most effective steps a solutions architect can take to protect an Amazon S3 bucket from accidental deletion are:\n\nA. Enable versioning on the S3 bucket.\nB. Enable MFA Delete on the S3 bucket.\n\nVersioning keeps multiple versions of objects in the S3 bucket, even when they are overwritten or deleted. This allows you to recover objects that have been accidentally deleted.\n\nMFA Delete requires you to enter a one-time password from a multi-factor authentication (MFA) device before you can delete an object in the S3 bucket. This helps to prevent accidental deletions.","comment_id":"1048277","timestamp":"1697764380.0","upvote_count":"1","poster":"AWSStudyBuddy"},{"upvote_count":"1","timestamp":"1697091240.0","comment_id":"1041421","content":"A+B is the correct answer","poster":"kagitala"},{"comment_id":"1039511","poster":"Tralfalgarlaw","timestamp":"1696942080.0","content":"Selected Answer: AB\nKeyword: accidental deletions","upvote_count":"1"},{"comment_id":"1028712","poster":"tom_cruise","comments":[],"content":"Selected Answer: A\nD has nothing to do with deletion, not sure why it is even shown as correct answer?","upvote_count":"1","timestamp":"1696845180.0"},{"comment_id":"1023451","poster":"awashenko","content":"Selected Answer: AB\nAgree A and B. D doesn't do anything for deletion. E helps more with deleting objects. C is more about access control than deletion.","timestamp":"1696280700.0","upvote_count":"1"},{"content":"The correct solution is AB, as you can see here:\n\nhttps://aws.amazon.com/it/premiumsupport/knowledge-center/s3-audit-deleted-missing-objects/\n\nIt states the following:\n\nTo prevent or mitigate future accidental deletions, consider the following features:\n\nEnable versioning to keep historical versions of an object.\nEnable Cross-Region Replication of objects.\nEnable MFA delete to require multi-factor authentication (MFA) when deleting an object version.","comment_id":"1021995","timestamp":"1696133280.0","upvote_count":"1","poster":"paniya93"},{"comment_id":"976044","poster":"Guru4Cloud","timestamp":"1691528580.0","upvote_count":"2","content":"Selected Answer: AB\nEnable versioning on the S3 bucket. This will create a history of all object versions in the bucket, including deleted objects. This way, even if an object is deleted, it can be restored from a previous version.\nEnable MFA Delete on the S3 bucket. This will require users to enter their MFA token in addition to their password in order to delete objects from the bucket. This adds an extra layer of protection against accidental deletion."},{"content":"Selected Answer: AB\nEnable versioning to ensure restore is possible, Enable two step verification of file deletion using MFA delete to ensure unwanted persons are unable to perform this action.","timestamp":"1691123640.0","poster":"TariqKipkemei","upvote_count":"1","comment_id":"971682"},{"content":"AB is the correct answer.\nAdmin please don't make us fail the exam😂","poster":"Bill__","timestamp":"1690948920.0","upvote_count":"4","comment_id":"969637"},{"upvote_count":"2","poster":"miki111","timestamp":"1689552480.0","comment_id":"953731","content":"Option AB is the right answer."},{"upvote_count":"1","content":"Selected Answer: AB\nTo prevent or mitigate future accidental deletions, consider the following features:\n\nEnable versioning to keep historical versions of an object.\nEnable Cross-Region Replication of objects.\nEnable MFA delete to require multi-factor authentication (MFA) when deleting an object version.","poster":"KFCR","comment_id":"944907","timestamp":"1688666400.0"},{"comment_id":"913309","content":"Selected Answer: AB\noptions A & B meet these requirements, hence A and B are the right answers.","upvote_count":"1","timestamp":"1685770680.0","poster":"Bmarodi"},{"upvote_count":"1","poster":"Chaudhry1997","timestamp":"1685196600.0","content":"Selected Answer: AB\nThe correct solution is AB","comment_id":"908033"},{"comment_id":"891081","timestamp":"1683419280.0","content":"Admin out here trying to get people to fail lol.\nA and B, folks. If somehow this presents as a question needing only one answer, MFA dlete is your go to.","poster":"pbpally","upvote_count":"1"},{"upvote_count":"1","timestamp":"1681776180.0","comment_id":"873182","content":"Selected Answer: AB\nHad this question on TD exam... A and B. Period.","poster":"acuaws"},{"content":"A and B seems are the good ones to me but couldnt I create policy to block all deletes and allow Put/Get, etc.?","poster":"darn","comment_id":"871106","timestamp":"1681576680.0","upvote_count":"1"},{"content":"Selected Answer: AB\nA+B will solve the problem","timestamp":"1680931440.0","poster":"cheese929","upvote_count":"1","comment_id":"864427"},{"timestamp":"1680728820.0","poster":"piavik","upvote_count":"1","content":"Selected Answer: AB\nPolicies and encryption do not affect delete protection","comment_id":"862472"},{"content":"Selected Answer: AB\nA. Enable versioning on the S3 bucket. Versioning allows multiple versions of an object to be stored in the same bucket. When versioning is enabled, every object uploaded to the bucket is automatically assigned a unique version ID. This provides protection against accidental deletion or modification of objects.\n\nB. Enable MFA Delete on the S3 bucket. MFA Delete requires the use of a multi-factor authentication (MFA) device to permanently delete an object or suspend versioning on a bucket. This provides an additional layer of protection against accidental or malicious deletion of objects.","timestamp":"1680271200.0","poster":"linux_admin","upvote_count":"1","comment_id":"857148"},{"content":"There no need to add default S3 encryption this is alrady enabled\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 are automatically encrypted at no additional cost and with no impact on performance. The automatic encryption status for S3 bucket default encryption configuration and for new object uploads is available in AWS CloudTrail logs, S3 Inventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API response header in the AWS Command Line Interface and AWS SDKs","comment_id":"827404","poster":"GalileoEC2","upvote_count":"1","timestamp":"1677789000.0"},{"upvote_count":"1","comment_id":"822152","content":"Selected Answer: AB\nA & B together solve this problem","timestamp":"1677398700.0","poster":"Sdraju"},{"timestamp":"1673053200.0","comment_id":"768167","poster":"SilentMilli","content":"Selected Answer: AB\nEnabling versioning on the S3 bucket and enabling MFA Delete on the S3 bucket will help protect the data from accidental deletion.\n\nVersioning allows the company to store multiple versions of an object in the same bucket. When versioning is enabled, S3 automatically archives all versions of an object (including all writes and deletes) in the bucket. This means that if an object is accidentally deleted, it can be recovered by restoring an earlier version of the object.\n\nMFA Delete adds an extra layer of protection by requiring users to provide additional authentication (through an MFA device) before they can permanently delete an object version. This helps prevent accidental or malicious deletion of objects by requiring users to confirm their intent to delete.\n\nBy using both versioning and MFA Delete, the company can protect the data in the S3 bucket from accidental deletion and provide a way to recover deleted objects if necessary.","upvote_count":"1"},{"upvote_count":"1","content":"As per white paper - \"versioning\" is one of the answer \n https://d0.awsstatic.com/whitepapers/protecting-s3-against-object-deletion.pdf","poster":"etikalas","comment_id":"759366","timestamp":"1672203420.0"},{"content":"Selected Answer: AB\nA, versioning is a way to protect buckets from accidental deletions\nB, MFA is a way to protect bucket from accidental deletions","comment_id":"751420","upvote_count":"2","timestamp":"1671566460.0","poster":"pazabal"},{"timestamp":"1671509940.0","poster":"Buruguduystunstugudunstuy","upvote_count":"3","comments":[{"timestamp":"1671509940.0","content":"***WRONG***\nOption C, creating a bucket policy, would not directly protect the data from accidental deletion. \nOption D, enabling default encryption, would help protect the data from unauthorized access but would not prevent accidental deletion. \nOption E, creating a lifecycle policy, can be used to automate the deletion of objects based on specified criteria, but would not prevent accidental deletion in this case.","upvote_count":"3","comment_id":"750490","poster":"Buruguduystunstugudunstuy"}],"content":"Selected Answer: AB\n***CORRECT***\nA. Enable versioning on the S3 bucket.\nB. Enable MFA Delete on the S3 bucket.\n\nEnabling versioning on an S3 bucket allows you to store multiple versions of an object in the same bucket. This means that you can recover an object that was accidentally deleted or overwritten. When versioning is enabled, deleted objects are not permanently deleted, but are instead marked as deleted and stored as a new version of the object. \n\nEnabling MFA (Multi-Factor Authentication) Delete on an S3 bucket adds an additional layer of security by requiring that you provide a valid MFA code before permanently deleting an object version. This can help prevent the accidental deletion of objects in the bucket.","comment_id":"750489"},{"timestamp":"1671429240.0","content":"Selected Answer: AB\nA and B","poster":"career360guru","upvote_count":"1","comment_id":"749501"},{"timestamp":"1671219960.0","content":"Selected Answer: AB\nEnable versioning on the S3 bucket. Most Voted\nEnable MFA Delete on the S3 bucket","poster":"NikaCZ","comment_id":"747539","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: AB\nI would accept D if they would have mentioned \"sensitive\" but it is not... A & B is the answer","comment_id":"746503","poster":"NiceGuy1169","timestamp":"1671138480.0"},{"poster":"parku","comment_id":"739365","timestamp":"1670526480.0","upvote_count":"2","content":"Selected Answer: AB\nVersioning + MFA Delete."},{"poster":"VJ_For_Azure_AWS","content":"A should not be an answer because you can delete version of files, whenever you delete file which has versions it will delete top version so basically it is allowing you to delete, you can keep deleting versions until you delete old file.","upvote_count":"2","comment_id":"738336","timestamp":"1670446620.0"},{"comment_id":"734356","timestamp":"1670064600.0","upvote_count":"1","poster":"hpipit","content":"Selected Answer: AB\nA & B, THE CORRECT RESPONSE"},{"upvote_count":"1","comment_id":"732831","timestamp":"1669911900.0","content":"Selected Answer: AB\nA and B, 100% CORRECT","poster":"hpipit"},{"content":"A and B","timestamp":"1669037340.0","poster":"Wpcorgan","comment_id":"723544","upvote_count":"1"},{"upvote_count":"2","comment_id":"710093","timestamp":"1667425260.0","poster":"Solarch","content":"AB, Versioning keeps a copy and can be retrieved. MFA ensures you have proper authorization to delete an item."},{"content":"AB for sure =))))))))))))))","poster":"ricenguyen208","upvote_count":"1","timestamp":"1667320440.0","comment_id":"709279"},{"content":"Selected Answer: AB\nthere is no relation between cncryption and delete protection. so D is discarded.","comment_id":"706450","poster":"raffaello44","upvote_count":"2","timestamp":"1666960320.0"},{"timestamp":"1666892280.0","upvote_count":"1","content":"Selected Answer: AB\nThere is no way it is not A and B.","poster":"ukwafabian","comment_id":"705778"},{"timestamp":"1666872900.0","content":"Selected Answer: AB\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\nYou can use the S3 Versioning feature to preserve, retrieve, and restore every version of every object stored in your buckets\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html\nWhen working with S3 Versioning in Amazon S3 buckets, you can optionally add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) delete.","upvote_count":"3","poster":"airraid2010","comment_id":"705532"},{"content":"Selected Answer: AB\nobviously AB here","upvote_count":"1","comment_id":"703184","poster":"Six_Fingered_Jose","timestamp":"1666627500.0"},{"comment_id":"697789","poster":"Jahangeer_17","content":"Correct answer is A & B.","upvote_count":"1","timestamp":"1666057020.0"},{"timestamp":"1665919500.0","comment_id":"696207","upvote_count":"1","content":"The answers are A and B","poster":"Incognito013"},{"timestamp":"1665802800.0","upvote_count":"1","content":"Selected Answer: AB\nAB-version and MFA to ensure.","comment_id":"695145","poster":"Rachness"},{"timestamp":"1665727920.0","comment_id":"694539","content":"Selected Answer: AB\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-audit-deleted-missing-objects/","poster":"rajendradba","upvote_count":"1"},{"upvote_count":"3","poster":"tt79","content":"AB is correct.","comment_id":"691072","timestamp":"1665402060.0"},{"poster":"tuloveu","timestamp":"1665369420.0","upvote_count":"2","content":"Selected Answer: AB\nA,B is my option.","comment_id":"690662"},{"comment_id":"690013","content":"I BELIVE A,C IS CORRECT!","poster":"Testtest123321","upvote_count":"3","timestamp":"1665306480.0"},{"comment_id":"689494","poster":"CloudGuru99","upvote_count":"3","content":"AB is the correct answer","timestamp":"1665245160.0"}],"exam_id":31,"answer_ET":"AB","url":"https://www.examtopics.com/discussions/amazon/view/84750-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"q9p1HuWNpzh673bAZFnm","answer":"AC","question_id":400,"choices":{"B":"Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.","C":"Upload the database dump to Amazon S3. Then import the database dump into Aurora.","E":"Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.","D":"Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.","A":"Import the RDS snapshot directly into Aurora."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/109297-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-05-15 15:14:00","question_text":"A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination.\n\nThe company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance.\n\nWhich solutions will create the new DB instance? (Choose two.)","unix_timestamp":1684156440,"answer_ET":"AC","answers_community":["AC (81%)","Other"],"answer_images":[],"question_images":[],"discussion":[{"upvote_count":"13","poster":"Axaus","comment_id":"900505","timestamp":"1684356960.0","content":"Selected Answer: AC\nA,C\nA because the snapshot is already stored in AWS. \nC because you dont need a migration tool going from MySQL to MySQL. You would use the MySQL utility."},{"comment_id":"907862","timestamp":"1685178180.0","poster":"oras2023","upvote_count":"8","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Import.html"},{"content":"Selected Answer: AC\nAnswer B is out, why upload to S3?\nThis is to call the most recent backup, answer D & E are out to use AWS Database Migration Service (AWS DMS).","timestamp":"1742880120.0","upvote_count":"1","poster":"tch","comment_id":"1409900"},{"content":"Selected Answer: CE\nAWS DMS does not support migrating data directly from an RDS snapshot. DMS can migrate data from a live RDS instance or from a database dump, but not from a snapshot. \nAlso dump can be migrated to aurora using sql client.","upvote_count":"2","comments":[{"upvote_count":"1","poster":"Mayur_B","timestamp":"1729043040.0","comment_id":"1298502","content":"Correct. \nHomogeneous migration in AWS Database Migration Service (DMS) refers to migrating databases between the same type of database engine. Here's an example:\n\nExample:\nSource Database: Amazon RDS for MySQL\nTarget Database: Amazon Aurora MySQL"}],"comment_id":"1281412","timestamp":"1725952920.0","poster":"rpmaws"},{"poster":"JackyCCK","upvote_count":"1","comment_id":"1190550","content":"If you can use option A - \"Import the RDS snapshot directly into Aurora\", why go S3 in option C ?\nNon sense, A and C cannot co-exist","timestamp":"1712426520.0"},{"timestamp":"1704029160.0","poster":"pentium75","comment_id":"1110583","content":"Selected Answer: AC\nA per https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Snapshot.html\n\nC per https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.ExtMySQL.html","upvote_count":"7"},{"comment_id":"1093770","upvote_count":"1","timestamp":"1702321920.0","poster":"aws94","content":"Selected Answer: AB\nA and B"},{"timestamp":"1701349620.0","upvote_count":"3","poster":"meowruki","content":"Selected Answer: AC\nSimilar : https://repost.aws/knowledge-center/aurora-postgresql-migrate-from-rds","comment_id":"1084376"},{"poster":"potomac","content":"Selected Answer: AD\nA and C","timestamp":"1699209180.0","comment_id":"1063175","upvote_count":"1"},{"upvote_count":"3","timestamp":"1698642720.0","poster":"TariqKipkemei","content":"Selected Answer: AC\nEither import the RDS snapshot directly into Aurora or upload the database dump to Amazon S3, then import the database dump into Aurora.","comment_id":"1057357"},{"upvote_count":"5","content":"AC:\n- store dump in s3 then upload to aurora\n- no need to store snapshot in s3 because is in AWS already","poster":"thanhnv142","comment_id":"1048885","timestamp":"1697812560.0"},{"timestamp":"1692964080.0","upvote_count":"3","comment_id":"990018","content":"Selected Answer: CE\nC and E are the solutions that can restore the backups into Amazon Aurora.\n\nThe RDS DB snapshot contains backup data in a proprietary format that cannot be directly imported into Aurora.\nThe mysqldump database dump contains SQL statements that can be imported into Aurora after uploading to S3.\nAWS DMS can migrate the dump file from S3 into Aurora.","poster":"Guru4Cloud"},{"poster":"james2033","upvote_count":"3","comment_id":"954983","content":"Selected Answer: AC\nAmazon RDS for MySQL --> Amazon Aurora MySQL-compatible.\n* mysqldump, database dump --> (C) Upload to Amazon S3, Import dump to Aurora.\n* DB snapshot --> (A) Import RDS Snapshot directly Aurora. The correct word should be \"migration\". \"Use console to migrate the DB snapshot and create an Aurora MySQL DB cluster with the same databases as the original MySQL DB instance.\"\n\nExclude B, because no need upload DB snapshot to Amazon S3. Exclude D, because no need Migration service. Exclude E, because no need Migration service. Use exclusion method is more easy for this question.\n\nRelated links:\n- Amazon RDS create database snapshot https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html\n- https://aws.amazon.com/rds/aurora/","timestamp":"1689658380.0"},{"timestamp":"1687063800.0","upvote_count":"3","poster":"marufxplorer","content":"CE\nSince the backup created by the solutions architect was a database dump using the mysqldump utility, it cannot be directly imported into Aurora using RDS snapshots. Amazon Aurora has its own specific backup format that is different from RDS snapshots","comments":[{"poster":"Guru4Cloud","comment_id":"990016","upvote_count":"2","content":"C and E are the solutions that can restore the backups into Amazon Aurora.\n\nThe RDS DB snapshot contains backup data in a proprietary format that cannot be directly imported into Aurora.\nThe mysqldump database dump contains SQL statements that can be imported into Aurora after uploading to S3.\nAWS DMS can migrate the dump file from S3 into Aurora.","timestamp":"1692964020.0"}],"comment_id":"926425"},{"comment_id":"915175","timestamp":"1685950440.0","poster":"antropaws","content":"Selected Answer: AC\nMigrating data from MySQL by using an Amazon S3 bucket\n\nYou can copy the full and incremental backup files from your source MySQL version 5.7 database to an Amazon S3 bucket, and then restore an Amazon Aurora MySQL DB cluster from those files.\n\nThis option can be considerably faster than migrating data using mysqldump, because using mysqldump replays all of the commands to recreate the schema and data from your source database in your new Aurora MySQL DB cluster. \n\nBy copying your source MySQL data files, Aurora MySQL can immediately use those files as the data for an Aurora MySQL DB cluster.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.ExtMySQL.html","upvote_count":"4"},{"upvote_count":"1","comment_id":"901433","content":"BE\nUpload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.\nUpload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora","poster":"omoakin","timestamp":"1684432980.0"},{"upvote_count":"2","comment_id":"899701","content":"Selected Answer: BC\nId say B and C\nYou can create a dump of your data using the mysqldump utility, and then import that data into an existing Amazon Aurora MySQL DB cluster.\n\nc>- Because Amazon Aurora MySQL is a MySQL-compatible database, you can use the mysqldump utility to copy data from your MySQL or MariaDB database to an existing Amazon Aurora MySQL DB cluster.\n\nB.- You can copy the source files from your source MySQL version 5.5, 5.6, or 5.7 database to an Amazon S3 bucket, and then restore an Amazon Aurora MySQL DB cluster from those files.","poster":"Efren","timestamp":"1684292040.0"},{"upvote_count":"1","comments":[{"comments":[{"upvote_count":"1","content":"using the mysqldump database dump provide valid solutions to restore into Aurora. Options A, B, and D using the RDS snapshot cannot directly restore into Aurora.","poster":"Guru4Cloud","timestamp":"1692964140.0","comment_id":"990019"}],"content":"If too be honestly can't decide between be and bc...","poster":"nosense","upvote_count":"1","timestamp":"1684158180.0","comment_id":"898336"},{"upvote_count":"2","poster":"nosense","comment_id":"899881","timestamp":"1684312860.0","content":"in the end, apparently the A and C.\na) because it creates a new DB\nb) no sense to load in s3. can directly\nc) yes, creates a new inst\nd and e migration"}],"timestamp":"1684156440.0","comment_id":"898301","poster":"nosense","content":"Selected Answer: BE\nRds required upload to s3"}],"topic":"1","exam_id":31,"answer_description":""}],"exam":{"name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"provider":"Amazon","id":31,"isMCOnly":true,"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":80},"__N_SSP":true}