{"pageProps":{"questions":[{"id":"xVHa2x6W9CE9KPeSSN3f","discussion":[{"content":"Selected Answer: B\nA company needs to know the maximum input size for a single prompt when choosing a Foundation Model (FM) in Amazon Bedrock.\n\nA. Temperature: This controls the randomness of the output, not the input prompt length. Temperature affects creativity, not input size.\nB. Context window: This defines the maximum length of the input prompt the model can process. It directly limits how much information can be included.\nC. Batch size: This is the number of prompts processed at once, affecting throughput, not individual prompt length. It's about processing multiple prompts efficiently.\nD. Model size: This relates to the model's overall capacity and complexity, not directly to the input prompt length. Size impacts performance, not input limits.\nTherefore, B. Context window is the correct answer.","poster":"Moon","timestamp":"1735646520.0","upvote_count":"5","comment_id":"1334768"},{"comment_id":"1355347","content":"Selected Answer: B\nContext window: The context window refers to the amount of text (or tokens) that a model can process at once. This is crucial when working with foundation models (FMs) like those available in Amazon Bedrock, as it defines the maximum input size for the prompt. The context window determines how much of the prompt the model can \"remember\" and use to generate responses. If the prompt exceeds the context window, the model will only process the portion of the prompt that fits within this limit, potentially missing important details.","poster":"Jessiii","timestamp":"1739326500.0","upvote_count":"1"},{"poster":"85b5b55","comment_id":"1348158","upvote_count":"1","content":"Selected Answer: B\nThe context-window is the input prompt for the model generation.","timestamp":"1738093980.0"},{"content":"Selected Answer: B\nThe correct answer is:\n\nB. Context window\nExplanation:\n\nThe context window of a foundation model (FM) determines how much information can fit into one prompt. It refers to the maximum number of tokens (words, characters, or subwords) that the model can process in a single input prompt, including the input and the output combined.\n\nThe context window size varies across different foundation models, and understanding this parameter is critical for applications like document summarization or question-answering systems where long inputs need to be processed.","poster":"eesa","comment_id":"1324271","upvote_count":"1","timestamp":"1733788740.0"},{"comment_id":"1324270","timestamp":"1733788680.0","upvote_count":"2","content":"Selected Answer: B\nB. Context window\n\nThe context window of a foundation model determines the maximum amount of text that can be processed in a single prompt. A larger context window allows for more complex and informative prompts, while a smaller context window limits the amount of information that can be provided.\n\nThe other options are not directly related to the maximum prompt length:\n\nTemperature: This parameter controls the randomness of the model's output.\nBatch size: This refers to the number of samples processed in a single batch during training or inference.\nModel size: This refers to the number of parameters in the model, which affects its complexity and performance.\nTherefore, when choosing a foundation model for a generative AI application, the company should carefully consider the context window to ensure that it can accommodate the desired input length.","poster":"eesa"},{"poster":"jove","upvote_count":"2","comment_id":"1307505","content":"Selected Answer: B\nThe context window refers to the maximum number of tokens (words or pieces of words) that a foundation model can process in a single input prompt.","timestamp":"1730831280.0"}],"answer":"B","answer_images":[],"unix_timestamp":1730831280,"answer_ET":"B","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/150803-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","isMC":true,"answers_community":["B (100%)"],"exam_id":14,"answer_description":"","choices":{"B":"Context window","A":"Temperature","C":"Batch size","D":"Model size"},"question_text":"A company wants to build a generative AI application by using Amazon Bedrock and needs to choose a foundation model (FM). The company wants to know how much information can fit into one prompt.\nWhich consideration will inform the company's decision?","timestamp":"2024-11-05 19:28:00","question_id":71,"topic":"1"},{"id":"dIEs7ota63KrgUnoTH79","answer_ET":"C","isMC":true,"discussion":[{"poster":"Jessiii","comment_id":"1355349","timestamp":"1739326560.0","upvote_count":"1","content":"Selected Answer: C\nExperiment and refine the prompt until the FM produces the desired responses: The behavior and tone of a chatbot powered by a foundation model (FM) are heavily influenced by how the prompt is crafted. By iterating on the prompt and fine-tuning it, you can guide the model to respond in a way that aligns with the company’s tone, whether it's formal, friendly, technical, or casual. This is a common practice in prompt engineering to get the model to generate output that matches specific requirements."},{"comment_id":"1348162","content":"Selected Answer: C\nContinued pre-taining of the datasets to produce responses to the company's tone.","upvote_count":"1","timestamp":"1738094220.0","poster":"85b5b55"},{"comment_id":"1344787","upvote_count":"1","timestamp":"1737555840.0","content":"Selected Answer: C\nC. Experiment and refine the prompt until the FM produces the desired responses.\n\nRefining the prompt is key to aligning the chatbot's responses with the company's tone and guidelines. Foundation models respond significantly to how prompts are phrased, making prompt engineering a powerful tool for achieving desired behavior.","poster":"nandhae"},{"upvote_count":"1","poster":"Moon","content":"Selected Answer: C\nC: Experiment and refine the prompt until the FM produces the desired responses.\n\nExplanation:\nTo ensure that the chatbot adheres to the company's tone and provides appropriate responses, prompt engineering is essential. By experimenting and refining the prompt, you can guide the foundation model (FM) to produce responses that align with the desired tone, style, and content. This approach allows you to set the context and expectations for the chatbot's replies.","comment_id":"1334773","timestamp":"1735646940.0"},{"poster":"ap6491","comment_id":"1332690","upvote_count":"1","content":"Selected Answer: C\nPrompt engineering is the most effective way to ensure that a foundation model (FM) produces outputs adhering to a company’s tone and specific requirements.\nBy iteratively testing and refining prompts, you can guide the FM to produce responses that align with the desired style, tone, and content accuracy.","timestamp":"1735344540.0"},{"poster":"jove","content":"Selected Answer: C\nRefining the prompt is the answer","comment_id":"1307507","upvote_count":"4","timestamp":"1730831340.0"}],"unix_timestamp":1730831340,"answer":"C","exam_id":14,"question_id":72,"answer_description":"","answers_community":["C (100%)"],"question_text":"A company wants to make a chatbot to help customers. The chatbot will help solve technical problems without human intervention.\nThe company chose a foundation model (FM) for the chatbot. The chatbot needs to produce responses that adhere to company tone.\nWhich solution meets these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/150804-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","answer_images":[],"topic":"1","question_images":[],"choices":{"C":"Experiment and refine the prompt until the FM produces the desired responses.","B":"Use batch inferencing to process detailed responses.","D":"Define a higher number for the temperature parameter.","A":"Set a low limit on the number of tokens the FM can produce."},"timestamp":"2024-11-05 19:29:00"},{"id":"vQ5lB53ipnOUChmSD13b","answer_images":[],"isMC":true,"question_text":"A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company wants to classify the sentiment of text passages as positive or negative.\n\nWhich prompt engineering strategy meets these requirements?","exam_id":14,"answer_ET":"A","discussion":[{"poster":"Johnny0107","comment_id":"1363434","content":"Selected Answer: A\nThis approach is known as few-shot prompting, where you include a few labeled examples to guide the model on how to classify sentiment.","upvote_count":"2","timestamp":"1740812520.0"},{"poster":"Jessiii","comment_id":"1355350","content":"Selected Answer: A\nProvide examples of text passages with corresponding positive or negative labels in the prompt followed by the new text passage to be classified: This approach uses few-shot learning, where you give the model clear examples of text passages labeled with sentiment (positive or negative). By providing these examples in the prompt, the model is better able to understand the task and generalize it to classify the new text passage correctly. This is a common and effective strategy for tasks like sentiment analysis.","timestamp":"1739326620.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1348167","timestamp":"1738094580.0","poster":"85b5b55","content":"Selected Answer: A\nSet the proper label with a few examples to the prompts"},{"timestamp":"1735652820.0","poster":"Moon","upvote_count":"4","content":"Selected Answer: A\nA: Provide examples of text passages with corresponding positive or negative labels in the prompt followed by the new text passage to be classified.\n\nExplanation:\nThis strategy is known as few-shot prompting, where the prompt includes a few examples of labeled data (text passages with positive or negative sentiment) before asking the model to classify the new text passage. This helps the large language model (LLM) understand the task and align its output with the desired format.\n\nWhy not the other options?\nB: Provide a detailed explanation of sentiment analysis and how LLMs work in the prompt:\nExplaining the concept of sentiment analysis is unnecessary for the model, as it does not improve the model's ability to classify text.\nC: Provide the new text passage to be classified without any additional context or examples:\nWithout examples, the LLM might not correctly infer the task or format of the output, leading to inconsistent or incorrect results.","comment_id":"1334826"},{"content":"Selected Answer: A\nThis approach uses few-shot learning, which is highly effective with large language models. By providing examples of text passages with their corresponding sentiment classifications, the LLM learns the context and pattern needed to classify the new passage.","comment_id":"1330869","upvote_count":"1","poster":"Gianiluca","timestamp":"1734969780.0"},{"upvote_count":"4","poster":"jove","content":"Selected Answer: A\nExplanation:\nBy providing examples of text passages along with their corresponding sentiment labels (positive or negative), the model can learn from these examples how to classify the sentiment of the new text passage effectively","comment_id":"1307509","timestamp":"1730831580.0"}],"question_images":[],"topic":"1","answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/150805-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","unix_timestamp":1730831580,"answer":"A","timestamp":"2024-11-05 19:33:00","answer_description":"","question_id":73,"choices":{"D":"Provide the new text passage with a few examples of unrelated tasks, such as text summarization or question answering.","B":"Provide a detailed explanation of sentiment analysis and how LLMs work in the prompt.","A":"Provide examples of text passages with corresponding positive or negative labels in the prompt followed by the new text passage to be classified.","C":"Provide the new text passage to be classified without any additional context or examples."}},{"id":"fEkJGwJgngMmDD1BVFgj","exam_id":14,"answers_community":["B (100%)"],"answer_ET":"B","answer":"B","timestamp":"2024-11-05 19:35:00","isMC":true,"question_text":"A security company is using Amazon Bedrock to run foundation models (FMs). The company wants to ensure that only authorized users invoke the models. The company needs to identify any unauthorized access attempts to set appropriate AWS Identity and Access Management (IAM) policies and roles for future iterations of the FMs.\nWhich AWS service should the company use to identify unauthorized users that are trying to access Amazon Bedrock?","choices":{"A":"AWS Audit Manager","C":"Amazon Fraud Detector","D":"AWS Trusted Advisor","B":"AWS CloudTrail"},"unix_timestamp":1730831700,"question_images":[],"discussion":[{"timestamp":"1739326680.0","poster":"Jessiii","upvote_count":"3","comment_id":"1355351","content":"Selected Answer: B\nAWS CloudTrail: CloudTrail records all API requests made to AWS services, including Amazon Bedrock. By using CloudTrail, the security company can track and log all access attempts, including any unauthorized attempts, to access Amazon Bedrock. This helps the company identify and respond to unauthorized access, and also provides detailed logs for setting up appropriate IAM policies and roles."},{"upvote_count":"2","comment_id":"1349731","content":"Selected Answer: B\nAWS CloudTrail is a service that records all API calls and user activity across AWS services, including Amazon Bedrock.","poster":"eyzzeuss","timestamp":"1738378800.0"},{"content":"Selected Answer: B\nUse AWS CloudTrail to track the API Calls to AWS resources.","poster":"85b5b55","comment_id":"1348169","timestamp":"1738094700.0","upvote_count":"1"},{"upvote_count":"1","poster":"nandhae","comment_id":"1344801","content":"Selected Answer: B\nB. AWS CloudTrail\n\nCloudTrail records API activity and user actions in your AWS account. It logs events such as unauthorized access attempts to Amazon Bedrock and other AWS services, making it the correct choice for identifying such attempts.","timestamp":"1737557280.0"},{"poster":"Moon","comment_id":"1334827","content":"Selected Answer: B\nB: AWS CloudTrail\n\nExplanation:\nAWS CloudTrail is a service that records all API calls and user activity across AWS services, including Amazon Bedrock. By analyzing CloudTrail logs, the company can identify unauthorized access attempts, track user activity, and audit the usage of foundation models. This information helps in setting appropriate AWS Identity and Access Management (IAM) policies and roles for future iterations of the models.","timestamp":"1735652940.0","upvote_count":"2"},{"timestamp":"1730831700.0","upvote_count":"3","content":"Selected Answer: B\nB. AWS CloudTrail is the most suitable service for identifying unauthorized access attempts to Amazon Bedrock, as it provides detailed logging and monitoring of API calls across AWS services, helping to enforce security and compliance.","poster":"jove","comment_id":"1307512"}],"url":"https://www.examtopics.com/discussions/amazon/view/150806-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","answer_description":"","topic":"1","answer_images":[],"question_id":74},{"id":"zkQQCqUVkPiDhcram2rz","answer_images":[],"unix_timestamp":1731287940,"isMC":true,"exam_id":14,"answers_community":["A (100%)"],"question_id":75,"choices":{"B":"Use Amazon CloudFront to deploy the model.","D":"Use AWS Batch to host the model and serve predictions.","A":"Use Amazon SageMaker Serverless Inference to deploy the model.","C":"Use Amazon API Gateway to host the model and serve predictions."},"answer_ET":"A","discussion":[{"comment_id":"1355352","timestamp":"1739326740.0","content":"Selected Answer: A\nUse Amazon SageMaker Serverless Inference to deploy the model: Amazon SageMaker Serverless Inference allows you to deploy machine learning models in a fully managed, serverless environment. You don't need to manage the underlying infrastructure (such as EC2 instances) to handle predictions. This is ideal for scenarios like yours, where the model needs to be deployed and used by a web application, and scalability and infrastructure management should be abstracted away.","poster":"Jessiii","upvote_count":"2"},{"content":"Selected Answer: A\nAmazon SageMaker helps to host the model, and serve predictions without managing infrastructure provisioning and configurations.","timestamp":"1738095000.0","comment_id":"1348170","upvote_count":"2","poster":"85b5b55"},{"upvote_count":"1","poster":"nandhae","content":"Selected Answer: A\nA. Use Amazon SageMaker Serverless Inference to deploy the model.\n\nAmazon SageMaker Serverless Inference is specifically designed for hosting ML models and serving predictions without requiring the management of underlying infrastructure. It automatically provisions compute resources as needed and is ideal for use cases like the one described.","timestamp":"1737557400.0","comment_id":"1344803"},{"content":"Selected Answer: A\nA: Use Amazon SageMaker Serverless Inference to deploy the model.\n\nExplanation:\nAmazon SageMaker Serverless Inference is a fully managed solution for deploying machine learning models without managing the underlying infrastructure. It automatically provisions compute capacity, scales based on request traffic, and serves predictions efficiently. This makes it an ideal choice for hosting a model and serving predictions for a web application with minimal management overhead.\nWhy not the other options?\nB: Use Amazon CloudFront to deploy the model:\nAmazon CloudFront is a content delivery network (CDN) \nC: Use Amazon API Gateway to host the model and serve predictions:\nAmazon API Gateway is used to create APIs for accessing services. \nD: Use AWS Batch to host the model and serve predictions:\nAWS Batch is designed for batch processing and job scheduling, not for real-time inference or hosting ML models for web applications.","poster":"Moon","comment_id":"1334830","timestamp":"1735653120.0","upvote_count":"1"},{"content":"Selected Answer: A\nServerless deployment: SageMaker Serverless Inference allows you to deploy ML models without managing any underlying infrastructure, which directly meets the company's requirement.","timestamp":"1731348780.0","poster":"Blair77","upvote_count":"1","comment_id":"1310239"},{"content":"A. Use Amazon SageMaker Serverless Inference to deploy the model.\nWith serverless inference, there's no need to manage any infra.","comment_id":"1309764","timestamp":"1731287940.0","upvote_count":"1","poster":"minime"}],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/151095-exam-aws-certified-ai-practitioner-aif-c01-topic-1-question/","answer_description":"","timestamp":"2024-11-11 02:19:00","question_text":"A company has developed an ML model for image classification. The company wants to deploy the model to production so that a web application can use the model.\nThe company needs to implement a solution to host the model and serve predictions without managing any of the underlying infrastructure.\nWhich solution will meet these requirements?","topic":"1","question_images":[]}],"exam":{"id":14,"isBeta":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":false,"numberOfQuestions":154,"name":"AWS Certified AI Practitioner AIF-C01"},"currentPage":15},"__N_SSP":true}