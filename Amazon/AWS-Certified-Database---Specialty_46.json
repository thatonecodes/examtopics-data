{"pageProps":{"questions":[{"id":"u8tbQEQxHaV1Col1LOHD","question_images":[],"timestamp":"2023-03-29 18:50:00","topic":"1","isMC":true,"question_id":226,"answer_description":"","discussion":[{"timestamp":"1696791300.0","content":"Selected Answer: BDE\nBDE are minimal efforts while the rest are copying the data and how about the future data? you can't be copying them all the time","poster":"Sathish_dbs","comment_id":"1028237","upvote_count":"4"},{"timestamp":"1693811460.0","content":"Selected Answer: BDE\nMAYBE BDE","poster":"chen0305_099","comment_id":"998281","upvote_count":"1"},{"comment_id":"955677","upvote_count":"1","content":"It's BDE","timestamp":"1689700260.0","poster":"Windy"},{"comment_id":"857344","content":"Selected Answer: BDE\nOption B (external tables) to connect Glue Data catalog tables from Redshift \nhttps://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_EXTERNAL_TABLE.html\n\nOption D (federated queries) to query Amazon RDS for PostgreSQL databases from Redshift\nhttps://docs.aws.amazon.com/redshift/latest/dg/federated-overview.html\n\nOption E (data sharing) to access data in other redshift clusters\nhttps://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html","upvote_count":"4","poster":"backbencher2022","timestamp":"1680286260.0"},{"poster":"rdiaz","comment_id":"854700","timestamp":"1680108600.0","content":"Maybe BDE","upvote_count":"1"}],"question_text":"A company is using Amazon Redshift. A database specialist needs to allow an existing Redshift cluster to access data from other Redshift clusters. Amazon RDS for PostgreSQL databases, and AWS Glue Data Catalog tables.\n\nWhich combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.)","url":"https://www.examtopics.com/discussions/amazon/view/104407-exam-aws-certified-database-specialty-topic-1-question-301/","answer_ET":"BDE","choices":{"A":"Take a snapshot of the required tables from the other Redshift clusters. Restore the snapshot into the existing Redshift cluster.","E":"Use data sharing to access data from the other Redshift clusters.","D":"Use federated queries to access data in Amazon RDS.","B":"Create external tables in the existing Redshift database to connect to the AWS Glue Data Catalog tables.","C":"Unload the RDS tables and the tables from the other Redshift clusters into Amazon S3. Run COPY commands to load the tables into the existing Redshift cluster.","F":"Use AWS Glue jobs to transfer the AWS Glue Data Catalog tables into Amazon S3. Create external tables in the existing Redshift database to access this data."},"unix_timestamp":1680108600,"answer":"BDE","answer_images":[],"answers_community":["BDE (100%)"],"exam_id":22},{"id":"KQNYnpxRcngmVkdXGr3u","discussion":[{"comment_id":"1027920","upvote_count":"3","timestamp":"1696764840.0","content":"Selected Answer: B\nIt's B&D:\nA - Not applicable for Oracle see:https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.TargetMetadata.html#:~:text=ParallelLoadThreads%20%E2%80%93%20Specifies%20the%20number%20of%20%20threads,Kafka%2C%20or%20Amazon%20Elasticsearch%20Service%20target%20is%2032.\n\nB - https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.FullLoad.html\n\nD - https://aws.amazon.com/blogs/database/speed-up-database-migration-by-using-aws-dms-with-parallel-load-and-filter-options/","poster":"roymunson"},{"poster":"Windy","timestamp":"1689762600.0","upvote_count":"1","comment_id":"956516","content":"It's BD"},{"content":"How about A and B? What is the data boundary meaning of option D?","upvote_count":"1","timestamp":"1685437980.0","comment_id":"910098","poster":"icttss"},{"timestamp":"1683809220.0","upvote_count":"1","content":"BD my vote. Please note Question is asking 2 answer choices...","poster":"tsk9921","comment_id":"895069"},{"comment_id":"884119","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/database/speed-up-database-migration-by-using-aws-dms-with-parallel-load-and-filter-options/\n B + D","upvote_count":"3","poster":"clarksu","timestamp":"1682752500.0"},{"poster":"backbencher2022","comment_id":"857350","upvote_count":"2","content":"Selected Answer: B\nB and D are correct options","timestamp":"1680287160.0"},{"upvote_count":"2","poster":"rdiaz","content":"Selected Answer: B\nB and d\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.FullLoad.html","timestamp":"1679845920.0","comment_id":"851237"}],"answer_images":[],"answers_community":["B (70%)","D (30%)"],"question_text":"A company is planning to migrate a 40 TB Oracle database to an Amazon Aurora PostgreSQL DB cluster by using a single AWS Database Migration Service (AWS DMS) task within a single replication instance. During early testing, AWS DMS is not scaling to the company's needs. Full load and change data capture (CDC) are taking days to complete.\n\nThe source database server and the target DB cluster have enough network bandwidth and CPU bandwidth for the additional workload. The replication instance has enough resources to support the replication. A database specialist needs to improve database performance, reduce data migration time, and create multiple DMS tasks.\n\nWhich combination of changes will meet these requirements? (Choose two.)","isMC":true,"question_images":[],"answer_description":"","exam_id":22,"topic":"1","unix_timestamp":1679845920,"timestamp":"2023-03-26 17:52:00","question_id":227,"url":"https://www.examtopics.com/discussions/amazon/view/103989-exam-aws-certified-database-specialty-topic-1-question-302/","answer_ET":"B","choices":{"D":"Use parallel load with different data boundaries for larger tables.","C":"Use a smaller set of tables with each DMS task. Set the MaxFullLoadSubTasks parameter to a lower value.","B":"Use a smaller set of tables with each DMS task. Set the MaxFullLoadSubTasks parameter to a higher value.","E":"Run the DMS tasks on a larger instance class. Increase local storage on the instance.","A":"Increase the value of the ParallelLoadThreads parameter in the DMS task settings for the tables."},"answer":"B"},{"id":"yZnYsTX9BUradKBsgnBM","timestamp":"2023-03-26 17:24:00","question_images":[],"topic":"1","isMC":true,"question_id":228,"answer_description":"","discussion":[{"comment_id":"1122313","poster":"MultiAZ","content":"Selected Answer: B\nOnly B makes sense.","upvote_count":"1","timestamp":"1705215240.0"},{"comment_id":"1001819","upvote_count":"3","poster":"Pranava_GCP","comments":[{"upvote_count":"1","poster":"Pranava_GCP","content":"\" Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.\"","comment_id":"1001820","timestamp":"1694110080.0"}],"content":"Selected Answer: B\nB. Deploy an Amazon Aurora MySQL DB cluster. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html","timestamp":"1694110020.0"},{"poster":"Windy","timestamp":"1689699480.0","content":"It's B.","comment_id":"955665","upvote_count":"1"},{"timestamp":"1683809400.0","comment_id":"895074","upvote_count":"1","poster":"tsk9921","content":"B is my vote"},{"timestamp":"1680287340.0","upvote_count":"1","comment_id":"857353","poster":"backbencher2022","content":"Selected Answer: B\nB is the most operationally efficient way to scale read operations"},{"poster":"rdiaz","upvote_count":"1","timestamp":"1679844240.0","content":"Selected Answer: B\nAurora ok","comment_id":"851207"}],"question_text":"A financial services company is running a MySQL database on premises. The database holds details about all customer interactions and the financial advice that the company provided. The write traffic to the database is well known and consistent. However, the read traffic is subject to significant and sudden increases for end-of-month reporting. The database is becoming overloaded during these periods of heavy read activity.\n\nThe company decides to move the database to AWS. A database specialist needs to propose a solution in the AWS Cloud that will scale to meet the variable read traffic requirements without affecting the performance of write traffic. Scaling events must not require any downtime.\n\nWhat is the MOST operationally efficient solution that meets these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/103986-exam-aws-certified-database-specialty-topic-1-question-303/","answer_ET":"B","choices":{"D":"Deploy an Amazon DynamoDB database. Create a DynamoDB auto scaling policy to adjust the read capacity of the database based on target utilization. Direct all read traffic and write traffic to the DynamoDB database.","B":"Deploy an Amazon Aurora MySQL DB cluster. Select a Cross-AZ configuration with an Aurora Replica. Create an Aurora Auto Scaling policy to adjust the number of Aurora Replicas based on CPU utilization. Direct all read-only reporting traffic to the reader endpoint for the DB cluster.","C":"Deploy an Amazon RDS for MySQL Multi-AZ database as a write database. Deploy a second RDS for MySQL Multi-AZ database that is configured as an auto scaling read-only database. Use AWS Database Migration Service (AWS DMS) to continuously replicate data from the write database to the read-only database. Direct all read-only reporting traffic to the reader endpoint for the read-only database.","A":"Deploy a MySQL primary node on Amazon EC2 in one Availability Zone. Deploy a MySQL read replica on Amazon EC2 in a different Availability Zone. Configure a scheduled scaling event to increase the CPU capacity and RAM capacity within the MySQL read replica the day before each known traffic surge. Configure a scheduled scaling event to reduce the CPU capacity and RAM capacity within the MySQL read replica the day after each known traffic surge."},"unix_timestamp":1679844240,"answer":"B","answer_images":[],"answers_community":["B (100%)"],"exam_id":22},{"id":"Oq1p5cmxPcdgltKkNktl","answer_description":"","answer_images":[],"topic":"1","question_id":229,"unix_timestamp":1679844000,"answers_community":["D (100%)"],"discussion":[{"comment_id":"1122318","timestamp":"1705215420.0","poster":"MultiAZ","upvote_count":"1","content":"Selected Answer: D\nD\nNo need to disable multi-az before creating a replica"},{"content":"It's D.","upvote_count":"2","timestamp":"1689699300.0","comment_id":"955662","poster":"Windy"},{"comment_id":"895076","content":"D is right","poster":"tsk9921","timestamp":"1683809580.0","upvote_count":"1"},{"comment_id":"851201","poster":"rdiaz","timestamp":"1679844000.0","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/database/using-in-region-read-replicas-in-amazon-rds-for-sql-server/","upvote_count":"3","comments":[{"poster":"Mintwater","content":"Thank you rdiaz, this is an exact link:\nWhen you create a read replica, Amazon RDS takes a snapshot of the primary DB instance and creates a new read-only instance from the snapshot. Creating or deleting the read replica doesn’t require any downtime on the primary DB instance. You can create up to fifteen read replicas.\nNote: the same region, EE","comment_id":"861488","upvote_count":"1","timestamp":"1680641400.0"}]}],"url":"https://www.examtopics.com/discussions/amazon/view/103984-exam-aws-certified-database-specialty-topic-1-question-304/","timestamp":"2023-03-26 17:20:00","exam_id":22,"answer_ET":"D","question_text":"A company has a Microsoft SQL Server 2017 Enterprise edition on Amazon RDS database with the Multi-AZ option turned on. Automatic backups are turned on and the retention period is set to 7 days. The company needs to add a read replica to the RDS DB instance.\n\nHow should a database specialist achieve this task?","isMC":true,"question_images":[],"choices":{"C":"Restore a snapshot to a new RDS DB instance and add the DB instance as a replica to the original database.","B":"Set the backup retention period to 0, add the read replica, and set the backup retention period to 7 days again.","A":"Turn off the Multi-AZ feature, add the read replica, and turn Multi-AZ back on again.","D":"Add the new read replica without making any other changes to the RDS database."},"answer":"D"},{"id":"Fd2QcOXbBVX6gJais4Os","answer":"C","question_id":230,"answer_images":[],"unix_timestamp":1679566980,"answer_ET":"C","question_text":"A company is using a 1 TB Amazon RDS for PostgreSQL DB instance to store user data. During a security review, a security engineer sees that the DB instance is not encrypted at rest.\n\nHow should a database specialist correct this issue with the LEAST amount of downtime and no data loss?","url":"https://www.examtopics.com/discussions/amazon/view/103654-exam-aws-certified-database-specialty-topic-1-question-305/","discussion":[{"comment_id":"1122319","poster":"MultiAZ","upvote_count":"1","content":"C gives the LEAST downtime\nB is easier and less risky, but requires hours of downtime","timestamp":"1705215540.0"},{"content":"Selected Answer: C\nC. use AWS Database Migration Service (AWS DMS)\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\n\n\" This pattern uses the AWS Database Migration Service (AWS DMS) to migrate and continuously replicate the data so that the cutover to the new, encrypted database can be done with minimal downtime. \"","upvote_count":"2","timestamp":"1694110800.0","poster":"Pranava_GCP","comment_id":"1001830"},{"comment_id":"955657","content":"It's C","timestamp":"1689699180.0","poster":"Windy","upvote_count":"1"},{"content":"Selected Answer: C\nC is right.","poster":"Kodoma","upvote_count":"1","comment_id":"909514","timestamp":"1685374020.0"},{"content":"D seems right choice considering 1 TB size and LEAST downtime req. Without these specific hints, snapshot is good (option B).","upvote_count":"1","poster":"tsk9921","comment_id":"895078","timestamp":"1683810060.0","comments":[{"comment_id":"895079","timestamp":"1683810120.0","poster":"tsk9921","upvote_count":"1","content":"I meant to say C (not D...sorry for fat finger) is right."}]},{"upvote_count":"3","timestamp":"1681689540.0","poster":"SeemaDataReader","comment_id":"872232","content":"This pattern uses the AWS Database Migration Service (AWS DMS) to migrate and continuously replicate the data so that the cutover to the new, encrypted database can be done with minimal downtime. \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html"},{"poster":"rdiaz","timestamp":"1679843760.0","upvote_count":"2","comment_id":"851195","content":"Selected Answer: C\nNo downtime. Dms","comments":[{"comment_id":"861461","timestamp":"1680637740.0","content":"The question requirement is \"with the LEAST amount of downtime and no data loss\".\nThus I will choose C.\nOr B could work, but DOWN TIME is needed.","upvote_count":"2","poster":"Mintwater"}]},{"comment_id":"848054","content":"1 TB Amazon RDS for PostgreSQL DB would take ages with B. So I would do it using DMS. Therefore, C.","upvote_count":"1","timestamp":"1679566980.0","poster":"openkg"}],"answers_community":["C (100%)"],"answer_description":"","question_images":[],"timestamp":"2023-03-23 11:23:00","choices":{"D":"Create an encrypted read replica. Once the read replica is in sync, promote it to primary. Modify the application to connect to the new primary instance.","C":"Create a new encrypted DB instance and use AWS Database Migration Service (AWS DMS) to migrate the existing database to the encrypted DB instance. Once the instances are in sync, modify the application to connect to the new DB instance.","A":"Modify the DB instance by using the RDS management console, and enable encryption. Apply the changes immediately.","B":"Create a manual DB instance snapshot and then create an encrypted copy of that snapshot. Use this snapshot to create a new encrypted DB instance. Modify the application to connect to the new DB instance."},"isMC":true,"exam_id":22,"topic":"1"}],"exam":{"numberOfQuestions":359,"id":22,"isBeta":false,"name":"AWS Certified Database - Specialty","isMCOnly":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","isImplemented":true},"currentPage":46},"__N_SSP":true}