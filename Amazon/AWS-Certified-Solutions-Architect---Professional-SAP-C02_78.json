{"pageProps":{"questions":[{"id":"70hzm7qFGDcn7qp2UmEy","answer_ET":"B","choices":{"D":"Create an AWS Site-to-Site VPN connection in the shared services account. Configure networking. Use AWS Marketplace VPN software in each development account to connect to the Site-to-Site VPN connection.","B":"Create a transit gateway in the shared services account. Create an AWS Resource Access Manager (AWS RAM) resource share for the transit gateway. Share the transit gateway with all the development accounts. Instruct the developers to accept the resource share. Configure networking.","A":"Create an AWS Resource Access Manager (AWS RAM) resource share for the DB cluster. Share the DB cluster with all the development accounts.","C":"Create an Application Load Balancer (ALB) that points to the IP address of the DB cluster. Create an AWS PrivateLink endpoint service that uses the ALB. Add permissions to allow each development account to connect to the endpoint service."},"question_text":"A company uses AWS Organizations to manage its development environment. Each development team at the company has its own AWS account. Each account has a single VPC and CIDR blocks that do not overlap.\n\nThe company has an Amazon Aurora DB cluster in a shared services account. All the development teams need to work with live data from the DB cluster.\n\nWhich solution will provide the required connectivity to the DB cluster with the LEAST operational overhead?","answers_community":["B (82%)","Other"],"answer":"B","question_images":[],"isMC":true,"answer_images":[],"discussion":[{"timestamp":"1712526720.0","content":"Selected Answer: B\nThe question asks about working with live data and providing CONNECTIVITY to the DB cluster. B is the correct as it provides both","poster":"matheusrdo","upvote_count":"10","comment_id":"1191225"},{"timestamp":"1712448900.0","content":"Selected Answer: B\nB\nI originally chose A since I thoughtAurora DB cluster is sharable\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-aur\nBut as Verri mentioned, with that share, it only allow you to CLONE the db rather than use it as live\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html#Aurora.Managing.Clone.Cross-Account","upvote_count":"8","comment_id":"1190676","poster":"pangchn"},{"content":"Creating a transit gateway (Option B) can be an effective way to provide connectivity to your Amazon Aurora DB cluster while minimizing operational overhead.","poster":"AzureDP900","upvote_count":"1","timestamp":"1731290040.0","comment_id":"1309779"},{"timestamp":"1720411200.0","poster":"vip2","upvote_count":"1","content":"Selected Answer: B\nfor live data, it should be B","comment_id":"1244068"},{"upvote_count":"1","timestamp":"1715882220.0","content":"Selected Answer: A\nFor me it's A.\nWe need to use the RAM only for the Aurora DB. We don't need to peer the VPCs with TransitGateway. Also less ops effort is option A. So Option B is unuseful complicated.","poster":"red_panda","comment_id":"1212535"},{"upvote_count":"3","content":"Selected Answer: B\nCorrect ans \"B\".","comment_id":"1200892","timestamp":"1713894840.0","poster":"titi_r"},{"timestamp":"1712015400.0","comment_id":"1187723","poster":"spencer_sharp","content":"Selected Answer: A\nSeemed A since B requires a lot setup work","upvote_count":"1"},{"timestamp":"1711728900.0","comments":[{"comment_id":"1242814","timestamp":"1720187640.0","upvote_count":"1","content":"Live data is catch here, A is for clone","poster":"c22ddd8"}],"upvote_count":"1","comment_id":"1185570","content":"Selected Answer: A\nLEAST operational overhead is \"A\".\nYou can share DB Cluster. https://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-aur","poster":"mav3r1ck"},{"comment_id":"1182074","upvote_count":"7","poster":"VerRi","timestamp":"1711331040.0","content":"Selected Answer: B\nA: Sharing DB cluster with RAM allows you to CLONE a shared, centrally managed DB cluster\nC: PrivateLink needs NLB not ALB\nD: WTF"},{"timestamp":"1711225620.0","poster":"pangchn","comments":[{"content":"Live data is catch here, A is for clone","timestamp":"1720187700.0","comment_id":"1242815","upvote_count":"1","poster":"c22ddd8"}],"comment_id":"1181136","upvote_count":"1","content":"Selected Answer: A\nI will go for A as the ref link provided by JOKERO \nif not, the transit gateway would be ideal too."},{"timestamp":"1710964680.0","upvote_count":"5","content":"Selected Answer: B\nC is wrong because for Private Link you need to use NLB not ALB.\n\nCorrect answer is B.","comment_id":"1178652","poster":"gustori99"},{"upvote_count":"4","comment_id":"1178441","timestamp":"1710948360.0","poster":"JOKERO","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-aur"},{"poster":"txxxxxf","content":"Selected Answer: B\nAWS PrivateLink requires an NLB (Network Load Balancer). Since the question mentions that IP addresses should not overlap, sharing via Transit Gateway might be a good approach.","timestamp":"1710895020.0","upvote_count":"6","comment_id":"1177845"},{"content":"Selected Answer: C\nUtilizing AWS PrivateLink to enable private connectivity between VPCs without the need for public IP addresses or internet gateways. Creating an ALB pointing to the DB cluster's IP address and then creating a PrivateLink endpoint service that uses the ALB allows each development account to securely connect to the DB cluster. This approach minimizes operational overhead and simplifies network connectivity.","comment_id":"1176977","poster":"CMMC","timestamp":"1710821940.0","upvote_count":"1"}],"question_id":386,"timestamp":"2024-03-19 05:19:00","exam_id":33,"unix_timestamp":1710821940,"url":"https://www.examtopics.com/discussions/amazon/view/136551-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","topic":"1"},{"id":"5VNNxFHAsjOkDFcKtMIM","timestamp":"2024-03-19 05:26:00","exam_id":33,"choices":{"D":"Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of AWS services. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance.","A":"Turn on billing alerts. Use AWS Cost Explorer to determine the costs for the past month. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached.","B":"Turn on billing alerts. Use AWS Cost Explorer to determine the average monthly costs for the past 3 months. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached.","C":"Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of Linked account. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance."},"answer_description":"","discussion":[{"upvote_count":"13","content":"Selected Answer: D\nAns \"D\" - more granular.\n\nQ: What is the difference between a linked account monitor in a payer account, and a services monitor in a linked account?\n\nA linked account monitor in a payer account will monitor the spend of all services, in total, for that linked account.\nA services monitor in a linked account will monitor the individual spend for each service for that linked account.\nFor example, if there is a spike in S3 spending, but a dip in EC2 spending of the same amount (net neutral change), the linked account monitor in the payer account will not detect this because it is monitoring the total account spend across all services. However, the services monitor in the linked account would detect the S3 spike since it is monitoring each service spend individually.","comment_id":"1200902","comments":[{"poster":"sarlos","comment_id":"1213110","timestamp":"1715999820.0","content":"Thanks for the explanation","upvote_count":"2"},{"content":"https://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/faqs/#:~:text=What%20is%20the%20difference%20between%20a%20linked%20account%20monitor%20in%20a%20payer%20account,%20and%20a%20services%20monitor%20in%20a%20linked%20account","poster":"titi_r","timestamp":"1713895980.0","upvote_count":"1","comment_id":"1200903"}],"timestamp":"1713895980.0","poster":"titi_r"},{"upvote_count":"5","comment_id":"1178569","poster":"Dgix","content":"Selected Answer: D\nOn reconsideration: D, as it deals with the individual services in an account, not just the total cost.","timestamp":"1710957360.0"},{"content":"Selected Answer: D\nBy creating a cost monitor with the monitor type set to \"AWS services,\" Cost Anomaly Detection will monitor the individual spend for each service within the linked account. This would allow the company to detect anomalies or spikes in spending for specific services, even if there is a corresponding decrease in another service that offsets the overall account spend.\n\nGiven the requirement to identify resources that cause an increase in cost, monitoring at the service level would provide more granular visibility and enable the company to pinpoint the specific services or resources responsible for cost increases.\n\nCreating a subscription to send daily AWS cost summaries to the operations team and specifying a threshold for cost variance would ensure that the team is notified when the cost increase for any individual service exceeds the defined threshold.","poster":"0b43291","timestamp":"1731720480.0","comment_id":"1312875","upvote_count":"1"},{"comment_id":"1309772","content":"Option D:\nUsing AWS Cost Anomaly Detection to create a cost monitor with a monitor type of AWS services: This option focuses on individual AWS services, making it easier to identify which specific resource is causing the increase in costs.\nThis approach provides detailed insights into each service's contribution to your company's overall expenses, making it easier to pinpoint the issue and take corrective action.\nConsidering all options, Option D seems to be the most suitable solution for identifying resources that cause an increase in costs and automatically notifying the company's operations team. This approach offers real-time monitoring, detailed insights into individual services' contributions, and automatic notifications when costs exceed specified thresholds.","timestamp":"1731289560.0","upvote_count":"1","poster":"AzureDP900"},{"upvote_count":"1","comment_id":"1296257","timestamp":"1728683040.0","poster":"JoeTromundo","content":"Selected Answer: D\nFor those who think the correct answer is C: A Linked Account monitor detects anomalies at the account level. While it can identify which account has unusual spending, it does NOT pinpoint the SPECIFIC SERVICES or RESOURCES causing the increase, as the statement requires."},{"comment_id":"1266084","upvote_count":"1","content":"Selected Answer: D\n\"identify resources that cause an increase in cost \"\nD for sure","timestamp":"1723680540.0","poster":"kgpoj"},{"timestamp":"1722108900.0","comment_id":"1256442","content":"Selected Answer: C\nMonitoring at the AWS service level can be useful, but it may not provide the same comprehensive view of costs across accounts as the linked account monitor type.\nTherefore, option C provides a more adaptive and comprehensive solution for detecting cost anomalies and notifying the operations team.","upvote_count":"1","poster":"salekali01"},{"poster":"seetpt","content":"Selected Answer: D\nD for me","comment_id":"1206057","upvote_count":"1","timestamp":"1714734000.0"},{"comment_id":"1197099","upvote_count":"1","poster":"tushar321","timestamp":"1713343320.0","content":"D. \nAn AWS service monitor will be applicable to all customers since it tracks and detects anomalies across any service they deploy\nhttps://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/faqs/#:~:text=An%20AWS%20service%20monitor%20will%20be%20applicable%20to%20all%20customers%20since%20it%20tracks%20and%20detects%20anomalies%20across%20any%20service%20they%20deploy"},{"poster":"thotwielder","upvote_count":"3","comment_id":"1191406","content":"Selected Answer: D\nc: identify abnormal accounts\nd: identify abnormal service, which is desired.","timestamp":"1712555280.0"},{"content":"Selected Answer: D\nvote D here\nA linked account monitor can track up to 10 different linked accounts. A linked account monitor tracks spending aggregated across all of the designated linked accounts. For example, if a linked account monitor tracks Account A and Account B, and then Account A’s usage spikes while Account B’s usage dips by the same amount, there will be no anomaly detected because it is a net neutral change.\nref\nhttps://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/faqs/","poster":"pangchn","comment_id":"1190677","upvote_count":"3","timestamp":"1712449320.0"},{"upvote_count":"1","timestamp":"1711764600.0","comment_id":"1185788","poster":"steed47","content":"Selected Answer: C\nC more granular"},{"timestamp":"1711553580.0","poster":"TonytheTiger","comment_id":"1184206","content":"Selected Answer: C\nOption C and Not Option D : Linked account - This monitor evaluates the total spend of an individual, or group of, member accounts. If your Organizations need to segment spend by team, product, services, or environment, this monitor is useful. The maximum number of member accounts that you can select for each monitor is 10.\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/getting-started-ad.html#monitor-type-def","upvote_count":"1"},{"poster":"VerRi","timestamp":"1711332960.0","comment_id":"1182095","content":"Selected Answer: C\nI will go with C because the scenario says, \"to create all new infrastructure in its AWS member accounts.\"","upvote_count":"1"},{"comment_id":"1181140","poster":"pangchn","timestamp":"1711226400.0","upvote_count":"4","content":"Selected Answer: D\nD seems more granular to detect thich resource in which account generated the bill.\nC seem only care about the balance across accounts as below\n\"linked account monitor can track up to 10 different linked accounts. A linked account monitor tracks spending aggregated across all of the designated linked accounts. For example, if a linked account monitor tracks Account A and Account B, and then Account A’s usage spikes while Account B’s usage dips by the same amount, there will be no anomaly detected because it is a net neutral change\" \nhttps://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/faqs/"},{"comment_id":"1178650","content":"Selected Answer: D\nAnswer is between C and D.\n\nChoosing monitor type AWS Services is more appropriate than Linked Account because the monitor AWS Services monitors all resources including resources from all member accounts of the organization. Also with Linked Accounts you can only add max 10 accounts to a single monitor. Therefore answer D is correct.","poster":"gustori99","upvote_count":"3","timestamp":"1710964560.0"},{"comment_id":"1178567","poster":"Dgix","timestamp":"1710957180.0","content":"Selected Answer: C\nC is the correct answer.\nD is not granular enough.","upvote_count":"2"},{"timestamp":"1710822360.0","poster":"CMMC","upvote_count":"3","content":"Selected Answer: C\nAWS Cost Anomaly Detection is specifically designed to detect unusual spending patterns or anomalies in AWS costs. By creating a cost monitor with a monitor type of Linked account, the solution focuses on the entire AWS account's spending, which is suitable for identifying unexpected increases in costs due to unused resources. Setting a threshold for cost variance allows the operations team to receive notifications when there are significant deviations from the expected spending pattern.","comment_id":"1176980"}],"topic":"1","answers_community":["D (80%)","C (20%)"],"url":"https://www.examtopics.com/discussions/amazon/view/136552-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"D","answer":"D","question_text":"A company used AWS CloudFormation to create all new infrastructure in its AWS member accounts. The resources rarely change and are properly sized for the expected load. The monthly AWS bill is consistent.\n\nOccasionally, a developer creates a new resource for testing and forgets to remove the resource when the test is complete. Most of these tests last a few days before the resources are no longer needed.\n\nThe company wants to automate the process of finding unused resources. A solutions architect needs to design a solution that determines whether the cost in the AWS bill is increasing. The solution must help identify resources that cause an increase in cost and must automatically notify the company's operations team.\n\nWhich solution will meet these requirements?","unix_timestamp":1710822360,"question_images":[],"question_id":387,"answer_images":[],"isMC":true},{"id":"saKHFTYHAICO0ATGZGMl","answer_images":[],"answer":"A","topic":"1","question_text":"A company is deploying a new web-based application and needs a storage solution for the Linux application servers. The company wants to create a single location for updates to application data for all instances. The active dataset will be up to 100 GB in size. A solutions architect has determined that peak operations will occur for 3 hours daily and will require a total of 225 MiBps of read throughput.\n\nThe solutions architect must design a Multi-AZ solution that makes a copy of the data available in another AWS Region for disaster recovery (DR). The DR copy has an RPO of less than 1 hour.\n\nWhich solution will meet these requirements?","question_images":[],"isMC":true,"exam_id":33,"choices":{"A":"Deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region.","D":"Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region. Create an AWS DataSync scheduled task to replicate the data from the production file system to the DR file system every 10 minutes.","B":"Deploy a new Amazon FSx for Lustre file system. Configure Bursting Throughput mode for the file system. Use AWS Backup to back up the file system to the DR Region.","C":"Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225 MiBps of throughput. Enable Multi-Attach for the EBS volume. Use AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region."},"timestamp":"2024-03-19 05:33:00","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/136553-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"timestamp":"1715279460.0","upvote_count":"20","comment_id":"1209037","poster":"e4bc18e","comments":[{"content":"I agree with your explanation, I will go with A","comment_id":"1309771","poster":"AzureDP900","upvote_count":"1","timestamp":"1731289260.0"}],"content":"So practically everyone here is wrong. because it is A. Here is why B is wrong because one there is no such thing as bursting mode for Lustre that is an EFS thing, but also Backup will not work for the RPO. C is wrong obviously because GP3 can't be shared. D is wrong because Datasync tasks cannot be scheduled for any more frequent then hourly so no D is wrong because you cannot schedule data sync tasks less then hourly so you don't meet the RPO. So all of those are easily wrong because they have bad information. They fooled everyone on A because all they say is the 'Active working set is 100GB\" not the entire filesystem. EFS accumulates bursting credits so for every 100GB of filesystem size you can burst up to 300MiBps for up to 72 minutes. So you provision 75MiBps because that would average out over time so you aren't being overcharged for the provisioned size."},{"comment_id":"1244060","content":"Selected Answer: A\nA scheduled task runs at a frequency that you specify, with a minimum interval of 1 hour.\n\nhttps://docs.aws.amazon.com/datasync/latest/userguide/task-scheduling.html","poster":"vip2","timestamp":"1720409460.0","upvote_count":"3"},{"timestamp":"1719679260.0","upvote_count":"4","poster":"Helpnosense","comment_id":"1239355","content":"Selected Answer: A\nA. EFS support cross region replication. e4bc18e already point why D is wrong."},{"upvote_count":"4","content":"Selected Answer: A\nbig thank to e4bc18e","poster":"trungtd","timestamp":"1718232060.0","comment_id":"1229529"},{"upvote_count":"3","comment_id":"1214435","timestamp":"1716215940.0","content":"Selected Answer: A\nA\nSolution write by e4bc18e","poster":"Zas1"},{"upvote_count":"2","timestamp":"1713900480.0","poster":"titi_r","content":"Selected Answer: D\nD is correct.\n\n\"You can use DataSync to transfer files between two FSx for OpenZFS file systems, and also move data to a file system in a different AWS Region or AWS account. You can also use DataSync with FSx for OpenZFS file systems for other tasks. For example, you can perform one-time data migrations, periodically ingest data for distributed workloads, and schedule replication for data protection and recovery.\"\nhttps://docs.aws.amazon.com/fsx/latest/OpenZFSGuide/migrate-files-to-fsx-datasync.html","comment_id":"1200941","comments":[{"content":"This is wrong a Datasync task cannot be schedule for any more frequent then one hour so the under 1 hour RPO is not met.","upvote_count":"2","comment_id":"1209032","poster":"e4bc18e","timestamp":"1715278980.0","comments":[{"poster":"titi_r","comment_id":"1213275","timestamp":"1716031380.0","content":"@e4bc18e, it seems you are right. Indeed, DataSync can go as granular as 1 hour.\nFound this:\n\"If the file system’s baseline throughput exceeds the Provisioned throughput amount, then it automatically uses the Bursting throughput...\"\nFor 1 TiB of metered data in Standard storage, it can burst to 300 MiBps read-only for 12 hours per day.\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html#throughput-modes","upvote_count":"1"}]}]},{"comment_id":"1190968","comments":[{"timestamp":"1713900300.0","comment_id":"1200938","poster":"titi_r","content":"“B” is wrong because with AWS Backup you can do a backup as frequent as 1 hour, but the RPO must be less than 1 hour.\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/creating-a-backup-plan.html#create-backup-plan-console","upvote_count":"1"}],"upvote_count":"1","timestamp":"1712496360.0","poster":"ovladan","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html#fsx-aggregate-perf"},{"content":"D: \nThe throughput is related to size of the EFS, but the question said the active set of the data will be only up to 100GB, with that size, the throughout will be lower than requested.\nso D:","comment_id":"1184784","upvote_count":"1","poster":"adelynllllllllll","timestamp":"1711632240.0"},{"content":"Selected Answer: A\nD involves managing separate file systems that do not natively offer a \"single location\" experience across regions without additional configuration and replication mechanisms.","upvote_count":"3","poster":"VerRi","comment_id":"1182122","timestamp":"1711335000.0"},{"upvote_count":"4","timestamp":"1711229100.0","content":"Selected Answer: D\nD\n\na sneaky question since my first impression is go for A but it is wrong due to the 75M throughput mode. What's the calculation here? one region has 3 AZ? so 75x3=225?. EFS is not provisioned in that way. Even that, the 225 is the total throughput where question asked 225 for read. Implied the total would be more like 225+XXX. Anyway, A is wrong.\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nC is wrong since EBS multi attach don't support gp3\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-volumes-multi.html","poster":"pangchn","comments":[{"poster":"pangchn","comment_id":"1181161","upvote_count":"3","content":"B is wrong where the hourly AWS backup job won't meet the RPO requirement (less than 1 hour)\n\nThe backup frequency determines how often AWS Backup creates a snapshot backup. Using the console, you can choose a frequency of every hour, 12 hours, daily, weekly, or monthly. You can also create a cron expression that creates snapshot backups as frequently as hourly. Using the AWS Backup CLI, you can schedule snapshot backups as frequently as hourly\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/creating-a-backup-plan.html","timestamp":"1711229220.0"}],"comment_id":"1181160"},{"comment_id":"1178574","timestamp":"1710957720.0","poster":"Dgix","content":"Selected Answer: D\nD is the answer. A would also have worked.","upvote_count":"1"},{"comments":[{"content":"Wrong Datasync tasks cannot be scheduled to be more frequent then hourly, so you cannot schedule data sync tasks to be every 10 Minutes. Apparently everyone is forgetting about burst credits for EFS. Probably something a little missing but it only says the \"Active working set\" is 100GB\" not the entire filesystem. For every 100GB of data of provisioned EFS space you can burst to 300MiBps for 72 minutes.","timestamp":"1715279160.0","upvote_count":"1","comment_id":"1209034","poster":"e4bc18e"}],"upvote_count":"1","comment_id":"1176983","content":"Selected Answer: D\nAmazon FSx for OpenZFS is a fully managed file system service that supports native replication between regions, making it well-suited for DR scenarios with a low RPO requirement. Using AWS DataSync for replication every 10 minutes ensures that the DR copy stays up to date with minimal data loss. This solution provides the required read throughput, data replication, and DR capabilities with less operational overhead.","timestamp":"1710822780.0","poster":"CMMC"}],"answers_community":["A (68%)","D (32%)"],"answer_description":"","unix_timestamp":1710822780,"question_id":388},{"id":"1M6NX3Fi5IPZpLSnjCd3","answer_description":"","unix_timestamp":1710823740,"question_text":"A company needs to gather data from an experiment in a remote location that does not have internet connectivity. During the experiment, sensors that are connected to a local network will generate 6 TB of data in a proprietary format over the course of 1 week. The sensors can be configured to upload their data files to an FTP server periodically, but the sensors do not have their own FTP server. The sensors also do not support other protocols. The company needs to collect the data centrally and move the data to object storage in the AWS Cloud as soon as possible after the experiment.\n\nWhich solution will meet these requirements?","topic":"1","question_id":389,"discussion":[{"timestamp":"1731288780.0","comment_id":"1309768","poster":"AzureDP900","content":"D sounds good to me.Option D proposes using an AWS Snowcone device:\n\n\n\nConfiguring it to use Amazon FSx.\n\nConfiguring the sensors to upload data to the device.\n\nSetting up AWS DataSync on the device to synchronize the uploaded data with an Amazon S3 bucket.\nThis approach allows for seamless data collection, synchronization, and loading into Amazon S3 without requiring additional configuration or latency introduced by EC2 instances or shell scripts.\nTherefore, Option D is the most suitable choice given the requirements.","upvote_count":"1"},{"comment_id":"1229531","poster":"trungtd","content":"Selected Answer: C\nSince the sensors only support FTP for data upload, installing and configuring an FTP server on the EC2 instance is essential. This setup allows the sensors to periodically upload their data files to the Snowcone device.","upvote_count":"2","timestamp":"1718232540.0"},{"content":"Snowcone is specialized for huge data migration.","comment_id":"1213114","timestamp":"1716001020.0","poster":"sarlos","upvote_count":"2"},{"poster":"VerRi","timestamp":"1711855740.0","upvote_count":"2","content":"Selected Answer: C\nSnowcone edge computing + FTP data transfer","comment_id":"1186563"},{"timestamp":"1711335360.0","upvote_count":"2","content":"Selected Answer: C\nC, because of FTP","comment_id":"1182130","poster":"VerRi"},{"upvote_count":"1","content":"Selected Answer: C\nagree on C, since need FTP server which is the only supported method. AWS snowball seems support EC2 too, but not in any answer","poster":"pangchn","comment_id":"1181163","timestamp":"1711229640.0"},{"comment_id":"1178576","poster":"Dgix","content":"Selected Answer: C\nC, for FTP.","timestamp":"1710957960.0","upvote_count":"1"},{"timestamp":"1710924900.0","poster":"djangoUnchained","content":"Selected Answer: C\nC is the only one which uses FTP","comment_id":"1178055","upvote_count":"2"},{"content":"Selected Answer: C\nSensors only support FTP protocol. Leverage the native capabilities of Snowcone and EC2, providing an efficient method for collecting data.","comment_id":"1176990","poster":"CMMC","timestamp":"1710823740.0","upvote_count":"2"}],"choices":{"A":"Order an AWS Snowball Edge Compute Optimized device. Connect the device to the local network. Configure AWS DataSync with a target bucket name, and unload the data over NFS to the device. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3.","B":"Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Create a shell script that periodically downloads data from each sensor. After the experiment, return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume.","C":"Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Install and configure an FTP server on the EC2 instance. Configure the sensors to upload data to the EC2 instance. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3.","D":"Order an AWS Snowcone device. Connect the device to the local network. Configure the device to use Amazon FSx. Configure the sensors to upload data to the device. Configure AWS DataSync on the device to synchronize the uploaded data with an Amazon S3 bucket. Return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume."},"answer":"C","timestamp":"2024-03-19 05:49:00","url":"https://www.examtopics.com/discussions/amazon/view/136558-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"question_images":[],"answer_ET":"C","exam_id":33,"answer_images":[],"answers_community":["C (100%)"]},{"id":"3XSXVc0dhiOU8J5AmjDf","exam_id":33,"isMC":true,"answer_ET":"B","answers_community":["B (72%)","A (18%)","10%"],"url":"https://www.examtopics.com/discussions/amazon/view/95093-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":390,"timestamp":"2023-01-13 22:24:00","unix_timestamp":1673645040,"topic":"1","question_images":[],"answer_description":"","discussion":[{"content":"Selected Answer: B\nB. Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint. This solution will provide low operational overhead as it utilizes the serverless capabilities of AWS Lambda and API Gateway, which automatically scales and manages the underlying infrastructure and resources. It also allows for the webhook logic to be easily managed and updated through the API Gateway interface.\n\nThe answer should be B because it is the best solution in terms of operational overhead.","poster":"masetromain","comments":[{"comments":[{"poster":"hobokabobo","timestamp":"1677375780.0","comments":[{"upvote_count":"2","comment_id":"821991","poster":"hobokabobo","timestamp":"1677376200.0","content":"As addition why B is still better: it hides the implementation details and decouples by introducing a interface.\nWith that a team for Aws may change what ever it needs to change to implement the interface. On the other hand on git side can use whatever deems necessary without caring about implementation details."}],"content":"I do agree with B. \n\nHowever on Git server side it does make no difference if one calls aws or do a rest call via gateway. \nEg. if you use Python it makes no difference if you use boto(call Lambda) or request(rest api) module.\nIf one implemets via shell it makes no difference if one uses aws-cli(invoke Lambda directly) or curl(do a rest call).\nSimilar for other implementations.","comment_id":"821979","upvote_count":"2"}],"comment_id":"774869","upvote_count":"8","poster":"masetromain","timestamp":"1673645040.0","content":"Option A would require updating the Git servers to call individual Lambda function URLs for each webhook, which would be more complex and time-consuming than calling a single API Gateway endpoint. \n\nOption C would require deploying the webhook logic to AWS App Runner, which would also be more complex and time-consuming than using an API Gateway. \n\nOption D would also require containerizing the webhook logic and creating an ECS cluster and Fargate, which would also add complexity and operational overhead compared to using an API Gateway."}],"comment_id":"774868","timestamp":"1673645040.0","upvote_count":"24"},{"timestamp":"1703579520.0","poster":"ninomfr64","upvote_count":"6","comments":[{"content":"Option A requires that you update the webhooks for each lambda function. This will create a considerable operational overhead not just for the initial change but going forward as well.\n\nAPI Gateway (B) decouple the functions from the Webhooks.","comment_id":"1214994","upvote_count":"2","poster":"Malcnorth59","timestamp":"1716299580.0"}],"comment_id":"1105831","content":"Selected Answer: A\nI need help here: what's wrong with Lambda Function URL?\n\nWith A I just need to handle my Lambda functions, updates go trough updating my aliases pointing to a new version. Here I am just missing all the capabilities provided by API Gateway that seems not to be requested (transformations, throttling, quotas, cache, api keys, auth, OpenAPI, ...). With B I still need to implement each webhook logic in a separate AWS Lambda function and update git server + I need to operates API Gateway.\n\nAny other option requires 2 or more services thus generating more operations, also:\nNot C as app runner is not a target for ALB (private IP, ECS, EC2 instance, Lambda)\nNot D as you cannot set Fargate as API Gateway target (while you can use ECS as target)\n\nCan you help me understand why B requires less operations overhead?"},{"poster":"glf","timestamp":"1737100860.0","upvote_count":"1","content":"Selected Answer: A\nB is plain wrong. With an HTTP API Gateway you're publishing your API on the Internet, which could be forbidden by security policy. Even if it is not, you at least need to setup authentication, with OAuth2 or custom Lambda authorizer. And in this case, you lose the advantage in terms of operational overhead.","comment_id":"1342041"},{"timestamp":"1727684460.0","upvote_count":"1","poster":"GabrielShiao","content":"Selected Answer: C\nDeploy the web hook logic to the Apprunner which takes a minor effort to build and deploy the container image automatically without underlying infrastructure management.","comment_id":"1291472"},{"poster":"amministrazione","comment_id":"1275493","timestamp":"1725094380.0","content":"B. Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.","upvote_count":"1"},{"timestamp":"1724473680.0","content":"C is the best one . Operational over head - exisiting web logic needs to be change into the lambda. But in C - just we can use the same logic just deployment activities. Please go with C","comment_id":"1271521","poster":"subbupro","upvote_count":"1"},{"content":"Selected Answer: B\nA: large operational overhead\nB: Choice\nC: App runner doesn't use ALB\nD: Unnecessary complexity with containers","poster":"Malcnorth59","timestamp":"1716299760.0","comment_id":"1214998","upvote_count":"2"},{"comments":[{"poster":"Fu7ed","timestamp":"1715318040.0","comment_id":"1209206","content":"choose B","upvote_count":"1"}],"timestamp":"1715317980.0","upvote_count":"2","poster":"Fu7ed","comment_id":"1209204","content":"https://aws.amazon.com/ko/solutions/implementations/git-to-s3-using-webhooks/"},{"comment_id":"1175601","upvote_count":"1","timestamp":"1710653640.0","content":"Selected Answer: B\nGiven the current answers, I think B is the only possible option with least overhead.\n\nC would have been a better candidate over B, if it mentioned to include the App Runner in a Target Group TG and assign TG as the target for the API Gateway. As it stands now, C is not correct because App Runner app can't be directly assigned as a target for API Gateway.","poster":"kz407"},{"content":"Selected Answer: A\nA, because is the solution with less operational overhead. Also option B also will create new lambda functions per webhook, and you have to define the specific path in the apigateway and integrate it with your specific lambda...","poster":"gofavad926","comment_id":"1175410","timestamp":"1710631800.0","upvote_count":"5"},{"upvote_count":"1","timestamp":"1709764980.0","comment_id":"1167538","content":"Selected Answer: B\nLambda function is the easiest way to implement the webhook logic. App Runner and ECS all requires more ops overhead. \nSo the answer is between A and B. Someone argue that using A introduces ops overhead of mapping every Lambda function to the webhooks, but actually with B, users don’t need to map Lambda function in git webhooks, but move the Lambda function mapping ops to API gateway. The mapping need to be done, that’s an ops overhead that cannot be ignored. \nI’m guessing the question designer prefers to use API GW, because the description “Update the Git servers to call the individual Lambda function URLs.” doesn’t look good. While, in reality, the repo developers create the Lambda function, and they know the URL, it’s very easy to launch the Lambda function from the web hook. No additional API GW is required.","poster":"bjexamprep"},{"timestamp":"1705975080.0","content":"Selected Answer: C\nYou can set App Runner as a target for ALB.\n\nAWS App Runner can use your code. You can use AWS App Runner to create and manage services based on two fundamentally different service sources: source code and source image. App Runner starts, runs, scales, and balances your service regardless of the source type. You can use the CI/CD capability of App Runner to track changes to your source image or code. When App Runner discovers a change, it automatically builds (for source code) and deploys the new version to your App Runner service","upvote_count":"1","comment_id":"1129168","comments":[{"timestamp":"1708953540.0","content":"Looks like App Runner is built more for deploying web applications rather than hosting webhook logics.","comment_id":"1159751","poster":"djeong95","upvote_count":"1"}],"poster":"master9"},{"timestamp":"1703778840.0","comment_id":"1107935","content":"A. is the right answer as no need to introduce gateway here","poster":"uas99","upvote_count":"2"},{"comment_id":"1088635","timestamp":"1701794040.0","upvote_count":"1","content":"Least operations is the key. App runner is a aws managed one and can deploy it easily, A and B we need to create lamda for each web hook it is very complex . So C would be correct","comments":[{"upvote_count":"2","timestamp":"1704023220.0","content":"ninomfr64 says that App runner cannot be a target for ALB, so that's the reason you cannot select C.","comment_id":"1110499","poster":"jpa8300"}],"poster":"subbupro"},{"comment_id":"1069097","comments":[{"timestamp":"1699857660.0","poster":"severlight","upvote_count":"1","content":"UPD: Don't see the exact reasons why A won't work for now, but B will work for sure.","comment_id":"1069099"}],"timestamp":"1699857600.0","upvote_count":"1","content":"Selected Answer: B\nDon't see the exact reasons to not choose A for now, but B will work for sure.","poster":"severlight"},{"timestamp":"1697651040.0","comment_id":"1047114","poster":"whenthan","upvote_count":"1","content":"Selected Answer: B\nreducing operational overhead!"},{"poster":"Andy97229","content":"Selected Answer: C\nB vs C. Looking at App Runner C makes more sense.","timestamp":"1697190540.0","comment_id":"1042533","upvote_count":"1"},{"upvote_count":"1","comments":[{"timestamp":"1699221720.0","comments":[{"timestamp":"1724241300.0","poster":"marcosallanluz","upvote_count":"1","comment_id":"1270072","content":"App Runner tem autoscaling é só configurar no ELB"}],"content":"Watch this video from AWS. At 4:05 he says Apprunner is serverless, and also there are no load balancers. Since the answer mentions load balancers it is incorrect.\n\nhttps://www.youtube.com/watch?v=HJsULvSJWes\n\nI found the video from this AWS post\n\nhttps://aws.amazon.com/blogs/containers/introducing-aws-app-runner/","poster":"SuperDuperPooperScooper","upvote_count":"3","comment_id":"1063324"}],"timestamp":"1695535380.0","poster":"sam_cao","comment_id":"1015513","content":"Selected Answer: C\nThe comments below supported Option B are only focusing on how Lambda + API Gateway can help reduce operational overhead. Thinking of the scenario in the question that we have already had the source code, wouldn't it be easier if we only specify the code repo on App Runner and let it process and finish the task? Implement all logic again would consume a lot more time."},{"upvote_count":"1","poster":"CuteRunRun","content":"Selected Answer: B\nI prefer B","comment_id":"971574","timestamp":"1691110080.0"},{"content":"Selected Answer: B\nAPI GW and Lambda. Here is your architecture: https://aws.amazon.com/solutions/implementations/git-to-s3-using-webhooks/","upvote_count":"5","comment_id":"944063","poster":"SmileyCloud","timestamp":"1688584200.0"},{"timestamp":"1688145960.0","upvote_count":"1","comment_id":"939287","poster":"NikkyDicky","content":"Selected Answer: B\nB makes sense"},{"timestamp":"1686135420.0","poster":"emiliocb4","content":"Selected Answer: C\nto accomplish the least operational requiment i will go with C.\nB seems to be too much disruptive to implement \"each logic\" in a separate lambda","comment_id":"917130","upvote_count":"3","comments":[{"poster":"sam_cao","timestamp":"1695535440.0","comment_id":"1015515","content":"I agree. We don't have any coding if we choose C.","upvote_count":"1"}]},{"poster":"Sarutobi","comment_id":"893868","content":"Interesting that there is no more debate here about option A. I still think B is the way to go because AWS recommends integrating with GitLab with https://aws-quickstart.github.io/quickstart-git2s3/ and that is what we use. But if option A works, it would be the \"LEAST operational overhead.\" I think masetromain talked about it, but I see it differently, basically, it can be a single Lambda function that reads the payload of the webhook to continue the pipeline, basically the same idea but without API-GW in front.","timestamp":"1683718920.0","upvote_count":"1","comments":[{"timestamp":"1691538960.0","content":"Option A works for sure, but managing API gateway is easier than managing function URLs in every single lambda function.","upvote_count":"1","comment_id":"976104","poster":"b3llman"}]},{"timestamp":"1683717060.0","comment_id":"893849","upvote_count":"1","poster":"gameoflove","content":"Selected Answer: B\nB, Is the best option as per the question"},{"comment_id":"867456","poster":"RaghavendraPrakash","content":"I go with C. With the options, we have Lambda and AppRunner. We dont know if that functionality can be repurposed with Lambda. However, the functionality can be deployed with AppRunner with least Operational Overhead.","upvote_count":"4","timestamp":"1681227480.0"},{"comment_id":"840428","content":"Selected Answer: B\nB makes sense ✅","upvote_count":"3","poster":"dev112233xx","timestamp":"1678925100.0"},{"poster":"moota","comment_id":"804868","content":"Selected Answer: B\nHere's what ChatGPT has to say.\nIn general, if you're looking for the option with the least operational overhead and you're comfortable with a fully managed, serverless environment, then AWS Lambda with API Gateway may be the better choice. However, if you require more control over your environment or need to use containers, then AWS App Runner with ALB may be the better option.","upvote_count":"2","timestamp":"1676072580.0"},{"content":"Selected Answer: B\nhttps://aws.amazon.com/solutions/implementations/git-to-s3-using-webhooks/","timestamp":"1674918780.0","comment_id":"790702","poster":"Untamables","upvote_count":"3"},{"comment_id":"780947","timestamp":"1674119700.0","upvote_count":"3","poster":"AjayD123","content":"Selected Answer: B\nApi Gateway with Lambda\nhttps://medium.com/mindorks/building-webhook-is-easy-using-aws-lambda-and-api-gateway-56f5e5c3a596"}],"question_text":"A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud. The company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an Application Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a serverless architecture.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer":"B","choices":{"C":"Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB endpoint.","A":"For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs.","B":"Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.","D":"Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargate as the target. Update the Git servers to call the API Gateway endpoint."},"answer_images":[]}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":529,"name":"AWS Certified Solutions Architect - Professional SAP-C02","provider":"Amazon","id":33,"isImplemented":true,"isBeta":false},"currentPage":78},"__N_SSP":true}