{"pageProps":{"questions":[{"id":"ELmceIqCB9mcYKpKGgGb","question_images":[],"unix_timestamp":1597292760,"topic":"1","answers_community":["D (100%)"],"isMC":true,"question_id":586,"answer_images":[],"choices":{"D":"Migrate mission-critical VMs with AWS SMS. Export the other VMs locally and transfer them to Amazon S3 using AWS Snowball. Use VM Import/Export to import the VMs into Amazon EC2.","C":"Export the VMs locally, beginning with the most mission-critical servers first. Use AWS Transfer for SFTP to securely upload each VM to Amazon S3 after they are exported. Use VM Import/Export to import the VMs into Amazon EC2.","B":"Use AWS Application Discovery Service to assess each application, and determine how to refactor and optimize each using AWS services or AWS Marketplace solutions.","A":"Set up a 1 Gbps AWS Direct Connect connection. Then, provision a private virtual interface, and use AWS Server Migration Service (SMS) to migrate the VMs into Amazon EC2."},"answer_ET":"A","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/28393-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"discussion":[{"content":"D. 40 TB transfer -> snowball. SMS only needed for the mission-critical VMs that would need live incremental replication with no downtime.","upvote_count":"25","poster":"Nemer","comment_id":"156950","timestamp":"1632127440.0"},{"upvote_count":"10","content":"To transfer 40TB of data in 10Mbps link, it will take 400 days. So transferring anything over that link in 3 months is not feasible. Rules C out. \n\nDirect Connect link is needed only while migration period. So ordering that for just 3 months doesn't seem correct. Also it's a costly option. Rules out A.\n\nAnd refactoring 100 applications in 3 months, doesn't sound right to me as well. Rules out B.\n\nSo left will be D. Problem with D is that Snowball transfer also takes some time, but I guess it's OK for non critical systems to be down for week. If we can use the on-prem servers while setupping AWS instances and then transfer only the delta of data, the downtime then will be minimized.","poster":"Justu","comment_id":"268821","timestamp":"1634504880.0"},{"upvote_count":"2","timestamp":"1658966820.0","poster":"hilft","content":"snowball. other options will take forever.","comment_id":"638363"},{"upvote_count":"1","content":"Selected Answer: D\nAWS Server Migration Service is better than VM I/E.","comment_id":"540370","poster":"kyo","timestamp":"1643972700.0"},{"content":"D correct.","upvote_count":"1","poster":"cldy","timestamp":"1641013740.0","comment_id":"514373"},{"poster":"AzureDP900","content":"D is right answer. A is not correct","comment_id":"496346","timestamp":"1638913860.0","upvote_count":"1"},{"poster":"tgv","comment_id":"435743","content":"DDD\n---","upvote_count":"1","timestamp":"1635970320.0"},{"comments":[{"timestamp":"1636125060.0","upvote_count":"4","content":"You did not read the question. \" The on-premises network throughput has reached capacity and would be costly to increase. Cost is a concern, so a 1 Gbps Direct Connect connection would definitely be too expensive.","poster":"Viper57","comment_id":"447144"}],"timestamp":"1635763740.0","upvote_count":"3","content":"I will go with A, firstly, there is no concern about the cost in the topic. customer's purpose is to migrate all service and data to aws in 3 months without any affect for end user.\nFor D, with snowball, the service and old data can be migrate to aws in weeks, but how to process the datas generated during the weeks? only 10Mb network obviously is not the correct solution. Customer sholud find a solution to keep the data consistent.","poster":"levy_k","comment_id":"428964"},{"comment_id":"415329","timestamp":"1635759000.0","upvote_count":"3","content":"I think the solution is D, the key is the cost limitation. There is a reason to put that in the question. The direct connect is the best solution if you don't have a budget problem. With D you can create a copy of the non-essentials VMs, work with the on-premise while the VMs arrive and are deployed in EC2.","poster":"zolthar_z"},{"content":"I'll go with D","upvote_count":"2","poster":"WhyIronMan","comment_id":"413246","timestamp":"1635374700.0"},{"content":"D, since customer is not willing to make additional cost with direct connect. Also not full 40 TB are to be migrated as some of data will offloaded to Snowball device.","comment_id":"406133","poster":"Kopa","upvote_count":"2","timestamp":"1635227700.0"},{"poster":"tvs","timestamp":"1635180540.0","upvote_count":"2","comment_id":"383022","content":"Should be D . SMS transfer vmdk to s3 which Need public virtual interface over DX. https://aws.amazon.com/blogs/apn/aws-server-migration-service-server-migration-to-the-cloud-made-easy/"},{"timestamp":"1635150480.0","upvote_count":"1","comment_id":"357676","content":"D is the solution.","poster":"blackgamer"},{"upvote_count":"2","content":"it's D","poster":"Waiweng","timestamp":"1635124200.0","comment_id":"351867"},{"upvote_count":"2","comment_id":"296572","content":"A for sure, direct connect can be establiched in 1 month and is more indicated for critical vm migration; for D, after VM import, how is feasible to do the data resync of about 3 days of data with 10Mbps full used?","poster":"natpilot","timestamp":"1635034260.0"},{"timestamp":"1634708400.0","comment_id":"272440","upvote_count":"2","content":"D - Assuming mission critical VMs are migrated using SMS via existing link and others via Snowball. All other options don't sound reasonable.","poster":"elf78"},{"upvote_count":"6","poster":"Ebi","comment_id":"268579","content":"A for sure,\nD is not the option, snowball migration takes weeks while non-critical VMs must be available during business hours.","timestamp":"1634245680.0","comments":[{"content":"After further review I guess this questions does not have any correct answer:\nA: Is not correct, as mentioned in other comments, private VIF will work for connecting to VPC not public services like S3\nB: No sense, it is talking discover not actual migration \nC: No sense, no needed to have SFTP\nD: Transferring VMs using Snowball will take weeks while as per question non-critical application are used during business hours \n\nVery bad question","timestamp":"1634716320.0","comment_id":"285391","upvote_count":"4","poster":"Ebi"}]},{"content":"A\nA - If you have bandwidth then within 3 months you can achieve this.\nB - It could be initial step but not the final ans\nC - could be a solution but not with 10Mbps\nD - two parts here (a) mission critical over 10Mbps is not an option (b) snowball takes 1 week whereas question asked for downtime during non-buisness hours only.","comments":[{"content":"agree with this 100%","comment_id":"300582","timestamp":"1635075900.0","poster":"certainly","upvote_count":"2"}],"timestamp":"1633788000.0","poster":"vipgcp","upvote_count":"7","comment_id":"255537"},{"comment_id":"253896","poster":"Bulti","upvote_count":"2","timestamp":"1633614480.0","comments":[{"upvote_count":"3","timestamp":"1633942800.0","comments":[{"timestamp":"1636030080.0","comment_id":"437506","upvote_count":"1","content":"D\nD is better than A because the question doesn't mention that the VMs change daily.","poster":"student22"},{"poster":"Viper57","comment_id":"463615","upvote_count":"1","timestamp":"1636125660.0","content":"Why do the VMs need to be down when they are shipped using snowball? A copy of the VMs can be made and then transferred using Snowball, they do not need to be turned off."}],"content":"I would like to change my answer to A. The reason being for non critical VMs they need to be down not just during off business hours but until the VMs are shipped using snowball and imported into aWS EC2 instances. And that is not what the questions suggests we do. So answer is A.","poster":"Bulti","comment_id":"258777"}],"content":"Answer is D. SMS will provide live incremental replication for mission critical VMs with no downtime. Snowball will be able to transfer 40TB of data."},{"timestamp":"1633404840.0","poster":"T14102020","content":"Correct is A. No sense to interrupt any services for transfer by snowball in D","upvote_count":"1","comment_id":"244765"},{"content":"I'll go with D","upvote_count":"3","poster":"jackdryan","timestamp":"1633240560.0","comment_id":"232467"},{"content":"i will go for D no need for 1 GBs","poster":"oopsy","upvote_count":"1","comment_id":"229559","timestamp":"1632971100.0"},{"timestamp":"1632796200.0","content":"1. some are mission critical, so downtime must be minimized.\n2. migration solution that can be performed within the next 3 months.\nSo I choose A.","upvote_count":"3","comments":[{"comment_id":"246970","timestamp":"1633574820.0","content":"The first step in the internal mechanism of sending AWS SMS is S3. S3 transfers over DX from on-premises must use Public VIF. Therefore, A cannot be the answer.","poster":"superbart","comments":[{"timestamp":"1634713620.0","content":"Under-rated comment.","poster":"rcher","comment_id":"276997","upvote_count":"2"}],"upvote_count":"6"}],"poster":"NNHAN","comment_id":"227708"},{"upvote_count":"1","content":"D, We can not use option A as the budget is not allowing the existing 10Mbps to get increased, how can we ask them to order a direct connect :)","poster":"liono","timestamp":"1632715140.0","comment_id":"211444"},{"comment_id":"208567","poster":"CYL","upvote_count":"1","timestamp":"1632669840.0","content":"D. Since servers can go offline, using Snowball to transfer the VM will be the easiest."},{"timestamp":"1632333240.0","content":"Answer is D it is the most cost effective solution whole meeting the requirements","comments":[{"upvote_count":"2","content":"I can not see it asked for cost effectiveness, fact is company can not use its existing link as itâ€™s already running at max capacity.","comments":[{"poster":"Aj1020","comments":[{"poster":"01037","upvote_count":"1","timestamp":"1633965840.0","content":"The on-premises network throughput has reached capacity and would be costly to increase.\n\nSo cost is a requirement.","comment_id":"266339"}],"timestamp":"1632612900.0","content":"Therefore A is the only solution that can work in the given scenario.","comment_id":"206705","upvote_count":"4"}],"comment_id":"206702","poster":"Aj1020","timestamp":"1632455400.0"}],"upvote_count":"2","comment_id":"205287","poster":"Bulti"},{"poster":"sk2022","timestamp":"1632320520.0","content":"it is B because there are 100 VMs so the application discovery service is the right tool to use for assessment and then the SA will figure out what to do and how to proceed.","upvote_count":"1","comment_id":"185634"},{"poster":"perio","upvote_count":"2","timestamp":"1632317220.0","content":"I think so, too. The answer is D.\nSuggested Answer B is wrong because AWS Application Discovery Service is not needed.","comment_id":"180274"},{"comment_id":"157898","poster":"Anila_Dhharisi","timestamp":"1632226080.0","content":"Yes this can be achived easily by Snowball. its D.","upvote_count":"3"}],"question_text":"A company needs to move its on-premises resources to AWS. The current environment consists of 100 virtual machines (VMs) with a total of 40 TB of storage.\nMost of the VMs can be taken offline because they support functions during business hours only, however, some are mission critical, so downtime must be minimized.\nThe administrator of the on-premises network provisioned 10 Mbps of internet bandwidth for the migration. The on-premises network throughput has reached capacity and would be costly to increase. A solutions architect must design a migration solution that can be performed within the next 3 months.\nWhich method would fulfill these requirements?","timestamp":"2020-08-13 06:26:00","answer_description":""},{"id":"wUbhh98BFnLthzQNUOwx","discussion":[{"upvote_count":"40","content":"ABE. \nExcluding: C does not reduce maintenance (MySQL IaaS), we need CloudFront for WAF (D is out), and F is not HA.","comment_id":"156966","comments":[{"poster":"sam422","timestamp":"1632675720.0","comment_id":"185698","upvote_count":"1","content":"C doesn't qualify for reduce maintenance"}],"poster":"Nemer","timestamp":"1632312300.0"},{"content":"yes its ABE.","comment_id":"157902","timestamp":"1632636780.0","poster":"Anila_Dhharisi","upvote_count":"8"},{"upvote_count":"1","comment_id":"837127","content":"Selected Answer: ABE\nTextbook question","timestamp":"1678632660.0","poster":"milofficial"},{"upvote_count":"1","timestamp":"1658763540.0","poster":"hilft","content":"ABE straight forward","comment_id":"636793"},{"poster":"TechX","comment_id":"627291","upvote_count":"2","timestamp":"1657002120.0","content":"Selected Answer: ABE\nIt's ABE"},{"poster":"aandc","timestamp":"1656826140.0","content":"Selected Answer: ABE\neasy one","comment_id":"626425","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: ABE\nA. high availability and performance for the web servers since they are multi-AZ, auto-scaled and load balanced.\nB. database are multi az and shifted to auroa db cluster so it is reliable and scalable\nE. static website content in S3 and cached in Cloudfront reduces latency . WAF increases security.","comment_id":"586289","poster":"tartarus23","timestamp":"1650019920.0"},{"upvote_count":"1","poster":"Ni_yot","comment_id":"558335","timestamp":"1646080800.0","content":"ABE for me. With this option, the web app and DB are highly available. And the latency and security is covered with the E answer."},{"content":"Selected Answer: ABE\nABE for sure!","upvote_count":"1","poster":"zoliv","comment_id":"533736","timestamp":"1643286900.0"},{"timestamp":"1641985380.0","comment_id":"522064","poster":"pititcu667","content":"Selected Answer: ABE\nThis is my choice.","upvote_count":"1"},{"poster":"weequan","comment_id":"497371","timestamp":"1639029960.0","content":"Selected Answer: BDE\nA need maintenence\nC need maintenence\nF not reliability","comments":[{"poster":"challenger1","comment_id":"504487","timestamp":"1639865520.0","content":"No..... ABE","upvote_count":"1"}],"upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: BDE\nA need maintenence\nC is not meet with reliability\nF not reliability","poster":"weequan","comment_id":"497368","timestamp":"1639029840.0"},{"content":"Selected Answer: ABE\nABE is right answer!","upvote_count":"3","poster":"AzureDP900","comment_id":"491106","timestamp":"1638320400.0"},{"poster":"AzureDP900","comment_id":"491105","upvote_count":"1","timestamp":"1638320340.0","content":"I will go with ABE !"},{"comment_id":"436151","content":"AAA BBB EEE\n---","upvote_count":"1","timestamp":"1635890460.0","poster":"tgv"},{"poster":"WhyIronMan","comment_id":"413248","upvote_count":"2","timestamp":"1635789600.0","content":"I'll go with A,B,E"},{"content":"ABE for sure.","poster":"blackgamer","comment_id":"357859","upvote_count":"1","timestamp":"1635656220.0"},{"upvote_count":"3","timestamp":"1635459840.0","poster":"Waiweng","content":"it's ABE","comment_id":"351870"},{"comment_id":"342079","upvote_count":"1","timestamp":"1635145380.0","content":"ABE. Aurora for reduced maintenance,","poster":"Amitv2706"},{"comment_id":"282440","timestamp":"1635057180.0","upvote_count":"2","poster":"kopper2019","content":"ABE meet all the requirements"},{"comment_id":"268581","timestamp":"1635042900.0","content":"Answer is ABE","poster":"Ebi","upvote_count":"4"},{"poster":"petebear55","timestamp":"1634856780.0","upvote_count":"2","comment_id":"254382","content":"I will go for ABE ......................."},{"timestamp":"1634822940.0","poster":"Bulti","comment_id":"253898","upvote_count":"2","content":"ABE is the answer"},{"content":"Correct ABE. For sure","comment_id":"244767","timestamp":"1634209200.0","upvote_count":"2","poster":"T14102020"},{"timestamp":"1633820940.0","comment_id":"242159","poster":"rscloud","content":"ABE is good fit","upvote_count":"1"},{"content":"I'll go with A,B,E","comment_id":"232472","timestamp":"1633760040.0","poster":"jackdryan","upvote_count":"3"},{"poster":"CYL","content":"ABE. Cloudfront to reduce latency, WAF for security protection against SQL injection and XXS, Aurora for scalable DB.","upvote_count":"2","timestamp":"1632904620.0","comment_id":"208570"},{"comments":[{"content":"not C, confronts with \"Reduce maintenance\".\nDB should be migrated to RDS","upvote_count":"1","comment_id":"457603","timestamp":"1636035780.0","poster":"tonikus"}],"content":"ABC looks good","comment_id":"157704","timestamp":"1632490980.0","upvote_count":"1","poster":"tapjungle"},{"timestamp":"1632452880.0","poster":"uyungdong","comment_id":"157319","content":"ABE is good one i think","upvote_count":"6"}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/28401-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1597293900,"exam_id":32,"answers_community":["ABE (86%)","14%"],"timestamp":"2020-08-13 06:45:00","isMC":true,"answer":"ABE","answer_images":[],"answer_description":"","question_images":[],"question_id":587,"question_text":"A company runs a popular public-facing ecommerce website. Its user base is growing quickly from a local market to a national market. The website is hosted in an on-premises data center with web servers and a MySQL database. The company wants to migrate its workload to AWS. A solutions architect needs to create a solution to:\nâœ‘ Improve security\nâœ‘ Improve reliability\nâœ‘ Improve availability\nâœ‘ Reduce latency\nâœ‘ Reduce maintenance\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)","choices":{"E":"Host static website content in Amazon S3. Use Amazon CloudFront to reduce latency while serving webpages. Use AWS WAF to improve website security.","D":"Host static website content in Amazon S3. Use S3 Transfer Acceleration to reduce latency while serving webpages. Use AWS WAF to improve website security.","F":"Migrate the database to a single-AZ Amazon RDS for MySQL DB instance.","C":"Use Amazon EC2 instances in two Availability Zones to host a highly available MySQL database cluster.","B":"Migrate the database to a Multi-AZ Amazon Aurora MySQL DB cluster.","A":"Use Amazon EC2 instances in two Availability Zones for the web servers in an Auto Scaling group behind an Application Load Balancer."},"answer_ET":"ABE"},{"id":"r0HKMa5B3St5TSO3Vkcj","topic":"1","question_images":[],"discussion":[{"timestamp":"1632945300.0","content":"D. - Docker with awslogs\nE. VM Import/Export + CW logs","poster":"cldy","upvote_count":"7","comment_id":"325802"},{"poster":"amministrazione","comment_id":"1266670","timestamp":"1723753200.0","upvote_count":"1","content":"D. Create a Dockerfile for the application. Create an AWS Elastic Beanstalk application using the Docker platform and the Dockerfile. Enable logging the Docker configuration to automatically publish the application logs. Enable log file rotation to Amazon S3.\nE. Use VM import/Export to import a virtual machine image of the server into AWS as an AMI. Create an Amazon Elastic Compute Cloud (EC2) instance from AMI, and install and configure the Amazon CloudWatch Logs agent. Create a new AMI from the instance. Create an AWS Elastic Beanstalk application using the AMI platform and the new AMI."},{"comment_id":"927782","timestamp":"1687197300.0","upvote_count":"1","content":"Selected Answer: AD\nThe correct options are A and D.\nOption A allows you to create an AWS Elastic Beanstalk application using the custom web server platform. This is a good option if you want to migrate the application without making any changes to the code. You can also enable log file rotation to Amazon S3, which will ensure that the logs are stored in a durable location.\n\nOption D allows you to create a Dockerfile for the application and then create an AWS Elastic Beanstalk application using the Docker platform and the Dockerfile. This is a good option if you want to containerize the application. You can also enable logging the Docker configuration to automatically publish the application logs. This will ensure that the logs are published to Amazon S3, which is a durable location.","comments":[{"upvote_count":"1","poster":"SkyZeroZx","comment_id":"927783","timestamp":"1687197300.0","content":"The other options are not as suitable for this scenario. For example, option B requires you to create an AWS OpsWorks stack, which is more complex and expensive than Elastic Beanstalk. Option C requires you to use Amazon Kinesis, which is a more complex solution than Amazon CloudWatch Logs. Option E requires you to create a new AMI, which is more complex and time-consuming than using Elastic Beanstalk."}],"poster":"SkyZeroZx"},{"comment_id":"870242","upvote_count":"1","content":"now , beanstalk does not work with AMI as Custom Platform.. the Useless discussion..","poster":"alexua","timestamp":"1681481400.0"},{"content":"D & E I believe. I rule out A because you dont have a custom web server platform with Beanstalk. You can use an EC2 for the custom web platform however.","comment_id":"844777","timestamp":"1679309400.0","poster":"CloudHandsOn","upvote_count":"1"},{"poster":"NathanvB99","upvote_count":"1","comment_id":"713870","timestamp":"1667915340.0","content":"DE. A falls of because thereâ€™s a custom platform for AWS Elastic Beanstalk but it works based on an AMI and not on an executable. This is what you do in E."},{"content":"Selected Answer: DE\nDocker with awslogs to S3 . Since ELB Cloudwatch agents doesnt send logs directly to S3, it is not A but E","poster":"Harithareddynn","comment_id":"646546","timestamp":"1660448940.0","upvote_count":"4"},{"poster":"bobsmith2000","content":"BE\nA. It sounds substantially wrong. There's no \"custom web server platform\" only \"custom platform\". And if that's the case the one doesn't have to specify a web server executable anywhere. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platforms.html\n\nB. That's completely feasible. https://aws.amazon.com/blogs/devops/running-docker-on-aws-opsworks/\n\nC. Wrong. There's no such thing as \"Docker layer\". A custom layer is required. https://docs.aws.amazon.com/opsworks/latest/userguide/workinglayers.html\n\nD. Wrong. The one is only able to request logs from EB instances. On an instance the logs appear via docker volumes. There are no mentions of any sctipts for sending logs anywhere which has to be embedded into custom ami for EB env. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.container.console.html#docker-env-cfg.dc-customized-logging\n\nE. That's ancually feasible. A custom image is fairly common pattern. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html","comments":[{"timestamp":"1652291520.0","comment_id":"600256","poster":"bobsmith2000","content":"My bad.\nE is wrong because there's no such a platform as \"AMI platform\".\nMoreover I misunderstood D. You create a volume to map logs from a container to a host and then it's possible to publish them to CloudWatch or rotate to S3 even using the web console.\n\nSo it seems to be BD","upvote_count":"1"}],"comment_id":"591832","timestamp":"1650903720.0","upvote_count":"1"},{"timestamp":"1647155160.0","poster":"sin99","comment_id":"566590","content":"B & D is best","upvote_count":"1"},{"content":"There is nothing here that indicates that we need to containerize this application. It is BeSpoke so it is customized. so E for sure and A. I am opting for A and E","comment_id":"547569","upvote_count":"1","timestamp":"1644907860.0","poster":"jyrajan69"},{"timestamp":"1639710240.0","upvote_count":"1","content":"B & E https://aws.amazon.com/blogs/devops/running-docker-on-aws-opsworks/\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html","poster":"vbal","comment_id":"503325"},{"content":"D. Create a Dockerfile for the application. Create an AWS Elastic Beanstalk application using the Docker platform and the Dockerfile. Enable logging the Docker configuration to automatically publish the application logs. Enable log file rotation to Amazon S3.\nE. Use VM import/Export to import a virtual machine image of the server into AWS as an AMI. Create an Amazon Elastic Compute Cloud (EC2) instance from AMI, and install and configure the Amazon CloudWatch Logs agent. Create a new AMI from the instance. Create an AWS Elastic Beanstalk application using the AMI platform and the new AMI.","poster":"cldy","comment_id":"495940","timestamp":"1638878580.0","upvote_count":"2"},{"comment_id":"488995","timestamp":"1638089160.0","upvote_count":"4","poster":"tiana528","content":"Selected Answer: AD\nA, D.\n\nA : The question mentions it uses a specific webserver of bespoke, and A mentions creating the customized platform. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platforms.html\n\nWhy E is incorrect : Because `Elastic Beanstalk installs a CloudWatch log agent with the default configuration settings on each instance it creates` : https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.cloudwatchlogs.html\nCloudwatch logs agent is already there, you don't need to do that complex steps to install it."},{"poster":"chat77","comment_id":"440622","timestamp":"1635397560.0","content":"BE\nD - Wrong, logs are not in S3","upvote_count":"2"},{"comment_id":"440175","poster":"Kinnam","timestamp":"1635393540.0","upvote_count":"2","content":"A, D and E are correct.\n\nA & E - https://aws.amazon.com/about-aws/whats-new/2017/02/aws-elastic-beanstalk-supports-custom-platforms/\n\nD- Docker platform runs natively on Linux. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html\nhttps://acloud.guru/forums/aws-certified-solutions-architect-professional/discussion/-KHL5b624o_6PR_KAW30/sample-question-\n\nCloudWatch Logs will store your log data indefinitely. S3 is equally durable. \n\nGoing with A & E since they allow \"custom platform\" comparing to D which offers \"Docker platform\"."},{"comment_id":"406245","content":"DE Correct","upvote_count":"1","poster":"Akhil254","timestamp":"1634818800.0"},{"timestamp":"1634281740.0","comments":[{"timestamp":"1639378740.0","poster":"GeniusMikeLiu","comment_id":"500421","content":"cloudwatch logs default expire time is indefinitely. That meams it is durable.","upvote_count":"1"}],"poster":"pradhyumna","content":"A and D because S3 is the durable location that the question hints.","upvote_count":"3","comment_id":"365666"},{"comment_id":"351047","comments":[{"timestamp":"1640802240.0","comment_id":"512555","content":"It only takes a Google search to see Opsworks supporting custom layer with Docker => https://aws.amazon.com/blogs/devops/running-docker-on-aws-opsworks/","upvote_count":"1","poster":"wahlbergusa"}],"upvote_count":"1","timestamp":"1633602120.0","content":"D & E\nA. Custom platform isn't needed.\nB. I don't think\"Create Dockerfile for the application. Create an AWS OpsWorks stack consisting of a custom layer\" is possible.\nC. Using Kinesis is kinda waste.","poster":"01037"},{"content":"Any one what about DE?","comment_id":"314041","comments":[{"poster":"ExtHo","comment_id":"317073","upvote_count":"1","timestamp":"1632352800.0","content":"Yes DE i think too"}],"poster":"ExtHo","upvote_count":"1","timestamp":"1632275940.0"}],"answer_description":"","answer_images":[],"isMC":true,"exam_id":32,"question_text":"You must architect the migration of a web application to AWS. The application consists of Linux web servers running a custom web server. You are required to save the logs generated from the application to a durable location.\nWhat options could you select to migrate the application to AWS? (Choose two.)","choices":{"C":"Create Dockerfile for the application. Create an AWS OpsWorks stack consisting of a Docker layer that uses the Dockerfile. Create custom recipes to install and configure Amazon Kinesis to publish the logs into Amazon CloudWatch.","E":"Use VM import/Export to import a virtual machine image of the server into AWS as an AMI. Create an Amazon Elastic Compute Cloud (EC2) instance from AMI, and install and configure the Amazon CloudWatch Logs agent. Create a new AMI from the instance. Create an AWS Elastic Beanstalk application using the AMI platform and the new AMI.","B":"Create Dockerfile for the application. Create an AWS OpsWorks stack consisting of a custom layer. Create custom recipes to install Docker and to deploy your Docker container using the Dockerfile. Create customer recipes to install and configure the application to publish the logs to Amazon CloudWatch Logs.","D":"Create a Dockerfile for the application. Create an AWS Elastic Beanstalk application using the Docker platform and the Dockerfile. Enable logging the Docker configuration to automatically publish the application logs. Enable log file rotation to Amazon S3.","A":"Create an AWS Elastic Beanstalk application using the custom web server platform. Specify the web server executable and the application project and source files. Enable log file rotation to Amazon Simple Storage Service (S3)."},"url":"https://www.examtopics.com/discussions/amazon/view/47645-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":588,"timestamp":"2021-03-18 12:46:00","answer_ET":"AD","answer":"AD","unix_timestamp":1616067960,"answers_community":["AD (56%)","DE (44%)"]},{"id":"E4nnqBBWV7lBaZWcqxmB","isMC":true,"topic":"1","choices":{"B":"When a new order is created, store the order information in Amazon SQS. Have AWS Lambda check the queue every 5 minutes and process any needed work. When an order needs to be shipped, have Lambda print the label in the warehouse. Once the label has been scanned, as it leaves the warehouse, have an Amazon EC2 instance update Amazon SQS.","C":"Update the application to store new order information in Amazon DynamoDB. When a new order is created, trigger an AWS Step Functions workflow, mark the orders as ×’â‚¬in progress×’â‚¬, and print a package label to the warehouse. Once the label has been scanned and fulfilled, the application will trigger an AWS Lambda function that will mark the order as shipped and complete the workflow.","D":"Store new order information in Amazon EFS. Have instances pull the new information from the NFS and send that information to printers in the warehouse. Once the label has been scanned, as it leaves the warehouse, have Amazon API Gateway call the instances to remove the order information from Amazon EFS.","A":"Use AWS Batch to configure the different tasks required to ship a package. Have AWS Batch trigger an AWS Lambda function that creates and prints a shipping label. Once that label is scanned, as it leaves the warehouse, have another Lambda function move the process to the next step in the AWS Batch job."},"exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/28344-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"answer_description":"","answer_ET":"C","timestamp":"2020-08-13 01:47:00","discussion":[{"comments":[{"comment_id":"456233","content":"Yes.\nAWS Batch is ruled out as it supports only EC2/Fargate based compute not lambdas.","poster":"joe16","upvote_count":"2","timestamp":"1636256640.0"}],"upvote_count":"18","comment_id":"156804","content":"I go for C. Use DynamoDB Streams to trigger lambda then trigger step function.","timestamp":"1632102540.0","poster":"Konnon"},{"poster":"Nemer","content":"C. Step functions Standard for order fulfillment.","timestamp":"1632234240.0","upvote_count":"10","comment_id":"156975"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/batch/latest/userguide/batch_cwet.html\nBatch can send to lands using event bridge","poster":"Rs1084","comments":[{"poster":"mnsait","timestamp":"1734005760.0","content":"\"Once that label is scanned, as it leaves the warehouse, have another Lambda function move the process to the next step in the AWS Batch job\". How do you propose to invoke the Lambda?\n\nMost appropriate option is C (Step Functions)","comment_id":"1325591","upvote_count":"1"}],"timestamp":"1699132620.0","comment_id":"1062386"},{"comment_id":"524437","content":"Selected Answer: C\nc because step functions can be used to handle the steps. workflow service would have been better but yeah.","upvote_count":"1","poster":"pititcu667","timestamp":"1642281240.0"},{"content":"C. Update the application to store new order information in Amazon DynamoDB. When a new order is created, trigger an AWS Step Functions workflow, mark the orders as ×’â‚¬in progress×’â‚¬, and print a package label to the warehouse. Once the label has been scanned and fulfilled, the application will trigger an AWS Lambda function that will mark the order as shipped and complete the workflow.","timestamp":"1639214820.0","poster":"cldy","comment_id":"499267","upvote_count":"1"},{"upvote_count":"1","comment_id":"491118","poster":"AzureDP900","content":"Selected Answer: C\nC is right answer!","timestamp":"1638321960.0"},{"upvote_count":"2","poster":"acloudguru","comment_id":"490359","content":"Selected Answer: C\nA does not make any sense. C should be the serverless and control whole process solution.","timestamp":"1638240480.0"},{"poster":"student22","upvote_count":"1","content":"C\nStep functions","comment_id":"437510","timestamp":"1635328560.0"},{"poster":"tgv","upvote_count":"1","comment_id":"436154","content":"CCC\n---","timestamp":"1635164460.0"},{"comment_id":"435707","poster":"blackgamer","content":"The answer is C.","timestamp":"1634827500.0","upvote_count":"1"},{"content":"I'll go with C","upvote_count":"1","poster":"WhyIronMan","comment_id":"413250","timestamp":"1634730720.0"},{"poster":"Kopa","content":"C, Typical DynamoDB and AWS Step Functions usage","comment_id":"406135","timestamp":"1634602260.0","upvote_count":"1"},{"content":"Why not b?","upvote_count":"1","timestamp":"1634510460.0","comments":[{"upvote_count":"1","comment_id":"367888","timestamp":"1634591880.0","poster":"pradhyumna","content":"Because EC2 instance is part of the solution which does not fit the requirement of serverless"}],"comment_id":"357656","poster":"[Removed]"},{"poster":"Waiweng","timestamp":"1634271660.0","comment_id":"351874","upvote_count":"4","content":"it's C"},{"upvote_count":"1","content":"Answer is C. keywords like \"human intervention\", workflow an be changed by step functions","timestamp":"1634008860.0","comment_id":"334084","poster":"KnightVictor"},{"comment_id":"268584","poster":"Ebi","upvote_count":"3","content":"Answer is C","timestamp":"1633930140.0"},{"timestamp":"1633768140.0","upvote_count":"1","comment_id":"254877","poster":"petebear55","content":"A: is not Serverless .. ans is C ... wish they would stop putting wrong answers in the answe r box"},{"upvote_count":"2","content":"Answer is C","poster":"Bulti","comment_id":"253899","timestamp":"1633625580.0"},{"poster":"T14102020","timestamp":"1633019520.0","content":"Correct is C. Step Function","upvote_count":"1","comment_id":"244770"},{"upvote_count":"3","timestamp":"1632962940.0","content":"I'll go with C","poster":"jackdryan","comment_id":"232475"},{"poster":"CYL","comment_id":"208577","upvote_count":"2","content":"C. The two serverless solutions are A and C. However, with workflow involved, AWS step functions will be a better fit.","timestamp":"1632859140.0"},{"upvote_count":"5","poster":"Anila_Dhharisi","comment_id":"157906","content":"its C for sure.","timestamp":"1632585600.0"},{"poster":"uyungdong","timestamp":"1632457560.0","content":"decoupled application. should be SQS or Step Functions. I go with C using Step Function for this scenario","comment_id":"157323","upvote_count":"3"}],"question_text":"A company has an internal application running on AWS that is used to track and process shipments in the company's warehouse. Currently, after the system receives an order, it emails the staff the information needed to ship a package. Once the package is shipped, the staff replies to the email and the order is marked as shipped.\nThe company wants to stop using email in the application and move to a serverless application model.\nWhich architecture solution meets these requirements?","answer":"C","question_id":589,"answers_community":["C (80%)","A (20%)"],"unix_timestamp":1597276020,"question_images":[]},{"id":"XqXY6Rrjcu4DBj6j7gfn","answer_ET":"A","answer":"C","unix_timestamp":1597296480,"timestamp":"2020-08-13 07:28:00","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/28407-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"topic":"1","question_text":"A company has developed a mobile game. The backend for the game runs on several virtual machines located in an on-premises data center. The business logic is exposed using a REST API with multiple functions. Player session data is stored in central file storage. Backend services use different API keys for throttling and to distinguish between live and test traffic.\nThe load on the game backend varies throughout the day. During peak hours, the server capacity is not sufficient. There are also latency issues when fetching player session data. Management has asked a solutions architect to present a cloud architecture that can handle the game's varying load and provide low-latency data access. The API model should not be changed.\nWhich solution meets these requirements?","isMC":true,"answer_images":[],"choices":{"B":"Implement the REST API using an Application Load Balancer (ALB). Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.","D":"Implement the REST API using AWS AppSync. Run the business logic in AWS Lambda. Store player session data in Amazon Aurora Serverless.","A":"Implement the REST API using a Network Load Balancer (NLB). Run the business logic on an Amazon EC2 instance behind the NLB. Store player session data in Amazon Aurora Serverless.","C":"Implement the REST API using Amazon API Gateway. Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on- demand capacity."},"discussion":[{"comment_id":"156986","timestamp":"1632116100.0","content":"C. Api Gateway . DynamoDB typical gaming use case.\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-gaming-use-cases-and-design-patterns/","upvote_count":"24","poster":"Nemer"},{"poster":"Jesuisleon","timestamp":"1684875600.0","comments":[{"poster":"SkyZeroZx","timestamp":"1688260200.0","comment_id":"940385","content":"The API model should not be changed. then mantain use API Rest\nAWS AppSync is GraphQL is a big change","upvote_count":"2"}],"content":"Why NOT D ? Can someone enlighten me ?\nbased on this link: https://aws.amazon.com/blogs/mobile/appsync-real-time-live-sports/","comment_id":"905230","upvote_count":"1"},{"content":"C is good","poster":"Ni_yot","upvote_count":"1","comment_id":"647724","timestamp":"1660662960.0"},{"timestamp":"1642340340.0","upvote_count":"1","poster":"pititcu667","content":"Selected Answer: C\nc -> api lambda dynamo classic use case","comment_id":"524969"},{"poster":"vbal","timestamp":"1638581520.0","content":"Answer shld be B based upon API model cant be changed as both ALB & API Gateway have similar capabilities...https://dashbird.io/blog/aws-api-gateway-vs-application-load-balancer/","comment_id":"493449","upvote_count":"1"},{"comment_id":"491122","timestamp":"1638322260.0","content":"Selected Answer: C\nC is perfect answer for this use case!","upvote_count":"1","poster":"AzureDP900"},{"poster":"andylogan","timestamp":"1635970500.0","upvote_count":"1","content":"It's C, typical use-case","comment_id":"443233"},{"poster":"tgv","upvote_count":"1","timestamp":"1635712560.0","comment_id":"436156","content":"CCC\n---"},{"content":"I'll go with C","upvote_count":"2","comment_id":"413256","timestamp":"1635523020.0","poster":"WhyIronMan"},{"comment_id":"358672","timestamp":"1635464940.0","poster":"blackgamer","upvote_count":"1","content":"C is the answer."},{"timestamp":"1634946360.0","poster":"Waiweng","upvote_count":"3","comment_id":"353682","content":"it;s C"},{"comment_id":"344768","timestamp":"1634405820.0","upvote_count":"2","poster":"gsw","content":"always avoid making radical changes to the architecture unless the question specifies it"},{"upvote_count":"1","timestamp":"1633792980.0","poster":"alisyech","comment_id":"321745","content":"C for sure"},{"poster":"certainly","comment_id":"300612","upvote_count":"2","content":"B would work. API gateway limit 10,000 Req. per Sec. while ALB doesn't have limit, hence it scale better","timestamp":"1633662120.0"},{"poster":"Ebi","upvote_count":"3","content":"I will with C","comment_id":"268585","timestamp":"1633198140.0"},{"poster":"01037","upvote_count":"3","comment_id":"266365","content":"C is the best option here, but it needs lots of work.\nWithout anymore requirement, C is the answer.","timestamp":"1632783720.0"},{"upvote_count":"1","timestamp":"1632754560.0","poster":"Bulti","comment_id":"253903","content":"C is the correct answer."},{"comment_id":"244779","upvote_count":"1","timestamp":"1632693720.0","poster":"T14102020","content":"Correct is C. API Gateway + Lambda + Dynamo"},{"content":"I'll go with C","upvote_count":"3","timestamp":"1632610380.0","poster":"jackdryan","comment_id":"232479"},{"poster":"CYL","upvote_count":"1","timestamp":"1632479460.0","comment_id":"208583","content":"C. NLB cannot be used to implement API. DynamoDB for user information is the most appropriate."},{"poster":"sam422","comments":[{"timestamp":"1632454740.0","content":"Sorry it is C, not B","upvote_count":"1","poster":"sam422","comment_id":"185736"}],"timestamp":"1632362460.0","comment_id":"185734","content":"Why not B,","upvote_count":"1"},{"content":"C is correct.","poster":"Anila_Dhharisi","comment_id":"157909","upvote_count":"2","timestamp":"1632360900.0"},{"poster":"uyungdong","timestamp":"1632216660.0","comment_id":"157325","content":"Go with C","upvote_count":"1"}],"answer_description":"","question_images":[],"question_id":590}],"exam":{"provider":"Amazon","isBeta":false,"isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"id":32},"currentPage":118},"__N_SSP":true}