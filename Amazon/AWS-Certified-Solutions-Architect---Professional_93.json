{"pageProps":{"questions":[{"id":"HX1ro0RnEdujKz2Z52V6","answer_description":"","exam_id":32,"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/5464-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"topic":"1","question_text":"A company collects a steady stream of 10 million data records from 100,000 sources each day. These records are written to an Amazon RDS MySQL DB. A query must produce the daily average of a data source over the past 30 days. There are twice as many reads as writes. Queries to the collected data are for one source\nID at a time.\nHow can the Solutions Architect improve the reliability and cost effectiveness of this solution?","timestamp":"2019-09-19 18:48:00","answers_community":["A (50%)","B (50%)"],"discussion":[{"comments":[{"timestamp":"1632403800.0","comment_id":"13750","content":"Also,\nA: does not mention any deletion of the old data! which will be much much costly!","poster":"Moon","comments":[{"poster":"G3","content":"Deletion of data is not a requirement as per the question. It only says the query needs to produce results using the last 30 days. there are twice reads as much as writes. I would go for B.","comment_id":"19041","timestamp":"1632471180.0","upvote_count":"3"}],"upvote_count":"4"},{"timestamp":"1634890320.0","comment_id":"269865","poster":"Firststack","content":"Amazon Kinesis Data Streams supports changes to the data record retention period of your data stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html","upvote_count":"1"}],"content":"I would go with \"B\".\nA: would be preferred if no replicas!, because four replicas will make it costly solution.\nB: TTL with DynamoDB will solve the database size cost. make it cost effective.\nC: is not good solution.\nD: Kinesis can store date unto 7 days.","upvote_count":"39","comment_id":"13612","poster":"Moon","timestamp":"1632172200.0"},{"content":"A\nA: Although Aurora is more expensive, it does improve the reliability.\nB\\C: DynamoDB is a NOSQL so I don’t think it’s suitable for this case.","upvote_count":"15","timestamp":"1632149760.0","poster":"donathon","comments":[{"poster":"Mobidic","comment_id":"55604","content":"The use case is THE classic use case for an NOSQL -> DynamoDB. I saw that before seeing the answers... You can tell that there is no 'relation' involved.","upvote_count":"2","timestamp":"1632900240.0"},{"timestamp":"1667785800.0","content":"How does Aurora being more expensive \"improve the cost-effectiveness\" of the solution?","comment_id":"712747","poster":"Byrney","upvote_count":"3"},{"timestamp":"1632735240.0","content":"For everyone supporting B, so are we just going to re-architect the database to go from SQL to NoSQL with no mention in the question of if that is acceptable. Seems like a huge inference to me.","upvote_count":"7","comment_id":"45345","poster":"AWSPro24","comments":[{"content":"No need to re architect, the Dynamo DB can be a staging DB just supporting then query","timestamp":"1633206720.0","poster":"qianhaopower","upvote_count":"1","comment_id":"93807"},{"upvote_count":"4","comment_id":"182814","poster":"ipindado2020","content":"Considering the use case... in fact it is a simple data store... no complex queries...\nAnd for sure A is very very expensive....\nThen B","timestamp":"1633954740.0"},{"poster":"tobstar86","timestamp":"1647757320.0","upvote_count":"1","comment_id":"571427","content":"NoSQL = Not Only SQL, it's fine to use"}]}],"comment_id":"12535"},{"comment_id":"1117231","timestamp":"1704778500.0","poster":"shammous","content":"Selected Answer: B\nThis is a typical use case (IOT). If we consider massive amounts of data, DynamoDB is the winner. In our case, we need a quick read/write data transaction system that DynamoDb can handle (Optimized for read/write). There is no mention of staying with an SQL database and converting to NoSQL is feasible. the suggested Aurora answer doesn't handle the fact that after 30 days, data can be discarded. So after a couple of months, the database will keep growing and incurring more and more costs! Which would be too expensive. I found a nice article comparing both dbs: https://dynobase.dev/dynamodb-vs-aurora/","upvote_count":"1"},{"poster":"SkyZeroZx","upvote_count":"1","content":"Selected Answer: A\nA is the correct answer\nalthough DynamoDB could work it implies rewriting the application and can bring more time and expense which is not the context of the question.\nThen A more seems","comment_id":"926365","comments":[{"timestamp":"1693036620.0","comment_id":"990587","content":"https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/\nI think 4 read replicas => more cost","upvote_count":"1","poster":"vn_thanhtung"}],"timestamp":"1687050060.0"},{"upvote_count":"1","poster":"dev112233xx","content":"Selected Answer: A\nA is the correct answer\nI don't think DynamoDB will be cheaper in this case... just tried to calculate the cost of DynamoDB for such HUGE data 10mx30days = 300m write records per month and 600m read records per month (twice) storage will be about 30tb> so price will be $4k>\n\nAurora will be cheaper for sure!! even with additional 4RR","timestamp":"1681323900.0","comment_id":"868691"},{"poster":"hobokabobo","timestamp":"1671075600.0","content":"Selected Answer: B\nA: looks possible and real live I would rather take that approach as in reality different Database means rewriting software .... but this reality is out of scope of this question and Aurora is pricy \nB: question explicitely says \"Queries to the collected data are for one source ID at a time.\". Yes we have no relations, we do not need a relational database and so we can go with dynamo db. Setting the TTL gives us the required retention. This is way cheaper than aurora. \nC) Again DynamoDB. It does not mention the TTL.\nD) Kinesis might be posible but whats the cache for. Odd.\n\nA reality sidemark: once an average over a day(daily average) generated it doesn't have to recalculated as it doesn't change. Even if you want average over 30 days you only need at most the total *or* avarage of aday and the number of items so processing so many records over and over again ... if I'd rewrite that to a rolling update would be cost effective.\nFor the exam I would go with B","comment_id":"745669","upvote_count":"1"},{"content":"Selected Answer: B\nB. Use Amazon DynamoDB with the source ID as the partition key and the timestamp as the sort key. Use a Time to Live (TTL) to delete data after 30 days.","timestamp":"1666066920.0","comment_id":"697876","upvote_count":"1","poster":"JohnPi"},{"content":"Selected Answer: A\nA is the answer.\nDynamo DB is for NO SQL.","timestamp":"1665816900.0","upvote_count":"1","comment_id":"695236","poster":"Dionenonly"},{"comment_id":"624514","poster":"Kinty1982","content":"I would go with \"A\":\n- Aurora supports MySQL so there is no need for architecture change\n- MySQL have aggregation functions where DynamoDB does not (only client-side, so we have to scan few TB and fetch them - that would be a massive cost) https://stackoverflow.com/questions/26298829/does-dynamodb-support-aggregate-functions-like-avg-max-min\n- Storing TBs of data would be cheaper in Aurora than in DynamoDB","upvote_count":"2","timestamp":"1656491640.0"},{"content":"So many answers for B with no justification for switching from SQL to NO-Sql?? Its a simple problem, already running MySql, so to improve the system add Read Replicas, because more reads than writes. No mention of having to delete data as a requirement, so unless someone can tell me why, my answer will have to be A.","comment_id":"569444","timestamp":"1647485700.0","upvote_count":"2","poster":"jyrajan69"},{"content":"B. Use Amazon DynamoDB with the source ID as the partition key and the timestamp as the sort key. Use a Time to Live (TTL) to delete data after 30 days.","comment_id":"496577","upvote_count":"1","timestamp":"1638943800.0","poster":"cldy"},{"content":"I will pick B","comment_id":"493997","upvote_count":"2","poster":"AzureDP900","timestamp":"1638657720.0"},{"comments":[{"poster":"TiredDad","comment_id":"414802","comments":[{"content":"When we query data from DynamoDB table using batchgetitem (https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html), A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. For example, if you ask to retrieve 100 items, but each individual item is 300 KB in size, the system returns 52 items (so as not to exceed the 16 MB limit). It also returns an appropriate UnprocessedKeys value so you can get the next page of results. If desired, your application can include its own logic to assemble the pages of results into one dataset. As such, when you have to write your application logic, you can query one table or 30 tables. With the 30 table approach, you can archive older tables or reduce WCU, RCU to make it cost effective.","timestamp":"1636102860.0","upvote_count":"1","comment_id":"414809","poster":"TiredDad"}],"content":"Same reason can be applied for ElastiCache. Max size supported is 170.6TB, so can't hold 30 days of data.\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/amazon-elasticache-for-redis-now-supports-up-to-250-nodes-per-cluster/","timestamp":"1636047360.0","upvote_count":"2"},{"timestamp":"1667602920.0","comment_id":"711427","poster":"AjayPrajapati","content":"I dont think it is 10 million from each source, it is 10 million from combined of all 10kk sources. I still would go with Dynamo. Aurora is expensive and it allready gives 6 replica by default so 4 read replica does not make sense","upvote_count":"1"}],"poster":"TiredDad","upvote_count":"5","content":"10 million records * 100,000 sources * 100 bytes (assuming each record of 100 bytes) = (10000000*100000*100)/(1024*1024*1024*1024) = 90.9 TB of data each day! \nMax size supported by Aurora MySQL is 128TB. It can not take 30 days of data in one Aurora instance!","timestamp":"1635752220.0","comment_id":"414799"},{"timestamp":"1635735840.0","comment_id":"414798","content":"10 million records * 100,000 sources * 100 bytes (assuming each record of 100 bytes) = (10000000*100000*100)/(1024*1024*1024*1024) = 90.9 TB of data each day! \nMax size supported by Aurora MySQL is 128TB. It can not take 30 days of data in one Aurora instance!","upvote_count":"1","poster":"TiredDad"},{"comment_id":"413893","upvote_count":"1","content":"\"reliability and cost effectiveness\"\nIn my opinion - don't mix up cost with the daily average. Cost is related to the total cost of running such queries and the cost of supporting infrastructure.\nB wrong - having one table to serve 300 million (and more) records will have a lot of constraints not to mention only one configuration of RCU/WCU that must support writing and reports\nC correct - as mentioned by other people","timestamp":"1635640860.0","poster":"DerekKey"},{"upvote_count":"1","content":"I'll go for B","comment_id":"410395","timestamp":"1635544200.0","poster":"WhyIronMan"},{"poster":"Waiweng","content":"it's B","upvote_count":"2","comment_id":"345925","timestamp":"1635506820.0"},{"poster":"kiev","upvote_count":"2","timestamp":"1635498900.0","comment_id":"293996","content":"I am going to A. It is almost criminal to use Dynamodb as it is not RDS. Option D only stores data by default for 7 days, so A is good for me unless the question says change of database is acceptable, I won't go near Dynamodb."},{"poster":"Kian1","upvote_count":"3","comment_id":"291029","timestamp":"1635498000.0","content":"going with B"},{"timestamp":"1635384900.0","comment_id":"281078","upvote_count":"2","poster":"Ebi","content":"I go with B"},{"content":"B or C? I would go for B in for better reliability and cost effectiveness. C failed to mention deleting old data after 30 days. as the data size grow in dynamo table, it will spit into more partitions. If the RCU and WCU remain constant, they are divided equally across the partitions. When the allocation of that partition is used up, you risk throttling. AWS will permit burst RCU and WCU at times but it is not assured. To improve performance and not increase cost, you need to reduce the size of the table","timestamp":"1635313020.0","poster":"certainly","upvote_count":"1","comment_id":"279581"},{"comment_id":"279252","content":"C is wrong or at least completely inefficient. The new table each day is with all the data of the day (so all partition keys/sourceIDs). Which means to get an average for a sourceID you will need to query 30 tables...","poster":"lostre","upvote_count":"2","timestamp":"1635311400.0"},{"content":"B. The question is asking for \"reliability and cost effectiveness\" that is achievable only with B. Answer C is wrong because the table will never be deleted and the cost will, of course, grow.....","poster":"Superomam","upvote_count":"1","comment_id":"271955","timestamp":"1634935500.0"},{"content":"B - I guess deleting the data is okay after the analysis is done. No other requirement for other reads.\nC - I would say the partition key is usually the date and the sort key the timestamp when you have time series tables.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-time-series.html","comment_id":"263365","poster":"cox1960","timestamp":"1634868660.0","upvote_count":"1"},{"content":"B or C.\nEven B is more cost-effective, I'm not sure we can tell that the data can be deleted just because there is one requirement that data of last 30 days is used by a query.\nBut also it's said that we shouldn't add any requirements based on our experience.","timestamp":"1634694480.0","poster":"01037","upvote_count":"2","comment_id":"252994"},{"comment_id":"245884","content":"I dont like this question ... for me the answer is C .,... based on my studies ... however i am still uncomfortable with the idea of moving mysql to no sql .... Seems an oversight or very badly put together question","upvote_count":"3","timestamp":"1634606340.0","poster":"petebear55"},{"upvote_count":"1","poster":"T14102020","comment_id":"242981","content":"Correct answer is B. DynamoDB and 30 days TTL","timestamp":"1634419440.0"},{"timestamp":"1634309940.0","upvote_count":"1","poster":"RLai","comment_id":"240945","content":"Best Practices for Handling Time Series Data in DynamoDB - C"},{"comment_id":"233725","poster":"PAUGURU","upvote_count":"2","timestamp":"1634237100.0","content":"B is the only reasonable answer.\nA-> add 4 RR? Why 4, maybe 1 is more than enough, so it cannot be a good answer."},{"poster":"Bulti","comment_id":"230721","content":"B is the right answer. The source ID as the partition key and Timestamp as sort key will help improve the query performance. I don't think we need to replace the RDS SQL server with Dynamo DB but populate the subset of data from RDS SQL Server into Dynamo DB enough to meet the requirements of the query results. Since we are not deleting the sourced data from RDS SQL, it is ok to delete the data from DynamoDB after 30 days using TTL. The data in DynamoDB can be recreated anytime from the data in RDS SQL.","upvote_count":"6","timestamp":"1634131200.0"},{"upvote_count":"3","comment_id":"230667","content":"I'll go for B","poster":"jackdryan","timestamp":"1634128740.0"},{"upvote_count":"1","comment_id":"229366","content":"seems B","poster":"gookseang","timestamp":"1634114220.0"},{"comment_id":"227602","poster":"Edgecrusher77","content":"I would go for B","upvote_count":"1","timestamp":"1634037780.0"},{"timestamp":"1633997880.0","comment_id":"208910","poster":"SamAWSExam99","content":"General design principles in Amazon DynamoDB recommend that you keep the number of tables you use to a minimum. For most applications, a single table is all you need. However, for time series data, you can often best handle it by using one table per application per period.","upvote_count":"1"},{"poster":"Paitan","comment_id":"196516","upvote_count":"1","content":"B is the right option. The query will provide the best result if stored in a single Dynamo DB table.","timestamp":"1633961100.0"},{"content":"B will be the answer, since it is \"cost effective\", it will be costly if keep data after 30 which is not use anymore.","poster":"fullaws","comment_id":"150226","upvote_count":"4","timestamp":"1633881060.0"},{"content":"C is correct, alot of data and it only query on Source ID which can be accelerate the query thru partition key. Schedule for 30 TTL delete data is good for even distribute query, but it will lost the data. A better recommended way will be using creating new table for each day with reduced the W/C R/C as the table toward 1 as the table get older","comment_id":"150223","poster":"fullaws","timestamp":"1633863060.0","upvote_count":"1"},{"comments":[{"content":"I think that would apply if the queries were being averaged over a fixed time period, e.g. the month of October. You'd want one table per query period. In this case, it's a sliding window for the query to use, the past 30 days, and that moves by one day every day. Having a separate table per day and having to query and aggregate values across 30 tables might be cumbersome , in my opinion... but I'm not a DBA, lol","timestamp":"1636257240.0","comment_id":"459304","poster":"kirrim","upvote_count":"1"}],"content":"Answer is C. For time series data, AWS recommends to create a new table each day.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-time-series.html","upvote_count":"1","comment_id":"143088","timestamp":"1633828620.0","poster":"garud"},{"upvote_count":"1","comment_id":"134511","poster":"NikkyDicky","comments":[{"timestamp":"1633771500.0","upvote_count":"2","comment_id":"137570","content":"Answer is C \nConsider a typical time series scenario, where you want to track a high volume of events. Your write access pattern is that all the events being recorded have today's date. Your read access pattern might be to read today's events most frequently, yesterday's events much less frequently, and then older events very little at all. One way to handle this is by building the current date and time into the primary key.\n\nThe following design pattern often handles this kind of scenario effectively:\n\nCreate one table per period, provisioned with the required read and write capacity and the required indexes.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-time-series.html","poster":"xhova"}],"timestamp":"1633721700.0","content":"B seems more likely"},{"timestamp":"1633651440.0","comment_id":"127302","content":"Dynamodb is suited for large amount of data input so i would also vote for B","upvote_count":"1","poster":"oatif"},{"upvote_count":"2","poster":"mat2020","timestamp":"1633499100.0","content":"I would go for ans: B","comment_id":"110382"},{"timestamp":"1633356600.0","comment_id":"103717","content":"For those who supports B: there is no statement that the data is not needed after 30 days. So delete the data with TTL looks suspicious. Also as twice more reads is a clue for read-replicas.","upvote_count":"1","comments":[{"upvote_count":"2","comments":[{"comment_id":"181510","upvote_count":"1","content":"for cost effective, having data evokes more costs. if you dont need, delete data automatically as set TTL is correct.\nI vote B for the answer.","timestamp":"1633937280.0","poster":"proxyolism"}],"timestamp":"1633486740.0","comment_id":"107874","poster":"JAWS1600","content":"I agree , why would you delete data after 30 days."}],"poster":"Oleksandr"},{"upvote_count":"2","poster":"meenu2225","comments":[{"upvote_count":"1","timestamp":"1634595120.0","poster":"petebear55","content":"exactly these guys should not be thinking of taking an exam if they think moving from mysql to no sql is the answer .... i will go for A","comment_id":"245881"}],"content":"So people who are sying Option B, have you guys taken Database transformation into factor, i.e. the cost involved in changing a Sql DB to no-sql and what sort of implecation it cna have on app?, I will go with A, may be expensive but akes more sense.","comment_id":"102897","timestamp":"1633287240.0"},{"content":"Option A: Multi-AZ is good but 4 RRs = more cost. Growing data in SQL!\nOption D: ElastiCache!\nOption C: 30 Tables is cumbersome with multiple queries for a single data source. Quite complex! Although, reliable and cost-effective.\nOption B: Are we okay with deleting data? It is not asked! Typically, such data becomes useless after analysis so I can understand selecting this option. \n\nConsider this though: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-time-series.html\nPoorly described question; I will go for B along with others.","timestamp":"1633199760.0","upvote_count":"3","poster":"Smart","comments":[{"timestamp":"1633689300.0","poster":"asdfasdfasd","content":"So C is the best option for time series data.\nRunning aggregate queries in parallel against 30 tables will give significant performance boost.\nEach table except current day will have 0 write units set.\nTables older than 30 days should be either downscaled to 0 read units or just deleted with single command.","upvote_count":"2","comment_id":"132087"}],"comment_id":"76633"},{"comment_id":"75208","content":"I vote for B ....\ni believe statemnet \"Queries to the collected data are for one source\nID at a time.\" does mention that there are no complex queries like RDS and nosql will be better utilized . \nAnd we are sure enough that 3 replicas are sure enough to handle load , it may become underutilized or not sufficient , lots of doubt over A","timestamp":"1633156860.0","poster":"AMITKR9580","upvote_count":"3"},{"content":"I first thought about A but finally, 4 read replicas are heavy to administer and regarding 30 days retention, B seems to be much more suited solution. I finally also opt for B.","upvote_count":"3","comment_id":"62308","timestamp":"1632988260.0","poster":"virtual"},{"content":"I go for B","timestamp":"1632817560.0","comment_id":"51234","poster":"amog","upvote_count":"3"},{"upvote_count":"1","content":"B is right answer","timestamp":"1632565440.0","poster":"dumma","comment_id":"44424"},{"poster":"arunkumar","timestamp":"1632515580.0","content":"B is the answer.","comment_id":"35876","upvote_count":"1"},{"comment_id":"30574","poster":"DirtyBerty","timestamp":"1632501480.0","upvote_count":"5","content":"Option: B\n\nDelete data after 30 days deals with \"cost effectiveness\"\n\n\"improve the reliability\", DynamoDB has built in high availability and durability see: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html"},{"comment_id":"16215","poster":"chaudh","content":"I go for B","timestamp":"1632460320.0","upvote_count":"3"},{"comment_id":"11897","timestamp":"1632118380.0","upvote_count":"5","content":"I would go for B.","poster":"huhupai"}],"question_id":461,"answer_ET":"A","unix_timestamp":1568911680,"choices":{"A":"Use Amazon Aurora with MySQL in a Multi-AZ mode. Use four additional read replicas.","C":"Use Amazon DynamoDB with the source ID as the partition key. Use a different table each day.","B":"Use Amazon DynamoDB with the source ID as the partition key and the timestamp as the sort key. Use a Time to Live (TTL) to delete data after 30 days.","D":"Ingest data into Amazon Kinesis using a retention period of 30 days. Use AWS Lambda to write data records to Amazon ElastiCache for read access."},"answer":"A"},{"id":"vLqHjJyuyokEowOJcc6T","question_id":462,"answer_images":[],"answer":"C","topic":"1","isMC":true,"timestamp":"2019-09-25 03:28:00","question_images":[],"answer_description":"Reference:\nhttps://aws.amazon.com/blogs/apn/oracle-database-encryption-options-on-amazon-rds/ https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.AdvSecurity.htm\n(DMS in transit encryption)\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html","choices":{"C":"Use AWS DMS to load and replicate the dataset between the on-premises Oracle database and the replication instance hosted on AWS. Provision an Amazon RDS for Oracle instance with Transparent Data Encryption (TDE) enabled and configure it as a target for the replication instance. Create a customer-managed AWS KMS master key to set it as the encryption key for the replication instance. Use AWS DMS tasks to load the data into the target RDS instance. During the application maintenance window and after the load tasks reach the ongoing replication phase, switch the database connections to the new database.","D":"Create a compressed full database backup of the on-premises Oracle database during an application maintenance window. While the backup is being performed, provision a 10 Gbps AWS Direct Connect connection to increase the transfer speed of the database backup files to Amazon S3, and shorten the maintenance window period. Use SSL/TLS to copy the files over the Direct Connect connection. When the backup files are successfully copied, start the maintenance window, and rise any of the Amazon RDS supported tools to import the data into a newly provisioned Amazon RDS for Oracle instance with encryption enabled. Wait until the data is fully loaded and switch over the database connections to the new database. Delete the Direct Connect connection to cut unnecessary charges.","A":"Provision an Amazon RDS for Oracle instance. Host the RDS database within a virtual private cloud (VPC) subnet with internet access, and set up the RDS database as an encrypted Read Replica of the source database. Use SSL to encrypt the connection between the two databases. Monitor the replication performance by watching the RDS ReplicaLag metric. During the application maintenance window, shut down the on-premises database and switch over the application connection to the RDS instance when there is no more replication lag. Promote the Read Replica into a standalone database instance.","B":"Provision an Amazon EC2 instance and install the same Oracle database software. Create a backup of the source database using the supported tools. During the application maintenance window, restore the backup into the Oracle database running in the EC2 instance. Set up an Amazon RDS for Oracle instance, and create an import job between the databases hosted in AWS. Shut down the source database and switch over the database connections to the RDS instance when the job is complete."},"unix_timestamp":1569374880,"answers_community":["C (100%)"],"discussion":[{"upvote_count":"20","poster":"Moon","content":"More toward \"C\".\nhttps://aws.amazon.com/blogs/apn/oracle-database-encryption-options-on-amazon-rds/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.AdvSecurity.html\n(DMS in transit encryption)\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html","comment_id":"13606","timestamp":"1632487500.0"},{"comment_id":"12539","comments":[{"comment_id":"13780","content":"Answer is C","poster":"donathon","upvote_count":"13","timestamp":"1632592500.0"}],"timestamp":"1632468180.0","poster":"donathon","content":"https://aws.amazon.com/blogs/database/best-practices-for-migrating-an-oracle-database-to-amazon-rds-postgresql-or-amazon-aurora-postgresql-source-database-considerations-for-the-oracle-and-aws-dms-cdc-environment/","upvote_count":"8"},{"content":"Selected Answer: C\nAgree with others on option C. However, what about this \"Network connectivity to the source Oracle database over the internal is allowed\". Nothing is mentioned about this access in C?","upvote_count":"1","timestamp":"1733761560.0","poster":"mnsait","comment_id":"1324134"},{"upvote_count":"1","content":"Selected Answer: C\nKeyword = AWS DMS for replication task in Database","comment_id":"926366","poster":"SkyZeroZx","timestamp":"1687050240.0"},{"comment_id":"496607","upvote_count":"2","content":"C. Use AWS DMS to load and replicate the dataset between the on-premises Oracle database and the replication instance hosted on AWS. Provision an Amazon RDS for Oracle instance with Transparent Data Encryption (TDE) enabled and configure it as a target for the replication instance. Create a customer-managed AWS KMS master key to set it as the encryption key for the replication instance. Use AWS DMS tasks to load the data into the target RDS instance. During the application maintenance window and after the load tasks reach the ongoing replication phase, switch the database connections to the new database.","poster":"cldy","timestamp":"1638948300.0"},{"comment_id":"494002","timestamp":"1638658020.0","content":"I will go with C","poster":"AzureDP900","upvote_count":"1"},{"content":"I'll go with C","upvote_count":"1","timestamp":"1635848340.0","poster":"WhyIronMan","comment_id":"410399"},{"content":"it's C with DMS","timestamp":"1635762480.0","poster":"Waiweng","upvote_count":"2","comment_id":"345930"},{"timestamp":"1635535560.0","content":"going with C","comment_id":"291032","poster":"Kian1","upvote_count":"2"},{"content":"C is the answer","poster":"Ebi","upvote_count":"3","timestamp":"1635309240.0","comment_id":"281079"},{"upvote_count":"2","content":"Correct answer is C. DMS tool","comment_id":"242985","timestamp":"1635249240.0","poster":"T14102020"},{"poster":"Bulti","comment_id":"230725","content":"Answer is C. Cannot use native Oracle replication tool and Direct Connect cannot be setup during the maintenance window.","upvote_count":"3","timestamp":"1635036660.0"},{"content":"I'll go with C","poster":"jackdryan","comment_id":"230673","timestamp":"1634336100.0","upvote_count":"3"},{"poster":"gookseang","content":"seems C","comment_id":"229369","timestamp":"1634291400.0","upvote_count":"1"},{"poster":"Paitan","content":"Definitely C.","comment_id":"196522","upvote_count":"1","timestamp":"1633928460.0"},{"upvote_count":"2","comment_id":"150250","poster":"fullaws","content":"C is correct, D will reduce availability time and seem impossible without native replication.","timestamp":"1633830240.0"},{"poster":"NikkyDicky","upvote_count":"2","timestamp":"1633683480.0","comment_id":"134514","content":"C clearly"},{"comment_id":"62320","comments":[{"comment_id":"62321","upvote_count":"1","content":"(apologize: although Direct Connect could help ...)","timestamp":"1633228440.0","poster":"virtual"}],"upvote_count":"3","content":"I think it's C here (also Direct Connect could help). it's the only response with TDE and KMS and the question states that we must encrypt.","timestamp":"1633221960.0","poster":"virtual"},{"content":"I think its D. \nhttps://aws.amazon.com/blogs/database/aws-dms-now-supports-r4-type-instances-and-learn-to-choose-the-right-instance-class-for-migrations-using-aws-dms/\n\nIt takes 3 days to transfer 3.5 TB, and this is 12 TB of data. The best is to take a Oracle data pump export and load it through third party udp transfer or Direct connect . The suggestion has been given by AWS in the following site. \n\nhttps://d1.awsstatic.com/whitepapers/strategies-for-migrating-oracle-database-to-aws.pdf\n\nDMS would not complete the transfer before 12 days with option C.","comments":[{"comment_id":"29393","upvote_count":"5","content":"This statement makes me feel D is not very good.\n\nWhile the backup is being performed, provision a 10 Gbps AWS Direct Connect connection\n\nIt is saying Direct connect can be setup immediately like VPN","timestamp":"1632858240.0","poster":"9Ow30"},{"content":"Question does not say it has to complete in \"X\" number of days. We could have it configured and running on a steady state (very low replication lag) for as long as the company can agree the cut over window.","poster":"Greg1234","upvote_count":"1","comment_id":"44682","comments":[{"poster":"Mobidic","timestamp":"1633202640.0","content":"First 6 months, DMS is free, really designed to allow to \"take your time\".","upvote_count":"1","comment_id":"55600"}],"timestamp":"1632995220.0"},{"comment_id":"76544","timestamp":"1633284240.0","upvote_count":"1","content":"Direct Connect won’t be setup within a month","poster":"Joeylee"},{"timestamp":"1633314780.0","poster":"[Removed]","upvote_count":"2","content":"Question states that you cannot use native replication tools of the database due to licensing restrictions. I think the only way out is using DMS.","comment_id":"88037"}],"timestamp":"1632856320.0","poster":"G3","comment_id":"22215","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/amazon/view/5666-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"answer_ET":"C","question_text":"A company is moving a business-critical application onto AWS. It is a traditional three-tier web application using an Oracle database. Data must be encrypted in transit and at rest. The database hosts 12 TB of data. Network connectivity to the source Oracle database over the internal is allowed, and the company wants to reduce operational costs by using AWS Managed Services where possible. All resources within the web and application tiers have been migrated. The database has a few tables and a simple schema using primary keys only; however, it contains many Binary Large Object (BLOB) fields. It was not possible to use the database's native replication tools because of licensing restrictions.\nWhich database migration solution will result in the LEAST amount of impact to the application's availability?"},{"id":"DT6vnlODNNey0aSHo16x","discussion":[{"timestamp":"1632294360.0","content":"AEF is correct","poster":"u4x","comment_id":"11055","upvote_count":"32"},{"upvote_count":"18","comment_id":"16220","timestamp":"1632453420.0","comments":[{"poster":"TechGuru","comment_id":"17784","timestamp":"1632492000.0","content":"ACE : VPC endpoints can be defined under private DNS, \nhttps://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/","upvote_count":"6"},{"poster":"uopspop","comments":[{"upvote_count":"1","comment_id":"107802","poster":"VrushaliD","timestamp":"1633140900.0","content":"agree. The link - https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html\ntalks the same.\nAnswer is AEF"},{"poster":"hollie","upvote_count":"1","comment_id":"771898","content":"But in your choice E, it already states using interface endpoint.","timestamp":"1673393820.0"}],"content":"aggree. Private DNS name is only available for interface endpoint.\nS3 and Dynamodb use the \"gateway endpoint\" which require setting in the route table.","timestamp":"1632614640.0","comment_id":"21781","upvote_count":"3"},{"upvote_count":"1","poster":"[Removed]","comment_id":"88040","timestamp":"1633078560.0","content":"DNS resolutions is required for Gateway Endpoints, too. \nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html#vpc-endpoints-limitations"}],"poster":"chaudh","content":"AEF. \nC is not correct. Private DNS Name for interface endpoint only while S3 is VPC endpoint."},{"comment_id":"1117236","poster":"shammous","upvote_count":"1","content":"Selected Answer: ADE\nTricky question as it's not clear if the application or job scheduler are on-premise or hosted in AWS. \"move some workloads onto AWS\" suggests that some other workloads are still on-premise. \"Each time a large analytics workload is completed\" can be on-premise, which triggers the creation of a new VPC programmatically through the use of APIs. \"The job scheduler needs only to communicate with the Amazon EC2 API to start new grid nodes\" suggests that the scheduler is not hosted in an EC2 instance but outside (on premise). All these clues made me choose answer D over F: \"Configure Amazon S3 endpoint policy to permit access only from the grid nodes.\" is a good security practice but is not among the listed requirements. Answer D is doable: https://chat.openai.com/share/47ec0d42-36e6-4e48-af11-39ad84da4dfb","timestamp":"1704780780.0"},{"poster":"Dionenonly","upvote_count":"1","content":"Selected Answer: AEF\nAEF is the answer","timestamp":"1663380780.0","comment_id":"671199"},{"comment_id":"649329","poster":"gnic","timestamp":"1660982580.0","content":"Selected Answer: AEF\nAEF for sure","upvote_count":"2"},{"content":"• Use Gateway Endpoint if the AWS service is either DynamoDB or S3.\n• Use Interface Endpoint for everything else.","comment_id":"626514","poster":"KiraguJohn","upvote_count":"1","timestamp":"1656844560.0"},{"timestamp":"1647018300.0","comments":[{"poster":"shammous","upvote_count":"1","content":"Are you sure the workload is in AWS?","timestamp":"1704780840.0","comment_id":"1117237"}],"content":"I think its AEF\nTake a look at GW Endpoint policies\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html#vpc-endpoints-policies-s3\nB - Not a valid answer will break the endpoint DNS access\nC - you dont need to configure anything the application - once a GW endpoint is set and RTB is updated all queries to s3.xxxx.amazonaws.com will be routed privately\nD - why do you need to integrate with on-prem DNS? the workload is in AWS\nSo AEF it is","comment_id":"565674","poster":"asfsdfsdf","upvote_count":"1"},{"content":"A. Enable VPC endpoints for Amazon S3 and DynamoDB.\nE. Enable an interface VPC endpoint for EC2.\nF. Configure Amazon S3 endpoint policy to permit access only from the grid nodes.","upvote_count":"1","timestamp":"1639127820.0","poster":"cldy","comment_id":"498471"},{"timestamp":"1638658140.0","poster":"AzureDP900","comment_id":"494003","content":"AEF is right","upvote_count":"1"},{"timestamp":"1638074940.0","upvote_count":"1","poster":"acloudguru","content":"Selected Answer: AEF\nThe link - https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html\ntalks the same.\nAnswer is AEF","comment_id":"488839"},{"comment_id":"450952","poster":"DonSp","timestamp":"1636296240.0","content":"A great question that most people got wrong. F is good in general but not a valid answer as there is no requirement. D on the other hand is required for \"without the need for reconfiguration for each new deployment\". Instead of an IP address which is different in each VPN, the scheduler needs a well-known DNS name. AE are obvious, so ADE.","upvote_count":"3"},{"upvote_count":"2","comment_id":"413912","content":"F correct - endpoint default policy allows access by any user or service within the VPC, using credentials from any AWS account, to any Amazon S3 resource; including Amazon S3 resources for an AWS account other than the account with which the VPC is associated","poster":"DerekKey","timestamp":"1636131900.0"},{"poster":"WhyIronMan","comments":[{"upvote_count":"1","comment_id":"410471","content":"If you've already set up access to your Amazon S3 resources from your VPC, you can continue to use Amazon S3 DNS names to access those resources after you've set up an endpoint.","timestamp":"1636130640.0","poster":"WhyIronMan"}],"upvote_count":"2","comment_id":"410404","timestamp":"1636113120.0","content":"I go for A, E, F."},{"upvote_count":"1","comment_id":"375030","poster":"chkmtess","timestamp":"1636082460.0","content":"Amazon S3 interface endpoints do not support the private DNS feature of interface endpoints\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html"},{"content":"A,E,F is correct","timestamp":"1635974040.0","poster":"Waiweng","comment_id":"345933","upvote_count":"2"},{"comment_id":"337970","comments":[{"poster":"01037","upvote_count":"1","comment_id":"337971","content":"As to C, Private DNS name is only available for interface endpoint.\nS3 and Dynamodb use the \"gateway endpoint\" which require setting in the route table.","timestamp":"1635897240.0"}],"upvote_count":"1","timestamp":"1635825900.0","poster":"01037","content":"A, E, I understand.\nF, there is no requirement in the question, and how to implement F using Amazon S3 endpoint policy?"},{"content":"This question appears to be missing some info because we don't know how the on-prem communicates with AWS. Assuming there's a VPN or DX because the AWS environment isn't allowed to connect to the internet, then ADE appears correct.","timestamp":"1635662460.0","comment_id":"326744","upvote_count":"1","poster":"chris1025"},{"poster":"Chubb","timestamp":"1635569220.0","content":"E is not necessary. Endpoint for S3 and dynamo DB is enough","upvote_count":"1","comment_id":"297193"},{"upvote_count":"1","content":"ACE for me. For those choosing F instead of C, just remember that there is nothing like VPC endpoint for EC2.","timestamp":"1635535380.0","comment_id":"294007","poster":"kiev","comments":[{"upvote_count":"2","poster":"Kian1","content":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/interface-vpc-endpoints.html","comment_id":"294969","timestamp":"1635565380.0"}]},{"upvote_count":"2","content":"I will go with AEF","poster":"Kian1","comment_id":"291038","timestamp":"1635517380.0"},{"poster":"Ebi","content":"C is not needed:\n\"If you've already set up access to your Amazon S3 resources from your VPC, you can continue to use Amazon S3 DNS names to access those resources after you've set up an endpoint.\"\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\n\nAnswer id AEF\nF: SourceIP cannot be used for IAM policies attached to role, user or bucket, it CAN be used for endpoint policy","timestamp":"1635430860.0","upvote_count":"3","comment_id":"281487"},{"comments":[{"content":"Ignore. this is wrong as the PrivateDNS name is useful only for non S3 and DynamoDB services when we want to use their DNS name to route through private and not public route.\nSwitching to AEF","comment_id":"279516","comments":[{"upvote_count":"1","poster":"certainly","comment_id":"290560","timestamp":"1635455580.0","content":"I also agree. AE. not C. but F is not even required for this question. am i missing something?"}],"timestamp":"1635346740.0","poster":"lostre","upvote_count":"2"}],"timestamp":"1635298200.0","comment_id":"279510","upvote_count":"1","content":"I think it is ACE. \nC is needed because to use the EC2 the correct DNS without Internet Gateway it needs to go through the private DNS.\nF is not a requirement. It is just a least privileges security practice\n\nACE for me","poster":"lostre"},{"content":"Seems AEF","comment_id":"275203","poster":"gookseang","upvote_count":"1","timestamp":"1635290940.0"},{"poster":"shamith","upvote_count":"1","timestamp":"1635084000.0","comment_id":"269691","content":"ACE\nhttps://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html"},{"poster":"01037","timestamp":"1635033240.0","comment_id":"253010","upvote_count":"2","content":"What does E mean?"},{"timestamp":"1634930760.0","comment_id":"250603","poster":"darthvoodoo","upvote_count":"6","content":"Shouldn't be ADE?\nA: VPC endpoint gateways are needed for both S3 and Dynamodb services\nD: If you don't do this, your on premises servers won't be able to resolve the EC2 endpoint names. \nE. VPC endpoint is needed because the job scheduler needs to communicate through it\nC: is incorrect. There is no private DNS name for the Amazon S3 endpoint gateways. S3 endpoint has a regional address that resolves to public ip addresses. \nF: provides security which is not a requirement here"},{"upvote_count":"1","content":"You do not set up an endpoint policy on answer F .. this is done via security groups when setting up ... the answer IS ACE NOT AEF","timestamp":"1634642400.0","comment_id":"245976","comments":[{"upvote_count":"1","timestamp":"1635617220.0","content":"Wrong, S3 endpoint is gateway type and uses a routing tables and policy instead of security groups. AEF","comment_id":"308613","poster":"miar"}],"poster":"petebear55"},{"comment_id":"242994","poster":"T14102020","content":"Correct answer is AEF. Endpoints for S3, DynamoDB, EC2. Policy for S3","comments":[{"comment_id":"245978","poster":"petebear55","timestamp":"1634728860.0","upvote_count":"1","content":"WRONG ACE ,..."}],"upvote_count":"2","timestamp":"1634553180.0"},{"poster":"PAUGURU","comment_id":"241453","upvote_count":"1","timestamp":"1634340960.0","content":"It's two different levels, the infrastructure level and the application level. A and E will set up the infrastructure and prepare the Endpointd and this will create a private DNS name to access s3 via the vcpe. But then you must instruct the application (that is infrastructure unaware!!!) to use this DNS name, if the application has hardcoded an HTTP GET to s3://s3.uswest-1.xxxx/bucket instead of the private DNS name (vpce.s3.xxxxx) it will try to resolve with a public IP. Answer is ACE"},{"poster":"Bulti","upvote_count":"5","timestamp":"1634295240.0","comment_id":"230815","comments":[{"comment_id":"233707","content":"Changing to A,E,F","upvote_count":"3","timestamp":"1634336040.0","poster":"jackdryan"},{"comment_id":"275023","comments":[{"timestamp":"1635138480.0","comment_id":"275024","poster":"Bulti","content":"In support of D and E. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/interface-vpc-endpoints.html","upvote_count":"1"}],"poster":"Bulti","content":"Answer is A, D,E. Darthvoodoo below is the only person who has nailed the right answer. A and E are the VPC endpoints for S3, Dynamo DB and EC2 API. Now only D is the other right answer among the remaining choices because the job scheduler is running on-prem and the DNS server on-prem needs to have an DNS entry for the private DNS of the EC2 VPC Endpoint. F is providing a solution that is not required and C is incorrect because Gateway Endpoint do not need private DNS enabled, its the Interface Endpoint which does.","upvote_count":"3","timestamp":"1635113040.0"}],"content":"Answer is A,E and F. Lot of food mentioned C instead of F. But to communicate with a VPC endpoint you do not need to change the dNS config as route prefix are used to communicate with a Gateway endpoint."},{"content":"I'll go with A,C,E","upvote_count":"1","comment_id":"230702","timestamp":"1634153940.0","poster":"jackdryan"},{"content":"seems ACE","poster":"gookseang","upvote_count":"1","timestamp":"1634103420.0","comment_id":"229374"},{"comment_id":"228766","poster":"srinivasa","content":"ACF is the right answer\nD: Interface endpoint doesn't exist for S3 and DynamoDB\nB: is not applicable","timestamp":"1634046060.0","upvote_count":"3"},{"poster":"Andrew_Pan_909","upvote_count":"2","content":"ACE\nB: No access to the internet mean we have to use Private DNS this is requested.\nD: No access to the on-premises DNS server, as no access to the internet nether onpremises proxy.\nF: Pls note this is tricky as S3 Policy can't use IPv4 CIDR after enable Endpoint, so it's not possible to permit only grid nodes. Only endpoint or vpc as the source for S3 policy when use S3 Endpoint. \nSo only ACE are correct (C maybe not necessary but not a wrong answer)","comment_id":"205599","timestamp":"1634033100.0"},{"upvote_count":"2","content":"A, E and F. This is an easy question.","comment_id":"196525","timestamp":"1633978860.0","poster":"Paitan"},{"comment_id":"188727","content":"AEF is correct","poster":"manoj101","timestamp":"1633877700.0","upvote_count":"1"},{"poster":"ipindado2020","timestamp":"1633816980.0","content":"I go for AEF...\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html#vpce-private-dns\n\nThere is no need to change the configuration of the DNS I think..\n\"For example, if your existing applications make requests to an AWS service, they can continue to make requests through the interface endpoint without requiring any configuration changes.\"","upvote_count":"4","comment_id":"182820"},{"content":"C is INCORRECT\n\nWhen you create a VPC endpoint for Amazon S3, any requests to an Amazon S3 endpoint within the Region (for example, s3.us-west-2.amazonaws.com) are routed to a private Amazon S3 endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC—the endpoint name remains the same, but the route to Amazon S3 stays entirely within the Amazon network, and does not access the public internet.\nhttps://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html","comment_id":"169416","timestamp":"1633755960.0","upvote_count":"4","poster":"citruslab"},{"upvote_count":"3","comment_id":"151342","content":"It is AEF.\nWhy C is not correct because AWS clearly specifies that you can continue using Amazon S3 DNS names for VPC endpoint that you were using prior to setting up endpoints. Read the very first sentence:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","timestamp":"1633635300.0","poster":"garud"},{"timestamp":"1633599480.0","upvote_count":"2","poster":"fullaws","comments":[{"poster":"darthvoodoo","content":"Nothing there to support C I am afraid","timestamp":"1634985060.0","upvote_count":"1","comment_id":"250606"}],"comment_id":"150265","content":"ACE is correct. https://docs.aws.amazon.com/vpc/latest/userguide/verify-domains.html to support C, endpoint policy default is allow"},{"timestamp":"1633491240.0","comment_id":"143146","upvote_count":"2","content":"its AEF","poster":"Anila_Dhharisi"},{"upvote_count":"1","timestamp":"1633467720.0","content":"ACE - Dont see a requirement for F in the question\nDefault policy should allow the access\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","poster":"khksoma","comment_id":"139754"},{"poster":"mat2020","comment_id":"132584","timestamp":"1633282860.0","upvote_count":"1","content":"Answer: AEF is correct"},{"poster":"NikkyDicky","comment_id":"132466","content":"AEF for sure","timestamp":"1633189920.0","upvote_count":"1"},{"poster":"chicagomassageseeker","upvote_count":"1","timestamp":"1633170360.0","comment_id":"119851","content":"AEF. \nF. S3 bucket policy need to allow the EC2 ( grids) to access S3."},{"timestamp":"1633087860.0","comment_id":"89272","content":"AEF is correct","poster":"Xabi","upvote_count":"1"},{"timestamp":"1633057080.0","upvote_count":"1","comment_id":"87806","content":"A & E are good. \nNot convinced for C, but not able to find other good choice.","poster":"fw"},{"upvote_count":"3","timestamp":"1632883560.0","content":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\n\nApplication access S3 by using S3 private DNS name, not s3 endpoint DNS name. S3 endpoint is only referenced in routing table. So C is incorrect. \n\nShould be F instead. Default S3 endpoint policy is quite loose.","poster":"Joeylee","comment_id":"76566"},{"upvote_count":"2","poster":"amog","comment_id":"51235","timestamp":"1632849360.0","content":"Should be A,C,E"},{"comment_id":"44426","upvote_count":"2","poster":"dumma","content":"A and E look correct, not sure about third one.","timestamp":"1632717720.0"},{"comment_id":"11710","timestamp":"1632409620.0","comments":[{"timestamp":"1632876960.0","comment_id":"55595","poster":"Mobidic","upvote_count":"8","content":"C is not a valid option because Gateway VPC Endpoints (S3 & DynamoDB) don't have private ip."}],"content":"ACE\nB: Doing this will means you have to have your own DNS servers.\nD: No sure what this will do.\nF: This is not in the requirement although it is a good to have. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html","upvote_count":"10","poster":"donathon"},{"poster":"huhupai","content":"ACE, https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html","comment_id":"11173","upvote_count":"6","timestamp":"1632310740.0"},{"poster":"dpvnme","comment_id":"10473","content":"ACE ....","upvote_count":"6","timestamp":"1632202800.0"}],"answer_ET":"AEF","timestamp":"2019-09-10 18:44:00","choices":{"B":"Disable Private DNS Name Support.","E":"Enable an interface VPC endpoint for EC2.","F":"Configure Amazon S3 endpoint policy to permit access only from the grid nodes.","C":"Configure the application on the grid instances to use the private DNS name of the Amazon S3 endpoint.","D":"Populate the on-premises DNS server with the private IP addresses of the EC2 endpoint.","A":"Enable VPC endpoints for Amazon S3 and DynamoDB."},"url":"https://www.examtopics.com/discussions/amazon/view/5013-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":463,"topic":"1","answer_description":"","answer_images":[],"unix_timestamp":1568133840,"isMC":true,"question_text":"A company has decided to move some workloads onto AWS to create a grid environment to run market analytics. The grid will consist of many similar instances, spun-up by a job-scheduling function. Each time a large analytics workload is completed, a new VPC is deployed along with job scheduler and grid nodes. Multiple grids could be running in parallel.\nKey requirements are:\n✑ Grid instances must communicate with Amazon S3 to retrieve data to be processed.\n✑ Grid instances must communicate with Amazon DynamoDB to track intermediate data.\n✑ The job scheduler needs only to communicate with the Amazon EC2 API to start new grid nodes.\nA key requirement is that the environment has no access to the internet, either directly or via the on-premises proxy. However, the application needs to be able to seamlessly communicate to Amazon S3, Amazon DynamoDB, and Amazon EC2 API, without the need for reconfiguration for each new deployment.\nWhich of the following should the Solutions Architect do to achieve this target architecture? (Choose three.)","exam_id":32,"question_images":[],"answers_community":["AEF (80%)","ADE (20%)"],"answer":"AEF"},{"id":"gIAVCycXociPPoGJG7CK","url":"https://www.examtopics.com/discussions/amazon/view/47115-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true,"question_id":464,"exam_id":32,"discussion":[{"poster":"nitinz","comment_id":"317702","upvote_count":"15","content":"True C is correct. Right from white page from AWS","timestamp":"1632414000.0"},{"timestamp":"1632082560.0","comment_id":"310979","content":"correct answer is C","upvote_count":"7","poster":"anandbabu"},{"timestamp":"1687051140.0","comment_id":"926368","content":"Selected Answer: C\nOption A is incorrect because it uses the AWS-DefaultPatchBaseline instead of the AWS-WindowsPatchBaseline, and it does not address the requirement to avoid simultaneous reboots on all Windows workloads.\n\nOption B is incorrect because it suggests using CloudWatch Events and cron expressions to schedule patching, which does not provide the same level of control as maintenance windows for coordinating patching activities.\n\nOption D is incorrect because it introduces additional complexity with State Manager documents, which are not necessary for the given requirements.\n\nTherefore, the correct choice is C.","poster":"SkyZeroZx","upvote_count":"1"},{"upvote_count":"1","timestamp":"1663381080.0","poster":"Dionenonly","comment_id":"671201","content":"Selected Answer: C\nC as per AWS whitepaper"},{"poster":"dcdcdc3","comment_id":"669072","timestamp":"1663163580.0","upvote_count":"1","content":"https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-about-aws-runpatchbaseline.html\nWill go with B as concurrency can be set on a single group"},{"timestamp":"1661868000.0","poster":"Rocketeer","comment_id":"654093","upvote_count":"1","content":"I think A is correct."},{"content":"C is correct","timestamp":"1650077700.0","poster":"pawanvu","upvote_count":"1","comment_id":"586572"},{"poster":"AzureDP900","comment_id":"494005","upvote_count":"1","content":"C is right","timestamp":"1638658260.0"},{"comment_id":"483747","poster":"acloudguru","timestamp":"1637540280.0","content":"Selected Answer: C\nC Define two non-overlapping AWS Systems Manager maintenance windows + Define two non-overlapping AWS Systems Manager maintenance windows","upvote_count":"2"},{"comment_id":"450105","timestamp":"1635885240.0","content":"Going for C","upvote_count":"1","poster":"Kopa"},{"comment_id":"437899","upvote_count":"1","poster":"tgv","content":"CCC\n---","timestamp":"1635702780.0"},{"content":"I'll go for C","timestamp":"1635650520.0","comment_id":"410469","poster":"WhyIronMan","upvote_count":"2"},{"poster":"Waiweng","comment_id":"346111","timestamp":"1635131040.0","content":"it's C","upvote_count":"3"},{"timestamp":"1635069720.0","content":"Requirement:\nIt is important that EC2 instance reboots do not occur at the same time on all Windows workloads\n\nC Define two non-overlapping AWS Systems Manager maintenance windows + Define two non-overlapping AWS Systems Manager maintenance windows","poster":"ExtHo","upvote_count":"3","comments":[{"upvote_count":"2","poster":"ExtHo","content":"2nd part after + AWS-DefaultPatchBaseline","timestamp":"1635090000.0","comment_id":"335881"}],"comment_id":"335880"},{"content":"C for sure.\nBut if using Rate controls, I think A also works","timestamp":"1634974080.0","comment_id":"335466","upvote_count":"1","poster":"01037"},{"poster":"Sunflyhome","comment_id":"328073","upvote_count":"4","comments":[{"timestamp":"1633479960.0","poster":"SD13","comment_id":"330710","comments":[{"timestamp":"1634525100.0","poster":"Amitv2706","upvote_count":"5","comment_id":"334557","content":"Agree. AWS-RunWindowsPatchBaseline does not exist.\nC is correct."}],"upvote_count":"5","content":"AWS-RunWindowsPatchBaseline does not exist."}],"timestamp":"1632455640.0","content":"B is correct. Within one maintenance window, a rate control can be used to defined to run task. You don't have to split servers into 2 groups , just only, for rebooting them in different time windows.\nhttps://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/"}],"answers_community":["C (100%)"],"answer_ET":"C","answer_images":[],"answer_description":"","timestamp":"2021-03-14 23:41:00","answer":"C","unix_timestamp":1615761660,"question_text":"An internal security audit of AWS resources within a company found that a number of Amazon EC2 instances running Microsoft Windows workloads were missing several important operating system-level patches. A Solutions Architect has been asked to fix existing patch deficiencies, and to develop a workflow to ensure that future patching requirements are identified and taken care of quickly. The Solutions Architect has decided to use AWS Systems Manager. It is important that EC2 instance reboots do not occur at the same time on all Windows workloads to meet organizational uptime requirements.\nWhich workflow will meet these requirements in an automated manner?","question_images":[],"choices":{"C":"Add a Patch Group tag with a value of either Windows Servers1 or Windows Servers2 to all existing EC2 instances. Ensure that all Windows EC2 instances are assigned this tag. Associate the AWS-DefaultPatchBaseline with both Windows Servers patch groups. Define two non-overlapping AWS Systems Manager maintenance windows, conduct patching within them, and associate each with a different patch group. Register targets with specific maintenance windows using the Patch Group tags. Assign the AWS-RunPatchBaseline document as a task within each maintenance window.","A":"Add a Patch Group tag with a value of Windows Servers to all existing EC2 instances. Ensure that all Windows EC2 instances are assigned this tag. Associate the AWS-DefaultPatchBaseline to the Windows Servers patch group. Define an AWS Systems Manager maintenance window, conduct patching within it, and associate it with the Windows Servers patch group. Register instances with the maintenance window using associated subnet IDs. Assign the AWS- RunPatchBaseline document as a task within each maintenance window.","D":"Add a Patch Group tag with a value of either Windows Servers1 or Windows Server2 to all existing EC2 instances. Ensure that all Windows EC2 instances are assigned this tag. Associate the AWS-WindowsPatchBaseline with both Windows Servers patch groups. Define two non-overlapping AWS Systems Manager maintenance windows, conduct patching within them, and associate each with a different patch group. Assign the AWS-RunWindowsPatchBaseline document as a task within each maintenance window. Create an AWS Systems Manager State Manager document to define commands to be executed during patch execution.","B":"Add a Patch Group tag with a value of Windows Servers to all existing EC2 instances. Ensure that all Windows EC2 instances are assigned this tag. Associate the AWS-WindowsPatchBaseline to the Windows Servers patch group. Create an Amazon CloudWatch Events rule configured to use a cron expression to schedule the execution of patching using the AWS Systems Manager run command. Assign the AWS-RunWindowsPatchBaseline document as a task associated with the Windows Servers patch group. Create an AWS Systems Manager State Manager document to define commands to be executed during patch execution."},"topic":"1"},{"id":"mHHCpNFxUPR59A2GmIMu","question_images":[],"question_id":465,"unix_timestamp":1615534800,"answer_ET":"ABE","answer":"ABE","answer_images":[],"question_text":"A company is storing data on Amazon Simple Storage Service (S3). The company's security policy mandates that data is encrypted at rest.\nWhich of the following methods can achieve this? (Choose three.)","exam_id":32,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/46642-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["ABE (100%)"],"discussion":[{"comment_id":"308643","poster":"ppshein","content":"My choice is A,B,E","timestamp":"1632876780.0","upvote_count":"9","comments":[{"poster":"nitinz","content":"only ABE does encryption at rest.","upvote_count":"2","comment_id":"314342","timestamp":"1633912320.0"}]},{"content":"A. Use Amazon S3 server-side encryption with AWS Key Management Service managed keys.\nB. Use Amazon S3 server-side encryption with customer-provided keys.\nE. Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key.","comment_id":"1266370","timestamp":"1723717980.0","poster":"amministrazione","upvote_count":"1"},{"content":"Selected Answer: ABE\nA, B, E is correct","poster":"kuongnp","comment_id":"1245794","timestamp":"1720657140.0","upvote_count":"1"},{"comment_id":"1045769","upvote_count":"1","timestamp":"1697535360.0","content":"Selected Answer: ABE\nYou can either use AWS managed key or Customer Managed Key to perform Server Side S3 bucket encryption, but no EC2 key-pair. EC2 key-pair is used to authenticate via SSH, not encrypt. You can also use your own methods to encrypt the data before upload to S3.","poster":"andersoncarvalho"},{"upvote_count":"1","content":"Wrong answers: C. Use Amazon S3 server-side encryption with EC2 key pair: Amazon S3 does not support using EC2 key pairs for server-side encryption. EC2 key pairs are primarily used for securely accessing EC2 instances.\n\nD. Use Amazon S3 bucket policies to restrict access to the data at rest: Bucket policies are used to control access to objects stored in S3 buckets, but they do not provide encryption at rest. Encryption at rest should be handled through one of the server-side encryption options mentioned above.\n\nOption F is also incorrect:\n\nF. Use SSL to encrypt the data while in transit to Amazon S3: SSL (Secure Sockets Layer) encryption is used to secure the data during transit between the client and the S3 service. While it helps protect data in transit, it does not provide encryption at rest, which is specifically required by the company's security policy.","poster":"ajchi1980","comment_id":"937922","timestamp":"1688035980.0"},{"comment_id":"903542","poster":"SkyZeroZx","content":"Selected Answer: ABE\nMy choice is A,B,E","timestamp":"1684703220.0","upvote_count":"1"},{"poster":"iamRohanKaushik","upvote_count":"1","timestamp":"1679375940.0","comment_id":"845548","content":"Selected Answer: ABE\nABE is correct"},{"content":"Selected Answer: ABE\nA, B & E are only suitable right answer","timestamp":"1678457340.0","comment_id":"835085","poster":"gameoflove","upvote_count":"1"},{"poster":"TigerInTheCloud","comment_id":"734087","upvote_count":"1","content":"Selected Answer: ABE\nC, D, and F are wrong","timestamp":"1670018820.0"},{"comment_id":"681974","poster":"emmanuelodenyire","upvote_count":"1","timestamp":"1664385960.0","content":"Selected Answer: ABE\nI see only these support encryption at rest"},{"content":"I will go for A,B,E","timestamp":"1662160980.0","comment_id":"657902","upvote_count":"1","poster":"skywalker"},{"timestamp":"1652262420.0","comment_id":"600019","upvote_count":"2","poster":"michaelbaib","content":"dont understand why encrypt 3 times??"},{"content":"vote ABE","poster":"bluesmile979","upvote_count":"1","timestamp":"1647409500.0","comment_id":"568789"},{"poster":"cldy","content":"A. Use Amazon S3 server-side encryption with AWS Key Management Service managed keys.\nB. Use Amazon S3 server-side encryption with customer-provided keys.\nE. Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key.","comment_id":"494256","timestamp":"1638702840.0","upvote_count":"1"},{"content":"ABE Correct","comment_id":"405774","poster":"Akhil254","timestamp":"1635752580.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1635345840.0","poster":"kidd5","content":"ABE is correct","comment_id":"360131"},{"content":"ABE is correct","upvote_count":"2","timestamp":"1635086280.0","comment_id":"327950","poster":"anandbabu"},{"upvote_count":"1","comment_id":"323194","timestamp":"1634592300.0","poster":"01037","content":"ABE for sure"},{"timestamp":"1634368860.0","poster":"cldy","comment_id":"322477","content":"A.B.E.","upvote_count":"1"},{"content":"ABE is right","comment_id":"312065","poster":"LoganIsh","upvote_count":"2","timestamp":"1633896300.0"}],"answer_description":"","topic":"1","timestamp":"2021-03-12 08:40:00","choices":{"C":"Use Amazon S3 server-side encryption with EC2 key pair.","A":"Use Amazon S3 server-side encryption with AWS Key Management Service managed keys.","B":"Use Amazon S3 server-side encryption with customer-provided keys.","F":"Use SSL to encrypt the data while in transit to Amazon S3.","D":"Use Amazon S3 bucket policies to restrict access to the data at rest.","E":"Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key."}}],"exam":{"provider":"Amazon","isImplemented":true,"numberOfQuestions":1019,"id":32,"lastUpdated":"11 Apr 2025","isMCOnly":false,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional"},"currentPage":93},"__N_SSP":true}