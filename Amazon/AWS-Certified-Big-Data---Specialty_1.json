{"pageProps":{"questions":[{"id":"wtblFbnqV0pZePZWbFAQ","question_images":[],"discussion":[{"upvote_count":"9","comment_id":"101830","poster":"Corram","timestamp":"1635687300.0","content":"A is correct.\nNot B - never load unstructured data to Redshift\nNot C - s3 event + lambda would be more suitable for incremental, continuous S3-Redshift integration. Here, we have one large bulk load, so event notifications don't make sense and lambda may not be able to handle all transformation in one call due to service limits.\nNot D - Normalization is the act of adjusting values on a scale, usually subtracting mean and dividing by standard deviation. That doesn't make sense here."},{"timestamp":"1636200780.0","content":"A is correct","poster":"TomHanks","comment_id":"156766","upvote_count":"1"},{"content":"A for sure","upvote_count":"1","comment_id":"149920","timestamp":"1635749160.0","poster":"NikkyDicky"},{"comment_id":"61323","content":"A is right","upvote_count":"1","poster":"yuvaraj228","timestamp":"1635669240.0"},{"poster":"san2020","timestamp":"1635415440.0","content":"Selected A.","upvote_count":"1","comment_id":"52129"},{"poster":"shouvanik","comment_id":"50565","timestamp":"1634421120.0","upvote_count":"3","content":"option a is correct. Using EMR, we can process un-structured data, and put schema on top of it before saving it to s3"},{"upvote_count":"2","timestamp":"1633553820.0","poster":"kalpanareddy","comment_id":"41786","content":"Answer is A"},{"content":"Answer is A.","timestamp":"1632351600.0","comment_id":"14948","upvote_count":"2","poster":"M2"},{"poster":"exams","comment_id":"11391","timestamp":"1632079260.0","content":"answer a is correct","upvote_count":"2"}],"answer_ET":"A","answer_description":"","answers_community":[],"answer_images":[],"exam_id":17,"question_id":1,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/5297-exam-aws-certified-big-data-specialty-topic-1-question-1/","unix_timestamp":1568708940,"timestamp":"2019-09-17 10:29:00","topic":"1","answer":"A","choices":{"B":"Load the unstructured data into Redshift, and use string parsing functions to extract structured data for inserting into the analysis schema.","D":"Normalize the data using an AWS Marketplace ETL tool, persist the results to Amazon S3, and use AWS Lambda to INSERT the data into Redshift.","A":"Transform the unstructured data using Amazon EMR and generate CSV data. COPY the CSV data into the analysis schema within Redshift.","C":"When the data is saved to Amazon S3, use S3 Event Notifications and AWS Lambda to transform the file contents. Insert the data into the analysis schema on Redshift."},"question_text":"A data engineer in a manufacturing company is designing a data processing platform that receives a large volume of unstructured data. The data engineer must populate a well-structured star schema in Amazon\nRedshift.\nWhat is the most efficient architecture strategy for this purpose?"},{"id":"BDDGvzEsvNzPfNUN4Kxa","answer":"A","answers_community":[],"exam_id":17,"question_id":2,"isMC":true,"answer_description":"","question_images":[],"question_text":"A company has several teams of analysts. Each team of analysts has their own cluster. The teams need to run\nSQL queries using Hive, Spark-SQL, and Presto with Amazon EMR. The company needs to enable a centralized metadata layer to expose the Amazon S3 objects as tables to the analysts.\nWhich approach meets the requirement for a centralized metadata layer?","url":"https://www.examtopics.com/discussions/amazon/view/3514-exam-aws-certified-big-data-specialty-topic-1-question-10/","choices":{"D":"Naming scheme support with automatic partition discovery from Amazon S3","A":"EMRFS consistent view with a common Amazon DynamoDB table","C":"s3distcp with the outputManifest option to generate RDS DDL","B":"Bootstrap action to change the Hive Metastore to an Amazon RDS database"},"timestamp":"2019-08-13 03:33:00","answer_ET":"A","discussion":[{"content":"correct answer is A. You can check it from https://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-metadata.html","poster":"muhsin","upvote_count":"8","comment_id":"6883","timestamp":"1632292260.0"},{"comments":[{"upvote_count":"1","poster":"itsmedude","comment_id":"134619","timestamp":"1635405240.0","content":"Thanks, i am unable to determine the answer as well, I'm going with B as well looking your clarification.....But, is there anyone here who can confirm if this question is coming in AWS Data analytics specialty exam?"}],"content":"Its B. There are basically three choices of having external Hive style data catalog - Hive catalog hosted in RDS, Glue catalog and pure Hive catalog hosted in EMR. Option A here talks about how EMRFS consistant view uses DynamoDB table as its checkpointing mechanism to track objects to make sure updates/deletes to S3 objects are not served in their default eventual consistancy mode. This dynamodb metadata tables has nothing to do with external Hive metastore/catalog.","timestamp":"1635390480.0","upvote_count":"6","poster":"maxlin","comment_id":"97093"},{"content":"The key sentence is \"to expose the Amazon S3 objects as tables\". So the answer is \"B\". If the sentence is changed to \"to ensure the Amazon S3 objects consistency\", the answer would be \"A\". Trick question.","timestamp":"1636129800.0","poster":"civ","comment_id":"285747","upvote_count":"1"},{"comment_id":"248511","content":"Does this pointer on using Hive elimiate B? \n\"Hive neither supports nor prevents concurrent write access to metastore tables. If you share metastore information between two clusters, you must ensure that you do not write to the same metastore table concurrently, unless you are writing to different partitions of the same metastore table.\"","timestamp":"1635878580.0","poster":"Shivibaheti","upvote_count":"1"},{"poster":"jsr2017","timestamp":"1635753720.0","content":"I think B, A is metadata about S3 path, and they are asking \"as tables\" then B","upvote_count":"1","comment_id":"134834"},{"upvote_count":"1","poster":"Debi_mishra","content":"Answer B. EMRFS consistent view is for different purpose, not for centralised metadata.","timestamp":"1635330540.0","comment_id":"95387"},{"content":"I think it's B. Consistent view using DDB is used to address S3 read after write consistency issue. And we all know best place to store hive meta store is at MySQL which is an RDS. So I think B is the correct answer.","poster":"Josh1981","timestamp":"1635311280.0","comment_id":"93253","upvote_count":"3"},{"comment_id":"88420","timestamp":"1635260400.0","content":"I'm still not sure it its A or B","poster":"srang","upvote_count":"1"},{"poster":"Bulti","upvote_count":"5","content":"Th correct answer is B. The question is asking about exposing a HCatalog to other clusters. It is not about the consistency issue while reading object manipulated by other clusters. Therefore externalizing the Hive Catalog into RDS or creating a Amazon Glue catalog table that all clusters in the region can access is the right answer.\n\nWhen you create a cluster with consistent view enabled, Amazon EMR uses an Amazon DynamoDB database to store object metadata and track consistency with Amazon S3. If consistent view determines that Amazon S3 is inconsistent during a file system operation, it retries that operation according to rules that you can define.","comment_id":"79514","timestamp":"1635253380.0"},{"poster":"Bulti","comment_id":"76496","timestamp":"1635134700.0","upvote_count":"1","content":"Answer : A - EMRFS consistent view tracks consistency using a DynamoDB table to track objects in Amazon S3 that have been synced with or created by EMRFS. All EMRFS clusters can use the same Dynamo DB table for each object in S3 whose metadata needs to be made available centrally across all analyst clusters."},{"comment_id":"74487","timestamp":"1635005040.0","content":"Correct Answer is B as two options for metastore one is AWS GLUE Data Catalog and second is RDS for External Metastore for Hive","poster":"YashBindlish","upvote_count":"2"},{"poster":"sam3787","content":"I think i will go with B\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html","upvote_count":"3","timestamp":"1634873400.0","comment_id":"55442"},{"upvote_count":"1","timestamp":"1634635680.0","poster":"san2020","comment_id":"52314","content":"my selection B"},{"comment_id":"42104","poster":"kalpanareddy","upvote_count":"1","timestamp":"1634527920.0","content":"A is correct anser\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emrfs-metadata.html"},{"comment_id":"21230","timestamp":"1634197500.0","comments":[{"content":"Agreed, the answer is B. Hive, SparkSQL and Presto all need a consistent metadata store to be able to query. Hive Metastore can be configured to be be in RDS (not DynamoDB).","timestamp":"1634365020.0","poster":"Kuntazulu","comment_id":"37285","upvote_count":"1"}],"upvote_count":"2","poster":"viduvivek","content":"Answer is B. \n\nConsistent view addresses an issue that can arise due to the Amazon S3 Data Consistency Model. \n\nFor example, if you add objects to Amazon S3 in one operation and then immediately list objects in a subsequent operation, the list and the set of objects processed may be incomplete."},{"timestamp":"1633972860.0","upvote_count":"3","comment_id":"19842","poster":"d00ku","content":"The EMRFS consisten view table does not hold any metadata about the structure inside the files or tables. A proper metadata store is required here as per the question. B should be the correct one."},{"poster":"M2","content":"A looks correct. When you create a cluster with consistent view enabled, Amazon EMR uses an Amazon DynamoDB database to store object metadata and track consistency with Amazon S3.","upvote_count":"3","timestamp":"1633853520.0","comment_id":"14960"},{"timestamp":"1633466520.0","content":"A, Presto does not give a shit to Hive if B is chosen","comment_id":"13634","poster":"pkfe","upvote_count":"2"},{"upvote_count":"2","comment_id":"11400","timestamp":"1633435860.0","content":"I think A it is","poster":"exams"},{"upvote_count":"2","comment_id":"11212","timestamp":"1633434000.0","poster":"apertus","content":"Shoud be B, refer link:https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html"},{"upvote_count":"3","content":"A is correct","poster":"pra276","timestamp":"1632862800.0","comment_id":"7515"},{"comment_id":"7388","upvote_count":"3","content":"A is correct answer","poster":"Jialu","timestamp":"1632294600.0"},{"poster":"mattyb123","comments":[{"poster":"muhsin","comment_id":"8576","upvote_count":"3","content":"for multiple cluster, we need to use EMRFS Consistent View Metadata","timestamp":"1633348860.0"}],"timestamp":"1632173220.0","upvote_count":"5","comment_id":"6640","content":"It's B, all the tools mentioned are part of Hadoop ecosystem and can share metadata by using HCatalog. To do that the metastore must persist on an external DB like RDS."}],"answer_images":[],"unix_timestamp":1565659980,"topic":"1"},{"id":"C4exXQ5nmgJprYBjin7q","url":"https://www.examtopics.com/discussions/amazon/view/4340-exam-aws-certified-big-data-specialty-topic-1-question-11/","unix_timestamp":1567115640,"question_images":[],"isMC":true,"question_id":3,"topic":"1","question_text":"An administrator needs to manage a large catalog of items from various external sellers. The administrator needs to determine if the items should be identified as minimally dangerous, dangerous, or highly dangerous based on their textual descriptions. The administrator already has some items with the danger attribute, but receives hundreds of new item descriptions every day without such classification.\nThe administrator has a system that captures dangerous goods reports from customer support team of from user feedback.\nWhat is a cost-effective architecture to solve this issue?","exam_id":17,"choices":{"B":"Build a Kinesis Streams process that captures and marks the relevant items in the dangerous goods reports using a Lambda function once more than two reports have been filed.","A":"Build a set of regular expression rules that are based on the existing examples, and run them on the DynamoDB Streams as every new item description is added to the system.","D":"Build a machine learning model with binary classification for dangerous goods and run it on the DynamoDB Streams as every new item description is added to the system.","C":"Build a machine learning model to properly classify dangerous goods and run it on the DynamoDB Streams as every new item description is added to the system."},"answer":"C","answer_ET":"C","timestamp":"2019-08-29 23:54:00","discussion":[{"poster":"mattyb123","comment_id":"8920","content":"C is correct. Binary only allows for two possible classes.","timestamp":"1632784560.0","upvote_count":"5"},{"comment_id":"97123","comments":[{"comment_id":"100342","upvote_count":"1","content":"Yeah, so if C were true, D would definitely also be true - another reason why D is wrong. Anyways, the technical reason for D being wrong is that binary classification only allows two possible classes (e.g. not dangerous and dangerous), but here we have three classes (\"minimally dangerous\", \"dangerous\" and \"highly dangerous\").","poster":"Corram","timestamp":"1635778140.0"}],"content":"Shouldn't it be D? D specifically talks about 'binary classification' but C talks about 'proper' classification.","timestamp":"1635687000.0","poster":"maxlin","upvote_count":"1"},{"comment_id":"93256","timestamp":"1635633960.0","poster":"Josh1981","content":"Either C or D, but it's not a binary classification problem so C is more appropriate.","upvote_count":"1"},{"upvote_count":"2","content":"my selection C","poster":"san2020","comment_id":"52317","timestamp":"1635423480.0"},{"poster":"kalpanareddy","timestamp":"1634928120.0","comment_id":"42106","upvote_count":"2","content":"C is correct as the problem statement itself saying its classification problem not prediction\n(Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule.)"},{"content":"C looks correct as others are not making much sense","upvote_count":"2","poster":"M2","comment_id":"14962","timestamp":"1634377020.0"},{"timestamp":"1633675140.0","poster":"exams","content":"Agree with C","upvote_count":"1","comment_id":"11401"},{"poster":"mattyb123","content":"AML does not support dynamodb so C and D are incorrect. I am thinking B","timestamp":"1632899460.0","comments":[{"timestamp":"1633141740.0","upvote_count":"1","comment_id":"11037","poster":"apertus","content":"It did not mention AML, C should be correct. similar case: https://aws.amazon.com/cn/blogs/machine-learning/anomaly-detection-on-amazon-dynamodb-streams-using-the-amazon-sagemaker-random-cut-forest-algorithm/"}],"upvote_count":"1","comment_id":"9721"}],"answers_community":[],"answer_description":"","answer_images":[]},{"id":"lVKWFduKMgktMhRXG42n","question_images":[],"discussion":[{"poster":"learner_iq","timestamp":"1635738420.0","comment_id":"120864","upvote_count":"1","content":"D is correct answer which is also regular practice."},{"upvote_count":"2","comment_id":"118149","poster":"scsb","timestamp":"1635423420.0","content":"I choose D"},{"poster":"Josh1981","comment_id":"93261","upvote_count":"3","content":"I'll go with D. DDB is more apt for storing configuration data instead of storing in a cache. Also Redis would have limitation is size so it's not a scale-able approach.","timestamp":"1635404520.0"},{"content":"D is the Correct Answer","upvote_count":"1","comment_id":"74490","poster":"YashBindlish","timestamp":"1635248640.0"},{"content":"my selection D","comment_id":"52318","poster":"san2020","upvote_count":"1","timestamp":"1635067440.0"},{"poster":"sck","comment_id":"46837","timestamp":"1634941620.0","upvote_count":"1","content":"agree, it should be D"},{"timestamp":"1634692020.0","comment_id":"39466","content":"AWS Big Data Blog post describes exactly option D\nhttps://aws.amazon.com/blogs/big-data/how-expedia-implemented-near-real-time-analysis-of-interdependent-datasets/\nRedis is costlier, need to run an instance continuously for the caching.\nhttps://aws.amazon.com/elasticache/pricing/\nFinally, Option D is the correct answer","upvote_count":"3","poster":"ME2000"},{"content":"Compared with D, option C redis is much cheaper than dynamodb.","timestamp":"1634558940.0","poster":"shwang","comment_id":"31388","upvote_count":"1"},{"timestamp":"1634295840.0","upvote_count":"1","poster":"cybe001","comment_id":"19181","content":"A, RDS is less expensive than Dynamo"},{"comment_id":"14964","timestamp":"1633940220.0","poster":"M2","content":"D is right as others are not cost effective compared to D","upvote_count":"2"},{"poster":"exams","content":"I think D","upvote_count":"2","comment_id":"11402","timestamp":"1633537560.0"},{"upvote_count":"2","timestamp":"1632937740.0","comment_id":"7516","poster":"pra276","content":"Answer is D: Since the question was s scalable and cost effective solution."},{"comments":[{"comment_id":"6636","comments":[{"poster":"mattyb123","timestamp":"1632849780.0","content":"Yes, due to the cost effectiveness","comment_id":"6800","upvote_count":"2"}],"upvote_count":"1","poster":"Jialu","content":"Because of the cost-effective ?","timestamp":"1632824760.0"}],"poster":"mattyb123","comment_id":"6470","upvote_count":"3","timestamp":"1632431340.0","content":"agreed. d"},{"poster":"jlpl","upvote_count":"1","content":"d? any thoughts?","comment_id":"6448","timestamp":"1632415800.0"}],"answer_ET":"C","answer_description":"","answers_community":[],"answer_images":[],"exam_id":17,"question_id":4,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/3423-exam-aws-certified-big-data-specialty-topic-1-question-12/","unix_timestamp":1565439000,"timestamp":"2019-08-10 14:10:00","topic":"1","choices":{"B":"Maintain data dependency information in an Amazon DynamoDB table. Use Amazon SNS and event notifications to publish data to fleet of Amazon EC2 workers. Once the task dependencies have been resolved, process the data with Amazon EMR.","C":"Maintain data dependency information in an Amazon ElastiCache Redis cluster. Use Amazon S3 event notifications to trigger an AWS Lambda function that maps the S3 object to Redis. Once the task dependencies have been resolved, process the data with Amazon EMR.","D":"Maintain data dependency information in an Amazon DynamoDB table. Use Amazon S3 event notifications to trigger an AWS Lambda function that maps the S3 object to the task associated with it in DynamoDB. Once all task dependencies have been resolved, process the data with Amazon EMR.","A":"Maintain data dependency information in Amazon RDS for MySQL. Use an AWS Data Pipeline job to load an Amazon EMR Hive table based on task dependencies and event notification triggers in Amazon S3."},"answer":"C","question_text":"A company receives data sets coming from external providers on Amazon S3. Data sets from different providers are dependent on one another. Data sets will arrive at different times and in no particular order.\nA data architect needs to design a solution that enables the company to do the following:\n✑ Rapidly perform cross data set analysis as soon as the data becomes available\n✑ Manage dependencies between data sets that arrive at different times\nWhich architecture strategy offers a scalable and cost-effective solution that meets these requirements?"},{"id":"3vu0QsrR2MLr4qnAMk9Y","answers_community":[],"answer_ET":"D","unix_timestamp":1567903320,"discussion":[{"comment_id":"52326","upvote_count":"1","poster":"san2020","timestamp":"1635329520.0","content":"my selection D"},{"poster":"kalpanareddy","upvote_count":"2","comment_id":"42111","timestamp":"1634120100.0","content":"looks D is appropriate answer\nhttps://aws.amazon.com/blogs/big-data/processing-amazon-dynamodb-streams-using-the-amazon-kinesis-client-library/"},{"timestamp":"1633275900.0","content":"D is the right answer","comment_id":"13735","poster":"bigdatalearner","upvote_count":"1"},{"upvote_count":"2","content":"Checkpoint helps to increase performance... D looks correct","comment_id":"11403","timestamp":"1632762480.0","poster":"exams"},{"content":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-ddb.html","comment_id":"10109","poster":"Hitu","timestamp":"1632193080.0","upvote_count":"3"}],"timestamp":"2019-09-08 02:42:00","isMC":true,"choices":{"B":"Increase the size of the Amazon EC2 instances to increase network throughput.","C":"Increase the minimum number of instances in the Auto Scaling group.","D":"Increase Amazon DynamoDB throughput on the checkpoint table.","A":"Increase the number of shards in the Amazon Kinesis stream to 80 for greater concurrency."},"question_id":5,"answer":"D","answer_images":[],"topic":"1","exam_id":17,"question_text":"A media advertising company handles a large number of real-time messages sourced from over 200 websites in real time. Processing latency must be kept low. Based on calculations, a 60-shard Amazon Kinesis stream is more than sufficient to handle the maximum data throughput, even with traffic spikes. The company also uses an Amazon Kinesis Client Library (KCL) application running on Amazon Elastic Compute Cloud (EC2) managed by an Auto Scaling group. Amazon CloudWatch indicates an average of 25% CPU and a modest level of network traffic across all running servers.\nThe company reports a 150% to 200% increase in latency of processing messages from Amazon Kinesis during peak times. There are NO reports of delay from the sites publishing to Amazon Kinesis.\nWhat is the appropriate solution to address the latency?","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/4879-exam-aws-certified-big-data-specialty-topic-1-question-13/","question_images":[]}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":85,"provider":"Amazon","isMCOnly":true,"id":17,"isImplemented":true,"name":"AWS Certified Big Data - Specialty","isBeta":false},"currentPage":1},"__N_SSP":true}