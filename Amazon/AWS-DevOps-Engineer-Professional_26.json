{"pageProps":{"questions":[{"id":"imL2oya7BWsvLxevtNa8","isMC":true,"answer_ET":"B","exam_id":35,"question_id":126,"answer":"B","answers_community":["B (80%)","10%","10%"],"choices":{"B":"Implement AWS CloudWatch Events for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders of deployment issues.","C":"Implement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders of deployment issues.","A":"Implement AWS CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders of deployment issues.","D":"Implement AWS CloudWatch Events for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code deployment issues, and create an Amazon SNS topic to notify stakeholders of deployment issues."},"question_text":"An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for each CodePipeline stage.\nDuring a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notifications during deployment to decrease resolution times.\nWhat should the DevOps Engineer do to create notifications when issues are discovered?","topic":"1","discussion":[{"upvote_count":"15","comment_id":"21925","timestamp":"1632164880.0","content":"I think the answer is B","comments":[{"timestamp":"1632380940.0","comment_id":"24207","poster":"BeastX","content":"I agree.","upvote_count":"1"}],"poster":"Hyunseok"},{"content":"I'll go with B","comment_id":"223310","timestamp":"1636096920.0","upvote_count":"6","poster":"jackdryan"},{"timestamp":"1690926960.0","poster":"ioripark","content":"Selected Answer: B\nI think B","upvote_count":"1","comment_id":"969439"},{"timestamp":"1679113260.0","upvote_count":"2","content":"Selected Answer: B\nAWS CloudWatch Events can be used to monitor events across different AWS resources, and a CloudWatch Event Rule can be created to trigger an AWS Lambda function when a deployment issue is detected in the pipeline. The Lambda function can then evaluate the issue and send a notification to the appropriate stakeholders through an Amazon SNS topic. This approach allows for real-time notifications and faster resolution times.","comment_id":"842453","poster":"Tika01"},{"content":"Selected Answer: B\nB looks good.","poster":"sasa33_p","comment_id":"826052","upvote_count":"1","timestamp":"1677686520.0"},{"comment_id":"797968","comments":[{"upvote_count":"1","poster":"Piccaso","content":"Sorry, I was wrong with A. AWS Config is to assess configuration, does not help to evaluate code deployment issues.","comment_id":"797973","timestamp":"1675516500.0"}],"upvote_count":"1","poster":"Piccaso","content":"Selected Answer: A\nA is the most managed approach.","timestamp":"1675516260.0"},{"content":"Correct answer is B. Using CloudWatch event, we can detect CodeDeploy State changes like Failures and then chose Lambda to determine the nature of the Deployment failure and have the root cause sent to SNS which can send an email to the subscribers.","upvote_count":"1","comment_id":"777278","poster":"Bulti","timestamp":"1673838420.0"},{"content":"It is B","comment_id":"753703","timestamp":"1671748080.0","poster":"saeidp","upvote_count":"1"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html","poster":"Maygam","comment_id":"739867","timestamp":"1670569740.0","upvote_count":"1"},{"poster":"MikeyJ","timestamp":"1662883680.0","content":"Selected Answer: B\nYou can use Amazon CloudWatch Events to detect and react to changes in the state of an instance or a deployment (an \"event\") in your CodeDeploy operations. Then, based on rules you create, CloudWatch Events will invoke one or more target actions when a deployment or instance enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions. You can select the following types of targets when using CloudWatch Events as part of your CodeDeploy operations:\n\nAWS Lambda functions\n\nKinesis streams\n\nAmazon SQS queues\n\nBuilt-in targets (EC2 CreateSnapshot API call, EC2 RebootInstances API call, EC2 StopInstances API call , and EC2 TerminateInstances API call)\n\nAmazon SNS topics\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html","upvote_count":"4","comment_id":"665938"},{"poster":"gmandala","comment_id":"252417","upvote_count":"2","content":"B it is","timestamp":"1636197240.0"},{"comment_id":"207579","poster":"ChauPhan","timestamp":"1635573360.0","upvote_count":"2","content":"B is the right one."},{"comment_id":"126210","poster":"df1228","upvote_count":"1","timestamp":"1635213180.0","content":"I go with B"},{"timestamp":"1634924580.0","comment_id":"82527","poster":"secreatUser","content":"I will stick with B.","upvote_count":"1"},{"poster":"Ebi","content":"Answer is ABE","comment_id":"62696","timestamp":"1634534220.0","upvote_count":"1","comments":[{"timestamp":"1634579460.0","upvote_count":"1","poster":"Ebi","content":"This is actually for question 99","comment_id":"64993"}]},{"poster":"yassu","content":"B is correct","timestamp":"1634258220.0","upvote_count":"1","comment_id":"62220"},{"upvote_count":"1","timestamp":"1633568880.0","content":"B looks right","poster":"toshko85","comment_id":"58713"},{"comment_id":"56746","content":"It is B","poster":"xaocho","timestamp":"1633360080.0","upvote_count":"1"},{"poster":"SamuelK","timestamp":"1633221540.0","content":"It should be A.\nD is wrong because AWS Inspector can't help here\nB and C is wrong because it's easier to look at CloudWatch Log then to create Lambda to evaluate code deployment issues.","comments":[{"upvote_count":"2","timestamp":"1633743720.0","comment_id":"59688","content":"Who will look at cloudwatch Log? according to your saying, there should be a person who will monitor the logs all the time. Is that possible?","poster":"hendry"},{"comment_id":"207580","timestamp":"1636090200.0","poster":"ChauPhan","upvote_count":"1","content":"AWS Config is not relevant here"}],"upvote_count":"1","comment_id":"53250"},{"timestamp":"1632997080.0","comment_id":"52828","upvote_count":"1","comments":[{"timestamp":"1635020820.0","upvote_count":"2","poster":"AWStamR","comment_id":"102777","content":"AWS Config if for checking configurations. i think the answer is B"}],"content":"A is right \nhttps://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html","poster":"AlexTun"},{"timestamp":"1632619380.0","comment_id":"37303","content":"I go with B","upvote_count":"2","poster":"dinhvu"},{"comment_id":"34190","poster":"un","upvote_count":"3","content":"B is the right option","timestamp":"1632439260.0"},{"timestamp":"1632432600.0","upvote_count":"1","content":"It should be B","comment_id":"31509","poster":"ppshein"},{"comment_id":"28429","upvote_count":"1","content":"I go with B","poster":"jiedee","timestamp":"1632416220.0"}],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/8320-exam-aws-devops-engineer-professional-topic-1-question-24/","answer_images":[],"question_images":[],"timestamp":"2019-11-16 10:33:00","unix_timestamp":1573896780},{"id":"w6dNBaeqPjTZgLFW0Ydb","topic":"1","choices":{"D":"Check to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs.","A":"Check that an Amazon CloudWatch Events rule has been created for the master branch to trigger the pipeline.","B":"Check that the CodePipeline service role has permission to access the CodeCommit repository.","C":"Check that the developer's IAM role has permission to push to the CodeCommit repository."},"answers_community":["A (75%)","13%","13%"],"question_images":[],"answer_ET":"A","question_text":"A development team is using AWS CodeCommit to version control application code and AWS CodePipeline to orchestrate software deployments. The team has decided to use a remote master branch as the trigger for the pipeline to integrate code changes. A developer has pushed code changes to the CodeCommit repository, but noticed that the pipeline had no reaction, even after 10 minutes.\nWhich of the following actions should be taken to troubleshoot this issue?","question_id":127,"discussion":[{"poster":"WhyIronMan","timestamp":"1633747020.0","comment_id":"326025","upvote_count":"18","comments":[{"content":"You are amazing.","poster":"Piccaso","upvote_count":"1","timestamp":"1675517580.0","comment_id":"797993"}],"content":"I'll go with A\nWhen you create a pipeline from CodePipeline during the step-by-step it creates a CloudWatch Event rule for a given branch and repo\nlike this:\n{\n \"source\": [\n \"aws.codecommit\"\n ],\n \"detail-type\": [\n \"CodeCommit Repository State Change\"\n ],\n \"resources\": [\n \"arn:aws:codecommit:us-east-1:xxxxx:repo-name\"\n ],\n \"detail\": {\n \"event\": [\n \"referenceCreated\",\n \"referenceUpdated\"\n ],\n \"referenceType\": [\n \"branch\"\n ],\n \"referenceName\": [\n \"master\"\n ]\n }\n}"},{"content":"A\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html","poster":"rscloud","comment_id":"311932","upvote_count":"6","timestamp":"1633091580.0"},{"comment_id":"1134956","poster":"xdkonorek2","upvote_count":"1","timestamp":"1706533440.0","content":"Selected Answer: B\nI'll go with B\nit's necessary for codepipeline to have codecommit permissions while it's not necessary to depend on a rule to start a pipeline"},{"poster":"nicat","content":"Selected Answer: D\nit is not necessary to create an Amazon CloudWatch Events rule specifically for triggering the pipeline in this scenario. The CodePipeline can be set up to directly monitor the CodeCommit repository for changes without the need for an additional CloudWatch Events rule.\nCreating an Amazon CloudWatch Events rule is typically required when you want to trigger an action based on events occurring in AWS services. However, in this case, CodePipeline is already integrated with CodeCommit and can directly monitor the repository for changes without the need for an additional CloudWatch Events rule.\n\nWhen you set up an AWS CodePipeline, the pipeline's execution events and logs are automatically sent to Amazon CloudWatch Logs. CodePipeline integrates with CloudWatch Logs to capture and store the logs for pipeline executions.","comment_id":"894349","upvote_count":"1","timestamp":"1683750540.0"},{"content":"Selected Answer: A\nSince the pipeline is triggered by changes in the remote master branch, an Amazon CloudWatch Events rule must be created for the branch to trigger the pipeline. Therefore, the first step in troubleshooting the issue should be to check that an Amazon CloudWatch Events rule has been created for the master branch to trigger the pipeline.","poster":"Tika01","comment_id":"842455","timestamp":"1679113380.0","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: A\nC was excluded in the first round. The developer's IAM role's permission should not be related.\nD looks unrelated.\nB is weird.","poster":"Piccaso","comment_id":"797992","timestamp":"1675517520.0"},{"content":"Answer A is correct.","poster":"Bulti","upvote_count":"1","comment_id":"777280","timestamp":"1673838540.0"},{"comment_id":"760305","poster":"ceros399","timestamp":"1672259760.0","content":"Selected Answer: A\nA:\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/triggering.html\n\nat the begining I though it was B, but If it were a permission problem you might have had an error pointing to it.","upvote_count":"1"},{"content":"Selected Answer: A\n\"the pipeline had no reaction\" => pipeline is not run, so BCD rule out","comment_id":"745503","timestamp":"1671056940.0","poster":"luk3k0","upvote_count":"1"},{"content":"Both A & B are possible to have issue and need to check","upvote_count":"1","poster":"kyozanuro","timestamp":"1668872880.0","comment_id":"722090"},{"upvote_count":"1","comment_id":"671860","comments":[{"upvote_count":"1","poster":"developer_404","content":"When you use the CodePipeline console to create a pipeline, events are enabled by default. In that case, we have to first verify whether the events are created or not. Then the 2nd step would be ensuring the IAM role also has been created with proper permissions.","timestamp":"1668042420.0","comment_id":"714836"}],"timestamp":"1663458840.0","poster":"lmimi","content":"I think B is also correct. As a pipeline trigger, it can be triggered by cloudwatch event, or it can periodically check the repository for update. If latter, then B is also correct."},{"poster":"sg0206","content":"A is correct \nB & C are wrong as developer is able to push the code that mean he has access.\nD is can be another option but it has not reaction means even logs are not generated","comment_id":"510791","timestamp":"1640665080.0","upvote_count":"2"},{"comment_id":"427979","timestamp":"1636095660.0","poster":"PatrickLi","content":"The question is asking what could be the possible cause of the problem. The answers are to check the things that is required to make the pipeline work. A CloudWatch Event can help in troubleshoot but it is not required to make it work, nor it can identify what is causing the problem. So I say B is more appropriate.","upvote_count":"3"},{"comments":[{"timestamp":"1635445260.0","poster":"amehim","content":"A developer has pushed code\nchanges to the CodeCommit repository ......so this shows that the developer has permission already. I go with A.","upvote_count":"1","comment_id":"423807"}],"comment_id":"378038","upvote_count":"2","poster":"kdpeiris","content":"C\nPermissions required to use the CodeCommit console\nTo allow users to use the CodeCommit console, the administrator must grant them permissions for CodeCommit actions. For example, you could attach the AWSCodeCommitPowerUser managed policy or its equivalent to a user or group.\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control-iam-identity-based-access-control.html","timestamp":"1635201720.0"},{"upvote_count":"2","timestamp":"1634996280.0","poster":"devopp","content":"Strange Question as no Context to troubleshoot. Sure A, but surely could be any number of other options as well ?","comment_id":"330228"},{"comment_id":"311980","poster":"faltu1985","timestamp":"1633190580.0","content":"Ans is A","upvote_count":"3"},{"timestamp":"1632208860.0","content":"ans: A","comment_id":"310321","poster":"Rajarshi","upvote_count":"2"}],"answer_images":[],"unix_timestamp":1615702680,"timestamp":"2021-03-14 07:18:00","exam_id":35,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/47016-exam-aws-devops-engineer-professional-topic-1-question-25/","isMC":true,"answer":"A"},{"id":"PSDc96xu9w32SNPvSL8G","choices":{"A":"The networking configuration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the CodeDeploy endpoint cannot be reached.","B":"The IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint.","C":"The target EC2 instances were not properly registered with the CodeDeploy endpoint.","E":"The appspec.yml file was not included in the application revision.","D":"An instance profile with proper permissions was not attached to the target EC2 instances."},"discussion":[{"timestamp":"1634951940.0","upvote_count":"14","poster":"WhyIronMan","content":"I'll go with A,D","comment_id":"326746","comments":[{"comment_id":"326748","upvote_count":"13","content":"Your instance might not be able to reach the CodeDeploy or S3 public endpoint using port 443. Try one of the following:\n\nIf an instance is provisioned in a private subnet, use a NAT gateway instead of an internet gateway in the route table. For more information, see NAT Gateways.\n\nThe instance you're deploying to might not have an IAM instance profile attached, or it might have an IAM instance profile attached that does not have the required permissions.","poster":"WhyIronMan","timestamp":"1635132060.0"}]},{"upvote_count":"2","timestamp":"1704622200.0","content":"AD is answer","poster":"hotblooded","comment_id":"1115706"},{"comment_id":"942514","content":"Selected Answer: AD\nThe correct answers are A and D and here's why:\nWhen the deployment or all lifecycle events are skipped whether on EC2 or onpremise instances, one of the following errors could occur:\n+\"The overall deployment failed because too many individual instances failed deployment\"\n+\"Too few healthy instances are available for deployment\"\n+\"Some instances in your deployment group are experiencing problems. (Error code: HEALTH_CONSTRAINTS)\"\n\nAnd most of the time, these errors are related to:\n1 - Codedeploy Agents is not installed/running on EC2/On-premise servers\n2 - IAM instance profile missing some permissions\n3 - Network communication (i.e. the agent on EC2/on premise server cannot reach the CodeDeploy endpoint via internet because of a proxy restricting communication etc.)\n4 - Time mismatch between codedeploy and codedeploy agent on EC2/On premise.","upvote_count":"4","timestamp":"1688459220.0","poster":"m4r0ck"},{"upvote_count":"1","comment_id":"842457","poster":"Tika01","content":"Selected Answer: AC\nOption A is a possible reason for the failure because CodeDeploy uses an endpoint in the CodeDeploy service to receive and manage deployments, and if the instances cannot reach the endpoint, the deployment will fail.\n\nOption C is another possible reason for the failure because CodeDeploy needs to know which instances should receive the deployment, and if they are not properly registered with the deployment group, the deployment will not be executed on them.","timestamp":"1679113620.0"},{"content":"A and D are the right answers.","poster":"Sadeel","upvote_count":"1","comment_id":"829102","timestamp":"1677945960.0"},{"timestamp":"1676630640.0","comment_id":"811763","content":"Selected Answer: AD\nAD is the best answer","upvote_count":"1","poster":"Murimi"},{"content":"Selected Answer: AD\nSabreen_Salama explanation.","upvote_count":"1","poster":"bihani","comment_id":"803982","timestamp":"1676005380.0"},{"content":"the answer is AD as below documentation https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events","timestamp":"1675795200.0","comment_id":"801289","poster":"Sabreen_Salama","upvote_count":"2"},{"comment_id":"798014","upvote_count":"1","timestamp":"1675519140.0","comments":[{"upvote_count":"1","poster":"Piccaso","content":"Sorry, AD are the answers \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events","comment_id":"798015","timestamp":"1675519500.0"}],"poster":"Piccaso","content":"Selected Answer: CD\nA B E do not match the \"Skipped\" status"},{"poster":"rrshah83","timestamp":"1674553380.0","comment_id":"786372","upvote_count":"2","content":"Selected Answer: AD\nADE all seem possible causes. But if appspec is missing, the error will call that out. It won’t just skip events."},{"timestamp":"1673840940.0","poster":"Bulti","upvote_count":"1","content":"Answer is A and D. There is a slight chance that the answer is D and E as well. But I will go with A and D. E is a possibility because all EC2 lifecycle events are supposed to be skipped when the Deployment ID is in the skipped state. And when the documentation mentions EC2 lifecycle events I am not sure if it means the CodeDeploy lifecycle hook events for EC2. If so then none of those lifecycle hook events can be executed if there is a missing appspec.yml file.","comment_id":"777293"},{"upvote_count":"1","timestamp":"1672018080.0","content":"A,D. \nI spent a bunch of time reviewing this \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events","comment_id":"756120","poster":"apcloud"},{"upvote_count":"1","timestamp":"1671748920.0","content":"A and D","comment_id":"753707","poster":"saeidp"},{"poster":"Maygam","timestamp":"1670571060.0","content":"Selected Answer: DE\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html","upvote_count":"2","comment_id":"739877"},{"poster":"developer_404","timestamp":"1668042840.0","upvote_count":"2","content":"Selected Answer: AD\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events","comment_id":"714841"},{"timestamp":"1666738140.0","content":"Selected Answer: AD\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events","upvote_count":"1","comment_id":"704223","poster":"nzin4x"},{"timestamp":"1663116360.0","content":"ACD\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events","poster":"Manh","comment_id":"668518","upvote_count":"1","comments":[{"comment_id":"681496","content":"A & D is correct.\nC answer is talking about IAM role - who will execute code deploy/pipeline.\nBut the link being refer. It's talking about Assume role between codedeploy and target endpoint.","upvote_count":"1","poster":"quixo","timestamp":"1664351760.0"}]},{"upvote_count":"1","content":"Selected Answer: AD\nOther than A which is quite obviously one of the 2 ans, i pick D as the 2nd part as the others are quite off.","comment_id":"655724","poster":"colinquek","timestamp":"1662001920.0"},{"poster":"[Removed]","timestamp":"1650644340.0","comment_id":"590106","content":"Selected Answer: AD\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events","upvote_count":"2"},{"comment_id":"391200","poster":"RLai","timestamp":"1635607620.0","upvote_count":"1","content":"I picked A & D"},{"timestamp":"1635316020.0","upvote_count":"2","comment_id":"351820","content":"A, C, D are right as per this reference: https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-even","poster":"JohnnieWalker"},{"comment_id":"323262","poster":"Jordanro","timestamp":"1633571100.0","content":"I will go with A,D","upvote_count":"1"},{"poster":"Bmaster","timestamp":"1633265700.0","content":"I will go with B D","upvote_count":"1","comment_id":"319002"},{"timestamp":"1632540300.0","comments":[{"timestamp":"1633612680.0","content":"You say CD bot your reference is for AD","poster":"WhyIronMan","upvote_count":"2","comment_id":"326745"}],"poster":"1234567J","upvote_count":"1","comment_id":"316949","content":"ans: CD \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events"}],"answer_images":[],"question_images":[],"answers_community":["AD (78%)","11%","6%"],"topic":"1","answer_ET":"AD","answer":"AD","url":"https://www.examtopics.com/discussions/amazon/view/47925-exam-aws-devops-engineer-professional-topic-1-question-26/","answer_description":"","question_id":128,"exam_id":35,"timestamp":"2021-03-22 08:56:00","isMC":true,"unix_timestamp":1616399760,"question_text":"A DevOps engineer is deploying a new version of a company's application in an AWS CodeDeploy deployment group associated with its Amazon EC2 instances.\nAfter some time, the deployment fails. The engineer realizes that all the events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.\nWhat are valid reasons for this failure? (Choose two.)"},{"id":"Aehf7OJJDZmT1CG89Vsl","discussion":[{"comment_id":"316938","content":"ans: D","upvote_count":"15","timestamp":"1632504060.0","poster":"1234567J"},{"timestamp":"1632674160.0","comment_id":"335046","content":"The answer is D. EventBridge is needed to detect the database failure. Lambda is needed to promote the replica as it's in another Region (manual promotion, otherwise). Storing and updating the endpoint in Parameter store is important in updating the application. Look at High Availability section of Aurora FAQ:\nhttps://aws.amazon.com/rds/aurora/faqs/","poster":"sb333","upvote_count":"13"},{"upvote_count":"1","comment_id":"815987","content":"Option D is the correct solution because it uses AWS Systems Manager Parameter Store to store the Aurora endpoint and Amazon EventBridge (Amazon CloudWatch Events) to detect database failures. It runs an AWS Lambda function to promote the read replica and update the endpoint URL stored in Parameter Store, which can be reloaded by the application if a database connection fails.","timestamp":"1676933700.0","poster":"DevOpsJagadGuru"},{"content":"Selected Answer: D\nCorrect Answer is D by method of elimination and either way, parameter store can be used to store configuration. Cloudtrail is not applicable to this question","upvote_count":"2","poster":"Murimi","timestamp":"1676631900.0","comment_id":"811783"},{"poster":"Piccaso","comments":[{"comment_id":"1205470","content":"Configure AWS CloudTrail to run an AWS Lambda function. Are you sure ? I don't think so. D is correct answer","upvote_count":"1","poster":"vn_thanhtung","timestamp":"1714648500.0"},{"content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Overview.Endpoints.HA","comment_id":"813021","upvote_count":"1","timestamp":"1676727480.0","comments":[{"timestamp":"1676727600.0","poster":"Piccaso","content":"https://docs.aws.amazon.com/lambda/latest/dg/with-cloudtrail.html","comment_id":"813023","upvote_count":"1"}],"poster":"Piccaso"}],"comment_id":"807815","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoint.Tutorial","timestamp":"1676323380.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nA looks most automatical.","timestamp":"1675519740.0","comments":[{"comment_id":"798017","poster":"Piccaso","upvote_count":"2","comments":[{"content":"CloudTrail can do the job https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/logging-using-cloudtrail.html","comments":[{"poster":"acloudguru","content":"no, it can't detect failure only API calls","timestamp":"1685318520.0","upvote_count":"1","comment_id":"908930"}],"upvote_count":"1","comment_id":"813016","timestamp":"1676727300.0","poster":"Piccaso"}],"content":"I think A is wrong. As \"fartosh 2\" said, CloudTrail does not help to detect failures of database.\nSame reason to exclude B. \nDo you guys know why B is shown as \"Correct Answer\" ?","timestamp":"1675519980.0"}],"poster":"Piccaso","comment_id":"798016"},{"timestamp":"1673841660.0","comment_id":"777300","content":"D is the right answer. We need CloudWatch events to detect errors from the Aurora CloudWatch logs and then invoke a Lambda function to promote the Read Replica in another region. The Lambda can run in the primary region and update the Parameter store in the Primary region with the newly promoted cluster address.","poster":"Bulti","upvote_count":"2"},{"poster":"apcloud","timestamp":"1672019760.0","comment_id":"756131","upvote_count":"3","content":"This one is hard to understand, but my bust understanding is: A and b are wrong because they use cloud trail, you would want event bridge or cloud watch events or something else. C is wrong because it’s using lambda to update cloud formation template when you really need to be adjusting the infra in real time not waiting on cloud formation. Knowing that D is the only possible choice that makes sense"},{"timestamp":"1671987540.0","content":"Selected Answer: A\nI think choice D not right \nREF : https://aws.amazon.com/blogs/architecture/implementing-multi-region-disaster-recovery-using-event-driven-architecture/ -> Parameter Store is hosted in multiple Availability Zones in an AWS Region ->\n\n\nI go with choice A from this link below.\nhttps://aws.amazon.com/blogs/database/cross-region-disaster-recovery-using-amazon-aurora-global-database-for-amazon-aurora-postgresql/","comments":[{"upvote_count":"2","content":"A talks about RDS, the question is about aurora","poster":"apcloud","timestamp":"1672018500.0","comments":[{"content":"Aurora is a managed RDS","poster":"Piccaso","comment_id":"813012","upvote_count":"1","timestamp":"1676727240.0"}],"comment_id":"756126"}],"upvote_count":"1","comment_id":"755839","poster":"Arkarter"},{"content":"I'll go with D","timestamp":"1671749520.0","poster":"saeidp","upvote_count":"1","comment_id":"753721"},{"content":"So why B is wrong?","comments":[],"poster":"Nickhiahiahia","timestamp":"1670944440.0","upvote_count":"1","comment_id":"744187"},{"poster":"alinato","comment_id":"722072","upvote_count":"2","comments":[{"comments":[],"poster":"fartosh","timestamp":"1674069420.0","comment_id":"780351","upvote_count":"1","content":"But answer A uses CloudTrail which cannot detect failures of the database. Thus, this cannot be correct."}],"content":"Selected Answer: A\nA: Route 53 is the only one which does not go down in case of region going down.\nD is incorrect because Parameter Store goes down together with region.","timestamp":"1668870300.0"},{"comment_id":"665960","poster":"MikeyJ","timestamp":"1662885540.0","upvote_count":"3","content":"For each of these databases, we store all configuration information in AWS Systems Manager Parameter Store, so the failover process can get all the information it needs to carry out the failover steps.\n\nWe have created a lambda which breaks down the entire failover process into two steps. Step 1 promotes the replica database to be read write, and Step 2 re-establishes resiliency by creating a new replica in the original primary region.\nhttps://developer.gs.com/blog/posts/building-multi-region-resiliency-with-amazon-rds-and-amazon-aurora"},{"comment_id":"583213","upvote_count":"4","poster":"friendofpenguin","content":"Selected Answer: D\nD - always choose decoupling options for endpoints, urls, passwords etc.","timestamp":"1649493540.0"},{"upvote_count":"1","content":"Selected Answer: D\nD looks right","comment_id":"581282","timestamp":"1649165880.0","poster":"jj22222"},{"content":"Correct Answer: D\nApplicatino can be coded to load endpoint value from AWS Parameter Store. Once the value is changed in the param store, the application can use the new value. Just need a reload.\n\nWrong answer is B, because Cloudtrail is for static content and media stream caching. But not for DB content.","upvote_count":"2","poster":"dzenadcu","comment_id":"572948","timestamp":"1647955140.0"},{"poster":"szl0144","comment_id":"508955","content":"answer is D","timestamp":"1640406960.0","upvote_count":"1"},{"content":"D, https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Overview.Endpoints.HA","upvote_count":"2","timestamp":"1633960020.0","poster":"awsqueen","comment_id":"356106"}],"answer_description":"","answer_ET":"D","unix_timestamp":1616398860,"answers_community":["D (58%)","A (33%)","8%"],"isMC":true,"exam_id":35,"answer":"D","choices":{"D":"Store the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge (Amazon CloudWatch Events) event that defects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Code the application to reload the endpoint from Parameter Store if a database connection fails.","B":"Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to run an AWS Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.","C":"Create an AWS Lambda function to modify the application's AWS Cloud Formation template to promote the replica, apply the template to update the stack, and point the application to the newly promoted instance. Create an Amazon CloudWatch alarm to trigger this Lambda function after the failure event occurs.","A":"Configure a latency-based Amazon Route 53 CNAME with health checks so it points to both the primary and replica endpoints. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and use that topic to trigger an AWS Lambda function that will promote the replica instance as the master."},"question_images":[],"timestamp":"2021-03-22 08:41:00","url":"https://www.examtopics.com/discussions/amazon/view/47924-exam-aws-devops-engineer-professional-topic-1-question-27/","question_id":129,"topic":"1","question_text":"A company has an application that is using a MySQL-compatible Amazon Aurora Multi-AZ DB cluster as the database. A cross-Region read replica has been created for disaster recovery purposes. A DevOps engineer wants to automate the promotion of the replica so it becomes the primary database instance in the event of a failure.\nWhich solution will accomplish this?","answer_images":[]},{"id":"O35XWC6KpWq64YngmkXu","isMC":true,"answer_images":[],"unix_timestamp":1573590060,"exam_id":35,"answer":"D","answer_ET":"D","answers_community":["D (100%)"],"discussion":[{"timestamp":"1632159120.0","content":"I believe D is the correct answer","poster":"marwan","comment_id":"21049","upvote_count":"22"},{"upvote_count":"12","timestamp":"1634116260.0","content":"A - when cloud watch agent can push the logs automatically, there is no reason to use s3 api\nB - when cloud watch agent can push the logs automatically, there is no reason to use s3 api. Also Macie is for PII\nC - \"Transfer all logs from AWS to the on-premises data cente\" make it a bad choice. Also ELK is more expensive and will need EC2 provisioning\nD - will work","comment_id":"95303","poster":"Raj9"},{"content":"Selected Answer: D\nD is right","comment_id":"1002582","poster":"frizzolo","timestamp":"1694183820.0","upvote_count":"1"},{"timestamp":"1675520340.0","poster":"Piccaso","comments":[{"comment_id":"1002580","timestamp":"1694183760.0","poster":"frizzolo","upvote_count":"1","content":"Why transferring logs to on-premise resources is a good thing?"}],"upvote_count":"1","comment_id":"798021","content":"Selected Answer: D\nA and B are excluded because CloudWatch Logs Agent is a good solution. \nBetween C and D, I prefer D, because C transfer logs from AWS to on-premises ..."},{"content":"D is the correct answer","timestamp":"1673841780.0","poster":"Bulti","upvote_count":"1","comment_id":"777301"},{"content":"Selected Answer: D\nwill work","timestamp":"1662002100.0","poster":"colinquek","upvote_count":"2","comment_id":"655725"},{"comment_id":"412610","upvote_count":"1","timestamp":"1636277280.0","content":"I believe it's D.","poster":"xxxdolorxxx"},{"upvote_count":"3","poster":"WhyIronMan","comment_id":"321846","timestamp":"1636260240.0","content":"I'll go with D"},{"poster":"dnevado","timestamp":"1636175100.0","comment_id":"234858","upvote_count":"1","content":"D definitely. Agent on premise and collect logs in S3 centralized"},{"poster":"jackdryan","content":"I'll go with D","timestamp":"1635265740.0","comment_id":"223223","upvote_count":"4"},{"comment_id":"206116","content":"C looks fine if the logs transfer to AWS ElasticSearch not the on-premise data center. I'll go with D","poster":"ChauPhan","upvote_count":"1","timestamp":"1634530380.0"},{"poster":"df1228","content":"I choose D","timestamp":"1634520720.0","upvote_count":"1","comment_id":"182288"},{"timestamp":"1634397900.0","upvote_count":"2","poster":"Augustoosouza","content":"Another important factor is \"Install the CloudWatch Logs agent on the on-premises servers\"\nwithout that we can't get logs from local servers. I choose D.","comment_id":"153730"},{"content":"Why not A ?","poster":"hanou","timestamp":"1633730280.0","comment_id":"77883","comments":[{"content":"EMR cluster would be too complicated and more cost. Exporting logs from on premise to AWS using the CLI would be also costly and inefficient. You want the Unified CW Logs Agent.","poster":"xlFireman","upvote_count":"1","timestamp":"1635236520.0","comment_id":"220497"}],"upvote_count":"1"},{"poster":"yassu","comment_id":"62160","upvote_count":"1","content":"D is correct","timestamp":"1633698060.0"},{"timestamp":"1633614240.0","comment_id":"58513","content":"C - logs aggregation \nhttps://medium.com/@sid_sharma/aws-emr-log-aggregation-and-visualization-using-lambda-elasticsearch-and-kibana-5b734fd5f812","poster":"AlexTun","upvote_count":"4","comments":[{"upvote_count":"1","comment_id":"220496","timestamp":"1634828640.0","poster":"xlFireman","content":"Incorrect, we want the most cost efficient method. This method described in C would accrue a lot of charges to move all the log files to AWS (existing and future logs). D's description just inputs current and future log files to an S3 bucket. Additionally, setting up an ELK stack would incur much more cost than an S3 data events > lambda function and Athena on top of that."}]},{"content":"It is D","timestamp":"1633555200.0","poster":"xaocho","comment_id":"55243","upvote_count":"1"},{"comment_id":"36062","content":"I choose D.\nThis one \"Transfer all logs from AWS to the on-premises data center\" makes C is not a good choice.","timestamp":"1633438140.0","poster":"dinhvu","upvote_count":"2"},{"content":"D is the right answer","timestamp":"1633215060.0","poster":"un","upvote_count":"3","comment_id":"33913"},{"content":"i think D is right","upvote_count":"2","poster":"un","comment_id":"33759","timestamp":"1632833040.0"},{"poster":"MichaelExam","upvote_count":"4","timestamp":"1632812580.0","content":"I think D is right.","comment_id":"30320"},{"upvote_count":"1","timestamp":"1632562560.0","content":"I think is right.","comment_id":"30318","poster":"MichaelExam"},{"poster":"jiedee","content":"i will go with D","upvote_count":"2","timestamp":"1632464160.0","comment_id":"28369"},{"poster":"BeastX","content":"agree, I choose D","comment_id":"21519","timestamp":"1632264720.0","upvote_count":"4"}],"choices":{"A":"Collect system logs and application logs by using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs, and store the logs in an S3 bucket in a central account. Build an Amazon EMR cluster to reduce the logs and derive the root cause.","D":"Collect system logs and application logs by using the Amazon CloudWatch Logs agent. Install a CloudWatch Logs agent for on-premises resources. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.","C":"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Transfer all logs from AWS to the on-premises data center. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs on premises.","B":"Collect system logs and application logs by using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Macie to write a query to search for the required specific event-related data point."},"question_text":"An application has microservices spread across different AWS accounts and is integrated with an on-premises legacy system for some of its functionality.\nBecause of the segmented architecture and missing logs, every time the application experiences issues, it is taking too long to gather the logs to identify the issues. A DevOps Engineer must fix the log aggregation process and provide a way to centrally analyze the logs.\nWhich is the MOST efficient and cost-effective solution?","url":"https://www.examtopics.com/discussions/amazon/view/8032-exam-aws-devops-engineer-professional-topic-1-question-28/","question_images":[],"timestamp":"2019-11-12 21:21:00","question_id":130,"topic":"1","answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":208,"isMCOnly":false,"id":35,"name":"AWS DevOps Engineer Professional","isBeta":false,"isImplemented":true},"currentPage":26},"__N_SSP":true}