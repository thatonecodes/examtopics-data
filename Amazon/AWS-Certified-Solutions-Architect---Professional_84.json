{"pageProps":{"questions":[{"id":"hvhizSu9bxkpriX8G4p9","unix_timestamp":1569470940,"topic":"1","timestamp":"2019-09-26 06:09:00","isMC":true,"answer":"A","answer_description":"","exam_id":32,"choices":{"C":"Use AWS CloudFormation StackSets to deploy the API layer in two regions. Add the database to an Auto Scaling group. Add a read replica to the database in the second region. Use Amazon Route 53 health checks on the database to trigger a DNS failover to the standby region if the health checks in the primary region fail. Promote the cross-region database replica to be the master and build out new read replicas in the standby region.","A":"Use AWS CloudFormation StackSets to deploy the API layer in two regions. Migrate the database to an Amazon Aurora with MySQL database cluster with multiple read replicas in one region and a read replica in a different region than the source database cluster. Use Amazon Route 53 health checks to trigger a DNS failover to the standby region if the health checks to the primary load balancer fail. In the event of Route 53 failover, promote the cross-region database replica to be the master and build out new read replicas in the standby region.","D":"Use Amazon ElastiCache for Redis Multi-AZ with an automatic failover to cache the database read queries. Use AWS OpsWorks to deploy the API layer, cache layer, and existing database layer in two regions. Use Amazon Route 53 health checks on the ALB to trigger a DNS failover to the standby region if the health checks in the primary region fail. Back up the MySQL database frequently, and in the event of a failure in an active region, copy the backup to the standby region and restore the standby database.","B":"Use Amazon ElastiCache for Redis Multi-AZ with an automatic failover to cache the database read queries. Use AWS OpsWorks to deploy the API layer, cache layer, and existing database layer in two regions. In the event of failure, use Amazon Route 53 health checks on the database to trigger a DNS failover to the standby region if the health checks in the primary region fail. Back up the MySQL database frequently, and in the event of a failure in an active region, copy the backup to the standby region and restore the standby database."},"answers_community":["A (100%)"],"question_text":"A company's application is increasingly popular and experiencing latency because of high volume reads on the database server.\nThe service has the following properties:\n✑ A highly available REST API hosted in one region using Application Load Balancer (ALB) with auto scaling.\n✑ A MySQL database hosted on an Amazon EC2 instance in a single Availability Zone.\nThe company wants to reduce latency, increase in-region database read performance, and have multi-region disaster recovery capabilities that can perform a live recovery automatically without any data or performance loss (HA/DR).\nWhich deployment strategy will meet these requirements?","question_id":416,"url":"https://www.examtopics.com/discussions/amazon/view/5721-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"discussion":[{"timestamp":"1632175320.0","comments":[{"upvote_count":"5","timestamp":"1632302760.0","poster":"PacoDerek","comment_id":"42527","content":"support A\nB/D: B also 2 regions. but B heath check on Database is quite weird, and both request \"a live recover automatically\" ,both B/D using a manual job of \"copy and restore\""}],"comment_id":"12715","upvote_count":"33","poster":"donathon","content":"A\nB: Not multi-region.\nC: Adding database to Auto Scaling group? Is that possible?\nD: Live recovery is required."},{"poster":"Ebi","comment_id":"282631","content":"No doubt answer is A","timestamp":"1635386700.0","upvote_count":"5"},{"content":"Selected Answer: A\nWithout a doubt, A","upvote_count":"1","comment_id":"926278","timestamp":"1687035540.0","poster":"SkyZeroZx"},{"timestamp":"1668289320.0","content":"Selected Answer: A\nWithout a doubt, A","upvote_count":"1","poster":"DarthYoda","comment_id":"716947"},{"timestamp":"1664815260.0","upvote_count":"1","comment_id":"685678","content":"Selected Answer: A\nBased on all comments","poster":"dmscountera"},{"comment_id":"626453","upvote_count":"1","timestamp":"1656830820.0","content":"Selected Answer: A\nonly A has a cross-region database replica setup","poster":"aandc"},{"upvote_count":"1","comment_id":"561454","content":"Yah it is A. Multi AZs with Aurora and Standy can easily be promoted. Then Cloudformation can be used to build out a new environment","poster":"Ni_yot","timestamp":"1646488560.0"},{"upvote_count":"1","content":"think it's A\nhttps://aws.amazon.com/blogs/database/deploy-multi-region-amazon-aurora-applications-with-a-failover-blueprint/","timestamp":"1642603380.0","comment_id":"527606","poster":"Yecine11y"},{"timestamp":"1638642060.0","content":"A is right","upvote_count":"2","comment_id":"493881","poster":"AzureDP900"},{"comment_id":"451151","timestamp":"1635987960.0","content":"It's A","upvote_count":"1","poster":"andylogan"},{"timestamp":"1635981600.0","poster":"Kopa","comment_id":"448681","content":"Aurora and Route 53 failover goes to A.","upvote_count":"1"},{"comment_id":"409610","content":"I'll go with A","upvote_count":"2","poster":"WhyIronMan","timestamp":"1635685980.0"},{"poster":"Waiweng","upvote_count":"2","content":"It's A","comment_id":"344405","timestamp":"1635607200.0"},{"content":"will go with A","comment_id":"290302","poster":"Kian1","timestamp":"1635516300.0","upvote_count":"2"},{"content":"A, Opsworks options are out and the one scaling the MySQL instance no go","poster":"kopper2019","comment_id":"281592","upvote_count":"1","timestamp":"1635361620.0"},{"poster":"sanjaym","content":"Go with A","timestamp":"1635194040.0","comment_id":"267970","upvote_count":"1"},{"upvote_count":"1","poster":"T14102020","timestamp":"1634818380.0","comment_id":"242469","content":"Answer is A. Aurora"},{"timestamp":"1634576160.0","comment_id":"230024","poster":"jackdryan","content":"I'll go with A","upvote_count":"3"},{"poster":"Bulti","content":"Answer is A. Aurora read-replicas in the same and the DR region with an automatic failover capability ( making the read-replica in the DR region the master upon DNS failover) is better than attempting to do the same with a customer managed solution on EC2 for SQL Server. Other options using ElasticCache do not provide automatic failover with no downtime.","timestamp":"1634210460.0","comment_id":"229178","upvote_count":"3"},{"upvote_count":"3","content":"A, aurora cross region replica, promote, stackset, DNS failover on load balancer, read replica","timestamp":"1633492620.0","comment_id":"149553","poster":"fullaws"},{"content":"A acceptable\nuse aurora","comment_id":"137720","poster":"noisonnoiton","upvote_count":"2","timestamp":"1633276980.0"},{"timestamp":"1633069140.0","content":"A for sure","upvote_count":"2","poster":"NikkyDicky","comment_id":"134413"},{"upvote_count":"2","timestamp":"1632861480.0","poster":"meenu2225","comment_id":"105577","content":"Clearcut winner is A"},{"content":"why not D? for reduce latency, and read performace, should be D","comments":[{"upvote_count":"1","poster":"[Removed]","content":"latency is due to high volume reads. Both read replicas and Elasticache is ok. On the other hand D requires manual work for copying and restoring which means downtime in the case of a failure whereas option A can automatically recover.","comment_id":"101220","timestamp":"1632720420.0"}],"upvote_count":"1","comment_id":"86312","poster":"zpei","timestamp":"1632370140.0"},{"timestamp":"1632341640.0","upvote_count":"2","comment_id":"61033","content":"Also A. CF with stacksets, Read-Replicas for Live-DR, DNS failover, seems to be good.","poster":"virtual"}],"question_images":[],"answer_ET":"A"},{"id":"f1JlkQOH68tcCZsY9KND","question_images":[],"answer":"BD","discussion":[{"poster":"ItsmeP","comment_id":"310252","upvote_count":"10","content":"B: use autoscaling, and that allow for increasing and reducing of utilization, plus the performance of M4 is better. \nD: using CloudFront, reduce the network local utilization, thus support the solution.","timestamp":"1632121380.0"},{"timestamp":"1633332060.0","upvote_count":"6","poster":"Waiweng","content":"It's BD","comment_id":"344409"},{"comment_id":"926281","timestamp":"1687035660.0","upvote_count":"1","poster":"SkyZeroZx","content":"Selected Answer: BD\nB: use autoscaling, and that allow for increasing and reducing of utilization, plus the performance of M4 is better.\nD: using CloudFront, reduce the network local utilization, thus support the solution."},{"upvote_count":"1","timestamp":"1678600620.0","poster":"milofficial","content":"Selected Answer: BD\nB & D; Autoscaling & CloudFront","comment_id":"836740"},{"comment_id":"685679","content":"Selected Answer: BD\nBased on all comments","timestamp":"1664815320.0","poster":"dmscountera","upvote_count":"1"},{"upvote_count":"1","comment_id":"636278","timestamp":"1658703060.0","content":"Autoscaling + Cloudfront","poster":"hilft"},{"content":"\"The web and application layers have been identified as network limited by operations.\" => what does this mean? Need more strong instance type? or ???","upvote_count":"2","timestamp":"1643887440.0","comment_id":"539640","poster":"HellGate"},{"timestamp":"1638642660.0","content":"B,D is prefect answer","upvote_count":"1","comment_id":"493885","poster":"AzureDP900"},{"content":"It's B D","comment_id":"451152","timestamp":"1635247980.0","poster":"andylogan","upvote_count":"1"},{"upvote_count":"3","poster":"WhyIronMan","comment_id":"409616","timestamp":"1634409000.0","content":"I'll go with B, D"},{"comment_id":"353407","upvote_count":"2","timestamp":"1634060880.0","poster":"victordun","content":"should be BD"},{"comment_id":"310325","timestamp":"1632190500.0","content":"i go with BD","upvote_count":"4","poster":"Nguyenhau"}],"unix_timestamp":1615696380,"isMC":true,"answer_description":"","answers_community":["BD (100%)"],"topic":"1","timestamp":"2021-03-14 05:33:00","answer_ET":"BD","question_id":417,"url":"https://www.examtopics.com/discussions/amazon/view/46994-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A company runs a three-tier application in AWS. Users report that the application performance can vary greatly depending on the time of day and functionality being accessed.\nThe application includes the following components:\n✑ Eight t2.large front-end web servers that serve static content and proxy dynamic content from the application tier.\n✑ Four t2.large application servers.\n✑ One db.m4.large Amazon RDS MySQL Multi-AZ DB instance.\nOperations has determined that the web and application tiers are network constrained.\nWhich of the following is a cost effective way to improve application performance? (Choose two.)","choices":{"D":"Create an Amazon CloudFront distribution to cache content","B":"Use AWS Auto Scaling and m4.large instances for the web and application tiers","A":"Replace web and app tiers with t2.xlarge instances","E":"Increase the size of the Amazon RDS instance to db.m4.xlarge","C":"Convert the MySQL RDS instance to a self-managed MySQL cluster on Amazon EC2"},"answer_images":[],"exam_id":32},{"id":"8jm9nv6LyheKyeRwda9d","timestamp":"2019-09-14 17:15:00","isMC":true,"choices":{"B":"Hold workflow information in an Amazon RDS instance with AWS Lambda functions polling RDS for status changes. Worker Lambda functions then process the next workflow steps. Amazon QuickSight will visualize workflow states directly out of Amazon RDS.","A":"Trigger Amazon CloudWatch alarms based upon message visibility in multiple Amazon SQS queues (one queue per workflow stage) and send messages via Amazon SNS to trigger AWS Lambda functions to process the next step. Use Amazon ES and Kibana to visualize Lambda processing logs to see the workflow states.","D":"Use Amazon SWF to create a workflow that handles a single batch of catalog records with multiple worker tasks to extract the data, transform it, and send it through Mechanical Turk. Use Amazon ES and Kibana to visualize AWS Lambda processing logs to see the workflow states.","C":"Build the workflow in AWS Step Functions, using it to orchestrate multiple concurrent workflows. The status of each workflow can be visualized in the AWS Management Console, and historical data can be written to Amazon S3 and visualized using Amazon QuickSight."},"answer_images":[],"answer":"D","question_images":[],"exam_id":32,"discussion":[{"comments":[{"comment_id":"245525","upvote_count":"6","timestamp":"1634829900.0","content":"The FAQ itself becomes a question. Have to go for D.","poster":"newme"}],"upvote_count":"47","content":"I would go for D, Use case #2: Processing large product catalogs using Amazon Mechanical Turk. https://aws.amazon.com/swf/faqs/","comment_id":"22055","poster":"huhupai","timestamp":"1632316800.0"},{"content":"C\nAWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Instead of writing a Decider program, you define state machines in JSON. AWS customers should consider using Step Functions for new applications. If Step Functions does not fit your needs, then you should consider Amazon Simple Workflow (SWF). Amazon SWF provides you complete control over your orchestration logic, but increases the complexity of developing applications. You may write decider programs in the programming language of your choice, or you may use the Flow framework to use programming constructs that structure asynchronous interactions for you. AWS will continue to provide the Amazon SWF service, Flow framework, and support all Amazon SWF customers.\nhttps://aws.amazon.com/swf/faqs/","comments":[{"comments":[{"poster":"DashL","comment_id":"393263","upvote_count":"6","content":"Steps Functions does not support Mechanical Turk.","timestamp":"1635774480.0"}],"content":"https://blog.mturk.com/tutorial-using-mturk-together-with-aws-lambda-c91d414496d3\nC is correct.","poster":"consultsk","comment_id":"252641","timestamp":"1634880840.0","upvote_count":"1"},{"upvote_count":"18","comments":[{"content":"it can use lambda to call mechanical turk, does not need to \"support\" it per say","poster":"hailiang","comment_id":"174839","upvote_count":"4","timestamp":"1633779180.0"},{"comment_id":"252643","content":"your assumptions are incorrect and may check the link: https://blog.mturk.com/tutorial-using-mturk-together-with-aws-lambda-c91d414496d3","upvote_count":"2","poster":"consultsk","timestamp":"1634958000.0"},{"comment_id":"695080","upvote_count":"1","timestamp":"1665796140.0","content":"it supports mechanical turk","poster":"sg0206"}],"content":"Step Functions do not support Mechanical Turk. You need SWF, so D","comment_id":"143799","poster":"MultiAZ","timestamp":"1633510320.0"}],"upvote_count":"27","poster":"donathon","comment_id":"12719","timestamp":"1632192600.0"},{"poster":"SkyZeroZx","upvote_count":"1","comment_id":"926282","timestamp":"1687035900.0","content":"Selected Answer: D\nI would go for D, Use case #2: Processing large product catalogs using Amazon Mechanical Turk. https://aws.amazon.com/swf/faqs/"},{"timestamp":"1665796080.0","upvote_count":"1","comment_id":"695079","comments":[{"content":"Step Functions does not work with Mechanical Turk","timestamp":"1668174060.0","comments":[{"comment_id":"1323470","upvote_count":"1","timestamp":"1733652000.0","poster":"mnsait","content":"sg0206 is right. AWS recommends using Step Functions instead of SWF. Step functions do support Mechanical Turk. Links are shared by others here."}],"upvote_count":"1","comment_id":"716088","poster":"wofu"}],"content":"C is the correct answer, we can implement human approval feature in step function.. SWF is old and not serverless feature.","poster":"sg0206"},{"upvote_count":"2","comment_id":"685682","content":"Selected Answer: D\nBased on all comments","poster":"dmscountera","timestamp":"1664815620.0"},{"comment_id":"643106","poster":"Mr_nobody79","content":"Selected Answer: D\nIt's D 100%. Mechanical Turk = Human intervention = SWF.","upvote_count":"2","timestamp":"1659735720.0"},{"content":"D. keyword here is SWF.","poster":"hilft","timestamp":"1658882640.0","upvote_count":"1","comment_id":"637701"},{"content":"Selected Answer: D\nMechanical Turk supports integration with SWF, not Step functions","poster":"azure_kai","timestamp":"1645471920.0","comment_id":"553171","upvote_count":"2"},{"timestamp":"1644717660.0","content":"D IS CORRECT, MECHANICAL TURK DOES NOT SUPPORT STEP FUNCTIONS. SWF IS USED WHERE STEP FUNCTIONS ARE NOT SUPPORTED.","comment_id":"546190","poster":"futen0326","upvote_count":"1"},{"content":"C. Build the workflow in AWS Step Functions, using it to orchestrate multiple concurrent workflows. The status of each workflow can be visualized in the AWS Management Console, and historical data can be written to Amazon S3 and visualized using Amazon QuickSight.","poster":"jj22222","comment_id":"542543","upvote_count":"1","timestamp":"1644251940.0"},{"content":"Selected Answer: D\nD. Mechanical Turk works well with SWF, and that is the only few cases where both services complement one another.","poster":"zoliv","timestamp":"1643892780.0","comment_id":"539701","upvote_count":"2"},{"poster":"cldy","timestamp":"1639216980.0","comment_id":"499300","content":"D. Use Amazon SWF to create a workflow that handles a single batch of catalog records with multiple worker tasks to extract the data, transform it, and send it through Mechanical Turk. Use Amazon ES and Kibana to visualize AWS Lambda processing logs to see the workflow states.","upvote_count":"1"},{"timestamp":"1638968040.0","content":"D is correct\nor miss it on the test","poster":"wem","comment_id":"496823","upvote_count":"1"},{"timestamp":"1638835800.0","comment_id":"495479","content":"Option C\n When should I use Amazon SWF vs. AWS Step Functions?\n\nAWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Instead of writing a Decider program, you define state machines in JSON. AWS customers should consider using Step Functions for new applications. If Step Functions does not fit your needs, then you should consider Amazon Simple Workflow (SWF). Amazon SWF provides you complete control over your orchestration logic, but increases the complexity of developing applications. You may write decider programs in the programming language of your choice, or you may use the Flow framework to use programming constructs that structure asynchronous interactions for you. AWS will continue to provide the Amazon SWF service, Flow framework, and support all Amazon SWF customers.","upvote_count":"1","comments":[{"timestamp":"1641198480.0","comment_id":"515495","upvote_count":"1","content":"Go for D - Use case #2: Processing large product catalogs using Amazon Mechanical Turk. Under SWF FAQ.","poster":"tkanmani76"}],"poster":"tkanmani76"},{"timestamp":"1638642900.0","poster":"AzureDP900","upvote_count":"1","comment_id":"493886","content":"D is correct answer"},{"content":"Selected Answer: C\nC,typical step function question and easy one, hope i can have it in my exam","timestamp":"1638241200.0","comment_id":"490371","poster":"acloudguru","upvote_count":"2"},{"poster":"StelSen","timestamp":"1636194000.0","comment_id":"455886","upvote_count":"2","content":"I will choose D. I won't debate SWF/Step. Rather I use another technique to prove D is correct. Management asked SA to design a workflow which can handle multiple concurrent Mech Turk Operations. PLEASE NOTE. It will be a single workflow with concurrent operations. Ans C says build multiple concurrent workflow, No need. Ans D, says single batch with multiple worker. One more Tips: AWS Mechanical Turk is just a outsourcing service performed by another human via marketplace. Isn't the good reason to use SWF?"},{"comment_id":"451153","content":"It's D","poster":"andylogan","upvote_count":"1","timestamp":"1636087680.0"},{"content":"Its D,\n\nProcessing large product catalogs using Amazon Mechanical Turk. While validating data in large catalogs, the products in the catalog are processed in batches. Different batches can be processed concurrently. For each batch, the product data is extracted from servers in the datacenter and transformed into CSV (Comma Separated Values) files required by Amazon Mechanical Turk’s Requester User Interface (RUI). The CSV is uploaded to populate and run the HITs (Human Intelligence Tasks). When HITs complete, the resulting CSV file is reverse transformed to get the data back into the original format. The results are then assessed and Amazon Mechanical Turk workers are paid for acceptable results. Failures are weeded out and reprocessed, while the acceptable HIT results are used to update the catalog. As batches are processed, the system needs to track the quality of the Amazon Mechanical Turk workers and adjust the payments accordingly. Failed HITs are re-batched and sent through the pipeline again.","upvote_count":"1","comment_id":"448702","poster":"Kopa","timestamp":"1636058880.0"},{"comment_id":"433159","poster":"denccc","upvote_count":"1","content":"It's D","timestamp":"1636016940.0"},{"comment_id":"409657","timestamp":"1635995880.0","content":"I'll go with D","upvote_count":"2","poster":"WhyIronMan"},{"content":"D Correct","upvote_count":"1","comment_id":"406687","poster":"Akhil254","timestamp":"1635789900.0"},{"timestamp":"1635710460.0","poster":"Waiweng","upvote_count":"2","comment_id":"344480","content":"go for D now"},{"content":"I will go with D because of Mechanical Turk is fully compatible with AWS SWF.","comment_id":"335444","timestamp":"1635658260.0","upvote_count":"2","poster":"ppshein"},{"poster":"Amitv2706","upvote_count":"1","content":"A will not solve the requirement of LEAST amount of implementation effort.\nD is correct answer as SWF can automate the process","timestamp":"1635624720.0","comment_id":"333696"},{"timestamp":"1635348600.0","comment_id":"323082","poster":"ExtHo","content":"C is incorrect because Step Functions do not directly support Mechanical Turk. You will need to use Amazon SWF for this scenario so D is correct one.","upvote_count":"2"},{"upvote_count":"1","timestamp":"1635250920.0","comment_id":"290309","comments":[{"timestamp":"1635333660.0","upvote_count":"1","comment_id":"295698","content":"Implementing thru SWF involves a huge effort","poster":"Satya1405"}],"poster":"Kian1","content":"will go with D"},{"comment_id":"282637","timestamp":"1635197580.0","upvote_count":"2","poster":"Ebi","content":"Answer is D"},{"upvote_count":"1","content":"d seems","comment_id":"275088","timestamp":"1635166320.0","poster":"gookseang"},{"poster":"kopper2019","comment_id":"269720","timestamp":"1634980680.0","upvote_count":"3","content":"D, \n\nhttps://tutorialsdojo.com/amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/\n\nsee below so D: SWF\n\nConsider using AWS Step Functions for all your new applications, since it provides a more productive and agile approach to coordinating application components using visual workflows. If you require external signals (deciders) to intervene in your processes, or you would like to launch child processes that return a result to a parent, then you should consider Amazon SWF."},{"upvote_count":"1","content":"I'll go with D","timestamp":"1634967660.0","comment_id":"267979","poster":"sanjaym"},{"timestamp":"1634782500.0","comment_id":"242478","upvote_count":"3","poster":"T14102020","content":"Correct answer is D. Mechanical Turk does not work with Step Function. For product catalogs it goes well with SWF.","comments":[{"comment_id":"252642","timestamp":"1634888280.0","upvote_count":"1","poster":"consultsk","content":"Mechanical Turk works with Step Functions: https://blog.mturk.com/tutorial-using-mturk-together-with-aws-lambda-c91d414496d3\nI would go with C"}]},{"poster":"petebear55","upvote_count":"3","content":"See Donathon is giving Red Herrings again !!.. Wonder if he works for Amazon ?","timestamp":"1634728680.0","comments":[{"upvote_count":"4","timestamp":"1635580440.0","comment_id":"333295","poster":"sarah_t","content":"I don't agree with this one (afaik MTurk only works with SWF). However, by and large donathon provides a lot of useful comments (more than most users here). \n\nYou don't have to blindly trust them, just follow the links they always include in their comments and check for yourself. The questions aren't here to just memorize, they are meant to make you think and learn for yourself ffs \n\nIf you disagree, reply with better information."}],"comment_id":"240908"},{"timestamp":"1634384400.0","poster":"jackdryan","upvote_count":"3","content":"I'll go with D","comment_id":"230028"},{"upvote_count":"3","poster":"Bulti","timestamp":"1634327340.0","comment_id":"229188","content":"Answer is D. I chose D over C even though I think C (Step function) is less effort than D, I would chose D is the exam due to the reference to using SWS with Mechanical Turk in SWS FAQ. I would like to play safe since this very use case is mentioned in the FAQ."},{"comment_id":"226715","content":"Answer is D, for MTurk you need SWF","upvote_count":"1","poster":"lostri","timestamp":"1634310840.0"},{"content":"c:\nUse case #2: Processing large product catalogs using Amazon Mechanical Turk. While validating data in large catalogs, the products in the catalog are processed in batches. Different batches can be processed concurrently. For each batch, the product data is extracted from servers in the datacenter and transformed into CSV (Comma Separated Values) files required by Amazon Mechanical Turk’s Requester User Interface (RUI). The CSV is uploaded to populate and run the HITs (Human Intelligence Tasks). When HITs complete, the resulting CSV file is reverse transformed to get the data back into the original format. The results are then assessed and Amazon Mechanical Turk workers are paid for acceptable results. Failures are weeded out and reprocessed, while the acceptable HIT results are used to update the catalog. As batches are processed, the system needs to track the quality of the Amazon Mechanical Turk workers and adjust the payments accordingly. Failed HITs are re-batched and sent through the pipeline again.","poster":"SamAWSExam99","upvote_count":"2","comments":[{"poster":"A_New_Guy","comment_id":"224965","upvote_count":"2","timestamp":"1634118960.0","content":"Did you mean D"}],"comment_id":"208732","timestamp":"1633836720.0"},{"upvote_count":"1","comment_id":"182660","timestamp":"1633830600.0","content":"D...\nMechanical Turk not integrated with step functions as today.","poster":"ipindado2020"},{"timestamp":"1633763580.0","poster":"achambok","comment_id":"164882","content":"True since SWF is replacing Batch","upvote_count":"1"},{"comment_id":"149563","timestamp":"1633645980.0","poster":"fullaws","upvote_count":"1","content":"D is correct"},{"comment_id":"137724","content":"D acceptable","timestamp":"1633481100.0","upvote_count":"1","poster":"noisonnoiton"},{"poster":"NikkyDicky","timestamp":"1633419840.0","content":"C, due to workflow visualization in step functions","comment_id":"134418","upvote_count":"1"},{"upvote_count":"4","timestamp":"1633343280.0","content":"D. Exact scenario mentioned in the below doc .Refer question : What are some use cases that can be solved with Amazon SWF? \nhttps://aws.amazon.com/swf/faqs/","comment_id":"97961","poster":"JohnyGaddar"},{"content":"D. Mechanical Turk does not work with Step Function. For product catalogs it goes well with SWF.\nhttps://aws.amazon.com/swf/","comment_id":"91673","timestamp":"1633025700.0","upvote_count":"1","poster":"JAWS1600"},{"poster":"2fadfe","timestamp":"1632958020.0","content":"D.\n Not C because step functions do not support mechanical turk","upvote_count":"2","comment_id":"88423"},{"upvote_count":"10","comment_id":"80672","poster":"AmazonAu","timestamp":"1632949800.0","content":"D:\nhttps://aws.amazon.com/swf/faqs/\n\nUse case #2: Processing large product catalogs using Amazon Mechanical Turk. While validating data in large catalogs, the products in the catalog are processed in batches. Different batches can be processed concurrently. For each batch, the product data is extracted from servers in the datacenter and transformed into CSV (Comma Separated Values) files required by Amazon Mechanical Turk’s Requester User Interface (RUI). The CSV is uploaded to populate and run the HITs (Human Intelligence Tasks). When HITs complete, the resulting CSV file is reverse transformed to get the data back into the original format. The results are then assessed and Amazon Mechanical Turk workers are paid for acceptable results. Failures are weeded out and reprocessed, while the acceptable HIT results are used to update the catalog. As batches are processed, the system needs to track the quality of the Amazon Mechanical Turk workers and adjust the payments accordingly. Failed HITs are re-batched and sent through the pipeline again."},{"poster":"[Removed]","timestamp":"1632664740.0","content":"This same use case is described in Amazon SWF: https://aws.amazon.com/swf/faqs/\nI believe it should be D","comment_id":"76263","upvote_count":"4"},{"upvote_count":"3","timestamp":"1632579840.0","content":"D for Mechanical Turk.","comment_id":"76195","poster":"Joeylee"},{"comment_id":"59987","content":"I believe A. I came across this question in the TutorialsDojo practice exams and Step Functions/SWF is wrong because Mechanical Turk is not an automated process with predictable response times, etc.","upvote_count":"4","timestamp":"1632524100.0","poster":"n1ch0las"},{"poster":"amog","upvote_count":"3","content":"C for \"LEAST amount of implementation effort\"","comment_id":"50034","timestamp":"1632334140.0"},{"content":"I will go for A, as Turk response may be very unpredictable in timing. Thus just wait for them to response by putting into a queue and trigger an action make better sense. \nSWF or Step function bascially wait for response. If MTurk does not response in months, this will result in SWF/Step to hang in the process waiting for response.","timestamp":"1632314940.0","comment_id":"21646","comments":[{"content":"Waiting in process shouldn't be a problem as Step Functions Standard Workflows can last for a year maximum. It also integrates with Lambda, SQS, SNS, and Activities on EC2/ECS. I don't see the problem using Step Functions here.","upvote_count":"2","poster":"Smart","timestamp":"1632918120.0","comment_id":"76497"}],"upvote_count":"2","poster":"skywalker"},{"upvote_count":"13","content":"I support answer \"C\".\nStep Function, is considered a replacement of SWF, in most cases. It has a built in graphical console visualization for the workflow as series of steps, and does not require much of effort to build. Therefore it is considered in this question as the least effort of implementation.\n\nA: requires a lot of effort.\nB: not working solution!!\nD: is working, but requires more efforts on deciders, and workers. Then ES and Kibana.","timestamp":"1632212820.0","comment_id":"13995","poster":"Moon"},{"upvote_count":"2","comment_id":"11695","timestamp":"1632118140.0","content":"I'll go with C. Step function is SWF 2.0","comments":[{"comment_id":"11696","timestamp":"1632169620.0","poster":"dpvnme","upvote_count":"4","content":"i changed my mind: https://aws.amazon.com/swf/faqs/"}],"poster":"dpvnme"},{"content":"d should be","comment_id":"11080","timestamp":"1632093180.0","upvote_count":"9","poster":"awsec2"}],"unix_timestamp":1568474100,"question_text":"An online retailer needs to regularly process large product catalogs, which are handled in batches. These are sent out to be processed by people using the\nAmazon Mechanical Turk service, but the retailer has asked its Solutions Architect to design a workflow orchestration system that allows it to handle multiple concurrent Mechanical Turk operations, deal with the result assessment process, and reprocess failures.\nWhich of the following options gives the retailer the ability to interrogate the state of every workflow with the LEAST amount of implementation effort?","url":"https://www.examtopics.com/discussions/amazon/view/5168-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","topic":"1","question_id":418,"answer_ET":"D","answers_community":["D (82%)","C (18%)"]},{"id":"aHFZJNfOSExyErCPZ5Gf","question_id":419,"question_text":"An organization has two Amazon EC2 instances:\n✑ The first is running an ordering application and an inventory application.\nThe second is running a queuing system.\n//IMG//\n\nDuring certain times of the year, several thousand orders are placed per second. Some orders were lost when the queuing system was down. Also, the organization's inventory application has the incorrect quantity of products because some orders were processed twice.\nWhat should be done to ensure that the applications can handle the increasing number of orders?","topic":"1","answers_community":["C (70%)","A (20%)","10%"],"answer_images":[],"answer":"C","timestamp":"2021-03-13 20:18:00","choices":{"A":"Put the ordering and inventory applications into their own AWS Lambda functions. Have the ordering application write the messages into an Amazon SQS FIFO queue.","D":"Put the ordering and inventory applications into their own Amazon EC2 instances. Write the incoming orders to an Amazon Kinesis data stream. Configure AWS Lambda to poll the stream and update the inventory application.","C":"Put the ordering and inventory applications into their own Amazon EC2 instances, and create an Auto Scaling group for each application. Use Amazon SQS standard queues for the incoming orders, and implement idempotency in the inventory application.","B":"Put the ordering and inventory applications into their own Amazon ECS containers, and create an Auto Scaling group for each application. Then, deploy the message queuing server in multiple Availability Zones."},"question_images":["https://www.examtopics.com/assets/media/exam-media/04241/0030200002.png"],"url":"https://www.examtopics.com/discussions/amazon/view/46930-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"C","isMC":true,"unix_timestamp":1615663080,"discussion":[{"poster":"nitinz","comment_id":"317590","content":"Correct answer is C.","timestamp":"1633209900.0","upvote_count":"28"},{"comment_id":"341896","poster":"Juan21","timestamp":"1633262160.0","upvote_count":"17","content":"Correct answer is A. With FIFO you avoid the problem of duplicate processing in the queue.","comments":[{"content":"A is right","comment_id":"598596","timestamp":"1652022420.0","poster":"user0001","upvote_count":"1"},{"upvote_count":"2","poster":"StelSen","comment_id":"455889","timestamp":"1635941580.0","content":"To me A is immediate failure. Moving an application from EC2 to lambda without knowing what was the application stack is not a good choice. Although lambda can run simple PHP web application with the help of API GATEWAY, there is no mention of API GAteway, So A failed in this case."},{"timestamp":"1635296880.0","upvote_count":"5","content":"Answer is C \nSQS + Idempotency","poster":"student22","comment_id":"441145","comments":[{"upvote_count":"1","content":"there is no such thing called Idempotency, it made up here","timestamp":"1652022360.0","poster":"user0001","comment_id":"598595"},{"upvote_count":"1","content":"Agree! I was rocketMQ prudcut support before.","comment_id":"641282","poster":"Eric0909","timestamp":"1659446700.0"}]},{"poster":"hbrand","timestamp":"1634901000.0","content":"Changing to C\nI originally thought it was A but seeing this \n\n\"FIFO queues are different. The ordering imposes a real throughput limit – currently 300 requests per second per queue...Fortunately, our conversations with customers have told us that FIFO applications are generally lower-throughput—10 messages per second or lower.\"\n\nchanges mine to C. Sure it can go to 3,000/s with batching but this is not mentioned. Along with this they mention in the question that the orders are in several thousands per second. \n\nhttps://aws.amazon.com/blogs/developer/how-the-amazon-sqs-fifo-api-works/","comment_id":"421888","upvote_count":"6"}]},{"comment_id":"1129002","content":"FIFO supports higher limits now https://aws.amazon.com/about-aws/whats-new/2023/08/amazon-sqs-increased-throughput-quota-fifo-high-throughput-mode/","poster":"marszalekm","timestamp":"1705954980.0","upvote_count":"1"},{"upvote_count":"1","poster":"mrgreatness","comment_id":"717564","content":"several thousand per second so C","timestamp":"1668380100.0"},{"poster":"mrgreatness","content":"C: FIFO queues help you avoid sending duplicates to a queue. If you retry the SendMessage action within the 5-minute deduplication interval, Amazon SQS doesn't introduce any duplicates into the queue.","comment_id":"717562","upvote_count":"1","timestamp":"1668379980.0"},{"upvote_count":"1","poster":"Netaji","comment_id":"715196","timestamp":"1668081780.0","content":"there is a difference between FIFO and a standard queue of SQS -- https://jayendrapatil.com/aws-sqs-standard-vs-fifo-queue/\nso the answer should be \"A\""},{"upvote_count":"1","content":"Selected Answer: C\nCould be A, but we do not know how long the total process could take (maybe more than 15 min) and the SQS FiFo is not for this use case, the keyword here is \"Idempotency\" which means you have to build your application making sure you do not process the same message twice. so the correct option is C","comment_id":"709863","timestamp":"1667399400.0","poster":"superuser784"},{"timestamp":"1665798180.0","poster":"sg0206","content":"A is the correct answer - \nFIFO Queues\nHigh Throughput: By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. If you require higher throughput, you can enable high throughput mode for FIFO on the Amazon SQS console, which will support up to 30,000 messages per second with batching, or up to 3,000 messages per second without batching.\nExactly-Once Processing: A message is delivered once and remains available until a consumer processes and deletes it. Duplicates aren't introduced into the queue.\n\nFirst-In-First-Out Delivery: The order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out).","comment_id":"695101","upvote_count":"1"},{"poster":"dmscountera","upvote_count":"2","comment_id":"685683","content":"Selected Answer: C\nBased on all comments","timestamp":"1664815680.0"},{"content":"Selected Answer: A\nHow about This?\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html","poster":"li_qiyang","comment_id":"681528","timestamp":"1664354460.0","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: C\nKeyword: Process twice\nSQS prevents duplicate processes\nAnswer: C","timestamp":"1663316100.0","comment_id":"670613","poster":"Dionenonly"},{"poster":"Mechanic","timestamp":"1648374780.0","upvote_count":"2","comment_id":"576100","content":"Selected Answer: C\nAnswer is C.\nKey concepts:\n1. Hight availability\n2. Thousands of requests/s\n3. Duplication in processes\nEC2 ASG & SQS solve all of that."},{"poster":"bfal","timestamp":"1647796800.0","upvote_count":"1","comment_id":"571758","content":"Correct answer is C\nC addresses the issues highlighted in the question. One of queueing message going down, and losing orders, and the last one is duplicate processor certain orders.\nAmazon SQS address the issue of messaging\nImplement idempotency in the application resolves the issue of processing duplicate order - aka At-least-once delivery \n(Design your applications to be idempotent (they should not be affected adversely when processing the same message more than once). https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html#standard-queues-at-least-once-delivery"},{"content":"Selected Answer: D\nWhy D:\nCan hadle more than 300 messages w/o loosing them.\nWill treat idempotency problem.\nIt’s AWS sudgested topology.\n\nWhy NOT A:\n“Put the ordering and inventory applications into their own AWS Lambda” … Lambda doesn’t support hosting application, API Gateway is required for that and it’s not mentioned.\n\nA is wrong. --SQS FIFO queues can only support 300 send, receive, or delete operations per second. and the question satats many thousand orders /second\n\nWhy NOT C:\nno one mentioned about \"idempotency\" term from C - this one requires a control database to check the value for duplicates. I do not see this in C.","poster":"Alexey79","timestamp":"1645348860.0","upvote_count":"1","comment_id":"551665"},{"upvote_count":"1","poster":"jyrajan69","timestamp":"1644897240.0","comment_id":"547514","content":"All answering C , focused on the high number of orders, but there is an issue with duplication, standard queues cannot handle this. Only FIFO can handle this part, so unless there is an answer to this, the answer is A"},{"upvote_count":"1","poster":"robsonchirara","timestamp":"1644869880.0","comment_id":"547340","content":"Selected Answer: C\nDecoupling systems and growing demand. Definitely SQS and EC2 ASG."},{"timestamp":"1642945620.0","poster":"RVivek","comment_id":"530561","upvote_count":"1","content":"Answer C\nA is wrong. --SQS FIFO Que can handle only 300 messages per second and the question satats many thousand orders /second"},{"comment_id":"495787","timestamp":"1638866700.0","poster":"cldy","upvote_count":"1","content":"C. Put the ordering and inventory applications into their own Amazon EC2 instances, and create an Auto Scaling group for each application. Use Amazon SQS standard queues for the incoming orders, and implement idempotency in the inventory application."},{"poster":"AzureDP900","content":"There is no requirement to process order on as it arrives so my answer is C","upvote_count":"2","timestamp":"1638643380.0","comment_id":"493890"},{"timestamp":"1638349860.0","poster":"backfringe","comment_id":"491467","content":"I go with C","upvote_count":"1"},{"poster":"Kopa","timestamp":"1635981240.0","upvote_count":"1","comment_id":"468045","content":"going for C, Fifo look to slow for thousand request per second."},{"timestamp":"1635622260.0","comment_id":"451154","poster":"andylogan","upvote_count":"1","content":"It's C"},{"content":"CCC\n---","timestamp":"1635202380.0","upvote_count":"2","poster":"tgv","comment_id":"437828"},{"comment_id":"413194","content":"and how will you deal with \"the organization's inventory application has the incorrect quantity of products because some orders were processed twice\". no one mentioned about \"idempotency\" term from C - this one requires a control database to check the value for duplicates. I do not see this in C.","poster":"mericov","timestamp":"1634597520.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1634484060.0","poster":"DerekKey","comment_id":"413084","content":"The only answer that addresses duplication of orders is C."},{"timestamp":"1634210040.0","content":"I'll go with C","comment_id":"409696","poster":"WhyIronMan","upvote_count":"1"},{"timestamp":"1634042520.0","content":"A is the most accurate answer. High throughput for FIFO queues support up to 30,000 messages per second. Option C looks like a good answer but it does not mention an ALB which is crucial to make the design work with autoscaling.","poster":"student2020","upvote_count":"1","comment_id":"405584"},{"content":"C is best answer","timestamp":"1634027880.0","upvote_count":"1","poster":"solo18","comment_id":"386158"},{"content":"C as FIFO queues support up to 3,000 transactions per second","upvote_count":"2","timestamp":"1633460220.0","poster":"ss160700","comment_id":"365215"},{"timestamp":"1633399500.0","upvote_count":"4","content":"C is Correct.\n\nA would also work but problem with A is that there would be several THOUSANDS of orders in a certain time of the year, and Lambda has concurrency limit of 1000 per region, which you'd need to increase by opening the case with support team.","comment_id":"353114","poster":"beebatov"},{"comment_id":"344483","content":"it's C","poster":"Waiweng","upvote_count":"4","timestamp":"1633337940.0"},{"comment_id":"309942","poster":"anandbabu","content":"correct answer is D","timestamp":"1632224760.0","upvote_count":"1"}],"exam_id":32,"answer_description":""},{"id":"ctDJR6FMYKNvkKqyOrLy","discussion":[{"content":"B\nA: While this will work, this is still going through public and because the traffic is HTTP, it is not encrypted so this cannot be a good solution.\nB: This uses privatelink and hence is better since you cannot change the script to download via HTTP.\nC: The EC2 needs network connectivity to S3 bucket.\nD: How would this work when the actual access should be from the VPC endpoint and not the EC2 instance itself?","comment_id":"12721","poster":"donathon","timestamp":"1632527760.0","comments":[{"poster":"JAWS1600","upvote_count":"2","comment_id":"94614","timestamp":"1633448820.0","content":"Requirement does not ask for secured solution ( https) . It requires HTTP."},{"comment_id":"42555","timestamp":"1632536700.0","content":"something more for D: You cannot use an IAM policy or bucket policy to allow access from a VPC IPv4 CIDR range (the private IPv4 address range). VPC CIDR blocks can be overlapping or identical, which may lead to unexpected results. Therefore, you cannot use the aws:SourceIp condition in your IAM policies for requests to Amazon S3 through a VPC endpoint. This applies to IAM policies for users and roles, and any bucket policies. If a statement includes the aws:SourceIp condition, the value fails to match any provided IP address or range.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","upvote_count":"9","poster":"PacoDerek"}],"upvote_count":"31"},{"comment_id":"105589","poster":"meenu2225","timestamp":"1633913220.0","content":"Correct option is B\nRemember the original setup is: Apache HTTP server that serves artifacts to clients on the local network, restricted by the perimeter firewall. Which mean the comms cannot be on internet it has to be either in private subnet or via endpoint service. Which means A & C are out of euqation because in both the traffic is via internet. Leaving only B and D.\nOut of these B makes more sense.","upvote_count":"14"},{"timestamp":"1667839140.0","poster":"alnadan","content":"Selected Answer: B\nB is the correct ans. \nhttps://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/","comment_id":"713164","upvote_count":"1"},{"poster":"bandaot","timestamp":"1666169760.0","content":"Selected Answer: A\nWhy so many people select B, based on this https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3\nAWS PrivateLink doesn't support webside endpoints and user must change to use the endpoint-specific DNS names. Even thought A also has the problem for the EC2 in public subnet and maybe not that secure to go thought the public internet, but it's still the only possible solution.","comment_id":"698846","upvote_count":"2"},{"upvote_count":"1","timestamp":"1664815980.0","comment_id":"685685","content":"Selected Answer: B\nBased on all comments","poster":"dmscountera"},{"comment_id":"577523","timestamp":"1648551480.0","upvote_count":"1","content":"Selected Answer: B\nB. Create a VPC endpoint and add it to the route table associated with subnets containing consumers. Configure the bucket policy to allow s3:ListBucket and s3:GetObject actions using the condition StringEquals and the condition key aws:sourceVpce matching the identification of the VPC endpoint.","poster":"jj22222"},{"timestamp":"1647526500.0","upvote_count":"1","comment_id":"569773","content":"it's c.","poster":"ozan11"},{"poster":"frankzeng","timestamp":"1643424720.0","content":"A. Need to use HTTP. The public subnet can go through NAT gateway","upvote_count":"2","comment_id":"535110"},{"upvote_count":"3","content":"Maybe im misreading it, but this feels like another badly written question to me. The consumers currently make HTTP calls to get the artifacts, and we are asking to not change that. Yet there is no mention of anything other than using S3, which by default doesnt support HTTP. I would expect the real answer to make reference to static website hosting in S3 tbh.","comment_id":"530550","poster":"lulz111","timestamp":"1642944120.0"},{"timestamp":"1638643680.0","upvote_count":"1","content":"B is right!","comment_id":"493895","poster":"AzureDP900"},{"upvote_count":"1","comment_id":"451158","poster":"andylogan","content":"It's B","timestamp":"1636290120.0"},{"comment_id":"413091","timestamp":"1636105020.0","content":"A wrong - \"aws:SourceIp matching the elastic IP address of the NAT gateway\" will not serve instances in public subnets\nB OK - aws:sourceVpce \nC wrong - no access to S3 from private subnets\nD wrong - with Vpce instead of aws:SourceIp you have to use aws:VpcSourceIp","poster":"DerekKey","upvote_count":"3"},{"upvote_count":"2","content":"I'll go with B","timestamp":"1635965580.0","poster":"WhyIronMan","comment_id":"409705"},{"poster":"Waiweng","upvote_count":"4","content":"It's B","comment_id":"344490","timestamp":"1635569520.0"},{"timestamp":"1635144120.0","upvote_count":"2","comment_id":"290315","poster":"Kian1","content":"will go with B"},{"content":"Answer is B, you don't need to update scripts:\n\n\"If you've already set up access to your Amazon S3 resources from your VPC, you can continue to use Amazon S3 DNS names to access those resources after you've set up an endpoint.\"","poster":"Ebi","timestamp":"1634954580.0","comment_id":"282655","upvote_count":"5"},{"poster":"rcher","comment_id":"276642","upvote_count":"3","timestamp":"1634886720.0","content":"B.\n\nWhy not D\n\nIf the request comes from a host that uses an Amazon VPC endpoint, then the aws:SourceIp key is not available. You should instead use a VPC-specific key such as aws:VpcSourceIp.\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-sourceip"},{"timestamp":"1634848080.0","poster":"T14102020","content":"B is correct. VPC endpoint and aws:sourceVpce","upvote_count":"1","comment_id":"242489"},{"content":"'active-active failover configuration. Use Amazon EC2 in an 'Auto Scaling group' ... D again if you look at these statements from the question as 'AUTO SCALING' creates a standby ready to go","upvote_count":"1","comment_id":"241065","timestamp":"1634406540.0","poster":"petebear55"},{"timestamp":"1634340480.0","comment_id":"230032","poster":"jackdryan","content":"I'll go with B","upvote_count":"3"},{"timestamp":"1634250360.0","content":"answer is B\nfor D SourceIP can be used for external ip addresses not the private ip addresses such as those coming from a private VPC","comment_id":"229904","upvote_count":"2","poster":"YouYouYou"},{"upvote_count":"1","timestamp":"1634154900.0","poster":"Bulti","comment_id":"229231","content":"B is correct standard way to access S3 bucket is through VPC endpoint and use condition in the bucket policy to allow access if the request is coming from that VPC endpoint. No changes are required to the script as this configuration will allow access to the S3 objects using S3 API"},{"poster":"bossgandy","content":"I think the key is to migrate an 'existing HTTP' server into AWS, which will serve it's customers. So, it's all going to be internal to AWS, so \"A\" doesn't make sense, because it assumes internet connectivity. Also, the VPC endpoint is for the migrated server which will then be accessed by it's clients. So, 'B' s the right answer.","timestamp":"1634115180.0","upvote_count":"3","comment_id":"173339"},{"upvote_count":"3","timestamp":"1634035680.0","comment_id":"149577","content":"B is correct","poster":"fullaws"},{"poster":"NikkyDicky","content":"B for sure","upvote_count":"2","comment_id":"134423","timestamp":"1633920600.0"},{"upvote_count":"2","content":"Option B does not state the consumer VPC to be in public Subnet. Here is more info about whitelisting NAT GW IP as SourceIP for S3 bucket\nhttps://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/","poster":"JAWS1600","comment_id":"94612","timestamp":"1633316220.0"},{"timestamp":"1633231080.0","content":"A. Key Requirement of the solution is \"Without Modifying Automation Scripts\" - A is the only option. The current solution is using HTTP, which is going to be adopted by solution A. B is close option . However B option would require to change automation scripts, to implement with VPC Endpoints.","comment_id":"94037","poster":"JAWS1600","upvote_count":"1","comments":[{"timestamp":"1635553440.0","comment_id":"333308","poster":"sarah_t","content":"Option A will block access from instances in public subnets as those don't go via NAT inctance/gateway","upvote_count":"2"}]},{"comments":[{"poster":"sarah_t","comment_id":"333306","upvote_count":"1","timestamp":"1635421860.0","content":"A: resources in the public subnet don't use NAT."}],"timestamp":"1633079580.0","upvote_count":"1","comment_id":"93319","poster":"Merlin1","content":"A: Using Default Routes to NAT GW from Public Subnets?\nB: I Guess were assuming that no \"Put Object\" in the question is a typo? Im Thinking it is and the answer is B"},{"poster":"virtual","content":"I think it's B. VPC Endpoint with sourceVpce make sense","upvote_count":"1","timestamp":"1633024560.0","comment_id":"61059"},{"poster":"Chinmoy","timestamp":"1632991200.0","content":"B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-cloudfront-website-access/\nhttps://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/","comment_id":"50770","upvote_count":"2"},{"comment_id":"43103","timestamp":"1632824100.0","upvote_count":"4","poster":"Biswaji","content":"I will choose A, because B and D will require automation script change, c requires internet access from ec2. With option a S3 public endpoint name can be pointed to same DNS CNAME as existing artifact server. also they are not tragetting for an https access within the migration timeline. so for me A looks as a good option to achieve without making changes to existing automation script."},{"timestamp":"1632233400.0","comment_id":"12332","poster":"awsec2","upvote_count":"1","content":"why b ?"}],"answer_images":[],"answer":"B","answer_ET":"B","timestamp":"2019-09-23 20:47:00","topic":"1","question_text":"A company is migrating its on-premises build artifact server to an AWS solution. The current system consists of an Apache HTTP server that serves artifacts to clients on the local network, restricted by the perimeter firewall. The artifact consumers are largely build automation scripts that download artifacts via anonymous\nHTTP, which the company will be unable to modify within its migration timetable.\nThe company decides to move the solution to Amazon S3 static website hosting. The artifact consumers will be migrated to Amazon EC2 instances located within both public and private subnets in a virtual private cloud (VPC).\nWhich solution will permit the artifact consumers to download artifacts without modifying the existing automation scripts?","choices":{"C":"Create an IAM role and instance profile for Amazon EC2 and attach it to the instances that consume build artifacts. Configure the bucket policy to allow the s3:ListBucket and s3:GetObjects actions for the principal matching the IAM role created.","B":"Create a VPC endpoint and add it to the route table associated with subnets containing consumers. Configure the bucket policy to allow s3:ListBucket and s3:GetObject actions using the condition StringEquals and the condition key aws:sourceVpce matching the identification of the VPC endpoint.","D":"Create a VPC endpoint and add it to the route table associated with subnets containing consumers. Configure the bucket policy to allow s3:ListBucket and s3:GetObject actions using the condition IpAddress and the condition key aws:SourceIp matching the VPC CIDR block.","A":"Create a NAT gateway within a public subnet of the VPC. Add a default route pointing to the NAT gateway into the route table associated with the subnets containing consumers. Configure the bucket policy to allow the s3:ListBucket and s3:GetObject actions using the condition IpAddress and the condition key aws:SourceIp matching the elastic IP address of the NAT gateway."},"question_images":[],"exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/5621-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","unix_timestamp":1569264420,"answers_community":["B (60%)","A (40%)"],"question_id":420,"isMC":true}],"exam":{"isMCOnly":false,"isImplemented":true,"id":32,"lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Amazon","name":"AWS Certified Solutions Architect - Professional","numberOfQuestions":1019},"currentPage":84},"__N_SSP":true}