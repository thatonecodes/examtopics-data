{"pageProps":{"questions":[{"id":"k8ArV2UtWn22TIWFgk4c","answer_description":"","timestamp":"2019-08-10 14:15:00","exam_id":17,"question_images":[],"answer":"A","choices":{"C":"Store the data in an HBase table with the IP address as the row key.","B":"Store the Amazon S3 objects with the following naming scheme: bucket_name/source=ip_address/ year=yy/month=mm/day=dd/hour=hh/filename.","D":"Store the events for an IP address as a single file in Amazon S3 and add metadata with keys: Hive_Partitioned_IPAddress.","A":"Store an index of the files by IP address in the Amazon DynamoDB metadata store for EMRFS."},"answer_ET":"A","unix_timestamp":1565439300,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/3425-exam-aws-certified-big-data-specialty-topic-1-question-28/","question_text":"A customer is collecting clickstream data using Amazon Kinesis and is grouping the events by IP address into\n5-minute chunks stored in Amazon S3.\nMany analysts in the company use Hive on Amazon EMR to analyze this data. Their queries always reference a single IP address. Data must be optimized for querying based on IP address using Hive running on Amazon\nEMR.\nWhat is the most efficient method to query the data with Hive?","topic":"1","answer_images":[],"question_id":21,"discussion":[{"timestamp":"1632904860.0","comment_id":"9621","poster":"muhsin","content":"Hi,\nit is not A. Hive on EMR can just use Aruro-RDS or Glue as external metastore\nit is not C. HBase no-sql database.\nit is not D. Copying all files into one single file does not help partioning.\nIt is B. partitioning in Hive supported in S3 based on typically date (this scenario ip address)","comments":[{"timestamp":"1632960960.0","upvote_count":"1","content":"thanks @muhsin","poster":"mattyb123","comments":[{"poster":"exams","content":"I support B","timestamp":"1633177800.0","upvote_count":"3","comment_id":"11530"}],"comment_id":"9658"}],"upvote_count":"15"},{"content":"After researching more I think the answer is B. When we create the hive metadata table for the bucket where these objects are stored, we will be able to query that data in S3 using a Single IP address as the table structure will have an IP address column and columns associated with month, year, day, hour.","timestamp":"1635553260.0","upvote_count":"11","comment_id":"79542","poster":"Bulti"},{"comment_id":"136119","content":"A is correct. \"for querying based on IP address using Hive running on Amazon\nEMR\" can be the clue. C can not be the answer because Hbase itself is not related to AWS services.","poster":"skytango","upvote_count":"1","timestamp":"1636164120.0","comments":[{"content":"After researching more, I would choose B. C also can be the alternative but B is simpler to implement.","poster":"skytango","timestamp":"1636252080.0","comment_id":"136125","upvote_count":"1"}]},{"timestamp":"1635677400.0","comment_id":"80699","poster":"faloameme","upvote_count":"2","content":"Answer: A \nQuery on a HIVE table that covers an index, avoids table scan. HIVE checks the index first and then goes to the particular column and performs the operation.\nUsing option B will require queries to specify all the column partitions in every where clause"},{"comment_id":"76210","timestamp":"1635102660.0","upvote_count":"1","poster":"Bulti","content":"Answer : D\nThis is a tough one. \nNot A- Its not efficient although this might not be even doable. \nNot B- I didn’t select “B” only because the question clearly states that “Their queries always reference a single IP address”. So why create more partitions ( year, month, day, hour) and introduce latency when reading the metadata catalog in Hive created from S3 while executing the Hive QL when the intent is to just query using IP address alone.\nNot C – This doesn’t make sense. Why would you use HBase table when your data is in S3 and Hive can query S3 objects directly using the metadata stored in EMR\nD is the correct answer- Query will be performant if there are fewer partitions in S3. Also as per the question there is a need to query the data only using IP address and therefore creating a metadata with Partition ID as the key is the right option."},{"comment_id":"52342","poster":"san2020","timestamp":"1634794800.0","upvote_count":"5","content":"my selection B"},{"comment_id":"39663","comments":[{"comment_id":"40598","timestamp":"1634494860.0","poster":"practicioner","content":"No. In our case, we use Kynesis for storing data and we have an opportunity to store with necessary structure. In your link lambda+ dynamoDB use for creating partition for hive before loading. It's not our case","upvote_count":"1"}],"poster":"ME2000","upvote_count":"1","timestamp":"1634048880.0","content":"This AWS Big Data Blog proves option A is correct.\nhttps://aws.amazon.com/blogs/big-data/data-lake-ingestion-automatically-partition-hive-external-tables-with-aws/"},{"timestamp":"1633891080.0","content":"I would choose B over C because analysts are, currently, using HIve and storing the data in organized manner on S3 would be efficient solution.","upvote_count":"3","poster":"Raju_k","comment_id":"22255"},{"upvote_count":"1","timestamp":"1633888680.0","content":"Q: What is the most efficient method to query the data with Hive? We can't design new solution with HBase .. Just have to use Hive only. B is the answer.","comment_id":"20350","poster":"harry_123"},{"upvote_count":"1","comments":[{"content":"How did you know that b was wrong?","timestamp":"1634882340.0","upvote_count":"7","comment_id":"71995","poster":"jiedee"}],"comment_id":"18255","content":"I support C, In my first attempt I choose B and was wong","poster":"asadao","timestamp":"1633362240.0"},{"upvote_count":"1","comments":[{"timestamp":"1632854940.0","upvote_count":"6","content":"it should be C. As per the link: Apache Hadoop is not a perfect big data framework for real-time analytics and this is when HBase can be used i.e. For real-time querying of data. HBase is an ideal big data solution if the application requires random read or random write operations or both. If the application requires to access some data in real-time then it can be stored in a NoSQL database. HBase has its own set of wonderful API’s that can be used to pull or push data. HBase can also be integrated perfectly with Hadoop MapReduce for bulk operations like analytics, indexing, etc. The best way to use HBase is to make Hadoop the repository for static data and HBase the data store for data that is going to change in real-time after some processing.\n\nHBase should be used when –\n\nThere is large amount of data.\nACID properties are not mandatory but just required.\nData model schema is sparse.\nWhen your applications needs to scale gracefully.","comment_id":"6792","poster":"mattyb123"}],"poster":"Shatamjeev","comment_id":"6741","content":"which answer is correct A OR C?","timestamp":"1632837300.0"},{"timestamp":"1632418500.0","upvote_count":"3","comments":[{"content":"https://www.dezyre.com/article/hive-vs-hbase-different-technologies-that-work-better-together/322","timestamp":"1632751200.0","poster":"mattyb123","upvote_count":"2","comment_id":"6653"}],"comment_id":"6450","content":"hbase? selected C is answered key?","poster":"jlpl"}],"answers_community":[]},{"id":"e6vxtCyxvYpJKOHnaZPF","discussion":[{"comment_id":"76201","content":"Answer : A\nNot B- Cannot change the primary table’s partition key once created. If GSI was created on transaction ID then we could use the base table for summary transactions and the GSI for transaction details. But even then its hard to reduce the total RCU and WCU from what they currently have as the RCU will now then distributed 90%/10% between the base table and the GSI table.\nNot C - No concept of a foreign key in Dynamo DB.\nNot D- Cannot create an LSI after the table is created. Also the table already has date as the sort key already. Besides there are limitation on the table size containing an LSI which is 10GB.\nAnswer is A- by forcing the consumers to use eventual consistency the cost of RCU can be reduced into half.","comments":[{"content":"That's true: No concept of a foreign key in Dynamo DB. So it must be A","upvote_count":"1","poster":"matthew95","comment_id":"114109","timestamp":"1636172040.0"}],"timestamp":"1635810000.0","poster":"Bulti","upvote_count":"6"},{"upvote_count":"5","content":"I think the answer is A as it will reduce cost and history use case don't need strongly consistent reads. The table's partition key and sort key are already correct as well.\nAre these questions really on actual exam. @mattyB123 did you appear for exam? Please let us know how was it?","comment_id":"8855","timestamp":"1633269540.0","comments":[{"upvote_count":"7","poster":"mattyb123","content":"Yes, @ranabhay majority of these questions were on my exam. But as you have noticed some of the selected answers are incorrect which is why i have been so active to discuss the reasons why for certain answers. As you can tell with these questions they aren't worded very well on purpose to make you either over or under think the solution.","timestamp":"1633575960.0","comment_id":"8911","comments":[{"upvote_count":"1","poster":"mattyb123","comment_id":"8918","timestamp":"1633761000.0","comments":[{"poster":"ranabhay","content":"Thanks","upvote_count":"1","comment_id":"9103","timestamp":"1633829580.0"},{"poster":"VB","content":"But.. the question says ..\"Which strategy will reduce the cost associated with the clients read queries while not degrading quality?\" ... when you change from STRONG to EVENTUAL consistency, are we not degrading the quality?","timestamp":"1634099760.0","comment_id":"11903","comments":[{"poster":"42Cert","timestamp":"1635464940.0","upvote_count":"2","comment_id":"63651","content":"yes, and not only for history. And nothing tells us that we are not already eventual consistency"}],"upvote_count":"3"}],"content":"@ranabhay i think your right must be A"}]},{"timestamp":"1634018940.0","upvote_count":"2","comments":[{"timestamp":"1634060820.0","poster":"exams","content":"Agree with A","comment_id":"11531","upvote_count":"1"}],"content":"for A, I think it degrade the quality. It can be D as it does not mentioned that you cannot re-creae the table","comment_id":"11043","poster":"apertus"}],"poster":"ranabhay"},{"timestamp":"1636258380.0","comment_id":"134924","poster":"hdesai","upvote_count":"2","content":"It has to be B- Question clearly says JSON has majority chunk of data which is being returned in result even though its not needed. Infrequent accessed bulk data has to be separated from small sized data to reduce cost of RCU which is best practice. One such practice is to store data in S3 and just put refrence in DynamoDB. \nAs someone pointed vertical partitioning is way to do this as mentioned in slide page 44.\nhttps://es.slideshare.net/AmazonWebServices/advanced-design-patterns-for-amazon-dynamodb-dat403-reinvent-2017"},{"comment_id":"128460","poster":"alopazo","upvote_count":"3","content":"B. Look at page 44:\n\nhttps://es.slideshare.net/AmazonWebServices/advanced-design-patterns-for-amazon-dynamodb-dat403-reinvent-2017","timestamp":"1636256160.0"},{"content":"90% of queries against the table are executed when building transaction history view. In that case option A makes more sense. It will not impact quality as much and reduce cost immediately. Option B and D requires rebuilding of table. This also requires further evaluation that what is projected in the index will meet most of the time query requirements or not.","upvote_count":"1","timestamp":"1636008300.0","poster":"ub19","comment_id":"91503"},{"poster":"srirampc","timestamp":"1635947820.0","upvote_count":"1","comment_id":"82432","content":"A compromises quality\nB cannot change partition key for existing table\nD cannnot add a LSI after table is created\nthis leaves C, even though this is not ideal it is possible. So, C is the answer."},{"comment_id":"75159","poster":"Bulti","upvote_count":"1","content":"A is correct because B and D are not practically doable. Cannot change the partition key and cannot create an LSI after the table is created.","timestamp":"1635526320.0"},{"poster":"Kuang","upvote_count":"1","timestamp":"1635243840.0","comment_id":"59128","content":"I choose A.\nB. GSI only provide eventual consistency(same as A), but it also needs to provision capacity.\nD. LSI max size for each partition key is 10GB, each customer generates 3GB data per month, so it will exceed the size limit."},{"timestamp":"1635169680.0","upvote_count":"1","poster":"san2020","comment_id":"52343","content":"my selection A"},{"comment_id":"47467","timestamp":"1634905440.0","upvote_count":"2","poster":"richardxyz","content":"For D, both the primary table and LSI contain the JSON attribute. When you query the primary table, it still returns the JSON attribute"},{"comments":[{"poster":"yuriy_ber","comment_id":"30430","timestamp":"1634834700.0","content":"One more thought concerning D - actually our table is already sorted by CustomerID and Date, so the only additional value is projecting Details to LSI. But it doesn't make sense - we can add a ProjectionExpression parameter to return only attributes excluding Details even without LSI, so additional LSI (and GSI) doesn't give us any advantage here. Really very irritating, so A seems only possible solution here but would degrage quality....","upvote_count":"2"}],"timestamp":"1634667060.0","poster":"yuriy_ber","upvote_count":"3","content":"I think B - yes we can not change partition key after creation but the option says \"Change the primary table to partition on TransactionID\" so we can migrate without downtime. For D - we can not create LSI after creation of table.\nFuthermore local secondary index shares provisioned throughput settings for read and write activity with the table it is indexing so there will be no improvement. GSI can be added later and has its own provisioned throughput settings for read and write activity that are separate from those of the table so would have positive impact on costs.","comment_id":"28574"},{"comment_id":"22251","poster":"Raju_k","timestamp":"1634453520.0","content":"I will choose B since there is a limitation of max. 10GB data size per partition key for tables with LSI index and it is given that avg customer generates approx. 3GB data per month which will make the data size exceed 10GB limit in few months per a customer.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html#LSI.ItemCollections.SizeLimit","upvote_count":"2"},{"timestamp":"1634334120.0","content":"C no doubts","poster":"asadao","comment_id":"18256","upvote_count":"2"},{"timestamp":"1634314380.0","poster":"Ro","upvote_count":"1","content":"\"Which strategy will reduce the cost associated with the clients read queries while not degrading quality?\" - Question is reduce the associated and latency is not quality so eventual consistent should be okay but A doesn't seem right. So C?","comment_id":"15715"},{"timestamp":"1634313360.0","content":"why not C? This item size is 250KB and DDB max item size is 200KB. If we cut down the usual read in 200KB. The cost will drop down.","comments":[{"poster":"42Cert","upvote_count":"1","comment_id":"63645","timestamp":"1635348120.0","content":"C looks to me like B but with relational terms (foreign key) not NoSQL ones"}],"comment_id":"14704","poster":"iwillsky","upvote_count":"1"},{"upvote_count":"1","comment_id":"14263","poster":"pkfe","comments":[{"content":"partition on transaction ID helps when going from history to one transaction detail.","poster":"42Cert","upvote_count":"1","comment_id":"63643","timestamp":"1635276660.0"}],"timestamp":"1634244600.0","content":"Attribute Projections is important skill of tuning Dynamo DB. so answer has to be one of projecting item. B partition using transaction ID looks weird. so D."},{"timestamp":"1634123340.0","content":"@mattyb123 you agree with A but what about performance if we choose strongly consistent , also share your exam score if possible","poster":"bigdatalearner","upvote_count":"1","comment_id":"13795"},{"upvote_count":"3","comment_id":"8646","timestamp":"1632706140.0","poster":"muhsin","content":"for b, you can not create a new partition key for the primary table after creating the table same as d. the concern is that we need to reduce read cost by LSI or GSI. for GSI, it will increase the cost more than others.\nD is correct.","comments":[{"comment_id":"8802","timestamp":"1633140300.0","poster":"mattyb123","content":"Thanks","upvote_count":"1"},{"timestamp":"1632779880.0","poster":"mattyb123","comment_id":"8710","content":"With d, what about implementation of an LSI? You would need to recreate the table to implement this strategy.","upvote_count":"1"},{"content":"https://stackoverflow.com/questions/24358412/dynamodb-change-range-key-column. Mentions you cannot change primary partition key on base table, you need to re-create a table and use DynamoDB streams to copy the data to the new table with the new key structure for base table, LSI & GSI.","poster":"mattyb123","comment_id":"8783","upvote_count":"2","timestamp":"1633089180.0"}]},{"timestamp":"1632645180.0","comment_id":"7550","poster":"jlpl","upvote_count":"2","content":"B is correct"},{"timestamp":"1632205560.0","poster":"mattyb123","comments":[{"comments":[{"comment_id":"7065","upvote_count":"5","timestamp":"1632623220.0","poster":"mattyb123","content":"Correct Answer is B not D."}],"content":"Agree , D is not the correct answer , we can't create LSI after table has been created","poster":"Jialu","upvote_count":"1","timestamp":"1632619860.0","comment_id":"6928"},{"upvote_count":"1","comments":[{"content":"I think the idea is to reduce the table RCUs (to 10%)","comment_id":"63648","poster":"42Cert","upvote_count":"1","timestamp":"1635369660.0"}],"timestamp":"1632564840.0","poster":"mattyb123","content":"anyone else agree or disagree? GSI do have separate WCUs & RCUs compared to the base table and LSIs so could increase cost. Be interested to hear anyone else's thoughts","comment_id":"6916"}],"upvote_count":"2","content":"Can't be 'd' as LSI's have to be created when creating the table. For this reason i think b","comment_id":"6654"}],"question_text":"An online retailer is using Amazon DynamoDB to store data related to customer transactions. The items in the table contains several string attributes describing the transaction as well as a JSON attribute containing the shopping cart and other details corresponding to the transaction. Average item size is 250KB, most of which is associated with the JSON attribute. The average customer generates 3GB of data per month.\nCustomers access the table to display their transaction history and review transaction details as needed.\nNinety percent of the queries against the table are executed when building the transaction history view, with the other 10% retrieving transaction details. The table is partitioned on CustomerID and sorted on transaction date.\nThe client has very high read capacity provisioned for the table and experiences very even utilization, but complains about the cost of Amazon DynamoDB compared to other NoSQL solutions.\nWhich strategy will reduce the cost associated with the clients read queries while not degrading quality?","topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/3521-exam-aws-certified-big-data-specialty-topic-1-question-29/","exam_id":17,"answer_images":[],"choices":{"C":"Vertically partition the table, store base attributes on the primary table, and create a foreign key reference to a secondary table containing the JSON data. Query the primary table for summary data and the secondary table for JSON details.","D":"Create an LSI sorted on date, project the JSON attribute into the index, and then query the primary table for summary data and the LSI for JSON details.","B":"Change the primary table to partition on TransactionID, create a GSI partitioned on customer and sorted on date, project small attributes into GSI, and then query GSI for summary data and the primary table for JSON details.","A":"Modify all database calls to use eventually consistent reads and advise customers that transaction history may be one second out-of-date."},"isMC":true,"answer_description":"","answers_community":[],"question_id":22,"answer_ET":"D","answer":"D","timestamp":"2019-08-13 05:20:00","unix_timestamp":1565666400},{"id":"MKFxKyvV7SN2iIQ53Skp","choices":{"B":"Request data center Temporary Auditor access to an AWS data center to verify the control mapping.","D":"Request Amazon DynamoDB system architecture designs to determine how to map the AWS responsibilities to the control that must be provided.","A":"Request AWS third-party audit reports and/or the AWS quality addendum and map the AWS responsibilities to the controls that must be provided.","C":"Request relevant SLAs and security guidelines for Amazon DynamoDB and define these guidelines within the applications architecture to map to the control framework."},"answers_community":[],"topic":"1","question_text":"A data engineer chooses Amazon DynamoDB as a data store for a regulated application. This application must be submitted to regulators for review. The data engineer needs to provide a control framework that lists the security controls from the process to follow to add new users down to the physical controls of the data center, including items like security guards and cameras.\nHow should this control mapping be achieved using AWS?","answer_images":[],"question_images":[],"answer_description":"","answer_ET":"A","exam_id":17,"isMC":true,"answer":"A","timestamp":"2019-09-17 10:30:00","discussion":[{"upvote_count":"1","poster":"ariane_tateishi","content":"A. Should be the right answer.\nhttps://aws.amazon.com/pt/compliance/soc-faqs/\nhttps://aws.amazon.com/pt/artifact/faq/","comment_id":"366553","timestamp":"1635844800.0"},{"content":"A :: AWS System and Organization Controls (SOC) Reports are independent third-party examination reports that demonstrate how AWS achieves key compliance controls and objectives. The purpose of these reports is to help you and your auditors understand the AWS controls established to support operations and compliance.","timestamp":"1635409740.0","poster":"N1ckname","upvote_count":"3","comment_id":"249215"},{"upvote_count":"2","poster":"san2020","comment_id":"52140","content":"Selected A","timestamp":"1635122040.0"},{"poster":"kalpanareddy","content":"My answer is A because it's the only way to get the control framework details and map the responsibilities. B, C and D are distractors.","upvote_count":"3","timestamp":"1633488600.0","comment_id":"41789"},{"comment_id":"14949","upvote_count":"2","timestamp":"1633476900.0","poster":"M2","content":"Answer is A"},{"poster":"exams","comment_id":"11393","upvote_count":"2","content":"A is correct","timestamp":"1633366440.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/5298-exam-aws-certified-big-data-specialty-topic-1-question-3/","question_id":23,"unix_timestamp":1568709000},{"id":"vUV6DpgnkBQs3c5DzANO","answer":"C","exam_id":17,"topic":"1","question_images":[],"isMC":true,"timestamp":"2019-08-11 00:38:00","choices":{"B":"Batch sensor data to Amazon Simple Storage Service (S3) every 15 minutes. Flow the data downstream to the end-consumer dashboard and to the anomaly detection application.","C":"Write sensor data records to Amazon Kinesis Firehose with Amazon Simple Storage Service (S3) as the destination. Consume the data with a KCL application for the end-consumer dashboard and anomaly detection.","D":"Write sensor data records to Amazon Relational Database Service (RDS). Build both the end-consumer dashboard and anomaly detection application on top of Amazon RDS.","A":"Write sensor data records to Amazon Kinesis Streams. Process the data using KCL applications for the end-consumer dashboard and anomaly detection workflows."},"question_id":24,"question_text":"A company that manufactures and sells smart air conditioning units also offers add-on services so that customers can see real-time dashboards in a mobile application or a web browser. Each unit sends its sensor information in JSON format every two seconds for processing and analysis. The company also needs to consume this data to predict possible equipment problems before they occur. A few thousand pre-purchased units will be delivered in the next couple of months. The company expects high market growth in the next year and needs to handle a massive amount of data and scale without interruption.\nWhich ingestion solution should the company use?","discussion":[{"upvote_count":"1","comment_id":"368246","poster":"ariane_tateishi","timestamp":"1636280820.0","content":"A. Should be the right answer, considering that the Kinesis stream have automatic scale, and considering that KCL is not used for Kinesis firehose.\nhttps://aws.amazon.com/pt/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/"},{"timestamp":"1635989340.0","comment_id":"133733","poster":"MichRox","upvote_count":"1","content":"is this an actual exam question? B and D are obviously incorrect, but then we have answer C which can be excluded due to KCL not being an available consumer for FIrehose (and the dashboard being real-time), but at the same time answer A doesn't really sit well with \"scaling without interruption\" (resharding takes time and if I'm not mistaken the shards are unavailable for some time during the operation). Of the 4, A seems the only viable solution, but far from ideal."},{"content":"based on the context of the question \"massive amount of data and scale without interruption.\nWhich ingestion solution should the company use?\" it is FH. Would like this to be A but question is on on scale. Adding shard is not done in parallel. It would take long time to add 100 shards, in this aspect there could be a service interruption. C is the answer.","comment_id":"82441","upvote_count":"1","timestamp":"1635969360.0","poster":"srirampc"},{"timestamp":"1635676260.0","poster":"Bulti","comment_id":"75165","upvote_count":"4","content":"Answer A: Spark /KCL does not read from KDF."},{"poster":"san2020","upvote_count":"2","timestamp":"1634609520.0","content":"my selection A","comment_id":"52344"},{"poster":"AdamSmith","comment_id":"46433","timestamp":"1634531340.0","upvote_count":"1","comments":[{"timestamp":"1635335220.0","poster":"Zinty","upvote_count":"1","comment_id":"72402","content":"What did you select ?"}],"content":"The wording of this question is really weird, real-time analysis implies Kinesis Streams (but you have to manage sharding), while massive scaling without interruption implies Kinesis Firehose since it is a managed service (but the minimum delay is 60 seconds)."},{"comment_id":"40609","upvote_count":"1","content":"\"that customers can see real-time dashboards\" - this is a key for right answer.\nI think \"A\", but C would be more appropriate without this phrase","timestamp":"1634344260.0","poster":"practicioner"},{"content":"the difference between fh and kinesis streams is whether managed services or not. fh support autoscaling but kinesis streams need manual processing for autoscaling. 2 seconds interval is not for analytics. it is just for sending data from sensors to kinesis streams. plz do not confuse sending time with processing time. the delay for buffers (60seconds) to S3 is reasonable. it does not hurt a real time processing...","timestamp":"1634164320.0","upvote_count":"2","comments":[{"timestamp":"1634230680.0","content":"you are right about auto scaling, however you can split shards for Kinesis. Another point, you can not consume the data directly from Firehose using KCL (you can aggregate data using KPL but only using Kinesis Data Streams before - https://docs.aws.amazon.com/streams/latest/dev/kpl-with-firehose.html). Another point - what you are going to do after you wrote your data to S3, how to provide real-time capabilities?","upvote_count":"3","comment_id":"28263","poster":"yuriy_ber"}],"poster":"Percival","comment_id":"27096"},{"upvote_count":"1","content":"I would choose A over C because Firehose has minimum delay of 60 seconds which is not ideal in this case (2 seconds frequency)\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/","poster":"Raju_k","comment_id":"22253","timestamp":"1634038920.0"},{"comment_id":"15980","upvote_count":"1","timestamp":"1633601940.0","poster":"Vikki","content":"What about Scale without interruption?? Can kinesis stream scale automatically? I don't think so"},{"comment_id":"15929","content":"But Kinesis Streams would be handled for scaling for spikes, FH handles it automatically","poster":"Ro","timestamp":"1633428000.0","upvote_count":"1"},{"comments":[{"content":"A is right","comment_id":"11532","poster":"exams","timestamp":"1633126680.0","upvote_count":"1"}],"poster":"Hitu","comment_id":"10208","upvote_count":"1","content":"A - https://docs.aws.amazon.com/streams/latest/dev/introduction.html","timestamp":"1632779880.0"},{"timestamp":"1632716520.0","upvote_count":"1","poster":"Jialu","content":"A is the right one","comment_id":"7392"},{"comment_id":"6892","timestamp":"1632629040.0","content":"answer is A. Firehose is not real-time ingestion method.","poster":"muhsin","upvote_count":"1"},{"content":"Thoughts on A? no FH delay with kinesis streams","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"6655","poster":"mattyb123","timestamp":"1632403440.0","content":"answer is A"}],"timestamp":"1632178320.0","poster":"mattyb123","comment_id":"6472"}],"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/3434-exam-aws-certified-big-data-specialty-topic-1-question-30/","answers_community":[],"answer_ET":"C","unix_timestamp":1565476680},{"id":"qDWvDvVTZiegIkiaCaHT","answer_description":"","answer_ET":"C","question_id":25,"url":"https://www.examtopics.com/discussions/amazon/view/3347-exam-aws-certified-big-data-specialty-topic-1-question-31/","answer":"C","isMC":true,"exam_id":17,"discussion":[{"timestamp":"1635945000.0","content":"I think D, ES. I think noise word elimination can be done in ES not in DDB.","poster":"Josh1981","upvote_count":"1","comment_id":"93617"},{"upvote_count":"4","comment_id":"76192","timestamp":"1635396360.0","poster":"Bulti","content":"D is the correct answer. I just search for the term facting in Big Data and Apache Solr showed up which is the underlying technology used by Elasticsearch."},{"poster":"san2020","timestamp":"1635125520.0","content":"my selection D","comment_id":"52345","upvote_count":"1"},{"comments":[{"content":"Dynamodb doesn't support schema so answer is D","comment_id":"56904","timestamp":"1635261000.0","poster":"balajisush0312","upvote_count":"2"}],"content":"Answer C is correct\nUltimately DynamoDB is a data store\nhttps://aws.amazon.com/blogs/compute/indexing-amazon-dynamodb-content-with-amazon-elasticsearch-service-using-aws-lambda/","upvote_count":"4","timestamp":"1634181900.0","comment_id":"39671","poster":"ME2000"},{"poster":"bc5468521","content":"i cannot believe most of user answers are wrong. Elasticsearch is a search engine with the document data store as a secondary database; it doesn't support key-value type data, so the right answer is C","comment_id":"39371","upvote_count":"3","timestamp":"1633986960.0"},{"timestamp":"1633538160.0","poster":"antoneti","content":"I would say D as well","comment_id":"28105","upvote_count":"1"},{"content":"Yes, answer is D","comment_id":"19236","upvote_count":"1","timestamp":"1633159380.0","poster":"cybe001"},{"comment_id":"11954","content":"Are you guys 100% sure it is Elasticsearch? ...","timestamp":"1633138860.0","upvote_count":"1","poster":"VB"},{"upvote_count":"1","content":"d is the correct answer","poster":"Jialu","timestamp":"1632591540.0","comments":[{"poster":"exams","upvote_count":"3","comment_id":"11533","timestamp":"1633072560.0","content":"Yep. D.. Elasticsearch supports all 4 points"}],"comment_id":"6582"},{"timestamp":"1632548820.0","content":"i think so as well","comment_id":"6306","upvote_count":"1","poster":"jlpl"},{"content":"isn't this answer D","timestamp":"1632178620.0","poster":"mattyb123","comment_id":"6276","upvote_count":"1"}],"choices":{"B":"Amazon Redshift","A":"Amazon Relational Database Service (RDS)","D":"Amazon Elasticsearch Service","C":"Amazon DynamoDB"},"question_images":[],"answers_community":[],"topic":"1","answer_images":[],"timestamp":"2019-08-07 23:31:00","question_text":"An organization needs a data store to handle the following data types and access patterns:\n✑ Faceting\n✑ Search\n✑ Flexible schema (JSON) and fixed schema\n✑ Noise word elimination\nWhich data store should the organization choose?","unix_timestamp":1565213460}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":85,"id":17,"provider":"Amazon","isImplemented":true,"isMCOnly":true,"name":"AWS Certified Big Data - Specialty"},"currentPage":5},"__N_SSP":true}