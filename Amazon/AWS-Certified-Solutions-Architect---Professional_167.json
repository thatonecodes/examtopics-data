{"pageProps":{"questions":[{"id":"mlwyGBnCz62GSUTMD35b","answer_description":"","answer":"C","question_images":[],"choices":{"D":"Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fleet of Amazon EC2 Spot instances. Create an Amazon DynamoDB table that contains information about each uploaded image and whether it has been processed. Use an Amazon EventBridge (Amazon CloudWatch Events) rule to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to reference an Elastic Load Balancer in front of the fleet of EC2 instances.","C":"Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple Queue Service (Amazon SQS) queue. Create a fleet of Amazon EC2 instances to pull messages from the SQS queue to process the images and place them in another S3 bucket Use Amazon CloudWatch metrics for queue depth to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the S3 bucket that contains the processed images.","A":"Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon CloudWatch Logs. Create a fleet of Amazon EC2 instances that use CloudWatch Logs to determine which images need to be processed. Place processed images in another directory in Amazon EFS Enable Amazon CloudFront and configure the origin to be the one of the EC2 instances in the fleet.","B":"Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple Notification Service (Amazon SNS). Create a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) to pull messages from Amazon SNS to process the images and place them in Amazon Elastic File System (Amazon EFS). Use Amazon CloudWatch metrics for the SNS message volume to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the ALB in front of the EC2 instances."},"unix_timestamp":1650604200,"answer_images":[],"topic":"1","question_id":831,"question_text":"A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users worldwide will upload their images. The service will then overlay text on the uploaded images, which will then be published on the company website.\nWhich design should a solutions architect implement?","answer_ET":"C","exam_id":32,"isMC":true,"answers_community":["C (100%)"],"timestamp":"2022-04-22 07:10:00","url":"https://www.examtopics.com/discussions/amazon/view/74088-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"upvote_count":"8","poster":"Bigbearcn","timestamp":"1650841680.0","content":"It's easy one. C","comment_id":"591281"},{"comment_id":"714959","content":"Selected Answer: C\nc - seems to be the most \"likely\" option\nA- EFS will be very expensive for this use case.\nB - SNS does not appear support native subscriber for ALB.\nD - As data is stored on EBS volumes, can imagine there is the potential for data loss.","timestamp":"1668060540.0","poster":"janvandermerwer","upvote_count":"2"},{"upvote_count":"2","poster":"Blair77","comment_id":"694193","content":"Selected Answer: C\nEasy! CCC","timestamp":"1665684720.0"},{"upvote_count":"3","comment_id":"632200","poster":"asfsdfsdf","timestamp":"1657980720.0","content":"C - decoupled application that can have 10,000 or more calls in a seconds, S3+SQS"},{"content":"Selected Answer: C\nC for sure.","comment_id":"615165","timestamp":"1655012700.0","upvote_count":"2","poster":"Chuky64"},{"comment_id":"589745","poster":"shailurtm2001","timestamp":"1650604200.0","content":"Correct C","upvote_count":"4"}]},{"id":"upEBSHiYDhRD5bVwlqqm","question_id":832,"exam_id":32,"question_images":[],"answer_images":[],"timestamp":"2019-12-19 13:15:00","answer":"C","answer_description":"","answers_community":["C (100%)"],"answer_ET":"C","topic":"1","unix_timestamp":1576757700,"url":"https://www.examtopics.com/discussions/amazon/view/10661-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"upvote_count":"12","comment_id":"32023","content":"C is correct because Cache won't represent full DB. Here, we need aggregation on whole DB which can be with Read Replica","poster":"cinopi","timestamp":"1632748740.0"},{"content":"C. Launch a RDS Read Replica connected to your Multi AZ master database and generate reports by querying the Read Replica.","upvote_count":"1","comment_id":"1266963","timestamp":"1723803660.0","poster":"amministrazione"},{"content":"Selected Answer: C\nC is correct this a classic example to use read replicas in RDS","comment_id":"921655","timestamp":"1686594840.0","upvote_count":"1","poster":"SkyZeroZx"},{"poster":"Mimek","content":"Selected Answer: C\nC\ntypical use case of RDS read-replicas","timestamp":"1648535160.0","upvote_count":"2","comment_id":"577287"},{"poster":"AMKazi","timestamp":"1643340480.0","content":"ans is c: \nhttps://aws.amazon.com/rds/features/multi-az/#:~:text=Several%20Amazon%20RDS%20engines%20allow,instances%20in%20a%20different%20AZ.\nB: is not possible. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nA Multi-AZ DB instance deployment has one standby DB instance that provides failover support, but doesn't serve read traffic.","comment_id":"534342","upvote_count":"1"},{"content":"C is correct.","comment_id":"514016","poster":"cldy","upvote_count":"1","timestamp":"1640944740.0"},{"content":"C\nThis is a typical use case for read-replica. It is an Architect Associate level question :)","comment_id":"477021","timestamp":"1636730400.0","upvote_count":"1","poster":"mnsait"},{"content":"C is correct.","poster":"01037","timestamp":"1635930540.0","comment_id":"366052","upvote_count":"1"},{"content":"C.\nRemember cache is not the DB. For reporting you need a replica DB.","timestamp":"1635880920.0","poster":"cldy","comment_id":"328651","upvote_count":"1"},{"upvote_count":"1","timestamp":"1635426360.0","content":"But if ElastiCache is using the write-through strategy and TTL is 30+ mins, then D is an option.","comment_id":"197109","poster":"encraze"},{"upvote_count":"1","content":"C is correct","timestamp":"1634938200.0","comment_id":"144618","poster":"fullaws"},{"content":"Why not B? The data is already available in standby RDS MySQL.","upvote_count":"1","poster":"learner4ever","comment_id":"141335","comments":[{"upvote_count":"2","comments":[{"comment_id":"366051","upvote_count":"1","timestamp":"1635914820.0","poster":"01037","content":"Agreed."}],"comment_id":"196626","timestamp":"1635421200.0","poster":"cpal012","content":"That is not accessible to you. You can't query from that DB"},{"poster":"Ni_yot","comment_id":"507183","content":"The standby DB is always offline. So not accessible","upvote_count":"2","timestamp":"1640184720.0"}],"timestamp":"1634675220.0"},{"timestamp":"1634596560.0","upvote_count":"2","content":"C is the answer.\n\nOne of the use cases of Read Replica is for business reporting and cached data wouldn't represent the entire reports.","comment_id":"136947","poster":"AWSChia"},{"comment_id":"54386","timestamp":"1634040360.0","poster":"miracle","upvote_count":"1","content":"Answer is C. Very simple."},{"upvote_count":"3","comment_id":"52934","poster":"cloudinvader","timestamp":"1633407180.0","content":"Yes its C, As Cache layer will keep most read content and report need to generate on the aggregation of latest changes in DB. So using ReadReplica we can create reports without impacting main RDS."},{"upvote_count":"2","comment_id":"50970","poster":"Gorha","timestamp":"1633204080.0","content":"C is correct!"},{"comment_id":"38013","upvote_count":"2","content":"Answer is C","poster":"amog","timestamp":"1632861660.0"},{"upvote_count":"2","comment_id":"31020","content":"Caching layer is already there. no need to add Read Replica.\nD is my answer","poster":"dojo","timestamp":"1632659160.0","comments":[{"content":"The caching wouldn't grantee all data required for reporting is there. It only keeps frequent reads","comment_id":"50972","timestamp":"1633249020.0","upvote_count":"7","poster":"Gorha"}]}],"choices":{"C":"Launch a RDS Read Replica connected to your Multi AZ master database and generate reports by querying the Read Replica.","D":"Generate the reports by querying the ElastiCache database caching tier.","A":"Continually send transaction logs from your master database to an S3 bucket and generate the reports off the S3 bucket using S3 byte range requests.","B":"Generate the reports by querying the synchronously replicated standby RDS MySQL instance maintained through Multi-AZ."},"question_text":"You are running a successful multitier web application on AWS and your marketing department has asked you to add a reporting tier to the application. The reporting tier will aggregate and publish status reports every 30 minutes from user-generated information that is being stored in your web application s database.\nYou are currently running a Multi-AZ RDS MySQL instance for the database tier. You also have implemented Elasticache as a database caching layer between the application tier and database tier.\nPlease select the answer that will allow you to successfully implement the reporting tier with as little impact as possible to your database.","isMC":true},{"id":"6wz9jliFKCFORWxHr54p","question_id":833,"exam_id":32,"question_images":[],"answer_images":[],"timestamp":"2022-04-25 01:24:00","answer":"D","answers_community":["D (100%)"],"answer_description":"","answer_ET":"D","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/74393-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1650842640,"choices":{"C":"Configure S3 Event Notifications for all current S3 buckets to invoke an AWS Lambda function every time objects are accessed. Store Lambda logs in the audit logs S3 bucket.","B":"Enable replication between all current S3 buckets and the audit logs S3 bucket. Enable S3 Versioning in the audit logs S3 bucket.","D":"Enable AWS CloudTrail, and use the audit logs S3 bucket to store logs. Enable data event logging for S3 event sources, current S3 buckets, and future S3 buckets.","A":"Enable server access logging for all current S3 buckets. Use the audit logs S3 bucket as a destination for audit logs."},"discussion":[{"poster":"CAIYasia","timestamp":"1720660800.0","upvote_count":"1","content":"AWS CloudTrail provides comprehensive logging of API calls, including S3 data events, which includes object-level operations like GetObject, PutObject, etc.\nData event logging for S3 ensures that every read and write operation at the object level is captured, which satisfies the requirement to log any event that retrieves data from S3 buckets.\nEnabling data event logging for both current and future S3 buckets ensures compliance with the new security policy and makes sure that no events are missed.\nUsing the audit logs S3 bucket to store the logs consolidates all audit logs in a centralized, designated account, enhancing security and manageability.\nEnabling server access logging (Option A) only provides information about requests to S3 buckets, but does not capture the detailed object-level events required by the security policy. Hence, CloudTrail is the more suitable solution for the specified requirements.","comment_id":"1245816"},{"timestamp":"1672763640.0","comment_id":"764868","upvote_count":"1","content":"Selected Answer: D\nI'll go with D","poster":"evargasbrz"},{"comment_id":"714952","content":"Selected Answer: D\nD - Is the standard configuration when we deploy environments.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-cloudtrail-logging-for-s3.html","poster":"janvandermerwer","upvote_count":"1","timestamp":"1668059640.0"},{"poster":"AwsBRFan","timestamp":"1665825120.0","comments":[{"content":"If you want access logs for cross account delivery, you will need to use something like kinesis firehose , so it can be done, but it's overkill and expensive.","poster":"devilman222","timestamp":"1723816020.0","comment_id":"1267188","upvote_count":"1"}],"upvote_count":"1","comment_id":"695310","content":"Selected Answer: D\nInfact , access logs does not support cross accout log delivery:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html"},{"content":"Selected Answer: D\nonly D will force the policy on for future buckets","poster":"asfsdfsdf","upvote_count":"2","comment_id":"632182","timestamp":"1657978980.0"},{"content":"Selected Answer: D\nD is correct.","comment_id":"591291","poster":"Bigbearcn","timestamp":"1650842640.0","upvote_count":"3","comments":[{"comment_id":"591678","content":"s3 access log don't support cross account log delivery","comments":[{"timestamp":"1665283740.0","comment_id":"689885","upvote_count":"1","poster":"[Removed]","content":"Cloudtrail data event for s3 supports cross account log delivery - https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html#logging-data-events-for-s3-resources-in-other-accounts"}],"poster":"Bigbearcn","upvote_count":"2","timestamp":"1650888720.0"}]}],"question_text":"A company has a new security policy. The policy requires the company to log any event that retrieves data from Amazon S3 buckets. The company must save these audit logs in a dedicated S3 bucket.\nThe company created the audit logs S3 bucket in an AWS account that is designated for centralized logging. The S3 bucket has a bucket policy that allows write- only cross-account access.\nA solutions architect must ensure that all S3 object-level access is being logged for current S3 buckets and future S3 buckets.\nWhich solution will meet these requirements?","isMC":true},{"id":"CsM0KyoAXOYkpEpAgAaj","choices":{"C":"Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.","D":"Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.","B":"Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.","A":"Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.","E":"Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page."},"answer_ET":"AE","question_images":[],"discussion":[{"timestamp":"1667271720.0","content":"Selected Answer: AE\n\"Save your custom error pages in a location that is accessible to CloudFront. We recommend that you store them in an Amazon S3 bucket, and that you don’t store them in the same place as the rest of your website or application’s content. If you store the custom error pages on the same origin as your website or application, and the origin starts to return 5xx errors, CloudFront can’t get the custom error pages because the origin server is unavailable.\"\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html","poster":"Jonfernz","upvote_count":"6","comment_id":"708810"},{"comment_id":"1260758","upvote_count":"1","timestamp":"1722794400.0","content":"A and C are the ones i will pick","poster":"Chungies"},{"content":"Selected Answer: AC\nAppears logical to me","timestamp":"1690551720.0","poster":"rsn","upvote_count":"1","comment_id":"965594"},{"timestamp":"1676037480.0","poster":"zWarez","comment_id":"804382","upvote_count":"1","content":"AE. CloudWatch cannot be used here since it's not repoint to error page in time."},{"timestamp":"1665492060.0","upvote_count":"1","poster":"JohnPi","comments":[{"timestamp":"1665492120.0","comment_id":"692111","poster":"JohnPi","content":"furthermore, \"The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs\"","upvote_count":"1"},{"upvote_count":"1","comments":[{"poster":"JohnPi","content":"https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-failing-health-checks-alb/","comment_id":"700810","upvote_count":"1","timestamp":"1666352280.0"}],"timestamp":"1665952140.0","content":"Not true. HTTP 502: Bad gateway -- https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html#http-502-issues","poster":"Naj_64","comment_id":"696534"},{"poster":"JohnPi","content":"AE is the answer","comment_id":"704372","timestamp":"1666757940.0","upvote_count":"1"}],"content":"Selected Answer: AB\nAB\nALB 502 means FailedHealthChecks \nDNS has a TTL and relies on the client's good behavior.","comment_id":"692109"},{"upvote_count":"1","comment_id":"655023","timestamp":"1661944920.0","poster":"gnic","content":"Selected Answer: AE\nAE no brain"},{"content":"Selected Answer: AE\nhttps://aws.amazon.com/blogs/aws/custom-error-pages-and-responses-for-amazon-cloudfront/","comment_id":"620694","timestamp":"1655945520.0","poster":"rockc","upvote_count":"1"},{"upvote_count":"1","timestamp":"1652711580.0","content":"I think DE\nAs in D you need a health-check and lambda mechanism to handle the incident \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html","comment_id":"602626","poster":"[Removed]"},{"comment_id":"600001","upvote_count":"3","timestamp":"1652260560.0","content":"AE\nC, there is not InternalError type error but InternalFailure. \nE, CloudFront provides the error page features exactly","poster":"titleone"},{"poster":"user0001","comment_id":"598738","comments":[{"comment_id":"602264","poster":"hfeng95","upvote_count":"1","timestamp":"1652642520.0","content":"Read the problem, you will see that they would like to have a customized error page while troubleshooting. A&E is correct in my opinion"}],"upvote_count":"2","content":"A/C , you need it only on failure, E does not provide this option","timestamp":"1652048040.0"},{"content":"AE. Custom error page for CF.","comment_id":"591098","poster":"Bigbearcn","timestamp":"1650808380.0","upvote_count":"2"},{"upvote_count":"3","poster":"shailurtm2001","comment_id":"590189","content":"It's AE https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html","timestamp":"1650651480.0"}],"answer_description":"","unix_timestamp":1650651480,"question_text":"A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB).\nThe company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.\nAfter an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.\nWhile the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.\nWhich combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)","answer":"AE","url":"https://www.examtopics.com/discussions/amazon/view/74161-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2022-04-22 20:18:00","answer_images":[],"isMC":true,"answers_community":["AE (80%)","10%","10%"],"topic":"1","question_id":834,"exam_id":32},{"id":"0PmekVkXOd3nsc8pnZJ5","answer_description":"","answer_ET":"B","isMC":true,"discussion":[{"comment_id":"590197","upvote_count":"5","content":"B correct https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/centralized-egress-to-internet.html","poster":"shailurtm2001","timestamp":"1650651960.0"},{"comment_id":"939427","timestamp":"1688162940.0","poster":"ajchi1980","content":"Selected Answer: B\nOption A (creating peering connections): Peering connections allow connectivity between VPCs but do not provide the ability to route internet traffic through a central egress VPC.\nOption B (creating a transit gateway): This is the correct approach for implementing a hub-and-spoke design. By creating a transit gateway and sharing it with the existing AWS accounts, the spoke VPCs can be attached to the transit gateway. Routing can be configured to direct all internet-bound traffic from the spoke VPCs to the egress VPC with the NAT gateway.\nOption C (creating a transit gateway in every account): While it is possible to create a transit gateway in every account, it would result in unnecessary complexity and management overhead. It is more efficient to have a single transit gateway shared across accounts.\nOption D (creating an AWS PrivateLink connection): AWS PrivateLink is used for private connectivity between VPCs and AWS services, and it does not provide the ability to route internet traffic through an egress VPC.","upvote_count":"1"},{"content":"B, Architecture diagram with sequence number for outbound flow (via egress vpc) - https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/NAT-gateway-centralized-egress-ra.pdf?did=wp_card&trk=wp_card","upvote_count":"1","timestamp":"1663089720.0","poster":"pixepe","comment_id":"668274"},{"timestamp":"1662303000.0","content":"B. Yes is correct ans","poster":"Ni_yot","comment_id":"659345","upvote_count":"1"},{"timestamp":"1657987140.0","poster":"asfsdfsdf","comment_id":"632248","upvote_count":"4","content":"Selected Answer: B\nB is the only correct one since peering is limited to 125"},{"upvote_count":"1","comment_id":"623813","content":"Selected Answer: B\nit's B","timestamp":"1656401520.0","poster":"TechX"}],"exam_id":32,"unix_timestamp":1650651960,"question_text":"A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.\nA solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.\nWhich set of additional steps should the solutions architect take to meet these requirements?","choices":{"D":"Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.","A":"Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.","C":"Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet.","B":"Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet."},"url":"https://www.examtopics.com/discussions/amazon/view/74163-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"question_images":[],"answers_community":["B (100%)"],"question_id":835,"answer":"B","topic":"1","timestamp":"2022-04-22 20:26:00"}],"exam":{"isImplemented":true,"isMCOnly":false,"name":"AWS Certified Solutions Architect - Professional","isBeta":false,"provider":"Amazon","numberOfQuestions":1019,"id":32,"lastUpdated":"11 Apr 2025"},"currentPage":167},"__N_SSP":true}