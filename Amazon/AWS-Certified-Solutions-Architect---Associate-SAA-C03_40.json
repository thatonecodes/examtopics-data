{"pageProps":{"questions":[{"id":"4uRxij0diErT4yXehfaN","exam_id":31,"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95027-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"A","question_text":"A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches.\n\nHow should the company move the data to Amazon S3 to meet these requirements?","answers_community":["A (88%)","12%"],"topic":"1","timestamp":"2023-01-13 13:07:00","answer_description":"","choices":{"C":"Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.","A":"Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.","B":"Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.","D":"Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3."},"answer_images":[],"answer":"A","unix_timestamp":1673611620,"question_id":196,"discussion":[{"upvote_count":"10","timestamp":"1703767740.0","comment_id":"1107774","content":"Selected Answer: A\nB - EMR cluster is for Big Data, has nothing to do with this\nC - invokes the function \"on a schedule\", but you want to capture events\nD - Could work, but would be overcomplex and would \"affect the speed of EC2 instance launches\" (which it should not)","comments":[{"content":"Right.","comment_id":"1130481","poster":"LoXoL","timestamp":"1706090880.0","upvote_count":"2"}],"poster":"pentium75"},{"comment_id":"1004720","timestamp":"1694432820.0","content":"Selected Answer: A\nThis solution meets the requirements because it is serverless and does not affect the speed of EC2 instance launches. Amazon CloudWatch metric streams can continuously stream CloudWatch metrics to destinations such as Amazon S3. Amazon Kinesis Data Firehose can capture, transform, and deliver streaming data into data lakes, data stores, and analytics services. It can directly put the data into Amazon S3, which can then be used for near-real-time updates in a dashboard.","upvote_count":"8","poster":"Guru4Cloud"},{"poster":"reviewmine","comment_id":"1153833","content":"Selected Answer: A\nAnswer A: Near real time --> Amazon Kinesis Data Firehose","timestamp":"1708338420.0","upvote_count":"5"},{"poster":"TariqKipkemei","upvote_count":"4","content":"Selected Answer: A\nYou can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations.\nMain usage scenarios for CloudWatch metric streams: Data lakeâ€” Create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake such as Amazon S3.\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html#:~:text=CloudWatch%20metric%20streams","comment_id":"1018512","timestamp":"1695794340.0"},{"upvote_count":"1","comments":[{"timestamp":"1703767620.0","content":"C invokes the Lambda function \"on a schedule\". It would collect the scaling status during its runs. But you don't want the hourly status, you want to report \"scaling events\".","poster":"pentium75","upvote_count":"2","comment_id":"1107768"}],"poster":"Valder21","timestamp":"1693902840.0","comment_id":"999258","content":"Selected Answer: C\nKinesis is for data streams not events. So, C"},{"content":"Selected Answer: A\nB. introduces unnecessary complexity and overhead for collecting and sending the EC2 Auto Scaling status data to S3. It is not the most efficient serverless solution for this specific requirement.\n\nC. would introduce delays in data updates, as it is not triggered in real-time. Additionally, it adds unnecessary overhead and complexity compared to using a direct data stream.\n\nD. introduces additional dependencies and management overhead. It may also impact the speed of EC2 instance launches, which is a requirement that needs to be avoided.\n\nOverall, option A provides a streamlined and serverless solution by leveraging CloudWatch metric streams and Kinesis Data Firehose to efficiently capture and store the EC2 Auto Scaling status data in S3 without affecting the speed of EC2 instance launches.","comment_id":"937667","upvote_count":"6","timestamp":"1688022120.0","poster":"cookieMr"},{"timestamp":"1687225440.0","poster":"markw92","content":"A: I was thinking D is the answer but the solution should not impact ec2 launches will make the difference and i fast read the question. A is a right choice.","upvote_count":"2","comment_id":"928085"},{"comment_id":"893464","content":"A because of near real time scenario","timestamp":"1683671160.0","upvote_count":"4","poster":"Rahulbit34"},{"content":"Selected Answer: C\nBoth A and C are applicable - no doubt there.\n\nC is more straightforward and to the point of the question imho.","timestamp":"1678794180.0","comment_id":"838793","poster":"UnluckyDucky","comments":[{"comment_id":"838796","content":"Changing my answer to *A* as the dashboard will provide near-real updates.\n\nUnless the lambda is configured to run every minute which is not common with schedules - it is not considered near real-time.","poster":"UnluckyDucky","upvote_count":"4","timestamp":"1678794540.0"},{"upvote_count":"2","poster":"JohnYu","comment_id":"1295041","content":"While EventBridge can capture events, scheduling Lambda functions to poll data is less efficient and may introduce latency, which would not meet the near-real-time requirement.","timestamp":"1728462720.0"}],"upvote_count":"3"},{"comment_id":"808795","content":"Selected Answer: A\nServerless solution and near real time","poster":"bdp123","upvote_count":"4","timestamp":"1676405580.0"},{"comment_id":"807727","poster":"Stanislav4907","upvote_count":"2","content":"Selected Answer: A\nnear real time -eliminates c","timestamp":"1676314980.0"},{"content":"Selected Answer: A\nAnswer is A","poster":"aakashkumar1999","comment_id":"798042","upvote_count":"2","timestamp":"1675521480.0"},{"poster":"devonwho","timestamp":"1675043880.0","content":"Selected Answer: A\nYou can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. One of the use cases is Data Lake: create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake such as Amazon S3.\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html","comment_id":"792277","upvote_count":"3"},{"timestamp":"1675021860.0","upvote_count":"2","poster":"Stanislav4907","comment_id":"792012","content":"Selected Answer: A\nOption C, using an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule to send the EC2 Auto Scaling status data directly to Amazon S3, may not be the best choice because it may not provide real-time updates to the dashboard.\n\nA schedule-based approach with an EventBridge rule and Lambda function may not be able to deliver the data in near real-time, as the EC2 Auto Scaling status data is generated dynamically and may not always align with the schedule set by the EventBridge rule.\n\nAdditionally, using a schedule-based approach with EventBridge and Lambda also has the potential to create latency, as there may be a delay between the time the data is generated and the time it is sent to S3.\n\nIn this scenario, using Amazon CloudWatch and Kinesis Data Firehose as described in Option A, provides a more reliable and near real-time solution."},{"content":"Selected Answer: A\nA seems to be the right answer. Don't think C could be correct as it says \"near real-time\" and C is on schedule","poster":"MikelH93","comment_id":"791955","upvote_count":"2","timestamp":"1675018740.0"},{"poster":"KAUS2","comments":[],"upvote_count":"2","timestamp":"1674822000.0","content":"Selected Answer: C\nC. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.","comment_id":"789578"},{"comments":[{"timestamp":"1703767680.0","comment_id":"1107771","poster":"pentium75","content":"But A is serverless.","upvote_count":"2"}],"poster":"techhb","comment_id":"779341","timestamp":"1673990580.0","upvote_count":"3","content":"Selected Answer: A\nA seemsright choice but serverless keyword confuses,and cloud watch metric steam is server less too."},{"upvote_count":"3","poster":"Aninina","content":"Selected Answer: A\nA. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.","comment_id":"778176","timestamp":"1673898540.0"},{"comment_id":"776786","upvote_count":"2","timestamp":"1673799240.0","comments":[],"poster":"mhmt4438","content":"Selected Answer: C\nC. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.\n\nThis approach will use a serverless solution (AWS Lambda) which will not affect the speed of EC2 instance launches. It will use the EventBridge rule to invoke the Lambda function on schedule to send the data to S3. This will meet the requirement of near-real-time updates in a dashboard as well. The Lambda function can be triggered by CloudWatch events that are emitted when Auto Scaling events occur. The function can then collect the necessary data and store it in S3. This direct sending of data to S3 will reduce the number of steps and hence it is more efficient and cost-effective."},{"content":"Selected Answer: A\nA is the correct answer.\n\"near-real-time\" => A & D\n\"The solution must not affect the speed of EC2 instance launches.\" => D is an incorrect","timestamp":"1673746800.0","upvote_count":"4","comment_id":"776086","poster":"Parsons"},{"comment_id":"774422","upvote_count":"2","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/81327-exam-aws-certified-solutions-architect-associate-saa-c02/","timestamp":"1673611620.0","poster":"bamishr"}]},{"id":"ypRWDZMSOOS3t4wXFQL4","answer_ET":"D","topic":"1","answer_description":"","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/95028-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"question_id":197,"discussion":[{"content":"Selected Answer: D\nNo, D should be correct.\n\n\"LEAST operational overhead\" => Should you fully manage service like Glue instead of manually like the answer A.","upvote_count":"15","poster":"Parsons","timestamp":"1673746980.0","comments":[{"content":"I also think it's D but remember that D requires writing ETL logic in AWS Glue (nothing in question says how complex it will be). AWS Lambda for CSV could be simple also (imagine NodeJS and millions of libraries support or Python's parsing) so both could be operationally on par to each other. Logically D makes more sense but in practice, AWS Glue rarely works with out of the box ETL and becomes a maintenance overhead in itself.","timestamp":"1704200280.0","comment_id":"1111904","poster":"awsgeek75","upvote_count":"1"}],"comment_id":"776089"},{"upvote_count":"5","timestamp":"1694432520.0","poster":"Guru4Cloud","content":"Selected Answer: D\nThis solution meets the requirements with the least operational overhead because AWS Glue is a fully managed ETL service that makes it easy to move data between data stores. AWS Glue can read .csv files from an S3 bucket and write the data into Parquet format in another S3 bucket. The AWS Lambda function can be triggered by an S3 PUT event when a new .csv file is uploaded, and it can start the AWS Glue ETL job to convert the file to Parquet format. This solution does not require managing any servers or clusters, which reduces operational overhead.","comment_id":"1004717"},{"upvote_count":"4","comment_id":"1018528","content":"Selected Answer: D\nAWS Glue can run your extract, transform, and load (ETL) jobs as new data arrives. For example, you can configure AWS Glue to initiate your ETL jobs to run as soon as new data becomes available in Amazon Simple Storage Service (S3). \nClearly you don't need a lambda function to initiate the ETL job.\n\nhttps://aws.amazon.com/glue/#:~:text=to%20initiate%20your-,ETL,-jobs%20to%20run\n\nOption A requires writing code to perform the file conversion.\n\nIn the exam option D would the best answer.","timestamp":"1695795000.0","poster":"TariqKipkemei"},{"upvote_count":"2","content":"D is correct","timestamp":"1688022360.0","poster":"cookieMr","comment_id":"937682"},{"upvote_count":"1","poster":"F629","comment_id":"927284","comments":[{"upvote_count":"2","timestamp":"1703767920.0","content":"Creating, maintaining and supporting custom code that does the same as a ready-made serverless service is NEVER \"least operational effort\".","comment_id":"1107776","poster":"pentium75"},{"upvote_count":"2","poster":"jaswantn","comments":[{"upvote_count":"1","comment_id":"1314661","content":"But what if the file conversion process exceeds more than 15 minutes for each file? How does Lambda fits into this picture?","poster":"JA2018","timestamp":"1732023480.0"}],"content":"Now Lambda support 1 GB to 10 GB.","timestamp":"1707264120.0","comment_id":"1142850"}],"content":"Selected Answer: A\nBoth A and D can works, but A is more simple. It's more close to the \"Least Operational effort\".","timestamp":"1687161240.0"},{"upvote_count":"3","comment_id":"863865","poster":"shanwford","content":"Selected Answer: D\nThe maximum size for a Lambda event payload is 256 KB - so (A) didn't work with 1GB Files. Glue is recommended for the Parquet Transformation of AWS.","timestamp":"1680870780.0"},{"poster":"jennyka76","comment_id":"803707","timestamp":"1675979700.0","upvote_count":"3","content":"ANS - d\nhttps://aws.amazon.com/blogs/database/how-to-extract-transform-and-load-data-for-analytic-processing-using-aws-glue-part-2/\n- READ ARTICLE -"},{"timestamp":"1674719700.0","poster":"aws4myself","upvote_count":"5","comments":[{"poster":"JA2018","timestamp":"1732023300.0","comments":[{"upvote_count":"1","timestamp":"1732023360.0","content":"IIRC, many folks on this forum mentioned that, rule of thumbs for AWS exams, opt for AWS managed services...","poster":"JA2018","comment_id":"1314660"}],"content":"But what if the file conversion process exceeds more than 15 minutes for each file? How does Lambda fits into this picture?","comment_id":"1314658","upvote_count":"1"},{"timestamp":"1675063980.0","content":"We also need to stay with the question, cost was not a consideration in the question.","poster":"LuckyAro","comment_id":"792521","upvote_count":"2"},{"timestamp":"1677424860.0","comment_id":"822598","content":"Cost is not a factor. AWS Glue is a fully managed service therefore, it's the least operational overhead","upvote_count":"5","poster":"nder"}],"comment_id":"788502","content":"Here A is the correct answer. The reason here is the least operational overhead.\nA ==> S3 - Lambda - S3\nD ==> S3 - Lambda - Glue - S3\n\nAlso, glue cannot convert on fly automatically, you need to write some code there. If you write the same code in lambda it will convert the same and push the file to S3\n\nLambda has max memory of 128 MB to 10 GB. So, it can handle it easily.\n\nAnd we need to consider cost also, glue cost is more. Hope many from this forum realize these differences."},{"content":"A is unlikely to work as Lambda may struggle with 1GB size: \"< 64 MB, beyond which lambda is likely to hit memory caps\", see https://stackoverflow.com/questions/41504095/creating-a-parquet-file-on-aws-lambda-function","timestamp":"1674553920.0","comment_id":"786381","poster":"JayBee65","upvote_count":"3"},{"comment_id":"784146","timestamp":"1674380640.0","content":"Should be D as Glue is self managed service and provides tel job for converting cab files to parquet off the shelf.","upvote_count":"2","poster":"jainparag1"},{"comment_id":"780229","upvote_count":"2","poster":"Joxtat","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html","timestamp":"1674060480.0"},{"timestamp":"1673998740.0","upvote_count":"2","content":"AWS Glue is right solution here.","comment_id":"779395","poster":"techhb"},{"timestamp":"1673897940.0","content":"Selected Answer: D\nI am thinking D. \n\n A says lambda will download the .csv...but to where? that seem manual based on that","upvote_count":"2","comment_id":"778166","poster":"mp165"},{"comment_id":"776803","poster":"mhmt4438","content":"Selected Answer: A\nI think A","upvote_count":"1","timestamp":"1673800080.0"},{"upvote_count":"1","comment_id":"774423","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/83201-exam-aws-certified-solutions-architect-associate-saa-c02/","timestamp":"1673611740.0","poster":"bamishr"}],"unix_timestamp":1673611740,"isMC":true,"timestamp":"2023-01-13 13:09:00","question_images":[],"choices":{"D":"Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.","B":"Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.","C":"Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.","A":"Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event."},"answers_community":["D (91%)","9%"],"question_text":"A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[]},{"id":"hEB3nU6FucP2hWDqoxqx","question_text":"A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable.\n\nWhich solution should a solutions architect recommend to meet these requirements?","choices":{"D":"Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.","B":"Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.","A":"Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.","C":"Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years."},"answers_community":["A (95%)","5%"],"topic":"1","answer_images":[],"answer_ET":"A","exam_id":31,"unix_timestamp":1673611860,"url":"https://www.examtopics.com/discussions/amazon/view/95030-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"timestamp":"2023-01-13 13:11:00","isMC":true,"answer":"A","discussion":[{"timestamp":"1688022780.0","poster":"cookieMr","comment_id":"937709","content":"Selected Answer: A\nA. suggests using AWS Backup, a centralized backup management service, to retain RDS backups. A backup vault is created, and a backup plan is defined with a daily schedule and a 2-year retention period for backups. RDS DB instances are assigned to this backup plan.\n\nB. it does not address the requirement for consistent and restorable backups. Snapshots are point-in-time backups and may not provide the desired level of consistency.\n\nC. it is not designed to provide the backup and restore functionality required for databases. It does not ensure the backups are consistent or provide an easy restore mechanism.\n\nD. it does not address the requirement for daily backups and retention of consistent backups. It focuses more on replication and change data capture rather than backup and restore.","upvote_count":"11"},{"content":"Selected Answer: A\nCreate a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.","upvote_count":"5","poster":"bamishr","timestamp":"1673611860.0","comment_id":"774426"},{"comment_id":"1179684","comments":[{"timestamp":"1732023600.0","poster":"JA2018","upvote_count":"1","content":"I thought, for AWS exams, as a rule of thumbs, we should opt for AWS managed services?","comment_id":"1314663"},{"upvote_count":"2","content":"If not specified, AWS Backup Vault Lock will not enforce a maximum retention period. If specified, backup and copy jobs to this vault with lifecycle retention periods longer than the maximum retention period will fail. Recovery points already saved in the vault prior to the vault lock's creation are not affected. The longest maximum retention period you can specify is 36500 days (approximately 100 years).","comment_id":"1267294","timestamp":"1723831620.0","poster":"kennylin689"}],"content":"The solution architect should recommend option B - Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.\n\nThis meets the requirements of:\n\nRetaining daily backups for a minimum of 2 years. The daily snapshots captured within the backup window provide consistent, restorable backups on a daily basis.\n\nAssigning a snapshot retention policy of 2 years ensures the snapshots are retained for the required period.\n\nUsing Amazon DLM allows automatically deleting snapshots older than 2 years to comply with the retention period in a cost-effective manner without manual administration.\n\nThe other option of using AWS Backup vault is not as suitable since it has limitations such as 35 day maximum retention for automated backups. Option B provides a native RDS solution capable of meeting the long term 2 year retention requirements.","poster":"Ojonugwa","timestamp":"1711057980.0","upvote_count":"3"},{"poster":"vijaykamal","comment_id":"1020799","comments":[{"timestamp":"1703774100.0","upvote_count":"3","poster":"pentium75","comment_id":"1107855","content":"\"AWS Backup is primarily used for managing backups of resources that may not have built-in backup capabilities\" says who?"}],"upvote_count":"2","content":"Selected Answer: B\nHere's why Option B is the best choice:\n\nBackup Window: Configuring a backup window for daily snapshots ensures that consistent backups are taken at the specified time each day. This helps maintain data integrity and consistency.\n\nSnapshot Retention Policy: Assigning a snapshot retention policy of 2 years to each RDS DB instance ensures that the backups are retained for the required duration.\n\nAmazon Data Lifecycle Manager (Amazon DLM): Amazon DLM can be used to automate the management of EBS snapshots, including RDS snapshots. You can configure Amazon DLM to schedule snapshot deletions, making it easier to manage the retention policy without manual intervention.\n\nOption A (AWS Backup) is primarily used for managing backups of resources that may not have built-in backup capabilities, but for Amazon RDS, it's better to use the built-in snapshot capabilities and Amazon DLM for snapshot retention.","timestamp":"1695992880.0"},{"content":"Selected Answer: A\nCreate a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.","timestamp":"1695795180.0","comment_id":"1018530","poster":"TariqKipkemei","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: A\nA. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan","comment_id":"1004711","timestamp":"1694432280.0","poster":"Guru4Cloud"},{"comment_id":"941854","upvote_count":"4","content":"Selected Answer: A\nBackups work with EBS, FSX, RDS. Its managed & can has vault option for more better control over backup retention","poster":"animefan1","timestamp":"1688388900.0"},{"poster":"markw92","upvote_count":"2","comment_id":"928088","content":"Why not B?","timestamp":"1687225620.0"},{"comment_id":"908288","content":"Selected Answer: A\nA is correct","poster":"_deepsi_dee29","timestamp":"1685240820.0","upvote_count":"1"},{"comments":[{"content":"Requirement is backup, not migration.","timestamp":"1703774160.0","poster":"pentium75","upvote_count":"3","comment_id":"1107856"}],"content":"Why not D?\n\nCreating tasks for ongoing replication using AWS DMS: You can create an AWS DMS task that captures ongoing changes from the source data store. You can do this capture while you are migrating your data. You can also create a task that captures ongoing changes after you complete your initial (full-load) migration to a supported target data store. This process is called ongoing replication or change data capture (CDC). AWS DMS uses this process when replicating ongoing changes from a source data store.","poster":"antropaws","timestamp":"1684993800.0","upvote_count":"1","comment_id":"906416"},{"comment_id":"852438","upvote_count":"2","poster":"gold4otas","content":"Selected Answer: A\nA. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.","timestamp":"1679947260.0"},{"content":"Selected Answer: A\nA is right choice","timestamp":"1673998860.0","upvote_count":"4","poster":"techhb","comment_id":"779397"},{"content":"Selected Answer: A\nA A A A A A","comment_id":"778202","timestamp":"1673900100.0","poster":"Aninina","upvote_count":"3"},{"content":"Selected Answer: A\nCorrect answer is A","poster":"mhmt4438","upvote_count":"3","comment_id":"776806","timestamp":"1673800260.0"}],"answer_description":"","question_id":198},{"id":"ugyvPKraOqHmraZLylof","timestamp":"2022-10-10 02:34:00","url":"https://www.examtopics.com/discussions/amazon/view/84940-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1665362040,"answer":"A","discussion":[{"poster":"Buruguduystunstugudunstuy","timestamp":"1672173120.0","content":"Selected Answer: A\nThe solution that will accomplish this goal is A: Turn on AWS Config with the appropriate rules.\n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use AWS Config to monitor and record changes to the configuration of your Amazon S3 buckets. By turning on AWS Config and enabling the appropriate rules, you can ensure that your S3 buckets do not have unauthorized configuration changes.","upvote_count":"60","comment_id":"759048","comments":[{"timestamp":"1672173180.0","comment_id":"759049","content":"AWS Trusted Advisor (Option B) is a service that provides best practice recommendations for your AWS resources, but it does not monitor or record changes to the configuration of your S3 buckets.\n\nAmazon Inspector (Option C) is a service that helps you assess the security and compliance of your applications. While it can be used to assess the security of your S3 buckets, it does not monitor or record changes to the configuration of your S3 buckets.\n\nAmazon S3 server access logging (Option D) enables you to log requests made to your S3 bucket. While it can help you identify changes to your S3 bucket, it does not monitor or record changes to the configuration of your S3 bucket.","upvote_count":"47","poster":"Buruguduystunstugudunstuy"}]},{"comment_id":"711498","content":"Configuration changes= AWS Config","upvote_count":"32","timestamp":"1667614080.0","poster":"gokalpkocer3"},{"timestamp":"1735576440.0","comment_id":"1334256","content":"Selected Answer: A\nAWS Config is the only one that provides continuous monitoring here.","poster":"satyaammm","upvote_count":"1"},{"poster":"MGKYAING","upvote_count":"1","timestamp":"1735269780.0","comment_id":"1332201","content":"Selected Answer: A\nTo review an AWS deployment and ensure that Amazon S3 buckets do not have unauthorized configuration changes, you need a service that can monitor and record the configuration state of AWS resources and evaluate them against desired configurations."},{"poster":"PaulGa","timestamp":"1723581840.0","upvote_count":"3","comment_id":"1265360","content":"Selected Answer: A\nAns A - as well explained by \"Buruguduystunstugudunstuy\" â€“ we are dealing with configuration here: ensuring that what we've designed continues to follow the rules"},{"poster":"andyngkh86","content":"ChatGPT give the answer is D","upvote_count":"2","comment_id":"1123402","timestamp":"1705325160.0"},{"content":"Selected Answer: A\nAnswer-A","upvote_count":"1","poster":"A_jaa","comment_id":"1121638","timestamp":"1705149480.0"},{"poster":"Ruffyit","comment_id":"1054487","upvote_count":"2","content":"A: https://aws.amazon.com/config/#:~:text=How%20it%20works-,AWS%20Config,-continually%20assesses%2C%20audits","timestamp":"1698318480.0"},{"poster":"TariqKipkemei","content":"Selected Answer: A\nAWS Config continually assesses, audits, and evaluates the configurations and relationships of your resources on AWS, on premises, and on other clouds. It normalizes changes into a consistent format and checks resource compliance with custom and managed rules before and after provisioning.\n\nhttps://aws.amazon.com/config/#:~:text=How%20it%20works-,AWS%20Config,-continually%20assesses%2C%20audits","timestamp":"1690951140.0","upvote_count":"4","comment_id":"969677"},{"timestamp":"1690216440.0","upvote_count":"2","comment_id":"961816","poster":"Guru4Cloud","content":"Selected Answer: A\nAWS Config provides a detailed inventory of the company's AWS resources and configuration history, and can be configured with rules to evaluate resource configurations for compliance with policies and best practices.\n\nThe solutions architect can enable AWS Config and configure rules specifically checking for S3 bucket settings like public access blocking, encryption settings, access control lists, etc. AWS Config will record configuration changes to S3 buckets over time, allowing the company to review changes and be alerted about any unauthorized modifications.\nBy. Claude.ai"},{"content":"Option A is the right answer for this.","poster":"miki111","timestamp":"1689533160.0","comment_id":"953567","upvote_count":"1"},{"poster":"cookieMr","content":"Selected Answer: A\nAWS Config is a service that provides a detailed view of the configuration of AWS resources in your account. By enabling AWS Config, you can capture configuration changes and maintain a record of resource configurations over time. It allows you to define rules that check for compliance with desired configurations and can generate alerts or automated actions when unauthorized changes occur.\n\nTo accomplish the goal of preventing unauthorized configuration changes in Amazon S3 buckets, you can configure AWS Config rules specifically for S3 bucket configurations. These rules can check for a variety of conditions, such as ensuring that encryption is enabled, access control policies are correctly configured, and public access is restricted.\n\nWhile options B, C, and D offer valuable services for various aspects of AWS deployment, they are not specifically focused on preventing unauthorized configuration changes in Amazon S3 buckets as effectively as enabling AWS Config.","upvote_count":"2","comment_id":"926583","timestamp":"1687079880.0"},{"content":"Don't be mistaken in thinking that it's Server access logs because that's for detailed records for requests made to S3. It's AWS Config because it records configuration changes.","poster":"Abrar2022","comment_id":"897294","upvote_count":"1","timestamp":"1684043940.0"},{"comment_id":"886623","timestamp":"1682966820.0","upvote_count":"1","poster":"Rahulbit34","content":"AWS truseted Adviser is for providing recommendation only.\nFor any configuration use AWS config\nInspecter is for scanning for any software vulnerabilities and unintended network exposure"},{"content":"Selected Answer: A\nTo accomplish the goal of ensuring that Amazon S3 buckets do not have unauthorized configuration changes, a solutions architect should turn on AWS Config with the appropriate rules. AWS Config enables continuous monitoring and recording of AWS resource configurations, including S3 buckets. By turning on AWS Config with the appropriate rules, the solutions architect can be notified of any unauthorized changes made to the S3 bucket configurations, allowing for prompt corrective action. Options B, C, and D are not directly related to monitoring and preventing unauthorized configuration changes to Amazon S3 buckets.","poster":"PhucVuu","upvote_count":"1","timestamp":"1682122200.0","comment_id":"876934"},{"poster":"channn","comment_id":"858616","content":"Selected Answer: A\nKey words:configuration changes","upvote_count":"1","timestamp":"1680421980.0"},{"timestamp":"1680261480.0","comment_id":"856963","upvote_count":"1","content":"Selected Answer: A\nOption A is the correct solution. AWS Config is a service that allows you to monitor and record changes to your AWS resources over time. You can use AWS Config to track changes to Amazon S3 buckets and their configuration settings, and set up rules to identify any unauthorized configuration changes. AWS Config can also send notifications through Amazon SNS to alert you when these changes occur.","poster":"linux_admin"},{"poster":"al64","upvote_count":"1","comment_id":"804911","timestamp":"1676080020.0","content":"Selected Answer: A\naws: A - aws config"},{"upvote_count":"1","poster":"Khushna","comment_id":"792601","timestamp":"1675070880.0","content":"AAAAaaaaaaaaaaaaaaaaa"},{"timestamp":"1673039100.0","upvote_count":"1","comment_id":"768083","poster":"SilentMilli","content":"Selected Answer: A\no ensure that Amazon S3 buckets do not have unauthorized configuration changes, a solutions architect should turn on AWS Config with the appropriate rules.\n\nAWS Config is a service that provides you with a detailed view of the configuration of your AWS resources. It continuously records configuration changes to your resources and allows you to review, audit, and compare these changes over time. By turning on AWS Config and enabling the appropriate rules, you can monitor the configuration changes to your Amazon S3 buckets and receive notifications when unauthorized changes are made."},{"comment_id":"750838","content":"Selected Answer: A\nunauthorized config changes = aws config","upvote_count":"1","timestamp":"1671538560.0","poster":"pazabal"},{"comment_id":"750402","upvote_count":"1","timestamp":"1671502560.0","content":"Selected Answer: A\nThe solution that will accomplish this goal is A: Turn on AWS Config with the appropriate rules.\n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use AWS Config to monitor and record changes to the configuration of your Amazon S3 buckets. By turning on AWS Config and enabling the appropriate rules, you can ensure that your S3 buckets do not have unauthorized configuration changes.","poster":"Buruguduystunstugudunstuy","comments":[{"upvote_count":"1","comment_id":"750403","content":"AWS Trusted Advisor (Option B) is a service that provides best practice recommendations for your AWS resources, but it does not monitor or record changes to the configuration of your S3 buckets.\n\nAmazon Inspector (Option C) is a service that helps you assess the security and compliance of your applications. While it can be used to assess the security of your S3 buckets, it does not monitor or record changes to the configuration of your S3 buckets.\n\nAmazon S3 server access logging (Option D) enables you to log requests made to your S3 bucket. While it can help you identify changes to your S3 bucket, it does not monitor or record changes to the configuration of your S3 bucket.","timestamp":"1671502560.0","poster":"Buruguduystunstugudunstuy"}]},{"upvote_count":"2","poster":"memiy12","timestamp":"1670318340.0","comment_id":"736672","content":"Selected Answer: A\nAWS Config"},{"content":"Selected Answer: A\nAWS config will monitor config changes","comment_id":"736584","timestamp":"1670309640.0","upvote_count":"1","poster":"AlaN652"},{"poster":"Wpcorgan","timestamp":"1669035360.0","comment_id":"723499","content":"A is correct","upvote_count":"1"},{"comment_id":"723206","content":"Selected Answer: A\nAWS config allows scrutiny of past chnages","upvote_count":"2","timestamp":"1669011780.0","poster":"ABCMail"},{"upvote_count":"4","content":"Selected Answer: A\nAWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance","timestamp":"1668326340.0","poster":"grzeev","comment_id":"717163"},{"comment_id":"712961","poster":"pspinelli19","content":"Selected Answer: A\nWith Config you can limit changes to your entire account/s.\nhttps://www.examtopics.com/discussions/amazon/view/27941-exam-aws-certified-solutions-architect-associate-saa-c02/","timestamp":"1667812560.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"709578","timestamp":"1667356980.0","content":"Answer is A. Trusted Advisor gives you a general check of your system and identifies ways to optimize your infrastructue and improve it. \nWhile AWS config is more about specific resource. Like stated ( S3 bucket). Config lets you select particular resource you want to evaluate .","poster":"Solarch"},{"upvote_count":"2","comment_id":"708051","content":"A is the right answer. The key word in the question is \"Review\" Hence. AWS config use case here, \"Evaluate resource configurations for potential vulnerabilities, and review your configuration history after potential incidents to examine your security posture.\"\n\nThough Trusted advisor is similar but what it does is that, it provides important \"recommendations\" to optimize your cloud deployments, improve resilience, and address security gaps.\n\nThe keyword for Trusted advisor is Recommendation.","timestamp":"1667165040.0","poster":"keezbadger"},{"upvote_count":"2","content":"Selected Answer: A\nA - according to the picture in the documentation... \"AWS Config automatically evaluates the recorded configuration against the configuration that you specify.\"\nhttps://d1.awsstatic.com/config-diagram-092122.974fe2a4cb6aae1fe564fdbbe30ab55841a9858e.png","timestamp":"1665932880.0","comment_id":"696319","poster":"123jhl0"},{"upvote_count":"1","timestamp":"1665803820.0","comment_id":"695147","content":"Selected Answer: A\nConfig - With AWS Config, you can dive deep into how your bucket was configured at any point in time. Additionally, Config rules enable you to check whether your S3 buckets have logging and versioning enabled\nhttps://aws.amazon.com/about-aws/whats-new/2016/10/record-and-govern-s3-bucket-configurations-with-aws-config/\n\nS3 only permissions check is done by Trust advisor - apart from other checks root MFA, SG open ports, RDS Public Snapshots , EBS Public Snapshots , IAM User - one min, Service limits","poster":"KVK16"},{"poster":"Sinaneos","content":"Selected Answer: A\nConfig is better to PREVENT changes, Trusted advisor would review breaches that have already happened","timestamp":"1665650460.0","comment_id":"693747","upvote_count":"2"},{"timestamp":"1665554580.0","upvote_count":"3","poster":"masetromain","content":"Selected Answer: A\nAnswere A : https://youtu.be/MJDuAvNEv64","comment_id":"692726"},{"timestamp":"1665551640.0","poster":"BoboChow","content":"Selected Answer: A\nAWS Trusted Advisor is just to analyze your AWS accounts and providers reconmendation.","upvote_count":"2","comments":[{"content":"Inspector is to analyze the running OS against known vulnerabilities","upvote_count":"2","poster":"BoboChow","comment_id":"692664","timestamp":"1665551640.0"}],"comment_id":"692663"},{"poster":"tuloveu","content":"Selected Answer: B\nTrusted Advisor good option.","comment_id":"690606","timestamp":"1665362040.0","comments":[{"poster":"17Master","upvote_count":"1","comment_id":"708074","timestamp":"1667169480.0","content":"why? - D is correct"},{"timestamp":"1695739680.0","upvote_count":"1","poster":"David_Ang","content":"bro \"B\" is not correct because trusted advisor is not going to do anything to help you guaranty that nobody is going to change something even inspector could do more telling you who change something.","comment_id":"1017886"}],"upvote_count":"2"}],"question_images":[],"isMC":true,"choices":{"B":"Turn on AWS Trusted Advisor with the appropriate checks.","D":"Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events).","C":"Turn on Amazon Inspector with the appropriate assessment template.","A":"Turn on AWS Config with the appropriate rules."},"topic":"1","answer_images":[],"answer_description":"","exam_id":31,"question_id":199,"answers_community":["A (98%)","2%"],"question_text":"A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes.\nWhat should a solutions architect do to accomplish this goal?","answer_ET":"A"},{"id":"1eylgDtuXEsu7mFizLGk","answer_ET":"D","answers_community":["D (90%)","10%"],"answer_images":[],"exam_id":31,"topic":"1","timestamp":"2023-01-14 22:28:00","question_images":[],"answer_description":"","isMC":true,"discussion":[{"timestamp":"1673800620.0","comment_id":"776814","poster":"mhmt4438","content":"Selected Answer: D\nD. Join the file system to the Active Directory to restrict access.\n\nJoining the FSx for Windows File Server file system to the on-premises Active Directory will allow the company to use the existing Active Directory groups to restrict access to the file shares, folders, and files after the move to AWS. This option allows the company to continue using their existing access controls and management structure, making the transition to AWS more seamless.","upvote_count":"21"},{"poster":"cookieMr","timestamp":"1688023020.0","comment_id":"937732","content":"Selected Answer: D\nD. allows the file system to leverage the existing AD infrastructure for authentication and access control.\n\nOption A is incorrect because mapping the AD groups to IAM groups is not applicable in this scenario. IAM is primarily used for managing access to AWS resources, while the requirement is to integrate with the on-premises AD for access control.\n\nOption B is incorrect because assigning a tag with a Restrict tag key and a Compliance tag value does not provide the necessary integration with the on-premises AD for access control. Tags are used for organizing and categorizing resources and do not provide authentication or access control mechanisms.\n\nOption C is incorrect because creating an IAM service-linked role linked directly to FSx for Windows File Server does not integrate with the on-premises AD. IAM roles are used within AWS for managing permissions and do not provide the necessary integration with external AD systems.","upvote_count":"8"},{"poster":"zdi561","upvote_count":"1","timestamp":"1736731980.0","comment_id":"1339764","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/storage/using-amazon-fsx-for-windows-file-server-with-an-on-premises-active-directory/#:~:text=Perform%20the%20following%20steps:,shown%20in%20the%20following%20screenshot)."},{"comment_id":"1223548","content":"Selected Answer: D\nWhen you create a file system with Amazon FSx, you join it to your Active Directory domain to provide user authentication and file- and folder-level access control.","upvote_count":"4","poster":"Lin878","timestamp":"1717413300.0"},{"poster":"bujuman","upvote_count":"3","content":"Selected Answer: D\nD is relevent and accurate answer when we consider this:\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/creating-joined-ad-file-systems.html\n\n\"When you create a new FSx for Windows File Server file system, you can configure Microsoft Active Directory integration so that it joins to your self-managed Microsoft Active Directory domain. To do this, provide the following information for your Microsoft Active Directory\"","comment_id":"1156449","timestamp":"1708613700.0"},{"upvote_count":"1","comment_id":"1125282","content":"Selected Answer: A\nThe on-premise AD already has restrictions via group in place so D makes no sense as the groups are already linked to file system.\n\"The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS.\"\n\nThe question is about linking the on-prem permissions to the new FSx server on AWS and this can only be done by A","poster":"awsgeek75","comments":[{"upvote_count":"1","comment_id":"1125286","content":"Actually neither A nor D make sense.\n\n\"A self-managed on-premises Active Directory controls access to the files and folders.\" This makes D sound useless and at the same time does not allow the on-prem AD to control file access on FSx.\n\nA uses IAM roles which is irrelevant to this setup. \nBC are totally wrong","poster":"awsgeek75","timestamp":"1705523160.0"}],"timestamp":"1705522860.0"},{"poster":"meowruki","upvote_count":"4","content":"Selected Answer: D\nOption A: Creating an Active Directory Connector and mapping groups to IAM groups is more relevant for AWS Directory Service, such as AWS Managed Microsoft AD, and not for integrating with existing on-premises Active Directory.\n\n Option B: Using tags is typically not used for access control purposes. Tags are metadata and are not directly involved in user authentication and authorization.\n\n Option C: Creating an IAM service-linked role directly linked to FSx for Windows File Server is not the standard approach for integrating with existing on-premises Active Directory.","comment_id":"1083926","timestamp":"1701311160.0"},{"timestamp":"1696486080.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD.html","comment_id":"1025311","upvote_count":"3","poster":"wrmari"},{"content":"Selected Answer: D\nThis allows the on-premises Active Directory to manage permissions to the FSx file shares, meeting the key requirement to use existing AD groups to control access after migrating to AWS.\n\nJoining FSx to the AD domain allows the native file system permissions, users, and groups to be applied from Active Directory. Access is handled seamlessly via the trust relationship between FSx and AD.\n\nThe other options would not leverage the existing AD identities and groups","upvote_count":"4","timestamp":"1694432100.0","comment_id":"1004700","poster":"Guru4Cloud","comments":[{"content":"The other options would not leverage the existing AD identities and groups:\n\nA) AD Connector and IAM groups would require re-mapping AD groups to IAM, adding complexity. Native AD integration is simpler.\n\nB) Tags and IAM groups also don't use native AD semantics.\n\nC) Service-linked roles are not applicable for managing end user access.\n\nSo D is the correct option to meet the requirements using the native Active Directory integration built into FSx for Windows.","timestamp":"1694432100.0","comment_id":"1004703","upvote_count":"3","poster":"Guru4Cloud"}]},{"timestamp":"1691069640.0","comment_id":"971151","upvote_count":"2","poster":"mtmayer","content":"Selected Answer: A\nThe AD is on-premisses... Your need the connector."},{"poster":"Mia2009687","upvote_count":"2","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/aws-ad-integration-fsxW.html","timestamp":"1688018220.0","comment_id":"937587"},{"comment_id":"857209","timestamp":"1680273540.0","upvote_count":"3","content":"Selected Answer: D\nOther options are referring to IAM based control which is not possible. Existing AD should be used without IAM.","poster":"kraken21"},{"comment_id":"844603","timestamp":"1679295840.0","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/storage/using-amazon-fsx-for-windows-file-server-with-an-on-premises-active-directory/","poster":"Abhineet9148232","upvote_count":"3"},{"upvote_count":"3","poster":"somsundar","timestamp":"1678691700.0","comment_id":"837710","content":"Answer D. Amazon FSx does not support Active Directory Connector .","comments":[{"poster":"JA2018","timestamp":"1732023960.0","content":"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/aws-ad-integration-fsxW.html","upvote_count":"1","comment_id":"1314669"}]},{"poster":"Abhineet9148232","comment_id":"832941","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD.html","timestamp":"1678278180.0","upvote_count":"4"},{"timestamp":"1676449260.0","comment_id":"809279","content":"Selected Answer: D\nNote:\nAmazon FSx does not support Active Directory Connector and Simple Active Directory. \n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/aws-ad-integration-fsxW.html","upvote_count":"4","poster":"Yelizaveta"},{"comments":[{"comment_id":"1314670","poster":"JA2018","upvote_count":"1","timestamp":"1732024020.0","content":"from: https://docs.aws.amazon.com/fsx/latest/WindowsGuide/aws-ad-integration-fsxW.html\n\nAmazon FSx does not support Active Directory Connector and Simple Active Directory."}],"content":"Selected Answer: A\nThe answer will be AD connector so : A, it will create a proxy between your onpremises AD which you can use to restrict access","poster":"aakashkumar1999","timestamp":"1675521780.0","upvote_count":"2","comment_id":"798050"},{"content":"Selected Answer: D\nOption D: Join the file system to the Active Directory to restrict access.\n\nJoining the FSx for Windows File Server file system to the on-premises Active Directory allows the company to use the existing Active Directory groups to restrict access to the file shares, folders, and files after the move to AWS. By joining the file system to the Active Directory, the company can maintain the same access control as before the move, ensuring that the compliance team can maintain compliance with the relevant regulations and standards.\n\nOptions A and B involve creating an Active Directory Connector or assigning a tag to map the Active Directory groups to IAM groups, but these options do not allow for the use of the existing Active Directory groups to restrict access to the file shares in AWS.\n\nOption C involves creating an IAM service-linked role linked directly to FSx for Windows File Server to restrict access, but this option does not take advantage of the existing on-premises Active Directory and its access control.","timestamp":"1675034940.0","poster":"Stanislav4907","upvote_count":"4","comment_id":"792158"},{"comment_id":"789593","timestamp":"1674823260.0","poster":"KAUS2","upvote_count":"3","content":"Selected Answer: A\nA is correct \nUse AD Connector if you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. You can also use AD Connector to join Amazon EC2 instances to your existing Active Directory domain.\nPls refer - https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html#adconnector","comments":[{"upvote_count":"2","comment_id":"919585","poster":"mbuck2023","timestamp":"1686337140.0","content":"wrong, answer is D. Amazon FSx does not support Active Directory Connector and Simple Active Directory. See also https://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD.html."}]},{"comment_id":"779402","upvote_count":"2","timestamp":"1673999580.0","content":"Going with D here","poster":"techhb"},{"upvote_count":"5","poster":"Aninina","comment_id":"775947","content":"Selected Answer: D\nD. Join the file system to the Active Directory to restrict access.\n\nThe best way to restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS is to join the file system to the on-premises Active Directory. This will allow the company to continue using the Active Directory groups to restrict access to the files and folders, without the need to create additional IAM groups or roles.\n\nBy joining the file system to the Active Directory, the company can continue to use the same access control mechanisms it already has in place and the security configuration will not change.\n\nOption A and B are not applicable to FSx for Windows File Server because it doesn't support the use of IAM groups or tags to restrict access.\n\nOption C is not appropriate in this case because FSx for Windows File Server does not support using IAM service-linked roles to restrict access.","timestamp":"1673731680.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/95343-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":200,"answer":"D","unix_timestamp":1673731680,"choices":{"A":"Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.","B":"Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.","C":"Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.","D":"Join the file system to the Active Directory to restrict access."},"question_text":"A companyâ€™s compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders.\n\nThe company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system.\n\nWhich solution will meet these requirements?"}],"exam":{"id":31,"provider":"Amazon","name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true,"numberOfQuestions":1019},"currentPage":40},"__N_SSP":true}