{"pageProps":{"questions":[{"id":"8oP1mdkW32at8eEr27TF","answer_description":"","discussion":[{"comment_id":"1335393","content":"Selected Answer: A\nA - Kinesis Data Streams in on-demand mode automatically scales based on traffic, making it an ideal choice for fluctuating traffic patterns.\nB - Kinesis Data Firehose is used for NEAR real-time data delivery. Also, AWS Glue is better suited for ETL and batch processing rather than real-time analysis.\nC - Kinesis Video Streams is designed for video or audio streaming data, not clickstream data.\nD - You can process data, but can't capture raw data first.\n\nAlso, AWS Lambda provides real-time data processing without the need for managing infrastructure.","timestamp":"1735803840.0","poster":"LeonSauveterre","upvote_count":"1"},{"upvote_count":"2","poster":"mk168898","content":"click stream => kinesis data streams","timestamp":"1729657620.0","comment_id":"1301870"},{"content":"B - C are out = Glue doesn't support real-time data processing.\nD - Why would you use Kinesis data ANALYTICS to ingest clickstream data instead of Amazon Kinesis DATA STREAM?","upvote_count":"2","comments":[{"timestamp":"1736110920.0","comment_id":"1336881","content":"https://docs.aws.amazon.com/glue/latest/dg/streaming-chapter.html","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"1368541","timestamp":"1741509600.0","poster":"tch","content":"this is AWS Glue Streaming... not AWS Glue"}],"poster":"Salilgen"}],"poster":"MatAlves","comment_id":"1287206","timestamp":"1726905000.0"},{"upvote_count":"3","content":"Selected Answer: A\nThis one is very tricky, need to read the context carefully:\n\"The company needs a scalable solution that can adapt to varying levels of traffic\" -> Both Firehouse and Stream are scalable. But, Firehose is automatic where Stream is not. However, the question does NOT say it should be automatic and Glue is not support real-time analysis.\nThats why go for A.\nB is very close to.","timestamp":"1718515860.0","poster":"EdricHoang","comment_id":"1231212"},{"upvote_count":"2","comment_id":"1220774","poster":"Scheldon","timestamp":"1716973320.0","content":"Selected Answer: A\nAnswerA\n\nApache Flink (previously known as Amazon Kinesis Data Analytics) seems to not allowing sent data directly to Lambda...\nGlue is allowing to integrate data from couple of sources in to one.\nHence I think A is correct answer\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\nhttps://aws.amazon.com/kinesis/data-streams/features/?nc=sn&loc=2"},{"poster":"f07ed8f","timestamp":"1716715320.0","upvote_count":"2","content":"Selected Answer: A\nSeem AWS Glue does not support process data in real time. I vote for A","comment_id":"1218873"},{"timestamp":"1716541680.0","poster":"f07ed8f","comment_id":"1217330","upvote_count":"1","content":"Selected Answer: B\nBoth Kinesis Data Streams and Firehose are scalable but Firehose offers automated scaling. I vote fore B"},{"poster":"sandordini","timestamp":"1714490580.0","upvote_count":"4","comment_id":"1204640","content":"Selected Answer: A\nI think Apache Flink (previously known as Amazon Kinesis Data Analytics) would also be fine, but as here it wants to combine it with Lambda, I would rather opt for Kinesis Data Streams + Lambda, so A, because of the figure on this page:\nhttps://aws.amazon.com/kinesis/"}],"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/139803-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"D":"Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to capture the clickstream data. Use AWS Lambda to process the data in real time.","C":"Use Amazon Kinesis Video Streams to capture the clickstream data. Use AWS Glue to process the data in real time.","A":"Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture the clickstream data. Use AWS Lambda to process the data in real time.","B":"Use Amazon Kinesis Data Firehose to capture the clickstream data. Use AWS Glue to process the data in real time."},"timestamp":"2024-04-30 17:23:00","isMC":true,"answers_community":["A (92%)","8%"],"exam_id":31,"answer":"A","topic":"1","question_images":[],"question_id":896,"answer_images":[],"question_text":"An ecommerce company wants to collect user clickstream data from the company's website for real-time analysis. The website experiences fluctuating traffic patterns throughout the day. The company needs a scalable solution that can adapt to varying levels of traffic.\n\nWhich solution will meet these requirements?","unix_timestamp":1714490580},{"id":"WIvA7sudpsyhsnqWfLLn","exam_id":31,"answer_images":[],"answers_community":["B (100%)"],"question_images":[],"timestamp":"2024-04-30 17:31:00","topic":"1","answer_ET":"B","isMC":true,"question_text":"A global company runs its workloads on AWS. The company's application uses Amazon S3 buckets across AWS Regions for sensitive data storage and analysis. The company stores millions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that are not versioning-enabled.\n\nWhich solution will meet these requirements?","discussion":[{"content":"sotrage lens used to identify non versioning","upvote_count":"1","comment_id":"1301871","poster":"mk168898","timestamp":"1729657620.0"},{"poster":"Scheldon","content":"Selected Answer: B\nAnswerB\nS3 Sorage Lens \"can also identify buckets that aren't following data-protection best practices, such as using S3 Replication or S3 Versioning. \"\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_basics_metrics_recommendations.html","upvote_count":"3","comment_id":"1220780","timestamp":"1716974700.0"},{"content":"Selected Answer: B\nYou can use the Versioning-enabled bucket count metric to see which buckets use S3 Versioning. Then, you can take action in the S3 console to enable S3 Versioning for other buckets.","upvote_count":"2","poster":"sandordini","comment_id":"1204641","timestamp":"1714491060.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/139804-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":897,"answer":"B","unix_timestamp":1714491060,"answer_description":"","choices":{"B":"Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across Regions.","C":"Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled across Regions.","D":"Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled across Regions.","A":"Set up an AWS CloudTrail event that has a rule to identify all S3 buckets that are not versioning-enabled across Regions."}},{"id":"lgsZtX8oFIDqdSfyISgK","exam_id":31,"answer_images":[],"question_images":[],"answers_community":["A (100%)"],"timestamp":"2022-10-18 15:12:00","topic":"1","answer_ET":"A","isMC":true,"question_text":"A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according to the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution.\nWhat should a solutions architect do to secure the audit documents?","discussion":[{"poster":"123jhl0","upvote_count":"16","comment_id":"698259","timestamp":"1666098720.0","content":"Selected Answer: A\nSame as Question #44"},{"poster":"satyaammm","content":"Selected Answer: A\nUsing MFA Delete is the most suitable option here as it avoids deletion of files.","upvote_count":"1","comment_id":"1336460","timestamp":"1736011380.0"},{"poster":"jaradat02","content":"Selected Answer: A\nA is the correct answer.","upvote_count":"1","comment_id":"1252417","timestamp":"1721563020.0"},{"poster":"awsgeek75","comment_id":"1110089","content":"Selected Answer: A\nAccidental deletion is the key. Deletion is allowed but MFA deletion ensures that deletion requires an additional step. https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html","timestamp":"1703974380.0","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: A\nEnable the versioning to ensure restoration in case of accidental deletion and MFA Delete for double verification before deletion.","poster":"TariqKipkemei","timestamp":"1692679200.0","comment_id":"987071"},{"timestamp":"1691759160.0","content":"Selected Answer: A\nVersioning will keep multiple variants of an object in case one is accidentally or intentionally deleted - the previous versions can still be restored.\n\nMFA Delete requires additional authentication to permanently delete an object version. This prevents accidental deletion","upvote_count":"3","comment_id":"978638","poster":"Guru4Cloud"},{"content":"B. Enabling MFA on the IAM user credentials adds an extra layer of security to the user authentication process. However, it does not specifically address the concern of accidental deletion of documents in the S3 bucket.\n\nC. Adding an S3 Lifecycle policy to deny the delete action during audit dates would prevent intentional deletions during specific time periods. However, it does not address accidental deletions that can occur at any time.\n\nD. Using KMS for encryption and restricting access to the KMS key provides additional security for the data stored in the S3 . However, it does not directly prevent accidental deletion of documents in the S3.\n\nEnabling versioning and MFA Delete on the S3 (option A) is the most appropriate solution for securing the audit documents. Versioning ensures that multiple versions of the documents are stored, allowing for easy recovery in case of accidental deletions. Enabling MFA Delete requires the use of multi-factor authentication to authorize deletion actions, adding an extra layer of protection against unintended deletions.","timestamp":"1687369080.0","poster":"cookieMr","upvote_count":"3","comment_id":"929782"},{"content":"Selected Answer: A\nA is answer.","poster":"beginnercloud","upvote_count":"1","timestamp":"1684757460.0","comment_id":"904016"},{"comment_id":"903020","content":"Selected Answer: A\nA is answer.","upvote_count":"1","timestamp":"1684656540.0","poster":"Bmarodi"},{"poster":"Robrobtutu","upvote_count":"1","timestamp":"1681589580.0","content":"Selected Answer: A\nA is correct.","comment_id":"871261"},{"comment_id":"778114","poster":"remand","upvote_count":"2","content":"Selected Answer: A\nonly accidental deletion should be avoided. IAM policy will completely remove their access.hence, MFA is the right choice.","timestamp":"1673894580.0"},{"comments":[],"upvote_count":"2","poster":"karbob","content":"what about : IAM policies are used to specify permissions for AWS resources, and they can be used to allow or deny specific actions on those resources.\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Sid\": \"DenyDeleteObject\",\n \"Effect\": \"Deny\",\n \"Action\": \"s3:DeleteObject\",\n \"Resource\": [\n \"arn:aws:s3:::my-bucket/my-object\",\n \"arn:aws:s3:::my-bucket\"\n ]\n }\n ]\n}","comment_id":"770873","timestamp":"1673298540.0"},{"upvote_count":"4","comments":[{"upvote_count":"3","comment_id":"759132","content":"Option B: Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account, would not provide protection against accidental deletion. \n\nOption C: Adding an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates, which would not provide protection against accidental deletion outside of the specified audit dates. \n\nOption D: Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key, would not provide protection against accidental deletion.","poster":"Buruguduystunstugudunstuy","timestamp":"1672181160.0"}],"timestamp":"1672181160.0","content":"Selected Answer: A\nThe solution architect should do Option A: Enable the versioning and MFA Delete features on the S3 bucket.\n\nThis will secure the audit documents by providing an additional layer of protection against accidental deletion. With versioning enabled, any deleted or overwritten objects in the S3 bucket will be preserved as previous versions, allowing the company to recover them if needed. With MFA Delete enabled, any delete request made to the S3 bucket will require the use of an MFA code, which provides an additional layer of security.","poster":"Buruguduystunstugudunstuy","comment_id":"759131"},{"content":"Selected Answer: A\nA is the right answer","timestamp":"1671226680.0","upvote_count":"1","poster":"career360guru","comment_id":"747605"},{"upvote_count":"1","content":"A is correct","timestamp":"1669048140.0","comment_id":"723749","poster":"Wpcorgan"},{"timestamp":"1668370920.0","upvote_count":"1","comment_id":"717506","content":"Selected Answer: A\nEnable the versioning and MFA Delete features on the S3 bucket.","poster":"Jtic"}],"url":"https://www.examtopics.com/discussions/amazon/view/85808-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":898,"answer":"A","unix_timestamp":1666098720,"choices":{"B":"Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account.","C":"Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates.","D":"Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key.","A":"Enable the versioning and MFA Delete features on the S3 bucket."},"answer_description":""},{"id":"YKItmS6mLd5eHgIoZgCZ","answer_ET":"A","topic":"1","answer_description":"","isMC":true,"discussion":[{"content":"Selected Answer: A\nAlthough it's not stated what is meant by 'rarely accessed', this scenario would primarily be a candidate for the Glacier Instant Retrieval tier as the storage price would be more than 3 times lower compared to Standard IA. In the specific case of files being more frequently retrieved than quarterly, it can qualify for consideration of Standard IA. \nActually, we don't have the required info, so we have to guess what they are thinking.. which is pretty lame, to be honest..","comment_id":"1204647","timestamp":"1714492080.0","comments":[{"comment_id":"1229807","comments":[{"poster":"Dantecito","timestamp":"1740151680.0","comment_id":"1359807","upvote_count":"1","content":"The 90-day minimum storage duration means that if you delete, overwrite, or transition an object before 90 days, AWS will charge you for the remaining unused storage days.\nIt's a bit confusing but it's not like you can only have the files for 90 days, all glaciers are intended to retain data for archival purposes and 4 years as the question says is archival."},{"content":"No, it isn't. It is just the opposite. \n\nObjects archived to S3 Glacier Flexible Retrieval have a minimum of 90 days of storage. If an object is deleted, overwritten, or transitioned before 90 days, a pro-rated charge equal to the storage charge for the remaining days will be incurred.\n\nhttps://aws.amazon.com/s3/faqs/?nc=sn&loc=7","poster":"Edwars","timestamp":"1719929700.0","upvote_count":"3","comment_id":"1240836"},{"upvote_count":"2","poster":"MatAlves","timestamp":"1726905420.0","content":"Hell no.\n\nS3 Glacier Instant Retrieval is designed for long-lived, rarely accessed data that is retained for months or years. Objects that are archived to S3 Glacier Instant Retrieval have a minimum of 90 days of storage","comment_id":"1287215"}],"poster":"sheilawu","upvote_count":"2","timestamp":"1718283180.0","content":"S3 Glacier Instant Retrieval only retain the file for maximun 90 days, that is why A is not correct answer.\nSo C is right answer."}],"poster":"sandordini","upvote_count":"13"},{"content":"Selected Answer: A\nB is out!\nstore the files for 4 years before the files can be deleted - for long term we should use S3 Glacier Instant Retrieval.\n\nfiles are frequently accessed in the first 30 days of object creation, it actually store in Amazon S3 Standard storage","upvote_count":"1","comment_id":"1366453","poster":"tch","timestamp":"1741397700.0","comments":[{"content":"Each file is approximately 5 MB and is stored in Amazon S3 Standard storage, meaning when file created .. files already store in Amazon S3 Standard storage.. move to move the files to S3 Glacier Instant Retrieval 30 days after object creation is fine...","upvote_count":"1","comment_id":"1366454","poster":"tch","timestamp":"1741397880.0"}]},{"upvote_count":"1","comment_id":"1356995","timestamp":"1739646420.0","content":"Selected Answer: C\nOption C is correct instead of A because S3 Standard-IA provides lower long-term storage costs with immediate access, whereas S3 Glacier Instant Retrieval has higher retrieval costs, making it less cost-effective for files that may still need to be accessed occasionally.","poster":"a8a1e0e"},{"comment_id":"1344133","timestamp":"1737461700.0","poster":"dfgdsfgfdgreg","content":"Selected Answer: A\nGlacier is much cheaper for storage than standard. The catch is you have to pay for 90 days of storage no matter what, even if the object is deleted. Here we will store data for 4 years so glacier is the cheapest solution.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1735806360.0","content":"Selected Answer: C\nA - S3 Glacier Instant Retrieval is designed for archival storage where objects are infrequently accessed but still require millisecond retrieval, so yes actually A will work. It's just that Glacier storage costs more than S3 Standard-IA (given that after 30 days, the files are only RARELY accessed), so this is not the most cost-effective option.\nB - For files that \"cannot be recreated,\" durability is crucial, so single Availability Zone is not OK.\nC - S3 Standard-IA is optimized for objects accessed less frequently but still requires rapid retrieval and high durability.\nD - After 4 years, let's just delete those files.","comment_id":"1335402","poster":"LeonSauveterre","comments":[{"comment_id":"1335404","upvote_count":"1","content":"Just a thought, if not constrained to these option, I would simply use Intelligent-Tiering to automatically move the files between access tiers (frequent access, infrequent access, etc) based on actual usage. Eventually, I'd delete these files after 4 years as well. I think this approach is better.","timestamp":"1735806540.0","poster":"LeonSauveterre"}]},{"poster":"MatAlves","upvote_count":"3","comment_id":"1287217","content":"Selected Answer: A\nInstant Retrieval = immediately accessible.\n\nCoose the S3 Glacier Instant Retrieval storage class when you need milliseconds access to low cost archive data.\n\nhttps://aws.amazon.com/s3/faqs/?nc=sn&loc=7","timestamp":"1726905540.0"},{"content":"Selected Answer: A\nGlacier Instant Retrieval is much cheaper, and it is intended for archival storage with very low access patterns","upvote_count":"3","poster":"scaredSquirrel","comment_id":"1281565","timestamp":"1725974460.0"},{"content":"Selected Answer: C\nWhile S3 Glacier Instant Retrieval offers immediate access, it has a minimum storage duration policy, Objects stored in S3 Glacier Instant Retrieval have a minimum storage duration of 90 days.","upvote_count":"2","poster":"Ucy","timestamp":"1722289620.0","comment_id":"1257713"},{"upvote_count":"4","poster":"FrozenCarrot","content":"Selected Answer: A\nS3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes.\n --- https://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/","comment_id":"1245219","timestamp":"1720574160.0"},{"comments":[{"timestamp":"1723804140.0","upvote_count":"2","comment_id":"1266969","content":"files can not be recreated -> OneZone IA not accepted","poster":"Johnoppong101"}],"comment_id":"1243726","poster":"EdricHoang","timestamp":"1720334460.0","upvote_count":"1","content":"Selected Answer: B\ncost effective -> OneZone IA\n\"The files must be immediately accessible\" -> cannot be Glacier"},{"upvote_count":"1","poster":"Mayank0502","timestamp":"1720295040.0","comment_id":"1243525","content":"Selected Answer: C\nthis option has most durability"},{"poster":"Lin878","comment_id":"1241437","content":"Selected Answer: C\n\"are rarely accessed after the first 30 days\" - not often \nI will go with \"C\".","timestamp":"1720013940.0","upvote_count":"1"},{"poster":"345645a","upvote_count":"2","comment_id":"1235864","content":"https://aws.amazon.com/es/s3/storage-classes/glacier/instant-retrieval/","timestamp":"1719155460.0"},{"poster":"sheilawu","comment_id":"1225926","timestamp":"1717740480.0","content":"Selected Answer: C\nmmediately accessible =>C","upvote_count":"1"},{"poster":"Scheldon","upvote_count":"4","content":"Selected Answer: C\nAnswerC\nWe cannot choose B because if that one zone will fail, company will not be able to recreate them.\nWe cannot choose D because we do not have to store files after 4y hence we can delete them (cost savings)\nWe cannot choose A - Glacier is less expensive (0,004 per GB) then S3-Standard - IA but is not allowing for instant access which is one of requirements (there is no information that data access shouldn't be accessible immedietly) we have only information that after 30d access to data is less frequently. Hence I think we need to choose S3 Standard - IA (answer C)","comment_id":"1220788","timestamp":"1716975840.0"},{"poster":"bujuman","comment_id":"1219375","timestamp":"1716794820.0","upvote_count":"2","content":"Selected Answer: C\nRequirements:\n- frequently accessed for 30 days \n- lower cost\n\nFeatures for S3 Standard-IA:\n- Infrequently accessed objects\n- Milliseconds to acces\n\nAccording to me, best option for this use case is C\nNB: Glacier better suits for lower cost, infrequent access."},{"comment_id":"1212509","upvote_count":"1","poster":"th1002","timestamp":"1715877900.0","content":"Selected Answer: C\nwhy do we need one zone, glacier instant for 30 days ? or why do we need to move to glacier after 4 years ?\nI think C is correct"},{"poster":"Karls","content":"B. Create an S3 Lifecycle policy to move the files to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days after object creation. Delete the files 4 years after object creation.\n\nThis option leverages S3 One Zone-IA, which offers a lower cost compared to S3 Standard-IA, while ensuring that files are immediately accessible during the first 30 days of their creation. Then, after this period, the files are moved to S3 One Zone-IA for less frequent access storage, further reducing costs. Finally, the files are deleted after 4 years, meeting the requirement for long-term retention.","comment_id":"1204935","upvote_count":"2","timestamp":"1714549860.0"}],"exam_id":31,"question_id":899,"url":"https://www.examtopics.com/discussions/amazon/view/139805-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after object creation. Delete the files 4 years after object creation.","D":"Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after object creation. Move the files to S3 Glacier Flexible Retrieval 4 years after object creation.","B":"Create an S3 Lifecycle policy to move the files to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days after object creation. Delete the files 4 years after object creation.","A":"Create an S3 Lifecycle policy to move the files to S3 Glacier Instant Retrieval 30 days after object creation. Delete the files 4 years after object creation."},"question_text":"A company needs to optimize its Amazon S3 storage costs for an application that generates many files that cannot be recreated. Each file is approximately 5 MB and is stored in Amazon S3 Standard storage.\n\nThe company must store the files for 4 years before the files can be deleted. The files must be immediately accessible. The files are frequently accessed in the first 30 days of object creation, but they are rarely accessed after the first 30 days.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_images":[],"question_images":[],"unix_timestamp":1714492080,"answers_community":["A (63%)","C (35%)","3%"],"timestamp":"2024-04-30 17:48:00","answer":"A"},{"id":"UoJ7JDpNGGg6JbuVZ3pD","topic":"1","choices":{"C":"Send user data to the regional S3 endpoints closest to the user. Configure an S3 cross-account replication rule to keep the S3 buckets synchronized.","D":"Set up Amazon S3 to use Multi-Region Access Points in an active-active configuration with a single global endpoint. Configure S3 Cross-Region Replication.","A":"Implement an active-active design between the two Regions. Configure the application to use the regional S3 endpoints closest to the user.","B":"Use an active-passive configuration with S3 Multi-Region Access Points. Create a global endpoint for each of the Regions."},"answer":"D","timestamp":"2024-04-29 19:37:00","discussion":[{"content":"Selected Answer: D\nUsing a Multi-region Accesspoint in an Active-Active setup will send data to the closest Region, without accessing the internet: \"send remote user data to the nearest S3 bucket with no public network congestion\"\nNot very easy to read and understand but it's all there: https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html","comment_id":"1204651","timestamp":"1714492740.0","upvote_count":"15","poster":"sandordini"},{"poster":"LeonSauveterre","comments":[{"poster":"LeonSauveterre","comment_id":"1335411","upvote_count":"1","content":"And yes, the single global endpoint in option D will automatically route traffic to the nearest bucket, which ensures low-latency, which is a plus.","timestamp":"1735807320.0"}],"comment_id":"1335408","timestamp":"1735807140.0","upvote_count":"1","content":"Selected Answer: D\nA - The \"implementation\" is so much more complicated than you think.\nB - Active-passive? Then you gotta promote the passive region to active manually in case of failure.\nC - This option does nothing to achieve low-latency or failover requirements.\nD - It simplifies configuration and management with a single global endpoint, and provides seamless failover and high availability through Multi-Region Access Points and Cross-Region Replication."},{"content":"Selected Answer: D\nD. Set up Amazon S3 to use Multi-Region Access Points in an active-active configuration with a single global endpoint. Configure S3 Cross-Region Replication.","comment_id":"1249089","poster":"muhammadahmer36","upvote_count":"2","timestamp":"1721147640.0"},{"upvote_count":"3","content":"D is correct","poster":"ike001","timestamp":"1718703540.0","comment_id":"1232354"},{"poster":"Scheldon","timestamp":"1716547680.0","upvote_count":"3","content":"Selected Answer: D\nAnswer D\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html\n\nWhen you create a Multi-Region Access Point, you specify a set of AWS Regions where you want to store data to be served through that Multi-Region Access Point. You can use S3 Cross-Region Replication (CRR) to synchronize data among buckets in those Regions. You can then request or write data through the Multi-Region Access Point global endpoint. Amazon S3 automatically serves requests to the replicated dataset from the closest available Region. Multi-Region Access Points are also compatible with applications that are running in Amazon virtual private clouds (VPCs), including those that are using AWS PrivateLink for Amazon S3.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html","comment_id":"1217384"},{"comments":[{"poster":"trongod05","comment_id":"1294299","timestamp":"1728308280.0","content":"It's D. In an active-active configuration, requests made to an S3 Multi-Region Access Pointâ€™s global endpoint automatically route over the AWS global network to the nearest S3 bucket. This allows applications to automatically avoid congested network segments on the public internet, improving application performance and reliability.","upvote_count":"2"}],"comment_id":"1204152","poster":"1223d0e","timestamp":"1714412220.0","upvote_count":"1","content":"To me it looks like C, the requirement is to send the request to the closest region"}],"question_images":[],"question_text":"A company runs its critical storage application in the AWS Cloud. The application uses Amazon S3 in two AWS Regions. The company wants the application to send remote user data to the nearest S3 bucket with no public network congestion. The company also wants the application to fail over with the least amount of management of Amazon S3.\n\nWhich solution will meet these requirements?","answers_community":["D (100%)"],"isMC":true,"answer_images":[],"exam_id":31,"question_id":900,"answer_ET":"D","answer_description":"","unix_timestamp":1714412220,"url":"https://www.examtopics.com/discussions/amazon/view/139744-exam-aws-certified-solutions-architect-associate-saa-c03/"}],"exam":{"id":31,"isMCOnly":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"isBeta":false,"numberOfQuestions":1019},"currentPage":180},"__N_SSP":true}