{"pageProps":{"questions":[{"id":"nShvNmt3hNNqUg62Hgt2","answer_ET":"D","exam_id":20,"answers_community":["D (67%)","B (33%)"],"question_text":"A medical company has a system with sensor devices that read metrics and send them in real time to an Amazon Kinesis data stream. The Kinesis data stream has multiple shards. The company needs to calculate the average value of a numeric metric every second and set an alarm for whenever the value is above one threshold or below another threshold. The alarm must be sent to Amazon Simple Notification Service (Amazon SNS) in less than 30 seconds.\nWhich architecture meets these requirements?","choices":{"D":"Use an Amazon Kinesis Data Analytics application to read from the Kinesis data stream and calculate the average per second. Send the results to an AWS Lambda function that sends the alarm to Amazon SNS.","C":"Use an Amazon Kinesis Data Firehose deliver stream to read the data from the Kinesis data stream and store it on Amazon S3. Have Amazon S3 trigger an AWS Lambda function that calculates the average per second and sends the alarm to Amazon SNS.","A":"Use an Amazon Kinesis Data Firehose delivery stream to read the data from the Kinesis data stream with an AWS Lambda transformation function that calculates the average per second and sends the alarm to Amazon SNS.","B":"Use an AWS Lambda function to read from the Kinesis data stream to calculate the average per second and sent the alarm to Amazon SNS."},"timestamp":"2021-05-03 18:04:00","discussion":[{"upvote_count":"31","comment_id":"350040","comments":[{"comment_id":"506014","content":"Agree\nAlso it can not be B because I think average needs to be calculated across different shards and Lambda processes one batch of records at a time from each shard.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","poster":"lakediver","upvote_count":"8","timestamp":"1640081640.0"}],"poster":"AjithkumarSL","timestamp":"1632215340.0","content":"It should be D, as KDF cannot achieve 30 Seconds SLA"},{"comment_id":"348789","timestamp":"1632204360.0","content":"Should be 'D'.","poster":"VikG12","upvote_count":"7"},{"timestamp":"1687089960.0","upvote_count":"3","comment_id":"926668","content":"Selected Answer: B\nThis question could be deprecated. Legacy KDA for SQL could invoke Lambda, not KDA for Flink . So, D is not feasible today. KDS to Lambda has always been possible and the batch window in the event source mapping can be set to 1 second.","poster":"cox1960"},{"comment_id":"798067","content":"Selected Answer: D\nFirehose is too slow, D it is.","upvote_count":"2","poster":"milofficial","timestamp":"1675522320.0"},{"comment_id":"705496","content":"D is the correct answer\n\nOptions A & C is wrong as Kinesis Data Firehose would not be able to achieve the 30 seconds requirement. \n\nOption B is wrong as Lambda function works with a single shard and works on a batch or batch window which would not provide the average.","upvote_count":"6","poster":"cloudlearnerhere","comments":[{"timestamp":"1666871280.0","poster":"cloudlearnerhere","content":"Continuous metric generation applications enable you to monitor and understand how your data is trending over time. Your applications can aggregate streaming data into critical information and seamlessly integrate it with reporting databases and monitoring services to serve your applications and users in real-time. With Kinesis Data Analytics, you can use SQL or Apache Flink code to continuously generate time-series analytics over time windows. For example, you can build a live leaderboard for a mobile game by computing the top players every minute and then sending it to Amazon DynamoDB. Or, you can track the traffic to your website by calculating the number of unique website visitors every five minutes and then sending the processed results to Amazon Redshift.","upvote_count":"3","comment_id":"705497","comments":[{"comment_id":"705498","timestamp":"1666871280.0","content":"Continuous metric generation applications enable you to monitor and understand how your data is trending over time. Your applications can aggregate streaming data into critical information and seamlessly integrate it with reporting databases and monitoring services to serve your applications and users in real-time. With Kinesis Data Analytics, you can use SQL or Apache Flink code to continuously generate time-series analytics over time windows. For example, you can build a live leaderboard for a mobile game by computing the top players every minute and then sending it to Amazon DynamoDB. Or, you can track the traffic to your website by calculating the number of unique website visitors every five minutes and then sending the processed results to Amazon Redshift.","upvote_count":"1","poster":"cloudlearnerhere"}]}],"timestamp":"1666871280.0"},{"poster":"maitis","content":"Selected Answer: D\nLambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch. Each batch contains records from a single shard/data stream","timestamp":"1659584340.0","upvote_count":"2","comment_id":"642160"},{"upvote_count":"1","comment_id":"637131","poster":"rocky48","timestamp":"1658807520.0","content":"Selected Answer: D\nSelected Answer: D"},{"upvote_count":"1","comment_id":"623989","timestamp":"1656422760.0","poster":"Ramshizzle","content":"Selected Answer: D\nAs others have explained it should be D."},{"content":"Ans: D\nB is wrong because Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch. Each batch contains records from a single shard/data stream.","comment_id":"568812","timestamp":"1647412800.0","upvote_count":"2","poster":"youonebe"},{"timestamp":"1642699020.0","content":"D\nnot A & C - KDF cannot achieve 30 seconds\nnot B - If I understood correctly It had to compute average value, and I don't think that it is for Lambda case","comment_id":"528613","poster":"pavelkuropatin","upvote_count":"1"},{"comments":[{"comment_id":"444778","upvote_count":"1","poster":"Marcinha","comments":[{"comment_id":"495180","content":"why you say the lambda takes 30 seconds to calculate an average?","timestamp":"1638799440.0","poster":"vasi_9969","upvote_count":"1"}],"content":"I Changed for D, because lambda is taking 30 seconds to calculate average. We need < 30 seconds.","timestamp":"1635983160.0"}],"comment_id":"442052","upvote_count":"1","timestamp":"1635681540.0","content":"I think B is correct.","poster":"Marcinha"},{"upvote_count":"2","content":"Correct answer is D. KDS->KDA->Lambda->SNS","timestamp":"1634564580.0","comment_id":"367858","poster":"Monika14Sharma"},{"timestamp":"1634063940.0","upvote_count":"4","content":"Why not B? Lambda can consume data from Kinesis Data Stream and process it. It can send alarm to SNS if limit breached. What is the problem in this simple solution?","comments":[{"comments":[{"upvote_count":"1","timestamp":"1699498080.0","content":"yeah me too.","comment_id":"1066074","poster":"LocalHero"}],"content":"This is what I do not understand as well. Lambda can act as a consumer to Kinesis Data Streams so why would you need Kinesis Data Analysis as an intermediary to calculate the metric and send it to SNS??","comment_id":"721320","poster":"allanm","upvote_count":"1","timestamp":"1668780540.0"}],"poster":"soni12390","comment_id":"362650"},{"comment_id":"353628","poster":"afantict","upvote_count":"1","timestamp":"1633410060.0","comments":[{"comment_id":"355995","comments":[{"poster":"arvindn","comment_id":"399201","upvote_count":"1","content":"In option D, KDA uses lambda to connect to SNS","timestamp":"1634759400.0"}],"upvote_count":"1","poster":"AjithkumarSL","content":"Don't think KDA can directly talk to SNS..","timestamp":"1633597800.0"},{"upvote_count":"1","content":"sorry, change to D","timestamp":"1633793700.0","comment_id":"356397","poster":"afantict"}],"content":"I think B is correct."}],"url":"https://www.examtopics.com/discussions/amazon/view/51709-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_images":[],"isMC":true,"answer_images":[],"answer_description":"","question_id":156,"topic":"1","unix_timestamp":1620057840,"answer":"D"},{"id":"izV373OFDEbpP8LGgjwe","question_text":"An IoT company wants to release a new device that will collect data to track sleep overnight on an intelligent mattress. Sensors will send data that will be uploaded to an Amazon S3 bucket. About 2 MB of data is generated each night for each bed. Data must be processed and summarized for each user, and the results need to be available as soon as possible. Part of the process consists of time windowing and other functions. Based on tests with a Python script, every run will require about 1 GB of memory and will complete within a couple of minutes.\nWhich solution will run the script in the MOST cost-effective way?","choices":{"C":"Amazon EMR with an Apache Spark script","A":"AWS Lambda with a Python script","D":"AWS Glue with a PySpark job","B":"AWS Glue with a Scala job"},"answers_community":["A (66%)","D (31%)","3%"],"exam_id":20,"question_images":[],"timestamp":"2021-05-03 18:05:00","answer_images":[],"answer_description":"","answer_ET":"A","isMC":true,"unix_timestamp":1620057900,"answer":"A","question_id":157,"discussion":[{"upvote_count":"23","poster":"adamstaros","comment_id":"479598","comments":[{"upvote_count":"3","timestamp":"1655612580.0","poster":"Booqq","comment_id":"618515","content":"D\nBecause:\nTumbling windows are distinct time windows that open and close at regular intervals. By default, Lambda invocations are statelessâ€”you cannot use them for processing data across multiple continuous invocations without an external database. However, with tumbling windows, you can maintain your state across invocations. This state contains the aggregate result of the messages previously processed for the current window. Your state can be a maximum of 1 MB per shard. If it exceeds that size, Lambda terminates the window early.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html#services-kinesis-windows"},{"poster":"nadavw","content":"Tumbling is for streaming analytics, and here it's a batch as data is in S3 source.","comment_id":"778602","timestamp":"1673934360.0","upvote_count":"2"}],"timestamp":"1637091420.0","content":"I think that answer should be \"A\". Lambda is the most cost effective solution and satisfies both memory and time requirements. Additionally lambda support \"tumbling windows\" https://aws.amazon.com/blogs/compute/using-aws-lambda-for-streaming-analytics/ so in my opinion \"A\" is the best option in this question."},{"timestamp":"1634287620.0","content":"Glue DataBrew supports window functions\nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.window.html\nwill go with D","upvote_count":"22","poster":"soni12390","comment_id":"362662"},{"timestamp":"1691218800.0","content":"Selected Answer: A\nA Python script is already provided, and rewriting it in Scala or Spark would be unnecessary work. Time windowing here is not referencing streaming, the files are already inside S3.","upvote_count":"2","comment_id":"972762","poster":"MLCL"},{"upvote_count":"1","content":"A - lambda invocation tumbling window functions","comment_id":"971519","timestamp":"1691104200.0","poster":"whenthan"},{"content":"A, not D. Confucius says \n\"Do not use a cannon to kill a mosquito.\"","comment_id":"858257","poster":"MeshterZYX","upvote_count":"4","timestamp":"1680379200.0"},{"poster":"AwsNewPeople","comment_id":"852200","upvote_count":"4","content":"Selected Answer: A\nIn normal answer sure I will go for D, but most cost effective and only require 1GB of memory, I will go for A\n\nAWS Lambda is a serverless compute service that runs code in response to events and automatically scales based on the incoming traffic. With Lambda, the user only pays for the compute time that the function uses, making it a cost-effective option. Since the Python script has been tested to run within a few minutes with 1 GB of memory, AWS Lambda can easily handle the processing requirements for this project.\n\nIn addition, since the data generated each night is relatively small (2 MB per bed), AWS Lambda's maximum payload size of 512 MB is more than enough to handle the incoming data. The processed data can also be easily uploaded to the Amazon S3 bucket.\n\nTherefore, AWS Lambda with a Python script would be the most cost-effective solution for this project, as it provides a serverless and scalable environment for running the Python script with the required memory and processing capabilities.","timestamp":"1679928240.0"},{"content":"Selected Answer: A\nMOST cost-effective way is Lambda","comment_id":"849352","timestamp":"1679666760.0","upvote_count":"1","poster":"akashm99101001com"},{"comment_id":"784297","content":"Answer A was impossible at the time exam DAS-C01 came out (13 APR 2020.) The AWS blog post announcing the new windowing feature for Lambda is dated 15 DEC 2020.","upvote_count":"2","timestamp":"1674391320.0","poster":"asyouwish"},{"timestamp":"1674136260.0","content":"Selected Answer: D\nin my opinion is D. time windowing , 1 GB of memory and a couple of minutes of execution...Glue is better","poster":"Erso","comment_id":"781186","upvote_count":"2"},{"comment_id":"778164","upvote_count":"1","timestamp":"1673897880.0","poster":"Arjun777","content":"Lamda to handle tumbling window - data should be limited to 1MB \nThese include:\n\nWindow start and end: the beginning and ending timestamps for the current tumbling window.\nState: an object containing the state returned from the previous window, which is initially empty. The state object can contain up to 1 MB of data.\nisFinalInvokeForWindow: indicates if this is the last invocation for the tumbling window. This only occurs once per window period.\nisWindowTerminatedEarly: a window ends early only if the state exceeds the maximum allowed size of 1 MB.\n\nTherefore its D - As glue job with Pyspark can handle this volume of data aggregation by each bed."},{"timestamp":"1673292000.0","upvote_count":"1","poster":"Chelseajcole","comment_id":"770798","content":"Maybe the question is testing do you know PySpark can do windowing function. PySpark is also Python script. In term of cost effective, Glue is cheaper cpmpare to EMR. So overall, I vote for D."},{"comment_id":"741894","timestamp":"1670776860.0","upvote_count":"3","poster":"silvaa360","content":"Selected Answer: A\nThe process is already tested with python, so there is not a concern on the fact that we might need a PySpark job to work with dataframes, etc.\nAlso it seems a bit overkill to set up a Spark job to process 2mb files. Again, if the quote saying that \"the script is tested with python\" was not present, I would choose PySpark or Scala. \nThe Python mention might also be for us to choose PySpark over Scala, but it is not a easy question and I think both answers are quite right.\n\nI think it must be A"},{"content":"Selected Answer: A\nA is way cost effective.","timestamp":"1669562700.0","poster":"thuyeinaung","comment_id":"728371","upvote_count":"2"},{"content":"Go for A due to cost effectiveness as Glue is much expensive as compare to Lambda","upvote_count":"3","comment_id":"705508","timestamp":"1666871820.0","poster":"cloudlearnerhere"},{"timestamp":"1666856700.0","poster":"Hussben","upvote_count":"1","content":"Selected Answer: A\nThe data size for every bed is 2 MB. In this case, Lambda should faster than Glub job","comment_id":"705348"},{"comment_id":"703989","poster":"Bansel","content":"A: This new feature introduces the concept of a tumbling window, which is a fixed-size, non-overlapping time interval of up to 15 minutes. To use this, you specify a tumbling window duration in the event-source mapping between the stream and the Lambda function. When you apply a tumbling window to a stream, items in the stream are grouped by window and sent to the processing Lambda function. The function returns a state value that is passed to the next invocation of the tumbling window.\nhttps://aws.amazon.com/blogs/compute/using-aws-lambda-for-streaming-analytics/","timestamp":"1666711920.0","upvote_count":"1"},{"comment_id":"699742","upvote_count":"1","content":"Selected Answer: D\nD over A for tumbling window","poster":"rav009","timestamp":"1666257780.0"},{"content":"Selected Answer: D\nAnswer:D","poster":"ryuhei","comment_id":"646255","upvote_count":"1","timestamp":"1660383780.0"},{"comment_id":"637490","upvote_count":"1","poster":"awsexpert69","content":"Selected Answer: D\nD is correct","timestamp":"1658847120.0"},{"poster":"rocky48","timestamp":"1658286660.0","upvote_count":"4","content":"Selected Answer: A\nAWS Lambda seems to be a cost-effective solution. Lambda can allocate up to 10 GB of memory i.e. RAM to a Lambda function. The maximum execution timeout for a function is 15 minutes. Lambda runs much faster for smaller tasks vs. Glue jobs which take longer to initialize due to the fact that it's using distributed processing. That being said, Glue leverages its parallel processing to run large workloads faster than Lambda. Lambda has a lifetime of fifteen minutes. It can be used to trigger a glue job as an event based activity. That is, when a file lands in S3 for example, we can have an event trigger which can run a glue job. Glue is a managed services for all data processing. If the data is very low maybe you can do it in lambda, but for some reason the process goes beyond fifteen minutes, then data processing would fail.","comment_id":"633812"},{"poster":"CloudTimes","comment_id":"613284","content":"Selected Answer: A\nI think that answer should be \"A\". Lambda is the most cost effective solution and satisfies both memory and time requirements.","timestamp":"1654694280.0","upvote_count":"1"},{"content":"Selected Answer: A\nThe data that needs to be processed is very small. Only 2MB per user. So using Scala or Spark is overkill and not cost-effective. 1GB of RAM and a few minutes of processing time is perfect for Lambda.","timestamp":"1654251300.0","upvote_count":"1","poster":"Ramshizzle","comment_id":"611020"},{"comment_id":"608240","timestamp":"1653707640.0","content":"Selected Answer: A\nAnswer is A","poster":"Bik000","upvote_count":"1"},{"poster":"SengarAshu","upvote_count":"2","content":"Selected Answer: D\nLambda ruled out because it takes couple of minutes to process data which will be very cost for lambda \nGlue DataBrew supports window functions\nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.functions.window.html\nwill go with D","timestamp":"1651474620.0","comment_id":"595925"},{"content":"Selected Answer: C\nI am surprised no one answered C. Glue is generally more expensive than EMR for a similar config. Several articles have explained with examples. Hence, both Glue options are ruled out. Lambda is not a good option to summarize data from several; different sensors in S3. EMR (potentially transient - running in the morning) would be the most cost effective Map - Reduce solution. Scenario is a typical Map-Reduce use case.","timestamp":"1651428600.0","upvote_count":"1","comment_id":"595693","poster":"rrshah83"},{"comment_id":"592946","timestamp":"1651044180.0","poster":"MWL","upvote_count":"1","content":"Selected Answer: A\nI vote for A, the test script is Python, so it should work with Lambda.\nAnd, the data is from S3, each file is for each bed, (it should means each S3 file is for each user). So we just need the python function to process this file to process personal info."},{"comment_id":"591960","poster":"VJ_RV","timestamp":"1650919920.0","content":"Guys - my pick is A. \nplease note the point ('made accessible as feasible') which kind of means that the resulting data needs to be easily accessible (S3->Quicksight) and feasible meaning cost-optimization, scalability and maintenance. This task sounds more of a initial analysis work in order to consider running it long-term on bigger volumes of data. Lambda is best suited for this.\n- the most cost-effective solution and easily scalable.\n- it is easier to debug Lambda than Glue. \n- the time to setup up the workflow is easier (S3 event notification -> Lambda) compared to Glue where we have to setup AWS glue catalog table, run crawler to update the table everyday and enable job bookmarks in order to deduplicate data.\n- regarding window functions note that we use Lambda functions with Kinesis data analytics in order to transform source data. Please do not underestimate Lambda. They are small yet extremely powerful for these tasks.\n\nIMO pyspark for 1 gb of RAM is overkill.","upvote_count":"3"},{"poster":"sbxme","timestamp":"1649230440.0","content":"A is correct because we can use only 1 GB of ram for using glue we need at least 1 dpu of allocation","upvote_count":"2","comment_id":"581680"},{"poster":"Dodai","comment_id":"547761","content":"A should be the answer, since Lambda supports Time windowing","upvote_count":"1","timestamp":"1644929940.0"},{"poster":"penelop","comment_id":"543820","timestamp":"1644416460.0","upvote_count":"3","content":"Selected Answer: D\nD is the correct answer. Pyspark is python."},{"poster":"Shivanikats","comment_id":"520531","comments":[{"poster":"Shivanikats","comment_id":"520576","upvote_count":"1","content":"I'll probably go with D though if it comes up in the exam. Reason being windowing function support with pyspark+glue","timestamp":"1641780300.0"}],"upvote_count":"1","content":"So is it A or D??","timestamp":"1641772020.0"},{"upvote_count":"2","timestamp":"1640525700.0","content":"We must as quickly as possible, so it must be A, other options can't be quick.","comment_id":"509618","poster":"wentjiang"},{"timestamp":"1640461800.0","content":"Between B and D, it is difficult to assess which is more cost-effective. Scala scripts have better performance than PySpark scripts. However, the question talks about cost. Hence, both B and D seem equally likely. Therefore, I would go with A.","poster":"justsaysid","comment_id":"509250","upvote_count":"1"},{"content":"A is MOST Cost effective among all. Tumbling is just to confuse","poster":"lakeswimmer","comment_id":"492576","timestamp":"1638454140.0","upvote_count":"1"},{"timestamp":"1636983720.0","comment_id":"478700","poster":"Fazil_Cp","upvote_count":"3","content":"I think the answer should be Option A - Which is most cost effective and also satisfies the memory and run time requirements. If we are choosing D , why not B ? both are using glue. C is anyways ruled out as it is not cost effective. Your thoughts please."},{"timestamp":"1636926300.0","upvote_count":"2","content":"\"A\" it is","poster":"aws2019","comment_id":"478353"},{"comment_id":"367861","poster":"Monika14Sharma","upvote_count":"2","timestamp":"1634880420.0","content":"Should be A"},{"poster":"ksaws","comment_id":"357930","timestamp":"1634285940.0","content":"its A , Lambda can handle 10 GB and 1 mins and can do python","comments":[{"comment_id":"398394","upvote_count":"2","poster":"Donell","comments":[{"timestamp":"1636222140.0","content":"Question also says based on a test with \"Python Script\". You cant say Lambda is ot suitable for windowing","comments":[{"upvote_count":"1","content":"Pyspark is python, so D is a better approach.","poster":"penelop","timestamp":"1644416400.0","comment_id":"543819"},{"content":"lambda supports tumbling window function","comment_id":"544907","poster":"sebastian_hoi","upvote_count":"1","timestamp":"1644533700.0"},{"upvote_count":"1","comment_id":"571583","poster":"CHRIS12722222","content":"The issue i have with options B & D is that it does not specify which version of glue it is. From Acloudguru data analytics course (chapter 9), Glue version impacts significantly on cost. With Glue0.9/1.0, even if the job runs for 1min, you will still be charged for 10mins minimum. Glue 2.0 pricing structure is a minimum of 1min and 1s increments. \n\nAnswer: Option A","timestamp":"1647777780.0"}],"upvote_count":"2","comment_id":"446261","poster":"Balendu"}],"content":"Its D not A.\nQuestion says \"Part of the process consists of time windowing and other functions\".\nLambda is not suitable for the above.","timestamp":"1634946300.0"}],"upvote_count":"3"},{"comment_id":"348791","comments":[{"timestamp":"1632765480.0","comment_id":"353632","poster":"afantict","comments":[{"comments":[{"poster":"afantict","timestamp":"1633423740.0","content":"A maybe","upvote_count":"4","comment_id":"356664"}],"content":"sorry, change to D","timestamp":"1632845640.0","poster":"afantict","upvote_count":"1","comment_id":"356398"}],"content":"why not B?","upvote_count":"1"}],"content":"Should be 'D'.","poster":"VikG12","timestamp":"1632598680.0","upvote_count":"8"}],"url":"https://www.examtopics.com/discussions/amazon/view/51710-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1"},{"id":"rVOFNqgitejIOUMKCOjl","answer_images":[],"topic":"1","exam_id":20,"timestamp":"2021-05-03 18:06:00","answer_ET":"A","unix_timestamp":1620057960,"isMC":true,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/51711-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":158,"answer":"A","discussion":[{"timestamp":"1635426420.0","content":"A is wrong, read that locks means: Locking is a protection mechanism that controls how many sessions can access a table at the same time. To solve a locking problem, identify the session (PID) that is holding the lock and then terminate the session. If the session doesn't terminate, reboot your cluster.\nSo... If you increase concurrency, you increase a number of sessions that makes things worse. \nHence C","comment_id":"423887","upvote_count":"17","comments":[{"comment_id":"874221","content":"Please explain how decreasing timeout help in this case?","poster":"yellowdev","timestamp":"1681867980.0","upvote_count":"1"},{"comment_id":"641756","timestamp":"1659521460.0","content":"You are only suggesting to decrease the timeout value. How does that help if data is missing to be copied? The best way to solve this problem is to increase re tries, wait less for a re try to complete and concurrently copy data to redshift. So ans is \"A\".","poster":"dushmantha","upvote_count":"2"}],"poster":"Dr_Kiko"},{"upvote_count":"11","poster":"VikG12","timestamp":"1633231740.0","content":"Not sure but should be A for number of re-tries and increased concurrency.","comment_id":"348793"},{"comment_id":"972774","content":"Selected Answer: D\nD makes sense, increasing timeout leaves time for locks to resolve, also, job concurrency should be 1 for COPY commands, it doesnt make sense to have multiple ones running.","upvote_count":"2","timestamp":"1691219880.0","poster":"MLCL"},{"comments":[{"comment_id":"859931","content":"Actually, timeout jobs aren't retried as per the docs. So even A is wrong from a retry point of view. The only thing that will allow us to wait for the lock to resolve themselves is an increase in timeout then - so really all answers are bad, but D is less bad then. Also the default timeout is 48 hours, so the fact that the job was set to 5 min timeout is short.","upvote_count":"4","poster":"mawsman","timestamp":"1680527340.0"}],"comment_id":"859924","poster":"mawsman","timestamp":"1680526860.0","content":"A is the least wrong, since it theoretically will increase fault tolerance. But it's an incorrect by design. Increasing concurrency will lead to an increase in locks, leading to more timeouts, leading to more retries, so once we reach the max retries, some data still might be missed. The right answer would be to set concurrency to 1 and increase retries while decreasing timeouts.\nB, C and D: Keep the number of retries at 0. - whatever the other configuration you set, when the job fails due to a lock that job will never be retried and data will be missed 100%.","upvote_count":"5"},{"comment_id":"851541","timestamp":"1679869440.0","upvote_count":"1","content":"Option C is the best solution to optimize fault tolerance and improve data availability. Keeping the number of retries at 0 will ensure that the job does not attempt to retry if it fails, which may cause further locks and missed data. Decreasing the timeout value will ensure that the job fails quickly if it is unable to complete the COPY command. Keeping the job concurrency at 1 will ensure that the job runs on a single node, which will reduce the chances of locks occurring.","poster":"rags1482"},{"poster":"akashm99101001com","timestamp":"1679313720.0","upvote_count":"2","content":"Selected Answer: A\nets go in reverse to answer this.\nD is out - Glue job that is scheduled to run every 5 minutes so timeout can't increase\nC is out - locks will still exist when spike is high\nB is out - jobs will fail due to decreased timeout when there is no concurrency.","comment_id":"844840"},{"upvote_count":"2","content":"Selected Answer: A\nlets go in reverse to answer this.\nD is out - Glue job that is scheduled to run every 5 minutes\nC is out - locks will still exist\nB is out - jobs will due to decreased timeout when there is no concurrency.","comment_id":"844839","timestamp":"1679313600.0","poster":"akashm99101001com"},{"timestamp":"1676698500.0","poster":"Gabba","upvote_count":"4","content":"Selected Answer: A\nLocks generally happen in case of DDL statements. Refer this link - https://docs.aws.amazon.com/redshift/latest/dg/r_LOCK.html\nQuestion mentions about Glue job reading data every 5 mins and inserting it in Redshift. In case of lock present on table, the job would timeout. If we don't retry, there is obviously data loss. \nJob generally completes in few seconds so decreasing timeout definitely makes sense and everyone agree. \nWe definitely need to increase retry count so if lock exist, job will retry with same data insertion in hope that lock is released.\nEven if lock is not released in next attempt and by the time next glue job sequence will pick the data for insertion and if we don't increase concurrency, initial failed job would lead to data loss.\nSo important to increase concurrency as well so that previous instance and current instance of glue job can run together.","comment_id":"812676"},{"timestamp":"1669888800.0","comment_id":"732474","upvote_count":"1","poster":"henom","content":"Selected Answer: D\nSuggested Concurrency is 1"},{"comment_id":"642259","poster":"rav009","timestamp":"1659601920.0","content":"D.\nIncreasing the timeout is not conflict with \"typically finish with a few seconds\", and it can be helpful in the peak time","upvote_count":"4"},{"timestamp":"1658452500.0","content":"Selected Answer: A\nSelected Answer: A","poster":"rocky48","comment_id":"634925","upvote_count":"1"},{"upvote_count":"3","timestamp":"1653178200.0","comment_id":"605054","content":"A looks right. Timeout should be reduced and retries should be increased. Also, concurrency should be increased so that the copy operation completes quickly. Concurrency will not cause locking as the copy command running in concurrency mode should be part of the same transaction. Hence there is no question of locking.","poster":"certificationJunkie"},{"content":"Selected Answer: C\nIncreasing number of retries will add stress on locks when Copy command completes in few seconds, there is no need to wait till 5 min rather, TimeOut should be decreased. Concurrency at 1 is perfect","comment_id":"602889","comments":[{"content":"if a copy command completes, system will not wait for 5 mins and there is no timeout scenario","poster":"certificationJunkie","timestamp":"1653178080.0","comment_id":"605053","upvote_count":"1"}],"poster":"Shammy45","upvote_count":"2","timestamp":"1652785440.0"},{"upvote_count":"2","timestamp":"1651820460.0","comment_id":"597599","poster":"MWL","content":"Selected Answer: A\nAgree with NTP's first answer, and added some comments.\nThe question requires \"Data avaibility\" and \"fault tolerant\", so data loss should be avoid.\nI aggree with your previoud answer:\n1. COPY finish within a few seconds, so decrease timeout and increase retries. It can reduce the table lock as you said.\n2. increase concurrent jobs, it can execute COPY and QUERY at the same time, if only they are not on the same table. And if only most of the COPY can finish in seconds, there will be no LOCK because of this."},{"comment_id":"573467","timestamp":"1648023540.0","upvote_count":"2","poster":"CHRIS12722222","comments":[{"content":"increased timeout will help because longer timeout means more waiting for glue job which means more chance for session holding the lock to complete before timeout occurs.","poster":"CHRIS12722222","timestamp":"1648033380.0","upvote_count":"1","comment_id":"573557"}],"content":"If locks exists, reads/writes are made to wait till the session holding the lock completes\nRef: https://docs.aws.amazon.com/redshift/latest/dg/r_LOCK.html\n\nTherefore, increasing timeout will help. Keeping concurrency at 1 is recommended to avoid multiple COPY command and prevent multiple instances of glue task. \n\nMaybe answer = D"},{"comments":[{"comments":[{"upvote_count":"1","poster":"MWL","content":"The question requires \"Data avaibility\" and \"fault tolerant\", so data loss should be avoid.\nI aggree with your previoud answer:\n1. COPY finish within a few seconds, so decrease timeout and increase retries. It can reduce the table lock as you said.\n2. increase concurrent jobs, it can execute COPY and QUERY at the same time, if only they are not on the same table. And if only most of the COPY can finish in seconds, there will be no LOCK because of this.","comment_id":"597597","timestamp":"1651820400.0"}],"comment_id":"506624","timestamp":"1640142240.0","content":"Change to C\nNumber of retries - Specify the number of times, from 0 to 10, that AWS Glue should automatically restart the job if it fails. Jobs that reach the timeout limit are not restarted.\nhttps://docs.aws.amazon.com/glue/latest/dg/add-job.html\nSo retry will not help in case of timeout. With only reducing timeout, we can only reduce the locks and the amount of data lost, it's ok because the question does not require no data lost","poster":"npt","upvote_count":"3"}],"poster":"npt","timestamp":"1640141880.0","upvote_count":"1","content":"A\nCOPY commands typically finish within a few seconds -> decrease timeout to 30 seconds. If there is a lock, this task will fail and release a lock, but we still loose the data. So we need to add more retries, e.g. 2 retries. If we add so many retires, there could be concurrent jobs, concurrent COPY is not recommended but this case of lock happens many times continuously is rare","comment_id":"506620"},{"poster":"lakediver","content":"I think should be A\nIncreasing retries alone can ensure data can be loaded. With 0 retry once job is failed it will not retry.","timestamp":"1640061840.0","comment_id":"505820","upvote_count":"1"},{"upvote_count":"7","poster":"Olga2022","comments":[{"upvote_count":"1","poster":"sly_tail","timestamp":"1680118800.0","comment_id":"854880","content":"I would say that D is a better fit here. It will make a job run longer. And that's what we need because of the lock. The higher TO the higher chance the job will be completed successfully. max concurrency == 1 makes sure no other jobs will start."}],"comment_id":"484448","timestamp":"1637604720.0","content":"Very unclear question to me. According the documentation:\nhttps://docs.aws.amazon.com/glue/latest/dg/add-job.html\nUnder \"Number of retries\" section \"Jobs that reach the timeout limit are not restarted.\" which means A will have no effect.\nAccording the job concurrency, AWS does not recommend concurrent COPY commands in Redshift. This eliminates A&B.\nIf we increase timeout and leave concurrency at 1, the job will fail (as said on the same doc page under \"Max concurrency\" section \"An error is returned when this threshold is reached\"). This eliminates D.\nThe only option left is C. However, I don't understand how this can help. Maybe to reduce the pressure on redshift locks."},{"timestamp":"1635364680.0","content":"I goes with A.\nWithout retry no matter what, if the job fails that particular data will never reach the Redshift.\nAs per AWS FAQ: Glue also provides default retry behavior that will retry all failures three times before sending out an error notification.","upvote_count":"2","poster":"Donell","comment_id":"398458"},{"poster":"Huy","content":"The question ask for handling concurrency of AWS Glue jobs. Purpose is when a job fail, it should be dispel as soonest. With that I think the correct answer is C","comment_id":"389760","timestamp":"1635268500.0","upvote_count":"3"},{"content":"Will go with A. \nJobs completes within sec, So time out can be reduced. Also retires can be applied on failure.","comment_id":"370353","poster":"tukai","timestamp":"1635134220.0","upvote_count":"4"},{"comment_id":"362674","poster":"soni12390","timestamp":"1634029200.0","content":"I meant decreasing the timeout of 5 mins, changed to C","upvote_count":"4"},{"timestamp":"1633773600.0","content":"Increased concurrency an cause locks so it should be either C or D. Also job is running every 5 mins and also waiting for 5 mins so reducing on wait time will help on avoiding locks. Will go with D.","upvote_count":"2","comment_id":"362671","poster":"soni12390"}],"choices":{"A":"Increase the number of retries. Decrease the timeout value. Increase the job concurrency.","D":"Keep the number of retries at 0. Increase the timeout value. Keep the job concurrency at 1.","B":"Keep the number of retries at 0. Decrease the timeout value. Increase the job concurrency.","C":"Keep the number of retries at 0. Decrease the timeout value. Keep the job concurrency at 1."},"question_text":"A company wants to provide its data analysts with uninterrupted access to the data in its Amazon Redshift cluster. All data is streamed to an Amazon S3 bucket with Amazon Kinesis Data Firehose. An AWS Glue job that is scheduled to run every 5 minutes issues a COPY command to move the data into Amazon Redshift.\nThe amount of data delivered is uneven throughout the day, and cluster utilization is high during certain periods. The COPY command usually completes within a couple of seconds. However, when load spike occurs, locks can exist and data can be missed. Currently, the AWS Glue job is configured to run without retries, with timeout at 5 minutes and concurrency at 1.\nHow should a data analytics specialist configure the AWS Glue job to optimize fault tolerance and improve data availability in the Amazon Redshift cluster?","answers_community":["A (72%)","D (17%)","11%"]},{"id":"8Yy4jbTS9cl5SHzEIlx7","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/51714-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2021-05-03 18:27:00","exam_id":20,"topic":"1","answer_description":"","answer_ET":"C","question_text":"A retail company leverages Amazon Athena for ad-hoc queries against an AWS Glue Data Catalog. The data analytics team manages the data catalog and data access for the company. The data analytics team wants to separate queries and manage the cost of running those queries by different workloads and teams.\nIdeally, the data analysts want to group the queries run by different users within a team, store the query results in individual Amazon S3 buckets specific to each team, and enforce cost constraints on the queries run against the Data Catalog.\nWhich solution meets these requirements?","question_images":[],"isMC":true,"answers_community":["C (100%)"],"unix_timestamp":1620059220,"question_id":159,"choices":{"C":"Create Athena workgroups for each team within the company. Set up IAM workgroup policies that control user access and actions on the workgroup resources.","D":"Create Athena query groups for each team within the company and assign users to the groups.","A":"Create IAM groups and resource tags for each team within the company. Set up IAM policies that control user access and actions on the Data Catalog resources.","B":"Create Athena resource groups for each team within the company and assign users to these groups. Add S3 bucket names and other query configurations to the properties list for the resource groups."},"discussion":[{"poster":"VikG12","timestamp":"1632318240.0","content":"C should be the one.","upvote_count":"24","comment_id":"348813"},{"content":"C. Should be the right answer. https://docs.aws.amazon.com/athena/latest/ug/workgroups.html","upvote_count":"5","poster":"ariane_tateishi","comment_id":"366371","timestamp":"1635569280.0"},{"timestamp":"1699495560.0","poster":"LocalHero","content":"Athena resouce group is not found.\nIs this correct recognization?\nso answer is C.I think.","comment_id":"1066065","upvote_count":"1"},{"comment_id":"886507","timestamp":"1682959080.0","content":"C: I passed the test","upvote_count":"1","poster":"pk349"},{"content":"C for me","upvote_count":"1","poster":"CleverMonkey092","timestamp":"1679744160.0","comment_id":"850079"},{"upvote_count":"5","poster":"cloudlearnerhere","timestamp":"1666872240.0","comment_id":"705517","content":"Correct answer is C as Athena workgroups can help isolate queries for teams, applications, and workloads. It also helps store the results in a separate S3 bucket and enforce cost constraints. Access to workgroups can be controlled using IAM. \nAWS doc says :\nWe recommend using workgroups to isolate queries for teams, applications, or different workloads. For example, you may create separate workgroups for two different teams in your organization. You can also separate workloads. For example, you can create two independent workgroups, one for automated scheduled applications, such as report generation, and another for ad-hoc usage by analysts. You can switch between workgroups. \n\nTo control access to workgroups, use resource-level IAM permissions or identity-based IAM policies. \nhttps://docs.aws.amazon.com/athena/latest/ug/workgroups.html"},{"poster":"rocky48","comment_id":"643506","timestamp":"1659816660.0","content":"Selected Answer: C\nSelected Answer: C","upvote_count":"1"},{"content":"Selected Answer: C\nAnswer is C","comment_id":"604862","upvote_count":"1","poster":"Bik000","timestamp":"1653132720.0"},{"timestamp":"1638294180.0","poster":"Thiya","comment_id":"490893","content":"Selected Answer: C\nC is the answer","upvote_count":"2"},{"poster":"SGES","timestamp":"1636103400.0","content":"C my option - Workgroups to isolate queries for teams, applications, or different workloads. For example, you may create separate workgroups for two different teams in your organization","upvote_count":"3","comment_id":"438221"},{"timestamp":"1633358820.0","content":"Looks like its B\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/","upvote_count":"1","comment_id":"355250","poster":"AjithkumarSL","comments":[{"timestamp":"1633872720.0","content":"I mean C","comment_id":"355323","upvote_count":"2","poster":"AjithkumarSL"}]}],"answer":"C"},{"id":"5MUoMp5LQaUpluO8HA6p","answers_community":["B (100%)"],"choices":{"A":"Run Lake Formation blueprints to move the data to Lake Formation. Once Lake Formation has the data, apply permissions on Lake Formation.","B":"To create the data catalog, run an AWS Glue crawler on the existing Parquet data. Register the Amazon S3 path and then apply permissions through Lake Formation to provide granular-level security.","D":"Create multiple IAM roles for different users and groups. Assign IAM roles to different data assets in Amazon S3 to create table-based and column-based access controls.","C":"Install Apache Ranger on an Amazon EC2 instance and integrate with Amazon EMR. Using Ranger policies, create role-based access control for the existing data assets in Amazon S3."},"isMC":true,"answer":"B","answer_images":[],"unix_timestamp":1620059880,"question_id":160,"topic":"1","answer_ET":"B","exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/51715-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_description":"","question_text":"A manufacturing company uses Amazon S3 to store its data. The company wants to use AWS Lake Formation to provide granular-level security on those data assets. The data is in Apache Parquet format. The company has set a deadline for a consultant to build a data lake.\nHow should the consultant create the MOST cost-effective solution that meets these requirements?","timestamp":"2021-05-03 18:38:00","question_images":[],"discussion":[{"timestamp":"1634505540.0","upvote_count":"39","comment_id":"394105","content":"I will go with Option B. Here Data is already there in S3 bucket in parquet format. We just need to register the S3 bucket with Lake Formation after the catalogue creation. Blueprints are Ideal way if the Data has to be brought to S3 from other sources. \nFrom Lake Formation FAQ:\n\nHow does Lake Formation organize my data in a data lake?\n\nA: You can use one of the blueprints available in Lake Formation to ingest data into your data lake. Lake Formation creates Glue workflows that crawl source tables, extract the data, and load it to S3. In S3, Lake Formation organizes the data for you, setting up partitions and data formats for optimized performance and cost. For data already in Amazon S3, you can register those buckets with Lake Formation to manage them.\n\nLake Formation also crawls your data lake to maintain a data catalog and provides an intuitive user interface for you to search entities (by type, classification, attribute, or free-form text.)","poster":"Donell","comments":[{"timestamp":"1635041880.0","upvote_count":"3","poster":"gunjan4392","comment_id":"395406","content":"Makes sense, B for me"},{"upvote_count":"3","content":"Agree Option B","poster":"lakeswimmer","comment_id":"493007","timestamp":"1638517140.0"}]},{"timestamp":"1632513420.0","comments":[{"content":"I agree that it looks attractive. However, I think that the fact that the data is all already in S3 doesn't require Blueprints. The Blueprints section mentions importing data into your datalake, and your datalake will store the data in S3.","comment_id":"616625","poster":"Ramshizzle","timestamp":"1655281320.0","upvote_count":"1"},{"comment_id":"721443","timestamp":"1668791100.0","poster":"allanm","upvote_count":"1","content":"It can't be A because your data is already in S3. Blueprints only work if you do not have an S3 data lake in place which is not the case for this question."}],"upvote_count":"15","comment_id":"348816","content":"Looks like 'A'.\nhttps://aws.amazon.com/blogs/big-data/building-securing-and-managing-data-lakes-with-aws-lake-formation/","poster":"VikG12"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/security-permissions-example-scenario.html","upvote_count":"1","comment_id":"971544","timestamp":"1691106840.0","poster":"whenthan"},{"comments":[{"timestamp":"1683180420.0","poster":"AWenger","upvote_count":"1","comments":[{"timestamp":"1684834680.0","content":"Did he respond?","poster":"gndu","upvote_count":"1","comment_id":"904788"}],"comment_id":"889191","content":"Hi, Are these questions still valid for the exams ? Please respond."}],"timestamp":"1682959200.0","upvote_count":"1","poster":"pk349","comment_id":"886509","content":"B: I passed the test"},{"content":"It's B\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/register-data-lake.html","upvote_count":"2","timestamp":"1679759220.0","poster":"koteshv","comment_id":"850257"},{"upvote_count":"1","comment_id":"744680","poster":"VijiTu","timestamp":"1670987880.0","content":"Answer B\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/register-data-lake.html"},{"upvote_count":"2","poster":"cloudlearnerhere","timestamp":"1666872360.0","comment_id":"705519","content":"B is the right answer as per doc:\nYou can use one of the blueprints available in Lake Formation to ingest data into your data lake. Lake Formation creates Glue workflows that crawl source tables, extract the data, and load it to S3. In S3, Lake Formation organizes the data for you, setting up partitions and data formats for optimized performance and cost. For data already in Amazon S3, you can register those buckets with Lake Formation to manage them.\nhttps://aws.amazon.com/lake-formation/faqs/\nOption A is wrong as duplicating the data in AWS Lake Formation would not be the most cost-effective approach.\n\nOption C is wrong as using EMR would not be the most cost-effective approach also it would take time to set up.\n\nOption D is wrong as S3 does not provide table-based or column-based access control."},{"comment_id":"669469","upvote_count":"1","content":"Selected Answer: B\nA: move data takes more effort and cost\nC: doesn't need EMR\nD: We should use Lake Formation not IAM, and IAM can't offer column level control.\nSo I like B","timestamp":"1663216200.0","poster":"klausyu999"},{"comment_id":"634371","timestamp":"1658380800.0","content":"Selected Answer: B\nOption B.","poster":"rocky48","upvote_count":"1"},{"timestamp":"1655281380.0","upvote_count":"2","comment_id":"616626","poster":"Ramshizzle","content":"Selected Answer: B\nUse Lake Formation for the permissions. No need to use Blueprints because all the data is already in S3. Only thing that is required to get the data into the data lake is use a Glue Crawler."},{"comment_id":"616350","upvote_count":"1","timestamp":"1655230860.0","content":"Answer: B","poster":"Ayaa4"},{"content":"it's A. B is ruled out as you don't have to run crawler etc. when you are using LakeFormation. It will do everything for you","comment_id":"608016","poster":"certificationJunkie","timestamp":"1653644640.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"605258","content":"Selected Answer: B\nI think Answer should be B","timestamp":"1653205800.0","poster":"Bik000"},{"comment_id":"595257","upvote_count":"1","poster":"jrheen","timestamp":"1651349700.0","content":"Answer : B"},{"poster":"Agn3001","comment_id":"561422","upvote_count":"1","content":"A - https://docs.aws.amazon.com/lake-formation/latest/dg/how-it-works.html\nLake formation can move data as well as provide granular access from its blue prints - which is mainly intended for cost effective/time efficient way to implement faster data lakes. Whereas option B is building datalake on our own. build vs buy analogy wise A will suit the question from quick turnaround point of view from deployment.","timestamp":"1646483640.0"},{"content":"B - the data is already in S3, no need to use a blueprint to import it","upvote_count":"1","comment_id":"560813","poster":"rb39","timestamp":"1646401860.0"},{"upvote_count":"1","content":"Answer B. You donâ€™t need blueprint since data is already in S3.","timestamp":"1639049520.0","poster":"irene7","comment_id":"497670"},{"comment_id":"482096","poster":"aws2019","timestamp":"1637355960.0","content":"B is right","upvote_count":"1"},{"upvote_count":"3","comment_id":"478446","content":"Important factors\n* Secure at the granular level.\n* Data already in S3 in Apache Parquet \n* Deadline to deliver\n* MOST COST-EFFECTIVE\n\nWhen choosing from A n B, Data already in the S3 will help answer B. \nDoes blueprint only work with ingestion or can't we use data already in S3 ? If it does then A will be the answer since it will help Deadline factor too.","timestamp":"1636948200.0","poster":"attaraya"},{"poster":"ThomasKalva","comment_id":"446299","timestamp":"1635865080.0","upvote_count":"5","content":"B is the right answer.. I recently built data mesh on the lake utilizing LakeFormation and in my experience B is the answer. There is just need to catalog data using crawler and render permissioning using LF, A only talks about blueprint for migration, we already have data in s3, no need to migrate as another user also commented."},{"timestamp":"1635440940.0","poster":"Dr_Kiko","comment_id":"423874","upvote_count":"1","content":"A; you configure granular level access from inside LF: https://aws.amazon.com/blogs/big-data/manage-fine-grained-access-control-using-aws-lake-formation/"},{"poster":"AWSandeep","comment_id":"423762","timestamp":"1635355380.0","upvote_count":"1","content":"Answer is B for sure.\n\"To create your data warehouse or data lake, you must catalog this data. The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data.\"\nhttps://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html"},{"poster":"uninit","timestamp":"1635073620.0","comment_id":"404725","content":"Looks like A as pointed out by VikG12 - \nhttps://aws.amazon.com/blogs/big-data/building-securing-and-managing-data-lakes-with-aws-lake-formation/\nAlso read this - \nhttps://www.dremio.com/subsurface/what-is-aws-lake-formation\n\n\nPoints in favor - \n- Lake formation can ingest, clean, encrypt and register existing S3 bucket content.\n- Lake Formation provides blueprints, which are templates for predefined sources that allow users to easily ingest data into S3. Workflows, which consists of AWS Glue crawlers, jobs and triggers, are created by selecting a blueprint and providing a data source such as a relational database, the S3 bucket to use as the target location, and the frequency at which the data should be synced as input parameters. Blueprints automatically discover the sourceâ€™s schema, convert the data to open format, and maintain a record of data that has already been imported and processed. The data gets read by crawlers and imported into S3.\nTherefore, you don't need to run any separate crawlers as mentioned in B","upvote_count":"2"},{"content":"Don't go blindly with what has been commented here to choose your answers. Please do your own research. Sometimes the popular comment may not give the right answers.","comment_id":"394112","poster":"Donell","upvote_count":"7","timestamp":"1634720640.0"},{"comment_id":"367882","content":"Its B for sure","comments":[{"timestamp":"1634358660.0","comment_id":"367885","upvote_count":"2","poster":"Monika14Sharma","content":"i meant A"}],"timestamp":"1632645180.0","upvote_count":"1","poster":"Monika14Sharma"},{"comment_id":"357953","upvote_count":"1","timestamp":"1632644940.0","poster":"ksaws","content":"agreed"}]}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","name":"AWS Certified Data Analytics - Specialty","numberOfQuestions":164,"isBeta":false,"id":20,"isImplemented":true,"isMCOnly":true},"currentPage":32},"__N_SSP":true}