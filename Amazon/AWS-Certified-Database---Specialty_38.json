{"pageProps":{"questions":[{"id":"jx5sAnJjJzT7UPKZM5Oj","topic":"1","timestamp":"2022-11-30 23:20:00","isMC":true,"discussion":[{"timestamp":"1669846800.0","poster":"Sab","comment_id":"732046","upvote_count":"8","content":"B\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html"},{"poster":"shubhary25","timestamp":"1671384300.0","upvote_count":"5","comment_id":"749069","content":"Selected Answer: B\nB.\nA gateway VPC endpoint"},{"content":"Selected Answer: B\nB. Create a gateway VPC endpoint for DynamoDB to provide access to the table\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ddb.html\n\n\n\"You can access Amazon DynamoDB from your VPC using gateway VPC endpoints. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to DynamoDB.\"","poster":"Pranava_GCP","timestamp":"1694025240.0","comment_id":"1000882","upvote_count":"4"},{"comment_id":"744958","upvote_count":"3","content":"B- VPC endpoint for DynamoDB","poster":"Hira101","timestamp":"1671012480.0"}],"question_id":186,"choices":{"C":"Use a network ACL to only allow access to the DynamoDB table from the VPC","B":"Create a gateway VPC endpoint for DynamoDB to provide access to the table","A":"Provision the DynamoDB table inside the same VPC that contains the Lambda functions","D":"Use a security group to only allow access to the DynamoDB table from the VPC"},"exam_id":22,"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/89463-exam-aws-certified-database-specialty-topic-1-question-266/","question_images":[],"answers_community":["B (100%)"],"answer_ET":"B","unix_timestamp":1669846800,"answer":"B","question_text":"A company uses AWS Lambda functions in a private subnet in a VPC to run application logic. The Lambda functions must not have access to the public internet. Additionally, all data communication must remain within the private network. As part of a new requirement, the application logic needs access to an Amazon DynamoDB table.\n\nWhat is the MOST secure way to meet this new requirement?"},{"id":"x81ChnFghFkfq6cxLJkN","answer_ET":"C","answer_description":"","question_images":[],"question_text":"A startup company is developing electric vehicles. These vehicles are expected to send real-time data to the AWS Cloud for data analysis. This data will include trip metrics, trip duration, and engine temperature. The database team decides to store the data for 15 days using Amazon DynamoDB.\n\nHow can the database team achieve this with the LEAST operational overhead?","exam_id":22,"timestamp":"2022-11-30 23:21:00","answers_community":["C (100%)"],"choices":{"A":"Implement Amazon DynamoDB Accelerator (DAX) on the DynamoDB table. Use Amazon EventBridge (Amazon CloudWatch Events) to poll the DynamoDB table and drop items after 15 days","C":"Turn on the TTL feature for the DynamoDB table. Use the TTL attribute as a timestamp and set the expiration of items to 15 days","D":"Create an AWS Lambda function to poll the list of DynamoDB tables every 15 days. Drop the existing table and create a new table","B":"Turn on DynamoDB Streams for the DynamoDB table to push the data from DynamoDB to another storage location. Use AWS Lambda to poll and terminate items older than 15 days."},"discussion":[{"timestamp":"1694025720.0","comment_id":"1000890","content":"Selected Answer: C\nC. Turn on the TTL feature for the DynamoDB table.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\n\n\"Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table and only consumes throughput when the deletion is replicated to additional Regions. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload’s needs.\"","upvote_count":"3","poster":"Pranava_GCP"},{"poster":"chen0305_099","upvote_count":"1","comment_id":"998238","timestamp":"1693806480.0","content":"Selected Answer: C\nＣＣＣＣＣＣＣ"},{"content":"C...Expiration after 15 days, use TTL feature","poster":"tsk9921","timestamp":"1683763440.0","upvote_count":"1","comment_id":"894464"},{"comment_id":"741404","content":"Selected Answer: C\nDefinitely C","poster":"amulbaba","upvote_count":"1","timestamp":"1670732820.0"},{"upvote_count":"3","content":"Selected Answer: C\nC is the answer, Less overhead","poster":"satishstechie","timestamp":"1669981800.0","comment_id":"733693"},{"upvote_count":"2","comment_id":"733691","timestamp":"1669981740.0","content":"Answer is C","poster":"satishstechie"},{"poster":"Sab","upvote_count":"2","timestamp":"1669846860.0","comment_id":"732047","content":"Answer C"}],"topic":"1","answer":"C","question_id":187,"unix_timestamp":1669846860,"isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/89464-exam-aws-certified-database-specialty-topic-1-question-267/"},{"id":"b5Z2KgieWSyevctWBIVT","answer_ET":"C","answer_description":"","question_images":[],"question_text":"A company is using an Amazon RDS Multi-AZ DB instance in its development environment. The DB instance uses General Purpose SSD storage. The DB instance provides data to an application that has I/O constraints and high online transaction processing (OLTP) workloads. The users report that the application is slow.\n\nA database specialist finds a high degree of latency in the database writes. The database specialist must decrease the database latency by designing a solution that minimizes operational overhead.\n\nWhich solution will meet these requirements?","timestamp":"2022-12-05 22:40:00","exam_id":22,"answers_community":["C (100%)"],"choices":{"A":"Eliminate the Multi-AZ deployment. Run the DB instance in only one Availability Zone","D":"Recreate the DB instance. Use Provisioned IOPS SSD storage. Reload the data from an automatic snapshot","B":"Recreate the DB instance. Use the default storage type. Reload the data from an automatic snapshot","C":"Switch the storage to Provisioned IOPS SSD on the DB instance that is running"},"discussion":[{"timestamp":"1705224420.0","poster":"MultiAZ","comment_id":"1122423","content":"Selected Answer: C\nC seems to be the answer.\nA can also help, especially for Dev. However, we do not have enough information to rule out the Multi-AZ. Also, the question does not mention price so C will do","upvote_count":"2"},{"comments":[{"comment_id":"1191831","content":"I agree with you. But be careful about what storage type your RDS instance migrates from or to. Any conversion between magnetic and SSD causes downtime according to the above website. It isn't applied to this case, though, just for your information.","timestamp":"1712609700.0","upvote_count":"1","poster":"koki2847"}],"comment_id":"1000911","timestamp":"1694027280.0","content":"Selected Answer: C\nC. Switch the storage to Provisioned IOPS SSD \n\nYou can change the settings of a DB instance to accomplish tasks such as adding additional storage or changing the DB instance class.\n\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.DBInstance.Modifying.html","poster":"Pranava_GCP","upvote_count":"3"},{"upvote_count":"3","poster":"Mardoyyy","content":"Selected Answer: C\nans is C,\nchanging from GP into privisoned IOPS does NOT make downtime and could apply immediately","comment_id":"744768","timestamp":"1670998020.0"},{"poster":"catboy","content":"D right answer","timestamp":"1670285340.0","comments":[{"comment_id":"738573","upvote_count":"1","timestamp":"1670468580.0","content":"Storage type can be modified.\nStorage type\n\nThe storage type that you want to use.\nIf you choose General Purpose SSD (gp3), you can provision additional Provisioned IOPS and Storage throughput under Advanced settings.\nIf you choose Provisioned IOPS SSD (io1), enter the Provisioned IOPS value.\nAfter Amazon RDS begins to modify your DB instance to change the storage size or type, you can't submit another request to change the storage size, performance, or type for six hours.\n\nFor more information, see Amazon RDS storage types\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.DBInstance.Modifying.html","poster":"Sab"},{"poster":"lollyj","content":"Answer C: I've tested and the storage and instance classes can be modified on the fly without downtime","timestamp":"1671039480.0","comment_id":"745333","upvote_count":"5"}],"upvote_count":"1","comment_id":"736418"},{"poster":"Sab","content":"Selected Answer: C\nC\nYou can modify storage class. There will be performance issue while storage is moved to new class.","comment_id":"736340","upvote_count":"1","timestamp":"1670276400.0"}],"topic":"1","answer":"C","unix_timestamp":1670276400,"isMC":true,"question_id":188,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/90144-exam-aws-certified-database-specialty-topic-1-question-268/"},{"id":"Lb2YpeHdCaMyT12Brftd","url":"https://www.examtopics.com/discussions/amazon/view/90812-exam-aws-certified-database-specialty-topic-1-question-269/","topic":"1","answer_images":[],"question_id":189,"isMC":true,"unix_timestamp":1670595900,"timestamp":"2022-12-09 15:25:00","answer_ET":"B","question_text":"A company wants to migrate its on-premises Oracle database to a managed open-source database engine in Amazon RDS by using AWS Database Migration Service (AWS DMS). A database specialist needs to identify the target engine in Amazon RDS based on the conversion percentage of database code objects such as stored procedures, functions, views, and database storage objects. The company will select the engine that has the least manual conversion effort.\n\nWhat should the database specialist do to identify the target engine?","answers_community":["B (78%)","A (22%)"],"answer_description":"","question_images":[],"answer":"B","choices":{"B":"Use the AWS Schema Conversion Tool (AWS SCT) multiserver assessor","C":"Use an AWS DMS pre-migration assessment","D":"Use the AWS DMS data validation tool","A":"Use the AWS Schema Conversion Tool (AWS SCT) database migration assessment report"},"discussion":[{"comment_id":"876060","content":"Selected Answer: B\nThe need is to identify appropriate target db. To determine the best target direction for your overall environment, create a multiserver assessment report.\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html","poster":"SeemaDataReader","upvote_count":"5","timestamp":"1682032320.0"},{"poster":"Sathish_dbs","content":"Selected Answer: A\nwhy do you need multi server when you want to analyse a single database and the single server SCT assessment also provide recommendation for target DB","comment_id":"1027765","upvote_count":"1","timestamp":"1696746120.0"},{"poster":"Pranava_GCP","upvote_count":"2","comment_id":"1000946","content":"Selected Answer: B\nB. Use AWS SCT multiserver assessment\n\n\"To determine the best target direction for your overall environment, create a multiserver assessment report.\nA multiserver assessment report evaluates multiple servers based on input that you provide for each schema definition that you want to assess. Your schema definition contains database server connection parameters and the full name of each schema. After assessing each schema, AWS SCT produces a summary, aggregated assessment report for database migration across your multiple servers. This report shows the estimated complexity for each possible migration target.\"\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html","timestamp":"1694030640.0"},{"comments":[{"timestamp":"1685105160.0","poster":"cnmc","content":"The \"multi\" part is the target... For example are you moving to RDS Postgres or Aurora Postgres. You need to read carefully before misleading people with comments that show you clearly do not understand anything","comment_id":"907360","upvote_count":"4"}],"poster":"redman50","upvote_count":"1","content":"Selected Answer: A\nDefinitely A. Where does it say multiserver assessor which is used to assess multiple databases in a single instance or multiple instances.","comment_id":"853469","timestamp":"1680022920.0"},{"timestamp":"1679661900.0","content":"Selected Answer: A\nI think it might be option B, the question talks about just one single Oracle database server, no multiple servers","poster":"dougporto1988","upvote_count":"1","comment_id":"849291"},{"poster":"OCHT","upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html","timestamp":"1675001820.0","comment_id":"791704"},{"content":"Selected Answer: A\nA.\nThis is a single Oracle Database according to the question, so, no need to go multiserver. It will provide effort on converting such objects in the database migration assessemnt report:\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Summary.html","poster":"tucobbad","comment_id":"761467","upvote_count":"1","timestamp":"1672345860.0"},{"poster":"khun","content":"Selected Answer: B\nB.\nA multiserver assessment report evaluates multiple servers based on input that you provide for each schema definition that you want to assess. Your schema definition contains database server connection parameters and the full name of each schema. After assessing each schema, AWS SCT produces a summary, aggregated assessment report for database migration across your multiple servers. This report shows the estimated complexity for each possible migration target.","comment_id":"750448","timestamp":"1671507300.0","upvote_count":"2"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_AssessmentReport.Multiserver.html","comment_id":"740198","upvote_count":"3","timestamp":"1670595900.0","poster":"amulbaba"}],"exam_id":22},{"id":"IBFm2bU2tA8HOYmzbzGR","topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/25515-exam-aws-certified-database-specialty-topic-1-question-27/","answer_description":"","answer_images":[],"choices":{"A":"Restore a snapshot from the production cluster into test clusters","B":"Create logical dumps of the production cluster and restore them into new test clusters","D":"Add an additional read replica to the production cluster and use that node for testing","C":"Use database cloning to create clones of the production cluster"},"exam_id":22,"question_id":190,"answers_community":["C (100%)"],"question_text":"A company is about to launch a new product, and test databases must be re-created from production data. The company runs its production databases on an\nAmazon Aurora MySQL DB cluster. A Database Specialist needs to deploy a solution to create these test databases as quickly as possible with the least amount of administrative effort.\nWhat should the Database Specialist do to meet these requirements?","question_images":[],"unix_timestamp":1594573860,"timestamp":"2020-07-12 19:11:00","answer":"C","discussion":[{"content":"C. \nhttps://aws.amazon.com/getting-started/hands-on/aurora-cloning-backtracking/\n\"Cloning an Aurora cluster is extremely useful if you want to assess the impact of changes to your database, or if you need to perform workload-intensive operations—such as exporting data or running analytical queries, or simply if you want to use a copy of your production database in a development or testing environment. You can make multiple clones of your Aurora DB cluster. You can even create additional clones from other clones, with the constraint that the clone databases must be created in the same region as the source databases.","comment_id":"139876","upvote_count":"17","timestamp":"1634318160.0","poster":"BillyMadison"},{"timestamp":"1690899960.0","poster":"IhorK","content":"Selected Answer: C\nWhen we do cloning, there is no physical overwriting of data to a new location, only meta-data is copied. Therefore, the cloning process is quite fast. However, further changes are isolated from each other in source and clone.","upvote_count":"1","comment_id":"969139"},{"timestamp":"1679998080.0","upvote_count":"1","poster":"ken_test1234","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html","comment_id":"853086"},{"poster":"f___16","content":"Selected Answer: C\nCloning is the best choice. It creates a new db cluster with the same data.","comment_id":"830603","timestamp":"1678086300.0","upvote_count":"1"},{"content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\nCreating a clone is faster and more space-efficient than physically copying the data using other techniques, such as restoring a snapshot.","timestamp":"1664469000.0","comment_id":"682848","poster":"Jiang_aws1","upvote_count":"2"},{"timestamp":"1651367040.0","upvote_count":"2","comment_id":"595376","poster":"novice_expert","content":"Selected Answer: C\nC. Use database cloning to create clones of the production cluster\n\nCloning is best choice with \"copy-on-write\" protocol, database becomes available in a few mins.\n\nhttps://aws.amazon.com/getting-started/hands-on/aurora-cloning-backtracking/\n\"Cloning an Aurora cluster is extremely useful if you want to assess the impact of changes to your database, or if you need to perform workload-intensive operations—such as exporting data or running analytical queries, or simply if you want to use a copy of your production database in a development or testing environment. You can make multiple clones of your Aurora DB cluster. You can even create additional clones from other clones, with the constraint that the clone databases must be created in the same region as the source databases."},{"poster":"AriraAWS","content":"Selected Answer: C\nCloning is best choice with \"copy-on-write\" protocol, database becomes available in a few mins.","upvote_count":"2","comment_id":"539863","timestamp":"1643910000.0"},{"upvote_count":"2","comment_id":"314795","content":"Answer C","timestamp":"1635907860.0","poster":"LMax"},{"content":"Ans: C","poster":"myutran","comment_id":"297913","upvote_count":"1","timestamp":"1635883440.0"},{"poster":"JobinAkaJoe","timestamp":"1635869340.0","upvote_count":"1","comment_id":"253149","content":"Answer is C\nAurora copy-on-write clones are best suited for this requirement"},{"poster":"BillyC","content":"Yes, C","comment_id":"165213","timestamp":"1635009960.0","upvote_count":"2"},{"timestamp":"1634037960.0","comment_id":"135572","upvote_count":"3","content":"C Database Clone is quickest and the right approach. Takes secs..","poster":"jnassp1"},{"content":"C is correct as we have least admin effort and quck","comment_id":"135404","poster":"Mickysingh","timestamp":"1633906980.0","upvote_count":"3"},{"content":"D here!","comment_id":"134226","upvote_count":"1","timestamp":"1633680000.0","poster":"BillyC"},{"comment_id":"133092","upvote_count":"2","timestamp":"1633193520.0","comments":[{"upvote_count":"5","timestamp":"1634743500.0","comment_id":"153509","content":"Restoring from snapshot is never quick.\nAnswer is C","poster":"Ebi"}],"poster":"[Removed]","content":"D is neither least administrative nor will it meet the requirement. A is simplest for me"}],"answer_ET":"C"}],"exam":{"isBeta":false,"id":22,"isMCOnly":false,"provider":"Amazon","isImplemented":true,"numberOfQuestions":359,"lastUpdated":"11 Apr 2025","name":"AWS Certified Database - Specialty"},"currentPage":38},"__N_SSP":true}