{"pageProps":{"questions":[{"id":"mx6rDJTEQnjiv4SFLDWq","unix_timestamp":1604271960,"question_id":391,"answer_images":[],"answer":"A","question_images":[],"exam_id":32,"answer_description":"","timestamp":"2020-11-02 00:06:00","topic":"1","answer_ET":"A","answers_community":[],"question_text":"An organization has a write-intensive mobile application that uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The application has scaled well, however, costs have increased exponentially because of higher than anticipated Lambda costs. The application's use is unpredictable, but there has been a steady 20% increase in utilization every month.\nWhile monitoring the current Lambda functions, the Solutions Architect notices that the execution-time averages 4.5 minutes. Most of the wait time is the result of a high-latency network call to a 3-TB MySQL database server that is on-premises. A VPN is used to connect to the VPC, so the Lambda functions have been configured with a five-minute timeout.\nHow can the Solutions Architect reduce the cost of the current architecture?\nA.\n✑ Replace the VPN with AWS Direct Connect to reduce the network latency to the on-premises MySQL database.\n✑ Enable local caching in the mobile application to reduce the Lambda function invocation calls.\n✑ Monitor the Lambda function performance; gradually adjust the timeout and memory properties to lower values while maintaining an acceptable execution time.\n✑ Offload the frequently accessed records from DynamoDB to Amazon ElastiCache.\nB.\n✑ Replace the VPN with AWS Direct Connect to reduce the network latency to the on-premises MySQL database.\n✑ Cache the API Gateway results to Amazon CloudFront.\n✑ Use Amazon EC2 Reserved Instances instead of Lambda.\n✑ Enable Auto Scaling on EC2, and use Spot Instances during peak times.\n✑ Enable DynamoDB Auto Scaling to manage target utilization.\nC.\n✑ Migrate the MySQL database server into a Multi-AZ Amazon RDS for MySQL.\n✑ Enable caching of the Amazon API Gateway results in Amazon CloudFront to reduce the number of Lambda function invocations.\n✑ Monitor the Lambda function performance; gradually adjust the timeout and memory properties to lower values while maintaining an acceptable execution time.\n✑ Enable DynamoDB Accelerator for frequently accessed records, and enable the DynamoDB Auto Scaling feature.\nD.\n✑ Migrate the MySQL database server into a Multi-AZ Amazon RDS for MySQL.\n✑ Enable API caching on API Gateway to reduce the number of Lambda function invocations.\n✑ Continue to monitor the AWS Lambda function performance; gradually adjust the timeout and memory properties to lower values while maintaining an acceptable execution time.\n✑ Enable Auto Scaling in DynamoDB.","isMC":false,"url":"https://www.examtopics.com/discussions/amazon/view/35686-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"content":"Answer is D. Because this is a write-intensive application, it makes sense to cache post, put calls in API Gateway. if it was a read intensive application then using CloudFront or even the client-side cache would have helped. So the answer is D.","upvote_count":"21","comments":[{"comment_id":"385171","comments":[{"poster":"student2020","timestamp":"1635225180.0","upvote_count":"3","comment_id":"403476","content":"I dont see how implementing a DX solution is cost effective. I think D is more cost effective. There is no mention of which part of the application has scaled well, so implementing auto-scaling on dynamodb is an acceptable answer"}],"content":"I would agree with D but my only concern is the \"The application has scaled well\" and in C-D solutions, we are enabling again the Autoscale of DynamoDB. It doesn't make any sense. I believe is A because it's also the most cost-effective answer.","poster":"NickGR","upvote_count":"2","timestamp":"1635122640.0"}],"timestamp":"1633321320.0","comment_id":"254519","poster":"Bulti"},{"upvote_count":"13","content":"D is my answer","comment_id":"267637","timestamp":"1633338180.0","poster":"Ebi"},{"content":"D is good Answer","comment_id":"1107647","poster":"BKhan","upvote_count":"1","timestamp":"1703757120.0"},{"timestamp":"1693097700.0","poster":"TravelKo","upvote_count":"1","comment_id":"991156","content":"I would go with D."},{"timestamp":"1687375500.0","upvote_count":"1","poster":"SkyZeroZx","comment_id":"929849","content":"I will go with D."},{"content":"The answer is A.\nHere the key word is \"reduce the cost of the current architecture\".\nIf you migrate 3TB MySQL to RDS, the Database Storage for Multi-AZ cost will cost $700 per month, which will result in a cost increase.","comment_id":"679211","poster":"fanq10","comments":[{"upvote_count":"1","poster":"fanq10","timestamp":"1664138220.0","content":"continue the previous post.....\nIf using instance type db.t3.medium costs about $100 per month, plus the Database Storage cost, it can increase the cost by $800 per month, which does not count Data Transfer cost yet. \nSo the Answer is A.","comment_id":"679215"},{"comment_id":"701353","timestamp":"1666417860.0","content":"Using rds reduce the tco cost of your datacenter for database. It's true that it will add rds cost, but reducing server tco plus dx solution is not cheap.","upvote_count":"1","poster":"examaws"}],"upvote_count":"1","timestamp":"1664137980.0"},{"timestamp":"1661956860.0","comment_id":"655198","content":"Correct Answer: D","poster":"Sizuma","upvote_count":"2"},{"timestamp":"1652878800.0","poster":"user0001","content":"A is wrong because write-intensive and this option use elastic cache","comment_id":"603293","upvote_count":"2"},{"upvote_count":"1","poster":"Kuang","timestamp":"1646714580.0","content":"D is my answer","comment_id":"563025"},{"content":"D.\n✑ Migrate the MySQL database server to Amazon RDS for MySQL Multi-AZ.\n✑ Enable API caching on the API Gateway to minimize Lambda function calls.\n✑ Continue to monitor the performance of the AWS Lambda function; progressively reduce the timeout and memory attributes while keeping an acceptable execution time.\n✑ Enable DynamoDB's auto scaling.","poster":"cldy","comment_id":"498612","timestamp":"1639139280.0","upvote_count":"1"},{"content":"D is more cost effective for sure","upvote_count":"2","comment_id":"493773","timestamp":"1638632820.0","poster":"AzureDP900"},{"timestamp":"1636025700.0","content":"It's D","comment_id":"450605","poster":"andylogan","upvote_count":"1"},{"content":"D.\nA make cost higher by direct connect.","poster":"nodogoshi","timestamp":"1635977940.0","upvote_count":"1","comment_id":"447883"},{"poster":"student22","comment_id":"440843","content":"Answer is D.\n\nMy main reason for selecting D over C is because C uses DynamoDB Accelerator which adds to the cost but might not be that effective because this is a 'write-intensive' app.","upvote_count":"2","timestamp":"1635560340.0"},{"content":"DDD\n---","poster":"tgv","upvote_count":"1","timestamp":"1635501360.0","comment_id":"437817"},{"timestamp":"1635449160.0","content":"D seems the best. Using DX is going to take a while to implement and may not be cheaper than the VPN. \n\nCaching at API Gateway is preferred. \n\nThis is a tough question, it gave me a headache reading it lol","poster":"AWS_Noob","comment_id":"430230","upvote_count":"2"},{"poster":"Akhil254","upvote_count":"1","comment_id":"406618","content":"D Correct","timestamp":"1635336600.0"},{"comment_id":"385526","content":"I go for D","timestamp":"1635166380.0","upvote_count":"1","poster":"rain_wu"},{"timestamp":"1635071040.0","upvote_count":"1","content":"I'll go with D","poster":"WhyIronMan","comment_id":"354815"},{"comment_id":"344432","timestamp":"1634993100.0","content":"I will go with D.","poster":"blackgamer","upvote_count":"3"},{"content":"D is correct","upvote_count":"4","comment_id":"343914","poster":"Waiweng","timestamp":"1634916900.0"},{"content":"A seems correct. It addresses the latency problem. \n\nC & E are out because DX is cheaper than Multi-AZ RDS (not stated as requirement anyway)\n* 3 TB multi-AZ RDS instance (db.m1.medium, 3 TB storage, multi-AZ): ~950 \n* 1GB DX connection with 1 TB monthly outbound data transfer: ~340\nDon't believe me, check for yourself using the AWS pricing calculator: https://calculator.aws/#/createCalculator/RDSMySQL\nSee here for DX example: https://aws.amazon.com/getting-started/hands-on/connect-data-center-to-aws/services-costs/ \n\nB addresses the latency issue but includes CloudFront and EC2 instances which won't help reduce cost.","comment_id":"333126","upvote_count":"4","comments":[{"comment_id":"425925","poster":"somebodyelse","timestamp":"1635438780.0","content":"D. I think the TCO with everything on AWS would be less....\nPerhaps the the TCO is more complex than choosing between DX and RDS? RDS would arguably be less expensive than the on-prem equivalent, removing VPN takes out cost and co-locating the DB with Lambda might increase the bandwidth between the two by 10-100X, this would reduce Lambda runtime and cost (which was the stated major cost blowout).","upvote_count":"1"},{"upvote_count":"2","content":"Short term maybe... but \"but there has been a steady 20% increase in utilization every month.\" so if you look at this in a long run:\n+ need to increase bandwidth on DX \n+Internet traffic constantly increasing which adds to the price\n+ operational overhead \n-------AWS is all about using their services, and from architectural point of view moving DB to RDS is is the logical way + RDS MySQL is not that expansive compared to Aurora.","poster":"peddyua","comment_id":"511753","timestamp":"1640748720.0"}],"poster":"sarah_t","timestamp":"1634913900.0"},{"comment_id":"327764","upvote_count":"2","poster":"aishvary123","content":"A is the answer","timestamp":"1634249580.0"},{"comment_id":"322677","poster":"ExtHo","upvote_count":"1","content":"D is the best choice\n\nMigrating the on-premises MySQL server to Amazon RDS provides the best latency for the Lambda functions which will significantly reduce the cost for execution time. API Gateway can cache the API request to reduce the Lambda invocation which can reduce the cost further. Auto Scaling for DynamoDB also reduces the cost by provisioning capacity depending on the current user traffic.","timestamp":"1634193780.0"},{"poster":"Pupu86","timestamp":"1633790400.0","content":"Since it is a \"write-intensive\" application from on-premise to AWS. it would be better to move the DB to AWS and with a steady increase in utilization of 20% month to month, there will be a time when the DX link capacity is insufficient to handle the load which will cost more than the DB migration (with an EC2 instance). So my answer is D.","comment_id":"308474","upvote_count":"1"},{"upvote_count":"2","comment_id":"293071","poster":"Kian1","timestamp":"1633558080.0","content":"going with D"},{"poster":"tipzzz","content":"i think answer is B, cause The application's use is unpredictable, but there has been a steady 20% increase in utilization every month.\nAWS EC2 instance is more cost-effective than AWS Lambda when dealing with high traffic\nWe will need a highter bandwith, so dx can be a good choice.\nDynamo autoscaling is ok for unpredictable use","upvote_count":"1","comment_id":"281110","timestamp":"1633369440.0"},{"content":"D since I don't thin Direct connect will reduce any cost","poster":"kopper2019","timestamp":"1633331880.0","comment_id":"259134","upvote_count":"4"},{"upvote_count":"2","poster":"T14102020","content":"Correct is D. Migrate RDS + API Gateway cache without CloudFront","comment_id":"244821","timestamp":"1633309560.0"},{"comment_id":"233487","poster":"srinivasa","comments":[{"content":"\"Most of the wait time is the result of a high-latency network call to a 3-TB MySQL database server that is on-premises\"\nSwitching to RDS will drastically reduce latency, therefore Lambda execution time, therefore cost","comment_id":"235980","timestamp":"1633030980.0","upvote_count":"3","poster":"gbrnq"}],"timestamp":"1632784020.0","upvote_count":"1","content":"How does Multi-AZ MySQL in AWS would reduce cost ?"},{"upvote_count":"3","comment_id":"232541","poster":"jackdryan","content":"I'll go with D","timestamp":"1632664860.0"},{"upvote_count":"3","comment_id":"227112","poster":"Dhananjaya","timestamp":"1632337320.0","content":"I would Say D. Enabling local cache does not look effective.\nhttps://acloud.guru/forums/aws-csa-pro-2019/discussion/-Lu6-JtAB2HnNjCHbazK/High%20Lambda%20execution%20time"},{"comment_id":"211005","upvote_count":"1","comments":[{"content":"good point, I change to A","comments":[{"upvote_count":"5","content":"D should be the answer - Cache is useful for frequent READ and not for WRITE.","comment_id":"231521","timestamp":"1632495180.0","poster":"cloudgc"}],"upvote_count":"2","poster":"Gmail78","timestamp":"1632219780.0","comment_id":"214943"}],"poster":"bbnbnuyh","content":"Write-intensive application meaning A is the best answer between A & D , https://cloudonaut.io/serverless-cache-for-dynamodb-with-elasticache/","timestamp":"1632175560.0"},{"poster":"Gmail78","comment_id":"210833","timestamp":"1632138840.0","upvote_count":"1","content":"A or D. \nA - the Direct Connect will decrease latency, not sure how the App cache will reduce Lambda invocations.\nD - API cache is definitely the key to reduce Lambda invocations. Not sure how RDS will assist on reducing the cost. Definitely better architecture.\nI vote for D"}]},{"id":"K4vomVo7oQz1GENFLDaI","answer_ET":"B","exam_id":32,"answers_community":["D (67%)","C (33%)"],"timestamp":"2019-09-23 20:20:00","unix_timestamp":1569262800,"question_text":"A company runs a video processing platform. Files are uploaded by users who connect to a web server, which stores them on an Amazon EFS share. This web server is running on a single Amazon EC2 instance. A different group of instances, running in an Auto Scaling group, scans the EFS share directory structure for new files to process and generates new videos (thumbnails, different resolution, compression, etc.) according to the instructions file, which is uploaded along with the video files. A different application running on a group of instances managed by an Auto Scaling group processes the video files and then deletes them from the\nEFS share. The results are stored in an S3 bucket. Links to the processed video files are emailed to the customer.\nThe company has recently discovered that as they add more instances to the Auto Scaling Group, many files are processed twice, so image processing speed is not improved. The maximum size of these video files is 2GB.\nWhat should the Solutions Architect do to improve reliability and reduce the redundant processing of video files?","url":"https://www.examtopics.com/discussions/amazon/view/5616-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"D","isMC":true,"discussion":[{"timestamp":"1632292560.0","content":"D is my choice.\nA & B are incorrect: Web is placed on single EC2 that is not HA, hosting the web on S3 will help to improve the reliability.\nC: lambda function should not be used ot process the video, it's suitable for short execution.\nD is best choice and SQS contains link of S3 with instruction is applied a lot in real world.","poster":"chaudh","comments":[{"comment_id":"18440","content":"i think lambda can be used to process the video. to wont take more than 15mins to process a video. D is not correct, it may replicate and the req. says clearly that we need to remove the redundancy","timestamp":"1632308760.0","poster":"AWS2020","upvote_count":"1","comments":[{"comment_id":"243618","content":"not remove but reduce.","upvote_count":"1","timestamp":"1634573460.0","poster":"newme"}]},{"poster":"9Ow30","timestamp":"1632592740.0","content":"I also vote for D\nIt has all the things as standard practice.\nKeep data in S3/ pointer in SQS.\nProcess data via lambda on trigger and use SQS queue length as a Auto scaling driver.","comment_id":"28851","upvote_count":"3"},{"poster":"Sunflyhome","comments":[{"comment_id":"410942","upvote_count":"2","poster":"Jupi","content":"The Lambda function is not for video processing, it just trigger the video processing. So timeout shouldn't be an issue.","timestamp":"1635980820.0"}],"upvote_count":"1","timestamp":"1635403080.0","comment_id":"325363","content":"Lambda has 5 minutes timeout. Video processing is a time-cost process. \nNo way to use Lambda to do de-coding or en-coding of video file (except it's very small like a couple of hundreds MB). \nD is a better than A in term of HA, Let AWS handle s3 stability than a single instance in A."},{"comments":[{"upvote_count":"3","content":"Lambda can be triggered directly from S3 when a file is uploaded.\n\nA has the web app on one single EC2 instance. Hardly a good architecture.","timestamp":"1635593760.0","comment_id":"333137","poster":"sarah_t"}],"timestamp":"1635404520.0","comment_id":"331153","poster":"SD13","upvote_count":"3","content":"D is missing the part \"How the lambda is triggered?\" and why do we rewrite the app? Just app modification would be fine. Going with A"},{"comment_id":"148421","content":"How can run a application from S3? C & D is incorrect because of this.","timestamp":"1633787100.0","poster":"AICOO","upvote_count":"3","comments":[{"upvote_count":"3","content":"yes, of course, you can. javascript based web app.","timestamp":"1634070960.0","poster":"b3llman","comment_id":"171366"},{"upvote_count":"2","timestamp":"1635645780.0","content":"https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-photo-album.html","poster":"sarah_t","comment_id":"333148"}]}],"comment_id":"16158","upvote_count":"29"},{"comment_id":"22312","upvote_count":"12","comments":[{"poster":"superuser784","comment_id":"708651","upvote_count":"1","content":"Good point, I also ruled out the API Gateway because it can last only 29 seconds, and not all internet connections are that fast enough to upload 2GB within that timeframe.","timestamp":"1667240520.0"}],"content":"C is incorrect. API gateway cannot be used to upload the 2GB video file to S3 as \"API Gateway supports a reasonable payload size limit of 10MB.\" https://sookocheff.com/post/api/uploading-large-payloads-through-api-gateway/\n\nI support D","timestamp":"1632369540.0","poster":"Frank1"},{"upvote_count":"1","comment_id":"1323152","timestamp":"1733583960.0","poster":"mnsait","content":"Selected Answer: C\nThe question calls out that \"many files are processed twice\". SQS does not commit on avoiding duplication. An issue with A, B and D is all of these use SQS.\n\nWhile C avoids SQS, it uses Lambda to process and store new videos and hence not the right solution either unless the videos are quite small. \n\nGiven the trade off, I choose C since the info on the size of videos is not mentioned."},{"upvote_count":"1","comments":[{"comment_id":"766215","poster":"maxh8086","content":"My Bad, its C","timestamp":"1672886760.0","upvote_count":"1"}],"timestamp":"1672886460.0","poster":"maxh8086","content":"https://youtu.be/rCTBXrV3EY8\nD","comment_id":"766212"},{"timestamp":"1671429840.0","content":"None of the answers work. Try it: either no email(A,D) or still duplicate processing(B), or limits are breached(C), cloudwatch trigger not possible(A) or lambda is called magically without any trigger at all(D) .\nSo might be A, C or D,","comment_id":"749512","poster":"hobokabobo","upvote_count":"1"},{"comment_id":"717477","content":"B - Solution Architects don't rewrite applications.","timestamp":"1668367680.0","poster":"LrdKanien","upvote_count":"3"},{"upvote_count":"1","timestamp":"1668082620.0","content":"difficult question, but I will choose A, this link helped in the choice: https://aws.amazon.com/blogs/media/processing-user-generated-content-using-aws-lambda-and-ffmpeg/","comment_id":"715206","poster":"resnef"},{"timestamp":"1668066000.0","poster":"Netaji","content":"SQS avoids the dup messages , if web site is static the \" D\"\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html","comment_id":"715008","upvote_count":"1"},{"content":"It's D for me","timestamp":"1666650540.0","poster":"mrgreatness","upvote_count":"1","comment_id":"703398"},{"comment_id":"507283","timestamp":"1640196060.0","content":"I'll go with D","upvote_count":"1","poster":"AzureDP900"},{"timestamp":"1639047180.0","upvote_count":"1","poster":"cldy","content":"D. Rewrite the web application to run from Amazon S3 and upload the video files to an S3 bucket. Each time a new file is uploaded, trigger an AWS Lambda function to put a message in an SQS queue containing the link and the instructions. Modify the video processing application to read from the SQS queue and the S3 bucket. Use the queue depth metric to adjust the size of the Auto Scaling group for video processing instances.","comment_id":"497636"},{"content":"Selected Answer: D\nuse SQS for the de coupling and S3 to be the storage.","upvote_count":"2","poster":"acloudguru","timestamp":"1637711400.0","comment_id":"485501"},{"comment_id":"450610","upvote_count":"1","timestamp":"1636224480.0","poster":"andylogan","content":"It's D"},{"content":"B is correct. \nA & D use SQS, in which redundant processing is possible. The main issue here is avoid redundant processing.\nC - Using API gateway to upload to S3 is unnecessary\nB is correct because - here you are avoiding to rewrite the application by simply syncing EFS files to S3 with a cron. There is no requirement to immediately process the files, so using cron is not a bad idea.","timestamp":"1636118580.0","comment_id":"440801","upvote_count":"4","poster":"chand0401"},{"upvote_count":"1","comment_id":"435630","poster":"tgv","timestamp":"1636088760.0","content":"DDD\n---"},{"upvote_count":"1","comment_id":"406624","poster":"Akhil254","content":"D Correct","timestamp":"1635850860.0"},{"comment_id":"368203","poster":"zolthar_z","comments":[{"content":"Also, A is not HA - uses a single EC2 instance to host the web site.","comment_id":"440853","upvote_count":"1","poster":"student22","timestamp":"1636194840.0","comments":[{"poster":"student22","upvote_count":"2","content":"Agree with D","comment_id":"440854","timestamp":"1636196640.0"}]},{"comment_id":"398974","poster":"vinodhg","content":"But you don't know if the website is static, so how sure are you to host it in S3?","upvote_count":"1","timestamp":"1635845520.0"}],"upvote_count":"3","timestamp":"1635835320.0","content":"Is a really hard question, A & D looks good but I will go with D for one reason. Trigger a lambda using S3 events is easiest than configure cloudwatch event."},{"poster":"WhyIronMan","content":"I'll go with A","upvote_count":"3","timestamp":"1635826920.0","comment_id":"354818"},{"upvote_count":"1","comment_id":"344435","poster":"blackgamer","timestamp":"1635794460.0","content":"Answer is D."},{"poster":"Waiweng","timestamp":"1635788880.0","content":"It's D","comment_id":"343935","upvote_count":"3"},{"comment_id":"331706","poster":"Amitv2706","upvote_count":"2","content":"Best Answer is D.\n\nHere is the explanation why:\n\nA - CloudWatch Events cannot trigger a Lambda function upon a new file upload in S3.\nB - These is no mechanism to reprocess the message if lambda function fails to process the video file\nC - API gateway can support max file size of 10 MB while upload. here its 2 GB","timestamp":"1635500700.0"},{"comment_id":"309894","upvote_count":"1","poster":"ItsmeP","content":"D is correct. \nA - Cloud watch to trigger is incorrect, it should be triggered from File put object\nB/C - Incorrect as Lambda can't process 2 GB file (without EFS that we can ignore as its new functionality and question is old)\nD","timestamp":"1635370380.0"},{"content":"since the web application purely stores the \"unprocessed\" videos in EFS, I was inclined to choose D as it helps to avoid a single instance bottleneck even though it's not really mentioned as a potential flaw. The key was to reduce duplicate processing and option D mentions \"Modify the video processing application to read from SQS and S3\" - I am assuming this is considered \"dual-processing\" of the same video file and defeats the purpose of having the SQS implemented. So I'm going with A instead.","comment_id":"308484","upvote_count":"2","comments":[{"content":"The processing application gets the instructions and pointer from SQS and reads the file from S3. That is not dual processing. \n\nDual processing happens when an app \"discovers\" an already processed file before it is deleted and processes it again. It can effectively be prevented by setting the appropriate visibility timeout in SQS.","poster":"sarah_t","upvote_count":"4","comment_id":"333142","timestamp":"1635612000.0"}],"poster":"Pupu86","timestamp":"1635327480.0"},{"content":"D.\n(X) A. should be to be enabled object-level API on CloudTrail to be received to CW events.\n(X) C. API Gateway filesize is limited to10MB.","upvote_count":"2","timestamp":"1635201180.0","poster":"gpark","comment_id":"294750"},{"content":"Answer Is C. Use Apigateway to get the signed URL and directly upload the data to S3.\nhttps://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/","upvote_count":"1","comment_id":"292308","timestamp":"1635138240.0","poster":"AJBA"},{"upvote_count":"1","comment_id":"291116","content":"Go with A, we don't need to rewrite the application.","poster":"wind","timestamp":"1635133380.0"},{"timestamp":"1635086220.0","poster":"Kian1","content":"Will go with A","comment_id":"289523","upvote_count":"3"},{"content":"A and D both are correct, but the best answer is D.\nB: Not a good solution using cron \nC: API Gateway has the limit of payload size, does not support 2GB download","upvote_count":"5","poster":"Ebi","timestamp":"1634997420.0","comment_id":"286554"},{"comments":[{"upvote_count":"1","comment_id":"289057","content":"on second thought, i will go with D","timestamp":"1635047340.0","poster":"bnagaraja9099"}],"poster":"bnagaraja9099","timestamp":"1634875020.0","comment_id":"284909","upvote_count":"1","content":"I will go with C. I think API Gateway is mentioned because the question states a single instance webserver to upload files to EFS. API GAteway to S3 makes it more reliable. A is a good option for decoupling, but SQS can cause redundancy if not handled properly"},{"content":"It should be A.","timestamp":"1634842560.0","upvote_count":"2","comment_id":"275565","poster":"lobidrulla"},{"comment_id":"253902","content":"D\nA - Why CLoudWatch to trigger?\nB - Why to synchorize with S3 if processing from EFS\nC - gateway?\nD - S3 event can be used to trigger Lambda (insted cloud watch)","poster":"vipgcp","timestamp":"1634814900.0","upvote_count":"2"},{"comment_id":"247756","comments":[{"comment_id":"333150","poster":"sarah_t","upvote_count":"1","timestamp":"1635750240.0","content":"A static website is sufficient for this use case. \n\nExample: https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-photo-album.html"}],"timestamp":"1634802300.0","upvote_count":"2","content":"A have a more correct descriptive flow. D have some flaws like it is not pozcible to run application from S3 unless it is a static website.","poster":"happpieee"},{"comment_id":"246666","content":"I Will go with C\nno need to modify or rewrite the application. Cron job will fulfill the req. to sync EFS content into s3. SES can send notification using SNS topic\n\nhttps://docs.aws.amazon.com/ses/latest/DeveloperGuide/monitor-sending-activity-using-notifications-email.html","poster":"rscloud","timestamp":"1634798340.0","upvote_count":"1"},{"timestamp":"1634524260.0","content":"Correct answer is A.","poster":"T14102020","upvote_count":"1","comment_id":"242025"},{"timestamp":"1634440800.0","poster":"jackdryan","comment_id":"229405","upvote_count":"2","content":"I'll go with A"},{"content":"I HAVE GOOGLED AND LOOKD AT OTHER WEBSITES ITES B YOU IDOIEST PEOPLE GIVES ANSWQERS WITHOUT KNOWING THE TRUTH","poster":"shwanjaff","upvote_count":"2","comment_id":"229397","timestamp":"1634416200.0"},{"upvote_count":"1","content":"Correct answer:\nSet up a cron job on the web server instance to synchronize the contents of the EFS share into Amazon S3. Trigger an AWS Lambda function every time a file is uploaded to process the video file and store the results in Amazon S3. Using Amazon CloudWatch Events, trigger an Amazon SES job to send an email to the customer containing the link to the processed file.","poster":"shwanjaff","timestamp":"1634414700.0","comment_id":"229394"},{"content":"Answer is A. B is out as it is won't reduce duplicate processing as much as A would . C is out because if the API Gateway payload and lambda payload restriction. D is out because you can only run a static website from S3 and not a dynamic one that allows customers to upload videos.","poster":"Bulti","comment_id":"228471","timestamp":"1634346540.0","upvote_count":"3"},{"timestamp":"1634333580.0","comment_id":"227450","content":"I go with C","upvote_count":"1","poster":"petebear55"},{"poster":"SamuelK","comment_id":"219306","content":"The answer is D. C is not right because of the max payload size limit on the API Gateway.\nMaximum payload to API gateway is 10 MB and maximum payload for Lambda is 6 MB, which cannot be increased","comments":[{"timestamp":"1634382060.0","comment_id":"229309","upvote_count":"1","content":"C incorrect caused use lambda to process 2G file. \nabout upload file to S3 via API gateway: https://serverlessfirst.com/serverless-photo-upload-api/","poster":"binhdx"}],"timestamp":"1634272740.0","upvote_count":"2"},{"comment_id":"215832","upvote_count":"2","poster":"vjt","timestamp":"1634238120.0","content":"Api is out of picture as max size is 10 MB So there goes C. \nCloud watch event cannot trigger lambda upon a new file upload in s3 as this event pattern is not supported. There goes A. \nCan cron work? yes but it is less reliable.\nSo only correct answer is D for sure."},{"upvote_count":"2","poster":"onlinebaba","timestamp":"1634154240.0","content":"The solution needs to address two things\n1. Improve reliability, because there is single EC2 instance\n2. Reduce the redundant processing of video files, so that compute time is utilized efficiently\n\nIssue #1 can be solved by decommissioning of EC2 instances and powering the website from S3 leveraging API gateway for file-upload.\n\nIssue #2 can be solved by triggering an S3 event which invokes a Lambda passing links and instructions to process and generate new files. This will eliminate redundancy.\n\nSES can be used to send emails.\n\nIMO the answer is C","comment_id":"181217"},{"poster":"Ganfeng","upvote_count":"2","content":"I support D","timestamp":"1634121720.0","comment_id":"179235"},{"comment_id":"173713","poster":"khan11","timestamp":"1634113500.0","upvote_count":"2","content":"Cant be A....Note that SQS saves messages not videos.... the url has to come to sqs via lambda...then EC2 process the files from the S3 and EC2 uses the url to know which file to fetch... So.......D."},{"upvote_count":"1","poster":"fullaws","comment_id":"148940","timestamp":"1634004540.0","content":"is A and D"},{"content":"D and B can be correct, the only thing that D superior than B is more reliable in term of availability compare to ec2.","poster":"fullaws","timestamp":"1633938180.0","comment_id":"148939","upvote_count":"1"},{"poster":"SlinkySideWinder","content":"very tricky question, \nA- Doesn't mention how links are sent to users for viewing but sounds like an viable option\nC - sounds like the feasible solution but uploading 2gb files via the api gateway would be messy since it can handle average size of 10mb unless the api gateway sent the user a presigned url to upload the file directly which the answer don't mention\nD - doesn't mention how user will get link apart from via SQS queue which doesn't sound right unless it sends to SNS.\n\nI am torn but C is a contender","timestamp":"1633875180.0","comment_id":"148762","comments":[{"poster":"newme","upvote_count":"1","comment_id":"243631","timestamp":"1634682780.0","content":"This is what I'm concerned about A&D.\nBut still, C is incorrect, because of API Gateway payload limit."}],"upvote_count":"1"},{"poster":"IAmNotLambda","content":"The question specifically reads \"improve\" reliability. D, however, looks good yet it requires \"re-writing\" the application. Re-writing and putting data directly on S3 requires code testing etc. It can yield better results but I will personally stick with \"improving reliability\" .. A it is in this case.\n\nAny thoughts?","comments":[{"content":"They didn't mention any time frame to have the solution....... so, i would go with D","comment_id":"145307","upvote_count":"1","poster":"learner4ever","timestamp":"1633784940.0"}],"timestamp":"1633779720.0","comment_id":"138568","upvote_count":"1"},{"content":"D more likely","comment_id":"134208","poster":"NikkyDicky","timestamp":"1633774020.0","upvote_count":"1"},{"comment_id":"126902","content":"The question is \"reduce the redundant processing of video files\" not “Completely eliminated”. so FIFO is not necessary. \nBetween A&D, I agreed with Krishna2637. Go for A.","timestamp":"1633701360.0","poster":"ricoyao","upvote_count":"1"},{"comment_id":"123722","poster":"chicagomassageseeker","content":"going it option D. see the below link that explain howto deal with large object like 2gb files. It clearly says , SQS message can have the S3 object location ( which is missing in option A).\n\nYou can use Amazon S3 and the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages. This is especially useful for storing and consuming messages up to 2 GB in size. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queue, consider using Amazon S3 for storing your data.\n\nYou can use the Amazon SQS Extended Client Library for Java library to do the following:\n\nSpecify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB.\nSend a message that references a single message object stored in an Amazon S3 bucket.\nGet the corresponding message object from an Amazon S3 bucket.\nDelete the corresponding message object from an Amazon S3 bucket.\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html","upvote_count":"2","timestamp":"1633602660.0"},{"timestamp":"1633574700.0","upvote_count":"1","comment_id":"110249","content":"D would be my answer as the files would be read from S3 bucket not from the queue.","poster":"oatif"},{"poster":"meenu2225","comment_id":"102058","comments":[{"upvote_count":"1","timestamp":"1633332900.0","content":"On a thought, D is the right answer. As it does makes it more reliable.\nB and C are clearly not the options as they both mention SES to deliver email, SES is for bulk emailing.","comment_id":"105506","poster":"meenu2225"}],"upvote_count":"1","content":"It is A in my opinion.Because:\nB: It is just a bad design out of the lot.\nc: it says use \"SES\" to deliver an email to the user. SES is not meant for this purpose. SES is for bulk emailing.\nD: It says to run the website from S3, S3 can only host a static website. There is no mention of website being static. Moreover, re-writing the upload function (Option A) is way eaiser and straightfoward in comparision to re-writing the entire website.","timestamp":"1633318380.0"},{"timestamp":"1633271940.0","poster":"Merlin1","upvote_count":"1","content":"Im thinking D since Lambda would not an ideal solution for processing 2GB files.","comment_id":"98177"},{"upvote_count":"1","poster":"Jeb","comment_id":"97950","content":"The best choice is A as FIFO queues provide exactly-once processing, which means that each message is delivered once and remains available until a consumer processes it and deletes it. Duplicates are not introduced into the queue. C and D - entails a lot of effort and no need to rewrite. B - it is not reliable as you have to wait for the schedule.","timestamp":"1633264560.0"},{"comment_id":"95192","poster":"JohnyGaddar","timestamp":"1633240680.0","content":"A - Not reliable as existing web app is running on single instance.Have single point of failure\nB - Lambda can execute for max 15 mins , might timeout \nC- Max payload size 10MB , therefore won't work + lambda function might timeout (15 min limit)\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html\nD - Correct ( SQS type is not mentioned (Standard/FIFO) - FIFO should be configured for exactly once delivery )","upvote_count":"3","comments":[{"content":"+1\nYes","upvote_count":"1","comment_id":"243628","poster":"newme","timestamp":"1634586300.0"}]},{"poster":"JAWS1600","comment_id":"93047","timestamp":"1633225440.0","upvote_count":"3","content":"Guys , reconsider your answers. C and D say \"Rewrite the web application to run directly from Amazon S3\" - How can you run the application directly from S3 ? That does not seem right.\nAmong option A and B. A will process duplicated ( no FIFO). B is the only remaining valid option"},{"timestamp":"1633128960.0","comment_id":"90939","upvote_count":"1","poster":"Vagrig","content":"Answer is C. \n1.API Gateway for manage a process upload. \n2. S3 trrigger to Lambda https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html - see Warning! For example, if the bucket triggers a Lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself. To avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects. \n3. AND you need to send e-mail - it is SES"},{"upvote_count":"2","comments":[{"poster":"likku","comment_id":"83499","content":"Sorry agree to D","upvote_count":"3","timestamp":"1633071420.0"}],"comment_id":"80760","timestamp":"1633066800.0","content":"Answer is A because \"Use Amazon CloudWatch Events to trigger an AWS Lambda function\" \ncompared to D option A has well suited point.","poster":"likku"},{"content":"A (Invalid): 1. S3 PutObject API requires Object-level Logging enabled to trigger CW Event which is not mentioned. 2. It is not clear whether this message includes instructions or not. \nB & C (Invalid): 1. CW Events cannot trigger SES Job. 2. Without queue, videos will continue getting processed twice.\nD (Valid): Everything is properly setup here. I was initially not sure about uploading 2GB videos files directly through S3, my thinking was that I will need API Gateway in some manner. However, I actually uploaded 2 GB video file through JS. (https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-photo-album.html#s3-example-photo-album-html)","poster":"Smart","upvote_count":"2","timestamp":"1633027560.0","comment_id":"75755"},{"timestamp":"1632990960.0","poster":"Joeylee","upvote_count":"2","comment_id":"75558","content":"D.\nA is talking about Whiting/reading 2TB file with SQS"},{"upvote_count":"1","timestamp":"1632980400.0","comment_id":"60962","poster":"koalasy","content":"I think the answer is A. Reasons I chose A\n1) let's read the question again which is: improve reliability and reduce and redundant processing of video file, this reliability is for processing the video file as well which means we have to deal with the video at least once and not deal with it for 2 times, as it is just improvement so the SLA is not high.\n2) C does not have a sqs QUEUE, this is not reliable, it will definitely remove all the duplicated processing action, but we do not need totally remove all the duplicated action just to improve is enough. \n3) D, run from S3 without mention API gateway, then we can not run it. did you hear any application run from S3? \nas no video process application change thus still use the original method to mail the related information to customer. \nTHUS ANSWER IS A."},{"timestamp":"1632929160.0","content":"A or C\nGuess C","poster":"amog","comment_id":"45285","upvote_count":"1"},{"upvote_count":"1","comment_id":"42460","poster":"PacoDerek","timestamp":"1632924540.0","content":"how to solve the retrigger issue? once the application process the orgin video, adding new video file (thumbnail, resolution，etc）， S3 or cloudwatch event will trigger a lambda again?"},{"timestamp":"1632924540.0","poster":"CloudFloater","upvote_count":"5","content":"Inclined towards D\nA: It is running on Single EC2 instance ; Not reliable; You have to \"rewrite\" the app, not \"modify\" to make it reliable.\nB. Same reason as above; AWS supports Cron on EC2 though, would choose this but for HA.\nC. API Gateway 10 MB Size Limitation as mentioned\nD. Lesser of the evils .. Have to \"rewrite\" the app and run from s3; Video files are static, so S3 is ok; SQS is not perfect without FIFO but the goal is to \"reduce\" as emphasized by sb333 above and dupes can be tolerated once in a while.","comment_id":"41724"},{"timestamp":"1632722040.0","upvote_count":"7","content":"A is the correct answer. No need to try to rewrite the application to use S3. Question does not indicate any issues with web application and does not ask to address any availability issues with it. The question refers to reliability in the context of redundant processing - and the whole goal of the question is to \"reduce\" redundant processing.","comment_id":"36175","poster":"sb333","comments":[{"comment_id":"36177","content":"In any case, good luck everyone on the exam. It's really important to read only what the question is asking and not try to look further than that or try to fix things that are not being asked to be fixed (because they made not need to be \"fixed\" at all, unless the question specifically asks us to address it).","upvote_count":"6","timestamp":"1632881880.0","poster":"sb333"},{"content":"\"D\" would have you rewrite the \"application\" to run from S3? What does that even mean? The web application or another part of the solution. Let's say they meant web application. Why change it? Will it work on S3? Is it static or dynamic content. If dynamic, you need more than S3. Unless there is something specific in the question that says to address the web application, then it shouldn't need to be addressed in the answer. As for the \"instruction file\", the explanation of the application already addresses that. It gets uploaded with the video file. But instead of EFS, we know want it to go to S3. When the application gets the message to read \"new files\", that would include the instruction file. The application already knows what to do with all the files it retrieves. So no need to put the instructions inside the SQS message (which is what \"D\" is stating it will do - put a link and the instructions themselves inside the SQS message). As I mentioned, the application already knows what to do with the video files and instruction files it retrieves from storage.","upvote_count":"3","comment_id":"36176","timestamp":"1632866880.0","poster":"sb333"}]},{"comment_id":"36131","timestamp":"1632675780.0","poster":"LunchTime","content":"CONTINUED...\nC. As I mentioned in my analysis of answer \"A\", an S3 trigger may miss some processing and hence other options do a better job of meeting the reliability requirement. Also, there is the issue that others have mentioned of the 10 MB API Gateway payload limitation (i.e., less then 2 GB).\n\nD. This is the best answer as it mentions including the instructions as part of the message. SQS is going to provide the greatest reliability which is specifically mentioned as a criterion. However, a downside to his answer is that it does not specifically mention SQS using FIFO, which would be required in order to avoid redundant processing. Selecting this answer makes the assumption that this has been configured. Another downside to this answer is it does not explicitly mention if it is uses S3 triggers or CloudWatch events. If it is using S3 events, an event could be missed as I indicated in my comments for Answer A. However, I feel that this answer mentioning having the instructions as part of the message outweighs the ambiguity of how the AWS Lambda function is triggered. The same goes for needing to assume that FIFO has been configured in SQS.","upvote_count":"3"},{"comment_id":"36130","content":"D is the correct answer.\n\nA. Looks good but it does not mention the “instruction file” that needs to be utilized to process the video file. What’s good is it explicitly mentions using CloudWatch events instead of an S3 trigger. S3 triggers may occasionally fail meaning the file won’t be processed using that architecture as per https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html: “Amazon S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer. On very rare occasions, events might be lost.” That would not meet the reliability requirement.\n\nB. Cron job is not efficient / timely. Also, it does not mention the instruction file, which each video file needs in order to be processed. And come on, this is an AWS exam so they are NOT going to want to you use Non-AWS tools! :D\n\nCONTINUED BELOW...","timestamp":"1632652320.0","poster":"LunchTime","upvote_count":"1"},{"content":"I've gone through this question multiple times and I think I'm going to go with \"A\" here. The question, depending on how you read it, to me really is talking about reliability related to the processing of video files (the fact that \"many files\" are processed twice). The question asks to make it more reliable by \"reducing\" the redundant processing. It doesn't say to eliminate it entirely. Just to reduce it. And it doesn't speak to changing the availability of other parts of the solution (like the single web application server, as we don't know the SLA this company has or what they have put into place to mitigate an issue with a single web application server). So the best answer for me is \"A\". It meets all the requirements to \"reduce\" the redundant processing (SQS has a slight chance of duplication, but only in certain circumstances - order of processing is not important and there is no requirement to eliminate duplication). There are some changes to the existing applications required, but it's not requiring a replatforming like \"C\" and \"D\".","comment_id":"35807","poster":"sb333","upvote_count":"1","timestamp":"1632643680.0"},{"upvote_count":"1","comment_id":"30180","content":"Not A and D as SQS will have duplicates. \nMay be not B, as it is too lag\nleft with C, My answer is C","timestamp":"1632596640.0","poster":"JayK"},{"content":"my choice is A. No incorrect message.","poster":"Teri","timestamp":"1632397920.0","comments":[{"content":"wrong, read it again. Support D as Frank1.","timestamp":"1632412260.0","poster":"Teri","upvote_count":"2","comment_id":"25901"}],"upvote_count":"1","comment_id":"25898"},{"content":"For C/D one thing I don't get is, how to run an application from S3? Also C mentioned using API gateway to upload the video file, but just as @Frank1 mentioned, API gateway payload limit is 10 Mb. \n\nI would choose A; if anyone can point out why A is wrong? Even it didn't mention FIFO, it can be configured I would think...","poster":"text","comment_id":"24939","timestamp":"1632369780.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1632267720.0","poster":"Marcos","comment_id":"15624","content":"It is written to \"reduce the redundant processing of video file..\"....I think that SQS could be used. SQS will reduce the redundant the processing of video files although some videos could be processed twice. It doesn't say that duplication is not allowed in any scenario. \nRewriting a web application to be hosted in S3 could take a lot of time."},{"comment_id":"13337","poster":"donathon","content":"B\nA\\D: If SQS is used, then FIFO must be configured to ensure that the messages are processed only once. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nC: API Gateway is used to manage API and not used this way.\nNote: I am not completely sure on this one.","comments":[{"comments":[{"content":"I think C is right answer. Thank cmm103","poster":"manhmaluc","comment_id":"13945","timestamp":"1632193740.0","upvote_count":"2"},{"timestamp":"1634208900.0","comment_id":"195743","poster":"deejiw","content":"API Gateway max payload is 10MB hence B is better imo","upvote_count":"2"}],"content":"I think it is C\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-s3.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html","comment_id":"13900","poster":"cmm103","upvote_count":"7","timestamp":"1632188940.0"},{"comments":[{"content":"Donathan, Not sure why you changed from B to C. Timelag is not the issue here. Can you run the web application from S3 directly, without knowing the details of application ( static vs dynamic). Also in B, there is no duplication , because every time a processed file is uploaded from NFS to S3 , email is sent . Unless there is a bug in the syncing or processing, there is no way that same frile can get copied from NFS to S3.\nOn top of that \"we cannot assume\" \"with might be lag\" condition.I will stick with your original B","upvote_count":"1","comment_id":"100297","timestamp":"1633301880.0","poster":"JAWS1600"},{"content":"We can eliminate C due to payload limitation on APIGateway.\nWe can eliminate B due to data redundancy between EFS and S3\n\nBetween A & D, In question it is mentioned that files will be uploaded by users who connect to web server (instead of application). With this statement I am eliminating D and going with A.\n\nMy answer is A.","poster":"Krishna2637","upvote_count":"5","timestamp":"1633507320.0","comment_id":"107933","comments":[{"poster":"b3llman","comments":[{"upvote_count":"1","poster":"newme","comment_id":"243616","content":"How does that matter?","timestamp":"1634556420.0"}],"comment_id":"171368","upvote_count":"2","content":"A is incorrect because we don't want to run the web app on a single instance.","timestamp":"1634109180.0"}]}],"upvote_count":"3","content":"C\nA\\D: If SQS is used, then FIFO must be configured to ensure that the messages are processed only once. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\nB: There is a time lag in the sync from EFS to S3.\nC: This is more instant compared to A and hence improved the reliability.","timestamp":"1632226020.0","poster":"donathon","comment_id":"14056"}],"timestamp":"1632188820.0","upvote_count":"3"},{"content":"C is my view","upvote_count":"1","timestamp":"1632086940.0","comment_id":"12323","poster":"awsec2","comments":[{"upvote_count":"1","timestamp":"1632137400.0","content":"why C? could you explain for me?","poster":"awspro","comment_id":"13140"}]}],"answer_images":[],"question_images":[],"topic":"1","answer_description":"Reference:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html","question_id":392,"choices":{"C":"Rewrite the web application to run directly from Amazon S3 and use Amazon API Gateway to upload the video files to an S3 bucket. Use an S3 trigger to run an AWS Lambda function each time a file is uploaded to process and store new video files in a different bucket. Using CloudWatch Events, trigger an SES job to send an email to the customer containing the link to the processed file.","D":"Rewrite the web application to run from Amazon S3 and upload the video files to an S3 bucket. Each time a new file is uploaded, trigger an AWS Lambda function to put a message in an SQS queue containing the link and the instructions. Modify the video processing application to read from the SQS queue and the S3 bucket. Use the queue depth metric to adjust the size of the Auto Scaling group for video processing instances.","B":"Set up a cron job on the web server instance to synchronize the contents of the EFS share into Amazon S3. Trigger an AWS Lambda function every time a file is uploaded to process the video file and store the results in Amazon S3. Using Amazon CloudWatch Events, trigger an Amazon SES job to send an email to the customer containing the link to the processed file.","A":"Modify the web application to upload the video files directly to Amazon S3. Use Amazon CloudWatch Events to trigger an AWS Lambda function every time a file is uploaded, and have this Lambda function put a message into an Amazon SQS queue. Modify the video processing application to read from SQS queue for new files and use the queue depth metric to scale instances in the video processing Auto Scaling group."}},{"id":"btnGC5WkSzNsTf76e2mw","answer_ET":"B","unix_timestamp":1569888540,"answer_description":"","answer":"B","exam_id":32,"choices":{"A":"Install and use an OS-native patching service to manage the update frequency and release approval for all instances. Use AWS Config to verify the OS state on each instance and report on any patch compliance issues.","D":"Migrate all applications to AWS OpsWorks and use OpsWorks automatic patching support to keep the OS up-to-date following the initial installation. Use AWS Config to provide audit and compliance reporting.","B":"Use AWS Systems Manager on all instances to manage patching. Test patches outside of production and then deploy during a maintenance window with the appropriate approval.","C":"Use AWS OpsWorks for Chef Automate to run a set of scripts that will iterate through all instances of a given type. Issue the appropriate OS command to get and install updates on each instance, including any required restarts during the maintenance window."},"topic":"1","question_images":[],"discussion":[{"timestamp":"1632210480.0","content":"B\nOnly Systems Manager can patch both OS effectively on AWS and on premise.","comments":[{"poster":"shammous","comments":[{"upvote_count":"1","comment_id":"279925","poster":"shammous","content":"OpsWorks way of patching would be to replace instances with AWS patched Windows AMIs. Linux servers are directly patched. This would be compliant with the requirement even if SSM is more efficient.","timestamp":"1634035620.0"},{"upvote_count":"2","comment_id":"445364","poster":"AWSum1","content":"The question is more related to Patching. Systems manager is correct in this case. Opsworks is more of a deployment tool then a patching tool","timestamp":"1635878280.0"},{"content":"I agree with Pb55 \nB\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\nPatch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to. You can also generate patch compliance reports that are sent to an Amazon Simple Storage Service (Amazon S3) bucket of your choice. You can generate one-time reports, or generate reports on a regular schedule. For a single instance, reports include details of all patches for the instance. For a report on all instances, only a summary of how many patches are missing is provided.","timestamp":"1643182920.0","poster":"RVivek","upvote_count":"1","comment_id":"532687"}],"comment_id":"279924","upvote_count":"2","content":"I'm afraid I'm the only one here feeling that there is something missing: audit and compliance.\nThis would be done by AWS Config mentioned in option D. SSM can indeed patch both OS but AWS OpsWorks can patch Linux servers directly but have a workaround to patch Windows ones: \"The simplest way to ensure that Windows is up to date is to replace your instances regularly, so that they are always running the latest AMI.\". Thus, option D will be viable.\nRef: https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-os-windows.html","timestamp":"1634008740.0"}],"comment_id":"13338","poster":"donathon","upvote_count":"23"},{"comment_id":"926199","content":"Selected Answer: B\nOption D would be the most secure and audit-ready option, but it would also require the Solutions Architect to migrate all applications to AWS OpsWorks, which could be a significant amount of work.\n\nOption B is the most secure, audit-ready, and compliant option that requires the least amount of effort. It uses AWS Systems Manager to manage the patching process, which is a secure and audit-ready service. Additionally, AWS Systems Manager can be used to test patches outside of production before deploying them in production. This helps to ensure that the patches are compatible with the company's applications and that they do not introduce any security vulnerabilities.","poster":"SkyZeroZx","upvote_count":"1","timestamp":"1687028880.0"},{"comment_id":"703399","upvote_count":"1","content":"B 100pc use this service all the time for this use case","timestamp":"1666650660.0","poster":"mrgreatness"},{"timestamp":"1658885940.0","poster":"hilft","comment_id":"637733","content":"B. there is a patch manager inside that will do the job.","upvote_count":"1"},{"timestamp":"1643764860.0","poster":"kyo","comment_id":"538289","upvote_count":"2","content":"Selected Answer: B\nSSM is the best for maintaining ec2 instances"},{"timestamp":"1640871180.0","poster":"cldy","comment_id":"513407","content":"B. \nSSM Patch Manager for both patching and compliance.","upvote_count":"1"},{"poster":"Ni_yot","comment_id":"506973","upvote_count":"1","timestamp":"1640169060.0","content":"B for me"},{"content":"My Answer: B\nPatching with the least amount of error = Systems Manager","comment_id":"499000","timestamp":"1639178160.0","poster":"challenger1","upvote_count":"1"},{"upvote_count":"1","poster":"AzureDP900","comment_id":"493781","timestamp":"1638633300.0","content":"This is B for sure."},{"poster":"andylogan","timestamp":"1635974340.0","upvote_count":"1","comment_id":"450611","content":"It's B"},{"content":"Answer is B","poster":"moon2351","comment_id":"447135","upvote_count":"1","timestamp":"1635900900.0"},{"upvote_count":"1","timestamp":"1635689340.0","content":"B Correct","comment_id":"406626","poster":"Akhil254"},{"content":"B\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\nPatch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to. You can also generate patch compliance reports that are sent to an Amazon Simple Storage Service (Amazon S3) bucket of your choice. You can generate one-time reports, or generate reports on a regular schedule. For a single instance, reports include details of all patches for the instance. For a report on all instances, only a summary of how many patches are missing is provided.","poster":"Pb55","comment_id":"397385","timestamp":"1635680100.0","upvote_count":"4"},{"timestamp":"1635321000.0","upvote_count":"1","comment_id":"362385","poster":"Radhaghosh","content":"Correct Answer is B"},{"content":"I'll go with B","comment_id":"354823","timestamp":"1635112140.0","poster":"WhyIronMan","upvote_count":"2"},{"upvote_count":"1","comment_id":"354821","timestamp":"1634866080.0","content":"I'll go with B","poster":"WhyIronMan"},{"content":"B is correct","timestamp":"1634719380.0","upvote_count":"1","poster":"Waiweng","comment_id":"343936"},{"upvote_count":"2","comment_id":"308493","poster":"Pupu86","timestamp":"1634678580.0","content":"with \"minimal\" effort in mind, I would steer towards option B instead of D"},{"upvote_count":"2","timestamp":"1634398140.0","poster":"Kian1","content":"will go with B","comment_id":"289529"},{"content":"B is answer","upvote_count":"3","timestamp":"1634110800.0","comment_id":"283328","poster":"Ebi"},{"timestamp":"1633883400.0","comment_id":"265923","content":"no brainer. B","upvote_count":"2","poster":"sanjaym"},{"comment_id":"245771","timestamp":"1633791120.0","upvote_count":"1","content":"System Manager for Patching \nB for sure","poster":"rscloud"},{"timestamp":"1633716660.0","upvote_count":"1","poster":"T14102020","content":"Correct is B. Key is System manager","comment_id":"242026"},{"content":"BBBBBBBBB","poster":"MeepMeep","upvote_count":"1","comment_id":"238644","timestamp":"1633616520.0"},{"comment_id":"229406","content":"I'll go with B","timestamp":"1633608780.0","poster":"jackdryan","upvote_count":"3"},{"content":"Answer is B.","timestamp":"1633388220.0","upvote_count":"1","poster":"Bulti","comment_id":"228539"},{"timestamp":"1633227120.0","upvote_count":"1","poster":"vjt","comment_id":"215834","content":"B it is."},{"timestamp":"1633202340.0","comment_id":"148942","content":"B is correct","poster":"fullaws","upvote_count":"1"},{"poster":"NikkyDicky","comment_id":"134211","content":"B for sure","timestamp":"1633199280.0","upvote_count":"1"},{"poster":"noisonnoiton","timestamp":"1632983820.0","upvote_count":"1","comment_id":"133731","content":"go with B\nBoth System Manager and OpsWorks are agent-installed, and OpsWorks is patched\nIs possible, but it is necessary to write a patch execution script for each OS"},{"content":"Easy: B is the answer.","poster":"meenu2225","upvote_count":"1","comment_id":"105496","timestamp":"1632929040.0"},{"timestamp":"1632849900.0","content":"I agree with B. I wish B mentioned CloudTrail auditing to make it a home run on \"audit ready\" though. Looks as though centralized auditing is mentioned as a Systems Manager feature. It looks like \"Configuration Compliance\" is the feature. https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about\n\nThe following is a good parallel\nhttps://aws.amazon.com/blogs/mt/how-moodys-uses-aws-systems-manager-to-patch-servers-across-multiple-cloud-providers/","comment_id":"50556","poster":"AWSPro24","upvote_count":"1"},{"upvote_count":"4","timestamp":"1632795900.0","content":"\"B\".\nexcept B, none of the options are providing proper patching solution. OpsWork is not a patching tool.","poster":"Moon","comment_id":"14304"}],"timestamp":"2019-10-01 02:09:00","question_text":"A Solutions Architect must establish a patching plan for a large mixed fleet of Windows and Linux servers. The patching plan must be implemented securely, be audit-ready, and comply with the company's business requirements.\nWhich option will meet these requirements with MINIMAL effort?","isMC":true,"question_id":393,"answer_images":[],"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/5903-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"DYn3kEnzRuECxirkdD4X","exam_id":32,"answer_images":[],"question_id":394,"question_text":"A Solutions Architect must design a highly available, stateless, REST service. The service will require multiple persistent storage layers for service object meta information and the delivery of content. Each request needs to be authenticated and securely processed. There is a requirement to keep costs as low as possible.\nHow can these requirements be met?","question_images":[],"answers_community":["C (100%)"],"choices":{"B":"Use AWS Fargate to host a container that runs a self-contained REST service. Set up an ECS service that is fronted by a cross-zone ALB. Use an Amazon Cognito user pool to control access to the API. Store request meta information in DynamoDB with Auto Scaling and static content in a secured S3 bucket. Generate presigned URLs when returning references to content stored in Amazon S3.","C":"Set up Amazon API Gateway and create the required API resources and methods. Use an Amazon Cognito user pool to control access to the API. Configure the methods to use AWS Lambda proxy integrations, and process each resource with a unique AWS Lambda function. Store request meta information in DynamoDB with Auto Scaling and static content in a secured S3 bucket. Generate presigned URLs when returning references to content stored in Amazon S3.","A":"Use AWS Fargate to host a container that runs a self-contained REST service. Set up an Amazon ECS service that is fronted by an Application Load Balancer (ALB). Use a custom authenticator to control access to the API. Store request meta information in Amazon DynamoDB with Auto Scaling and static content in a secured S3 bucket. Make secure signed requests for Amazon S3 objects and proxy the data through the REST service interface.","D":"Set up Amazon API Gateway and create the required API resources and methods. Use an Amazon API Gateway custom authorizer to control access to the API. Configure the methods to use AWS Lambda custom integrations, and process each resource with a unique Lambda function. Store request meta information in an Amazon ElastiCache Multi-AZ cluster and static content in a secured S3 bucket. Generate presigned URLs when returning references to content stored in Amazon S3."},"url":"https://www.examtopics.com/discussions/amazon/view/5617-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"content":"C\nThis is definitely a API Gateway question. Also note the questions says “multiple persistent”. ElastiCache is temporary store and hence DynamoDB make more sense.\nA\\B: Fargate is containerization.","poster":"donathon","comment_id":"13339","timestamp":"1632157860.0","upvote_count":"36","comments":[{"upvote_count":"6","poster":"shammous","content":"Cognito also plays a role here as it's cheap and easy to setup.","timestamp":"1634976180.0","comment_id":"279927"}]},{"comments":[{"upvote_count":"1","content":"Self Contained REST service!!!\nWhat's wrong with it?","poster":"newme","comment_id":"243648","timestamp":"1634534520.0"}],"upvote_count":"14","content":"Agree with answer \"C\".\nThe Cognito can be used for authentication. API Gateway for Stateless REST service.\nA: uses custom authenticator. Self Contained REST service!!!\nB: Self Contained REST service!!!\nD: Elasticache is not persistent.","timestamp":"1632266820.0","poster":"Moon","comment_id":"14297"},{"comment_id":"926204","content":"Selected Answer: C\nThe correct answer is C.\n\nThis solution meets all of the requirements:\n\nIt is highly available because it uses Amazon API Gateway, which is a highly scalable and reliable service.\nIt is stateless because the Lambda functions are stateless.\nIt uses multiple persistent storage layers for service object meta information and the delivery of content. DynamoDB is used for meta information, and S3 is used for static content.\nEach request is authenticated and securely processed. Cognito is used for authentication, and Lambda functions are used to process requests securely.\nCosts are kept as low as possible by using Lambda functions, which are only charged when they are invoked.","timestamp":"1687029240.0","poster":"SkyZeroZx","upvote_count":"1"},{"upvote_count":"2","poster":"cossstin","comment_id":"761033","content":"C - api gateway + cognito for authentication","timestamp":"1672319580.0"},{"comment_id":"610193","poster":"kangtamo","content":"Selected Answer: C\nC looks good to me.","upvote_count":"1","timestamp":"1654091040.0"},{"upvote_count":"1","timestamp":"1639942800.0","comment_id":"505049","content":"C definitely! It's the cheapest option by far","poster":"ebase4"},{"timestamp":"1639128420.0","content":"C. Set up Amazon API Gateway and create the required API resources and methods. Use an Amazon Cognito user pool to control access to the API. Configure the methods to use AWS Lambda proxy integrations, and process each resource with a unique AWS Lambda function. Store request meta information in DynamoDB with Auto Scaling and static content in a secured S3 bucket. Generate presigned URLs when returning references to content stored in Amazon S3.","upvote_count":"1","comment_id":"498481","poster":"cldy"},{"content":"C is right for given scnerio.","poster":"AzureDP900","timestamp":"1638633600.0","comment_id":"493784","upvote_count":"1"},{"content":"It's C","timestamp":"1636251600.0","poster":"andylogan","upvote_count":"1","comment_id":"450613"},{"timestamp":"1636185900.0","poster":"nodogoshi","content":"Surely C. Lowest Cost.","upvote_count":"1","comment_id":"447888"},{"content":"I'll go with C","comment_id":"409482","upvote_count":"1","poster":"WhyIronMan","timestamp":"1636171140.0"},{"poster":"Akhil254","upvote_count":"1","timestamp":"1636156800.0","content":"C Correct","comment_id":"406641"},{"comment_id":"397388","timestamp":"1636145640.0","content":"Has to be C or D and D has no persistent storage. Therefore C.","poster":"Pb55","upvote_count":"2"},{"content":"Correct Answer C\nFor REST API, AWS Fargate ruled out, which brings Option C & D. \nNow Between Option C & D, C is the better option as it Uses Cognito user pool.","timestamp":"1635840900.0","poster":"Radhaghosh","comment_id":"362389","upvote_count":"1"},{"comment_id":"344439","timestamp":"1635819420.0","poster":"blackgamer","upvote_count":"1","content":"Agree with C. Elastic cache is only for the cache purpose, so D Is not correct. DynamoDB is suitable to store object metadata."},{"content":"C is correct","upvote_count":"1","timestamp":"1635818580.0","poster":"Waiweng","comment_id":"343940"},{"comment_id":"308505","content":"https://medium.com/@lakshmanLD/lambda-proxy-vs-lambda-integration-in-aws-api-gateway-3a9397af0e6d","timestamp":"1635578940.0","poster":"Pupu86","upvote_count":"1"},{"content":"I will go with C","poster":"Kian1","upvote_count":"2","comment_id":"289532","timestamp":"1635431460.0"},{"content":"I go with C","upvote_count":"4","poster":"Ebi","comment_id":"283332","timestamp":"1635056340.0"},{"comments":[{"timestamp":"1634856840.0","content":"for elasticity, in addition to provisioned capacity to have a min/max for RCU and WCU.","comment_id":"261094","poster":"cox1960","upvote_count":"1"}],"content":"Why do we need Auto Scaling for DynamoDB (from A to C)?","upvote_count":"1","timestamp":"1634718960.0","poster":"MichaelHuang","comment_id":"247692"},{"comment_id":"242033","content":"C is correct. Lambda cheaper then Fargate","comments":[{"content":"\"Lambda cheaper then Fargate\"\n\nWe don't know how much access will be, how long an API will take, how much memory is needed, so how do we compare them?","timestamp":"1634710020.0","poster":"newme","comment_id":"243653","upvote_count":"1"}],"poster":"T14102020","upvote_count":"1","timestamp":"1634476740.0"},{"timestamp":"1634368860.0","poster":"jackdryan","comment_id":"229409","content":"I'll go with C","upvote_count":"2"},{"upvote_count":"4","comment_id":"228547","poster":"Bulti","timestamp":"1634304120.0","content":"C is correct as it meets the key requirements asked in the question that of \"authentication\" and \"security\". D does meet the requirement but I wouldn't go for it because using Cognito User pool to authenticate the user and later using the Cognito Authorizer on API Gateway to validate the Cognito User Pool Token is a much better option than creating a custom authentication and authorization implementation. Besides I think ElasticCache is not a good persistence solution as compared to Dynamo DB."},{"comment_id":"148948","content":"Can use custom authorizer, but C is more preferable as use dynamodb compare to Elasticache","timestamp":"1634045940.0","poster":"fullaws","upvote_count":"1"},{"poster":"fullaws","timestamp":"1633992900.0","comment_id":"148945","content":"C is correct, High available (A and B eliminate, because single container), Highly secure (D not recommended to use custom authorizer)","upvote_count":"1"},{"timestamp":"1633662240.0","comment_id":"145308","content":"will go with C","comments":[{"poster":"CKW","comment_id":"302302","timestamp":"1635535440.0","content":"Agreed with C. The following link explain:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html","upvote_count":"1"}],"poster":"learner4ever","upvote_count":"1"},{"comment_id":"134214","upvote_count":"1","content":"C more likely","timestamp":"1633590060.0","poster":"NikkyDicky"},{"content":"C is the right answer.\nI quiet like A as well, but compared to C, A is bit more expensive due to Fargate and ECS.","timestamp":"1633541460.0","poster":"meenu2225","comment_id":"105512","upvote_count":"2"},{"content":"JohnyGaddar - Yes Elasticache Redis can provide persistence. However it requires additional step - Backup /Restore. One of those tricky choices. :-)\nYou are right C is the best answer. \nPlease refer to https://aws.amazon.com/elasticache/redis/faqs/","timestamp":"1633483920.0","poster":"JAWS1600","comment_id":"98421","upvote_count":"1"},{"content":"A/B - both works woubd be costlier than lamda. Others mentioned A is not HA which I don't agree with \nD - although elastic cache also supports persistent storage(via REDIS) but the essence of elastic cache is to be ephemeral storage and also in this scenario using REDIS as persistent store would make the architecture unnecessary complicated without any clear cut cost benefits\nC - correct","poster":"JohnyGaddar","upvote_count":"1","timestamp":"1633293000.0","comment_id":"95765"},{"content":"It seems that the scenario is looking for a simple serverless design with HA & cost control. \n- A single Fargate container will not be HA and running continuously with not help with cost control.\n- Elastic Cache running continuously with not help with cost control. \n- Pre-signed URLs will simplify things?\nWith all this in mind, I am choosing C.","poster":"Smart","comment_id":"75825","timestamp":"1633288620.0","upvote_count":"3"},{"upvote_count":"3","comment_id":"75564","timestamp":"1633255200.0","poster":"Joeylee","content":"D mentioned eCache it should not be use for data storage so it is wrong.\nA is not HA \nB and C are both fine but lambda is much cheaper so go with C.\nNothing wrong with container service as Lambda is backed by Container service itself"},{"timestamp":"1632937980.0","content":"Answer is C\nFargate for container","upvote_count":"3","comment_id":"45288","poster":"amog"},{"comment_id":"36754","comments":[{"upvote_count":"1","timestamp":"1634174700.0","comment_id":"222127","poster":"Rajeev","content":"What about the cost? DynamoDB in AutoScaling mode will require more RCUs whereas ElastiCache would be cheaper. Any thoughts on that..."}],"upvote_count":"5","poster":"LunchTime","content":"CONTINUED FROM ABOVE...\nAlso, Donathon and Moon stated that ElastiCache is NOT persistent. This is NOT necessarily true. In using ElastiCache you have two implementation options: MemCache or Redis. MemCache does not provide persistent storage but Redis DOES. As such, “D” including ElastiCache does not mean it is incorrect (See https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/backups.html: \n\"Q: Does Amazon ElastiCache for Redis support Redis persistence? Yes, you can achieve persistence by snapshotting your Redis data using the Backup and Restore feature.\"\n\nElastiCache has better performance then DynamoDB and is a better solution from that perspective. BUT, DynamoDB will also work well here. Since there is ambiguity as to the whether the ElastiCache implementation is using MemCache or Redis, but no ambiguity as to whether DynamoDB will support the needed persistence, I have decided to make “C” my choice.\n\nNote that if answer “D” also said that it would use DAX alongside DynamoDB, then that would make it faster the using ElastiCache (Redis) and would have tilted my answer back to “D”.","timestamp":"1632739500.0"},{"poster":"LunchTime","timestamp":"1632610920.0","comment_id":"36753","upvote_count":"3","content":"CONTINUED FROM ABOVE...\nI initially felt strongly that “D” was the best answer but in weighing all these factors I am now leaning towards “C” and that would be my selection at this point.\n\nD. This answer meets all the requirements and I believe is in ways better then answer “C”. However, I have still selected “C” as my answer, as per my explanation below and my comments for “C”. One key difference between “C” and “D” is “C” uses “AWS Lambda proxy integrations” and “D” uses “AWS Lambda custom integrations”. Advantages and disadvantages are listed in the following link: https://medium.com/@lakshmanLD/lambda-proxy-vs-lambda-integration-in-aws-api-gateway-3a9397af0e6d. \nIn a nutshell, Custom Integrations are more powerful, easier to document and less prone to human error. The downside is they are more work to implement. Since time and cost are not specifically mentioned, this is a better answer then “C”. \nCONTINUED BELOW..."},{"content":"CONTINUED FROM ABOVE...\nC. This answer nails all the requirements and is my choice for the best answer. HOWEVER, D is preferable in some ways. Arguably, AWS Lambda custom integrations (D) is preferable to using AWS Lambda Proxy integration. The other key part to this question is the “multiple persistent storage layers” requirement. Donathon and Moon stated that ElastiCache is NOT persistent. This is NOT necessarily true. In using ElastiCache you have two implementation options: MemCache or Redis. MemCache does not provide persistent storage but Redis DOES. As such, “D” including ElastiCache does not mean it is incorrect (See https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/backups.html: Q: Does Amazon ElastiCache for Redis support Redis persistence? Yes, you can achieve persistence by snapshotting your Redis data using the Backup and Restore feature. ). ElastiCache is a better solution then DynamoDB. However, DynamoDB (answer D) would also work, and there is no question about that verses the question of which implementation of ElastiCache is being used (MemCache vs. Redis). \nCONTINUED BELOW...","upvote_count":"3","timestamp":"1632430500.0","comment_id":"36749","comments":[{"upvote_count":"2","content":"Great explanation Lunch!","timestamp":"1634148480.0","comment_id":"177234","poster":"Carupano"}],"poster":"LunchTime"},{"upvote_count":"2","poster":"LunchTime","timestamp":"1632367800.0","content":"C is the correct Answer. \n\nAlthough both donathon and moon agree, I thought the considerations needed in this question were worthy of additional discussion.\n\nA. Custom authenticator is not the best option. Using Fargate is fine, but you need a better way to call it then an ALB/ELB, which does not have security integrated into it.\n\nB. Similar issue as with A, although it is an improvement since it uses Cognito.\nCONTINUED BELOW...","comment_id":"36748"},{"comment_id":"12324","poster":"awsec2","content":"why D ? any idea","upvote_count":"1","timestamp":"1632114420.0"}],"isMC":true,"answer_ET":"D","unix_timestamp":1569262920,"answer":"C","topic":"1","answer_description":"","timestamp":"2019-09-23 20:22:00"},{"id":"AhKmsja1ssTXpAcSGqEU","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/5341-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["D (100%)"],"answer_description":"","discussion":[{"timestamp":"1632905700.0","upvote_count":"34","poster":"donathon","content":"D\nA: This will grant too much access.\nB: Should be SAML based due to the AD Group.\nC: This will block the developer from other access that they may need. Key is “any AWS services”.","comment_id":"12831"},{"poster":"Moon","upvote_count":"19","content":"agree with \"donathon\", Answer D is the correct one for the same reasons.\nThe tricks here are:\n- SAML for AD federation and authentication\n- PowerUserAccess vs AdministrativeAccess. (PowerUSer has less privilege, which is the required once for developers). Admin, has more rights.\n\nThe description of \"PowerUser access\" given by AWS is “Provides full access to AWS services and resources, but does not allow management of Users and groups.”","timestamp":"1632917940.0","comment_id":"14300"},{"content":"Selected Answer: D\nagree with \"donathon\", Answer D is the correct one for the same reasons.\nThe tricks here are:\n- SAML for AD federation and authentication\n- PowerUserAccess vs AdministrativeAccess. (PowerUSer has less privilege, which is the required once for developers). Admin, has more rights.\n\nThe description of \"PowerUser access\" given by AWS is “Provides full access to AWS services and resources, but does not allow management of Users and groups.”","upvote_count":"1","timestamp":"1687376760.0","comment_id":"929868","poster":"SkyZeroZx"},{"poster":"mimadour21698","content":"Selected Answer: D\nGo for D","upvote_count":"1","comment_id":"895385","timestamp":"1683835080.0"},{"content":"D - federate access, so just one role which can be assumed and attached the power user policy and then another policy with condition to restrict region. I recreated this. 100pc D","upvote_count":"1","poster":"mrgreatness","timestamp":"1666651260.0","comment_id":"703402"},{"content":"C is ruled out as \"The company would also like to allow Developers to launch Amazon EC2 in only one region, without limiting access to other services in any region.\"","timestamp":"1662277380.0","comments":[{"upvote_count":"1","content":"Hence D is only correct answer.","timestamp":"1662277380.0","comment_id":"659047","poster":"pixepe"}],"comment_id":"659045","upvote_count":"1","poster":"pixepe"},{"upvote_count":"1","timestamp":"1640197740.0","comment_id":"507293","poster":"AzureDP900","content":"D is more logical answer based on question --- > without restricting access to other services in any region\n\nC only giving access to EC2 service and nothing else.."},{"poster":"cldy","comment_id":"498430","content":"D. Set up SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer managed policy that denies access to Amazon EC2 in each region except for the one required.","upvote_count":"1","timestamp":"1639126140.0"},{"upvote_count":"1","comment_id":"493786","poster":"AzureDP900","timestamp":"1638633900.0","content":"D is right"},{"poster":"seyik","content":"C.\nAWS Service Catalog allows you to centrally manage deployed IT services and your applications, resources, and metadata. This helps you achieve consistent governance and meet your compliance requirements, while enabling users to quickly deploy only the approved IT services they need. With AWS Service Catalog AppRegistry, organizations can understand the application context of their AWS resources.\nhttps://aws.amazon.com/servicecatalog/?aws-service-catalog.sort-by=item.additionalFields.createdDate&aws-service-catalog.sort-order=desc","timestamp":"1636297980.0","comment_id":"461947","upvote_count":"1"},{"poster":"andylogan","timestamp":"1636279440.0","upvote_count":"2","comment_id":"450614","content":"It's D - exclude C because \"deny all the Developers access to any AWS services except AWS Service Catalog.\""},{"comment_id":"409486","poster":"WhyIronMan","upvote_count":"1","content":"I'll go with D","timestamp":"1636263180.0"},{"content":"D Correct","comment_id":"406640","upvote_count":"1","poster":"Akhil254","timestamp":"1635939960.0"},{"timestamp":"1635040860.0","poster":"Radhaghosh","comment_id":"362396","comments":[{"timestamp":"1635121800.0","upvote_count":"2","comment_id":"362398","content":"I am sorry for typo. Correct Option is D","poster":"Radhaghosh"}],"upvote_count":"1","content":"Two aspects in this question. The first is \"restricting the level of access that Developers have to the AWS Management Console without impacting their productivity\" -- This eliminates Option A (as user should have \"PowerUserAccess\" role. Second point is \"The company would also like to allow Developers to launch Amazon EC2 in only one region, without limiting access to other services in any region.\" - this eliminates Option C. Now to provide access via on-premises Active Directory groups, you need SAML. So Correct Option is C"},{"comment_id":"343946","poster":"Waiweng","content":"D is correct","upvote_count":"2","timestamp":"1634925480.0"},{"comment_id":"289536","content":"Will go with D","timestamp":"1634649840.0","poster":"Kian1","upvote_count":"2"},{"content":"D is answer","poster":"Ebi","upvote_count":"4","timestamp":"1634535540.0","comment_id":"283334"},{"upvote_count":"2","comment_id":"266522","timestamp":"1634373900.0","poster":"sanjaym","content":"D for sure."},{"comment_id":"242036","upvote_count":"2","timestamp":"1634038980.0","poster":"T14102020","content":"D is correct."},{"upvote_count":"3","content":"I'll go with D","poster":"jackdryan","comment_id":"229411","timestamp":"1634017260.0"},{"comment_id":"228551","content":"Answer is D.","upvote_count":"2","poster":"Bulti","timestamp":"1633831440.0"},{"timestamp":"1633817940.0","poster":"Kibana01","upvote_count":"1","comment_id":"150002","content":"D is correct, in option C check the word \"only\"."},{"timestamp":"1633742460.0","content":"D is correct","comment_id":"148953","poster":"fullaws","upvote_count":"1"},{"content":"agree to D","comment_id":"145312","upvote_count":"1","poster":"learner4ever","timestamp":"1633734840.0"},{"upvote_count":"1","comment_id":"134216","poster":"NikkyDicky","timestamp":"1633529760.0","content":"D for sure"},{"poster":"meenu2225","upvote_count":"1","content":"D is the correct option.","comment_id":"105524","timestamp":"1633527480.0"},{"timestamp":"1633207920.0","comment_id":"105515","poster":"meenu2225","upvote_count":"1","content":"D is the one"},{"content":"Should be D","comment_id":"45289","poster":"amog","timestamp":"1633204500.0","upvote_count":"2"},{"content":"Yep . D","poster":"awsec2","timestamp":"1632713040.0","comment_id":"12325","upvote_count":"1"},{"comment_id":"11509","upvote_count":"2","content":"C would deny the developer from access other resources. I would go with D.","timestamp":"1632140940.0","poster":"dpvnme"}],"topic":"1","question_id":395,"question_images":[],"exam_id":32,"question_text":"A large company experienced a drastic increase in its monthly AWS spend. This is after Developers accidentally launched Amazon EC2 instances in unexpected regions. The company has established practices around least privileges for Developers and controls access to on-premises resources using Active Directory groups. The company now want to control costs by restricting the level of access that Developers have to the AWS Management Console without impacting their productivity. The company would also like to allow Developers to launch Amazon EC2 in only one region, without limiting access to other services in any region.\nHow can this company achieve these new security requirements while minimizing the administrative burden on the Operations team?","answer_ET":"D","timestamp":"2019-09-18 05:11:00","isMC":true,"answer":"D","unix_timestamp":1568776260,"choices":{"D":"Set up SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer managed policy that denies access to Amazon EC2 in each region except for the one required.","A":"Set up SAML-based authentication tied to an IAM role that has an AdministrativeAccess managed policy attached to it. Attach a customer managed policy that denies access to Amazon EC2 in each region except for the one required.","B":"Create an IAM user for each Developer and add them to the developer IAM group that has the PowerUserAccess managed policy attached to it. Attach a customer managed policy that allows the Developers access to Amazon EC2 only in the required region.","C":"Set up SAML-based authentication tied to an IAM role that has a PowerUserAccess managed policy and a customer managed policy that deny all the Developers access to any AWS services except AWS Service Catalog. Within AWS Service Catalog, create a product containing only the EC2 resources in the approved region."}}],"exam":{"isMCOnly":false,"isBeta":false,"isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","id":32,"provider":"Amazon","numberOfQuestions":1019,"lastUpdated":"11 Apr 2025"},"currentPage":79},"__N_SSP":true}