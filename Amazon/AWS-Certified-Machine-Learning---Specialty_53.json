{"pageProps":{"questions":[{"id":"Oa732YjJr3MGg0pEjCgh","topic":"1","question_text":"An agriculture company wants to improve crop yield forecasting for the upcoming season by using crop yields from the last three seasons. The company wants to compare the performance of its new scikit-learn model to the benchmark.\n\nA data scientist needs to package the code into a container that computes both the new model forecast and the benchmark. The data scientist wants AWS to be responsible for the operational maintenance of the container.\n\nWhich solution will meet these requirements?","discussion":[{"content":"Selected Answer: A\nA. Enough to compare the performance of new and old scikit-learn models.\nD. Good, but with additional overhead.","upvote_count":"2","timestamp":"1729229820.0","poster":"VerRi","comment_id":"1299547"},{"content":"Selected Answer: D\nAmazon SageMaker provides built-in containers for common machine learning frameworks, including scikit-learn, which are designed to handle operational maintenance such as patching, scaling, and monitoring.https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-docker-containers-scikit-learn-spark.html","comment_id":"1286121","comments":[{"upvote_count":"1","timestamp":"1730564220.0","poster":"spinatram","comment_id":"1306235","content":"D is right. Customer wants AWS handle the container maintenance"}],"poster":"Tkhan1","timestamp":"1726717560.0","upvote_count":"2"},{"comment_id":"1283570","poster":"CW0106","content":"Selected Answer: A\nhttps://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html","upvote_count":"1","timestamp":"1726307940.0"},{"poster":"luccabastos","timestamp":"1726264620.0","upvote_count":"1","comment_id":"1283391","comments":[{"content":"you can not directly push to fargate. first push to ecr","comment_id":"1306234","poster":"spinatram","upvote_count":"1","timestamp":"1730564160.0"}],"content":"Selected Answer: C\nAWS Fargate"},{"poster":"GS_77","upvote_count":"1","comment_id":"1279979","timestamp":"1725707760.0","content":"Selected Answer: D\nD appears to be the best choice"}],"answers_community":["D (43%)","A (43%)","14%"],"timestamp":"2024-09-07 13:16:00","answer_description":"","choices":{"B":"Package the code into a custom-built container. Push the container to Amazon Elastic Container Registry (Amazon ECR).","C":"Package the code into a custom-built container. Push the container to AWS Fargate.","A":"Package the code as the training script for an Amazon SageMaker scikit-learn container.","D":"Package the code by extending an Amazon SageMaker scikit-learn container."},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/147149-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"isMC":true,"answer":"D","unix_timestamp":1725707760,"question_images":[],"question_id":261,"answer_ET":"D"},{"id":"8coNMRrHHBMKK7NKDGa6","discussion":[{"comments":[{"comment_id":"1558790","poster":"ef12052","timestamp":"1744085640.0","content":"scaling firehose can handle it","upvote_count":"1"}],"content":"Selected Answer: D\nFirehose does stream directly BUT without Kinesis Data Streams we dont have buffering and scaling, which is a requirement of the problem","poster":"2eb8df0","timestamp":"1742321760.0","comment_id":"1400234","upvote_count":"2"},{"comment_id":"1399827","timestamp":"1742242020.0","poster":"ef12052","upvote_count":"1","content":"Selected Answer: D\nfirehouse does not stream directly.."},{"content":"Selected Answer: D\nNeed Amazon Kinesis to handle sudden changers in the data flow and good scaling. AWS firehouse does not have this capability","comment_id":"1334508","timestamp":"1735605360.0","poster":"587df71","upvote_count":"1"},{"content":"Selected Answer: C\nC is more cost-effective","poster":"VerRi","comment_id":"1299551","upvote_count":"1","timestamp":"1729230420.0"},{"poster":"GS_77","content":"Selected Answer: C\nC is the best option in this case.","upvote_count":"3","comment_id":"1279977","timestamp":"1725707700.0"}],"question_images":[],"answer_ET":"C","exam_id":26,"question_text":"A cybersecurity company is collecting on-premises server logs, mobile app logs, and IoT sensor data. The company backs up the ingested data in an Amazon S3 bucket and sends the ingested data to Amazon OpenSearch Service for further analysis. Currently, the company has a custom ingestion pipeline that is running on Amazon EC2 instances. The company needs to implement a new serverless ingestion pipeline that can automatically scale to handle sudden changes in the data flow.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_images":[],"timestamp":"2024-09-07 13:15:00","topic":"1","answers_community":["C (50%)","D (50%)"],"isMC":true,"answer_description":"","question_id":262,"answer":"C","choices":{"A":"Create two Amazon Data Firehose delivery streams to send data to the S3 bucket and OpenSearch Service. Configure the data sources to send data to the delivery streams.","B":"Create one Amazon Kinesis data stream. Create two Amazon Data Firehose delivery streams to send data to the S3 bucket and OpenSearch Service. Connect the delivery streams to the data stream. Configure the data sources to send data to the data stream.","C":"Create one Amazon Data Firehose delivery stream to send data to OpenSearch Service. Configure the delivery stream to back up the raw data to the S3 bucket. Configure the data sources to send data to the delivery stream.","D":"Create one Amazon Kinesis data stream. Create one Amazon Data Firehose delivery stream to send data to OpenSearch Service. Configure the delivery stream to back up the data to the S3 bucket. Connect the delivery stream to the data stream. Configure the data sources to send data to the data stream."},"unix_timestamp":1725707700,"url":"https://www.examtopics.com/discussions/amazon/view/147148-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"72RPs4iukvc13V6mIARX","answer":"B","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/146687-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"unix_timestamp":1725039240,"answers_community":["B (100%)"],"answer_ET":"B","isMC":true,"timestamp":"2024-08-30 19:34:00","answer_images":[],"answer_description":"","question_text":"A bank has collected customer data for 10 years in CSV format. The bank stores the data in an on-premises server. A data science team wants to use Amazon SageMaker to build and train a machine learning (ML) model to predict churn probability. The team will use the historical data. The data scientists want to perform data transformations quickly and to generate data insights before the team builds a model for production.\n\nWhich solution will meet these requirements with the LEAST development effort?","topic":"1","choices":{"C":"Upload the data into the SageMaker Data Wrangler console directly. Allow SageMaker and Amazon QuickSight to access the data that is in an Amazon S3 bucket. Perform data transformations in Data Wrangler and save the transformed data into a second S3 bucket. Use QuickSight to generate data insights.","B":"Upload the data into an Amazon S3 bucket. Allow SageMaker to access the data that is in the bucket. Import the data from the S3 bucket into SageMaker Data Wrangler. Perform data transformations and generate insights within Data Wrangler.","D":"Upload the data into an Amazon S3 bucket. Allow SageMaker to access the data that is in the bucket. Import the data from the bucket into SageMaker Data Wrangler. Perform data transformations in Data Wrangler. Save the data into a second S3 bucket. Use a SageMaker Studio notebook to generate data insights.","A":"Upload the data into the SageMaker Data Wrangler console directly. Perform data transformations and generate insights within Data Wrangler."},"question_id":263,"discussion":[{"comment_id":"1300381","timestamp":"1729416960.0","content":"Selected Answer: B\nOptions A and C involve directly uploading data to SageMaker Data Wrangler, which might not be as scalable or efficient for large datasets. Option D adds an extra step of using SageMaker Studio notebooks, which increases the complexity and development effort.","upvote_count":"1","poster":"MultiCloudIronMan"},{"poster":"Tkhan1","comment_id":"1286123","timestamp":"1726718160.0","content":"Selected Answer: B\nB is the correct option .\nA is not an option as 10 years data will be too much for local upload or on premise . So an intermediate storage is needed which is S3 .","upvote_count":"1"},{"content":"Selected Answer: B\nAnswer is B","timestamp":"1725060240.0","upvote_count":"1","comment_id":"1275302","poster":"aragon_saa"},{"poster":"GS_77","upvote_count":"1","content":"Selected Answer: B\noption B is the most straightforward and efficient solution for the data science team to quickly perform data transformations and generate insights before building a model","comment_id":"1275195","timestamp":"1725039240.0"}]},{"id":"k5q2Bj7SDZDwUV71vqZ4","topic":"1","answer":"B","answer_description":"","timestamp":"2024-09-07 12:59:00","answers_community":["B (86%)","14%"],"discussion":[{"comment_id":"1304099","upvote_count":"1","timestamp":"1730139780.0","poster":"MultiCloudIronMan","content":"Selected Answer: B\nBest of both worlds, elastic and provisioned."},{"comment_id":"1299556","timestamp":"1729231260.0","poster":"VerRi","upvote_count":"1","content":"Selected Answer: B\nA is more expensive"},{"timestamp":"1726719000.0","comment_id":"1286128","upvote_count":"3","content":"Selected Answer: B\nOn-demand Serverless Inference is ideal for workloads which have idle periods between traffic spurts.Optionally, you can also use Provisioned Concurrency with Serverless Inference. Serverless Inference with provisioned concurrency is a cost-effective option when you have predictable bursts in your traffic.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html","poster":"Tkhan1"},{"timestamp":"1726265160.0","poster":"luccabastos","content":"Selected Answer: A\nThe traffic is expected. Provisioned resouces have minimal cost.","comment_id":"1283393","upvote_count":"1"},{"content":"Selected Answer: B\nBy choosing serverless inference with provisioned concurrency, the media company can benefit from low latency during peak traffic periods while optimizing costs by only paying for the actual inference requests","poster":"GS_77","comment_id":"1279972","timestamp":"1725706740.0","upvote_count":"1"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/147146-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":264,"unix_timestamp":1725706740,"question_text":"A media company wants to deploy a machine learning (ML) model that uses Amazon SageMaker to recommend new articles to the company’s readers. The company's readers are primarily located in a single city.\n\nThe company notices that the heaviest reader traffic predictably occurs early in the morning, after lunch, and again after work hours. There is very little traffic at other times of day. The media company needs to minimize the time required to deliver recommendations to its readers. The expected amount of data that the API call will return for inference is less than 4 MB.\n\nWhich solution will meet these requirements in the MOST cost-effective way?","isMC":true,"exam_id":26,"answer_ET":"B","question_images":[],"choices":{"A":"Real-time inference with auto scaling","D":"A batch transform task","B":"Serverless inference with provisioned concurrency","C":"Asynchronous inference"}},{"id":"YtkQO39or2Ze8N8eNR7n","answer_ET":"D","answer":"D","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/146689-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"C":"Set ParameterRanges to the narrow range Inferred from previous hyperparameter jobs.","B":"Set RetryStrategy to a value of 1.","A":"Set Strategy to the Bayesian value.","D":"Set TrainingJobEarlyStoppingType to the AUTO value."},"unix_timestamp":1725039780,"answer_images":[],"exam_id":26,"timestamp":"2024-08-30 19:43:00","question_id":265,"question_text":"A machine learning (ML) engineer is using Amazon SageMaker automatic model tuning (AMT) to optimize a model's hyperparameters. The ML engineer notices that the tuning jobs take a long time to run. The tuning jobs continue even when the jobs are not significantly improving against the objective metric.\n\nThe ML engineer needs the training jobs to optimize the hyperparameters more quickly.\n\nHow should the ML engineer configure the SageMaker AMT data types to meet these requirements?","discussion":[{"content":"Selected Answer: D\nstop the job when it is not significantly improving the objective metric","comment_id":"1299557","timestamp":"1729231500.0","upvote_count":"1","poster":"VerRi"},{"upvote_count":"2","content":"Selected Answer: D\nIf you are using the AWS SDK for Python (Boto3), set the TrainingJobEarlyStoppingType field of the HyperParameterTuningJobConfig object that you use to configure the tuning job to AUTO.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html","timestamp":"1726719900.0","comment_id":"1286130","poster":"Tkhan1"},{"comment_id":"1275306","timestamp":"1725060720.0","poster":"aragon_saa","content":"Selected Answer: A\nAnswer is A","upvote_count":"1"},{"comment_id":"1275203","content":"Selected Answer: D\nSet TrainingJobEarlyStoppingType to the AUTO value","upvote_count":"2","timestamp":"1725039780.0","poster":"GS_77"}],"answers_community":["D (83%)","A (17%)"],"answer_description":"","topic":"1"}],"exam":{"id":26,"provider":"Amazon","isBeta":false,"isMCOnly":false,"name":"AWS Certified Machine Learning - Specialty","numberOfQuestions":369,"isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":53},"__N_SSP":true}