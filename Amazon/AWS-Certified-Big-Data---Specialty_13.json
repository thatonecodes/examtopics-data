{"pageProps":{"questions":[{"id":"iKKzJUvBLT24tT3Xy7G4","discussion":[{"timestamp":"1632825180.0","poster":"Zire","upvote_count":"9","content":"A is the solution for the least amount of work. Quick and secure access to Dynamo db using dynamodb:LeadingKeys see: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html","comment_id":"16089"},{"poster":"Bulti","comment_id":"77503","content":"Upon further research I realized that you can use condition keys on the permission policy attached to the IAM role that is used to access Dynamo DB using temporary IAM user credentials. So A is the right answer not B.","timestamp":"1635419880.0","upvote_count":"6"},{"comment_id":"149060","timestamp":"1636068420.0","poster":"Royk2020","content":"Answer is A. Dynamodb has fine grain access control (FGAC) which further integrated with IAM","upvote_count":"2"},{"poster":"matthew95","comment_id":"109944","timestamp":"1635769080.0","upvote_count":"3","content":"A is possible, look at this: https://stelligent.com/2016/07/12/cross-account-access-control-with-amazon-sts-for-dynamodb/\nAfter read this article I consider A or D, but this is OLTP solution that A should be the right answer."},{"content":"D is right answer as it says access to specific fields. S3 access is object level.","comment_id":"103764","upvote_count":"1","comments":[{"content":"requires the LEAST amount of management work","poster":"notcloudguru","timestamp":"1635894780.0","upvote_count":"1","comment_id":"122250"},{"poster":"DerekKey","timestamp":"1636296180.0","upvote_count":"1","comment_id":"337574","content":"D is wrong - you have thousands of suppliers - it means that you have to create thousands of views"}],"poster":"k115","timestamp":"1635702600.0"},{"comments":[{"poster":"DerekKey","upvote_count":"1","timestamp":"1636107120.0","content":"Not TRUE -> Assume Role where the role has permission to selected subset of table data\nBTW there is no information about restrictions applied to temporary credentials, this way Federated Users and AWS Services would not be able to work","comment_id":"337571"}],"comment_id":"75846","timestamp":"1635399660.0","upvote_count":"2","content":"Answer : B\nNot A- Using temporary credentials you cannot access a Dynamo DB table using a fine-grained authorization policy. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html\nNot C – Its not manageable to create one table per supplier as there are thousands of supplier as per the question.\nNot D- This is a possible solution but not with least amount of management. You will need to create as many view as there are supplier roles in RedShift. \nB- This is the best choice- It uses the out-of-the-box STS functionality to associate the appropriate IAM Role to the temp user credentials for the user who logs in via SAML federation into AWS and then grants access to the files using resource based polices in S3.","poster":"Bulti"},{"upvote_count":"1","comment_id":"74081","poster":"susan8840","content":"B. least amount of work","timestamp":"1635385260.0"},{"timestamp":"1635110100.0","poster":"Galla","comment_id":"72234","upvote_count":"1","content":"Why not C?"},{"comment_id":"71935","content":"Answer is A\nB&C&D are out because it's a oltp scene\nany thoughts?","upvote_count":"3","poster":"jiedee","timestamp":"1635000240.0"},{"poster":"san2020","timestamp":"1634920140.0","upvote_count":"1","comment_id":"52383","content":"my selection D"},{"timestamp":"1634555700.0","comment_id":"42130","poster":"AdityaB","comments":[{"upvote_count":"1","poster":"AdamSmith","content":"Do you mean B?","timestamp":"1634890800.0","comment_id":"47177"}],"content":"D seems to be best answer -- \n\nHere are some of the things that you can build using fine-grained access control:\n\n A mobile app that displays information for nearby airports, based on the user’s location. The app can access and display attributes such airline names, arrival times, and flight numbers. However, it cannot access or display pilot names or passenger counts.\n A mobile game which stores high scores for all users in a single table. Each user can update their own scores, but has no access to the other ones.\nhttps://aws.amazon.com/blogs/aws/fine-grained-access-control-for-amazon-dynamodb/","upvote_count":"1"},{"timestamp":"1634451600.0","poster":"ME2000","upvote_count":"1","content":"Answer B\nthe LEAST amount of management work = Firehose + S3 + Lambda (Automated)\n the appropriate level of access control = AWS STS + security policies","comment_id":"38374"},{"timestamp":"1633788900.0","upvote_count":"5","comment_id":"36123","content":"There is no need to store data..we need only latest data at any point of time.\nOnce delivered - that timestamp has to be persisted. So Firehose and S3 combination can be eliminated. Left with A and C....\nIn DynamoDB, you have the option to specify conditions when granting permissions using an IAM policy (see Access Control). For example, you can:\n\nGrant permissions to allow users read-only access to certain items and attributes in a table or a secondary index.\n\nGrant permissions to allow users write-only access to certain attributes in a table, based upon the identity of that user.\nso A...","poster":"PK1234"},{"poster":"awsexpert","upvote_count":"1","timestamp":"1633754460.0","comment_id":"35506","content":"A is correct"},{"comment_id":"35156","timestamp":"1633747740.0","poster":"fabula898","comments":[{"poster":"zhengtoronto","timestamp":"1634597160.0","content":"Actually you can load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk","upvote_count":"2","comment_id":"45750"}],"content":"B is correct.\nFirehose cannot put records directly into Redshift.\nhttps://docs.aws.amazon.com/ja_jp/firehose/latest/dev/what-is-this-service.html#data-flow-diagrams","upvote_count":"1"},{"timestamp":"1633135200.0","upvote_count":"3","comment_id":"20413","poster":"BigEv","content":"I vote for D\n\"you can assign a different set of permissions to the view. A user might be able to query the view, but not the underlying table. Creating the view excluding the sensitive columns (or rows) should be useful in this scenario.\"\n\nhttp://www.silota.com/blog/rethink-database-views-redshift/"},{"poster":"asadao","content":"D no doubts","comment_id":"18265","timestamp":"1633113540.0","upvote_count":"4"},{"timestamp":"1632757980.0","poster":"M2","upvote_count":"4","comments":[{"poster":"Corram","comment_id":"101698","content":"Maintaining long term Redshift users causes more amount of management work than granting temporary access credentials.","timestamp":"1635510660.0","upvote_count":"3"}],"comment_id":"16031","content":"why no d? it is possible to restrict access using view."},{"timestamp":"1632105780.0","poster":"mattyb123","comments":[{"content":"B is the correct one","timestamp":"1632190020.0","comment_id":"6797","upvote_count":"2","poster":"Jialu"},{"comment_id":"7584","timestamp":"1632202620.0","poster":"pra276","content":"B is correct.","upvote_count":"3"}],"comment_id":"6780","upvote_count":"3","content":"Thoughts on D? It's either B or D due to least amount of management work."}],"question_images":[],"choices":{"B":"Send the tracking data to Amazon Kinesis Firehose. Use Amazon S3 notifications and AWS Lambda to prepare files in Amazon S3 with appropriate data for each suppliers roles. Generate temporary AWS credentials for the suppliers users with AWS STS. Limit access to the appropriate files through security policies.","A":"Send the tracking data to Amazon Kinesis Streams. Use AWS Lambda to store the data in an Amazon DynamoDB Table. Generate temporary AWS credentials for the suppliers users with AWS STS, specifying fine-grained security policies to limit access only to their applicable data.","D":"Send the tracking data to Amazon Kinesis Firehose. Store the data in an Amazon Redshift cluster. Create views for the suppliers users and roles. Allow suppliers access to the Amazon Redshift cluster using a user limited to the applicable view. B","C":"Send the tracking data to Amazon Kinesis Streams. Use Amazon EMR with Spark Streaming to store the data in HBase. Create one table per supplier. Use HBase Kerberos integration with the suppliers users. Use HBase ACL-based security to limit access for the roles to their specific table and columns."},"question_text":"A solutions architect for a logistics organization ships packages from thousands of suppliers to end customers.\nThe architect is building a platform where suppliers can view the status of one or more of their shipments.\nEach supplier can have multiple roles that will only allow access to specific fields in the resulting information.\nWhich strategy allows the appropriate level of access control and requires the LEAST amount of management work?","answer_description":"","question_id":61,"answer":"Explanation","answer_images":[],"exam_id":17,"timestamp":"2019-08-14 03:24:00","answer_ET":"Explanation","answers_community":[],"unix_timestamp":1565745840,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/3565-exam-aws-certified-big-data-specialty-topic-1-question-64/","topic":"1"},{"id":"4FVYght3CbIBJnLkFd46","topic":"1","answer_ET":"C","answer_description":"","answer_images":[],"unix_timestamp":1561444440,"discussion":[{"upvote_count":"9","content":"Agreed A. Line is the best to show change over a long time period","comment_id":"6474","timestamp":"1634125440.0","comments":[{"upvote_count":"4","poster":"pra276","comment_id":"7585","content":"Answer is A: Use line charts to compare changes in measure values over period of time \nhttps://docs.aws.amazon.com/quicksight/latest/user/line-charts.html","timestamp":"1634199660.0"}],"poster":"mattyb123"},{"comment_id":"74086","comments":[{"timestamp":"1635485940.0","poster":"Corram","content":"you mean A?","comment_id":"99321","upvote_count":"3"}],"content":"C because it focus on customer contacts for one region not all regions (a particular region)","timestamp":"1634980800.0","poster":"susan8840","upvote_count":"2"},{"upvote_count":"4","comment_id":"52384","timestamp":"1634700300.0","content":"my selection A","poster":"san2020"},{"comment_id":"20414","poster":"BigEv","content":"Ans is A\nThe concern is the workload on the weekend, so a time-series chart is required","upvote_count":"3","timestamp":"1634518500.0"},{"upvote_count":"1","poster":"cybe001","comment_id":"19404","content":"Answer is A, line graphs are for trends","timestamp":"1634416800.0"},{"poster":"Depu","comment_id":"3143","content":"Why not A? How trend can be shown on geographical map?","timestamp":"1633384800.0","upvote_count":"4"}],"timestamp":"2019-06-25 08:34:00","exam_id":17,"question_id":62,"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/1995-exam-aws-certified-big-data-specialty-topic-1-question-65/","question_images":[],"isMC":true,"answers_community":[],"choices":{"A":"A line graph plotting customer contacts vs. time, with a line for each region","D":"A bar graph plotting region vs. volume of social media contacts","C":"A map of regions with a heatmap overlay to show the volume of customer contacts","B":"A pie chart per region plotting customer contacts per day of week"},"question_text":"A companys social media manager requests more staff on the weekends to handle an increase in customer contacts from a particular region. The company needs a report to visualize the trends on weekends over the past 6 months using QuickSight.\nHow should the data be represented?"},{"id":"16Rm49CsAi06M4dZG449","discussion":[{"timestamp":"1633707240.0","upvote_count":"8","content":"B looks to be the right answer.\n\nNotes: \n(*) GZIP compression uses more CPU resources than Snappy or LZO, but provides a higher compression ratio. GZip is often a good choice for cold data, which is accessed infrequently. \n(*) Snappy or LZO are a better choice for hot data, which is accessed frequently.\n(*) BZip2 can also produce more compression than GZip for some types of files, at the cost of some speed when compressing and decompressing. \n(*) For MapReduce, if you need your compressed data to be splittable, BZip2, LZO, and Snappy formats are splittable, but GZip is not. \n\nRefer : https://docs.cloudera.com/documentation/enterprise/5-3-x/topics/admin_data_compression_performance.html","poster":"viduvivek","comment_id":"19486"},{"timestamp":"1635941820.0","content":"Ans is D. A 5GB GZIP file will turn out be bigger when compressed with SNAPPY. And snappy is not splittable","comment_id":"150135","poster":"Royk2020","upvote_count":"1"},{"comments":[{"timestamp":"1634919480.0","upvote_count":"1","comment_id":"123444","content":"B is still valid:\n\nBzip2 if splitting.\nSnappy if compressing.","poster":"notcloudguru"}],"timestamp":"1634776560.0","poster":"Corram","comment_id":"100319","upvote_count":"1","content":"D was my choice. Bzip2 is splittable, but snappy is not. So B seems odd. Avro is splittable, so this conversion should help."},{"comment_id":"87247","poster":"emailtorajivk","timestamp":"1634755380.0","upvote_count":"1","content":"For\ninstance, if you are aggregating your data (using the ingest tool of your choice) and the aggregated data files are\nLarge File\nHTTP Range Request: 64MB\nHTTP Range Request: 64MB\nHTTP Range Request: 64MB\nHTTP Range Request: 64MB\nMap Task\nMap Task\nMap Task\nMap Task\nEMR Cluster\nS3 Bucket\nAmazon Web Services – Best Practices for Amazon EMR August 2013\nPage 16 of 38\nbetween 500 MB to 1 GB, GZIP compression is an acceptable data compression type. However, if your data aggregation\ncreates files larger than 1 GB, its best to pick a compression algorithm that supports splitting.\n\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf"},{"content":"my selection B","comment_id":"52148","timestamp":"1634588580.0","upvote_count":"2","poster":"san2020"},{"poster":"kalpanareddy","timestamp":"1634141160.0","content":"Answer is B\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf","comment_id":"41804","upvote_count":"1"},{"upvote_count":"3","comment_id":"14956","timestamp":"1633478340.0","content":"B looks correct. bzip2 splittable or snappy compress-decompress speed very fast","poster":"M2"},{"upvote_count":"1","comment_id":"14289","content":"Snappy is not split table as well however it does compression and decompression quickly as compare to gzip so Answer would still be B.","timestamp":"1632485160.0","poster":"bigdatalearner"},{"comment_id":"13588","poster":"bigdatalearner","upvote_count":"3","timestamp":"1632359280.0","content":"B is right answer : reason Bzip2 and snappy can split files and gzip can't split"},{"poster":"exams","content":"B looks good","upvote_count":"3","timestamp":"1632101880.0","comment_id":"11397"}],"answer_images":[],"unix_timestamp":1568709120,"answer":"B","choices":{"D":"Use Avro rather than gzip for the archives.","B":"Use bzip2 or Snappy rather than gzip for the archives.","C":"Decompress the gzip archives and store the data as CSV files.","A":"Reduce the HDFS block size to increase the number of task processors."},"url":"https://www.examtopics.com/discussions/amazon/view/5299-exam-aws-certified-big-data-specialty-topic-1-question-7/","question_text":"A large grocery distributor receives daily depletion reports from the field in the form of gzip archives od CSV files uploaded to Amazon S3. The files range from 500MB to 5GB. These files are processed daily by an EMR job.\nRecently it has been observed that the file sizes vary, and the EMR jobs take too long. The distributor needs to tune and optimize the data processing workflow with this limited information to improve the performance of the\nEMR job.\nWhich recommendation should an administrator provide?","answer_description":"","isMC":true,"question_id":63,"question_images":[],"exam_id":17,"timestamp":"2019-09-17 10:32:00","answer_ET":"B","topic":"1","answers_community":[]},{"id":"Mdwe702LlUs1kOP10Ukp","question_images":[],"choices":{"D":"After receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis PutRecord API. Use the exponential back-off algorithm for retries until a successful response is received.","C":"Each web server buffers the requests until the count reaches 500 and sends them to Amazon Kinesis using the Amazon Kinesis PutRecord API.","A":"After receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis PutRecord API. Use the sessionID as a partition key and set up a loop to retry until a success response is received.","B":"After receiving a request, each web server sends it to Amazon Kinesis using the Amazon Kinesis Producer Library .addRecords method."},"answers_community":[],"answer_description":"","discussion":[{"comment_id":"46009","upvote_count":"6","poster":"AdamSmith","comments":[{"timestamp":"1634804820.0","upvote_count":"2","poster":"srirampc","content":"PutRecords is right. Not sure if there a .addrecords","comment_id":"81064"}],"timestamp":"1634123760.0","content":"The KPL provides the following features out of the box:\n\n Batching of puts using PutRecords (the Collector in the architecture diagram)\n Tracking of record age and enforcement of maximum buffering times (all components)\n Per-shard record aggregation (the Aggregator)\n Retries in case of errors, with ability to distinguish between retryable and non-retryable errors (the Retrier)\n Per-shard rate limiting to prevent excessive and pointless spamming (the Limiter)\n Useful metrics and a highly efficient CloudWatch client (not shown in diagram)\n\nhttps://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\n\nthe answer should be B"},{"content":"Keywords : near-real-time, most reliable and fault-tolerant technique\nAnswer is B : Kinesis Producer Library","upvote_count":"1","poster":"jove","comment_id":"149335","timestamp":"1636060860.0"},{"content":"B is right answer.\n\nThere seems to be typo in question - it must be addUserRecord method. https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/","upvote_count":"1","poster":"hdesai","timestamp":"1636028640.0","comment_id":"134475"},{"upvote_count":"2","content":"D\nhttps://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html","comment_id":"130710","timestamp":"1635906300.0","poster":"jsr2017"},{"comment_id":"96424","content":"The answer should be B however KPL does not have a method named .addRecords. It looks like typo, as the KPL method is addUserRecord.","poster":"agm84","upvote_count":"4","timestamp":"1635515040.0"},{"comment_id":"96081","upvote_count":"4","content":"D is the correct answer \nA,is worng need to setup back of retry .\nB. dont' exist addrecords api\nC.it is anti-patern","poster":"kkyong","timestamp":"1635468000.0"},{"comment_id":"95302","content":"A is correct. B - there is no addrecords in KPL. D - There is no backoff algorithm in Kinesis Agent.","upvote_count":"1","poster":"Debi_mishra","timestamp":"1635352620.0"},{"content":"After researching further, I think B is the right answer. KPL has built in fault tolerance with configurable retry mechanism and there is no need to write custom fault-tolerant logic to do so. Besides KPL allows us to batch the records ( aggregation and collection) out of the box without having to write custom code to achieve scalability.","comment_id":"76596","upvote_count":"2","timestamp":"1634710620.0","poster":"Bulti","comments":[{"upvote_count":"2","content":"There is no addRecords method in KPL, so B is wrong.","timestamp":"1635879660.0","comment_id":"100332","poster":"Corram"}]},{"comment_id":"75014","poster":"Bulti","comments":[{"timestamp":"1636087620.0","comment_id":"225446","content":"Agree with D. Reference: https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/","upvote_count":"1","poster":"Anjoy"}],"upvote_count":"4","timestamp":"1634626260.0","content":"D is the right answer. You need to do a back off with retry. You can do it exponentially. This usually happens due to a hot partition. To ensure fault tolerance if using an PutRecord API you will need to handle ProvisionedThroughputExceeded exceptions and the way to do that is to use backoff with retry mechanism. \n\nOption A seems to suggest we are doing a retry but in a loop. However if we don't do a back-off (meaning wait for a certain duration) before we retry, the system would continue to fail."},{"upvote_count":"2","poster":"YashBindlish","content":"B is the Correct Answer","comments":[{"timestamp":"1635847380.0","content":"There is no .addRecords method in KPL.","comments":[{"poster":"MihirB","timestamp":"1636195200.0","content":"Is it that option B implies . addUserRecord() method when it refers to .addRecord() method, because if that is the case, the correct answer should be B.","comment_id":"253016","upvote_count":"1"}],"upvote_count":"1","comment_id":"100328","poster":"Corram"}],"timestamp":"1634471340.0","comment_id":"74481"},{"upvote_count":"1","timestamp":"1634470620.0","content":"Answer is B. KPL is used when\nHigh performance, long-running producers\nAutomated and configurable retry mechanism\nSync and Async API(better perf for Async)\n100B records (high volume of data)","comment_id":"65433","poster":"axlrose"},{"poster":"san2020","timestamp":"1634260380.0","comment_id":"52149","content":"my selection A","upvote_count":"1"},{"content":"why not B? seems to me you need to reinvent the wheel in A while kpl can do all that already","comments":[{"content":"There is no .addRecords method in KPL.","comment_id":"100324","timestamp":"1635753060.0","upvote_count":"2","poster":"Corram"}],"poster":"hailiang","timestamp":"1633898280.0","upvote_count":"2","comment_id":"34427"},{"upvote_count":"3","comments":[{"timestamp":"1633834740.0","comment_id":"20470","content":"replying to myself -> there is no exponential back-off algorithm for KPL. It uses a more aggressive strategy to do retries. A is correct.","upvote_count":"5","poster":"d00ku"}],"content":"exponential back-off algorithm appears in answer D -> this seems to be the correct one.","timestamp":"1633797540.0","poster":"d00ku","comment_id":"19839"},{"poster":"M2","upvote_count":"2","timestamp":"1632834960.0","content":"A looks correct.","comment_id":"14957"},{"content":"A is correct","comments":[{"content":"The loop in A might create spamming due to excessive retries,. For example, KPL has a Rate limiting feature to deal with this, but our manual soultion here does not. Therefore A looks wrong to me.","comment_id":"100333","upvote_count":"1","poster":"Corram","timestamp":"1635889140.0"}],"upvote_count":"4","comment_id":"11398","poster":"exams","timestamp":"1632765780.0"},{"content":"Option A is correct because there is concept of back-off algorithm in Kinesis\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-producer-adv-retries-rate-limiting.html","poster":"kn","timestamp":"1632121080.0","comment_id":"4058","upvote_count":"4","comments":[{"timestamp":"1632721020.0","poster":"Jialu","upvote_count":"3","content":"A is the correct answer","comment_id":"6791"}]}],"topic":"1","isMC":true,"unix_timestamp":1563072900,"answer":"A","question_id":64,"timestamp":"2019-07-14 04:55:00","answer_images":[],"exam_id":17,"question_text":"A web-hosting company is building a web analytics tool to capture clickstream data from all of the websites hosted within its platform and to provide near-real-time business intelligence. This entire system is built on\nAWS services. The web-hosting company is interested in using Amazon Kinesis to collect this data and perform sliding window analytics.\nWhat is the most reliable and fault-tolerant technique to get each website to send data to Amazon Kinesis with every click?","url":"https://www.examtopics.com/discussions/amazon/view/2485-exam-aws-certified-big-data-specialty-topic-1-question-8/","answer_ET":"A"},{"id":"wVyxuc3nTWwxogp9DE1m","choices":{"B":"Define a Primary Key with StreamName as Partition Key and TimeStamp followed by ServerName as Sort Key. Define a Global Secondary Index with ServerName as partition key and TimeStamp followed by StreamName.","D":"Define a Primary Key with ServerName as Partition Key. Define a Local Secondary Index with TimeStamp as Partition Key. Define a Global Secondary Index with StreamName as Partition Key and TimeStamp as Sort Key.","A":"Define a Primary Key with ServerName as Partition Key and TimeStamp as Sort Key. Do NOT define a Local Secondary Index or Global Secondary Index.","C":"Define a Primary Key with ServerName as Partition Key. Define a Local Secondary Index with StreamName as Partition Key. Define a Global Secondary Index with TimeStamp as Partition Key."},"question_text":"A customer has an Amazon S3 bucket. Objects are uploaded simultaneously by a cluster of servers from multiple streams of data. The customer maintains a catalog of objects uploaded in Amazon S3 using an\nAmazon DynamoDB table. This catalog has the following fileds: StreamName, TimeStamp, and ServerName, from which ObjectName can be obtained.\nThe customer needs to define the catalog to support querying for a given stream or server within a defined time range.\nWhich DynamoDB table scheme is most efficient to support these queries?","isMC":true,"question_images":[],"answer_ET":"A","topic":"1","answer":"A","discussion":[{"poster":"mattyb123","upvote_count":"7","timestamp":"1632196920.0","content":"Option B. You can use composite primary keys using a combination of (StreamName as the partition key and TimeStamp as the sort key) and (ServerName as the partition key and TimeStamp as the sort key) which would allow you to meet the requirements of the question.","comments":[{"poster":"pra276","timestamp":"1632624720.0","comment_id":"7650","upvote_count":"1","comments":[{"content":"I think b has typo . no need to have a sort key like in B. \nin my opinion, or means that both is needed but not at the same time.","timestamp":"1633225200.0","comment_id":"8574","poster":"muhsin","comments":[{"comment_id":"11399","upvote_count":"1","poster":"exams","timestamp":"1633381260.0","content":"With all the confusion, I still think B is the answer"}],"upvote_count":"2"},{"comment_id":"7721","content":"Can't you create a primary key on two fields so combine streamname + timestamp as hash/primary key then create the sort key on server name. Then you can create your GSI table with no problem. Just my thoughts, as the question is written quite badly to deceive you. When i attempted the test last time i did choose A but didnt score too well for Collection or Storage section so i was thinking B as it also you to look up the streamname.","poster":"mattyb123","upvote_count":"1","timestamp":"1633132800.0"}],"content":"How can you create this? Define a Primary Key with StreamName as Partition Key and TimeStamp followed by ServerName as Sort Key. You can create streamname as partition key and timestamp as sortkey and cannot be timestamp and servername together as sort key. The question tells customer needs to define the catalog to support querying for a given stream or Server. Here it is OR not mandatory to be both so better option i would think is A."},{"upvote_count":"1","timestamp":"1634577960.0","comment_id":"40573","content":"You are right in terms of the best approaches. Anyway, pay attention to this \"and TimeStamp followed by ServerName as Sort Key\" - this is absolutely wrong.","poster":"practicioner"}],"comment_id":"6639"},{"timestamp":"1636217340.0","content":"C and D - Wrong, because LSI wrongly mentioned with different partition key. LSI has same partition key.\nA - Wrong because it does not allow query based on stream name. GSI is must for this\nB - Correct by compromise, but phrasing is so messed up. I read it as below. \nPrimary table : Partition Key : Stream Name, Sort Key : Timestamp, Attribute : Serrvername\nGSI : Partition Key: Server name, Sort Key: Timestamp, Attribute: Stream\nI read it like this because, \"timestamp, followed by servername, as Sort key\". Two comma missing, so Sort Key was mentioned for Timestamp, and followed by is a just a an additional attribute on same table , after sort key","upvote_count":"1","poster":"Abhi09","comment_id":"150217"},{"poster":"fw","upvote_count":"3","comment_id":"97125","timestamp":"1635960420.0","content":"Option B.\nThe sort key can be composite. Although it looks ugly.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html"},{"content":"None of them are exact solution. A sounds more appropriate...but still doesnt address query using StreamName","timestamp":"1635934140.0","upvote_count":"1","comment_id":"95320","poster":"Debi_mishra"},{"upvote_count":"1","content":"Refer to this link to confirm that you can create multiple LSIs on a table.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html","comment_id":"75057","timestamp":"1635926280.0","poster":"Bulti"},{"poster":"Bulti","comments":[{"comment_id":"101846","upvote_count":"1","content":"Wording should be corrected: You don't create 2 LSI's, but one sort key (timestamp) and one LSI (ServerName or StreamName, resp.)","timestamp":"1636068780.0","poster":"Corram"}],"content":"Option B. None of the option suggest the use of ServerName as a GSI which is requirement except for Option B. What is throwing people off is the phrase \" TimeStamp followed by ServerName as Sort Key\" and \" TimeStamp followed by StreamName\" in Option B. My interpretation is that it is suggesting we create 2 LSIs 1) TimeStamp 2) ServerName when StreamName is the partitionKey and similarly create 2 LSIs 1) TimeStamp and 2) StreamName when ServerName is the partitionKey. So I will go with Option B","comment_id":"75056","timestamp":"1635903360.0","upvote_count":"1"},{"timestamp":"1635893760.0","comment_id":"74485","upvote_count":"1","poster":"YashBindlish","content":"B is the Correct Answer"},{"poster":"IDvarma","content":"Option A.\n(Read this section: Example 1: Working with log data)\nhttps://aws.amazon.com/blogs/database/using-sort-keys-to-organize-data-in-amazon-dynamodb/","upvote_count":"1","timestamp":"1635613200.0","comment_id":"65449","comments":[{"timestamp":"1635678060.0","content":"But it mentions \"compound sort key\" concept in Example 2: Working with chat messages, which seems to refer to option B.","poster":"oscarchichun","upvote_count":"1","comment_id":"68467"}]},{"timestamp":"1635202500.0","content":"B!\nComposite Primary Key = partitionKey+compositeSortKey\nStreamNmae + ServerName#TImeStamp, ServerName + StreamName#TimeStamp","comment_id":"60188","poster":"drneon","upvote_count":"3"},{"comment_id":"52313","upvote_count":"2","timestamp":"1635048780.0","poster":"san2020","content":"my selection B"},{"upvote_count":"1","poster":"ME2000","timestamp":"1634540940.0","content":"Eliminate wrong answers.\nSo A is the only visible and correct answer.","comment_id":"38846"},{"content":"A is correct since the requirement states Server OR Stream. Answer accommodates the requirement.","upvote_count":"1","comments":[{"content":"Good catch so is the final answer A ?","comment_id":"51032","poster":"venkataws","timestamp":"1634907780.0","upvote_count":"1"},{"timestamp":"1634133240.0","comment_id":"30294","content":"\"given stream or server within a defined time range\" can be understood as query in different queries, not at the same time, especially in the context of DynamoDB with multiple possible query patters would make absolute sense","upvote_count":"2","poster":"yuriy_ber"}],"timestamp":"1634114160.0","comment_id":"19841","poster":"d00ku"},{"content":"It is A","upvote_count":"1","comment_id":"18251","timestamp":"1634076540.0","poster":"asadao"},{"timestamp":"1633950780.0","poster":"M2","comment_id":"14958","content":"for c and d, LSI & GSI both are not required\nfor a, there is no way of querying streamname \n b is correct answer.","upvote_count":"1"},{"upvote_count":"2","timestamp":"1632597420.0","poster":"pra276","content":"A is correct.","comment_id":"7514"},{"upvote_count":"3","poster":"muhsin","comment_id":"6882","content":"no way of querying StreamName at option A. A is wrong","timestamp":"1632299160.0"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/3513-exam-aws-certified-big-data-specialty-topic-1-question-9/","timestamp":"2019-08-13 03:31:00","answer_description":"","question_id":65,"answers_community":[],"exam_id":17,"unix_timestamp":1565659860}],"exam":{"isMCOnly":true,"provider":"Amazon","isImplemented":true,"numberOfQuestions":85,"id":17,"name":"AWS Certified Big Data - Specialty","lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":13},"__N_SSP":true}