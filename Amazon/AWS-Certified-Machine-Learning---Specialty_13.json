{"pageProps":{"questions":[{"id":"xvXuj0TaQDAgzqRE7Kjq","answers_community":["D (90%)","10%"],"question_text":"A manufacturing company asks its machine learning specialist to develop a model that classifies defective parts into one of eight defect types. The company has provided roughly 100,000 images per defect type for training. During the initial training of the image classification model, the specialist notices that the validation accuracy is 80%, while the training accuracy is 90%. It is known that human-level performance for this type of image classification is around 90%.\nWhat should the specialist consider to fix this issue?","topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/75020-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"isMC":true,"question_id":61,"answer":"D","choices":{"B":"Making the network larger","D":"Using some form of regularization","A":"A longer training time","C":"Using a different optimizer"},"question_images":[],"answer_ET":"D","discussion":[{"comment_id":"595655","upvote_count":"16","content":"D - over fitting problem.","timestamp":"1667326440.0","poster":"bluer1"},{"content":"Selected Answer: D\nThe specialist should consider using some form of regularization to fix this issue. Regularization techniques such as dropout or L2 regularization can help prevent overfitting, which can occur when the model performs well on the training data but poorly on the validation data.\n\nOption A, a longer training time, might not necessarily fix the issue and could lead to overfitting if the model is already performing well on the training data.\n\nOption B, making the network larger, could also lead to overfitting and may not be necessary if the current network architecture is sufficient to perform the classification task.\n\nOption C, using a different optimizer, might not necessarily fix the issue and could lead to slower convergence or worse performance.\n\nTherefore, option D, using some form of regularization, is the most appropriate solution to consider in this situation.","timestamp":"1692257700.0","poster":"AjoseO","upvote_count":"6","comment_id":"811694"},{"upvote_count":"1","poster":"vkbajoria","content":"Selected Answer: D\nsome form of regularization","comment_id":"1187074","timestamp":"1727737860.0"},{"upvote_count":"1","poster":"giustino98","comment_id":"1061994","timestamp":"1714809240.0","content":"Selected Answer: B\nI wouldn't go with D since it doesn't seem an overfitting problem considering training accuracy is not so high. So the main problem here is to get an higher accuracy even on training set. I would go with A or B"},{"timestamp":"1707560040.0","poster":"ArturoZapatero","comment_id":"977422","upvote_count":"1","content":"A - IMO it's an underfitting problem, as training accuracy is not better than baseline error (human accuracy). Would consider B as well, but it may actually decrease accuracy."},{"upvote_count":"1","content":"Selected Answer: D\ntypical overfitting problem","comment_id":"965803","poster":"Mickey321","timestamp":"1706481000.0"},{"upvote_count":"1","poster":"ystotest","comment_id":"726815","content":"Selected Answer: D\ntypical overfitting problem","timestamp":"1685015520.0"},{"content":"C - It is not a overfitting problem as the training accuracy stands at 90%, which is at same level of human performance. That means the algorithm used is not optimized for this problem. So, some other algorithm should applied for this problem.","timestamp":"1679779440.0","poster":"DD4","comment_id":"679168","upvote_count":"2"},{"upvote_count":"2","timestamp":"1672837080.0","poster":"KlaudYu","comment_id":"626957","content":"I'd go A. Regularization could not guarantee higher validation accuracy."},{"timestamp":"1670858460.0","upvote_count":"1","comments":[{"content":"I mean looks like a overfitting problem....","timestamp":"1670859000.0","upvote_count":"2","comment_id":"615337","poster":"rhuanca"}],"poster":"rhuanca","content":"I believe answer is B , because clearly it is a overfiting problem , if we reduce complexity the accurate will reduce close to 80% ... But human works can reach up to 90% .","comment_id":"615334"}],"timestamp":"2022-05-01 18:14:00","answer_images":[],"unix_timestamp":1651421640},{"id":"gtvPEQ86FZHD5ifqaeKy","url":"https://www.examtopics.com/discussions/amazon/view/74991-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"E":"Use Amazon Translate to translate from Spanish to English, if necessary. Use Amazon SageMaker Neural Topic Model (NTM) to find the topics.","B":"Use an Amazon SageMaker seq2seq algorithm to translate from Spanish to English, if necessary. Use a SageMaker Latent Dirichlet Allocation (LDA) algorithm to find the topics.","A":"Use an Amazon SageMaker BlazingText algorithm to find the topics independently from language. Proceed with the analysis.","C":"Use Amazon Translate to translate from Spanish to English, if necessary. Use Amazon Comprehend topic modeling to find the topics.","D":"Use Amazon Translate to translate from Spanish to English, if necessary. Use Amazon Lex to extract topics form the content."},"discussion":[{"comment_id":"598379","comments":[{"poster":"tgaos","comment_id":"608600","timestamp":"1653791100.0","content":"The SageMaker seq2seq algorithm is a supervised learning algorithm. And it needs to train then translate. translate can directly use to translate from Spanish to English","upvote_count":"7"},{"timestamp":"1671591000.0","comment_id":"751809","poster":"hamimelon","content":"The question did not say you cannot build a custom model. They have a ML specialist, so building a custom model shouldn't be a problem.","upvote_count":"3"}],"upvote_count":"28","poster":"LydiaGom","timestamp":"1651966680.0","content":"C and E\nB needs to build custom model"},{"timestamp":"1651344960.0","content":"It asked 2 answers, but I can see only one answer. Please advise. Thanks!","upvote_count":"9","comment_id":"595219","poster":"NILKK"},{"comment_id":"1411107","timestamp":"1743123360.0","poster":"Carpediem78","content":"Selected Answer: E\n(Choose two.)\nC,E","upvote_count":"1"},{"poster":"MultiCloudIronMan","upvote_count":"1","content":"Selected Answer: C\nC E - Amazon Translate can handle the translation from Spanish to English, ensuring all comments are in a single language.\nAmazon Comprehend provides robust topic modeling capabilities to identify the most discussed topics in the translated comments.\nUse Amazon Translate to translate from Spanish to English, if necessary. Use Amazon SageMaker Neural Topic Model (NTM) to find the topics.\n\n\nAmazon SageMaker Neural Topic Model (NTM) is an unsupervised learning algorithm designed for topic modeling, which can effectively identify topics in the translated comments","timestamp":"1729518360.0","comment_id":"1301035"},{"poster":"amlgeek","content":"CE are the answers.\nC use Amazon Comprehend for topic modeling - use LDA (https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html)\nE is using NTM (https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html)\nFor ease of use, I will start with Amazon Comprehend and go to NTM if the success criteria isn't met.","comment_id":"1294512","upvote_count":"1","timestamp":"1728348840.0"},{"poster":"MJSY","comment_id":"1292633","upvote_count":"1","content":"Selected Answer: E\nC, E is the correct answer.","timestamp":"1727930040.0"},{"comment_id":"1226765","poster":"Chiquitabandita","upvote_count":"1","content":"Selected Answer: E\nC & E based on comments, but you are not allowed to select multiple choices.","timestamp":"1717856700.0"},{"upvote_count":"1","timestamp":"1708712280.0","comment_id":"1157375","content":"C and E \n- Use translate so that text is in common language\n- In options with translate only Comprehend and NTM allow for topic modeling (C & E)\nOther options Blazingtext is for text classification, not topic modelling, LDA is requireres user specified topics and Lex is for conversational interfaces","poster":"AIWave"},{"content":"Selected Answer: C\nC & E\nOption C (Amazon Translate and Amazon Comprehend): This is a strong combination. Amazon Translate can be used to translate Spanish comments into English, and then Amazon Comprehend, which supports topic modeling, can be used to identify the most discussed topics.\n\nOption E (Amazon Translate and Amazon SageMaker Neural Topic Model): This is also a viable combination. Amazon Translate would handle the translation of Spanish comments, and the Neural Topic Model (NTM) in Amazon SageMaker can then be used for topic modeling. NTM uses neural networks for topic discovery and is well-suited for analyzing large sets of text data.","timestamp":"1704980880.0","poster":"CloudHandsOn","comment_id":"1119833","upvote_count":"2"},{"timestamp":"1699954440.0","upvote_count":"1","content":"B and E\nI dont think amazon comprehend can do topic modelling.\nLDA is used for topic modelling","comment_id":"1070186","poster":"geoan13"},{"timestamp":"1695639060.0","poster":"ixdb","comment_id":"1016716","content":"BCE are all right. https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/algos.html\nLDA and NTM are all topic modeling tools.","upvote_count":"1"},{"timestamp":"1695122220.0","upvote_count":"1","poster":"loict","comment_id":"1011220","content":"Selected Answer: E\nA. NO - BlazingText is word2vec, will not do topic modeling alone\nB. NO - Translate better than custom seq2seq\nC. NO - NTM better than LDA used by Comprehend\nD. NO - Lex is for chatbots\nE. YES"},{"comment_id":"1004544","timestamp":"1694419080.0","poster":"teka112233","content":"Selected Answer: E\nThe right answers are C & E\nThe other steps are not suitable because:\nA. The BlazingText algorithm is for word embeddings and text classification, not topic modeling.\nB. The LDA algorithm is an unsupervised learning algorithm that requires a user-specified number of topics.\nD. Amazon Lex is for building conversational interfaces, not extracting topics from content","upvote_count":"1"},{"comment_id":"1000559","timestamp":"1694001060.0","poster":"Sharath1783","upvote_count":"3","content":"Selected Answer: B\nCorrect answer is BE"},{"poster":"chet100","timestamp":"1693084920.0","content":"It has to be B + C .. for spanish to English use Translate. For Topics it has to be LDA.","upvote_count":"1","comment_id":"991054","comments":[{"comment_id":"991059","content":"Sorry and NTM.. in that case, C is a winner for translation.. then pick E to be consistent.. final answer B + E.","timestamp":"1693085160.0","upvote_count":"1","poster":"chet100"}]},{"poster":"kaike_reis","upvote_count":"3","timestamp":"1691587740.0","content":"Selected Answer: E\nFor me: B - C - E are correct: it's solved translation + topic modelling.\nThe question is not well construct from my POV.","comment_id":"976689"},{"content":"Selected Answer: C\nC and E","timestamp":"1690577880.0","comment_id":"965825","poster":"Mickey321","upvote_count":"1"},{"poster":"ADVIT","timestamp":"1688616900.0","upvote_count":"1","content":"B + E, I think","comments":[{"timestamp":"1688617020.0","comment_id":"944312","poster":"ADVIT","upvote_count":"1","content":"Per https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html it's C +E."}],"comment_id":"944309"},{"content":"Selected Answer: C\nC and E","comment_id":"936279","poster":"SRB1337","timestamp":"1687937580.0","upvote_count":"1"},{"comment_id":"884341","poster":"daidaidai","upvote_count":"1","timestamp":"1682773680.0","content":"Selected Answer: C\nC and E, The most common topic modeling algorithm is called Latent Dirichlet Allocation (LDA), Amazon Comprehend uses a Latent dirichlet allocation-based learning model to determine the topics in a set of documents. https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html.\nDespite its popularity and success, one limitation of this algorithm is that it makes an assumption that the distribution of words in a document follows a Dirichlet distribu- tion.The NTM algorithm relaxes this assumption and aims to learn a latent representa- tion without prior assumptions."},{"comment_id":"811688","upvote_count":"1","poster":"AjoseO","content":"Option A suggests using an unsupervised learning algorithm, BlazingText, to find the topics independently of language. This can be a good approach if the content is in multiple languages and we don't want to translate them first.\n\nOption C suggests using Amazon Comprehend, which can detect the language and then perform topic modeling on the content in that language. If any comments are in Spanish, they will be translated to English using Amazon Translate before being fed to Amazon Comprehend.\n\nTogether, these options cover both cases - comments in English and comments in Spanish, without having to translate everything.","timestamp":"1676626020.0","comments":[{"upvote_count":"1","comment_id":"824347","poster":"GiyeonShin","content":"In Option A, BlazingText has two modes: word2vec, text classification.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\nWord2Vec is an unsupervised learning algorithm, but it is for word embeddings, so it is not appropriate to find the topics from corpus. \nText classfication isn't also appropriate to find the topics, because there is no given specified topics. So Option A is not an answer","timestamp":"1677554580.0"}]},{"poster":"CertArvind","upvote_count":"3","content":"Answer should be B and C [B,C]","comment_id":"791922","timestamp":"1675016460.0"},{"upvote_count":"4","comment_id":"781952","poster":"BTRYING","timestamp":"1674198780.0","content":"Selected Answer: C\nCE - https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html"},{"timestamp":"1673908980.0","upvote_count":"3","comment_id":"778350","poster":"Tomatoteacher","content":"Selected Answer: E\nC and E, Comprehend uses LDA, and manually creating seq2seq is unnecessary here. \nTis truly C and E. Please change options so you can choose 2 answers pls.","comments":[{"content":"If the question specifies \"least development\" you are correct. For me: B-C-E are correct. The question is missing info.","upvote_count":"2","poster":"kaike_reis","comment_id":"976692","timestamp":"1691587800.0"}]},{"comment_id":"776948","poster":"yemauricio","timestamp":"1673808840.0","upvote_count":"2","content":"Selected Answer: C\nbest answer"},{"content":"Selected Answer: C\nAmazon Comprehend uses a Latent dirichlet allocation-based learning model to determine the topics in a set of documents. It examines each document to determine the context and meaning of a word. The set of words that frequently belong to the same context across the entire document set make up a topic.","upvote_count":"3","timestamp":"1669560120.0","comment_id":"728340","poster":"ystotest"},{"comment_id":"631497","timestamp":"1657823880.0","poster":"Morsa","content":"B and D as two answers were expected. LDA and NTM are algorithms that build topics","upvote_count":"1"},{"upvote_count":"2","comments":[{"comment_id":"629732","poster":"rjekstein","content":"C and E Amazon comprehend topic modeling can be unspervised.\nhttps://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html\n\nYou can use Amazon Comprehend to examine the content of a collection of documents to determine common themes. For example, you can give Amazon Comprehend a collection of news articles, and it will determine the subjects, such as sports, politics, or entertainment. The text in the documents doesn't need to be annotated.","timestamp":"1657486020.0","upvote_count":"7"}],"comment_id":"622281","timestamp":"1656198600.0","poster":"ovokpus","content":"Selected Answer: B\nB or E, changed my mind, because Comprehend is supervised while LDA and NTM are unsupervised. There is no mention of training and labeling in the question"},{"poster":"ovokpus","upvote_count":"5","comment_id":"621328","content":"C & E seems the best choices here","timestamp":"1656019980.0"},{"upvote_count":"2","content":"B and E","timestamp":"1655741820.0","poster":"[Removed]","comment_id":"619372"},{"comment_id":"598970","content":"Selected Answer: B\nB and C\nhttps://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html","timestamp":"1652090400.0","upvote_count":"4","poster":"siju13"},{"content":"Selected Answer: D\nI would choose B and D. Amazon Lex is able to extract topic.","upvote_count":"1","comment_id":"598407","timestamp":"1651974780.0","poster":"DJiang"},{"timestamp":"1651502880.0","upvote_count":"1","comment_id":"596088","content":"Selected Answer: B\nB and E","poster":"rohit07cf"}],"answer_images":[],"answer":"C","answer_ET":"C","topic":"1","question_text":"A machine learning specialist needs to analyze comments on a news website with users across the globe. The specialist must find the most discussed topics in the comments that are in either English or Spanish.\nWhat steps could be used to accomplish this task? (Choose two.)","isMC":true,"answer_description":"","question_images":[],"answers_community":["C (41%)","E (30%)","B (27%)","3%"],"timestamp":"2022-04-30 20:56:00","unix_timestamp":1651344960,"exam_id":26,"question_id":62},{"id":"xPRJFeeXKUFqZR6dad61","question_id":63,"unix_timestamp":1652180760,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/75415-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"B","question_text":"A machine learning (ML) specialist is administering a production Amazon SageMaker endpoint with model monitoring configured. Amazon SageMaker Model\nMonitor detects violations on the SageMaker endpoint, so the ML specialist retrains the model with the latest dataset. This dataset is statistically representative of the current production traffic. The ML specialist notices that even after deploying the new SageMaker model and running the first monitoring job, the SageMaker endpoint still has violations.\nWhat should the ML specialist do to resolve the violations?","discussion":[{"upvote_count":"6","comments":[{"timestamp":"1669696620.0","upvote_count":"3","content":"Agree, the answer is B. \nFrom the document, the violation file contains several checks and \"The violations file is generated as the output of a MonitoringExecution\" . https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-violations.html.","comment_id":"608605","poster":"tgaos"}],"poster":"edvardo","content":"I would go with B:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-create-baseline.html","timestamp":"1668085560.0","comment_id":"599555"},{"poster":"AIWave","upvote_count":"2","timestamp":"1724431080.0","comment_id":"1157384","content":"Selected Answer: B\nThe baseline job computes baseline statistics and constraints for the new training set. By using this updated baseline, Model Monitor can better detect any drift or violations in the production traffic."},{"upvote_count":"3","poster":"CloudHandsOn","comment_id":"1119866","timestamp":"1720701660.0","content":"Selected Answer: B\nB. Run the Model Monitor baseline job again on the new training set: This is a key step after retraining the model. Since the model has been retrained with a new dataset, the baseline against which its predictions are compared should also be updated. Running the baseline job again on the new training set and configuring Model Monitor to use this new baseline will ensure that the monitoring is relevant to the current state of the model and the data it's processing.\n\nD. Retrain the model again with a combination of the original and new training sets: While retraining the model can be a good approach in some scenarios, there's no indication in this case that the issue lies with the model's performance itself. The issue seems to be with the Model Monitor's baseline not aligning with the current model."},{"timestamp":"1716395220.0","upvote_count":"1","comment_id":"1077657","poster":"sukye","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-violations.html"},{"timestamp":"1706483340.0","comment_id":"965837","poster":"Mickey321","content":"Selected Answer: B\nrunning the Model Monitor baseline job again on the new training set and configuring Model Monitor to use the new baseline, is the most appropriate step to resolve the violations and ensure the SageMaker endpoint's performance is in line with expectations.","upvote_count":"1"},{"timestamp":"1692256740.0","comment_id":"811679","poster":"AjoseO","content":"Selected Answer: B\nRunning the Model Monitor baseline job again with the new training set and configuring Model Monitor to use the new baseline is a valid option to resolve the violations. \n\nBy running the baseline job with the new training set, a new baseline is created, which can be used to compare with the new data to detect any drifts in the data distribution. Then, the updated baseline can be set as the new baseline for monitoring the endpoint. \n\nSo, option B is also a valid solution to resolve the violations.","upvote_count":"3"}],"answers_community":["B (90%)","10%"],"timestamp":"2022-05-10 13:06:00","isMC":true,"exam_id":26,"answer_ET":"B","answer_images":[],"choices":{"C":"Delete the endpoint and recreate it with the original configuration.","A":"Manually trigger the monitoring job to re-evaluate the SageMaker endpoint traffic sample.","B":"Run the Model Monitor baseline job again on the new training set. Configure Model Monitor to use the new baseline.","D":"Retrain the model again by using a combination of the original training set and the new training set."},"topic":"1","question_images":[]},{"id":"GLHkIj8T4Cr4gi9kuzfM","url":"https://www.examtopics.com/discussions/amazon/view/74397-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"topic":"1","unix_timestamp":1650845640,"question_text":"A company supplies wholesale clothing to thousands of retail stores. A data scientist must create a model that predicts the daily sales volume for each item for each store. The data scientist discovers that more than half of the stores have been in business for less than 6 months. Sales data is highly consistent from week to week. Daily data from the database has been aggregated weekly, and weeks with no sales are omitted from the current dataset. Five years (100 MB) of sales data is available in Amazon S3.\nWhich factors will adversely impact the performance of the forecast model to be developed, and which actions should the data scientist take to mitigate them?\n(Choose two.)","question_id":64,"answer_images":[],"answer":"AC","timestamp":"2022-04-25 02:14:00","exam_id":26,"discussion":[{"comment_id":"591312","timestamp":"1650845640.0","upvote_count":"30","comments":[{"upvote_count":"12","timestamp":"1663423140.0","comment_id":"671579","poster":"rb39","content":"B - no reason to assume there is not enough variance\nD - missing data can be assumed to be 0, no need to ask for empty data\nE - no reason to ask for two years of data having one already"}],"content":"Selected Answer: AC\nAC would be my answer. As half the stores have only been open for 6 months, no seasonality would be captured. The aggregation of the daily also removes trends we see during the week which is also not great when we are looking for the daily predicated sales figure","poster":"cron0001"},{"poster":"MultiCloudIronMan","upvote_count":"1","comment_id":"1303605","content":"Selected Answer: CD\nMissing data and achieving daily predictions with weekly data will be issues.","timestamp":"1730037240.0"},{"comment_id":"1228302","content":"Selected Answer: AD\nI would go for AD\nA : Many stores have been in business for < 6 months --> unable to capture seasonality\nD : Zero sales are also sales records and will result in bias if omitted.","poster":"Antoh1978","upvote_count":"2","timestamp":"1718090520.0"},{"upvote_count":"1","timestamp":"1711927500.0","comments":[{"content":"I changed my mind. It should be C and D. Since both of them foundation aspect of training.","poster":"vkbajoria","comment_id":"1194289","timestamp":"1712917020.0","upvote_count":"1"}],"content":"Selected Answer: AD\nSince half of the stores are 6 months old seasonality would be a problem for them.\ninstead of omitting weeks with no sales could lead to bias, requesting zero entries will help in predicting better","comment_id":"1187080","poster":"vkbajoria"},{"comment_id":"1161324","content":"Selected Answer: AD\nA as missing seasonality is an issue for the majority of the stores.\nD as we need to impute zeros as we would otherwise miss data.\n\nC won't do anything on performance.","timestamp":"1709103180.0","poster":"Stokvisss","upvote_count":"2"},{"comment_id":"1147174","content":"Selected Answer: CD\nThe factors that will adversely impact the performance of the forecast model are:\nSales data is aggregated by week. This will reduce the granularity and resolution of the data, and\nmake it harder to capture the daily patterns and variations in sales volume. The data scientist should\nrequest daily sales data from the source database to enable building a daily model, which will be\nmore accurate and useful for the prediction task.\nSales data is missing zero entries for item sales. This will introduce bias and incompleteness in the\ndata, and make it difficult to account for the items that have no demand or are out of stock. The data\nscientist should request that item sales data from the source database include zero entries to enable\nbuilding the model, which will be more robust and realistic","timestamp":"1707649860.0","upvote_count":"2","poster":"kyuhuck"},{"poster":"CloudHandsOn","timestamp":"1704984600.0","comment_id":"1119872","content":"Selected Answer: CD\nC. Aggregated Weekly Data: Since the objective is to predict daily sales volume, weekly aggregated data might mask important daily trends and variations. Requesting daily sales data will provide a finer granularity of information that is crucial for building an accurate daily sales prediction model.\n\nD. Missing Zero Entries for Item Sales: The omission of weeks with no sales can lead to biased predictions, as the model might not correctly account for periods of no sales. Including zero entries for item sales would provide a more accurate representation of sales patterns, including the absence of sales, which is valuable information for the model.\n\nBased on this analysis, the factors that would most adversely impact the model's performance are the aggregated weekly data (Option C) and the omission of weeks with no sales (Option D).","upvote_count":"2"},{"upvote_count":"1","comment_id":"1082408","timestamp":"1701166620.0","content":"Selected Answer: AC\nA - six months is likely not enough to detect clear seasonality\nC - Can do weekly from daily but cant reliably do daily from weekly","poster":"endeesa"},{"timestamp":"1691588640.0","poster":"kaike_reis","upvote_count":"1","comment_id":"976704","content":"Selected Answer: AC\nLetters A and C are correct: we want to do a daily model (our base is on weeks) and we need to deal with new stores VS old stores. It is important to emphasize that the letter D also makes sense: we need to know the days when there were no sales, however the way it is written means saving lines (days of sales) with zero in the database, which is not practical."},{"comment_id":"965842","content":"Selected Answer: AC\nthe two factors that will adversely impact the forecast model's performance are seasonality detection for new stores and the aggregation of sales data on a weekly basis. The data scientist should request categorical data to relate new stores with historical data and request daily sales data from the source database to build a daily model, respectively, to mitigate these issues effectively.","timestamp":"1690578900.0","upvote_count":"2","poster":"Mickey321"},{"content":"Selected Answer: AD\nAD. rest makes no sense.","poster":"SRB1337","upvote_count":"1","timestamp":"1687976100.0","comment_id":"936986"},{"upvote_count":"2","timestamp":"1676625240.0","comment_id":"811671","poster":"AjoseO","content":"Selected Answer: AC\nA. Since more than half of the stores have been in business for less than 6 months, it will be challenging to detect seasonality patterns for these new stores. Therefore, one solution is to request categorical data to relate new stores with similar stores that have more historical data. This will help the model to identify common patterns and accurately forecast sales for new stores.\n\nC. Since the sales data is aggregated by week, it may not be possible to identify daily patterns or trends. Hence, one solution is to request daily sales data from the source database to enable building a daily model. This will help the model to identify daily patterns and improve its forecasting accuracy."},{"comment_id":"633331","upvote_count":"4","content":"Selected Answer: CD\nI go with CD. How could we ignore the days with 0 sales? The model should be trained so that it can predict 0 sales days as well.","timestamp":"1658204760.0","poster":"peterfish"},{"comment_id":"615150","content":"B, C, D are possible. A couldn't be an answer because the model must predict daily sales volumes while A says 'Request categorical data'.","poster":"KlaudYu","upvote_count":"1","timestamp":"1655009400.0"}],"question_images":[],"answer_ET":"AC","answer_description":"","answers_community":["AC (71%)","CD (18%)","12%"],"choices":{"A":"Detecting seasonality for the majority of stores will be an issue. Request categorical data to relate new stores with similar stores that have more historical data.","D":"The sales data is missing zero entries for item sales. Request that item sales data from the source database include zero entries to enable building the model.","C":"Sales data is aggregated by week. Request daily sales data from the source database to enable building a daily model.","B":"The sales data does not have enough variance. Request external sales data from other industries to improve the model's ability to generalize.","E":"Only 100 MB of sales data is available in Amazon S3. Request 10 years of sales data, which would provide 200 MB of training data for the model."}},{"id":"5IZT4lWoUS4gNezj82oI","answers_community":["CDF (53%)","CEF (43%)","3%"],"timestamp":"2022-04-30 02:34:00","question_text":"An ecommerce company is automating the categorization of its products based on images. A data scientist has trained a computer vision model using the Amazon\nSageMaker image classification algorithm. The images for each product are classified according to specific product lines. The accuracy of the model is too low when categorizing new products. All of the product images have the same dimensions and are stored within an Amazon S3 bucket. The company wants to improve the model so it can be used for new products as soon as possible.\nWhich steps would improve the accuracy of the solution? (Choose three.)","answer_images":[],"choices":{"B":"Use the Amazon Rekognition DetectLabels API to classify the products in the dataset.","C":"Augment the images in the dataset. Use open source libraries to crop, resize, flip, rotate, and adjust the brightness and contrast of the images.","F":"Check whether there are class imbalances in the product categories, and apply oversampling or undersampling as required. Store the new dataset in Amazon S3.","E":"Use Amazon Rekognition Custom Labels to train a new model.","A":"Use the SageMaker semantic segmentation algorithm to train a new model to achieve improved accuracy.","D":"Use a SageMaker notebook to implement the normalization of pixels and scaling of the images. Store the new dataset in Amazon S3."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74934-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer_ET":"CDF","unix_timestamp":1651278840,"answer_description":"","exam_id":26,"question_images":[],"question_id":65,"answer":"CDF","discussion":[{"timestamp":"1654069560.0","comment_id":"610055","upvote_count":"8","content":"B CE is correct.","poster":"tgaos"},{"upvote_count":"1","poster":"MultiCloudIronMan","content":"Selected Answer: CDF\n(Option C): Using open source libraries to crop, resize, flip, rotate, and adjust the brightness and contrast of the images can increase the diversity of the training data, helping the model generalize better to new products1.\n (Option D): Normalizing and scaling the images can help the model learn more effectively by ensuring that the input data is consistent2.\n (Option F): Addressing class imbalances can prevent the model from being biased towards more frequent classes, improving its overall accuracy.","comment_id":"1288617","timestamp":"1727184420.0"},{"comment_id":"1229600","content":"Selected Answer: CDF\nThe questions says \"The images for each product are classified according to specific product lines.\" why do we need Amazon Rekognition Custom Labels then?","poster":"salim1905","timestamp":"1718249580.0","upvote_count":"1","comments":[{"timestamp":"1728349620.0","comment_id":"1294514","content":"Because the goal is to increase the accuracy of the existing model, not using a built-in service.","poster":"amlgeek","upvote_count":"1"}]},{"timestamp":"1707380460.0","poster":"kyuhuck","upvote_count":"4","comment_id":"1144264","content":"Selected Answer: CEF\nOption C is correct because augmenting the images in the dataset can help the model learn more\nfeatures and generalize better to new products. Image augmentation is a common technique to\nincrease the diversity and size of the training data.\nOption E is correct because Amazon Rekognition Custom Labels can train a custom model to detect\nspecific objects and scenes that are relevant to the business use case. It can also leverage the existing\nmodels from Amazon Rekognition that are trained on tens of millions of images across many\ncategories.\nOption F is correct because class imbalance can affect the performance and accuracy of the model, as\nit can cause the model to be biased towards the majority class and ignore the minority class. Applying\noversampling or undersampling can help balance the classes and improve the model's ability to learn\nfrom the data"},{"timestamp":"1701167100.0","comment_id":"1082420","content":"Selected Answer: CDF\nassuming improve accuracy of the (existing) solution","poster":"endeesa","upvote_count":"3"},{"comment_id":"991473","poster":"Mickey321","content":"Selected Answer: CEF\nHopefully final answer this time CEF. was initially looking for D but changed to E now","timestamp":"1693142340.0","upvote_count":"3"},{"upvote_count":"1","comment_id":"988815","timestamp":"1692849600.0","content":"Selected Answer: CDF\nC & F for sure the confusion between D and E but lets go for D as E will need more steps","poster":"Mickey321"},{"timestamp":"1691589000.0","comment_id":"976708","poster":"kaike_reis","comments":[{"comment_id":"976710","upvote_count":"3","content":"NVM, D is wrong!","poster":"kaike_reis","timestamp":"1691589060.0"}],"content":"Selected Answer: CDF\nThe question asks for quick solutions and to improve the classifier's accuracy. Since we want a quick fix, I'm going to avoid solutions that requires a new model implementation. Therefore, the alternatives that can improve the performance of the current classification are: Letter F, C and D. Letters B and E would bring a new development cost from zero and Letter A does not solve the classification problem.","upvote_count":"4"},{"comment_id":"966484","poster":"Mickey321","timestamp":"1690642680.0","comments":[{"comment_id":"966498","content":"See community answer is CEF due to images all same dimension so D removed.","upvote_count":"1","timestamp":"1690644600.0","poster":"Mickey321"}],"upvote_count":"2","content":"Selected Answer: CDF\nthe three steps that would improve the accuracy of the solution are C (data augmentation), D (image normalization and scaling), and F (addressing class imbalances)"},{"poster":"vbal","content":"CEF : C&F for Overfitting; E : \"Rekognition DetectLabel\" is the general image labeling capability of Amazon Rekognition, which provides predefined labels for common objects and concepts out-of-the-box. On the other hand, \"Rekognition Custom Labels\" allows you to create custom models to detect specific labels or objects that are not covered by the default labels,","comment_id":"950237","timestamp":"1689203760.0","upvote_count":"1"},{"upvote_count":"1","poster":"ADVIT","content":"CEF better choose","comment_id":"944657","timestamp":"1688647740.0"},{"content":"Selected Answer: CDF\nThis is CDF. No idea why this is unclear here.","poster":"SRB1337","timestamp":"1687978080.0","comment_id":"937016","upvote_count":"2"},{"upvote_count":"2","content":"The problem is about \"Overfitting\", because the new products doesn't work well. It is not about simply improve model accuracy.\nC is great answer, augmentation is for overfitting.\nD is wrong, because normalization of pixel is not for overfitting, and \"all images have the same dimensions. no need for scaling, they are already scaled. \nF is for imbalance data. if the data is imbalanced, they should perform poor on both training and testing data(new product). And the new product should perform bad only on those cold category, not overall poor performance. \nB and E are all about Rokognition, one is Rekognition Detect label, a built-in image classification model; one is Rekognition Custom Labels, a pre-trained with fine-tuning model.","timestamp":"1683127680.0","poster":"ZSun","comment_id":"888682"},{"upvote_count":"1","poster":"cox1960","timestamp":"1682708880.0","content":"Selected Answer: BCE\nyou fix images (C), train Rekognition with these images (E) and finally infer to get the classes (B)","comment_id":"883842"},{"timestamp":"1682338080.0","comment_id":"879296","upvote_count":"1","content":"Selected Answer: CDF\nAmazon Rekognition custom label model requires time, expertise, and resources, often taking months to complete. Additionally, it often requires thousands or tens-of-thousands of hand-labeled images to provide the model with enough data to accurately make decisions.\nThe solution must be quick.\nhttps://aws.amazon.com/rekognition/custom-labels-features/","poster":"Ahmedhadi_","comments":[{"content":"Sorry, never mind my answer it's actually CEF.","comment_id":"882586","poster":"Ahmedhadi_","comments":[{"upvote_count":"1","comment_id":"888686","timestamp":"1683127860.0","content":"not F, think carefully what is imbalanced data? what is its effect? does it only affect new product?","poster":"ZSun"}],"timestamp":"1682595420.0","upvote_count":"1"}]},{"comment_id":"859327","content":"Selected Answer: CEF\nCEF is correct","upvote_count":"2","poster":"Mllb","timestamp":"1680466260.0"},{"poster":"Chelseajcole","timestamp":"1678018260.0","comment_id":"829901","upvote_count":"2","content":"Selected Answer: CEF\nD seems doing nothing with new product"},{"upvote_count":"2","content":"Selected Answer: CEF\nCEF is correct in my opinion.\nD - The normalization and scaling would not provide the most dramatic improvement of the model.\nE - By using transfer learning, we are able to utilize a pre-trained model that can recognize a lot of the lower-level features and retain only the last few layers for this specific purpose.","comment_id":"819771","timestamp":"1677187680.0","poster":"Siyuan_Zhu"},{"timestamp":"1669743660.0","content":"Selected Answer: CDF\nCDF is correct. IMO","upvote_count":"1","poster":"hichemck","comment_id":"730684"},{"comment_id":"713203","poster":"dunhill","upvote_count":"3","content":"I would go CEF.\nA. This is for a different kind of problem. No need to use it\nB. No, because it is not pretrained on our data and won't improve it.\nD. Usually we don't use SageMaker notebook to implement the normalization of pixels and scaling of the images as the tool...","timestamp":"1667842320.0"},{"upvote_count":"4","comment_id":"604725","content":"IMO BDE are correct:\nA - Its not a segmentation problem, we just know the item \nB - Its a prebuilt model so is quick \nC - Yes\nD - Create a sagemaker notebook is not worth\nE - Yes is an amazon prebuilt model\nF - Its not worth on future, will u check it after new item is added? Its not worth","poster":"vanluigi","timestamp":"1653112560.0"},{"poster":"edvardo","content":"A. This is for a different kind of problem. No need to use it\nB. No, because it is not pretrained on our data and won't improve it.\nC. YES.\nD. Maybe? It is a quick and dirty solution doing on a notebook, but it is quick and the normalization helps for this tasks.\nE. Maybe? But we have trained a model, so we might suspect this option won't be much better than our solution as is.\nF. YES. \n\nI would go with CDF","timestamp":"1651663560.0","upvote_count":"4","comment_id":"596740"},{"content":"D&F are wrong due to ''new dataset\"?","timestamp":"1651278840.0","comment_id":"594747","upvote_count":"3","poster":"ckkobe24"}]}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty","isImplemented":true,"isMCOnly":false,"id":26,"isBeta":false},"currentPage":13},"__N_SSP":true}