{"pageProps":{"questions":[{"id":"AXY5BuxygxTv1RS0qQmC","question_images":[],"question_id":31,"timestamp":"2022-10-14 00:18:00","answer_ET":"A","answer_images":[],"question_text":"A company has an automobile sales website that stores its listings in a database on Amazon RDS. When an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems.\nWhich design should a solutions architect recommend?","unix_timestamp":1665699480,"answer_description":"","choices":{"D":"Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.","B":"Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume.","C":"Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the targets.","A":"Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume."},"topic":"1","isMC":true,"answers_community":["A (51%)","D (44%)","2%"],"exam_id":31,"answer":"A","discussion":[{"comment_id":"723536","upvote_count":"90","poster":"romko","content":"Selected Answer: A\nInteresting point that Amazon RDS event notification doesn't support any notification when data inside DB is updated.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.overview.html\nSo subscription to RDS events doesn't give any value for Fanout = SNS => SQS\n\nB is out because FIFO is not required here.\n\nA is left as correct answer","comments":[{"poster":"Tsige","comment_id":"1299015","timestamp":"1729126500.0","content":"Option A suggests creating an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume. While this approach can work, it has a few limitations compared to Option D:\n\nScalability and Fan-out: Option A uses a single SQS queue, which means all target systems would need to poll the same queue. This can become a bottleneck if multiple systems need to process the data simultaneously. Option D, on the other hand, uses an SNS topic to fan out the event to multiple SQS queues, allowing each target system to have its own queue. This improves scalability and ensures that each target system can process the data independently.\nOption D offers a more scalable, decoupled, and flexible solution for handling the event notifications and distributing the data to multiple target systems.","upvote_count":"4"},{"content":"But... SQS is a queue and is incapable of sending messages to \"multiple target systems\". SNS is pub/sub and topics can be subscribed by multiple apps to update when such an even occurs. Moreover Amazon RDS uses native capabilities of DBs like Postgres, MS SQL for change data capture. This can be used to send notifications to SNS","timestamp":"1719047280.0","poster":"mknarula","upvote_count":"4","comment_id":"1235282"},{"timestamp":"1680187320.0","upvote_count":"3","comment_id":"855876","poster":"nauman001","content":"Listing the Amazon RDS event notification categories.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.ListingCategories.html:"},{"comments":[{"upvote_count":"4","timestamp":"1670879580.0","comment_id":"743342","content":"The key is \"Fanned out\" due to \"Multiple target systems\" need to update","poster":"Jiang_aws1"},{"upvote_count":"4","comment_id":"750797","timestamp":"1671536400.0","poster":"JayBee65","content":"Please provide reference for this claim: \" event notification by RDS stream or advance audit DML\""}],"timestamp":"1670879460.0","upvote_count":"2","comment_id":"743340","poster":"Jiang_aws1","content":"D is connect \nRDS event notification by RDS stream or advance audit DML so it is possible"}],"timestamp":"1669036860.0"},{"timestamp":"1672863300.0","poster":"ksolovyov","comments":[{"comment_id":"782326","upvote_count":"1","content":"I agree with it requiring a native function or stored procedure, but can they in turn invoke a Lambda function? I have only seen this being possible with Aurora, but not RDS - and I'm not able to find anything googling for it either. I guess it has to be possible, since there's no other option that fits either.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html","timestamp":"1674224100.0","poster":"BlueVolcano1","comments":[{"poster":"BlueVolcano1","comment_id":"782329","content":"To add to that though, A also states to only use SQS (no SNS to SQS fan-out), which doesn't seem right as the message needs to go to multiple targets?","upvote_count":"12","timestamp":"1674229740.0"}]}],"comment_id":"766049","content":"Selected Answer: A\nRDS events only provide operational events such as DB instance events, DB parameter group events, DB security group events, and DB snapshot events. What we need in the scenario is to capture data-modifying events (INSERT, DELETE, UPDATE) which can be achieved thru native functions or stored procedures.","upvote_count":"15"},{"content":"Selected Answer: A\nRDS event notifications won't capture row-level changes directly, and adding complexity with SQS and SNS may be unnecessary.","upvote_count":"1","comment_id":"1412900","timestamp":"1743345120.0","poster":"CloudExpert01"},{"upvote_count":"3","timestamp":"1741106460.0","comment_id":"1365010","content":"Selected Answer: D\nSQS wonâ€™t work for multiple targets. Once a message is consumed by a target it will be removed from the queue","poster":"ChhatwaniB"},{"content":"Selected Answer: D\nWe need to send same msg to multiple targets that a data bas been removed. Not different msgs to different targets. so option d is much better solution.","poster":"Saloniip","comment_id":"1358630","upvote_count":"1","timestamp":"1739953200.0"},{"poster":"zdi561","comment_id":"1348648","upvote_count":"4","content":"Selected Answer: D\nA is not right because one SQS consumed by multiple targets with different process is not possible. Ideally it should be sent to multiple SQS. D is working ok.","timestamp":"1738167960.0"},{"timestamp":"1737301140.0","content":"Selected Answer: D\nOption D: Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.\n\nCorrect Choice: This approach leverages RDS event notifications and SNS for fanning out updates to multiple SQS queues. Each target can then independently process updates using Lambda functions, making it scalable and modular.","upvote_count":"4","poster":"CristiaNNN","comment_id":"1343067"},{"upvote_count":"1","comment_id":"1339758","timestamp":"1736730600.0","content":"Selected Answer: D\nConclusÃ£o:\nA opÃ§Ã£o D (SNS para distribuiÃ§Ã£o e SQS para entrega confiÃ¡vel) Ã© a soluÃ§Ã£o mais adequada, escalÃ¡vel e resiliente para lidar com a necessidade de envio de dados a vÃ¡rios sistemas de destino apÃ³s a venda de um automÃ³vel.","poster":"Rcosmos"},{"comment_id":"1339221","poster":"4729e6c","timestamp":"1736608680.0","content":"Selected Answer: D\nOption D is the best choice because it uses RDS event notifications to trigger an SNS topic that fans out to multiple SQS queues, enabling each target system to process events independently and reliably. This approach ensures scalability, decoupling, and durable message storage. Option A is not ideal because RDS does not natively trigger Lambda functions; an external polling mechanism would be required, increasing complexity and operational overhead.","upvote_count":"2"},{"content":"Selected Answer: D\nThe combination of Amazon SNS and Amazon SQS together with AWS Lambda provides a robust, scalable and efficient solution for distributing RDS events to multiple target systems. This architecture ensures that data is transmitted reliably and that each target system can process the information independently, optimally meeting business requirements.","poster":"Mischi","timestamp":"1734905760.0","comment_id":"1330596","upvote_count":"2"},{"poster":"dipenich","content":"Selected Answer: D\nThe requirement is to remove automobile listings from the website after a sale and send the data to multiple target systems in a decoupled, reliable, and scalable manner. Hereâ€™s how Option D meets the requirements:\n\n Event Notification from RDS:\n Amazon RDS can publish event notifications to Amazon SNS topics. When a record is updated (e.g., when a sale is recorded), this triggers an event.\n\n Fan-out to Multiple SQS Queues:\n The SNS topic fans out the event notification to multiple SQS queues, allowing parallel processing for different target systems.\n Each target system consumes messages from its respective queue, ensuring reliable and independent data delivery.\n\n AWS Lambda for Target Updates:\n Lambda functions subscribed to the SQS queues process the messages and perform updates in the target systems.\n\n Decoupled Design:\n The use of SNS for fan-out and SQS for message queuing decouples the systems. This ensures that issues with one target system do not impact the others.","comment_id":"1327536","timestamp":"1734368460.0","upvote_count":"3"},{"poster":"EllenLiu","upvote_count":"2","content":"Selected Answer: D\nkey point: Even though lambda is integrated with RDS, it is not best practices for multiple consumers.\nSNS + SQS Fan-Out should be chosen\nhttps://aws.amazon.com/getting-started/hands-on/send-fanout-event-notifications/?nc1=h_ls","comment_id":"1326664","timestamp":"1734229800.0"},{"upvote_count":"3","poster":"f51a8bd","content":"Selected Answer: D\nD is correct\nA and B: AWS Lambda cannot be triggered directly from Amazon RDS. Additionally, FIFO (in B) is not necessary unless there is a strict requirement for order.","timestamp":"1734071160.0","comment_id":"1326032"},{"comment_id":"1315951","content":"Selected Answer: A\nExplanation:\nAWS Lambda: Lambda functions can be used to automatically trigger actions in response to updates in your database. You can create a Lambda function that listens for changes in the RDS database (e.g., when a listing is marked as sold) and sends that information to other systems.\n\nAmazon SQS: The Lambda function would send the information to an SQS queue, which allows asynchronous message processing. Multiple target systems (consumers) can then read from the SQS queue and process the data as needed. SQS ensures the delivery of messages and can handle scaling to multiple consumers.","poster":"0de7d1b","upvote_count":"1","timestamp":"1732213200.0"},{"timestamp":"1729589880.0","comment_id":"1301506","content":"D. Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.","upvote_count":"2","poster":"Mish"},{"timestamp":"1729219500.0","content":"Selected Answer: A\nA for sure","comment_id":"1299521","upvote_count":"1","poster":"ChymKuBoy"},{"upvote_count":"4","poster":"Omariox","content":"Selected Answer: D\nYou have to fan out first with SNS","comment_id":"1291448","timestamp":"1727680440.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/85427-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"vJmLus6hMAKJFrW3r78G","discussion":[{"comments":[{"comment_id":"757957","poster":"PassNow1234","upvote_count":"2","timestamp":"1672099140.0","content":"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\n\nCorrect"}],"poster":"123jhl0","content":"Selected Answer: D\nA - No as \"specific users can delete\"\nB - No as \"nonspecific amount of time\"\nC - No as \"prevent the data from being change\"\nD - The answer: \"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\" https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html","timestamp":"1666110000.0","comment_id":"698347","upvote_count":"38"},{"timestamp":"1665968760.0","comments":[{"comment_id":"1015998","comments":[{"upvote_count":"1","content":"HAHAHA","poster":"reviewmine","timestamp":"1708072860.0","comment_id":"1151833"}],"content":"they were trying to speak in binary lol","upvote_count":"7","timestamp":"1695572340.0","poster":"oddnoises"}],"poster":"Chunsli","comment_id":"696722","upvote_count":"19","content":"typo -- 10 delete the objects => TO delete the objects"},{"comment_id":"1343065","timestamp":"1737301080.0","upvote_count":"1","poster":"CristiaNNN","content":"Selected Answer: D\nOption D: Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.\n\nCorrect Choice: This approach leverages RDS event notifications and SNS for fanning out updates to multiple SQS queues. Each target can then independently process updates using Lambda functions, making it scalable and modular."},{"timestamp":"1736243520.0","comment_id":"1337515","upvote_count":"1","poster":"satyaammm","content":"Selected Answer: D\nS3 Bucket Versioning helps deal with modifications while the legal hold coveys no deletes or changes until removed."},{"upvote_count":"1","timestamp":"1734364680.0","comment_id":"1327496","poster":"aatikah","content":"Selected Answer: B\nD is wrong\nA legal hold can make objects immutable, but the question specifies new objects must remain unchangeable by default. Legal holds must be applied manually to individual objects, so they are not practical for this use case."},{"comment_id":"1284662","content":"Ans D - \"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\"\nhttps://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/batch-ops-legal-hold.html \nThe other options do not make sense for the situation in hand.","timestamp":"1726488660.0","upvote_count":"2","poster":"PaulGa"},{"comment_id":"1278875","timestamp":"1725536280.0","content":"Selected Answer: D\nThe Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\n\nYou can use S3 Batch Operations with Object Lock to add legal holds to many Amazon S3 objects at once. You can do this by listing the target objects in your manifest and submitting that list to Batch Operations. Your S3 Batch Operations job with Object Lock legal hold runs until completion, until cancellation, or until a failure state is reached.","poster":"huaze_lei","upvote_count":"2"},{"comment_id":"1253109","content":"Selected Answer: D\nD is the best choice","timestamp":"1721655600.0","upvote_count":"3","poster":"jaradat02"},{"timestamp":"1714242720.0","comment_id":"1203239","content":"Selected Answer: B\nAdding legal holds to objects and managing permissions for users to delete objects does not provide the same level of data immutability and retention control as S3 Object Lock.","poster":"professorx123","upvote_count":"2"},{"comment_id":"1123717","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nA: WORM doesn't allow delete by some users\nC: Irrelevant \nD: Permission only allows putting legal hold on objects. Not a complete solution\nB: Closest apart from 100 years as question is asking for indefinite. Governance allows modification by some users","upvote_count":"2","poster":"awsgeek75","timestamp":"1705355580.0"},{"comments":[{"poster":"LoXoL","comment_id":"1117363","timestamp":"1704794040.0","content":"Isn't Legal Hold a subcategory of Object Lock? Object Lock itself doesn't imply anything imho: you should go either for a Retention Mode OR Legal Hold. Why would you go for B if they ask \"for a nonspecific amount of time\"? Open to change my mind.","upvote_count":"1"}],"upvote_count":"4","comment_id":"1105725","poster":"pentium75","timestamp":"1703568780.0","content":"Selected Answer: B\n\"With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the objects if necessary.\"\n\nD is wrong because it applies Object Lock AND Legal Hold, which are two different things that achieve similar results. 'Adding the s3:PutObjectLegalHold permission' to user's policies would allow them to remove the Legal Hold but NOT the Object Lock. (Also, it would probably make more sense to add the permissions to the bucket policy, not the \"IAM policies of users\".)"},{"comment_id":"1039318","timestamp":"1696927680.0","upvote_count":"2","content":"Selected Answer: D\nI only picked this because of restricted users who can delete, and the easiest way of achieving this is them assuming the role","poster":"Abitek007"},{"content":"Selected Answer: D\n\"The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects\" = A legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\ns3:PutObjectLegalHold permission is required in your IAM role to add or remove legal hold from objects.","poster":"TariqKipkemei","timestamp":"1693282500.0","comment_id":"992762","upvote_count":"3"},{"poster":"Guru4Cloud","timestamp":"1692110040.0","comment_id":"981763","upvote_count":"2","content":"Selected Answer: D\nThe Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed."},{"timestamp":"1689928980.0","upvote_count":"2","comment_id":"958241","content":"Selected Answer: D\nMy understanding is that the s3:PutObjectLegalHold permission allows certain users to apply or remove the legal hold on objects in the S3 bucket. However, having the permission to apply or remove the legal hold does not necessarily mean users can override the hold set by another user.\n\nOnce the legal hold is set on an object, it is in effect until the hold is removed by the user who applied it or an admin with the necessary permissions. Other users, even if they have the s3:PutObjectLegalHold permission, won't be able to remove the hold unless they are granted access by the user who originally applied it.","poster":"RupeC"},{"poster":"omoakin","upvote_count":"3","comment_id":"904832","content":"I go with option B as they still need some specific users to be able to make changes so Gov mode is the best choice and 100 yrs is like infinity as well haha","timestamp":"1684839780.0"},{"poster":"KZM","content":"Selected Answer: D\nThe correct answer is D.","timestamp":"1677595800.0","upvote_count":"1","comment_id":"824924"},{"comments":[{"comment_id":"986357","poster":"slackbot","timestamp":"1692610560.0","content":"FFS 100 years = indefinitely. no company has a policy of keeping data for more than 10 years.\nhaving specific admins run 2 additional commands every time they want to modify an object, is really in sync with nowadays automation processes.\ninstead of commenting each letter from the question, start thinking. if you were to decide, would you make your users always run commands before modifying or would you rather allow them to directly modify?","upvote_count":"2"}],"poster":"WherecanIstart","timestamp":"1677579120.0","upvote_count":"4","comment_id":"824606","content":"Selected Answer: D\nOption B specifies a retention period of 100 years which contradicts what the question asked for.....\n\"The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects\"\nSetting the retention period of 100 years is specific and the company wants new data/objects to remain unchanged for nonspecific amount of time.\n\nCorrect answer is D\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html"},{"upvote_count":"2","comment_id":"812299","content":"Selected Answer: D\n\"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\" https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html","poster":"bdp123","timestamp":"1676663400.0"},{"content":"Selected Answer: D\nretention period of 100 Years prevents the object to be deleted bevor the retention period expires, so it's not a good fit.","upvote_count":"2","timestamp":"1676182980.0","comment_id":"806023","poster":"Yelizaveta"},{"timestamp":"1673465640.0","comment_id":"772839","comments":[{"content":"If users have the policy s3:PutObjectLegalHold then they can remove the legal hold before deleting.","comment_id":"950870","timestamp":"1689266880.0","poster":"MutiverseAgent","upvote_count":"1"}],"upvote_count":"5","poster":"nadir_kh","content":"it is B.\nOnce a legal hold is enabled, regardless of the object's retention date or retention mode, the object version cannot be deleted until the legal hold is removed.\n\nQuestion says: \"Specific users must have ability to delete objects\""},{"comment_id":"767475","poster":"John_Zhuang","upvote_count":"2","content":"Selected Answer: D\nWhile S3 bucket governance mode does allow certain users with permissions to alter retention/delete objects, the 100 years in Option B makes it invalid. \n\nCorrect answer is option D. \n\"With Object Lock you can also place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. \"\nhttps://aws.amazon.com/s3/features/object-lock/\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html#object-lock-legal-holds","timestamp":"1672998180.0"},{"upvote_count":"2","timestamp":"1672649580.0","content":"Selected Answer: D\nWith Object Lock, you can also place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. Legal holds can be freely placed and removed by any user who has the s3:PutObjectLegalHold permission.\nB - No as \"nonspecific amount of time\" otherwise B will meet the requirement with legal hold attached.","poster":"aba2s","comment_id":"763604"},{"poster":"FNJ1111","upvote_count":"2","content":"Wouldn't D require s3:GetBucketObjectLockConfiguration IAM permission? If so, D is incomplete and wouldn't meet the requirement.\n(from the link shared above)","comment_id":"760341","timestamp":"1672261800.0"},{"timestamp":"1672043880.0","content":"Selected Answer: B\nCorrect answer : B\nRetention mode - Governance:\nâ€¢ Most users can't overwrite or delete an object version or alter its lock settings\nâ€¢ Some users have special permissions to change the retention or delete the object","comment_id":"757260","upvote_count":"2","poster":"Silvestr"},{"upvote_count":"2","content":"Selected Answer: B\nTo meet the requirements specified in the question, the solution architect should choose Option B: Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket's default retention mode for new objects.\n\nS3 Object Lock is a feature of Amazon S3 that allows you to apply a retention period to objects in your bucket, during which time the objects cannot be deleted or overwritten. By enabling versioning on the bucket, you can ensure that all versions of an object are retained, including any deletions or overwrites. By setting a retention period of 100 years, you can ensure that the objects remain unchangeable for a long time.\n\nBy using governance mode as the default retention mode for new objects, you can ensure that the retention period is applied to all new objects that are uploaded to the bucket. This will prevent the objects from being deleted or overwritten until the retention period expires.","poster":"Buruguduystunstugudunstuy","timestamp":"1671660300.0","comment_id":"752801","comments":[{"comments":[{"content":"Legal holds are used to prevent objects that are subject to legal or compliance requirements from being deleted or overwritten, even if their retention period has expired. While legal holds can be useful for preventing the accidental deletion of important objects, they do not prevent the objects from being changed. S3 Object Lock can be used to prevent objects from being deleted or overwritten for a specified retention period, but a legal hold does not provide this capability.\n\nIn addition, the s3:PutObjectLegalHold permission allows users to place a legal hold on an object, but it does not prevent the object from being changed. To prevent the objects from being changed for a nonspecific amount of time, the solution architect should use S3 Object Lock and set a longer retention period on the objects.","poster":"Buruguduystunstugudunstuy","comment_id":"752804","timestamp":"1671660480.0","upvote_count":"3"}],"timestamp":"1671660360.0","content":"Why other options are wrong\nOption A (creating an S3 Glacier vault and applying a WORM vault lock policy) would not meet the requirement to prevent the objects from being changed, because S3 Glacier is a storage class for long-term data archival and does not support read-write operations. \n\nOption C (using CloudTrail to track API events and restoring modified objects from backup versions) would not prevent the objects from being changed in the first place.\n\nOption D (adding a legal hold and the s3:PutObjectLegalHold permission to IAM policies) would not meet the requirement to prevent the objects from being changed for a nonspecific amount of time.","poster":"Buruguduystunstugudunstuy","upvote_count":"1","comment_id":"752802"}]},{"upvote_count":"1","content":"Selected Answer: D\nOption D","comment_id":"749252","poster":"career360guru","timestamp":"1671403020.0"},{"comment_id":"741962","timestamp":"1670782080.0","upvote_count":"1","poster":"sanyoc","content":"Selected Answer: D\n\"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\""},{"poster":"adelegend","upvote_count":"1","content":"Answer is D, the key here is that no specific retention period was set by the company and this is exactly what differentiates Legal hold from Governance\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html","comment_id":"736756","timestamp":"1670325900.0"},{"comment_id":"733909","upvote_count":"1","poster":"AWSExam2021","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\n\nWith Object Lock you can also place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. Legal holds can be freely placed and removed by any user who has the s3:PutObjectLegalHold permission.","timestamp":"1669998360.0"},{"content":"Selected Answer: B\nAnswer is 100% B.\n\nGovernance mode\nYou should use the Governance mode if you want to protect objects from being deleted by most users during a pre-defined retention period, but at the same time want some users with special permissions to have the flexibility to alter the retention settings or delete the objects.\n\nLegal Hold works as an infinite retention period. Once applied it is not possible to delete any object until the hold is released manually. The hold can only be removed by users with special permissions.\n\nA retention period specifies a fixed period of time during which an object remains locked. During this period, your object is WORM-protected and canâ€™t be overwritten or deleted. You apply a retention period either in number of days or number of years with the minimum being 1-day and no maximum limit.\nA legal hold provides the same protection as a retention period, but it has no expiration date. Instead, a legal hold remains in place until you explicitly remove it.","comments":[{"comment_id":"753020","poster":"a070112","upvote_count":"1","content":"You think 100 years of retention period is \"nonspecific amount of time\"?","timestamp":"1671690780.0"},{"content":"Legal Hold works as an infinite retention period, which is being asked for \"to remain unchangeable for a nonspecific amount of time \"","timestamp":"1671537420.0","comment_id":"750812","poster":"JayBee65","upvote_count":"1"},{"timestamp":"1669615860.0","upvote_count":"1","content":"Legal hold, no one can delete objects. Governance, those with special permissions can delete","poster":"Cizzla7049","comment_id":"728828","comments":[{"timestamp":"1669834260.0","upvote_count":"5","content":"s3:PutObjectLegalHold permission allows users to remove the legal hold on the objects, So they can delete even if legal hold is there.","poster":"mj98","comment_id":"731871"}]}],"timestamp":"1669615680.0","comment_id":"728824","poster":"Cizzla7049","upvote_count":"3"},{"timestamp":"1668805800.0","poster":"nadjar007","content":"D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html","upvote_count":"1","comment_id":"721583"},{"content":"Selected Answer: B\nhttps://aws.amazon.com/pt/blogs/storage/protecting-data-with-amazon-s3-object-lock/","poster":"Cizzla7049","comment_id":"721024","comments":[{"comments":[{"comment_id":"728817","poster":"Cizzla7049","comments":[{"upvote_count":"1","timestamp":"1675003860.0","poster":"G3","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html#object-lock-legal-holds \n\nLegal holds can be freely placed and removed by any user who has the s3:PutObjectLegalHold permission. For a complete list of Amazon S3 permissions.","comment_id":"791741"}],"content":"Go read it again. You miss this part of the question \n\n\"Only specific users in the company's AWS account can have the ability 10 delete the objects\"\n\nNo one can delete for legal hold","timestamp":"1669615440.0","upvote_count":"1"}],"content":"Answer: D\nQuestion says: \"indefinite amount of time\" -> \"legal hold\"\nYour provided link describes this feature as well:\n\"A legal hold provides the same protection as a retention period, but it has no expiration date. Instead, a legal hold remains in place until you explicitly remove it.\"","timestamp":"1669109880.0","upvote_count":"2","comment_id":"724220","poster":"attila9778"}],"timestamp":"1668738420.0","upvote_count":"1"},{"comment_id":"716049","timestamp":"1668170280.0","upvote_count":"1","poster":"mabotega","content":"Selected Answer: B\nRetention mode in governance mode is the correct way to some users have the ability to delete object in S3 -> https://aws.amazon.com/pt/blogs/storage/protecting-data-with-amazon-s3-object-lock/","comments":[{"content":"Which is why IAM policy attached with s3:PutObjectLegalHold permission for some users. Ans is D","upvote_count":"1","poster":"mj98","comment_id":"731872","timestamp":"1669834320.0"},{"timestamp":"1669109820.0","content":"Question says: \"indefinite amount of time\" -> \"legal hold\"\nYour provided link describes this feature as well:\n\"A legal hold provides the same protection as a retention period, but it has no expiration date. Instead, a legal hold remains in place until you explicitly remove it.\"","upvote_count":"1","comments":[],"comment_id":"724218","poster":"attila9778"}]}],"answer_ET":"D","isMC":true,"unix_timestamp":1665968760,"answer_images":[],"question_id":32,"question_text":"A company needs to store data in Amazon S3 and must prevent the data from being changed. The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects. Only specific users in the company's AWS account can have the ability 10 delete the objects.\nWhat should a solutions architect do to meet these requirements?","timestamp":"2022-10-17 03:06:00","choices":{"D":"Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects.","A":"Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.","C":"Create an S3 bucket. Use AWS CloudTrail to track any S3 API events that modify the objects. Upon notification, restore the modified objects from any backup versions that the company has.","B":"Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucketâ€™s default retention mode for new objects."},"question_images":[],"answers_community":["D (80%)","B (20%)"],"exam_id":31,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/85634-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"D","answer_description":""},{"id":"xrdfpvlyrKtGIqLb8qCt","answer":"A","question_text":"A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user names and passwords that are stored locally in a file. The company wants to minimize the operational overhead of credential management.\nWhat should a solutions architect do to accomplish this goal?","answers_community":["A (96%)","4%"],"question_images":[],"question_id":33,"exam_id":31,"answer_description":"","isMC":true,"topic":"1","answer_images":[],"discussion":[{"timestamp":"1665216900.0","upvote_count":"101","comments":[{"upvote_count":"29","comment_id":"706796","content":"READ!!! AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.","timestamp":"1666998600.0","poster":"17Master"},{"content":"A - additionally, Aurora manages the settings for the secret and rotates the secret every seven days by default.","upvote_count":"3","comment_id":"1178888","poster":"hro","timestamp":"1710981960.0"},{"content":"ty bro, I was confused about that and you just mentioned the \"key\" phrase, B doesn't support autorotation","comment_id":"698037","poster":"iCcma","timestamp":"1666081380.0","upvote_count":"3"}],"content":"Selected Answer: A\nB is wrong because parameter store does not support auto rotation, unless the customer writes it themselves, A is the answer.","comment_id":"689131","poster":"Sinaneos"},{"upvote_count":"8","poster":"cookieMr","timestamp":"1687074720.0","content":"Selected Answer: A\nOption A: Using AWS Secrets Manager and enabling automatic rotation is the recommended solution for minimizing the operational overhead of credential management. AWS Secrets Manager provides a secure and centralized service for storing and managing secrets, such as database credentials. By leveraging Secrets Manager, the application can retrieve the database credentials programmatically at runtime, eliminating the need to store them locally in a file. Enabling automatic rotation ensures that the database credentials are regularly rotated without manual intervention, enhancing security and compliance.","comment_id":"926522"},{"upvote_count":"1","comment_id":"1409968","content":"Selected Answer: A\nThis is asked in the exam I have taken today","poster":"Kamatchi","timestamp":"1742893740.0"},{"poster":"MundiChor","timestamp":"1740697560.0","content":"Selected Answer: A\nOrder of elimination:\nB. Parameter Store can store secrets. You can enforce TTL to expire credentials but you cannot auto rotate\nC and D are plan overhead and not secure enough.","upvote_count":"1","comment_id":"1362767"},{"poster":"ak240519","upvote_count":"1","timestamp":"1739461140.0","comment_id":"1356189","content":"Selected Answer: A\nA is the correct answer. as Instance store doesn't support auto rotation"},{"content":"Selected Answer: A\nAWS Secrets Manager is a service that helps users manage, rotate, and retrieve secrets for applications, services, and IT resources. It can be used to secure secrets for AWS Cloud, third-party services, and on-premises\n-Automatic rotation: Secrets can be automatically rotated","comment_id":"1350833","upvote_count":"1","poster":"adamatic","timestamp":"1738580040.0"},{"timestamp":"1735141080.0","upvote_count":"2","comment_id":"1331623","content":"Selected Answer: A\nAWS Secrets Manager:\nIs a managed service specifically designed to securely store and retrieve secrets, such as database credentials, API keys, and SSH keys.\nProvides features like automatic rotation, which helps to reduce the risk of compromised credentials.\nIntegrates seamlessly with many AWS services, including Amazon RDS (which Aurora is a part of).","poster":"MGKYAING"},{"comment_id":"1311626","content":"Answer is A\nhttps://aws.amazon.com/cn/blogs/security/how-to-connect-to-aws-secrets-manager-service-within-a-virtual-private-cloud/\nhttps://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/","poster":"Gizmo2022","timestamp":"1731529740.0","upvote_count":"1"},{"content":"Selected Answer: A\nThis is an ideal solution. Secrets Manager can rotate credentials automatically and ensures that the EC2 instances retrieve the most recent credentials securely.","upvote_count":"3","comment_id":"1290479","timestamp":"1727492220.0","poster":"Abishek016"},{"comment_id":"758984","poster":"Buruguduystunstugudunstuy","timestamp":"1726904520.0","comments":[{"comment_id":"758986","content":"Option B, using AWS Systems Manager Parameter Store and turning on automatic rotation, would not be suitable for storing secrets, such as database credentials, as it is intended for storing system parameters. \n\nOption C, creating an S3 bucket to store objects that are encrypted with an AWS KMS encryption key and migrating the credential file to the S3 bucket, would not provide automatic rotation of the secrets. \n\nOption D, creating an encrypted EBS volume and migrating the credential file to the new EBS volume, would not provide automatic rotation of the secrets.","upvote_count":"1","timestamp":"1672171140.0","poster":"Buruguduystunstugudunstuy"}],"upvote_count":"3","content":"Selected Answer: A\nOption A, using AWS Secrets Manager and turning on automatic rotation, would be the best solution to minimize the operational overhead of credential management.\n\nAWS Secrets Manager is a service that makes it easier to manage secrets, such as database credentials, by storing and rotating them automatically. By turning on automatic rotation, you can ensure that the secrets are regularly rotated, reducing the risk of unauthorized access to the database. This would minimize the operational overhead of credential management, as you would not have to manually rotate the secrets or update the EC2 instances with the new credentials."},{"content":"A: READ!!! AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.\n\nIt says SSM Parameter store cant rotate automatically.","timestamp":"1726904460.0","upvote_count":"2","comment_id":"1054227","poster":"Ruffyit"},{"comment_id":"1266276","comments":[{"comment_id":"1272892","upvote_count":"1","timestamp":"1724688960.0","content":"Same thing I thought. Answer D seems simpler, but option A is the best approach.","poster":"iyiola_daniel"}],"content":"Everybody here voting A, but only the master user's password of the Aurora database can be automatically stored and rotated. Who uses the master user's credentials in their application ? It looks to me like a serious security issue...\n\nMoreover answer A is not complete, missing steps are:\n- create IAM role to get secret\n- assign IAM role to EC2 instance\n- adapt the application to retrieve the secret from Secrets Manager instead of erading the file\n- make sure retrieval occurs every week\nI dont' call that minimizing operational overhead... Answer D is a lot more simple.\n\nIn a real situation, none of these answers are relevant.","timestamp":"1726904460.0","upvote_count":"3","poster":"jallaix"},{"upvote_count":"1","comment_id":"1273932","timestamp":"1724836260.0","poster":"griggrig","content":"Selected Answer: A\nOption A , because of leas overhead."},{"timestamp":"1720935660.0","content":"Selected Answer: A\nParameter Store: Storing and managing a database connection string or API endpoint URL that doesnâ€™t require frequent rotation.\n\nSecrets Manager: Storing and managing database credentials that need to be rotated regularly for security compliance.","poster":"parth_g_mehta","upvote_count":"1","comment_id":"1247627"},{"content":"Answer A is correct","timestamp":"1717295940.0","poster":"JalimRabeiBR","comment_id":"1222985","upvote_count":"1"},{"comment_id":"1173598","upvote_count":"1","content":"Selected Answer: A\nSecrets Manager, as The Mandalorian would say \"this is the way!\"","timestamp":"1710437820.0","poster":"OctavioBatera"},{"content":"Selected Answer: A\nSSM has no automatic rotation.","upvote_count":"1","poster":"TilTil","comment_id":"1173070","timestamp":"1710387600.0"},{"timestamp":"1710075840.0","comment_id":"1170331","content":"The most suitable option for minimizing operational overhead of credential management in this scenario is:\n\nB. Use AWS Systems Manager Parameter Store. Turn on automatic rotation.\n\nAWS Systems Manager Parameter Store is a service that helps you manage configuration data, including sensitive information such as passwords and database strings, in a central, secure store. With automatic rotation enabled, the credentials can be automatically updated at scheduled intervals, reducing the manual effort required for credential management.","poster":"Shalini10dec","upvote_count":"1"},{"poster":"Kanagarajd","upvote_count":"1","content":"Selected Answer: A\nSecret manager with auto rotation.","timestamp":"1709441100.0","comment_id":"1164498"},{"timestamp":"1705172700.0","comment_id":"1121976","upvote_count":"1","content":"Selected Answer: A\nBCD are extremely high operational overhead and not secure like A","poster":"awsgeek75"},{"upvote_count":"1","content":"Selected Answer: A\nA is correct","poster":"A_jaa","timestamp":"1705149120.0","comment_id":"1121619"},{"timestamp":"1699275540.0","content":"Selected Answer: B\nB becasue the user wants reduce costs and SSM Parameter Store layer Standard is free and the type SecureString uses KMS","comment_id":"1063852","poster":"ifaby","upvote_count":"3"},{"content":"It should be \"A.\"","upvote_count":"2","poster":"AbirAbu","timestamp":"1697387220.0","comment_id":"1044296"},{"content":"Selected Answer: A\nA - SECREATS MANAGER","comment_id":"1022575","timestamp":"1696182360.0","poster":"santbot","upvote_count":"1"},{"timestamp":"1696172580.0","upvote_count":"3","comment_id":"1022391","poster":"Mandar15","content":"Selected Answer: A\nAurora automatically stores and manages database credentials in AWS Secrets Manager. Aurora rotates database credentials regularly, without requiring application changes. Secrets Manager secures database credentials from human access and plain text view."},{"poster":"NaaVeeN","content":"If most Voted answers is done by us, then Who is marking the answers as Correct ?","timestamp":"1696063080.0","upvote_count":"4","comment_id":"1021353"},{"timestamp":"1695790560.0","upvote_count":"1","content":"Selected Answer: A\nSecret manager and auto rotation does the job","poster":"novice16","comment_id":"1018440"},{"timestamp":"1694624280.0","comment_id":"1006817","poster":"Blackberry","upvote_count":"1","content":"Can admit update only ðŸ’¯ answers. It's confusing most voted or correct answer which to prefer"},{"upvote_count":"1","content":"Selected Answer: A\nA it is","timestamp":"1694075520.0","comment_id":"1001338","poster":"akshunn"},{"comment_id":"993690","upvote_count":"1","timestamp":"1693368540.0","poster":"Hassaoo","content":"A is Right Because secret manager is meant for Rds Integration"},{"poster":"PLN6302","upvote_count":"1","content":"Option A","comment_id":"988240","timestamp":"1692790320.0"},{"upvote_count":"2","poster":"kapalulz","timestamp":"1691267220.0","comment_id":"973357","content":"Selected Answer: A\nparameter store does not support auto rotation and AWS Secrets Manager does."},{"poster":"TariqKipkemei","content":"Selected Answer: A\nMinimize the operational overhead of credential management = AWS Secrets Manager","timestamp":"1690517640.0","comment_id":"965272","upvote_count":"1"},{"content":"Selected Answer: A\nThe answer is A without hesitation, in the AWS console: Secret manager => store a ne secret => choose the \"Credentials for Amazon RDS database\" option","comment_id":"962439","upvote_count":"1","timestamp":"1690269600.0","poster":"hakim1977"},{"poster":"oguzbeliren","upvote_count":"1","content":"Parameter store doesn't support password rotation, but secret manger does.","timestamp":"1689948180.0","comment_id":"958588"},{"poster":"Guru4Cloud","content":"Selected Answer: A\nHere is the explanation:\n\n AWS Secrets Manager is a service that provides a secure and convenient way to store, manage, and rotate secrets. Secrets Manager can be used to store database credentials, SSH keys, and other sensitive information.\n AWS Secrets Manager also supports automatic rotation, which can help to minimize the operational overhead of credential management. When automatic rotation is enabled, Secrets Manager will automatically generate new secrets and rotate them on a regular schedule","comment_id":"957543","timestamp":"1689856440.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1689418800.0","poster":"james2033","content":"Selected Answer: A\nA","comment_id":"952296"},{"content":"Option A MET THE REQUIREMENT","comment_id":"949980","poster":"miki111","upvote_count":"1","timestamp":"1689178200.0"},{"timestamp":"1686679860.0","poster":"TienHuynh","content":"Selected Answer: A\nA is a right choice","comment_id":"922440","upvote_count":"1"},{"comment_id":"915702","timestamp":"1685993400.0","poster":"hypnozz","upvote_count":"2","content":"A is the right choice...Because, in Both you can force to rotate or delete de secrets, but, in secrets manager you can use a lambda to generate automatic secrets, also, itÂ´s integrated with RDS, like Aurora...and itÂ´s mostly used for that propouse. You can use parameter store, but you have to update the parameters by yourself"},{"content":"Selected Answer: A\nOption A meets this goal.","comment_id":"912236","timestamp":"1685634540.0","upvote_count":"1","poster":"Bmarodi"},{"comment_id":"899805","content":"Selected Answer: A\nA is correct","timestamp":"1684308540.0","poster":"beginnercloud","upvote_count":"1"},{"poster":"cheese929","timestamp":"1684047000.0","content":"Selected Answer: A\nA is the most logial answer.","upvote_count":"1","comment_id":"897326"},{"upvote_count":"1","poster":"cheese929","comment_id":"895746","content":"A is correct","timestamp":"1683878220.0"},{"comment_id":"882307","timestamp":"1682578020.0","upvote_count":"1","poster":"CLOUDUMASTER","content":"Key term used is with less operational overhead, systems parameter store allows you to do the same at no additional cost and where Secrets manager charges 0.40 per secret. Nowhere in the text does is say that autorotation is a must and furthermore, enabling auto rotation is available of parameter store.."},{"poster":"Azaa","content":"Selected Answer: A\nFrom the AWS doc it clearly saying choose `AWS Secrets manager` if it requires auto rotation\n```\nTo implement password rotation lifecycles, use AWS Secrets Manager. You can rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle using Secrets Manager. For more information, see What is AWS Secrets Manager? in the AWS Secrets Manager User Guide.","comment_id":"880914","upvote_count":"3","timestamp":"1682462280.0"},{"poster":"chibaniMed","content":"Selected Answer: A\nAWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. However, Parameter Store doesn't provide automatic rotation services for stored secrets. Instead, Parameter Store enables you to store your secret in Secrets Manager, and then reference the secret as a Parameter Store parameter.","timestamp":"1682407860.0","comment_id":"880080","upvote_count":"1"},{"upvote_count":"1","comment_id":"876444","poster":"mrgeee","timestamp":"1682075520.0","content":"Selected Answer: A\nDefinitely A"},{"upvote_count":"1","content":"A is correct:\nSecrets Manager: It was designed specifically for confidential information (like database credentials, API keys) that needs to be encrypted, so the creation of a secret entry has encryption enabled by default. It also gives additional functionality like rotation of keys.","comment_id":"876286","poster":"Dinoo_Man","timestamp":"1682065140.0"},{"timestamp":"1681666440.0","comment_id":"871986","poster":"CLOUDUMASTER","content":"B sounds logical there is no question saying that auto rotation is supposed to be a key component though","upvote_count":"2"},{"comment_id":"866634","upvote_count":"1","poster":"SaranJ","timestamp":"1681161360.0","content":"Selected Answer: A\noperational overhead of credential management is the key term which means secret rotation and encryption and these are features of Secret Manager.\nAlthough Aurora doesn't have built-in integration with Secret Manager but focus on Keyword of the question gives an Answer = A"},{"content":"Selected Answer: A\nKeywords: \n- User names and passwords that are stored locally in a file -> minimize the operational overhead of credential management. (Improve security with lowest operational overhead)\n\nA: Correct - with AWS Secret Manager the the username and password will be encrypted with KMS and it also has the automatic rotation.\nB: Incorrect - AWS Systems Manager Parameter Store don't have automatic rotation\nC: Incorrect - Actually, we can apply this but we will do a lot of things compare to AWS Secret Manager.\nD: Incorrect - We can do this but it is required more operational overhead than AWS Secret Manager","comment_id":"862789","poster":"PhucVuu","timestamp":"1680772320.0","upvote_count":"4"},{"content":"Selected Answer: A\nTo minimize the operational overhead of credential management, a solutions architect should use AWS Secrets Manager. AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources without the upfront investment and on-going maintenance costs of operating your own infrastructure. It enables you to retrieve clear text secrets such as database credentials and API keys securely, rotate secrets safely, and manage access with detailed audit logs. In this case, the architect can store the user names and passwords in AWS Secrets Manager and configure automatic rotation of credentials to enhance security. The application can then retrieve the credentials from Secrets Manager at runtime instead of from a file stored locally on each EC2 instance, which minimizes operational overhead. Therefore, Option A is the correct answer.","comment_id":"859105","poster":"gx2222","timestamp":"1680454260.0","upvote_count":"1"},{"content":"Selected Answer: A\nKey words: database user and password; minimize the operational.\nOption B: AWS Systems Manager Parameter Store does not support automatic rotation of secrets.\nOption C: additional steps is required\nOption D: requires additional management","poster":"channn","comment_id":"858574","upvote_count":"1","timestamp":"1680417060.0"},{"comment_id":"857475","poster":"acuaws","upvote_count":"2","content":"Selected Answer: B\nB\nParameter Store DOES NOT provide automatic rotation.","timestamp":"1680305280.0"},{"timestamp":"1680210060.0","comment_id":"856266","upvote_count":"1","poster":"linux_admin","content":"Selected Answer: A\nOption A suggests using AWS Secrets Manager to manage the user names and passwords used to connect to the Amazon Aurora database. By storing the credentials in Secrets Manager, the company can centrally manage the credentials and control access to them. With automatic rotation turned on, Secrets Manager can automatically generate new credentials at regular intervals, minimizing the operational overhead of credential management."},{"comment_id":"854299","poster":"jdr75","content":"CORRECT IS A, not B, and it's evident. Again Examtopis offers the incorrect anwer as the correct.\nWho is responsible of choosing the incorrect answer in examtopics as the correct one? -- someone should think about this, because it's not serious.","upvote_count":"3","timestamp":"1680087420.0"},{"content":"Selected Answer: B\nWith AWS Secrets Manager, the application code can retrieve credentials securely by calling Secrets Manager APIs, eliminating the need to store secrets in the code or configuration files.\nSo if we select A, there are a lot code change (from read pwd from file to call API). \n\nAWS Systems Manager Parameter Store can use AWS Secrets Manager to manage password. \nAnd \"You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter. \"\nIt is easy to switch to use â€œthe unique nameâ€.\n\nSo B is better than A. (B include A)","upvote_count":"1","timestamp":"1678598400.0","comment_id":"836727","poster":"sanking"},{"timestamp":"1677333540.0","poster":"bilel500","comment_id":"821537","upvote_count":"1","content":"Selected Answer: A\nAWS Secrets Manager enables you to rotate, manage, and retrieve database credentials, API keys and other secrets throughout their lifecycle. It also makes it really easy for you to follow security best practices such as encrypting secrets and rotating these regularly."},{"upvote_count":"1","poster":"KittieHearts","comment_id":"821129","content":"Selected Answer: A\nIt's a secret key, it will be A.","timestamp":"1677294420.0"},{"timestamp":"1677144120.0","poster":"ahalamri","comment_id":"819014","upvote_count":"1","content":"Selected Answer: A\nA is Correct."},{"poster":"cheese929","comment_id":"814051","content":"Selected Answer: A\nA is correct.","upvote_count":"1","timestamp":"1676809020.0"},{"timestamp":"1676563740.0","comment_id":"810886","poster":"Bilal_2019","upvote_count":"1","content":"Selected Answer: A\nTo minimize the operational overhead of credential management, a solutions architect could implement the following:\n\nUse AWS Secrets Manager to store and manage the database user names and passwords securely.\n\nUpdate the application to retrieve the user names and passwords from AWS Secrets Manager instead of from the local file.\n\nBy using AWS Secrets Manager, the company can centrally manage the database user names and passwords and enforce security best practices such as regular rotation of secrets, fine-grained access control, and audit trail of secret usage. This can help simplify the credential management and improve the security of the application."},{"upvote_count":"1","timestamp":"1676563680.0","poster":"Bilal_2019","comment_id":"810884","content":"Selected Answer: B\nYes, using AWS Systems Manager Parameter Store is a good option for minimizing the operational overhead of credential management in this scenario. Here's what you can do:\n\nStore the database credentials securely in the Parameter Store as a SecureString data type.\nEnable automatic rotation of the credentials to periodically generate a new set of credentials and update the stored value in the Parameter Store.\nModify the application to retrieve the credentials from the Parameter Store at runtime, rather than storing them locally in a file.\nBy doing this, the management and rotation of the credentials can be automated, which reduces the operational overhead and ensures the security of the system."},{"poster":"Bilal_2019","content":"Chat GPT Chooses A with explanation as below\n\nTo minimize the operational overhead of credential management, a solutions architect could implement the following:\n\nUse AWS Secrets Manager to store and manage the database user names and passwords securely.\n\nUpdate the application to retrieve the user names and passwords from AWS Secrets Manager instead of from the local file.\n\nBy using AWS Secrets Manager, the company can centrally manage the database user names and passwords and enforce security best practices such as regular rotation of secrets, fine-grained access control, and audit trail of secret usage. This can help simplify the credential management and improve the security of the application.","comment_id":"810878","upvote_count":"1","comments":[{"poster":"Bilal_2019","timestamp":"1676563620.0","upvote_count":"1","content":"Also, chooses B \n\nYes, using AWS Systems Manager Parameter Store is a good option for minimizing the operational overhead of credential management in this scenario. Here's what you can do:\n\nStore the database credentials securely in the Parameter Store as a SecureString data type.\nEnable automatic rotation of the credentials to periodically generate a new set of credentials and update the stored value in the Parameter Store.\nModify the application to retrieve the credentials from the Parameter Store at runtime, rather than storing them locally in a file.\nBy doing this, the management and rotation of the credentials can be automated, which reduces the operational overhead and ensures the security of the system.","comment_id":"810882"}],"timestamp":"1676563560.0"},{"timestamp":"1676273100.0","content":"Selected Answer: A\nSelected Answer: A","poster":"buiducvu","upvote_count":"1","comment_id":"807139"},{"comment_id":"778884","poster":"BlueVolcano1","content":"Selected Answer: A\nThere's no automatic rotation in Parameter Store.","timestamp":"1673957460.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"778056","poster":"nalindm","content":"Selected Answer: A\nparameter store does not support auto rotation","timestamp":"1673891640.0"},{"upvote_count":"1","poster":"jannymacna","content":"A. use AWS secrets Manager...\nNo debate","timestamp":"1673578440.0","comment_id":"774049"},{"timestamp":"1673509380.0","comment_id":"773232","upvote_count":"1","poster":"Abdel42","content":"Selected Answer: A\nB is wrong because it does not support automatic rotation"},{"timestamp":"1673025660.0","upvote_count":"2","comment_id":"767918","poster":"SilentMilli","content":"Selected Answer: A\nAWS Secrets Manager is a secrets management service that enables you to store, manage, and rotate secrets such as database credentials, API keys, and SSH keys. Secrets Manager helps you secure your secrets by automatically rotating them according to a schedule that you specify. This can help minimize the operational overhead of managing and rotating secrets, as well as reduce the risk of secrets being compromised. With Secrets Manager, you can store the user names and passwords for your Amazon Aurora database in a central location and have them automatically rotated on a schedule that you specify. The EC2 instances can then retrieve the secrets as needed, rather than having to store them locally in a file."},{"poster":"aba2s","upvote_count":"2","timestamp":"1672993680.0","comment_id":"767413","content":"Selected Answer: A\nB is wrong because parameter store does not support auto rotation.\nplease see this link at the end https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/"},{"comment_id":"764240","content":"In the question, it was said to be a file stored locally on the instance. It seems unencrypted. If you look at this part, it seems that the parameter store is correct.","poster":"AdiosAmigo","upvote_count":"1","timestamp":"1672723380.0","comments":[{"comment_id":"765267","content":"Except that parameter store does not support auto-rotation. So the statement is incorrect.","upvote_count":"1","timestamp":"1672809720.0","poster":"xicomynor"}]},{"comment_id":"763468","timestamp":"1672627260.0","upvote_count":"1","content":"Selected Answer: A\nits A","poster":"techhb"},{"upvote_count":"1","comment_id":"759724","timestamp":"1672223880.0","poster":"Ello2023","content":"A. Less admin overheads."},{"timestamp":"1671752880.0","poster":"melearning","comment_id":"753757","upvote_count":"1","content":"Selected Answer: A\nThe operation overhead should be minimal. This can be done in secret manager."},{"content":"Parameter Store supports automatic rotation through its Secrets Manager integration as per the documentation \nA - is correct in this scenario.","timestamp":"1671520140.0","upvote_count":"2","poster":"wombles","comment_id":"750589"},{"comment_id":"749627","timestamp":"1671438480.0","content":"Selected Answer: A\nTo implement password rotation lifecycles, use AWS Secrets Manager. You can rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle using Secrets Manager. For more information","poster":"psr83","upvote_count":"2"},{"content":"It should be A;","poster":"kvenikoduru","upvote_count":"1","timestamp":"1671381540.0","comment_id":"749042"},{"comment_id":"747452","timestamp":"1671211920.0","upvote_count":"1","poster":"NikaCZ","content":"Selected Answer: A\nAWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle."},{"timestamp":"1671171900.0","poster":"rajesh1216","comment_id":"746872","upvote_count":"1","content":"Selected Answer: B\nIt's A for obvious reason"},{"upvote_count":"1","comment_id":"746362","poster":"NikaCZ","content":"Selected Answer: A\nAWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources.","timestamp":"1671125460.0"},{"upvote_count":"1","comment_id":"746327","poster":"yoben84","content":"Selected Answer: A\nsomething is weird with these answers. the community can't be all wrong","timestamp":"1671122940.0"},{"content":"Selected Answer: A\nNow, this is absurd. B is DEFINITELY not right simply because Parameter store DOES NOT support auto-rotation of keys. \nCorrect Answer is A\nC, D- Too much operational overhead, you can achieve the same thing using Secrets Manager w/o doing all that.","poster":"Nandan747","timestamp":"1671100140.0","upvote_count":"1","comment_id":"745943"},{"content":"Selected Answer: A\nSecrets Manager permit rotation","upvote_count":"1","timestamp":"1670591040.0","comment_id":"740122","poster":"SilentMilli"},{"poster":"benaws","timestamp":"1670585220.0","comment_id":"740033","content":"Obviously A","upvote_count":"1"},{"timestamp":"1670368680.0","comment_id":"737276","poster":"zabouchedid","content":"Selected Answer: A\nA","upvote_count":"1"},{"poster":"shyam_yadav","content":"Ans is A","upvote_count":"1","timestamp":"1669999920.0","comment_id":"733923"},{"poster":"DannisExamTopic","comment_id":"732363","upvote_count":"1","content":"Selected Answer: A\nsecrets manager is newer","timestamp":"1669879740.0"},{"content":"A is correct","upvote_count":"1","timestamp":"1669034280.0","poster":"Wpcorgan","comment_id":"723475"},{"timestamp":"1668838860.0","content":"Selected Answer: A\nAWS secrets manager is most logical","poster":"ABCMail","upvote_count":"1","comment_id":"721800"},{"content":"A option is correct.","poster":"Kartikey140","timestamp":"1668494160.0","comment_id":"718543","upvote_count":"1"},{"comment_id":"716355","poster":"al20220711","timestamp":"1668210000.0","upvote_count":"2","content":"The advantage of Systems Manager over Secrets Manager is costing. But it doesn't mention in the question. SO A is better."},{"timestamp":"1665659700.0","upvote_count":"4","poster":"KVK16","content":"Selected Answer: A\nAWS Secrets Manager, a service that makes it easier to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You can configure Secrets Manager to rotate secrets automatically, which can help you meet your security and compliance needs. Secrets Manager offers built-in integrations for MySQL, PostgreSQL, and Amazon Aurora on Amazon RDS, and can rotate credentials for these databases natively. You can control access to your secrets by using fine-grained AWS Identity and Access Management (IAM) policies. To retrieve secrets, employees replace plaintext secrets with a call to Secrets Manager APIs, eliminating the need to hard-code secrets in source code or update configuration files and redeploy code when secrets are rotated.","comment_id":"693820"},{"timestamp":"1665543720.0","content":"Selected Answer: A\nAWS Secrets Manager is newer to SSM Parameter Store, and I'm not sure the later one can turn on automatic rotation","poster":"BoboChow","comment_id":"692588","upvote_count":"2"},{"poster":"josh_fan","timestamp":"1665539400.0","upvote_count":"1","content":"Selected Answer: A\nA","comment_id":"692557"},{"timestamp":"1665410940.0","content":"Selected Answer: A\nno rotation option on parameter store. \nnever store credentials on buckets, its a hazard. \nits A!!","poster":"ogerber","comment_id":"691233","upvote_count":"4"},{"timestamp":"1665406980.0","poster":"D2w","upvote_count":"1","comment_id":"691161","content":"Selected Answer: A\nA, is meant for storing secrets e.g password ect"},{"content":"Selected Answer: A\nA is correct.","comment_id":"690304","timestamp":"1665329220.0","upvote_count":"3","poster":"tuloveu"},{"poster":"rein_chau","comment_id":"689335","content":"Selected Answer: A\nA is correct.","upvote_count":"3","timestamp":"1665234840.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/84682-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application to the S3 bucket.","D":"Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate the credential file to the new EBS volume. Point the application to the new EBS volume.","A":"Use AWS Secrets Manager. Turn on automatic rotation.","B":"Use AWS Systems Manager Parameter Store. Turn on automatic rotation."},"timestamp":"2022-10-08 10:15:00","answer_ET":"A","unix_timestamp":1665216900},{"id":"zcMZnWQNz5QMU0m4c2tO","discussion":[{"comments":[{"poster":"jdr75","content":"presigned URL is for download the data from S3, not for uploads, so the user does not upload anything. C is no correct.","timestamp":"1680708600.0","comments":[{"upvote_count":"7","poster":"mauroicardi","timestamp":"1710452520.0","comment_id":"1173764","comments":[{"poster":"Vandaman","timestamp":"1739964900.0","comment_id":"1358689","content":"This makes sense. I will change my answers to CD","upvote_count":"1"}],"content":"A user who does not have AWS credentials to upload a file can use a presigned URL to perform the upload.\n\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-presigned-urls.html"},{"poster":"EricYu2023","comments":[{"poster":"PoisonBlack","upvote_count":"9","comments":[{"poster":"AF_1221","timestamp":"1683048960.0","comment_id":"887757","upvote_count":"5","content":"preassigned URL is for upload or download for temporary time and for specific users outside the company"}],"comment_id":"883992","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html","timestamp":"1682735760.0"},{"upvote_count":"4","timestamp":"1683048840.0","poster":"AF_1221","content":"but for temporary purpose not for permanent","comments":[{"upvote_count":"2","comment_id":"1135870","poster":"tuso","timestamp":"1706624220.0","comments":[{"content":"that's right","poster":"AnhNguyen99","upvote_count":"1","timestamp":"1720771500.0","comment_id":"1246585"}],"content":"So? You only need a presigned URL for the moment you upload the image, not forever"}],"comment_id":"887754"}],"comment_id":"873210","upvote_count":"13","timestamp":"1681778760.0","content":"Presigned URL can be use for upload."}],"comment_id":"862236","upvote_count":"16"},{"upvote_count":"4","comments":[{"poster":"MutiverseAgent","timestamp":"1689268260.0","content":"About your comments regarding option B)... But if images are being saved directly to S3 instead of the EBS/SSD storage of E2 instances as they originally were, the new approach will reduce coupling and improve performance. Also you have to consider the security concerns about presign URLs as the question does not mention if users are public or private.","comment_id":"950882","upvote_count":"2"},{"poster":"Yelizaveta","content":"Here it means to decouple the processes, so that the web server don't have to do the resizing, so it doesn't slow down. The customers access the web server, so the web server have to be involved in the process, and how the others already wrote, the pre-signed URL is not the right solution because, of the explanation you can read in the other comments. \n\nAnd additional! \"Configure the application to upload images directly from EACH USER'S BROWSER to Amazon S3 through the use of a pre-signed URL\"\n\nI am not an expert, but I can't imagine that you can store an image that an user uploads in his browser etc.","comment_id":"806028","timestamp":"1676183820.0","upvote_count":"7"}],"content":"Why other options are wrong\nOption A, Configuring the application to upload images to S3 Glacier, is not relevant to improving the performance of image uploads. \n\nOption B, Configuring the webserver to upload the original images to Amazon S3, is not a recommended solution as it would not reduce coupling within the application or improve performance. \n\nOption E, Creating an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images, is not a recommended solution as it would not be able to resize images in a timely manner and would not improve performance.","comment_id":"752808","timestamp":"1671660960.0","poster":"Buruguduystunstugudunstuy"}],"timestamp":"1671660960.0","poster":"Buruguduystunstugudunstuy","upvote_count":"59","content":"Selected Answer: CD\nTo meet the requirements of reducing coupling within the application and improving website performance, the solutions architect should consider taking the following actions:\n\nC. Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a pre-signed URL. This will allow the application to upload images directly to S3 without having to go through the web server, which can reduce the load on the web server and improve performance.\n\nD. Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image. This will allow the application to resize images asynchronously, rather than having to do it synchronously during the upload request, which can improve performance.","comment_id":"752807"},{"content":"Selected Answer: BD\nWhy would anyone vote C? signed URL is for temporary access. also, look at the vote here: https://www.examtopics.com/discussions/amazon/view/82971-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"835527","comments":[{"poster":"sheilawu","timestamp":"1715661420.0","content":"I agree with you. C seems a temporary solution if a download or upload demend is urgent.","comment_id":"1211217","upvote_count":"1"}],"timestamp":"1678490760.0","poster":"fkie4","upvote_count":"36"},{"upvote_count":"1","content":"Selected Answer: CD\nsee https://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/\nB separate concerns and has the best performance","poster":"zdi561","comment_id":"1348665","timestamp":"1738170720.0"},{"content":"Selected Answer: BD\nBD looks accurate. Presigned URLs provide short time access to users to upload or download objects to/from S3. This approach is not feasible to renew URL every time and give it to every random users who plans to access website.\nPresigned URL generally used for occasional sharing of private files.","upvote_count":"1","poster":"salman7540","comment_id":"1328866","timestamp":"1734586620.0"},{"timestamp":"1734116280.0","comment_id":"1326223","poster":"Tjazz04","content":"Selected Answer: CD\nCD is the more appropriate solution bec. when the user uploads the images, it will directly uploaded to the S3 while if BD, when the user uploads the images, it will first go to the web server then to the S3 bucket and This can cause a slow upload process since the web server is processing the download from the user, then upload to the s3 bucket.","upvote_count":"2"},{"content":"Selected Answer: BE\nWhile C can reduce web server load, it doesn't address image resizing, which is a critical requirement. So B & D would be the answers.","upvote_count":"1","timestamp":"1733012580.0","poster":"architect_kags","comment_id":"1320402"},{"upvote_count":"2","comment_id":"1300201","comments":[{"timestamp":"1729380240.0","content":"I'm sorry it's B D","poster":"Abdullah2004","upvote_count":"1","comment_id":"1300211"}],"poster":"Abdullah2004","timestamp":"1729379220.0","content":"Selected Answer: CD\nIt's very clear C D"},{"comment_id":"1299522","timestamp":"1729219740.0","poster":"ChymKuBoy","upvote_count":"2","content":"Selected Answer: CD\nC, D for sure"},{"poster":"PaulGa","comment_id":"1284664","timestamp":"1726488840.0","upvote_count":"3","content":"Selected Answer: BD\nAns B, D - \n1st: Upload the original images to Amazon S3\n2nd: Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded to resize the image."},{"comment_id":"1271218","timestamp":"1724406480.0","content":"Selected Answer: CD\nI am voting for C and D. \nB and D could also work, but when you upload the images to the EC2 and then to S3 it could cause additional performance and network traffic load.","poster":"bignatov","upvote_count":"2"},{"comment_id":"1256319","content":"I don't understand why the expiration time of presigned urls is being questioned. You can certainly design an upload flow that uses the SDK to create a new presigned url before uploading.","upvote_count":"2","poster":"Moo","timestamp":"1722088200.0"},{"poster":"jaradat02","content":"Selected Answer: BD\nB and D is the way to go","comment_id":"1253113","timestamp":"1721656140.0","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: CD\nCD - No point having the instance do extra work when we can use pre signed URLs and let the user directly upload to S3, hence B is not an operationally efficient option. Furthermore B results in more traffic through the instance which is inefficient.","comment_id":"1248264","timestamp":"1721039280.0","poster":"creamymangosauce"},{"upvote_count":"1","comment_id":"1241784","poster":"jatric","content":"Selected Answer: BD\nC is not a valid option to upload the image is to have presigned url to retrieve the file/image from S3","timestamp":"1720062300.0"},{"content":"Selected Answer: BD\nBD is more reasonable than CD","comment_id":"1228847","poster":"MomenAWS","upvote_count":"3","timestamp":"1718170380.0"},{"upvote_count":"2","content":"BD is the answer, idk why anyone would choose c ?? pre-signed urls are for security pursoses","comment_id":"1191160","poster":"soufiyane","timestamp":"1712518440.0"},{"upvote_count":"2","timestamp":"1707177600.0","comments":[{"timestamp":"1708553700.0","poster":"sirasdf","upvote_count":"2","comment_id":"1155912","content":"GPT4 is wrong. It does address the performance issue with the image processing will be done by lambda and not on the server"}],"poster":"MikeJANG","comment_id":"1141627","content":"Selected Answer: CD\nGPT4\noption B would offload the storage to S3 but still involves the web server in the upload process, which does not fully address the performance issues."},{"timestamp":"1707177300.0","poster":"MikeJANG","upvote_count":"1","content":"GPT4\noption B would offload the storage to S3 but still involves the web server in the upload process, which does not fully address the performance issues.","comment_id":"1141621"},{"content":"Selected Answer: CD\nC: presigned URL can be used to upload images to S3 https://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html\nD: scalable event processing for image resizing using lambda\n\nA: Glacier?\nB: Can work and maybe improves the performance also as the webserver is not resizing the image (if D is used in combination with this). However, C is better\nE: Irrelevant","upvote_count":"4","comment_id":"1123724","timestamp":"1705356000.0","poster":"awsgeek75"},{"timestamp":"1704802620.0","upvote_count":"3","poster":"bujuman","comment_id":"1117457","content":"Selected Answer: CD\nB could be excluded because of these two points:\n- During upload requests, the website resizes the images to a standard size and stores the resized images in Amazon S3. \n- Users are experiencing slow upload requests to the website."},{"poster":"Wang87","timestamp":"1704216060.0","content":"Selected Answer: AD\nC is out of question as URL is not generated by user and highest validity is 7 days. So if user needs to access same file after 7 days it would be very troublesome.","comment_id":"1112100","comments":[{"content":"How is S3 glacier going to help with uploads?","comment_id":"1123720","upvote_count":"3","timestamp":"1705355700.0","poster":"awsgeek75"}],"upvote_count":"1"},{"timestamp":"1701870360.0","comment_id":"1089383","poster":"Cyberkayu","content":"based on decoupling requirement, Answer B still go thru Web server before drop the file into S3.\nAnswer CD","comments":[{"comment_id":"1105727","content":"It's about decoupling upload and scaling process, the upload can still go through the web server, that's what a web server is for.","upvote_count":"1","timestamp":"1703568960.0","poster":"pentium75"}],"upvote_count":"2"},{"poster":"slimen","comment_id":"1061101","content":"Selected Answer: BD\npre-signed URL is temporary\ndecoupling meas preventing server from uplaoding and doing the resizing at the same time\nso separating the processing into 2 parts (uplaod, then notify, then resize) is considered decoupling","upvote_count":"5","timestamp":"1698989760.0"},{"upvote_count":"2","comment_id":"1046212","timestamp":"1697561220.0","poster":"xplusfb","content":"Selected Answer: BD\nC section seriously nonsense. many logical args given for BD i'll not write again."},{"timestamp":"1695169200.0","poster":"baggam","comment_id":"1011801","content":"Selected Answer: CD\nCD is correct","upvote_count":"2"},{"timestamp":"1694078820.0","comment_id":"1001374","content":"This is a social media company, so random users are uploading images. These are not employees. The signed URL has to be sent to the user and they only have a certain amount of time to use it. That's a disaster for a social media company. No way C is the answer. Lambda all the way.","poster":"numark","upvote_count":"4"},{"timestamp":"1694037180.0","poster":"MarcusLEK","comment_id":"1000982","content":"Selected Answer: BD\nwhile technically its possible to upload with pre-signed urls, its also worth mentioning that pre-signed urls have a time validity, so I think it might not be suitable to long term use.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html#:~:text=User%20Guide.-,Expiration%20time%20for%20presigned%20URLs,-A%20presigned%20URL","upvote_count":"4"},{"poster":"judyda","content":"Selected Answer: CD\nhttps://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/PresignedUrlUploadObject.html","comment_id":"999999","timestamp":"1693957320.0","upvote_count":"2"},{"comment_id":"992931","comments":[{"content":"wrong, they both for upload/download.","upvote_count":"2","poster":"Iconique","timestamp":"1695325020.0","comment_id":"1013372"}],"poster":"KawtarZ","content":"C is not correct. the pre-signed urls are for download only, not upload.","upvote_count":"1","timestamp":"1693299360.0"},{"poster":"TariqKipkemei","timestamp":"1693283340.0","comment_id":"992770","content":"Selected Answer: CD\nMain requirement is decoupling and improve performance for which option C&D suit best.\nYou may use presigned URLs to allow someone to upload an object to your Amazon S3 bucket.\n https://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html#:~:text=You%20may%20use-,presigned%20URLs,-to%20allow%20someone\n \nTechnically option D would work, but with the overhead of EC2/HDD/SDD.","upvote_count":"2","comments":[{"poster":"slimen","timestamp":"1698989700.0","comment_id":"1061100","content":"pre-signed URL is temporary\ndecoupling meas preventing server from uplaoding and doing the resizing at the same time\nso separating the processing into 2 parts (uplaod, then notify, then resize) is considered decoupling","upvote_count":"1"}]},{"comment_id":"986572","content":"Selected Answer: CD\nCD is much more efficient.","upvote_count":"2","timestamp":"1692626040.0","poster":"mtmayer","comments":[{"comment_id":"1105728","upvote_count":"2","poster":"pentium75","content":"But the application would need to request individual presigned URLs for each upload as these are temporary, this is quite an overhead.","timestamp":"1703569080.0"}]},{"comment_id":"981892","timestamp":"1692121500.0","content":"Selected Answer: BD\nCorrect answers are BD","poster":"Guru4Cloud","upvote_count":"2"},{"comment_id":"958771","content":"Selected Answer: CD\nI'll go with C,D \nB still involves the EC2 instances handling the image uploads and resizing, which does not improve website performance and increases coupling within the application.","upvote_count":"2","timestamp":"1689961980.0","poster":"ofdengiz"},{"poster":"sosda","timestamp":"1689124440.0","comment_id":"949392","upvote_count":"1","content":"Selected Answer: BD\npresign url is not operational efficient"},{"timestamp":"1689091920.0","upvote_count":"2","comment_id":"949132","content":"I will go with B and D. Pre signed URL is temporary thing.\n\nA presigned URL remains valid for the period of time specified when the URL is generated. If you create a presigned URL with the Amazon S3 console, the expiration time can be set between 1 minute and 12 hours. If you use the AWS CLI or AWS SDKs, the expiration time can be set as high as 7 days.","poster":"vini15"},{"content":"Selected Answer: BD\nCorrect answers are BD","comment_id":"925987","timestamp":"1687009020.0","upvote_count":"1","poster":"Kostya"},{"poster":"omoakin","content":"BC BC BC","comment_id":"904836","upvote_count":"1","timestamp":"1684840080.0"},{"timestamp":"1684823520.0","poster":"Abrar2022","upvote_count":"2","content":"pre-signed URL is not the correct answer as it allows you to grant temporary access to users who don't have permission to directly run AWS operations in your account.","comment_id":"904629"},{"upvote_count":"3","content":"Selected Answer: BD\nB D are correct options.","comment_id":"889442","timestamp":"1683211500.0","poster":"rushi0611"},{"timestamp":"1683008040.0","poster":"shinejh0528","content":"Selected Answer: BD\nC : in AWS skill builder, there's similar question. If you want to choose pre-signed URL, then there should be needs for security to download a specific or own user.","comment_id":"887065","upvote_count":"3"},{"timestamp":"1682507520.0","poster":"seifshendy99","upvote_count":"3","content":"Selected Answer: BD\nI think C doesn't make sense","comment_id":"881528"},{"comment_id":"879666","upvote_count":"5","timestamp":"1682364180.0","poster":"kruasan","content":"Selected Answer: CD\nYour app can generate pre-signed URLs every time you want to allow an arbitrary party to perform an upload or a download of an S3 object by simply using the HTTP protocol and without having to know anything about S3 or the AWS authentication protocols.\nhttps://fourtheorem.com/the-illustrated-guide-to-s3-pre-signed-urls/"},{"comment_id":"875402","content":"Selected Answer: CD\nB is wrong because bucket are private by default there will be explicit statement to switch to public.\nSo C is the correct one. Using the presinged URL works even with a private/public bucket","upvote_count":"3","timestamp":"1681978920.0","poster":"kakka22"},{"timestamp":"1681028940.0","content":"Correct ans is B and C\n\nWith B there is no web server needed to resize the images, reducing coupling within the app. This offloads resizing task from the web server to S3, which can handle taks more efficiently.\nWith C the upload requests will be faster since the images are uploaded directly to S3. The presigned URL is temp URL that is generated by app which grants permission to upload a specific object to S3.\n\nOption D is not best solution since it involves lambda which may introduce additional latency in image upload process.\n\nOption A and E are not relevant to the scenario","poster":"sam20231","comment_id":"865359","upvote_count":"1"},{"comment_id":"865346","content":"ANSWER- CD : Overview of serverless uploading to S3\nWhen you upload directly to an S3 bucket, you must first request a signed URL from the Amazon S3 service. You can then upload directly using the signed URL. This is two-step process for your application front end(https://aws.amazon.com/blogs/compute/uploading-to-amazon-s3-directly-from-a-web-or-mobile-application/)","timestamp":"1681025940.0","upvote_count":"1","poster":"TECHNOWARRIOR"},{"comments":[{"timestamp":"1681545660.0","poster":"Musti35","upvote_count":"4","comment_id":"870751","content":"presigned URl used while uploading ."}],"timestamp":"1680708720.0","poster":"jdr75","upvote_count":"3","comment_id":"862237","content":"Selected Answer: BD\nNOT A: S3 Glacier for archive data\nNOT C: presigned URL is for download the data from S3, not for uploads, so the user does not upload anything. \nNOT E: this is not a scheduled demand, but a \"live\" demand."},{"content":"Selected Answer: BD\nto me : BD","timestamp":"1679931300.0","poster":"kampatra","upvote_count":"3","comment_id":"852223"},{"timestamp":"1679124900.0","poster":"SuketuKohli","upvote_count":"3","comment_id":"842547","content":"you can use a presigned URL to optionally share objects or allow your customers/users to upload objects to buckets without AWS security credentials or permissions. https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html"},{"poster":"Grace83","upvote_count":"1","comment_id":"842500","content":"B + D looks right to me.","timestamp":"1679118960.0"},{"timestamp":"1678487400.0","upvote_count":"1","comment_id":"835498","poster":"avengu","content":"B+D looks correct as creating & using presigned url is not operationally efficient"},{"content":"B+D MAKES SENSE","poster":"aakashkumar1999","upvote_count":"1","comment_id":"794073","timestamp":"1675165260.0"},{"comment_id":"788467","content":"Selected Answer: BD\nno presigned url full fills","timestamp":"1674716820.0","upvote_count":"3","poster":"kdinesh95"},{"upvote_count":"5","poster":"remand","content":"Selected Answer: CD\nCD - pre-signed URL makes sense","comment_id":"778466","timestamp":"1673921580.0"},{"comment_id":"778079","timestamp":"1673892420.0","poster":"Jiggs007","content":"B + D more sense for me.\nEvent notifications â€“ Trigger workflows that use Amazon Simple Notification Service (Amazon SNS), Amazon Simple Queue Service (Amazon SQS), and AWS Lambda when a change is made to your S3 resources.","comments":[{"content":"agree, pre-signed URL doesnt seem like a good choice \"When you create a presigned URL for your object, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (GET to download the object), and an expiration date and time. The presigned URLs are valid only for the specified duration.\"","comment_id":"787929","upvote_count":"3","poster":"ces26015","timestamp":"1674666540.0"}],"upvote_count":"4"},{"upvote_count":"1","content":"Selected Answer: BD\nB and D","poster":"Ello2023","timestamp":"1673817300.0","comment_id":"777071","comments":[{"upvote_count":"1","comment_id":"798838","timestamp":"1675602240.0","content":"If the webserver handle also the upload that would increase the TIGHT COUPLING of UPLOADING and STORING and PROCESSING. If users uploads directly to S3 the APP would focus on resizing the final image and storing it","poster":"CaoMengde09"}]},{"timestamp":"1673681040.0","upvote_count":"4","content":"Selected Answer: BD\nThere is no point to use presigned URL for that case.","poster":"goodmail","comment_id":"775161"},{"poster":"mj61","content":"A,C, E are not as efficient or operationally efficient as the B and D:\nA. Configuring the application to upload images to S3 Glacier would not reduce the coupling within the application and would not improve website performance.\nC. Uploading images directly from the user's browser to S3 would not reduce the coupling within the application and could increase the load on the application server.\nE. Creating an Amazon EventBridge rule that invokes an AWS Lambda function on a schedule to resize uploaded images would not be real-time and would not work well with a large number of image uploads.","upvote_count":"3","timestamp":"1673573880.0","comment_id":"773994"},{"upvote_count":"4","content":"B + D correct. \nC incorrect\nThe presigned URLs are valid only for the specified duration. \nWhen you create a presigned URL for your object, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (GET to download the object), and an expiration date and time.","comment_id":"762728","timestamp":"1672506180.0","poster":"Mindvision"},{"timestamp":"1672475580.0","poster":"Zerotn3","content":"Selected Answer: CD\nAs chat gpt support","comment_id":"762559","upvote_count":"3"},{"upvote_count":"3","poster":"SoluAWS","content":"D is the best option as the user does not need to wait he/she will get the instant response that the image is uploaded. Once the image gets uploaded triggering the lambda function after that to resize the image (and might delete the original image and keep the resized image). This is the most efficient solution. \nand D is the first half of the logic.\n\nSo, B & D.","comment_id":"757944","timestamp":"1672098120.0"},{"upvote_count":"5","content":"Selected Answer: CD\nshld be c & d","timestamp":"1672041360.0","comment_id":"757193","poster":"NV305"},{"content":"Selected Answer: BD\nB & D looks correct","comment_id":"755430","upvote_count":"3","poster":"k1kavi1","timestamp":"1671948180.0"},{"content":"I think BD is the best fit for the question.\nI don't understand the pre-signed URL","poster":"crazydog82","comment_id":"752904","timestamp":"1671671160.0","upvote_count":"1"},{"content":"New image in S3 - B\nD - Image size -Lambda-storing to DynamoDB","upvote_count":"1","poster":"duriselvan","timestamp":"1671549120.0","comment_id":"751085"},{"upvote_count":"1","content":"E: \"on a schedule to resize uploaded images\" -> not event-triggered","comment_id":"749766","timestamp":"1671449520.0","poster":"NineKim"},{"content":"Selected Answer: BD\nB and D","upvote_count":"2","poster":"career360guru","comment_id":"749160","timestamp":"1671395160.0"},{"upvote_count":"3","content":"Selected Answer: BD\nnot mention in the question the social media have an application. so it was web base social media. the change should be on the web file on web server (server side not client set)","comment_id":"744114","timestamp":"1670940780.0","poster":"wly_al"},{"content":"Selected Answer: BD\nvoted BD","comment_id":"743379","upvote_count":"1","poster":"SilentMilli","timestamp":"1670883960.0"},{"poster":"sanyoc","comment_id":"741965","timestamp":"1670782260.0","upvote_count":"2","content":"Selected Answer: BD\nvoted for BD"},{"poster":"lapaki","content":"Selected Answer: BD\nPresigned url seems odd here","comment_id":"740688","upvote_count":"2","timestamp":"1670649360.0"},{"content":"Selected Answer: BD\nC is not correct is introduces operation overhead","upvote_count":"2","comment_id":"737099","timestamp":"1670349840.0","poster":"javitech83"},{"timestamp":"1670245200.0","content":"Selected Answer: CD\n\"Users are experiencing slow upload requests to the website.\"\nI think keypoint is here, if app still uploads to web server , then how we have improvements?","comment_id":"735948","comments":[{"poster":"Karldds22","content":"Because initially the website resizes the images, option B just causes the upload without resizing process. For me still B and D.","timestamp":"1671346500.0","comment_id":"748644","upvote_count":"1"}],"poster":"K0nAn","upvote_count":"4"},{"comments":[{"upvote_count":"1","comment_id":"753023","poster":"a070112","content":"Sharing Presigned URL to each user doesn't sound operationally efficient to me, I will choose BD","timestamp":"1671690900.0"}],"upvote_count":"2","timestamp":"1670068140.0","poster":"Wajif","content":"Selected Answer: CD\nI would go C&D simply because it is operationally efficient and will have the users directly access S3 and avoid slowness. The slow user experience need to be resolved.","comment_id":"734408"},{"upvote_count":"2","content":"Selected Answer: CD\nItâ€™s not a common solution to upload from user browser directly to S3. But it says â€œreduce couplingâ€. I think it means transferring between web server and S3 is unnecessary.","timestamp":"1669551900.0","poster":"leonnnn","comment_id":"728190"},{"timestamp":"1669538940.0","comments":[{"content":"By default, all S3 objects are private. Only the object owner has permission to access them. However, the object owner can optionally share objects with others by creating a presigned URL. Why would C even be relevant here? Objects are not needed to be shared amongst different application users.","upvote_count":"1","comment_id":"750822","poster":"JayBee65","timestamp":"1671537900.0"}],"content":"Selected Answer: CD\nB: UI -> Web Server -> S3 (this is not efficient and secure, you are transferring files twice).\n\nC: based on the above, this is one transfer only (so more efficient!)\n\nE: EventBridge + Lambda (this requires extra integration between S3 and EventBridge). \n\nD: simply S3 event notifications to Lambda is more operationally efficient than E. \n\n\nAnswer: C+D \n\nSupporting documentation: \nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html\nhttps://fullstackdojo.medium.com/s3-upload-with-presigned-url-react-and-nodejs-b77f348d54cc#:~:text=S3%20upload%20with%20presigned%20url%20%E2%80%94%20React%20and,upload%20the%20file%20with%20the%20returned%20presigned%20URL.","comment_id":"728057","upvote_count":"3","poster":"ocbn3wby"},{"comment_id":"725839","upvote_count":"2","content":"Selected Answer: BD\nRemember to have operationally efficiency in mind. It usually goes with configure rather than create. For Presigned URL's you have to assign it individually to the user. Example of Presiged URL's use case: For access to premium users on a premium content on a website .","timestamp":"1669294080.0","poster":"Vic_d_gr8"},{"poster":"Wilson_S","comment_id":"724104","timestamp":"1669093380.0","upvote_count":"1","content":"Selected Answer: BD\nâ€œOpertationally efficientâ€"},{"upvote_count":"1","poster":"Wpcorgan","timestamp":"1669054380.0","comment_id":"723818","content":"B and D"},{"content":"Selected Answer: BE\nReason:\noption B and E.\nConfigure the web server to upload the original images to Amazon S3. \n\nEvent Bridge has the capability to replay , reliable delivery, and archive so in case if lambda fails to process it can be retried in case of connection failures as the messages will be available in Event bridge.\nA:: NO. cannot upload directly to Glacier. you need to do it first to S3\nC: NO. Presigned URLS expire after some time\nD: Event bridge is better . If you configure S3 Event Notifications directly to lambda, the event will be lost during connection failures are no way to reprocess in case of failure by lambda to process.","timestamp":"1668525840.0","comment_id":"718907","poster":"rjam","upvote_count":"2"},{"comment_id":"713608","poster":"Deplake","timestamp":"1667896920.0","content":"Selected Answer: BD\nC is not looking correct for me, you should handle somehow which image names are already uploaded in order to prevent conflicts and security could be the case as well","upvote_count":"2"},{"upvote_count":"3","poster":"baiy","timestamp":"1667724120.0","comment_id":"712203","content":"Selected Answer: BD\nI go for BD, presigned URL does not improve performance, I don't see the meaning of using."},{"comment_id":"712027","content":"presigned URL does not improve performance which is what question asking here.","poster":"gokalpkocer3","upvote_count":"1","timestamp":"1667685120.0"},{"comment_id":"708146","content":"Upload image directly from user browser? Will it incur extra support effort to the IT team? Does it means extra operational effort?","timestamp":"1667183700.0","upvote_count":"2","comments":[{"timestamp":"1667183760.0","upvote_count":"3","poster":"Stephhhh","content":"I will go for B and D, given the operation effort is a consideration","comment_id":"708148"}],"poster":"Stephhhh"},{"comments":[{"timestamp":"1667034000.0","comment_id":"707081","poster":"USalo","content":"What about security? It can be potential issue if not properly covered on front-end, but handling that adding additional overhead.","upvote_count":"1"}],"content":"Selected Answer: CD\nThis is a tough question but i will go with C and D\nwith B: client browser -> web server -> s3\nwith C: client browser -> s3 directly,\n\nmeaning C should be a better solution than B (setting up pre-signed URLs do feel like a chore though)\n\nam not 100% sure on this.","poster":"Six_Fingered_Jose","upvote_count":"5","comment_id":"704819","timestamp":"1666798200.0"}],"answers_community":["CD (53%)","BD (45%)","1%"],"topic":"1","question_id":34,"answer_ET":"CD","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/86471-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"timestamp":"2022-10-26 17:30:00","question_images":[],"isMC":true,"exam_id":31,"unix_timestamp":1666798200,"answer":"CD","question_text":"A social media company allows users to upload images to its website. The website runs on Amazon EC2 instances. During upload requests, the website resizes the images to a standard size and stores the resized images in Amazon S3. Users are experiencing slow upload requests to the website.\nThe company needs to reduce coupling within the application and improve website performance. A solutions architect must design the most operationally efficient process for image uploads.\nWhich combination of actions should the solutions architect take to meet these requirements? (Choose two.)","choices":{"D":"Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image.","B":"Configure the web server to upload the original images to Amazon S3.","E":"Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images.","A":"Configure the application to upload images to S3 Glacier.","C":"Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a presigned URL"}},{"id":"1g2RSlf5rmXrKyEGCyBj","choices":{"C":"Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled.","A":"Add a second ActiveMQ server to another Availability Zone. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.","B":"Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.","D":"Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled."},"unix_timestamp":1666184340,"exam_id":31,"question_id":35,"answer_images":[],"answer":"D","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/85910-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"question_text":"A company recently migrated a message processing system to AWS. The system receives messages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application running on Amazon EC2. The consumer application processes the messages and writes results to a MySQL database running on Amazon EC2. The company wants this application to be highly available with low operational complexity.\nWhich architecture offers the HIGHEST availability?","timestamp":"2022-10-19 14:59:00","answers_community":["D (98%)","2%"],"isMC":true,"answer_ET":"D","answer_description":"","discussion":[{"timestamp":"1666184340.0","content":"Selected Answer: D\nAnswer is D as the \"HIGHEST available\" and less \"operational complex\"\nThe \"Amazon RDS for MySQL with Multi-AZ enabled\" option excludes A and B\nThe \"Auto Scaling group\" is more available and reduces operational complexity in case of incidents (as remediation it is automated) than just adding one more instance. This excludes C.\n\nC and D to choose from based on \nD over C since is configured","poster":"123jhl0","upvote_count":"23","comment_id":"699048"},{"comment_id":"1337584","timestamp":"1736260680.0","poster":"satyaammm","upvote_count":"2","content":"Selected Answer: D\nUsing RDS Multi-AZ is suitable for high availability and using EC2 Auto Scaling Groups is suitable for operational overhead."},{"content":"Selected Answer: D\nAns D - extends the architecture rather than complicating it: Amazon MQ with active/standby brokers configured across two Availability Zones; Auto Scaling group for the consumer EC2 instances across two Availability Zones; Amazon RDS for MySQL with Multi-AZ enabled.","comment_id":"1284689","timestamp":"1726490640.0","upvote_count":"3","poster":"PaulGa"},{"content":"Selected Answer: D\n- Active MQ with active/standby\n- Auto scaling across 2 AZs\n- RDS with Multi AZ.\n\nThese provide the highest availability and least complexity (using Amazon MQ, auto scaling and RDS, all managed services)","upvote_count":"3","timestamp":"1725536940.0","comment_id":"1278884","poster":"huaze_lei"},{"timestamp":"1721680020.0","poster":"jaradat02","content":"Selected Answer: D\nD is obviously the highest available.","upvote_count":"2","comment_id":"1253269"},{"content":"Definitely it's D","timestamp":"1720547280.0","upvote_count":"1","comment_id":"1245021","poster":"effiecancode"},{"content":"Selected Answer: D\nD: Managed and auto-scaling, resilient and HA service for each tier. This is well-architected too.","comment_id":"1124367","timestamp":"1705424220.0","upvote_count":"3","poster":"awsgeek75"},{"upvote_count":"3","timestamp":"1698599520.0","poster":"Ruffyit","content":"Using Amazon MQ with active/standby brokers provides highly available message queuing across AZs.\n\nAdding an Auto Scaling group for consumer EC2 instances across 2 AZs provides highly available processing.\n\nUsing RDS MySQL with Multi-AZ provides a highly available database.\n\nThis architecture provides high availability for all components of the system - queue, processing, and database.","comment_id":"1057008"},{"poster":"prabhjot","timestamp":"1696566120.0","upvote_count":"3","comment_id":"1026230","content":"Ans is C - C. Option C uses Amazon MQ with active/standby brokers, adds an additional consumer EC2 instance, and uses Amazon RDS for MySQL with Multi-AZ enabled. Amazon RDS Multi-AZ automatically replicates your database to another AZ and provides automated failover. This ensures high availability for both the messaging system and the database. Option D- bring More scalabilty rather HA","comments":[{"poster":"pentium75","timestamp":"1703570160.0","content":"D automates replacement of failed instances, thus it has higher availability than C.","upvote_count":"3","comment_id":"1105735"}]},{"poster":"TariqKipkemei","timestamp":"1693284240.0","comment_id":"992776","content":"Selected Answer: D\nHIGHEST availability. Definitely option D.","upvote_count":"2"},{"upvote_count":"3","poster":"Guru4Cloud","comment_id":"982483","content":"Selected Answer: D\nThe key reasons are:\n\nAmazon MQ active/standby brokers across AZs for queue high availability\nAuto Scaling group with consumer EC2 instances across AZs for redundant processing\nRDS MySQL with Multi-AZ for database high availability\nThis combines the HA capabilities of MQ, EC2 and RDS to maximize fault tolerance across all components. The auto scaling also provides flexibility to scale processing capacity as needed.","timestamp":"1692185640.0"},{"timestamp":"1692116940.0","comment_id":"981839","poster":"Guru4Cloud","upvote_count":"3","content":"Selected Answer: D\nD is the correct answer.\n\nUsing Amazon MQ with active/standby brokers provides highly available message queuing across AZs.\n\nAdding an Auto Scaling group for consumer EC2 instances across 2 AZs provides highly available processing.\n\nUsing RDS MySQL with Multi-AZ provides a highly available database.\n\nThis architecture provides high availability for all components of the system - queue, processing, and database."},{"timestamp":"1689766260.0","upvote_count":"3","content":"Selected Answer: D\nKeyword Amazon RDS, has C and D. Then D has \"Auto Scaling group\", choose D.","poster":"james2033","comment_id":"956557"},{"comment_id":"950593","upvote_count":"2","poster":"MNotABot","timestamp":"1689246540.0","content":"D\nWith 3 options with Amazon MQ --> A is odd one out / Then ASG with M-AZ was an easy choice"},{"timestamp":"1687431360.0","comment_id":"930405","poster":"cookieMr","content":"Selected Answer: D\nAmazon MQ with active/standby brokers configured across two AZ ensures high availability for the message broker. In case of a failure in one AZ, the other AZ's broker can take over seamlessly.\n\nAdding an ASG for the consumer EC2 instances across two AZ provides redundancy and automatic scaling based on demand. If one consumer instance becomes unavailable or if the message load increases, the ASG can automatically launch additional instances to handle the workload.\n\nUsing RDS for MySQL with Multi-AZ enabled ensures high availability for the database. Multi-AZ automatically replicates the database to a standby instance in another AZ. If a failure occurs, RDS automatically fails over to the standby instance without manual intervention.\n\nThis architecture combines high availability for the message broker (Amazon MQ), scalability and redundancy for the consumer EC2 instances (ASG), and high availability for the database (RDS Multi-AZ). It offers the highest availability with low operational complexity by leveraging managed services and automated failover mechanisms.","upvote_count":"3"},{"comment_id":"926187","content":"Selected Answer: D\nCorrect answer D","poster":"Kostya","timestamp":"1687026360.0","upvote_count":"1"},{"poster":"Bmarodi","content":"Selected Answer: D\nto achieve ha + low operational complexity, the solution architect has to choose option D, which fulfill these requirements.","timestamp":"1686141180.0","comment_id":"917215","upvote_count":"1"},{"comment_id":"905563","upvote_count":"1","content":"Auto scaling and Multi-AZ enabled for high availability.","poster":"Abrar2022","timestamp":"1684910880.0"},{"comment_id":"841750","upvote_count":"1","timestamp":"1679039340.0","poster":"Erbug","content":"you can find some details about Amazon MQ active/standby broker for high availability https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html"},{"poster":"Abdel42","upvote_count":"1","comment_id":"788509","content":"Selected Answer: D\nD as the Auto Scaling group offer the highest availability between all solutions","timestamp":"1674720300.0"},{"comments":[{"content":"Option A addresses some potential points of failure, but it does not address the potential for the consumer application to become unavailable due to an Availability Zone outage. \n\nOption B addresses some potential points of failure, but it does not address the potential for the database to become unavailable due to an Availability Zone outage. \n\nOption C addresses some potential points of failure, but it does not address the potential for the consumer application to become unavailable due to an Availability Zone outage.","poster":"Buruguduystunstugudunstuy","timestamp":"1671661320.0","comment_id":"752814","upvote_count":"1"}],"poster":"Buruguduystunstugudunstuy","timestamp":"1671661260.0","upvote_count":"3","content":"Selected Answer: D\nOption D offers the highest availability because it addresses all potential points of failure in the system:\n\nAmazon MQ with active/standby brokers configured across two Availability Zones ensures that the message queue is available even if one Availability Zone experiences an outage.\n\nAn Auto Scaling group for the consumer EC2 instances across two Availability Zones ensures that the consumer application is able to continue processing messages even if one Availability Zone experiences an outage.\n\nAmazon RDS for MySQL with Multi-AZ enabled ensures that the database is available even if one Availability Zone experiences an outage.","comment_id":"752813"},{"content":"Selected Answer: D\nOption D","timestamp":"1671395400.0","upvote_count":"2","comment_id":"749163","poster":"career360guru"},{"comment_id":"723856","poster":"Wpcorgan","timestamp":"1669059480.0","upvote_count":"1","content":"D is correct"},{"comment_id":"705475","timestamp":"1666869540.0","poster":"UWSFish","upvote_count":"1","comments":[{"content":"Fault tolerance goes up a level from HA. Active Standby contributes to HA.","comment_id":"734787","poster":"Wajif","timestamp":"1670118660.0","upvote_count":"1"},{"content":"Amazon RDS > MySQL, hence A and B are eliminated","timestamp":"1668052860.0","poster":"nullvoiddeath","comment_id":"714911","upvote_count":"1"}],"content":"Selected Answer: A\nI don't know about D. Active/Standby adds to fault tolerance but does nothing for HA."},{"comment_id":"704821","upvote_count":"1","timestamp":"1666798320.0","poster":"Six_Fingered_Jose","content":"Selected Answer: D\nagree with D"}]}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"provider":"Amazon","numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isMCOnly":true,"isImplemented":true,"id":31},"currentPage":7},"__N_SSP":true}