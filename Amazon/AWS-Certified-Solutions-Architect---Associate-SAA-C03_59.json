{"pageProps":{"questions":[{"id":"RYH8tt7RNwVVJOON4QFZ","question_text":"A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run.\n\nCurrently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"isMC":true,"exam_id":31,"question_id":291,"question_images":[],"answers_community":["C (70%)","B (28%)","3%"],"discussion":[{"comments":[{"timestamp":"1685239800.0","comment_id":"908282","poster":"omoakin","upvote_count":"4","content":"scheduled scaling...."},{"comment_id":"1155164","poster":"jjcode","content":"works loads can vary, how can you predict something that is random?","upvote_count":"2","timestamp":"1708481940.0"},{"comment_id":"895459","comments":[{"content":"Your explanation is contradicting your answer. Since \"the company does not have the resources to analyze the required capacity trend for the ASG\", how come they can create and ASG based on a historic trend? \nC doesn't make sense for me.","comment_id":"1089343","timestamp":"1701866820.0","poster":"Murtadhaceit","upvote_count":"4"}],"content":"But you can make a vague estimation according to the resources used; you don't need to make machine learning models to do that. You only need common sense.","timestamp":"1683845760.0","upvote_count":"1","poster":"ealpuche"}],"timestamp":"1678644360.0","poster":"fkie4","content":"Selected Answer: C\nB is NOT correct. the question said \"The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts.\". \nanswer B said \"Set the appropriate desired capacity, minimum capacity, and maximum capacity\". \nhow can someone set desired capacity if he has no resources to analyze the required capacity. \nRead carefully Amigo","comment_id":"837275","upvote_count":"26"},{"content":"Selected Answer: B\nA scheduled scaling policy allows you to set up specific times for your Auto Scaling group to scale out or scale in. By creating a scheduled scaling policy for the Auto Scaling group, you can set the appropriate desired capacity, minimum capacity, and maximum capacity, and set the recurrence to weekly. You can then set the start time to 30 minutes before the batch jobs run, ensuring that the required capacity is provisioned before the jobs run.\n\nOption C, creating a predictive scaling policy for the Auto Scaling group, is not necessary in this scenario since the company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. This would require analyzing the required capacity trends for the Auto Scaling group counts to determine the appropriate scaling policy.","upvote_count":"6","timestamp":"1679392560.0","poster":"neverdie","comment_id":"845746","comments":[{"content":"(typo above) C is correct..","comment_id":"857940","upvote_count":"1","timestamp":"1680355200.0","poster":"[Removed]"},{"timestamp":"1679759760.0","content":"Look at fkie4 comment... no way to know desired capacity!!! -> B not correct","upvote_count":"1","comments":[{"content":"the text says\n1.-\"A transaction processing company has weekly scripted batch jobs\", there is a Schedule\n2.-\" The company does not have the resources to analyze the required capacity trends for the Auto Scaling \" Do not use\nthe answer is B","comment_id":"916389","poster":"Lalo","upvote_count":"4","timestamp":"1686063840.0"}],"comment_id":"850264","poster":"MssP"},{"content":"B is correct. \"Predictive scaling uses machine learning to predict capacity requirements based on historical data from CloudWatch.\", meaning the company does not have to analyze the capacity trends themselves. https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html","timestamp":"1680355080.0","poster":"[Removed]","comment_id":"857937","upvote_count":"3"}]},{"content":"Selected Answer: B\nB is correct answer ...........................................","poster":"9820ad3","upvote_count":"1","comment_id":"1413861","timestamp":"1743381420.0"},{"comment_id":"1329434","poster":"Denise123","timestamp":"1734697080.0","content":"Selected Answer: C\nOption A - Dynamic scaling policy is reactive not proactive.\nOption B - Scheduled policy is not suitable as it says the work loads can vary \nOption C - Correct\nOption D - Irrelevant","upvote_count":"1"},{"timestamp":"1734509940.0","content":"Selected Answer: C\n\"The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. \" ==> this means this company can take advantage of cloudwatch metric and auto scaling on AWS to set up a predictive scaling group, so that no need to have any resources for this kind of task any more.??","upvote_count":"1","comment_id":"1328338","poster":"EllenLiu"},{"comment_id":"1306042","poster":"studydue","timestamp":"1730513160.0","content":"Answer should be B\nPref: https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html\nPref: https://docs.aws.amazon.com/autoscaling/ec2/userguide/predictive-scaling-policy-overview.html\nkeyword for this question: \"s. The company does not have\nthe resources to analyze the required capacity trends for the Auto Scaling group counts\"","upvote_count":"2"},{"poster":"Hkayne","timestamp":"1714461900.0","upvote_count":"1","content":"Selected Answer: C\nB or C.\nI think C because the company needs an automated way to modify the autoscaling desired capacity","comment_id":"1204391"},{"timestamp":"1708481460.0","upvote_count":"3","content":"How does C works with : transactions can vary, clearly C is designed for workloads that are predictable, if the transactions can vary then predictive scaling will not work. The only one that will work is scheduled since its based on time not workload intensity.","comment_id":"1155154","poster":"jjcode"},{"comment_id":"1109427","upvote_count":"6","content":"Selected Answer: C\nC per https://docs.aws.amazon.com/autoscaling/ec2/userguide/predictive-scaling-create-policy.html. \n\nB is out because it wants the company to 'set the desired/minimum/maximum capacity' but \"the company does not have the resources to analyze the required capacity\".","timestamp":"1703914380.0","poster":"pentium75"},{"content":"Lambda did not appear to take over scripting/batch job, what a surprise","timestamp":"1702652460.0","upvote_count":"4","poster":"Cyberkayu","comment_id":"1097426"},{"comment_id":"1050727","upvote_count":"5","timestamp":"1697984040.0","content":"Selected Answer: B\nFrom GPT4:\nmong the provided options, creating a scheduled scaling policy (Option B) is the most direct and efficient way to ensure that the necessary capacity is provisioned 30 minutes before the weekly batch jobs run, with the least operational overhead. Here's a breakdown of Option B:\n\nB. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.\n\n Scheduled scaling allows you to change the desired capacity of your Auto Scaling group based on a schedule. In this case, setting the recurrence to weekly and adjusting the start time to 30 minutes before the batch jobs run will ensure that the necessary capacity is available when needed, without requiring manual intervention.","comments":[{"comment_id":"1160117","upvote_count":"2","poster":"TheFivePips","content":"yeah chatgpt told me this, so maybe dont take its word as gospel:\n\nUpon reviewing the question again, it appears that the requirements emphasize the need to provision capacity 30 minutes before the batch jobs run and the company's constraint of not having resources to analyze capacity trends. In this context, the most suitable solution is C.\n\nPredictive Scaling can use historical data to forecast future capacity needs.\nConfiguring the policy to scale based on CPU utilization with a target value of 60% aligns with the baseline CPU utilization mentioned in the scenario.\nSetting instances to pre-launch 30 minutes before the jobs run provides the desired capacity just in time.","timestamp":"1708986420.0"}],"poster":"daniel1"},{"timestamp":"1697169000.0","content":"Selected Answer: C\nPredictive scaling: increases the number of EC2 instances in your Auto Scaling group in advance of daily and weekly patterns in traffic flows. If you have regular patterns of traffic increases use predictive scaling, to help you scale faster by launching capacity in advance of forecasted load. You don't have to spend time reviewing your application's load patterns and trying to schedule the right amount of capacity using scheduled scaling. Predictive scaling uses machine learning to predict capacity requirements based on historical data from CloudWatch. The machine learning algorithm consumes the available historical data and calculates capacity that best fits the historical load pattern, and then continuously learns based on new data to make future forecasts more accurate.","upvote_count":"2","poster":"TariqKipkemei","comment_id":"1042244"},{"upvote_count":"2","poster":"bsbs1234","comment_id":"1016152","content":"should be C. Question does not say how long the job will run. don't know when to set the end time in the schedule policy.","timestamp":"1695586500.0"},{"poster":"MrAWSAssociate","timestamp":"1687381920.0","content":"Selected Answer: C\nC is correct!","upvote_count":"2","comment_id":"929940"},{"content":"Selected Answer: C\nif the baseline CPU utilization is 60%, then that's enough information needed to determaine you to predict some aspect of the usage in the future. So key word \"predictive\" judging by past usage.","timestamp":"1686463800.0","comment_id":"920455","poster":"Abrar2022","upvote_count":"2"},{"upvote_count":"1","content":"BBBBBBBBBBBBB","poster":"omoakin","timestamp":"1685239740.0","comment_id":"908281"},{"content":"Selected Answer: B\nB.\nyou can make a vague estimation according to the resources used; you don't need to make machine-learning models to do that. You only need common sense.","poster":"ealpuche","upvote_count":"3","timestamp":"1683845820.0","comment_id":"895461"},{"timestamp":"1682934420.0","comment_id":"886057","poster":"kruasan","upvote_count":"2","content":"Selected Answer: C\nUse predictive scaling to increase the number of EC2 instances in your Auto Scaling group in advance of daily and weekly patterns in traffic flows.\n\nPredictive scaling is well suited for situations where you have:\n\nCyclical traffic, such as high use of resources during regular business hours and low use of resources during evenings and weekends\n\nRecurring on-and-off workload patterns, such as batch processing, testing, or periodic data analysis\n\nApplications that take a long time to initialize, causing a noticeable latency impact on application performance during scale-out events\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html"},{"timestamp":"1679289840.0","comment_id":"844558","poster":"MLCL","upvote_count":"2","content":"Selected Answer: C\nThe second part of the question invalidates option B, they don't know how to procure requirements and need something to do it for them, therefore C."},{"upvote_count":"3","content":"Selected Answer: C\nIn general, if you have regular patterns of traffic increases and applications that take a long time to initialize, you should consider using predictive scaling. Predictive scaling can help you scale faster by launching capacity in advance of forecasted load, compared to using only dynamic scaling, which is reactive in nature.","timestamp":"1679151120.0","poster":"asoli","comment_id":"842847"},{"timestamp":"1678965300.0","comment_id":"840871","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html","poster":"WherecanIstart","upvote_count":"4"},{"comment_id":"836755","poster":"UnluckyDucky","content":"Selected Answer: B\n\"The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts\"\nUsing predictive schedule seems appropriate here, however the question says the company doesn't have the resources to analyze this, even though forecast does it for you using ML.\n\nThe job runs weekly therefore the easiest way to achieve this with the LEAST operational overhead, seems to be scheduled scaling.\n\nBoth solutions achieve the goal, B imho does it better, considering the limitations.\n\nPredictive Scaling:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html\nScheduled Scaling:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html","timestamp":"1678603020.0","upvote_count":"2"},{"poster":"samcloudaws","comment_id":"832214","content":"Selected Answer: B\nScheduled scaling seems mostly simplest way to solve this","upvote_count":"5","timestamp":"1678216080.0"},{"upvote_count":"2","content":"Selected Answer: C\n\"The company needs to provision the capacity 30 minutes before the jobs run.\" This means the ASG group needs to scale BEFORE the CPU utilization hits 60%. Dynamic scaling only responds to a scaling metric setup such as average CPU utilization at 60% for 5 minutes. The forecasting option is automatic, however, it does require some time for it to be effective since it needs the EC2 utilization in the past to predict the future.","timestamp":"1677884460.0","comment_id":"828457","poster":"Steve_4542636"},{"poster":"nder","comment_id":"823574","content":"Selected Answer: A\nDynamic Scaling policy is the least operational overhead.","upvote_count":"1","timestamp":"1677495420.0"},{"poster":"dpmahendra","content":"B Scheduled scaling","comment_id":"822101","timestamp":"1677392220.0","upvote_count":"2","comments":[{"timestamp":"1677392280.0","content":"C: Use predictive scaling to increase the number of EC2 instances in your Auto Scaling group in advance of daily and weekly patterns in traffic flows.","comment_id":"822102","upvote_count":"2","poster":"dpmahendra"}]},{"upvote_count":"1","comment_id":"818946","poster":"LuckyAro","comments":[{"content":"What about provision Capacity 30 minutes before?? Only B C make this, no?","upvote_count":"1","timestamp":"1679759520.0","poster":"MssP","comment_id":"850262"}],"timestamp":"1677138960.0","content":"Selected Answer: A\nThis solution automates the capacity provisioning process based on the actual workload, without requiring any manual intervention. With dynamic scaling, the Auto Scaling group will automatically adjust the number of instances based on the actual workload. The target value for the CPU utilization metric is set to 60%, which is the baseline CPU utilization that is noted on each run, indicating that this is a reasonable level of utilization for the workload. This solution does not require any scheduling or forecasting, reducing the operational overhead."},{"comment_id":"818332","timestamp":"1677095520.0","poster":"bdp123","content":"Selected Answer: C\nanswer is C","upvote_count":"2"},{"timestamp":"1676972160.0","poster":"Neha999","content":"https://www.examtopics.com/discussions/amazon/view/83336-exam-aws-certified-solutions-architect-associate-saa-c02/","upvote_count":"2","comment_id":"816404"}],"answer_ET":"C","timestamp":"2023-02-21 10:36:00","choices":{"A":"Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.","D":"Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%.","C":"Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.","B":"Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run."},"unix_timestamp":1676972160,"url":"https://www.examtopics.com/discussions/amazon/view/100204-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","answer":"C","topic":"1"},{"id":"ODzJnJC39jhA8evMXuvk","question_text":"A solutions architect is designing a company’s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answers_community":["C (100%)"],"answer_images":[],"isMC":true,"answer":"C","choices":{"A":"Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.","D":"Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.","C":"Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.","B":"Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones."},"unix_timestamp":1677005760,"answer_ET":"C","question_images":[],"topic":"1","exam_id":31,"question_id":292,"timestamp":"2023-02-21 19:56:00","answer_description":"","discussion":[{"content":"Selected Answer: C\nA. Multiple EC2 instances to be configured and updated manually in case of DR.\nB. Amazon RDS=Multi-AZ while it asks to be multi-region\nC. correct, see comment from LuckyAro\nD. Manual process to start the DR, therefore same limitation as answer A","timestamp":"1694164020.0","comment_id":"832858","poster":"AlessandraSAA","upvote_count":"10"},{"timestamp":"1692770760.0","poster":"LuckyAro","content":"C: Migrate MySQL database to an Amazon Aurora global database is the best solution because it requires minimal operational overhead. Aurora is a managed service that provides automatic failover, so standby instances do not need to be manually configured. The primary DB cluster can be hosted in the primary Region, and the secondary DB cluster can be hosted in the DR Region. This approach ensures that the data is always available and up-to-date in multiple Regions, without requiring significant manual intervention.","upvote_count":"9","comment_id":"818958"},{"upvote_count":"1","content":"hello friends, question required: The DR design needs to include multiple AWS Regions, but the correct answer is B, how it comes, because the DR here is on AZ not Different Region so the i would go with D","poster":"gulmichamagaun5","comment_id":"1105386","timestamp":"1719324540.0"},{"poster":"TariqKipkemei","comment_id":"1042248","timestamp":"1712980440.0","upvote_count":"3","content":"Selected Answer: C\nLEAST operational overhead = Serverless = Amazon Aurora global database"},{"timestamp":"1709645400.0","upvote_count":"2","poster":"Guru4Cloud","content":"Selected Answer: C\nAmazon Aurora global database can span and replicate DB Servers between multiple AWS Regions. And also compatible with MySQL.","comment_id":"999422"},{"content":"C, Why B? B is multi zone in one region, C is multi region as it was requested","comments":[{"comment_id":"904510","upvote_count":"2","timestamp":"1700715660.0","content":"\" The DR design needs to include multiple AWS Regions.\"\nwith the requirement \"DR SITE multiple AWS region\" -> B is wrong, because it deploy multy AZ (this is not multi region)","poster":"lucdt4"}],"comment_id":"851278","timestamp":"1695746880.0","poster":"GalileoEC2","upvote_count":"2"},{"poster":"KZM","timestamp":"1692949140.0","comment_id":"821349","upvote_count":"4","content":"Amazon Aurora global database can span and replicate DB Servers between multiple AWS Regions. And also compatible with MySQL."},{"poster":"LuckyAro","comments":[{"timestamp":"1692770640.0","comment_id":"818953","upvote_count":"6","poster":"LuckyAro","content":"Sorry, Posted right answer to the wrong question, mistakenly clicked the next question, sorry."}],"upvote_count":"1","comment_id":"818945","content":"With dynamic scaling, the Auto Scaling group will automatically adjust the number of instances based on the actual workload. The target value for the CPU utilization metric is set to 60%, which is the baseline CPU utilization that is noted on each run, indicating that this is a reasonable level of utilization for the workload. This solution does not require any scheduling or forecasting, reducing the operational overhead.","timestamp":"1692770040.0"},{"content":"C is the answer as RDS is only multi-zone not multi region.","upvote_count":"2","poster":"geekgirl22","comment_id":"818513","timestamp":"1692737520.0"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html","timestamp":"1692727140.0","comment_id":"818343","poster":"bdp123","upvote_count":"2"},{"poster":"SMAZ","comment_id":"817370","timestamp":"1692662640.0","upvote_count":"2","content":"C\noption A has operation overhead whereas option C not."},{"timestamp":"1692639060.0","upvote_count":"4","poster":"alexman","content":"Selected Answer: C\nC mentions multiple regions. Option B is within the same region","comment_id":"817064"},{"content":"ANSWER - B ?? NOT SURE","timestamp":"1692636960.0","poster":"jennyka76","comment_id":"817021","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/amazon/view/100302-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"0ztLNmI49hruiet2Holi","question_id":293,"exam_id":31,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/100202-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"A","question_images":[],"discussion":[{"timestamp":"1677139860.0","upvote_count":"16","comment_id":"818965","poster":"LuckyAro","content":"Selected Answer: A\nA. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.\n\nAmazon SQS has a limit of 256 KB for the size of messages. To handle messages larger than 256 KB, the Amazon SQS Extended Client Library for Java can be used. This library allows messages larger than 256 KB to be stored in Amazon S3 and provides a way to retrieve and process them. Using this solution, the application code can remain largely unchanged while still being able to process messages up to 50 MB in size."},{"upvote_count":"7","comment_id":"816401","poster":"Neha999","timestamp":"1676971980.0","content":"A\nFor messages > 256 KB, use Amazon SQS Extended Client Library for Java\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-messages.html"},{"comment_id":"1300099","content":"who would know this...","poster":"tonywang0418","timestamp":"1729357620.0","upvote_count":"3"},{"timestamp":"1719251640.0","comment_id":"1236530","poster":"Gape4","content":"Selected Answer: A\nTo send messages larger than 256 KiB, you can use the Amazon SQS Extended Client Library for Java...","upvote_count":"2"},{"timestamp":"1697169660.0","poster":"TariqKipkemei","content":"Selected Answer: A\nThe Amazon SQS Extended Client Library for Java enables you to manage Amazon SQS message payloads with Amazon S3. This is especially useful for storing and retrieving messages with a message payload size greater than the current SQS limit of 256 KB, up to a maximum of 2 GB.","upvote_count":"4","comment_id":"1042260"},{"poster":"Guru4Cloud","content":"Selected Answer: A\nThe SQS Extended Client Library enables storing large payloads in S3 while referenced via SQS. The application code can stay almost entirely unchanged - it sends/receives SQS messages normally. The library handles transparently routing the large payloads to S3 behind the scenes","upvote_count":"2","timestamp":"1693913220.0","comment_id":"999417"},{"content":"Selected Answer: A\nQuote \"The Amazon SQS Extended Client Library for Java enables you to manage Amazon SQS message payloads with Amazon S3.\" and \"An extension to the Amazon SQS client that enables sending and receiving messages up to 2GB via Amazon S3.\" at https://github.com/awslabs/amazon-sqs-java-extended-client-lib","timestamp":"1689659940.0","poster":"james2033","upvote_count":"2","comment_id":"955004"},{"content":"Selected Answer: A\nAmazon SQS has a limit of 256 KB for the size of messages. \n\nTo handle messages larger than 256 KB, the Amazon SQS Extended Client Library for Java can be used.","upvote_count":"2","timestamp":"1686463980.0","poster":"Abrar2022","comment_id":"920456"},{"upvote_count":"2","timestamp":"1680115260.0","comment_id":"854844","poster":"gold4otas","content":"The Amazon SQS Extended Client Library for Java enables you to publish messages that are greater than the current SQS limit of 256 KB, up to a maximum of 2 GB.\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html"},{"content":"Selected Answer: A\nhttps://github.com/awslabs/amazon-sqs-java-extended-client-lib","upvote_count":"4","timestamp":"1677096000.0","comment_id":"818345","poster":"bdp123"},{"upvote_count":"5","poster":"Arathore","comment_id":"817300","timestamp":"1677025380.0","content":"Selected Answer: A\nTo send messages larger than 256 KiB, you can use the Amazon SQS Extended Client Library for Java. This library allows you to send an Amazon SQS message that contains a reference to a message payload in Amazon S3. The maximum payload size is 2 GB."}],"isMC":true,"answer":"A","timestamp":"2023-02-21 10:33:00","question_text":"A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB.\n\nWhich solution will meet these requirements with the FEWEST changes to the code?","topic":"1","answer_description":"","answers_community":["A (100%)"],"choices":{"C":"Change the limit in Amazon SQS to handle messages that are larger than 256 KB.","A":"Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.","D":"Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.","B":"Use Amazon EventBridge to post large messages from the application instead of Amazon SQS."},"unix_timestamp":1676971980},{"id":"ZVsoHN0cHGB9pxvN6aie","answer_ET":"A","isMC":true,"unix_timestamp":1677031740,"answers_community":["A (100%)"],"discussion":[{"content":"Selected Answer: A\nCloudFront=globally\nLambda@edge = Authorization/ Latency\nCognito=Authentication for Web apps","upvote_count":"15","comment_id":"820997","timestamp":"1692908220.0","poster":"Lonojack"},{"comment_id":"1226493","poster":"Lin878","timestamp":"1733632560.0","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/external-server-authorization-with-lambdaedge/","upvote_count":"2"},{"poster":"Cyberkayu","content":"fewer than 100 users but scattered around the globe, lowest latency.\n\nShould have do nothing, most cost effective.","comment_id":"1097450","upvote_count":"3","timestamp":"1718458260.0"},{"poster":"TariqKipkemei","timestamp":"1712981040.0","upvote_count":"3","comment_id":"1042265","content":"Selected Answer: A\nUse Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally"},{"comment_id":"999404","upvote_count":"4","timestamp":"1709644560.0","poster":"Guru4Cloud","content":"Selected Answer: A\nAmazon Cognito is a serverless authentication service that can be used to easily add user sign-up and authentication to web and mobile apps. It is a good choice for this scenario because it is scalable and can handle a small number of users without any additional costs.\n\nLambda@Edge is a serverless compute service that can be used to run code at the edge of the AWS network. It is a good choice for this scenario because it can be used to perform authorization checks at the edge, which can improve the login latency.\n\nAmazon CloudFront is a content delivery network (CDN) that can be used to serve web content globally. It is a good choice for this scenario because it can cache web content closer to users, which can improve the performance of the web application."},{"content":"Selected Answer: A\nA is perfect.","upvote_count":"2","timestamp":"1701252360.0","comment_id":"909170","poster":"antropaws"},{"comment_id":"858369","content":"Selected Answer: A\nLambda@Edge for authorization\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/adding-http-security-headers-using-lambdaedge-and-amazon-cloudfront/","upvote_count":"3","timestamp":"1696204140.0","poster":"kraken21"},{"timestamp":"1692772860.0","poster":"LuckyAro","upvote_count":"3","comment_id":"818983","content":"Selected Answer: A\nAmazon CloudFront is a global content delivery network (CDN) service that can securely deliver web content, videos, and APIs at scale. It integrates with Cognito for authentication and with Lambda@Edge for authorization, making it an ideal choice for serving web content globally.\n\nLambda@Edge is a service that lets you run AWS Lambda functions globally closer to users, providing lower latency and faster response times. It can also handle authorization logic at the edge to secure content in CloudFront. For this scenario, Lambda@Edge can provide authorization for the web application while leveraging the low-latency benefit of running at the edge."},{"timestamp":"1692727440.0","comment_id":"818350","content":"Selected Answer: A\nCloudFront to serve globally","poster":"bdp123","upvote_count":"2"},{"comment_id":"817376","content":"A\nAmazon Cognito for authentication and Lambda@Edge for authorizatioN, Amazon CloudFront to serve the web application globally provides low-latency content delivery","timestamp":"1692662940.0","upvote_count":"4","poster":"SMAZ"}],"choices":{"D":"Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally.","C":"Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.","A":"Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.","B":"Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally."},"topic":"1","question_id":294,"timestamp":"2023-02-22 03:09:00","answer_images":[],"question_images":[],"exam_id":31,"question_text":"A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible.\n\nWhich solution will meet these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/100341-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","answer":"A"},{"id":"BhvfSBSx36aXX1rWbS3o","isMC":true,"question_text":"A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array’s support contract. Some of the data is accessed frequently, but much of the data is inactive.\n\nA solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution.\n\nWhich type of storage gateway should the solutions architect provision to meet these requirements?","question_id":295,"url":"https://www.examtopics.com/discussions/amazon/view/100220-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Amazon FSx File Gateway","D":"Amazon S3 File Gateway","B":"Tape Gateway","A":"Volume Gateway"},"question_images":[],"answer_images":[],"exam_id":31,"discussion":[{"timestamp":"1692773340.0","upvote_count":"17","content":"Selected Answer: D\nAmazon S3 File Gateway provides on-premises applications with access to virtually unlimited cloud storage using NFS and SMB file interfaces. It seamlessly moves frequently accessed data to a low-latency cache while storing colder data in Amazon S3, using S3 Lifecycle policies to transition data between storage classes over time.\n\nIn this case, the company's aging NAS array can be replaced with an Amazon S3 File Gateway that presents the same NFS and SMB shares to the client workstations. The data can then be migrated to Amazon S3 and managed using S3 Lifecycle policies","poster":"LuckyAro","comment_id":"818987"},{"comment_id":"816529","timestamp":"1692612540.0","poster":"everfly","content":"Selected Answer: D\nAmazon S3 File Gateway provides a file interface to objects stored in S3. It can be used for a file-based interface with S3, which allows the company to migrate their NAS array data to S3 while maintaining the same look and feel for client workstations. Amazon S3 File Gateway supports SMB and NFS protocols, which will allow clients to continue to access the data using these protocols. Additionally, Amazon S3 Lifecycle policies can be used to automate the movement of data to lower-cost storage tiers, reducing the storage cost of inactive data.","upvote_count":"7"},{"timestamp":"1719718800.0","poster":"pentium75","content":"Selected Answer: D\nA - provides virtual disk via iSCSI\nB - provides virtual tape via iSCSI\nC - provides access to FSx via SMB","upvote_count":"3","comment_id":"1109436"},{"upvote_count":"4","poster":"TariqKipkemei","timestamp":"1712981340.0","content":"Selected Answer: D\nThe Amazon S3 File Gateway enables you to store and retrieve objects in Amazon Simple Storage Service (S3) using file protocols such as Network File System (NFS) and Server Message Block (SMB).","comment_id":"1042273"},{"content":"Selected Answer: D\nIt provides an easy way to lift-and-shift file data from the existing NAS to Amazon S3. The S3 File Gateway presents SMB and NFS file shares that client workstations can access just like the NAS shares.\nBehind the scenes, it moves the file data to S3 storage, storing it durably and cost-effectively.\nS3 Lifecycle policies can be used to transition less frequently accessed data to lower-cost S3 storage tiers like S3 Glacier.\nFrom the client workstation perspective, access to files feels seamless and unchanged after migration to S3. The S3 File Gateway handles the underlying data transfers.\nIt is a simple, low-cost gateway option tailored for basic file share migration use cases.","upvote_count":"4","timestamp":"1709642700.0","poster":"Guru4Cloud","comment_id":"999381"},{"comment_id":"955047","poster":"james2033","timestamp":"1705566720.0","upvote_count":"4","content":"Selected Answer: D\n- Volume Gateway: https://aws.amazon.com/storagegateway/volume/ (Remove A, related iSCSI)\n\n- Tape Gateway https://aws.amazon.com/storagegateway/vtl/ (Remove B)\n\n- Amazon FSx File Gateway https://aws.amazon.com/storagegateway/file/fsx/ (C)\n\n- Why not choose C? Because need working with Amazon S3. (Answer D, and it is correct answer) https://aws.amazon.com/storagegateway/file/s3/"},{"content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/storage/how-to-create-smb-file-shares-with-aws-storage-gateway-using-hyper-v/","timestamp":"1693825260.0","comment_id":"828949","poster":"siyam008","upvote_count":"3"},{"content":"Selected Answer: D\nhttps://aws.amazon.com/about-aws/whats-new/2018/06/aws-storage-gateway-adds-smb-support-to-store-objects-in-amazon-s3/","comment_id":"818352","upvote_count":"4","timestamp":"1692727560.0","poster":"bdp123"}],"unix_timestamp":1676981340,"topic":"1","answer":"D","timestamp":"2023-02-21 13:09:00","answers_community":["D (100%)"],"answer_description":"","answer_ET":"D"}],"exam":{"provider":"Amazon","isBeta":false,"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isImplemented":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31},"currentPage":59},"__N_SSP":true}