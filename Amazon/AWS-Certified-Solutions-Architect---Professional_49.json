{"pageProps":{"questions":[{"id":"XlI11vexj4EPQrjGCMoN","question_text":"Which EC2 functionality allows the user to place the Cluster Compute instances in clusters?","question_images":[],"isMC":true,"topic":"1","answer_images":[],"unix_timestamp":1634300220,"choices":{"C":"GPU units","D":"Cluster placement group","A":"Cluster group","B":"Cluster security group"},"answers_community":["D (100%)"],"timestamp":"2021-10-15 14:17:00","exam_id":32,"question_id":241,"discussion":[{"timestamp":"1687353180.0","upvote_count":"1","comment_id":"929543","content":"Selected Answer: D\nD. Cluster placement group","poster":"SkyZeroZx"},{"upvote_count":"1","comment_id":"650850","timestamp":"1661264160.0","content":"It is D","poster":"Ni_yot"},{"poster":"jj22222","comment_id":"577934","content":"Selected Answer: D\nD. Cluster placement group","timestamp":"1648597500.0","upvote_count":"1"},{"content":"Yes - D","poster":"Bhagirathi","upvote_count":"2","timestamp":"1634690880.0","comment_id":"462575"}],"answer":"D","answer_description":"The Amazon EC2 cluster placement group functionality allows users to group cluster compute instances in clusters.\nReference:\nhttps://aws.amazon.com/ec2/faqs/","answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/64149-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"92Xi9qZArznCXcjnwysZ","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/55212-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"unix_timestamp":1623545400,"choices":{"D":"The user can specify where the EBS will be created","C":"The EBS volume will be created on the same tenant hardware assigned to the dedicated instance","A":"The EBS volume will not be created on the same tenant hardware assigned to the dedicated instance","B":"AWS does not allow a dedicated EBS backed instance launch"},"topic":"1","isMC":true,"answer_images":[],"answers_community":["A (100%)"],"exam_id":32,"timestamp":"2021-06-13 02:50:00","answer":"A","question_id":242,"discussion":[{"comment_id":"929544","timestamp":"1687353240.0","poster":"SkyZeroZx","content":"Selected Answer: A\nA. The EBS volume will not be created on the same tenant hardware assigned to the dedicated instance","upvote_count":"1"},{"comment_id":"498431","content":"A. The EBS volume will not be created on the same tenant hardware assigned to the dedicated instance","timestamp":"1639126200.0","poster":"cldy","upvote_count":"2"},{"poster":"01037","comment_id":"380753","upvote_count":"1","timestamp":"1632642720.0","content":"I don't think there is such thing as \"a dedicated EBS backed instance\".\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\nAmazon EBS with Dedicated Instances\nWhen you launch an Amazon EBS-backed Dedicated Instance, the EBS volume doesn't run on single-tenant hardware."}],"answer_description":"The dedicated instances are Amazon EC2 instances that run in a Virtual Private Cloud (VPC) on hardware that is dedicated to a single customer. When a user launches an Amazon EBS-backed dedicated instance, the EBS volume does not run on single-tenant hardware.\nReference:\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/dedicated-instance.html","question_text":"A user has launched a dedicated EBS backed instance with EC2. You are curious where the EBS volume for this instance will be created.\nWhich statement is correct about the EBS volume's creation?"},{"id":"q4uTWhhXAOXXTvGdnBIF","url":"https://www.examtopics.com/discussions/amazon/view/3173-exam-aws-certified-solutions-architect-professional-topic-1/","unix_timestamp":1564928460,"answer_ET":"B","exam_id":32,"question_images":[],"topic":"1","answer_description":"","answer":"B","question_text":"You have recently joined a startup company building sensors to measure street noise and air quality in urban areas. The company has been running a pilot deployment of around 100 sensors for 3 months each sensor uploads 1KB of sensor data every minute to a backend hosted on AWS.\nDuring the pilot, you measured a peak or 10 IOPS on the database, and you stored an average of 3GB of sensor data per month in the database.\nThe current deployment consists of a load-balanced auto scaled Ingestion layer using EC2 instances and a PostgreSQL RDS database with 500GB standard storage.\nThe pilot is considered a success and your CEO has managed to get the attention or some potential investors. The business plan requires a deployment of at least 100K sensors which needs to be supported by the backend. You also need to store sensor data for at least two years to be able to compare year over year\nImprovements.\nTo secure funding, you have to make sure that the platform meets these requirements and leaves room for further scaling.\nWhich setup win meet the requirements?","answer_images":[],"question_id":243,"discussion":[{"timestamp":"1632212220.0","poster":"anola","upvote_count":"21","comments":[{"upvote_count":"1","content":"For 100k sensors, based on the given data, the data processed is 3TB. And for 2 years, 72TB (not 104 TB). I would say, the answer is C","poster":"sraakesh95","comment_id":"1232435","timestamp":"1718715660.0"},{"content":">For one month we will have = 1440 * 30 = 43200 KB of data\n\nIt is a little confusing, but I think this calculation is incorrect, as the question says there is 3GB data per month.","poster":"tiana528","upvote_count":"5","comment_id":"16708","timestamp":"1632507360.0"},{"content":"This calculation doesnt add up . Are we sure the answer is B ?","poster":"Praths","upvote_count":"2","comment_id":"19256","timestamp":"1633017360.0"},{"poster":"JAWS1600","comment_id":"98614","content":"1440 KB per Day . 1440*730=1051,200 KB for 2 years PER Sensor. for 100K sensors =\n1051,200,00,000 KB = 97.9 TB . ( 1051,200,00,000/1024MB/1024GB/1024TB). It is still over 96TB. But cutting close.","timestamp":"1634666760.0","upvote_count":"1"},{"comment_id":"314340","content":"You do not need a calculator for this, B is correct answer. Any seasoned Architect will use a flow control and flow control can be done by SQS.","poster":"nitinz","upvote_count":"3","timestamp":"1635727560.0"}],"comment_id":"5841","content":"Correct answer: \"B\"\n\nFor one day we will have = 24 hrs * 60 min * 1 KB = 1440 KB of data\nFor one month we will have = 1440 * 30 = 43200 KB of data\nFor 100k sensors we will have '4.32' TB of data in a month\n\nThis transates to '103,68' TB of data for 2 years\n\nOption \"C\" is incorrect as the question does not mention how the large data gets scaled or how the ingestion of large data will be handled. Also '96TB' of storage is insufficient when compared to '103.68' TB of data derived above."},{"timestamp":"1632412920.0","comment_id":"9412","upvote_count":"9","content":"I think the the explaination is talking about the answer B, using Dynamo DB to ingest data and saving data in Redshift(3GB*1000*24=72TB).","poster":"Judy"},{"timestamp":"1724667360.0","content":"Selected Answer: B\nTricky one. By exclusion I have removed all that do not scale the RDS cluster to a minimum retention. So A and D go away. \n\nThen we have an indication of Redshift usage with the \"comparison year over year\" this is a clear BI reference, so RedShift comes into play. \nBut Redshift is not good as an ingestion platform. It is more of a read heavy platform, so here Dynamo comes into play. Knowing AWS and after 12+years working with it and doing exams I would go with B.","poster":"Coffeinerd","upvote_count":"3","comment_id":"1272617"},{"content":"B. Ingest data into a DynamoDB table and move old data to a Redshift cluster","timestamp":"1723717860.0","upvote_count":"1","poster":"amministrazione","comment_id":"1266367"},{"poster":"Greanny","timestamp":"1712068020.0","comment_id":"1188104","content":"chatgpt - Given the requirements and the need for scalability, cost-effectiveness, and long-term storage, option B seems to be the most suitable:\n\nIngesting data into DynamoDB provides scalability and flexibility.\nMoving old data to Redshift allows for efficient long-term storage and analysis.\nThis approach ensures that the platform can support the increased number of sensors and the required retention period while leaving room for further scaling.","upvote_count":"1"},{"comment_id":"1074447","poster":"nideesh","timestamp":"1700369100.0","upvote_count":"1","content":"Selected Answer: D\ncurrent IOPS is 10 and 3GB per month.\nThere will be increase from 100 to 100k sensors almost 1000 times.\nRDS instance needs to be upgraded for CPU as well as storage.\nA) SQS does not meet storage requirement Also there is multiple EC2 instance\nB) Application needs to be changed from SQL to No SQL, which differs from PILOT project\nC) redsdhift is data warehouse for analytics"},{"timestamp":"1691608260.0","upvote_count":"1","content":"Answer: b","poster":"Andy85","comment_id":"976967"},{"timestamp":"1691045280.0","poster":"autobahn","comments":[{"upvote_count":"1","timestamp":"1712151180.0","content":"From the documentation:\n“ The maximum size of any item collection for a table which has one or more local secondary indexes is 10 GB. This does not apply to item collections in tables without local secondary indexes, and also does not apply to item collections in global secondary indexes. Only tables that have one or more local secondary indexes are affected.”\n\nYou can also split data into multiple tables in case you have a LSI.","poster":"awsylum","comment_id":"1188692"}],"content":"It cannot be B because Dynamo DB has a restriction of 10GB per partition and the max # of partitions is 3000. So, per table max storage is only 30TB whereas here we want 72 TB if two years data needs to be kept","comment_id":"970822","upvote_count":"1"},{"content":"It doesn't look like there is a correct answer at all.\n\nB - the tests were conducted at SQL DB (PostgreSQL, RDS) why people vote for DynamoDB at all which is NoSQL.\nC - seems could work out for a while, but doesn't have anything for future scaling and there is no information about IOPS as well.\nA, D - doesn't meet the minimum requirements.","poster":"kondratyevmn","upvote_count":"2","timestamp":"1685619600.0","comments":[{"content":"The original system was a POC. The production system definitely could use DynamoDB and the clue in the question is the 1KB per minute of data which is exactly the write request unit that DynamoDB supports. You’re thinking of a RDS migration to DynamoDB which can be complicated. But that’s not the scenario.","comment_id":"1188687","upvote_count":"1","timestamp":"1712150280.0","poster":"awsylum"}],"comment_id":"912022"},{"poster":"alexua","content":"making the simple estimation of required storage - it get > 100TB, so option Redshift (96TB) won't serve. The correct is \"B\"","comment_id":"847614","upvote_count":"1","timestamp":"1679530020.0"},{"comment_id":"834818","upvote_count":"1","timestamp":"1678441380.0","content":"Selected Answer: B\nAs we need Data to be store for 2 years and dynamodb have high storage capacity","poster":"gameoflove"},{"comment_id":"734076","upvote_count":"3","timestamp":"1670017380.0","content":"Selected Answer: B\nA, No enough storage space,\nB, Seems okay\nC, Writing to Redshift directly without helpers, such as Kinesis, would be terribly slow. \nD, No enough storage space\n\nSo B is the choice","poster":"TigerInTheCloud"},{"comment_id":"650970","content":"B is correct","upvote_count":"1","poster":"rochester","timestamp":"1661284380.0"},{"timestamp":"1648587300.0","comment_id":"577859","upvote_count":"1","poster":"jj22222","content":"Selected Answer: B\nb looks right"},{"content":"I will go for C, as the question is only focusing on the backend part, which is the data layer. The ingestion layer is being handled by instances with autoscaling and load balancer, which is fine.","poster":"andypham","comment_id":"444977","timestamp":"1636286820.0","upvote_count":"1"},{"poster":"Osemk","content":"Answer is B","upvote_count":"2","comment_id":"415844","timestamp":"1636281540.0"},{"comment_id":"413723","timestamp":"1636265520.0","upvote_count":"1","poster":"Gustava6272","content":"I think A is better answer for IOPS , as size can always be increase . RDS has 80K IOPS, which can be increased by SQS ( even size matches 1K to 256KB)"},{"poster":"Akhil254","content":"B Correct","comment_id":"405772","timestamp":"1636177920.0","upvote_count":"1"},{"content":"Answer is B. DynamoDB is perfect for scale and Redshift is meant for data warehousing. It cannot handle streaming data ingestion.","poster":"nik_aws","timestamp":"1636165320.0","upvote_count":"2","comment_id":"390652"},{"timestamp":"1636145820.0","poster":"santhoshmp","comment_id":"378435","content":"answer is B , redshift is a data warehouse","upvote_count":"2"},{"upvote_count":"1","comment_id":"327948","timestamp":"1636092480.0","poster":"anandbabu","content":"i will go with B"},{"poster":"01037","content":"I'll go for B","upvote_count":"1","timestamp":"1635976380.0","comment_id":"323183"},{"comment_id":"322475","poster":"cldy","upvote_count":"1","timestamp":"1635772800.0","content":"As an architect if i get this requirement in real project I will go for B DynamoDB - scaling is crucial ...."},{"upvote_count":"2","content":"I go for \"B\" based my past experiences.... Option \"C\" will be a nightmare after day2 operations.","timestamp":"1635626460.0","comment_id":"292996","poster":"RomanTsai"},{"content":"For 100 devices, we need 10 IOPS and 3G per months\nSo for 100K devices, we need 10K IOPS and 3T per month (or 3T x 24 = 72T per 2 years).\nB is correct as we need quick IO process, scalable, no storage limit to handle data then store it to RedShift.\nChoose C. RedShift from beginning seem not correct, Redshift is Datawarehouse and can't process quick data, it's used to store data. Furthermore, 96T is fixed and we can't do scalable","poster":"ChauPhan","comment_id":"212120","timestamp":"1635581700.0","upvote_count":"2"},{"poster":"NarendraNK","timestamp":"1635381240.0","content":"\" C \". The question says 3G every month for 100 devices. So, for 100K devices, it becomes almost 3000G or 3T. For 2 years, it adds up to 72T which can be well served by the 96T cluster.\nAnd, for IoT, redshift is the recommended setup.","upvote_count":"2","comment_id":"192731"},{"poster":"srknbngl","upvote_count":"1","timestamp":"1635275760.0","comment_id":"190063","content":"Answer is B"},{"poster":"cpal012","content":"Redshift is a data warehouse and can not ingest large amounts of streaming data. You'd usually need S3 to buffer. Can't be the right answer.","timestamp":"1635273180.0","comment_id":"189928","upvote_count":"1"},{"poster":"un","content":"B is correct answer","comment_id":"189192","timestamp":"1635233640.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1635159900.0","poster":"kanavpeer","content":"its C, tricky question. Redshift also uses postgresSQL in the backend.","comment_id":"168992"},{"upvote_count":"1","comment_id":"143140","poster":"fullaws","timestamp":"1635158280.0","content":"B is correct answer"},{"timestamp":"1635142680.0","content":"Answer should be C - need to store the data for year on year compare for improvement. replacing RDS with redshift would be more recommendable as it will simplify the architecture compare to load data first in dynamo DB and move old data to redshift","poster":"askAditya","upvote_count":"1","comment_id":"138981"},{"poster":"noisonnoiton","content":"go with B","comment_id":"122341","timestamp":"1635123480.0","upvote_count":"2"},{"comment_id":"106895","upvote_count":"5","poster":"JAWS1600","content":"B. Whenever you think of Key-Value - always opt to choose NoSQl - DynamoDB in this case.\nSensor data is key-value data.","timestamp":"1635017460.0"},{"content":"A - Ruled out as DB needs to be adjusted\nD - Ruled out - DB will grow to 72 TiB in 2 years - RD/PG can handle only 64 Tib\nC - Ruled out - Redshift is a warehousing solution\n\nB - DynamoDB is scalable - no issues with increasing the IOPS .... the reference to Redshift is just to confuse","comment_id":"104519","timestamp":"1634901480.0","poster":"rajs","upvote_count":"6"},{"content":"Answer is B. Redshift should not be used for OLAP processing. Also its not highly available db. two daggers.","timestamp":"1634734500.0","upvote_count":"1","comment_id":"100502","poster":"adixit"},{"content":"B is My answer, since Redshift can't handle 10k data ingestion. DynamoDB is a scalable solution and Redshift used for DW","comment_id":"92614","upvote_count":"1","timestamp":"1634487360.0","poster":"FreeSwan"},{"upvote_count":"1","poster":"shahul_rahila_raisa","comment_id":"74157","timestamp":"1634396460.0","content":"Correct answer: \"B\""},{"comment_id":"56667","poster":"xueyue2","upvote_count":"3","content":"B should be the answer. \n1. leaves room for further scaling. -- Dynamo DB\n2. store sensor data for at least two years to be able to compare year over year\nImprovements. --- RedShift","timestamp":"1634332920.0"},{"upvote_count":"2","content":"B is Correct","poster":"BillyC","comment_id":"49612","timestamp":"1634137380.0"},{"poster":"Musk","content":"I'll go for B","comment_id":"47991","upvote_count":"1","timestamp":"1633767780.0"},{"content":"option C may not be the choice as Redshift need to data load from S3 or via Kinesis, it seems we cannot make the sensors to write data into Redshift directly\nThus, option B looks better choice","timestamp":"1633698300.0","comment_id":"47593","upvote_count":"1","poster":"Jshuen"},{"upvote_count":"2","poster":"jay1ram2","comment_id":"45011","content":"My answer is B. Even though Redshift is capable of storing data. It is certainly not capable of handling currency for 100K sensor data (At least 1000 TPS) or it is built for small frequent transactions. However, DynamoBD can easily handle 1000 WPS. Then, moving the data from DynanoDB to Redshift for long term analytics is the right approach","timestamp":"1633696980.0"},{"upvote_count":"1","poster":"Deeman","timestamp":"1633664820.0","content":"C because in my view, it is not looking for an Archival solution immediately, It is looking at volumes and how year on year data could be compared. It is also not possible for an ACID based database Strong consistency as performance would be affected. A NoSQL Database would do the job and do it well, hence the choice of Redshift...","comment_id":"36517"},{"comment_id":"35925","timestamp":"1633522020.0","comments":[{"upvote_count":"1","timestamp":"1633639020.0","comment_id":"36223","content":"So confusing to B\nhttps://acloud.guru/forums/aws-certified-solutions-architect-professional/discussion/-KXB-Oe1w6QSQZG7CzXY/sensor\nB or C?","poster":"AnNguyen"}],"upvote_count":"1","poster":"AnNguyen","content":"Answer is C\n\"PostgreSQL RDS database\": We need an relational database here, right?"},{"timestamp":"1633375320.0","upvote_count":"1","poster":"simonyu","comment_id":"35380","content":"For C, there is no reason to change DB to warehouse solution."},{"content":"So i really like this site more and more because you get to see how other people arrive at answers and i think it helps us all process.\n\nFor me the answer was C. Because the proposal is 1000x more sensors, you will have 1000 times more data. So from 3mb per month to 3TB per month which is 36TB per year for 2 years. Some of you mentioned scaling, but this solution does not ask us to take scaling into account. I think the issue is what would we do in the real world and what is the exam specifically asking. I have found if I just answer the questions with the information I have been given, i have gotten more questions right.","comments":[{"content":"It does ask us to take scaling into account, the last sentence says \"leaves room for further scaling\"","timestamp":"1634316060.0","upvote_count":"2","poster":"xueyue2","comment_id":"56666"}],"poster":"ReggieR2","comment_id":"32850","timestamp":"1633347060.0","upvote_count":"3"},{"comment_id":"24175","upvote_count":"1","poster":"Smartphone","timestamp":"1633245300.0","content":"Correct Answer is B: Handle 10K IOPS ingestion and store data into Redshift for analysis."},{"comment_id":"18748","upvote_count":"5","poster":"skywalker","content":"I go for \"B\"... coz they will have lots of sensors and tons of data coming over the years. Having a scalable database should help to help these requirement. \n\nRedshift are not suitable for real time data ingest and thus \"C\" shouldn't be consider.","timestamp":"1632900240.0"},{"content":"C? \n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\n\nDetermining the Number of Nodes\n\nThe number of nodes that you choose depends on the size of your dataset and your desired query performance. Using the dense storage node types as an example, if you have 32 TB of data, you can choose either 16 ds2.xlarge nodes or 2 ds2.8xlarge nodes. If your data grows in small increments, choosing the ds2.xlarge node size allows you to scale in increments of 2 TB. If you typically see data growth in larger increments, a ds2.8xlarge node size might be a better choice.","comments":[{"poster":"AmazonAu","timestamp":"1632663180.0","content":"I mean B.","upvote_count":"1","comment_id":"17334"}],"comment_id":"17333","upvote_count":"1","timestamp":"1632607920.0","poster":"AmazonAu"},{"content":"For future expansion, go with dynamodb","comment_id":"10628","upvote_count":"2","poster":"dpvnme","timestamp":"1632470700.0"}],"answers_community":["B (89%)","11%"],"timestamp":"2019-08-04 16:21:00","choices":{"A":"Add an SQS queue to the ingestion layer to buffer writes to the RDS instance","C":"Replace the RDS instance with a 6 node Redshift cluster with 96TB of storage","B":"Ingest data into a DynamoDB table and move old data to a Redshift cluster","D":"Keep the current architecture but upgrade RDS storage to 3TB and 10K provisioned IOPS"},"isMC":true},{"id":"JNgiFeQ6liZ0c5vVHJzf","timestamp":"2020-01-09 16:29:00","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/11666-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"exam_id":32,"answer_description":"","topic":"1","question_text":"To serve Web traffic for a popular product your chief financial officer and IT director have purchased 10 m1.large heavy utilization Reserved Instances (RIs), evenly spread across two availability zones; Route 53 is used to deliver the traffic to an Elastic Load Balancer (ELB). After several months, the product grows even more popular and you need additional capacity. As a result, your company purchases two C3.2xlarge medium utilization Ris. You register the two c3.2xlarge instances with your ELB and quickly find that the m1.large instances are at 100% of capacity and the c3.2xlarge instances have significant capacity that's unused.\nWhich option is the most cost effective and uses EC2 capacity most effectively?","discussion":[{"timestamp":"1632346560.0","comment_id":"37121","content":"Answer is D\nUse a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin","upvote_count":"16","poster":"amog"},{"poster":"Chinta","content":"Correct answer is B","upvote_count":"1","comment_id":"1289229","timestamp":"1727310480.0"},{"upvote_count":"1","poster":"tutrieuchau","content":"Selected Answer: D\nAnswer is D, The company have already purchase Reversed Instance (RIs) of C3, so turn off ore remove it will waste money","timestamp":"1723789320.0","comment_id":"1266859"},{"poster":"amministrazione","content":"D. Use a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin.","timestamp":"1723724520.0","upvote_count":"1","comment_id":"1266433"},{"upvote_count":"1","content":"Selected Answer: A\nA - looks like the most viable option, though you need to give up RIs for c3.2xlarge instance (which is not cost effective, as you have to pay for it, unless there is an exception and Support cancel it at early stage.) It's also the most effective use of the EC2 capacity, as there is auto-scaling, meaning capacity meets the dynamic demand. Last but not least, if you check pricing calculator, c3 is way more expensive instances then m1. \n\nB - No. Not scalable. Not cost effective to shut down RIs in havy utilization.C -  No. latency based routing is for multi region deployment where as here is just multi AZD - No. No point to buy one more ELB; weighted round robin policy is used for a different use case, e.g. associate multiple resources with a single domain name + software testing. We don't have this requirement. In addition to that, it's not the most effective use of the EC2 capacity, as there is no auto-scaling.","comment_id":"941710","timestamp":"1688379540.0","poster":"kondratyevmn"},{"content":"Selected Answer: D\nAnswer is D\nUse a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin\n\nConsidered the use of RIS all pay for use on or off instance","upvote_count":"2","poster":"SkyZeroZx","timestamp":"1686340440.0","comment_id":"919611"},{"poster":"TechX","upvote_count":"1","timestamp":"1657070400.0","comment_id":"627662","content":"Selected Answer: D\nIt's D"},{"upvote_count":"1","poster":"Akhil254","comment_id":"405807","timestamp":"1636193280.0","content":"D Correct"},{"upvote_count":"1","comment_id":"397990","poster":"backfringe","timestamp":"1636078800.0","content":"I go with D as there are RIs"},{"timestamp":"1635954960.0","content":"D is correct","poster":"01037","upvote_count":"2","comment_id":"335485"},{"poster":"cldy","upvote_count":"2","comment_id":"323198","content":"D.\nalthough not very cost effective with 2 ELBs but D solves the requirement with weighted R53 routing.","timestamp":"1635798900.0"},{"content":"A, B if you have already purchase Reversed Instance (RIs) so if you shutdown either of them, you still lose the cost.\nC. It does not work well, as Route53 can't route to EC2 unless it has EIP because its IP will change when rebooted. You will need EIPs for all EC2s. So it is impossible solution.\nD make sense, the traffic for each instance was loaded-balance however the improper type between C2 and M1 cause the CPU on M1 meanwhile no load on C2. You need another ELB, one for C2 group and one for M1 group; then route the traffic 80% to C1 and 20% to C1 for example.","timestamp":"1635783120.0","upvote_count":"3","comment_id":"231982","poster":"ChauPhan"},{"poster":"kneel","upvote_count":"1","comment_id":"203850","comments":[{"content":"R53 doesnt work that way, it is not a intra-regional load balancer. The closest it can get to inter regional load balancing is a multi-value response and then letting the client decide which IP to use","timestamp":"1635324540.0","poster":"cpal012","comment_id":"207332","upvote_count":"1"},{"timestamp":"1635551940.0","poster":"cpal012","comment_id":"207335","comments":[{"comment_id":"209205","content":"Thank you. Latency-based routing only works If the application is hosted in multiple AWS Regions\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency","timestamp":"1635753300.0","poster":"newme","upvote_count":"1"}],"content":"Plus as R53 is regional, latency records are also regional. All these servers are in the same region and any such latency record would apply to them all equally","upvote_count":"2"}],"content":"I think C is the correct answer. The question asks for the \"most cost effective\" option. Option C removes the ELB which is cheaper than option D which adds another ELB. Additionally, Option C utilizes Latency Based routing which ensures none of the four instances are overloaded.","timestamp":"1635178920.0"},{"content":"Answer is D","comment_id":"190154","timestamp":"1634554140.0","poster":"srknbngl","upvote_count":"2"},{"upvote_count":"2","content":"D is correct","comment_id":"143401","poster":"fullaws","timestamp":"1634385480.0"},{"comment_id":"123299","poster":"noisonnoiton","upvote_count":"4","content":"go with D","timestamp":"1634230500.0"},{"poster":"drneon","comment_id":"66670","upvote_count":"2","content":"Is it possible to guarantee effective use of EC2 capacity with route53 weighted routing?\nAnswer is B","timestamp":"1633881720.0","comments":[{"upvote_count":"3","comment_id":"66675","timestamp":"1633923480.0","content":"But you have to use many RIs - m1.large, not turn off. \nso answer is D.","poster":"drneon"}]},{"content":"A & B are wrong - not utilizing Reserved Instances (waste). C will have same current problem. D is not that quite cost-effective but it resolves the problem(currently, 3/2020, you can do weighting at your ELB itself).","timestamp":"1633580940.0","comment_id":"66012","comments":[{"upvote_count":"2","poster":"pleasespammelater","timestamp":"1634287500.0","content":"Yep, D is correct (Note that the weighted routing for ALB's was added in Nov 2019 https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/ )","comment_id":"126411"}],"poster":"Smart","upvote_count":"4"},{"timestamp":"1633376100.0","upvote_count":"3","poster":"BillyC","comment_id":"49654","comments":[{"upvote_count":"1","poster":"mandrakenet","content":"D is not cost effective, lack autoscaling","timestamp":"1633446540.0","comment_id":"60857"}],"content":"D is Correct!"},{"comment_id":"42462","content":"D Use a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin http://jayendrapatil.com/tag/connection-draining/","poster":"dumma","upvote_count":"4","timestamp":"1632998220.0"},{"comments":[{"upvote_count":"1","content":"The instances are reserved. You can shut them off but you still have to pay for them.","comment_id":"207331","poster":"cpal012","timestamp":"1635184920.0"}],"timestamp":"1632422460.0","comment_id":"39023","content":"Not A: The 10 M1s are already at 100%. Doesnt make sense to add 10 more. Its better to use the C3\nNot C: this doesn't guarantee effective use of EC2 capacity\nNot D: Inefficient to have 2 ELBs\n\nB is the better answer.","upvote_count":"2","poster":"ReggieR2"}],"answer_images":[],"answer":"D","answer_ET":"D","unix_timestamp":1578583740,"question_id":244,"choices":{"A":"Configure Autoscaling group and Launch Configuration with ELB to add up to 10 more on-demand m1.large instances when triggered by Cloudwatch. Shut off c3.2xlarge instances.","C":"Route traffic to EC2 m1.large and c3.2xlarge instances directly using Route 53 latency based routing and health checks. Shut off ELB.","D":"Use a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin.","B":"Configure ELB with two c3.2xlarge instances and use on-demand Autoscaling group for up to two additional c3.2xlarge instances. Shut off m1.large instances."},"answers_community":["D (80%)","A (20%)"]},{"id":"2k46SmFrpCzR2bWyBsww","question_text":"Which system is used by Amazon Machine Images paravirtual (PV) virtualization during the boot process?","discussion":[{"comment_id":"576104","poster":"Mechanic","timestamp":"1648375260.0","content":"Selected Answer: D\nD.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html","upvote_count":"1"},{"timestamp":"1640740440.0","content":"D. PV-GRUB","upvote_count":"1","poster":"eightddy","comment_id":"511685"},{"comment_id":"495744","poster":"cldy","upvote_count":"2","content":"D. PV-GRUB","timestamp":"1638862920.0"}],"answer":"D","timestamp":"2021-12-07 08:42:00","answer_images":[],"isMC":true,"answers_community":["D (100%)"],"answer_ET":"D","answer_description":"Amazon Machine Images that use paravirtual (PV) virtualization use a system called PV-GRUB during the boot process. PV-GRUB is a paravirtual boot loader that runs a patched version of GNU GRUB 0.97. When you start an instance, PV-GRUB starts the boot process and then chain loads the kernel specified by your image's menu.lst file.\nReference:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/UserProvidedKernels.html","question_images":[],"exam_id":32,"question_id":245,"unix_timestamp":1638862920,"choices":{"B":"PV-AMI","C":"PV-WORM","A":"PV-BOOT","D":"PV-GRUB"},"url":"https://www.examtopics.com/discussions/amazon/view/67276-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1"}],"exam":{"name":"AWS Certified Solutions Architect - Professional","numberOfQuestions":1019,"id":32,"isBeta":false,"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Amazon","isMCOnly":false},"currentPage":49},"__N_SSP":true}