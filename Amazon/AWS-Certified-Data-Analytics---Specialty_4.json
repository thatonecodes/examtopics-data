{"pageProps":{"questions":[{"id":"2bVHXVUjTtq9b0qiMM1O","question_images":[],"unix_timestamp":1637086380,"topic":"1","choices":{"A":"The finance department grants Lake Formation permissions for the tables to the external account for the marketing department.","B":"The finance department creates cross-account IAM permissions to the table for the marketing department role.","C":"The marketing department creates an IAM role that has permissions to the Lake Formation tables."},"url":"https://www.examtopics.com/discussions/amazon/view/66163-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_text":"A company has a marketing department and a finance department. The departments are storing data in Amazon S3 in their own AWS accounts in AWS\nOrganizations. Both departments use AWS Lake Formation to catalog and secure their data. The departments have some databases and tables that share common names.\nThe marketing department needs to securely access some tables from the finance department.\nWhich two steps are required for this process? (Choose two.)","answer_images":[],"discussion":[{"poster":"ryuhei","timestamp":"1661597460.0","content":"Selected Answer: AC\nWhy are there only 3 options?\nI think the answer is A and C.","upvote_count":"7","comment_id":"652540"},{"poster":"vkbajoria","comment_id":"546054","timestamp":"1644692520.0","content":"Answer is A & B","upvote_count":"5","comments":[{"timestamp":"1648304340.0","content":"Correct answer is A & C. Read links below\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/cross-account-permissions.html\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/regranting-locations.html","comment_id":"575588","upvote_count":"3","poster":"CHRIS12722222"}]},{"timestamp":"1688393100.0","comment_id":"941910","poster":"theawsguys","upvote_count":"3","content":"Option B is not required for this process. Cross-account IAM permissions are not directly used for granting access to Lake Formation tables. Instead, Lake Formation permissions are managed within the Lake Formation service itself, and access to the tables is granted through IAM roles and permissions configured within the respective AWS accounts.\n\nTo summarize, the finance department grants Lake Formation permissions to the marketing department's account, and the marketing department creates an IAM role with the necessary permissions to access the shared tables. These two steps enable secure access to the required tables between the departments."},{"upvote_count":"4","timestamp":"1682965860.0","poster":"pk349","content":"AC: I passed the test","comment_id":"886594"},{"timestamp":"1660679280.0","upvote_count":"1","content":"I think A and B. Why is the marketing department the one creating the IAM role","comment_id":"647800","poster":"Dun6"},{"upvote_count":"2","comments":[{"timestamp":"1658986500.0","content":"There are usually 5 options given in a question.","poster":"rocky48","comment_id":"638489","upvote_count":"2"}],"timestamp":"1658986500.0","poster":"rocky48","content":"Selected Answer: AC\nSelected Answer: AC","comment_id":"638487"},{"upvote_count":"1","content":"Selected Answer: AC\nA & C - why was already explained","comment_id":"605285","poster":"f4bi4n","timestamp":"1653207600.0"},{"timestamp":"1637086380.0","comment_id":"479556","poster":"goutes","upvote_count":"2","comments":[{"timestamp":"1638670680.0","poster":"tobsam","comments":[{"content":"Agree A&B\nFor further reading please see \nhttps://docs.aws.amazon.com/lake-formation/latest/dg/sharing-catalog-resources.html","upvote_count":"5","comment_id":"506647","comments":[{"timestamp":"1645154760.0","poster":"sejar","content":"By this link, it has to be A & C \nA: finance department grants Lake Formation permissions for the tables to the external account for the marketing department.\n[aws link: You can share Data Catalog resources (databases and tables) with external AWS accounts by granting Lake Formation permissions on the resources to the external accounts]\nC: The marketing department creates an IAM role that has permissions to the Lake Formation tables\n[aws link: When you share a resource with an AWS organization, you're sharing the resource with all accounts at all levels in that organization. The data lake administrator in each external account must then grant permissions on the shared resources to principals in their account.]\nA would share the access to lake formation to the marketing account, & the administrator from marketing account creates a role with permission to lake formation table ( role can be assigned to any principal )","upvote_count":"13","comment_id":"549956"}],"poster":"lakediver","timestamp":"1640143980.0"}],"content":"I disagree. Answer is A & B. \nhttps://docs.aws.amazon.com/lake-formation/latest/dg/lake-formation-permissions.html\n...Lake Formation uses a combination of Lake Formation permissions and IAM permissions. The IAM permissions model consists of IAM policies.","upvote_count":"9","comment_id":"494040"}],"content":"B , C ?"}],"answer_description":"","timestamp":"2021-11-16 19:13:00","question_id":16,"answers_community":["AC (100%)"],"answer":"AC","isMC":true,"answer_ET":"AC","exam_id":20},{"id":"t1xMrIUQkAwNOGMVD6JE","answer":"B","discussion":[{"comment_id":"492581","timestamp":"1638455040.0","content":"B - KEY - both tables are huge and have common key . \nALL-distribution style for the product table not correct because of size\nEVEN distribution style for the product table - may not necessarily help.","upvote_count":"16","poster":"lakeswimmer","comments":[{"upvote_count":"3","poster":"iris22","comment_id":"573963","content":"ref: https://docs.aws.amazon.com/redshift/latest/dg/t_designating_distribution_styles.html","timestamp":"1648080840.0"}]},{"comment_id":"474631","upvote_count":"5","content":"Option B - As both tables have a common key and are used widely in reports.https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html","timestamp":"1636435680.0","poster":"Fazil_Cp"},{"timestamp":"1682965920.0","content":"B: I passed the test","poster":"pk349","comment_id":"886597","upvote_count":"3"},{"content":"B. A KEY distribution style for both tables would be the best option for optimal query performance.\n\nExplanation:\nWhen using a KEY distribution style, Redshift distributes the rows according to the values in one column that serves as the distribution key. If the product_sku column is used as the distribution key for both the product table and the transactions table, rows with the same product_sku value will be stored on the same node. This will reduce the amount of data that needs to be moved between nodes during a query, resulting in faster query performance.","upvote_count":"4","poster":"rags1482","timestamp":"1679963460.0","comment_id":"852617"},{"poster":"Chelseajcole","comment_id":"771715","upvote_count":"2","timestamp":"1673377920.0","content":"Even Walmart product table might not reach 100G size...I think the question is not well-stated"},{"upvote_count":"2","comment_id":"720327","content":"Selected Answer: B\nThis question is very tricky. Usually product table is a dimension table and it should be small.\nBut here \"The tables are over 100GB in size\"\nSo B.","poster":"rav009","timestamp":"1668673920.0"},{"timestamp":"1666894320.0","comment_id":"705819","content":"Correct answer is C as the key requirement is to have optimal query performance, it would be better to use ALL distribution style for the product dimension table and EVEN distribution style for the transactions fact table. \n\nOptions A & D are wrong as they would not provide optimal query performance.\n\nOption B is wrong as the KEY distribution style for both tables can be used but it won't provide an optimal query performance.","comments":[{"timestamp":"1666894320.0","poster":"cloudlearnerhere","comments":[{"poster":"chdorrego","comment_id":"736377","timestamp":"1670282040.0","upvote_count":"1","content":"Agree with everything you said but the 100GB size in both tables makes it better to distribute it by Key."}],"content":"Distribute the fact table and its largest dimension table on their common columns. Choose the largest dimension based on the size of dataset that participates in the most common join, not just the size of the table. If a table is commonly filtered, using a WHERE clause, only a portion of its rows participate in the join. Such a table has less impact on redistribution than a smaller table that contributes more data. Designate both the dimension table's primary key and the fact table's corresponding foreign key as DISTKEY. If multiple tables use the same distribution key, they are also collocated with the fact table. Your fact table can have only one distribution key. Any tables that join on another key isn't collocated with the fact table.","upvote_count":"1","comment_id":"705820"}],"upvote_count":"2","poster":"cloudlearnerhere"},{"content":"Selected Answer: B\nOption B - As both tables have a common key and are used widely in reports.","poster":"fqc","upvote_count":"1","comment_id":"644091","timestamp":"1659959280.0"},{"timestamp":"1658286780.0","poster":"rocky48","content":"Selected Answer: B\nOption B - As both tables have a common key and are used widely in reports.","comment_id":"633813","upvote_count":"1"},{"timestamp":"1656733560.0","content":"Selected Answer: A\nThe question doesn't tell what type of queries are common. Key distribution is good if there are joins. All-style can be ruled out due to the size concerns. So in this case I would use Even distribution style. There is no evidence to tell that Product column is the most suitable colum for partitioning.","upvote_count":"1","comment_id":"625911","poster":"dushmantha"},{"timestamp":"1654252140.0","content":"Selected Answer: B\nI also think answer B. But the question is unclear in my opinion and it could be C. \nWhat pushes me to B is the fact that it is not mentioned explicitly that the product table is small or can be considered a Fact-Table. It is explicitly mentioned that both tables share a common field.","upvote_count":"1","poster":"Ramshizzle","comment_id":"611029"},{"comment_id":"608241","timestamp":"1653707640.0","poster":"Bik000","upvote_count":"1","content":"Selected Answer: B\nAnswer is B"},{"timestamp":"1651044360.0","upvote_count":"1","poster":"MWL","comment_id":"592947","content":"Selected Answer: B\nB - KEY - both tables are huge and have common key ."},{"upvote_count":"2","timestamp":"1650378060.0","poster":"MWL","comment_id":"588227","content":"Selected Answer: B\nChoose B based on lakeswimmer 's comment."},{"comment_id":"581684","upvote_count":"1","timestamp":"1649230740.0","content":"B is correct because there is common column between table to join and query","poster":"sbxme"},{"poster":"pidkiller","timestamp":"1648323720.0","upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html\n\nDistribute the fact table and one dimension table on their common columns.\n\nYour fact table can have only one distribution key. Any tables that join on another key aren't collocated with the fact table. Choose one dimension to collocate based on how frequently it is joined and the size of the joining rows. Designate both the dimension table's primary key and the fact table's corresponding foreign key as the DISTKEY.","comment_id":"575745"},{"comment_id":"575743","poster":"pidkiller","content":"The question is not clear enough.\nIt really depends on the queries run and the size of EACH table.\n\nI would answer B assuming that when you query for a product, you might want to see all transactions of this product. All product is a dimension, it could still be a really large table to have an ALL distribution. Maybe if both tables are partitioned by the sku, performance would be better.","timestamp":"1648323540.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1647618300.0","comment_id":"570628","poster":"moon2351","content":"Selected Answer: C\nI think Answer is C"},{"comment_id":"557182","upvote_count":"2","poster":"simo40010","comments":[{"upvote_count":"1","timestamp":"1654252020.0","content":"I understand this reasoning. I think you do mean that the Products Table is a Facts table and therefore the distribution style for this should be ALL. However, it is not mentioned that this is the case and it is also not mentioned that the Products Table is small. \n\nI would also assume that the transactions table is much larger, but it is unclear.","poster":"Ramshizzle","comment_id":"611028"}],"content":"Selected Answer: C\nC: because the products table is a dimension table and should be present in all the cluster nodes to minimize data movements between the nodes when joining the two tables . The transactions table is a fact table and should be distributed by hash key or using the round robin algorithm (both the tables together have 100 GB size , not each of them. My guess is that the transactions table is the biggest because it's the fact table)","timestamp":"1645945680.0"},{"content":"the answer should be B\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html","upvote_count":"3","timestamp":"1637815860.0","comment_id":"486444","poster":"Chints01"},{"upvote_count":"2","comment_id":"478354","poster":"aws2019","content":"Option B","timestamp":"1636926360.0"},{"timestamp":"1636915440.0","comment_id":"478288","upvote_count":"2","content":"I think it is B because we can't decide between C & D without knowing the type of queries.","poster":"goutes"},{"content":"Option C : distributing product table in all nodes will help to fast the join queries with transaction table","timestamp":"1636802640.0","comments":[{"content":"ALL distribution\n\nA copy of the entire table is distributed to every node. Where EVEN distribution or KEY distribution place only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in.\n\nALL distribution multiplies the storage required by the number of nodes in the cluster, and so it takes much longer to load, update, or insert data into multiple tables. ALL distribution is appropriate only for relatively slow moving tables; that is, tables that are not updated frequently or extensively. Because the cost of redistributing small tables during a query is low, there isn't a significant benefit to define small dimension tables as DISTSTYLE ALL.\n\nRef: https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html","upvote_count":"1","timestamp":"1647787260.0","comment_id":"571668","poster":"CHRIS12722222"}],"upvote_count":"3","comment_id":"477418","poster":"attaraya"}],"question_id":17,"question_text":"A human resources company maintains a 10-node Amazon Redshift cluster to run analytics queries on the company's data. The Amazon Redshift cluster contains a product table and a transactions table, and both tables have a product_sku column. The tables are over 100 GB in size. The majority of queries run on both tables.\nWhich distribution style should the company use for the two tables to achieve optimal query performance?","timestamp":"2021-11-09 06:28:00","unix_timestamp":1636435680,"topic":"1","answers_community":["B (73%)","C (20%)","7%"],"isMC":true,"exam_id":20,"answer_description":"","choices":{"A":"An EVEN distribution style for both tables","D":"An EVEN distribution style for the product table and an KEY distribution style for the transactions table","C":"An ALL distribution style for the product table and an EVEN distribution style for the transactions table","B":"A KEY distribution style for both tables"},"url":"https://www.examtopics.com/discussions/amazon/view/65691-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_images":[],"answer_images":[],"answer_ET":"B"},{"id":"R5VXNbStjfS9lTtJKSRB","discussion":[{"comment_id":"478307","content":"C - https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html","upvote_count":"12","poster":"Kash12345","timestamp":"1636917240.0"},{"comment_id":"705825","upvote_count":"5","content":"C is the right answer\nTo exclude Amazon S3 storage classes while creating a dynamic frame, use excludeStorageClasses in additionalOptions. AWS Glue automatically uses its own Amazon S3 Lister implementation to list and exclude files corresponding to the specified storage classes.\nglueContext.create_dynamic_frame.from_catalog(\n database = \"my_database\",\n tableName = \"my_table_name\",\n redshift_tmp_dir = \"\",\n transformation_ctx = \"my_transformation_context\",\n additional_options = {\n \"excludeStorageClasses\" : [\"GLACIER\", \"DEEP_ARCHIVE\"]\n }\n)\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html","poster":"cloudlearnerhere","timestamp":"1666894620.0"},{"content":"C: I passed the test","poster":"pk349","comment_id":"886598","upvote_count":"2","timestamp":"1682965980.0"},{"comment_id":"744664","content":"Selected Answer: C\nSelected C","poster":"rocky48","timestamp":"1670986260.0","upvote_count":"1"},{"upvote_count":"1","poster":"Nubosperta","content":"Selected Answer: C\nSelected C","comment_id":"708715","timestamp":"1667248800.0"},{"upvote_count":"1","comment_id":"650939","poster":"muhsin","timestamp":"1661277840.0","content":"C \n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html"},{"comment_id":"650115","poster":"Dun6","timestamp":"1661147340.0","upvote_count":"1","content":"C please \nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html#aws-glue-programming-etl-storage-classes-table"},{"comment_id":"648379","upvote_count":"1","content":"Selected Answer: C\ndue to https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html","poster":"rudramadhu","timestamp":"1660815960.0"},{"content":"Go with C - due to https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html\nWhy A wrong? - Exclude patterns can be used to ignore the file-paths/folders which are already been crawled before(or files from which data is not required). Since this reduces the number of files that crawlers needs to list everytime, crawler runtime is also reduced accordingly","poster":"rudramadhu","timestamp":"1660020480.0","comment_id":"644345","upvote_count":"1"},{"upvote_count":"1","comment_id":"642959","timestamp":"1659700680.0","poster":"alfredofmt","content":"Selected Answer: B\nA - WRONG, exclude patterns are configured at the crawler level with static basic patterns, they can't be parametrized with the current date. To accomplish this, you would need to re-create the same crawler every day with a new pattern.\n\nB - CORRECT, the only available way to exclude objects to be crawled is to physically moving them to a separate prefix or bucket.\n\nC - WRONG, excludeStorageClasses applies to a Glue ETL job that reads the table, but the table has already been crawled.\n\nD - WRONG, exclude patterns are configured at the crawler level with static basic patterns, they can't be parametrized with the current date. To accomplish this, you would need to re-create the same crawler every day with a new pattern."},{"upvote_count":"2","comment_id":"641805","content":"Answer is A \nIt allows you to \"exclude pattern\" only.\nFor Include, it is talking about \"include path\". \nhttps://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude","poster":"ClementChan","timestamp":"1659527160.0"},{"content":"Guys Please READ. The question is AIMING to CRAWL data not PROCESS DATA with an ETL Process. The correct answer is \"A\". If you want to exclude certain files in your ETL process you might be using the \"C\" answer.","upvote_count":"2","comments":[{"timestamp":"1700556420.0","content":"Answer C:\nThe crawler can access data stores directly as the source of the crawl, or it can use existing tables in the Data Catalog as the source. If the crawler uses existing catalog tables, it crawls the data stores that are specified by those catalog tables.\n\nhttps://docs.aws.amazon.com/glue/latest/dg/define-crawler.html\n\nYou can specify storage class exclusions to be used by an AWS Glue ETL job as a table parameter in the AWS Glue Data Catalog. You can include this parameter in the CreateTable operation using the AWS Command Line Interface (AWS CLI) or programmatically using the API. For more information, see Table Structure and CreateTable. \n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html","comment_id":"1076129","poster":"roymunson","upvote_count":"1"}],"timestamp":"1658487120.0","poster":"carbita","comment_id":"635174"},{"comments":[{"timestamp":"1674830400.0","poster":"gopi_data_guy","content":"@MBP911, We can also add exclude storage class option in Glue catalog table. So C makes sense.\nhttps://docs.amazonaws.cn/en_us/glue/latest/dg/aws-glue-programming-etl-storage-classes.html#aws-glue-programming-etl-storage-classes-table","upvote_count":"1","comment_id":"789689"}],"upvote_count":"2","content":"The link provided talks about \"You can specify storage class exclusions to be used by an ***AWS Glue ETL job ****as a table parameter in the AWS Glue Data Catalog\".\nThe question is talking about how to make the ***Glue CRAWLER*** avoid files in those classes. The fact that the Q mentions a timestamp in the file name suggests there is a filename pattern based timestamp which can be used an include pattern in the crawler config. So shouldn't it be D?","comment_id":"632046","poster":"MBP911","timestamp":"1657957680.0"},{"comments":[{"poster":"lakediver","upvote_count":"4","content":"Agree C\nfor further reading see this \nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-storage-classes.html","comment_id":"506696","timestamp":"1640147340.0"}],"poster":"tobsam","comment_id":"494704","upvote_count":"5","timestamp":"1638741120.0","content":"Answer is C\nOption A is wrong. #Exclude patterns: These enable you to exclude certain files or tables from the crawl."}],"answer_ET":"C","choices":{"A":"Use the exclude patterns feature of AWS Glue to identify the S3 Glacier files for the crawler to exclude.","C":"Use the excludeStorageClasses property in the AWS Glue Data Catalog table to exclude files on S3 Glacier storage.","B":"Schedule an automation job that uses AWS Lambda to move files from the original S3 bucket to a new S3 bucket for S3 Glacier storage.","D":"Use the include patterns feature of AWS Glue to identify the S3 Standard files for the crawler to include."},"answers_community":["C (75%)","B (25%)"],"answer_images":[],"isMC":true,"question_images":[],"exam_id":20,"unix_timestamp":1636917240,"question_text":"A company receives data from its vendor in JSON format with a timestamp in the file name. The vendor uploads the data to an Amazon S3 bucket, and the data is registered into the company's data lake for analysis and reporting. The company has configured an S3 Lifecycle policy to archive all files to S3 Glacier after 5 days.\nThe company wants to ensure that its AWS Glue crawler catalogs data only from S3 Standard storage and ignores the archived files. A data analytics specialist must implement a solution to achieve this goal without changing the current S3 bucket configuration.\nWhich solution meets these requirements?","topic":"1","timestamp":"2021-11-14 20:14:00","question_id":18,"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/66044-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_description":""},{"id":"2lj7Cayj4JWk2niZ2Ti0","discussion":[{"upvote_count":"13","content":"Selected Answer: CE\nlzo format focuses on high compression & decompression speed. So C & E","comments":[{"timestamp":"1711309440.0","content":"The bzip2 and LZO compression formats are splittable, but are not recommended if you want performance and compatibility.","comment_id":"1181940","upvote_count":"1","poster":"jove"}],"poster":"irene7","comment_id":"498372","timestamp":"1639120500.0"},{"poster":"priyashri_13","content":"C & D . As per the link https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/ preferred compression is Gzip . \nFor Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable. When they are not an option, then try BZip2 or Gzip with an optimal file size.","comment_id":"536640","timestamp":"1643598720.0","comments":[{"content":"Agree with C & D. Please refer - https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nFor Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable","timestamp":"1659587460.0","poster":"rudramadhu","comment_id":"642166","upvote_count":"1"}],"upvote_count":"12"},{"content":"Selected Answer: CE\nC & E are correct answers","timestamp":"1700336880.0","comment_id":"1074244","poster":"akarsh17","upvote_count":"1"},{"poster":"LocalHero","upvote_count":"1","content":"Generrally, high compression equal low speed decompression.\ngzip is more high compression than lzo.\nso I choose E.\nC and E are correct I think.","timestamp":"1699617900.0","comment_id":"1067213"},{"comment_id":"1011632","content":"C AND D ARE THE CORRECT ANSWERS","upvote_count":"1","timestamp":"1695149520.0","poster":"SKIRAR"},{"timestamp":"1694658840.0","upvote_count":"1","content":"Selected Answer: CD\nSeems no one mentioned that LZO is splitable on text file while gzip is not. When we use parquet format, we intend to utilize its features that it is splitable, compressible. CSV are file is one example of the text file. So it seems LZO should be chosen over Gzip. Refer to https://aws.amazon.com/cn/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","comments":[{"poster":"zbyroger0902","content":"My bad, I meant CE","timestamp":"1694658900.0","upvote_count":"1","comment_id":"1007115"}],"comment_id":"1007113","poster":"zbyroger0902"},{"poster":"AbNada","content":"C & D is the correct Answer for me","comment_id":"992582","upvote_count":"1","timestamp":"1693249860.0"},{"timestamp":"1691112960.0","upvote_count":"1","content":"Selected Answer: CE\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/building-a-performance-efficient-data-pipeline.html","poster":"whenthan","comment_id":"971599"},{"comment_id":"942213","content":"D and E would not convert the data to a column format, so I am not sure why and how it would help improve the latency, therefore only viable options are B (use Athena to unload parquet files), and C (use glue to convert to parquet and partition the data). \n\nAlso the questions asks to choose two solutions.","timestamp":"1688419020.0","upvote_count":"1","poster":"wally_1995"},{"content":"CE: I passed the test","comment_id":"886601","upvote_count":"1","timestamp":"1682966040.0","poster":"pk349"},{"poster":"rags1482","upvote_count":"2","comment_id":"852631","timestamp":"1679965860.0","content":"GZip is often a good choice for cold data, which is accessed infrequently. Snappy or LZO are a better choice for hot data, which is accessed frequently."},{"content":"Selected Answer: CE\nAnswer should be C and E.","poster":"rocky48","comment_id":"744668","upvote_count":"1","timestamp":"1670986620.0"},{"upvote_count":"6","content":"Correct answers are C & D as AWS recommends using either Parquet or ORC columnar data stores and BZips or Gzip compression. \nFor Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable. When they are not an option, then try BZip2 or Gzip with an optimal file size.\nOption A is wrong as MySQL Workbench does not improve query performance. It instead increases operational overhead.\n\nOption B is wrong as Athena is not ideal for running batch jobs. Use Glue instead.\n\nOption E is wrong as although .lzo is supported and can provide better compression and decompression speeds, Gzip is recommended for Athena as per the AWS documentation.","poster":"cloudlearnerhere","comment_id":"705831","timestamp":"1666895400.0"},{"content":"Selected Answer: CE\nE over D:\nhttps://docs.aws.amazon.com/athena/latest/ug/compression-formats.html\nIt is stated that Lzo compression format has less compression than Gzip, but this makes decompression much faster, which will result in improved query performance and since that's what's needed my answer is E. There is no cost concern in the question so it doesn't matter if files stored in s3 are bigger with Lzo compression.","poster":"Nubosperta","comment_id":"698525","timestamp":"1666128180.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1663211940.0","content":"Selected Answer: CD\ncd as exp[lained by others","poster":"he11ow0rId","comment_id":"669445"},{"upvote_count":"1","comment_id":"605245","timestamp":"1653205200.0","poster":"Bik000","content":"Selected Answer: BC\nAnswer should be B & C"},{"poster":"Shammy45","timestamp":"1652785800.0","comment_id":"602891","upvote_count":"1","content":"Selected Answer: CD\nGZIP is default compression format for Parquet"},{"poster":"MWL","comment_id":"597606","content":"Selected Answer: BC\nB, C. Check VJ_RV 's comment","upvote_count":"1","timestamp":"1651821360.0"},{"upvote_count":"3","poster":"VJ_RV","content":"Unpopular Opinion:\nB&C\nhttps://aws.amazon.com/premiumsupport/knowledge-center/athena-query-output-different-format/","comment_id":"595724","comments":[{"comment_id":"595725","upvote_count":"2","content":"Cost is not an issue here. So running a daily unload job on Athena and storing in Parquet format for later querying makes lot of sense over gzip or lzo formats","poster":"VJ_RV","timestamp":"1651439340.0"}],"timestamp":"1651439280.0"},{"content":"Selected Answer: CD\nhttps://docs.aws.amazon.com/athena/latest/ug/compression-formats.html\nfor D vs E: if compression latency would be an issue (lzo has less than gzip), then why compress at all?\nThe query latency comes from I/O, thus the default gzip with better compression ratio is better (from my understanding)","upvote_count":"5","timestamp":"1651212540.0","comment_id":"594261","poster":"Teraxs"},{"upvote_count":"4","content":"C and E\nThe reason for Option E over D is, GZIP is more suitable for Cold Infrequently accessible Data and SNAPPY and LZO are suitable for warm and frequently accessible data. https://docs.cloudera.com/cloudera-manager/7.4.2/managing-clusters/topics/cm-choosing-configuring-data-compression.html#:~:text=GZIP%20compression%20uses%20more%20CPU,data%2C%20which%20is%20accessed%20frequently.","timestamp":"1638049500.0","comment_id":"488520","poster":"Thiya"},{"poster":"aws2019","upvote_count":"4","comment_id":"482262","content":"C & D ans","timestamp":"1637382660.0"},{"upvote_count":"8","poster":"Olga2022","comment_id":"479456","timestamp":"1637076120.0","content":"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nIt says: For Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable. When they are not an option, then try BZip2 or Gzip with an optimal file size.\nAccording to this I would choose C&D"},{"poster":"attaraya","comment_id":"478510","comments":[{"content":"Question mention which solutions not combination. Since the data is CSV format gzip will better work then LZO ,\nhttps://docs.aws.amazon.com/athena/latest/ug/compression-formats.html","timestamp":"1637563080.0","comment_id":"483905","upvote_count":"2","poster":"ali98"}],"timestamp":"1636961280.0","content":"Factors to consider\n* Data stored in S3. \n* Daily csv file creation and pushed to S3.\n* Recent subset of data.\n* Options to boost query performance.\nI choose Answer C and Answer E ( over Answer D\n\nhttps://docs.aws.amazon.com/athena/latest/ug/compression-formats.html","upvote_count":"1"},{"comment_id":"478424","poster":"Chints01","content":"Answer should be C and E. Any thoughts?\n\nA - does not make sense to hae the long drawn process for connecting via ODBC/JDBC drivers\nB - we don't use athena to extract data - it is only used to query data\nC - Correct\nD - gzip provides higher compression but will also have higher latency (which is an issue to be resolved in this question)\nE - Correct - lzo provides less compression and hence better latency when querying the data","upvote_count":"5","timestamp":"1636942020.0"},{"comment_id":"477934","upvote_count":"4","poster":"ali98","content":"Answer: C,D \nhttps://docs.aws.amazon.com/athena/latest/ug/compression-formats.html","timestamp":"1636863600.0"}],"unix_timestamp":1636863600,"question_id":19,"question_text":"A company analyzes historical data and needs to query data that is stored in Amazon S3. New data is generated daily as .csv files that are stored in Amazon S3.\nThe company's analysts are using Amazon Athena to perform SQL queries against a recent subset of the overall data. The amount of data that is ingested into\nAmazon S3 has increased substantially over time, and the query latency also has increased.\nWhich solutions could the company implement to improve query performance? (Choose two.)","isMC":true,"answer_description":"","answer_ET":"CE","exam_id":20,"question_images":[],"timestamp":"2021-11-14 05:20:00","answer":"CE","choices":{"C":"Run a daily AWS Glue ETL job to convert the data files to Apache Parquet and to partition the converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis.","A":"Use MySQL Workbench on an Amazon EC2 instance, and connect to Athena by using a JDBC or ODBC connector. Run the query from MySQL Workbench instead of Athena directly.","B":"Use Athena to extract the data and store it in Apache Parquet format on a daily basis. Query the extracted data.","D":"Run a daily AWS Glue ETL job to compress the data files by using the .gzip format. Query the compressed data.","E":"Run a daily AWS Glue ETL job to compress the data files by using the .lzo format. Query the compressed data."},"url":"https://www.examtopics.com/discussions/amazon/view/65984-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1","answers_community":["CE (63%)","CD (30%)","7%"],"answer_images":[]},{"id":"IxPlA5PrdjmolEKRP2yf","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/65752-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"C","question_id":20,"discussion":[{"timestamp":"1638225600.0","comment_id":"490251","upvote_count":"16","poster":"awsmani","content":"Its C - https://docs.aws.amazon.com/athena/latest/ug/encrypting-query-results-stored-in-s3.html\n\nAthena query result location supports CSE-KMS, SSE-KMS or SSE-S3 only not customer provided encryption keys"},{"content":"Its C - https://docs.aws.amazon.com/athena/latest/ug/encrypting-query-results-stored-in-s3.html\n\nAthena query result location supports CSE-KMS, SSE-KMS or SSE-S3 only not customer provided encryption keys","comment_id":"1067231","upvote_count":"1","timestamp":"1699619760.0","poster":"markstudy"},{"content":"C: I passed the test","timestamp":"1682966040.0","poster":"pk349","comment_id":"886602","upvote_count":"1"},{"upvote_count":"1","poster":"Arjun777","content":"Use server-side encryption with AWS KMS managed customer master keys (SSE-KMS CMKs) for the primary dataset. Use server-side encryption with S3 managed encryption keys (SSE-S3) for the other datasets.\n\nUsing SSE-KMS CMKs will allow the data engineer to use custom keys for encrypting the query results for the primary dataset, and AWS KMS provides a way to audit key usage through CloudTrail logs. Using SSE-S3 for other datasets will provide generic encryption. Server-side encryption is preferred over client-side encryption as it reduces the complexity and overhead of key management, and it enables other S3 features such as lifecycle policies, cross-region replication, and so on.","comment_id":"808860","timestamp":"1676409540.0"},{"timestamp":"1666895520.0","poster":"cloudlearnerhere","comment_id":"705832","upvote_count":"3","content":"Correct answer is C as AWS KMS managed customer master keys (SSE-KMS CMKs) can be used for encryption primary dataset as it would provide auditing as well. SSE-S3 is fine for other datasets. \nOption A is wrong as SSE-S3 should not be used for the primary dataset as it does not provide custom keys. \n\nOption B is wrong as Athena does not support server-side encryption with customer-provided encryption keys (SSE-C). \n\nOption D is wrong as generic encryption is fine for other datasets, so SSE-S3 should be fine."},{"poster":"rudramadhu","content":"Answer - C","comment_id":"644716","upvote_count":"1","timestamp":"1660089240.0"},{"poster":"rocky48","upvote_count":"1","content":"Selected Answer: C\nAnswer C","comment_id":"643524","timestamp":"1659818160.0"},{"comment_id":"478359","content":"I reckon it should be B. We need custom keys so that should be SSE-C not KMS CMK. SSE-C events should be logged in cloudtrail for auditing purposes.","upvote_count":"1","poster":"Kash12345","timestamp":"1636927020.0","comments":[{"upvote_count":"1","timestamp":"1637202120.0","comment_id":"480390","poster":"ali98","content":"Can't use customer-provided encryption keys (SSE-C) for Ahena"}]},{"timestamp":"1636536660.0","content":"Answer is C > Use server-side encryption with AWS KMS managed customer master keys (SSE-KMS CMKs) for the primary dataset. Use server-side encryption with S3 managed encryption keys (SSE-S3) for the other datasets...Auditing of the encryptions are one of the requirement for primary data sets ,hence KMS for primary and SSE-S3 for rest","comments":[{"timestamp":"1636863300.0","comment_id":"477933","content":"Agree - Answer C","poster":"ali98","upvote_count":"1"}],"upvote_count":"4","comment_id":"475304","poster":"Fazil_Cp"}],"isMC":true,"unix_timestamp":1636536660,"answer_ET":"A","timestamp":"2021-11-10 10:31:00","exam_id":20,"question_images":[],"topic":"1","choices":{"C":"Use server-side encryption with AWS KMS managed customer master keys (SSE-KMS CMKs) for the primary dataset. Use server-side encryption with S3 managed encryption keys (SSE-S3) for the other datasets.","D":"Use client-side encryption with AWS Key Management Service (AWS KMS) customer managed keys for the primary dataset. Use S3 client-side encryption with client-side keys for the other datasets.","B":"Use server-side encryption with customer-provided encryption keys (SSE-C) for the primary dataset. Use server-side encryption with S3 managed encryption keys (SSE-S3) for the other datasets.","A":"Use server-side encryption with S3 managed encryption keys (SSE-S3) for the primary dataset. Use SSE-S3 for the other datasets."},"answer_description":"Reference:\nhttps://d1.awsstatic.com/product-marketing/S3/Amazon_S3_Security_eBook_2020.pdf","answer_images":[],"question_text":"A company is sending historical datasets to Amazon S3 for storage. A data engineer at the company wants to make these datasets available for analysis using\nAmazon Athena. The engineer also wants to encrypt the Athena query results in an S3 results location by using AWS solutions for encryption. The requirements for encrypting the query results are as follows:\n✑ Use custom keys for encryption of the primary dataset query results.\n✑ Use generic encryption for all other query results.\n✑ Provide an audit trail for the primary dataset queries that shows when the keys were used and by whom.\nWhich solution meets these requirements?"}],"exam":{"isMCOnly":true,"name":"AWS Certified Data Analytics - Specialty","provider":"Amazon","id":20,"lastUpdated":"11 Apr 2025","numberOfQuestions":164,"isImplemented":true,"isBeta":false},"currentPage":4},"__N_SSP":true}