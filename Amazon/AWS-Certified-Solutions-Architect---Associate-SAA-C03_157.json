{"pageProps":{"questions":[{"id":"9Ag1fTO3JVCSRJfF4dXh","topic":"1","answers_community":["C (100%)"],"answer_description":"","discussion":[{"timestamp":"1709571120.0","poster":"[Removed]","content":"Selected Answer: C\ndata preparation = Glue DataBrew https://docs.aws.amazon.com/databrew/latest/dg/what-is.html\nstate handling = DataBrew with Step Functions https://docs.aws.amazon.com/step-functions/latest/dg/connect-databrew.html","upvote_count":"12","comment_id":"1165815"},{"poster":"Andy_09","content":"Option C","timestamp":"1707203100.0","comment_id":"1141873","upvote_count":"9"},{"upvote_count":"1","content":"Selected Answer: C\nBoth c and d are workable, but AWS Data Pipeline is considered outdated because AWS has introduced newer, more efficient, and serverless alternatives for data orchestration and ETL (Extract, Transform, Load) tasks. While AWS Data Pipeline is still available, it is not actively promoted or updated by AWS, and many companies have shifted to AWS Glue, Step Functions, and EventBridge for data workflow automation.\n\nhttps://aws.amazon.com/blogs/big-data/build-event-driven-data-quality-pipelines-with-aws-glue-databrew/","comment_id":"1349700","poster":"FlyingHawk","timestamp":"1738364220.0"},{"comment_id":"1331298","timestamp":"1735088700.0","upvote_count":"2","poster":"LeonSauveterre","content":"Selected Answer: C\nA - Lambda function alone is incompetent for retries or running in a specific order.\nB - Athena cannot automatically handle errors or running in a specific order.\nC - YES and fully managed. See below.\nD - Data Pipeline with retries and error handling is A LOT OF WORK.\nTakeaway:\n1. Glue DataBrew simplifies the data preparation process with a visual interface, and it integrates with S3 for input/output.\n2. Step Functions efficiently manages parallel and sequential workflows while minimizing operational overhead for state management, error handling, and retries."},{"poster":"Scheldon","comment_id":"1235252","timestamp":"1719042900.0","content":"Selected Answer: C\nAnswerC\n\nWith Step Functions' built-in controls, you can examine the state of each step in your workflow to make sure that your application runs in order and as expected. \n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\n\nAWS Glue is a serverless data integration service that makes it easy for analytics users to discover, prepare, move, and integrate data from multiple sources.\n\nhttps://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html","upvote_count":"3"},{"comment_id":"1166738","upvote_count":"3","content":"Selected Answer: C\nc looks correct","poster":"asdfcdsxdfc","timestamp":"1709671140.0"}],"answer_ET":"C","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/133019-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"B":"Use Amazon Athena to process the data. Use Amazon EventBridge Scheduler to invoke Athena on a regular internal.","A":"Use an AWS Lambda function to process the data as soon as the data is uploaded to the S3 bucket. Invoke other Lambda functions at regularly scheduled intervals.","C":"Use AWS Glue DataBrew to process the data. Use an AWS Step Functions state machine to run the DataBrew data preparation jobs.","D":"Use AWS Data Pipeline to process the data. Schedule Data Pipeline to process the data once at midnight."},"timestamp":"2024-02-06 08:05:00","answer":"C","isMC":true,"answer_images":[],"unix_timestamp":1707203100,"question_text":"A company's marketing data is uploaded from multiple sources to an Amazon S3 bucket. A series of data preparation jobs aggregate the data for reporting. The data preparation jobs need to run at regular intervals in parallel. A few jobs need to run in a specific order later.\n\nThe company wants to remove the operational overhead of job error handling, retry logic, and state management.\n\nWhich solution will meet these requirements?","question_id":781,"exam_id":31},{"id":"YyWiIVtyrlQxLznDX7QU","answers_community":["C (82%)","D (18%)"],"timestamp":"2024-02-06 08:08:00","url":"https://www.examtopics.com/discussions/amazon/view/133021-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C","answer_ET":"C","question_images":[],"unix_timestamp":1707203280,"discussion":[{"upvote_count":"17","poster":"hajra313","content":"Standard queues provide at-least-once delivery, which means that each message is delivered at least once.\n\nFIFO queues provide exactly-once processing , which means that each message is delivered once and remains available until a consumer processes it and deletes it. Duplicates are not introduced into the queue. OPTION C","comment_id":"1143250","timestamp":"1707302760.0"},{"upvote_count":"3","poster":"Scheldon","timestamp":"1719043200.0","content":"Selected Answer: C\nAnswerC\n\nSQS FIFO was created for such tasks\n\nUnlike standard queues, FIFO queues don't introduce duplicate messages. FIFO queues help you avoid sending duplicates to a queue. If you retry the SendMessage action within the 5-minute deduplication interval, Amazon SQS doesn't introduce any duplicates into the queue.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html","comment_id":"1235254"},{"upvote_count":"3","content":"Selected Answer: C\nC over D, because\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html Processing dynamo streams with lambda can cause duplication.\nSQS FIFO can be configured for High Throughput to exceed the 3000/s (batched) limit https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/high-throughput-fifo.html\n\nI previously worked with payments and would argue that either option doesn't fully solve duplications. Events might be sent multiple times from source, you definitely want to perform de-duplication and have some sort of idempotent processing for them, instead of just blindly processing each thing you're given.","timestamp":"1710236400.0","poster":"escalibran","comment_id":"1171579"},{"upvote_count":"2","timestamp":"1709671680.0","poster":"asdfcdsxdfc","content":"Selected Answer: C\nc is correct","comment_id":"1166742"},{"content":"Option C:\nFIFO queues\nExactly-Once Processing – A message is delivered once and remains available until a consumer processes and deletes it. Duplicates aren't introduced into the queue.\nFirst-In-First-Out Delivery – The order in which messages are sent and received is strictly preserved.","comment_id":"1162899","upvote_count":"2","timestamp":"1709230860.0","poster":"shahreh1"},{"timestamp":"1708550760.0","upvote_count":"3","content":"Selected Answer: C\nOption C Fifo","poster":"FZA24","comment_id":"1155868"},{"timestamp":"1708378080.0","upvote_count":"2","content":"SQS can have duplicate messages in case of problems with the timeout window.","poster":"Mikado211","comment_id":"1154306"},{"content":"Selected Answer: C\n\"The application does not process duplicate payments\" is the key point, which leads us directly to SQS FIFO","comment_id":"1153549","upvote_count":"3","timestamp":"1708287360.0","poster":"haci"},{"upvote_count":"3","comment_id":"1146883","poster":"Cali182","timestamp":"1707608280.0","comments":[{"upvote_count":"1","comment_id":"1152340","poster":"jaswantn","content":"Option D...If you need to handle millions of transactions each day, you might need to consider other approach instead of SQS FIFO. And amongst the given options, we have DynmamoDB that maintains order in the streams.","comments":[{"upvote_count":"3","content":"I'm not sure if the answer is DynamoDB as well, but answering your question, SQS Fifo can handle 300 messages/second without batching, 3,000 messages/second with batching. Assuming we're using the 300/sec option, with 86,400 seconds in a day, that gives you 25,920,000 messages, so in short, yes SQS can handle millions of requests each day.\nNot to mention DynamoDB doesn't provide the exactly-once processing the SQS offer and clearly requested in the question. That's just my train of thought, I'm happy to be corrected.","comments":[{"poster":"jaswantn","timestamp":"1708829880.0","upvote_count":"1","comment_id":"1158309","content":"Dynamodb streams with partition key can be used to implement exactly once processing. There are many options with dynamodb to check for already processed item, and can be filtered out so that they are processed only once."},{"content":"This calculation limits the number of transactions to 25 million a day. What if there are transactions exceeding this limit? As question say .... millions of transactions a day; that could be 70,80 or 90 millions also. In that case how SQS FIFO would perform?\nHappy to be corrected with more convincing facts","upvote_count":"1","timestamp":"1708829100.0","comment_id":"1158300","poster":"jaswantn"}],"comment_id":"1153724","poster":"NayeraB","timestamp":"1708320120.0"}],"timestamp":"1708137960.0"}],"content":"Selected Answer: D\nOption D\nDynamoDB Streams helps ensure the following:\n\nEach stream record appears exactly once in the stream.\n\nFor each item that is modified in a DynamoDB table, the stream records appear in the same sequence as the actual modifications to the item.\n\nDynamoDB Streams writes stream records in near-real time so that you can build applications that consume these streams and take action based on the contents."},{"poster":"kempes","timestamp":"1707320280.0","content":"Option c","comment_id":"1143475","upvote_count":"2"},{"comment_id":"1141875","poster":"Andy_09","content":"Option B","timestamp":"1707203280.0","upvote_count":"1"}],"isMC":true,"answer_images":[],"question_id":782,"question_text":"A solutions architect is designing a payment processing application that runs on AWS Lambda in private subnets across multiple Availability Zones. The application uses multiple Lambda functions and processes millions of transactions each day.\n\nThe architecture must ensure that the application does not process duplicate payments.\n\nWhich solution will meet these requirements?","choices":{"A":"Use Lambda to retrieve all due payments. Publish the due payments to an Amazon S3 bucket. Configure the S3 bucket with an event notification to invoke another Lambda function to process the due payments.","B":"Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple Queue Service (Amazon SQS) queue. Configure another Lambda function to poll the SQS queue and to process the due payments.","D":"Use Lambda to retrieve all due payments. Store the due payments in an Amazon DynamoDB table. Configure streams on the DynamoDB table to invoke another Lambda function to process the due payments.","C":"Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Configure another Lambda function to poll the FIFO queue and to process the due payments."},"answer_description":"","exam_id":31,"topic":"1"},{"id":"Dgq3H0he5UdQ7vnfxala","timestamp":"2024-02-06 08:09:00","question_text":"A company runs multiple workloads in its on-premises data center. The company's data center cannot scale fast enough to meet the company's expanding business needs. The company wants to collect usage and configuration data about the on-premises servers and workloads to plan a migration to AWS.\n\nWhich solution will meet these requirements?","question_images":[],"unix_timestamp":1707203340,"answer":"B","answers_community":["B (100%)"],"choices":{"A":"Set the home AWS Region in AWS Migration Hub. Use AWS Systems Manager to collect data about the on-premises servers.","B":"Set the home AWS Region in AWS Migration Hub. Use AWS Application Discovery Service to collect data about the on-premises servers.","C":"Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS Trusted Advisor to collect data about the on-premises servers.","D":"Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS Database Migration Service (AWS DMS) to collect data about the on-premises servers."},"discussion":[{"timestamp":"1710924900.0","content":"Still the planning stage, C and D is out.","poster":"Kezuko","comment_id":"1178054","upvote_count":"5"},{"timestamp":"1719043380.0","upvote_count":"4","poster":"Scheldon","content":"Selected Answer: B\nAnswerB\nAWS Migration Hub delivers a guided end-to-end migration and modernization journey through discovery, assessment, planning, and execution. \nhttps://aws.amazon.com/migration-hub/\nAWS Application Discovery Service helps you plan your migration to the AWS cloud by collecting usage and configuration data about your on-premises servers and databases. Application Discovery Service is integrated with AWS Migration Hub and AWS Database Migration Service Fleet Advisor. \nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html","comment_id":"1235257"},{"comments":[{"comment_id":"1175714","timestamp":"1710668820.0","poster":"Ipergorta","upvote_count":"3","content":"Sorry B"}],"upvote_count":"1","content":"Option D","timestamp":"1710668760.0","comment_id":"1175711","poster":"Ipergorta"},{"timestamp":"1709671860.0","comment_id":"1166745","content":"Selected Answer: B\nB is correct","poster":"asdfcdsxdfc","upvote_count":"2"},{"comment_id":"1165818","timestamp":"1709571360.0","upvote_count":"4","content":"Selected Answer: B\nAWS Application Discovery Service helps you plan your migration to the AWS cloud by collecting usage and configuration data about your on-premises servers and databases. https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html","poster":"[Removed]"},{"upvote_count":"4","timestamp":"1707203340.0","comment_id":"1141877","poster":"Andy_09","content":"Option B"}],"topic":"1","exam_id":31,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/133022-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","question_id":783,"answer_ET":"B","isMC":true},{"id":"DG5HasE7aTslsxzSfovK","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/133023-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"answer_images":[],"answers_community":["A (100%)"],"answer":"A","choices":{"A":"Deploy an AWS Control Tower environment in the Organizations management account. Enable AWS Security Hub and AWS Control Tower Account Factory in the environment.","C":"Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ). Submit an RFC to self-service provision Amazon GuardDuty in the MALZ.","D":"Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ). Submit an RFC to self-service provision AWS Security Hub in the MALZ.","B":"Deploy an AWS Control Tower environment in a dedicated Organizations member account. Enable AWS Security Hub and AWS Control Tower Account Factory in the environment."},"answer_description":"","question_images":[],"unix_timestamp":1707203460,"exam_id":31,"discussion":[{"comment_id":"1350728","content":"Selected Answer: A\nThis question was in my today's exam.","timestamp":"1738553220.0","upvote_count":"2","poster":"FlyingHawk"},{"content":"Selected Answer: A\nA - Control Tower enforces centralized logging, monitoring, and account management. Security Hub monitors compliance with AWS Foundational Security Best Practices (FSBP). Account Factory simplifies the creation of new accounts with preconfigured settings.\nB - Control Tower is intended to be deployed in the management account, not a member account, which deviates from AWS-recommended practices.\nC - GuardDuty focuses on security threats rather than compliance monitoring for FSBP.\nD - AMS Accelerate is more suitable for enterprises with highly complex requirements. Also, more operational overheads and cost more, but the question wants \"to prevent additional work and to minimize costs\".","comment_id":"1331300","upvote_count":"1","timestamp":"1735089780.0","poster":"LeonSauveterre"},{"content":"Why not B?","poster":"LuongTo","upvote_count":"1","timestamp":"1730089080.0","comment_id":"1303816"},{"poster":"Kezuko","timestamp":"1710925020.0","comment_id":"1178056","upvote_count":"4","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/controltower/latest/userguide/security-hub-controls.html"},{"timestamp":"1710668820.0","comment_id":"1175712","upvote_count":"1","poster":"Ipergorta","content":"Option D"},{"content":"Selected Answer: A\nA is correct","poster":"asdfcdsxdfc","comment_id":"1166747","upvote_count":"2","timestamp":"1709671980.0"},{"content":"Selected Answer: A\nOption A","timestamp":"1707324840.0","upvote_count":"4","comment_id":"1143536","poster":"kempes"},{"content":"Option A","poster":"Andy_09","timestamp":"1707203460.0","upvote_count":"2","comment_id":"1141879"}],"question_text":"A company has an organization in AWS Organizations that has all features enabled. The company requires that all API calls and logins in any existing or new AWS account must be audited. The company needs a managed solution to prevent additional work and to minimize costs. The company also needs to know when any AWS account is not compliant with the AWS Foundational Security Best Practices (FSBP) standard.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","topic":"1","question_id":784,"timestamp":"2024-02-06 08:11:00"},{"id":"IzRgYs5lWxL0IHK2euq0","answer":"C","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/133024-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"timestamp":"2024-02-06 08:11:00","question_text":"A company has stored 10 TB of log files in Apache Parquet format in an Amazon S3 bucket. The company occasionally needs to use SQL to analyze the log files.\n\nWhich solution will meet these requirements MOST cost-effectively?","topic":"1","question_images":[],"unix_timestamp":1707203460,"answers_community":["C (100%)"],"discussion":[{"poster":"LeonSauveterre","content":"Selected Answer: C\nA - Aurora is cool but migrating 10 TB of data incurs significant costs and operational overhead.\nB - Redshift Spectrum allows querying data directly in S3 without loading it into Redshift, but costs are really high especially for infrequent use.\nC - Athena is serverless and charges only for the data scanned by queries. Glue Crawler automatically extracts metadata and schema information from the Parquet files. No need to migrate anything.\nD - Just by the look of it I know I'll go bankrupt if I choose that.","timestamp":"1735090740.0","upvote_count":"1","comment_id":"1331310"},{"timestamp":"1730028540.0","upvote_count":"2","content":"Selected Answer: C\nS3 + SQL = Athena","poster":"sandordini","comment_id":"1203047"},{"content":"Selected Answer: C\nApache Parquet => Glue Crawler","poster":"Kezuko","timestamp":"1726815480.0","upvote_count":"3","comment_id":"1178057"},{"content":"Selected Answer: C\nc is correct","comment_id":"1166748","timestamp":"1725562500.0","poster":"asdfcdsxdfc","upvote_count":"2"},{"content":"Selected Answer: C\nOption C","upvote_count":"4","comment_id":"1143687","poster":"kempes","timestamp":"1723049580.0"},{"comment_id":"1141880","upvote_count":"4","timestamp":"1722921060.0","poster":"Andy_09","content":"Option C"}],"choices":{"C":"Create an AWS Glue crawler to store and retrieve table metadata from the S3 bucket. Use Amazon Athena to run SQL statements directly on the data in the S3 bucket.","A":"Create an Amazon Aurora MySQL database. Migrate the data from the S3 bucket into Aurora by using AWS Database Migration Service (AWS DMS). Issue SQL statements to the Aurora database.","B":"Create an Amazon Redshift cluster. Use Redshift Spectrum to run SQL statements directly on the data in the S3 bucket.","D":"Create an Amazon EMR cluster. Use Apache Spark SQL to run SQL statements directly on the data in the S3 bucket."},"exam_id":31,"question_id":785,"answer_images":[],"answer_ET":"C"}],"exam":{"isBeta":false,"id":31,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isMCOnly":true,"isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025"},"currentPage":157},"__N_SSP":true}