{"pageProps":{"questions":[{"id":"mWkA9u6szc730DKqrmFE","unix_timestamp":1676379720,"topic":"1","answer":"A","discussion":[{"poster":"God_Is_Love","upvote_count":"13","timestamp":"1694915820.0","comment_id":"841559","content":"Selected Answer: A\nAmazon Connect is a cloud-based contact center service that allows you to set up a virtual call center for your business. It provides an easy-to-use interface for managing customer interactions through voice and chat. Amazon Connect integrates with other AWS services, such as Amazon S3 and Amazon Kinesis, to help you collect, store, and analyze customer data for insights into customer behavior and trends.\n\nOn the other hand, Amazon Pinpoint is a marketing automation and analytics service that allows you to engage with your customers across different channels, such as email, SMS, push notifications, and voice. It helps you create personalized campaigns based on user behavior and enables you to track user engagement and retention.\n\nWhile both services allow you to communicate with your customers, they serve different purposes. Amazon Connect is focused on customer support and service, while Amazon Pinpoint is focused on marketing and engagement."},{"poster":"alexsanteeno","timestamp":"1718897820.0","upvote_count":"2","content":"\"LEAST OPERATIONAL OVERHEAD\" - is key word in a question. Its not so easy to migrate any on-premise infra to any AWS. Looking at the answers here I see no one eve done that before and just answering as from AWS docs.\nThe easiest way to migrate any on-premise infra - ec2","comment_id":"1101798"},{"upvote_count":"1","poster":"career360guru","content":"Selected Answer: A\nOption A","comment_id":"1079780","timestamp":"1716611700.0"},{"poster":"rrrrrrrrrr1","timestamp":"1705073280.0","content":"Why not b though? SNS is easy as heck to use.","comments":[{"comment_id":"1160174","timestamp":"1724713140.0","upvote_count":"5","poster":"VerRi","content":"\"managed, interactive, two-way experience\" means a personalised and customised message, so it should be Pinpoint here."},{"comment_id":"949836","content":"nvm text message surveys are probably a pinpoint thing. I was thinking like a link to a survey.","upvote_count":"3","poster":"rrrrrrrrrr1","timestamp":"1705073280.0"}],"comment_id":"949834","upvote_count":"1"},{"poster":"NikkyDicky","comment_id":"943139","content":"Selected Answer: A\nA - basic AWS connect use case","upvote_count":"1","timestamp":"1704405900.0"},{"poster":"Maria2023","timestamp":"1703402460.0","comment_id":"932195","content":"Selected Answer: A\nAmazon connect + Pinpoint are the best choice here","upvote_count":"1"},{"timestamp":"1701192660.0","comment_id":"908715","poster":"Roontha","upvote_count":"1","content":"Answer: A"},{"comment_id":"852107","upvote_count":"1","content":"Selected Answer: A\nUse Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.","poster":"mfsec","timestamp":"1695818820.0"},{"timestamp":"1692110880.0","upvote_count":"4","content":"Selected Answer: A\nThe solution that will meet the company's requirements with the LEAST ongoing operational overhead and send two-way experience survey is to use Amazon Connect to replace the old call center hardware and use Amazon Pinpoint to send text message surveys to customers. Amazon Connect is a fully managed, cloud-based contact center service that is easy to set up and configure, while Amazon Pinpoint can be used to send text message surveys and gather responses. By using these services, the company can offload the operational overhead of running and maintaining the call center hardware and survey system to AWS.","poster":"c73bf38","comment_id":"809757"},{"upvote_count":"2","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-two-way.html","timestamp":"1692010920.0","poster":"spd","comment_id":"808394"}],"answer_ET":"A","choices":{"C":"Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers.","B":"Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.","A":"Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.","D":"Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers."},"isMC":true,"timestamp":"2023-02-14 14:02:00","answers_community":["A (100%)"],"question_text":"A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.\n\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?","answer_images":[],"question_images":[],"exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/99155-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":86,"answer_description":""},{"id":"Cs40GhL5ABUoowUIWEpm","answer_ET":"D","question_text":"A company is building a call center by using Amazon Connect. The company’s operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.\n\nWhich solution will provide DR with the LOWEST RTO?","answers_community":["D (79%)","13%","8%"],"topic":"1","answer":"D","unix_timestamp":1676479260,"answer_description":"","exam_id":33,"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/99304-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-02-15 17:41:00","answer_images":[],"question_id":87,"choices":{"C":"Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function.","D":"Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function.","A":"Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template.","B":"Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region."},"discussion":[{"upvote_count":"10","poster":"nyxs_19","content":"Selected Answer: D\nThe solution that will provide DR with the LOWEST RTO (Recovery Time Objective) is option D.\n\nOption D provisions a new Amazon Connect instance with all existing users and contact flows in a second Region. It also sets up an Amazon Route 53 health check for the URL of the Amazon Connect instance, an Amazon CloudWatch alarm for failed health checks, and an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. This option allows for the fastest recovery time because all the necessary components are already provisioned and ready to go in the second Region. In the event of a disaster, the failed health check will trigger the AWS Lambda function to deploy the CloudFormation template to provision the claimed phone numbers, which is the only missing component.","comment_id":"811431","timestamp":"1676608920.0"},{"poster":"spd","content":"Selected Answer: D\nD looks most appropriate","upvote_count":"9","timestamp":"1676838720.0","comment_id":"814510"},{"timestamp":"1742397900.0","comment_id":"1400569","poster":"29fb203","content":"Selected Answer: C\nC accounts for the phone numbers and all other resources. D doesn't","upvote_count":"1"},{"timestamp":"1729378320.0","poster":"Sin_Dan","comment_id":"1300197","content":"Selected Answer: C\nSetting up phone numbers is more complex and time consuming, than setting up users. Option D waits until the disaster happens to provision the phone numbers. Option C is right, because it is quicker as compared to option D. Also, it makes sure the users are not duplicated upfront.","upvote_count":"1"},{"upvote_count":"1","comment_id":"1299832","timestamp":"1729293780.0","poster":"cashyc","content":"Selected Answer: C\nby pre-provisioning a new Amazon Connect instance in a second AWS Region with the necessary contact flows and phone numbers already in place. The remaining task at the time of disaster recovery is to deploy the users, which can be done using an AWS Lambda function triggered by a CloudWatch alarm when the primary instance becomes unavailable, as determined by a Route 53 health check."},{"poster":"marszalekm","comment_id":"1116950","content":"Amazon Connect is not on the list of services required for this exam. At least as of 08.01.24 https://d1.awsstatic.com/training-and-certification/docs-sa-pro/AWS-Certified-Solutions-Architect-Professional_Exam-Guide.pdf","timestamp":"1704742800.0","upvote_count":"5"},{"timestamp":"1700894280.0","poster":"career360guru","upvote_count":"1","content":"Selected Answer: D\nOption D","comment_id":"1079781"},{"upvote_count":"1","poster":"severlight","content":"Selected Answer: D\nAmazon Connect gives you a URL, for which you can add a record in route 53 and hence have a health check.","comment_id":"1073213","timestamp":"1700217900.0"},{"content":"Selected Answer: D\nD seems to fit all requirements, however C & D seem to be very similar. Only difference is whether to upload users or phone numbers through Cloud Formation. It seems users, routing profiles, queues, and flows get created with ReplicateInstance API\nhttps://docs.aws.amazon.com/connect/latest/adminguide/create-replica-connect-instance.html","poster":"SK_Tyagi","comment_id":"985915","timestamp":"1692546540.0","upvote_count":"3"},{"content":"Selected Answer: B\nApparently Route 53 can't manage Amazon Connect DNS names or health checks.\nhttps://docs.aws.amazon.com/connect/latest/adminguide/update-your-connect-domain.html#new-domain-custom","timestamp":"1690688700.0","upvote_count":"1","comment_id":"966825","poster":"MRL110"},{"poster":"NikkyDicky","timestamp":"1688501580.0","content":"Selected Answer: D\nD i guess","comment_id":"943142","upvote_count":"1"},{"poster":"Maria2023","content":"Selected Answer: B\nI vote for B since I was not able to find a way to make Route53 serve the Amazon connect URL and therefore it cannot perform healthcheck. If someone has more information on this - please share","comment_id":"932212","timestamp":"1687585500.0","upvote_count":"1"},{"content":"why not letter C \n\"CloudFormation template that provisions all users\" insted of \"CloudFormation template that provisions claimed phone numbers\" of letter D","timestamp":"1684963380.0","upvote_count":"3","poster":"SkyZeroZx","comment_id":"906161"},{"upvote_count":"1","content":"Selected Answer: B\nI'm voting B because i don't think it's possible to use Amazon Route 53 health check to verify the availability of Amazon Connect","poster":"dev112233xx","comment_id":"866360","timestamp":"1681138320.0"},{"timestamp":"1679954940.0","poster":"Eshu2009","upvote_count":"1","comment_id":"852518","content":"why not C?","comments":[{"comments":[{"comment_id":"1355215","upvote_count":"1","timestamp":"1739313960.0","content":"Same thinking i had","poster":"shmoeee"}],"poster":"ninomfr64","upvote_count":"3","timestamp":"1706452020.0","comment_id":"1134153","content":"I think, but I was not able to very it, that if your instance is active and you have phone numbers configured it is receiving actual phone traffic that is a and Active/Active scenario, however you do not have users (aka Agents) configured to handle calls. This is just me guessing"}]},{"timestamp":"1679921340.0","content":"Selected Answer: D\nD. Provision a new Amazon Connect instance with all existing users and contact flows in a second Region.","upvote_count":"3","comment_id":"852109","poster":"mfsec"},{"comment_id":"812448","upvote_count":"3","content":"Selected Answer: D\nD is the better solution.","timestamp":"1676672580.0","poster":"c73bf38"},{"upvote_count":"2","content":"Selected Answer: B\nB. Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region will provide disaster recovery with the LOWEST Recovery Time Objective.","comment_id":"809749","comments":[{"poster":"c73bf38","content":"Thanks for pointing that out, D is the better solution.","timestamp":"1676672520.0","upvote_count":"2","comment_id":"812447"},{"poster":"Musk","content":"With D you can have a quicker reaction if you use high-resolution CloudWatch alarms that alert as soon as 10-second or 30-second periods. Additionally, contact flows are alredy there so you don't need to deploy when the error occurs.","comment_id":"810994","timestamp":"1676571240.0","upvote_count":"5"}],"poster":"c73bf38","timestamp":"1676479260.0"}]},{"id":"Hnmkfz5UTNQ5Me74ycGd","answers_community":["B (91%)","9%"],"url":"https://www.examtopics.com/discussions/amazon/view/107088-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"topic":"1","choices":{"B":"In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require the data customers to subscribe to the data product.","C":"Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product.","D":"Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS Data Exchange. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts.","A":"Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshift. Require the data customers to subscribe to the data product."},"timestamp":"2023-04-23 07:08:00","question_images":[],"answer_ET":"B","question_text":"A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to perform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift tables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits the files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has become difficult.\n\nThe company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","isMC":true,"answer_images":[],"question_id":88,"answer_description":"","answer":"B","discussion":[{"timestamp":"1714099380.0","content":"Selected Answer: B\nThe company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data. With B, customer can get data from Redshift directly with no time lag and additional operations.","comment_id":"881048","poster":"youngmanaws","upvote_count":"10"},{"poster":"renegadedme","timestamp":"1714039080.0","content":"Selected Answer: B\nI think it's B.\nAccording to https://aws.amazon.com/data-exchange/why-aws-data-exchange/redshift-data-tables/\n\nCustomers can find and subscribe to third-party data in AWS Data Exchange and directly query the data in minutes in Amazon Redshift without extracting, transforming, or loading it. \n\nIn B, customers can query Redshift directly. No need to use S3 periodically. Minimizes operational overhead.","comment_id":"880196","upvote_count":"9"},{"comment_id":"944006","poster":"NikkyDicky","timestamp":"1720200840.0","content":"Selected Answer: B\nit's a B","upvote_count":"1"},{"content":"Selected Answer: B\nKeyword is datashare\nhttps://docs.aws.amazon.com/redshift/latest/dg/adx-getting-started.html","comment_id":"941906","upvote_count":"5","poster":"SmileyCloud","timestamp":"1720015320.0"},{"upvote_count":"3","poster":"easytoo","content":"b-b-b-b-bb-b-b-b-b-b\nLEAST operational overhead...\nOption (A) uses AWS Data Exchange for APIs, which requires you to create an Amazon API Gateway Data API service integration with Amazon Redshift. This is a more complex solution than using a datashare.\n\nOption (C) uses AWS Data Exchange for S3, which requires you to download the data from Amazon Redshift to Amazon S3 periodically. This is also a more complex solution than using a datashare.\n\nOption (D) publishes the data to an Open Data on AWS Data Exchange, which does not allow you to configure subscription verification. This means that anyone can access the data, which is not ideal for a company that wants to protect its proprietary algorithms.","comment_id":"928639","timestamp":"1718904240.0"},{"poster":"TECHNOWARRIOR","timestamp":"1718700540.0","comment_id":"926561","content":"AWS Data Exchange for APIs enables customers to discover and utilize third-party APIs in the cloud, with authentication using AWS IAM credentials and SDKs. It simplifies access permissions and governance. Users can access data APIs from numerous providers. On the other hand, AWS Data Exchange Datashare focuses on licensing access to Amazon Redshift data. It utilizes AWS-native authentication and automatically adds customers as data consumers. With read-only access, customers can retrieve objects from datashares. While both services integrate with AWS, Data Exchange for APIs is geared towards API usage, while Data Exchange Datashare is centered around licensing access to Amazon Redshift data.","upvote_count":"5"},{"content":"Answer : B\nhttps://www.youtube.com/watch?v=BeIoTSql4IM \n(AWS Data Exchange for Amazon Redshift demo | Amazon Web Services)","poster":"Roontha","comment_id":"908705","timestamp":"1716908640.0","upvote_count":"3"},{"timestamp":"1715888520.0","comment_id":"899550","content":"Selected Answer: B\nB is the closest one but is not correct either.\nhttps://docs.amazonaws.cn/en_us/redshift/latest/dg/adx-getting-started-producer.html, like every thing else in AWS you need policy to grant access and that is missing in B.","poster":"Sarutobi","upvote_count":"2"},{"comment_id":"880035","comments":[{"comment_id":"928637","content":"yup! was about to say the same.","comments":[{"upvote_count":"1","comment_id":"1062720","timestamp":"1730796000.0","poster":"yorkicurke","content":"hahahaaha"}],"poster":"easytoo","upvote_count":"6","timestamp":"1718903940.0"}],"upvote_count":"1","poster":"nqg54118","timestamp":"1714026480.0","content":"Selected Answer: C\nデータの顧客数は大幅に増加した対策にS3"},{"poster":"OCHT","upvote_count":"2","timestamp":"1713933180.0","comment_id":"879019","content":"Selected Answer: C\nThe correct answer is C. Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product.\n\nExporting the data to an Amazon S3 bucket periodically ensures that customers have access to the most recent data when the company publishes it.\nAWS Data Exchange for S3 allows you to share data with customers easily and manage their subscriptions.\nSubscription verification helps confirm the identity of customers before sharing data with them.\nThis solution minimizes operational overhead as it leverages AWS Data Exchange and Amazon S3, which are managed services.\nThe unique keywords combination in this option that makes it easier to remember is Amazon S3, AWS Data Exchange, and subscription verification."},{"timestamp":"1713848880.0","poster":"Yowie351","comment_id":"877884","upvote_count":"2","content":"Selected Answer: B\nAnswer is B. https://aws.amazon.com/data-exchange/?adx-cards2.sort-by=item.additionalFields.eventDate&adx-cards2.sort-order=desc"}],"unix_timestamp":1682226480},{"id":"BLa50lBPoOgmICh3gHbP","answer":"A","timestamp":"2023-04-23 07:11:00","choices":{"D":"Publish events to an Amazon EventBndge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group that is behind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event bus to retry events. Write messages to a dead-letter queue if the application cannot process the messages.","A":"Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination to the function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target.","C":"Write events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda function. Configure the Lambda function to process the events.","B":"Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue."},"question_text":"A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of events that the solution receives. If a processing error occurs, the event must move into a separate queue for review.\n\nWhich solution will meet these requirements?","exam_id":33,"answer_images":[],"unix_timestamp":1682226660,"discussion":[{"poster":"Sarutobi","comment_id":"880670","upvote_count":"30","timestamp":"1682442660.0","content":"Selected Answer: B\nI would go with B just because of the wording. I believe A should work just fine, but the question asks for \"scale in and out based on the number of events.\" In my opinion, that is what SNS->Lambda->SQS(DLQ) would do, too; I think the SNS->Lambda scale in/out behavior is more implicit. So I will go with B here because it is more explicit."},{"upvote_count":"11","timestamp":"1700222760.0","comment_id":"1073286","poster":"SuperDuperPooperScooper","content":"Selected Answer: A\nConfiguring scaling based on the age of the oldest message is nowhere near as good as scaling based on size of the Queue for this use case.\n\nage of the oldest message will grow linearly based on time. If there is a dramatic spike in the Queue size due to increased traffic, like 100X increase in size. Then the queue will have grown a lot but the oldest message will only increase in age linearly, so the scaling will not be able to realize how much the workload has increased.","comments":[{"timestamp":"1724651280.0","upvote_count":"1","comment_id":"1272462","content":"just B","poster":"helloworldabc"},{"comment_id":"1261481","content":"there will just be a lag in scaling, but eventually this metric will scale as needed","upvote_count":"1","timestamp":"1722923460.0","poster":"mns0173"},{"upvote_count":"1","content":"makes sense","timestamp":"1701004140.0","poster":"sonyaws","comment_id":"1080653"},{"timestamp":"1701340380.0","content":"very good explanation. Moreover, go serverless as much as possible. EC2 vs Lambda - Lamda is always preferred.","comment_id":"1084243","poster":"jainparag1","upvote_count":"1"}]},{"upvote_count":"1","comment_id":"1362532","content":"Selected Answer: B\nBy utilizing the ApproximateAgeOfOldestMessage metric, you can scale out and scale in based on the workload, ensuring that your application can handle increases in traffic.","timestamp":"1740662340.0","poster":"itsjunukim"},{"content":"Selected Answer: B\n100 % its B, My reasons are: \n\n1. SNS is for pub/sub, not event processing. SNS sends events to multiple subscribers but does not provide queue-based scaling.\n\n2. Lambda also has concurrency limits, which might cause failures at high event rates.","timestamp":"1739385420.0","upvote_count":"1","comment_id":"1355727","poster":"820b83f"},{"poster":"kylix75","timestamp":"1737720960.0","comment_id":"1346103","upvote_count":"1","content":"Selected Answer: B\nThe correct answer is B.\n\nReasons:\n1. SQS + Auto Scaling provides event-based scalability\n2. ApproximateAgeOfOldestMessage metric enables workload-based scaling\n3. SQS native dead-letter queue handles error messages\n4. Most resilient and cost-effective solution for event processing at scale\n\nIssues with other options:\nA: SNS doesn't store messages for reprocessing\nC: DynamoDB Streams has scalability and retention limitations\nD: ALB + EC2 is more complex and expensive than serverless processing"},{"upvote_count":"2","content":"Selected Answer: B\nWhile A would probably work fine most of the time, B is more resilient. Once a message is in the Q, it will either be marked as complete or go to DLQ. In A, in edge cases like lambda exceeding concurrency limit, the message would be throttled after the SNS returns success to the sender.... Without SNS DLQ, the message would be lost.","poster":"ahhatem","timestamp":"1734451080.0","comment_id":"1328015"},{"timestamp":"1730301600.0","comment_id":"1305049","upvote_count":"3","poster":"FZA24","content":"Selected Answer: B\nscale in scale out => ALB\na separate queue => DLQ"},{"timestamp":"1729627860.0","comment_id":"1301707","upvote_count":"2","content":"Selected Answer: A\n• By sending event details to an Amazon SNS topic and configuring an AWS Lambda function as a subscriber, the solution automatically scales with the number of incoming events. \n• Lambda functions scale in and out based on the event load without manual intervention. \n• Adding an on-failure destination to the Lambda function that targets an Amazon SQS queue ensures that any processing errors move the event into a separate queue for review. \n• This setup meets both the scalability and error-handling requirements efficiently.","poster":"Woody1848"},{"upvote_count":"2","poster":"Sin_Dan","timestamp":"1729378920.0","content":"Selected Answer: B\nOption A uses Lambda to process the solution. However, we don't know if the processing finishes within 15 mins or not. Also, SNS isn’t as well-suited for handling large event queues as SQS, and scaling based on message queue metrics is not supported in this configuration.\nSo, the correct option is definitely B.","comment_id":"1300199"},{"timestamp":"1728721020.0","content":"Selected Answer: B\nOnly B and D mention about reviewing error in a separate queue by dead letter Q, with D never use SQS where this is supported.","comment_id":"1296427","poster":"Daniel76","upvote_count":"2"},{"timestamp":"1723744800.0","content":"Selected Answer: B\nPeople who are choosing A. Have you done associate level certs as this does not make any sense. You shouldnt be attempting this exam if that's how lost you are.","comments":[{"comment_id":"1316260","timestamp":"1732275720.0","content":"chill bro","poster":"amsf96","upvote_count":"4"}],"poster":"Syre","upvote_count":"2","comment_id":"1266600"},{"comments":[{"content":"Dude, check you link again, which says \"ApproximateNumberOfMessages\" not \" ApproximateAgeOfOldestMessage\", so answer will be option A.","comment_id":"1266262","poster":"Linuslin","upvote_count":"3","timestamp":"1723702980.0"}],"comment_id":"1261920","poster":"ChungFTF","content":"Selected Answer: B\nThe Auto Scaling group of EC2 instances can automatically adjust the number of instances based on the ApproximateAgeOfOldestMessage metric. This ensures that the solution scales dynamically with the volume of events, maintaining efficient processing.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","upvote_count":"2","timestamp":"1723000560.0"},{"timestamp":"1721883240.0","upvote_count":"2","comment_id":"1254713","poster":"tsangckl","content":"Selected Answer: B\nB for sure"},{"timestamp":"1720537620.0","upvote_count":"2","poster":"vip2","comment_id":"1244966","content":"Selected Answer: B\nI do think the B is correct way with ApproximateNumberOfMessages instead of ApproximateAgeOfOldestMessage. there is no such metric ...AgeOfOldestMessage"},{"poster":"Bobshaw","timestamp":"1714834800.0","upvote_count":"2","comment_id":"1206531","content":"Selected Answer: B\npublishing events to an SQS queue, creating an EC2 Auto Scaling group that scales based on the queue's ApproximateAgeOfOldestMessage metric, and configuring the application to write failed messages to a dead-letter queue provides a scalable, fault-tolerant, and cost-effective solution for event processing with the ability to handle processing errors separately."},{"content":"Selected Answer: A\nVote for A","upvote_count":"3","timestamp":"1714666020.0","poster":"seetpt","comment_id":"1205698"},{"upvote_count":"2","comment_id":"1173427","content":"Selected Answer: B\nOption B. Question states, \" must move to into a separate queue for review\" Dead-Letter queues give you this capability for debugging or troubleshooting the issue. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","timestamp":"1710422100.0","poster":"TonytheTiger"},{"upvote_count":"6","content":"Selected Answer: A\nA fulfils all objectives.\nIn B, ApproximateAgeOfOldestMessage doesn't translate to a reliable scaling pattern, and EC2s are implied.\nC does not implement a dead-letter queue\nD is overengineered.","comment_id":"1167958","timestamp":"1709813400.0","poster":"Dgix"},{"poster":"TheCloudGuruu","content":"Selected Answer: B\nSQS with DLQ","comment_id":"1154961","upvote_count":"2","timestamp":"1708458180.0"},{"upvote_count":"5","timestamp":"1707702780.0","content":"Selected Answer: A\nApproximateAgeOfOldestMessage metric may not be as responsive as needed, and it doesn't directly address the requirement for handling processing errors by moving events to a separate queue for review.","comment_id":"1147781","poster":"pri32"},{"timestamp":"1706820480.0","content":"Selected Answer: B\nThe question typical AWS thrown words and leaving gaps.. but still going for B","comment_id":"1137912","poster":"AimarLeo","upvote_count":"2"},{"poster":"GoKhe","content":"I would go with A. \nScaling in/out based on message age does not align with what question asks i.e. it should be based on the number of events. So, B is not right here.","comment_id":"1101459","comments":[{"poster":"07c2d2a","comment_id":"1132182","timestamp":"1706230080.0","content":"Scaling based on how long someone is waiting is another way of basing it on the number of events, but I see what you mean. Lambda will scale based on the number 1:1 and B will scale in whatever configuration you want based on time, not number of events specifically.","upvote_count":"1"}],"upvote_count":"4","timestamp":"1703070840.0"},{"poster":"ayadmawla","comment_id":"1095566","upvote_count":"3","timestamp":"1702479420.0","content":"Selected Answer: B\nAnswer is B\nI would go with A, except a Dead Letter Q is not an SQS queue. There are only two types of SQS Queues, namely, Standard and FIFO. \n\nA DLQ is a special message queue (not SQS). See here for confirmation: https://aws.amazon.com/what-is/dead-letter-queue/#:~:text=A%20dead%2Dletter%20queue%20(DLQ)%20is%20a%20special%20type,communication%20in%20a%20distributed%20system."},{"poster":"HappyPrince","comment_id":"1094105","timestamp":"1702354320.0","content":"Selected Answer: A\nI prefer A as the solution is serverless.","upvote_count":"4"},{"upvote_count":"4","timestamp":"1701773940.0","comment_id":"1088402","content":"Selected Answer: A\nI would go for A as the question mentions specifically about scaling based on the 'number of events' and option B goes for age of theoldest message in the queue. Option B does sound deliberate to distract.","comments":[{"poster":"Shenannigan","timestamp":"1718467740.0","comment_id":"1231049","content":"Actually I would go with B based on this\n\nhttps://repost.aws/knowledge-center/sqs-troubleshoot-oldestmessage-metric#:~:text=If%20the%20queue%20has,to%20maintain%20the%20backlog.","upvote_count":"1"}],"poster":"shaaam80"},{"poster":"career360guru","content":"Selected Answer: A\nOption A is better with assumption that SNS will scale in and scale out Lambda depending on number of incoming messages.","comment_id":"1080361","timestamp":"1700949720.0","comments":[{"poster":"career360guru","timestamp":"1703934360.0","comment_id":"1109629","upvote_count":"1","content":"Option B is the right answer."}],"upvote_count":"3"},{"poster":"heatblur","timestamp":"1700862360.0","comment_id":"1079614","content":"Selected Answer: B\nB is the answer --- SQS is the right tool for the job.","upvote_count":"2"},{"comment_id":"1079604","content":"Selected Answer: B\nSQS with DLQ","timestamp":"1700859600.0","upvote_count":"2","poster":"jpes"},{"comment_id":"1074860","content":"Selected Answer: A\nA is the answer","poster":"BECAUSE","timestamp":"1700419380.0","upvote_count":"3"},{"timestamp":"1700220060.0","comment_id":"1073248","upvote_count":"2","poster":"severlight","content":"Selected Answer: B\nB, just a guess between A and B"},{"comment_id":"1072112","content":"Selected Answer: B\nThe answer is B\n\"If a processing error occurs, the event must move into a separate queue for review\" so a DLQ is mandatory, so it excludes A and C\nThen you need a place to publish events, so both B and D but you do not need an ALB to process message from a source like Eventbridge (or SQS) so D can be excluded.\nOnly lasts B.","poster":"Mikado211","timestamp":"1700105100.0","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: B\nSQS with DLQ. And the link below mentions that ApproximateAgeOfOldestMessage can be used for scaling.\n\nThe ApproximateAgeOfOldestMessage metric is useful when applications have time-sensitive messages and developers need to ensure that messages are processed within a specific time period. Developers can use these alerts to take action, for example, scaling up their consumers to process messages more quickly.\n\nhttps://aws.amazon.com/about-aws/whats-new/2016/08/new-amazon-cloudwatch-metric-for-amazon-sqs-monitors-the-age-of-the-oldest-message/","timestamp":"1699651800.0","poster":"yurdaor","comment_id":"1067532"},{"comment_id":"1051652","timestamp":"1698052860.0","poster":"hansean","content":"Selected Answer: A\nI with GO A. Because \"ApproximateAgeOfOldestMessage\" is not match with the \"Number of events\" of Option B.","upvote_count":"4"},{"poster":"AK2020","content":"Selected Answer: A\nA is correct","comment_id":"1046561","timestamp":"1697604360.0","upvote_count":"3"},{"timestamp":"1697389080.0","poster":"Certified101","comment_id":"1044313","content":"Selected Answer: A\nA is correct - wrong scaling action used for answer B.","upvote_count":"3"},{"content":"Selected Answer: B\nThe question asks to \"process events\", which should be going to SQS. If the wording was more around messages, then yeah, i would say SNS.","upvote_count":"2","timestamp":"1693612920.0","poster":"CloudHandsOn","comment_id":"996478"},{"poster":"Soweetadad","upvote_count":"5","comment_id":"993638","content":"Selected Answer: A\nIt's A. Cant be B due to this link. It would work, except the parameter should be based on backlog and not Approximate age of old message. https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","timestamp":"1693359780.0"},{"comment_id":"985925","content":"Selected Answer: A\nMy initial reaction was going with B, but then did some research on the scaling parameter. It should be NumberOfMessagesReceived rather than ApproximateAgeOfOldestMessage","poster":"SK_Tyagi","timestamp":"1692548520.0","upvote_count":"3"},{"timestamp":"1690689660.0","comment_id":"966831","content":"Selected Answer: B\nA would work just fine. But watch out for the wording here: \"the event must move into a separate queue for review.\"\nThis indicates that the primary solution is also a queue in the first place which sends failed events to a \"separate queue\".","upvote_count":"2","comments":[{"upvote_count":"1","poster":"MRL110","content":"EDIT: This indicates that the primary solution is also a queue in the first place and failed events will be moved to a \"separate queue\".","timestamp":"1690689780.0","comment_id":"966833"}],"poster":"MRL110"},{"comment_id":"963272","content":"Selected Answer: A\nA is the answer.\nB will not work because 'ApproximateAgeOfOldestMessage' is the wrong thing to scale on, should be 'ApproximateNumberOfMessagesVisible '. \nC/D nope.","poster":"breadops","upvote_count":"4","timestamp":"1690339140.0"},{"upvote_count":"1","timestamp":"1690232460.0","poster":"ggrodskiy","comment_id":"962053","content":"Correct A."},{"content":"Selected Answer: A\nselect A","timestamp":"1688796780.0","comment_id":"946212","upvote_count":"3","poster":"nicecurls"},{"upvote_count":"3","poster":"NikkyDicky","timestamp":"1688579160.0","comment_id":"944017","content":"Selected Answer: A\nA seems like a good option"},{"comments":[{"timestamp":"1688392620.0","poster":"dkx","comment_id":"941901","upvote_count":"1","content":"Additionally, Dead Letter Queues (DLQ) have been available since 2016 and are a great way to handle asynchronous failure situations. Destinations provide more useful capabilities by passing additional function execution information, including code exception stack traces, to more destination services."}],"upvote_count":"4","comment_id":"941886","timestamp":"1688391480.0","content":"A. Yes, because this uses the concept of Lambda Destinations. Destinations gives you the ability to handle the Failure of function invocations along with their Success. When a function invocation fails, such as when retries are exhausted or the event age has been exceeded (hitting its TTL), Destinations routes the record to the destination resource for every failed invocation for further investigation or processing.\nhttps://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/\n\nB. No, because the Amazon SQS metric 'ApproximateAgeOfOldestMessage' is the approximate age of the oldest non-deleted message in the queue. Our requirement is to scale on the 'number of events' and not the 'age' of a message. \n\nC. No, because this option fails to clearly mention how to process errors\n\nD. No, because an ALB is not currently available as an EventBridge target. See: https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html","poster":"dkx"},{"poster":"Piccaso","upvote_count":"3","comment_id":"937034","timestamp":"1687978620.0","content":"Selected Answer: A\nB uses EC2. Lambda function is better."},{"content":"Selected Answer: A\nLambda will automatically scale on number of events received.\n\nScaling of auto-scaling group should not be done based on \"ApproximateAgeOfOldestMessage\" metric, instead \"ApproximateNumberOfMessages\" should be used. Further more, question asks to scale based on number of messages received, and \"ApproximateAgeOfOldestMessage\" doesn't provide that indication.\n\nRef: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","timestamp":"1687819920.0","upvote_count":"4","poster":"pupsik","comment_id":"934846"},{"content":"Selected Answer: A\nI vote for A only because of the metric, used in B - ApproximateAgeOfOldestMessage. This is not how you set up autoscaling for SQS.","poster":"Maria2023","timestamp":"1687586520.0","comment_id":"932239","upvote_count":"5"},{"poster":"easytoo","timestamp":"1687282140.0","upvote_count":"1","content":"b-b-b-b-b-b-b-b","comment_id":"928643"},{"timestamp":"1685901720.0","content":"Answer : A\nSNS -> SQS -> Lambda\nhttps://stackoverflow.com/questions/42656485/sns-to-lambda-vs-sns-to-sqs-to-lambda","comment_id":"914855","upvote_count":"3","poster":"Roontha"},{"timestamp":"1684720020.0","poster":"y0eri","upvote_count":"4","comments":[{"poster":"clownfishman","content":"there is such a cloudwatch metrics - the question is whether AutoScaling group can read that to scale in/out\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-cloudwatch-metrics.html","comment_id":"919745","upvote_count":"1","timestamp":"1686364800.0"}],"comment_id":"903630","content":"Selected Answer: A\nNo age metric for B: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sqs/get-queue-attributes.html"},{"timestamp":"1684013700.0","poster":"momo3321","upvote_count":"2","content":"Selected Answer: B\nB for sure. Option A doesn't provide any mechanism for scale based on the number of events while SQS is scalable & fully managed by AWS, EC2 ASG can achive the scaling by events number with \"ApproximateAgeOfOldestMessage\" metric and DLQ can target for messages that were not successfully processed.","comment_id":"897067"},{"upvote_count":"3","content":"Selected Answer: A\nI think EC2 Autoscaling is based on metric \"ApproximateNumberOfMessagesVisible\", and not in the metric of B. I choose A.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","poster":"2aldous","timestamp":"1683661320.0","comment_id":"893396"},{"timestamp":"1682311440.0","poster":"OCHT","upvote_count":"5","comment_id":"879025","content":"Selected Answer: A\nThe solution that will meet the requirements is Option A. Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination to the function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target.\n\nThis solution allows for automatic scaling of the AWS Lambda function based on the number of events received. If a processing error occurs, the event will be moved to the specified SQS queue for review."},{"timestamp":"1682226660.0","upvote_count":"8","comment_id":"877888","content":"Selected Answer: B\nSQSQ with DLQ","poster":"Yowie351"}],"isMC":true,"answers_community":["A (52%)","B (48%)"],"question_images":[],"question_id":89,"url":"https://www.examtopics.com/discussions/amazon/view/107090-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"A","answer_description":"","topic":"1"},{"id":"5UTawkjCVvaAiaVxCKA3","answer_description":"","answer":"C","topic":"1","isMC":true,"unix_timestamp":1670852220,"question_text":"A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.\nThe website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.\nWhich solution meets these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/91203-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"answers_community":["C (85%)","D (15%)"],"exam_id":33,"answer_ET":"C","question_id":90,"timestamp":"2022-12-12 14:37:00","discussion":[{"timestamp":"1727062200.0","content":"Selected Answer: C\nThis solution meets the requirements by using multiple managed services offered by AWS which can reduce the operational overhead. Hosting the web application in Amazon S3 would make it highly available, scalable and can handle variable traffic. The uploaded videos can be stored in S3 and processed using S3 event notifications that trigger a Lambda function, which calls the Amazon Rekognition API to categorize the videos. SQS can be used to process the event notifications and also it is a managed service.\nThis solution eliminates the need to manage EC2 instances, EBS volumes and the custom software. Additionally, using Lambda function in this case, eliminates the need for managing additional servers to process the SQS queue which will reduce operational overhead.\n\nBy using this solution, the company can benefit from the scalability, reliability, and cost-effectiveness that these services offer, which can help to reduce operational overhead and improve the overall performance and security of the application.","comments":[{"comment_id":"1017335","timestamp":"1695697500.0","poster":"Mahakali","comments":[{"upvote_count":"2","comment_id":"1281168","timestamp":"1725904800.0","poster":"AWSum1","content":"ECS is managed to an extent, but the question fails to elaborate, no mention of fargate etc. There's unnecessary mentions of spot instances to confuse you. The web application has static content which can be hosted in S3 instead of ECS"}],"upvote_count":"1","content":"Any explanation on option A ?"}],"comment_id":"774730","upvote_count":"32","poster":"masetromain"},{"upvote_count":"12","comments":[{"timestamp":"1736978940.0","content":"You absolutely can host a static website in Amazon S3, I do it all the time (you create DNS records pointing to the S3 bucket), although putting CloudFront in front of it would be better. S3 web hosting even allows custom 404 error pages, and selecting a default (index.html) page, etc.","upvote_count":"1","poster":"Kirkster","comment_id":"1341268"},{"upvote_count":"1","poster":"7f6aef3","timestamp":"1712886180.0","comment_id":"1194074","content":"Rekognition no consulta directamente EBS, pero puedes cargar datos en un recurso de almacenamiento compatible con Rekognition, como S3, para que Rekognition realice análisis sobre esos datos."},{"upvote_count":"1","content":"Rekognition does not query EBS directly, but you can upload data to a Rekognition-compatible storage resource, such as S3, for Rekognition to perform analysis on that data.","comment_id":"1194075","timestamp":"1712886240.0","poster":"7f6aef3"},{"timestamp":"1710609540.0","comment_id":"1175152","upvote_count":"2","poster":"gofavad926","content":"\"The company wants to re-architect the application \"..."},{"comment_id":"994861","comments":[{"timestamp":"1694005920.0","poster":"Bloops","upvote_count":"1","content":"\"The website contains static content\"\n\nContains do not means that all the website is just static","comments":[{"content":"They also do not mention the website has any dynamic content so there's that","poster":"Six_Fingered_Jose","upvote_count":"10","comment_id":"1003159","timestamp":"1694260140.0"}],"comment_id":"1000646"}],"poster":"Arnaud92","upvote_count":"3","timestamp":"1693464900.0","content":"But it is specifically specified that the web app is just static content..."},{"poster":"jpa8300","content":"D is right and it is valid, but C seems to me a more complete and better solution. And I agree that the site seems to be only static content. Usually, when it has dynamic content it is mentioned int he question.","upvote_count":"2","comment_id":"1108776","timestamp":"1703860020.0"}],"content":"D. Because, you cannot host web application in S3, only static web assets. ElasticBeanStalk provides an easy way to onboard autoscaling web apps with minimal operational overheads.","comment_id":"863086","poster":"RaghavendraPrakash","timestamp":"1680793920.0"},{"upvote_count":"1","content":"Selected Answer: C\nAnswer A doesn't really address the operational burden, and the Spot instance stuff is a distractor. Answer C (hosting the static website in S3, along with the videos, and switching to Rekognition with S3 events) provides the most reduction in operational burden, and as a plus is also going to be the lowest-cost solution.","timestamp":"1736979060.0","comment_id":"1341269","poster":"Kirkster"},{"upvote_count":"1","comment_id":"1326082","poster":"Drake17","timestamp":"1734086340.0","content":"Selected Answer: C\nThe Amazon Rekognition Video API facilitates the analysis of videos either stored in an Amazon S3 bucket or streamed via Amazon Kinesis Video Streams.\n\nhttps://docs.aws.amazon.com/rekognition/latest/dg/how-it-works-operations-intro.html"},{"comment_id":"1307684","upvote_count":"1","timestamp":"1730871780.0","content":"Selected Answer: C\n'static website' = Amazon S3\n'store videos'= Amazon S3\n'video analysis' = Amazon Rekognition\n'reduce operational overhead, managed service' = S3 events, AWS Lambda, Amazon SQS","poster":"TariqKipkemei"},{"upvote_count":"5","timestamp":"1727062260.0","poster":"cudbyanc","content":"Selected Answer: C\nThe answer is C.\n\nThis solution eliminates the need for managing and scaling EC2 instances for the web application and the worker environment for processing the SQS queue. Instead, Amazon S3 can host the web application, and store the uploaded videos, which can trigger S3 event notifications to send messages to the SQS queue. Then, an AWS Lambda function can process the messages in the SQS queue and use Amazon Rekognition API to categorize the videos. This approach also takes advantage of AWS-managed services, such as S3, SQS, and Lambda, which reduces operational overhead and dependency on third-party software.","comment_id":"821734"},{"content":"Selected Answer: C\nC. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.\n\nExplanation:\nHosting the Web Application in Amazon S3:\n\nCost-effective and Scalable: Amazon S3 is a cost-effective and scalable solution for hosting static web content. It can handle variable traffic efficiently without the need to manage servers.\nStatic Content Hosting: Ideal for serving static content like HTML, CSS, JavaScript, and media files.","timestamp":"1727062260.0","poster":"Bereket","upvote_count":"1","comment_id":"1231217"},{"content":"Selected Answer: C\nWhile I vote for C, I do think however that whether we can go with C really depends on the application codebase.\nThe use case mentions that the application enables file uploads. We know that handling files require a backend, if your application is written in something like Java. If that's the case, you won't be able to host your application in S3. The phrase \"website contains static content\" is really vague, as it does not reveal anything about the backend of the application.\nNow, the fact that the application has EBS to store Video files give up a hint, that suggests that the application has some BE code. \nI am taking a hint from \"re-architect\" I assume involves some revamping of the applications codebase. So, here's how I'd go about \"re-architecting\"\n1. Move storage of files to S3.\n2. Eliminate the BE codebase, revamp the FE codebase to rely entirely on AWS JS SDK and handle file uploads with that. Now you don't need to manage any compute resources at all.\n3. Go about the rest of the solution.","timestamp":"1727062260.0","upvote_count":"1","poster":"kz407","comment_id":"1174567"},{"comment_id":"1095047","upvote_count":"2","content":"Selected Answer: C\nThe mention of static content really throws this question off and clearly the community thinks this as well. The argument of static website vs static content being the key to selecting D isn't really a strong argument but that doesn't exclude D from being a viable solution. Operational overhead is minimized with Elastic Beanstalk and removes dependencies on third party tools/software.","poster":"924641e","timestamp":"1727062260.0","comments":[{"timestamp":"1710377700.0","upvote_count":"1","poster":"24Gel","comment_id":"1173004","content":"thanks, this is the best explain"}]},{"comments":[{"poster":"grire974","timestamp":"1704838080.0","upvote_count":"1","comment_id":"1117882","content":"per my previous comment; s3 is the only viable data source for rekognition https://aws.amazon.com/rekognition/faqs/#:~:text=Amazon%20Rekognition%20Video%20operations%20can,are%20MPEG%2D4%20and%20MOV. \n\nfrom my experience this is the same too with similar services like elastic transcoder"}],"upvote_count":"1","content":"Selected Answer: C\nIf it were D - how would Rekognition access the videos to classify? Rekognition would need to ssh into the EBS volume of various beanstalk instances running under an ASG (impossible as far as I know). I agree though - I think the wording is terrible for 'contains static content'; as how on earth would this type of app practically run on s3 alone for login/ user auth etc.. would need to be coupled with other serverless products such as lambda/cognito etc.","comment_id":"1117873","timestamp":"1727062260.0","poster":"grire974"},{"content":"C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.","comment_id":"1275450","poster":"amministrazione","timestamp":"1725091260.0","upvote_count":"1"},{"upvote_count":"1","content":"I saw this question in other question bank (owner of the questions) and it is A, reason is assuming is moving files back and forth cannot be static page, so it is A.","comment_id":"1261815","poster":"ff32d79","timestamp":"1722974040.0"},{"timestamp":"1718560500.0","comment_id":"1231483","content":"Selected Answer: C\nOnly Answer C is the solution that covers all the requirements, where the videos are stored, how SQS messages are produced and consumed, how web app is hosted.","upvote_count":"1","poster":"Helpnosense"},{"timestamp":"1710339420.0","poster":"MoT0ne","comment_id":"1172628","upvote_count":"1","content":"re-architect the application to reduce operational overhead"},{"poster":"subbupro","timestamp":"1701770220.0","content":"Elastic bean stack is not required , it is a static content only, better can go with S3. So Answer is C","comment_id":"1088351","upvote_count":"1"},{"content":"C videos in Amazon S3","comment_id":"1080918","upvote_count":"1","poster":"abeb","timestamp":"1701020820.0"},{"comment_id":"1079793","upvote_count":"2","poster":"KevinYao","content":"Selected Answer: D\nWeb application is never hosted in S3, that is storage normally.","timestamp":"1700895780.0"},{"poster":"severlight","comment_id":"1068313","timestamp":"1699765980.0","upvote_count":"1","content":"Selected Answer: C\nC is a well-explained and detailed solution. For D it isn't like that, for instance, there is no solution provided for storing images."},{"poster":"M4D3V1L","comment_id":"1023801","timestamp":"1696326360.0","upvote_count":"2","content":"It's A, I had the same question in Jon Bonzo's tests and the right answer is A."},{"comment_id":"1015374","content":"I go with D. \"web site has static content\" it's not the same be static web site. And web site on S3 does not go with https, so upload the video without Authentication & SSL/TLS !!???","upvote_count":"1","poster":"alexua","timestamp":"1695512100.0"},{"upvote_count":"1","comment_id":"998177","content":"Selected Answer: C\nThe case is similar to the blogs below, and seem normally Amazon Rekognition is trigger by AWS Lambda function.\nhttps://aws.amazon.com/tw/blogs/architecture/detecting-solar-panel-damage-with-amazon-rekognition-custom-labels/","timestamp":"1693800180.0","poster":"Simon523"},{"timestamp":"1692750660.0","comment_id":"987839","content":"Selected Answer: C\nWhile AWS Elastic Beanstalk can simplify deployment, it might not fully meet the requirement of removing dependencies on third-party software, as it still requires using Amazon Rekognition. This option introduces additional complexity by maintaining a separate worker environment for SQS queue processing.","poster":"whenthan","upvote_count":"2"},{"upvote_count":"4","content":"Answer D.\nIt says: \"The website contains static content...\", not \"It's a static website\".\nStill, even if you argue that it's possible to host a web application in S3 with a combination of S3 + Lambda + ..., you would fall into increasing the operational overhead with so many moving parts.\n\nAWS Elastic Beanstalk is a platform as a service used for deploying and scaling web applications and services and, although it won't make everything serverless (they are not asking for that), it will make management and deployment easier while still using AWS Managed Services.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html","comment_id":"972005","timestamp":"1691144760.0","comments":[{"upvote_count":"3","content":"Why would they specify that the web app contains static content if not 100% static content ? It wouldn't make sense here. You have to assume that it is a static website.","comment_id":"994863","poster":"Arnaud92","timestamp":"1693465020.0","comments":[{"content":"It can't be a static website because users are able to upload contents to it. It is a dynamic website. The scenario mentions static content because that is part of the overall solutions.","upvote_count":"1","timestamp":"1705103760.0","comment_id":"1121178","poster":"8608f25"}]}],"poster":"chico2023"},{"upvote_count":"2","comment_id":"960503","content":"Selected Answer: C\nThe main concern with option D is that it still relies on managing EC2 instances for both the web application and the worker environment, which might not be the most cost-effective and operationally efficient solution compared to the serverless architecture in option C.","poster":"Russs99","timestamp":"1690122420.0"},{"upvote_count":"4","poster":"giancarlooooo","comment_id":"943705","content":"Selected Answer: D\nThe answer is D because the question says \"re-architect\" so you don't want to intervene on the software, but only on the management. If the question said \"re-factoring\" then it would have been C","comments":[{"upvote_count":"1","content":"I support answer D but re-arc & re-fac mean the same thing. [Ref: https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/]","comment_id":"966796","timestamp":"1690685640.0","poster":"chiajy"}],"timestamp":"1688558040.0"},{"comment_id":"942189","poster":"Mom305","timestamp":"1688414220.0","upvote_count":"2","content":"Selected Answer: C\nLambda to cover the serverless approach, S3 is way better than EFS, and SQS proces the events from S3"},{"content":"Selected Answer: C\nC most fitting","timestamp":"1687915260.0","upvote_count":"1","poster":"NikkyDicky","comment_id":"935956"},{"comment_id":"929202","poster":"Jonalb","content":"Selected Answer: C\nStatic content! guyssssss","upvote_count":"2","timestamp":"1687335720.0"},{"comment_id":"926385","content":"Selected Answer: C\nC is more serverless solutions","poster":"SkyZeroZx","upvote_count":"1","timestamp":"1687055940.0"},{"poster":"easytoo","comment_id":"918723","upvote_count":"1","content":"c-c-c-c-c-c-c-c-c-c","timestamp":"1686263820.0"},{"poster":"muurilopes","comment_id":"916766","content":"Selected Answer: D\nThe application needs a backend to process video uploads","upvote_count":"2","timestamp":"1686101640.0"},{"comment_id":"903143","comments":[{"comment_id":"994864","poster":"Arnaud92","timestamp":"1693465200.0","content":"why would they mention that the website has just some static content ? it make no sense here.","upvote_count":"1"},{"timestamp":"1687770900.0","poster":"BATSIE","comment_id":"934235","content":"Yes, you can host videos on Amazon S3. Amazon S3 is an object storage service that can store and retrieve any amount of data, including videos, images, and other media files.\n\nWhile Amazon S3 can be used to host static websites, it is not limited to just that use case. You can use Amazon S3 to store and serve any type of file, including videos. You can also use Amazon S3 in combination with other AWS services such as Amazon CloudFront to deliver video content to users with low latency and high transfer speed","upvote_count":"1"}],"timestamp":"1684668480.0","content":"Selected Answer: D\nHow is it possible to host a website in S3??. the website has a STATIC \"content\" but website itself is NOT STATIC","poster":"dev112233xx","upvote_count":"7"},{"content":"Selected Answer: C\nThis solution eliminates the need for managing and scaling EC2 instances for the web application and the worker environment for processing the SQS queue.","comment_id":"865080","poster":"nexus2020","timestamp":"1680990180.0","upvote_count":"7"},{"comment_id":"852795","upvote_count":"3","poster":"mfsec","content":"Selected Answer: C\nHost the web application in Amazon S3","timestamp":"1679979900.0"},{"comment_id":"832518","content":"ANS is D\n\nPoint to consider, \"reduce operational overhead using AWS managed services\" and not to redesign. Therefore, EC2 will be replaced with ElasticBeans","comments":[{"upvote_count":"1","timestamp":"1681756860.0","content":"No. The key point here is: The company wants to RE-ARCHITECT the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software. \n\nRead the question carefully.","poster":"AlbertS82","comment_id":"872990"}],"timestamp":"1678248600.0","upvote_count":"3","poster":"mKrishna"},{"comment_id":"831772","poster":"kiran15789","upvote_count":"3","timestamp":"1678185720.0","content":"Selected Answer: C\nThis solution eliminates the need for managing and scaling EC2 instances for the web application and the worker environment for processing the SQS queue."},{"timestamp":"1677077280.0","upvote_count":"2","poster":"PSPaul","content":"Vote C","comment_id":"817943"},{"timestamp":"1677043380.0","content":"Logical answer : Key here is reduced operational head and use aws managed services which takes to serverless solutions. which is Lambda and Rekognition (aws managed). Mounting to EFS is overhead and moreover is good for file system, in future can pose problem scaling it with large video content in future. S3 is good for static videos storage obviously, So C is correct.","poster":"God_Is_Love","upvote_count":"1","comment_id":"817487"},{"poster":"Musk","comments":[{"poster":"c73bf38","comment_id":"810173","content":"The most appropriate solution would be to use Amazon S3 for storing the uploaded videos, and hosting the web application. This approach reduces operational overhead, and removes dependencies on third-party software. S3 event notifications can be used to publish events to an SQS queue, which can then be processed using AWS Lambda functions that call the Amazon Rekognition API to categorize the videos.","upvote_count":"2","timestamp":"1676512020.0"}],"timestamp":"1674932340.0","comment_id":"790976","content":"I don't like C. It says that it CONTAINS static content, but it does not say ONLY static content. The S3 would not be suitable.","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: C\nI vote C.\nThe serverless architecture reduces operational overhead the most for the requirement.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-a-react-based-single-page-application-to-amazon-s3-and-cloudfront.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\nhttps://docs.aws.amazon.com/rekognition/latest/dg/video-analyzing-with-sqs.html","poster":"Untamables","comment_id":"759396","timestamp":"1672206000.0"},{"comment_id":"753403","upvote_count":"3","timestamp":"1671720960.0","content":"Selected Answer: C\nno brainer","poster":"spencer_sharp"},{"comments":[{"poster":"masetromain","upvote_count":"1","timestamp":"1670865840.0","comment_id":"743091","content":"https://www.examtopics.com/discussions/amazon/view/35889-exam-aws-certified-solutions-architect-professional-topic-1/"}],"content":"Selected Answer: C\nwebsite contains static content = S3\nI go with C","timestamp":"1670863860.0","comment_id":"743055","poster":"masetromain","upvote_count":"5"},{"upvote_count":"2","timestamp":"1670852220.0","poster":"zhangyu20000","content":"Correct answer is C","comment_id":"742849"}],"choices":{"B":"Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.","C":"Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.","A":"Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.","D":"Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos."},"answer_images":[]}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","provider":"Amazon","isImplemented":true,"numberOfQuestions":529,"id":33,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isBeta":false},"currentPage":18},"__N_SSP":true}