{"pageProps":{"questions":[{"id":"TAjVEdgTA2hrZvQo0YFX","answer_description":"","discussion":[{"upvote_count":"6","content":"Selected Answer: A\nIn Amazon Aurora Serverless, the performance of the database is controlled by the instance class. In the current situation, an increase in client connections is causing delays and potential connection loss in the database. To support a maximum of 2,000 concurrent connections, it is necessary to upgrade to a higher-performance instance class.\n\nThe db.r3.xlarge instance class offers better performance compared to db.t3. Therefore, making this change will improve the performance of the database. By applying the changes immediately, you can resolve issues such as delays and connection losses.\n\nHence, the most cost-effective solution that meets the requirements is to change the instance class to db.r3.xlarge and apply the changes immediately.","comments":[{"comment_id":"1190899","poster":"koki2847","timestamp":"1712485800.0","upvote_count":"1","content":"I agree with you. Additionally, C is definitely incorrect because we have to reboot to apply the new max_connection values. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.setting-capacity.html#aurora-serverless-v2.max-connections"},{"content":"We recommend using the T DB instance classes only for development, test, or other nonproduction servers. For more detailed recommendations for the T instance classes","timestamp":"1692659100.0","upvote_count":"1","comment_id":"986958","poster":"r_a_y_p_r"}],"poster":"TQM__9MD","comment_id":"951037","timestamp":"1689288900.0"},{"comment_id":"1122333","timestamp":"1705216380.0","poster":"MultiAZ","content":"Selected Answer: C\nC\nFirst, the question is obviously wrong. In Serverless, you have no instance class or size. \nHowever, since most of the question and the answers talk about specific instance sizes, I will ignore the \"Serverless\" keyword. \nTherefore, \"C\" is the most cost-effective solution. It may not be the most performant or even stable, but it is cost-effective.","upvote_count":"1"},{"timestamp":"1696794600.0","content":"the question is wrong? does aurora server less has instance type? it says its running on t3 medium","upvote_count":"1","poster":"Sathish_dbs","comment_id":"1028249"},{"poster":"roymunson","content":"Selected Answer: C\nDefnitely C:\n\nThe maximum number of connections allowed to an Aurora MySQL DB instance is determined by the max_connections parameter in the instance-level parameter group for the DB instance.\n\nThe following table lists the resulting default value of max_connections for each DB instance class available to Aurora MySQL. You can increase the maximum number of connections to your Aurora MySQL DB instance by scaling the instance up to a DB instance class with more memory, or by setting a larger value for the max_connections parameter in the DB parameter group for your instance, up to 16,000.\n\nThe db.t3.medium has a default value of 90. So setting the max_connections = 2000 (by creating a new parameter group) should fit the bill. Remember we are looking for a solution that keeps our costs as low as possible. Changing the instance class to db.r3.xlarge would also work but it will cost you more.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html","upvote_count":"3","comment_id":"1027948","timestamp":"1696767540.0"},{"timestamp":"1692463920.0","comment_id":"985188","content":"None of the answer fulfil the requirements.\n- Aurora Serverless db only support min ACUs to max ACUs. No db class available.\n- Change parameter group required reboot.\nIn short, to meet requirements, we need to:\n1. Change maxACUs to 16\n2. Create new parameter group, change max_connection to 2000, apply, and reboot\nSee:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.setting-capacity.html#aurora-serverless-v2.parameter-groups\nFor legacy Aurora, we need to:\n1. Change Aurora db server class to db.r3.xlarge\n2. Create new parameter group, change max_connection to 2000, and apply\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html\nJust keep in mind where to go in case AWS fix this questions during the real exam.","upvote_count":"1","poster":"zanhsieh"},{"upvote_count":"4","comment_id":"966718","content":"Selected Answer: C\nI'm not sure if we can modify the instance class for Aurora Serverless. So I think A and D are not right. We can not edit the default parameter group. This only leaves C possible. So We will go with C.","poster":"Windy","timestamp":"1690674480.0"},{"comment_id":"952264","poster":"Windy","timestamp":"1689417120.0","content":"Why not D?","upvote_count":"1"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/115088-exam-aws-certified-database-specialty-topic-1-question-310/","question_id":236,"answers_community":["C (57%)","A (43%)"],"choices":{"C":"Create a new parameter group for the MySQL engine that the database uses. Set the max_connections value to 2,000. Assign the parameter group to the DB instance. Apply the changes immediately.","B":"Edit the default parameter group for the MySQL engine that the database uses. Change the max_connections value to 2,000. Reboot the DB instance to apply the new value.","D":"Modify the instance class to db.t3.large. Apply the changes immediately.","A":"Modify the instance class to db.r3.xlarge. Apply the changes immediately."},"answer_images":[],"timestamp":"2023-07-14 00:55:00","question_text":"A company needs to troubleshoot its Amazon Aurora Serverless MySQL database. The company selected a db.t3, medium instance class for the database's initial deployment. The database experienced light usage, and performance was normal.\n\nAs the number of client connections increases, the application that is connected to the database is experiencing higher latency and occasional lost connections. A database specialist determines that the database needs to support a maximum of 2,000 simultaneous connections.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer":"C","isMC":true,"answer_ET":"C","topic":"1","exam_id":22,"unix_timestamp":1689288900},{"id":"CjNXLfu6OBXNwz5xTSXK","answer_description":"","answer_images":[],"question_images":[],"answers_community":["A (83%)","D (17%)"],"question_text":"A database administrator needs to save a particular automated database snapshot from an Amazon RDS for Microsoft SQL Server DB instance for longer than the maximum number of days.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?","unix_timestamp":1689288960,"isMC":true,"exam_id":22,"discussion":[{"comment_id":"1105884","content":"Selected Answer: D\nmost operationally efficient solution for saving a particular automated database snapshot for longer than the maximum retention period.\nnot A since it will take more time and additional resources.","poster":"Ashy1313","upvote_count":"1","timestamp":"1703586060.0"},{"content":"Selected Answer: A\noperationally efficient = manual snapshot, that's it , no additional procedures or moving to S3 needed.","upvote_count":"2","comment_id":"1028250","poster":"Sathish_dbs","timestamp":"1696795020.0"},{"content":"Selected Answer: A\nAns: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html\n\nCopying the automated backup\n- To retain a particular automated snapshot for longer than 35 days, you can copy the snapshot. \n- The copy is stored as a manual snapshot. \n- You can retain this manual snapshot for as long as you need. \n- Unlike automated backups, manual snapshots aren't subject to the backup retention period. \n- Manual snapshots don't expire.","poster":"DrCloud","upvote_count":"2","timestamp":"1695900240.0","comment_id":"1019773"},{"poster":"zanhsieh","comment_id":"985234","timestamp":"1692451320.0","content":"Selected Answer: A\nA.\nManual snapshots can be retained indefinitely, allowing you to keep the snapshot for as long as you need. Same apply to MS SQL Server RDS.","upvote_count":"3"},{"upvote_count":"3","timestamp":"1692403980.0","comment_id":"984847","content":"Selected Answer: A\nA. Create a manual copy of the snapshot.\n Manual snapshots can be retained indefinitely, allowing you to keep the snapshot for as long as you need.","poster":"hecong"},{"upvote_count":"1","poster":"Monknil","timestamp":"1689821160.0","content":"Not sure on this one, but looks like D maybe.","comment_id":"957064"},{"content":"B\nhttps://repost.aws/knowledge-center/rds-automated-snapshot-retain-longer","comment_id":"956887","timestamp":"1689789540.0","poster":"rn30","comments":[{"content":"Per the link you provided above, the answer should be A. \"To retain a particular automated snapshot for longer than 35 days, you can copy the snapshot. \".","upvote_count":"1","poster":"Windy","comment_id":"966709","timestamp":"1690673280.0"}],"upvote_count":"1"},{"poster":"Windy","comment_id":"952272","content":"I think it is A.","upvote_count":"2","timestamp":"1689417360.0"},{"comment_id":"951041","poster":"TQM__9MD","upvote_count":"1","content":"Selected Answer: D\nD is answer\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Procedural.Importing.html","timestamp":"1689288960.0"}],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/115089-exam-aws-certified-database-specialty-topic-1-question-311/","timestamp":"2023-07-14 00:56:00","answer_ET":"A","topic":"1","question_id":237,"choices":{"C":"Change the retention period of the snapshot to 45 days.","B":"Export the contents of the snapshot to an Amazon S3 bucket.","A":"Create a manual copy of the snapshot.","D":"Create a native SQL Server backup. Save the backup to an Amazon S3 bucket."}},{"id":"Z5Wp5uB1OynBOIOWNu48","question_images":[],"exam_id":22,"question_text":"A company runs an Amazon Aurora MySQL DB instance for one of its critical applications. The company’s marketing department sends promotional email messages to customers based on the data in this database. A database engineer needs to make the data from all the tables available in the company’s Amazon S3 data lake. The database engineer wants to perform an export from a snapshot to populate the S3 data lake with the contents of the database.\n\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)","isMC":true,"topic":"1","answers_community":["ADE (87%)","13%"],"discussion":[{"content":"ADE\nRefer https://www.youtube.com/watch?v=lyNGeDg6EII","upvote_count":"6","poster":"rn30","timestamp":"1689790020.0","comment_id":"956894"},{"timestamp":"1694155320.0","comment_id":"1002207","poster":"Pranava_GCP","content":"Selected Answer: ADE\nA,D,E is the correct answer.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ExportSnapshot.html","upvote_count":"5"},{"content":"Selected Answer: ADE\nADE\nWe need IAM role, not IAM user\nWe need the least overhead","comment_id":"1122044","upvote_count":"2","poster":"MultiAZ","timestamp":"1705178940.0"},{"poster":"DanShone","upvote_count":"3","timestamp":"1694696520.0","comment_id":"1007665","content":"Selected Answer: ADE\nB: IAM User, this must be a Role so D\nC: Option A is better\nF: Question doesn't mention archiving the data anywhere"},{"content":"Selected Answer: ADE\nADE.\nB: No. Shall provide IAM role, not the IAM user.\nC: No. Opt A has better scope.\nF: No. This operation would be cost saving but NOT LEAST operation overhead.","timestamp":"1692452400.0","comment_id":"985241","poster":"zanhsieh","upvote_count":"3"},{"comment_id":"957063","upvote_count":"3","poster":"Monknil","content":"A,D,E is the correct answer.","timestamp":"1689820980.0"},{"upvote_count":"2","timestamp":"1689289140.0","content":"Selected Answer: ABC\nA,B,C is answer\n\nA. Create an existing automatic snapshot or a manual snapshot of the DB instance. This saves the content of the database as a snapshot.\n\nB. Identify the S3 bucket to export to and provide access to the bucket using an IAM user. Attach an IAM policy to the IAM user with permissions such as s3:PutObject*, s3:GetObject*, s3:ListBucket, s3:DeleteObject*, and s3:GetBucketLocation. Attach an IAM role to the DB instance. This allows the DB instance to have access to the S3 bucket.\n\nC. Create a copy of the existing automatic snapshot or manual snapshot of the DB instance. This creates a copy of the snapshot.\n\nTherefore, the combination of steps that meets these requirements with minimal operational overhead is A, B, and C.","comment_id":"951042","poster":"TQM__9MD"}],"url":"https://www.examtopics.com/discussions/amazon/view/115090-exam-aws-certified-database-specialty-topic-1-question-312/","answer_description":"","unix_timestamp":1689289140,"answer_images":[],"question_id":238,"timestamp":"2023-07-14 00:59:00","answer_ET":"ADE","answer":"ADE","choices":{"A":"Use an existing automated snapshot or manual snapshot, or create a manual snapshot of the DB instance.","F":"Create a symmetric AWS Key Management Service (AWS KMS) key for server-side encryption. Export the snapshot to Amazon S3 Glacier Flexible Retrieval.","C":"Create a copy of an existing automated snapshot or manual snapshot of the DB instance.","E":"Identify the S3 bucket for export. Provide access to the S3 bucket by using an IAM role. Attach an IAM policy with s3:PutObject*, s3:GetQpject*, s3:ListBucket, s3:DeleteObject*, and s3:GetBucketLocation permissions to the IAM role. Attach the IAM role to the DB instance.","B":"Identify the S3 bucket for export. Provide access to the S3 bucket by using an IAM user. Attach an IAM policy with s3:PutObject*, s3:GetObject*, s3:ListBucket, s3:DeleteObject*, and s3:GetBucketLocation permissions to the IAM user. Attach the IAM role to the DB instance.","D":"Create a symmetric AWS Key Management Service (AWS KMS) key for server-side encryption. Export the snapshot to Amazon S3."}},{"id":"w0Qov81kWmG3ckpL3Xva","answer":"AE","answers_community":["AE (47%)","CE (40%)","13%"],"discussion":[{"content":"Selected Answer: CE\nBase on comments","timestamp":"1713091200.0","comment_id":"1195446","poster":"tsangckl","upvote_count":"1"},{"poster":"missipssamarsh","timestamp":"1711973340.0","upvote_count":"2","comments":[{"comments":[{"comment_id":"1191104","content":"thanks for sharing the video link","upvote_count":"2","timestamp":"1712511780.0","poster":"Doox"}],"content":"video clearly states that we can't DeleteTaskLogs if instance is in \"storage-full\" status. Has to be in \"Available\" status.","poster":"Doox","timestamp":"1712511720.0","upvote_count":"1","comment_id":"1191102"}],"content":"Selected Answer: CE\nhttps://repost.aws/knowledge-center/dms-replication-instance-storage-full","comment_id":"1187405"},{"comment_id":"1122046","timestamp":"1705179060.0","poster":"MultiAZ","content":"Selected Answer: CE\nC+E\nWe need to provide more storage, not bigger instances. Also, deleting the logs will help","upvote_count":"2"},{"timestamp":"1696795800.0","upvote_count":"1","poster":"Sathish_dbs","content":"Selected Answer: AE\nIf the replication DB instance is in a storage-full status, you can’t delete logs.","comment_id":"1028253"},{"poster":"roymunson","upvote_count":"2","content":"Selected Answer: AE\nWhile C is a good action to prevent full storage you simply can't delete logs when you've already recieved the status of full stroage. So C is out.\n\n\"If the replication DB instance is in a storage-full status, you can't delete logs. You must increase the allocated storage size, or delete the replication task. To increase the storage size of a replication DB instance:\"\n\nIt's A and E: Just check:\nhttps://repost.aws/knowledge-center/dms-replication-instance-storage-full","timestamp":"1696768440.0","comment_id":"1027957"},{"upvote_count":"1","comment_id":"1007673","timestamp":"1694696820.0","poster":"DanShone","content":"Selected Answer: CE\nhttps://repost.aws/knowledge-center/dms-replication-instance-storage-full\nhttps://docs.aws.amazon.com/cli/latest/reference/dms/modify-replication-instance.html"},{"content":"Selected Answer: AC\nhttps://repost.aws/knowledge-center/dms-replication-instance-storage-full","timestamp":"1692251520.0","upvote_count":"2","comments":[{"poster":"Skarlex77","timestamp":"1706973900.0","comment_id":"1139364","content":"in the link you provided it is mentionned : \"If the replication DB instance is in a storage-full status, you can't delete logs. You must increase the allocated storage size, or delete the replication task. \" and they specifically say you should Increase the storage size of the replication DB instance in that case.\nRead carefully so you don't mislead people","upvote_count":"2"}],"comment_id":"983256","poster":"tonyro2"},{"content":"Answer is C,E.","upvote_count":"1","poster":"Monknil","comment_id":"957054","timestamp":"1689819240.0"},{"timestamp":"1689790620.0","poster":"rn30","content":"CE\nhttps://repost.aws/knowledge-center/dms-replication-instance-storage-full\nhttps://docs.aws.amazon.com/cli/latest/reference/dms/modify-replication-instance.html","upvote_count":"2","comment_id":"956900","comments":[{"content":"In the first link you provided, it says that \"If the replication DB instance is in a storage-full status, you can't delete logs. You must increase the allocated storage size, or delete the replication task\". This statement makes C not possible because the replication instance entered storage-full status.","poster":"Windy","upvote_count":"3","comment_id":"966686","comments":[{"timestamp":"1690726020.0","content":"Thanks Windy. I now feel A,E is the right answer.","comment_id":"967196","comments":[{"upvote_count":"1","comments":[{"comments":[{"content":"I changed my mind. As MultiAZ says, changing instance type by itself does nothing with the problem. Modifying MemoryLimitTotal and MemoryKeepTime values is also needed to avoid swapping. So C and E are more feasible. But the question actually looks for a help to solve, we don't have to resolve directly. The wording is too vague.","comment_id":"1190345","poster":"koki2847","timestamp":"1712400060.0","upvote_count":"1"}],"content":"But this quotation asks which option works and not a combination of options. So A and E is right because they basically do the same.","timestamp":"1712398020.0","poster":"koki2847","comment_id":"1190333","upvote_count":"1"}],"content":"Not really. Once you provide more storage (coming from E), you can also delete some logs (this is C). So C+E is the correct answer.\nThe instance size will add CPU and RAM, not storage.","comment_id":"1122049","poster":"MultiAZ","timestamp":"1705179240.0"}],"poster":"Monknil","upvote_count":"1"}],"timestamp":"1690671360.0"}]},{"timestamp":"1689694200.0","comment_id":"955572","poster":"Windy","content":"The answer is AE.","upvote_count":"1"},{"poster":"TQM__9MD","upvote_count":"4","comment_id":"951043","content":"Selected Answer: AE\nA,E is answer\n\nA. Change the size of the replication instance to a larger instance type supported.\nIf the replication instance has reached the storage full status, it indicates that the size of the replication instance may be insufficient for the storage capacity. By changing the instance size to a larger supported type, you can increase the storage capacity.\n\nE. Increase the amount of storage allocated to the replication instance using the modify-replication-instance API.\nIf the replication instance has reached the storage full status, it suggests that the allocated storage for the replication instance might be insufficient. By using the modify-replication-instance API, you can increase the amount of storage to address the issue.\n\nTherefore, the options that would be helpful to resolve this issue are A and E.","timestamp":"1689289200.0"}],"topic":"1","choices":{"A":"Change the size of the replication instance to a larger supported instance type.","C":"Use the AWS CLI to modify the replication task settings with ‘{“Logging”: {“DeleteTaskLogs”: true}}’.","B":"Use the AWS Management Console to modify the replication task settings to the limited large binary object (LOB) mode and set the value to 16.","E":"Use the modify-replication-instance API to increase the amount of storage allocated to the replication instance.","D":"Use the AWS CLI to modify the replication task settings with ‘{“Logging”: {“CloudWatchLogGroup”: null}}’."},"answer_ET":"AE","url":"https://www.examtopics.com/discussions/amazon/view/115091-exam-aws-certified-database-specialty-topic-1-question-313/","exam_id":22,"timestamp":"2023-07-14 01:00:00","unix_timestamp":1689289200,"question_text":"A large financial services company is using AWS Database Migration Service (AWS DMS) to migrate databases from on-premises to the AWS Cloud. During the migration of one of the databases, the AWS DMS replication instance entered a storage-full status. A database administrator needs to troubleshoot and fix the issue.\n\nWhich options would help the database administrator resolve this issue? (Choose two.)","question_id":239,"answer_images":[],"answer_description":"","isMC":true,"question_images":[]},{"id":"NG3hA1DnlW1Ab9BVvJ43","question_images":[],"answers_community":["A (88%)","13%"],"answer_description":"","discussion":[{"content":"Selected Answer: A\nThe answer is A\nStreams (with old+new images) are a prerequisite for Global Table.","timestamp":"1705179840.0","comment_id":"1122050","upvote_count":"2","poster":"MultiAZ"},{"comment_id":"1014888","timestamp":"1695471720.0","poster":"Pranava_GCP","upvote_count":"3","content":"Selected Answer: A\nA. Enable DynamoDB streams. Configure streams for new and old images. Create a global table replica in us-west-2. Monitor the progress of the replication until the status changes to Active.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html"},{"comment_id":"1007676","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/aws/new-convert-your-single-region-amazon-dynamodb-tables-to-global-tables/","upvote_count":"1","poster":"DanShone","timestamp":"1694697000.0"},{"content":"I changed the answer to A. We must enable streams for the DynamoDB global table. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_reqs_bestpractices.html","comment_id":"966677","timestamp":"1690670820.0","upvote_count":"1","poster":"Windy"},{"timestamp":"1689687360.0","content":"Answer is B.","comment_id":"955485","poster":"Windy","upvote_count":"1"},{"comment_id":"951045","content":"Selected Answer: A\nBy enabling DynamoDB Streams and configuring both new and old images in the stream, you ensure that all changes to the DynamoDB table are captured. Creating a global table replica in the us-west-2 region allows for real-time replication of the table data. By monitoring the progress of replication and ensuring the status is active, you can verify the successful replication of the data.\n\nThis solution ensures that all existing table data and future additions are included in an automated manner, replicating the data from the us-east-1 region to the us-west-2 region.\n\nTherefore, the solution that meets these requirements is option A.","upvote_count":"2","poster":"TQM__9MD","timestamp":"1689289260.0"}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/115093-exam-aws-certified-database-specialty-topic-1-question-314/","unix_timestamp":1689289260,"question_text":"A company wants to implement a design that includes multiple AWS Regions to support disaster recovery for its application. The company currently has a static website that is hosted on Amazon S3 and Amazon CloudFront. The application connects to an existing Amazon DynamoDB database in the us-east-1 Region. The DynamoDB table was recently created and was initialized with a large amount of company data. The company wants to replicate the database in real time to the us-west-2 Region.\n\nA database specialist needs to perform the replication, which must include all existing table data and any new data that is added in the future, in an automated way.\n\nWhich solution will meet these requirements?","question_id":240,"choices":{"C":"Enable DynamoDB streams. Configure streams for new and old images. Create a global table replica in us-west-2. Copy all existing data from us-east-1 to us-west-2 by using an export and batch import.","A":"Enable DynamoDB streams. Configure streams for new and old images. Create a global table replica in us-west-2. Monitor the progress of the replication until the status changes to Active.","D":"Create a global table replica in us-west-2. Copy all existing data from us-east-1 to us-west-2 by using an export and batch import.","B":"Create global table replica in us-west-2. Monitor the progress of the replication until the status changes to Active."},"exam_id":22,"timestamp":"2023-07-14 01:01:00","answer":"A","isMC":true,"answer_ET":"A","answer_images":[]}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":false,"isImplemented":true,"numberOfQuestions":359,"isBeta":false,"provider":"Amazon","id":22,"name":"AWS Certified Database - Specialty"},"currentPage":48},"__N_SSP":true}