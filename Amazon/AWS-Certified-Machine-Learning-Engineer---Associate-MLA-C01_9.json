{"pageProps":{"questions":[{"id":"cWeeHTzdH9g6zDeWmsyR","discussion":[{"timestamp":"1733364540.0","content":"Selected Answer: A\nA. Use zero buffering to minimize latency by delivering data immediate.\nTune batch size to optimize throughput & ensures sub-second delivery for real-time dashboards.","poster":"Saransundar","comment_id":"1322189","upvote_count":"3"},{"upvote_count":"3","timestamp":"1732742280.0","poster":"GiorgioGss","comment_id":"1318903","content":"Selected Answer: A\nAlthough is quite new solution , A will do the trick:\nhttps://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/"}],"question_id":41,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/152187-exam-aws-certified-machine-learning-engineer-associate-mla/","question_text":"A company has implemented a data ingestion pipeline for sales transactions from its ecommerce website. The company uses Amazon Data Firehose to ingest data into Amazon OpenSearch Service. The buffer interval of the Firehose stream is set for 60 seconds. An OpenSearch linear model generates real-time sales forecasts based on the data and presents the data in an OpenSearch dashboard.\nThe company needs to optimize the data ingestion pipeline to support sub-second latency for the real-time dashboard.\nWhich change to the architecture will meet these requirements?","answer":"A","question_images":[],"exam_id":27,"choices":{"B":"Replace the Firehose stream with an AWS DataSync task. Configure the task with enhanced fan-out consumers.","D":"Replace the Firehose stream with an Amazon Simple Queue Service (Amazon SQS) queue.","A":"Use zero buffering in the Firehose stream. Tune the batch size that is used in the PutRecordBatch operation.","C":"Increase the buffer interval of the Firehose stream from 60 seconds to 120 seconds."},"unix_timestamp":1732742280,"timestamp":"2024-11-27 22:18:00","answers_community":["A (100%)"],"isMC":true,"answer_description":"","answer_images":[],"answer_ET":"A"},{"id":"wz0aotDgSs9E2tOSMp5a","answer":"A","question_images":[],"question_text":"A company has trained an ML model in Amazon SageMaker. The company needs to host the model to provide inferences in a production environment.\nThe model must be highly available and must respond with minimum latency. The size of each request will be between 1 KB and 3 MB. The model will receive unpredictable bursts of requests during the day. The inferences must adapt proportionally to the changes in demand.\nHow should the company deploy the model into production to meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/152188-exam-aws-certified-machine-learning-engineer-associate-mla/","isMC":true,"exam_id":27,"answers_community":["A (100%)"],"answer_images":[],"timestamp":"2024-11-27 22:20:00","choices":{"C":"Install SageMaker Operator on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Deploy the model in Amazon EKS. Set horizontal pod auto scaling to scale replicas based on the memory metric.","A":"Create a SageMaker real-time inference endpoint. Configure auto scaling. Configure the endpoint to present the existing model.","B":"Deploy the model on an Amazon Elastic Container Service (Amazon ECS) cluster. Use ECS scheduled scaling that is based on the CPU of the ECS cluster.","D":"Use Spot Instances with a Spot Fleet behind an Application Load Balancer (ALB) for inferences. Use the ALBRequestCountPerTarget metric as the metric for auto scaling."},"topic":"1","unix_timestamp":1732742400,"discussion":[{"content":"Selected Answer: A\nSageMaker real-time endpoint: Purpose built for Auto scaling, low latency, handles bursts.","poster":"Saransundar","comment_id":"1321978","upvote_count":"2","timestamp":"1733325420.0"},{"timestamp":"1732742400.0","poster":"GiorgioGss","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/machine-learning/configuring-autoscaling-inference-endpoints-in-amazon-sagemaker/","comment_id":"1318904","upvote_count":"1"}],"question_id":42,"answer_ET":"A","answer_description":""},{"id":"lOd8JMFkH5CSLnhp5bjS","question_id":43,"topic":"1","answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/152189-exam-aws-certified-machine-learning-engineer-associate-mla/","answer_ET":"D","question_text":"An ML engineer needs to use an Amazon EMR cluster to process large volumes of data in batches. Any data loss is unacceptable.\nWhich instance purchasing option will meet these requirements MOST cost-effectively?","isMC":true,"timestamp":"2024-11-27 22:26:00","choices":{"D":"Run the primary node and core nodes on On-Demand Instances. Run the task nodes on Spot Instances.","A":"Run the primary node, core nodes, and task nodes on On-Demand Instances.","C":"Run the primary node on an On-Demand Instance. Run the core nodes and task nodes on Spot Instances.","B":"Run the primary node, core nodes, and task nodes on Spot Instances."},"unix_timestamp":1732742760,"answer":"D","answers_community":["D (100%)"],"question_images":[],"discussion":[{"poster":"Saransundar","timestamp":"1733357280.0","comments":[{"content":"Unacceptable data loss: \nPrimary node : Cluster doesn't start sometimes If spot only used \nCore nodes: Possible of partial data loss HDFS\nTask nodes: No data loss and do not hold persistent data","poster":"Saransundar","upvote_count":"1","timestamp":"1733357700.0","comment_id":"1322152"}],"comment_id":"1322151","upvote_count":"1","content":"Selected Answer: D\nAcceptable data loss: Spot can be used but\nyou can't change an instance purchasing option while a cluster is running. To change from On-Demand to Spot Instances or vice versa, for the primary and core nodes, you must terminate the cluster and launch a new one. For task nodes, you can launch a new task instance group or instance fleet, and remove the old one."},{"poster":"GiorgioGss","upvote_count":"4","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-spot-instances\n\"The task nodes process data but do not hold persistent data in HDFS. If they terminate because the Spot price has risen above your maximum Spot price, no data is lost\"","comment_id":"1318906","timestamp":"1732742760.0"}],"exam_id":27},{"id":"tL78AdRGg5RSPI5Ny5VY","question_text":"A company wants to improve the sustainability of its ML operations.\nWhich actions will reduce the energy usage and computational resources that are associated with the company's training jobs? (Choose two.)","choices":{"D":"Use AWS Trainium instances for training.","C":"Deploy models by using AWS Lambda functions.","A":"Use Amazon SageMaker Debugger to stop training jobs when non-converging conditions are detected.","B":"Use Amazon SageMaker Ground Truth for data labeling.","E":"Use PyTorch or TensorFlow with the distributed training option."},"answer_images":[],"answer_ET":"AD","answer":"AD","question_images":[],"discussion":[{"upvote_count":"1","poster":"Saransundar","comment_id":"1322149","content":"Selected Answer: AD\nBlog: https://aws.amazon.com/blogs/machine-learning/optimizing-mlops-for-sustainability/\nSustainability Goals: instances are up to 25% more energy efficient than comparable accelerated computing EC2 instances;\nhttps://aws.amazon.com/ai/machine-learning/trainium/\n\nSageMaker debugger helps to optimize resource consumption by detecting under-utilization of system resources, identifying training problems, and using built-in rules to monitor and stop training jobs as soon as bugs are detected.","timestamp":"1733356740.0"},{"comment_id":"1318908","timestamp":"1732742940.0","upvote_count":"1","poster":"GiorgioGss","content":"Selected Answer: AD\nI will go with A and D... they seems the most logic ones here."}],"unix_timestamp":1732742940,"timestamp":"2024-11-27 22:29:00","answers_community":["AD (100%)"],"isMC":true,"exam_id":27,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/152190-exam-aws-certified-machine-learning-engineer-associate-mla/","topic":"1","question_id":44},{"id":"iwIVVLdCnylMGTIARl4n","answer_ET":"A","question_images":[],"discussion":[{"poster":"ninomfr64","comment_id":"1330096","upvote_count":"6","content":"Selected Answer: A\nA. Yes, Clarify allows to get bias - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html\nB. No, the built-in image sagemaker-model-monitor-analyzer provides a range of model monitoring capabilities (constraint suggestion, statistics generation, constraint validation against a baseline, and emitting Amazon CloudWatch metrics) but you need Clarify for bias\nC. No, Glue Data Quality doesn't analyze bias\nD. No, well from a Notebook you can execute pretty much everything including a Clarify Job, however notebooks are for experiments and models development not for enabling real-time application features","timestamp":"1734793860.0"},{"comment_id":"1423480","timestamp":"1743617880.0","upvote_count":"1","poster":"Laxma99","content":"Selected Answer: A\nSageMaker Clarify can be used to analyze bias drift in models. By integrating this with a Lambda function, the workflow can be triggered on-demand whenever the application requires bias monitoring."},{"content":"Selected Answer: A\nSageMaker Clarify can be used to analyze bias drift in models. By integrating this with a Lambda function, the workflow can be triggered on-demand whenever the application requires bias monitoring.","poster":"S_201996","comment_id":"1331334","upvote_count":"1","timestamp":"1735097100.0"},{"poster":"tigrex73","comment_id":"1318752","timestamp":"1732721220.0","content":"Selected Answer: A\nSageMaker Clarify is a tool designed to detect and monitor bias in datasets and models. It provides built-in capabilities for bias analysis, both pre-training (data bias) and post-training (model bias). Using AWS Lambda to invoke the job ensures automation and on-demand execution, reducing operational complexity while meeting the requirement for monitoring bias drift.","upvote_count":"2"},{"timestamp":"1732712340.0","poster":"GiorgioGss","upvote_count":"2","comment_id":"1318656","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-data-bias.html"}],"timestamp":"2024-11-27 13:59:00","choices":{"C":"Use AWS Glue Data Quality to monitor bias.","B":"Invoke an AWS Lambda function to pull the sagemaker-model-monitor-analyzer built-in SageMaker image.","A":"Configure the application to invoke an AWS Lambda function that runs a SageMaker Clarify job.","D":"Use SageMaker notebooks to compare the bias."},"answer_images":[],"answer":"A","answers_community":["A (100%)"],"exam_id":27,"unix_timestamp":1732712340,"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/152137-exam-aws-certified-machine-learning-engineer-associate-mla/","question_id":45,"isMC":true,"question_text":"Case Study -\nA company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring.\nThe application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3.\nThe company needs to run an on-demand workflow to monitor bias drift for models that are deployed to real-time endpoints from the application.\nWhich action will meet this requirement?"}],"exam":{"name":"AWS Certified Machine Learning Engineer - Associate MLA-C01","isMCOnly":false,"isBeta":false,"isImplemented":true,"id":27,"provider":"Amazon","lastUpdated":"11 Apr 2025","numberOfQuestions":106},"currentPage":9},"__N_SSP":true}