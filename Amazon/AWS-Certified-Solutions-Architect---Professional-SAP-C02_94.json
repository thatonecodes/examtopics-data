{"pageProps":{"questions":[{"id":"1OIvzj9rmN7ifn7LOHZi","exam_id":33,"choices":{"D":"Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.","A":"Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an AWS Key Management Service (AWS KMS) customer managed key. In the key policy, include a statement to deny the creation of unencrypted EBS volumes.","B":"Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an SCP to deny the creation of unencrypted EBS volumes.","C":"Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes."},"question_images":[],"timestamp":"2024-06-28 23:27:00","discussion":[{"upvote_count":"2","content":"Option D meet the requirements of automatically encrypting existing unencrypted EBS volumes and preventing development teams from creating unencrypted EBS volumes, you can configure an AWS Config managed rule that identifies unencrypted EBS volumes. The automatic remediation action should be to create a new encrypted EBS volume and replace the old one. Additionally, modifying the AWS account setting for EBS encryption to always encrypt new EBS volumes ensures that no more unencrypted EBS volumes are created in the future.","timestamp":"1732584540.0","comment_id":"1317856","poster":"AzureDP900"},{"comments":[{"poster":"alexbraila","content":"SCPs are not available in this case, as the question is about a standalone AWS account, not an organization","timestamp":"1734685140.0","comment_id":"1329369","upvote_count":"2"}],"comment_id":"1316487","poster":"sammyhaj","upvote_count":"1","content":"Selected Answer: B\nIssue is that NONE prevent new EBS volumes to be launched without encryption, albiet systems manager can remediate it isn't ideal. regardless you need a solution to PREVENT 100% unecrypted drives, and this is only done via SCP. other items can be circumvented by CLI","timestamp":"1732307640.0"},{"timestamp":"1730336220.0","comment_id":"1305252","upvote_count":"2","poster":"Daniel76","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-encrypt-existing-and-new-amazon-ebs-volumes.html\n1. Use AWS Config to detects an unencrypted EBS volume. Not fleet manager. B and C out.\n2. System manager runbook is the automation that create encrypted copy of the EBS snapshot and replace the encrypted EBS with the encrypted copy. The KMS is used for the encryption and it cannot be used to deny creation of unencrypted EBS volume (A is out)."},{"timestamp":"1730040240.0","comment_id":"1303624","upvote_count":"2","content":"My answer is D \n\nProactive Remediation: The AWS Config rule and Automation runbook identify and remediate existing unencrypted volumes automatically.\nPreventive Measure: The modified AWS account setting ensures that all new EBS volumes created in the account are automatically encrypted.\n\nThis approach provides a comprehensive solution that addresses both existing unencrypted volumes and future volume creation.\n\n\nWhy not Option A: While it addresses the issue of existing unencrypted volumes, it doesn't prevent future unencrypted volume creation.","poster":"PSPaul"},{"upvote_count":"3","content":"Selected Answer: D\nD is correct instead of A because AWS support change account setting for EBS encryption","poster":"vip2","comment_id":"1243181","timestamp":"1720241340.0"},{"upvote_count":"2","timestamp":"1719972960.0","content":"Selected Answer: D\nUse config to find unencrypted EBS. Change the default setting.","poster":"Helpnosense","comment_id":"1241116"},{"content":"D\nEnabling default encryption for EBSs prevents the creation of unencrypted EBSs.","upvote_count":"2","poster":"kupo777","comment_id":"1239231","timestamp":"1719660240.0"},{"upvote_count":"2","comment_id":"1238936","poster":"awsaz","timestamp":"1719610020.0","content":"Selected Answer: D\nthe answer is D"}],"question_text":"A company has deployed applications to thousands of Amazon EC2 instances in an AWS account. A security audit discovers that several unencrypted Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company’s security policy requires the EBS volumes to be encrypted.\n\nThe company needs to implement an automated solution to encrypt the EBS volumes. The solution also must prevent development teams from creating unencrypted EBS volumes.\n\nWhich solution will meet these requirements?","answer_ET":"D","answer_images":[],"answers_community":["D (90%)","10%"],"answer_description":"","answer":"D","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/143035-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":466,"isMC":true,"unix_timestamp":1719610020},{"id":"VBjdvA5pAImd1IjnuAzf","choices":{"B":"Use Amazon CloudWatch to monitor service quotas that are published under the AWS/Usage metric namespace. Set an alarm for when the math expression metric/SERVICE_QUOTA(metric)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).","D":"Create an AWS Config rule to evaluate whether the Fargate SERVICE_QUOTA is greater than 80. Use Amazon Simple Email Service (Amazon SES) to notify the development team when the AWS Config rule is not compliant.","C":"Create an AWS Lambda function to poll detailed metrics from the ECS cluster. When the number of running Fargate tasks is greater than 80, invoke Amazon Simple Email Service (Amazon SES) to notify the development team.","A":"Use Amazon CloudWatch to monitor the Sample Count statistic for each service in the ECS cluster. Set an alarm for when the math expression sample count/SERVICE_QUOTA(service)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS)."},"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/143206-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":467,"answers_community":["B (100%)"],"isMC":true,"discussion":[{"content":"B\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Quotas-Visualize-Alarms.html","comment_id":"1241030","upvote_count":"5","timestamp":"1719952320.0","poster":"kupo777"},{"timestamp":"1730337060.0","poster":"Daniel76","content":"Selected Answer: B\nC and D are out due to using SES and not SNS for simple notification.\nAmong A and B, A is out because CloudWatch alarm was set with SERVICE_QUOTA but the option choose to monitor sample count stats instead of service quota published.","comment_id":"1305258","upvote_count":"1"},{"timestamp":"1727414640.0","poster":"Chakanetsa","upvote_count":"3","comment_id":"1289816","content":"Selected Answer: B\nWhy this is the correct choice:\nAWS/Usage metric namespace: This namespace provides detailed metrics for service usage and quotas, including the number of running ECS Fargate tasks. You can use this data to monitor how close you are to the service quotas.\nService quota monitoring: The math expression metric/SERVICE_QUOTA(metric)*100 allows you to calculate the percentage of the quota being used, making it easy to set an alarm when usage reaches 80%.\nCloudWatch Alarm: This is a native and efficient way to monitor service usage, and you can easily configure notifications via Amazon SNS to alert the development team when the threshold is crossed."},{"upvote_count":"2","comment_id":"1245027","poster":"c22ddd8","content":"Selected Answer: B\nService Quota","timestamp":"1720547940.0"}],"topic":"1","answer_images":[],"unix_timestamp":1719952320,"exam_id":33,"answer_description":"","timestamp":"2024-07-02 22:32:00","question_text":"A company is running a large containerized workload in the AWS Cloud. The workload consists of approximately 100 different services. The company uses Amazon Elastic Container Service (Amazon ECS) to orchestrate the workload.\n\nRecently the company’s development team started using AWS Fargate instead of Amazon EC2 instances in the ECS cluster. In the past, the workload has come close to running the maximum number of EC2 instances that are available in the account.\n\nThe company is worried that the workload could reach the maximum number of ECS tasks that are allowed. A solutions architect must implement a solution that will notify the development team when Fargate reaches 80% of the maximum number of tasks.\n\nWhat should the solutions architect do to meet this requirement?","answer_ET":"B","answer":"B"},{"id":"ZtRhpMn1i0fmiyR77FtW","timestamp":"2023-01-14 12:42:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95198-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"topic":"1","unix_timestamp":1673696520,"exam_id":33,"answers_community":["B (98%)","2%"],"answer_description":"","answer_ET":"B","choices":{"D":"Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new application setup as an origin. Create an AWS Global Accelerator accelerator. Add both of the CloudFront distributions as endpoints.","A":"Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 health checks.","C":"Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the ALB. Set up the failover routing algorithm on the ALB.","B":"Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary."},"question_id":468,"question_text":"A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).\n\nThe company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an origin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.\n\nA solutions architect must configure the application so that itis highly available and fault tolerant.\n\nWhich solution meets these requirements?","answer":"B","answer_images":[],"discussion":[{"comment_id":"775298","content":"Selected Answer: B\nThe correct answer is B. Provisioning an ALB, an Auto Scaling group, and EC2 instances in a different AWS region provides redundancy and failover capability for the application. By creating a second origin for the new ALB in the second region, the CloudFront distribution can automatically route traffic to the healthy origin in case of an issue with the primary origin. This ensures that the application remains highly available and fault-tolerant.\n\nOption A is not correct because it uses Route 53 failover records, which can result in increased latency and DNS resolution time for clients. Option C is not correct because it doesn't provide redundancy for the load balancer, which is a critical component of the application. Option D is not correct because it does not provide redundancy for the application in case of an issue with the primary origin in the first region.","upvote_count":"27","poster":"masetromain","timestamp":"1673696520.0"},{"upvote_count":"11","comment_id":"822137","timestamp":"1677397440.0","poster":"God_Is_Love","content":"For HA, always user second region but its there in all options. Here Cloudfront distribution multiple origin groups is the key point Solution Architects should know of. Configuring 2nd origin as ALB --> EC2 instances target group in another regions setup makes highly available. If Cloudfront detects that response is Http error (fault) code like 4XX,5XX etc, it will failover to secondary origin (ALB of another region) which makes this fault tolerant. Answer is B.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"},{"upvote_count":"1","content":"B. Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary.","poster":"amministrazione","comment_id":"1275503","timestamp":"1725094740.0"},{"content":"Selected Answer: A\nThis architecture is an active-active DR strategy. You would do it with R53 failover because R53 has healthchecks, and once the primary is down all requests go to the failover. With CloudFront failover, all requests would continue to hit the failed primary before being routed to the failover distribution, which increases latency and possibly compounds problems in the failed stack. Interestigly, the best solution would actually be a combination between A and B, as this blog post shows:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/improve-web-application-availability-with-cloudfront-and-route53-hybrid-origin-failover/","comment_id":"1258237","poster":"8693a49","timestamp":"1722345180.0","upvote_count":"1"},{"timestamp":"1711040820.0","upvote_count":"2","comments":[{"poster":"8693a49","comment_id":"1258239","upvote_count":"1","content":"You can add CloudFront distros to R53 using alias records: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html","timestamp":"1722345360.0"}],"content":"Selected Answer: B\nA is wrong because CloudFront distros can't be added to Route 53.\nB is correct\nC is wrong because ALBs are single region and don't do failover.\nD would work, but is overengineered in this context.","comment_id":"1179454","poster":"Dgix"},{"upvote_count":"2","content":"Selected Answer: B\nOption B is correct because it involves creating a redundant setup in another AWS Region with its own ALB, Auto Scaling group, and EC2 instances. By updating the CloudFront distribution to include a second origin for the new ALB and creating an origin group with primary and secondary origins, CloudFront can automatically route traffic to the secondary origin if the primary is unhealthy. This setup leverages CloudFront’s global reach to improve availability and fault tolerance without the need for DNS-level changes.\nOption A is not correct because it suggests creating a secondary deployment and updating the Route 53 A record to be a failover record with both CloudFront distributions as values. While Route 53 health checks and failover records can improve availability, CloudFront distributions themselves cannot be directly specified as values in A records for failover purposes. This option might lead to confusion in its implementation details.","timestamp":"1707509220.0","comment_id":"1145761","poster":"8608f25"},{"upvote_count":"2","comment_id":"1126455","poster":"bjexamprep","content":"Selected Answer: B\nWho the hell cooked this terrible question design.\nUsually, HA means single region, DR means cross region. The question is asking HA while all the answer are using cross region solutions.\nWhen Dynamic content is involved, the dynamic content has to be store in a persistent storage, while question says the dynamic content is store on the EC2 instances in an ASG, which means the EC2 instances are ephemeral. \nAnd when Dynamic content is involved, no matter HA or DR, a replication component must be built so that the Dynamic content will be replicated to the other side so that it can be available when the event happens. While, none of the answers mentions replication at all.","timestamp":"1705642980.0"},{"timestamp":"1704269340.0","upvote_count":"3","content":"Selected Answer: B\nNot A. CloudFront is a global service, having two distributions will not increase fault-tolerance\nNot C. Single ALB is a single-point-of-failure and also you cannot have Target Group in a different region\nNot D. CloudFront is a global service, having two distributions will not increase fault-tolerance and combining CloudFront with AWS Global Accelerator makes no sense\n\nB is correct as provisioning an ALB, an Auto Scaling group, and EC2 instances in a different AWS region provides redundancy and failover capability for the application. The origin group is the right way to enable failover for CloudFront distributions origin","poster":"ninomfr64","comment_id":"1112595"},{"poster":"holymancolin","timestamp":"1700053380.0","content":"Selected Answer: B\nNot Create a second CloudFront Distribution, it's update the distribution with multi origins.\n\nRef:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html#concept_origin_groups.creating\n\"Make sure the distribution has more than one origin. If it doesn’t, add a second origin.\"","upvote_count":"1","comment_id":"1071456"},{"content":"it's a B","comment_id":"940981","upvote_count":"1","poster":"NikkyDicky","timestamp":"1688309340.0"},{"content":"Selected Answer: B\nBoth A and B would work, but A is tangibly worse in terms of performing fail-over (because it relies on DNS) and gains you little, since CloudFront is highly available by its nature, making a second CF distribution doesn't improve your application's robustness.","comment_id":"934420","poster":"[Removed]","upvote_count":"2","timestamp":"1687782120.0"},{"poster":"mfsec","content":"Selected Answer: B\nProvision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region.","upvote_count":"1","timestamp":"1680004080.0","comment_id":"853196"},{"upvote_count":"1","poster":"dev112233xx","content":"Selected Answer: B\nB is the best solution with very high availability (compared to the R53 failover solution)","timestamp":"1679001840.0","comment_id":"841364"},{"content":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html","timestamp":"1677944640.0","comment_id":"829082","poster":"Ajani","upvote_count":"1"},{"content":"Selected Answer: B\nB looks good.","poster":"Sarutobi","comment_id":"824174","timestamp":"1677533760.0","upvote_count":"1"},{"poster":"masssa","upvote_count":"2","content":"Selected Answer: B\nB is correct.\nC is not correct, because ALB is regional service, so ALB have to be added too.","timestamp":"1674284040.0","comment_id":"783072"}]},{"id":"O8tekC4TjKudb4HUwl1b","question_text":"A company has several AWS Lambda functions written in Python. The functions are deployed with the .zip package deployment type. The functions use a Lambda layer that contains common libraries and packages in a .zip file. The Lambda .zip packages and Lambda layer .zip file are stored in an Amazon S3 bucket.\n\nThe company must implement automatic scanning of the Lambda functions and the Lambda layer to identify CVEs. A subset of the Lambda functions must receive automated code scans to detect potential data leaks and other vulnerabilities. The code scans must occur only for selected Lambda functions, not all the Lambda functions.\n\nWhich combination of actions will meet these requirements? (Choose three.)","choices":{"D":"Enable scanning in the Monitor settings of the Lambda functions that need code scans.","E":"Tag Lambda functions that do not need code scans. In the tag, include a key of InspectorCodeExclusion and a value of LambdaCodeScanning.","C":"Enable Amazon GuardDuty. Enable the Lambda Protection feature in GuardDuty.","B":"Activate Lambda standard scanning and Lambda code scanning in Amazon Inspector.","A":"Activate Amazon Inspector. Start automated CVE scans.","F":"Use Amazon Inspector to scan the 3 bucket that contains the Lambda .zip packages and the Lambda layer .zip file for code scans."},"timestamp":"2024-07-06 06:52:00","unix_timestamp":1720241520,"answers_community":["ABE (100%)"],"isMC":true,"answer_ET":"ABE","exam_id":33,"answer_images":[],"question_id":469,"question_images":[],"answer_description":"","discussion":[{"content":"Selected Answer: ABE\nA, B and E\nInspector for Lamda std scanning and code scanning\nLambda Function with monitor setting to code scan\nTag for conditional function, not for all functions","poster":"vip2","upvote_count":"5","comment_id":"1243183","timestamp":"1720241520.0"},{"comment_id":"1296989","upvote_count":"2","content":"Selected Answer: ABE\nA: Amazon Inspector can automatically scan your Lambda functions for known vulnerabilities (CVEs) in the dependencies of the functions. This action will initiate the security scanning of Lambda functions and Lambda layers to detect vulnerabilities.\nB: Amazon Inspector provides enhanced scanning features for Lambda functions. This includes both standard scanning (for CVEs in dependencies and layers) and code scanning (for potential vulnerabilities, like data leaks, directly in the code). \nE: https://docs.aws.amazon.com/lambda/latest/dg/governance-code-scanning.html#:~:text=To%20exclude%20a%20Lambda%20function,Value%3ALambdaStandardScanning. \n\"To exclude a Lambda function from code scans, tag the function with the following key-value pair:\nKey:InspectorCodeExclusion\nValue:LambdaCodeScanning\"","poster":"JoeTromundo","timestamp":"1728835080.0"},{"upvote_count":"2","content":"Selected Answer: ABE\nA: Need to Activate Amazon Inspector first\nB: For **CVE**, need to use **Lambda standard scanning**\nB: For **data leaks**, need to use Lambda code scanning\nE: Tag Lambda functions that do not need code scans","poster":"kgpoj","timestamp":"1726295700.0","comment_id":"1283514"},{"timestamp":"1725606480.0","comment_id":"1279448","poster":"guruguru","upvote_count":"1","content":"ABE, \nhttps://docs.aws.amazon.com/inspector/latest/user/scanning-lambda.html\nTo exclude a Lambda function from Lambda standard scanning, tag the function with the following key-value pair:\nKey:InspectorExclusion\nValue:LambdaStandardScanning"}],"topic":"1","answer":"ABE","url":"https://www.examtopics.com/discussions/amazon/view/143390-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"ZpvBe70FOCVXO7Z9Bl5n","unix_timestamp":1719952140,"answer_ET":"C","answers_community":["C (100%)"],"question_images":[],"answer":"C","exam_id":33,"timestamp":"2024-07-02 22:29:00","isMC":true,"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/143205-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Create a network ACL that blocks inbound traffic on port 80. Associate the network ACL with all subnets in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts.","C":"Create VPC endpoints for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a VPC peering connection to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts.","A":"Create a network ACL that blocks outbound traffic on port 80. Associate the network ACL with all subnets in the application account. In the application account and the core account, deploy one EC2 instance that runs a custom VPN server. Create a VPN tunnel to access the private VPC. Update the route table in the application account.","B":"Create private VIFs for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route table in the core account."},"answer_images":[],"discussion":[{"content":"Selected Answer: C\nWhy not B?","poster":"SIJUTHOMASP","timestamp":"1735342260.0","upvote_count":"1","comment_id":"1332681"},{"upvote_count":"2","poster":"Daniel76","content":"Selected Answer: C\nAftee delete NAT gateway theres no need to block outbound port 80. Use vpc interface endpoint to keep traffic private. \nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html","timestamp":"1726564920.0","comment_id":"1285118"},{"content":"C is right \nHere's why:\n\n\n\nThe company needs to prevent all EC2 instances in the application account from accessing the internet, which means they can't use a NAT gateway.\n\nThey need to access Amazon S3 and Systems Manager, so creating VPC endpoints for these services is the way to go.\n\nA VPC peering connection between the two accounts will allow the EC2 instances in the application account to access the patch source repository in the core account.\n\nUpdating the route tables in both accounts is necessary to ensure that traffic is properly routed.","timestamp":"1725119700.0","comment_id":"1275637","poster":"AzureDP900","upvote_count":"1"},{"content":"Selected Answer: C\nanswer : C","comment_id":"1241673","poster":"Alagong","timestamp":"1720042920.0","upvote_count":"3"},{"content":"A, D\nA block of Port.80 is not enough.\nB\nprivate VIFs is inadequate.\nThe correct answer is C.","comment_id":"1241029","upvote_count":"3","poster":"kupo777","timestamp":"1719952140.0"}],"question_id":470,"question_text":"A company is changing the way that it handles patching of Amazon EC2 instances in its application account. The company currently patches instances over the internet by using a NAT gateway in a VPC in the application account.\n\nThe company has EC2 instances set up as a patch source repository in a dedicated private VPC in a core account. The company wants to use AWS Systems Manager Patch Manager and the patch source repository in the core account to patch the EC2 instances in the application account. The company must prevent all EC2 instances in the application account from accessing the internet.\n\nThe EC2 instances in the application account need to access Amazon S3, where the application data is stored. These EC2 instances need connectivity to Systems Manager and to the patch source repository in the private VPC in the core account.\n\nWhich solution will meet these requirements?"}],"exam":{"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Amazon","name":"AWS Certified Solutions Architect - Professional SAP-C02","numberOfQuestions":529,"isMCOnly":true,"id":33,"isBeta":false},"currentPage":94},"__N_SSP":true}