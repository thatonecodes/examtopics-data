{"pageProps":{"questions":[{"id":"SrFvswl7D65n7EmCTDNn","discussion":[{"poster":"MarDog","comment_id":"938573","upvote_count":"16","content":"C. Look at #3 in the below. \nhttps://container-devsecops.awssecworkshops.com/04-testing/","comments":[{"timestamp":"1717629180.0","content":"Link is dead","poster":"Gomer","comment_id":"1225053","upvote_count":"1"}],"timestamp":"1688072880.0"},{"comment_id":"928327","content":"Selected Answer: B\nB, we need to run tests only when pull request is created and we need to publish test results, not only badge.","timestamp":"1687255380.0","poster":"madperro","upvote_count":"11"},{"upvote_count":"1","content":"Selected Answer: C\nC covers both event scenarios: PR creation and update (when updating the source branch, for example, with a new commit.","poster":"lgallard","comment_id":"1342284","timestamp":"1737137880.0"},{"timestamp":"1717629120.0","comment_id":"1225052","poster":"Gomer","content":"Selected Answer: C\n\"Automated Code Review on Pull Requests using AWS CodeCommit and AWS CodeBuild\"\n\"The solution comprises of the following components:\"\n\"Amazon EventBridge: AWS service to receive pullRequestCreated and pullRequestSourceBranchUpdated events and trigger Amazon EventBridge rule.\"\nhttps://aws.amazon.com/blogs/devops/automated-code-review-on-pull-requests-using-aws-codecommit-and-aws-codebuild/","upvote_count":"3"},{"poster":"seetpt","comment_id":"1205172","timestamp":"1714581720.0","content":"Selected Answer: C\ni go with C","upvote_count":"1"},{"comment_id":"1180026","timestamp":"1711107180.0","comments":[{"comment_id":"1256586","poster":"shammous","upvote_count":"1","timestamp":"1722142620.0","content":"I agree the C is the closed correct answer but it doesn't mention pullRequestStatusChanged (not sure why you mention it in your comment). \n\"The primary events in AWS CodeCommit that can trigger the pipeline are:\n- pullRequestCreated: This event occurs when a new pull request is created.\n- pullRequestSourceBranchUpdated: This event occurs when the source branch of an existing pull request is updated (e.g. when new commits are pushed to the branch).\"\nThe other events that might be considered but I would exclude are: \n- pullRequestStatusChanged: This event occurs when the status of a pull request changes, from closed to open (which we can consider), or from open to close (which we shouldn't consider in our case).\n- pullRequestMerged: This event occurs when a pull request is merged. I would exclude it because we are looking to test before merging."}],"poster":"zijo","upvote_count":"4","content":"C is the answer to ensure code reviewers more easily see the results of automated tests as part of the pull request review\npullRequestStatusChanged event is triggered whenever the status of a pull request changes. This could include transitions like: Open to Closed (pull request is merged or marked as closed)\nClosed to Open (pull request is reopened)\npullRequestCreated event is triggered whenever a new pull request is created in a CodeCommit repository.\npullRequestSourceBranchUpdated event is triggered whenever there are updates (new commits) pushed to the source branch of an open pull request"},{"content":"B is correct: we need to react when there is merge request (pullRequestCreated event)\nA: we need to react when there is merge request, not when the status of merge request is changed (pullRequestStatusChanged event)\nC: we only need to react when there is merge request, not when a sourcebranch is updated (pullRequestSourceBranchUpdated events)\nD: we need to react when there is merge request, not when the status of merge request is changed (pullRequestStatusChanged event)","timestamp":"1706693640.0","comments":[{"comment_id":"1242556","timestamp":"1720163400.0","upvote_count":"4","poster":"gg_robin","content":"If the source is updated after the PR is created, you don't run any tests against those changes."}],"comment_id":"1136589","poster":"thanhnv142","upvote_count":"2"},{"timestamp":"1703085780.0","comment_id":"1101679","poster":"DucSiu","content":"Why not B?","upvote_count":"2"},{"upvote_count":"2","content":"Answer is C: the pullRequestStatusChanged only has two values (OPEN|CLOSED) so If there is any update in the code the tests will not run.\n\nhttps://docs.aws.amazon.com/codecommit/latest/APIReference/API_PullRequestStatusChangedEventMetadata.html","timestamp":"1700570280.0","poster":"zolthar_z","comment_id":"1076286"},{"poster":"DZ_Ben","content":"Selected Answer: C\nI'll go for C. Tbh, I don't think we will need a lambda here as the event rule can definitely trigger the code pipeline & code build.","comment_id":"1060359","timestamp":"1698910440.0","upvote_count":"2"},{"timestamp":"1694391900.0","poster":"RVivek","comment_id":"1004371","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/devops/automated-code-review-on-pull-requests-using-aws-codecommit-and-aws-codebuild/","upvote_count":"4"},{"timestamp":"1692014940.0","upvote_count":"2","content":"Correct C.","poster":"ggrodskiy","comment_id":"980759"},{"upvote_count":"2","comment_id":"969738","poster":"vherman","content":"Selected Answer: C\nС \nrun tests on pull requests created and when source branch receives new commits to re-run tests","timestamp":"1690954740.0"},{"content":"Selected Answer: C\nNot sure why so much discussion. Triggers Rule: A CloudWatch Event Rule is triggered based on the following events: pullRequestSourceBranchUpdated or pullRequestCreated.\nC is only viable option. I mean it even tells you the answer in the question \"development team needed to perform rollbacks to branches in the codebase\".\nAns is C.","poster":"lunt","comment_id":"966489","upvote_count":"2","timestamp":"1690643160.0"},{"upvote_count":"3","poster":"tartarus23","comment_id":"927961","timestamp":"1687209780.0","content":"Selected Answer: C\nThis approach allows testing whenever a pull request is created or the source branch of a pull request is updated. When the tests are complete, the AWS Lambda function posts the test status badge as a comment on the pull request, providing visual feedback to reviewers directly in the context of the pull request review.\n\nIt's important to note that CodeBuild creates a build badge that provides status about the last build, which might not directly reflect the test results of the specific pull request. Posting the test results would provide more accurate and relevant information but doing so might require additional scripting or tooling not described in the available options."},{"content":"Selected Answer: C\nC is the correct IMHO.\n\nA pull request is just a branch that the requestor is asking to be merged in master/main. When you create a pull request you set the branch, that is the start, you have to use the current contents of the branch to execute the tests. When time passes developers add commits to that branch or force-push it, changing the contents of the PR's branch. That is the moment in which you have to trigger the tests. The PR comments and discussions may change, but that does not change the code so no need to perform new tests.\nYou only test when the PR is created and every time the branch is pushed (updated).","poster":"bcx","timestamp":"1685513340.0","upvote_count":"3","comment_id":"910895"},{"content":"Why not B?","poster":"youonebe","timestamp":"1685360820.0","comment_id":"909351","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: C\nD is incorrect.\n\npullRequestStatusChanged event\nIn this example event, a user who assumed a role named Admin with a session name of Mary_Major closed a pull request with the ID of 1. The pull request was not merged.\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/monitoring-events.html#pullRequestMergeStatusUpdated","poster":"qan1257","comment_id":"908983","timestamp":"1685327220.0"},{"comment_id":"898493","upvote_count":"3","timestamp":"1684167240.0","poster":"ipsingh","comments":[{"comment_id":"898494","timestamp":"1684167300.0","content":"C is right answer.","poster":"ipsingh","upvote_count":"2"}],"content":"D is Wring because pullRequestStatusChanged can trigger even when a PR is closed. Why should we run a test when a PR is closed. \n\nTest should run only when PR is raised or the source branch of the PR (yet to be merged) has changed."},{"upvote_count":"4","comment_id":"896686","content":"Selected Answer: C\nC is right, because if source branch is updated, tests must be rerun. Badge is Faled/Success in codecommit in PR, to block PR if build failed.","poster":"ele","timestamp":"1683983640.0"},{"content":"Selected Answer: D\nD is correct","upvote_count":"1","timestamp":"1683648060.0","comment_id":"893276","poster":"meisme"},{"comment_id":"888098","timestamp":"1683081900.0","poster":"haazybanj","upvote_count":"2","content":"Selected Answer: D\nTo automate testing of pull requests in CodeCommit and ensure that reviewers more easily see the results of automated tests as part of the pull request review, the DevOps engineer can use Amazon EventBridge to react to the pullRequestStatusChanged event and create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Then, the Lambda function can be programmed to post the CodeBuild test results as a comment on the pull request when the test results are complete. Therefore, the correct answer is option D."},{"upvote_count":"2","timestamp":"1682914860.0","comment_id":"885855","content":"Selected Answer: D\nD. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.","poster":"haazybanj"},{"upvote_count":"4","comment_id":"872169","timestamp":"1681678080.0","poster":"alce2020","content":"Selected Answer: C\nI'll go for C"},{"poster":"boledadian","timestamp":"1681507140.0","content":"go with D","comment_id":"870481","upvote_count":"1"},{"timestamp":"1681503540.0","poster":"jqso234","comments":[{"content":"why not B?","comment_id":"881075","poster":"[Removed]","upvote_count":"1","timestamp":"1682481720.0"}],"content":"Selected Answer: D\nD is likely the best answer for this scenario because it addresses the original requirement of automating testing of pull requests in CodeCommit and ensuring that reviewers see the results of automated tests as part of the pull request review. \nOption C reacts to pullRequestCreated and pullRequestSourceBranchUpdated events, but it also only posts the CodeBuild badge as a comment, which may not provide enough detail for reviewers to make an informed decision about the pull request.\n\nbased on the requirements of automating testing of pull requests in CodeCommit and ensuring that reviewers see the results of automated tests as part of the pull request review, D is the most suitable answer.","comment_id":"870455","upvote_count":"3"},{"poster":"Dimidrol","upvote_count":"5","content":"Selected Answer: C\nC for me. https://aws.amazon.com/ru/blogs/devops/validating-aws-codecommit-pull-requests-with-aws-codebuild-and-aws-lambda/","comment_id":"863683","timestamp":"1680859560.0"}],"answer_images":[],"question_text":"A development team uses AWS CodeCommit for version control for applications. The development team uses AWS CodePipeline, AWS CodeBuild. and AWS CodeDeploy for CI/CD infrastructure. In CodeCommit, the development team recently merged pull requests that did not pass long-running tests in the code base. The development team needed to perform rollbacks to branches in the codebase, resulting in lost time and wasted effort.\nA DevOps engineer must automate testing of pull requests in CodeCommit to ensure that reviewers more easily see the results of automated tests as part of the pull request review.\nWhat should the DevOps engineer do to meet this requirement?","question_id":306,"answers_community":["C (65%)","B (20%)","D (15%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/105492-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_ET":"C","topic":"1","answer":"C","choices":{"B":"Create an Amazon EventBridge rule that reacts to the pullRequestCreated event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.","D":"Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.","C":"Create an Amazon EventBridge rule that reacts to pullRequestCreated and pullRequestSourceBranchUpdated events. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.","A":"Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review."},"answer_description":"","timestamp":"2023-04-07 11:26:00","exam_id":23,"unix_timestamp":1680859560,"isMC":true},{"id":"T9Hjz8wGpmSW2icQdD9Y","question_text":"A company has deployed an application in a production VPC in a single AWS account. The application is popular and is experiencing heavy usage. The company’s security team wants to add additional security, such as AWS WAF, to the application deployment. However, the application's product manager is concerned about cost and does not want to approve the change unless the security team can prove that additional security is necessary.\nThe security team believes that some of the application's demand might come from users that have IP addresses that are on a deny list. The security team provides the deny list to a DevOps engineer. If any of the IP addresses on the deny list access the application, the security team wants to receive automated notification in near real time so that the security team can document that the application needs additional security. The DevOps engineer creates a VPC flow log for the production VPC.\nWhich set of additional steps should the DevOps engineer take to meet these requirements MOST cost-effectively?","question_id":307,"question_images":[],"discussion":[{"poster":"madperro","timestamp":"1703074140.0","content":"Selected Answer: A\nA meets the requirements at the lowest cost.","comment_id":"928334","upvote_count":"8"},{"poster":"habros","upvote_count":"5","comment_id":"946411","content":"Selected Answer: A\nopensearch cost a lot of $$$. Athena got tons of things to do afterwards. It's used purely for interactive query. \n\nnatively, vpcflow sends logs to s3 or cloudwatch logs. no brainer answer","timestamp":"1704718320.0"},{"timestamp":"1722425820.0","upvote_count":"2","poster":"thanhnv142","content":"A is correct: push all VPC flow log to cloudwatch logs. Create metric filter to find denied IP addresses. Create cloudwatch alarm with the metric filter as input. Alarm's action is send noti to Security team via SNS\nB: \"Configure the alert to automatically notify the security team\": alert cannot notify by itself. Must use SNS\nC: This option uses both S3 bucket and \"Amazon OpenSearch Service cluster\" to store log files, which would cost a lot of money and unnecessary \nD: This option uses both S3 bucket and VPC flow log to store log files, which is costly","comment_id":"1136790"},{"upvote_count":"2","content":"Selected Answer: A\nA most cost effective","comment_id":"896691","poster":"ele","timestamp":"1699888920.0"},{"poster":"haazybanj","content":"Selected Answer: A\nTo meet the requirements most cost-effectively, the DevOps engineer should create a log group in Amazon CloudWatch Logs and configure the VPC flow log to capture accepted traffic and to send the data to the log group. Then, create an Amazon CloudWatch metric filter for IP addresses on the deny list and create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Finally, use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.\n\nOption A is the correct answer. It provides a cost-effective solution that meets the requirements. The CloudWatch alarm notifies the security team in near real-time when traffic from an IP address on the deny list is detected. This will help the security team document that the application needs additional security. This solution only requires the use of AWS services that the company is already using, and does not require any additional services or tools.","timestamp":"1698987300.0","upvote_count":"4","comment_id":"888105"},{"poster":"haazybanj","timestamp":"1698819720.0","content":"D. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.","upvote_count":"1","comment_id":"885858","comments":[{"content":"Option D does not mention an event bridge rule","upvote_count":"1","timestamp":"1709670720.0","comment_id":"999800","poster":"BaburTurk","comments":[{"poster":"zolthar_z","content":"This answer is for a previous question","comment_id":"1077760","upvote_count":"1","timestamp":"1716400920.0"}]}]},{"upvote_count":"2","poster":"alce2020","content":"Selected Answer: A\nA seems correct","timestamp":"1697383440.0","comment_id":"871030"}],"exam_id":23,"url":"https://www.examtopics.com/discussions/amazon/view/106266-exam-aws-certified-devops-engineer-professional-dop-c02/","answer":"A","answer_ET":"A","choices":{"C":"Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture accepted traffic and to send the data to the S3 bucket. Configure an Amazon OpenSearch Service cluster and domain for the log files. Create an AWS Lambda function to retrieve the logs from the S3 bucket, format the logs, and load the logs into the OpenSearch Service cluster. Schedule the Lambda function to run every 5 minutes. Configure an alert and condition in OpenSearch Service to send alerts to the security team through an Amazon Simple Notification Service (Amazon SNS) topic when access from the IP addresses on the deny list is detected.","A":"Create a log group in Amazon CloudWatch Logs. Configure the VPC flow log to capture accepted traffic and to send the data to the log group. Create an Amazon CloudWatch metric filter for IP addresses on the deny list. Create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.","D":"Create a log group in Amazon CloudWatch Logs. Create an Amazon S3 bucket to hold query results. Configure the VPC flow log to capture all traffic and to send the data to the log group. Deploy an Amazon Athena CloudWatch connector in AWS Lambda. Connect the connector to the log group. Configure Athena to periodically query for all accepted traffic from the IP addresses on the deny list and to store the results in the S3 bucket. Configure an S3 event notification to automatically notify the security team through an Amazon Simple Notification Service (Amazon SNS) topic when new objects are added to the S3 bucket.","B":"Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture all traffic and to send the data to the S3 bucket. Configure Amazon Athena to return all log files in the S3 bucket for IP addresses on the deny list. Configure Amazon QuickSight to accept data from Athena and to publish the data as a dashboard that the security team can access. Create a threshold alert of 1 for successful access. Configure the alert to automatically notify the security team as frequently as possible when the alert threshold is met."},"unix_timestamp":1681572240,"isMC":true,"timestamp":"2023-04-15 17:24:00","answers_community":["A (100%)"],"answer_images":[],"answer_description":"","topic":"1"},{"id":"502rq6VFWOL1Zsy8xBK4","question_id":308,"exam_id":23,"discussion":[{"poster":"tartarus23","content":"Selected Answer: AE\nExplanation:\nThe manual approval action (A) will allow the QA team to inspect the build artifact and run their internal penetration testing tool before the deployment to the production environment proceeds.\n\nUsing an AWS Lambda function (E) would provide an automated way to call the REST API of the penetration testing tool. This would allow for the tests to be conducted automatically within the pipeline. This is beneficial because it ensures consistency in the testing process and could be run programmatically, reducing manual steps.","timestamp":"1687209960.0","comment_id":"927963","upvote_count":"8"},{"content":"Selected Answer: AE\nOption D (updating the pipeline to directly call the REST API for the penetration testing tool) is not recommended because it tightly couples the pipeline with the QA team's tool, making it less flexible and harder to maintain. Using a Lambda function as an intermediary provides better separation of concerns and easier maintainability.","poster":"iulian0585","upvote_count":"2","timestamp":"1722934920.0","comment_id":"1261567"},{"upvote_count":"1","comments":[{"timestamp":"1722502920.0","comment_id":"1259275","content":"For option A, keywords: conduct manual tests","poster":"jamesf","upvote_count":"1"}],"comment_id":"1255431","poster":"jamesf","timestamp":"1721969820.0","content":"Selected Answer: AE\nShould be AE\nAlthough there are limitation 15mins of Lambda function. \nBut Option D is wrong as CodePipeline does not have the ability to execute HTTP requests \"directly\".\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html"},{"comment_id":"1180141","timestamp":"1711122660.0","poster":"zijo","content":"This is tricky but AD should be a better choice because of the 15 min timeout of Lambda functions. To call REST API in CodePipeline these are the two options\nFor complex API calls, security requirements, and access to external resources, an AWS Lambda function is the recommended approach.\nFor simple API calls with limited requirements, consider the inline script approach within CodeBuild, but with caution due to security and maintainability limitations.","upvote_count":"1"},{"poster":"Shasha1","timestamp":"1709118840.0","comment_id":"1161551","upvote_count":"4","content":"AE there is no way to call REST API directly in the code pipeline, it is possible invoke via Lambda function only"},{"comment_id":"1156276","upvote_count":"3","content":"Selected Answer: AE\nCodePipeline does not have the ability to execute HTTP requests \"directly\".","timestamp":"1708594620.0","poster":"dzn"},{"timestamp":"1706708880.0","comment_id":"1136802","content":"A and D are correct: a manual approval action between the test actions and deployment actions allows tester to verify and test built artifacts before allowing deploying to production \nB and C: no mentions of test and deployment env\nE: manual test take more than 15 minutes, which is the maximum execution time of lambda","poster":"thanhnv142","upvote_count":"1"},{"upvote_count":"2","poster":"a54b16f","comment_id":"1123483","content":"Selected Answer: AE\nD is wrong, alternative option ( not using Lambda, for example, if the pen testing will take more than 15 minutes) is using codebuild, either add a new codebuild for pen testing, or update existing unit testing codebuild to include pen testing. You should never run Pen testing inside codepipeline directly , it lacks the hooks to collect test result, inform result, etc","timestamp":"1705332060.0"},{"comments":[{"poster":"shammous","content":"The lambda function would just invoke the REST API, it won't execute the pen test itself. An asynchronous mechanism involving SQS could handle the waiting time between the requesting sending and the response receiving, which can indeed last more than 15mn.","timestamp":"1722146280.0","upvote_count":"1","comment_id":"1256600"}],"upvote_count":"4","comment_id":"1063758","content":"Selected Answer: AD\nTricky one:\nCodeDeploy can't do actions directly like invoke REST API but code Build can.\ne.g. it's mentioned to test build artifacts.. So after the build artifact is created This means the solution uses Code Build even not from Code Build you can setup a python script and run it directly using Code Build Command: \nI'd not use Lambda as an alternative due to the time taken for penetration tests would take more than 15 mins. and the pipeline would failed with Lambda execution timeout.","timestamp":"1699269000.0","poster":"2pk"},{"comment_id":"980593","timestamp":"1692000480.0","content":"conducting manual tests might takes more than 15m.","poster":"Seoyong","upvote_count":"1"},{"content":"A, \nE in practice is a cheap and handy off-switch, recommended, for some contributors to CI/CD that we don't control directly. However, no idea what the writer of the question wanted.","comment_id":"969680","poster":"s50600822","upvote_count":"1","timestamp":"1690951380.0"},{"comment_id":"962508","timestamp":"1690273500.0","content":"why don't you choose D","poster":"DavidPham","upvote_count":"1"},{"content":"Selected Answer: AE\nI’ll choose AE. I can tie up multiple REST calls in a Lambda and customize it as I wish. A web hook is not flexible in this aspect I feel.","timestamp":"1689119880.0","upvote_count":"2","comment_id":"949371","poster":"habros"},{"timestamp":"1687255920.0","comment_id":"928337","poster":"madperro","comments":[{"poster":"cocegas","timestamp":"1693525500.0","comment_id":"995543","upvote_count":"1","content":"But there is no option to invoke call an API directly = https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html#integrations-invoke"}],"upvote_count":"1","content":"Selected Answer: AD\nAD, Lambda is not needed, a webhook can call REST API directly."},{"comment_id":"910906","content":"Selected Answer: AE\n\"AWS Lambda is a compute service that lets you run code without provisioning or managing servers. You can create Lambda functions and add them as actions in your pipelines. Because Lambda allows you to write functions to perform almost any task, you can customize the way your pipeline works. \"\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html","timestamp":"1685514180.0","upvote_count":"3","poster":"bcx"},{"content":"Selected Answer: AD\nA & D, lambda (E) is extra and not needed.","poster":"qsergii","timestamp":"1685348460.0","upvote_count":"2","comment_id":"909192"},{"poster":"Akaza","upvote_count":"2","timestamp":"1684916340.0","comment_id":"905634","content":"Selected Answer: AE\nYepp A, E for me"},{"comment_id":"885860","upvote_count":"3","timestamp":"1682915040.0","poster":"haazybanj","content":"Selected Answer: AE\nA & E look right"},{"content":"Selected Answer: AE\nA & E sound right","poster":"alce2020","upvote_count":"2","timestamp":"1681571820.0","comment_id":"871026"}],"answer":"AE","unix_timestamp":1681571820,"topic":"1","answers_community":["AE (80%)","AD (20%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/106265-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"E":"Update the pipeline to invoke an AWS Lambda function that calls the REST API for the penetration testing tool.","D":"Update the pipeline to directly call the REST API for the penetration testing tool.","B":"Modify the buildspec.yml file for the compilation stage to require manual approval before completion.","A":"Insert a manual approval action between the test actions and deployment actions of the pipeline.","C":"Update the CodeDeploy deployment groups so that they require manual approval to proceed."},"answer_ET":"AE","isMC":true,"answer_description":"","question_images":[],"question_text":"A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps:\n1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests.\n2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment.\n3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment.\nThe quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call.\nWhich combination of actions should the DevOps engineer take to fulfill this request? (Choose two.)","timestamp":"2023-04-15 17:17:00"},{"id":"Bneq5TmS47od5mrbrwaa","timestamp":"2023-04-15 17:12:00","question_images":[],"question_id":309,"discussion":[{"content":"Selected Answer: A\nI think it is A, We can have failover with CloudFront, but it can't have weighted routing, Here is the link of how auromatic failover works in the CloudFront\n\nhttps://disaster-recovery.workshop.aws/en/services/networking/cloudfront/cloudfront-origin-group.html","poster":"davdan99","timestamp":"1704646620.0","upvote_count":"7","comment_id":"1116025"},{"content":"A\nKeywords: web application - ElasticBeanstalk, weighted routing required. \nDynamoDB Global Table required.\n\nAs understand, Cloudfront not support weighted routing.","comment_id":"1255436","upvote_count":"3","poster":"jamesf","timestamp":"1721970720.0"},{"upvote_count":"1","poster":"auxwww","content":"Selected Answer: A\nA - correct - Elasticbeanstalk - option of ALB to register route53 with active-active alias with health checks and weighted routing\n\"In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record.\"\n\n\nD - incorrect because no ALB in front of ASG.","timestamp":"1721786220.0","comment_id":"1254047"},{"comments":[{"upvote_count":"1","content":"Just to clarify, the scenarios is absolutely desrivinb \"weighted routing\" with 99% to 1% traffic split between regions for normal operation (unbalanced Active/Active).","poster":"Gomer","timestamp":"1717708080.0","comment_id":"1225781"}],"poster":"Gomer","upvote_count":"1","content":"Selected Answer: D\nIf the requirement is for \"1% of requests should route to the secondary region to continuously\", then that means the secondary region is in continuously in an Active state (Active/Active). A \"request\" is not a health check. You also have to have auto-scaling to dynamically pick up any extra traffic. The question is a little weird, in I don't know you you dynamically adjust the weighted routing policy to steer all traffice to the secondary region. I just know that \"D\" is the closest choice to meeting the specified requirements. This is absolutely and \"Active-Active\" design using weighted routing at some level, and auto-scaling just meets the demaind wherever it comes from. I think \"latency-based\" routing would make more sence, but the requirements are clearly describing \"weighted routing\" and Active-Active design.\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/latency-based-routing-leveraging-amazon-cloudfront-for-a-multi-region-active-active-architecture/","comment_id":"1225778","timestamp":"1717707240.0"},{"poster":"seetpt","timestamp":"1714582500.0","content":"Selected Answer: A\nA is ok","comment_id":"1205174","upvote_count":"1"},{"poster":"DanShone","upvote_count":"2","content":"Selected Answer: A\nA is correct\nB + C no DynamoDB Global Tables\nD - does not use Route53","timestamp":"1710673080.0","comment_id":"1175736"},{"poster":"thanhnv142","timestamp":"1706709000.0","comment_id":"1136805","upvote_count":"2","content":"A is correct: beanstalk is literally designed for this specific purpose"},{"comment_id":"1113638","poster":"Jaguaroooo","content":"It is A, 1% of the traffic should be going to the 2ndary site. so that's weighted routing.","upvote_count":"2","timestamp":"1704369180.0"},{"poster":"DucSiu","comments":[{"content":"We have to use DynamoDB Global tables for make db acessable from 2 regions.","poster":"davdan99","upvote_count":"1","timestamp":"1704646800.0","comment_id":"1116028"}],"upvote_count":"1","timestamp":"1703173980.0","comment_id":"1102696","content":"Why not B?"},{"poster":"denccc","comment_id":"1061206","content":"It's A","timestamp":"1699002480.0","upvote_count":"2"},{"comment_id":"1060071","timestamp":"1698874440.0","upvote_count":"1","content":"A is correct answer","poster":"rahulsingha2112"},{"timestamp":"1698010260.0","poster":"ekki","upvote_count":"1","content":"Answer is D.\n\"Testing Regional failover\"\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/latency-based-routing-leveraging-amazon-cloudfront-for-a-multi-region-active-active-architecture/","comment_id":"1051128","comments":[{"poster":"zijo","upvote_count":"1","content":"The title of this page mentions Active-Active scenario and not Active-Passive as mentioned in this question.","timestamp":"1711123260.0","comment_id":"1180151"}]},{"timestamp":"1693939380.0","upvote_count":"1","comments":[{"timestamp":"1697722860.0","poster":"BaburTurk","upvote_count":"2","content":"A- Route 53 does offer the capability to automatically route traffic to the secondary region in case of a disruption. In the context of the requirements specified, option A seems to be a feasible solution as it involves the use of AWS Elastic Beanstalk for deployment, DynamoDB global tables for session data replication, and a weighted routing policy in Route 53 for traffic distribution across regions. The health checks can ensure that traffic is routed to the secondary region automatically in case of a disruption in the main region.\n\nTherefore, considering the ability of Route 53 to automatically reroute traffic, option A appears to be the most appropriate solution for meeting the specified disaster recovery requirements.","comment_id":"1047961"}],"comment_id":"999808","poster":"BaburTurk","content":"Selected Answer: D\nOption A uses Elastic Beanstalk, which is not as scalable as Auto Scaling groups.\nD is correct"},{"poster":"mamila","upvote_count":"1","content":"The answer is NONE of them, none of them specified both weighted and failover routing policies.","comment_id":"967381","timestamp":"1690740060.0"},{"comment_id":"953239","content":"Selected Answer: A\nD is not offering scaling on DRP. A offers scaling by using BeanStalk.","upvote_count":"2","timestamp":"1689504360.0","poster":"totopopo"},{"comment_id":"939196","upvote_count":"4","timestamp":"1688140140.0","poster":"csG13","content":"Selected Answer: A\nGoing for A given that DynamoDB global tables can replicate data across selected regions in near real-time. Clearly weighting and failover, thus Route53 should be selected. \n\nIt's not D because Cloudfront cannot do weighted routing."},{"poster":"madperro","content":"Selected Answer: D\nA and D are very similar but with using different services (BeanStalk vs CloudFront). However A is using R53 traffic distribution and D is using CF traffic distribution. I think D is better in this case. Note that not all applications will run easily on BeanStalk too.","timestamp":"1687322700.0","upvote_count":"3","comment_id":"929053"},{"poster":"Flyingdagger","timestamp":"1685416740.0","upvote_count":"2","comment_id":"909836","content":"Answer seems to be B as we need to do automatic \nfailover"},{"content":"How about D you guys? \nOption D provides a comprehensive solution that covers all the specified requirements:\n\nLaunching the application in Auto Scaling groups ensures scalability and resilience in both regions.\n\nUsing DynamoDB global tables allows for near-real-time replication of session data between the regions, ensuring data consistency.\n\nEnabling an Amazon CloudFront weighted distribution across regions allows for the routing of a percentage of requests to the secondary region for continuous verification of system functionality.\n\nPointing the Amazon Route 53 DNS record at the CloudFront distribution enables automatic traffic routing between the regions based on the weighted distribution configuration","timestamp":"1684916640.0","poster":"Akaza","comments":[{"upvote_count":"2","timestamp":"1685069580.0","content":"I saw absolutely no reason for ALB or Cloudfront, according to the requirements in the question. All the requirements for a DR can be fulfilled by the services that were described in Answer A.\nI hope it helps!","comment_id":"907007","poster":"TroyMcLure"}],"upvote_count":"1","comment_id":"905639"},{"poster":"haazybanj","upvote_count":"3","content":"Selected Answer: A\nA looks good","comment_id":"885862","timestamp":"1682915100.0"},{"comment_id":"871022","upvote_count":"2","timestamp":"1681571520.0","content":"Selected Answer: A\nA is correct","poster":"alce2020"}],"topic":"1","answer_ET":"A","question_text":"A company is hosting a web application in an AWS Region. For disaster recovery purposes, a second region is being used as a standby. Disaster recovery requirements state that session data must be replicated between regions in near-real time and 1% of requests should route to the secondary region to continuously verify system functionality. Additionally, if there is a disruption in service in the main region, traffic should be automatically routed to the secondary region, and the secondary region must be able to scale up to handle all traffic.\nHow should a DevOps engineer meet these requirements?","answer_images":[],"answer_description":"","exam_id":23,"unix_timestamp":1681571520,"answers_community":["A (81%)","D (19%)"],"choices":{"B":"In both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing policy with health checks to distribute the traffic across the regions.","A":"In both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB global tables for session data. Use an Amazon Route 53 weighted routing policy with health checks to distribute the traffic across the regions.","C":"In both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and use Amazon RDS for PostgreSQL with cross-region replication for session data. Deploy the web application with client-side logic to call the API Gateway directly.","D":"In both regions, launch the application in Auto Scaling groups and use DynamoDB global tables for session data. Enable an Amazon CloudFront weighted distribution across regions. Point the Amazon Route 53 DNS record at the CloudFront distribution."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/106264-exam-aws-certified-devops-engineer-professional-dop-c02/","answer":"A"},{"id":"l6bJPc79MHZWaCuzGoy2","answer_images":[],"isMC":true,"question_text":"A company runs an application on Amazon EC2 instances. The company uses a series of AWS CloudFormation stacks to define the application resources. A developer performs updates by building and testing the application on a laptop and then uploading the build output and CloudFormation stack templates to Amazon S3. The developer's peers review the changes before the developer performs the CloudFormation stack update and installs a new version of the application onto the EC2 instances.\nThe deployment process is prone to errors and is time-consuming when the developer updates each EC2 instance with the new application. The company wants to automate as much of the application deployment process as possible while retaining a final manual approval step before the modification of the application or resources.\nThe company already has moved the source code for the application and the CloudFormation templates to AWS CodeCommit. The company also has created an AWS CodeBuild project to build and test the application.\nWhich combination of steps will meet the company’s requirements? (Choose two.)","answer_description":"","topic":"1","answers_community":["AD (68%)","BD (29%)","2%"],"answer":"AD","url":"https://www.examtopics.com/discussions/amazon/view/106441-exam-aws-certified-devops-engineer-professional-dop-c02/","question_id":310,"unix_timestamp":1681678620,"timestamp":"2023-04-16 22:57:00","exam_id":23,"discussion":[{"content":"Selected Answer: AD\n(A) This step sets up the environment to use AWS CodeDeploy for application deployments. CodeDeploy uses an agent installed on the EC2 instances to perform the deployment tasks.\n\n(D) This option uses CodePipeline to orchestrate the process. CodeBuild is used to build and test the application. CloudFormation is used to prepare the infrastructure updates as change sets. A manual approval step is inserted before applying the changes. After approval, the CloudFormation change sets are applied, and then CodeDeploy is invoked to deploy the new version of the application to the EC2 instances.","poster":"tartarus23","timestamp":"1687210140.0","comment_id":"927965","upvote_count":"15"},{"comments":[{"comment_id":"987307","upvote_count":"8","timestamp":"1692697860.0","comments":[{"upvote_count":"3","poster":"Karamen","timestamp":"1698368460.0","content":"@fanq10 \nyou are right.\nCodeDeploy doesn't require registering EC2 instances, it fillters by tag","comment_id":"1055023"}],"content":"- EC2 needs to install the CodeDeploy agent.\n- CodeDeploy does not need to register EC2 instances, instead of it uses tag filter.\nTherefore, B is incorrect, A is correct. Final answer: AD","poster":"fanq10"}],"comment_id":"985101","content":"Selected Answer: BD\nThere is no application group in CodeDeploy","poster":"ky11223344","upvote_count":"5","timestamp":"1692439200.0"},{"timestamp":"1743285660.0","comment_id":"1411878","content":"Selected Answer: CD\nBoth options automate the application deployment process, trigger manual approval, and ensure controlled updates and deployments to EC2 instances using CodePipeline, CodeBuild, CloudFormation, and CodeDeploy","poster":"Srikantha","upvote_count":"1"},{"upvote_count":"2","comment_id":"1255452","timestamp":"1721971200.0","content":"Selected Answer: AD\nAD\nA - To run CodeDeploy on EC2, need agent. \nD - The approval step will trigger both CloudFormation and CodeDeploy\n\nB incorrect as no mention of installing agent on EC2 and CodeDeploy doesn't require registering EC2 instances, it filters by tag. \nC incorrect as The approval step does not affect CloudFormation updates, which is not accepted\nE incorrect as The approval step only allows CodeDeploy but no have CloudFormation updates","poster":"jamesf"},{"upvote_count":"2","timestamp":"1720852200.0","comment_id":"1247143","content":"Selected Answer: BD\nB:\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\nYou can configure automatic installation and updates of the CodeDeploy agent when you create your deployment group in the console.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/applications.html\nAfter you configure instances, but before you can deploy a revision, you must create an application in CodeDeploy. An application is simply a name or container used by CodeDeploy to ensure the correct revision, deployment configuration, and deployment group are referenced during a deployment.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/application-revisions.html\nIn CodeDeploy, a revision contains a version of the source files CodeDeploy will deploy to your instances or scripts CodeDeploy will run on your instances.\nhttps://aws.amazon.com/ru/blogs/devops/using-codedeploy-environment-variables/","poster":"alex_heavy"},{"poster":"xdkonorek2","comment_id":"1239740","timestamp":"1719764760.0","upvote_count":"1","content":"Selected Answer: AD\nfor those who vote B: what is creating environment in code deploy?\n\nI think application group means application and registering instances with code deploy is basically creating deployment group, and instance is not registered manually it has to be tagged"},{"poster":"seetpt","comment_id":"1205175","content":"Selected Answer: AD\nAD is ok","upvote_count":"1","timestamp":"1714582620.0"},{"comment_id":"1168869","timestamp":"1709908260.0","poster":"4555894","upvote_count":"3","content":"Selected Answer: AD\nA- https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html \nD - This option correctly utilizes AWS CodePipeline to invoke the CodeBuild job and create CloudFormationchange sets. It adds a manual approval step before executing the change sets and starting the AWSCodeDeploy deployment. This ensures that the deployment process is automated while retaining the\nfinal manual approval step."},{"upvote_count":"1","poster":"Shasha1","content":"AD\nNeeds to install code deploy agent and give necessary permission for access S3 bucket where it will be stored the application revision. then EC2 instance will download the application revision from the S3 bucket. Therefore Answer A is correct. if we use System manager only the EC2 instance can be installed and updated automatically.","timestamp":"1709124000.0","comment_id":"1161624"},{"poster":"thanhnv142","upvote_count":"2","timestamp":"1706709660.0","comment_id":"1136813","content":"A and D are correct: A - To run codedploy on EC2, need agent. D - The approval step will trigger both cloudformation and codedeploy\nB: no mention of installing agent on EC2\nC: The approval step doesnot affect CloudFormation updates, which is not accepted\nE: The approval step only allows codedeploy but not CloudFormation updates"},{"poster":"Jaguaroooo","upvote_count":"1","timestamp":"1704370080.0","content":"AD is the correct answer, you need the CD agent in order to use code deploy. And D is correct also because you can do your testing during codebuild and finally do a change set review and then approval.","comment_id":"1113650"},{"comment_id":"1063749","poster":"2pk","upvote_count":"4","comments":[{"poster":"Jaguaroooo","upvote_count":"1","timestamp":"1704370260.0","content":"i agree with you on the explanation that there are no application group. but how can you use code deploy to push code to the EC2's if they have no agents on them?","comment_id":"1113653"}],"timestamp":"1699267980.0","content":"Selected Answer: BD\nCode deploy agent installation can be skipped when you setting up Code Deploy group .. e.g.\nYou can configure automatic installation and updates of the CodeDeploy agent when you create your deployment group in the console.\nWhy A is wrong - cause there is no application group only deployment group and when setting up deployment group you can setup agent installation automatically.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-create.html#:~:text=Note-,You%20can%20configure%20automatic%20installation%20and%20updates%20of%20the%20CodeDeploy%20agent%20when%20you%20create%20your%20deployment%20group%20in%20the%20console.,-Did%20this%20page"},{"upvote_count":"2","comment_id":"929059","poster":"madperro","content":"Selected Answer: AD\nAD is right.","timestamp":"1687323240.0"},{"content":"The CodeDeploy agent must be installed on your Amazon EC2 instance before using it in CodeDeploy deployments","timestamp":"1686039540.0","upvote_count":"1","poster":"noriknic","comment_id":"916041"},{"upvote_count":"2","content":"There is nothing like application group in code deploy answer is BD","timestamp":"1685417040.0","comment_id":"909839","poster":"Flyingdagger"},{"upvote_count":"2","comment_id":"896714","content":"Selected Answer: AD\nAD need codedeploy agent, and review CFN changes set, before run update","poster":"ele","timestamp":"1683985500.0"},{"upvote_count":"2","comment_id":"891222","content":"Selected Answer: AD\nA and D are correct","poster":"PhuocT","timestamp":"1683444720.0"},{"timestamp":"1681678620.0","comment_id":"872175","poster":"alce2020","upvote_count":"1","content":"Selected Answer: BD\nB and D are correct"}],"question_images":[],"answer_ET":"AD","choices":{"A":"Create an application group and a deployment group in AWS CodeDeploy. Install the CodeDeploy agent on the EC2 instances.","B":"Create an application revision and a deployment group in AWS CodeDeploy. Create an environment in CodeDeploy. Register the EC2 instances to the CodeDeploy environment.","E":"Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.","C":"Use AWS CodePipeline to invoke the CodeBuild job, run the CloudFormation update, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.","D":"Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, run the CloudFormation change sets and start the AWS CodeDeploy deployment."}}],"exam":{"isMCOnly":true,"numberOfQuestions":355,"lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Amazon","id":23,"name":"AWS Certified DevOps Engineer - Professional DOP-C02","isBeta":false},"currentPage":62},"__N_SSP":true}