{"pageProps":{"questions":[{"id":"HOGCni5Cm6cTOgnFoQby","question_images":[],"answers_community":["C (100%)"],"timestamp":"2023-05-18 20:03:00","question_id":451,"isMC":true,"question_text":"A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC.\n\nWhich storage solution meets these requirements?","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/109665-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"C","answer":"C","topic":"1","answer_images":[],"unix_timestamp":1684432980,"discussion":[{"comments":[{"comments":[{"content":"D is not valid answer because \"the solution must be highly available\" then it must be multi-AZ: every AZ requires a mount target","upvote_count":"1","timestamp":"1735026600.0","poster":"Salilgen","comment_id":"1331052"}],"poster":"dkw2342","timestamp":"1726045800.0","comment_id":"1170954","content":"\"A single mount target can only be used to mount the file system on a single EC2 instance. Multiple access points are used to provide access to the file system from different VPCs.\"\n\nThis is clearly wrong. You can have exactly one EFS mount target per subnet (AZ), and of course this mount target can be used by many clients (EC2 instances, containers etc.) - see diagram here for example: https://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html\n\nIn my opinion, C and D are equally valid answers.","upvote_count":"2"},{"upvote_count":"2","comment_id":"1054492","poster":"unbendable","timestamp":"1714130220.0","content":"Amazon FSx ONTAP supports clients mounting it with NFS. https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/attach-linux-client.html. Though A is not clear about which FSx product is used"}],"poster":"Felix_br","content":"Selected Answer: C\nThe other options are incorrect for the following reasons:\n\nA. Amazon FSx Multi-AZ deployments Amazon FSx is a managed file system service that provides access to file systems that are hosted on Amazon EC2 instances. Amazon FSx does not support native protocols, such as NFS.\nB. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes Amazon EBS is a block storage service that provides durable, block-level storage volumes for use with Amazon EC2 instances. Amazon EBS Multi-Attach volumes can be attached to multiple EC2 instances at the same time, but they cannot be mounted by multiple Linux instances through native protocols, such as NFS.\nD. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points A single mount target can only be used to mount the file system on a single EC2 instance. Multiple access points are used to provide access to the file system from different VPCs.","upvote_count":"12","timestamp":"1701704340.0","comment_id":"914682"},{"timestamp":"1700337780.0","poster":"cloudenthusiast","upvote_count":"9","content":"Selected Answer: C\nAmazon EFS is a fully managed file system service that provides scalable, shared storage for Amazon EC2 instances. It supports the Network File System version 4 (NFSv4) protocol, which is a native protocol for Linux-based systems. EFS is designed to be highly available, durable, and scalable.","comment_id":"901434"},{"timestamp":"1720465500.0","poster":"awsgeek75","upvote_count":"3","content":"Selected Answer: C\nA: FSx is a File Server, not a mountable file system\nB: EBS can't be mounted on on-prem devices\nD: Access points are not same as mount points\nC: EFS support multi mount targets and on-prem devices: https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-mount-helper-direct.html","comment_id":"1117012"},{"upvote_count":"3","comment_id":"1045583","timestamp":"1713332400.0","content":"EFS POSIX LINUX","poster":"iwannabeawsgod"},{"poster":"Guru4Cloud","content":"Selected Answer: C\nC. Amazon Elastic File System (Amazon EFS) with multiple mount targets","comment_id":"988447","upvote_count":"3","timestamp":"1708710000.0"},{"poster":"boubie44","timestamp":"1700880000.0","comments":[{"upvote_count":"3","poster":"lucdt4","timestamp":"1701107520.0","content":"the requirement is mountable by multiple Linux \n-> C (multiple mount targets)","comment_id":"908081"}],"content":"i don't understand why not D?","comment_id":"906225","upvote_count":"1"}],"choices":{"B":"Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes","A":"Amazon FSx Multi-AZ deployments","D":"Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points","C":"Amazon Elastic File System (Amazon EFS) with multiple mount targets"},"exam_id":31},{"id":"d2y9VH7RusoRCknglFtV","answer_description":"","unix_timestamp":1684317240,"isMC":true,"question_images":[],"answer_ET":"C","exam_id":31,"question_text":"A 4-year-old media company is using the AWS Organizations all features feature set to organize its AWS accounts. According to the company's finance team, the billing information on the member accounts must not be accessible to anyone, including the root user of the member accounts.\n\nWhich solution will meet these requirements?","choices":{"D":"Convert from the Organizations all features feature set to the Organizations consolidated billing feature set.","C":"Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to the root organizational unit (OU).","B":"Attach an identity-based policy to deny access to the billing information to all users, including the root user.","A":"Add all finance team users to an IAM group. Attach an AWS managed policy named Billing to the group."},"discussion":[{"upvote_count":"8","timestamp":"1700337840.0","poster":"cloudenthusiast","content":"Selected Answer: C\nService Control Policies (SCP): SCPs are an integral part of AWS Organizations and allow you to set fine-grained permissions on the organizational units (OUs) within your AWS Organization. SCPs provide central control over the maximum permissions that can be granted to member accounts, including the root user.\n\nDenying Access to Billing Information: By creating an SCP and attaching it to the root OU, you can explicitly deny access to billing information for all accounts within the organization. SCPs can be used to restrict access to various AWS services and actions, including billing-related services.\n\nGranular Control: SCPs enable you to define specific permissions and restrictions at the organizational unit level. By denying access to billing information at the root OU, you can ensure that no member accounts, including root users, have access to the billing information.","comment_id":"901436"},{"upvote_count":"5","comments":[{"comment_id":"1208309","poster":"TwinSpark","timestamp":"1731066960.0","upvote_count":"3","content":"i can understand this information coming from the famous course in udemy. I tought same, but after some research i now think it is a wrong information.\n\"SCPs affect all users and roles in attached accounts, including the root user. The only exceptions are those described in Tasks and entities not restricted by SCPs.\"\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html#:~:text=SCPs%20affect%20all%20users%20and,affect%20any%20service%2Dlinked%20role."}],"poster":"Kiki_Pass","comment_id":"968018","timestamp":"1706707800.0","content":"but SCP do not apply to the management account (full admin power)?"},{"comment_id":"1063249","timestamp":"1714933560.0","content":"Selected Answer: C\nSCP is for authorization","poster":"potomac","upvote_count":"3"},{"timestamp":"1708709160.0","poster":"Guru4Cloud","content":"Selected Answer: C\nC. Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to the root organizational unit (OU)","comment_id":"988436","upvote_count":"3"},{"content":"C Crt 100%","upvote_count":"2","poster":"PRASAD180","timestamp":"1704111540.0","comment_id":"939828"},{"poster":"TariqKipkemei","content":"Selected Answer: C\nService control policy are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled.","timestamp":"1703570940.0","comment_id":"934026","upvote_count":"4"},{"poster":"Abrar2022","upvote_count":"2","content":"By denying access to billing information at the root OU, you can ensure that no member accounts, including root users, have access to the billing information.","comment_id":"914762","timestamp":"1701709260.0"},{"timestamp":"1700222040.0","upvote_count":"2","content":"Selected Answer: C\nc for me","comment_id":"899950","poster":"nosense"}],"answer":"C","answers_community":["C (100%)"],"timestamp":"2023-05-17 11:54:00","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/109509-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":452,"answer_images":[]},{"id":"9w8H2Tyie8NPjqCCLEif","isMC":true,"answer_images":[],"unix_timestamp":1684398060,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/109637-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C","choices":{"C":"Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.","B":"Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.","D":"Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days.","A":"Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days."},"topic":"1","question_text":"An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received.\n\nA solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days.\n\nWhich solution will meet these requirements with the LEAST development effort?","answers_community":["C (80%)","B (20%)"],"discussion":[{"upvote_count":"10","poster":"pentium75","timestamp":"1719810540.0","content":"Selected Answer: C\n\"Configuring an Amazon SNS dead-letter queue for a subscription ... \nA dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can target for messages that can't be delivered to subscribers successfully\", this is exactly what C says. https://docs.aws.amazon.com/sns/latest/dg/sns-configure-dead-letter-queue.html\n\nB, an SQS queue \"between the application and Amazon SNS\" would change the application logic. SQS cannot push messages to the \"on-premises https endpoint\", rather the destination would have to retrieve messages from the queue. Besides, option B would eventually deliver the messages that failed on the first attempt, which is NOT what is asked for. The goal is to retain undeliverable messages for analysis (NOT to deliver them), and this is typically achieved with a dead letter queue.","comment_id":"1110995"},{"poster":"awsgeek75","timestamp":"1720465980.0","content":"Selected Answer: C\nLEAST development effort!\nA: Custom dead letter queue using Kinesis Data Stream (laughable solution!) so lots of coding\nB: Change app logic to put SQS between SNS and the app. Also too much coding\nD: Same as A, too much code change\nC: SNS dead letter queue is by default a SQS que so no coding required","upvote_count":"5","comment_id":"1117018"},{"timestamp":"1721092260.0","comment_id":"1123825","upvote_count":"3","content":"A dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can target for messages that can't be delivered to subscribers successfully.https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html","poster":"osmk"},{"timestamp":"1718559360.0","content":"Selected Answer: C\nProblem here SNS dead letter queue is a SQS queue, so technically speaking both B and C are right. But I suppose that they want us to speak about SNS dead letter queue, that nobody do... meh, frustrating.","comments":[{"upvote_count":"6","comment_id":"1098463","timestamp":"1718559720.0","content":"Aaaah ok.\n\nSo with B == you place the SQS queue between the application and the SNS topic\nwith C == you place the SQS queue as a DLQ for the SNS topic\n\nOf course it's C !","poster":"Mikado211"}],"poster":"Mikado211","upvote_count":"4","comment_id":"1098462"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-configure-dead-letter-queue.html","poster":"aws94","timestamp":"1718143920.0","upvote_count":"3","comment_id":"1093948"},{"upvote_count":"2","comment_id":"1052499","poster":"daniel1","content":"Selected Answer: C\nGPT4 to the rescue:\nThe most appropriate solution would be to configure an Amazon SNS dead letter queue with an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days (Option C). This setup would ensure that any undelivered messages are retained in the SQS queue for up to 14 days for analysis, with minimal development effort required.","comments":[{"upvote_count":"9","timestamp":"1717194720.0","comment_id":"1084847","poster":"ealpuche","content":"ChatGPT is not a reliable source."}],"timestamp":"1713933480.0"},{"upvote_count":"1","timestamp":"1713727440.0","comment_id":"1049771","content":"Selected Answer: B\nI like (B) since it is put SQS before SNS so we could prepare for retention. (C) dead letter Queue is kind of \"rescue\" effort. Also (C) should mention reprocessing dead letter.","poster":"Wayne23Fang","comments":[{"content":"\"Reprocessing dead letters\" is not desired here. They want to \"retain messages that are not delivered and analyze the messages for up to 14 days\", which is what C does.","comment_id":"1110992","timestamp":"1719810120.0","poster":"pentium75","upvote_count":"1"}]},{"upvote_count":"2","poster":"thanhnv142","timestamp":"1713666000.0","content":"C is correct. It used a combination of SNS and SQS so it better than B.","comment_id":"1049224"},{"content":"Selected Answer: C\nC is the answer","poster":"iwannabeawsgod","comment_id":"1045584","timestamp":"1713332760.0","upvote_count":"2"},{"comments":[{"content":"C \"Configure an Amazon SNS dead letter queue\"\nAWS \"Configuring an Amazon SNS dead-letter queue\"\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-configure-dead-letter-queue.html","upvote_count":"2","comment_id":"1110996","poster":"pentium75","timestamp":"1719810600.0"},{"upvote_count":"6","timestamp":"1711093740.0","poster":"RDM10","comment_id":"1013639","content":"https://docs.aws.amazon.com/sns/latest/dg/sns-configure-dead-letter-queue.html"}],"comment_id":"1010917","content":"B is correct Answer. SQS Retain messages in queues for up to 14 days\nC is incorrect because there is nothing called Amazon SNS dead letter queue","upvote_count":"2","poster":"Devsin2000","timestamp":"1710813300.0"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html","upvote_count":"2","timestamp":"1708907280.0","poster":"lemur88","comment_id":"990424"},{"upvote_count":"2","poster":"Guru4Cloud","content":"Selected Answer: C\nC. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.\n By using an Amazon SQS queue as the target for the dead letter queue, you ensure that the undelivered messages are reliably stored in a queue for up to 14 days. Amazon SQS allows you to specify a retention period for messages, which meets the retention requirement without additional development effort.","timestamp":"1708709040.0","comment_id":"988435"},{"poster":"mtmayer","timestamp":"1708127700.0","comment_id":"983060","upvote_count":"3","content":"Selected Answer: B\nDead Letter is a SQS feature not SNS. \nA dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can target for messages that can't be delivered to subscribers successfully. Messages that can't be delivered due to client errors or server errors are held in the dead-letter queue for further analysis or reprocessing. For more information, see Configuring an Amazon SNS dead-letter queue for a subscription and Amazon SNS message delivery retries.\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html","comments":[{"comment_id":"1110993","content":"\"See Configuring an Amazon SNS (!) dead-letter queue\", exactly, thus C.","timestamp":"1719810240.0","poster":"pentium75","upvote_count":"2"}]},{"content":"Selected Answer: B\nIn SNS, DLQs store the messages that failed to be delivered to subscribed endpoints. For more information, see Amazon SNS Dead-Letter Queues.\n\nIn SQS, DLQs store the messages that failed to be processed by your consumer application. This failure mode can happen when producers and consumers fail to interpret aspects of the protocol that they use to communicate. In that case, the consumer receives the message from the queue, but fails to process it, as the message doesn’t have the structure or content that the consumer expects. The consumer can’t delete the message from the queue either. After exhausting the receive count in the redrive policy, SQS can sideline the message to the DLQ. For more information, see Amazon SQS Dead-Letter Queues.\n\nhttps://aws.amazon.com/blogs/compute/designing-durable-serverless-apps-with-dlqs-for-amazon-sns-amazon-sqs-aws-lambda/","poster":"xyb","timestamp":"1707168660.0","comments":[{"content":"\"Configuring an Amazon SNS dead-letter queue for a subscription \n\nA dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can target for messages that can't be delivered to subscribers successfully. \"","poster":"pentium75","timestamp":"1719810300.0","upvote_count":"2","comment_id":"1110994"}],"upvote_count":"2","comment_id":"973327"},{"comment_id":"934029","upvote_count":"2","content":"C is best to handle this requirement. Although good to note that dead-letter queue is an SQS queue. \n\n\"A dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can target for messages that can't be delivered to subscribers successfully. Messages that can't be delivered due to client errors or server errors are held in the dead-letter queue for further analysis or reprocessing.\"\n\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html#:~:text=A%20dead%2Dletter%20queue%20is%20an%20Amazon%20SQS%20queue","poster":"TariqKipkemei","timestamp":"1703571540.0"},{"content":"C - Amazon SNS dead letter queues are used to handle messages that are not delivered to their intended recipients. When a message is sent to an Amazon SNS topic, it is first delivered to the topic's subscribers. If a message is not delivered to any of the subscribers, it is sent to the topic's dead letter queue.\n\nAmazon SQS is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS queues can be configured to have a retention period, which is the amount of time that messages will be kept in the queue before they are deleted.\n\nTo meet the requirements of the company, you can configure an Amazon SNS dead letter queue that has an Amazon SQS target with a retention period of 14 days. This will ensure that any messages that are not delivered to the on-premises warehouse application will be stored in the Amazon SQS queue for up to 14 days. The company can then analyze the messages in the Amazon SQS queue to determine why they were not delivered.","timestamp":"1701706500.0","comment_id":"914726","poster":"Felix_br","upvote_count":"3"},{"upvote_count":"3","timestamp":"1700715360.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html","comment_id":"904509","poster":"Yadav_Sanjay"},{"content":"Selected Answer: C\nThe message retention period in Amazon SQS can be set between 1 minute and 14 days (the default is 4 days). Therefore, you can configure your SQS DLQ to retain undelivered SNS messages for 14 days. This will enable you to analyze undelivered messages with the least development effort.","timestamp":"1700490900.0","comment_id":"902580","upvote_count":"5","poster":"Rob1L"},{"poster":"nosense","timestamp":"1700388600.0","content":"Selected Answer: C\nA is a good solution, but it requires to modify the application. The application would need to be modified to send messages to the Amazon Kinesis Data Stream instead of the on-premises HTTPS endpoint.\nOption B is not a good solution. The application would need to be modified to send messages to the Amazon SQS queue instead of the on-premises HTTPS endpoint.\nOption D is not a good solution because Amazon DynamoDB is not designed for storing messages for long periods of time.\nOption C is the best solution because it does not require any changes to the application","upvote_count":"2","comment_id":"901748"},{"content":"Selected Answer: B\nBy adding an Amazon SQS queue as an intermediary between the application and Amazon SNS, you can retain undelivered messages for analysis. Amazon SQS provides a built-in retention period that allows you to specify how long messages should be retained in the queue. By setting the retention period to 14 days, you can ensure that the undelivered messages are available for analysis within that timeframe. This solution requires minimal development effort as it leverages Amazon SQS's capabilities without the need for custom code development.","timestamp":"1700338140.0","comment_id":"901438","poster":"cloudenthusiast","upvote_count":"4","comments":[{"comment_id":"901440","timestamp":"1700338140.0","upvote_count":"2","poster":"cloudenthusiast","comments":[{"content":"Agree with you\n\nA dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can target for messages that can't be delivered to subscribers successfully.","comment_id":"903002","timestamp":"1700559480.0","upvote_count":"1","poster":"Efren"}],"content":"Amazon Simple Notification Service (Amazon SNS) does not directly support dead letter queues. The dead letter queue feature is available in services like Amazon Simple Queue Service (Amazon SQS) and AWS Lambda, but not in Amazon SNS."}]},{"timestamp":"1700302860.0","poster":"Efren","content":"ChatGP says is SQS.. not sure","upvote_count":"1","comment_id":"900911"}],"timestamp":"2023-05-18 10:21:00","answer_ET":"C","question_images":[],"exam_id":31,"question_id":453},{"id":"8ZTtnkozYLPa74nnSOc9","question_id":454,"choices":{"C":"Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.","B":"Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.","A":"Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.","D":"Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive."},"answer":"B","question_images":[],"question_text":"A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable.\nWhich solution will meet these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/85211-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","isMC":true,"answer_images":[],"answers_community":["B (70%)","C (24%)","5%"],"answer_ET":"B","timestamp":"2022-10-12 01:44:00","topic":"1","discussion":[{"comment_id":"692941","poster":"masetromain","content":"Selected Answer: B\nI think the answer is B:\nUsers access the files randomly\n\nS3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns, independent of object size or retention period. You can use S3 Intelligent-Tiering as the default storage class for virtually any workload, especially data lakes, data analytics, new applications, and user-generated content.\n\nhttps://aws.amazon.com/fr/s3/storage-classes/intelligent-tiering/","timestamp":"1665569940.0","comments":[{"comment_id":"944026","poster":"MutiverseAgent","timestamp":"1688579520.0","upvote_count":"3","content":"Agree, S3 Intelligent-Tiering meets all the requirements. The very important/crucial consideration here to satisfy that all files withing a year are instantly accessible is that the two options \"Archive Access\" and \"Deep Archive Access\" are not enabled in the \"Archive rule actions\" section present in the \"Intelligent-Tiering Archive configurations\" of the bucket. Those options are not enabled by default so this answer will work."},{"content":"What about if the file you have not accessed 360 days and intelligent tier moved the file to Glacier and on 364 day you want to access the file instantly ?\n\nI think C is right choice","comment_id":"825724","timestamp":"1677667500.0","comments":[{"poster":"boringtangent","upvote_count":"3","timestamp":"1712931000.0","content":"bro u r forgetting cost effectiveness which is the requirement 1 yr in s3 standard will cost more than s3 intelligent tire.","comment_id":"1194383"}],"upvote_count":"6","poster":"sachin"},{"content":"It says \"S3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns\".\nHowever, the statement says access pattern is predictable. It says there is frequent access about 1year.","timestamp":"1677065460.0","upvote_count":"2","comment_id":"817764","poster":"habibi03336","comments":[{"poster":"lofzee","content":"Helps to read sometimes","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"1302052","poster":"PaulEkwem","timestamp":"1729686600.0","content":"Hey, don't be rude"}],"timestamp":"1716613020.0","comment_id":"1217992"},{"comment_id":"1195951","content":"It syas \"... access the files INFREQUENTLY after 1 year\"","upvote_count":"1","timestamp":"1713174420.0","poster":"CarlosMarin"},{"content":"it doesnt say predictable, it says files are accessed random. Random = Unpredictable. Answer is B","timestamp":"1679008380.0","upvote_count":"12","comment_id":"841413","poster":"killbots"}]},{"comment_id":"902275","upvote_count":"6","comments":[{"comment_id":"949621","upvote_count":"2","poster":"RupeC","content":"With S3 Intelligent-Tiering, you can define rules that determine when objects should be moved from the frequent access tier to the infrequent access tier, or vice versa, within S3 Standard storage classes.","timestamp":"1689152940.0"},{"comment_id":"1027466","upvote_count":"7","poster":"IngenieriaEGlobal","timestamp":"1696690440.0","content":"The Answer is B. S3 Intelligent-Tiering stores objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access. For a small monthly monitoring and automation fee per object, S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access tier. There are no retrieval fees in S3 Intelligent-Tiering. If an object in the infrequent access tier is accessed later, it is automatically moved back to the frequent access tier. No additional tiering fees apply when objects are moved between access tiers within the S3 Intelligent-Tiering storage class. S3 Intelligent-Tiering is designed for 99.9% availability and 99.999999999% durability, and offers the same low latency and high throughput performance of S3 Standard","comments":[{"comment_id":"1129499","timestamp":"1706012700.0","poster":"Visinho","content":"Are you not going to pay for Athena usage?","upvote_count":"1"}]},{"content":"C involves moving \"the files to S3 Glacier Instant Retrieval\" which is not cost-effective since \"a delay in retrieving older files is acceptable.\"","upvote_count":"3","comment_id":"1105090","timestamp":"1703490240.0","poster":"pentium75"}],"content":"Answer is C, why not intelligent Tiering\n \nIf the Intelligent-Tiering data transitions to Glacier after 180 days instead of 1 year, it would still be a cost-effective solution that meets the requirements.\n\nWith files stored in Amazon S3 Intelligent-Tiering, the data is automatically moved to the appropriate storage class based on its access patterns. In this case, if the data transitions to Glacier after 180 days, it means that files that are infrequently accessed beyond the initial 180 days will be stored in Glacier, which is a lower-cost storage option compared to S3 Standard.","poster":"ssoffline","timestamp":"1684540680.0"}],"upvote_count":"45"},{"poster":"Lilibell","content":"The answer is B","upvote_count":"12","timestamp":"1665531840.0","comment_id":"692501"},{"content":"Selected Answer: B\nOption A Incorrect – S3 Glacier Instant Retrieval is more expensive than necessary for frequently accessed files, and tag-based querying is inefficient.\n\nOption C Incorrect – Using S3 Glacier Instant Retrieval after 1 year is more expensive than needed, and storing metadata in S3 is less efficient than Athena.\n\nOption D Incorrect – S3 Glacier Deep Archive has long retrieval times, and maintaining metadata in RDS adds extra costs.\n\nOption B Correct – S3 Intelligent-Tiering optimizes costs, and S3 Glacier Flexible Retrieval balances cost with acceptable retrieval delays, while Athena and S3 Glacier Select enable efficient queries.","poster":"sammo08","comment_id":"1359688","timestamp":"1740127200.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1346070","timestamp":"1737716400.0","content":"Selected Answer: B\nThe B and C are both good options, but since Athena is free for use with S3, it definitely the B-option. \n\nAthena queries data directly from Amazon S3. There are no additional storage charges for querying your data with Athena.","poster":"dariar"},{"comment_id":"1335381","timestamp":"1735800300.0","poster":"ricktechie66","upvote_count":"1","content":"Selected Answer: C\nThis solution best meets the requirements while being cost-effective because:\n\n1. **Initial storage in S3 Standard:** Provides the fastest access for files less than 1 year old, meeting the requirement for quick retrieval of newer files\n2. **Metadata storage in S3 Standard:** Enables efficient querying and searching of files without additional database costs\n3. **Lifecycle policy to S3 Glacier Instant Retrieval:** Provides cost optimization for older files while still maintaining relatively quick retrieval when needed."},{"content":"Selected Answer: B\nOption B strikes the right balance between cost-effectiveness and performance, ensuring that frequently accessed data is easily retrievable and older data is archived at lower cost with fast retrieval options for both new and older data.","poster":"Rohan_Butala","timestamp":"1734453000.0","comment_id":"1328039","upvote_count":"1"},{"upvote_count":"3","content":"If you are like me and thought Athena would make this cost-inefficient:\n\nAthena is free for s3 querying.\nIt has a cost per TB for SQL Queries.\nAnd a different cost for Apache apps.\n\nhttps://aws.amazon.com/athena/pricing/","timestamp":"1730824380.0","poster":"_adoo","comment_id":"1307454"},{"poster":"JonesNick","content":"Keyword : MOST cost-effectively\nAthena is great tool for analyzing data in S3. But it comes with the cost.\nSo answer is C.","comment_id":"1306170","upvote_count":"1","timestamp":"1730552280.0"},{"timestamp":"1719636900.0","upvote_count":"2","poster":"jatric","comment_id":"1239036","content":"Selected Answer: B\ncost effective to retrieve the file of 1 year or less. Standard S3 is more cost effective than intelligent tiering."},{"comment_id":"1238406","timestamp":"1719525060.0","poster":"344bba0","upvote_count":"4","content":"Selected Answer: C\nThe answer is C.\n* Intelligent tiering may not guarantee frequent, but random access and fast searches within a year.\n* Athena is a great analysis solution, but it is an unnecessary cost for search purposes only."},{"comment_id":"1235047","upvote_count":"2","poster":"mknarula","timestamp":"1719016020.0","content":"Answer is C. The only difference between choice B and C is Glacier Storage class. The question states clearly that \"giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible\". This is possible via instant retrieval and not flexible retrieval."},{"timestamp":"1713968580.0","comment_id":"1201446","poster":"ManikRoy","upvote_count":"1","content":"Selected Answer: B\noption B is the only one that mentions use of Amazon Athena and Glacier select for querying, So I'll go with it though I would have preferred using S3 standard storage in place of S3 intelligent tiering for the first year."},{"content":"Answer is C - unfortunately. S3 Glacier Instant Retrieval - MOST Cost effective deep storage for retrieving files as quickly as possible.","comment_id":"1190582","timestamp":"1712429940.0","upvote_count":"1","poster":"hro"},{"upvote_count":"1","timestamp":"1710937680.0","content":"Selected Answer: B\nB is good answer: Athena is good option to query data in S3. And before 1 year data are randomly use, for this, intelligent tiering is good option.","poster":"LIORAGE","comment_id":"1178251"},{"content":"Selected Answer: C\nThe company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible\" -- What if S3 Intelligent-Tiering transitioned the data that's under 1 year old into a storage class that takes a long time to access?","poster":"chickenmf","upvote_count":"1","timestamp":"1709591340.0","comment_id":"1166063"},{"timestamp":"1708432200.0","poster":"dsshahu01","content":"Selected Answer: C\nThe answer is C\n\nS3 standard for first part (intelligent tiering is much better and cost-effective)\nglacier instant retrieval because of the statement after an year needs to retrieved as soon as possible) \n\nAlso why ruling out B is because of Athena - it becomes expensive if data is retrieved using it after scanning all the data in glacier per request.","upvote_count":"1","comment_id":"1154706"},{"upvote_count":"2","comment_id":"1122513","timestamp":"1705234380.0","poster":"awsgeek75","content":"Selected Answer: B\nA: It does not account for random pattern of first year\nC: \"Store search metadata for each archive in Amazon S3 Standard storage\"... this part is wrong for me. Storing metadata forever in S3 so that it can be queries? This is why I won't select it.\nD: RDS is costly for storage and query just to know where your S3 object is.\nB is correct as Intelligent Tiering takes care of random frequency in first year in most cost effective way. Older object will end up in S3 glacier with flexible retrieval so cost effective. Athena doesn't care where your object is (s3 standard, IA or Glacier) and queries work."},{"upvote_count":"1","poster":"bujuman","comment_id":"1111947","content":"Selected Answer: B\nAccording to the fact that S3 Std, Std-IA and One Zone-IA are Higher cost, frequent access storage classes and the fact that S3 Intelligent-Tiering is an additional storage class that provides flexibility for data with unknown or changing access patterns. It automates the movement of your objects between storage classes to optimize cost. Plus the requirement for MOST cost-effective solution, Answer B seems to be the right solution","timestamp":"1704204360.0"},{"poster":"JTruong","comment_id":"1108940","upvote_count":"1","timestamp":"1703870580.0","content":"If you watch Stephen Mareek's Udemy SSA video - anything after 1 year has to go with Amazon Athena"},{"timestamp":"1703782260.0","poster":"upliftinghut","comments":[{"poster":"upliftinghut","timestamp":"1703782740.0","upvote_count":"1","comment_id":"1107982","content":"quite tricky because B has better cost with flexible retrieval for files after 1 year. If counting in operation overhead then C is better. For cost-optimized, B can probably be better. Tricky question then"}],"content":"Selected Answer: C\nC is the most cost-effective given no Athena and the archive files don't need instant access. A delay is acceptable","comment_id":"1107978","upvote_count":"1"},{"comment_id":"1094001","timestamp":"1702345560.0","content":"Selected Answer: C\nI picked \"B\" first, then switched to \"C\" because it asks for MORE cost-effective = Athena might be pricey: however: \nAccess Patterns:\nIf your data access patterns are predictable and consistent, and you do not require automatic tiering based on access frequency, S3 Standard might be a straightforward and cost-effective choice.\nVariable Access Patterns:\nIf your data access patterns are variable or unpredictable, and you want automatic cost optimization based on access frequency, S3 Intelligent-Tiering might provide cost savings.","poster":"fb4afde","upvote_count":"2"},{"poster":"wantu","timestamp":"1701178500.0","content":"Selected Answer: B\nos usuarios acceden a los archivos de forma aleatoria. S3 Intelligent-Tiering es la clase de almacenamiento ideal para datos con patrones de acceso desconocidos, cambiantes o impredecibles, independientemente del tamaño del objeto o el período de retención. Puede utilizar S3 Intelligent-Tiering como clase de almacenamiento predeterminada para prácticamente cualquier carga de trabajo, especialmente lagos de datos, análisis de datos, nuevas aplicaciones y contenido generado por el usuario.","comment_id":"1082606","upvote_count":"2"},{"timestamp":"1700151120.0","poster":"xogete","content":"Selected Answer: C\ni think B would be for least operation overhead, but C only uses S3 which would make it most cost effective, no?","comment_id":"1072560","upvote_count":"1"},{"upvote_count":"2","poster":"wabosi","comment_id":"1067941","content":"Selected Answer: B\nI vote for B, key points to me are:\n\"randomly within 1 year\" my mind goes to intelligent-tiering\n\"A delay in retrieving older files is acceptable\" my mind goes to Glacier Flexible Retrieval after 1 year because they don't need it immediately\n\"MOST cost-effectively\" there are no retrieval charges in S3 intelligent-tiering storage, on top of this Glacier Flexible Retrieval is cheaper than Glacier Instant Retrieval, if they accept retrieval time in 5 – 12 hours, bulk is free","timestamp":"1699718160.0"},{"content":"I think the answer is C is cost effective","poster":"SAA463","comment_id":"1058022","timestamp":"1698682680.0","upvote_count":"1"},{"timestamp":"1697272080.0","poster":"ACloud_Guru15","comment_id":"1043255","content":"Selected Answer: C\nConsidering the cost-effective solution that meets the requirements, option B (Store individual files in Amazon S3 Intelligent-Tiering, use S3 Lifecycle policies to move the files to S3 Glacier after 1 year, query and retrieve the files that are in Amazon S3 by using Amazon Athena, and query and retrieve the files that are in S3 Glacier by using S3 Glacier Select) seems to be the most appropriate. It ensures efficient access to recent and infrequently accessed files, while also managing costs effectively.","upvote_count":"1"},{"poster":"tom_cruise","comment_id":"1028798","upvote_count":"1","content":"Selected Answer: B\nI think the reason C is the correct answer is because it is cheaper than B; and the question is asking the MOST cost effective solution.","timestamp":"1696850040.0"},{"timestamp":"1696715280.0","content":"Selected Answer: C\nI m on \"C\" camp. It is hard to beat S3 solution for Cost-effective. C ) uses only S3 not other cost like Athena or RDB. The other comment below to support (C) are reasonable like ssoffline's.","poster":"Wayne23Fang","upvote_count":"1","comment_id":"1027625"},{"poster":"awashenko","comment_id":"1023463","timestamp":"1696282320.0","content":"Selected Answer: B\nSo for me it came down to B and D. I choose B because of this statement \"Users access the files randomly within 1 year of the call\" \nHad it said they access the files all the time on a regular basis for the first year, I would have went with D but because they access those files at random I think B is the better choice.","upvote_count":"1","comments":[{"comment_id":"1023465","timestamp":"1696282560.0","content":"Although, now that I'm thinking about it. S3 is cheaper than RDS so C would have been the better choice if that statement wasn't there.","poster":"awashenko","upvote_count":"1"}]},{"timestamp":"1696027320.0","poster":"ABS_AWS","content":"Correct answer is C \nAs B has got Athena mentioned which is not fit as per question.","upvote_count":"2","comment_id":"1021085"},{"comment_id":"1004426","content":"Selected Answer: B\n\"For archive data that needs immediate access, such as medical images, news media assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost\nstorage with milliseconds retrieval. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3\nGlacier Flexible Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5-12 hours.\" https://aws.amazon.com/about-aws/whats-new/2021/11/amazon-s3-glacier-instant-retrieval-storage-class/","poster":"[Removed]","timestamp":"1694404680.0","upvote_count":"1"},{"content":"B is the right answer..","comment_id":"998717","upvote_count":"1","poster":"benacert","timestamp":"1693847520.0"},{"comment_id":"997228","content":"Selected Answer: C\nThe question is about Cost-effective. Athena search of S3 is probably too much. It cost at least 2.5 times of simple S3 Sql query.","upvote_count":"1","poster":"Wayne23Fang","timestamp":"1693704120.0"},{"comments":[{"content":"Exactly what i thought","poster":"Yonimoni","timestamp":"1692538920.0","upvote_count":"1","comment_id":"985826"}],"timestamp":"1692186420.0","comment_id":"982499","poster":"Syruis","content":"Selected Answer: C\nC and not B just because Athena will be costly.","upvote_count":"4"},{"comment_id":"976660","content":"Selected Answer: B\nI would recommend option B.\n\nThe key reasons are:\n\nS3 Intelligent-Tiering automatically moves files between frequent and infrequent access tiers based on actual access patterns, optimizing cost.\nLifecycle policies can move older files to Glacier Flexible Retrieval after 1 year, which has higher latency and lower cost than S3.\nAthena allows querying the metadata of files in S3 without retrieving the files themselves.\nGlacier Select can directly query files in Glacier without needing to restore the entire file.","poster":"Guru4Cloud","upvote_count":"2","timestamp":"1691585520.0"},{"upvote_count":"2","poster":"TariqKipkemei","content":"Selected Answer: B\nUsers access the files randomly = Amazon S3 Intelligent-Tiering\nUsers access the files infrequently = S3 Glacier Flexible Retrieval\nAbility to query files as quickly as possible = Amazon Athena, S3 Glacier Select","timestamp":"1691385120.0","comment_id":"974410"},{"poster":"miki111","content":"Option B is the right answer.","comment_id":"953739","timestamp":"1689554100.0","upvote_count":"1"},{"poster":"RupeC","content":"Selected Answer: B\nAs S3 Intelligent-Tiering, you can define rules that determine when objects should be moved from the frequent access tier to the infrequent access tier, or vice versa, within S3 Standard storage classes that move, could be set to 12 months.","comment_id":"949623","upvote_count":"1","timestamp":"1689153060.0"},{"upvote_count":"3","timestamp":"1687774080.0","content":"Selected Answer: B\nI asked ChatGPT In conclusion, C is possible, but C is not cost-effective.\nBoth options B and C can meet the requirements, but option B with S3 Intelligent-Tiering may provide more cost savings as it optimizes storage costs based on access patterns, automatically moving files to the most appropriate tier. However, if the priority is primarily on fast retrieval times for files less than 1-year-old and the cost difference is not a significant concern, option C with Amazon S3 Standard storage and S3 Glacier Instant Retrieval can be a valid and cost-effective choice as well.","poster":"VitaminNmineral","comment_id":"934279"},{"upvote_count":"3","poster":"cookieMr","content":"Selected Answer: B\nOption A would not optimize the retrieval of files less than 1-year-old, as the files would be stored in S3 Glacier, which has longer retrieval times compared to S3 Intelligent-Tiering.\n\nOption C adds complexity by involving two storage classes and may not provide the most cost-effective solution.\n\nOption D would require additional infrastructure with RDS for storing metadata and retrieval from S3 Glacier Deep Archive, which may not be necessary and could incur higher costs.\n\nOption B is the most suitable and cost-effective solution for optimizing file retrieval based on the access patterns described. Amazon S3 Intelligent-Tiering is a storage class that automatically moves objects between two access tiers: frequent access and infrequent access, based on their access patterns. By storing the files in S3 Intelligent-Tiering, the files less than 1-year-old will be kept in the frequent access tier, allowing for quick retrieval.","timestamp":"1687342080.0","comment_id":"929318"},{"comment_id":"926732","timestamp":"1687096500.0","upvote_count":"1","poster":"HassanYoussef","content":"Selected Answer: B\nThe answer is (B) because of the random access on files and the query service needed is Athena."},{"content":"For all of you that (incorrectly, in my opinion) select Answer B: you are forgetting that an object is moved to Deep Archive access after 180 days of inactivity (here's the link with the details: https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html)\n\nConsidering the above, it could happen that an object is required after day 180 of the first year, in that case the object is not immediately reachable, so one of the requirements are not met.\n\nThe correct answer should the C","poster":"ruqui","comment_id":"900205","comments":[{"poster":"Itsume","comment_id":"920811","timestamp":"1686496920.0","upvote_count":"2","content":"It says: \"before your specified number of days of no access (\"for example\", 180 days)\" so this number of days is just an example. Also the files are to be deleted one year after being made and this would keep the files for a specified time after the last use which means that if they are used within that year it would be saved for more than the intended year. Therefore, b is correct."}],"timestamp":"1684330980.0","upvote_count":"3"},{"poster":"Abrar2022","comment_id":"899935","content":"Random> unpredictable> Intelligent-tiering","upvote_count":"1","comments":[{"comment_id":"928482","poster":"oiccic99","upvote_count":"1","content":"But it says that for the first year, access must be instant, so it can't be intelligent tiering, because if he moves do deep archive before 1 year, you can't access instantly the resource","timestamp":"1687268580.0"}],"timestamp":"1684316400.0"},{"poster":"pbpally","upvote_count":"4","timestamp":"1683423000.0","comment_id":"891103","content":"Selected Answer: B\nKey notes here:\n1. \"...randomly within 1 year of the call,..\" Randomly = unpredictable -> Intelligent Tiering\n2. \"but users access the files infrequently after 1 year\" coupled with \"retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable\" -> Glacier Flexible Retrieval (has options for expedited = 1-5 minutes, standard = 3-5 hours, and bulk = 5-12 hours).\nLast but not least is \"giving users the ability to QUERY\". Query = Athena. It's literally a serverless query service to analyze data stored explicitly in S3."},{"upvote_count":"1","poster":"Rahulbit34","comment_id":"886718","timestamp":"1682973720.0","content":"Access file randomly - triggers the option or Intelligent tiering. Glacier is for achievable and Athena for quick queries. Answer is B."},{"timestamp":"1680942840.0","upvote_count":"1","comment_id":"864549","content":"Selected Answer: C\nS3 Intelligent-Tiering is the ideal for data with irregular access pattern. Since the requirement here states that files older that 1 year is retrieve infrequently (the pattern is fixed), there is no need to use S3 Intelligent-Tiering, S3 Standard is more suitable. And data more than 1 year can be moved to S3 glacier. In addition, answer C uses search metadata when storing the data, this allows the files to be retrieve as quickly as possible when required. Thus answer should be C.","poster":"cheese929"},{"upvote_count":"2","timestamp":"1680615000.0","poster":"TheAbsoluteTruth","content":"Selected Answer: B\nLa opción B parece ser la solución más rentable para cumplir con los requisitos de la empresa. Al almacenar los archivos en Amazon S3 Intelligent-Tiering, se pueden utilizar las políticas de ciclo de vida para mover los archivos a S3 Glacier Flexible Retrieval después de 1 año, lo que permite ahorrar costos de almacenamiento a largo plazo. Para acceder a los archivos que se encuentran en Amazon S3, se puede utilizar Amazon Athena, lo que permite una recuperación rápida y eficiente de los archivos que tienen menos de 1 año. Para acceder a los archivos que se encuentran en S3 Glacier, se puede utilizar S3 Glacier Select, lo que permite la recuperación de datos selectiva y reduce los costos de recuperación. Esta solución también es escalable, lo que significa que puede manejar grandes volúmenes de datos y un alto número de usuarios.","comment_id":"861091"},{"comment_id":"858649","upvote_count":"1","content":"Selected Answer: B\nKey word 'durable' for a intelligent-tiering. \nAthena for S3 query.","timestamp":"1680424560.0","poster":"channn"},{"timestamp":"1680273240.0","poster":"linux_admin","comment_id":"857201","content":"Selected Answer: B\nOption B, Store individual files in Amazon S3 Intelligent-Tiering, is a cost-effective solution as it automatically moves objects between four access tiers (frequent, infrequent, archive, and deep archive) based on changing access patterns and automatically optimizes costs for the company. The S3 Lifecycle policies can be used to move files to S3 Glacier Flexible Retrieval after 1 year, which has a retrieval time of minutes to hours. Amazon Athena can be used to query and retrieve files that are still in S3 Intelligent-Tiering, and S3 Glacier Select can be used to query and retrieve files that have been moved to S3 Glacier Flexible Retrieval.","upvote_count":"1"},{"content":"\"As quickly as possible\" is the key point for retrieval of files which are less than 1 year old. So, Option C is the answer.","comments":[{"comment_id":"877292","poster":"trungtranqn912023","upvote_count":"1","content":"\" S3 Glacier Instant Retrieval after 1 year.\"\nthe data after 1 year not require quick access . So it more expensive and not fit with requirement","timestamp":"1682169180.0"}],"timestamp":"1679869200.0","upvote_count":"1","poster":"jaswantn","comment_id":"851538"},{"content":"Selected Answer: B\n\"임의로 파일 액세스\"","timestamp":"1679137680.0","poster":"UNGMAN","comment_id":"842682","upvote_count":"1"},{"comment_id":"823881","content":"Selected Answer: B\nI originally thought C but changed my mind to B.\nIntelligent tiering will always only move object to object storage classes with milliseconf latency\nhttps://aws.amazon.com/s3/storage-classes/\nI was originally concerned a file would go to some storage class after several months but before a year to a storage class with higher latency but that is not the case.","poster":"Steve_4542636","upvote_count":"1","timestamp":"1677511260.0"},{"upvote_count":"7","content":"I disagree with B, it says clearly access are less than 1-year-old as quickly as possible, use intelligent, if a data is not accessed after 3 months it will be moved to archive then you lose this requirement.","comment_id":"819788","timestamp":"1677189060.0","poster":"user_deleted"},{"content":"Selected Answer: C\nC is correct. B does not make any sense because the company want to grant users the ability to retrieve and query the files that are \"less than one year old as quickly as possible.\" Intelligent-tiering moves files that have been unassessed for 30 days to S3-IA, 90 days to S3 Glacier, and 180 days to S3 Glacier Deep Archive. This is problematic because Glacier and Glacier Deep Archive both have high retrieval times.","comment_id":"819694","upvote_count":"7","timestamp":"1677183300.0","comments":[{"timestamp":"1677244080.0","content":"Edit: 90 days to Glacier Instant Access.","comment_id":"820492","poster":"Andrew123123","upvote_count":"1"}],"poster":"Andrew123123"},{"poster":"BlueVolcano1","content":"Selected Answer: B\nOption B. S3 Intelligent-Tiering seems best as files are accessed randomly in the first year. After 1 year, a delay in retrieving files is acceptable, so it makes sense to move them to Glacier Flexible Retrieval after 1 year. Archives can be restored for free from there using the bulk option. To query and retrieve files, S3 Select/Glacier Select and Athena are best suited and cheap, as you only pay for what you use.\n\nA: Instant Retrieval is not the most cost-effective for the requirement, as requirements say a delay in retrieving files older than 1 year is acceptable.\n\nC: Same as A - Instant Retrieval is not the most cost-effective solution for the requirements.\n\nD: It's unnecessary to use RDS to query files when you have S3 Select, Glacier Select, Athena and Redshift Spectrum, all allowing you to query S3/Glacier, at varying levels of complexity.","upvote_count":"3","comment_id":"780997","timestamp":"1674122700.0"},{"content":"Selected Answer: D\nI would go against the majority and select D on this one. This is the most cost effective. Using S3 intelligent tiering is more costly and the delay to retrieve is acceptable.","comment_id":"771217","timestamp":"1673341080.0","poster":"JohnnyBG","upvote_count":"7"},{"upvote_count":"1","poster":"SilentMilli","timestamp":"1673055360.0","comment_id":"768187","content":"Selected Answer: B\nTo meet these requirements in a cost-effective manner, the company can store individual files in Amazon S3 Intelligent-Tiering. Amazon S3 Intelligent-Tiering is a storage class that automatically moves data to the most cost-effective storage tier based on access patterns. By storing the files in Amazon S3 Intelligent-Tiering, the company can ensure that the files that are less than 1 year old are quickly and easily accessible to users, while still optimizing costs by automatically moving older files to a lower-cost storage tier. The company can use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. To query and retrieve the files, the company can use Amazon Athena to query and retrieve the files that are in Amazon S3, and S3 Glacier Select to query and retrieve the files that are in S3 Glacier."},{"content":"Selected Answer: B\nThe answer is B because flexible retrival.","timestamp":"1671657480.0","upvote_count":"2","comment_id":"752762","poster":"QueTeddyJR"},{"upvote_count":"1","poster":"pazabal","comment_id":"751729","content":"Selected Answer: B\nB, most cost-effective storage tier based on usage patterns (compared to the others here). Frequently accessed files within first year will remain in Standard (fast access), whereas infrequently accessed files after first year will move to Glacier Flexible retrieval tier. Lifecycle policy will automate the transition after 1 year. Athena allows you to analyze data stored in S3 with SQL, so it can be used along w Select (queries data stored in S3 glacier) to retrieve only the necessary data.\nA, Data needs to be accessed within minutes, not for infrequent access after 1 year. \nC, More expensive \nD, More expensive","timestamp":"1671584340.0"},{"timestamp":"1671511440.0","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: B\n***CORRECT***\nOption B is the most cost-effective solution for meeting the requirements described.\n\nIn Option B, the files are stored in Amazon S3 Intelligent-Tiering, which automatically moves infrequently accessed data to a lower-cost storage tier based on usage patterns. This means that the files that are accessed frequently within the first year will be stored in the most efficient storage tier, while files that are not accessed as frequently can be moved to a lower-cost tier after 1 year.\n\nOption B also uses S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year, which allows the company to store the files at a lower cost while still being able to retrieve them within a reasonable amount of time. The files can be queried and retrieved from S3 Glacier Flexible Retrieval using S3 Glacier Select, which allows for efficient querying of data stored in S3 Glacier.","upvote_count":"2","comment_id":"750502","comments":[{"poster":"Buruguduystunstugudunstuy","comments":[{"content":"I don't agree Re: D not meeting the requirements for fast retrieval of files. Option D has S3 Standard storage, according to aws \"Access to the object: Milliseconds\". As well as \"moving objects from other tiers into S3-Intelligent Tearing is expensive: it costs $0.01 to transition 1,000 objects, which can add up quickly if you've got a lot of objects to transition.\" Option D is the most cost effective solution.","comment_id":"1082487","poster":"Maru86","upvote_count":"1","timestamp":"1701171060.0"}],"content":"In comparison--- \nOption A stores the files in S3 Glacier Instant Retrieval, which is a storage tier that is optimized for fast retrieval of data. This option may not be as cost-effective because it requires the data to be stored in a more expensive storage tier, even if it is not accessed frequently. \nOption C stores the files in S3 Standard storage and moves them to S3 Glacier Instant Retrieval after 1 year, which is a similar approach to Option A and may not be as cost-effective. \nOption D stores the files in S3 Standard storage and moves them to S3 Glacier Deep Archive after 1 year, which is the lowest-cost storage tier but may not meet the requirement for fast retrieval of files that are less than 1 year old.","upvote_count":"2","comment_id":"750504","timestamp":"1671511500.0"}]},{"content":"Selected Answer: B\nQuery the data NOT metadata, so Athena with S3 intelligent tiering suits the requirement.","timestamp":"1671445860.0","upvote_count":"1","comment_id":"749723","poster":"Nandan747"},{"comments":[{"timestamp":"1672356720.0","upvote_count":"1","poster":"Idriss10","content":"Users access the files randomly\nso B","comment_id":"761559"}],"timestamp":"1670608140.0","content":"I think C is correct. \"retrieve files that are less than 1-year-old as quickly as possible\" hence Amazon S3 Standard is the correct one. S3 standard has 99.99% availability, and S3 Intelligent-Tiering has 99.9% availability. Details is here: https://aws.plainenglish.io/aws-s3-different-types-of-storage-types-available-in-s3-3550e0b87580","poster":"Anny_Me","comment_id":"740311","upvote_count":"2"},{"timestamp":"1670249340.0","comment_id":"735980","upvote_count":"1","content":"One point worth noting is that the question specified querying the files, not the file metadata, which makes C and D probably wrong","poster":"JayBee65"},{"comment_id":"735788","content":"Selected Answer: C\nI choose C","poster":"RBSK","timestamp":"1670231280.0","upvote_count":"2"},{"poster":"Incognito013","comment_id":"733325","content":"Selected Answer: B\nB - Keyword \"Random\"","timestamp":"1669945620.0","upvote_count":"1"},{"comment_id":"732878","timestamp":"1669914900.0","upvote_count":"1","content":"Selected Answer: B\nB is the correct and best choice","poster":"hpipit"},{"content":"Selected Answer: B\nof course the answer is a B","upvote_count":"1","comment_id":"732689","poster":"9014","timestamp":"1669903560.0"},{"content":"Selected Answer: B\nBetween B and C I will choose B because it gives a way to query the file data using Athena. There is no way given in C to query the file but just the ability to retreive.","upvote_count":"2","poster":"Wajif","comment_id":"727250","timestamp":"1669435740.0"},{"content":"Selected Answer: C\nhttps://aws.amazon.com/s3/pricing/\n\nA: S3 Glacier Instant Retrieval*** - For long-lived archive data accessed once a quarter with instant retrieval in milliseconds \n\nWhile the users access the files randomly (not specified once/quarter) - Answer A does not fit. \n\nB: When we are talking about S3 Intelligent Tiering - this is the most cost effective. However, Glacier Select defeats the purpose: it's focused on extracting data FROM a bigger file and not the file itself. \nhttps://aws.amazon.com/blogs/aws/s3-glacier-select/\nD: Involves RDS, increases the cost. Not much difference between Deep archive and instant retrieval (D vs C). \n\nI'd go for C.","poster":"ocbn3wby","timestamp":"1669053480.0","upvote_count":"5","comments":[{"timestamp":"1670231220.0","poster":"RBSK","content":"I do go for C.\n\nB - Not correct in my opinion, Athena and Glacier Select are meant for Big Data retrieval and read part of the file instead of complete object. In this case its Call transcript and B is not relevant","upvote_count":"2","comment_id":"735787"},{"comment_id":"735976","content":"I'm not sure what you mean by your definition of Glacier Select, but the link you specify says \"Glacier Select.. Most legacy archival solutions, like on premise tape libraries, have highly restricted data retrieval throughput and are unsuitable for rapid analytics or processing. If you want to make use of data stored on one of those tapes you might have to wait for weeks to get useful results. In contrast, *** cold data stored in Glacier can now be easily queried within minutes.***\nThis unlocks a lot of exciting new business value for your archived data. Glacier Select allows you to to perform filtering directly against a Glacier object using standard SQL statements.\"\nThis is what is required, so makes B the correct answer...","timestamp":"1670249040.0","poster":"JayBee65","upvote_count":"2"}],"comment_id":"723807"},{"content":"B is correct","upvote_count":"1","timestamp":"1669037700.0","comment_id":"723555","poster":"Wpcorgan"},{"upvote_count":"1","comment_id":"722534","content":"B is correct because it's the most cost effective solution overall for random access (S3 Intelligent-tiering) for first year AND cheaper option for S3 Glacier storage classes after one year (Glacier-flexible is cheaper than Glacier Instant)","timestamp":"1668940260.0","poster":"pepgua"},{"content":"I don't think S3 Glacier can do metadata search. Thus B is more reasonable.","poster":"hollie","comment_id":"721022","timestamp":"1668738000.0","upvote_count":"1"},{"poster":"grzeev","comment_id":"717338","timestamp":"1668347340.0","upvote_count":"1","content":"Selected Answer: B\nS3 Intelligent-Tiering is the first cloud object storage class that delivers automatic cost savings by moving data between two access tiers — frequent access and infrequent access — when access patterns change, and is ideal for data with unknown or changing access patterns"},{"poster":"Adrianjavier","upvote_count":"1","content":"Selected Answer: B\nB for sure","comment_id":"717012","timestamp":"1668299460.0"},{"timestamp":"1668274200.0","content":"Flexible tiering wins","comment_id":"716833","poster":"AbhiJo","upvote_count":"2"},{"comment_id":"716669","upvote_count":"1","timestamp":"1668253500.0","poster":"mabotega","content":"Selected Answer: B\nhttps://aws.amazon.com/pt/blogs/aws/s3-glacier-select/"},{"comments":[{"comment_id":"735982","content":"C only allows the file metadata to be queried but the users need to be able to query files, so how can it be correct? Glacier Select allows queries against the files themselves.","timestamp":"1670249520.0","upvote_count":"1","poster":"JayBee65"}],"comment_id":"713439","upvote_count":"1","content":"Answer should be C only.: Reasons:\nFile should be accessed as quickly as possible within 1 year. S3 Standard may be the choice. Overall solution should be cost efficient\nA -> Uses Glacier Instant retreival but the put/get costs are more than S3 standard. \nB -> Uses Intelligent tiering which may have overhead cost for intelligently doing the tiers. Glacier flexible retreival may have additional costs\nD -> Ruled out - they are using RDS which may incur additional costs compared to other options.","timestamp":"1667875440.0","poster":"KADSM"},{"poster":"17Master","comment_id":"709887","content":"Selected Answer: B\nS3 Intelligent-Tiering + Glacier Flexible Retrieval is more cost-effective.","upvote_count":"1","timestamp":"1667401200.0"},{"upvote_count":"1","timestamp":"1667030400.0","comment_id":"707054","content":"The answer looks B","poster":"SimonPark"},{"timestamp":"1666307280.0","poster":"BoboChow","content":"Selected Answer: C\nA: you need to distinguish the storage within one year.\nB:Select is not an archive retrieval option\nC:seems like right\nD:Querying the files from Amazon RDS is not a good choice","upvote_count":"2","comment_id":"700367","comments":[{"comment_id":"706876","timestamp":"1667007600.0","poster":"BoboChow","upvote_count":"1","content":"I changed my mind.\nCompared to Glacier Instant Retrieval, Glacier Flexible Retrieval is more cost-effective."}]},{"timestamp":"1666092240.0","upvote_count":"3","comment_id":"698167","content":"NOTE:\"A delay in retrieving older files is acceptable.\"\n\"For archive data that needs immediate access, such as medical images, news media assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost storage with milliseconds retrieval. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5-12 hours.\"\n\nhttps://aws.amazon.com/about-aws/whats-new/2021/11/amazon-s3-glacier-instant-retrieval-storage-class/","poster":"Kikiokiki"},{"content":"Selected Answer: A\nI think this is a bit tricky, as you have to pay attention to the sum of costs related at each answer:\nA: S3 Glacier Instant Retrieval for all storage (>1yo & +1yo)\nB: S3 Intelligent Tiering (>1yo storage) + S3 Glacier Flexible Retrieval (+1yo storage) + Amazon Athena + S3 Glacier Select !!!\nC: S3 Standard (>1yo storage) + S3 Glacier Instant Retrieval (+1yo storage)\nD: S3 Standard (>1yo storage) + S3 Glacier Deep Archive (+1yo storage) + Amazon RDS\n\nBased on this, I'd choose A, but not sure I can retrieve files by tags... If it's not possible, then the cheapest after A is C.","comment_id":"697197","poster":"123jhl0","timestamp":"1665997080.0","comments":[{"upvote_count":"2","comment_id":"723806","timestamp":"1669053420.0","content":"https://aws.amazon.com/s3/pricing/\n\nA: S3 Glacier Instant Retrieval*** - For long-lived archive data accessed once a quarter with instant retrieval in milliseconds \n\nWhile the users access the files randomly (not specified once/quarter) - Answer A does not fit. \n\nB: When we are talking about S3 Intelligent Tiering - this is the most cost effective. However, Glacier Select defeats the purpose: it's focused on extracting data FROM a bigger file and not the file itself. \nhttps://aws.amazon.com/blogs/aws/s3-glacier-select/\nD: Involves RDS, increases the cost. Not much difference between Deep archive and instant retrieval (D vs C). \n\nI'd go for C.","poster":"ocbn3wby"}],"upvote_count":"3"},{"upvote_count":"1","timestamp":"1665822600.0","poster":"BadouBoy","content":"I think its D. Could you please check re-again this question and explain to me ?","comment_id":"695282"}],"exam_id":31,"unix_timestamp":1665531840},{"id":"m2xGvqbeFEOcg3R9rRSJ","topic":"1","choices":{"B":"Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.","A":"Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.","D":"Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis. Turn on point-in-time recovery for the table.","C":"Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket."},"discussion":[{"content":"Selected Answer: B\nContinuous backups is a native feature of DynamoDB, it works at any scale without having to manage servers or clusters and allows you to export data across AWS Regions and accounts to any point-in-time in the last 35 days at a per-second granularity. Plus, it doesn’t affect the read capacity or the availability of your production tables.\n\nhttps://aws.amazon.com/blogs/aws/new-export-amazon-dynamodb-table-data-to-data-lake-amazon-s3/","poster":"elmogy","comment_id":"908728","upvote_count":"12","timestamp":"1701194760.0"},{"comment_id":"1117024","timestamp":"1720466820.0","poster":"awsgeek75","content":"Selected Answer: B\nA: Impacts RCU \nC: Requires coding of Lambda to read from stream to S3\nD: More coding in Lambda\nB: AWS Managed solution with no coding","upvote_count":"5"},{"comment_id":"1063255","upvote_count":"4","content":"Selected Answer: B\nDynamoDB export to S3 is a fully managed solution for exporting DynamoDB data to an Amazon S3 bucket at scale.","poster":"potomac","timestamp":"1714933980.0"},{"upvote_count":"4","poster":"baba365","timestamp":"1711498860.0","comment_id":"1018222","content":"A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table… for C.U.D events ( Create, Update, Delete) and its logs are retained for only 24hrs ."},{"comment_id":"988425","timestamp":"1708708320.0","content":"Selected Answer: B\nExport the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.","poster":"Guru4Cloud","upvote_count":"3"},{"timestamp":"1707292680.0","poster":"ukivanlamlpi","upvote_count":"4","comment_id":"974431","content":"Selected Answer: C\ncontinous backup, no impact to availability ==> DynamoDB stream\nB. export is one off, noy continuous and demand on read capacity"},{"poster":"hsinchang","comment_id":"963260","upvote_count":"4","content":"minimal amount of coding rules out Lambda","timestamp":"1706243400.0"},{"comment_id":"939103","upvote_count":"2","comments":[{"poster":"pentium75","timestamp":"1719810720.0","comment_id":"1110998","content":"ChatGPT is usually wrong on these topics.","upvote_count":"2"},{"content":"Wrong.\n\"DynamoDB full exports are charged based on the size of the DynamoDB table (table data and local secondary indexes) at the point in time for which the export is done. DynamoDB incremental exports are charged based on the size of data processed from your continuous backups for the time period being exported.\"\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/S3DataExport.HowItWorks.html","upvote_count":"1","timestamp":"1719028980.0","poster":"Gajendr","comment_id":"1103178"}],"content":"ChatGpt answer is C and it indicates continuous backup process uses DynamoDB stream actually","timestamp":"1703949120.0","poster":"Chris22usa"},{"upvote_count":"1","comment_id":"934034","timestamp":"1703572440.0","poster":"TariqKipkemei","content":"Selected Answer: B\nUsing DynamoDB table export, you can export data from an Amazon DynamoDB table from any time within your point-in-time recovery window to an Amazon S3 bucket. Exporting a table does not consume read capacity on the table, and has no impact on table performance and availability."},{"upvote_count":"2","timestamp":"1701176580.0","comment_id":"908561","poster":"norris81","content":"Selected Answer: B\nhttps://repost.aws/knowledge-center/back-up-dynamodb-s3\nhttps://aws.amazon.com/blogs/aws/new-amazon-dynamodb-continuous-backups-and-point-in-time-recovery-pitr/\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\n\nThere is no edit"},{"content":"Selected Answer: B\nContinuous Backups: DynamoDB provides a feature called continuous backups, which automatically backs up your table data. Enabling continuous backups ensures that your table data is continuously backed up without the need for additional coding or manual interventions.\n\nExport to Amazon S3: With continuous backups enabled, DynamoDB can directly export the backups to an Amazon S3 bucket. This eliminates the need for custom coding to export the data.\n\nMinimal Coding: Option B requires the least amount of coding effort as continuous backups and the export to Amazon S3 functionality are built-in features of DynamoDB.\n\nNo Impact on Availability and RCUs: Enabling continuous backups and exporting data to Amazon S3 does not affect the availability of your application or the read capacity units (RCUs) defined for the table. These operations happen in the background and do not impact the table's performance or consume additional RCUs.","comment_id":"901442","poster":"cloudenthusiast","timestamp":"1700338320.0","upvote_count":"4"},{"timestamp":"1700303220.0","content":"Selected Answer: B\nDynamoDB Export to S3 feature\nUsing this feature, you can export data from an Amazon DynamoDB table anytime within your point-in-time recovery window to an Amazon S3 bucket.","poster":"Efren","upvote_count":"3","comment_id":"900918"},{"timestamp":"1700303100.0","poster":"Efren","upvote_count":"3","content":"B also for me","comment_id":"900915"},{"timestamp":"1700255760.0","upvote_count":"1","comment_id":"900457","comments":[],"poster":"norris81","content":"https://repost.aws/knowledge-center/back-up-dynamodb-s3\nhttps://aws.amazon.com/blogs/aws/new-amazon-dynamodb-continuous-backups-and-point-in-time-recovery-pitr/\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html"}],"answer_ET":"B","question_images":[],"answer_description":"","answers_community":["B (89%)","11%"],"exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/109577-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":455,"unix_timestamp":1684350960,"answer":"B","isMC":true,"timestamp":"2023-05-17 21:16:00","answer_images":[],"question_text":"A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are defined for the table.\n\nWhich solution meets these requirements?"}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","id":31,"isBeta":false,"isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","numberOfQuestions":1019,"isImplemented":true},"currentPage":91},"__N_SSP":true}