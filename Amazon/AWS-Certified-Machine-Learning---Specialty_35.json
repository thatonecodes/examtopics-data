{"pageProps":{"questions":[{"id":"PCmydsz48TbmmTjurgyo","exam_id":26,"answer_images":[],"question_id":171,"url":"https://www.examtopics.com/discussions/amazon/view/112462-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"answer":"CD","answer_ET":"CD","discussion":[{"timestamp":"1694523420.0","content":"Selected Answer: CD\nA. NO - we can't change the model for transfer learning\nB. NO - we can't change the model for transfer learning\nC. YES - lst file is how we give input to SageMaker (https://medium.com/@texasdave2/itty-bitty-lst-file-format-converter-for-machine-learning-image-classification-on-aws-sagemaker-b3828c7ba9cc)\nD. YES - obvious \nE. NO - there is no extra metadata we want to provide (https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html)","upvote_count":"5","comment_id":"1005765","poster":"loict"},{"upvote_count":"1","comment_id":"1293376","timestamp":"1728112920.0","content":"Selected Answer: CD\n.lst file is the correct way to pass metadata of training files to sagemaker training job.\nC is correct not E.\n\nno doubt for D.","poster":"MJSY"},{"poster":"JonSno","comment_id":"1206723","comments":[{"poster":"cloudera3","comment_id":"1248376","upvote_count":"1","content":"Link for more information on the .lst requirement for training and validation channels: https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html#IC-inputoutput","timestamp":"1721049600.0"}],"upvote_count":"3","timestamp":"1714868760.0","content":"Create a .lst File (Option C):\nExplanation: The .lst file is a standard format used with Amazon SageMaker's image classification algorithm, which lists image files and their corresponding labels. This file is crucial for SageMaker to read and map the images correctly for training purposes. The .lst file needs to be uploaded to Amazon S3.\nInitiate Transfer Learning (Option D):\nExplanation: Transfer learning allows you to leverage pre-trained weights from existing models (like ImageNetV2) and fine-tune them using your own data. In this case, training with the 10,000 new labeled images helps the model recognize less common animal species. Transfer learning is more efficient since the model has already been trained on similar data."},{"timestamp":"1709569680.0","poster":"AIWave","content":"Selected Answer: DE\nA - no need for full training only transfer learning\nB - no built in inception model in sagemaker and no transfer learning\nC - no data augmentation could introduce inaccuracies \nD - yes! Transfer learning \nE - yes, augmentation improves accuracy","comment_id":"1165792","upvote_count":"1"},{"poster":"endeesa","timestamp":"1701206220.0","content":"Selected Answer: DE\nWe need to augment the existing images so E makes sense","comment_id":"1082958","upvote_count":"1"},{"timestamp":"1699186860.0","poster":"giustino98","upvote_count":"4","content":"Selected Answer: CD\nA False - no transfer learning\nB False - no transfer learning\nC True - \"If you use the Image format for training, specify train, validation, train_lst, and validation_lst channels as values for the InputDataConfig parameter of the CreateTrainingJob request. Specify the individual image data (.jpg or .png files) for the train and validation channels. Specify one .lst file in each of the train_lst and validation_lst channels. Set the content type for all four channels to application/x-image.\"\nD True - it uses transfer learning\nE False - \"To include metadata with your dataset in a training job, use an augmented manifest file. \" Here we don't have any metadata","comment_id":"1062872"},{"timestamp":"1698208440.0","upvote_count":"1","content":"Selected Answer: DE\nI would go with E. as we have a hint in the question, that we need to use a Pipe mode, and E is used for pipe mode","poster":"DimLam","comment_id":"1053447"},{"content":"Selected Answer: DE\nD is obvious. Reason for option E is because they want to train model in Pipe mode and using an augmented manifest file in JSON Lines format enables trraining in Pipe mode as per this link - https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\nRefer to the section - Train with Augmented Manifest Image Format in this link for more details.\nUsing an augmented manifest file is an alternative to preprocessing when you have labeled data. For training jobs using labeled data, you typically need to preprocess the dataset to combine input data with metadata before training. If your training dataset is large, preprocessing can be time consuming and expensive.","timestamp":"1696712940.0","upvote_count":"2","poster":"backbencher2022","comment_id":"1027617"},{"comment_id":"997459","content":"Selected Answer: DE\nD is obvious. Keyword here is pipe mode. \"The augmented manifest format enables you to do training in Pipe mode using image files without needing to create RecordIO files.\" Hence, E.","poster":"goku58","timestamp":"1693730340.0","upvote_count":"2"},{"timestamp":"1692776340.0","comment_id":"988064","poster":"Mickey321","content":"Selected Answer: DE\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html","upvote_count":"1"},{"comment_id":"984496","content":"Selected Answer: DE\nLetter A - B deviate from what is asked in the scope of the question. Correct alternatives are E - D.\nTo understand that C is wrong, look here: https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\n\nTL;DR - .lst file is only for classification task","poster":"kaike_reis","upvote_count":"2","timestamp":"1692363120.0"},{"timestamp":"1690304460.0","upvote_count":"2","content":"Selected Answer: DE\nAugmented manifest format enables you to do training in Pipe mode using files without needing to create RecordIO files (.rec)","comment_id":"962915","poster":"awsarchitect5"},{"content":"D+E https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html","comment_id":"946122","poster":"ADVIT","timestamp":"1688786700.0","upvote_count":"1"},{"upvote_count":"4","timestamp":"1687999140.0","poster":"brianb08","comment_id":"937285","content":"Selected Answer: CD\nC. Create a .lst file that contains a list of image files and corresponding class labels. Upload the .lst file to Amazon S3.\nD. Initiate transfer learning. Train the model by using the images of less common species.\n\nDetails provided in this blog post:\nhttps://aws.amazon.com/blogs/machine-learning/classify-your-own-images-using-amazon-sagemaker/"},{"comment_id":"926033","upvote_count":"1","timestamp":"1687012380.0","content":"Selected Answer: DE\nCorrect Ans DE","poster":"SandeepGun"}],"question_images":[],"answers_community":["CD (52%)","DE (48%)"],"unix_timestamp":1687012380,"answer_description":"","question_text":"A company is creating an application to identify, count, and classify animal images that are uploaded to the company’s website. The company is using the Amazon SageMaker image classification algorithm with an ImageNetV2 convolutional neural network (CNN). The solution works well for most animal images but does not recognize many animal species that are less common.\n\nThe company obtains 10,000 labeled images of less common animal species and stores the images in Amazon S3. A machine learning (ML) engineer needs to incorporate the images into the model by using Pipe mode in SageMaker.\n\nWhich combination of steps should the ML engineer take to train the model? (Choose two.)","choices":{"D":"Initiate transfer learning. Train the model by using the images of less common species.","E":"Use an augmented manifest file in JSON Lines format.","A":"Use a ResNet model. Initiate full training mode by initializing the network with random weights.","B":"Use an Inception model that is available with the SageMaker image classification algorithm.","C":"Create a .lst file that contains a list of image files and corresponding class labels. Upload the .lst file to Amazon S3."},"topic":"1","timestamp":"2023-06-17 16:33:00"},{"id":"39gT4eOQPqDLHlXkQ8PU","unix_timestamp":1687184940,"choices":{"B":"Use Amazon SageMaker Feature Store to store features for model training and inference. Create an online store for both online inference and model training. Create an IAM role for data scientists to access and search through feature groups.","C":"Create one Amazon S3 bucket to store online inference features. Create a second S3 bucket to store offline model training features. Turn on versioning for the S3 buckets and use tags to specify which tags are for online inference features and which are for offline model training features. Use Amazon Athena to query the S3 bucket for online inference. Connect the S3 bucket for offline model training to a SageMaker training job. Create an IAM policy that allows data scientists to access both buckets.","D":"Create two separate Amazon DynamoDB tables to store online inference features and offline model training features. Use time-based versioning on both tables. Query the DynamoDB table for online inference. Move the data from DynamoDB to Amazon S3 when a new SageMaker training job is launched. Create an IAM policy that allows data scientists to access both tables.","A":"Use Amazon SageMaker Feature Store to store features for model training and inference. Create an online store for online inference. Create an offline store for model training. Create an IAM role for data scientists to access and search through feature groups."},"timestamp":"2023-06-19 16:29:00","answers_community":["A (92%)","8%"],"isMC":true,"topic":"1","discussion":[{"comment_id":"927604","upvote_count":"8","content":"Selected Answer: A\nAnswer is A - \"SageMaker Feature Store consists of an online and an offline mode for managing features. The online store is used for low-latency real-time inference use cases. The offline store is primarily used for batch predictions and model training.\" https://aws.amazon.com/blogs/machine-learning/speed-ml-development-using-sagemaker-feature-store-and-apache-iceberg-offline-store-compaction/","timestamp":"1703003340.0","poster":"asdfzxc"},{"content":"Selected Answer: B\nBest Choice: B. Amazon SageMaker Feature Store\n\nThis option offers a centralized and efficient solution that meets all the requirements:\n\nFeature Storage: SageMaker Feature Store acts as a single repository for features used in both offline training and online inference.\nOnline Store: Creating an online store within Feature Store eliminates the need for a separate S3 bucket for inference features, simplifying management.\nFeature History Tracking: Feature Store automatically tracks feature lineage and versions, allowing you to see changes and roll back if needed.\nData Science Team Access: IAM roles can be created to grant data scientists access to search and explore features within Feature Groups in the store.","upvote_count":"1","comment_id":"1176616","poster":"Pannu29","timestamp":"1726668360.0"},{"poster":"endeesa","timestamp":"1716923880.0","upvote_count":"1","comment_id":"1082961","content":"Selected Answer: A\nA meets all requirements, and looks like the easiest to setup"},{"timestamp":"1710255660.0","comment_id":"1005771","upvote_count":"1","poster":"loict","content":"Selected Answer: A\nA. YES - online store will be faster for inference, offline store cheaper for batch\nB. NO - online store for offline will be too expensive\nC. NO - want to use Feature store\nD. NO - want to use Feature store"},{"comment_id":"988225","upvote_count":"1","timestamp":"1708694580.0","poster":"Mickey321","content":"Selected Answer: A\nAmazon SageMaker Feature Store is a managed service that makes it easy to store and manage features for machine learning models. It provides a scalable and reliable way to store features, and it supports both online inference and offline model training.\nCreating separate online and offline stores in SageMaker Feature Store will allow the music streaming company to optimize the storage and performance of their features for each use case. The online store can be configured to be highly available and performant, while the offline store can be configured to be cost-effective and scalable."}],"answer_images":[],"answer_ET":"A","exam_id":26,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/112605-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","question_text":"A music streaming company is building a pipeline to extract features. The company wants to store the features for offline model training and online inference. The company wants to track feature history and to give the company’s data science teams access to the features.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","answer":"A","question_id":172},{"id":"OeCG9Jl7d6XaassiShfm","topic":"1","isMC":true,"choices":{"B":"Use an object detection algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an XGBoost algorithm to determine hair style and hair color.","D":"Use a semantic segmentation algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an XGBoost algorithm to determine hair style and hair.","A":"Use an object detection algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an ResNet-50 algorithm to determine hair style and hair color.","C":"Use a semantic segmentation algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an ResNet-50 algorithm to determine hair style and hair color."},"unix_timestamp":1687291140,"discussion":[{"poster":"587df71","timestamp":"1735252860.0","content":"Selected Answer: C\nGoing for object detection to identify hair is easy but customer needs hair style. So in this contest I believe Semantic Segmentation does a good job.","comment_id":"1332141","upvote_count":"1"},{"comment_id":"1301224","upvote_count":"1","content":"Selected Answer: C\nDefinitely 'C' Semantic Segmentation is the algorithm for colour segmentation like hair","poster":"MultiCloudIronMan","timestamp":"1729536540.0"},{"comment_id":"1284475","poster":"Tkhan1","content":"Selected Answer: C\nC. Semantic Segmentation allows for pixel-wise classification of the video frames, meaning it can precisely identify and isolate a visitor’s hair by labeling each pixel in the image as belonging to hair (or other categories).Object detection uses bounding boxes, which would not effectively isolate hair, especially in cases where hair might not have clear boundaries.","upvote_count":"1","timestamp":"1726462560.0"},{"content":"Selected Answer: A\nCopied from ChatGPT:\n\"Semantic segmentation provides a pixel-level classification of the image, meaning it labels each pixel in the image with the class of the object it belongs to. However, it does not inherently detect whether the object is present in the image. Instead, it assumes that the objects of interest are already present and segments the image accordingly.\"\nSince the input is video stream, Not all the frames(images) contain hair! Therefore I would go for A.","upvote_count":"1","comment_id":"1235841","timestamp":"1719151080.0","poster":"1e6c709"},{"comment_id":"1205212","poster":"rookiee1111","timestamp":"1714595220.0","content":"Selected Answer: C\nsemantic segmentation identifies hair, and Resnet for type and color.","upvote_count":"2"},{"upvote_count":"2","timestamp":"1714140420.0","poster":"Denise123","comment_id":"1202648","content":"Selected Answer: A\nI was sure that it was option C but but when we want to select the option requiring the least amount of effort, it must be A. Hair color is detected by ResNet-50, not by semantic algorithm. So, object detection algorithms are generally easier to implement and fine-tune compared to semantic segmentation algorithms. They can accurately locate and extract specific objects, such as hair, from the video frames, simplifying the subsequent analysis. Additionally, ResNet-50 is a widely used pre-trained model for image classification tasks, making it relatively straightforward to apply for determining hair style and hair color"},{"upvote_count":"1","timestamp":"1712945460.0","content":"Selected Answer: D\nWe need Semantic Segmentation to identify the hair style and color by pixel level mapping","poster":"vkbajoria","comments":[{"content":"I mean to select \"C\"","timestamp":"1712945460.0","comment_id":"1194469","upvote_count":"1","poster":"vkbajoria"}],"comment_id":"1194468"},{"timestamp":"1707283980.0","poster":"kyuhuck","content":"Selected Answer: C\nCHAT GPT4= C","comments":[{"timestamp":"1711045800.0","upvote_count":"1","content":"Claude 3 Sonnet = C","comment_id":"1179498","poster":"F1Fan"}],"comment_id":"1142971","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: C\nI doubt that object detection will detect hair better than a semantic segmentation","timestamp":"1701206460.0","poster":"endeesa","comment_id":"1082963"},{"timestamp":"1697813580.0","poster":"backbencher2022","content":"Selected Answer: A\nWill go with A (Object detection) as semantic segmentation requires labelling every pixel in a picture which is more effort compared to object detection","comment_id":"1048902","upvote_count":"1"},{"comment_id":"1010117","timestamp":"1694992380.0","poster":"jopaca1216","upvote_count":"3","content":"C is the right.\nSemantic Segmentation = Pixel Level category assingment\nRestnet50 = used for image recognition and computer vision tasks"},{"content":"C\nThe backbone is a network that produces reliable activation maps of image features. The decoder is a network that constructs the segmentation mask from the encoded activation maps. Amazon SageMaker semantic segmentation provides a choice of pre-trained or randomly initialized ResNet50 or ResNet101 as options for backbones. The backbones come with pre-trained artifacts that were originally trained on the ImageNet classification task. These are reliable pre-trained artifacts that users can use to fine-tune their FCN or PSP backbones for segmentation. Alternatively, users can initialize these networks from scratch. Decoders are never pre-trained.\n\nSemantic Segmentation algorithm is now available in Amazon SageMaker | AWS Machine Learning Blog\nhttps://aws.amazon.com/cn/blogs/machine-learning/semantic-segmentation-algorithm-is-now-available-in-amazon-sagemaker/","upvote_count":"1","poster":"boledadian","comment_id":"990651","timestamp":"1693046340.0"},{"upvote_count":"1","content":"Selected Answer: A\nLetters B - D are wrong as they ultimately use a tabular classification model for an image problem, so we discard it. As we want a solution with the least effort, it is known that object detection requires less training effort than semantic segmentation, in addition to being able to keep the visitor's hair in the frame. Therefore, the correct alternative is Letter A.","poster":"kaike_reis","timestamp":"1692363420.0","comment_id":"984502"},{"content":"Selected Answer: A\nSegmentation is too heavy","poster":"marcoforexam","comment_id":"982628","upvote_count":"2","timestamp":"1692192840.0"},{"poster":"Mickey321","upvote_count":"1","comment_id":"972456","content":"Selected Answer: C\nUsing ResNet-50, you can determine hair style and hair color by passing the identified hair region from the semantic segmentation algorithm as input. ResNet-50 can classify the hair region into one of the 1000 categories from the ImageNet database, such as curly, straight, blonde, brunette, etc.\n\nOption C is the best option for your problem because it allows you to efficiently and accurately identify and classify a visitor’s hair in video frames using two powerful deep learning algorithms: semantic segmentation and ResNet-50.","timestamp":"1691178600.0"},{"comment_id":"962900","timestamp":"1690303380.0","poster":"awsarchitect5","upvote_count":"2","content":"Selected Answer: A\nXgboost not for Image detection"},{"poster":"ADVIT","content":"ChatGPT say it's A.","upvote_count":"1","timestamp":"1688786940.0","comment_id":"946125"},{"upvote_count":"2","poster":"RRST","timestamp":"1687291140.0","content":"A\n\nLeast Effort","comment_id":"928764"}],"answer_images":[],"answers_community":["A (47%)","C (47%)","5%"],"answer_ET":"A","timestamp":"2023-06-20 21:59:00","question_id":173,"answer":"A","question_text":"A beauty supply store wants to understand some characteristics of visitors to the store. The store has security video recordings from the past several years. The store wants to generate a report of hourly visitors from the recordings. The report should group visitors by hair style and hair color.\n\nWhich solution will meet these requirements with the LEAST amount of effort?","answer_description":"","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/112719-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26},{"id":"oYtRPTs3rlaj0AefAfCE","unix_timestamp":1687012860,"answers_community":["C (100%)"],"answer_ET":"C","timestamp":"2023-06-17 16:41:00","answer_description":"","question_id":174,"isMC":true,"answer_images":[],"exam_id":26,"discussion":[{"comment_id":"1186461","timestamp":"1727646060.0","poster":"vkbajoria","upvote_count":"1","content":"Selected Answer: C\nSageMaker Clarify can do the work"},{"comment_id":"1082964","poster":"endeesa","content":"Selected Answer: C\nThe model is trained already, so C. I imagine using a debugger in production is nuts","upvote_count":"1","timestamp":"1716924420.0"},{"timestamp":"1708693680.0","content":"Selected Answer: C\nThe solution that will meet these requirements with the least development effort is C. Using SageMaker Clarify to generate the explanation report, and attaching the report to the predicted results . This solution allows the financial services company to use SageMaker Clarify, a feature that provides machine learning (ML) model transparency and explainability, to generate the explanation report for each loan approval prediction. SageMaker Clarify can provide feature importance scores, which indicate how much each feature contributes to the prediction, and SHAP values, which measure how each feature affects the prediction compared to the average prediction . The company can use these metrics to generate and attach the explanation report that contains the reason for why the customer was approved or denied for a loan.","upvote_count":"2","poster":"Mickey321","comment_id":"988217"},{"poster":"ADVIT","upvote_count":"1","timestamp":"1704691860.0","comment_id":"946126","content":"C, SageMaker Clarify can give explanation Why."},{"poster":"SandeepGun","content":"Selected Answer: C\nSagemaker Clarity is better option","timestamp":"1702831260.0","comment_id":"926037","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/amazon/view/112463-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"C","choices":{"A":"Use SageMaker Model Debugger to automatically debug the predictions, generate the explanation, and attach the explanation report.","C":"Use SageMaker Clarify to generate the explanation report. Attach the report to the predicted results.","B":"Use AWS Lambda to provide feature importance and partial dependence plots. Use the plots to generate and attach the explanation report.","D":"Use custom Amazon CloudWatch metrics to generate the explanation report. Attach the report to the predicted results."},"question_images":[],"topic":"1","question_text":"A financial services company wants to automate its loan approval process by building a machine learning (ML) model. Each loan data point contains credit history from a third-party data source and demographic information about the customer. Each loan approval prediction must come with a report that contains an explanation for why the customer was approved for a loan or was denied for a loan. The company will use Amazon SageMaker to build the model.\n\nWhich solution will meet these requirements with the LEAST development effort?"},{"id":"IpLZsxEpc113JXlBV3z3","question_images":[],"answer_description":"","choices":{"B":"A scatter plot with points colored by target variable that uses t-Distributed Stochastic Neighbor Embedding (t-SNE) to visualize the large number of input variables in an easier-to-read dimension.","C":"A scatter plot showing the performance of the objective metric over each training iteration.","A":"A histogram showing whether the most important input feature is Gaussian.","D":"A scatter plot showing the correlation between maximum tree depth and the objective metric."},"answer":"D","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/10264-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"content":"This is a very tricky question. The idea is to reconfigure the ranges of the hyperparameters. A refers to a feature, not a hyperparameter. A is out. C refers to training the model, not optimizing the range of hyperparameters. C is out. Now it gets tricky. D will let you find determine what the approximately best tree depth is. That's good. That's what you're trying to do but it's only one of many hyperparameters. It's the best choice so far. B is tricky. t-SNE does help you visualize multidimensional data but option B refers to input variables, not hyperparameters. For this very tricky question, I would do with D. It's the only one that accomplishes the task of limiting the range of a hyperparameter, even if it is only one of them.","comments":[{"timestamp":"1634277060.0","poster":"cnethers","comment_id":"282670","content":"It's good to see someone keeping a thoughtful and curious mind to this question. I too have the same conclusion, not an easy question.","upvote_count":"3"},{"timestamp":"1656211620.0","poster":"ovokpus","content":"But, how do you optimize hyperparameters without training experiments? That is why C is the best option. You get a value for each unique combination of hyperparameters.","comment_id":"622322","upvote_count":"1"},{"upvote_count":"1","content":"B is also wrong as t-SNE picture is not actionable - good visual but ... that's it. try pictures here https://lvdmaaten.github.io/tsne/","timestamp":"1636186440.0","poster":"Dr_Kiko","comment_id":"433778"},{"content":"When you are tuning hyperparameters you are literally training multiple models and searching for the best ones.","upvote_count":"2","timestamp":"1645386420.0","poster":"AddiWei","comment_id":"552221"}],"upvote_count":"48","poster":"cloud_trail","timestamp":"1634014140.0","comment_id":"278177"},{"poster":"heihei","upvote_count":"14","comment_id":"29244","timestamp":"1632148080.0","content":"B doesn't make sense\nI think it's D"},{"upvote_count":"1","poster":"JonSno","timestamp":"1739834340.0","content":"Selected Answer: D\nThe goal is to reduce training time and costs by optimizing the hyperparameter tuning process. In tree-based ensemble models (e.g., XGBoost, Random Forest, or Gradient Boosting), tree depth is one of the most influential hyperparameters affecting:\n\nModel complexity: Deeper trees increase complexity but may lead to overfitting.\nTraining time: More depth means more splits, significantly increasing computation.\nPerformance (AUC score in this case): There is typically an optimal depth that balances underfitting and overfitting.\nA scatter plot showing the correlation between tree depth and the AUC metric will allow the ML Specialist to:\n\nIdentify whether increasing depth leads to diminishing returns.\nChoose an optimal tree depth that balances performance with training efficiency.\nReduce the search space of hyperparameters, speeding up tuning and lowering costs.","comment_id":"1358025"},{"comment_id":"1231910","content":"A. No, doesn't help to set/reduce hyperparameter value/range\nB. No, honestly this is gibberish to me\nC. No, doesn't help to reduce hyperparameter value range\nD. YES, this help me understand how to set max tree depth hyperparameter","timestamp":"1718623860.0","upvote_count":"1","poster":"ninomfr64"},{"upvote_count":"1","content":"Option C.\nSee it is doing a scatter plot on the metric for each iteration.\nEach iteration is running with a certain set of hyper parameters.\nSo if I plot this. and I find which iteration has the best metric, I could simply pick up those set of hyperparameters.\nD will only led to the tuning of maximum tree depth.\nI am not sure which option would satisfy the goal to decrease cost but just looking at maximum tree depth doesnt seem right to me. It might be a way to just look at the tree depth and tune just that parameter and since you are only tuning 1 paramter, it may be cheaper, but would that lead to a usable model?\nI think it should be option C.","poster":"VR10","comment_id":"1154074","timestamp":"1708356780.0"},{"timestamp":"1705635360.0","poster":"Regu7","upvote_count":"1","content":"On what basis the correct answers are provided in this platform? Are they assuming this is the correct answer or it is taken from somewhere ?","comment_id":"1126395"},{"timestamp":"1699600020.0","poster":"elvin_ml_qayiran25091992razor","comment_id":"1066995","content":"Selected Answer: D\nD IS THE CORRECT","upvote_count":"1"},{"upvote_count":"1","poster":"Rejju","timestamp":"1694659860.0","content":"Selected Answer: C\nOption D, can also be useful in hyperparameter tuning for tree-based ensemble models, especially if the maximum tree depth is one of the hyperparameters you want to optimize.\nHowever, when the goal is to decrease training time and costs by reconfiguring input hyperparameter ranges, a scatter plot showing the performance of the objective metric over each training iteration (Option C) is generally more directly related to the hyperparameter tuning process. It helps you track how the model's performance changes during hyperparameter tuning, which is critical for making decisions about which hyperparameter ranges to explore further.\nOption D is valuable for understanding the relationship between maximum tree depth and the objective metric, but it might not provide as comprehensive insights into the overall hyperparameter tuning process compared to Option C.","comment_id":"1007125"},{"comment_id":"1006435","poster":"loict","timestamp":"1694598420.0","upvote_count":"1","content":"Selected Answer: D\nA. NO - it is about data discovery\nB. NO - it is about data discovery\nC. MIGHT - (NO) is a training iteration the overnight training the question is referring to ? (YES) Or each HPO training within each night ?\nD. YES - the less ambiguous answer"},{"upvote_count":"1","timestamp":"1694439660.0","comment_id":"1004863","poster":"DavidRou","content":"I think that C should be the right answer. The specialist can monitor how the model works by changing hyperparameters' values in each training iteration."},{"content":"Selected Answer: D\nOption D","upvote_count":"1","timestamp":"1693329660.0","comment_id":"993340","poster":"Mickey321"},{"timestamp":"1690484820.0","poster":"kaike_reis","comment_id":"964981","upvote_count":"3","content":"Selected Answer: D\nA and B are wrong, because is totally out of question context. C is for monitoring a model, it doesn't help to change your HP range. D is the only answer that applies to the question."},{"content":"Selected Answer: C\nI think it should be c","poster":"Venkatesh_Babu","upvote_count":"1","comment_id":"961681","timestamp":"1690207620.0"},{"timestamp":"1687225380.0","content":"Selected Answer: C\nBy plotting the performance of the objective metric (AUC) over each training iteration, the Specialist can analyze how different hyperparameter configurations affect the model's performance. This visualization helps in understanding which hyperparameter combinations lead to better results and allows the Specialist to identify areas of improvement.","poster":"CKS1210","upvote_count":"1","comment_id":"928084"},{"timestamp":"1685802900.0","comment_id":"913684","poster":"mirik","content":"D: By analyzing this relationship, the Specialist can adjust the range of maximum tree depth values used during hyperparameter tuning to decrease training time and costs.","upvote_count":"1"},{"content":"Selected Answer: D\nD Seems like the best answer. When answer is considered correct who is making that call an is there any justification provided for us to learn from?","timestamp":"1684885800.0","comment_id":"905342","upvote_count":"2","poster":"earthMover"},{"upvote_count":"2","timestamp":"1678264200.0","comment_id":"832699","poster":"Valcilio","content":"Selected Answer: D\nIt's about parameters, not about dimensionality."},{"poster":"hug_c0sm0s","timestamp":"1677502680.0","content":"Selected Answer: D\nBING chat chooses this answer, and provides an explanation:The correct answer is D. A scatter plot showing the correlation between maximum tree depth and the objective metric. This visualization will help the Machine Learning Specialist to understand how changes in maximum tree depth affect the performance of the model with respect to the objective metric (AUC). By analyzing this relationship, the Specialist can adjust the range of maximum tree depth values used during hyperparameter tuning to decrease training time and costs.","upvote_count":"1","comment_id":"823693"},{"upvote_count":"1","content":"Selected Answer: C\n- C\nThis visualization will allow the Machine Learning Specialist to track the performance of the model over time as different hyperparameter configurations are tried. By analyzing this plot, the Specialist can identify which hyperparameters are leading to better model performance and adjust the input hyperparameter range(s) accordingly, with the goal of decreasing the time it takes to train the models and decrease costs.","comment_id":"814386","comments":[{"content":"Agree, this is my thought process too. I hope the exam is no where near this nebulous","poster":"cpal012","timestamp":"1680634920.0","comment_id":"861426","upvote_count":"1"}],"timestamp":"1676831820.0","poster":"bakarys"},{"comment_id":"758163","poster":"mavios","timestamp":"1672119300.0","content":"The correct answer is definitely D. Because it is talking about Hyper Parameter, it also is requiring the specialist to reduce time and cost. For an ensemble of trees model, the only hyper parameters that affect cost is the number of trees and the maximum depth of trees. and only D is speaking to these two facts","upvote_count":"2"},{"upvote_count":"1","content":"I agree with D, just that it mentions only one hyperparameter.","comment_id":"739563","poster":"Peeking","timestamp":"1670540100.0"},{"poster":"[Removed]","content":"Selected Answer: D\nd is correct","upvote_count":"1","timestamp":"1655221380.0","comment_id":"616298"},{"timestamp":"1645350180.0","comment_id":"551679","upvote_count":"1","poster":"John_Pongthorn","content":"hyperparameter combination, this phrase should appear in one of the choices"},{"poster":"John_Pongthorn","content":"I read form somewhere reliable.\nAwnseer Choice shoud be\nUse a scatter plot to visualize the results for each Area Under the Curve (AUC)-hyperparameter combination.","upvote_count":"1","comment_id":"551676","timestamp":"1645350120.0"},{"timestamp":"1644456660.0","comment_id":"544231","poster":"AddiWei","upvote_count":"1","content":"Objective is \"reduce the time it takes to train\". You reduce the time by reducing training iterations."},{"upvote_count":"2","poster":"technoguy","content":"Linear SVM is wrong. since the data cannot be separated with line","timestamp":"1636227240.0","comment_id":"439032"},{"comment_id":"433777","timestamp":"1635919020.0","poster":"Dr_Kiko","content":"A makes no sense; B is visualization (educate yourself how a t-sne picture looks like - good luck using it for tuning). Leaning towards D","upvote_count":"1"},{"content":"B is answer. D is identify only one hyperparameter. I think there are several hyperparameters need to be combined to get the best AUC value. And that is the t-SNE used for.","comment_id":"402479","timestamp":"1634627220.0","comments":[{"content":"Ah, may be I was wrong. B is to \"visualize the large number of input variables\". D makes sense regarding that max_depth is to simplify the model and decrease the training time.","timestamp":"1634795280.0","upvote_count":"1","poster":"Huy","comment_id":"402493"}],"upvote_count":"1","poster":"Huy"},{"comment_id":"296947","upvote_count":"4","poster":"SophieSu","timestamp":"1634529300.0","content":"Focus on 2 points: Tree Model and Hyperparameter tuning. Definitely D"},{"timestamp":"1633850880.0","upvote_count":"1","comment_id":"261526","poster":"harmanbirstudy","content":"I think its A ..\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"},{"content":"B is right...\n\nit actually does make sense to have B, it is used for dimensional reductions, but it does this by properly understanding the parameters of the model and visualizing the multi dimensional data into clusters, thus understanding what the model is doing to optimize itself, \n\nAlso, reducing the dimensionality of a model allows it to train faster, thus reducing the cost, which works for the required goal...\nhttps://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding","timestamp":"1633791000.0","poster":"ybad","upvote_count":"1","comment_id":"225963"},{"poster":"yeetusdeleetus","upvote_count":"1","content":"Operator wants to decrease hyperparameter ranges.\n\nOnly D involves visualizing a hyperparameter, which could be used to change the ranges given in grid search of, e.g., 90% of the given values are always bad.","timestamp":"1633567560.0","comment_id":"212179"},{"content":"D\nhttps://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d","comment_id":"190827","timestamp":"1633415700.0","upvote_count":"1","poster":"Thai_Xuan"},{"comment_id":"190826","timestamp":"1633175820.0","upvote_count":"1","poster":"Thai_Xuan","content":"D, as is seen here\nhttps://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae"},{"content":"A is wrong for sure; makes no sense at all\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets so B is wrong.\n\nBetween C and D, I would say D as the assumption is that it would save time to set the number of training iterations and see what the max depth should be","comment_id":"170754","timestamp":"1633088640.0","upvote_count":"1","poster":"syu31svc"},{"comment_id":"98700","timestamp":"1632948900.0","upvote_count":"1","content":"go for D","poster":"roytruong"},{"content":"Answer is C here. With the plot, you can identify a group of iterations with highest AUC.","timestamp":"1632831060.0","comments":[{"timestamp":"1656030360.0","upvote_count":"2","poster":"ovokpus","comment_id":"621384","content":"Agreed, because each iteration is a unique combination of the hyperparameters employed in the training. BAM!"}],"upvote_count":"5","poster":"zxl","comment_id":"95633"},{"comment_id":"57270","poster":"rajs","timestamp":"1632543540.0","upvote_count":"6","content":"B does not answer the question ..... though what you are saying is correct\n\nQuestion is asking how the operator can minimize the training time....the operator can control the Hyperparameter Range & Objective .... a scatter plot will give an idea on both and since D covers both aspects\n\nD is the answer","comments":[{"poster":"georgeZ","upvote_count":"3","comments":[{"upvote_count":"1","timestamp":"1634074440.0","content":"The question specifically asks how to reconfigure hyperparameter range, not reduce training time. Always read the question very carefully. 3 hours gives you plenty of time to do that.","comments":[{"comment_id":"621385","poster":"ovokpus","content":"Right back at you. Read again, the goal of reconfiguring the hyperparemeter range is to \"reduce the time required for training\"","upvote_count":"1","timestamp":"1656030420.0"}],"comment_id":"278182","poster":"cloud_trail"}],"content":"Why not C? Checking and optimize the epoch number I will reduce the training time more efficently.","timestamp":"1632640380.0","comment_id":"67396"}]},{"poster":"ComPah","content":"prefer D\nAnalyze the correlation between objective metric and individual hyperparameters\nFrom HPO_Analyze_TuningJob_Results.ipynb in sagemaker examples","upvote_count":"5","comment_id":"43845","timestamp":"1632239160.0"},{"content":"Lookd like B is the correct answer. Using t-SNE you can visualize multidimensional data. For each iteration set of hyper parameters are used. Using t-SNE we can identify patterns in various hyper parameters that are used in each iterations which can be used to identify optimal set of hyper parameters to reduce the cost.\nhttps://www.datacamp.com/community/tutorials/introduction-t-sne","upvote_count":"6","comment_id":"40487","timestamp":"1632214680.0","comments":[{"comment_id":"278183","timestamp":"1634100660.0","upvote_count":"1","poster":"cloud_trail","content":"How? Option B mentions input features, not hyperparameters. And it mentions that the points in the plot are colored by target variable. Hyperparameters have no relation whatsoever to the target variable, so clearly,, the scatterplot is of input features, not hyperparameters."}],"poster":"cybe001"}],"isMC":true,"timestamp":"2019-12-13 08:57:00","unix_timestamp":1576223820,"question_text":"A Machine Learning Specialist kicks off a hyperparameter tuning job for a tree-based ensemble model using Amazon SageMaker with Area Under the ROC Curve\n(AUC) as the objective metric. This workflow will eventually be deployed in a pipeline that retrains and tunes hyperparameters each night to model click-through on data that goes stale every 24 hours.\nWith the goal of decreasing the amount of time it takes to train these models, and ultimately to decrease costs, the Specialist wants to reconfigure the input hyperparameter range(s).\nWhich visualization will accomplish this?","answer_images":[],"answers_community":["D (76%)","C (24%)"],"exam_id":26,"answer_ET":"D","question_id":175}],"exam":{"isImplemented":true,"id":26,"provider":"Amazon","isBeta":false,"numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty","lastUpdated":"11 Apr 2025","isMCOnly":false},"currentPage":35},"__N_SSP":true}