{"pageProps":{"questions":[{"id":"O103GC1OQDMqnJVGxkaZ","question_id":146,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/132353-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true,"answer_images":[],"choices":{"C":"Check for entries in Amazon CloudWatch for the newly created EMR cluster. Change the AWS Step Functions state machine code to use Amazon EMR on EKS. Change the IAM access policies and the security group configuration for the Step Functions state machine code to reflect inclusion of Amazon Elastic Kubernetes Service (Amazon EKS).","D":"Query the flow logs for the VPC. Determine whether the traffic that originates from the EMR cluster can successfully reach the data providers. Determine whether any security group that might be attached to the Amazon EMR cluster allows connections to the data source servers on the informed ports.","E":"Check the retry scenarios that the company configured for the EMR jobs. Increase the number of seconds in the interval between each EMR task. Validate that each fallback state has the appropriate catch for each decision state. Configure an Amazon Simple Notification Service (Amazon SNS) topic to store the error messages.","A":"Use AWS CloudFormation to automate the Step Functions state machine deployment. Create a step to pause the state machine during the EMR jobs that fail. Configure the step to wait for a human user to send approval through an email message. Include details of the EMR task in the email message for further analysis.","B":"Verify that the Step Functions state machine code has all IAM permissions that are necessary to create and run the EMR jobs. Verify that the Step Functions state machine code also includes IAM permissions to access the Amazon S3 buckets that the EMR jobs use. Use Access Analyzer for S3 to check the S3 access properties."},"exam_id":21,"answer":"BD","answer_ET":"BD","unix_timestamp":1706477940,"answers_community":["BD (81%)","Other"],"timestamp":"2024-01-28 22:39:00","question_images":[],"question_text":"A company uses AWS Step Functions to orchestrate a data pipeline. The pipeline consists of Amazon EMR jobs that ingest data from data sources and store the data in an Amazon S3 bucket. The pipeline also includes EMR jobs that load the data to Amazon Redshift.\nThe company's cloud infrastructure team manually built a Step Functions state machine. The cloud infrastructure team launched an EMR cluster into a VPC to support the EMR jobs. However, the deployed Step Functions state machine is not able to run the EMR jobs.\nWhich combination of steps should the company take to identify the reason the Step Functions state machine is not able to run the EMR jobs? (Choose two.)","discussion":[{"comment_id":"1138098","upvote_count":"5","content":"Selected Answer: BD\nhttps://docs.aws.amazon.com/step-functions/latest/dg/procedure-create-iam-role.html\nhttps://docs.aws.amazon.com/step-functions/latest/dg/service-integration-iam-templates.html","poster":"rralucard_","timestamp":"1722565860.0"},{"timestamp":"1726726140.0","comment_id":"1177082","content":"Selected Answer: BD\nPermissions of course and we need to see if the traffic is blocked at any hops because they mention that EMR is IN vpc so... flow-logs","upvote_count":"5","poster":"GiorgioGss"},{"comment_id":"1426383","content":"Selected Answer: DE\nE> As par as I know, Step function does not require S3 access permission that EMR trying to access. so that eliminates E\nD and E make sense while E is bit less likely troubleshooting, but still valid","poster":"sam_pre","comments":[{"timestamp":"1743666780.0","content":"Sorry for the typo, should be B > ... is eliminated","poster":"sam_pre","comment_id":"1426391","upvote_count":"1"}],"upvote_count":"1","timestamp":"1743666540.0"},{"timestamp":"1727965560.0","poster":"lucas_rfsb","content":"Selected Answer: BD\nI'd go in BD","comment_id":"1188715","upvote_count":"3"},{"upvote_count":"1","comment_id":"1173453","content":"B&D. \nE is not an option to identify the failure reason.","timestamp":"1726314600.0","poster":"kj07"},{"content":"Selected Answer: BE\nBE. In others are are redflag keywords","timestamp":"1722195540.0","comment_id":"1134471","poster":"atu1789","upvote_count":"2"}],"topic":"1"},{"id":"VthOUreYtkaYTJitsZwQ","answers_community":["C (73%)","B (27%)"],"question_text":"A company is developing an application that runs on Amazon EC2 instances. Currently, the data that the application generates is temporary. However, the company needs to persist the data, even if the EC2 instances are terminated.\nA data engineer must launch new EC2 instances from an Amazon Machine Image (AMI) and configure the instances to preserve the data.\nWhich solution will meet this requirement?","timestamp":"2024-01-28 22:46:00","exam_id":21,"question_images":[],"topic":"1","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/132354-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","unix_timestamp":1706478360,"question_id":147,"choices":{"A":"Launch new EC2 instances by using an AMI that is backed by an EC2 instance store volume that contains the application data. Apply the default settings to the EC2 instances.","B":"Launch new EC2 instances by using an AMI that is backed by a root Amazon Elastic Block Store (Amazon EBS) volume that contains the application data. Apply the default settings to the EC2 instances.","D":"Launch new EC2 instances by using an AMI that is backed by an Amazon Elastic Block Store (Amazon EBS) volume. Attach an additional EC2 instance store volume to contain the application data. Apply the default settings to the EC2 instances.","C":"Launch new EC2 instances by using an AMI that is backed by an EC2 instance store volume. Attach an Amazon Elastic Block Store (Amazon EBS) volume to contain the application data. Apply the default settings to the EC2 instances."},"answer":"C","isMC":true,"answer_images":[],"answer_description":"","discussion":[{"timestamp":"1714210440.0","comment_id":"1203051","content":"Selected Answer: C\nCCCCCCC - you need to attach an extra EBS volume\n\nWhen an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume.\nref: https://repost.aws/knowledge-center/deleteontermination-ebs","upvote_count":"12","poster":"khchan123"},{"upvote_count":"5","comment_id":"1215820","poster":"hnk","content":"Selected Answer: C\nC is correct","timestamp":"1716390240.0"},{"timestamp":"1740930240.0","poster":"saqib839","comment_id":"1364014","content":"Selected Answer: C\nThe correct answer is C.\n\nExplanation:\n\nWhen you launch an EC2 instance from an AMI, the root volume’s DeleteOnTermination attribute is set to True by default, which means the data on that volume will be deleted when the instance is terminated. To persist data beyond the lifetime of the instance without changing any settings, you should store the data on an additional (non-root) EBS volume because non-root volumes are not automatically deleted on termination.","upvote_count":"2"},{"upvote_count":"2","comment_id":"1364012","timestamp":"1740929880.0","poster":"saqib839","content":"Selected Answer: B\nThe correct answer is B.\n\nExplanation:\n\nAmazon EBS volumes are persistent, meaning that the data remains intact even after an EC2 instance is terminated, provided that the volume isn’t set to be deleted on termination. By using an AMI that is backed by a root Amazon EBS volume that contains the application data, the data engineer ensures that the application data is stored persistently. In contrast, EC2 instance store volumes are ephemeral and would lose data when the instance terminates."},{"comment_id":"1359687","timestamp":"1740127140.0","poster":"Chanduchanti","content":"Selected Answer: C\nWhen an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume.","upvote_count":"2"},{"timestamp":"1739693460.0","content":"Selected Answer: C\nCheck in the option B and C the default settings are mentioned. By default an EC2 instance whenever terminates, its root volume also gets terminated. So launch new EC2 instances by using an AMI that is backed by an EC2 instance store volume. Attach an Amazon Elastic Block Store (Amazon EBS) volume to contain the application data. Apply the default settings to the EC2 instances.","upvote_count":"3","comment_id":"1357179","poster":"saransh_001"},{"timestamp":"1729421760.0","upvote_count":"4","poster":"mohamedTR","comment_id":"1300404","content":"Selected Answer: C\nB: by default, delete on termination is checked"},{"content":"Selected Answer: B\nBy using an AMI backed by an Amazon EBS root volume, you ensure that the application data is preserved, even if the EC2 instances are terminated, because EBS volumes persist independently of the EC2 lifecycle.","timestamp":"1728474960.0","poster":"mohamedTR","comment_id":"1295128","upvote_count":"2"},{"upvote_count":"2","poster":"ElFaramawi","timestamp":"1727802240.0","content":"Selected Answer: B\nThis is because Amazon EBS volumes are persistent, meaning the data is preserved even if the EC2 instance is terminated, which meets the requirement to persist the data. C is incorrect because it suggests launching instances using an EC2 instance store volume, which is ephemeral. Even though it proposes attaching an Amazon EBS volume for data, the root volume remains an instance store.","comment_id":"1292053"},{"content":"Selected Answer: C\nUsing default setting means B won’t work.","timestamp":"1722973020.0","poster":"portland","upvote_count":"3","comment_id":"1261813"},{"poster":"sdas1","timestamp":"1720943160.0","content":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/preserving-volumes-on-termination.html\n\nRoot volume\nBy default, when you launch an instance the DeleteOnTermination attribute for the root volume of an instance is set to true. Therefore, the default is to delete the root volume of the instance when the instance terminates.\n\nNon-root volume\nBy default, when you attach a non-root EBS volume to an instance, its DeleteOnTermination attribute is set to false. Therefore, the default is to preserve these volumes.\n\nAnswer is C","upvote_count":"2","comment_id":"1247670"},{"content":"Selected Answer: C\nits C!!! B with default setting will delete the EBS volume on termination","poster":"GustonMari","comment_id":"1245432","timestamp":"1720607220.0","upvote_count":"3"},{"content":"Selected Answer: B\nAmazon EBS volumes provide persistent block storage for EC2 instances. Data written to an EBS volume is independent of the EC2 instance lifecycle. Even if the EC2 instance is terminated, ***the data on the EBS volume remains intact***. Launching new EC2 instances from an AMI backed by an EBS volume containing the application data ensures the data persists across instance restarts or terminations","poster":"pypelyncar","upvote_count":"3","comment_id":"1227581","timestamp":"1717978920.0"},{"timestamp":"1716122580.0","content":"Selected Answer: B\nlaunch EC2 using AMI with root EBS that contains data","comment_id":"1213776","upvote_count":"1","poster":"VerRi"},{"timestamp":"1715836800.0","content":"B: the root EBS volume will be deleted on termination by default.\nC: the EBS is independent from EC2 Termination","upvote_count":"4","comment_id":"1212262","poster":"ampersandor"},{"timestamp":"1715051100.0","comment_id":"1207694","comments":[{"timestamp":"1715051220.0","poster":"HunkyBunky","upvote_count":"4","comment_id":"1207695","content":"And \"Delete on Termination\" flag by defaults sets to true, so better to use additional volume for application data"}],"poster":"HunkyBunky","content":"Selected Answer: C\nC - Looks better, because it will save data in all cases","upvote_count":"5"},{"content":"Selected Answer: C\nccccccc","comment_id":"1195272","timestamp":"1713070380.0","poster":"Christina666","upvote_count":"5"},{"comment_id":"1189370","content":"Can someone explain why C is NOT right?","poster":"Luke97","upvote_count":"2","timestamp":"1712238360.0"},{"upvote_count":"3","comment_id":"1177084","timestamp":"1710835860.0","poster":"GiorgioGss","content":"Selected Answer: B\nThis question is more for practitioner exam :)"},{"timestamp":"1706862000.0","poster":"rralucard_","comment_id":"1138296","content":"Selected Answer: B\nAmazon EBS volumes are network-attached, and they persist independently of the life of an EC2 instance. By using an AMI backed by an Amazon EBS volume, the root device for the instance is an EBS volume, which means the data will persist.","upvote_count":"2"},{"timestamp":"1706478360.0","upvote_count":"1","content":"Selected Answer: B\nVoting for B","comment_id":"1134480","poster":"atu1789"}]},{"id":"ndYOG8udGockGxKmqDLL","answer":"B","answer_description":"","timestamp":"2024-02-02 09:33:00","question_id":148,"answer_ET":"B","choices":{"B":"Athena workgroup","C":"Athena data source","A":"Athena query settings","D":"Athena query editor"},"question_images":[],"unix_timestamp":1706862780,"url":"https://www.examtopics.com/discussions/amazon/view/132667-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"discussion":[{"poster":"GiorgioGss","comment_id":"1177095","timestamp":"1710836280.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html\n\"To use Apache Spark in Amazon Athena, you create an Amazon Athena workgroup that uses a Spark engine.\"","upvote_count":"9"},{"poster":"pypelyncar","content":"Selected Answer: C\nThe Athena data source acts as a bridge between Athena and other analytics engines, such as Apache Spark. By using the Athena data source connector, you can access data stored in various formats (e.g., CSV, JSON, Parquet) and locations (e.g., Amazon S3, Apache Hive Metastore) through Spark applications","upvote_count":"5","timestamp":"1717979400.0","comment_id":"1227583"},{"poster":"lsj900605","upvote_count":"2","comment_id":"1313056","timestamp":"1731759360.0","content":"Selected Answer: B\nIt is B, not C. \nThe workgroup is for organizing, controlling, and monitoring queries. \nThe Data source is the mechanism that enables Spark to query data via Athena. It allows Spark to interact with Athena. \nThe question focuses on enabling Apache Spark within Athena to generate analytics instead of using SQL. Thus, you must create a Spark-enabled workgroup"},{"upvote_count":"2","timestamp":"1729927140.0","poster":"theloseralreadytaken","comment_id":"1303118","content":"Selected Answer: B\nAthena datasource doesn't specifially enable Spark access"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html\nTo get started with Apache Spark on Amazon Athena, you must first create a Spark enabled workgroup. After you switch to the workgroup, you can create a notebook or open an existing notebook. When you open a notebook in Athena, a new session is started for it automatically and you can work with it directly in the Athena notebook editor.","upvote_count":"2","poster":"andrologin","timestamp":"1720667160.0","comment_id":"1245849"},{"timestamp":"1716891540.0","content":"C. Athena data source\n\nThe Athena data source is a specific connector or library that allows Apache Spark to interact with data stored in Amazon Athena. This connector enables Spark to read data from Athena tables directly into Spark DataFrames or RDDs (Resilient Distributed Datasets), allowing you to perform analytics and transformations using Spark's capabilities.","upvote_count":"4","poster":"lalitjhawar","comment_id":"1220134"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html","upvote_count":"3","timestamp":"1711803720.0","comment_id":"1186101","poster":"blackgamer"},{"poster":"kj07","content":"B is the correct answer.\nhttps://aws.amazon.com/blogs/big-data/explore-your-data-lake-using-amazon-athena-for-apache-spark/\nYou need an Athena workgroup as a prerequisite to use Apache Spark.","timestamp":"1710398220.0","upvote_count":"1","comment_id":"1173174"},{"content":"B. is the correct answer.\nTo use Apache Spark in Amazon Athena, you create an Amazon Athena workgroup that uses a Spark engine.\nhttps://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html","upvote_count":"3","timestamp":"1709751600.0","comment_id":"1167424","poster":"damaldon"},{"upvote_count":"2","comment_id":"1138312","poster":"rralucard_","timestamp":"1706862780.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/athena/latest/ug/notebooks-spark.html"}],"answers_community":["B (72%)","C (28%)"],"isMC":true,"question_text":"A company uses Amazon Athena to run SQL queries for extract, transform, and load (ETL) tasks by using Create Table As Select (CTAS). The company must use Apache Spark instead of SQL to generate analytics.\nWhich solution will give the company the ability to use Spark to access Athena?","exam_id":21,"topic":"1"},{"id":"28k2apVlfYPh0NRnUHV1","answer_images":[],"answer_description":"","answers_community":["C (94%)","6%"],"discussion":[{"content":"Selected Answer: C\nUse code that writes data to Amazon S3 to invoke the Boto3 AWS Glue create_partition API call. This approach ensures that the Data Catalog is updated as soon as new data is written to S3, providing the least latency in reflecting new partitions.","upvote_count":"8","timestamp":"1722580800.0","poster":"rralucard_","comment_id":"1138318"},{"content":"Selected Answer: C\nBy embedding the Boto3 create_partition API call within the code that writes data to S3, you achieve near real-time synchronization. The Data Catalog is updated immediately after a new partition is created in S3.","poster":"pypelyncar","timestamp":"1733798100.0","upvote_count":"4","comment_id":"1227584"},{"comment_id":"1222501","content":"Selected Answer: C\nThe explanation could be more precise regarding the interaction with Amazon S3 and AWS Glue. The key point is that the process should be triggered immediately when new data is added to S3. This can be achieved through event-driven architecture, which indeed makes the solution intuitive and efficient.","poster":"tgv","timestamp":"1733034840.0","upvote_count":"2"},{"comment_id":"1217479","content":"Selected Answer: C\nadd partition after writing the data in s3","upvote_count":"1","poster":"valuedate","timestamp":"1732461000.0"},{"upvote_count":"1","content":"Selected Answer: D\nIt's about \"synchronizing AWS Glue Data Catalog with S3\". So for me it's D - using MSCK REPAIR TABLE for existing S3 partitions (https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html)","comment_id":"1211298","poster":"DevoteamAnalytix","comments":[{"upvote_count":"1","comment_id":"1211898","content":"Least latency","timestamp":"1731675780.0","poster":"megadba"}],"timestamp":"1731578820.0"},{"comment_id":"1195095","content":"The answer is D","timestamp":"1728844920.0","poster":"okechi","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: C\nIt's pure event-driven so... C","timestamp":"1726726980.0","comment_id":"1177101","poster":"GiorgioGss"},{"upvote_count":"1","poster":"atu1789","content":"Selected Answer: C\nC. Least latency","comment_id":"1134533","timestamp":"1722202200.0"}],"exam_id":21,"answer_ET":"C","question_images":[],"question_text":"A company needs to partition the Amazon S3 storage that the company uses for a data lake. The partitioning will use a path of the S3 object keys in the following format: s3://bucket/prefix/year=2023/month=01/day=01.\nA data engineer must ensure that the AWS Glue Data Catalog synchronizes with the S3 storage when the company adds new partitions to the bucket.\nWhich solution will meet these requirements with the LEAST latency?","isMC":true,"choices":{"C":"Use code that writes data to Amazon S3 to invoke the Boto3 AWS Glue create_partition API call.","B":"Manually run the AWS Glue CreatePartition API twice each day.","A":"Schedule an AWS Glue crawler to run every morning.","D":"Run the MSCK REPAIR TABLE command from the AWS Glue console."},"url":"https://www.examtopics.com/discussions/amazon/view/132364-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":149,"answer":"C","unix_timestamp":1706484600,"topic":"1","timestamp":"2024-01-29 00:30:00"},{"id":"665xyLJVNoyrnepY4oFx","answer":"B","isMC":true,"answer_ET":"B","question_id":150,"question_images":[],"discussion":[{"content":"Selected Answer: B\nThat's exactly the purpose of AppFlow: \"fully-managed integration service that enables you to securely exchange data between software as a service (SaaS) applications, such as Salesforce, and AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Redshift. For example, you can ingest contact records from Salesforce to Amazon Redshift or pull support tickets from Zendesk to an Amazon S3 bucket.\"\n\nhttps://docs.aws.amazon.com/appflow/latest/userguide/what-is-appflow.html","poster":"tgv","comment_id":"1222504","upvote_count":"5","timestamp":"1733035140.0"},{"timestamp":"1733798340.0","poster":"pypelyncar","comment_id":"1227589","content":"Selected Answer: B\nthe media company can leverage a fully managed service that simplifies the process of ingesting data from their third-party SaaS applications into an Amazon S3 bucket, with minimal operational overhead. Additionally, AppFlow can integrate with Amazon Redshift, allowing the company to load the ingested data directly into their analytics environment for further processing and analysis","upvote_count":"4"},{"upvote_count":"3","poster":"GiorgioGss","comment_id":"1177108","timestamp":"1726727220.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/appflow/latest/userguide/flow-tutorial.html"},{"content":"B seems the right choice here.","comment_id":"1173177","poster":"kj07","timestamp":"1726288860.0","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://d1.awsstatic.com/solutions/guidance/architecture-diagrams/integrating-third-party-saas-data-using-amazon-appflow.pdf\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software as a Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. It can store the raw data pulled from SaaS applications in Amazon S3, and integrates with AWS Glue Data Catalog to catalog and store metadata","comment_id":"1138323","upvote_count":"2","poster":"rralucard_","timestamp":"1722581340.0"}],"unix_timestamp":1706863740,"choices":{"C":"AWS Glue Data Catalog","A":"Amazon Managed Streaming for Apache Kafka (Amazon MSK)","D":"Amazon Kinesis","B":"Amazon AppFlow"},"url":"https://www.examtopics.com/discussions/amazon/view/132669-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["B (100%)"],"timestamp":"2024-02-02 09:49:00","exam_id":21,"question_text":"A media company uses software as a service (SaaS) applications to gather data by using third-party tools. The company needs to store the data in an Amazon S3 bucket. The company will use Amazon Redshift to perform analytics based on the data.\nWhich AWS service or feature will meet these requirements with the LEAST operational overhead?","answer_description":"","answer_images":[],"topic":"1"}],"exam":{"id":21,"lastUpdated":"11 Apr 2025","isBeta":false,"isMCOnly":true,"provider":"Amazon","isImplemented":true,"numberOfQuestions":207,"name":"AWS Certified Data Engineer - Associate DEA-C01"},"currentPage":30},"__N_SSP":true}