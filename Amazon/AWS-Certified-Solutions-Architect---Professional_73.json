{"pageProps":{"questions":[{"id":"WrvUiphD2jBoJKiDedU9","answer":"B","discussion":[{"comment_id":"13346","content":"B\nA: IO1 or provisioned IOPS SSD is more expensive.\nB: GP1 with 2TB volumes has 6000 IOPS. If we add additional 1TB it will increase by another 3000 IOPS. Note: Bursting and I/O credits are only relevant to volumes under 1,000 GiB, where burst performance exceeds baseline performance.\nC\\D: Not enough information is actually provided for us so we donâ€™t know if the instance can use shared storage or not.","timestamp":"1632330720.0","comments":[{"poster":"Moon","content":"Answer \"B\", well explained.","comment_id":"14701","upvote_count":"3","timestamp":"1632723300.0"},{"comment_id":"386193","poster":"01037","content":"Thanks for the explanation.\n\nhttps://aws.amazon.com/ebs/pricing/\n\ngp2\n$0.10 per GB-month of provisioned storage\nio1\n$0.125 per GB-month of provisioned storage AND $0.065 per provisioned IOPS-month","timestamp":"1636198140.0","upvote_count":"2"}],"upvote_count":"54","poster":"donathon"},{"content":"1TB for 3000 IOPS\nAnswer is B\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html","timestamp":"1632899400.0","comment_id":"44768","upvote_count":"7","poster":"amog"},{"content":"Selected Answer: B\nB: GP1 with 2TB volumes has 6000 IOPS. If we add additional 1TB it will increase by another 3000 IOPS. Note: Bursting and I/O credits are only relevant to volumes under 1,000 GiB, where burst performance exceeds baseline performance.","comment_id":"606296","timestamp":"1653335100.0","poster":"p2010","upvote_count":"1"},{"content":"B: CORRECT","upvote_count":"1","timestamp":"1640850780.0","comment_id":"513125","poster":"cldy"},{"comment_id":"506339","timestamp":"1640110680.0","content":"B is right","upvote_count":"1","poster":"AzureDP900"},{"content":"gp2: baseline performance scales linearly at 3 IOPS per GiB of volume size.","comment_id":"502385","poster":"vbal","upvote_count":"1","timestamp":"1639590780.0"},{"timestamp":"1638769080.0","content":"B. Increase the size of the gp2 volumes in each instance to 3 TB.","upvote_count":"1","poster":"cldy","comment_id":"494924"},{"comment_id":"491229","poster":"AzureDP900","timestamp":"1638331620.0","content":"B is right","upvote_count":"1"},{"content":"If you carefully read the requirements they are asking for \"most effectively\". No mention of COST!!! So anything to do with cost should not be part of the consideration. That said, an argument can be made that option A: IO1 IOPS SSD is the MOST EFFECTIVE!","comments":[{"poster":"user0001","comment_id":"598232","upvote_count":"2","timestamp":"1651945140.0","content":"I totally agree with you, there is no mention of most cost-effective , A is a better option in this case as you wont pay for additional storage"},{"content":"read ques .. \"most COST effective\"","poster":"nsvijay04b1","upvote_count":"1","timestamp":"1667544120.0","comment_id":"710941"}],"upvote_count":"3","poster":"sashenka","timestamp":"1637255340.0","comment_id":"480889"},{"timestamp":"1636268580.0","comment_id":"449911","upvote_count":"1","content":"It's B","poster":"andylogan"},{"upvote_count":"1","comment_id":"409365","content":"B: Increase from 2TB (6000IOPS) to 9000IOPS, difference is 1TB(3000IOPS).","timestamp":"1636208640.0","poster":"runtheworld"},{"timestamp":"1636166100.0","comment_id":"334654","upvote_count":"1","content":"I'll go with B","poster":"WhyIronMan"},{"comment_id":"290696","timestamp":"1636122120.0","content":"B for sure.","poster":"wind","upvote_count":"1"},{"timestamp":"1635984540.0","comment_id":"289381","upvote_count":"2","poster":"Kian1","content":"Ans B for me"},{"upvote_count":"4","poster":"Ebi","timestamp":"1635681240.0","comment_id":"281811","content":"Answer is B"},{"upvote_count":"1","poster":"sanjaym","timestamp":"1635656820.0","content":"B for sure.","comment_id":"262911"},{"poster":"gookseang","timestamp":"1635650220.0","content":"B for sure","comment_id":"242239","upvote_count":"1"},{"poster":"T14102020","timestamp":"1635575160.0","comment_id":"241911","upvote_count":"1","content":"B is correct answer."},{"content":"B .. this is another shitty AWS typeof question amazon does to seemingly persecute students!! its confusing .. because they mention in the question how i/o performance is important .. thus one would be inclinded to i01 .. especially if a database was mentioned .. however after reading this .. il go for B https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html","poster":"petebear55","upvote_count":"2","timestamp":"1635428760.0","comment_id":"231327"},{"upvote_count":"2","content":"I'll go with B","timestamp":"1635165480.0","poster":"jackdryan","comment_id":"228040"},{"upvote_count":"2","timestamp":"1635127680.0","comment_id":"220492","content":"option A ~939.49 USD\noption B ~454.49 USD","poster":"YouYouYou"},{"upvote_count":"1","comment_id":"219312","timestamp":"1634701920.0","content":"Answer is B","poster":"Bulti"},{"content":"B\nBaseline I/O performance for General Purpose SSD storage is 3 IOPS for each GiB, with a minimum of 100 IOPS. This relationship means that larger volumes have better performance. For example, baseline performance for a 100-GiB volume is 300 IOPS. Baseline performance for a 1-TiB volume is 3,000 IOPS. And baseline performance for a 5.34-TiB volume is 16,000 IOPS.","poster":"onlinebaba","upvote_count":"2","timestamp":"1634254380.0","comment_id":"182071"},{"comment_id":"166129","content":"Why not C? EFS performance peaks at 2TB and is way cheaper than gp2 , regardless of the workload considerations. There was never a question whether the solution would fit the workload...It's a stupid question since no architect in his right mind wouldn't want to know the workload considerations first, but nevertheless it didn't ask for them, so we shouldn't either.","poster":"MrP","upvote_count":"1","timestamp":"1634247180.0","comments":[{"timestamp":"1635482340.0","content":"Isn't EBS cheaper?\nEBS gp2\nhttps://aws.amazon.com/ebs/pricing/\n$0.10 per GB-month of provisioned storage\nEFS\nhttps://aws.amazon.com/efs/pricing/\nStandard Storage (GB-Month) $0.30","poster":"newme","comment_id":"235798","upvote_count":"1"}]},{"timestamp":"1633620660.0","content":"B is correct, 1TB -> 3000 IOPS","upvote_count":"1","comment_id":"148412","poster":"fullaws"},{"content":"ans B, increasing the size by 1TB increases the IOPS by 3000 for gp2 which is cost effective compared to option A.","comment_id":"141041","poster":"shputhan","timestamp":"1633577160.0","upvote_count":"1"},{"timestamp":"1633559640.0","comment_id":"134085","upvote_count":"1","content":"B for sure","poster":"NikkyDicky"},{"content":"go with B\nGP1 with 2TB volumes has 6000 IOPS. If we add additional 1TB it will increase by another 3000 IOPS","comment_id":"133532","upvote_count":"1","poster":"noisonnoiton","timestamp":"1633018200.0"},{"comment_id":"54546","content":"Thanks for good explanation","poster":"virtual","upvote_count":"1","timestamp":"1632950700.0"}],"timestamp":"2019-10-01 03:21:00","answer_description":"Reference:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html","answers_community":["B (100%)"],"question_id":361,"question_text":"A company is currently running a production workload on AWS that is very I/O intensive. Its workload consists of a single tier with 10 c4.8xlarge instances, each with 2 TB gp2 volumes. The number of processing jobs has recently increased, and latency has increased as well. The team realizes that they are constrained on the IOPS. For the application to perform efficiently, they need to increase the IOPS by 3,000 for each of the instances.\nWhich of the following designs will meet the performance goal MOST cost effectively?","choices":{"C":"Create a new Amazon EFS file system and move all the data to this new file system. Mount this file system to all 10 instances.","A":"Change the type of Amazon EBS volume from gp2 to io1 and set provisioned IOPS to 9,000.","B":"Increase the size of the gp2 volumes in each instance to 3 TB.","D":"Create a new Amazon S3 bucket and move all the data to this new bucket. Allow each instance to access this S3 bucket and use it for storage."},"exam_id":32,"question_images":[],"isMC":true,"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/5906-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"unix_timestamp":1569892860,"topic":"1"},{"id":"iapvTFQrWIslKr2Ds4TU","answers_community":["D (100%)"],"question_text":"A company's data center is connected to the AWS Cloud over a minimally used 10 Gbps AWS Direct Connect connection with a private virtual interface to its virtual private cloud (VPC). The company internet connection is 200 Mbps, and the company has a 150 TB dataset that is created each Friday. The data must be transferred and available in Amazon S3 on Monday morning.\nWhich is the LEAST expensive way to meet the requirements while allowing for data transfer growth?","choices":{"B":"Create a VPC endpoint for Amazon S3. Copy the data to Amazon S3 by using the VPC endpoint, forcing the transfer to use the Direct Connect connection.","A":"Order two 80 TB AWS Snowball appliances. Offload the data to the appliances and ship them to AWS. AWS will copy the data from the Snowball appliances to Amazon S3.","C":"Create a VPC endpoint for Amazon S3. Set up a reverse proxy farm behind a Classic Load Balancer in the VPC. Copy the data to Amazon S3 using the proxy.","D":"Create a public virtual interface on a Direct Connect connection, and copy the data to Amazon S3 over the connection."},"answer_images":[],"answer":"D","question_id":362,"unix_timestamp":1566829380,"answer_description":"","topic":"1","exam_id":32,"timestamp":"2019-08-26 16:23:00","discussion":[{"poster":"donathon","comment_id":"13347","comments":[{"timestamp":"1644962220.0","comments":[{"comment_id":"1166355","timestamp":"1709634660.0","upvote_count":"1","poster":"Mehdi221","content":"Agree. B is totally possible but costs ~ 1,500 bucks.\nSince we are looking for the *LEAST expensive way*, D is the solution. \nB would have been the solution had the question been about security. Tricky question."},{"timestamp":"1652072580.0","poster":"feizz","content":"yes, aws now support on prem to s3 by private link which is using the interface endpoints","upvote_count":"1","comment_id":"598842"}],"upvote_count":"4","poster":"wahlbergusa","content":"The explanation of B in this comment (and all the comments below) is wrong. S3 Interface Endpoints supports communication from on-prem to S3. \n\nSee : https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/\n\nHowever, VPC Endpoints are charged per GB (whichever the data transfer occurs) hence D is still the correct answer.","comment_id":"548097"}],"content":"D\nA: Too long.\nB\\C: VPC endpoints are for communications between VPC and S3. You will need a public virtual interface on DC to connect to S3 when the data is on premise. To connect to AWS public endpoints, such as an Amazon Elastic Compute Cloud (Amazon EC2) or Amazon Simple Storage Service (Amazon S3), with dedicated network performance, use a public virtual interface.A public virtual interface allows you to connect to all AWS public IP spaces globally. Direct Connect customers in any Direct Connect location can create public virtual interfaces to receive Amazonâ€™s global IP routes, and they can access publicly routable Amazon services in any AWS Regions (except the AWS China Region).","upvote_count":"48","timestamp":"1632563340.0"},{"upvote_count":"20","timestamp":"1632103440.0","poster":"Huy","comment_id":"8373","content":"A is not practical as Snow Ball takes more than 1 week.\nB is not valid because Direct Connect can't access VPC Endpoint.\nC and D are Ok but C is not cost effective because you have to setup a proxy farm.\n\nD should be correct","comments":[{"timestamp":"1632187920.0","poster":"dpvnme","content":"Yeah, D is the correct answer","comment_id":"10942","upvote_count":"5"},{"comment_id":"453435","poster":"kirrim","upvote_count":"3","content":"B used to be invalid, but AWS published a solution to access an S3 VPC Endpoint via private DX (the second part of this document):\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/\n\nSo based on that, AWS will support B now. I'm not saying I still wouldn't just do the public VIF (D) to keep things easy, just saying that B is possible now.","timestamp":"1636239300.0"}]},{"comment_id":"648247","upvote_count":"3","timestamp":"1660786860.0","poster":"foureye2004","content":"B,C,D is a valid method:\nB: https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/ if you create VPC Endpoint for S3 with Interface type\nC: https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf work on 2 type of VPC endpoint.\nD: it a function of DX. Must be work.\n\nNow, point to LEAST EXPENSIVE\nB: Fee of proxy farm.\nC: Fee of S3 interface endpoint ($/hour) and data processed \nD: Free (with S3 ingress) \n\nSo, D is the best choice!"},{"comments":[{"upvote_count":"2","timestamp":"1662286800.0","poster":"Network_1","content":"Private VIF is connected to VPC. You need Public VIF to connect to S3.","comment_id":"659166"}],"poster":"jyrajan69","content":"The question states, a minimally used 10 Gbps AWS Direct Connect connection with a private virtual interface to its virtual private cloud (VPC). Therefore there is an existing virtual interface, so why are we creating another one as stated in answer D. For me the simpler option is B, unless someone can give a valid reason","timestamp":"1650868980.0","upvote_count":"1","comment_id":"591409"},{"poster":"sophiaabigail","content":"There are few exams as grinding for the candidates as the AWS Solutions Architect Professional exam. The failure rate of the exam is well above 72%. This means that less than 28% of the candidates who take the AWS Solutions Architect Professional exam manage to clear it. Now, this is a daunting number.\nhttps://192168l254.com.mx/ES","timestamp":"1649487120.0","upvote_count":"1","comment_id":"583166"},{"upvote_count":"3","comment_id":"582153","poster":"SaiKrish123","timestamp":"1649305500.0","content":"Selected Answer: D\nTo connect to s3 using direct connect Public VIF is must"},{"comment_id":"514342","content":"D correct.","upvote_count":"1","timestamp":"1641009420.0","poster":"cldy"},{"comment_id":"513151","poster":"cldy","content":"D: CORRECT","timestamp":"1640853480.0","upvote_count":"1"},{"timestamp":"1640437500.0","upvote_count":"1","comment_id":"509120","content":"D is the right choice. https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/","poster":"tkanmani76"},{"poster":"AzureDP900","upvote_count":"1","comment_id":"491237","content":"D is the best answer for given use case","timestamp":"1638331860.0"},{"poster":"nsei","content":"Answer is C based on this link\nhttps://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf","timestamp":"1636255080.0","upvote_count":"1","comment_id":"464359"},{"comment_id":"449913","poster":"andylogan","content":"It's D","timestamp":"1636184640.0","upvote_count":"1"},{"comment_id":"440633","upvote_count":"1","content":"D\nD - Data can be copied privately using Public VIF on DX.\nB - VPC endpoints are not accessible through DX.","poster":"student22","timestamp":"1636174860.0"},{"timestamp":"1636057980.0","content":"The Answer is C. I really hate AWS for putting questions to a VERY specific solution. See my below link, it literally is the use case they are asking for. \n\nhttps://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf","poster":"cloudbruv","comment_id":"436544","upvote_count":"4"},{"poster":"Shran","timestamp":"1636053840.0","content":"Answer D\nAmazonâ€™s primary recommended method is to run multiple VIFs, mixing both public and private.","upvote_count":"1","comment_id":"407572"},{"content":"D.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/","poster":"Pb55","upvote_count":"1","comment_id":"397157","timestamp":"1636044960.0"},{"upvote_count":"1","timestamp":"1635716940.0","comment_id":"376826","content":"Should be C\nB : not possible to use Direct Connect with VPC Endpoint : https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/?nc1=h_ls\nD : Possible to use a Public VIF (https://aws.amazon.com/premiumsupport/knowledge-center/connect-private-network-dx-vif/) but not secure (where is the VPN connexion over it, it is not mentioned in the question), plus you add to the transfer rate an extra cost for the appliance (https://aws.amazon.com/directconnect/pricing/) whereas on C you just pay for the transfer rate of a set of EC2 playing the role of proxy, and the LB allows for future growth","poster":"tekkart","comments":[{"upvote_count":"1","comment_id":"376829","poster":"tekkart","timestamp":"1635753300.0","comments":[{"comment_id":"376839","timestamp":"1635757380.0","upvote_count":"2","content":"I will vote for D because not satisfied with the term proxy used here to copy some data... \"Organizations usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection. \"","comments":[{"content":"It is difficult because in the question it is state that the existing connexion is under-used, why not reuse the existing, plus this \"proxy\" is a more secured solution to filter the access to S3 and cheaper than adding a new unsecured Public VIF ,and scalable\nC should be thought over carefully","comments":[{"upvote_count":"1","content":"Plus the transfers occur during the weekend, so the existing Private VIF connexion is not much used, there is room for leveraging it","timestamp":"1635878220.0","comment_id":"376842","poster":"tekkart"}],"poster":"tekkart","upvote_count":"1","comment_id":"376840","timestamp":"1635829440.0"}],"poster":"tekkart"}],"content":"the default with C is that it does not state how to connect from the on-prem to the VPC S3, Thus D may be the only viable option"},{"comment_id":"402920","upvote_count":"1","timestamp":"1636048140.0","poster":"student2020","content":"Connection to VPC through DX is not secure by default. However, connection to S3 can use http or https for security. D is the best option, public VIF via DX."}]},{"poster":"blackgamer","upvote_count":"1","content":"D is the correct answer as S3 is public service.","comment_id":"343460","timestamp":"1635629880.0"},{"poster":"WhyIronMan","comment_id":"334658","timestamp":"1635606780.0","content":"I'll go with D","upvote_count":"3"},{"timestamp":"1635446460.0","content":"D seems correct answer \nC doesnt mention Direct Connect so it appears data transfer over internet which is not viable to transfer this amount of data over the weekend","comment_id":"329352","upvote_count":"1","poster":"Amitv2706"},{"comment_id":"307153","timestamp":"1635330240.0","upvote_count":"1","poster":"ramseycon","content":"B is the answer, as one of the benefit of vocal endpoint is to reduce data transfer charges to public aws services"},{"comment_id":"306728","poster":"Pupu86","content":"The question has indicated that the 10Gbps DX is used minimally, indicating that the link has been under-utilised. The question went on to indicate that they are using an internet connection (slow) to move datasets from on-premises to S3. Thus the logical mitigation move would be to create a public VIF to connect DX to S3 then make use of the DX to transfer the dataset over the DX link rather than the internet link. My answer is D","timestamp":"1635290460.0","upvote_count":"1"},{"upvote_count":"1","poster":"wind","content":"D is correct.","comment_id":"290701","timestamp":"1635279240.0"},{"upvote_count":"2","timestamp":"1635244920.0","poster":"Kian1","content":"I go with D","comment_id":"289382"},{"comment_id":"284615","content":"i go with C","poster":"LoganIsh","timestamp":"1635138120.0","upvote_count":"1"},{"comment_id":"281812","timestamp":"1635023460.0","content":"Answer is D","poster":"Ebi","upvote_count":"2"},{"timestamp":"1634857740.0","upvote_count":"1","content":"D for sure. VCP Endpoint not required for direct connect.","poster":"sanjaym","comment_id":"262920"},{"poster":"gookseang","timestamp":"1634728740.0","content":"D for sure","upvote_count":"2","comment_id":"242242"},{"comment_id":"241918","timestamp":"1634724480.0","content":"Correct is D. Direct Connect with public interface.","poster":"T14102020","upvote_count":"3"},{"upvote_count":"2","poster":"newme","timestamp":"1634543460.0","comment_id":"236246","comments":[{"comments":[{"content":"It seems B means using VPCE directly, which is impossible.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html","comment_id":"386198","upvote_count":"1","timestamp":"1636024620.0","poster":"01037"}],"upvote_count":"1","comment_id":"386197","timestamp":"1635960960.0","poster":"01037","content":"I think B could work.\nBut D is simpler."}],"content":"D.\nThought B at first.\nBut using B, we have to transfer the data into VPC (maybe an EC2 instances) first, then from VPC to S3.\nBecause the vpce can only be used inside vpc."},{"poster":"petebear55","timestamp":"1634537460.0","upvote_count":"2","content":"For the extra question thats been added to this discussion .. the answer is B .. introduce a sqs queue to ease the write burden ... memcached etc is used for caching so would be more appropriate in READ situations not WRITE situations .. also C is wrong because its a No SQL s database and the question mentions a MySQL type database .... be aware of these slippery questions AWS likes to put in there chaps","comment_id":"231332"},{"content":"I'll go with D","comment_id":"228043","upvote_count":"2","timestamp":"1634535720.0","poster":"jackdryan"},{"poster":"Bulti","timestamp":"1634447220.0","comment_id":"219315","content":"Answer is D","upvote_count":"2"},{"timestamp":"1634382960.0","comment_id":"194619","poster":"OrlandoP","comments":[{"content":"The answer to this question^^^^, is either C or D. I would say D for there is less effort needed.","comment_id":"212452","upvote_count":"1","poster":"BigMomma4752","timestamp":"1634387400.0"},{"comment_id":"234635","upvote_count":"2","poster":"cloudgc","timestamp":"1634542500.0","content":"- unpredictable load\n- does not change the underlying data mode\nAnswer - B"},{"poster":"RLai","comment_id":"253012","content":"Decoupling the application write requests from the database write operation - B","upvote_count":"1","timestamp":"1634742900.0"}],"content":"New question: A company is planning a large event where a promotional offer will be introduced. The companyâ€™s website is\nhosted on AWS and backed by an Amazon RDS for PostgreSQL DB instance. The website explains the\npromotion and includes a sign-up page that collects user information and preferences. Management expects\nlarge and unpredictable volumes of traffic periodically, which will create many database writes. A solutions\narchitect needs to build a solution that does not change the underlying data model and ensures that\nsubmissions are not dropped before they are committed to the database.\nWhich solution meets these requirements?\nA. Immediately before the event, scale up the existing DB instance to meet the anticipated demand. Then\nscale down after the event.\nB. Use Amazon SQS to decouple the application and database layers. Configure an AWS Lambda function to\nwrite items from the queue into the database.\nC. Migrate to Amazon DynamoDB and manage throughput capacity with automatic scaling.\nD. Use Amazon ElastiCache for Memcached to increase write capacity to the DB instance.","upvote_count":"3"},{"poster":"ishuiyutian","timestamp":"1634366040.0","content":"Answer D. The reason is that the company has a 150-TB dataset that is created each Friday. So this is an \"ongoing basis\". \n\nAWS Direct Connect provides you with dedicated, fast connections from your premises to the AWS services network. If you need to transfer large quantities of data to AWS services on an \"ongoing basis\", AWS Direct Connect might be the right choice.\n\nSnowball can be a strong alternative to Direct Connect if you need to transfer data in large batches or as a \"one-time transfer\", potentially from distributed locations. For these workloads, Snowball can be a simpler, more cost-effective option than setting up a new Direct Connect connection to transfer your data and then terminating the connection upon completion.","upvote_count":"1","comment_id":"194154"},{"timestamp":"1634351640.0","upvote_count":"2","poster":"liono","comment_id":"170635","content":"D is correct, reference: https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/"},{"poster":"fullaws","timestamp":"1634322480.0","content":"D is correct","comment_id":"148414","upvote_count":"1"},{"timestamp":"1634226900.0","upvote_count":"1","poster":"sweand","comment_id":"135634","content":"I will go with C as D takes time to establish public VIF. \nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/create-vif.html"},{"upvote_count":"2","comment_id":"134087","content":"D for sure","poster":"NikkyDicky","timestamp":"1634167140.0"},{"content":"go with D\nDirect Connect can't access VPC Endpoint","poster":"noisonnoiton","timestamp":"1633890720.0","comment_id":"133533","upvote_count":"1"},{"poster":"mat2020","upvote_count":"2","comment_id":"133260","content":"answer D","timestamp":"1633587600.0"},{"poster":"oatif","comment_id":"117980","upvote_count":"2","timestamp":"1633451760.0","content":"D is correct, you can connect to AWS public services, after you have create a public virtual interface on your direct connect connection."},{"content":"The latest 2020 ,It's not possible to directly access an S3 bucket through a private virtual interface (VIF) using Direct Connect.\nAnswer is D\nHow can I access my Amazon S3 bucket over Direct Connect?\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/","comment_id":"111611","poster":"ramikhreim","upvote_count":"2","timestamp":"1633200960.0"},{"timestamp":"1633074300.0","content":"150TB TB on 10GBPs would take about 30 hours.( appx)\nDC with private EP of S3 will do the job","comment_id":"107258","poster":"JAWS1600","upvote_count":"2"},{"upvote_count":"2","timestamp":"1632955260.0","comment_id":"80534","content":"D is correct as we're using it right now. However, I have noticed the transfer speed will not go pass 5Gb even though we provisioned a 10Gb Public VIF. Maybe it's just Megaport which is our provider.","poster":"jgtran"},{"timestamp":"1632897180.0","upvote_count":"1","content":"Will option D allow for data transfer growth? Once the data grows the data cant be transferred between Friday to Monday.","poster":"ghostrider8001","comment_id":"64136"},{"content":"Should be D","comment_id":"44773","upvote_count":"2","timestamp":"1632883680.0","poster":"amog"},{"timestamp":"1632872460.0","comment_id":"41161","poster":"markpark","content":"my choice is D","upvote_count":"2"},{"comment_id":"19112","poster":"skywalker","upvote_count":"3","content":"I go for \"D\"","timestamp":"1632796680.0"},{"upvote_count":"6","content":"Answer \"D\". The required time is 1 day, 10 hours to transfer over 10G. Use the Public VIF over Direct connect to reach S3 endpoint.","poster":"Moon","timestamp":"1632788280.0","comment_id":"14700"},{"upvote_count":"4","timestamp":"1632548520.0","poster":"awsec2","content":"D is right","comment_id":"11052"}],"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/4107-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"D"},{"id":"JTyiM698jLhc7kNprROA","choices":{"B":"Ensure that no CIDR ranges are overlapping, and attach a virtual private gateway (VGW) to each VPC. Provision an IPsec tunnel between each VGW and enable route propagation on the route table. Configure security groups on each service to allow the CIDR ranges of the VPCs in the other accounts. Enable VPC Flow Logs, and use an Amazon CloudWatch Logs subscription filter for rejected traffic. Create an IAM role and allow the Security team to call the AssumeRole action for each account.","D":"Create a Network Load Balancer (NLB) for each microservice. Attach the NLB to a PrivateLink endpoint service and whitelist the accounts that will be consuming this service. Create an interface endpoint in the consumer VPC and associate a security group that allows only the security group IDs of the services authorized to call the producer service. On the producer services, create security groups for each microservice and allow only the CIDR range of the allowed services. Create VPC Flow Logs on each VPC to capture rejected traffic that will be delivered to an Amazon CloudWatch Logs group. Create a CloudWatch Logs subscription that streams the log data to a security account.","A":"Create a VPC peering connection between the VPCs. Use security groups on the instances to allow traffic from the security group IDs that are permitted to call the microservice. Apply network ACLs and allow traffic from the local VPC and peered VPCs only. Within the task definition in Amazon ECS for each of the microservices, specify a log configuration by using the awslogs driver. Within Amazon CloudWatch Logs, create a metric filter and alarm off of the number of HTTP 403 responses. Create an alarm when the number of messages exceeds a threshold set by the Security team.","C":"Deploy a transit VPC by using third-party marketplace VPN appliances running on Amazon EC2, dynamically routed VPN connections between the VPN appliance, and the virtual private gateways (VGWs) attached to each VPC within the region. Adjust network ACLs to allow traffic from the local VPC only. Apply security groups to the microservices to allow traffic from the VPN appliances only. Install the awslogs agent on each VPN appliance, and configure logs to forward to Amazon CloudWatch Logs in the security account for the Security team to access."},"answer_images":[],"timestamp":"2019-09-09 15:04:00","question_text":"A company has created an account for individual Development teams, resulting in a total of 200 accounts. All accounts have a single virtual private cloud (VPC) in a single region with multiple microservices running in Docker containers that need to communicate with microservices in other accounts. The Security team requirements state that these microservices must not traverse the public internet, and only certain internal services should be allowed to call other individual services. If there is any denied network traffic for a service, the Security team must be notified of any denied requests, including the source IP.\nHow can connectivity be established between service while meeting the security requirements?","url":"https://www.examtopics.com/discussions/amazon/view/4944-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"answers_community":["D (100%)"],"question_images":[],"answer_description":"","question_id":363,"answer":"D","answer_ET":"D","unix_timestamp":1568034240,"discussion":[{"comments":[{"poster":"examacc","upvote_count":"5","comment_id":"24029","content":"C cannot be right. as it mentions to allow traffic from VPN appliance only(that mean they are looking to do NAT that way loacal security group will never see which connections to deny). C is scalable but has issues. I think D is better answer for this.","timestamp":"1633436040.0"}],"poster":"Warrenn","comment_id":"18862","content":"C is not correct as a VPN solution between VPC's would require traffic traversing the internet secure yes but it will traverse the internet. D would be the correct answer providing \"only the CIDR range the allowed services\" meant only the CIDR range of the producer services as only the ELB would be sending traffic to those services not the consumers directly.","timestamp":"1633247100.0","upvote_count":"19"},{"content":"Selected Answer: D\nD for me","comment_id":"893264","timestamp":"1683647340.0","poster":"mimadour21698","upvote_count":"1"},{"poster":"joanneli77","content":"The answer is C even though it is awful. Today this is TransitGateway to VPC, but prior to that architecture TransitVPC was normal. A transit VPC is EC2 hosting VPN software, so it's EC2-to-EC2 VPN. Yes, that's awful, but that's what drove Transit Gateway architecture into being. I'd be shocked if this question still existed on this old exam.","comments":[{"timestamp":"1665261840.0","content":"agree. that's 3rd party VPN appliance is equivalent to transit gw .","upvote_count":"1","poster":"heany","comment_id":"689669"}],"comment_id":"673657","timestamp":"1663624260.0","upvote_count":"4"},{"poster":"hilft","content":"D. PrivateLink","comment_id":"635816","upvote_count":"2","timestamp":"1658622480.0"},{"poster":"aandc","upvote_count":"2","comment_id":"626407","content":"Selected Answer: D\nkeyword \"PrivateLink\"","timestamp":"1656822540.0"},{"poster":"Ni_yot","content":"D for me.","comment_id":"557582","timestamp":"1645992060.0","upvote_count":"1"},{"content":"D: CORRECT","timestamp":"1640853840.0","upvote_count":"1","comment_id":"513156","poster":"cldy"},{"timestamp":"1640111280.0","poster":"AzureDP900","comment_id":"506348","upvote_count":"1","content":"I will go with D"},{"comment_id":"449933","poster":"andylogan","timestamp":"1636184820.0","upvote_count":"2","content":"It's D"},{"content":"Im going for D","timestamp":"1636148460.0","upvote_count":"2","comment_id":"445739","poster":"Kopa"},{"timestamp":"1636117320.0","poster":"StelSen","comment_id":"440637","upvote_count":"2","content":"Option-D seems better than other options."},{"comment_id":"391551","upvote_count":"1","content":"None of the answers are correct. In the question one of the key items is \"If there is any denied network traffic for a service, the Security team must be notified of any denied requests\". \nA - Provides notification, but will hit the VPC peering limit of 125\nB/C/D- Provides no notification","timestamp":"1635976620.0","poster":"DashL"},{"comment_id":"386209","poster":"01037","upvote_count":"4","content":"D\n\nA: The maximum quota is 125 peering connections per VPC. Also too complex.\nB: Virtual private gateways per Region is 5. Also too complex.\nC: No obvious difference with B, I think.","timestamp":"1635919020.0"},{"poster":"nisoshabangu","timestamp":"1635913200.0","upvote_count":"1","content":"D is the correct answer, I have implemeted a similar solution in my environment.","comment_id":"373262"},{"content":"Correct Answer is D","timestamp":"1635892320.0","poster":"Radhaghosh","comment_id":"362468","upvote_count":"1"},{"content":"I'll go with D","poster":"WhyIronMan","comment_id":"334659","timestamp":"1635864900.0","upvote_count":"3"},{"poster":"ksl4u","upvote_count":"3","comment_id":"305989","content":"D is correct","timestamp":"1635838140.0"},{"comment_id":"289394","upvote_count":"3","content":"go with D","timestamp":"1635814680.0","poster":"Kian1"},{"poster":"Ebi","timestamp":"1635771360.0","content":"I go with D","upvote_count":"4","comment_id":"284423"},{"poster":"sanjaym","comment_id":"262922","timestamp":"1635714300.0","content":"D to me.","upvote_count":"1"},{"timestamp":"1635679380.0","poster":"sundaraws2020","upvote_count":"2","comment_id":"253291","content":"D - https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service-overview.html"},{"poster":"gookseang","content":"I will go D","upvote_count":"1","comment_id":"242248","timestamp":"1635348840.0"},{"upvote_count":"1","timestamp":"1635161400.0","poster":"T14102020","content":"Correct answer is D","comment_id":"241931"},{"timestamp":"1635057240.0","poster":"newme","content":"Why is C not correct?\nAnd as to D, isn't endpoint and endpoint service created in provider VPC?","comment_id":"238016","upvote_count":"1"},{"comment_id":"228047","upvote_count":"4","content":"I'll go with D","timestamp":"1635040620.0","poster":"jackdryan"},{"comment_id":"220482","poster":"petebear55","timestamp":"1634908500.0","upvote_count":"1","content":"id go for D ... this question is very poorly written"},{"timestamp":"1634843940.0","poster":"Bulti","comment_id":"219798","content":"Answer is D","upvote_count":"1"},{"poster":"ishuiyutian","comment_id":"207501","timestamp":"1634720580.0","content":"D is correct","upvote_count":"1"},{"poster":"AlwaysLearning2020","comment_id":"189699","timestamp":"1634702940.0","upvote_count":"3","content":"See? This is another question that was set before AWS Introduced TGW, why would AWS \"promo\" the use of third-parties solutions? Unless it ones cannot \"make-it\" ;-)"},{"comments":[{"timestamp":"1635847500.0","content":"D does not require peering. VPC endpoints are designed for this exact scenario.","comment_id":"332799","poster":"sarah_t","upvote_count":"1"}],"upvote_count":"1","content":"I would go with C. \nD) the reference of security group in other VPC might require the VPC peering, to my understanding. -- can someone correct me on this?","comment_id":"174442","poster":"Ganfeng","timestamp":"1634658240.0"},{"upvote_count":"3","poster":"fullaws","comment_id":"148527","content":"D is correct, C required transverse to internet.","timestamp":"1634657940.0"},{"upvote_count":"3","content":"D for sure","poster":"NikkyDicky","timestamp":"1634513100.0","comment_id":"134090"},{"content":"go with D\n\nAWS PrivateLink allows you to create multiple accounts and Amazon\nYou can connect to the service in your VPC. No more internet gateways, VPC peering connections\nYou don't need to configure a connection or Transit VPC to establish a connection.\nTransit VPCs can be geographically different, or run under separate AWS accounts.\nAmazon Virtual Private Cloud as a global network relay center in general\nConnect to Amazon VPC.","upvote_count":"1","comment_id":"133534","poster":"noisonnoiton","timestamp":"1634356320.0"},{"upvote_count":"2","poster":"jgtran","comment_id":"80877","content":"I think D is the correct answer.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html","comments":[{"upvote_count":"2","content":"Interface VPC Endpoint limit is 50 by default. We can call AWS to increase this limit. This document does not state what is the hard limit.","timestamp":"1634280120.0","comment_id":"93026","poster":"JAWS1600"}],"timestamp":"1634148000.0"},{"upvote_count":"2","content":"For people thinking that answer would be wrong because it requires third party component, AWS does work with third party for some of the solutions.","comment_id":"57836","poster":"ashp","timestamp":"1634062380.0"},{"poster":"timmmmmmmmmmmmmmmmmm","comments":[{"upvote_count":"1","content":"The question speaks of two security groups. One on the consumer side and one on the producer side. Although you can't use the one on the producer side to provide the source IP address (since the traffic on the producer side is sourced by the NLB), you can use the one of the consumer side. The VPC Flow Logs in each VPC are going to capture the rejected traffic based on the consumer-side security group within the VPC.","comment_id":"64067","poster":"sb333","timestamp":"1634080140.0"}],"upvote_count":"2","timestamp":"1633997640.0","comment_id":"52725","content":"I think you can only go with C because privatelink will do the NAT then you lose the source IP address"},{"comment_id":"52646","upvote_count":"3","content":"D is correct: https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html","timestamp":"1633782420.0","poster":"Gorha"},{"comment_id":"44785","content":"Answer is D","timestamp":"1633617720.0","upvote_count":"2","poster":"amog"},{"content":"D is answer","timestamp":"1633576200.0","poster":"markpark","comment_id":"41163","upvote_count":"1"},{"timestamp":"1633556820.0","content":"D is the answer. \n\nThe connection is private. Security group in the same region across accounts can be referenced. SG can be attached to interface endpoint directly. For the service endpoint SG can be attached to the NLB.","comment_id":"34515","poster":"arunkumar","upvote_count":"1"},{"upvote_count":"1","content":"Is it C or D?","timestamp":"1633043340.0","comments":[{"comments":[{"poster":"cloudlabadm","timestamp":"1634114040.0","upvote_count":"2","comment_id":"73551","content":"Hi, are you referring to C or D? Seems the feature is for C."}],"upvote_count":"4","comment_id":"17259","content":"i think the Answer is D. Building on the Software VPN design mentioned above, you can create a global transit network on AWS. A transit VPC is a common strategy for connecting multiple, geographically disperse VPCs and remote networks in order to create a global network transit center. A transit VPC simplifies network management and minimizes the number of connections required to connect multiple VPCs and remote networks. The following figure illustrates this design.","timestamp":"1633231140.0","poster":"AWS2020"}],"comment_id":"15557","poster":"Marcos"},{"poster":"Moon","upvote_count":"2","timestamp":"1632831000.0","comment_id":"14699","content":"Answer is \"C\". Use service VPC.\nD: Even solution \"D\" was great, but it mistakenly reference security group IDs, which only can be done in VPC peering.","comments":[{"content":"Hi Moon, I read it a few times before I understand. \"allows only the security group IDs (this is the source so it's in the same VPC as the privatelink endpoint) of the services authorized to call the producer service. On the producer services, create security groups for each microservice and allow only the CIDR (this is the destination VPC and hence CIDR must be used instead of SG IDs.) range the allowed services. \"","upvote_count":"4","comments":[{"upvote_count":"1","poster":"student2020","timestamp":"1636039980.0","comment_id":"402927","content":"If you use a service endpoint and a VPC endpoint, the traffic from the consumer is source NAT'd to the Ip address of the NLB. There is no way to create a SG with CIDR ranges from the consumer VPCs. I still think its the answer, but that part is incorrect."}],"poster":"donathon","timestamp":"1633218060.0","comment_id":"15708"}]},{"comment_id":"13351","poster":"donathon","upvote_count":"2","timestamp":"1632794400.0","comments":[{"timestamp":"1633293300.0","comment_id":"20664","upvote_count":"1","content":"I cannot see where this question mentioned \"On Premise\".","poster":"Frank1"},{"poster":"shammous","upvote_count":"3","comment_id":"280411","timestamp":"1635719520.0","content":"This is an incomplete statement: \"PrivateLink are used for On Premise accessing private AWS resources\"?\nThe real definition of PL is \"AWS PrivateLink provides private connectivity between VPCs and services hosted on AWS or on-premises\".\nThis make D a viable solution. C is ruled out as it involves VPN (public internet), which is against the requirement \"microservices must not traverse the public internet\"."},{"content":"Update:\nAnswer = D\nAWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify the network architecture. It seems like the next VPC peering. https://aws.amazon.com/privatelink/\nA: Hits VPC peering limit of 125. https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html\nB: No notification to security for denied request.\nC: If VPN is used, the source IP may not be known.","comments":[{"content":"\" If VPN is used, the source IP may not be known\"? The issue with VPN is that traverse the public internet. VPC Flow Logs suggested in option D allows to get the Source IP.","poster":"shammous","comment_id":"280415","timestamp":"1635750840.0","upvote_count":"2"}],"timestamp":"1633137360.0","comment_id":"15706","upvote_count":"26","poster":"donathon"}],"content":"C\nA: Hits VPC peering limit of 125. https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html\nB: No notification to security for denied request.\nD: PrivateLink are used for On Premise accessing private AWS resources."},{"timestamp":"1632564600.0","content":"C is best","comment_id":"11051","upvote_count":"1","poster":"awsec2"},{"upvote_count":"1","poster":"dpvnme","content":"With 200 VPCs, C is the best choice.","comment_id":"10946","timestamp":"1632436440.0"},{"poster":"IO","timestamp":"1632349320.0","content":"Max VPC peering 125 VPC, here there are 200 so the A is excluded.\nThe B would exclude it for the management complexity and because the Security Team must be notified, instead here it is given an IAM role with AssumeRole for the other accounts, which however requires an action, there is no notification.\nC would exclude it because it requires third parties.","comment_id":"10826","upvote_count":"3"},{"poster":"One_picese","comment_id":"10549","upvote_count":"1","timestamp":"1632329520.0","content":"I don't think the right answer is C,Would you please refer to following link.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html#vpc-endpoint-policies"},{"comments":[{"timestamp":"1632606120.0","poster":"piotr","upvote_count":"3","content":"Because you cannot establish tunnel between two VGW.","comment_id":"11308"}],"timestamp":"1632158640.0","upvote_count":"2","poster":"DalianYifang","content":"Why not B?","comment_id":"10283"}],"isMC":true,"topic":"1"},{"id":"SRuYH6RFH4PsTpfjJplN","answer_ET":"C","timestamp":"2019-09-13 03:58:00","question_text":"A company runs a dynamic mission-critical web application that has an SLA of 99.99%. Global application users access the application 24/7. The application is currently hosted on premises and routinely fails to meet its SLA, especially when millions of users access the application concurrently. Remote users complain of latency.\nHow should this application be redesigned to be scalable and allow for automatic failover at the lowest cost?","exam_id":32,"choices":{"A":"Use Amazon Route 53 failover routing with geolocation-based routing. Host the website on automatically scaled Amazon EC2 instances behind an Application Load Balancer with an additional Application Load Balancer and EC2 instances for the application layer in each region. Use a Multi-AZ deployment with MySQL as the data layer.","C":"Use Amazon Route 53 latency-based routing to route to the nearest region with health checks. Host the website in Amazon S3 in each region and use Amazon API Gateway with AWS Lambda for the application layer. Use Amazon DynamoDB global tables as the data layer with Amazon DynamoDB Accelerator (DAX) for caching.","B":"Use Amazon Route 53 round robin routing to distribute the load evenly to several regions with health checks. Host the website on automatically scaled Amazon ECS with AWS Fargate technology containers behind a Network Load Balancer, with an additional Network Load Balancer and Fargate containers for the application layer in each region. Use Amazon Aurora replicas for the data layer.","D":"Use Amazon Route 53 geolocation-based routing. Host the website on automatically scaled AWS Fargate containers behind a Network Load Balancer with an additional Network Load Balancer and Fargate containers for the application layer in each region. Use Amazon Aurora Multi-Master for Aurora MySQL as the data layer."},"question_images":[],"answer_description":"Reference:\nhttps://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-3/","discussion":[{"comment_id":"13370","timestamp":"1632333780.0","comments":[{"timestamp":"1635609120.0","comments":[{"upvote_count":"2","comment_id":"397164","timestamp":"1635655740.0","content":"Itâ€™s C.\nMulti region improves SLA due to latency routing\nS3 hosts static and lambda processes dynamic aspects of website.\nhttps://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-3/","poster":"Pb55"}],"content":"What about S3 for dynamic website & failed SLA. How can C be correct?","upvote_count":"4","poster":"Pb55","comment_id":"397162"},{"content":"Lambda has its limitation that cannot handle such concurrent request","upvote_count":"2","comment_id":"350826","timestamp":"1634760840.0","poster":"ppshein"}],"poster":"donathon","upvote_count":"39","content":"C\nhttps://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/\nA\\D: Should be latency based routing the ensure latency is at a minimum. Remember the users are spread globally not to specific regions where you can maybe use geo to spread the load across just a few region.\nB: Similar to A."},{"comments":[{"content":"Good point.\nSLA rules out A&C.","timestamp":"1634936700.0","comment_id":"386218","upvote_count":"1","poster":"01037"},{"upvote_count":"6","content":"The only problem with D is multi-master Aurora is regional, there is no mention of data sync across regions. C seems correct.","poster":"SD13","comment_id":"331103","timestamp":"1634654940.0"},{"content":"Answer D is not correct. Answer C is correct because there is a Route53 with health checks in front.","upvote_count":"2","poster":"Kelvin","comment_id":"335032","timestamp":"1634737440.0"},{"upvote_count":"1","poster":"prz","timestamp":"1636000200.0","comment_id":"435241","content":"API GW is very costly regarding \"millions of users access the application concurrently.\""},{"comment_id":"446788","upvote_count":"3","content":"C is better.\n2 points:\n1. Latency is the key. And it's better to use latency-base routing policy.\n2. DDB global tables support multi-regions, but Aurora multi-masters does not .\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html#aurora-multi-master-architecture\nCurrently, all DB instances in a multi-master cluster must be in the same AWS Region.\nYou can't enable cross-Region replicas from multi-master clusters.","timestamp":"1636200300.0","poster":"WillCloud"},{"poster":"TomPaschenda","comment_id":"426256","upvote_count":"5","timestamp":"1635933900.0","content":"C is correct - the SLA problem can be solved by having API Gateway & Lambda in multiple regions and using Route 53 health checks.\nD cannot be correct because Route 53 Health Check is missing Aurora Multi-Master does not support cross-region, see https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html"}],"comment_id":"148534","timestamp":"1633016280.0","upvote_count":"35","content":"D is correct\nBelow is summary of those discussion\nA. RDS 99.95%, no across region strategy\nB. Round robin to different region (latency), Aurora 99.99% meet\nC. S3 not for dynamic web content, API Gateway & lambda 99.95% \nD. ECS, EC2, Fargate, NLB, Aurora 99.99% meet, using geolcation-based routing is enough as regional service is 99.99%","poster":"fullaws"},{"content":"Selected Answer: C\nC seems correct for me","timestamp":"1688480640.0","comment_id":"942863","poster":"SkyZeroZx","upvote_count":"1"},{"upvote_count":"1","comment_id":"711910","content":"Will go with D as millions of concurrent request NLB","timestamp":"1667670120.0","poster":"sou123454"},{"poster":"Cloud_noob","comment_id":"703579","upvote_count":"1","timestamp":"1666676640.0","content":"Selected Answer: C\nC seems correct for me"},{"upvote_count":"1","poster":"sg0206","content":"C - Will go with C","comment_id":"693866","timestamp":"1665662700.0"},{"comment_id":"544397","timestamp":"1644480120.0","upvote_count":"1","content":"Answer is D.\nThe users are worldwide so Geo location based routing is better. \nMoreover AWS always promotes its services in questions. So aurora multi master and Fargate is better option.\nCost for Fargate will be cost efficient.","poster":"Ishu_awsguy"},{"comment_id":"506355","content":"C is correct","upvote_count":"1","poster":"AzureDP900","timestamp":"1640111880.0"},{"poster":"cldy","timestamp":"1639128060.0","comment_id":"498473","upvote_count":"1","content":"C. Use Amazon Route 53 latency-based routing to route to the nearest region with health checks. Host the website in Amazon S3 in each region and use Amazon API Gateway with AWS Lambda for the application layer. Use Amazon DynamoDB global tables as the data layer with Amazon DynamoDB Accelerator (DAX) for caching."},{"comment_id":"452707","upvote_count":"1","timestamp":"1636266960.0","content":"D \n---","poster":"student22"},{"poster":"andylogan","timestamp":"1636265760.0","upvote_count":"1","comment_id":"449937","content":"It's D"},{"comment_id":"448411","upvote_count":"1","poster":"DonSp","content":"A, C, and D can be eliminated for not fulfilling all requirements.\nA: SLA not met\nC: does not allow dynamic web applications, Lambda does not support the required concurrency\nD: no health checks, Aurora Multi-Master is in one region only\nB does not look great but fulfills the requirements.","timestamp":"1636227180.0"},{"content":"Option-D looks correct. With Option-C two flaws. Lambda concurrency limitations (https://stackoverflow.com/questions/36766553/aws-lambda-concurrent-request-limit-and-how-to-increase-that) and S3 can't be used for Dynamic Website.","comment_id":"440644","upvote_count":"1","poster":"StelSen","timestamp":"1636200000.0"},{"timestamp":"1636128180.0","upvote_count":"1","poster":"AWS_Noob","comment_id":"438837","content":"D\n\nMillions of concurrent requests. NLB is needed here, lambda only can handle 10 000 concurrent requests per sec of I remember correctly"},{"timestamp":"1635986760.0","upvote_count":"1","poster":"denccc","comment_id":"429226","content":"for me it's C"},{"poster":"Japs","comment_id":"410053","upvote_count":"2","content":"D - Aurora is cheapest and it supports multi region for read-replicas","timestamp":"1635822480.0"},{"timestamp":"1635741660.0","upvote_count":"3","poster":"DerekKey","content":"Read the question: \"redesigned\" to be \"scalable\" and allow for \"automatic failover\". You can only choose between B and C. Roud robin is not suitable due to potential latency happening sometime for some of the users. You should use latency-based routing or geolocation-based routing depending on you application/system usage. \nI would choose C\nDAX will lower DynamoDB cost and S3 with Lambda is potentially cheaper than Fargate and load balancers. From experience - you have to run at least one container per task no matter their utilization is. Having millions of user accessing web site running in conatiners it would generate a lot of $ for ECS. Instead redesign your app to use S3 and API Gateway.","comment_id":"408854"},{"poster":"Akhil254","upvote_count":"1","comment_id":"407524","content":"C Correct","timestamp":"1635701100.0"},{"content":"I go with C. Option D is costly and does not include API gateway","timestamp":"1635469740.0","upvote_count":"1","comment_id":"390805","poster":"Training"},{"comment_id":"387775","content":"D: cheaper version, Dynamo Globa tables, latency based","timestamp":"1635237900.0","comments":[{"comment_id":"387778","timestamp":"1635351780.0","upvote_count":"1","content":"Sorry i mean c","poster":"Kopa"}],"upvote_count":"1","poster":"Kopa"},{"comment_id":"386219","comments":[{"upvote_count":"1","timestamp":"1635159120.0","comment_id":"386223","poster":"01037","content":"Chose A at first because only A mentions failover.\nBut failover is not a requirement here."}],"upvote_count":"1","content":"D.\nThough cost is difficult to tell.\n\nSLA rules out A&C.\nB: round robin obviously doesn't fit the scene well.","poster":"01037","timestamp":"1634943240.0"},{"poster":"nisoshabangu","upvote_count":"1","content":"I go with D","comment_id":"373268","timestamp":"1634892420.0"},{"timestamp":"1634889420.0","upvote_count":"2","poster":"zolthar_z","content":"The Answer is C, a thought D at beginning but the key is in the DB, aurora multi-master only works in the same region, instead DynamoDb is globally (Allowing access the data faster). Also you can increase the lambda concurrency (1000 default) with a support case","comment_id":"366292"},{"comment_id":"362473","timestamp":"1634841000.0","poster":"Radhaghosh","upvote_count":"6","content":"Two important phrase \"dynamic mission-critical web application\" and \"millions of users access the application concurrently\". My question is who ever is supporting Option C. How will you scale lambda to that many executions? and 2nd Hosting application S3 is suitable for Static website, not for Dynamic Website.\nConsidering millions of concurrent user, it has to be via NLB. Now between B & D, I will select D, as this has geolocation-based routing compared to round robin routing (for Option B).\nSo my pick is Option D"},{"upvote_count":"2","poster":"jdlang","comment_id":"341606","content":"Its D. Only - Remote users complain of latency when connecting to the app on-prem. The geolocation-based routing would fix that, even though the question does not ask for it to be fixed - Fargate is scalable and NLB would handle the traffic for millions of users, Aurora is cheaper than Dynamo, so at the lowest cost... and why would they want DAX ?","timestamp":"1634757480.0"},{"upvote_count":"1","poster":"WhyIronMan","content":"I'll go with D","comment_id":"334666","timestamp":"1634717160.0"},{"comment_id":"329526","poster":"Amitv2706","content":"Correct answer is C as its a complete redesign so static content on S3 and dynamic on lambda.\n\nD. cant be correct as geo-location based routing cant be used to rectify latency as latency based routing is the correct solution for it.","timestamp":"1634645460.0","upvote_count":"2"},{"comment_id":"315267","upvote_count":"3","timestamp":"1634594040.0","poster":"nitinz","content":"C is very tempting BUT 2 problems with it, S3 can not do Dynamic content, S3 for static and dynamic content goes to instance. Also lambda limits to 1000, you can increase to few thousand not millions. Hence C is out. Only left with viable option which is D."},{"timestamp":"1634589060.0","upvote_count":"1","content":"C & D are closely matched in terms of fulfilling the requirements pertaining to this question. However, I am more steered towards D as Network Load Balancers does the job well for concurrent million of users scenarios when I was clearing my Advanced Network Specialty paper 1 month ago.","poster":"Pupu86","comment_id":"306856"},{"poster":"kiev","content":"There is no way C is correct. Lambda can't run for 24/7 hours. I would reluctantly go with D","timestamp":"1634557740.0","upvote_count":"2","comment_id":"304029"},{"comments":[{"content":"how can Lambda works with millions of users concurrently?","timestamp":"1634488500.0","upvote_count":"1","poster":"bnh_fedi","comment_id":"297667"}],"comment_id":"293221","upvote_count":"1","timestamp":"1634439600.0","content":"Should be C\nA and D is not correct because of global users. C to reduce latency by S3 and DynamoDB\n\nhttps://www.sevenmentor.com/amazon-web-services-training-institute-in-pune.php","poster":"pallavi7mentor"},{"comment_id":"292938","upvote_count":"1","timestamp":"1634375640.0","poster":"kiev","content":"I am going for B"},{"poster":"wind","content":"focus on SLA, D will be the only choice.","comment_id":"290718","upvote_count":"1","timestamp":"1634356200.0"},{"comment_id":"289396","timestamp":"1634327460.0","poster":"Kian1","content":"I go with D","upvote_count":"3"},{"upvote_count":"6","poster":"AJBA","comment_id":"288075","content":"Based on SLA, I can only see in Option D.\nAPI gateway & Lambda :- 99.95\nDynamodb, fargate & Aurora Multi-Master :- 99.99.\nDon't confuse by looking around, If the questions want 99.99 SLA, so we have to think that way.","timestamp":"1634305380.0"},{"comments":[{"comment_id":"297668","content":"How can Lambda works with millions of user concurrently?","upvote_count":"2","timestamp":"1634492160.0","poster":"bnh_fedi"}],"poster":"Ebi","comment_id":"284003","content":"Answer is C","timestamp":"1634259660.0","upvote_count":"4"},{"comment_id":"283687","timestamp":"1634147340.0","upvote_count":"2","poster":"bnagaraja9099","content":"i ll go with D"},{"timestamp":"1633972860.0","comment_id":"262923","poster":"sanjaym","content":"I will go with C","upvote_count":"1"},{"poster":"MichaelHuang","upvote_count":"2","timestamp":"1633931640.0","comment_id":"246972","content":"My pick is C, the reasons D is not selected:\n1) Route 53 geolocation-based routing\n2) Aurora Multi-Master"},{"timestamp":"1633864920.0","poster":"gookseang","upvote_count":"1","content":"seems C","comment_id":"242326"},{"timestamp":"1633666200.0","comment_id":"241936","upvote_count":"1","content":"Correct answer is D. Aurora Multi-Master is key and without Lambda 99.95% SLA","poster":"T14102020"},{"poster":"newme","upvote_count":"2","timestamp":"1633646280.0","comment_id":"238873","content":"According to SLA and cost, D is the best choice.\nI just have one question, why everybody says Lambda is NOT good for 24/7.\nIn my opinion, Lambda a good fit for 24/7, becaus access must be fluctuating during the day, and Lambda runs only when being needed."},{"comments":[{"timestamp":"1633564020.0","comment_id":"233509","content":"I am also changing to D","poster":"jackdryan","upvote_count":"1"}],"comment_id":"232035","timestamp":"1633563360.0","upvote_count":"3","content":"I'm now inclined to go for D as with C .. S3 does not serve dynamic content","poster":"petebear55"},{"timestamp":"1633493640.0","upvote_count":"1","comment_id":"228051","content":"I'll go with C","poster":"jackdryan"},{"upvote_count":"1","content":"c .. its got global tables and nosql db and dax for caching .... ideallly one would like to see cloudfront in there somewhere as well ... but users should note lambda which is another clue as its used for edge computing around the world","timestamp":"1633492140.0","poster":"petebear55","comment_id":"220551"},{"content":"Answer is D use to SLA req and also did to the fact that we are using geo location based Route53 routing which will keep the user coming from a specific geo location in the same region and therefore the inability of the AWS Aurora multi master to provide cross region data replication will not be a big issue when compared with the critical SLA requirement of 99.9% which option C cannot meet. Plus DaX does have an issue with global tables in Option C especially when latency based routing is used by Route53.","poster":"Bulti","upvote_count":"3","timestamp":"1633463700.0","comment_id":"219824"},{"timestamp":"1633375560.0","comment_id":"213976","upvote_count":"3","content":"A very tricky question and C and D both looks good architecture to support the use case.\nHowever, Lambda is not advisable if you are running your application 24x7 and it also has concurrency limit of 1000 per region. Not sure it will be able to handle the load when million of users are connected. \nMy Take is D here because it ticks all checkboxes.","poster":"vjt"},{"upvote_count":"1","content":"C is correct","poster":"ishuiyutian","timestamp":"1633324260.0","comment_id":"207502"},{"poster":"raj0101","timestamp":"1633279020.0","upvote_count":"1","comment_id":"180596","content":"D - millions of users = NLB , Round Robin not good for global traffic."},{"comment_id":"178265","upvote_count":"1","timestamp":"1633258380.0","poster":"smithyt","content":"The answer is D question states SLA is 99.99% C is 99.95%(api gateway and lambda)"},{"comment_id":"174484","poster":"Ganfeng","content":"C is correct","timestamp":"1633256520.0","upvote_count":"1"},{"comment_id":"159743","poster":"fullaws","content":"C is correct","upvote_count":"1","timestamp":"1633252800.0"},{"upvote_count":"4","comment_id":"159004","timestamp":"1633227360.0","comments":[{"timestamp":"1634570940.0","poster":"ele","content":"Correct C. I thought D first, but the Aurora multi-master is one region only, and there is no failover in D: \nâ€¢ Currently, all DB instances in a multi-master cluster must be in the same AWS Region.\nâ€¢ You can't enable cross-Region replicas from multi-master clusters.","upvote_count":"1","comment_id":"305879"}],"poster":"Mavuk","content":"C as D says multi master cross region but that is not supported \"Currently, all DB instances in a multi-master cluster must be in the same AWS Region.\""},{"upvote_count":"1","timestamp":"1632963840.0","comment_id":"140591","content":"Theres 0 shot its C. it says it fails when millions of users use it.","poster":"balisongjam"},{"content":"C for sure","poster":"NikkyDicky","timestamp":"1632945300.0","upvote_count":"2","comment_id":"134092"},{"content":"go with D\nFargate","poster":"noisonnoiton","timestamp":"1632932460.0","comment_id":"133538","upvote_count":"3"},{"comment_id":"133257","upvote_count":"2","timestamp":"1632908700.0","content":"Answer : C","poster":"mat2020"},{"content":"C is correct, A is definitely wrong because we have to keep our costs low. How can we keep our costs low by creating many EC2 instances behinds ELB's with MySQL RDS spread over multiple zones?","poster":"oatif","timestamp":"1632717360.0","comment_id":"117994","upvote_count":"1"},{"comment_id":"117819","content":"C. Latency Routing Policy has fail over capabilty when health checks are enabled.","poster":"chicagomassageseeker","timestamp":"1632678420.0","upvote_count":"1"},{"content":"I looked at the options from the SLA perspective of each service in the solution. RDS has an SLA of 99.95% So, option A is out. API Gatway and Lambda have SLA of 99.5% so option C is out. That leaves option B and D. Round Robin is not a great method to route global traffic and Aurora replicas does not help dynamic applications so, that eliminates option B. ECS, EC2, Fargate, NLB, and Aurora all have availability SLA of 99.99% so, for me the answer is D.","upvote_count":"11","timestamp":"1632597660.0","poster":"jay1ram2","comments":[{"content":"Good point","timestamp":"1632723120.0","upvote_count":"2","comment_id":"126162","poster":"ricoyao"},{"poster":"inf","comment_id":"130559","content":"Answer: D\nFollowing on from what jay1ram2 stated, which are all valid points, and focusing on why its not C:\n- S3 SLA is only 99.9 (uptime) - durability is 11 9s but we don't care about that here, we need availability. Rules out C\n- Dynamic web application - the question doesn't state that only the app layer is dynamic - web layer might be dynamic - and with no mention of lambda/api for the web app - rules out C\n- Lambda doesn't scale well considering millions of concurrent requests - note the limits per region of a soft 1000 - not sure AWS will grant a an increase request of 1,000,000 to cater for millions of requests - Rules out C","upvote_count":"5","timestamp":"1632881580.0"}],"comment_id":"113701"},{"content":"Its 24X7 application . lambda is not a good choice for running 24X7 application. Would go with D","poster":"JAWS1600","comments":[{"comment_id":"113198","timestamp":"1632584520.0","content":"Can Aurora cope with millions of uses globally.","upvote_count":"2","poster":"dayody"}],"timestamp":"1632550680.0","comment_id":"107273","upvote_count":"1"},{"upvote_count":"1","comment_id":"105050","content":"According to me D is the right answer.\nIC:t cannot be C as the website is Dynamic and S3 can't host Dynamic wesbite.\nB:It cannot be B as it only menions read replicas.\nA: Good option bt it cannot expand to millions of request.\nNLB does a great job in handling millions of request / sec. \nOut of the lot D makes more sense.","poster":"meenu2225","comments":[{"upvote_count":"2","timestamp":"1632584100.0","comment_id":"109852","poster":"meenu2225","content":"Guys having a second look at the question C seems like a right answer. Option D lacks failover."}],"timestamp":"1632506580.0"},{"poster":"HerrTeuer","content":"but apigateway and lambda has limit, while millions of users may access the application concurrently, so it's A","timestamp":"1632495360.0","comment_id":"103625","upvote_count":"1"},{"comment_id":"94019","content":"One more point , just digged in, DAX does not play well with global tables, unless we have read intensive application.\nhttps://stackoverflow.com/questions/55104651/how-does-invalidation-work-in-aws-dynamodb-dax-multi-region","timestamp":"1632493680.0","upvote_count":"1","poster":"JAWS1600"},{"comment_id":"94017","timestamp":"1632434520.0","content":"The ask is \"scalable and automatic failover with lowest cost\" D fits in that very well.","poster":"JAWS1600","upvote_count":"2"},{"poster":"JAWS1600","comment_id":"94013","content":"I understand that latency will be reduced with option C. However COst will go up using DAX","upvote_count":"1","timestamp":"1632427440.0"},{"upvote_count":"2","content":"Deliverables : \nHA, Millions of users, reduce latency, scalable, Automatic Failover, Lowest cost.\nI don't like any of these solutions. \nA. Addresses HA, Latency and failover user count and is scalable, But expensive because its deployed in all regions. \nB. is Good except for the Round Robin part...The only option tat seems to consider cost.\nC. Is good except for S3 hosting dynamic content and possible throttling issues on Lambda and API Gateway.\nD. works but is expensive, No Failover\nSounds like the least of all evils is C but \"dynamic website\" seems to rule out S3. So why not A then? it seems to be the only one without a gotcha! If JavaScript is meant to be inferred here then that would point more to C but that's a hefty inference.","timestamp":"1632425700.0","comment_id":"88322","poster":"Merlin1"},{"content":"Should be C\nA and D is not correct because of global users. C to reduce latency by S3 and DynamoDB","upvote_count":"4","timestamp":"1632391620.0","poster":"amog","comment_id":"44804"},{"timestamp":"1632096960.0","upvote_count":"5","poster":"dpvnme","content":"C would be my pick","comments":[{"upvote_count":"2","comment_id":"11359","poster":"dpvnme","timestamp":"1632196260.0","comments":[{"content":"Dynamic functionality will be added to the web pages using JavaScript to call remote RESTful APIs with AWS Lambda and API Gateway. Answer C is correct. https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/","upvote_count":"5","poster":"sb333","comment_id":"11636","timestamp":"1632320700.0"}],"content":"nvm, S3 can't host dynamic content"},{"comment_id":"150036","poster":"7thGuest","timestamp":"1633069080.0","content":"For me, C is the best answer... for those concerned about the SLA in Option C, the question asks: \"How should this application be redesigned to be scalable and allow for automatic failover at the lowest cost?\" It doesn't mention that it should meet a specific SLA.","upvote_count":"4"},{"comment_id":"315268","content":"Its D, reasoning:-\nC is very tempting BUT 2 problems with it, S3 can not do Dynamic content, S3 for static and dynamic content goes to instance. Also lambda limits to 1000, you can increase to few thousand not millions. Hence C is out. Only left with viable option which is D.","upvote_count":"2","timestamp":"1634643960.0","poster":"nitinz"}],"comment_id":"10947"}],"question_id":364,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/5129-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["C (100%)"],"isMC":true,"answer_images":[],"unix_timestamp":1568339880,"answer":"C"},{"id":"w1PxwXIRkzjYV948MkOj","question_text":"A company manages more than 200 separate internet-facing web applications. All of the applications are deployed to AWS in a single AWS Region. The fully qualified domain names (FQDNs) of all of the applications are made available through HTTPS using Application Load Balancers (ALBs). The ALBs are configured to use public SSL/TLS certificates.\nA Solutions Architect needs to migrate the web applications to a multi-region architecture. All HTTPS services should continue to work without interruption.\nWhich approach meets these requirements?","question_id":365,"answer":"D","exam_id":32,"choices":{"A":"Request a certificate for each FQDN using AWS KMS. Associate the certificates with the ALBs in the primary AWS Region. Enable cross-region availability in AWS KMS for the certificates and associate the certificates with the ALBs in the secondary AWS Region.","B":"Generate the key pairs and certificate requests for each FQDN using AWS KMS. Associate the certificates with the ALBs in both the primary and secondary AWS Regions.","C":"Request a certificate for each FQDN using AWS Certificate Manager. Associate the certificates with the ALBs in both the primary and secondary AWS Regions.","D":"Request certificates for each FQDN in both the primary and secondary AWS Regions using AWS Certificate Manager. Associate the certificates with the corresponding ALBs in each AWS Region."},"topic":"1","answer_ET":"D","isMC":true,"discussion":[{"content":"A & B mention about KMS which is not valid for SSL/TLS Cert Managment. \nACM is regional service and each region must maintain its owns certificate list. So D should be correct in my opinion.","upvote_count":"40","comment_id":"8375","timestamp":"1632143520.0","poster":"Huy"},{"comment_id":"24030","timestamp":"1633802340.0","upvote_count":"18","poster":"examacc","content":"Answer is D. As per AWS FAQ \nTo use a certificate with Elastic Load Balancing for the same site (the same fully qualified domain name, or FQDN, or set of FQDNs) in a different Region, you must request a new certificate for each Region in which you plan to use it."},{"poster":"cldy","comment_id":"513193","upvote_count":"1","content":"D is correct.","timestamp":"1640857440.0"},{"upvote_count":"1","timestamp":"1640112060.0","poster":"AzureDP900","comment_id":"506358","content":"D is right!"},{"upvote_count":"2","content":"D. Request certificates for each FQDN in both the primary and secondary AWS Regions using AWS Certificate Manager. Associate the certificates with the corresponding ALBs in each AWS Region.","timestamp":"1639128180.0","poster":"cldy","comment_id":"498475"},{"timestamp":"1638332400.0","upvote_count":"1","poster":"AzureDP900","content":"Good explanation, D is right","comment_id":"491246"},{"timestamp":"1636257660.0","poster":"andylogan","comment_id":"449974","content":"It's D","upvote_count":"2"},{"poster":"AWS_Noob","content":"D. \n\nACM is regional. \nACM is for SSL / TLS","timestamp":"1636232460.0","upvote_count":"2","comment_id":"438838"},{"timestamp":"1635833460.0","content":"D is the correct answer","upvote_count":"1","poster":"nisoshabangu","comment_id":"373274"},{"timestamp":"1635696060.0","content":"ACM is regional service. so for every region certificate to be provisioned. Answer is D\nOption with KMS is just distraction","poster":"Radhaghosh","comment_id":"362476","upvote_count":"2"},{"poster":"WhyIronMan","comment_id":"334670","timestamp":"1635694140.0","content":"I'll go with D","upvote_count":"1"},{"content":"is D.\nhttps://docs.aws.amazon.com/acm/latest/userguide/acm-regions.html","timestamp":"1635664260.0","upvote_count":"1","comment_id":"292779","poster":"natpilot"},{"upvote_count":"2","content":"going with D","comment_id":"289428","timestamp":"1635654420.0","poster":"Kian1"},{"timestamp":"1635626100.0","content":"Answer is D","comment_id":"284000","upvote_count":"3","poster":"Ebi"},{"comment_id":"262927","timestamp":"1635482520.0","content":"It's D","upvote_count":"2","poster":"sanjaym"},{"comment_id":"242331","content":"D for sure","upvote_count":"1","poster":"gookseang","timestamp":"1635459480.0"},{"content":"Correct answer is D. Certificate should be in every region.","timestamp":"1635420120.0","poster":"T14102020","comment_id":"241923","upvote_count":"4"},{"timestamp":"1635259860.0","upvote_count":"1","content":"I'll go with D","comment_id":"228054","poster":"jackdryan"},{"timestamp":"1635213480.0","comment_id":"223551","content":"Answer D I guess\nThis should help in the explanation: https://aws.amazon.com/fr/blogs/aws/new-application-load-balancer-sni/","upvote_count":"1","poster":"Edgecrusher77"},{"content":"C AND D ARE THE SAME ANSWER JUST WRITTEN A BIT DIFFERENTLY","poster":"petebear55","upvote_count":"2","timestamp":"1635116280.0","comments":[{"timestamp":"1636106340.0","upvote_count":"1","comment_id":"408856","content":"You are wrong. Read the answers again","poster":"DerekKey"}],"comment_id":"221025"},{"comment_id":"219829","content":"D is correct","poster":"Bulti","upvote_count":"1","timestamp":"1635093240.0"},{"comment_id":"213988","upvote_count":"1","content":"D is correct.","poster":"vjt","timestamp":"1635066600.0"},{"upvote_count":"1","timestamp":"1635061800.0","content":"D is correct","poster":"ishuiyutian","comment_id":"207503"},{"upvote_count":"1","content":"https://aws.amazon.com/certificate-manager/faqs/?nc=sn&loc=5\nQ: Can I copy a certificate between AWS Regions?\nYou cannot copy ACM-managed certificates between regions at this time. You can copy private certificates that you export from ACM and certificates you issue directly from your ACM Private CA without using ACM for certificate and private key management.\nabove statement confirms that answer is 'D'","timestamp":"1634926260.0","poster":"Sagardonthineni","comment_id":"207324"},{"content":"I think is D.","timestamp":"1634914080.0","upvote_count":"1","poster":"exergeng","comment_id":"182736"},{"timestamp":"1634844840.0","content":"D is correct, ALB SSL termination require ACM from the same region as ALB","upvote_count":"1","comment_id":"148535","poster":"fullaws"},{"poster":"NikkyDicky","comment_id":"134094","content":"D for sure","timestamp":"1634771160.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"133545","content":"go with D\nACM's certificate, like most AWS resources, is a regional resource, multiple AWS\nUse certificates that support ELB for the same FQDN or FQDN set in the region.\nTo do so, you need to request or import a certificate for each region. Certificate provided by ACM\nIn this case, you must revalidate each domain name in the certificate for each region. Across regions\nThe certificate cannot be copied.","timestamp":"1634657580.0","poster":"noisonnoiton"},{"content":"answer D","poster":"mat2020","comment_id":"133256","timestamp":"1634417340.0","upvote_count":"1"},{"timestamp":"1634391540.0","upvote_count":"1","poster":"manoj101","comment_id":"125843","content":"D: To deploy a certificate on your load balancer, the certificate must be in the same Region as the load balancer."},{"comment_id":"111641","content":"D is the answer not c i submit wrong.","upvote_count":"1","poster":"ramikhreim","timestamp":"1634323980.0"},{"timestamp":"1634313960.0","upvote_count":"1","poster":"ramikhreim","comment_id":"111639","content":"Certificates in ACM are regional resources. To use a certificate with Elastic Load Balancing for the same fully qualified domain name (FQDN) or set of FQDNs in more than one AWS region, you must request or import a certificate for each region. For certificates provided by ACM, this means you must revalidate each domain name in the certificate for each region. You cannot copy a certificate between regions.\nhttps://docs.aws.amazon.com/acm/latest/userguide/acm-regions.html"},{"upvote_count":"1","content":"D is the right answer.\nFor eg: If you have deployed cloudfront you know that you have to request the certs in N. Virgina and the region where the app is deployed.","timestamp":"1634283840.0","poster":"meenu2225","comment_id":"105052"},{"poster":"Divine","timestamp":"1634183460.0","content":"Answer is D","comment_id":"83774","upvote_count":"2"},{"poster":"amog","timestamp":"1634113260.0","upvote_count":"3","comment_id":"44807","content":"Answer is D"},{"content":"D\nhttps://docs.aws.amazon.com/acm/latest/userguide/acm-regions.html\n\nCertificates in ACM are regional resources. To use a certificate with Elastic Load Balancing for the same fully qualified domain name (FQDN) or set of FQDNs in more than one AWS region, you must request or import a certificate for each region. For certificates provided by ACM, this means you must revalidate each domain name in the certificate for each region. You cannot copy a certificate between regions.","timestamp":"1634110680.0","upvote_count":"5","poster":"CloudFloater","comment_id":"41708"},{"poster":"markpark","upvote_count":"2","timestamp":"1634095860.0","comment_id":"41165","content":"the answer is D"},{"content":"Its D as per FAQ https://aws.amazon.com/certificate-manager/faqs/ and question is \"Can I use the same ACM certificate in more than one AWS Region?\"","upvote_count":"7","comment_id":"31354","poster":"JayK","timestamp":"1634028120.0"},{"content":"Answer is D:","comment_id":"22266","poster":"pra276","upvote_count":"2","timestamp":"1633785540.0"},{"content":"Answer is C. Remember the question stated \"...in a single AWS Region\" so D is incorrect as there is no secondary AWS region was mentioned.","upvote_count":"1","comments":[{"content":"the question says migrate the web application to a multi-region architecture. I feel it has to be D.","upvote_count":"3","comment_id":"18675","poster":"G3","timestamp":"1633585260.0"}],"timestamp":"1633312920.0","comment_id":"18515","poster":"eddkzew"},{"upvote_count":"7","timestamp":"1633308900.0","poster":"huhupai","comment_id":"17300","content":"D, To use a certificate with Elastic Load Balancing for the same site (the same fully qualified domain name, or FQDN, or set of FQDNs) in a different Region, you must request a new certificate for each Region in which you plan to use it."},{"comment_id":"13372","timestamp":"1633146360.0","upvote_count":"5","poster":"donathon","content":"C\nThe answer for C and D is actually very close. I have to read 5 times before I understand. C binds both certificate to both ALB where D binds 1 each. This means D will not work. ALB now can work with multiple certificates using SNI. With SNI support weâ€™re making it easy to use more than one certificate with the same ALB. The most common reason you might want to use multiple certificates is to handle different domains with the same load balancer. https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/ \nA\\B: Itâ€™s ACM not KMS.\nC: The most common reason you might want to use multiple certificates is to handle different domains with the same load balancer.","comments":[{"poster":"AWS2020","comment_id":"17260","timestamp":"1633245600.0","content":"Agreed ANS is C. using multiple certificates with the same ELB","upvote_count":"1"}]},{"poster":"donathon","comments":[{"upvote_count":"10","content":"Sorry my answer is D not C. My explanation already explained SNI is multiple certificates with the same ALB to handle different domains.","comment_id":"14063","timestamp":"1633235880.0","poster":"donathon","comments":[{"content":"Yes if using CloudFront it's one cert for the distribution requested in N.VA. if ELB(ALB) need a new cert for each region.\n\nhttps://aws.amazon.com/certificate-manager/faqs/ find on \"ACM Certificates\"\n\n\"Q: Can I use the same ACM certificate in more than one AWS Region?\n\nIt depends on whether youâ€™re using Elastic Load Balancing or Amazon CloudFront. To use a certificate with Elastic Load Balancing for the same site (the same fully qualified domain name, or FQDN, or set of FQDNs) in a different Region, you must request a new certificate for each Region in which you plan to use it. To use an ACM certificate with Amazon CloudFront, you must request the certificate in the US East (N. Virginia) region. ACM certificates in this region that are associated with a CloudFront distribution are distributed to all the geographic locations configured for that distribution.\"","comment_id":"51334","poster":"AWSPro24","upvote_count":"4","timestamp":"1634173440.0"}]}],"content":"C\nThe answer for C and D is actually very close. I have to read 5 times before I understand. C binds both certificate to both ALB where D binds 1 each. This means D will not work. ALB now can work with multiple certificates using SNI. With SNI support weâ€™re making it easy to use more than one certificate with the same ALB. The most common reason you might want to use multiple certificates is to handle different domains with the same load balancer. https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/ \nA\\B: Itâ€™s ACM not KMS.\nC: The most common reason you might want to use multiple certificates is to handle different domains with the same load balancer.","comment_id":"13371","timestamp":"1633115700.0","upvote_count":"1"},{"timestamp":"1633036260.0","content":"Yes, I would go with D","comment_id":"10948","upvote_count":"2","poster":"dpvnme"},{"comment_id":"10556","poster":"One_picese","upvote_count":"1","comments":[{"upvote_count":"1","content":"ALB supports multi cert.","timestamp":"1634181300.0","comment_id":"53887","poster":"SamuelK"}],"timestamp":"1632938940.0","content":"It seem ELB does not support multiple SSL certs \nPlease check answer 16 of Jayendra's Blog as following:\nhttp://jayendrapatil.com/aws-elastic-load-balancing/"},{"timestamp":"1632575760.0","upvote_count":"1","content":"I think that the right answer is D,please check following link\nhttps://aws.amazon.com/certificate-manager/\nhttps://datanextsolutions.com/blog/generate-ssl-certificates-using-aws-certificate-manager/","comment_id":"10553","poster":"One_picese"},{"content":"Ans is d","upvote_count":"2","comment_id":"10287","poster":"DalianYifang","timestamp":"1632436500.0"}],"question_images":[],"answers_community":[],"timestamp":"2019-08-26 16:48:00","unix_timestamp":1566830880,"answer_description":"Certificates in ACM are regional resources. To use a certificate with Elastic Load Balancing for the same fully qualified domain name (FQDN) or set of FQDNs in more than one AWS region, you must request or import a certificate for each region. For certificates provided by ACM, this means you must revalidate each domain name in the certificate for each region. You cannot copy a certificate between regions.\nReference:\nhttps://docs.aws.amazon.com/acm/latest/userguide/acm-regions.html","url":"https://www.examtopics.com/discussions/amazon/view/4109-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[]}],"exam":{"name":"AWS Certified Solutions Architect - Professional","id":32,"isBeta":false,"provider":"Amazon","isMCOnly":false,"numberOfQuestions":1019,"isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":73},"__N_SSP":true}