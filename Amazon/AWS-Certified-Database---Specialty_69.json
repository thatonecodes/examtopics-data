{"pageProps":{"questions":[{"id":"y1l5MQzKMs6NfPjpq3Oc","isMC":true,"question_text":"A company developed an AWS CloudFormation template used to create all new Amazon DynamoDB tables in its AWS account. The template configures provisioned throughput capacity using hard-coded values. The company wants to change the template so that the tables it creates in the future have independently configurable read and write capacity units assigned.\nWhich solution will enable this change?","unix_timestamp":1596202200,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/27047-exam-aws-certified-database-specialty-topic-1-question-82/","question_images":[],"timestamp":"2020-07-31 15:30:00","answer":"B","exam_id":22,"answer_description":"","choices":{"C":"Add values for the rcuCount and wcuCount parameters as outputs of the template. Configure DynamoDB to provision throughput capacity using the stack outputs.","D":"Add values for the rcuCount and wcuCount parameters to the Mappings section of the template. Replace the hard-coded values with calls to the Ref intrinsic function, referencing the new parameters.","A":"Add values for the rcuCount and wcuCount parameters to the Mappings section of the template. Configure DynamoDB to provision throughput capacity using the stack's mappings.","B":"Add values for two Number parameters, rcuCount and wcuCount, to the template. Replace the hard-coded values with calls to the Ref intrinsic function, referencing the new parameters."},"discussion":[{"content":"I will say 'B' too . the key for me to decide is the word 'number' since he wcu and rcu are numeric values.Not sure if my interpretation is correct ,but , will go with B","upvote_count":"1","timestamp":"1678313580.0","poster":"sk1974","comment_id":"833405"},{"content":"Selected Answer: B\nA and D are out, parameters are useful when you know the value for me mapping are like constant in any program language we can use them for Region, Accounts, AZ, etc and the question says \"...allocate independently variable read and write capacity...\" the word variable is key for me here.\n\nC- is out Output is for importing values into other stack.\n\nB is correct Use the optional Parameters section to customize your templates. \nParameters enable you to input custom values to your template each time you create or update a stack. from https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\n\n\n\nD -","poster":"rlnd2000","upvote_count":"3","comment_id":"615274","timestamp":"1655032080.0"},{"upvote_count":"1","poster":"Radhaghosh","timestamp":"1653154800.0","content":"Selected Answer: B\n\"The organization want to modify the template in order to allocate independently variable read and write capacity units to future tables.\" this is the vital sentence in the question. Only Option is Parameter. Option B","comment_id":"604964"},{"timestamp":"1651325820.0","comment_id":"595044","upvote_count":"1","poster":"novice_expert","content":"Selected Answer: B\nAdd values for two Number parameters, rcuCount and wcuCount, to the template. Replace the hard-coded values with calls to the Ref intrinsic function, referencing the new parameters."},{"comment_id":"492655","content":"Selected Answer: B\nOption B","timestamp":"1638461220.0","poster":"GMartinelli","upvote_count":"2"},{"upvote_count":"1","content":"I got this Question in exam.","poster":"guru_ji","comment_id":"446067","timestamp":"1635521820.0"},{"upvote_count":"1","content":"definetly A","timestamp":"1635456240.0","poster":"umatrilok","comment_id":"403339"},{"comment_id":"358892","timestamp":"1634743500.0","poster":"Aesthet","upvote_count":"1","content":"probably B"},{"upvote_count":"3","comments":[{"upvote_count":"1","content":"Mapping is not used like this way. Check https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html. We should use Parameter here so that the user could input what RCU/WCU they need when deploying a new stack.","poster":"Zhongkai","comments":[{"upvote_count":"1","content":"Input parameter and FindInMap\nYou can use an input parameter with the Fn::FindInMap function to refer to a specific value in a map. For example, suppose you have a list of regions and environment types that map to a specific AMI ID. You can select the AMI ID that your stack uses by using an input parameter (EnvironmentType). To determine the region, use the AWS::Region pseudo parameter, which gets the AWS Region in which you create the stack.\nI am not saying that B is not right answer though","poster":"Aesthet","comment_id":"358887","comments":[{"timestamp":"1670935140.0","poster":"ImprovMAN","upvote_count":"1","comment_id":"744005","content":"You can't include parameters, pseudo parameters, or intrinsic functions in the Mappings section. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html"}],"timestamp":"1634437560.0"}],"comment_id":"356072","timestamp":"1634119020.0"}],"content":"Why not A? It is obvious that there are multiple tables and rcu and wcu should be defined for each of them. I think mapping is a must here.","comment_id":"300038","timestamp":"1634057040.0","poster":"Windy"},{"timestamp":"1633879020.0","upvote_count":"3","poster":"myutran","comment_id":"299184","content":"Ans: B"},{"content":"B is my choice","upvote_count":"1","timestamp":"1633797660.0","comment_id":"253532","poster":"JobinAkaJoe"},{"upvote_count":"3","poster":"Ashoks","comment_id":"212872","content":"yes B for dynamic and not pre-determined values","timestamp":"1633240920.0"},{"comment_id":"161787","upvote_count":"4","timestamp":"1633142160.0","content":"GOing with B\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html","poster":"BillyMadison"},{"timestamp":"1633121700.0","poster":"Ebi","content":"B is correct","upvote_count":"1","comment_id":"159761"},{"poster":"Kitty0403","comment_id":"152296","timestamp":"1632824700.0","upvote_count":"1","content":"Answer is B"},{"upvote_count":"1","content":"B or C, please Thoughts","poster":"BillyC","timestamp":"1632540240.0","comment_id":"149789"}],"answer_images":[],"answers_community":["B (100%)"],"question_id":341,"answer_ET":"B"},{"id":"PGIWieQBQNN2vYPjOlSz","answer_ET":"D","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/26804-exam-aws-certified-database-specialty-topic-1-question-83/","question_id":342,"question_images":[],"discussion":[{"upvote_count":"1","timestamp":"1691322540.0","comment_id":"973778","poster":"IhorK","content":"Selected Answer: D\n- complex analytical queries\n- less than 1 second\n- 2 Regions\nAmazon Aurora Global Database cross-Region replication latencies below 1 second, recovery point objective (RPO) of 1 second and a recovery time objective (RTO) of less than 1 minute."},{"poster":"renfdo","timestamp":"1674986820.0","content":"Selected Answer: D\nUse an Amazon Aurora global database.","upvote_count":"1","comment_id":"791537"},{"upvote_count":"4","poster":"Arun32","comment_id":"725076","timestamp":"1669201860.0","content":"Surely D :\nGlobal Database uses storage-based replication with typical latency of less than 1 second, using dedicated infrastructure that leaves your database fully available to serve application workloads.\nhttps://aws.amazon.com/rds/aurora/global-database/"},{"upvote_count":"1","comment_id":"595278","poster":"novice_expert","content":"Selected Answer: D\nUse an Amazon Aurora global database.","timestamp":"1651351260.0"},{"upvote_count":"2","comment_id":"439509","timestamp":"1635837120.0","poster":"guru_ji","content":"Correct Answer: D\n\nDynamo DB ==>> No direct analytical queries (No joins)."},{"poster":"LB","timestamp":"1635741360.0","content":"D - Aurora Global Database - The solution needs to support complex analytical queries. Which eliminates Dynamodb from the equation.","comment_id":"438100","upvote_count":"2"},{"comment_id":"396928","poster":"sbhujbal","upvote_count":"3","timestamp":"1635261900.0","content":"option B is not possible since Quicksight cannot talk with DynamoDB and hence only possible option is D"},{"poster":"Aesthet","upvote_count":"3","content":"D final answer","timestamp":"1635069180.0","comment_id":"358896"},{"timestamp":"1634908380.0","content":"Ans: D","poster":"myutran","comment_id":"299196","upvote_count":"3"},{"poster":"halol","content":"why it's not A, I can see A and D make sense !","comment_id":"229407","timestamp":"1634581440.0","upvote_count":"1","comments":[{"timestamp":"1635299880.0","poster":"aws4myself","content":"Because they have asked for 1 sec replication lag.","upvote_count":"1","comment_id":"433356"}]},{"comment_id":"212875","content":"I would go with D. Covers cross region 1 sec latency and complex query","timestamp":"1634246880.0","poster":"Ashoks","upvote_count":"4"},{"timestamp":"1633945920.0","upvote_count":"2","content":"D - \"This means that committed transactional changes from the writer are replicated globally to the Regions that you select, typically within 1 second.\" -- based on link below","poster":"pdboi3355","comment_id":"205902"},{"upvote_count":"2","comment_id":"178901","content":"Answer is B\nComplex query and analytics is key to use RDS, not DynamoDB","timestamp":"1633761660.0","comments":[{"comment_id":"178903","upvote_count":"2","poster":"goodh32","timestamp":"1633839540.0","content":"**editing: \nAnswer is for Aurora (D) - which is Relational ready for complex queries"}],"poster":"goodh32"},{"upvote_count":"3","comment_id":"178636","timestamp":"1633747440.0","comments":[{"comment_id":"295150","poster":"GeeBeeEl","timestamp":"1634680620.0","content":"I dont agree with you, see https://aws.amazon.com/blogs/database/how-to-perform-advanced-analytics-and-build-visualizations-of-your-amazon-dynamodb-data-by-using-amazon-athena/","upvote_count":"2","comments":[{"upvote_count":"1","content":"That blog suggests to export data from DynamoDB to S3 first and use Athena to read from S3. This process is slow. In this question, the dashboard requires an instant access to the application's data. \n\nAnswer is D","timestamp":"1640315280.0","comment_id":"508286","poster":"jove"}]}],"poster":"qwertyuio","content":"D. Quicksight don't support Dynamodb"},{"timestamp":"1633579860.0","comment_id":"159769","content":"Answer is D","upvote_count":"1","poster":"Ebi"},{"poster":"Kitty0403","timestamp":"1632658320.0","upvote_count":"1","comment_id":"145353","content":"Answer is B","comments":[{"content":"Changing to B\nhttps://aws.amazon.com/blogs/database/aurora-postgresql-disaster-recovery-solutions-using-amazon-aurora-global-database/","comments":[{"timestamp":"1633300620.0","upvote_count":"2","content":"Sorry I meant D","poster":"Kitty0403","comment_id":"152304","comments":[{"upvote_count":"2","timestamp":"1633312020.0","comment_id":"152307","content":"1 sec is the key.. dyanmodb takes 2000ms for replication","poster":"Kitty0403"}]}],"poster":"Kitty0403","upvote_count":"2","timestamp":"1632863340.0","comment_id":"152303"}]},{"poster":"steves","content":"Going with B, less than 1 second replication required.","comment_id":"145181","upvote_count":"1","timestamp":"1632219300.0"}],"choices":{"C":"Use an Amazon RDS for MySQL DB instance deployed in the us-east-1 Region with a read replica instance in the ap-northeast-1 Region. Have the dashboard application read from the read replica.","A":"Use an Amazon RDS DB instance deployed in the us-east-1 Region with a read replica instance in the ap-northeast-1 Region. Create an Amazon ElastiCache cluster in the ap-northeast-1 Region to cache application data from the replica to generate the dashboards.","B":"Use an Amazon DynamoDB global table in the us-east-1 Region with replication into the ap-northeast-1 Region. Use Amazon QuickSight for displaying dashboard results.","D":"Use an Amazon Aurora global database. Deploy the writer instance in the us-east-1 Region and the replica in the ap-northeast-1 Region. Have the dashboard application read from the replica ap-northeast-1 Region."},"exam_id":22,"unix_timestamp":1595871360,"answer":"D","topic":"1","isMC":true,"timestamp":"2020-07-27 19:36:00","question_text":"A retail company with its main office in New York and another office in Tokyo plans to build a database solution on AWS. The company's main workload consists of a mission-critical application that updates its application data in a data store. The team at the Tokyo office is building dashboards with complex analytical queries using the application data. The dashboards will be used to make buying decisions, so they need to have access to the application data in less than 1 second.\nWhich solution meets these requirements?","answers_community":["D (100%)"],"answer_images":[]},{"id":"qJzf7SegRs87RKytc9U4","exam_id":22,"timestamp":"2020-07-09 09:25:00","answer_images":[],"answer_description":"","isMC":true,"question_images":[],"discussion":[{"poster":"BillyMadison","upvote_count":"12","timestamp":"1633423200.0","comment_id":"147599","content":"B&C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html#USER_LogAccess.PostgreSQL.log_retention_period\nTo set the retention period for system logs, use the rds.log_retention_period parameter. You can find rds.log_retention_period in the DB parameter group associated with your DB instance. The unit for this parameter is minutes. For example, a setting of 1,440 retains logs for one day. The default value is 4,320 (three days). The maximum value is 10,080 (seven days). Your instance must have enough allocated storage to contain the retained log files.\nTo retain older logs, publish them to Amazon CloudWatch Logs. For more information, see Publishing PostgreSQL Logs to CloudWatch Logs.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.PostgreSQL.html#USER_LogAccess.PostgreSQL.PublishtoCloudWatchLogs"},{"content":"Selected Answer: BC\nBC are correct.\n\nA is wrong, can not change default param group\nE is wrong, can not update postgresql.conf file on RDS, maybe can do on EC2 ?","upvote_count":"1","comment_id":"1009359","poster":"Pranava_GCP","timestamp":"1694902620.0"},{"content":"Selected Answer: BC\nyou can not alter default parameter group , i read the documentation of rds and there is no integration to export logs to s3","comment_id":"854333","upvote_count":"1","timestamp":"1680089940.0","poster":"ken_test1234"},{"poster":"backbencher2022","upvote_count":"1","comment_id":"845171","content":"Selected Answer: BC\nClearly B&C as per this AWS URL - https://aws.amazon.com/premiumsupport/knowledge-center/track-failed-login-rds-postgresql/","timestamp":"1679339820.0"},{"poster":"sk1974","comment_id":"833408","content":"Why not B&D . There is no need to analyze the logs in Cloudwatch.They have to be retained in 180 days. Isn't storing data in S3 cheaper than storing them in Cloudwatch ?","upvote_count":"1","timestamp":"1678314000.0"},{"poster":"ninjalight25","timestamp":"1677999360.0","upvote_count":"1","content":"Selected Answer: AC\nTo meet the logging and retention requirements, the following steps should be taken:\n\nUpdate the log_connections parameter in the default parameter group to enable connection logging.\n\nEnable publishing of database engine logs to Amazon CloudWatch Logs and set the event expiration to 180 days. This will allow the logging of all database connection requests to be retained for 180 days.","comment_id":"829694"},{"upvote_count":"1","timestamp":"1666799280.0","content":"why A,E? this is misleading there is no way you can edit postgresql.conf in RDS... correct is B&C","comment_id":"704838","poster":"subzzzero"},{"timestamp":"1651365120.0","content":"Selected Answer: BC\nB. Create a custom parameter group, update the log_connections parameter, and associate the parameter with the DB instance\n\nC. Enable publishing of database engine logs to Amazon CloudWatch Logs and set the event expiration to 180 days","poster":"novice_expert","upvote_count":"2","comment_id":"595368"},{"content":"Selected Answer: BC\nB and C.\n\nA wrong:\nParameter groups\nEach Amazon RDS PostgreSQL instance is associated with a parameter group that contains the engine specific configurations. The engine configurations also include several parameters that control PostgreSQL logging behavior. AWS provides the parameter groups with default configuration settings to use for your instances. However, to change the default settings, you must create a clone of the default parameter group, modify it, and attach it to your instance.\n\nTo set logging parameters for a DB instance, set the parameters in a DB parameter group and associate that parameter group with the DB instance. For more information, see Working with DB parameter groups.","upvote_count":"3","comment_id":"530458","timestamp":"1642935000.0","poster":"soyyodario"},{"comment_id":"365692","content":"B and C for sure.","timestamp":"1636208280.0","upvote_count":"1","poster":"Dip11"},{"poster":"Aesthet","content":"BC final answer","timestamp":"1636179720.0","comment_id":"358895","upvote_count":"2"},{"upvote_count":"4","content":"Ans: BC","timestamp":"1635607560.0","comment_id":"299201","poster":"myutran"},{"upvote_count":"1","poster":"JobinAkaJoe","timestamp":"1635548520.0","comment_id":"253535","content":"B and C"},{"content":"Yes B&C","poster":"Ashoks","upvote_count":"2","timestamp":"1634005320.0","comment_id":"212876"},{"content":"BC correct answer","upvote_count":"1","comment_id":"159771","poster":"Ebi","timestamp":"1633948980.0"},{"poster":"Kitty0403","timestamp":"1633370160.0","content":"Answer is B and C","upvote_count":"1","comment_id":"145354"},{"content":"agreed with B&C\nexport logs to Cloudwatch Logs","comment_id":"141904","upvote_count":"2","poster":"SaulGoodman","timestamp":"1633051500.0"},{"timestamp":"1632318600.0","comment_id":"139457","poster":"k115","content":"I will go for B & C","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/amazon/view/25211-exam-aws-certified-database-specialty-topic-1-question-84/","topic":"1","choices":{"E":"Connect to the RDS PostgreSQL host and update the log_connections parameter in the postgresql.conf file","B":"Create a custom parameter group, update the log_connections parameter, and associate the parameter with the DB instance","D":"Enable publishing of database engine logs to an Amazon S3 bucket and set the lifecycle policy to 180 days","C":"Enable publishing of database engine logs to Amazon CloudWatch Logs and set the event expiration to 180 days","A":"Update the log_connections parameter in the default parameter group"},"question_id":343,"answer_ET":"BC","answers_community":["BC (89%)","11%"],"answer":"BC","question_text":"A company is using Amazon RDS for PostgreSQL. The Security team wants all database connection requests to be logged and retained for 180 days. The RDS for PostgreSQL DB instance is currently using the default parameter group. A Database Specialist has identified that setting the log_connections parameter to 1 will enable connections logging.\nWhich combination of steps should the Database Specialist take to meet the logging and retention requirements? (Choose two.)","unix_timestamp":1594279500},{"id":"xyqyx6ztVexx7wEE3Kwd","unix_timestamp":1595245020,"answer_images":[],"choices":{"A":"Check that Amazon S3 has an IAM role granting read access to Neptune","E":"Check that Neptune has an IAM role granting read access to Amazon S3","C":"Check that a Neptune VPC endpoint exists","D":"Check that Amazon EC2 has an IAM role granting read access to Amazon S3","B":"Check that an Amazon S3 VPC endpoint exists"},"exam_id":22,"url":"https://www.examtopics.com/discussions/amazon/view/26224-exam-aws-certified-database-specialty-topic-1-question-85/","question_text":"A Database Specialist is creating a new Amazon Neptune DB cluster, and is attempting to load data from Amazon S3 into the Neptune DB cluster using the\nNeptune bulk loader API. The Database Specialist receives the following error:\n`Unable to connect to s3 endpoint. Provided source = s3://mybucket/graphdata/ and region = us-east-1. Please verify your\nS3 configuration.`\nWhich combination of actions should the Database Specialist take to troubleshoot the problem? (Choose two.)","answer_description":"","discussion":[{"poster":"k115","upvote_count":"16","timestamp":"1632088680.0","content":"B & E https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-IAM.html","comment_id":"139443"},{"poster":"BillyMadison","upvote_count":"8","content":"BE \nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html\n“An IAM role for the Neptune DB instance to assume that has an IAM policy that allows access to the data files in the S3 bucket. The policy must grant Read and List permissions.”\n“An Amazon S3 VPC endpoint. For more information, see the Creating an Amazon S3 VPC Endpoint section.”","comment_id":"147606","timestamp":"1633822860.0"},{"poster":"Germaneli","upvote_count":"2","comment_id":"1010804","content":"Selected Answer: AB\nA. Neptune wants to read from S3, so S3 needs to provide read access.\nB. Yes, we can make use of an Amazon S3 VPC endpoint to access data.\nx C. There's nothing like a Neptune VPC endpoint.\nx D. We're not dealing with EC2 here.\nx E. Neptune wants to read from S3, not he other way around.","timestamp":"1695060660.0"},{"poster":"Pranava_GCP","timestamp":"1694581380.0","content":"Selected Answer: BE\nB. Check that an Amazon S3 VPC endpoint exists\nE. Check that Neptune has an IAM role granting read access to Amazon S3\n\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html","comment_id":"1006210","upvote_count":"1"},{"content":"Selected Answer: BE\nMy final answer","upvote_count":"2","comment_id":"751540","timestamp":"1671572640.0","poster":"lollyj"},{"upvote_count":"1","timestamp":"1651415580.0","poster":"novice_expert","comment_id":"595625","content":"Selected Answer: BE\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html\nB. Check that an Amazon S3 VPC endpoint exists\nE. Check that Neptune has an IAM role granting read access to Amazon S3\nAn IAM role for the Neptune DB instance to assume that has an IAM policy that allows access to the data files in the S3 bucket. The policy must grant Read and List permissions."},{"poster":"RotterDam","timestamp":"1646540340.0","upvote_count":"3","content":"Selected Answer: BE\nBE are the correct answers\n- S3 VPC endpoint has to exist\n- Neptune must have the IAM role and policies with access to S3 bucket","comment_id":"561777"},{"poster":"soyyodario","content":"Selected Answer: BE\nB and E\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html","upvote_count":"2","timestamp":"1642935420.0","comment_id":"530461"},{"upvote_count":"1","content":"BE final answer","poster":"Aesthet","comment_id":"358898","timestamp":"1636190700.0"},{"poster":"manan728","upvote_count":"4","comment_id":"344998","content":"Guys this question was asked in my exam that I passed. B and E are correct.","timestamp":"1636055520.0"},{"timestamp":"1635722760.0","poster":"myutran","content":"Ans: BE","comment_id":"299204","upvote_count":"2"},{"poster":"Umer24","timestamp":"1635176580.0","content":"send me a note ryan23680 at yahoo for new aws database questions to find the correct answers.","upvote_count":"1","comment_id":"262717"},{"content":"Question-121\nA company's database specialist disabled TLS on an Amazon DocumentDB cluster to perform benchmarking tests. A few days after this change was implemented, a database specialist trainee accidentally deleted multiple tables. The database specialist restored the database from available snapshots. An hour after restoring the cluster. the database specialist is still unable to connect to the new cluster endpoint. What should the database specialist do to connect to the new. restored Amazon DocumentDB cluster?\nA. Change the restored cluster's parameter group to the original cluster's custom parameter group.\nB. Change the restored cluster's parameter group to the Amazon DocumentDB default parameter group.\nC. Configure the interface VPC endpoint and associate the new Amazon DocumentDB cluster.\nD. Run the synclnstances command in AWS DataSync.","poster":"Umer24","comment_id":"257843","comments":[{"poster":"Glendon","upvote_count":"2","comment_id":"274413","timestamp":"1635348420.0","content":"What is the answer for this? A?"}],"upvote_count":"1","timestamp":"1635164040.0"},{"content":"B and E my choice","poster":"JobinAkaJoe","upvote_count":"1","timestamp":"1634541240.0","comment_id":"253537"},{"timestamp":"1634113380.0","content":"Ans is B&E","poster":"Ashoks","comment_id":"212879","upvote_count":"2"},{"poster":"Ebi","content":"Answer is BE","comment_id":"159775","upvote_count":"2","timestamp":"1633945320.0"},{"comment_id":"145926","poster":"BillyC","timestamp":"1633737660.0","upvote_count":"2","content":"B and E here"},{"comment_id":"145355","poster":"Kitty0403","upvote_count":"4","timestamp":"1633723320.0","content":"Answer is B and E"},{"comment_id":"141911","content":"A and D.\nNeptune assume the role that give S3ReadOnly and S3 VPC gateway endpoint should exist","upvote_count":"1","timestamp":"1633066500.0","poster":"SaulGoodman"}],"isMC":true,"question_id":344,"question_images":[],"answer_ET":"BE","answers_community":["BE (82%)","AB (18%)"],"topic":"1","answer":"BE","timestamp":"2020-07-20 13:37:00"},{"id":"wINGKrhv9kTDU9XyGohn","answers_community":["A (100%)"],"question_images":[],"choices":{"C":"Modify the DB instance allocated storage to meet the forecasted requirements.","A":"Configure RDS Storage Auto Scaling.","B":"Configure RDS instance Auto Scaling.","D":"Monitor the Amazon CloudWatch FreeStorageSpace metric daily and add storage as required."},"question_text":"A database specialist manages a critical Amazon RDS for MySQL DB instance for a company. The data stored daily could vary from .01% to 10% of the current database size. The database specialist needs to ensure that the DB instance storage grows as needed.\nWhat is the MOST operationally efficient and cost-effective solution?","discussion":[{"poster":"ken_test1234","upvote_count":"2","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling","timestamp":"1680090240.0","comment_id":"854338"},{"content":"Selected Answer: A\nStorage auto scaling","poster":"novice_expert","timestamp":"1651249020.0","comment_id":"594587","upvote_count":"2"},{"poster":"Dantas","comment_id":"566016","upvote_count":"2","content":"Selected Answer: A\nThere's no Amazon RDS for MySQL DB instance autoscaling service (it's only available for Aurora). And it wouldn't solve the storage issue.","timestamp":"1647077640.0"},{"content":"Selected Answer: A\nagree with other comments","poster":"tugboat","upvote_count":"2","comment_id":"554987","timestamp":"1645662960.0"},{"comment_id":"529913","timestamp":"1642862280.0","poster":"peacegrace","upvote_count":"2","content":"Selected Answer: A\nCost Effective"},{"poster":"Shunpin","comment_id":"510153","content":"Ans: A\nFrom the AWS Console, you can only see storage section with autoscaling.","timestamp":"1640594880.0","upvote_count":"3"},{"poster":"shuraosipov","upvote_count":"4","timestamp":"1638398340.0","content":"Selected Answer: A\nAnswer is A.\nIf your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage.\n\nhttps://aws.amazon.com/about-aws/whats-new/2019/06/rds-storage-auto-scaling/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling","comment_id":"491982"},{"timestamp":"1635889920.0","upvote_count":"4","poster":"gelsm","content":"A. Configure RDS Storage Auto Scaling.\n\nhttps://aws.amazon.com/vi/about-aws/whats-new/2019/06/rds-storage-auto-scaling/\n\"operationally efficient and cost-effective\"\nRDS Storage Auto Scaling automatically scales storage capacity in response to growing database workloads, with zero downtime.\nThere is no additional cost for RDS Storage Auto Scaling.","comment_id":"414342"},{"poster":"Dip11","content":"A is correct ans.","comment_id":"365696","timestamp":"1635632640.0","upvote_count":"1"},{"timestamp":"1634878860.0","content":"Ans.A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling","comment_id":"364068","poster":"yossanjp","upvote_count":"1"},{"poster":"Aesthet","timestamp":"1634189580.0","content":"A final answer","upvote_count":"2","comment_id":"358900"},{"upvote_count":"1","timestamp":"1633285500.0","poster":"thanhphan","content":"I'll go with A. \nhttps://aws.amazon.com/vi/about-aws/whats-new/2019/06/rds-storage-auto-scaling/","comment_id":"356028"},{"timestamp":"1632739860.0","poster":"shantest1","upvote_count":"3","content":"A. Answer","comment_id":"326622"}],"answer_images":[],"timestamp":"2021-04-02 14:50:00","url":"https://www.examtopics.com/discussions/amazon/view/48806-exam-aws-certified-database-specialty-topic-1-question-86/","exam_id":22,"topic":"1","answer_description":"","unix_timestamp":1617367800,"question_id":345,"answer":"A","isMC":true,"answer_ET":"A"}],"exam":{"numberOfQuestions":359,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":false,"id":22,"provider":"Amazon","name":"AWS Certified Database - Specialty"},"currentPage":69},"__N_SSP":true}