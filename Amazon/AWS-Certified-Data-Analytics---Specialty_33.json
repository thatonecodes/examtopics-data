{"pageProps":{"questions":[{"id":"oGvaBq4tWJ8jTX6S2ABw","isMC":true,"answers_community":["C (100%)"],"answer_images":[],"question_id":161,"exam_id":20,"discussion":[{"comment_id":"348824","upvote_count":"23","poster":"VikG12","content":"If the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table.","timestamp":"1633939020.0"},{"poster":"VikG12","comment_id":"348823","timestamp":"1633036740.0","content":"Should be 'C'.","upvote_count":"11"},{"upvote_count":"1","content":"Selected Answer: C\nIf the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table.","comment_id":"926713","timestamp":"1687094280.0","poster":"cox1960"},{"timestamp":"1682959200.0","content":"C: I passed the test","upvote_count":"1","poster":"pk349","comment_id":"886510"},{"comments":[{"content":"No it's wrong.\n\"A new shard iterator is returned by every GetRecords request (as NextShardIterator), which you then use in the next GetRecords request (as ShardIterator). Typically, this shard iterator does not expire before you use it. However, you may find that shard iterators expire because you have not called GetRecords for more than 5 minutes, or because you've performed a restart of your consumer application.\n\nIf the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table. For more information, see Using a Lease Table to Track the Shards Processed by the KCL Consumer Application.\"\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly\nHence C","comment_id":"854864","upvote_count":"2","timestamp":"1680117420.0","poster":"sly_tail"}],"comment_id":"854860","timestamp":"1680117300.0","content":"A. Seems like A per this https://github.com/awslabs/amazon-kinesis-client/issues/701","upvote_count":"1","poster":"sly_tail"},{"poster":"maxvis","timestamp":"1677932340.0","comment_id":"828904","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly","upvote_count":"3"},{"poster":"Arjun777","content":"The ExpiredIteratorExceptions error indicates that the KCL is attempting to read records from a shard that has been closed or split. When a shard is split, two new shards are created, and records are redistributed among them. Any open iterator to the original shard becomes invalid and may return this error.\n\nTo resolve this issue, the data analyst should increase the number of worker threads that process the stream records. This will help ensure that records are read from the new shards as quickly as possible and reduce the likelihood of the KCL attempting to read from a closed or split shard.\n\nOption A is therefore the correct answer. Increasing the provisioned read or write capacity units assigned to the stream's Amazon DynamoDB table would not resolve this issue.","timestamp":"1676490900.0","comment_id":"809915","upvote_count":"1"},{"timestamp":"1666872480.0","upvote_count":"4","comment_id":"705522","poster":"cloudlearnerhere","content":"C is the right answer\nFor each Amazon Kinesis Data Streams application, KCL uses a unique lease table (stored in a Amazon DynamoDB table) to keep track of the shards in a KDS data stream that are being leased and processed by the workers of the KCL consumer application.\n\nFor each Amazon Kinesis Data Streams application, KCL uses a unique lease table (stored in a Amazon DynamoDB table) to keep track of the shards in a KDS data stream that are being leased and processed by the workers of the KCL consumer application.\n\nFor each Amazon Kinesis Data Streams application, KCL uses a unique lease table (stored in a Amazon DynamoDB table) to keep track of the shards in a KDS data stream that are being leased and processed by the workers of the KCL consumer application."},{"poster":"rocky48","upvote_count":"3","comment_id":"633819","timestamp":"1658289300.0","content":"Selected Answer: C\nShould be 'C'."},{"poster":"dushmantha","content":"KCL uses DynamoDB for checkpointing and coordination. There for DynamoDB table shoud be fast enough to handle the throughput. This exception means that it is not, so we need to incrase WCUs.","comment_id":"618523","upvote_count":"2","timestamp":"1655613720.0"},{"comment_id":"617319","timestamp":"1655395500.0","upvote_count":"1","poster":"lexaneon","content":"\"C\"\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly"},{"poster":"YahiaAglan74","content":"Selected Answer: C\nGo with answer C","comment_id":"606920","timestamp":"1653428940.0","upvote_count":"1"},{"comment_id":"603227","timestamp":"1652870340.0","content":"such a vague question. The question does not even mention that kinesis data stream is writing to Dynamodb. It just mentions some application is reading data from Kinesis using KCL. But C looks most sensible with assumption that application is writing output to DynamoDb","upvote_count":"3","poster":"certificationJunkie"},{"comment_id":"536392","timestamp":"1643567760.0","content":"Shard Iterator Expires Unexpectedly\nA new shard iterator is returned by every GetRecords request (as NextShardIterator), which you then use in the next GetRecords request (as ShardIterator). Typically, this shard iterator does not expire before you use it. However, you may find that shard iterators expire because you have not called GetRecords for more than 5 minutes, or because you've performed a restart of your consumer application.\n\nIf the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table. For more information, see Using a Lease Table to Track the Shards Processed by the KCL Consumer Application.","upvote_count":"1","poster":"datamech001"},{"upvote_count":"1","timestamp":"1636926960.0","comment_id":"478358","poster":"aws2019","content":"Agree with C"},{"poster":"ThomasKalva","content":"C; read text from blog\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly\nIf the shard iterator expires immediately, before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table. For more information, see Using a Lease Table to Track the Shards Processed by the KCL Consumer Application.","comment_id":"446305","upvote_count":"5","timestamp":"1635378900.0"},{"poster":"guerilla1987","comment_id":"398586","upvote_count":"1","content":"Go with C:\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#shard-iterator-expires-unexpectedly","timestamp":"1634234520.0"},{"poster":"Donell","timestamp":"1634010360.0","content":"C is correct.\nExpiredIteratorException => increase WCU","comment_id":"398488","upvote_count":"2"}],"answer_description":"","timestamp":"2021-05-03 18:49:00","topic":"1","question_text":"A company has an application that uses the Amazon Kinesis Client Library (KCL) to read records from a Kinesis data stream.\nAfter a successful marketing campaign, the application experienced a significant increase in usage. As a result, a data analyst had to split some shards in the data stream. When the shards were split, the application started throwing an ExpiredIteratorExceptions error sporadically.\nWhat should the data analyst do to resolve this?","unix_timestamp":1620060540,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/51718-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"C","answer":"C","choices":{"D":"Decrease the provisioned write capacity units assigned to the stream's Amazon DynamoDB table.","C":"Increase the provisioned write capacity units assigned to the stream's Amazon DynamoDB table.","A":"Increase the number of threads that process the stream records.","B":"Increase the provisioned read capacity units assigned to the stream's Amazon DynamoDB table."}},{"id":"SSBOQhAo5lzujfutvsth","timestamp":"2022-04-22 05:32:00","answer_ET":"A","question_images":[],"topic":"1","answer_description":"","question_id":162,"answer_images":[],"answer":"A","discussion":[{"comment_id":"859900","content":"A until the 29th of May 2023, afterwards C, since streaming ingestion with be GA for 6 months https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-redshift-real-time-streaming-ingestion-kds-msk/","poster":"mawsman","timestamp":"1680525120.0","upvote_count":"8"},{"timestamp":"1666872660.0","content":"A is the right answer\n\nYou can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.\n\nOption B is wrong as an AWS Glue Spark job running every 5 mins is not the quickest way.\n\nOption C is wrong as Kinesis Data Streams does not integrate with Redshift, also it would have a limit on message size.\n\nOption D is wrong as Kinesis Data Firehose would still add a delay of min 60 secs, also it would have a limit on message size. \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html","upvote_count":"6","poster":"cloudlearnerhere","comment_id":"705524"},{"timestamp":"1699493940.0","content":"I think that this question already make near realtime solution.\nso A is correct answer.\nthis is worst question I think.","upvote_count":"1","comment_id":"1066061","poster":"LocalHero"},{"timestamp":"1682959260.0","poster":"pk349","content":"A: I passed the test","comment_id":"886512","upvote_count":"1"},{"timestamp":"1666860180.0","upvote_count":"1","content":"Selected Answer: D\nI think D is faster.","poster":"Hussben","comment_id":"705375"},{"timestamp":"1666337640.0","upvote_count":"1","content":"Selected Answer: A\nKDS can't send data directly to RD so ruled out C.\nD has a 60 sec buffer time so ruled out\nSo A","comment_id":"700621","poster":"rav009"},{"comment_id":"637088","poster":"rocky48","timestamp":"1658805240.0","content":"Selected Answer: A\nAnswer - A","upvote_count":"2"},{"upvote_count":"1","poster":"Ramshizzle","comment_id":"619651","timestamp":"1655795580.0","content":"Selected Answer: A\nI guess it should be A as well. Triggering a data upload upon S3 upload seems like a perfect solution in this case.\n\nHowever, I'm thrown off a little bit by the SHORTEST TIME INTERVAL. Because in theory streaming the data directly into Redshift could be faster. This should then be done via FireHose, because Kinesis Data Streams cannot stream into Redshift directly without some other type of service."},{"upvote_count":"1","comment_id":"604916","poster":"f4bi4n","content":"Selected Answer: A\nFor now A, later C ;)\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html","timestamp":"1653141120.0"},{"poster":"jrheen","upvote_count":"1","timestamp":"1651357740.0","content":"Answer - A","comment_id":"595331"},{"comment_id":"589712","upvote_count":"4","content":"Selected Answer: A\nEvent driven requirement => S3 event stream","timestamp":"1650598320.0","poster":"rb39"}],"answers_community":["A (90%)","10%"],"choices":{"A":"Use S3 event notifications to trigger an AWS Lambda function to copy the vehicle reference data into Amazon Redshift immediately when the reference data is uploaded to Amazon S3.","D":"Send the reference data to an Amazon Kinesis Data Firehose delivery stream. Configure Kinesis with a buffer interval of 60 seconds and to directly load the data into Amazon Redshift.","B":"Create and schedule an AWS Glue Spark job to run every 5 minutes. The job inserts reference data into Amazon Redshift.","C":"Send reference data to Amazon Kinesis Data Streams. Configure the Kinesis data stream to directly load the reference data into Amazon Redshift in real time."},"url":"https://www.examtopics.com/discussions/amazon/view/74081-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"question_text":"A company is building a service to monitor fleets of vehicles. The company collects IoT data from a device in each vehicle and loads the data into Amazon\nRedshift in near-real time. Fleet owners upload .csv files containing vehicle reference data into Amazon S3 at different times throughout the day. A nightly process loads the vehicle reference data from Amazon S3 into Amazon Redshift. The company joins the IoT data from the device and the vehicle reference data to power reporting and dashboards. Fleet owners are frustrated by waiting a day for the dashboards to update.\nWhich solution would provide the SHORTEST delay between uploading reference data to Amazon S3 and the change showing up in the owners' dashboards?","unix_timestamp":1650598320,"exam_id":20},{"id":"I8KW9e4nq5szLMkVf0wZ","isMC":true,"answers_community":["ACE (83%)","ACF (17%)"],"answer_images":[],"exam_id":20,"question_id":163,"discussion":[{"comment_id":"350395","poster":"AjithkumarSL","timestamp":"1632858120.0","content":"yes.. I go with ACE.. \nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html\n\"Note : The cluster can reside only in one Availability Zone or subnet.\"","comments":[{"content":"I did not know that, thank you very much. Undoubtedly the correct answer is ACF.","upvote_count":"1","comment_id":"945596","poster":"juanife","timestamp":"1688729700.0"}],"upvote_count":"35"},{"poster":"yogen","comment_id":"510280","content":"ACE, for those in doubts for F - EMR cluster can only be launched in single availability zone, if availability zone failure is to be considered then a read replica of EMR cluster is configured in another availability zone with shared storage space. But this option is not there in the choice. so ACE is the correct answer\nhttps://aws.amazon.com/getting-started/hands-on/optimize-amazon-emr-clusters-with-ec2-spot/","timestamp":"1640607000.0","upvote_count":"9"},{"comment_id":"886513","poster":"pk349","content":"ACE: I passed the test","upvote_count":"1","timestamp":"1682959380.0"},{"content":"Selected Answer: ACE\nE not F because Amazon EMR clusters with multiple primary nodes are not tolerant to Availability Zone failures. \n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha-considerations.html","poster":"mawsman","timestamp":"1680525180.0","upvote_count":"1","comment_id":"859903"},{"timestamp":"1680400620.0","comment_id":"858441","content":"Selected Answer: ACE\nThis question appears in the Stephane Maarek's Udemy course.","poster":"Aina","upvote_count":"1","comments":[{"comment_id":"1080823","poster":"Tabby_cloudy","timestamp":"1701017100.0","content":"where ? I must have missed it.","upvote_count":"2"}]},{"poster":"Chelseajcole","content":"https://repost.aws/questions/QUvOaZvA5BT56skWO0iu2kZA/emr-in-2-a-zs-and-high-availability","upvote_count":"1","comment_id":"788915","timestamp":"1674750360.0"},{"upvote_count":"1","comment_id":"786823","content":"Amazon EMR clusters with multiple primary nodes are not tolerant to Availability Zone failures. In the case of an Availability Zone outage, you lose access to the Amazon EMR cluster.\nE - is not correct here","timestamp":"1674586620.0","poster":"Mang2000"},{"upvote_count":"6","poster":"cloudlearnerhere","content":"Correct answers are A, C & E\nOption A as the cluster is not persistent and terminated each business day, it would be best to use EMRFS and S3 as an external persistence layer.\nOption C as AWS Glue Data Catalog can be used as the metastore for Apache Hive.\nOption E as Multiple master nodes are hosted in a single AZ or subnet. \n\n\nOption B is wrong as HDFS would need a persistent cluster.\nOption D is wrong as the MySQL database should be external and not installed on the master nodes.\nOption F is wrong as multiple master nodes cannot be hosted in multiple AZs but in a single AZ.","timestamp":"1666872840.0","comment_id":"705529"},{"poster":"JHJHJHJHJ","timestamp":"1663954440.0","upvote_count":"1","comment_id":"677342","content":"E is correct (ACE) \nAmazon docs lists E"},{"poster":"Dun6","timestamp":"1660536840.0","content":"I go with ACE","comment_id":"647017","upvote_count":"1"},{"upvote_count":"1","poster":"rocky48","comment_id":"637100","content":"Selected Answer: ACE\nSelected Answer: ACE","timestamp":"1658805900.0"},{"upvote_count":"1","poster":"samsanta2012","content":"Selected Answer: ACF\nAmazon EMR supports multiple master nodes to enable high availability for EMR applications. EMR clusters with multiple master nodes are not tolerant of Availability Zone failures. In the case of an Availability Zone outage, you lose access to the EMR cluster. In the event that the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations simultaneously.","comment_id":"617414","timestamp":"1655416080.0"},{"poster":"Bik000","content":"Selected Answer: ACE\nMy Answer is A, C & E","comment_id":"604823","upvote_count":"1","timestamp":"1653130320.0"},{"poster":"MWL","comment_id":"593659","timestamp":"1651132980.0","upvote_count":"1","content":"Selected Answer: ACE\nMany explaination below."},{"upvote_count":"1","comments":[{"poster":"Japanese1","timestamp":"1645373760.0","upvote_count":"1","comment_id":"551995","content":"I'm not a fluent speaker of English, so it's possible that I didn't understand the intent of the question."}],"content":"A, C, F\nhttps://aws.amazon.com/jp/blogs/news/setting-up-read-replica-clusters-with-hbase-on-amazon-s3/","poster":"Japanese1","comment_id":"551993","timestamp":"1645373640.0"},{"timestamp":"1637468340.0","upvote_count":"1","comment_id":"483005","content":"I go with ACE..","poster":"aws2019"},{"poster":"nirmalmarathon","content":"The correct answer shown is BCF but in discussions it’s apparent the answe should be ACF. What’s the actual answer?","upvote_count":"1","timestamp":"1635928020.0","comments":[{"timestamp":"1645630680.0","comment_id":"554614","poster":"cnmc","content":"You must be new here... Always open the discussion to see the actual, correct answer","upvote_count":"3"}],"comment_id":"414233"},{"timestamp":"1634731740.0","content":"The question says that the EMR cluster must be highly available. E does not consider AZ failure. So F should be the choice. Answers - A,C,F","upvote_count":"1","comment_id":"403089","poster":"asg76","comments":[{"comment_id":"478797","upvote_count":"3","content":"An EMR cluster with multiple master nodes can reside only in one Availability Zone or subnet. Therefore must be E","poster":"brownest","timestamp":"1636992360.0"}]},{"poster":"bryankeb","content":"ACE\nhttps://aws.amazon.com/about-aws/whats-new/2019/04/amazon-emr-announces-support-for-multiple-master-nodes-to-enable-high-availability-for-EMR-applications/","comment_id":"350000","upvote_count":"6","timestamp":"1632785100.0"},{"upvote_count":"1","timestamp":"1632225360.0","comments":[{"upvote_count":"2","comment_id":"352285","content":"Changing to ACE","timestamp":"1633094940.0","poster":"VikG12"}],"comment_id":"348827","poster":"VikG12","content":"Should be A,C,F"}],"answer_description":"","timestamp":"2021-05-03 18:53:00","topic":"1","unix_timestamp":1620060780,"question_text":"A company is migrating from an on-premises Apache Hadoop cluster to an Amazon EMR cluster. The cluster runs only during business hours. Due to a company requirement to avoid intraday cluster failures, the EMR cluster must be highly available. When the cluster is terminated at the end of each business day, the data must persist.\nWhich configurations would enable the EMR cluster to meet these requirements? (Choose three.)","url":"https://www.examtopics.com/discussions/amazon/view/51719-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_images":[],"answer_ET":"ACE","answer":"ACE","choices":{"D":"MySQL database on the master node as the metastore for Apache Hive","A":"EMR File System (EMRFS) for storage","E":"Multiple master nodes in a single Availability Zone","C":"AWS Glue Data Catalog as the metastore for Apache Hive","F":"Multiple master nodes in multiple Availability Zones","B":"Hadoop Distributed File System (HDFS) for storage"}},{"id":"WBr6KyY9lptH9Fn3a4Mu","url":"https://www.examtopics.com/discussions/amazon/view/51724-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"question_id":164,"answer_description":"","question_images":[],"answer_images":[],"topic":"1","unix_timestamp":1620061440,"answers_community":["C (100%)"],"choices":{"D":"Use QuickSight Enterprise edition. Configure 1 administrator and 1,000 reader users. Configure an S3 data source and import the data into SPICE. Automatically refresh every 24 hours.","C":"Use QuickSight Enterprise edition. Configure 50 author users and 1,000 reader users. Configure an Athena data source and import the data into SPICE. Automatically refresh every 24 hours.","B":"Use QuickSight Standard edition. Configure 50 author users and 1,000 reader users. Configure an Athena data source with a direct query option.","A":"Load the data into an Amazon Redshift cluster by using the COPY command. Configure 50 author users and 1,000 reader users. Use QuickSight Enterprise edition. Configure an Amazon Redshift data source with a direct query option."},"timestamp":"2021-05-03 19:04:00","discussion":[{"comment_id":"398515","timestamp":"1634863800.0","comments":[{"content":"Answer C might be the best for saving time and money but Standard edition is best for saving money, and since the solution is expected for Cost effective, shouldn't B is the correct answer?","poster":"mickies9","comment_id":"416530","comments":[{"poster":"sanpak","content":"yeah, but what about refresh after 24 hrs? it is there in option C.","upvote_count":"2","comment_id":"500376","timestamp":"1639369620.0"}],"timestamp":"1635111840.0","upvote_count":"1"}],"content":"I think its C.\n\nWhen you create or edit a dataset, you choose to use either SPICE or a direct query, unless the dataset contains uploaded files. Importing (also called ingesting) your data into SPICE can save time and money:\n\nYour analytical queries process faster.\n\nYou don't need to wait for a direct query to process.\n\nData stored in SPICE can be reused multiple times without incurring additional costs. If you use a data source that charges per query, you're charged for querying the data when you first create the dataset and later when you refresh the dataset.\n\nQuotas for SPICE are as follows:\n\n2,047 Unicode characters for each field\n\n127 Unicode characters for each column name\n\n2,000 columns for each file\n\n1,000 files for each manifest\n\nFor Standard edition, 25 million (25,000,000) rows or 25 GB for each dataset\n\nFor Enterprise edition, 250 million (250,000,000) rows or 500 GB for each dataset\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/spice.html\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html","upvote_count":"26","poster":"Donell"},{"poster":"VikG12","timestamp":"1632256560.0","upvote_count":"12","content":"Looks like C.","comment_id":"348841"},{"comment_id":"1130852","poster":"AWSUMDBA","upvote_count":"2","timestamp":"1706112840.0","content":"standard edition can only have 100 users \nhttps://docs.aws.amazon.com/quicksight/latest/user/managing-users-standard.html"},{"upvote_count":"2","timestamp":"1682959380.0","content":"C: I passed the test","comment_id":"886515","poster":"pk349"},{"poster":"Arjun777","timestamp":"1676489700.0","comment_id":"809885","upvote_count":"1","content":"B is not the answer. SPICE can load upto 50GB of data as per the below link: Amazon QuickSight now supports larger SPICE datasets on the Enterprise Edition. Earlier each SPICE dataset could hold up to 250 million rows and 500GB of data. Now, all new SPICE datasets can accommodate up to 500 million rows (or 500GB) of data in the Enterprise Edition and 25 million rows (or 25GB) for Standard Edition. This raises the limit for your datasets, letting you accelerate dashboards with more data. See here for details."},{"timestamp":"1658455080.0","comment_id":"634943","poster":"rocky48","upvote_count":"1","content":"Selected Answer: C\nAnswer is C"},{"poster":"Teraxs","timestamp":"1651217100.0","upvote_count":"1","comment_id":"594288","content":"Selected Answer: C\nas explained by others"},{"timestamp":"1647775020.0","upvote_count":"1","poster":"ShilaP","content":"Answer is C","comment_id":"571560"},{"timestamp":"1640224440.0","poster":"npt","content":"C\nStandard edition does not support Readers\nhttps://aws.amazon.com/vi/quicksight/pricing/\nCompare table","upvote_count":"5","comment_id":"507486"},{"timestamp":"1640064720.0","poster":"lakediver","content":"Answer C\nOnly Enterprise Edition supports Readers. So eliminate B. \nNeed 50 authors so eliminate D \nBetween A and C - SPICE is most cost effective.","comment_id":"505832","upvote_count":"7"},{"timestamp":"1637418720.0","content":"C seems correct.","upvote_count":"1","comment_id":"482619","poster":"aws2019"},{"timestamp":"1634503740.0","comment_id":"396327","poster":"gunjan4392","content":"C seems correct. As mentioned by @aqs:\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html","upvote_count":"1"},{"comment_id":"381931","upvote_count":"2","timestamp":"1634152380.0","content":"Importing to SPICE still has the limit of 10GB. I think the answer should be B.\nhttps://aws.amazon.com/quicksight/pricing/?nc=sn&loc=2&dn=1","poster":"tukai"},{"upvote_count":"2","poster":"aqs","comment_id":"379575","timestamp":"1633708860.0","content":"C , based on article : https://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html , Spice Enterprise edition now allows upto 500 GB Dataset."},{"comment_id":"374252","poster":"KingD","content":"After intense research B is the most cost effective option check out the links below and see for yourselves. thanks\nhttps://aws.amazon.com/quicksight/pricing/\nhttps://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html","comments":[{"poster":"Huy","content":"Standard version supports only 100 users. Direct query option doesn't guarantee it is cheaper because there are 1000 users who scan your data via Athena.","upvote_count":"4","comments":[{"timestamp":"1634884080.0","poster":"owl2","comment_id":"405659","upvote_count":"1","comments":[{"comment_id":"478809","poster":"brownest","timestamp":"1636993140.0","upvote_count":"3","content":"https://docs.aws.amazon.com/quicksight/latest/user/managing-users-standard.html"}],"content":"Where did you see that ?"}],"timestamp":"1634368920.0","comment_id":"390062"}],"upvote_count":"2","timestamp":"1633558860.0"},{"timestamp":"1633406700.0","poster":"Chike","content":"Cant import more than 10GB into spice. How then do you visualize without bringing it into SPICE. B looks like the best option from all","upvote_count":"3","comment_id":"371500"},{"timestamp":"1632636000.0","comment_id":"357270","upvote_count":"4","comments":[{"timestamp":"1633060800.0","upvote_count":"4","content":"It wont be a cost effective solution as data is being brought into SPICE which is almost 200GB. only 10 GB spice data is give free of cost per user, for rest 190GB charge will be applied. So it should ideally be B.","comments":[{"upvote_count":"4","timestamp":"1635398460.0","content":"SPICE means the data will be reused; whereas 1000 users quering 200GB each with Athena.. that'll be costly.\nAnswer C","comment_id":"423854","poster":"Dr_Kiko"}],"comment_id":"367893","poster":"Monika14Sharma"}],"poster":"Heer","content":"ANSWER:C\nEXPLANATION:We do have author ,reader and admin type of roles /users defined in QuickSight and the question doesn't talk about the admin role .50 BI professional who developed and will be using the dashboard (Author) and 1000 users(Reader )."}],"answer":"C","question_text":"A retail company wants to use Amazon QuickSight to generate dashboards for web and in-store sales. A group of 50 business intelligence professionals will develop and use the dashboards. Once ready, the dashboards will be shared with a group of 1,000 users.\nThe sales data comes from different stores and is uploaded to Amazon S3 every 24 hours. The data is partitioned by year and month, and is stored in Apache\nParquet format. The company is using the AWS Glue Data Catalog as its main data catalog and Amazon Athena for querying. The total size of the uncompressed data that the dashboards query from at any point is 200 GB.\nWhich configuration will provide the MOST cost-effective solution that meets these requirements?","answer_ET":"C","isMC":true}],"exam":{"provider":"Amazon","isMCOnly":true,"name":"AWS Certified Data Analytics - Specialty","numberOfQuestions":164,"lastUpdated":"11 Apr 2025","id":20,"isBeta":false,"isImplemented":true},"currentPage":33},"__N_SSP":true}