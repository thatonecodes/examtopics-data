{"pageProps":{"questions":[{"id":"2GFIIbRNweYL936xgUVd","answer_images":[],"discussion":[{"comments":[{"content":"Agree with providing sources but disagree that the dead-letter would allow to \"retrieve client error details\", the SQS deadletter would hold the message, not any information about the error. This is exactly what option A does, allow you to automatically replay messages and see the error logging in the external (to AWS) application","poster":"robotgeek","upvote_count":"2","timestamp":"1696943760.0","comment_id":"1039537"}],"timestamp":"1677686160.0","upvote_count":"17","comment_id":"826045","poster":"Domdom120","content":"Selected Answer: B\nB.\nThe questions states: \"A SysOps administrator must implement a solution to retrieve client error details to help resolve this issue.\"\n\nSupporting answer:\n\"Amazon SQS supports dead-letter queues (DLQ), which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.\"\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\n\nI'm really disappointed in the comments sections for this exam in particular. If you're going to take the time to comment, please include reference documentation that supports your choice."},{"timestamp":"1682031060.0","comments":[{"content":"I agree, DLQ shows just Queue connection error.","timestamp":"1692191460.0","upvote_count":"1","comment_id":"982591","poster":"jipark"}],"comment_id":"876045","poster":"Gomer","content":"Selected Answer: A\nAnswer is absolutely \"A\" based on the links already provided. You need to replay the events, and try and identify why the monitoring app isn't able to contact EventBridge API. If you replace the monitoring app with SQS que, your just going to log Auto Scaling events. That isn't going to help you figure out why monitoring app isn't getting these same events through the EventBridge API. The SQS \"B\" answer may be easier than \"A\", but it isn't going to do any good on debugging EventBridge API access issue (probably a permissions/role/policy issue)","upvote_count":"8"},{"comment_id":"1319803","upvote_count":"2","content":"Selected Answer: B\nB>>This option is most suitable because Amazon EventBridge allows you to configure a dead-letter queue (DLQ) to handle events that fail to be delivered to the target. By directing these failed events to an Amazon SQS queue, you can store the events and analyze them to determine why the delivery failed, which may include client error details. This solution requires minimal operational effort as it leverages AWS's built-in mechanisms for error handling without the need to create additional resources or logging mechanisms.Creating an archive and replaying events is a more complex solution that might be necessary if you need to resend the events after fixing the issue. However, it doesn't directly address the requirement to retrieve error details and involves more operational steps compared to setting up a dead-letter queue.","poster":"numark","timestamp":"1732898100.0"},{"content":"Selected Answer: B\nDead-letter queues (DLQ) are a feature in EventBridge that allows failed events (such as client errors when invoking an API destination) to be sent to a dead-letter queue (in this case, an Amazon SQS queue).","poster":"rex3","upvote_count":"1","timestamp":"1727851020.0","comment_id":"1292244"},{"timestamp":"1722087960.0","comment_id":"1256315","content":"Selected Answer: A\nEventBridge Archive can retain event copies and replay them for debugging.","upvote_count":"2","poster":"VerRi"},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-archive.html","comment_id":"990102","timestamp":"1692969360.0","poster":"AWSdeveloper08","upvote_count":"6"},{"upvote_count":"4","poster":"Christina666","timestamp":"1690250880.0","content":"Selected Answer: A\nEventBridge can now archive and replay events:\n\nYou can now create an encrypted archive of the events published to an event bus. You can archive all events, or filter them using the same pattern matching syntax used by EventBridge rules. You can store events indefinitely, or set up a retention period after which older events are automatically removed from the archive.\nYou can also replay the events stored in an archive. Events are replayed to all rules defined for the event bus (but not to managed rules created by other AWS services) or to the rules you specify. Replayed events contain an extra replay-name field in case you need to recognize them. When starting a replay, you define a time frame, and only events within that time frame are replayed. Currently, you can only replay events to the same event bus from which they were archived.","comment_id":"962231"},{"timestamp":"1690171860.0","upvote_count":"1","content":"why not C? https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html","comment_id":"961110","poster":"RayHK"},{"timestamp":"1681806840.0","upvote_count":"2","content":"Selected Answer: B\nBased on the requirements to retrieve client error details with the least operational effort, option B would be the best solution. Adding an SQS standard queue as a dead-letter queue for the target will capture and store any failed messages, including error details. The messages can then be processed later to retrieve the error details, without requiring any additional configurations or modifications to the monitoring solution.\n\nOption A requires creating an EventBridge (CloudWatch Events) archive and replaying events, increasing logging, and then examining the error details. This solution requires additional effort and may not necessarily capture the exact error details that occurred.","poster":"Vivec","comment_id":"873411"},{"poster":"thatTeller30","comment_id":"852217","upvote_count":"2","content":"Selected Answer: A\nIm in my opinion its A - https://aws.amazon.com/blogs/aws/new-archive-and-replay-events-with-amazon-eventbridge/\nmainly because in the question it states they use the EventBridge API, meaning nothing goes into SQS and its unrelated to the question. By Default there's not invocation between EventBridge and SQS. U can however, create an Invoke Rule from EventBridge into SQS.","timestamp":"1679930760.0"},{"timestamp":"1677804300.0","comment_id":"827569","content":"A is the correct answer from my point of view. \nAfter carefully reading the question, I agree with \"yeacuz\" argument from the link below.\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-archive.html\nIn EventBridge, you can create an archive of events so that you can easily replay them at a later time. For example, you might want to replay events to recover from errors or to validate new functionality in your application.","upvote_count":"1","poster":"braveheart22"},{"content":"Option D, which is to configure the EventBridge (CloudWatch Events) rule to send error messages to an Amazon Simple Notification Service (Amazon SNS) topic, would be more efficient in terms of operational effort compared to Option C. This is because Option D requires only a simple configuration of the EventBridge (CloudWatch Events) rule and does not require the creation of a second EventBridge rule or a Lambda function. The error messages can be easily retrieved from the Amazon SNS topic and examined. I apologize for the error in my previous answer.","timestamp":"1676312820.0","comment_id":"807694","poster":"awsguru1998","comments":[{"comment_id":"824128","content":"Why would you sent error messages to SNS? Doesn't make any sense to me. An Amazon SNS topic is a logical access point that acts as a communication channel. A topic lets you group multiple endpoints (such as AWS Lambda, Amazon SQS, HTTP/S, or an email address).","upvote_count":"1","poster":"defmania00","timestamp":"1677529620.0"}],"upvote_count":"1"},{"comment_id":"773064","poster":"yeacuz","content":"Selected Answer: A\nAnswer is A:\n\n\"In EventBridge, you can create an archive of events so that you can easily replay them at a later time. For example, you might want to replay events to recover from errors or to validate new functionality in your application.\"\n\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-archive.html","timestamp":"1673490480.0","upvote_count":"4"},{"timestamp":"1673350500.0","content":"Selected Answer: B\nAnswer is B","upvote_count":"3","poster":"zolthar_z","comment_id":"771339"}],"answer":"A","answer_ET":"A","isMC":true,"answers_community":["A (51%)","B (49%)"],"timestamp":"2023-01-10 12:35:00","question_text":"A company is implementing a monitoring solution that is based on machine learning. The monitoring solution consumes Amazon EventBridge (Amazon CloudWatch Events) events that are generated by Amazon EC2 Auto Scaling. The monitoring solution provides detection of anomalous behavior such as unanticipated scaling events and is configured as an EventBridge (CloudWatch Events) API destination.\n\nDuring initial testing, the company discovers that the monitoring solution is not receiving events. However, Amazon CloudWatch is showing that the EventBridge (CloudWatch Events) rule is being invoked. A SysOps administrator must implement a solution to retrieve client error details to help resolve this issue.\n\nWhich solution will meet these requirements with the LEAST operational effort?","unix_timestamp":1673350500,"choices":{"C":"Create a second EventBridge (CloudWatch Events) rule for the same event pattern to target an AWS Lambda function. Configure the Lambda function to invoke the monitoring solution and to record the results to Amazon CloudWatch Logs. Examine the errors in the logs.","B":"Add an Amazon Simple Queue Service (Amazon SQS) standard queue as a dead-letter queue for the target. Process the messages in the dead-letter queue to retrieve error details.","D":"Configure the EventBridge (CloudWatch Events) rule to send error messages to an Amazon Simple Notification Service (Amazon SNS) topic.","A":"Create an EventBridge (CloudWatch Events) archive for the event pattern to replay the events. Increase the logging on the monitoring solution. Use replay to invoke the monitoring solution. Examine the error details."},"question_id":126,"exam_id":34,"url":"https://www.examtopics.com/discussions/amazon/view/94671-exam-aws-certified-sysops-administrator-associate-topic-1/","topic":"1","question_images":[],"answer_description":""},{"id":"PhVdfU03VnNMPdDPxLHx","question_images":[],"topic":"1","discussion":[{"poster":"Domdom120","timestamp":"1725199620.0","upvote_count":"9","comment_id":"826054","content":"Selected Answer: B\nB. Compliance mode is required for this situation. Comparison and reference below:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html\n\nIn governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary. You can also use governance mode to test retention-period settings before creating a compliance-mode retention period.\n\nIn compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account. When an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened. Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period."},{"timestamp":"1720002960.0","upvote_count":"8","poster":"zolthar_z","content":"Selected Answer: B\nAnswer is B, governance mode allows some users to delete/modify the data","comment_id":"764630"},{"timestamp":"1729465980.0","upvote_count":"2","content":"Selected Answer: B\nNobody on customer side including root can delete objects before they expire (unless maybe if you delete the entire account). I do know there is a \"force\" option for deleting buckets with existing objects. Would be interesting to see if that would do it. I\"m sure Amazon can always delete files if they have to (and customer insisted they do it).","poster":"Gomer","comment_id":"876062"},{"timestamp":"1723570260.0","comment_id":"807748","content":"B The S3 Object Lock feature in compliance mode can be used to enforce a retention period for objects in the bucket. This ensures that the backups are protected from deletion for at least 3 months after they are created, which meets the requirement. The retention period can be set at the bucket level, or at the object level.","poster":"awsguru1998","upvote_count":"2"}],"answer":"B","timestamp":"2023-01-03 13:36:00","answers_community":["B (100%)"],"answer_description":"","isMC":true,"answer_ET":"B","choices":{"D":"Enable S3 Object Lock on a new S3 bucket in governance mode. Place all backups in the new S3 bucket with a retention period of 3 months.","C":"Enable S3 Versioning on the existing S3 bucket. Configure S3 Lifecycle rules to protect the backups.","A":"Configure an IAM policy that denies the s3:DeleteObject action for all users. Three months after an object is written, remove the policy.","B":"Enable S3 Object Lock on a new S3 bucket in compliance mode. Place all backups in the new S3 bucket with a retention period of 3 months."},"url":"https://www.examtopics.com/discussions/amazon/view/93697-exam-aws-certified-sysops-administrator-associate-topic-1/","question_id":127,"exam_id":34,"answer_images":[],"question_text":"A company is storing backups in an Amazon S3 bucket. The backups must not be deleted for at least 3 months after the backups are created.\n\nWhat should a SysOps administrator do to meet this requirement?","unix_timestamp":1672749360},{"id":"eNjfE1JJKIRhZAO2PppP","answer_description":"","answer":"C","answer_images":[],"unix_timestamp":1672754100,"question_images":[],"answers_community":["C (100%)"],"exam_id":34,"answer_ET":"C","discussion":[{"upvote_count":"6","comment_id":"876762","content":"Selected Answer: C\nI was very confused by B and C, because they both do alerts. However, only \"Budgets\" lets yo select specific services and regions, and alarm based on forecast expenses. I made up a table of the differences between CW Billing Alarms and Budgets alarms, which is too big to post. However, these links helped gel my thoughts on this:\nhttps://aws.amazon.com/blogs/aws/aws-budgets-update-track-cloud-costs-and-usage/\nhttps://stackoverflow.com/questions/67680601/whats-the-difference-between-alarm-budget-and-cloudwatch-alarms-billing","poster":"Gomer","timestamp":"1713724620.0"},{"upvote_count":"4","content":"Selected Answer: C\nas to cost, let's use \"AWS Budget\" on this exam.","poster":"jipark","comment_id":"982608","timestamp":"1723814340.0"},{"timestamp":"1721873700.0","content":"Selected Answer: C\nOption A (Create an AWS Cost and Usage Report. Analyze the results in Amazon Athena. Configure an alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when costs reach 75% of the threshold. Subscribe the email distribution list to the topic.) is a bit complex and not directly related to tracking data transfer costs between AWS Regions. AWS Cost and Usage Reports and Amazon Athena are typically used for analyzing detailed cost and usage data, but they are not the most straightforward solution for this specific requirement.\n\nOption B (Create an Amazon CloudWatch billing alarm to detect when costs reach 75% of the threshold. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email distribution list to the topic.) is focused on billing alarms, which can track costs, but it may not specifically address data transfer costs between AWS Regions.","comments":[{"content":"Option D (Set up a VPC flow log. Set up a subscription filter to an AWS Lambda function to analyze data transfer. Configure the Lambda function to send a notification to the email distribution list when costs reach 75% of the threshold.) is an overcomplicated approach. VPC flow logs are primarily used for monitoring network traffic, and while they can be used for analyzing data transfer, they are not the most suitable option for tracking costs and sending cost-related alerts.\n\nOption C (Use AWS Budgets to create a cost budget for data transfer costs. Set an alert at 75% of the budgeted amount. Configure the budget to send a notification to the email distribution list when costs reach 75% of the threshold.) is the best option. AWS Budgets is a service designed explicitly for tracking and managing costs. By setting up a budget specifically for data transfer costs between AWS Regions and configuring an alert at 75% of the threshold, the SysOps administrator can receive timely alerts through email when the costs approach the defined threshold.","timestamp":"1721873700.0","comment_id":"962237","upvote_count":"3","poster":"Christina666"}],"poster":"Christina666","comment_id":"962236","upvote_count":"4"},{"timestamp":"1712990160.0","comment_id":"869155","content":"Selected Answer: C\nmaybe C\n\nUse AWS Budgets to create a cost budget for data transfer costs. Set an alert at 75% of the budgeted amount. Configure the budget to send a notification to the email distribution list when costs reach 75% of the threshold.\n\nhttps://aws.amazon.com/blogs/aws/aws-budgets-update-track-cloud-costs-and-usage/","upvote_count":"2","poster":"noahsark"},{"comment_id":"807721","poster":"awsguru1998","upvote_count":"2","content":"B is the most straightforward and easiest option, as it only requires creating an Amazon CloudWatch billing alarm and configuring it to send a message to an Amazon SNS topic when costs reach 75% of the threshold. Option C would achieve the same result, but it involves creating a budget and setting up alerts, which may be more complex to set up and maintain compared to creating a CloudWatch billing alarm.","timestamp":"1707850740.0"},{"content":"Selected Answer: C\ncertainly C","poster":"dangji","upvote_count":"2","timestamp":"1704524880.0","comment_id":"767364"},{"comment_id":"764706","poster":"zolthar_z","content":"Selected Answer: C\nAns is C","timestamp":"1704290100.0","upvote_count":"2"}],"topic":"1","choices":{"A":"Create an AWS Cost and Usage Report. Analyze the results in Amazon Athena. Configure an alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when costs reach 75% of the threshold. Subscribe the email distribution list to the topic.","D":"Set up a VPC flow log. Set up a subscription filter to an AWS Lambda function to analyze data transfer. Configure the Lambda function to send a notification to the email distribution list when costs reach 75% of the threshold.","C":"Use AWS Budgets to create a cost budget for data transfer costs. Set an alert at 75% of the budgeted amount. Configure the budget to send a notification to the email distribution list when costs reach 75% of the threshold.","B":"Create an Amazon CloudWatch billing alarm to detect when costs reach 75% of the threshold. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email distribution list to the topic."},"timestamp":"2023-01-03 14:55:00","isMC":true,"question_text":"A SysOps administrator needs to track the costs of data transfer between AWS Regions. The SysOps administrator must implement a solution to send alerts to an email distribution list when transfer costs reach 75% of a specific threshold.\n\nWhat should the SysOps administrator do to meet these requirements?","question_id":128,"url":"https://www.examtopics.com/discussions/amazon/view/93712-exam-aws-certified-sysops-administrator-associate-topic-1/"},{"id":"pUCeGLYJTaZ4ArXbIWjF","unix_timestamp":1672754160,"exam_id":34,"timestamp":"2023-01-03 14:56:00","question_text":"A company needs to archive all audit logs for 10 years. The company must protect the logs from any future edits.\n\nWhich solution will meet these requirements?","question_images":[],"answer_ET":"B","question_id":129,"discussion":[{"content":"B. Store the data in an Amazon S3 Glacier vault. Configure a vault lock policy for write-once, read-many (WORM) access. This will ensure that the data is stored securely with encryption and that it cannot be modified or deleted once it has been stored in the vault. The write-once, read-many (WORM) access provided by a vault lock policy helps ensure data immutability and compliance with regulatory requirements for long-term data retention.","timestamp":"1723568400.0","poster":"awsguru1998","comment_id":"807724","upvote_count":"6"},{"upvote_count":"2","poster":"dangji","timestamp":"1720242540.0","content":"Selected Answer: B\nB\nhttps://docs.aws.amazon.com/zh_tw/AmazonS3/latest/userguide/object-lock.html","comment_id":"767365"},{"content":"Selected Answer: B\nAns is B","timestamp":"1720007760.0","comment_id":"764707","upvote_count":"2","poster":"zolthar_z"}],"answer_description":"","isMC":true,"topic":"1","answers_community":["B (100%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/93713-exam-aws-certified-sysops-administrator-associate-topic-1/","choices":{"D":"Store the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Configure multi-factor authentication (MFA).","B":"Store the data in an Amazon S3 Glacier vault. Configure a vault lock policy for write-once, read-many (WORM) access.","A":"Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Configure AWS Key Management Service (AWS KMS) encryption.","C":"Store the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Configure server-side encryption."},"answer":"B"},{"id":"a4lbJEVPqpo2sQ6l6Yo4","topic":"1","answer_images":[],"exam_id":34,"timestamp":"2023-01-03 14:54:00","answers_community":["C (100%)"],"answer_description":"","isMC":true,"question_text":"A company’s AWS Lambda function is experiencing performance issues. The Lambda function performs many CPU-intensive operations. The Lambda function is not running fast enough and is creating bottlenecks in the system.\n\nWhat should a SysOps administrator do to resolve this issue?","url":"https://www.examtopics.com/discussions/amazon/view/93711-exam-aws-certified-sysops-administrator-associate-topic-1/","answer":"C","choices":{"A":"In the CPU launch options for the Lambda function, activate hyperthreading.","D":"Load the required code into a custom layer.","B":"Turn off the AWS managed encryption.","C":"Increase the amount of memory for the Lambda function."},"question_id":130,"unix_timestamp":1672754040,"question_images":[],"answer_ET":"C","discussion":[{"upvote_count":"7","content":"Selected Answer: C\nAns is C: https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html#configuration-memory-console","comment_id":"764709","poster":"zolthar_z","timestamp":"1688385540.0"},{"timestamp":"1708097100.0","upvote_count":"3","content":"Selected Answer: C\nLambda is AWS managed server,\nthus cannot choose CPU, but can add Memory.","comment_id":"982619","poster":"jipark"},{"timestamp":"1706156280.0","comment_id":"962240","upvote_count":"3","comments":[{"content":"Hey are you prepping for a license right now? I see you killing it in these comments, you sure go in detail.","timestamp":"1720244040.0","comment_id":"1115021","poster":"LudiVoss","comments":[{"comment_id":"1163156","timestamp":"1725147060.0","upvote_count":"1","poster":"Yowie351","content":"Chat GPT mate..."}],"upvote_count":"1"},{"timestamp":"1706156340.0","content":"Option C (Increase the amount of memory for the Lambda function) is the correct choice. AWS Lambda allows you to configure the amount of memory allocated to a function, and this has a direct impact on the available CPU power. When you increase the memory, Lambda automatically allocates proportional CPU power, which can significantly improve the performance of CPU-intensive operations. The more memory you allocate, the more CPU power the function receives, leading to faster execution times.\n\nOption D (Load the required code into a custom layer) is unrelated to addressing the performance issues caused by CPU-intensive operations. Custom layers in AWS Lambda are used to share code libraries and dependencies among multiple functions. While it can be helpful for code reuse and organization, it won't directly resolve the CPU bottleneck issue.","comment_id":"962241","upvote_count":"4","poster":"Christina666"}],"content":"Selected Answer: C\nOption A (In the CPU launch options for the Lambda function, activate hyperthreading) is not a valid option because AWS Lambda manages the underlying infrastructure, including the CPU configuration. SysOps administrators do not have direct access to adjust hyperthreading settings for Lambda functions.\n\nOption B (Turn off the AWS managed encryption) is unrelated to the performance issue caused by CPU-intensive operations. AWS managed encryption refers to the automatic encryption of data at rest for Lambda function code and other resources. Disabling encryption won't improve the Lambda function's performance.","poster":"Christina666"},{"comment_id":"826065","poster":"Domdom120","upvote_count":"2","timestamp":"1693578240.0","content":"Selected Answer: C\nC.\nZolthar provided correct reference."},{"poster":"ashu27","upvote_count":"2","comment_id":"764705","content":"you can improve performance by increasing the memory allocation, even if the function doesn’t use all of the memory.","timestamp":"1688385240.0"}]}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":477,"isImplemented":true,"isBeta":false,"provider":"Amazon","isMCOnly":false,"name":"AWS Certified SysOps Administrator - Associate","id":34},"currentPage":26},"__N_SSP":true}