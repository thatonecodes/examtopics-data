{"pageProps":{"questions":[{"id":"10nEb1UusF1lt2skeJvB","answers_community":["A (100%)"],"choices":{"C":"Use the mongodump and mongorestore tools to migrate the data from the source MongoDB deployment to Amazon DocumentDB (with MongoDB compatibility).","B":"Create an AWS Database Migration Service (AWS DMS) replication instance, a source endpoint for MongoDB, and a target endpoint of a MongoDB image that is hosted on Amazon EC2","D":"Use the mongodump and mongorestore tools to migrate the data from the source MongoDB deployment to a MongoDB image that is hosted on Amazon EC2.","A":"Create an AWS Database Migration Service (AWS DMS) replication instance, a source endpoint for MongoDB, and a target endpoint of Amazon DocumentDB (with MongoDB compatibility)."},"answer":"A","answer_description":"","answer_ET":"A","unix_timestamp":1662542640,"question_text":"An information management services company is storing JSON documents on premises. The company is using a MongoDB 3.6 database but wants to migrate to\nAWS. The solution must be compatible, scalable, and fully managed. The solution also must result in as little downtime as possible during the migration.\nWhich solution meets these requirements?","answer_images":[],"topic":"1","question_images":[],"question_id":161,"discussion":[{"upvote_count":"2","poster":"Pranava_GCP","timestamp":"1694129460.0","comment_id":"1001943","content":"Selected Answer: A\nA is the Answer\n\nAmazon DocumentDB (with MongoDB compatibility) ----> fully managed\nAWS DMS ----> with minimal downtime during the migration"},{"timestamp":"1669908300.0","content":"Selected Answer: A\nA is the Answer \n\nhttps://docs.aws.amazon.com/documentdb/latest/developerguide/docdb-migration.html#docdb-migration-approaches","poster":"examineme","comment_id":"732766","upvote_count":"2"},{"poster":"JeanGat","content":"Selected Answer: A\nDMS is the slowest, but has the least downtime. From link: https://docs.aws.amazon.com/documentdb/latest/developerguide/docdb-migration.html#docdb-migration-approaches","upvote_count":"2","comment_id":"681040","timestamp":"1664301660.0"},{"upvote_count":"1","poster":"SonamDhingra","timestamp":"1663148160.0","content":"Selected Answer: A\nA is correct. DMS for no downtime","comment_id":"668840"},{"content":"Selected Answer: A\nAccording to the link, little downtime is achieved with DMS approach, option A. https://docs.aws.amazon.com/documentdb/latest/developerguide/docdb-migration.html#docdb-migration-approaches","comment_id":"662299","upvote_count":"3","poster":"mbar94","timestamp":"1662542640.0"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/80863-exam-aws-certified-database-specialty-topic-1-question-243/","exam_id":22,"timestamp":"2022-09-07 11:24:00"},{"id":"KeyBSthBI04hCOd74rCa","answers_community":["D (100%)"],"answer_images":[],"choices":{"C":"Stopped DB instances will automatically restart if the number of attempted connections exceeds the threshold set.","D":"Stopped DB instances will automatically restart if the instance is not manually started after 7 days.","A":"When stopping the DB instance, the option to create a snapshot should have been selected.","B":"When stopping the DB instance, the duration for stopping the DB instance should have been selected."},"question_text":"A company stores critical data for a department in Amazon RDS for MySQL DB instances. The department was closed for 3 weeks and notified a database specialist that access to the RDS DB instances should not be granted to anyone during this time. To meet this requirement, the database specialist stopped all the\nDB instances used by the department but did not select the option to create a snapshot. Before the 3 weeks expired, the database specialist discovered that users could connect to the database successfully.\nWhat could be the reason for this?","question_id":162,"topic":"1","answer":"D","question_images":[],"unix_timestamp":1662542760,"url":"https://www.examtopics.com/discussions/amazon/view/80864-exam-aws-certified-database-specialty-topic-1-question-244/","answer_ET":"D","timestamp":"2022-09-07 11:26:00","isMC":true,"discussion":[{"comment_id":"744464","timestamp":"1670965320.0","comments":[{"upvote_count":"1","poster":"chikorita","timestamp":"1697634480.0","content":"sad story :(","comment_id":"1046937"}],"upvote_count":"6","content":"it cost me almost $2000 because of this, it was by SQL Server enterprise which automatically restart for weeks","poster":"Mardoyyy"},{"timestamp":"1709721360.0","comment_id":"1167091","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html#USER_StopInstance.Operation","upvote_count":"1","poster":"HSong"},{"upvote_count":"2","timestamp":"1694130120.0","comment_id":"1001945","poster":"Pranava_GCP","content":"Selected Answer: D\nD is the answer.\n\n\"By default, you can stop an Amazon RDS database instance for up to seven days at a time. After seven days, the instance restarts so that it doesn't miss any maintenance updates.\nTo stop your instance for more than seven days, you can use Step Functions to automate the workflow without missing a maintenance window.\"\n\nhttps://repost.aws/knowledge-center/rds-stop-seven-days"},{"timestamp":"1669908120.0","poster":"examineme","content":"Selected Answer: D\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-stop-seven-days/","upvote_count":"1","comment_id":"732762"},{"upvote_count":"1","timestamp":"1663148220.0","comment_id":"668841","poster":"SonamDhingra","content":"Selected Answer: D\nD is correct"},{"content":"Selected Answer: D\nIt's D - https://aws.amazon.com/premiumsupport/knowledge-center/rds-stop-seven-days/","timestamp":"1662542760.0","comment_id":"662300","upvote_count":"3","poster":"mbar94"}],"exam_id":22,"answer_description":""},{"id":"L9DjSZXCm9VcBsDWFlKI","timestamp":"2022-09-07 12:20:00","question_images":[],"choices":{"B":"Use Amazon Redshift for relational data and JSON data.","C":"Use Amazon RDS for relational data. Use Amazon Neptune for JSON data","A":"Use Amazon Redshift for relational data. Use Amazon DynamoDB for JSON data","D":"Use Amazon Redshift for relational data. Use Amazon S3 for JSON data."},"answer_ET":"B","question_id":163,"answer_description":"","unix_timestamp":1662546000,"exam_id":22,"isMC":true,"question_text":"A company uses an on-premises Microsoft SQL Server database to host relational and JSON data and to run daily ETL and advanced analytics. The company wants to migrate the database to the AWS Cloud. Database specialist must choose one or more AWS services to run the company's workloads.\nWhich solution will meet these requirements in the MOST operationally efficient manner?","answers_community":["B (82%)","A (18%)"],"topic":"1","answer_images":[],"answer":"B","discussion":[{"timestamp":"1664072700.0","comment_id":"678349","poster":"Usman0506","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/super-overview.html. It's B.","upvote_count":"7"},{"upvote_count":"1","poster":"woorkim","timestamp":"1736662080.0","comment_id":"1339406","content":"Selected Answer: B\nB because:\n\nAmazon Redshift can handle both data types efficiently:\n\nNative support for relational data\nSUPER data type for JSON data\n\n\nSingle service means:\n\nSimpler operations\nOne system to monitor\nOne set of backups\nOne security model\n\n\nWell-suited for existing workloads:\n\nNative support for ETL\nBuilt for advanced analytics\nSQL interface familiar to SQL Server users"},{"content":"Selected Answer: B\nB. Use Amazon Redshift \n\n\"By using semistructured data support in Amazon Redshift, you can ingest and store semistructured data in your Amazon Redshift data warehouses. Using the SUPER data type and PartiQL language, Amazon Redshift expands data warehouse capability to integrate with both SQL and NoSQL data sources. This way, Amazon Redshift enables efficient analytics on relational and semistructured stored data such as JSON.\"\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/super-overview.html","upvote_count":"1","poster":"Pranava_GCP","timestamp":"1694134620.0","comment_id":"1001990"},{"content":"Selected Answer: B\nUse the SUPER data type if you need to insert or update small batches of JSON data with low latency\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/super-overview.html#:~:text=Use%20the%20SUPER%20data%20type%20if%20you%20need%20to%20insert%20or%20update%20small%20batches%20of%20JSON%20data%20with%20low%20latency","comment_id":"850809","timestamp":"1679817240.0","poster":"dougporto1988","upvote_count":"1"},{"comment_id":"829369","upvote_count":"1","timestamp":"1677960900.0","content":"https://docs.aws.amazon.com/redshift/latest/dg/json-functions.html A is the answer . AWS itself is recommending not to use Redshift for JSON . It recommends using SUPER","poster":"sk1974"},{"poster":"Sathish_dbs","timestamp":"1673215320.0","content":"requirement is complex analytics which is never supported by DynamoDB...","comment_id":"769855","upvote_count":"1"},{"upvote_count":"3","comment_id":"752859","content":"Selected Answer: B\nB.\nBy using semistructured data support in Amazon Redshift, you can ingest and store semistructured data in your Amazon Redshift data warehouses. Using the SUPER data type and PartiQL language, Amazon Redshift expands data warehouse capability to integrate with both SQL and NoSQL data sources. This way, Amazon Redshift enables efficient analytics on relational and semistructured stored data such as JSON.","poster":"khun","timestamp":"1671665100.0"},{"comment_id":"695652","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/super-overview.htm","poster":"awsjjj","upvote_count":"1","timestamp":"1665861900.0","comments":[{"content":"\" to run daily ETL and advanced analytics\" is a keyword here","comment_id":"695684","poster":"awsjjj","upvote_count":"1","timestamp":"1665864060.0"}]},{"timestamp":"1664928480.0","comment_id":"686497","content":"B is correct I would say. I wouldn't use DynamoDB for advanced analytics, so that takes out A","upvote_count":"1","poster":"Zimboguru"},{"upvote_count":"1","timestamp":"1663148280.0","poster":"SonamDhingra","content":"Selected Answer: A\nA is correct","comment_id":"668843"},{"comment_id":"662337","content":"Selected Answer: A\nETL and advanced analytics - Redshift, JSON Data for sure DynamoDB. It's A.","poster":"mbar94","upvote_count":"2","timestamp":"1662546000.0","comments":[{"poster":"Germaneli","comment_id":"1025060","timestamp":"1696442520.0","upvote_count":"3","content":"Don't agree. DynamoDB cannot perform Advanced analytics, as demanded. It would be capable of hosting JSON data, yes, but that's not enough.\nHence, B."}]}],"url":"https://www.examtopics.com/discussions/amazon/view/80875-exam-aws-certified-database-specialty-topic-1-question-245/"},{"id":"y4EcbQdAXFebU4LRCNwt","choices":{"B":"Deploy an Amazon RDS Multi-AZ DB instance.","D":"Deploy multiple Amazon RDS DB instances. Use Amazon Route 53 DNS with failover health checks configured.","A":"Deploy an Amazon RDS DB instance with a read replica.","C":"Deploy Amazon DynamoDB global tables."},"answers_community":["B (100%)"],"question_text":"A company plans to migrate a MySQL-based application from an on-premises environment to AWS. The application performs database joins across several tables and uses indexes for faster query response times. The company needs the database to be highly available with automatic failover.\nWhich solution on AWS will meet these requirements with the LEAST operational overhead?","isMC":true,"timestamp":"2022-09-07 12:21:00","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/80876-exam-aws-certified-database-specialty-topic-1-question-246/","answer_images":[],"question_id":164,"exam_id":22,"unix_timestamp":1662546060,"answer_description":"","answer_ET":"B","question_images":[],"discussion":[{"upvote_count":"5","poster":"mbar94","comment_id":"662339","timestamp":"1662546060.0","content":"Selected Answer: B\nHigh availability = Multi AZ"},{"content":"Selected Answer: B\nB. Deploy an Amazon RDS Multi-AZ \n\nhttps://aws.amazon.com/rds/features/multi-az/\n\n\"Support high availability for your application with automatic database failover that completes as quickly as 60 seconds with zero data loss and no manual intervention.\"\n\n\"In an Amazon RDS Multi-AZ deployment, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention.\"","comment_id":"1002016","upvote_count":"4","poster":"Pranava_GCP","timestamp":"1694138340.0"},{"upvote_count":"1","poster":"mbadioum","content":"B: High Availability and Automatic Failover","timestamp":"1681308960.0","comment_id":"868446"},{"poster":"Don2021","timestamp":"1671810960.0","content":"B: its MySQL, multiple joins, failover","upvote_count":"1","comment_id":"754345"},{"comment_id":"675650","poster":"supratip","upvote_count":"3","content":"Selected Answer: B\nHigh availability = Multi AZ","timestamp":"1663810200.0"}],"answer":"B"},{"id":"IxNprtoRLIzz1xCan1FS","answer_ET":"B","unix_timestamp":1662547080,"discussion":[{"timestamp":"1702303020.0","comment_id":"1093535","poster":"silvaa360","upvote_count":"1","content":"\"read multiple items that have a specific partition key and sort key\"\n\nThe sentence iftself is wrong, or I'm completely blind. The query will only read multiple items if you have a pk+sk and you query by pk, but if you have pk+sk and query with both, only one result will be presented."},{"comment_id":"926426","content":"A and D are out of the question since we cannot consume all the capacity.\nBetween B and C. Since C is using BatchGetItem so it can read at max 100 items at a time while B is using Query API that can read as many items as matched with the query. Moreover reading a single item with GetItem is way faster than Querying that item as suggested in C. \nSo B is my answer.","upvote_count":"2","timestamp":"1687064220.0","poster":"MrAliMohsan"},{"content":"Parellel scan does not meet the requirement of \"without consuming all the provisioned throughput for the tables?\" as it can consume fully quickly unless if we handle it carefully. so Option B is the right one.","upvote_count":"1","timestamp":"1673217780.0","poster":"Sathish_dbs","comment_id":"769879"},{"comments":[{"content":"it says should not consume all throughput but A does that.\n\nA parallel scan with a large number of workers can easily consume all of the provisioned throughput for the table or index being scanned. It is best to avoid such scans if the table or index is also incurring heavy read or write activity from other applications.\n\nTo control the amount of data returned per request, use the Limit parameter. This can help prevent situations where one worker consumes all of the provisioned throughput, at the expense of all other workers.","upvote_count":"2","timestamp":"1673217660.0","poster":"Sathish_dbs","comment_id":"769878"}],"content":"Option should be A. As filter will still bring all the associated records which doesnt help the use case instead use the parallelscan which scan operation can logically divide the table into multiple segments.","poster":"saikirankshatriya","comment_id":"720133","upvote_count":"2","timestamp":"1668650160.0"},{"comment_id":"686499","timestamp":"1664929320.0","upvote_count":"3","poster":"Zimboguru","content":"Why not A\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan\n> The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition.\n\nTo address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel."},{"upvote_count":"2","comment_id":"681051","comments":[{"timestamp":"1712701680.0","content":"You are right. And it may do so in parallel which loads on throughput. \"In order to minimize response latency, BatchGetItem may retrieve items in parallel\" https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","upvote_count":"1","poster":"koki2847","comment_id":"1192536"}],"poster":"JeanGat","timestamp":"1664302320.0","content":"Selected Answer: B\nB. Comparing to C - which has BatchGetItem as the last step. This will do up to 100 GetItem requests. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.ReadData.html\n\nDefinitely not A or D."},{"content":"Selected Answer: B\nAgree, it's B.","comment_id":"662352","poster":"mbar94","timestamp":"1662547080.0","upvote_count":"1"}],"topic":"1","answer_description":"","question_text":"A social media company is using Amazon DynamoDB to store user profile data and user activity data. Developers are reading and writing the data, causing the size of the tables to grow significantly. Developers have started to face performance bottlenecks with the tables.\nWhich solution should a database specialist recommend to read items the FASTEST without consuming all the provisioned throughput for the tables?","question_id":165,"timestamp":"2022-09-07 12:38:00","answer":"B","choices":{"B":"Use the Scan API operation with a filter expression that allows multiple items to be read. Use the Query API operation to read multiple items that have a specific partition key and sort key. Use the GetItem API operation to read a single item.","A":"Use the Scan API operation in parallel with many workers to read all the items. Use the Query API operation to read multiple items that have a specific partition key and sort key. Use the GetItem API operation to read a single item.","C":"Use the Scan API operation with a filter expression that allows multiple items to be read. Use the Query API operation to read a single item that has a specific primary key. Use the BatchGetItem API operation to read multiple items.","D":"Use the Scan API operation in parallel with many workers to read all the items. Use the Query API operation to read a single item that has a specific primary key Use the BatchGetItem API operation to read multiple items."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/80879-exam-aws-certified-database-specialty-topic-1-question-247/","question_images":[],"answers_community":["B (100%)"],"answer_images":[],"exam_id":22}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":false,"name":"AWS Certified Database - Specialty","isBeta":false,"provider":"Amazon","isImplemented":true,"numberOfQuestions":359,"id":22},"currentPage":33},"__N_SSP":true}