{"pageProps":{"questions":[{"id":"19JuLq4yWc1wW1oPGiIq","answer_ET":"B","unix_timestamp":1679389380,"question_id":371,"answer_images":[],"isMC":true,"timestamp":"2023-03-21 10:03:00","discussion":[{"upvote_count":"14","comment_id":"1054306","poster":"TariqKipkemei","content":"Selected Answer: B\nBoth Amazon S3 File Gateway and AWS DataSync are suitable for this scenario.\nBut there is a requirement for 'LEAST administrative overhead'.\nOption C involves the creation of an entirely new application to consume the DataSync API, this rules out this option.","timestamp":"1714110120.0"},{"timestamp":"1696223520.0","comment_id":"858534","poster":"channn","content":"Selected Answer: B\nKey words: \n1. near-real-time (A is out)\n2. LEAST administrative (C n D is out)","upvote_count":"10"},{"content":"Selected Answer: D\nFirstly the requirement is not about hybrid cloud e.g. use the data migrated to aws from on-premise, so B is gone. Secondly it is near real time, A is gone. Thirdly less admistratively, C is one since an application is a headache. D is the best , it only involves a script. SFTP is good for light file transfer. Here there are only hundreds of files in a day.","comment_id":"1343277","upvote_count":"1","poster":"zdi561","timestamp":"1737335640.0"},{"poster":"Guru4Cloud","comment_id":"992177","comments":[{"timestamp":"1719732900.0","poster":"pentium75","upvote_count":"4","content":"\"Create an application\" hell no, the application must run somewhere etc., this is massive \"administrative overhead\".","comment_id":"1110415"}],"upvote_count":"2","timestamp":"1709131620.0","content":"Selected Answer: C\nThis option has the least administrative overhead because:\n\nUsing DataSync avoids having to rewrite the business system to use a new file gateway or SFTP endpoint.\nCalling the DataSync API from an application allows automating the data transfer instead of running scheduled tasks or scripts.\nDataSync directly transfers files from the network share to S3 without needing an intermediate server"},{"timestamp":"1701337560.0","comment_id":"910026","upvote_count":"3","poster":"antropaws","content":"Selected Answer: B\nB. Data Sync is better for one time migrations."},{"comments":[{"upvote_count":"4","content":"The other options would require more ongoing administrative effort:\n\nA) AWS DataSync would require creating and managing scheduled tasks and monitoring them. \n\nC) Using the DataSync API would require developing an application and then managing and monitoring it. \n\nD) The SFTP option would require creating scripts, managing SFTP access and keys, and monitoring the file transfer process.\n\nSo overall, the S3 File Gateway requires the least amount of ongoing management and administration as it presents a simple file share interface but handles the upload to S3 in a fully managed fashion. The business system can continue writing to a network share as is, while the files are transparently uploaded to S3.\n\nThe S3 File Gateway is the most hands-off, low-maintenance solution in this scenario.","comment_id":"883833","timestamp":"1698519240.0","poster":"kruasan"}],"upvote_count":"5","poster":"kruasan","timestamp":"1698519240.0","comment_id":"883832","content":"Selected Answer: B\nThe correct solution here is:\n\nB. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.\n\nThis option requires the least administrative overhead because:\n\n- It presents a simple network file share interface that the business system can write to, just like a standard network share. This requires minimal changes to the business system.\n\n- The S3 File Gateway automatically uploads all files written to the share to an S3 bucket in the background. This handles the transfer and upload to S3 without requiring any scheduled tasks, scripts or automation.\n\n- All ongoing management like monitoring, scaling, patching etc. is handled by AWS for the S3 File Gateway."},{"content":"Selected Answer: B\nA - creating a scheduled task is not near-real time.\nB - The S3 File Gateway caches frequently accessed data locally and automatically uploads it to Amazon S3, providing near-real-time access to the data.\nC - creating an application that uses the DataSync API in the automation workflow may provide near-real-time data access, but it requires additional development effort.\nD - it requires additional development effort.","timestamp":"1696071060.0","poster":"elearningtakai","upvote_count":"5","comment_id":"856931"},{"upvote_count":"2","poster":"zooba72","comment_id":"855192","timestamp":"1696039500.0","content":"Selected Answer: B\nIt's B. DataSync has a scheduler and it runs on hour intervals, it cannot be used real-time"},{"comments":[{"upvote_count":"3","timestamp":"1695665640.0","poster":"Buruguduystunstugudunstuy","content":"Answer A, using AWS DataSync to transfer the files to Amazon S3 and creating a scheduled task that runs at the end of each day, is not the best solution because it does not meet the requirement of storing the CSV reports in near-real time for analysis.\n\nAnswer B, creating an Amazon S3 File Gateway and updating the business system to use a new network share from the S3 File Gateway, is not the best solution because it requires additional configuration and management overhead.\n\nAnswer D, deploying an AWS Transfer for the SFTP endpoint and creating a script to check for new files on the network share and upload the new files using SFTP, is not the best solution because it requires additional scripting and management overhead","comment_id":"850436"}],"comment_id":"850431","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: C\nThe correct answer is C. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow.\n\nTo store the CSV reports generated by the business system in the AWS Cloud in near-real time for analysis, the best solution with the least administrative overhead would be to use AWS DataSync to transfer the files to Amazon S3 and create an application that uses the DataSync API in the automation workflow.\n\nAWS DataSync is a fully managed service that makes it easy to automate and accelerate data transfer between on-premises storage systems and AWS Cloud storage, such as Amazon S3. With DataSync, you can quickly and securely transfer large amounts of data to the AWS Cloud, and you can automate the transfer process using the DataSync API.","timestamp":"1695665340.0","upvote_count":"4"},{"timestamp":"1695395100.0","comment_id":"847317","content":"Selected Answer: B\nI think B is the better answer, \"LEAST administrative overhead\"\nhttps://aws.amazon.com/storagegateway/file/?nc1=h_ls","poster":"COTIT","upvote_count":"5"},{"timestamp":"1695392760.0","poster":"andyto","comment_id":"847260","content":"B - S3 File Gateway.\nC - this is wrong answer because data migration is scheduled (this is not continuous task), so condition \"near-real time\" is not fulfilled","upvote_count":"4"},{"upvote_count":"1","content":"C is the best ans","comments":[{"timestamp":"1695662700.0","content":"Why not A? There is no scheduled job?","upvote_count":"2","comment_id":"850400","poster":"lizzard812"}],"timestamp":"1695279780.0","comment_id":"845690","poster":"Thief"}],"choices":{"D":"Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.","B":"Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.","A":"Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.","C":"Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow."},"question_text":"A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis.\n\nWhich solution will meet these requirements with the LEAST administrative overhead?","exam_id":31,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/103452-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"B","question_images":[],"answers_community":["B (86%)","12%"],"topic":"1"},{"id":"m1fbwAJl9pmF3LwhmjgJ","choices":{"B":"Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identified storage tier.","C":"Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval.","A":"Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.","D":"Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-Infrequent Access (S3 One Zone-IA)."},"discussion":[{"content":"Selected Answer: A\nUnknown access patterns for the data = S3 Intelligent-Tiering","timestamp":"1717047720.0","upvote_count":"8","comment_id":"909935","poster":"TariqKipkemei"},{"upvote_count":"5","comment_id":"858538","timestamp":"1712034840.0","content":"Selected Answer: A\nKey words: 'The company does not know access patterns for all the data', so A.","poster":"channn"},{"comment_id":"992166","timestamp":"1724848500.0","content":"Selected Answer: A\nCreate an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.","upvote_count":"3","poster":"Guru4Cloud"},{"content":"Selected Answer: A\nThe correct answer is A. \n\nCreating an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering would be the most efficient solution to optimize the cost of S3 usage. S3 Intelligent-Tiering is a storage class that automatically moves objects between two access tiers (frequent and infrequent) based on changing access patterns. It is a cost-effective solution that does not require any manual intervention to move data to different storage classes, unlike the other options.","poster":"Buruguduystunstugudunstuy","timestamp":"1711396740.0","comments":[{"poster":"Buruguduystunstugudunstuy","comment_id":"850425","timestamp":"1711396740.0","content":"Answer B, Using the S3 storage class analysis tool to determine the correct tier for each object and manually moving objects to the identified storage tier would be time-consuming and require more operational overhead. \n\nAnswer C, Transitioning objects to S3 Glacier Instant Retrieval would be appropriate for data that is accessed less frequently and does not require immediate access. \n\nAnswer D, S3 One Zone-IA would be appropriate for data that can be recreated if lost and does not require the durability of S3 Standard or S3 Standard-IA.","upvote_count":"3"}],"comment_id":"850424","upvote_count":"5"},{"upvote_count":"3","timestamp":"1711127400.0","content":"Selected Answer: A\nFor me is A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.\n\nWhy?\n\"S3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns\"\nhttps://aws.amazon.com/s3/storage-classes/intelligent-tiering/","poster":"COTIT","comment_id":"847324"},{"poster":"Bofi","content":"Selected Answer: A\nOnce the data traffic is unpredictable, Intelligent-Tiering is the best option","comment_id":"846268","upvote_count":"3","timestamp":"1711048260.0"},{"upvote_count":"2","poster":"NIL8891","comment_id":"845269","content":"Selected Answer: A\nCreate an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.","timestamp":"1710971040.0"},{"comment_id":"845112","poster":"Maximus007","timestamp":"1710956100.0","content":"Selected Answer: A\nA: as exact pattern is not clear","upvote_count":"3"}],"timestamp":"2023-03-20 18:35:00","answer_images":[],"topic":"1","answers_community":["A (100%)"],"question_id":372,"answer_ET":"A","unix_timestamp":1679333700,"answer":"A","question_images":[],"answer_description":"","isMC":true,"exam_id":31,"question_text":"A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","url":"https://www.examtopics.com/discussions/amazon/view/103404-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"oYWPn0rswNj7QsbCO259","discussion":[{"comment_id":"850422","timestamp":"1679773860.0","poster":"Buruguduystunstugudunstuy","upvote_count":"16","content":"Selected Answer: BD\nTo resolve the issue of slow page loads for a rapidly growing e-commerce website hosted on AWS, a solutions architect can take the following two actions:\n\n1. Set up an Amazon CloudFront distribution\n2. Create a read replica for the RDS DB instance\n\nConfiguring an Amazon Redshift cluster is not relevant to this issue since Redshift is a data warehousing service and is typically used for the analytical processing of large amounts of data.\n\nHosting the dynamic web content in Amazon S3 may not necessarily improve performance since S3 is an object storage service, not a web application server. While S3 can be used to host static web content, it may not be suitable for hosting dynamic web content since S3 doesn't support server-side scripting or processing.\n\nConfiguring a Multi-AZ deployment for the RDS DB instance will improve high availability but may not necessarily improve performance."},{"content":"Selected Answer: BD\nCloud Front and Read Replica","timestamp":"1679428740.0","upvote_count":"5","poster":"acts268","comment_id":"846294"},{"upvote_count":"4","poster":"pentium75","content":"Selected Answer: BD\nA - Redshift is for OLAP, not OLTP\nB - Caching, reduces page load time and server load\nC - S3 can't host dynamic (!) content\nD - Read Replica is meant for increasing DB performance\nE - Multi-AZ is meant for HA (not asked here)","comment_id":"1110430","timestamp":"1704016980.0"},{"timestamp":"1693225980.0","content":"Selected Answer: BD\nThe two options that will best help resolve the slow page loads are:\n\nB) Set up an Amazon CloudFront distribution\n\nand\n\nE) Configure a Multi-AZ deployment for the RDS DB instance\n\nExplanation:\n\nCloudFront can cache static content globally and improve latency for static content delivery.\nMulti-AZ RDS improves performance and availability of the database driving dynamic content.","poster":"Guru4Cloud","comment_id":"992162","upvote_count":"4","comments":[{"poster":"MatAlves","upvote_count":"1","comment_id":"1282498","content":"Wrong. Multi-AZ is meant to \"enhance availability by deploying a standby instance in a second AZ, and achieve fault tolerance in the event of an AZ or database instance failure.\"\n\nIt does not improve performance.","timestamp":"1726123860.0"}]},{"content":"Selected Answer: BD\nBD is correct.","upvote_count":"4","comment_id":"910036","timestamp":"1685433120.0","poster":"antropaws"},{"content":"Selected Answer: BD\nResolve latency = Amazon CloudFront distribution and read replica for the RDS DB","poster":"TariqKipkemei","timestamp":"1685425560.0","comment_id":"909938","upvote_count":"5"},{"poster":"SamDouk","comment_id":"854867","content":"Selected Answer: BD\nB and D","upvote_count":"3","timestamp":"1680117780.0"},{"timestamp":"1679743980.0","poster":"klayytech","comment_id":"850076","content":"Selected Answer: BD\nThe website’s users are experiencing slow page loads.\n\nTo resolve this issue, a solutions architect should take the following two actions:\n\nCreate a read replica for the RDS DB instance. This will help to offload read traffic from the primary database instance and improve performance.","upvote_count":"3"},{"poster":"zooba72","content":"Selected Answer: BD\nQuestion asked about performance improvements, not HA. Cloudfront & Read Replica","timestamp":"1679715120.0","upvote_count":"3","comment_id":"849778"},{"upvote_count":"3","timestamp":"1679712840.0","poster":"thaotnt","comment_id":"849764","content":"Selected Answer: BD\nslow page loads. >>> D"},{"content":"Selected Answer: BD\nRead Replica will speed up Reads on RDS DB.\nE is wrong. It brings HA but doesn't contribute to speed which is impacted in this case. Multi-AZ is Active-Standby solution.","upvote_count":"1","timestamp":"1679600760.0","poster":"andyto","comment_id":"848618"},{"upvote_count":"2","content":"Selected Answer: BE\nI agree with B & E.\nB. Set up an Amazon CloudFront distribution. (Amazon CloudFront is a content delivery network (CDN) service)\nE. Configure a Multi-AZ deployment for the RDS DB instance. (Good idea for loadbalance the DB workflow)","comments":[{"comment_id":"1110429","content":"Multi-AZ for HA, Read Replica for Scalability\n\nhttps://aws.amazon.com/rds/features/read-replicas/?nc1=h_ls","timestamp":"1704016920.0","upvote_count":"2","poster":"pentium75"}],"comment_id":"847330","timestamp":"1679505540.0","poster":"COTIT"},{"upvote_count":"1","content":"B and E ( as there is nothing mention about read transactions)","timestamp":"1679503260.0","poster":"Santosh43","comments":[{"comment_id":"1126245","poster":"awsgeek75","timestamp":"1705615680.0","content":"Why E? There is nothing mentioned about High Availability also. E is wrong because Multi AZ won't help with scaling","upvote_count":"2"}],"comment_id":"847284"},{"content":"Selected Answer: BD\nCloudfront and Read Replica. We don't need HA here.","timestamp":"1679488440.0","upvote_count":"4","poster":"Akademik6","comment_id":"847053"},{"upvote_count":"1","poster":"Bofi","content":"Selected Answer: BE\nAmazon CloudFront can handle both static and Dynamic contents hence there is not need for option C l.e hosting the static data on Amazon S3. RDS read replica will reduce the amount of reads on the RDS hence leading a better performance. Multi-AZ is for disaster Recovery , which means D is also out.","comment_id":"846272","timestamp":"1679426340.0"},{"content":"Selected Answer: BC\nCloudFont with S3","timestamp":"1679389860.0","comment_id":"845697","poster":"Thief","upvote_count":"1","comments":[{"upvote_count":"3","comment_id":"1110428","timestamp":"1704016860.0","content":"S3 can't host \"dynamic content\"","poster":"pentium75"}]},{"content":"Selected Answer: BE\nB and E","poster":"NIL8891","upvote_count":"2","comment_id":"845267","timestamp":"1679348580.0"}],"question_images":[],"question_id":373,"answer":"BD","question_text":"A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website’s users are experiencing slow page loads.\n\nWhich combination of actions should a solutions architect take to resolve this issue? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/103423-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1679348580,"answer_ET":"BD","answer_images":[],"topic":"1","isMC":true,"timestamp":"2023-03-20 22:43:00","choices":{"B":"Set up an Amazon CloudFront distribution.","E":"Configure a Multi-AZ deployment for the RDS DB instance.","D":"Create a read replica for the RDS DB instance.","A":"Configure an Amazon Redshift cluster.","C":"Host the dynamic web content in Amazon S3."},"answers_community":["BD (90%)","8%"],"answer_description":"","exam_id":31},{"id":"e5UDVHgUozaESstDCDKP","question_id":374,"choices":{"C":"Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.","B":"Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.","D":"Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.","A":"Purchase an EC2 Instance Savings Plan Optimize the Lambda functions’ duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances."},"exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/103598-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"discussion":[{"comments":[{"comment_id":"850419","poster":"Buruguduystunstugudunstuy","upvote_count":"9","timestamp":"1679773440.0","content":"Answer A is not the best solution because connecting the Lambda functions directly to the private subnet that contains the EC2 instances may not be scalable as the number of Lambda functions increases. Additionally, using an EC2 Instance Savings Plan may not provide savings on the costs of running Lambda functions.\n\nAnswer B is not the best solution because connecting the Lambda functions to a public subnet may not be as secure as connecting them to a private subnet. Also, keeping the EC2 instances in a private subnet helps to ensure that they are not directly accessible from the public internet.\n\nAnswer D is not the best solution because keeping the Lambda functions in the Lambda service VPC may not provide direct network access to the EC2 instances, which may impact the performance of the application."}],"comment_id":"850418","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: C\nAnswer C is the best solution that meets the company’s requirements.\n\nBy purchasing a Compute Savings Plan, the company can save on the costs of running both EC2 instances and Lambda functions. The Lambda functions can be connected to the private subnet that contains the EC2 instances through a VPC endpoint for AWS services or a VPC peering connection. This provides direct network access to the EC2 instances while keeping the traffic within the private network, which helps to minimize network latency.\n\nOptimizing the Lambda functions’ duration, memory usage, number of invocations, and amount of data transferred can help to further minimize costs and improve performance. Additionally, using a private subnet helps to ensure that the EC2 instances are not directly accessible from the public internet, which is a security best practice.","upvote_count":"18","timestamp":"1679773440.0"},{"poster":"TariqKipkemei","content":"Selected Answer: C\nImplement Compute Savings Plan because it applies to Lambda usage as well, then connect the Lambda functions to the private subnet that contains the EC2 instances","upvote_count":"7","timestamp":"1698299820.0","comment_id":"1054316"},{"comment_id":"1282500","timestamp":"1726124100.0","upvote_count":"2","poster":"MatAlves","content":"\"Savings Plans are a flexible pricing model that offer low prices on Amazon EC2, AWS Lambda, and AWS Fargate usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1 or 3 year term.\"\n- That already excludes A and B.\n\nThe question requires to \"keep network latency between the services low\", which can be achieved by connecting the Lambda functions to the private subnet that contains the EC2 instances.\n\nC is the answer."},{"upvote_count":"2","comment_id":"1229796","content":"C\nhttps://aws.amazon.com/savingsplans/compute-pricing/","poster":"[Removed]","timestamp":"1718282160.0"},{"upvote_count":"4","poster":"Guru4Cloud","content":"Selected Answer: C\nA Compute Savings Plan covers both EC2 and Lambda and allows maximizing savings on all resources.\nOptimizing Lambda configuration reduces costs.\nConnecting the Lambda functions to the private subnet with the EC2 instances provides direct network access between them, keeping latency low.\nThe Lambda functions are isolated in the private subnet rather than public, improving security.","timestamp":"1693224060.0","comment_id":"992128"},{"timestamp":"1691105400.0","content":"CCCCCCCCCCCCCCCCCCCC","upvote_count":"2","comment_id":"971531","poster":"jaehoon090"},{"upvote_count":"2","poster":"elearningtakai","comment_id":"855243","content":"Selected Answer: C\nConnect Lambda to Private Subnet contains EC2","timestamp":"1680147180.0"},{"upvote_count":"6","content":"Selected Answer: C\nCompute savings plan covers both EC2 & Lambda","comment_id":"849782","timestamp":"1679715780.0","poster":"zooba72"},{"comment_id":"848359","content":"C. I would go with C, because Compute savings plans cover Lambda as well.","poster":"Zox42","timestamp":"1679584560.0","upvote_count":"5"},{"poster":"andyto","upvote_count":"1","comments":[{"comment_id":"848567","timestamp":"1679597280.0","upvote_count":"3","poster":"abitwrong","content":"EC2 Instance Savings Plans apply to EC2 usage only. Compute Savings Plans apply to usage across Amazon EC2, AWS Lambda, and AWS Fargate. (https://aws.amazon.com/savingsplans/faq/)\n \nLambda functions need direct network access to the EC2 instances for the application to work and these EC2 instances are in the private subnet. So the correct answer is C."}],"timestamp":"1679503260.0","comment_id":"847287","content":"A. I would go with A. Saving and low network latency are required.\nEC2 instance savings plans offer savings of up to 72%\nCompute savings plans offer savings of up to 66%\nPlacing Lambda on the same private network with EC2 instances provides the lowest latency."}],"answers_community":["C (100%)"],"question_text":"A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work.\n\nThe application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low.\n\nWhich solution will meet these requirements?","timestamp":"2023-03-22 17:41:00","answer_ET":"C","answer_images":[],"answer":"C","topic":"1","isMC":true,"answer_description":"","unix_timestamp":1679503260},{"id":"PIdIVRLp57d3O7baPD0m","answer":"B","answer_ET":"B","topic":"1","timestamp":"2023-03-22 13:42:00","exam_id":31,"isMC":true,"question_id":375,"choices":{"C":"Turn off the S3 Block Public Access feature on the S3 bucket in the production account.","A":"Attach the Administrator Access policy to the development account users.","D":"Create a user in the production account with unique credentials for each team member.","B":"Add the development account as a principal in the trust policy of the role in the production account."},"url":"https://www.examtopics.com/discussions/amazon/view/103585-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","question_images":[],"unix_timestamp":1679488920,"discussion":[{"comments":[{"comment_id":"892200","upvote_count":"10","timestamp":"1715174520.0","content":"Thanks good luck for all","poster":"SkyZeroZx"},{"content":"thank you!","poster":"Kimnesh","timestamp":"1724131620.0","upvote_count":"6","comment_id":"985563"}],"comment_id":"875830","upvote_count":"78","content":"well, if you made it this far, it means you are persistent :) Good luck with your exam!","timestamp":"1713635100.0","poster":"kels1"},{"timestamp":"1712199300.0","content":"Selected Answer: B\nBy adding the development account as a principal in the trust policy of the IAM role in the production account, you are allowing users from the development account to assume the role in the production account. This allows the team members to access the S3 bucket in the production account without granting them unnecessary privileges.","poster":"gpt_test","comment_id":"860568","upvote_count":"9"},{"poster":"digital_eye","comment_id":"1410950","timestamp":"1743088980.0","content":"Selected Answer: B\nGood luck everyone","upvote_count":"1"},{"timestamp":"1729922400.0","content":"Selected Answer: B\nAdd the development account as a principal in the trust policy of the role in the production account","upvote_count":"3","poster":"TariqKipkemei","comment_id":"1054319"},{"comment_id":"992102","upvote_count":"5","poster":"Guru4Cloud","timestamp":"1724844840.0","content":"Selected Answer: B\nThe best solution is B) Add the development account as a principal in the trust policy of the role in the production account.\n\nThis allows cross-account access to the S3 bucket in the production account by assuming the IAM role. The development account users can assume the role to gain temporary access to the production bucket."},{"comment_id":"943121","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/\n\nAn AWS account accesses another AWS account – This use case is commonly referred to as a cross-account role pattern. It allows human or machine IAM principals from one AWS account to assume this role and act on resources within a second AWS account. A role is assumed to enable this behavior when the resource in the target account doesn’t have a resource-based policy that could be used to grant cross-account access.","timestamp":"1720121820.0","upvote_count":"3","poster":"nilandd44gg"},{"poster":"elearningtakai","content":"Selected Answer: B\nAbout Trust policy – The trust policy defines which principals can assume the role, and under which conditions. A trust policy is a specific type of resource-based policy for IAM roles. \n\nAnswer A: overhead permission Admin to development. \nAnswer C: Block public access is a security best practice and seems not relevant to this scenario.\nAnswer D: difficult to manage and scale","upvote_count":"3","timestamp":"1711777200.0","comment_id":"855244"},{"poster":"Buruguduystunstugudunstuy","timestamp":"1711395300.0","comment_id":"850408","upvote_count":"3","content":"Selected Answer: B\nAnswer A, attaching the Administrator Access policy to development account users, provides too many permissions and violates the principle of least privilege. This would give users more access than they need, which could lead to security issues if their credentials are compromised.\n\nAnswer C, turning off the S3 Block Public Access feature, is not a recommended solution as it is a security best practice to enable S3 Block Public Access to prevent accidental public access to S3 buckets.\n\nAnswer D, creating a user in the production account with unique credentials for each team member, is also not a recommended solution as it can be difficult to manage and scale for large teams. It is also less secure, as individual user credentials can be more easily compromised."},{"upvote_count":"2","timestamp":"1711368660.0","poster":"klayytech","content":"Selected Answer: B\nThe solution that will meet these requirements while complying with the principle of least privilege is to add the development account as a principal in the trust policy of the role in the production account. This will allow team members to access Amazon S3 buckets in two different AWS accounts while complying with the principle of least privilege. \n\nOption A is not recommended because it grants too much access to development account users. Option C is not relevant to this scenario. Option D is not recommended because it does not comply with the principle of least privilege.","comment_id":"850105"},{"timestamp":"1711111320.0","comment_id":"847065","content":"Selected Answer: B\nB is the correct answer","upvote_count":"3","poster":"Akademik6"}],"answers_community":["B (100%)"],"question_text":"A solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts: a development account and a production account. The team currently has access to S3 buckets in the development account by using unique IAM users that are assigned to an IAM group that has appropriate permissions in the account.\n\nThe solutions architect has created an IAM role in the production account. The role has a policy that grants access to an S3 bucket in the production account.\n\nWhich solution will meet these requirements while complying with the principle of least privilege?","answer_images":[]}],"exam":{"lastUpdated":"11 Apr 2025","isImplemented":true,"id":31,"isMCOnly":true,"numberOfQuestions":1019,"provider":"Amazon","isBeta":false,"name":"AWS Certified Solutions Architect - Associate SAA-C03"},"currentPage":75},"__N_SSP":true}