{"pageProps":{"questions":[{"id":"xaLtbqruhIONtnSgIg5u","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/48839-exam-aws-certified-database-specialty-topic-1-question-144/","question_images":[],"exam_id":22,"answer_description":"","answer":"A","question_id":51,"discussion":[{"poster":"Jaypdv","comments":[{"upvote_count":"3","content":"And C. does not automatically scale as in the requirement","comment_id":"328549","timestamp":"1634078760.0","poster":"Jaypdv","comments":[{"poster":"shantest1","content":"A seems to be correct \nI think so as well, c is in reverse","comment_id":"329611","upvote_count":"1","timestamp":"1634744820.0"}]},{"upvote_count":"2","timestamp":"1634836080.0","content":"For 5000 RCU you are assuming strongly consistent reads. If it's an eventual consistent read you only need 2500 RCU","poster":"manan728","comment_id":"342650"}],"content":"I'll go for A.\nI think C. is a trap; if you are writing 5000 items at 2KB/second then you need 10000 WCU and 5000 RCU (assuming eventually consistent reads). C. has it in reverse.","timestamp":"1632754560.0","upvote_count":"14","comment_id":"327823"},{"content":"Selected Answer: A\nthe next closest answer would be DynamoDB with Auto scaling but that it not part of the answers hence on-demand is the best answer","comment_id":"1040798","timestamp":"1697036880.0","poster":"Sathish_dbs","upvote_count":"1"},{"poster":"IhorK","content":"Selected Answer: A\nWith 2KB size for 5,000 orders per second we need 10 000 WCU (1 table write/sec = 1 WCU with blocks of 1KB). C suggest 5,000 write capacity units (WCUs), not enough.","timestamp":"1691404260.0","comment_id":"974587","upvote_count":"1"},{"upvote_count":"1","poster":"awsjjj","content":"Selected Answer: A\n1250 RCUs can read 10,000 KB. A is correct","comments":[{"content":"1250 RCUs can read 10,000 KB if its eventual consistent read. A is correct","upvote_count":"1","comment_id":"696863","poster":"awsjjj","timestamp":"1665981780.0"},{"comment_id":"698655","poster":"awsjjj","timestamp":"1666152660.0","upvote_count":"2","content":"2500 RCU for reads"}],"timestamp":"1665981780.0","comment_id":"696860"},{"content":"Selected Answer: A\nI'll go for A. C is not scalable & cost affective.","upvote_count":"1","comment_id":"602858","poster":"praffuln","timestamp":"1652778420.0"},{"upvote_count":"1","content":"Selected Answer: A\n25 milliseconds\nOutside of the sale, the amount of traffic is really minimal\nThe business seeks the most cost-effective solution possible that is both highly accessible and scales automatically.","comment_id":"594801","timestamp":"1651284900.0","poster":"novice_expert"},{"timestamp":"1640114280.0","poster":"jove","upvote_count":"3","content":"Selected Answer: A\nIt is A","comment_id":"506385"},{"comment_id":"490603","upvote_count":"4","poster":"GMartinelli","timestamp":"1638269460.0","content":"Selected Answer: A\nOption A"},{"content":"he number of orders and exact schedule for the sale will vary each day. During the sale, approximately 10,000 concurrent users will look at the deals before buying items. Outside of the sale, the traffic volume is very low\n==> Setting provisioning DynamoDB fix read 5000/write 10000 with will waste the resource when the traffic is low. It is not cost-effective.\nI go with A","timestamp":"1636261920.0","comment_id":"434304","upvote_count":"3","poster":"ChauPhan"},{"poster":"Aesthet","timestamp":"1636232580.0","comment_id":"361616","content":"A final answer","upvote_count":"2"},{"comment_id":"357005","timestamp":"1635199740.0","poster":"Zhongkai","upvote_count":"3","content":"c does not mention auto scaling. in addition, 10000 WCU are needed. I will go with A"},{"upvote_count":"1","timestamp":"1635014100.0","poster":"db_interest","comments":[{"content":"Correction. A is the answer","poster":"db_interest","comment_id":"359635","upvote_count":"1","timestamp":"1635798060.0"}],"comment_id":"344852","content":"C seems correct\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"},{"upvote_count":"1","timestamp":"1632367020.0","poster":"novak18","content":"Answer is C","comment_id":"326864"}],"isMC":true,"answer_ET":"A","answers_community":["A (100%)"],"question_text":"An online retail company is planning a multi-day flash sale that must support processing of up to 5,000 orders per second. The number of orders and exact schedule for the sale will vary each day. During the sale, approximately 10,000 concurrent users will look at the deals before buying items. Outside of the sale, the traffic volume is very low. The acceptable performance for read/write queries should be under 25 ms. Order items are about 2 KB in size and have a unique identifier. The company requires the most cost-effective solution that will automatically scale and is highly available.\nWhich solution meets these requirements?","answer_images":[],"unix_timestamp":1617384720,"timestamp":"2021-04-02 19:32:00","choices":{"A":"Amazon DynamoDB with on-demand capacity mode","D":"Amazon Aurora with one writer node and two cross-Region Aurora Replicas","C":"Amazon DynamoDB with provisioned capacity mode with 5,000 write capacity units (WCUs) and 10,000 read capacity units (RCUs)","B":"Amazon Aurora with one writer node and an Aurora Replica with the parallel query feature enabled"}},{"id":"rI7Jsfg2EbMtWydQkF40","answer_ET":"B","unix_timestamp":1617384540,"answer_images":[],"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/48838-exam-aws-certified-database-specialty-topic-1-question-145/","answer":"B","answer_description":"","question_text":"A ride-hailing application uses an Amazon RDS for MySQL DB instance as persistent storage for bookings. This application is very popular and the company expects a tenfold increase in the user base in next few months. The application experiences more traffic during the morning and evening hours.\nThis application has two parts:\nâœ‘ An in-house booking component that accepts online bookings that directly correspond to simultaneous requests from users.\nâœ‘ A third-party customer relationship management (CRM) component used by customer care representatives. The CRM uses queries to access booking data.\nA database specialist needs to design a cost-effective database solution to handle this workload.\nWhich solution meets these requirements?","isMC":true,"choices":{"B":"Use Amazon DynamoDB to accept the bookings. Enable DynamoDB Streams and associate an AWS Lambda function to capture changes and push the booking data to an Amazon SQS queue. This triggers another Lambda function that pulls data from Amazon SQS and writes it to the RDS for MySQL DB instance used by the CRM.","D":"Use Amazon DynamoDB to accept the bookings. Enable DynamoDB Streams and associate an AWS Lambda function to capture changes and push the booking data to Amazon Athena, which is used by the CRM.","A":"Use Amazon ElastiCache for Redis to accept the bookings. Associate an AWS Lambda function to capture changes and push the booking data to the RDS for MySQL DB instance used by the CRM.","C":"Use Amazon ElastiCache for Redis to accept the bookings. Associate an AWS Lambda function to capture changes and push the booking data to an Amazon Redshift database used by the CRM."},"topic":"1","timestamp":"2021-04-02 19:29:00","question_id":52,"exam_id":22,"discussion":[{"content":"In A \"AWS Lambda function to capture changes\" capture changes to what? ElastiCache? The main use of ElastiCache is to cache frequently read data. Also \"the company expects a tenfold increase in the user base\" and \"correspond to simultaneous requests from users\" suggest DynamoDB imo. Since we already have MySQL RDS I prefer answer B","comments":[{"comment_id":"391671","poster":"learnazureportal","comments":[{"upvote_count":"4","comments":[{"comments":[{"upvote_count":"1","content":"Athena is more cost-effective than RDS.\nI prefer D.","timestamp":"1696089360.0","poster":"JasonZhu","comment_id":"1021646"},{"upvote_count":"1","content":"No one is asking for a Athena DynamoDB connector. In D) it says \"write to Athena\". You don't write to athena, athena is not a DB/storage solution. Athena reads from S3, and in the case you refer, it can have some connectors.\n\nAsnwer is B","timestamp":"1702215720.0","comment_id":"1092518","poster":"silvaa360"}],"timestamp":"1685140980.0","poster":"cnmc","content":"Athena can read from DynamoDB... This kind of answers show that you guys are only studying for the cert without any actual experience. Setting up a whole new RDS clusters just so some CRM can read from it will cost your company thousands of dollars per month.","comment_id":"907613","upvote_count":"1"}],"content":"Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL.\nIt is DB query service serverless for data stored on S3. It is itself not a DB storage","poster":"ChauPhan","comment_id":"434321","timestamp":"1635055680.0"}],"upvote_count":"4","content":"Actually, It is D. based on the increasing the overall, we cannot go for B.","timestamp":"1634333460.0"}],"upvote_count":"11","poster":"Aesthet","comment_id":"361615","timestamp":"1634067180.0"},{"timestamp":"1633147020.0","comment_id":"344994","poster":"manan728","upvote_count":"7","comments":[{"content":"you are trying to mention score to give gradability to your answer, you only get pass or fail but not the score","timestamp":"1645974900.0","poster":"user0001","comment_id":"557399","upvote_count":"1"},{"poster":"guru_ji","upvote_count":"2","timestamp":"1635623160.0","content":"I gave exam today. Only 60% questions came in actual exam from this 145 set.","comments":[{"timestamp":"1636769640.0","poster":"palanikumar_n","comment_id":"477245","content":"Do you have the 145 questions.I could see only 112 question in the site","upvote_count":"1"}],"comment_id":"444717"}],"content":"This question wasn't asked but in summary a lot of questions exactly from this 145 set questions were asked. I passed with a score of 794. Bunch of new questions on Aurora upgrades and maintenance windows, MS SQL server and Oracle migrations. All the best ðŸ‘"},{"comment_id":"917871","poster":"aviathor","upvote_count":"1","content":"Selected Answer: B\nAlthough I do not understant why one should use one Lambda function to read from the DunamoDB stream, write to an SQS queue and then another Lambda to read from SQS and write to RDS... B is what makes most sense. \n\nBut I would have considered using only one Lambda function and no SQS.","timestamp":"1686204120.0"},{"timestamp":"1682292480.0","content":"I go with Answer D. \nFor booking we need a persistent DB which is a dynamoDB but querying the data through Athena is more cost effective way.","upvote_count":"1","comment_id":"878862","poster":"manig"},{"poster":"lollyj","upvote_count":"1","comment_id":"754540","content":"Selected Answer: B\nChose this option because this architecture definitely needs a SQS component being that it's a ride hailing application. Only makes sense","timestamp":"1671829020.0"},{"upvote_count":"1","timestamp":"1651257660.0","content":"Selected Answer: B\nno use of Elasticache + Redis ( A, C)\nAthena is not DB (D)","poster":"novice_expert","comment_id":"594646"},{"timestamp":"1647174960.0","content":"Selected Answer: B\n- Redis is an in-memory data store, therefore not suitable for bookings persistence. A and C are incorrect.\n- Athena is an interactive query service, not a database. D is incorrect.","poster":"Dantas","comment_id":"566820","upvote_count":"4"},{"poster":"user0001","comment_id":"557398","timestamp":"1645974780.0","content":"B is the answer , we need the data in mysql","upvote_count":"1"},{"content":"Selected Answer: B\nCRM needs data in MySQL","upvote_count":"2","poster":"tugboat","comment_id":"555576","timestamp":"1645737900.0"},{"upvote_count":"2","content":"i go with B\nyou need to save data to MySQL \nA is not persistence","timestamp":"1645030260.0","comment_id":"548780","poster":"user0001"},{"upvote_count":"1","comment_id":"493692","content":"ElastiCache, either MemcaheD or Redis, is used for data frequently read. And Amazon Athena is for querying data stored in Amazon S3. Considering all the features provided by the services, Option B is the best option.","poster":"scottkerker","timestamp":"1638620520.0"},{"content":"Answer is B","comment_id":"486349","poster":"jove","upvote_count":"1","timestamp":"1637800800.0"},{"timestamp":"1637751660.0","upvote_count":"4","poster":"GMartinelli","comment_id":"485872","content":"Selected Answer: B\nItÂ´s B"},{"comment_id":"466125","timestamp":"1635954000.0","content":"should be A","poster":"Amy2009","upvote_count":"3"},{"upvote_count":"1","content":"@guru_ji ? only 60% ? , hm... you can send me email","comment_id":"455754","poster":"VNKSEC","timestamp":"1635849900.0"},{"timestamp":"1635582300.0","comment_id":"444716","upvote_count":"1","content":"I gave exam today. Only 60% questions came in actual exam.","poster":"guru_ji"},{"poster":"guru_ji","timestamp":"1635431520.0","comment_id":"440203","content":"Answer: B","upvote_count":"1"},{"poster":"ChauPhan","comments":[{"timestamp":"1635536100.0","comment_id":"441107","content":"ok. Agree.","upvote_count":"1","poster":"guru_ji"},{"timestamp":"1637074380.0","poster":"andy909","content":"Agree.","upvote_count":"1","comment_id":"479446"}],"content":"Redis is usually used to cache to reduce the DB load, not store the data.\nBetween B and D, I choose B. \nPlease note that Athena is DB query service serverless, it is not DB store.\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL","upvote_count":"2","timestamp":"1634962320.0","comment_id":"434319"},{"content":"Athena is not for data store.","comment_id":"430889","upvote_count":"1","poster":"pcpcpc888","timestamp":"1634934960.0"},{"upvote_count":"2","poster":"Hits_23","timestamp":"1634601240.0","content":"Answer is B. There is no mention of Athena/Redshift used by CRM. Company has data store in RDS MySQL, so that would be used by CRM team.","comment_id":"409243"},{"content":"After checking the Lambda, agree with Aesthet. B is better than A","timestamp":"1634272500.0","poster":"Huy","upvote_count":"4","comment_id":"362324"},{"timestamp":"1633329360.0","poster":"Huy","comment_id":"361193","content":"Going with A. Already has RDS MySQL. We just need a cache layer. I prefer RedisCache although I'm not sure it is cheaper than DynamoDB.","upvote_count":"1"},{"timestamp":"1632925560.0","upvote_count":"2","comment_id":"342667","comments":[{"content":"Going with A because more you think you really don't need two persistent data stores especially when cost is a concern. And it does not make sense either to have 2 persistent stores.","poster":"manan728","upvote_count":"2","comment_id":"342972","timestamp":"1632932820.0"}],"poster":"manan728","content":"B seems more legit but there's also a cost factor to consider too."},{"comment_id":"342662","content":"In order to access booking data the CRM app must use the RDS database as it contains the booking data. So the Athena option is out of scope.","poster":"manan728","timestamp":"1632510360.0","upvote_count":"1"},{"timestamp":"1632483180.0","upvote_count":"3","content":"Doesn't the solution have to include RDS MySQL since that is the persistent storage for the data? Athena is serverless technology that allows you to read data from S3 using SQL statements. I am going with B.","poster":"[Removed]","comment_id":"341193"},{"comments":[{"comments":[{"content":"where in the answers provided have you seen the option to read from athena? Your comment is true but not relevant","poster":"Aesthet","comment_id":"361610","upvote_count":"1","comments":[{"timestamp":"1633548300.0","poster":"Aesthet","upvote_count":"1","content":"to read with Athena from dynamodb*","comment_id":"361611"}],"timestamp":"1633471560.0"}],"comment_id":"344868","poster":"db_interest","timestamp":"1633076340.0","content":"Athena can read from DynamoDB.\nhttps://docs.aws.amazon.com/athena/latest/ug/athena-prebuilt-data-connectors-dynamodb.html","upvote_count":"1"}],"poster":"giter","upvote_count":"1","timestamp":"1632289800.0","content":"Between B &D. I choose B. \nwe cannot push data to Amazon Athena","comment_id":"327998"},{"poster":"Jaypdv","timestamp":"1632099960.0","upvote_count":"2","content":"D. makes the most sense to me","comment_id":"327824"},{"content":"I guess the answer should be D","upvote_count":"1","timestamp":"1632086280.0","poster":"novak18","comment_id":"326860"}],"question_images":[]},{"id":"cfv8zyYCGGfGT2LZJKRv","timestamp":"2021-10-25 13:59:00","answer_description":"","answer_ET":"B","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/64693-exam-aws-certified-database-specialty-topic-1-question-146/","exam_id":22,"answers_community":["B (100%)"],"question_id":53,"discussion":[{"timestamp":"1632419520.0","upvote_count":"6","poster":"grekh001","content":"B is correct.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\n\n\"If the request specifies strongly consistent reads, DAX passes the request through to DynamoDB. The results from DynamoDB are not cached in DAX. Instead, they are simply returned to the application.\"","comment_id":"467439"},{"poster":"Pranava_GCP","upvote_count":"1","comment_id":"1005330","content":"Selected Answer: B\nB. Strongly consistent reads are always passed through DAX to DynamoDB.","timestamp":"1694488920.0"},{"poster":"lollyj","content":"Selected Answer: B\nMy thoughts - if strongly consistency reads are requested it will pass through DAX to the DB.","comment_id":"754585","upvote_count":"1","timestamp":"1671835620.0"},{"content":"Selected Answer: B\nstrong consistency => bypass DAX","comment_id":"594157","timestamp":"1651195620.0","poster":"novice_expert","upvote_count":"3"},{"timestamp":"1646528280.0","comment_id":"561719","content":"Selected Answer: B\nStrongly Consistent Reads are considered PASS THROUGH and will never update DAX\nInterestingly - TransactionGetItems is also a PASS THROUGH but TransactionWriteItems is not!!","upvote_count":"2","poster":"RotterDam"},{"upvote_count":"3","comment_id":"556193","timestamp":"1645815720.0","content":"B:\nIf the request specifies eventually consistent reads (the default behavior), it tries to read the item from DAX:\n\nIf DAX has the item available (a cache hit), DAX returns the item to the application without accessing DynamoDB.\nIf DAX does not have the item available (a cache miss), DAX passes the request through to DynamoDB. When it receives the response from DynamoDB, DAX returns the results to the application. But it also writes the results to the cache on the primary node.\n\n\nIf the request specifies strongly consistent reads, DAX passes the request through to DynamoDB. The results from DynamoDB are not cached in DAX. Instead, they are simply returned to the application.","poster":"kped21"}],"question_images":[],"topic":"1","isMC":true,"unix_timestamp":1635163140,"question_text":"An online advertising website uses an Amazon DynamoDB table with on-demand capacity mode as its data store. The website also has a DynamoDB Accelerator\n(DAX) cluster in the same VPC as its web application server. The application needs to perform infrequent writes and many strongly consistent reads from the data store by querying the DAX cluster.\nDuring a performance audit, a systems administrator notices that the application can look up items by using the DAX cluster. However, the QueryCacheHits metric for the DAX cluster consistently shows 0 while the QueryCacheMisses metric continuously keeps growing in Amazon CloudWatch.\nWhat is the MOST likely reason for this occurrence?","answer_images":[],"choices":{"A":"A VPC endpoint was not added to access DynamoDB.","C":"DynamoDB is scaling due to a burst in traffic, resulting in degraded performance.","B":"Strongly consistent reads are always passed through DAX to DynamoDB.","D":"A VPC endpoint was not added to access CloudWatch."}},{"id":"X3pRUIMJO341BO1muJN5","unix_timestamp":1635163020,"answer_ET":"B","question_text":"A financial company recently launched a portfolio management solution. The backend of the application is powered by Amazon Aurora with MySQL compatibility.\nThe company requires an RTO of 5 minutes and an RPO of 5 minutes. A database specialist must configure an efficient disaster recovery solution with minimal replication lag.\nWhich approach should the database specialist take to meet these requirements?","discussion":[{"comment_id":"935255","poster":"[Removed]","upvote_count":"1","content":"Option D could be also correct, but according to documentation:\n\"The promotion process takes a few minutes to complete. When you promote a read replica, replication is stopped and the DB instances are rebooted. When the reboot is complete, the read replica is available as a new DB cluster.\"\nTherefore it does not suit the condition, that RTO schould be less than 5 minutes","timestamp":"1687862940.0"},{"timestamp":"1682293080.0","content":"B\nAurora Global cluster RPO typically < 1Mins and RTO depends on how soon we detach the other region cluster from Global cluster and promote to primary. With simple Lambda/jenkins job this can be easily achievable within < 5 Mins.","comment_id":"878865","upvote_count":"2","poster":"manig"},{"poster":"OCHT","upvote_count":"1","comment_id":"791440","content":"Selected Answer: B\nVerfied.","timestamp":"1674973860.0"},{"upvote_count":"1","poster":"sachin","comment_id":"620203","timestamp":"1655877780.0","content":"B Aurora Global Tables. ( As an alternative to cross-Region read replicas, you can scale read operations with minimal lag time by using an Aurora global database.)"},{"timestamp":"1651256520.0","comment_id":"594640","upvote_count":"1","content":"Selected Answer: B\n5 min RTO 5 min RPO\n Aurora GLOBAL RPO 1second and RTO 1 minute","poster":"novice_expert"},{"upvote_count":"4","comment_id":"566800","poster":"Dantas","timestamp":"1647173580.0","content":"Selected Answer: B\nAmazon Aurora global databases span multiple AWS Regions, enabling low latency global reads and providing fast recovery from the rare outage that might affect an entire AWS Region. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\nhttps://www.amazonaws.cn/en/rds/aurora/faqs/"},{"upvote_count":"4","comment_id":"561175","timestamp":"1646454480.0","poster":"RotterDam","content":"Selected Answer: B\nquestion is vague. what does \"The firm demands a response time of five minutes and a response time of five minutes. \" mean? I assume they mean RTO and RPO. If 5 minutes is the same for both then Aurora GLOBAL will suffice (RPO 1second and RTO 1 minute)"},{"upvote_count":"1","poster":"tugboat","content":"Selected Answer: C\nEfficient is key - takes our global DBs\nRead-replicas are efficient and low-latency","timestamp":"1645737180.0","comment_id":"555566","comments":[{"upvote_count":"2","poster":"BreeRawkus","timestamp":"1649761680.0","content":"Global DB uses storage replication = low replication latency, Answer is B","comment_id":"584666"}]},{"timestamp":"1645028640.0","comment_id":"548756","poster":"user0001","upvote_count":"1","content":"Answer B"},{"poster":"mnzsql365","upvote_count":"4","content":"B for me\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.html","timestamp":"1639292700.0","comment_id":"499835"},{"comment_id":"497842","timestamp":"1639064040.0","poster":"nood","content":"B for me\nhttps://aws.amazon.com/blogs/database/how-to-choose-the-best-disaster-recovery-option-for-your-amazon-aurora-mysql-cluster/","upvote_count":"2"},{"comment_id":"477840","upvote_count":"1","content":"If cost is not a concern, global database is a better option than read replica.","timestamp":"1636843680.0","poster":"jove"},{"comments":[{"poster":"johnconnor","upvote_count":"3","timestamp":"1636659900.0","content":"I think you are right, the other options would work but B seems to be the faster","comment_id":"476449"}],"comment_id":"467438","timestamp":"1635882300.0","content":"I think the answer is B\n\nhttps://aws.amazon.com/about-aws/whats-new/2019/11/aurora-supports-in-place-conversion-to-global-database/\n\nhttps://aws.amazon.com/blogs/database/how-to-choose-the-best-disaster-recovery-option-for-your-amazon-aurora-mysql-cluster/\n\n\"Aurora Global Database provides the lowest consistent RTO and RPO option while requiring the least management overhead.\"","poster":"grekh001","upvote_count":"4"}],"choices":{"B":"Configure an Amazon Aurora global database and add a different AWS Region.","C":"Configure a binlog and create a replica in a different AWS Region.","D":"Configure a cross-Region read replica.","A":"Configure AWS Database Migration Service (AWS DMS) and create a replica in a different AWS Region."},"answers_community":["B (91%)","9%"],"answer_images":[],"answer":"B","answer_description":"","timestamp":"2021-10-25 13:57:00","exam_id":22,"question_images":[],"topic":"1","isMC":true,"question_id":54,"url":"https://www.examtopics.com/discussions/amazon/view/64692-exam-aws-certified-database-specialty-topic-1-question-147/"},{"id":"ADOXBccA4tUDDdbKgL5M","question_id":55,"timestamp":"2021-11-12 00:04:00","exam_id":22,"question_text":"A company hosts an internal file-sharing application running on Amazon EC2 instances in VPC_A. This application is backed by an Amazon ElastiCache cluster, which is in VPC_B and peered with VPC_A. The company migrates its application instances from VPC_A to VPC_B. Logs indicate that the file-sharing application no longer can connect to the ElastiCache cluster.\nWhat should a database specialist do to resolve this issue?","topic":"1","answers_community":["D (100%)"],"answer_images":[],"discussion":[{"content":"Why not B? https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-privatelink.html","comment_id":"946028","timestamp":"1688768400.0","upvote_count":"1","poster":"SamDDD"},{"poster":"novice_expert","timestamp":"1651264320.0","content":"Selected Answer: D\nYou can update security groups to reference peer VPC group\nhttps://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html","comment_id":"594686","upvote_count":"3"},{"content":"Selected Answer: D\nHas to be D","upvote_count":"3","timestamp":"1646459580.0","poster":"RotterDam","comment_id":"561250"},{"upvote_count":"2","comment_id":"555601","timestamp":"1645740720.0","content":"Selected Answer: D\npeering is best option","poster":"tugboat"},{"poster":"Hariru","comment_id":"535697","upvote_count":"1","content":"Why not B?","timestamp":"1643486880.0"},{"upvote_count":"1","timestamp":"1639014480.0","poster":"2025flakyt","comment_id":"497274","content":"D is correct"},{"content":"Answer is D","comment_id":"488469","upvote_count":"1","poster":"jove","timestamp":"1638042360.0"},{"poster":"leunamE","comment_id":"476551","comments":[{"poster":"GMartinelli","content":"I donÂ´t know if its possible.. Since SGs are attached to the VPC, can you use it to reference on another VPC?","comment_id":"487371","comments":[{"comment_id":"497273","timestamp":"1639014300.0","content":"You can update security groups to reference peer VPC group\nhttps://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html","poster":"2025flakyt","upvote_count":"1"}],"upvote_count":"1","timestamp":"1637931900.0"},{"poster":"johnconnor","comment_id":"477881","content":"I think you are right","upvote_count":"2","timestamp":"1636853280.0"}],"content":"Option D.","upvote_count":"4","timestamp":"1636671840.0"}],"unix_timestamp":1636671840,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/65860-exam-aws-certified-database-specialty-topic-1-question-148/","isMC":true,"answer":"D","answer_ET":"D","question_images":[],"choices":{"D":"Modify the ElastiCache security group by adding an inbound rule that allows traffic from the EC2 instances' security group to the ElastiCache cluster.","A":"Create a second security group on the EC2 instances. Add an outbound rule to allow traffic from the ElastiCache cluster security group.","B":"Delete the ElastiCache security group. Add an interface VPC endpoint to enable the EC2 instances to connect to the ElastiCache cluster.","C":"Modify the ElastiCache security group by adding outbound rules that allow traffic to VPC_B's CIDR blocks from the ElastiCache cluster."}}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","name":"AWS Certified Database - Specialty","provider":"Amazon","id":22,"isMCOnly":false,"numberOfQuestions":359,"isImplemented":true},"currentPage":11},"__N_SSP":true}