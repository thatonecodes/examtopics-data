{"pageProps":{"questions":[{"id":"jJhyjoiqW8osOGk2weTe","answer_images":[],"timestamp":"2023-01-16 14:40:00","choices":{"D":"Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed.","A":"Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.","C":"Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.","B":"Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics."},"isMC":true,"question_images":[],"discussion":[{"upvote_count":"17","comment_id":"832551","comments":[{"content":"A is wrong. \nOpsCenter, a capability of AWS Systems Manager, provides a central location where operations engineers and IT professionals can manage operational work items (OpsItems) related to AWS resources. An OpsItem is any operational issue or interruption that needs investigation and remediation. Using OpsCenter, you can view contextual investigation data about each OpsItem, including related OpsItems and related resources. You can also run Systems Manager Automation runbooks to resolve OpsItems.","comment_id":"832565","upvote_count":"4","poster":"God_Is_Love","timestamp":"1678253580.0"},{"content":"fyi Pricing looks cheap too - https://aws.amazon.com/compute-optimizer/pricing/","timestamp":"1678252920.0","upvote_count":"2","comment_id":"832556","poster":"God_Is_Love"}],"timestamp":"1678252500.0","content":"Selected Answer: C\nAWS Compute Optimize helps analyze the usage patterns of AWS resources, such as EC2 instances and Auto Scaling groups, and makes recommendations on how to optimize them for performance and cost using machine learning algorithms.It then generates recommendations that can be used to adjust instance types, purchase options, and other parameters.It provides two types of recommendations:\n Recommended instance types - recommends instance types that are more cost-effective and better suited to the workload requirements.\n Recommended purchase options - recommends purchasing options, such as Reserved Instances or Savings Plans, that can help customers save money on their compute resources.","poster":"God_Is_Love"},{"poster":"amministrazione","timestamp":"1725266220.0","comment_id":"1276548","content":"C. Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.","upvote_count":"1"},{"comment_id":"1147759","timestamp":"1707699780.0","upvote_count":"2","poster":"saggy4","content":"Selected Answer: C\nA - Not possible\nD - Costliest Option possible\nnow between B and C\nThe question mentions high-memory EC2 instances.\nYou cannot get memory metrics without the Cloudwatch agent installed hence C."},{"upvote_count":"1","content":"Selected Answer: C\nOption C is most cost effective choice.","poster":"career360guru","comment_id":"1104303","timestamp":"1703365080.0"},{"comments":[{"upvote_count":"1","timestamp":"1704118320.0","content":"You are correct that in the FAQ you've linked it says 24 hours, but in other places of the AWS documentation it says 12 hours, like here: https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-getting-recommendations.html#viewing-recommendations\nor here: https://docs.aws.amazon.com/awssupport/latest/user/compute-optimizer-with-trusted-advisor.html\nSeems like even AWS doesn't know :D So I would still go with C.","comment_id":"1111244","poster":"carpa_jo"}],"comment_id":"1103789","poster":"wmp7039","content":"C is incorrect : When you first opt in Compute Optimizer, it may take up to 24 hours to fully analyze the AWS resources in your account. https://aws.amazon.com/compute-optimizer/faqs/","timestamp":"1703289480.0","upvote_count":"1"},{"timestamp":"1702863540.0","comment_id":"1099378","upvote_count":"1","poster":"atirado","content":"Selected Answer: C\nOption A is not in the running because it will require incurring further expense to address the cost issue.\n\nOption D is expensive - the Enterprise Support plan charges a minimum flat fee minimum or a % of your AWS bill. This could be a large amount for the company's hundreds of instances.\n\nOption B is expensive - Detailed monitoring scales based on the number of metrics and the number of resources. The company has hundreds of instances so this option could potentially be more expensive than D. \n\nOption C - Compute Optimizer will provide improvement suggestions based on 14 prior days usage data from the moment it was enabled. Moreover, the default service option is free. Nothing is said about the custom metrics being used for the CloudWatch agent but it could be the most expensive of all options if mis-used. So either cost 0 or incredibly large if used carelessly."},{"content":"Selected Answer: C\nC. need CW agent for RAm util","timestamp":"1688398800.0","poster":"NikkyDicky","comment_id":"942004","upvote_count":"1"},{"poster":"Fredonly","upvote_count":"1","timestamp":"1682110500.0","content":"Selected Answer: C\nC- Compute Optimizer is the easiest solution","comment_id":"876809"},{"upvote_count":"1","poster":"mfsec","timestamp":"1679895720.0","comments":[{"comment_id":"851762","upvote_count":"1","poster":"mfsec","timestamp":"1679895780.0","content":"*Compute"}],"content":"Selected Answer: C\nC - cost optimizer","comment_id":"851761"},{"poster":"spd","timestamp":"1677937560.0","content":"Selected Answer: C\nC is correct - Optimzer","upvote_count":"2","comment_id":"829002"},{"timestamp":"1677409380.0","comment_id":"822280","content":"Selected Answer: A\nOption C may be a good solution to rightsize the EC2 instances but may incur additional cost for installing the Amazon CloudWatch agent on each of the EC2 instances.\n\nThe MOST cost-effective solution to analyze the company’s Amazon EC2 instances and Amazon EBS volumes is to create a dashboard using AWS Systems Manager OpsCenter. The OpsCenter dashboard can be configured to visualize the Amazon CloudWatch metrics associated with the EC2 instances and their EBS volumes. By reviewing the dashboard periodically, usage patterns can be identified, and EC2 instances can be right-sized based on the peaks in the metrics.","upvote_count":"1","comments":[{"poster":"God_Is_Love","content":"Bro, install cost is 0. Simple linux command > sudo yum install amazon-cloudwatch-agent","timestamp":"1678253340.0","upvote_count":"2","comment_id":"832561"}],"poster":"kiran15789"},{"upvote_count":"3","comment_id":"778000","timestamp":"1673890020.0","poster":"masetromain","content":"Selected Answer: C\nThe correct answer is C. Installing the Amazon CloudWatch agent on each of the EC2 instances and turning on AWS Compute Optimizer allows the solutions architect to analyze the environment and make recommendations on the sizing of the EC2 instances in a cost-effective way. AWS Compute Optimizer analyzes the utilization of the instances and recommends the optimal instance types for the workloads. This solution is more cost-effective than creating a dashboard and reviewing it periodically, or signing up for the AWS Enterprise Support plan and waiting for Trusted Advisor recommendations."},{"upvote_count":"1","comment_id":"777715","poster":"zhangyu20000","content":"C is correct, with computer optimizer","timestamp":"1673876400.0"}],"answer":"C","exam_id":33,"answer_ET":"C","question_id":36,"question_text":"A solutions architect must analyze a company’s Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern.\n\nThe solutions architect must analyze the environment and take action based on the findings.\n\nWhich solution meets these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/95543-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","unix_timestamp":1673876400,"topic":"1","answers_community":["C (97%)","3%"]},{"id":"lcpmf5LSYt9Xna0sqP8C","answer_images":[],"answers_community":["B (83%)","Other"],"discussion":[{"timestamp":"1692802020.0","poster":"bititan","comment_id":"819414","upvote_count":"15","content":"Selected Answer: B\nFollow below link. It has both option to be used for this scenarios. But default kms key can not be used so B\nhttps://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/"},{"comment_id":"876422","upvote_count":"6","comments":[{"poster":"ninomfr64","upvote_count":"1","timestamp":"1721651160.0","content":"Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. This sounds like a trust policy to me","comment_id":"1128747"}],"timestamp":"1697885640.0","poster":"Sarutobi","content":"Selected Answer: B\nAlthough I think B is the best, it is missing to mention of the trust policy in the application account."},{"content":"Selected Answer: B\nA = Secret is not a RAM sharable resource. But who can recall this full list? Thus my reasoning is, I would expect more details for sharing via RAM like enable AWS Org sharing, assign permission (actions allowed on the shared resource) and select the external principal.\nB = correct see https://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/\nC = cannot cross-account access AWS managed KMS key as you do not have control on key policy\nD = SCP can only remove permissions. Even tough an SCP doesn't prevent you from accessing a secret, you still need to have IAM user permission and/or resource based policy in place to actually access","upvote_count":"4","timestamp":"1721652420.0","comment_id":"1128773","poster":"ninomfr64"},{"comment_id":"1105131","content":"option b","timestamp":"1719300420.0","upvote_count":"1","poster":"horyoryo"},{"timestamp":"1719177900.0","comment_id":"1104357","upvote_count":"1","poster":"career360guru","content":"Selected Answer: B\nOption B"},{"timestamp":"1717889940.0","poster":"bjexamprep","comment_id":"1091395","upvote_count":"2","content":"Selected Answer: B\nEven B is the best answer among all the options, actually B is not correct. Without permission to access the KMS key, B cannot decrypt the secret.","comments":[{"content":"I was wrong. It is using AWS managed default encryption key, so it doesn't need the permission to access KMS key. The flaw of B is trust relationship policy.","timestamp":"1726101480.0","upvote_count":"1","poster":"bjexamprep","comment_id":"1171403"}]},{"content":"Selected Answer: B\nthe Secrets Manager keys cannot be shared with RAM, key policy(resource policy) for the default KMS key managed by AWS cannot be changed, role is identity and can be granted access to assume other role","timestamp":"1715832660.0","comment_id":"1072188","upvote_count":"1","poster":"severlight"},{"upvote_count":"3","poster":"rlf","content":"Answer is B.\nOption A is wrong. AWS RAM can not share AWS Secrets Manager ( see shareable resources in https://docs.aws.amazon.com/ram/latest/userguide/shareable.html )","comment_id":"1049627","timestamp":"1713714900.0"},{"comments":[{"content":"who said we can share secrets using RAM??\ni just checked under RAM and allowed sharable AWS services\nAWS Secrets Manager is NOT one of those\nAnswer is B","comment_id":"1004662","upvote_count":"4","timestamp":"1710162480.0","poster":"chikorita"}],"upvote_count":"1","poster":"uC6rW1aB","comment_id":"1002998","timestamp":"1709975040.0","content":"Selected Answer: A\nBoth Option A and Option B give repository administrators access to the repository and eliminate the need to manually share secrets.\nOption A is a relatively simple process of sharing secrets with AWS RAM and setting up an IAM role within the DBA account.\nOption B requires creating an IAM role in two different AWS accounts and setting cross-account permissions, which is a more complicated process.\nSo, while both A and B accomplish the goal, option A is simpler and more straightforward."},{"upvote_count":"1","content":"Selected Answer: B\nAs several people have highlighted, we refer to the blog https://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/\n\nWant to provide the following comment to emphasize why \"C\" is NOT even possible.\nIn Option C, its mentioned that the default AWS Managed CMK is used by the secrets manager. \nWe cannot provide any custom permissions to the AWS Managed CMK and by extension, its not possible to allow cross account access to it. So, only Option B is valid.","comment_id":"996771","poster":"venvig","timestamp":"1709379240.0"},{"comment_id":"942008","poster":"NikkyDicky","upvote_count":"1","content":"Selected Answer: B\nits a b","timestamp":"1704303840.0"},{"upvote_count":"1","timestamp":"1702521420.0","poster":"Jackhemo","content":"Guys, you want to know the right answer? Copy paste the whole question to olabiba.ai\nThe answer is B","comment_id":"922635"},{"upvote_count":"2","poster":"OCHT","comments":[{"poster":"Maria2023","timestamp":"1703241480.0","upvote_count":"4","comment_id":"930299","content":"I couldn't find any option to share Secret Manager resources via RAM, did anyone try it?"}],"comment_id":"884401","content":"Selected Answer: A\nOption A is the correct answer because it meets the requirement of giving the database administrators access to the database and eliminates the need to manually share the secrets. AWS Resource Access Manager (AWS RAM) enables you to share AWS resources with other accounts within your organization or organizational units (OUs) in AWS Organizations. By using AWS RAM to share the secrets from the application account with the DBA account, you can eliminate the need for manual sharing of secrets.\n\nOption B involves creating an IAM role in the application account and another IAM role in the DBA account. The DBA-Admin role in the DBA account would need to assume the DBA-Secret role in the application account to access the secrets. This approach adds complexity and does not eliminate the need for manual sharing of secrets.\n\nIn summary, Option A is a simpler and more efficient solution that meets the requirements.","timestamp":"1698597600.0"},{"content":"Selected Answer: B\nB is correct, D doesn't make sense! SCP doesn't give any permission.. it just defines what can be allowed. you still need an IAM role/policy","upvote_count":"2","poster":"dev112233xx","comment_id":"865553","timestamp":"1696859760.0"},{"poster":"mfsec","upvote_count":"2","timestamp":"1695793500.0","content":"Selected Answer: B\nB is the best choice","comment_id":"851765"},{"content":"Selected Answer: B\nHas to be B because C is not possible.\nI get that you can't share access to the default KMS key, but how does it work to share access through a cross account role? How does the role in the DBA account decrypt the secrets that are encrypted by the default key if the role doesn't have permissions to that key?","timestamp":"1695302340.0","comment_id":"846112","upvote_count":"4","poster":"DWsk"},{"comment_id":"834301","content":"Selected Answer: B\ncross account assume role","upvote_count":"2","poster":"kiran15789","timestamp":"1694279460.0"},{"poster":"sambb","upvote_count":"4","timestamp":"1694152500.0","comment_id":"832654","content":"Selected Answer: B\nCross account assumerole is needed. You can't directly grant access to the secret from the DBA account to the application account because the key policy for the default KMS key is not modifiable."},{"content":"Selected Answer: B\nhttps://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2020/09/17/PatternSecretsManager2.png\nApp account has the RDS and Secrets manager. So, first, app team should allow to share the secret with DBA account thru \"DBA-Secret\" IAM role. and DBA (thru DBA-Admin role) should assume that role to access secret. This is common design pattern. So option which has DBA-Secret IAM role is the answer which is B","comments":[{"content":"* I meant option which says DBA-Secret role in app account (owner account) is the answer","upvote_count":"1","comment_id":"832590","timestamp":"1694146800.0","poster":"God_Is_Love"}],"upvote_count":"3","poster":"God_Is_Love","comment_id":"832589","timestamp":"1694146680.0"},{"timestamp":"1693041600.0","upvote_count":"3","content":"Selected Answer: B\nOption B is the correct answer because it creates an IAM role named DBA-Secret in the application account and grants the required permissions to access the secrets. In the DBA account, it creates an IAM role named DBA-Admin, grants the required permissions to assume the DBA-Secret role in the application account, and attaches the DBA-Admin role to the EC2 instance for access to the cross-account secrets. This eliminates the need to manually share the secrets and provides access to the database administrators to the database.","poster":"kiran15789","comment_id":"822299"},{"upvote_count":"2","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/ the diagram here is pretty much exactly the scenario described in this question. B for the win","timestamp":"1692564960.0","poster":"[Removed]","comment_id":"815988"},{"timestamp":"1692112560.0","content":"Selected Answer: B\nOption B is the correct solution to meet the requirements.\n\nIn this solution, an IAM role named DBA-Secret is created in the application account, and the required permissions to access the secrets are granted to this role. In the DBA account, an IAM role named DBA-Admin is created, and the required permissions to assume the DBA-Secret role in the application account are granted to this role. The DBA-Admin role is then attached to the EC2 instance to access the cross-account secrets.\n\nThis solution follows the principle of least privilege, where the IAM roles have only the necessary permissions to access the secrets. Also, it eliminates the need for manual sharing of secrets and provides a secure way to access the secrets by leveraging cross-account IAM roles.","upvote_count":"4","poster":"c73bf38","comment_id":"809784"},{"comments":[{"upvote_count":"5","content":"Why not? It's called role chaining and been available since cross-accounts IAM permissions. Used it numerous times. The userY>Acct1:RoleA>Acct2:RoleB. Acct2:RoleB permissions is only valid for 1hr on CLI/API.","timestamp":"1692203640.0","poster":"lunt","comments":[{"content":"Thanks, it should be B then","poster":"spd","upvote_count":"2","timestamp":"1692465960.0","comment_id":"814427"}],"comment_id":"811003"}],"comment_id":"807904","timestamp":"1691961900.0","upvote_count":"1","content":"Selected Answer: C\nIt can not be B - How one role assume to other role ?","poster":"spd"},{"timestamp":"1691378220.0","upvote_count":"4","poster":"Signup_Nickname","comments":[{"timestamp":"1691789880.0","upvote_count":"6","content":"Not C. In the link you shared, there is this big note.\n\nNote: You can't use the AWS KMS default key for the account. The AWS KMS default key is created, managed, and used on your behalf by an AWS service that runs on AWS Key Management Service. The AWS KMS default key is unique to your AWS account and Region. Only the service that created the AWS managed key can use it.","comment_id":"805751","poster":"moota"}],"comment_id":"800562","content":"Selected Answer: C\nYou must use the full AWS KMS key ARN to access a secret from another AWS account.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/secrets-manager-share-between-accounts/"},{"upvote_count":"1","comment_id":"799086","content":"Selected Answer: B\nB makes complete sense. Other options have issues.","timestamp":"1691253240.0","poster":"Musk"},{"content":"Selected Answer: B\nI voted for B, because: \nSCPs are a type of policy that is used to set fine-grained permissions at the root level of an AWS organization. Using SCPs in this scenario could result in overly permissive access, which may not meet the organization's security and compliance requirements.\nAdditionally, using SCPs to manage access to the secrets could make it more difficult to track who has access to the secrets, as SCPs are applied at the organization level and may not be tied to specific IAM roles or users.\nOption B provides a more secure and controlled solution, as it uses cross-account role assumption and IAM roles to manage access to the secrets, which is a more secure and controlled way to manage access to AWS resources in a multi-account environment.","upvote_count":"3","poster":"tatdatpham","timestamp":"1690909800.0","comment_id":"795435"},{"content":"Selected Answer: D\nSCP will provide more granular permissions for DBA","upvote_count":"1","timestamp":"1690668600.0","comments":[{"poster":"Musk","upvote_count":"3","comment_id":"799085","content":"SCP do not provide permissions. It just removes permissions.","timestamp":"1691253180.0"}],"comment_id":"792195","poster":"zozza2023"},{"timestamp":"1690628160.0","upvote_count":"2","comment_id":"791642","content":"A is incorrect.Because aRAM cannot share secrets manager resouce.\nhttps://docs.aws.amazon.com/ja_jp/ram/latest/userguide/shareable.html","poster":"masssa","comments":[{"upvote_count":"2","timestamp":"1690668540.0","comment_id":"792194","content":"RAM can share secret manager ressours bu as masetromain said \"Option A is incorrect because while using AWS Resource Access Manager (AWS RAM) to share the secrets would provide the DBA account access to the secrets, it does not eliminate the need for manual sharing of the secrets as the DBA team would still need to manually access the shared secrets in the DBA account.\"","comments":[{"content":"I don't think it can as per this list: https://docs.aws.amazon.com/ram/latest/userguide/shareable.html","upvote_count":"1","timestamp":"1691253300.0","comment_id":"799089","poster":"Musk"}],"poster":"zozza2023"}]},{"comment_id":"786490","poster":"Nicocacik","upvote_count":"2","timestamp":"1690193580.0","content":"Selected Answer: B\nIt can't be D because SCP doesn't give access. A permission is needed in the application account. The correct answer has to be B"},{"content":"Selected Answer: D\nI swith for D\n option D, using an SCP to allow access to the secrets from the DBA account, is a more appropriate solution for the requirements given in the problem. Using an SCP allows for more granular control over cross-account access, and ensures that the DBA-Admin role in the DBA account is only able to perform the actions that are explicitly allowed by the SCP, rather than being granted all permissions to access the secrets. Additionally, using an SCP is more secure than using IAM roles and policies because SCP uses a deny-all by default approach while IAM policies use an allow-all by default approach.","upvote_count":"4","comment_id":"781375","timestamp":"1689779400.0","poster":"masetromain"},{"timestamp":"1689736380.0","comment_id":"780739","content":"D, for all dbas. B & C to create IAM role are not so perfect.\nIf a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action.","poster":"jhonivy","upvote_count":"2"},{"upvote_count":"1","comments":[{"comment_id":"778009","poster":"masetromain","timestamp":"1689521400.0","content":"Option A is incorrect because while using AWS Resource Access Manager (AWS RAM) to share the secrets would provide the DBA account access to the secrets, it does not eliminate the need for manual sharing of the secrets as the DBA team would still need to manually access the shared secrets in the DBA account.\n\nOption C is incorrect because it does not provide a way for the DBA team to access the secrets stored in the application account and instead focuses on the permissions of the default AWS managed key.\n\nOption D is incorrect because it uses an SCP to allow access to the secrets which is not necessary as the IAM role DBA-Admin with the required permissions to assume the DBA-Secret role in the application account is sufficient to access the secrets.","comments":[{"upvote_count":"2","comment_id":"804672","poster":"Ilk","content":"In option C, it is clearly stated that required permissions for secrets AND keys are granted. Why did you state that there is no way to access to the secrets for DBA team?","timestamp":"1691688180.0"}],"upvote_count":"1"}],"comment_id":"778007","poster":"masetromain","timestamp":"1689521400.0","content":"Selected Answer: B\nThe correct answer is B.\nIn this solution, the application account creates an IAM role, DBA-Secret, with the required permissions to access the secrets stored in the Secrets Manager. In the DBA account, the DBA-Admin role is created and granted the required permissions to assume the DBA-Secret role in the application account. This allows the DBA-Admin role to access the secrets stored in the application account, eliminating the need for manual sharing."},{"upvote_count":"2","comment_id":"777718","poster":"zhangyu20000","content":"B is correct","timestamp":"1689507720.0"}],"timestamp":"2023-01-16 14:42:00","exam_id":33,"question_id":37,"url":"https://www.examtopics.com/discussions/amazon/view/95544-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"B","isMC":true,"unix_timestamp":1673876520,"answer_description":"","choices":{"D":"In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.","A":"Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.","B":"In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets","C":"In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."},"answer_ET":"B","question_images":[],"topic":"1","question_text":"A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts.\n\nIn an AWS application account, the company’s application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed m the application account.\n\nThe application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets.\n\nWhich solution will meet these requirements?"},{"id":"NhB6D6Vb1rbMYNf20a4P","timestamp":"2023-01-16 14:44:00","answer_images":[],"answers_community":["CE (100%)"],"choices":{"B":"Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1.","A":"Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance type.","E":"Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.","C":"Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.","D":"Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU."},"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/95545-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"CE","unix_timestamp":1673876640,"answer_description":"","question_text":"A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.\n\nBecause of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.\n\nA solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing maintenance.\n\nWhich combination of steps will meet these requirements? (Choose two.)","discussion":[{"upvote_count":"5","timestamp":"1696687980.0","content":"Selected Answer: CE\nC. Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU. This will ensure that all resources deployed in the organization reside in the ap-northeast-1 Region.\n\nE. Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU. This will ensure that EC2 instances deployed in the DataOps OU use only the predefined list of instance types.","comment_id":"863968","poster":"OCHT","comments":[{"content":"Option D is incorrect because it suggests using the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. However, the ec2:Region condition key is not a valid condition key for EC2 actions. Instead, the aws:RequestedRegion condition key should be used to restrict access to specific AWS Regions.\n\nAdditionally, applying the SCP to the root OU, the DataOps OU, and the Research OU is unnecessary because applying the SCP to the root OU alone will ensure that the restriction applies to all accounts in the organization, including those in the DataOps and Research OUs.\n\nIn summary, option D is incorrect because it suggests using an invalid condition key and because applying the SCP to multiple OUs is unnecessary.","poster":"OCHT","timestamp":"1696688040.0","comment_id":"863970","upvote_count":"3"}]},{"timestamp":"1719178320.0","poster":"career360guru","comment_id":"1104358","upvote_count":"1","content":"Selected Answer: CE\nOption C & E"},{"content":"Selected Answer: CE\nVery straightforward","timestamp":"1709379840.0","upvote_count":"2","poster":"venvig","comment_id":"996778"},{"poster":"dtha1002","timestamp":"1706611440.0","comment_id":"966971","content":"Selected Answer: CE\nC for all resources region\nand E for DataOps OU launch instantce type","upvote_count":"1"},{"comment_id":"942010","timestamp":"1704303960.0","upvote_count":"1","content":"Selected Answer: CE\nits CE","poster":"NikkyDicky"},{"upvote_count":"1","content":"Selected Answer: CE\nSCP's are the most efficient here","poster":"mfsec","timestamp":"1695793620.0","comment_id":"851767"},{"timestamp":"1690910040.0","content":"Selected Answer: CE\nWith AWS Org, consider SCP first.\nIn this scenario, Only C,D,E are mention about SCP, but D apply for all, not only the DataOps OU","poster":"tatdatpham","upvote_count":"4","comment_id":"795438"},{"comments":[{"content":"Option A is incorrect because it only restricts access to specific instance types, but it does not restrict access to a specific region.\n\nOption B is incorrect because it is applied to IAM users rather than OUs, which would not effectively apply the restriction to all resources in the organization.\n\nOption D is incorrect because it uses the ec2:Region condition key which would not allow to restrict the instances types only in the DataOps OU.\n\nBy creating an SCP that uses the aws:RequestedRegion condition key and restricting access to all regions except ap-northeast-1 and applying it to the root OU, this ensures that all resources deployed in the organization will reside in the ap-northeast-1 Region.\n\nBy creating an SCP that uses the ec2:InstanceType condition key and restricts access to specific instance types and applying it to the DataOps OU, this ensures that all EC2 instances deployed in the DataOps OU will use the predefined list of instance types.","comment_id":"778021","upvote_count":"1","timestamp":"1689521940.0","poster":"masetromain"}],"poster":"masetromain","timestamp":"1689521760.0","comment_id":"778018","upvote_count":"4","content":"Selected Answer: CE\nThe correct options are C and E.\n\nOption C: Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.\n\nThis option is correct because it allows the company to restrict access to all AWS regions except for ap-northeast-1. This ensures that all resources deployed in the organization must reside in the ap-northeast-1 region. By applying the SCP to the root OU, it ensures that all accounts and OUs under the root will be affected.\n\nOption E: Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.\n\nThis option is correct because it allows the company to restrict access to specific instance types, which is required for the DataOps OU. By applying the SCP to the DataOps OU, it ensures that only resources deployed in the DataOps OU will be affected by the restriction."},{"timestamp":"1689507840.0","content":"CE is correct","poster":"zhangyu20000","upvote_count":"1","comment_id":"777721"}],"answer_ET":"CE","question_images":[],"isMC":true,"exam_id":33,"question_id":38},{"id":"rcE9mDgV5jni2w4nZ42R","exam_id":33,"unix_timestamp":1673876760,"timestamp":"2023-01-16 14:46:00","question_images":[],"discussion":[{"poster":"SK_Tyagi","upvote_count":"6","content":"Selected Answer: AC\nSNS being the publisher, SQS is subscribing","comment_id":"985126","timestamp":"1708347240.0"},{"comment_id":"1042965","upvote_count":"5","timestamp":"1713047760.0","poster":"rlf","content":"AC. \nAmazon SNS supports cross-region deliveries. \nhttps://docs.aws.amazon.com/sns/latest/dg/sns-cross-region-delivery.html"},{"content":"Selected Answer: AC\nSNS in Region A, SQS + Lambda in Region A & B, S3 Bucket in Region A","poster":"SeemaDataReader","comment_id":"1132206","timestamp":"1721951400.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1719178800.0","comment_id":"1104361","content":"Selected Answer: AC\nA and C","poster":"career360guru"},{"content":"Selected Answer: AC\nDeploy the SQS queue with the Lambda function to other Regions.\nSubscribe the SQS queue in each Region to the SNS topic.","upvote_count":"3","comment_id":"1043285","timestamp":"1713086160.0","poster":"Passexam4sure_com"},{"content":"Selected Answer: AC\nIt's an AC","poster":"NikkyDicky","timestamp":"1704304200.0","comment_id":"942017","upvote_count":"2"},{"timestamp":"1703250600.0","comments":[{"comment_id":"1190882","upvote_count":"1","poster":"awsleffe","timestamp":"1728293940.0","content":"SNS is the publisher and must stay in same region"}],"upvote_count":"3","content":"Selected Answer: AC\nBasically, you need to replicate it all except the bucket in the other regions. The question is explained very vaguely however","poster":"Maria2023","comment_id":"930416"},{"upvote_count":"3","timestamp":"1698356340.0","comment_id":"882036","content":"Selected Answer: AC\nA, C is correct.\nIt looks like Fan out pattern.","poster":"Parsons"},{"comment_id":"875235","comments":[{"upvote_count":"2","timestamp":"1699375440.0","poster":"Diego1414","comment_id":"891478","content":"It's SNS that publishes not SQS"}],"timestamp":"1697768640.0","poster":"Kampton","content":"Why would need to deploy SQS with Lambda? Makes no sense! It’s BE.","upvote_count":"1"},{"content":"What does it mean in Option A that Lambda deploys SQS?","upvote_count":"1","comment_id":"862659","poster":"Asagumo","timestamp":"1696560540.0"},{"timestamp":"1695793800.0","comment_id":"851769","content":"Selected Answer: AC\nAC - SQS","poster":"mfsec","upvote_count":"2"},{"upvote_count":"1","content":"support A,C. https://www.examtopics.com/discussions/amazon/view/74009-exam-aws-certified-solutions-architect-professional-topic-1/","poster":"Zek","timestamp":"1694073240.0","comment_id":"831721"},{"comment_id":"798211","timestamp":"1691163120.0","poster":"MasterP007","content":"A & C - Deploy & Subscribe SQS.","upvote_count":"1"},{"timestamp":"1690669260.0","upvote_count":"3","comment_id":"792200","poster":"zozza2023","content":"Selected Answer: AC\nA and C"},{"content":"Selected Answer: AC\nOption A is correct because deploying the SQS queue with the Lambda function to other regions will allow the application to process URLs in those regions and compare differences in site localization.\n\nOption C is correct because subscribing the SQS queue in each region to the SNS topic in the existing region will allow the application to publish URLs to the existing SNS topic and have those URLs processed in other regions.\n\nOption B is incorrect because subscribing the SNS topic in each region to the SQS queue in the existing region would not allow URLs to be processed in other regions.\n\nOption D is incorrect because configuring the SQS queue to publish URLs to SNS topics in each region would not ensure that the URLs are processed in those regions.\n\nOption E is incorrect because deploying the SNS topic and Lambda function to other regions without the SQS queue would not allow the application to process URLs in those regions.","poster":"masetromain","comment_id":"778025","timestamp":"1689522000.0","upvote_count":"4"},{"comment_id":"777725","upvote_count":"1","timestamp":"1689507960.0","poster":"zhangyu20000","content":"AC is correct"}],"answer_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/95546-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","question_text":"A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket.\n\nThe company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region. Results must be written to the existing S3 bucket in the current Region.\n\nWhich combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)","answers_community":["AC (100%)"],"question_id":39,"answer_ET":"AC","answer":"AC","choices":{"E":"Deploy the SNS topic and the Lambda function to other Regions.","D":"Configure the SQS queue to publish URLs to SNS topics in each Region.","C":"Subscribe the SQS queue in each Region to the SNS topic.","B":"Subscribe the SNS topic in each Region to the SQS queue.","A":"Deploy the SQS queue with the Lambda function to other Regions."},"isMC":true},{"id":"hUDXgAksFcLBNXhCvsN3","question_text":"A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.\n\nWhich strategy should the solutions architect use?","answer":"C","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95547-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","exam_id":33,"answer_ET":"C","answer_description":"","discussion":[{"content":"C is correct. only eventbridge can run scheduled task","upvote_count":"16","timestamp":"1689508140.0","comment_id":"777726","poster":"zhangyu20000"},{"poster":"Maria2023","comment_id":"930421","upvote_count":"6","content":"Selected Answer: C\nIf there wasn't a schedule element I would choose AWS Batch because it pretty much loads a container and does the job, especially since it's like a 20-minute job. However the step functions part doesn't help with the scheduling part, hence I go for C","timestamp":"1703251140.0"},{"comment_id":"1366039","content":"Selected Answer: C\nOption C clearly includes EventBridge for scheduling, aligning with the requirement to run tasks every 4 hours. While AWS Batch is technically better for CPU-intensive workloads, the lack of explicit EventBridge integration in Option B makes C the correct answer under AWS’s service design principles.","poster":"Longc","upvote_count":"1","timestamp":"1741297680.0"},{"timestamp":"1741134960.0","content":"Selected Answer: B\nB. Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.","comment_id":"1365168","poster":"albert_kuo","upvote_count":"1"},{"comment_id":"1356826","poster":"9d7a975","timestamp":"1739620680.0","upvote_count":"2","content":"Selected Answer: B\nB: Usa uma máquina de estado do AWS Step Functions para invocar o trabalho do AWS Batch a cada 4 horas.\nPor que não é a letra C : Embora possa executar containers, é mais adequado para aplicações de longa duração"},{"poster":"TonytheTiger","upvote_count":"1","content":"Option C : https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/run-event-driven-and-scheduled-workloads-at-scale-with-aws-fargate.html","timestamp":"1728135180.0","comment_id":"1189940"},{"poster":"ninomfr64","upvote_count":"2","content":"A = CW Log cannot invoke lambda every 4 hours\nB = Step Function cannot invoke batch job every 4 hour (unless you use an EventBridhe scheduled event)\nC = correct (but I do not like when Fargate is mentioned as a standalone service, as it is a serverless compute option for some some services)\nD = CodeDeploy cannot run an application every 4 hours","timestamp":"1721657640.0","comment_id":"1128863"},{"timestamp":"1721215200.0","comment_id":"1124989","content":"none. \"highly CPU intensive\" means no Fargate. scheduling means eventbridge.","upvote_count":"2","poster":"cox1960"},{"comment_id":"1117439","content":"https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/\nLambda's max running time is 15 mins, cannot support up to 20mins application.","poster":"holymancolin","upvote_count":"1","timestamp":"1720517880.0"},{"upvote_count":"1","content":"https://aws.amazon.com/tutorials/scheduling-a-serverless-workflow-step-functions-amazon-eventbridge-scheduler/\nStep Function cannot schedule a job. Step Function needs EventBridge as the scheduler.","timestamp":"1719575700.0","comment_id":"1107825","poster":"haha001"},{"timestamp":"1719179520.0","poster":"career360guru","comment_id":"1104365","content":"Selected Answer: C\nB is not possible as Step Function can not be used to run scheduled a job every 4 hour","upvote_count":"1"},{"upvote_count":"2","comments":[{"content":"you can´t garantee with spot instances that they're available every 4 hours, C is the answer","comment_id":"1162777","poster":"teo2157","timestamp":"1724939820.0","upvote_count":"3"}],"timestamp":"1711375140.0","comment_id":"1016762","poster":"task_7","content":"Selected Answer: D\ncontainers are well-suited for applications that are built in microservices architecture, where each service is a self-contained unit that performs a specific task. These types of applications are typically designed to be scalable and easy to deploy, making them a good fit for containerization.\nI feel D is the best option"},{"upvote_count":"1","comment_id":"1003038","timestamp":"1709980740.0","poster":"uC6rW1aB","content":"Selected Answer: C\nI think Both B 、C is missing some key point\nOption B does not explain how to AWS Step Functions to trigger an AWS Batch job regually, in this case 4 hours per run.\nOption C does not explain how to use EventBridge to call the Fargate task, which is not native support, it might involved lambda to achive."},{"timestamp":"1704304320.0","upvote_count":"1","poster":"NikkyDicky","comment_id":"942021","content":"Selected Answer: C\nC. schedule -> eventbridge"},{"comment_id":"902207","timestamp":"1700429820.0","upvote_count":"3","poster":"rbm2023","content":"Selected Answer: C\nThe application is a Linux binary which can be packaged into a container, then run on AWS Fargate and scheduled using Event Bridge.\n# Use a base image that matches your application's runtime environment\nFROM ubuntu:latest\n# Copy the Linux binary into the container\nCOPY myapp /usr/local/bin/myapp\n# Set the entry point to execute the binary\nENTRYPOINT [\"/usr/local/bin/myapp\"]"},{"poster":"mfsec","upvote_count":"1","timestamp":"1695793860.0","content":"Selected Answer: C\nC - Fargate is the best choice here","comment_id":"851772"},{"comment_id":"778029","poster":"masetromain","content":"Selected Answer: C\nC. Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.\n\nAWS Fargate is a serverless compute engine for containers that allows running containerized workloads without managing the underlying EC2 instances. This eliminates the need to provision, configure, and scale clusters of virtual machines to run containers.\n\nAmazon EventBridge (formerly CloudWatch Events) allows scheduling tasks using cron or rate expressions, which can be used to invoke the Fargate task every 4 hours. This will allow for cost-effective and scalable solution, as the infrastructure is managed by AWS and the application can run in a serverless fashion, only incurring costs when the task is running.","comments":[{"timestamp":"1689522240.0","upvote_count":"4","content":"The other options are not appropriate in this scenario:\n\nOption A: Running the application on AWS Lambda would not be appropriate, as Lambda is designed to run event-driven, short-lived functions, and not CPU-intensive, long-running tasks.\nOption B: AWS Batch is a service for running batch jobs, and it may not be the most appropriate service for this scenario, as the application is not a batch job but a long running task.\nOption D: Using Amazon EC2 Spot Instances would not be the best option for this scenario because the application is running for up to 20 minutes and EC2 Spot instances can be terminated at any time.","poster":"masetromain","comment_id":"778034"}],"timestamp":"1689522120.0","upvote_count":"4"}],"timestamp":"2023-01-16 14:49:00","choices":{"A":"Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.","C":"Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.","B":"Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.","D":"Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours."},"answers_community":["C (78%)","13%","9%"],"question_id":40,"question_images":[],"unix_timestamp":1673876940}],"exam":{"provider":"Amazon","isImplemented":true,"numberOfQuestions":529,"lastUpdated":"11 Apr 2025","id":33,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isMCOnly":true,"isBeta":false},"currentPage":8},"__N_SSP":true}