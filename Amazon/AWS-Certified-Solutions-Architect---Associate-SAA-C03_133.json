{"pageProps":{"questions":[{"id":"X7Yuw7Da4SD1nXBog8sP","answer_images":[],"timestamp":"2023-12-29 17:13:00","question_text":"A company's application uses Network Load Balancers, Auto Scaling groups, Amazon EC2 instances, and databases that are deployed in an Amazon VPC. The company wants to capture information about traffic to and from the network interfaces in near real time in its Amazon VPC. The company wants to send the information to Amazon OpenSearch Service for analysis.\n\nWhich solution will meet these requirements?","topic":"1","answer":"B","question_images":[],"answers_community":["B (95%)","5%"],"choices":{"C":"Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use Amazon Kinesis Data Streams to stream the logs from the trail to OpenSearch Service.","B":"Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data to the log group. Use Amazon Kinesis Data Firehose to stream the logs from the log group to OpenSearch Service.","D":"Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use Amazon Kinesis Data Firehose to stream the logs from the trail to OpenSearch Service.","A":"Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data to the log group. Use Amazon Kinesis Data Streams to stream the logs from the log group to OpenSearch Service."},"exam_id":31,"question_id":661,"answer_ET":"B","unix_timestamp":1703866380,"isMC":true,"discussion":[{"poster":"pentium75","upvote_count":"9","content":"Selected Answer: B\nCloudTrail is for logging administrative actions, we need CloudWatch. We want the data in another AWS service (OpenSearch), not Kinesis, thus we need Firehose, not Streams.","comment_id":"1112788","timestamp":"1704288600.0"},{"poster":"meenkaza","comment_id":"1108887","content":"Selected Answer: B\nAmazon CloudWatch Logs and VPC Flow Logs (Option B): VPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC. By configuring VPC Flow Logs to send the log data to a log group in Amazon CloudWatch Logs, you can then use Amazon Kinesis Data Firehose to stream the logs from the log group to Amazon OpenSearch Service for analysis. This approach provides near real-time streaming of logs to the analytics service.","upvote_count":"5","timestamp":"1703866380.0"},{"comments":[{"poster":"FlyingHawk","timestamp":"1735614000.0","comment_id":"1334583","content":"While Amazon Kinesis Data Streams can stream data, it requires additional setup (e.g., Lambda functions) to process and send logs to OpenSearch Service. This adds complexity compared to Kinesis Data Firehose, which is purpose-built for this use case.","upvote_count":"2"}],"content":"Selected Answer: A\nbase on the research, it should be Answer A, because question is asking for a \"near real time\" which Kinesis Data Stream is offering the data with less than 1 second latency. But Kinese Data Firehost is offering the data with more than 1 second.\n\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/integrations.html#integrations-kinesis\nhttps://stackoverflow.com/questions/44608274/is-there-any-difference-in-processing-times-between-aws-kinesis-firehose-and-str\nhttps://docs.aws.amazon.com/streams/latest/dev/using-other-services-cw-logs.html","timestamp":"1719396480.0","poster":"Jacky_S","upvote_count":"1","comment_id":"1237374"},{"timestamp":"1719396360.0","poster":"Jacky_S","content":"base on the research, it should be Answer A, because question is asking for a \"near real time\" which Kinesis Data Stream is offering the data with less than 1 second latency. But Kinese Data Firehost is offering the data with more than 1 second.\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/integrations.html#integrations-kinesis\nhttps://stackoverflow.com/questions/44608274/is-there-any-difference-in-processing-times-between-aws-kinesis-firehose-and-str","comment_id":"1237372","upvote_count":"1"},{"comment_id":"1192060","upvote_count":"2","poster":"zinabu","content":"log analysis place= aws cloudwatch log\ndata capturing on the entire vpc=aws flow log\nnear real time data analysis and send to OpenSearch service= kinesis data fire hose","timestamp":"1712644980.0"},{"content":"Selected Answer: B\nOpenSearch patterns for CloudWatch Logs:\n\n1) \"Near Real Time\": CloudWatch logs --> Subscription Filter --> Kinesis Data Firehose --> Amazon OpenSearch (option *B*)\n\n2) \"Real Time\": CloudWatch logs --> Subscription Filter --> Lambda --> Amazon OpenSearch","poster":"1Alpha1","comment_id":"1146905","upvote_count":"5","timestamp":"1707613560.0"}],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/129718-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"rxStj15O03tATE71M3s9","question_images":[],"unix_timestamp":1703907660,"answers_community":["A (50%)","B (47%)","3%"],"exam_id":31,"question_id":662,"answer":"A","question_text":"A company is developing an application that will run on a production Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has managed node groups that are provisioned with On-Demand Instances.\n\nThe company needs a dedicated EKS cluster for development work. The company will use the development cluster infrequently to test the resiliency of the application. The EKS cluster must manage all the nodes.\n\nWhich solution will meet these requirements MOST cost-effectively?","topic":"1","answer_images":[],"choices":{"D":"Create a managed node group that contains only On-Demand Instances.","C":"Create an Auto Scaling group that has a launch configuration that uses Spot Instances. Configure the user data to add the nodes to the EKS cluster.","A":"Create a managed node group that contains only Spot Instances.","B":"Create two managed node groups. Provision one node group with On-Demand Instances. Provision the second node group with Spot Instances."},"isMC":true,"discussion":[{"poster":"pentium75","content":"Selected Answer: A\nI think the question is easy to misunderstand, whether you should create the whole setup or just the development cluster. But from the wording (\"The [production] EKS cluster has (!) managed node groups ... The company needs a dedicated EKS cluster for development work\"), I conclude that we should only create the development cluster.\n\nAs this will be used \"infrequently\" for testing purposes only, and it must be \"most cost-effective\", I'd go with A - new cluster with \"one managed node group that contains only Spot instances\".","timestamp":"1720006980.0","comment_id":"1112798","upvote_count":"9","comments":[{"content":"The wording of question and options is so confusing. The last line is a throw off also \"The EKS cluster must manage all the nodes\" Which EKS cluster? A new one or the existing one.\nBoth A and B are correct depending on how you decipher the question.\nI really hope the exam question uses better language!","poster":"awsgeek75","comment_id":"1121897","upvote_count":"4","timestamp":"1720884000.0"},{"timestamp":"1727628120.0","upvote_count":"1","content":"I hate this question.... I think I will go with B just because wording also. A company is developing an application that \"WILL\" run on a production Amazon Elastic Kubernetes Service","poster":"Drew3000","comment_id":"1185621"}]},{"comment_id":"1138701","timestamp":"1722607740.0","content":"Selected Answer: B\nThis question is convoluted and missing some details.\nWe need:\n- control plane running on on-demand EC2s\n- worker nodes running on spot instances\n\nRead this to understand correct solution: \nhttps://aws.amazon.com/blogs/containers/amazon-eks-now-supports-provisioning-and-managing-ec2-spot-instances-in-managed-node-groups/","upvote_count":"5","poster":"frmrkc"},{"upvote_count":"1","comment_id":"1409848","poster":"tch","timestamp":"1742863260.0","content":"Selected Answer: B\nin the exam.... answer A is too easy... \nAWS want the candidate to apply the two workload... the PROD vs DEV..."},{"timestamp":"1740985980.0","poster":"CTao","upvote_count":"1","content":"Selected Answer: A\ninfrequently + cost effective -> Spot","comment_id":"1364302"},{"timestamp":"1740831180.0","comment_id":"1363519","poster":"ieffiong","content":"Selected Answer: A\nFor a development Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is used infrequently to test application resiliency, the most cost-effective solution is to create a managed node group consisting solely of Spot Instances (Option A). Spot Instances offer significant cost savings—up to 90% compared to On-Demand Instances—making them ideal for non-critical, intermittent workloads like development and testing.","upvote_count":"1"},{"timestamp":"1739719560.0","comment_id":"1357275","comments":[{"poster":"ieffiong","timestamp":"1740830400.0","upvote_count":"1","content":"This has nothing to do with production instances though. The question is referring to the development instance.","comment_id":"1363511"}],"poster":"Dantecito","upvote_count":"1","content":"Selected Answer: B\nI'll go with B because no matter what production should not go in spot instances"},{"upvote_count":"1","content":"Selected Answer: D\nClearly prod and dev are two different envs and two separate EKS cluster. In order to do meaningful resilience test, dev must matches the prod, so D is the answer.","comment_id":"1353726","timestamp":"1739079720.0","poster":"zdi561"},{"poster":"bujuman","comments":[{"timestamp":"1735614900.0","poster":"FlyingHawk","comment_id":"1334592","content":"I think this requirment mean to use manage node group to manage all worker nodes, the nodes for control panel will be managed by AWS, so A is correct","upvote_count":"1"}],"timestamp":"1729937880.0","comment_id":"1202532","content":"Selected Answer: B\nIf we look closer to the last requirement \"The EKS cluster must manage all the nodes.\" Option B is the only feasable and cost-effective one.","upvote_count":"3"},{"timestamp":"1723363740.0","content":"Selected Answer: A\nBased on the document [1], we can know that only self-managed node group can deploy the container on EC2 dedicated hosts . Which mean that customer need to manually create launch template, auto scaling group, and register it to the EKS cluster. The creation process should be same as general EC2 auto scaling creation. For now, EKS managed node group only supported on-demand and spot.\n\nMOST cost-effectively: *Spot Instances*\n\nhttps://repost.aws/questions/QUugoX4f1gRHW0MGHRTHFFFA/how-to-create-eks-cluster-with-dedicated-host-node-group","upvote_count":"3","comment_id":"1147124","poster":"1Alpha1"},{"comment_id":"1127814","content":"Selected Answer: A\n\"The company will use the development cluster infrequently to test the resiliency of the application\" = Spot instances = cost effective","upvote_count":"2","poster":"anikolov","timestamp":"1721559360.0"},{"timestamp":"1720627740.0","poster":"06042022","upvote_count":"3","comment_id":"1118909","content":"Selected Answer: B\nThe keywords are infrequent and resiliency..\n\nThis solution allows you to have a mix of On-Demand Instances and Spot Instances within the same EKS cluster. You can use the On-Demand Instances for the development work where you need dedicated resources and then leverage Spot Instances for testing the resiliency of the application. Spot Instances are generally more cost-effective but can be terminated with short notice, so using a combination of On-Demand and Spot Instances provides a balance between cost savings and stability.\n\nOption A (Create a managed node group that contains only Spot Instances) might be cost-effective, but it could introduce potential challenges for tasks that require dedicated resources and might not be the best fit for all scenarios."},{"comment_id":"1116306","upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1735615080.0","poster":"FlyingHawk","comment_id":"1334595","content":"A dedicated EKS cluster means a separate EKS cluster that is isolated from the production cluster. This ensures that development work does not interfere with the production environment"}],"content":"Selected Answer: B\nThe GBT vote A, I know the spot instance is the cheapest, but the question says \"dedicated EKS cluster for development\", so I vote B","timestamp":"1720395360.0","poster":"mr123dd"},{"upvote_count":"1","comment_id":"1115981","content":"Selected Answer: A\nOption A leverages the cost savings of Spot Instances, which is ideal for a development environment where the application is tested infrequently, and there is flexibility in when the nodes can be interrupted. This aligns with the goal of cost-efficiency and takes advantage of EKS's ability to manage the nodes directly.","timestamp":"1720359960.0","poster":"OSHOAIB"},{"poster":"cciesam","timestamp":"1719767580.0","comment_id":"1109910","comments":[],"upvote_count":"1","content":"Selected Answer: B\nB is the best ans."},{"comment_id":"1109377","content":"Option B","poster":"Naijaboy99","timestamp":"1719711660.0","comments":[{"comment_id":"1112800","poster":"pentium75","timestamp":"1720007040.0","content":"Why do you think so?","upvote_count":"1"}],"upvote_count":"2"}],"answer_ET":"A","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/129827-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-12-30 04:41:00"},{"id":"Tf53KX3VLne9rsAmb2PR","url":"https://www.examtopics.com/discussions/amazon/view/129719-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"discussion":[{"content":"Selected Answer: B\nSSE-KMS with Customer Managed Key (Option B): This option allows you to create a customer managed key using AWS KMS. With a customer managed key, you have full control over key lifecycle management, including the ability to create, rotate, and disable keys with minimal effort. SSE-KMS also integrates with AWS Identity and Access Management (IAM) for fine-grained access control.","poster":"meenkaza","timestamp":"1703866740.0","comment_id":"1108894","upvote_count":"11"},{"timestamp":"1726594200.0","comment_id":"1285382","poster":"MatAlves","upvote_count":"6","content":"Selected Answer: B\nHaving both awsgeek75 and pentium75 in the comment section makes me more confident about my own answers."},{"upvote_count":"1","timestamp":"1709155800.0","poster":"rubiteb","content":"Selected Answer: C\nCustomer needs to control the 'user's ability' and not the management of the keys. Option C will prevent users to have this ability.","comment_id":"1162012"},{"upvote_count":"3","timestamp":"1705166580.0","comment_id":"1121901","content":"Selected Answer: B\nHas to be customer manages to AC are not useful\nD is just wrong way of doing this\nB give full control to customer while using S3 server side encryption.","poster":"awsgeek75"},{"poster":"pentium75","comment_id":"1112802","content":"Selected Answer: B\nA and C do not allow the company \"to fully control the ability of users to create, rotate, and disable encryption keys\". D is anything but \"minimal effort\".","upvote_count":"4","timestamp":"1704289620.0"},{"timestamp":"1703915520.0","content":"Selected Answer: B\nOption B should be correct","poster":"Riajul","comment_id":"1109444","upvote_count":"3"}],"answer":"B","answer_images":[],"question_id":663,"choices":{"C":"Create an AWS managed key by using AWS Key Management Service (AWS KMS). Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).","B":"Create a customer managed key by using AWS Key Management Service (AWS KMS). Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).","A":"Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to store the sensitive data.","D":"Download S3 objects to an Amazon EC2 instance. Encrypt the objects by using customer managed keys. Upload the encrypted objects back into Amazon S3."},"question_images":[],"unix_timestamp":1703866740,"answer_ET":"B","exam_id":31,"answer_description":"","topic":"1","timestamp":"2023-12-29 17:19:00","answers_community":["B (96%)","4%"],"question_text":"A company stores sensitive data in Amazon S3. A solutions architect needs to create an encryption solution. The company needs to fully control the ability of users to create, rotate, and disable encryption keys with minimal effort for any data that must be encrypted.\n\nWhich solution will meet these requirements?"},{"id":"llwcvz7GPjCgS61Sl0uR","answer_ET":"ACE","discussion":[{"poster":"te1973","timestamp":"1717654440.0","upvote_count":"8","content":"This is a good example for a completely non-sense AWS exam question. In order to delete the object like requested in the question you need (E). This is required in either versioned or non-versioned buckets. Basically the task is done here. But let's assume we want to make it extra secure and retain the files for 30 days. Then we need object lock (A). You cannot have object lock without versioning (B). You also need to set a retention period then (C). So you either have A,B,C,E or you have E. Choosing exactly 3 options is completely nonsense here. But what do i know.","comment_id":"1225175"},{"timestamp":"1704290880.0","poster":"pentium75","upvote_count":"5","comments":[{"poster":"pentium75","comment_id":"1112825","timestamp":"1704290880.0","content":"C: Yes, \"default retention period\" specifies how long object lock will be applied to new objects by default, we need this to protect objects from deletion.\n\nD: No, S3 Lifecycle Policy can \"transition\" or \"expire\" but not \"protect\".\n\nE: Yes, this will delete the objects after 30 days (C just removes the object lock after 30 days but does not delete the objects).\n\nF: No, 'tag with a retention period' is not common AWS wording, \"tags\" are something different in AWS context","upvote_count":"4"}],"comment_id":"1112824","content":"Selected Answer: ACE\nIn theory, E alone would be enough because the objects are \"retained for 30 days\" without any configuration as long as no one deletes them. But let's assume that they want us to prevent deletion.\n\nA: Yes, required to prevent deletion. Object Lock requires Versioning, so if we 'create an S3 bucket that has S3 Object Lock enabled' that this also has object versioning enabled, otherwise we would not be able to create it.\n\nB: No. We need versioning, but we cannot \"create\" the bucket twice. If we create it \"with object lock enabled\" then versioning is enabled too, but NOT the other way round (creating it with versioning enabled will not automatically enable object lock)."},{"upvote_count":"1","comments":[{"content":"To use S3 Object Lock with a bucket (or objects within a bucket), you must first enable versioning for the bucket, as you won’t be able to turn versioning on later. Retention periods and legal holds apply to individual object versions","comment_id":"1395637","timestamp":"1741965360.0","poster":"tch","upvote_count":"1"}],"comment_id":"1395633","poster":"tch","timestamp":"1741964700.0","content":"Selected Answer: ACE\nTo manage the lifecycle of your objects, create an S3 Lifecycle configuration for your bucket. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:\n• Transition actions – These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them. \n• \n• Expiration actions – These actions define when objects expire. Amazon S3 deletes expired objects on your behalf. For example, you might to choose to expire objects after they have been stored for a regulatory compliance period."},{"comment_id":"1353920","timestamp":"1739107980.0","upvote_count":"1","content":"Selected Answer: ABE\nC is not right becasue A standard S3 object has no default retention period, meaning it can be deleted at any time unless you specifically configure a retention period using the \"S3 Object Lock\" feature, which allows you to set a custom retention period for individual objects or an entire bucket, with the minimum retention period being one day, B is right because you have to do it , it does not matter you have to do it to lock a object or do it yourself. D is a general description of action A.","poster":"zdi561"},{"comment_id":"1328803","content":"Selected Answer: ABE\nFor enabling S3 object lock we need to enable Bucket versioning.. SO and must be ABE","timestamp":"1734572400.0","upvote_count":"2","poster":"omega_coaching"},{"timestamp":"1726594920.0","upvote_count":"1","content":"Selected Answer: ACE\n\"Object Lock works only in buckets that have S3 Versioning enabled\"\n\nHowever, we can't have 2 options (A and B) telling to create the bucket. So, A is only possible if versioning is already enabled.\n\nWe need retention period (C), since this is not a case for legal holds:\n\n\"Object Lock provides two ways to manage object retention: retention periods and legal holds.\"\n\nE - obvious reasons.\n\nRef. https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html","poster":"MatAlves","comment_id":"1285388"},{"timestamp":"1724072040.0","content":"this is shit","upvote_count":"2","comment_id":"1268677","poster":"chwieobjom"},{"upvote_count":"4","poster":"mohammadthainat","timestamp":"1711596300.0","content":"Selected Answer: ACE\n1- The S3 backups must be retained for 30 days --> \n\nFor that you must enable S3 Object Lock (versioning must be enabled) in Compliance Mode and set Retention Period to 30 days. Thus, to achieve this you need 3 options <A, B and C>\n\n\n2- The S3 backups must be automatically deleted after 30 days. --> \n\nFor that you must Create Lifecycle Rule with action Expire current versions of objects (versioning must be enabled) and set Expiration Period to 30 days. Thus to achieve this you need 2 options <B and E>\n\n<B> is a must here as both locking the objects and deleting them can't be achieved without it. But, when choosing \"A.Create an S3 bucket that has S3 Object Lock enabled.\" this explicitly indicated that versioning is enabled in your bucket.","comment_id":"1184527"},{"upvote_count":"5","timestamp":"1705763880.0","comment_id":"1127350","content":"Selected Answer: ADE\nB: No versioning is required\nD: Lifecycle is for transitioning or expiring. There is no protection lifecycle policy\nF: No such tag\n\nEnable object lock, retain for 30 days (https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html) and expire after 30 days.","comments":[{"poster":"MatAlves","comment_id":"1285387","content":"\"Object Lock works only in buckets that have S3 Versioning enabled\"\n\nHowever, I still agree with ACE, since the bucket has already been created, so we can't have 2 answers telling to create the bucket. \n\nAnd yes, for this case, we need retention period:\n\n\"Object Lock provides two ways to manage object retention: retention periods and legal holds.\"\n\nRef. https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html","upvote_count":"3","timestamp":"1726594740.0"},{"content":"I meant ACE! not ADE!","poster":"awsgeek75","upvote_count":"3","timestamp":"1705763880.0","comment_id":"1127351"}],"poster":"awsgeek75"},{"timestamp":"1703981580.0","content":"ABE -> https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\nA. Create an S3 bucket that has S3 Object Lock enabled. -> You set a Retention period of 30 days with this feature.\nB. Create an S3 bucket that has object versioning enabled -> Object Lock works only in buckets that have S3 Versioning enabled\nE. Configure an S3 Lifecycle policy to expire the objects after 30 days. -> It is valid using the lifecicle policy.","comments":[{"poster":"PegasusForever","upvote_count":"2","comment_id":"1115458","content":"After analyzing the question deeply and reading: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html, I keep A and B, change E per C.\nA. Create an S3 bucket that has S3 Object Lock enabled.\nB. Create an S3 bucket that has object versioning enabled.\nChange E must be automatically deleted after 30 days(objects will be marked as expired not deleted). per C. Configure a default retention period of 30 days for the objects. It feature delete the object.","timestamp":"1704577800.0","comments":[{"content":"Selected Answer: ACE\nA. Create an S3 bucket that has S3 Object Lock enabled. Enable the S3 Object Lock feature on S3.\nC. Configure a default retention period of 30 days for the objects. To lock the objects for 30 days.\nE. Configure an S3 Lifecycle policy to expire the objects after 30 days. -> to delete the objects after 30 days.","upvote_count":"2","timestamp":"1704765960.0","comment_id":"1117156","poster":"PegasusForever"}]}],"comment_id":"1110155","upvote_count":"2","poster":"PegasusForever"},{"timestamp":"1703968440.0","content":"Selected Answer: ACE\nACE is the correct ans.","comment_id":"1109960","poster":"cciesam","upvote_count":"5"},{"upvote_count":"1","poster":"Riajul","comments":[{"poster":"pentium75","comment_id":"1112816","upvote_count":"2","timestamp":"1704290400.0","content":"Why? \n\nS3 Lifecycle Policy can \"transition\" or \"expire\" but not \"protect\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-expire-general-considerations.html"}],"comment_id":"1109450","timestamp":"1703915640.0","content":"Selected Answer: ADE\nADE should be correct"},{"comment_id":"1109369","poster":"Naijaboy99","timestamp":"1703907060.0","upvote_count":"2","content":"Correct Answer is A C E"},{"timestamp":"1703866920.0","poster":"meenkaza","comments":[{"content":"To manage the lifecycle of your objects, create an S3 Lifecycle configuration for your bucket. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:\n• Transition actions – These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them. \n• \n• Expiration actions – These actions define when objects expire. Amazon S3 deletes expired objects on your behalf. For example, you might to choose to expire objects after they have been stored for a regulatory compliance period.","timestamp":"1741964760.0","upvote_count":"1","comment_id":"1395634","poster":"tch"}],"comment_id":"1108899","upvote_count":"1","content":"Selected Answer: ADE\nA. Create an S3 bucket that has S3 Object Lock enabled.\n\nS3 Object Lock provides the ability to enforce retention periods on objects, preventing deletion or modification for a specified duration.\nD. Configure an S3 Lifecycle policy to protect the objects for 30 days.\n\nBy configuring a lifecycle policy, you can define a transition action to move objects to the S3 Glacier storage class (or any other storage class) after 30 days.\nE. Configure an S3 Lifecycle policy to expire the objects after 30 days."}],"question_images":[],"question_id":664,"url":"https://www.examtopics.com/discussions/amazon/view/129721-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-12-29 17:22:00","answers_community":["ACE (62%)","ADE (27%)","12%"],"question_text":"A company wants to back up its on-premises virtual machines (VMs) to AWS. The company's backup solution exports on-premises backups to an Amazon S3 bucket as objects. The S3 backups must be retained for 30 days and must be automatically deleted after 30 days.\n\nWhich combination of steps will meet these requirements? (Choose three.)","unix_timestamp":1703866920,"choices":{"A":"Create an S3 bucket that has S3 Object Lock enabled.","F":"Configure the backup solution to tag the objects with a 30-day retention period","D":"Configure an S3 Lifecycle policy to protect the objects for 30 days.","E":"Configure an S3 Lifecycle policy to expire the objects after 30 days.","B":"Create an S3 bucket that has object versioning enabled.","C":"Configure a default retention period of 30 days for the objects."},"answer_images":[],"answer":"ACE","topic":"1","isMC":true,"exam_id":31,"answer_description":""},{"id":"FXHRprXg9khQyDS33GHr","question_images":[],"question_text":"A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.\nWhat should the solutions architect do to meet these requirements?","question_id":665,"answers_community":["A (93%)","6%"],"answer_description":"","isMC":true,"choices":{"D":"Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails.","C":"Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.","B":"Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.","A":"Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails."},"answer":"A","answer_ET":"A","unix_timestamp":1665917520,"exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/85593-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"upvote_count":"21","comment_id":"696192","timestamp":"1665917520.0","poster":"KVK16","content":"Selected Answer: A\nDirect Connect + VPN best of both"},{"upvote_count":"19","comment_id":"713014","poster":"mabotega","content":"Selected Answer: A\nDirect Connect goes throught 1 Gbps, 10 Gbps or 100 Gbps and the VPN goes up to 1.25 Gbps. \n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html","timestamp":"1667821800.0"},{"poster":"akeed","timestamp":"1742229840.0","upvote_count":"1","content":"Selected Answer: D\nHighly available means the system is up and running most of the time, not 100% of the time. Therefore, minor outages are acceptable.","comment_id":"1399707"},{"comment_id":"1348191","poster":"Dharmarajan","content":"Selected Answer: A\nSince the direct connect takes a lot of time to setup and is much more expensive, the point of consideration is - if those are not in the criteria, we assume Direct connection is alright.","upvote_count":"1","timestamp":"1738097940.0"},{"poster":"PaulGa","content":"Selected Answer: A\nAnd A - as primary use Direct Connect to provide a solid connection. Company is not too worried about backup (other than it works) so use cheaper VPN Site-to-Site.","upvote_count":"2","timestamp":"1726060860.0","comment_id":"1282132"},{"poster":"48cd959","comment_id":"1176099","content":"Selected Answer: A\nAns A-\nDirect Connect because company needs consistent connection.\nAs a back up, company wants cheaper solution so VPN Site to Site connection should be okay.","upvote_count":"3","timestamp":"1710715440.0"},{"upvote_count":"4","timestamp":"1705251060.0","comment_id":"1122687","poster":"awsgeek75","content":"Selected Answer: A\nHA low latency + minimize cost + acceptable slow traffic if primary fails\nB: VPN tunnel will be slow\nC: 2 direct connect will be expensive\nD: Backup connection for what?\nA: Direct connect + VPN as a backup works"},{"upvote_count":"6","poster":"Ruffyit","timestamp":"1698414480.0","comment_id":"1055629","content":"A highly available connection with consistent low latency = AWS Direct Connect\nMinimize costs and accept slower traffic if the primary connection fails = VPN connection"},{"poster":"benacert","upvote_count":"1","content":"A is the right choice to save cost","timestamp":"1693851540.0","comment_id":"998759"},{"timestamp":"1691689140.0","upvote_count":"3","comment_id":"977962","poster":"Guru4Cloud","content":"Selected Answer: A\nHighly available connectivity using Direct Connect for consistent low latency and high throughput.\nCost optimization by using a VPN as a slower, lower cost backup for when Direct Connect fails.\nAutomatic failover to the VPN when Direct Connect fails."},{"timestamp":"1691555040.0","upvote_count":"1","poster":"TariqKipkemei","content":"Selected Answer: A\nA highly available connection with consistent low latency = AWS Direct Connect\nMinimize costs and accept slower traffic if the primary connection fails = VPN connection","comment_id":"976238"},{"timestamp":"1690525980.0","upvote_count":"1","poster":"hsinchang","comment_id":"965323","content":"Selected Answer: A\nSlower traffic when primary fails, so the backup plan needs a cheaper solution, and the primary requires high performance, so A."},{"poster":"oguzbeliren","content":"Even though, there are a lots of variable affecting the cost of the connection, VPN connection is cheaper than the Direct Connect most of the time since VPN Connection doesn't require any dedicated physical circuit involved.","comment_id":"959862","timestamp":"1690063860.0","upvote_count":"1"},{"poster":"miki111","content":"Option A is the right answer.","comment_id":"956847","upvote_count":"1","timestamp":"1689786480.0"},{"content":"Selected Answer: A\nOptions B and C propose using multiple VPN connections for private connectivity and as backups. While VPNs can serve as backups, they may not provide the same level of consistent low latency and high availability as Direct Connect connections. Additionally, provisioning multiple VPN tunnels can increase operational complexity and costs.\n\nOption D suggests using the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails. While this approach can be automated, it does not provide the same level of immediate failover capabilities as having a separate backup connection in place.\n\nTherefore, option A, provisioning an AWS Direct Connect connection to a Region and provisioning a VPN connection as a backup, is the most suitable solution that meets the company's requirements for connectivity, cost-effectiveness, and high availability.","comment_id":"929511","upvote_count":"4","poster":"cookieMr","timestamp":"1687351320.0"},{"content":"Selected Answer: A\nhigly available - > direct connect beecause connection can go up to 10GBPs and VPN 1.5GBPs as backup","timestamp":"1682134980.0","comment_id":"877014","poster":"th3k33n","upvote_count":"2"},{"comment_id":"857274","poster":"linux_admin","timestamp":"1680278820.0","upvote_count":"1","content":"Selected Answer: A\nOption A is the correct solution to meet the requirements of the company. Provisioning an AWS Direct Connect connection to a Region will provide a private and dedicated connection with consistent low latency. As the company requires a highly available connection, a VPN connection can be provisioned as a backup if the primary Direct Connect connection fails. This approach will minimize costs and provide the required level of availability."},{"upvote_count":"2","poster":"devonwho","content":"Selected Answer: A\nWith AWS Direct Connect + VPN, you can combine AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This solution combines the benefits of the end-to-end secure IPSec connection with low latency and increased bandwidth of the AWS Direct Connect to provide a more consistent network experience than internet-based VPN connections.\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html","timestamp":"1675113840.0","comment_id":"793360"},{"upvote_count":"2","comments":[{"poster":"J3nkinz","upvote_count":"3","content":"The company requires a highly available connection with consistent low latency to an AWS Region, this is provided by Direct Connect as primary connection. The company allows a slower connection only for the backup option, so A is the right answer","comment_id":"780911","timestamp":"1674117660.0"}],"content":"Why not B? Two VPNs on different connections? Direct Connect costs a fortune?","comment_id":"777787","poster":"dev1978","timestamp":"1673880000.0"},{"timestamp":"1671947700.0","content":"DX for low latency connect and the company accept slower traffic if the primary connection fails. So we should choose VPN for backup purpose. And the question also mark : minimize cost.","upvote_count":"1","poster":"thanhch","comment_id":"755427"},{"timestamp":"1671567900.0","content":"Selected Answer: C\nThis a tricky question but let's try to understand the requirements of the question.\n\nThe company requires VS The company needs.\n\nThe main difference between need and require is that needs are goals and objectives a business must achieve, whereas require or requirements are the things we need to do in order to achieve a need.","upvote_count":"2","comments":[{"timestamp":"1671567960.0","poster":"Buruguduystunstugudunstuy","upvote_count":"4","comment_id":"751436","comments":[{"content":"2 Direct connections will not minimize costs. Correct Answer is A","poster":"studynoplay","upvote_count":"2","timestamp":"1683236640.0","comment_id":"889662"},{"timestamp":"1671568020.0","comments":[{"poster":"Buruguduystunstugudunstuy","comment_id":"751441","upvote_count":"1","timestamp":"1671568380.0","content":"See pricing for more info.\nhttps://aws.amazon.com/directconnect/pricing/","comments":[{"upvote_count":"2","content":"I love your comments!","timestamp":"1674890340.0","comment_id":"790374","poster":"ocbn3wby"}]},{"timestamp":"1684477260.0","comment_id":"901689","upvote_count":"2","poster":"ruqui","content":"You forgot to consider that \"the company is willing to accept slower traffic if the primary connection fails\", so option A is the best answer"}],"comment_id":"751437","content":"Using VPN connections as a backup, as described in options A and B, is not the best solution because VPN connections are typically slower and less reliable than Direct Connect connections. Additionally, having two VPN connections to the same Region may not provide the desired level of availability and may not meet the company's requirement for low latency.\n\nOption D, which involves using the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails, is not a valid option because the Direct Connect failover attribute is not available in the AWS CLI.","poster":"Buruguduystunstugudunstuy","upvote_count":"6"}],"content":"To meet the requirements specified in the question, the best solution is to provision two AWS Direct Connect connections to the same Region. This will provide a highly available connection with consistently low latency to the AWS Region and minimize costs by eliminating internet usage fees. Provisioning a second Direct Connect connection as a backup will ensure that there is a failover option available in case the primary connection fails."}],"comment_id":"751433","poster":"Buruguduystunstugudunstuy"},{"poster":"career360guru","content":"Selected Answer: A\nOption A","upvote_count":"1","comment_id":"749395","timestamp":"1671418020.0"},{"upvote_count":"1","poster":"koreanmonkey","timestamp":"1669526460.0","comment_id":"727929","content":"Selected Answer: A\nA is rigth I thought wrong"},{"content":"Selected Answer: C\nI think VPN is not right solution for \"low latency\"\nSo how about C?","comments":[{"content":"The question mention that \"The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails\" so VPN as secondary option is acceptable","upvote_count":"3","poster":"AlaN652","timestamp":"1670475060.0","comment_id":"738628"}],"timestamp":"1669525740.0","upvote_count":"2","comment_id":"727928","poster":"koreanmonkey"},{"upvote_count":"1","timestamp":"1669041120.0","content":"A is correct","comment_id":"723615","poster":"Wpcorgan"}],"answer_images":[],"timestamp":"2022-10-16 12:52:00","topic":"1"}],"exam":{"numberOfQuestions":1019,"isBeta":false,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isMCOnly":true,"lastUpdated":"11 Apr 2025","isImplemented":true,"id":31,"provider":"Amazon"},"currentPage":133},"__N_SSP":true}