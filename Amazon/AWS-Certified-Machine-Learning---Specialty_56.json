{"pageProps":{"questions":[{"id":"bmWnISdQ3unJ6quuN4e2","answer_images":[],"timestamp":"2024-09-15 14:46:00","isMC":true,"choices":{"D":"Use an Amazon SageMaker Data Wrangler data flow to create and modify a data preparation pipeline. Manually add the feature scores.","A":"Use the Amazon SageMaker Data Wrangler multicollinearity measurement features and the principal component analysis (PCA) algorithm to calculate the variance of the dataset along multiple directions in the feature space.","B":"Use an Amazon SageMaker Data Wrangler quick model visualization to find feature importance scores that are between 0.5 and 1.","C":"Use the Amazon SageMaker Data Wrangler bias report to identify potential biases in the data related to feature engineering."},"unix_timestamp":1726404360,"exam_id":26,"question_id":276,"answer":"B","answer_description":"","answers_community":["B (60%)","D (20%)","A (20%)"],"url":"https://www.examtopics.com/discussions/amazon/view/147603-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","question_text":"A company needs to develop a model that uses a machine learning (ML) model for risk analysis. An ML engineer needs to evaluate the contribution each feature of a training dataset makes to the prediction of the target variable before the ML engineer selects features.\n\nHow should the ML engineer predict the contribution of each feature?","question_images":[],"answer_ET":"B","discussion":[{"poster":"ef12052","timestamp":"1742544060.0","content":"Selected Answer: B\n\"evaluate the contribution each feature of a training dataset makes to the prediction of the target\" -> B.","comment_id":"1401468","upvote_count":"1"},{"comments":[{"poster":"MultiCloudIronMan","timestamp":"1730544120.0","comment_id":"1306146","upvote_count":"3","content":"Changed my mind, #B' is the correct answer, Option D involves using Amazon SageMaker Data Wrangler to create and modify a data preparation pipeline and manually adding the feature scores. While this approach can work, it introduces additional manual steps and complexity compared to Option B."}],"poster":"MultiCloudIronMan","timestamp":"1729419720.0","upvote_count":"1","comment_id":"1300392","content":"Selected Answer: A\nThe question is asking for contribution of each feature not to view only the features that make the highest contribution which .5 and 1 scores shows."},{"upvote_count":"1","content":"Selected Answer: B\nShould be B","comment_id":"1299129","timestamp":"1729157100.0","poster":"SamHan"},{"timestamp":"1727776740.0","comment_id":"1291896","poster":"kupo777","upvote_count":"4","content":"B is the correct answer.\nThe most effective way for ML engineers to assess the contribution that each feature in the training dataset makes to the prediction of the target variable is to use the B. Amazon SageMaker Data Wrangler Quick Model Visualization to find feature importance scores between 0.5 and 1. This method provides a quick visualization of the importance of each feature and can be used to make selection decisions.\n\nOther selections\n\nA uses Principal Component Analysis (PCA) to calculate variance, but does not directly assess the contribution of a feature.\nC identifies bias and is not suitable for assessing contribution.\nD is related to data preparation but is not a method to evaluate the contribution of a feature."},{"poster":"MultiCloudIronMan","comment_id":"1287840","upvote_count":"1","timestamp":"1727021640.0","content":"Selected Answer: B\nThis approach directly addresses the need to evaluate the contribution of each feature to the prediction of the target variable by providing feature importance scores, which are crucial for understanding and selecting the most impactful features."},{"comment_id":"1284114","poster":"CW0106","content":"Selected Answer: D\nShould be D.","upvote_count":"1","timestamp":"1726404360.0"}]},{"id":"5fczwFENyy6AV5AXpIDg","unix_timestamp":1726457640,"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/147689-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"content":"Selected Answer: D\nA & D both work, D is real time, A is less effort but near real time. it is very hard to choose","comment_id":"1364209","poster":"sfwewv","timestamp":"1740970260.0","upvote_count":"1"},{"comment_id":"1336909","content":"Selected Answer: A\nFirehose can use record backup and preserve original records to S3, in addition of transformation using Lambda","poster":"4bc91ae","upvote_count":"1","timestamp":"1736123160.0"},{"comment_id":"1331526","timestamp":"1735126260.0","upvote_count":"1","poster":"LeoD","content":"Selected Answer: A\nA. (For the \"real-time\", well, Firehose = near real time, compared to other options which have significant issues, using Firehose here does not have big problem. I assume this question is calling real-time and near real time interchangeably :( )\nC is incorrect - Reason #1: This is Real-Time Data Ingestion (The data needs to be ingested in real time from the devices into Amazon S3), which is not suitable to use Glue. Reason #2: Glue cannot be used for transformation, only Lambda could be used here. Although Glue could be used for record format conversion, but only ORC and Parquet are supported in that case, which is irrelevant with this scenario.\nD - Kinesis Data Streams is unnecessary"},{"timestamp":"1730562180.0","content":"D.\nfirehose can not store data and not real time, it is near real time\nfirehose cant invoke glue, you can use firehose with lambda for transformation. And spark job requires more effort than lambda.\nAlso C does not store raw data before transformation","upvote_count":"1","comment_id":"1306223","poster":"spinatram"},{"poster":"VerRi","content":"Selected Answer: A\nleast operational effort","upvote_count":"2","timestamp":"1729368900.0","comment_id":"1300156"},{"comment_id":"1287844","upvote_count":"1","timestamp":"1727022300.0","content":"Selected Answer: A\ncopilot - This solution meets the requirements by:\n\nIngesting data in real-time using Amazon Kinesis Data Firehose.\nTransforming the data using an AWS Lambda function during the ingestion process.\nStoring the transformed data in Amazon S3.\nEnabling source record backup to store raw data and failed transformation records in specific S3 locations.\nIf you have any more questions or need further assistance, feel free to ask!","poster":"MultiCloudIronMan"},{"poster":"CW0106","comment_id":"1284439","upvote_count":"2","timestamp":"1726457640.0","content":"Selected Answer: C\nC, A -X firehose can't store data."}],"timestamp":"2024-09-16 05:34:00","exam_id":26,"answer":"A","topic":"1","question_id":277,"answer_ET":"A","answer_images":[],"choices":{"C":"Use Amazon Data Firehose with Amazon S3 as the destination. Configure Firehose to invoke an Apache Spark job in AWS Glue for data transformation. Enable source record backup and configure the error prefix.","D":"Use Amazon Kinesis Data Streams in front of Amazon Data Firehose. Use Kinesis Data Streams with AWS Lambda to store raw data in Amazon S3. Configure Firehose to invoke a Lambda function for data transformation with Amazon S3 as the destination.","A":"Use Amazon Data Firehose with Amazon S3 as the destination. Configure Firehose to invoke an AWS Lambda function for data transformation. Enable source record backup on Firehose.","B":"Use Amazon Managed Streaming for Apache Kafka. Set up workers in Amazon Elastic Container Service (Amazon ECS) to move data from Kafka brokers to Amazon S3 while transforming it. Configure workers to store raw and unsuccessfully transformed data in different S3 buckets."},"question_text":"A company is building a predictive maintenance system using real-time data from devices on remote sites. There is no AWS Direct Connect connection or VPN connection between the sites and the company's VPC. The data needs to be ingested in real time from the devices into Amazon S3.\n\nTransformation is needed to convert the raw data into clean .csv data to be fed into the machine learning (ML) model. The transformation needs to happen during the ingestion process. When transformation fails, the records need to be stored in a specific location in Amazon S3 for human review. The raw data before transformation also needs to be stored in Amazon S3.\n\nHow should an ML specialist architect the solution to meet these requirements with the LEAST effort?","answer_description":"","answers_community":["A (63%)","C (25%)","13%"]},{"id":"86T3doFMBbG7qgzDr6k9","choices":{"A":"Define the feature variables and target variable for the churn prediction model.","E":"Manually export the training data to Amazon S3.","C":"Write a CREATE MODEL SQL statement to create a model.","F":"Use the SQL prediction function to run predictions.","D":"Use Amazon Redshift Spectrum to train the model.","B":"Use the SOL EXPLAIN_MODEL function to run predictions."},"isMC":true,"question_images":[],"answers_community":["ACF (100%)"],"topic":"1","question_text":"A company wants to use machine learning (ML) to improve its customer churn prediction model. The company stores data in an Amazon Redshift data warehouse.\n\nA data science team wants to use Amazon Redshift machine learning (Amazon Redshift ML) to build a model and run predictions for new data directly within the data warehouse.\n\nWhich combination of steps should the company take to use Amazon Redshift ML to meet these requirements? (Choose three.)","answer_ET":"ACF","answer_images":[],"question_id":278,"url":"https://www.examtopics.com/discussions/amazon/view/147151-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"ACF","discussion":[{"upvote_count":"1","poster":"MultiCloudIronMan","content":"Selected Answer: ACF\nfrom copilot - The correct combination of steps to use Amazon Redshift ML for building a customer churn prediction model and running predictions directly within the data warehouse are:\n\nA. Define the feature variables and target variable for the churn prediction model.\n\nC. Write a CREATE MODEL SQL statement to create a model.\n\nF. Use the SQL prediction function to run predictions.","timestamp":"1727022780.0","comment_id":"1287849"},{"comment_id":"1279992","upvote_count":"3","timestamp":"1725709560.0","poster":"GS_77","content":"Selected Answer: ACF\nThe correct steps are A,C,F"}],"timestamp":"2024-09-07 13:46:00","exam_id":26,"unix_timestamp":1725709560,"answer_description":""},{"id":"bNIe0SoY9ZxqtfUzYD6e","exam_id":26,"question_id":279,"answers_community":["A (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/10056-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2019-12-09 21:04:00","answer_description":"","answer":"A","isMC":true,"answer_ET":"A","question_text":"A Data Science team is designing a dataset repository where it will store a large amount of training data commonly used in its machine learning models. As Data\nScientists may create an arbitrary number of new datasets every day, the solution has to scale automatically and be cost-effective. Also, it must be possible to explore the data using SQL.\nWhich storage scheme is MOST adapted to this scenario?","discussion":[{"upvote_count":"15","timestamp":"1647997500.0","comment_id":"28259","poster":"rsimham","content":"Ans: A (S3) is most cost effective"},{"comment_id":"324532","content":"A : S3 cost effective + athena ( not c redshift dont support unstructured data)","upvote_count":"7","timestamp":"1651425540.0","poster":"sonalev419"},{"comment_id":"1358045","timestamp":"1739838240.0","content":"Selected Answer: A\nAmazon S3 (Simple Storage Service) is the best choice because it:\n\nScales automatically to store an arbitrary number of datasets.\nIs cost-effective, as S3 charges only for storage used, unlike provisioned databases.\nSupports querying datasets with SQL using Amazon Athena.\nIs highly durable (99.999999999% durability) and optimized for large datasets.\nHow It Works in This Scenario?\nStore datasets in S3 as files in Parquet, ORC, or CSV format.\nUse AWS Glue Data Catalog to create table metadata.\nQuery the datasets using Amazon Athena (serverless SQL querying on S3).\nAutomatically scale without worrying about storage limits.","upvote_count":"1","poster":"JonSno"},{"timestamp":"1725669960.0","content":"Selected Answer: A\n'cost effective' --> AWS S3","comment_id":"1167624","upvote_count":"1","poster":"james2033"},{"comment_id":"1006596","timestamp":"1710339600.0","upvote_count":"1","content":"Selected Answer: A\nA. YES - S3 + Athena/Presto\nB. NO - no SQL support\nC. NO - expensive to scale \nD. NO - DynamoDB is NoSQL","poster":"loict"},{"comment_id":"1005455","poster":"DavidRou","content":"Selected Answer: A\nAWS S3 + Athena will do it","timestamp":"1710231720.0","upvote_count":"1"},{"content":"Selected Answer: A\nThe most appropriate storage scheme for this scenario is option A: Store datasets as files in Amazon S3.\n\nAmazon S3 is a highly scalable and cost-effective object storage service that can store a large amount of data. S3 can scale automatically to accommodate a large number of datasets, making it a good option for storing the training data used in machine learning models. Additionally, S3 supports SQL querying through Amazon Athena or Amazon Redshift Spectrum, allowing data scientists to easily explore the data.","poster":"AjoseO","timestamp":"1691652360.0","upvote_count":"2","comment_id":"804171"},{"upvote_count":"2","content":"\"store a large amount of training data commonly used in its machine learning models\".. well it cannot be anything other than S3. Athena can query S3 cataloged data with SQL commands.\nAnwser is A","timestamp":"1650894240.0","comment_id":"262616","poster":"harmanbirstudy"},{"poster":"Stephen_C","content":"Amazon Redshift is not cost-effective.","upvote_count":"1","comment_id":"253606","timestamp":"1650713220.0"},{"comment_id":"180758","upvote_count":"3","content":"I would say C\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\n\"For workloads that require ever-growing storage, managed storage lets you automatically scale your data warehouse storage capacity without adding and paying for additional nodes.\"","poster":"syu31svc","timestamp":"1649143200.0","comments":[{"upvote_count":"5","timestamp":"1649237880.0","content":"Data warehouse is not needed. For exploring data using SQL, you can use Athena","poster":"HaiHN","comment_id":"200789"},{"upvote_count":"1","poster":"kwangje","comment_id":"248545","content":"Amazon Redshift is a fast, fully managed data warehouse that makes it simple and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools. It allows you to run complex analytic queries against petabytes of structured data using sophisticated query optimization, columnar storage on high-performance storage, and massively parallel query execution. Most results come back in seconds.","timestamp":"1650518160.0"}]},{"comment_id":"98743","poster":"roytruong","content":"s3 is right","upvote_count":"1","timestamp":"1648972200.0"},{"poster":"cybe001","upvote_count":"3","comment_id":"38278","content":"A, S3 is most cost effective","timestamp":"1648427940.0"}],"unix_timestamp":1575921840,"choices":{"B":"Store datasets as files in an Amazon EBS volume attached to an Amazon EC2 instance.","C":"Store datasets as tables in a multi-node Amazon Redshift cluster.","D":"Store datasets as global tables in Amazon DynamoDB.","A":"Store datasets as files in Amazon S3."},"topic":"1","answer_images":[]},{"id":"rRs1o6yfwApRl1Nky7cw","answer_images":[],"exam_id":26,"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/147984-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1727022960,"answers_community":["D (100%)"],"choices":{"A":"Principal component analysis (PCA)","D":"Convolutional neural network (CNN)","B":"Recurrent neural network (RNN)","C":"К-nearest neighbors (k-NN)"},"timestamp":"2024-09-22 18:36:00","question_images":[],"answer_description":"","question_text":"A company’s machine learning (ML) team needs to build a system that can detect whether people in a collection of images are wearing the company’s logo. The company has a set of labeled training data.\n\nWhich algorithm should the ML team use to meet this requirement?","topic":"1","isMC":true,"answer_ET":"D","discussion":[{"comment_id":"1358465","timestamp":"1739913600.0","content":"Selected Answer: D\nA. PCA: No, it's for dimensionality reduction, not image classification.\nB. RNN: No, it's designed for sequential data, not for images.\nC. k-NN: No, it's a basic classifier that doesn't effectively extract image features.\nD. CNN: Sí, because Convolutional Neural Networks are designed to extract spatial features from images for classification tasks.","poster":"italiancloud2025","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nfrom copilot- The correct answer is D. Convolutional neural network (CNN).\n\nCNNs are highly effective for image recognition tasks, including detecting logos in images. They can automatically learn and extract features from images, making them well-suited for identifying specific patterns, such as logo","comment_id":"1287853","poster":"MultiCloudIronMan","timestamp":"1727022960.0"}],"question_id":280}],"exam":{"lastUpdated":"11 Apr 2025","isImplemented":true,"id":26,"isMCOnly":false,"isBeta":false,"numberOfQuestions":369,"provider":"Amazon","name":"AWS Certified Machine Learning - Specialty"},"currentPage":56},"__N_SSP":true}