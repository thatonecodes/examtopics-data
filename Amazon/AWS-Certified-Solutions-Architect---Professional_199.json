{"pageProps":{"questions":[{"id":"rrhD6YRKsbSR7IKU6PNb","question_text":"A financial services company is operating a highly regulated workload on premises. The company is trying to modernize its monolithic core payments application by changing to a microservices-based architecture with containers. The company is waiting for regulatory approval to run this workload on AWS. In the meantime, the company wants to start deploying the containerized application on premises.\n\nA solutions architect needs to design a solution that gives the company the ability to run and update existing and new workloads even if the company loses network connectivity to an AWS Region.\n\nWhich solution will meet these requirements?","timestamp":"2022-12-04 15:11:00","question_images":[],"discussion":[{"upvote_count":"1","poster":"backbencher2022","comment_id":"1268058","timestamp":"1723985940.0","content":"Selected Answer: C\nC is the correct option. ECS anywhere is ruled out because it requires a network connection between on-prem and AWS region. Correct option is EKS anywhere (c)"},{"timestamp":"1705066980.0","upvote_count":"1","comment_id":"1120808","content":"Selected Answer: C\n\"design a solution that gives the company the ability to run and update existing and new workloads even if the company loses network connectivity to an AWS Region.\"\nC - EKS Anywhere can be run on-premises completely disconnected and/or air-gapped\nB - incorrect EKS Anywhere is not provisioned on AWS \nhttps://aws.amazon.com/eks/eks-anywhere/faqs/\nA & D - incorrect. A loss of network activity will leave tasks running but no updates can be made. \nhttps://aws.amazon.com/ecs/anywhere/faqs/","poster":"3a632a3"},{"content":"Selected Answer: C\nThe answer is consistent with https://aws.amazon.com/eks/eks-anywhere/getting-started/\nAlso, it fulfills the requirement of network outage having no impact on the cluster - in the FAQ https://aws.amazon.com/eks/eks-anywhere/faqs/, we read:\nQ: Does EKS Anywhere require internet connectivity to an AWS region?\nIn the case of partially disconnected clusters, there is no impact on your applications running on the clusters but since intermittent disconnects can last several hours, features like the EKS console in the AWS console will show the state from the time of disconnect, with eventual consistency restored once the connection returns.","comments":[{"poster":"vn_thanhtung","timestamp":"1693316400.0","upvote_count":"1","comments":[{"comment_id":"993155","upvote_count":"1","timestamp":"1693316520.0","poster":"vn_thanhtung","content":"I think on the node?"}],"content":"Launch the workload's containers on the cluster ?","comment_id":"993151"}],"upvote_count":"1","poster":"bosmanx","comment_id":"987253","timestamp":"1692694740.0"},{"content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/containers/introducing-amazon-ecs-anywhere/","poster":"SkyZeroZx","upvote_count":"1","comment_id":"947649","timestamp":"1688951040.0"},{"timestamp":"1687208400.0","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/containers/introducing-amazon-ecs-anywhere/","upvote_count":"1","comment_id":"927939","poster":"Sudeepshiv"},{"comment_id":"927936","poster":"Sudeepshiv","timestamp":"1687208280.0","upvote_count":"1","content":"Answer D, can work in disconnected mode"},{"content":"Selected Answer: D\nECS anywhere. EKS anywhere is for Kubernetes and i don't see any mention to Kubernetes in the question, i assume that company is using Docker (so ECS) for the microservices\nhttps://aws.amazon.com/ecs/anywhere/","poster":"dev112233xx","upvote_count":"1","comment_id":"885138","timestamp":"1682852820.0"},{"poster":"Vash2303","timestamp":"1674937260.0","upvote_count":"2","content":"Selected Answer: B\nIt cannot be A because for option-A, it must have a stable connection to the AWS Region\nEKS-D supports fully disconnected set up","comment_id":"791058"},{"upvote_count":"1","poster":"Heer","content":"Option B","timestamp":"1674616320.0","comment_id":"787245"},{"content":"Selected Answer: B\nTypical use case of EKS Anywhere and Distro. B","timestamp":"1674236280.0","poster":"sndychvn","comment_id":"782589","upvote_count":"1"},{"content":"Correct B.\nEKS-D is available to install and manage yourself. You can run EKS-D on-premises, in a cloud, or on your own systems. EKS-D provides a path to having essentially the same Amazon EKS Kubernetes distribution running wherever you need to run it.\nhttps://distro.eks.amazonaws.com/","poster":"ggrodskiy","upvote_count":"1","timestamp":"1670163060.0","comment_id":"735130"}],"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/89969-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true,"question_id":991,"answer_images":[],"topic":"1","exam_id":32,"answer_ET":"B","unix_timestamp":1670163060,"answers_community":["B (33%)","D (33%)","C (33%)"],"answer_description":"","choices":{"C":"Download and run the Amazon EKS Anywhere installer on the company's managed infrastructure on premises. Create an Amazon EKS Anywhere cluster on premises. Launch the workload's containers on the cluster.","B":"Install Amazon EKS Distro on the company's managed infrastructure on premises. Register the on-premises servers or VMs with an Amazon EKS Anywhere cluster on AWS. Launch the workload's containers on the cluster.","A":"Install AWS Systems Manager, Docker, and Amazon Elastic Container Service (Amazon ECS) agents on the company's managed infrastructure on premises. Register the on-premises servers or VMs with an Amazon ECS Anywhere cluster on AWS. Launch the workload's containers on the cluster.","D":"Use the Amazon Elastic Container Service (Amazon ECS) control plane for an Amazon ECS Anywhere cluster. Install the ECS agent on the company's managed infrastructure on premises. Launch the workload's containers on the cluster."}},{"id":"m7IzW1FLoawL3EZ59RLi","url":"https://www.examtopics.com/discussions/amazon/view/91799-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"B","discussion":[{"poster":"nyunyu","comment_id":"746996","upvote_count":"7","timestamp":"1671182280.0","content":"Selected Answer: B\nCorrect BB"},{"comment_id":"1027555","content":"There is an issue in this question. The secrets is encrypted by default AWS managed key which cannot share with other accounts. need to be CMK (not default to be shared)","upvote_count":"1","poster":"DiaaCloud","timestamp":"1696698180.0"},{"comment_id":"984643","timestamp":"1692374040.0","content":"Selected Answer: A\nAnswer explained above","upvote_count":"1","comments":[{"upvote_count":"1","content":"https://docs.aws.amazon.com/ram/latest/userguide/shareable.html\nI don't think so, RAM can not sharing AWS Secrets Manager","poster":"vn_thanhtung","timestamp":"1692483180.0","comment_id":"985447"}],"poster":"rsn"},{"poster":"rsn","content":"I feel B is not correct. There is no mention of setting up a trust policy in the role (in application account) that allows DBA account to assume this role. Without this, the solution propose in B cannot work. I feel the answer is A","upvote_count":"1","comment_id":"984641","timestamp":"1692373980.0"},{"poster":"wendy_abigail","timestamp":"1688246820.0","upvote_count":"1","comment_id":"940330","content":"Selected Answer: B\nB - it is literally this blog\nhttps://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/\nnot C because it's not possible to edit key policy for AWS managed KMS key"},{"poster":"Jesuisleon","timestamp":"1685575740.0","upvote_count":"1","comments":[{"upvote_count":"1","poster":"Jesuisleon","content":"pls. ignore C, the correct answer is B.\nIN C we can't set DBA-Admin the permissions in application account. We need a role in application account have repective permissions and let DBA-Admin assume role in application account. B is right.","timestamp":"1685576520.0","comment_id":"911608"}],"comment_id":"911598","content":"Selected Answer: C\nB is apparently WRONG.\n DBA-Secret role doesn't have permission to access to AWS managed key, how could B address \"eliminates the need to manually share the secrets\" ?"}],"isMC":true,"question_images":[],"question_text":"A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts.\n\nIn an AWS application account, the company's application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed in the application account.\n\nThe application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets.\n\nWhich solution will meet these requirements?","answer_description":"","question_id":992,"exam_id":32,"answer_images":[],"topic":"1","choices":{"D":"In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.","C":"In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.","B":"In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.","A":"Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."},"unix_timestamp":1671182280,"answer":"B","timestamp":"2022-12-16 10:18:00","answers_community":["B (80%)","10%","10%"]},{"id":"Uf4c7Bk3tSuszCh5LYVg","url":"https://www.examtopics.com/discussions/amazon/view/88406-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","topic":"1","question_text":"A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway. The application data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning by using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.\n\nA solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.\n\nWhich solution will meet these requirements MOST cost-effectively?","exam_id":32,"question_images":[],"isMC":true,"choices":{"B":"Use AWS Database Migration Service (AWS DMS), Amazon EventBridge (Amazon CloudWatch Events), and AWS Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB Streams, EventBridge (CloudWatch Events), and Lambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.","D":"Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.","A":"Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront with origin failover to route traffic to the secondary Region during a DR scenario.","C":"Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."},"question_id":993,"answer":"C","timestamp":"2022-11-23 06:52:00","discussion":[{"timestamp":"1721391720.0","comment_id":"1251187","content":"Selected Answer: C\nWhy is B marked as the correct solution? You wouldn't use DMS.","upvote_count":"1","poster":"devilman222"},{"content":"I really suspect you can use aws backup to copy database files from one region to another region under RPO 2 hrs.","poster":"Jesuisleon","upvote_count":"2","comment_id":"922322","timestamp":"1686668760.0"},{"comments":[{"comment_id":"907524","timestamp":"1685122260.0","poster":"rbm2023","content":"although they are correct, option D is not the most cost effective, it will require you to have live enviroment in both regions which is going to cost you, a lot more than just keep backups and the recovering considering the RTO and RPO in the scenario.","upvote_count":"1"}],"timestamp":"1675005180.0","content":"Selected Answer: C\nC and D are crorect solution\nC is the answer as it is cost less https://aws.amazon.com/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/","comment_id":"791761","poster":"zozza2023","upvote_count":"2"},{"poster":"ggrodskiy","timestamp":"1670165580.0","content":"Correct C.\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html","upvote_count":"4","comment_id":"735154"},{"timestamp":"1669182720.0","content":"Selected Answer: C\nC is going to be cost effective, right? Also, cloudformation to deploy infrastructure is already in place.","comment_id":"724916","poster":"pvrhere","upvote_count":"4"}],"answer_ET":"C","answers_community":["C (100%)"],"answer_images":[],"unix_timestamp":1669182720},{"id":"Kmq4Nilo5RTUx8WFuaml","url":"https://www.examtopics.com/discussions/amazon/view/94592-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2023-01-09 15:30:00","topic":"1","question_id":994,"choices":{"D":"Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.","C":"Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.","F":"Use AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint.","A":"Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database.","B":"Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database.","E":"Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint."},"exam_id":32,"question_text":"A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions architect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the migration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must be identical to the source database at completion of the migration process.\n\nAll applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The RDS for Oracle DB instance is in a private subnet.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)","unix_timestamp":1673274600,"answer_ET":"ACE","question_images":[],"answers_community":["ACE (83%)","BCF (17%)"],"isMC":true,"discussion":[{"timestamp":"1692377280.0","content":"Selected Answer: BCF\nI feel it is BCF. SCT is efficient for large datasets. It just does not support CDC. For CDC, we can use DMS.","poster":"rsn","comments":[{"content":"The migration strategy must replicate all existing data and any new data that is created during the migration, With F only migrate change data => F Wrong","poster":"vn_thanhtung","comment_id":"985453","upvote_count":"1","timestamp":"1692484080.0"},{"content":"\"Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance\" => This is crazy, You should prepare better before the exam","upvote_count":"1","poster":"vn_thanhtung","comment_id":"985454","timestamp":"1692484260.0"}],"upvote_count":"1","comment_id":"984670"},{"content":"Selected Answer: ACE\nAce are right","timestamp":"1687210200.0","poster":"Sudeepshiv","upvote_count":"1","comment_id":"927967"},{"poster":"zozza2023","content":"Selected Answer: ACE\nACE for me","timestamp":"1674940440.0","upvote_count":"2","comment_id":"791099"},{"poster":"ggrodskiy","comment_id":"770750","content":"Correct ACE for me","upvote_count":"3","comments":[{"comment_id":"770754","upvote_count":"1","timestamp":"1673288820.0","poster":"ggrodskiy","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html"}],"timestamp":"1673288700.0"},{"content":"Selected Answer: ACE\nI like ACE","timestamp":"1673274600.0","comment_id":"770519","upvote_count":"2","poster":"syaldram"}],"answer_description":"","answer_images":[],"answer":"ACE"},{"id":"h9wzj9AWNAtvLtQhH22l","url":"https://www.examtopics.com/discussions/amazon/view/91758-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"B":"Create a CloudFormation stack set that creates the VPC. Use the stack set to import the VPC into the stack.","C":"Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the existing resources.","D":"Create a new CloudFormation template that creates the VPC. Use the AWS Serverless Application Model (AWS SAM) CLI to import the VPC.","A":"Create a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPC resources and configuration. Use AWS CDK to import the VPC into the stack and to manage the VPC."},"question_text":"A company is running its solution on AWS in a manually created VPC. The company is using AWS CloudFormation to provision other parts of the infrastructure. According to a new requirement, the company must manage all infrastructure in an automatic way.\n\nWhat should the company do to meet this new requirement with the LEAST effort?","discussion":[{"poster":"devilman222","content":"Selected Answer: C\nI said C and then stupidly looked at the \"correct answer\" and wasted time before looking the face that everyone picked C. Tip, never look at the \"correct answer\"","upvote_count":"1","timestamp":"1724091840.0","comment_id":"1268887"},{"comment_id":"861766","upvote_count":"1","poster":"yama234","timestamp":"1680671700.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import-existing-stack.html"},{"comment_id":"844526","content":"Correct B\nC requires the need for manual updates, the limited control over imported resources, and the requirement to manually create resource dependencies.\nOption B, creating a CloudFormation Stack Set to manage the VPC, is likely the better solution as it allows for managing multiple stacks in an automated way, without requiring manual updates or resource dependency management.","timestamp":"1679285880.0","poster":"TajSidKazi","upvote_count":"2"},{"content":"Correct C for me","timestamp":"1673289000.0","upvote_count":"2","poster":"ggrodskiy","comment_id":"770761"},{"poster":"Kende","comment_id":"760256","content":"Selected Answer: C\nC is the one.","timestamp":"1672256400.0","upvote_count":"1"},{"timestamp":"1671159300.0","content":"Selected Answer: C\ni will go with CloudFormation template","poster":"due","comment_id":"746726","upvote_count":"2"}],"answer":"C","exam_id":32,"topic":"1","unix_timestamp":1671159300,"answer_description":"","question_images":[],"answer_images":[],"question_id":995,"isMC":true,"answer_ET":"C","answers_community":["C (100%)"],"timestamp":"2022-12-16 03:55:00"}],"exam":{"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","id":32,"provider":"Amazon","isBeta":false,"numberOfQuestions":1019,"isMCOnly":false,"isImplemented":true},"currentPage":199},"__N_SSP":true}