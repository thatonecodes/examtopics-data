{"pageProps":{"questions":[{"id":"W6FNHejA43lwcy2zpNUI","choices":{"C":"Create a reference to the /tmp local directory. Store the result files and log file by using the directory reference. Append the log entry to the log file.","D":"Create a reference to the /opt storage directory. Store the result files and log file by using the directory reference. Append the log entry to the log file.","B":"Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach enabled volume. Attach the EBS volume to all Lambda functions. Update the Lambda function code to download the log file, append the log entries, and upload the modified log file to Amazon EBS.","A":"Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in Lambda. Store the result files and log file in the mount point. Append the log entries to the log file."},"url":"https://www.examtopics.com/discussions/amazon/view/88801-exam-aws-certified-developer-associate-topic-1-question-347/","isMC":true,"topic":"1","discussion":[{"poster":"Ankit1010","comment_id":"817114","upvote_count":"1","content":"A\n\nhttps://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/","timestamp":"1677010260.0"},{"comment_id":"728050","poster":"k1kavi1","content":"Selected Answer: A\nEFS is correct","upvote_count":"2","timestamp":"1669538520.0"},{"timestamp":"1669475400.0","content":"Selected Answer: A\nA for me","upvote_count":"2","comment_id":"727589","poster":"michaldavid"},{"timestamp":"1669432440.0","comment_id":"727232","upvote_count":"1","poster":"DrCloud","content":"Ans: A\nNeed: share files and append results into an existing file ==> EFS\n\nhttps://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/\nFor appending to existing files, EFS is also a preferred option to using Amazon S3."}],"question_id":276,"question_text":"A developer is creating an AWS Lambda function in VPC mode. An Amazon S3 event will invoke the Lambda function when an object is uploaded into an S3 bucket. The Lambda function will process the object and produce some analytic results that will be recorded into a file. Each processed object will also generate a log entry that will be recorded into a file.\n\nOther Lambda functions, AWS services, and on-premises resources must have access to the result files and log file. Each log entry must also be appended to the same shared log file. The developer needs a solution that can share files and append results into an existing file.\n\nWhich solution should the developer use to meet these requirements?","question_images":[],"answers_community":["A (100%)"],"answer_ET":"A","answer":"A","answer_images":[],"answer_description":"","exam_id":25,"timestamp":"2022-11-26 04:14:00","unix_timestamp":1669432440},{"id":"MaElwfoRa0VEuqonUDcz","answers_community":["A (100%)"],"unix_timestamp":1669432920,"exam_id":25,"url":"https://www.examtopics.com/discussions/amazon/view/88802-exam-aws-certified-developer-associate-topic-1-question-348/","answer_description":"","answer_ET":"A","answer_images":[],"topic":"1","discussion":[{"comment_id":"1067247","poster":"kyoharo","upvote_count":"1","content":"Selected Answer: A\nA. Add the permissions to an IAM policy. Attach the policy to a role. Attach the role to the EC2 instance profile.","timestamp":"1699621680.0"},{"poster":"Paul_101","upvote_count":"1","content":"Selected Answer: A\nI'm also going with A","timestamp":"1694137080.0","comment_id":"1002005"},{"content":"Selected Answer: A\nGoing with A","comment_id":"728052","timestamp":"1669538580.0","upvote_count":"2","poster":"k1kavi1"},{"upvote_count":"1","comment_id":"727590","poster":"michaldavid","timestamp":"1669475460.0","content":"Selected Answer: A\nA is correct"},{"comment_id":"727234","poster":"DrCloud","timestamp":"1669432920.0","upvote_count":"2","content":"Ans: A\nhttps://d1.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf"}],"timestamp":"2022-11-26 04:22:00","question_id":277,"choices":{"D":"Add the permissions to an IAM policy. Use IAM web identity federation to access the S3 bucket with the policy.","A":"Add the permissions to an IAM policy. Attach the policy to a role. Attach the role to the EC2 instance profile.","B":"Add the permissions inline to an IAM group. Attach the group to the EC2 instance profile.","C":"Add the permissions to an IAM policy. Attach the policy to a user. Attach the user to the EC2 instance profile."},"question_text":"A developer is creating a new batch application that will run on an Amazon EC2 instance. The application requires read access to an Amazon S3 bucket. The developer needs to follow security best practices to grant S3 read access to the application.\n\nWhich solution meets these requirements?","isMC":true,"answer":"A","question_images":[]},{"id":"ZGzzXNOr3sylc2mRWTEX","choices":{"B":"Use AWS Step Functions to process the data as Standard Workflows.","C":"Use AWS Step Functions to process the data as Synchronous Express Workflows.","D":"Use AWS Step Functions to process the data as Asynchronous Express Workflows.","A":"Use Amazon Simple Notification Service (Amazon SNS) to process the data through an HTTP or HTTPS endpoint."},"url":"https://www.examtopics.com/discussions/amazon/view/88417-exam-aws-certified-developer-associate-topic-1-question-349/","isMC":true,"topic":"1","discussion":[{"poster":"Mom305","timestamp":"1674079740.0","upvote_count":"8","content":"Selected Answer: D\nD, \"at least Once\" for Asynchronous and \"at most once\" for Synchronous. Check the Table values for Execution Guarantee https://docs.aws.amazon.com/step-functions/latest/dg/express-at-least-once-execution.html","comment_id":"780470"},{"comment_id":"726337","timestamp":"1669341840.0","content":"Ans: D\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-standard-vs-express.html\nExpress Workflows are ideal for high-volume, event-processing workloads such as IoT data ingestion, streaming data processing and transformation.\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-express-synchronous.html\nResults are written to DynamoDB. You can use Asynchronous Express Workflows when you don't require immediate response output, such as messaging services or data processing that other services don't depend on.\nhttps://aws.amazon.com/step-functions/pricing/","upvote_count":"6","poster":"DrCloud"},{"poster":"mistral","content":"Selected Answer: C\nC\nThe orchestrator must not miss a transaction - Synchronous","comment_id":"818573","timestamp":"1677111960.0","upvote_count":"1"},{"poster":"Ankit1010","comment_id":"817104","timestamp":"1677009840.0","comments":[{"upvote_count":"1","timestamp":"1677009900.0","content":"Option B, using Standard Workflows in AWS Step Functions, would be more expensive and slower than using Asynchronous Express Workflows.\n\nOption C, using Synchronous Express Workflows, would be faster than Standard Workflows but may not be able to handle the required throughput of tens of thousands of transactions per second.","poster":"Ankit1010","comment_id":"817105"}],"content":"D\n\nOption D, using Asynchronous Express Workflows, is designed for high-throughput and high-concurrency scenarios, making it the best fit for this use case. It is designed to provide at-least-once processing with retries and has a lower cost compared to Standard Workflows.","upvote_count":"3"},{"content":"Selected Answer: C\nThe orchestrator must process each transaction at least once. Therefore it needs to be an Express workflow. And since the orchestrator must not miss any transactions we need to ensure that the process is completed. Or in other words, we need to get a success or failure response. Therefore, we need to use Synchronous Express Workflow.","upvote_count":"4","timestamp":"1676084760.0","poster":"pancman","comment_id":"804939"},{"timestamp":"1674350940.0","comment_id":"783856","upvote_count":"4","poster":"Phinx","content":"Selected Answer: D\nD it is."},{"upvote_count":"3","timestamp":"1674079680.0","content":"D, \"at least Once\" for Asynchronous and \"at most once\" for Synchronous. Check the Table values for Execution Guarantee https://docs.aws.amazon.com/step-functions/latest/dg/express-at-least-once-execution.html","poster":"Mom305","comment_id":"780469"},{"comment_id":"773574","poster":"Mark1000","timestamp":"1673533800.0","upvote_count":"2","content":"D\nhttps://docs.aws.amazon.com/step-functions/latest/dg/express-at-least-once-execution.html\n\nAsynchronous Express Workflows --> At-least-once workflow execution"},{"upvote_count":"5","poster":"KT_Yu","comment_id":"771580","timestamp":"1673365500.0","content":"Selected Answer: D\nAsynchronous Express Workflows: At-least-once workflow execution.\n\nSynchronous Express Workflows: At-most-once workflow execution."},{"content":"D is correct as atleast once","comment_id":"767043","timestamp":"1672953120.0","upvote_count":"2","poster":"zebtig"},{"poster":"braveheart22","upvote_count":"2","content":"D is definitely the right answer for me. Asynchronous Express Workflows is the same as configuring SQS dead letter queue that will ensure that each transaction must be processed at least once.","comment_id":"766137","timestamp":"1672873080.0"},{"comments":[{"comment_id":"746831","timestamp":"1671168120.0","content":"Totally agree. C and D are almost the same. Except that the questions says \"at least once\", which is guaranteed by Asynchronous Express Workflows.","upvote_count":"3","poster":"xicomynor"}],"timestamp":"1671079380.0","content":"I Vote D\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-standard-vs-express.html\nAsynchronous Express Workflows: At-least-once workflow execution.\n\nSynchronous Express Workflows: At-most-once workflow execution.","comment_id":"745696","poster":"BelloMio","upvote_count":"3"},{"upvote_count":"3","comment_id":"728060","timestamp":"1669539300.0","poster":"k1kavi1","content":"Selected Answer: C\nGoing with Synchronous Express Workflows"},{"upvote_count":"1","timestamp":"1669475640.0","comment_id":"727594","content":"Selected Answer: C\nI think C as well","poster":"michaldavid"},{"upvote_count":"2","content":"I think it's C\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-express-synchronous.html","poster":"absolutic","timestamp":"1669188600.0","comment_id":"724964"}],"question_id":278,"question_images":[],"question_text":"A developer is creating a serverless orchestrator that performs a series of steps to processes incoming IoT data. The orchestrator transforms the data, performs a series of calculations, and stores the results in Amazon DynamoDB. The entire process is completed in less than a minute.\n\nThe orchestrator must process tens of thousands of transactions each second. The orchestrator must not miss a transaction and must process each transaction at least once.\n\nWhich solution will meet these requirements MOST cost-effectively?","answers_community":["D (65%)","C (35%)"],"answer":"D","answer_ET":"D","answer_images":[],"answer_description":"","exam_id":25,"timestamp":"2022-11-23 08:30:00","unix_timestamp":1669188600},{"id":"RBIvftnyqh3xVKUlpGS2","choices":{"B":"Configure event source mapping for the Lambda function","C":"Map an Amazon SNS topic to the DynamoDB streams","A":"Change the StreamViewType parameter value to NEW_AND_OLD_IMAGES for the DynamoDB table","D":"Increase the maximum execution time (timeout) setting of the Lambda function"},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/4365-exam-aws-certified-developer-associate-topic-1-question-35/","topic":"1","discussion":[{"comment_id":"8974","poster":"YashBindlish","timestamp":"1632459420.0","content":"Correct Answer is \"B\"","comments":[{"content":"Create an event source mapping to tell Lambda to send records from your stream to a Lambda function. You can create multiple event source mappings to process the same data with multiple Lambda functions, or process items from multiple streams with a single function.","comment_id":"23372","poster":"newbie2019","upvote_count":"19","timestamp":"1632651720.0","comments":[{"timestamp":"1636002900.0","upvote_count":"6","poster":"Pavan_Nagineni","content":"Yes , simply this is synchronous invocation to poll for the streams.\nSync - Kinesis Streams/Dynamo DB Streams/SQS/ALB [Lamda has to poll with event source mappers]\nAsync - S3 event/Cloudwatch even/SNSt triggers [lamda will be triggered by services]\nB. Configure event source mapping for the Lambda function","comments":[{"poster":"Huy","comment_id":"406087","upvote_count":"3","timestamp":"1636012920.0","content":"Dont understand what you are saying. Check this pls https://acloud.guru/forums/aws-cda-2018/discussion/-MAeecIlPK6-VmdONvqa/what%20is%20the%20difference%20between%20asynchronous%20Invocation%20and%20Event%20source%20Mapping"}],"comment_id":"357638"}]}],"upvote_count":"27"},{"timestamp":"1679062260.0","upvote_count":"6","poster":"dp8719823","content":"Selected Answer: B\nOption B is correct.\n\nExplanation:\n\nTo process stream records in a DynamoDB table, you can create an AWS Lambda function and configure an event source mapping to the stream. This allows your Lambda function to be invoked whenever there are changes to the table.\n\nEnabling DynamoDB streams alone is not enough to trigger the Lambda function. You also need to create an event source mapping.\n\nOption A is incorrect because the StreamViewType parameter is used to determine the information included in the stream records.\n\nOption C is incorrect because mapping an SNS topic to DynamoDB streams would not trigger the Lambda function.\n\nOption D is incorrect because increasing the maximum execution time (timeout) setting of the Lambda function would not enable DynamoDB table updates to trigger the Lambda function.","comment_id":"842041"},{"timestamp":"1734346080.0","comment_id":"1327285","comments":[{"comment_id":"1327286","upvote_count":"1","comments":[{"poster":"sumanshu","upvote_count":"1","timestamp":"1734346140.0","comments":[{"timestamp":"1734346200.0","upvote_count":"1","poster":"sumanshu","content":"C) Eliminated - While you can integrate DynamoDB Streams with SNS, this is not necessary for Lambda to be triggered. Lambda can directly be triggered by DynamoDB Streams without needing SNS in between. This option adds unnecessary complexity.","comment_id":"1327288"}],"content":"D) Eliminated - The problem is not about the Lambda function's timeout but about the Lambda function not being triggered at all","comment_id":"1327287"}],"poster":"sumanshu","content":"A) Eliminated - This setting controls which data is included in the DynamoDB stream (new item, old item, or both)","timestamp":"1734346140.0"}],"poster":"sumanshu","upvote_count":"1","content":"Selected Answer: B\nB) Correct - You need to explicitly configure an event source mapping between the DynamoDB stream and the Lambda function. This tells AWS Lambda to listen to the changes in the DynamoDB stream and invoke the Lambda function when a change occurs in the table."},{"timestamp":"1714481940.0","upvote_count":"1","poster":"blondy_chess","comment_id":"1204574","content":"Selected Answer: B\nCorrect Answer is \"B\""},{"timestamp":"1705294620.0","poster":"AsmaZoheb","upvote_count":"1","comment_id":"1123066","content":"Selected Answer: B\ni think so B"},{"comment_id":"1022832","upvote_count":"1","content":"Selected Answer: B\nOption A (Changing the StreamViewType) is not the correct approach because the StreamViewType parameter value specifies the information included in the stream records (e.g., new and/or old images of the items), but it does not configure the Lambda trigger.","poster":"sara_exam_topics","timestamp":"1696224960.0"},{"content":"B looks correct","upvote_count":"1","comment_id":"968197","poster":"TJ1901","timestamp":"1690814460.0"},{"timestamp":"1681374540.0","comment_id":"869226","poster":"moonhope","upvote_count":"1","content":"B\nhttps://amazon-dynamodb-labs.com/design-patterns/ex8streams.html"},{"timestamp":"1677016800.0","poster":"Dominicwild12","comment_id":"817216","upvote_count":"3","content":"Selected Answer: B\nTo enable DynamoDB table updates to trigger a Lambda function, you need to create an event source mapping for the Lambda function. So the correct option here would be:\n\nB. Configure event source mapping for the Lambda function\n\nEnabling DynamoDB streams is a prerequisite for setting up the event source mapping. The event source mapping enables the Lambda function to read data from the stream, which triggers the function."},{"upvote_count":"2","timestamp":"1675409760.0","content":"A is Correct!\nWhen the question itself says DynamoDB streams is enabled and Trigger is also set then by default in the Lambda console even if you cannot see DynamoDB as the Trigger, it will be enabled!!\nHence New_And_Old_Images is the correct option because if 'update' item changes are made in DynamoDB table will not be fetched in Lambda hence New_And_Old_Images to be considered","poster":"Jay1299","comment_id":"796842"},{"comment_id":"743371","timestamp":"1670882880.0","upvote_count":"2","content":"Selected Answer: B\nB is the correct answer","poster":"fabriciollf"},{"timestamp":"1668691560.0","comments":[{"content":"i made an error, past data won't be sent, merly the new stuff will have the old stuff next to it. answer is still B.","poster":"dark_cherrymon","upvote_count":"1","comment_id":"720477","comments":[{"upvote_count":"1","poster":"dark_cherrymon","content":"\"Configuring a stream as an event source\nCreate an event source mapping to tell Lambda to send records from your stream to a Lambda function.\"\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html","comment_id":"720481","timestamp":"1668692040.0"}],"timestamp":"1668691920.0"}],"poster":"dark_cherrymon","comment_id":"720475","content":"Selected Answer: B\nit's not A because that would return old stuff as well and they want only new updates","upvote_count":"2"},{"comment_id":"654602","poster":"icebox0e","timestamp":"1661910720.0","upvote_count":"2","content":"Selected Answer: B\nCorrect Answer is \"B\""},{"timestamp":"1648446600.0","upvote_count":"2","comment_id":"576643","content":"B is correct.","poster":"sak1996"},{"content":"Selected Answer: B\nB: More details here https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html","poster":"UdayOgra","timestamp":"1643103960.0","comment_id":"532018","upvote_count":"4"},{"poster":"narenkd","content":"Correct Answer is \"B\"","comment_id":"426403","upvote_count":"1","timestamp":"1636079040.0"},{"timestamp":"1635876660.0","comment_id":"351962","upvote_count":"1","content":"Answer: B","poster":"VAG1595"},{"comment_id":"173637","poster":"saeidp","timestamp":"1635525180.0","content":"B is correct","upvote_count":"1"},{"content":"\"B\" After enabling DynamoDB Streams, go to Lambda console and enable source mapping","timestamp":"1635505020.0","upvote_count":"2","comment_id":"154360","poster":"WilsonNF"},{"poster":"Sakarai","timestamp":"1635303000.0","comment_id":"141306","content":"Correct Ans : B","upvote_count":"1"},{"timestamp":"1635225300.0","upvote_count":"2","content":"B. Configure event source mapping for the Lambda function","poster":"wackyGuru","comment_id":"139925"},{"upvote_count":"1","content":"B. Configure event source mapping for the Lambda function","comment_id":"139182","poster":"wackyGuru","timestamp":"1635098400.0"},{"content":"Resp: B","poster":"Scarback","upvote_count":"1","comment_id":"127984","timestamp":"1634788860.0"},{"content":"B. Configure event source mapping for the Lambda function","comment_id":"67697","upvote_count":"1","timestamp":"1634582520.0","poster":"kinetic1g"},{"upvote_count":"3","content":"B. Configure event source mapping for the Lambda function","poster":"awscertified","timestamp":"1633957740.0","comment_id":"46933"},{"content":"https://docs.aws.amazon.com/en_us/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html","comment_id":"36573","timestamp":"1633926120.0","comments":[{"comment_id":"220037","timestamp":"1635737640.0","upvote_count":"1","content":"\"Enter the following command to create the trigger. Replace streamARN with the actual stream ARN.\n\naws lambda create-event-source-mapping \\\n --region us-east-1 \\\n --function-name publishNewBark \\\n --event-source streamARN \\\n --batch-size 1 \\\n --starting-position TRIM_HORIZON\"","poster":"ucsdmiami2020"}],"poster":"doofus","upvote_count":"3"},{"upvote_count":"2","poster":"pooseelover","timestamp":"1633643340.0","comment_id":"29532","content":"An event source mapping is an AWS Lambda resource that reads from an event source and invokes a Lambda function. You can use event source mappings to process items from a stream or queue in services that don't invoke Lambda functions directly."},{"upvote_count":"3","content":"Ans: B","poster":"Dev1","timestamp":"1632730740.0","comment_id":"26171"}],"question_id":279,"question_images":[],"question_text":"A Developer has been asked to create an AWS Lambda function that is triggered any time updates are made to items in an Amazon DynamoDB table. The function has been created, and appropriate permissions have been added to the Lambda execution role. Amazon DynamoDB streams have been enabled for the table, but the function is still not being triggered.\nWhich option would enable DynamoDB table updates to trigger the Lambda function?","answers_community":["B (100%)"],"answer_ET":"B","answer":"B","answer_images":[],"answer_description":"","exam_id":25,"timestamp":"2019-08-30 10:07:00","unix_timestamp":1567152420},{"id":"ti4u0FJ1mzSMxpdH0d3E","unix_timestamp":1669188720,"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/88419-exam-aws-certified-developer-associate-topic-1-question-350/","answer_description":"","question_text":"A developer wants to reduce risk when deploying a new version of an existing AWS Lambda function. To test the Lambda function, the developer needs to split the traffic between the existing version and the new version of the Lambda function.\n\nWhich solution will meet these requirements?","question_id":280,"discussion":[{"upvote_count":"6","poster":"mrbig00","content":"Selected Answer: B\nB. Create a function alias. Configure the alias to split the traffic between the two versions of the Lambda function.\n\nTo split traffic between the existing and new versions of a Lambda function, the developer can create a function alias and configure it to split the traffic between the two versions. A function alias is a pointer to a specific version of a Lambda function, and can be used to route traffic to the appropriate version. The developer can use the AWS Management Console or the AWS CLI to create and configure the function alias.","comment_id":"744868","timestamp":"1671006000.0"},{"upvote_count":"3","content":"Selected Answer: B\n100% B is the correct response","timestamp":"1673865000.0","poster":"hpipit","comment_id":"777553"},{"poster":"k1kavi1","comment_id":"728062","timestamp":"1669539480.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html","upvote_count":"2"},{"upvote_count":"2","comment_id":"728028","poster":"kapil206001","timestamp":"1669536480.0","content":"B\nUse routing configuration on an alias to send a portion of traffic to a second function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version.\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html"},{"upvote_count":"2","poster":"michaldavid","comment_id":"727595","content":"Selected Answer: B\nB for me","timestamp":"1669475700.0"},{"comment_id":"726308","poster":"DrCloud","comments":[{"comment_id":"818980","upvote_count":"2","timestamp":"1677141480.0","poster":"DrCloud","content":"Correct Ans: B. \nD is not correct. Sorry about it!"}],"timestamp":"1669338540.0","content":"Ans: D\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html\nYou can use versions to manage the deployment of your functions. For example, you can publish a new version of a function for beta testing without affecting users of the stable production version. Lambda creates a new version of your function each time that you publish the function. The new version is a copy of the unpublished version of the function.\nNote:\nLambda doesn't create a new version if the code in the unpublished version is the same as the previous published version.","upvote_count":"1"},{"content":"It is B","comment_id":"724967","timestamp":"1669188720.0","upvote_count":"1","poster":"absolutic"}],"answers_community":["B (100%)"],"answer":"B","isMC":true,"timestamp":"2022-11-23 08:32:00","choices":{"A":"Configure a weighted routing policy in Amazon Route 53. Associate the versions of the Lambda function with the weighted routing policy.","B":"Create a function alias. Configure the alias to split the traffic between the two versions of the Lambda function.","C":"Create an Application Load Balancer (ALB) that uses the Lambda function as a target. Configure the ALB to split the traffic between the two versions of the Lambda function.","D":"Create the new version of the Lambda function as a Lambda layer on the existing version. Configure the function to split the traffic between the two layers."},"answer_images":[],"answer_ET":"B","exam_id":25}],"exam":{"isMCOnly":true,"isBeta":false,"id":25,"name":"AWS Certified Developer Associate","isImplemented":true,"provider":"Amazon","lastUpdated":"11 Apr 2025","numberOfQuestions":443},"currentPage":56},"__N_SSP":true}