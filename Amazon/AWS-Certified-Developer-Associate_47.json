{"pageProps":{"questions":[{"id":"chmTgtOjrW5oFimIGWaJ","question_images":[],"isMC":true,"answer":"A","answer_description":"","unix_timestamp":1669298880,"discussion":[{"timestamp":"1670934480.0","poster":"mrbig00","content":"Selected Answer: A\nThe correct answer is the S3 object key only. When using SSE-S3 to encrypt objects in Amazon S3, the encryption keys are managed by Amazon S3. To decrypt and download encrypted objects using the GetObject API call, the developer only needs to provide the S3 object key. Amazon S3 will use the encryption keys that it manages to decrypt the object before returning it to the developer. Providing the encryption key or the ARN of the AWS KMS key would not be necessary because the encryption keys are managed by Amazon S3. Providing a salted HMAC value of the encryption key would not be possible because the developer does not have access to the encryption key.","upvote_count":"7","comment_id":"743993"},{"upvote_count":"1","comment_id":"939057","poster":"rcaliandro","content":"Selected Answer: A\nAbsolutely A because the default server-side encryption SSE-S3 is managed by AWS and we dont't have to worry about manage or send keys to the server. The only thing we need to get an object from S3 is the object key (let's say: bucket + path + objectName). AWS will decrypt the object for us.","timestamp":"1688125500.0"},{"content":"Selected Answer: A\nAgree with A","poster":"k1kavi1","upvote_count":"1","comment_id":"732496","timestamp":"1669890600.0"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html\nEncryption request headers, like x-amz-server-side-encryption, should not be sent for GET requests if your object uses server-side encryption with KMS keys (SSE-KMS) or server-side encryption with Amazon S3–managed encryption keys (SSE-S3). If your object does use these types of keys, you’ll get an HTTP 400 BadRequest error.","timestamp":"1669889820.0","poster":"SoMaL69","comment_id":"732487"},{"timestamp":"1669298880.0","comment_id":"725901","content":"Ans: A\nAsk is, \"LEAST amount of information\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html#API_GetObject_Examples\nhttps://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/get-object.html\nSample Request:\nThe following request returns the object my-image.jpg.\n GET /my-image.jpg HTTP/1.1\n Host: bucket.s3.<Region>.amazonaws.com\n Date: Mon, 3 Oct 2016 22:32:00 GMT\n Authorization: authorization string\n\nThe following example uses the get-object command to download an object from Amazon S3:\naws s3api get-object --bucket text-content --key dir/my_images.tar.bz2 my_images.tar.bz2","poster":"DrCloud","upvote_count":"2"}],"question_id":231,"choices":{"B":"The S3 object key and the encryption key","D":"The S3 object key and a randomly salted Hash-based Message Authentication Code (HMAC) value of the encryption key","A":"The S3 object key only","C":"The S3 object key and the Amazon Resource Name (ARN) of the AWS Key Management Service (AWS KMS) key"},"answers_community":["A (100%)"],"timestamp":"2022-11-24 15:08:00","answer_images":[],"topic":"1","exam_id":25,"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/88536-exam-aws-certified-developer-associate-topic-1-question-306/","question_text":"A developer uses server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to store data in Amazon S3. The developer needs to decrypt and download the encrypted objects by using the GetObject API call.\n\nWhat is the LEAST amount of information that the developer must provide in the API call to meet this requirement?"},{"id":"PYvFdaxVSoTRYtFT6kHs","unix_timestamp":1669301400,"question_images":[],"discussion":[{"content":"Selected Answer: D\nSo A and C have s3:DeleteObject as allowed method and those are to be excluded. The B has the method s3:CreateBucket and s3:GetBucketLocation that allow the application to create a bucket and to get location but we can't add or download elements from the bucket. So, also the B is incorrect.\nThe correct answer iisssssss................. D! \nWe do need s3:GetObject to retrive elements from the bucket and s3:PutObject to upload the object on the cloud","comment_id":"939061","timestamp":"1688126040.0","poster":"rcaliandro","upvote_count":"3"},{"poster":"mrbig00","timestamp":"1670934600.0","upvote_count":"2","comment_id":"743996","content":"Selected Answer: D\nTo ensure that users can create but not remove files from the Amazon S3 bucket, the developer should apply the following IAM permissions to the users:\n\ns3:PutObject: This permission allows users to add objects to the bucket.\ns3:GetObject: This permission allows users to download the objects in the bucket.\nThe developer should not grant the s3:DeleteObject permission, which would allow users to remove objects from the bucket. By not granting this permission, the company can maintain copies of all files uploaded by users for compliance purposes, while still allowing users to access the data through the application."},{"poster":"k1kavi1","upvote_count":"2","content":"Selected Answer: D\nChoosing D","timestamp":"1669469280.0","comment_id":"727541"},{"timestamp":"1669467060.0","content":"Selected Answer: D\nD is correct","comment_id":"727516","poster":"michaldavid","upvote_count":"1"},{"comment_id":"725967","timestamp":"1669301400.0","poster":"DrCloud","content":"Ans: D\nTo accomplish: \"can create but not remove files\" \n-- Need: \"Put Object\" \n-- Don't need: \"Delete Object\"\nhttps://docs.aws.amazon.com/cli/latest/reference/s3api/put-object.html","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/amazon/view/88545-exam-aws-certified-developer-associate-topic-1-question-307/","answer":"D","isMC":true,"answer_description":"","answer_ET":"D","topic":"1","exam_id":25,"question_text":"A developer is managing an application that uploads user files to an Amazon S3 bucket named companybucket. The company wants to maintain copies of all the files uploaded by users for compliance purposes, while ensuring users still have access to the data through the application.\n\nWhich IAM permissions should be applied to users to ensure they can create but not remove files from the bucket?","answer_images":[],"choices":{"C":"","D":"","B":"","A":""},"question_id":232,"timestamp":"2022-11-24 15:50:00","answers_community":["D (100%)"]},{"id":"770sb9rmDGpBnQtfNl8O","answer":"A","choices":{"A":"Read the table by using eventually consistent reads.","C":"Read the table by using transactional reads.","B":"Read the table by using strongly consistent reads.","D":"Read the table by using strongly consistent PartiQL queries."},"question_id":233,"isMC":true,"question_images":[],"answer_description":"","unix_timestamp":1669302900,"exam_id":25,"timestamp":"2022-11-24 16:15:00","answer_ET":"A","answers_community":["A (100%)"],"topic":"1","discussion":[{"poster":"DrCloud","timestamp":"1669302900.0","upvote_count":"12","content":"Ans: A\nKey points: \"Read heavy\", \"access data quickly\", \"can tolerate stale data\"\nTo achieve: \"FEWEST\" possible (RCUs) \n\nFor items up to 4 KB in size, one RCU can perform one strongly consistent read request per second. \nFor items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second. \nTransactional read requests require two RCUs to perform one read per second for items up to 4 KB. \n\nFor example, \na strongly consistent read of an 8 KB item would require two RCUs, \nan eventually consistent read of an 8 KB item would require one RCU, \nand a transactional read of an 8 KB item would require four RCUs.\n\nhttps://aws.amazon.com/dynamodb/pricing/provisioned/","comment_id":"725993"},{"upvote_count":"1","timestamp":"1693944240.0","poster":"Paul_101","content":"Selected Answer: A\nI agree with A","comment_id":"999859"},{"comment_id":"939063","upvote_count":"1","poster":"rcaliandro","content":"Selected Answer: A\nA is the correct answer, I totally agree with you. In order to have a quick access to data and stale data can be tolerate, we can use an eventually consinstent read that will use half of RCUs compared to a strongly consistent read. The strongly consistent read ensure that the user retrive the most recent data even if the case is not updated but require more effort.","timestamp":"1688126400.0"},{"content":"Selected Answer: A\nThe correct solution is option A. Reading the table by using eventually consistent reads will meet the requirements with the fewest possible read capacity units (RCUs). When using eventually consistent reads, DynamoDB returns the latest data that it has in its cache, which may be slightly out of date. This is acceptable in this case because the microservice can tolerate stale data. Using eventually consistent reads will reduce the number of RCUs required to read the table because DynamoDB does not need to wait for all copies of the data to be updated before returning the data. Using strongly consistent reads or transactional reads would require more RCUs because these read modes require DynamoDB to wait for all copies of the data to be updated before returning the data. Using PartiQL queries would not be necessary because the requirements can be met using regular read operations.","comment_id":"743997","poster":"mrbig00","upvote_count":"1","timestamp":"1670934660.0"},{"timestamp":"1669477200.0","content":"Selected Answer: A\nA. Read the table by using eventually consistent reads.","upvote_count":"1","poster":"k1kavi1","comment_id":"727617"},{"content":"Selected Answer: A\nAAAAAAA","upvote_count":"1","comment_id":"727517","poster":"michaldavid","timestamp":"1669467240.0"}],"question_text":"A company is developing a microservice that will manage customer account data in an Amazon DynamoDB table. Insert, update, and delete requests will be rare. Read traffic will be heavy. The company must have the ability to access customer data quickly by using a customer ID. The microservice can tolerate stale data.\n\nWhich solution will meet these requirements with the FEWEST possible read capacity units (RCUs)?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/88550-exam-aws-certified-developer-associate-topic-1-question-308/"},{"id":"KHRUR5HmrMncngdrs4uW","url":"https://www.examtopics.com/discussions/amazon/view/88555-exam-aws-certified-developer-associate-topic-1-question-309/","answer_ET":"B","timestamp":"2022-11-24 17:16:00","question_id":234,"question_text":"A developer deploys an ecommerce application on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group. The EC2 instances are based on an Amazon Machine Image (AMI) that uses an Amazon Elastic Block Store (Amazon EBS) root volume. After deployment, the developer notices that a third of the instances seem to be idle. These instances are not receiving requests from the load balancer. The developer verifies that all the instances are registered with the load balancer. The developer must implement a solution to allow the EC2 instances to receive requests from the load balancer.\n\nWhich action will meet this requirement?","exam_id":25,"answer_images":[],"unix_timestamp":1669306560,"answer":"B","answer_description":"","question_images":[],"topic":"1","answers_community":["B (64%)","C (36%)"],"isMC":true,"discussion":[{"comment_id":"744000","content":"Selected Answer: B\nThe correct action is to enable all Availability Zones for the ALB. This will allow the ALB to distribute requests across all registered instances, regardless of which Availability Zone they are in. By enabling all Availability Zones, the developer can ensure that the EC2 instances that are not receiving traffic will start receiving requests from the ALB. Reregistering the failed instances with the ALB or using the instance refresh feature to redeploy the EC2 Auto Scaling group would not solve the problem because the instances are already registered and the instance refresh feature does not support AMIs with EBS root volumes. Restarting the EC2 instances that are not receiving traffic would not solve the problem because the instances are registered with the ALB and are not failing.","upvote_count":"15","timestamp":"1670934840.0","comments":[{"upvote_count":"1","content":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-subnets.html","comment_id":"776128","poster":"vn_thanhtung","timestamp":"1673751120.0"},{"upvote_count":"1","comment_id":"765950","timestamp":"1672853580.0","content":"hey qq please, if all the AZ were not enabled will the instances be registered still please?","poster":"by116549"},{"poster":"AgboolaKun","upvote_count":"1","comment_id":"913092","content":"Why is enabling AZs necessary here? I don't get it. \n\nThe issue is with the unhealthy instances and instance refresh as explained in this article: https://aws.amazon.com/blogs/compute/introducing-instance-refresh-for-ec2-auto-scaling/ will resolve the issue.","timestamp":"1685751180.0"}],"poster":"mrbig00"},{"timestamp":"1676227860.0","poster":"pancman","comment_id":"806688","content":"Selected Answer: C\nC is correct. The instances that are not receiving traffic are most likely in an unhealthy state. So they need to be replaced with healthy instances. This can be accomplished by an instance refresh.","upvote_count":"5"},{"upvote_count":"1","content":"C\n\n\"Amazon EC2 Auto Scaling is used for a wide variety of workload types and applications. EC2 Auto Scaling helps you maintain application availability through a rich feature set. This feature set includes integration into Elastic Load Balancing, automatically replacing unhealthy instances, balancing instances across Availability Zones, provisioning instances across multiple pricing options and instance types, dynamically adding and removing instances, and more.\"\n\nhttps://aws.amazon.com/blogs/compute/introducing-instance-refresh-for-ec2-auto-scaling/","comment_id":"1087172","poster":"odisor","timestamp":"1701645420.0"},{"timestamp":"1688126880.0","upvote_count":"1","content":"Selected Answer: B\nAlso for me B is the correct answer. Since the EC2 instances are correctly running, they are correctly attacched to the ALB and 1/3 of the instances don't receive traffic, it seems that one Availability Zone is not configured for the Application Load Balancer. To solve the problem it is possible to add the AZ in the ALB Configurations.","comment_id":"939066","poster":"rcaliandro"},{"poster":"k1kavi1","upvote_count":"2","timestamp":"1669890900.0","comment_id":"732502","content":"Selected Answer: C\nAgreed"},{"upvote_count":"2","content":"Selected Answer: C\nCCCCCCC","comment_id":"727518","poster":"michaldavid","timestamp":"1669467360.0"},{"comment_id":"726034","upvote_count":"2","timestamp":"1669306560.0","content":"Ans: C\nhttps://aws.amazon.com/blogs/compute/introducing-instance-refresh-for-ec2-auto-scaling/","poster":"DrCloud","comments":[{"timestamp":"1697881560.0","upvote_count":"1","poster":"kaes","content":"\"Today, we are launching Instance Refresh. This is a new feature in EC2 Auto Scaling that enables automatic deployments of instances in Auto Scaling Groups (ASGs), in order to release new application versions or make infrastructure updates.\"\nThe purpose of it is not to restart unhealthy instances but to release updates!\nSo B is correct answer","comment_id":"1049390"}]}],"choices":{"C":"Use the instance refresh feature to redeploy the EC2 Auto Scaling group.","D":"Restart the EC2 instances that are not receiving traffic.","B":"Enable all Availability Zones for the ALB.","A":"Reregister the failed instances with the ALB."}},{"id":"9PG7wsawhD9DtJCfPPnU","answer_ET":"B","topic":"1","question_images":[],"isMC":true,"timestamp":"2020-08-12 14:38:00","exam_id":25,"choices":{"D":"Use Amazon EBS and file synchronization software to achieve eventual consistency among the Auto Scaling group.","C":"Use instance storage and share it between instances launched from the same Amazon Machine Image (AMI).","B":"Use Amazon S3 and rearchitect the application so all uploads are placed in S3.","A":"Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance on boot."},"question_text":"A Developer is migrating an on-premises application to AWS. The application currently takes user uploads and saves them to a local directory on the server. All uploads must be saved and made immediately available to all instances in an Auto Scaling group.\nWhich approach will meet these requirements?","unix_timestamp":1597235880,"answer_images":[],"answers_community":["B (93%)","7%"],"url":"https://www.examtopics.com/discussions/amazon/view/28267-exam-aws-certified-developer-associate-topic-1-question-31/","question_id":235,"answer_description":"","answer":"B","discussion":[{"poster":"RicardoD","comment_id":"243734","content":"B is the answer\nBest approach is to have everything in S3, so any instance spun into the ASG can have access to the uploaded data","timestamp":"1634954760.0","upvote_count":"23"},{"comment_id":"423771","upvote_count":"22","timestamp":"1636153800.0","content":"EFS is the only true answer !","poster":"yaizkazani"},{"poster":"sumanshu","content":"Selected Answer: B\nA) Eliminated - While EBS can be used for persistent storage, it is not designed to be shared between multiple instances easily","comments":[{"comments":[{"timestamp":"1734344220.0","comment_id":"1327270","upvote_count":"1","poster":"sumanshu","content":"D) Eliminated - While EBS is suitable for persistent storage, synchronizing EBS volumes between multiple instances can be complex and would require additional file synchronization software"}],"upvote_count":"1","comment_id":"1327269","poster":"sumanshu","content":"C) Eliminated- Instance storage (also known as ephemeral storage) is local to each instance and is not shared between instances. When an EC2 instance in an Auto Scaling group is terminated and replaced, any data in instance storage is lost","timestamp":"1734344220.0"}],"comment_id":"1327267","upvote_count":"2","timestamp":"1734344160.0"},{"upvote_count":"1","comment_id":"1310475","timestamp":"1731397560.0","poster":"JonasKahnwald","content":"Selected Answer: B\nAmazon S3 is designed for high availability and durability, and it allows multiple instances to access the same data simultaneously. By rearchitecting the application to upload files directly to S3, you ensure that all instances in the Auto Scaling group can access the uploads immediately."},{"timestamp":"1701838440.0","content":"Although EFS would best option but since its not here S3 is the most correct among the rest","upvote_count":"1","poster":"Akbar263","comment_id":"1089017"},{"content":"Selected Answer: B\nUsing Amazon EBS with file synchronization software would require additional complexity and may not provide the immediate consistency required for uploads to be available to all instances in real-time. It could lead to synchronization delays and potential issues in an Auto Scaling group.","comment_id":"1022817","upvote_count":"1","poster":"sara_exam_topics","timestamp":"1696223820.0"},{"timestamp":"1687889160.0","comment_id":"935684","poster":"rcaliandro","content":"Selected Answer: B\nB is the right answer, be careful that the EC2 instances have a role with the right policy to write and read content from the specific bucket","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\nS3 seems logical here","poster":"_charissi","comment_id":"904229","timestamp":"1684774500.0"},{"upvote_count":"3","poster":"imvb88","timestamp":"1684138560.0","content":"Selected Answer: B\nOf course B is obvious. But why are others wrong? \nA - need to reboot instance? -> out\nC - instance storage cannot shared between instances\nD - only achieve eventual consistency while the requirement is \"immediately available\"","comment_id":"898131"},{"poster":"taliakoren","timestamp":"1683690960.0","upvote_count":"2","content":"Selected Answer: D\nAmazon Elastic Block Store (Amazon EBS)","comment_id":"893580"},{"upvote_count":"1","content":"Selected Answer: B\nthe best approach to meet the requirements of the migration would be to use Amazon S3 and rearchitect the application so that all uploads are placed in S3","poster":"MrTee","comment_id":"878294","timestamp":"1682251620.0"},{"timestamp":"1681750740.0","poster":"RuTech","upvote_count":"1","comment_id":"872904","content":"Selected Answer: B\nI am going with B"},{"content":"Selected Answer: B\nA lot of effort on D, B is scalable and more secure for future deployments","comment_id":"870921","upvote_count":"1","timestamp":"1681562820.0","poster":"OtavioC"},{"content":"I guess D is the answer with EBS multi-attach: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\n\nVolume can be shared ASAP to all EC2 instances in the ASG.","upvote_count":"1","timestamp":"1680893640.0","poster":"Yahya_Badawy","comment_id":"864180"},{"timestamp":"1679353980.0","poster":"qiaoli","upvote_count":"1","comment_id":"845334","content":"Selected Answer: B\nThe best solution should be EFS. But there is no EFS in the option. B is the next best.\n\nThis is my thoughts\nWith S3, there are efforts to modify the code, for new files, S3 has strong consistence. But for update/delete files, S3 has eventually consistence only.\n\nFor D, data sync application could not make sure strong consistence. And there are a lot efforts for configuration. Advantage is that, there is no need to modify the code."},{"timestamp":"1679061600.0","comment_id":"842031","content":"Selected Answer: B\nB. Use Amazon S3 and rearchitect the application so all uploads are placed in S3.\n\nStoring user uploads in a local directory on the server is not a scalable or reliable solution, especially in a distributed environment with an Auto Scaling group. Amazon S3 provides a highly scalable and durable object storage service that is designed for storing and retrieving any amount of data from anywhere. By rearchitecting the application to use S3, all uploads can be saved to S3 and made immediately available to all instances in the Auto Scaling group. This approach provides scalability, durability, and availability, which are all important considerations when migrating to the cloud.","poster":"dp8719823","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nThe eventual consistency rules out D.","poster":"alamamkota123","comment_id":"776246","timestamp":"1673770320.0"},{"poster":"fabriciollf","upvote_count":"1","timestamp":"1670888340.0","comment_id":"743418","content":"Selected Answer: B\nB is the correct answer"},{"comments":[{"comment_id":"720174","content":"i was thinking about D because it has to be stored on local server. but that's how it currently operates, that's not the requirement. the requirement is that it becomes avaliable immidately to everyone and it's not D because file syronization is eventually consistant","timestamp":"1668656160.0","poster":"dark_cherrymon","upvote_count":"1"}],"poster":"dark_cherrymon","timestamp":"1668655800.0","upvote_count":"3","content":"Selected Answer: B\nwhy do i keep hearing D, it's B ofcourse.","comment_id":"720171"},{"upvote_count":"1","timestamp":"1667005920.0","content":"Selected Answer: B\nAnswer has to be B","poster":"Obosie","comment_id":"706863"},{"timestamp":"1662178500.0","content":"it seem it should be EFS or FSx.. but if the option is like given i vote to choose B","poster":"sindra","comment_id":"658051","upvote_count":"3"},{"comment_id":"546080","content":"Selected Answer: B\nAnswer: B.","upvote_count":"2","timestamp":"1644698340.0","poster":"netk"},{"poster":"JP_PA","timestamp":"1643732760.0","upvote_count":"2","content":"Selected Answer: B\nANS: B","comment_id":"538035"},{"comment_id":"503018","content":"Selected Answer: B\nB is the answer","timestamp":"1639669140.0","upvote_count":"2","poster":"reve666"},{"upvote_count":"1","comment_id":"353049","content":"Answer: B","poster":"VAG1595","timestamp":"1635728460.0"},{"poster":"Pavan_Nagineni","comment_id":"343791","upvote_count":"2","content":"Better to go with B. Use Amazon S3 and rearchitect the application so all uploads are placed in S3.\nEven though you could do EBS attachment to ASG launch config userdata for ec2 instances going to serve , But you need to select the ASG in single AZ where your EBS is located otherwise it will not work since EBS is AZ locked.","timestamp":"1635500520.0"},{"upvote_count":"2","poster":"weril","timestamp":"1634764740.0","comment_id":"215558","content":"for me it should be done by EFS but there is no such a option. In my opinion I will stick with B"},{"comment_id":"194206","timestamp":"1634620440.0","content":"B is correct","upvote_count":"1","poster":"Chinta"},{"comments":[{"upvote_count":"2","comment_id":"164896","content":"for overwrites not puts","poster":"awswannabe","timestamp":"1634222580.0"},{"content":"Not anymore. Since Dec 2020 S3 is now strongly consistent.","timestamp":"1648255320.0","comment_id":"575313","upvote_count":"1","poster":"altonh"},{"timestamp":"1634747640.0","comment_id":"212348","content":"new uploads will not be available for other instances currently running, only for new ones, so it's B","poster":"Maicon","upvote_count":"2"}],"comment_id":"160552","upvote_count":"1","timestamp":"1634143020.0","content":"Considering \"Immediately Available\", I would go with A\nS3 is eventually consistent.","poster":"FHS"},{"upvote_count":"1","timestamp":"1633830240.0","comment_id":"160021","content":"answer: B","poster":"requiem"},{"timestamp":"1632737760.0","poster":"jodeepak","comment_id":"159666","content":"Answer B","upvote_count":"3"},{"content":"B. Use Amazon S3 and rearchitect the application so all uploads are placed in S3.","upvote_count":"3","comment_id":"157090","poster":"WilsonNF","timestamp":"1632688020.0"},{"upvote_count":"3","comment_id":"157008","poster":"saeidp","content":"B is correct","timestamp":"1632338880.0"},{"poster":"Chintoo","content":"Answer should be B","timestamp":"1632199200.0","comment_id":"156412","upvote_count":"2"}]}],"exam":{"isBeta":false,"id":25,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true,"numberOfQuestions":443,"name":"AWS Certified Developer Associate","provider":"Amazon"},"currentPage":47},"__N_SSP":true}