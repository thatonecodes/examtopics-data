{"pageProps":{"questions":[{"id":"Zgh8bfS6eyq4PMUpSZZa","unix_timestamp":1662140640,"answer":"AB","question_id":46,"question_images":[],"choices":{"A":"Create a new Elastic Beanstalk environment that connects to the DB instance.","C":"Use the Elastic Beanstalk CLI to decouple the DB instance.","D":"Use the AWS CLI to decouple the DB instance.","E":"Modify the current Elastic Beanstalk environment to connect to the DB instance.","B":"Create a new DB instance from a snapshot of the previous DB instance."},"answers_community":["AB (33%)","BE (30%)","13%","Other"],"timestamp":"2022-09-02 19:44:00","exam_id":25,"topic":"1","answer_description":"","answer_ET":"AB","question_text":"A company is using AWS Elastic Beanstalk to deploy a three-tier application. The application uses an Amazon RDS DB instance as the database tier. The company wants to decouple the DB instance from the Elastic Beanstalk environment.\nWhich combination of steps should a developer lake to meet this requirement? (Choose two.)","discussion":[{"content":"Correct answer A and B.\nReference-https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/","timestamp":"1662140640.0","poster":"VivekSolutionArchitect","comment_id":"657671","comments":[{"timestamp":"1679290200.0","upvote_count":"3","poster":"qiaoli","comment_id":"844560","content":"I think creating and using a new db will lead to data loss because snapshot is never latest data; and possibly downtime during the db cut-over."},{"upvote_count":"6","poster":"Merrick","content":"I cannot find that creating new DB instance from a snapshot(B) in the reference, which is guiding to connect between new EB and old RDS DB instance","timestamp":"1663909800.0","comment_id":"676772"}],"upvote_count":"19"},{"timestamp":"1738995900.0","poster":"apolinho","upvote_count":"1","content":"Selected Answer: AE\nThe best approach to decouple the database is to:\n\n1. Modify the current environment (E) to connect to an external RDS instance.\n\n\n2. Create a new environment (A) if necessary, ensuring the database remains independent.","comment_id":"1353256"},{"poster":"ShiaH","content":"Selected Answer: BE\nThe task is to decouple the 2-tiers which only requires for one to become independent of the other. So creating a snapshot of the current DB and maintaining the current elastic beanstalk instance for the application, so achieve the goal.","upvote_count":"1","timestamp":"1738408380.0","comment_id":"1349835"},{"upvote_count":"1","timestamp":"1738070760.0","poster":"avinashk99","content":"Selected Answer: BE\nCreating a new Elastic Beanstalk environment is not required to decouple the database. The goal is to make the database independent while keeping the current Elastic Beanstalk environment running and connected to the new standalone RDS instance.","comment_id":"1347918"},{"timestamp":"1734264540.0","comment_id":"1326838","content":"Selected Answer: BE\nA) Eliminated - Creating a new Elastic Beanstalk environment is not required to decouple the database. The goal is to make the database independent while keeping the current Elastic Beanstalk environment running and connected to the new standalone RDS instance.","poster":"sumanshu","comments":[{"timestamp":"1734264600.0","poster":"sumanshu","comment_id":"1326839","upvote_count":"2","comments":[{"timestamp":"1734264600.0","comment_id":"1326840","poster":"sumanshu","content":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html","upvote_count":"1"}],"content":"B & E - Correct - After creating the standalone DB instance, the Elastic Beanstalk environment configuration must be updated to connect to the new DB instance. This involves modifying the database endpoint, username, and password in the Elastic Beanstalk environment variables"}],"upvote_count":"2"},{"poster":"thucta96dn","comment_id":"1303850","timestamp":"1730098980.0","upvote_count":"1","content":"Selected Answer: B\nB. Of course"},{"poster":"kishkish","upvote_count":"2","content":"Selected Answer: DE\nThese two steps that would meet the requirement to decouple the DB instance","timestamp":"1708443600.0","comment_id":"1154823"},{"content":"It would be D,E\nThese two steps that would meet the requirement to decouple the DB instance","timestamp":"1708443540.0","poster":"kishkish","comment_id":"1154822","upvote_count":"2"},{"timestamp":"1705509180.0","content":"a and b","poster":"gilleep_17","upvote_count":"1","comment_id":"1125130"},{"timestamp":"1705272180.0","upvote_count":"1","content":"Selected Answer: AB\ni think so AB for me","comment_id":"1122934","poster":"AsmaZoheb"},{"content":"BD, chatGTP 4","timestamp":"1699817700.0","comment_id":"1068827","poster":"hulongdou","upvote_count":"1"},{"timestamp":"1699719540.0","comment_id":"1067955","poster":"kyoharo","upvote_count":"2","content":"Selected Answer: AB\nA,B\n ** Elastic Beanstalk Migration: Decouple RDS ** \n1- Create a snapshot of RDS DB ( as a safeguard)\n2- Go to the RDS console and protect the RDS databse from deletion\n3- Create a new Elastic Beanstalk environement, without RDS, point your application to existing RDS\n4- Perform a CNAME swap (blue/green) or Route 53 updata, confirm working \n5- Terminate the old environment (RDS won't be deleted)\n6- Delete CloudFormation stack (in deletefailed state)"},{"upvote_count":"1","comment_id":"1024465","timestamp":"1696396320.0","poster":"dexdinh91","content":"Selected Answer: CD\nI think CD are correct answer, no need to create and modify the current app"},{"poster":"ja1092m","timestamp":"1693269960.0","upvote_count":"1","content":"Thoughts? https://repos\nt.aws/knowledge-center/decouple-rds-from-beanstalk","comment_id":"992690"},{"comment_id":"988252","poster":"ninomfr64","upvote_count":"1","content":"Selected Answer: CD\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.db.html#using-features.decoupling-config-files.db:~:text=Follow%20these%20steps%20to%20decouple%20the%20database%20from%20your%20Elastic%20Beanstalk%20environment.%20You%20can%20use%20the%20EB%20CLI%20or%20the%20AWS%20CLI%20to%20complete%20the%20steps\n\nFollow these steps to decouple the database from your Elastic Beanstalk environment. You can use the EB CLI or the AWS CLI to complete the steps, thus C and D","timestamp":"1692790920.0"},{"content":"Selected Answer: DE\nFound this in the aws docs: https://aws.amazon.com/about-aws/whats-new/2021/10/aws-elastic-beanstalk-database-decoupling-elastic-beanstalk-environment/\nYou can now decouple db from the beanstock using console or cli. Means we can decouple the db and change previous configuration to point to the decoupled db.","poster":"Magic_Dumpling","timestamp":"1691799480.0","comment_id":"979045","upvote_count":"1"},{"comment_id":"939901","content":"Selected Answer: AB\nWe can protect RDS DB by checking delete protection option. Once DB is protected then delete previous environment and create new one. Connect to old DB.","timestamp":"1688213100.0","poster":"Yasser001","upvote_count":"2"},{"content":"Selected Answer: AB\nTo be honest I am not hundred percent sure abot this question but I opted for B and E. But, doing some research, it seems that it's not possible to modify the current beanstalk enviroment and we need to create a new one from scratch. \nSo, A and B would make more sense for me.\n\n(https://repost.aws/knowledge-center/decouple-rds-from-beanstalk)","upvote_count":"2","timestamp":"1687878840.0","poster":"rcaliandro","comment_id":"935530"},{"poster":"MrTee","comment_id":"877837","content":"Selected Answer: BE\nB. Create a new DB instance from a snapshot of the previous DB instance. This approach allows the developer to create a new Amazon RDS DB instance that is independent of the Elastic Beanstalk environment. The developer can create a snapshot of the previous DB instance and use it to create a new DB instance with the same data.\n\nE. Modify the current Elastic Beanstalk environment to connect to the new DB instance. This approach allows the developer to update the Elastic Beanstalk environment to use the new, independent DB instance. The developer can modify the environment’s configuration to specify the connection details for the new DB instance.","upvote_count":"4","timestamp":"1682221140.0"},{"poster":"nearavenac","comment_id":"862528","content":"Selected Answer: AC\nCorrect answer A and C.","timestamp":"1680734460.0","upvote_count":"1"},{"comment_id":"844559","content":"Selected Answer: AC\nI vote for AC. referring to this blog https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\nThe process is 1> create a new environment with application only and connect to the old db (option A); 2> migrate traffic to the new environment; 3> decouple the db from the old environment (option C); 4> delete the old environment;\nB. is not correct, because if to create a new db, there their will be data loss, between the time when snapshot was taken and the the time when the new db is online.\nE. is not correct because there is no new db created.\nD. I am not quite sure if aws cli is useful to decouple the current database from the old environment.","poster":"qiaoli","timestamp":"1679290020.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1677109380.0","poster":"Kirkster","comment_id":"818552","content":"Selected Answer: AC\nIt's a crappy question, but the two essential steps are to create a new Beanstalk environment that points to the existing database, then decouple the database from the original Beanstalk environment. Yes, the support page says to make a snapshot, but that's in case you mess up and forget to change the RDS database deletion policy to not delete the database. You also need to do a blue-green deployment so that traffic hits the new environment (which points to the existing database but isn't coupled to it). You can decouple the database using the Beanstalk console, the AWS CLI or the eb CLI. I think the eb CLI is easiest, but sadly this means there is more than one correct answer here."},{"comment_id":"806963","timestamp":"1676250240.0","upvote_count":"1","poster":"may2021_r","content":"Option A creates a new Elastic Beanstalk environment that is configured to connect to the existing RDS instance. This separates the RDS instance from the original environment, allowing it to be managed separately.\nOption B creates a new DB instance from a snapshot of the previous DB instance. This ensures that the new instance has the same data as the original instance, so that the application can continue to function as expected."},{"content":"Selected Answer: AB\nA and B","comment_id":"804756","poster":"Krt5894","upvote_count":"1","timestamp":"1676060880.0"},{"poster":"ayoubmk","comment_id":"779167","content":"Selected Answer: AB\nSteps :\nCreate a snapshot of your RDS DB and create a new RDS DB from it. This does not retain the security group, parameter group, or options of your DB. So it might be preferable to create a new empty RDS DB where you can configure everything and then restore the contents from a DB dump like mysqldump.\n\nSave a configuration of your Elastic Beanstalk env and download it from your S3 EBS bucket under resources/templates/<your-app-name>/.\n\nModify the config to remove all references to RDS, and upload the modified file to your S3 bucket.\n\nCreate a new environment from the saved configuration. You should now have an env without a managed RDS DB. Some settings might not be carried over in this process. For example, I had to reconfigure the load balancer for my new env.\n\nOnce everything is working in the new env you can use \"Swap Environment URLs\" or point your DNS record to the new load balancer.","upvote_count":"1","timestamp":"1673978340.0"},{"upvote_count":"3","content":"Correct Answer is And B since below are the steps for decouple..\nCreate a snapshot of RDS DB (as a\nsafeguard)\n2. Go to the RDS console and protect\nthe RDS database from deletion\n3. Create a new Elastic Beanstalk\nenvironment, without RDS, point your\napplication to existing RDS\n4. perform a CNAME swap (blue/green)\nor Route 53 update, confirm working\n5. Terminate the old environment (RDS\nwon’t be deleted)\n6. Delete CloudFormation stack (in\nDELETE_FAILED state)","comment_id":"766019","timestamp":"1672859700.0","comments":[{"comment_id":"988238","timestamp":"1692790320.0","content":"Your procedure doen't include \"Create a new DB instance from a snapshot of the previous DB instance\" that is option B","upvote_count":"2","poster":"ninomfr64"}],"poster":"ShriniW"},{"comments":[{"upvote_count":"1","comment_id":"766021","poster":"ShriniW","comments":[{"comment_id":"809964","poster":"zek","content":"This does not make sense ..","timestamp":"1676493480.0","upvote_count":"3"}],"timestamp":"1672859820.0","content":"actually we create a sanpshot , we create new Beanstalk without RDS and then point new beanstalk to older RDS.Hence Aand B"}],"comment_id":"765211","timestamp":"1672804020.0","poster":"efex","upvote_count":"1","content":"Selected Answer: BE\nThe answer should B-E. \nWe can update the current environment parameter and can connect the new Database. We don't need to create a new environment."},{"content":"the best answer ois A&B","upvote_count":"1","poster":"ramireze","timestamp":"1665694680.0","comment_id":"694280"},{"content":"Selected Answer: AB\nA and B is the right answer\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/","poster":"haazybanj","upvote_count":"1","comment_id":"689514","timestamp":"1665247440.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/79519-exam-aws-certified-developer-associate-topic-1-question-14/","answer_images":[],"isMC":true},{"id":"M5dKD78kmLQBaQixO4Yh","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/78836-exam-aws-certified-developer-associate-topic-1-question-140/","question_images":[],"topic":"1","unix_timestamp":1661991000,"choices":{"B":"For the primary key of the table, specify the unique identifier as the partition key. Create a local secondary index (LSI) based on the email address.","C":"For the primary key of the table, specify the email address as the partition key and specify the unique identifier as the sort key.","D":"For the primary key of the table, specify the unique identifier as the partition key. Create a global secondary index (GSI) based on the email address.","A":"For the primary key of the table, specify the unique identifier as the partition key and specify the email address as the sort key."},"answer_description":"","question_text":"A developer is designing an Amazon DynamoDB table for an application. The application will store user information that includes a unique identifier and an email address for each user. The application must be able to query the table by using either the unique identifier or the email address.\nHow should the developer design the DynamoDB table to meet these requirements?","timestamp":"2022-09-01 02:10:00","answer":"D","question_id":47,"answers_community":["D (100%)"],"answer_ET":"D","answer_images":[],"exam_id":25,"discussion":[{"content":"Selected Answer: D\nThe main point in the question is that they want to query table by using EITHER unique identifier OR email address.\n\nA and B is not enough if we only has one primary key in a table\n\nAssump that we configure base table has partition key is \"unique identifier\" so that we can query base on this key\n\nFor B), the LSI index must have the same partition key with base table. So in case the assumption above, you cannot query only email address\n\nFor D) the GSI can have different partition key and sort key from base table. So we can create partition key is \"email address\" and sort key is \"unique identifier\" for GSI. Then we can use only \"email address\" for query","upvote_count":"8","timestamp":"1670556900.0","poster":"aws_leo","comment_id":"739711"},{"poster":"SD_CS","comment_id":"1141816","timestamp":"1707199140.0","upvote_count":"1","content":"Selected Answer: D\nThe partition key must be also decided by the query pattern. The query can happen with the unique if or the email address. So its best to keep the unique id as the partition key of the primary table. However this will not solve the requirement of querying with the email address. So create a GSI with Email as the partition key. This cannot be LSI because the LSI can not have a different partition key, just a different sort key."},{"content":"Selected Answer: D\nTo design the indexes we have to study the table. The table is composed by an unique identifier and by an email. We can notice that both of the fields are unique and are optimal to partition the table. So, in both cases we don't need a sort key because will be unuseful (doesn't make sense to order only one row).\nSo we can create the primary key using the userID as partition key.\nRegarding the second query, doesn't make sense to create a LSI because the partion key is shared with the primary key. Instead, we have to create a GSI by using the email address (also in this case we don't need the sort key because the email is unique). So, D is the correct answer","upvote_count":"1","poster":"rcaliandro","comment_id":"936960","timestamp":"1687973460.0"},{"upvote_count":"1","comment_id":"881640","timestamp":"1682514420.0","content":"Selected Answer: D\nD - Query for LSI will require to specify the partition key with email since its an index local to that partition and we are required to query by email id which means we are required to retreive records which exist in multiple partitions .","poster":"Rpod"},{"poster":"captainpike","timestamp":"1679400180.0","comment_id":"845902","upvote_count":"1","content":"What's the difference between implementing A and D? I thought setting up A you would always get an index, \n\nA. For the primary key of the table, specify the unique identifier as the partition key and specify the email address as the sort key.)\nD. For the primary key of the table, specify the unique identifier as the partition key. Create a global secondary index (GSI) based on the email address.","comments":[{"timestamp":"1680478740.0","poster":"shahs10","content":"A means we are querying based on unique identifier and sorting based on email address while D means we can query based on both unique identifier and email address","comment_id":"859410","upvote_count":"1"}]},{"comment_id":"815276","upvote_count":"2","content":"Selected Answer: D\nThe correct answer is D. I initially thought it is B but no.\nLSI is alternate sort key , we want to search using either of the keys, so GSI\nGSI --> provides you option to define alternate primary key","timestamp":"1676899860.0","poster":"aarti_k"},{"timestamp":"1673791740.0","content":"In general, you should use global secondary indexes rather than local secondary indexes. The exception is when you need strong consistency in your query results, which a local secondary index can provide but a global secondary index cannot (global secondary index queries only support eventual consistency).","upvote_count":"1","comment_id":"776647","poster":"sichilam"},{"comment_id":"776635","poster":"sichilam","upvote_count":"1","content":"Global Secondary index\nIt is D","timestamp":"1673790720.0"},{"comment_id":"771999","upvote_count":"1","poster":"Dirisme","timestamp":"1673409360.0","content":"Selected Answer: D\nThe Query operation in Amazon DynamoDB finds items based on primary key values."},{"upvote_count":"4","content":"Selected Answer: D\nIt is D. At first I thought it was B, but the requirement is to be able to search by one or the other, the unique id or the email. With a local index, you need the primary key as well. Only a global index would allow you to search using only the email (once the primary key is set to the unique id)","comment_id":"681918","poster":"AulaitQM","timestamp":"1664382540.0"},{"comment_id":"680138","timestamp":"1664224200.0","upvote_count":"3","content":"Selected Answer: D\nOption D - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html","poster":"Spamuel"},{"upvote_count":"3","content":"Selected Answer: D\nWe need different primary key in second index so we need global secondary index","poster":"Jabol","timestamp":"1663671780.0","comment_id":"674064"},{"comment_id":"662730","comments":[],"upvote_count":"2","timestamp":"1662569760.0","content":"question sucks its trying to convey this: GSI can be a single attribute key which is called a partition key or it can be a composite key with two attributes,","poster":"peyto"},{"content":"Selected Answer: D\nI think its D","comments":[{"poster":"m_t_kd","content":"Pls specify reason","comment_id":"657444","comments":[{"comment_id":"1099332","content":"We don't need a sort key here. Becuase both unique_id and email_id are unique for each row. For LSI partition key will be same as partition key of the table only sort key will differ. But our requirement is to query by both. So have unique_id as base table partition key and email_id as the GSI partition key is the approach.","timestamp":"1702857480.0","poster":"Dipak25","upvote_count":"1"}],"upvote_count":"1","timestamp":"1662123300.0"}],"timestamp":"1661991000.0","comment_id":"655611","poster":"Chhotu_DBA","upvote_count":"3"}]},{"id":"OVDdsnseKicLOX1xHggv","url":"https://www.examtopics.com/discussions/amazon/view/78814-exam-aws-certified-developer-associate-topic-1-question-141/","question_images":[],"answer_description":"","question_id":48,"timestamp":"2022-09-01 00:27:00","isMC":true,"answer_ET":"C","answer_images":[],"answer":"C","exam_id":25,"topic":"1","question_text":"A developer has an application that asynchronously invokes an AWS Lambda function. The developer wants to store messages that resulted in failed invocations of the Lambda function so that the application can retry the call later.\nWhat should the developer do to accomplish this goal with the LEAST operational overhead?","answers_community":["C (88%)","13%"],"choices":{"A":"Set up Amazon CloudWatch Logs log groups to filter and store the messages in an Amazon S3 bucket. Import the messages in Lambda. Run the Lambda function again.","B":"Configure Amazon EventBridge (Amazon CloudWatch Events) to send the messages to Amazon Simple Notification Service (Amazon SNS) to initiate the Lambda function again.","C":"Implement a dead-letter queue for discarded messages. Set the dead-letter queue as an event source for the Lambda function.","D":"Send Amazon EventBridge (Amazon CloudWatch Events) events to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to pull messages from the SQS queue. Run the Lambda function again."},"unix_timestamp":1661984820,"discussion":[{"upvote_count":"1","poster":"SD_CS","comment_id":"1141817","content":"Selected Answer: C\nUse the SQS DLQ as a lambda destination for failed async messages","timestamp":"1707199260.0"},{"content":"Selected Answer: C\nThe correct answer is C. even if the call is asynchronous, we can anyway create an SQS queue to be configured as DLQ (Dead-Letter Queue). Then we can set the queue as an event source for the lambda function in order to retry and reprocess the events","timestamp":"1687973700.0","comment_id":"936964","poster":"rcaliandro","upvote_count":"1"},{"comment_id":"823171","content":"This question was on the exam today (Feb 2023)","timestamp":"1677471360.0","upvote_count":"2","poster":"pancman"},{"upvote_count":"1","timestamp":"1676899980.0","poster":"aarti_k","content":"Selected Answer: C\nAsynchronous invocation allow you to configure SQS queue, park all error in DLQ","comment_id":"815281"},{"upvote_count":"2","content":"Selected Answer: C\nC is the way to go.","timestamp":"1674191100.0","poster":"Phinx","comment_id":"781897"},{"poster":"sichilam","content":"DLQ is the answer C","timestamp":"1673792280.0","comment_id":"776654","upvote_count":"1"},{"comment_id":"769972","comments":[{"content":"DLQ for Lambda is for async invocations.","upvote_count":"1","comment_id":"769973","timestamp":"1673229360.0","poster":"jv_"}],"upvote_count":"1","timestamp":"1673229180.0","content":"C answer","poster":"jv_"},{"comment_id":"729910","poster":"michaldavid","upvote_count":"2","timestamp":"1669697400.0","content":"Selected Answer: C\nCCCCCCCCC"},{"timestamp":"1669585920.0","upvote_count":"1","poster":"RyanDDD","content":"Selected Answer: C\nAsynchronous invocation – You can configure a dead-letter queue on the function to capture events that weren't successfully processed. \nEvent source mappings – Event source mappings that read from streams retry the entire batch of items. \nFor event source mappings that read from a queue, you determine the length of time between retries and destination for failed events by configuring the visibility timeout and redrive policy on the source queue. \nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html","comment_id":"728639"},{"poster":"robbyboss","timestamp":"1663522440.0","upvote_count":"2","comment_id":"672625","content":"Selected Answer: C\nImplement a dead-letter queue for discarded messages."},{"comments":[{"comments":[{"content":"Keyword “retry the call later”\nSNS is perfect for fanout situations (multiple invocations) but in this case it does not apply","comment_id":"690315","poster":"habros","timestamp":"1665330960.0","upvote_count":"1"}],"content":"Why not B?","timestamp":"1662721080.0","upvote_count":"1","comment_id":"664444","poster":"Danbraga"}],"upvote_count":"4","content":"Selected Answer: C\nAnswer is C, SIMPLEST form\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html","timestamp":"1662326520.0","poster":"m_t_kd","comment_id":"659549"},{"comment_id":"655554","upvote_count":"2","content":"Selected Answer: A\nI think its A","poster":"Chhotu_DBA","timestamp":"1661984820.0"}]},{"id":"rueYFSX4mkjMIwITSUaa","url":"https://www.examtopics.com/discussions/amazon/view/78816-exam-aws-certified-developer-associate-topic-1-question-142/","answer_description":"","question_images":[],"question_id":49,"timestamp":"2022-09-01 00:31:00","isMC":true,"answer_ET":"C","answer":"C","exam_id":25,"answer_images":[],"topic":"1","question_text":"A developer is writing an application in Python. The application runs on AWS Lambda. The application generates a file and needs to upload this file to Amazon S3.\nThe developer must implement this upload functionality with the least possible change to the application code.\nWhich solution meets these requirements?","choices":{"B":"Include the AWS SDK for Python in the Lambda function. Use the SDK to upload the file.","C":"Use the AWS SDK for Python that is installed in the Lambda environment to upload the file.","A":"Make an HTTP request directly to the S3 API to upload the file.","D":"Use the AWS CLI that is installed in the Lambda environment to upload the file."},"answers_community":["C (76%)","B (24%)"],"unix_timestamp":1661985060,"discussion":[{"content":"Selected Answer: C\nC seems correct","upvote_count":"7","poster":"Chhotu_DBA","timestamp":"1661985060.0","comment_id":"655558"},{"comments":[{"comment_id":"1043901","upvote_count":"1","timestamp":"1697351220.0","poster":"nmc12","content":"require least posible change, so that is C.\nwith B is complex"}],"timestamp":"1688395320.0","poster":"tuongthuy","content":"Selected Answer: B\nOption B is the best solution to meet the requirements of the developer with the least possible change to the application code. By including the SDK in the Lambda function, the developer can use the SDK to upload the file to S3 without making significant changes to the application code.\n\nOption C is not recommended because using the SDK that is installed in the Lambda environment could lead to version conflicts and unexpected behavior if the SDK version in the environment is different from the version used by the developer.","comment_id":"941943","upvote_count":"2"},{"comment_id":"936966","timestamp":"1687973880.0","upvote_count":"1","content":"Selected Answer: C\nC is the correct one: \"Use the AWS SDK for Python that is installed in the Lambda environment to upload the file\"","poster":"rcaliandro"},{"content":"Selected Answer: B\nB is Correct","upvote_count":"1","poster":"Syre","comment_id":"869593","timestamp":"1681401900.0"},{"comment_id":"863218","poster":"RajinderKaur","upvote_count":"1","content":"Reason why it's not B:\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html\nYou can run Python code in AWS Lambda. Lambda provides runtimes for Python that run your code to process events. Your code runs in an environment that includes the SDK for Python (Boto3), with credentials from an AWS Identity and Access Management (IAM) role that you manage.","timestamp":"1680804600.0"},{"poster":"shahs10","content":"Selected Answer: B\nWhat is the difference between B and C?","upvote_count":"1","comment_id":"859413","timestamp":"1680479220.0","comments":[{"timestamp":"1683352980.0","comment_id":"890520","comments":[{"timestamp":"1683353040.0","poster":"BATSIE","content":"B******","comment_id":"890521","upvote_count":"1"}],"upvote_count":"2","content":"is also not the best choice since it requires the developer to include the SDK for Python in the application code, which may increase the size of the deployment package and the time it takes to deploy the function.","poster":"BATSIE"}]},{"comment_id":"769977","poster":"jv_","upvote_count":"1","timestamp":"1673229660.0","content":"answer C"},{"poster":"michaldavid","content":"Selected Answer: C\nCCCCCCCCC","upvote_count":"1","comment_id":"729911","timestamp":"1669697460.0"},{"poster":"dark_cherrymon","comment_id":"721661","comments":[{"comment_id":"721665","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1668817140.0","content":"my copy paste didn't work, here's the quote i was going for \n\n\"Because the boto3 module is already available in the AWS Lambda Python runtimes, don’t bother including boto3 and its dependency botocore in your Lambda deployment zip file.\"","comment_id":"721667","poster":"dark_cherrymon"}],"content":"\"On 2022-05-30, support for Python 3.6 was ended. This follows the Python Software Foundation end of support for the runtime which occurred on 2021-12-23. For more information, see this blog post.\"\n\nhttps://www.serverlessops.io/blog/aws-lambda-and-python-boto3-bundling#:~:text=Because%20the%20boto3%20module%20is,t%20bother%20bundling%20that%20either.","timestamp":"1668817080.0","poster":"dark_cherrymon"}],"upvote_count":"1","content":"Selected Answer: C\nit's probably still C because even with A you need a presigned url\n\nthough i would mention that boto is ending support for python so this question may not be so relevant. though you probably can use the older versions of python\n\nhttps://github.com/boto/boto3\n\n\"On 2022-05-30, support for Python 3.6 was ended. This follows the Python Software Foundation end of support for the runtime which occurred on 2021-12-23. For more information, see this blog post.\"","timestamp":"1668816780.0"},{"comment_id":"690319","poster":"habros","content":"Selected Answer: C\nBoto3 comes with the Lambda environment by default\nhttps://aws.amazon.com/sdk-for-python/","timestamp":"1665331080.0","upvote_count":"3"}]},{"id":"0poFxVxHu3mQUkwP3vqL","isMC":true,"question_text":"An application that is hosted on an Amazon EC2 instance needs access to files that are stored in an Amazon S3 bucket. The application lists the objects that are stored in the S3 bucket and displays a table to the user. During testing, a developer discovers that the application does not show any objects in the list.\nWhat is the MOST secure way to resolve this issue?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/78817-exam-aws-certified-developer-associate-topic-1-question-143/","discussion":[{"upvote_count":"1","comments":[{"poster":"nmc12","content":"add ec2 instance ID to S3 bucket is not valid.\nthat why B is true answer!!!","comment_id":"1043904","upvote_count":"1","timestamp":"1697351520.0"}],"timestamp":"1689494940.0","poster":"aws1234567","content":"IS D!! THE MOST SECURE WAY!!","comment_id":"953150"},{"content":"Selected Answer: B\nB \"Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket\"","timestamp":"1687974000.0","poster":"rcaliandro","comment_id":"936968","upvote_count":"1"},{"comment_id":"777134","upvote_count":"2","poster":"sichilam","content":"B it is","timestamp":"1673822220.0"},{"timestamp":"1671996240.0","upvote_count":"1","poster":"by116549","content":"According to this URL ListBucket will list the buckets, the question is after the objects in the bucket:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/API_ListBuckets.html","comment_id":"755947","comments":[{"content":"The s3:ListBucket permission allows the user to use the Amazon S3 GET Bucket (List Objects) operation\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-policy-language-overview.html","poster":"ninomfr64","timestamp":"1693231020.0","comment_id":"992260","upvote_count":"1"}]},{"timestamp":"1669697460.0","content":"Selected Answer: B\nBBBBBBB","upvote_count":"1","poster":"michaldavid","comment_id":"729914"},{"content":"Selected Answer: B\nB i was kinda looking at D, but the account number thing didn't ring a bell","upvote_count":"2","comment_id":"721675","poster":"dark_cherrymon","timestamp":"1668817560.0"},{"timestamp":"1668548040.0","content":"Selected Answer: A\nEvent though we should follow least privilege, but s3:listbucket will help list the bucket but not the objects.","comment_id":"719148","upvote_count":"2","poster":"ManasChuri","comments":[{"comment_id":"825528","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-policy-language-overview.html#:~:text=the%20s3%3AListBucket%20permission%20allows%20the%20user%20to%20use%20the%20Amazon%20S3%20GET%20Bucket%20(List%20Objects)%20operation","poster":"thesagarmahajan","timestamp":"1677645240.0","upvote_count":"3"},{"upvote_count":"3","poster":"thuyeinaung","timestamp":"1671969960.0","comment_id":"755617","content":"I had a same thought with you but when I checked the documentation ⇒ \"s3:ListBucket permission allows the user to use the Amazon S3 GET Bucket (List Objects) operation\""}]},{"timestamp":"1664700120.0","poster":"habros","content":"Selected Answer: B\nB. least principle of privilege, so no s3:*. to view all files in bucket, s3:listbucket is sufficient","comment_id":"684703","upvote_count":"2"},{"comment_id":"655559","timestamp":"1661985120.0","upvote_count":"4","content":"Selected Answer: B\nB is correct","poster":"Chhotu_DBA"}],"choices":{"D":"Update the S3 bucket policy by including the S3:ListBucket permission and by setting the Principal element to specify the account number of the EC2 instance.","B":"Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket.","A":"Update the IAM instance profile that is attached to the EC2 instance to include the S3:' permission for the S3 bucket.","C":"Update the developer's user permissions to include the S3:ListBucket permission for the S3 bucket."},"answer_ET":"B","timestamp":"2022-09-01 00:32:00","unix_timestamp":1661985120,"question_id":50,"answer":"B","answer_images":[],"exam_id":25,"question_images":[],"answers_community":["B (83%)","A (17%)"],"answer_description":""}],"exam":{"id":25,"isBeta":false,"numberOfQuestions":443,"isImplemented":true,"provider":"Amazon","isMCOnly":true,"lastUpdated":"11 Apr 2025","name":"AWS Certified Developer Associate"},"currentPage":10},"__N_SSP":true}