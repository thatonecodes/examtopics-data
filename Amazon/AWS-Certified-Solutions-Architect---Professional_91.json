{"pageProps":{"questions":[{"id":"t8BUuOv0p19AOnZaIwuV","timestamp":"2019-09-23 07:23:00","answer_images":[],"isMC":true,"question_text":"The Security team needs to provide a team of interns with an AWS environment so they can build a serverless video transcoding application. The project will use\nAmazon S3, AWS Lambda, Amazon API Gateway, Amazon Cognito, Amazon DynamoDB, and Amazon Elastic Transcoder.\nThe interns should be able to create and configure the necessary resources, but they may not have access to create or modify AWS IAM roles. The Solutions\nArchitect creates a policy and attaches it to the interns' group.\nHow should the Security team configure the environment to ensure that the interns are self-sufficient?","answer_description":"","answers_community":["A (100%)"],"choices":{"B":"Create a policy that allows creation of all project-related resources, including roles that allow access only to specified resources.","D":"Create a policy that allows creation of project-related resources only. Require the interns to raise a request for roles to be created with the Security team. The interns will provide the requirements for the permissions to be set in the role.","C":"Create roles with the required service permissions, which are assumable by the services. Have the interns create and use a bastion host to create the project resources in the project subnet only.","A":"Create a policy that allows creation of project-related resources only. Create roles with required service permissions, which are assumable by the services."},"answer_ET":"A","discussion":[{"upvote_count":"48","timestamp":"1633174620.0","poster":"sb333","content":"The answer is A. Interns are given permissions to create project-related resources. They then just have to associate the appropriate roles (with the required service permissions, pre-created by the Security team) as they create the resources. This allows the interns to setup the environment without having to give them permissions to create or modify any roles themselves.","comment_id":"49819"},{"poster":"donathon","timestamp":"1632150120.0","content":"B\nA\\C: This will allow the interns to create and modify other S3 or Lambda resources etc that does not belongs to the project.\nB: The most restrictive policy is enforced. Assuming Permission boundary is used to restrict access to those specific project related resources.\nWhen AWS evaluates the identity-based policies and permissions boundary for a user, the resulting permissions are the intersection of the two categories. That means that when you add a permissions boundary to a user with existing identity-based policies, you might reduce the actions that the user can perform. Alternatively, when you remove a permissions boundary from a user, you might increase the actions they can perform. An explicit deny in either of these policies overrides the allow.\nD: Not self-sufficient.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html","upvote_count":"14","comments":[{"comment_id":"31189","upvote_count":"8","poster":"neshin","timestamp":"1633172460.0","content":"\"but they may not have access to create or modify AWS IAM roles\""},{"comment_id":"334446","poster":"Amitv2706","upvote_count":"3","timestamp":"1635402300.0","content":"for B- Do you think interns would be aware of the exact resources/names in advance ?"}],"comment_id":"12230"},{"timestamp":"1698580380.0","upvote_count":"1","poster":"santosrk","comment_id":"1056787","content":"A : Because the services that project will use are already identified :\nAmazon S3, AWS Lambda, Amazon API Gateway, Amazon Cognito, Amazon DynamoDB, and Amazon Elastic Transcoder."},{"upvote_count":"1","content":"Selected Answer: A\nyou should always remember this from the Security Best practices in IAM, give only the \"least-privilege permissions\", so A.","poster":"SkyZeroZx","timestamp":"1687046400.0","comment_id":"926340"},{"timestamp":"1667591700.0","upvote_count":"1","poster":"superuser784","comment_id":"711343","content":"Selected Answer: A\nyou should always remember this from the Security Best practices in IAM, give only the \"least-privilege permissions\", so A."},{"content":"Selected Answer: A\nA. Create a policy that allows creation of project-related resources only. Create roles with required service permissions, which are assumable by the services.","timestamp":"1648578480.0","poster":"jj22222","comment_id":"577825","upvote_count":"1"},{"comment_id":"493979","poster":"AzureDP900","timestamp":"1638655080.0","upvote_count":"1","content":"A is right!"},{"comment_id":"437871","poster":"tgv","upvote_count":"1","timestamp":"1636195020.0","content":"AAA\n---"},{"content":"\"they may not have access to create or modify AWS IAM roles\" + \"interns are self-sufficient\"\nA correct\nB wrong - \"allows creation\" & \"including roles\"\nC wrong - \"bastion\"\nD wrong - \"Require the interns to raise a request\"","comment_id":"413813","poster":"DerekKey","upvote_count":"3","timestamp":"1636173000.0"},{"timestamp":"1635974880.0","upvote_count":"1","content":"I'll go with A","poster":"WhyIronMan","comment_id":"410243"},{"content":"A Correct","poster":"Akhil254","upvote_count":"1","comment_id":"406859","timestamp":"1635738180.0"},{"upvote_count":"3","comment_id":"345837","content":"It's A","timestamp":"1635623460.0","poster":"Waiweng"},{"comment_id":"299331","content":"ANSWER IS A. In B, interns can create all resources and even roles which isn't right","timestamp":"1635316260.0","poster":"kiev","upvote_count":"1"},{"content":"going with A","upvote_count":"1","comment_id":"290997","timestamp":"1635213840.0","poster":"Kian1"},{"timestamp":"1634807820.0","poster":"Ebi","upvote_count":"3","content":"Initially I was thinking of B, but after reading all the comments I go with A too.","comment_id":"281609"},{"timestamp":"1634609340.0","comment_id":"242862","content":"Correct answer is A. Create roles","poster":"T14102020","upvote_count":"3"},{"poster":"Bulti","comment_id":"230672","upvote_count":"7","content":"Answer is A. The best way to make the interns self-sufficient is to create a policy on the intern group that allows them to create or configure the necessary resources like API Gateway, AWS Cognito etc. for their project and assign service roles created by the security team on the services that would be assumed by these services to talk to other services.","timestamp":"1634361960.0"},{"content":"I'll go with A","timestamp":"1634361240.0","poster":"jackdryan","comment_id":"230613","upvote_count":"5"},{"poster":"shakthi000005","timestamp":"1634239560.0","upvote_count":"3","content":"Ans is A. Bcoz B states they will have access to role which should not be","comment_id":"161288"},{"poster":"balisongjam","comment_id":"156732","upvote_count":"4","timestamp":"1634067240.0","content":"bros. read the question \" but they may not have access to create or modify AWS IAM roles. \" Obviously not B. Its A."},{"content":"A is correct, need role which consumable by lamda","comment_id":"150099","upvote_count":"2","poster":"fullaws","timestamp":"1634025180.0"},{"content":"agree with B","timestamp":"1633916880.0","poster":"mat2020","upvote_count":"2","comment_id":"110374"},{"upvote_count":"3","poster":"JAWS1600","content":"This is a very fricky * ( tricky). The two wordings in option A and B mean the same. \n\"project-related resources only\" is same as \" all project-related resources\".\nB is the right answer","comment_id":"107848","timestamp":"1633913280.0"},{"timestamp":"1633802100.0","upvote_count":"1","comment_id":"107842","content":"B looks straight forward. \nA has this fishy statement . I dont get it - \"Create roles with required service permissions, which are assumable by the services.\" Is this a service role ?","poster":"JAWS1600"},{"timestamp":"1633249800.0","comment_id":"70561","content":"B is most self-sufficient of all options. However, as others pointed out, it doesn't meet requirements of denying role creation/modification. A seems correct.","poster":"Smart","upvote_count":"5"},{"content":"Answer is A. Intern should be limit to access some resource, not all resource","upvote_count":"5","timestamp":"1633175580.0","poster":"amog","comment_id":"51124"},{"poster":"chaudh","timestamp":"1632981120.0","upvote_count":"8","content":"I think A. Policy for group of interns and Roles for services to perform task.\nB is for all project-related reousces ==> interns can operate on other resources.","comment_id":"16213"},{"comment_id":"13793","poster":"Moon","content":"Agree with \"B\".","upvote_count":"2","timestamp":"1632960300.0"},{"timestamp":"1632287940.0","comment_id":"12246","content":"why is b","poster":"awsec2","upvote_count":"1"}],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/5589-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"topic":"1","question_id":451,"exam_id":32,"unix_timestamp":1569216180},{"id":"oXWynsMAPdNuYcJaZimU","exam_id":32,"answer_description":"","question_images":[],"answers_community":["C (63%)","A (38%)"],"choices":{"C":"Develop a script that uses Amazon Athena to query and analyze the files on Amazon S3. Then use Amazon QuickSight to connect to Athena and perform the visualization.","B":"Develop a stored procedure invoked from a MySQL database running on Amazon EC2 to analyze the files in Amazon S3. Then use a fast in-memory BI tool running on Amazon EC2 to visualize the data.","A":"Launch a transient Amazon EMR cluster daily and develop an Apache Hive script to analyze the files on Amazon S3. Shut down the Amazon EMR cluster when the job is complete. Then use Amazon QuickSight to connect to Amazon EMR and perform the visualization.","D":"Use a commercial extract, transform, load (ETL) tool that runs on Amazon EC2 to prepare the data for processing. Then switch to a faster and cheaper BI tool that runs on Amazon EC2 to visualize the data from Amazon S3."},"url":"https://www.examtopics.com/discussions/amazon/view/69120-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"comment_id":"514785","upvote_count":"7","content":"should be A","poster":"Buggie","timestamp":"1641091980.0","comments":[{"upvote_count":"3","timestamp":"1642326000.0","comment_id":"524807","content":"why not C?","poster":"GeniusMikeLiu","comments":[{"content":"seamless transition","poster":"bobsmith2000","comment_id":"593266","timestamp":"1651073640.0","comments":[{"upvote_count":"3","poster":"user0001","timestamp":"1651877940.0","content":"A is wrong , because Amazon S3 data has been vetted and does not need any extra changes , you dont need hive to run queries .","comment_id":"597908"}],"upvote_count":"1"}]}]},{"upvote_count":"1","timestamp":"1704397380.0","content":"Selected Answer: C\nC: Eliminates the overhead of Hadoop.\nHadoop is only used for querying the data from the BI tool. It specifically states, no transformation steps are required on the data. EMR is not necessary. If you use Athena to query directly from S3 then there is no need to run EMR. Plus if you are running a transient cluster it will not be quarriable by QuickSight when it is not running.","poster":"3a632a3","comment_id":"1114032"},{"comment_id":"926341","content":"Selected Answer: C\nAmazon Athena == Simple Query and serverless \nThen C better than A","upvote_count":"2","poster":"SkyZeroZx","timestamp":"1687046520.0"},{"content":"Selected Answer: A\nIll go with A, questions clearly says \"The company wants to reduce or eliminate the overhead costs associated with managing the HADOOP cluster and the BI TOOL\", with option C you are just querying and visualizing the data ALREADY creaded by the hadoop cluster(which is managed be the company) , in option A you give that responsability to AWS EMR which is what we want","upvote_count":"4","poster":"superuser784","timestamp":"1667592480.0","comment_id":"711346"},{"timestamp":"1660683000.0","content":"General Question: Are all the 800+ questions still applicable for the current Professional Arch Exam ? I remember seeing some old posts that only the first 300+ are valid... appreciate any insights","poster":"Biden","comment_id":"647823","upvote_count":"4"},{"content":"It's point that \"The visualization is straightforward and takes just a few simple aggregation processes.\"\nIt's simple query.\nwhy don't you use Athena?\nC is more reasonable answer.","upvote_count":"2","comment_id":"632820","poster":"MarkChoi","timestamp":"1658118240.0"},{"comment_id":"631716","poster":"CloudHandsOn","upvote_count":"2","content":"First answer chosen was C.","timestamp":"1657885020.0"},{"comment_id":"622613","upvote_count":"2","content":"Selected Answer: C\nC.\n\nBecause \"The organization wishes to minimize or eliminate overhead expenses associated with administering the Hadoop cluster\".","poster":"[Removed]","timestamp":"1656252420.0"},{"comment_id":"610038","upvote_count":"1","poster":"Anhdd","content":"Selected Answer: A\nwhen it comes to Hadoop cluster -> think about EMR. And with EMR we using Hive to querry anh large dataset","timestamp":"1654065840.0"},{"upvote_count":"2","poster":"Racinely","content":"Selected Answer: C\nATHENA","comment_id":"607727","timestamp":"1653580860.0"},{"content":"Selected Answer: A\nThe question asks for seamless transition.\nFrom Hadoop it's gonna be AWS EMR","poster":"bobsmith2000","upvote_count":"1","comment_id":"593265","timestamp":"1651073580.0"},{"comment_id":"560243","upvote_count":"3","timestamp":"1646327160.0","content":"Selected Answer: C\nB seems good except for using a cost-efficient tool, which may not be efficient in quality.\nC is the answer.\nAthena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there’s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.\nhttps://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc","poster":"Mechanic"},{"poster":"Changwha","content":"The Answer is C\n\nThe data on Amazon S3 has been curated and does not require any additional transformations steps\n\nThis means we dont need EMR and can get away with Athena","upvote_count":"4","comment_id":"556499","timestamp":"1645847760.0"},{"comments":[{"upvote_count":"1","poster":"bobsmith2000","content":"It's affective only with big files.\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","comment_id":"609735","timestamp":"1654002240.0"}],"upvote_count":"1","comment_id":"513889","content":"IS Athena good choice considering the huge volume of file?","poster":"guruaws2021","timestamp":"1640928120.0"}],"answer":"C","question_id":452,"topic":"1","question_text":"A company is running a commercial Apache Hadoop cluster on Amazon EC2. This cluster is being used daily to query large files on Amazon S3. The data on\nAmazon S3 has been curated and does not require any additional transformations steps. The company is using a commercial business intelligence (BI) tool on\nAmazon EC2 to run queries against the Hadoop cluster and visualize the data.\nThe company wants to reduce or eliminate the overhead costs associated with managing the Hadoop cluster and the BI tool. The company would like to move to a more cost-effective solution with minimal effort. The visualization is simple and requires performing some basic aggregation steps only.\nWhich option will meet the company's requirements?","isMC":true,"answer_ET":"C","unix_timestamp":1640928120,"timestamp":"2021-12-31 06:22:00","answer_images":[]},{"id":"Tcb4wxMw8AhPQos6s41O","question_images":[],"question_id":453,"answer_images":[],"question_text":"A large multinational company runs a timesheet application on AWS that is used by staff across the world. The application runs on Amazon EC2 instances in an\nAuto Scaling group behind an Elastic Load Balancing (ELB) load balancer, and stores data in an Amazon RDS MySQL Multi-AZ database instance.\nThe CFO is concerned about the impact on the business if the application is not available. The application must not be down for more than two hours, but he solution must be as cost-effective as possible.\nHow should the Solutions Architect meet the CFO's requirements while minimizing data loss?","answer_ET":"D","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/5593-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","unix_timestamp":1569219120,"topic":"1","answers_community":["D (60%)","C (40%)"],"choices":{"C":"Configure a 1-day window of 60-minute snapshots of the Amazon RDS Multi-AZ database instance which is copied to another region. Create an AWS CloudFormation template of the application infrastructure that uses the latest copied snapshot. When an issue occurs, use the AWS CloudFormation template to create the environment in another region. Update the DNS record to point to the other region's ELB.","A":"In another region, configure a read replica and create a copy of the infrastructure. When an issue occurs, promote the read replica and configure as an Amazon RDS Multi-AZ database instance. Update the DNS record to point to the other region's ELB.","D":"Configure a read replica in another region. Create an AWS CloudFormation template of the application infrastructure. When an issue occurs, promote the read replica and configure as an Amazon RDS Multi-AZ database instance and use the AWS CloudFormation template to create the environment in another region using the promoted Amazon RDS instance. Update the DNS record to point to the other region's ELB.","B":"Configure a 1-day window of 60-minute snapshots of the Amazon RDS Multi-AZ database instance. Create an AWS CloudFormation template of the application infrastructure that uses the latest snapshot. When an issue occurs, use the AWS CloudFormation template to create the environment in another region. Update the DNS record to point to the other region's ELB."},"timestamp":"2019-09-23 08:12:00","exam_id":32,"discussion":[{"comment_id":"12244","comments":[{"poster":"Pb55","timestamp":"1635821040.0","upvote_count":"1","content":"The application must not be down for more than two hours, but he solution must be as cost-effective as possible.How should the Solutions Architect meet the CFOג€™s requirements while minimizing data loss? \nMinimise data loss = D.","comment_id":"397594"},{"comment_id":"404673","content":"Read replica is not good enough, it serves only in disaster scenarios. But, what happens if there is any human mistake and the change would replicate to replicas as well. So, the answer is C","upvote_count":"1","poster":"ctrue","timestamp":"1635855900.0"},{"content":"There is NO requirement for RPO (data loss) being less than 1 hour, only RTO being less than 2 hours. Cost effective is major requirement so C is better.","comment_id":"143570","upvote_count":"4","comments":[{"poster":"AlwaysLearning2020","content":"Yes. RPO is not mentioned explicitly, but \"A large multinational company runs a timesheet application on AWS that is used by staff across the world....\" So what? Once data is loss for 60 mins, the company will ask all the staffs across the world to reenter their timesheet line-by-line again ? I wonder how complex with that be...? Hmmm.... I will vote 'D'.","upvote_count":"7","timestamp":"1634332620.0","comments":[{"comment_id":"842385","timestamp":"1679099880.0","content":"I'm a Cloud SA and I'm recertifying, meaning I've passed the exam before.\n\nAlways remember to answer the question the \"AWS way\" even if the answers are stupid. This means to answer the questions using whatever new service or thing they are peddling at any given time, and to watch out for silly tricks they add to questions.\n\nIn this case, you need to make sure the answer meets ONLY the requirements that were specifically called out, even if the answer is stupid. There was only an RTO mentioned here, not an RPO, and the question specifically calls out minimizing costs.\n\nSo the correct answer is C because it will be cheaper than D.","poster":"atlasga","upvote_count":"2","comments":[{"comments":[{"timestamp":"1693200960.0","poster":"rohanpro","content":"But In D, we are minimising the data loss which is also an requirement","comment_id":"991820","upvote_count":"1"}],"poster":"SkyZeroZx","content":"Agree with you i'm cloud SA Associate certified , C is more cheap than D.","timestamp":"1687046880.0","comment_id":"926342","upvote_count":"1"}]}],"comment_id":"194336"}],"poster":"MultiAZ","timestamp":"1633456500.0"}],"content":"D\nA: Does not address the EC2 instances in Auto Scaling group which the application runs on.\nB\\C: Data loss of 60 minutes is too long?\nhttps://aws.amazon.com/rds/details/read-replicas/","upvote_count":"40","timestamp":"1632134880.0","poster":"donathon"},{"timestamp":"1632245580.0","comments":[{"comment_id":"159847","content":"Most cost efficient is the key requirement...there's no mention of zero data loss being a necessity. If cost is key then C is correct.","poster":"Stec1980","timestamp":"1633748700.0","upvote_count":"1","comments":[{"content":"it says that cost-effective as possible, but the main key requirement is to minimize the data loss. D is what I would go with","upvote_count":"2","comments":[{"comment_id":"842388","poster":"atlasga","upvote_count":"1","content":"See my comment above (although I'm a year and a half late to the party here).","timestamp":"1679100000.0"}],"poster":"SamuelK","timestamp":"1634549040.0","comment_id":"223835"}]}],"comment_id":"13766","content":"Prefer \"D\".\nB/C: has a big data loss. While the question is asking for minimal data loss.","upvote_count":"17","poster":"Moon"},{"comment_id":"1117346","content":"Selected Answer: D\nIt's a trade-off between cost-effectiveness and minimizing data loss. Option C is less expensive but involves 1h data loss or less. On the other hand, Option D ensures less data loss of just a few seconds but would cost more as the read replica will always be running with continuous asynchronous syncing. As the question mentions \"With minimum data loss\", D is fitting the requirement (despite mentioning cost effectiveness early on)","timestamp":"1704792900.0","upvote_count":"1","poster":"shammous"},{"upvote_count":"1","content":"Selected Answer: C\nThe solution MUST be as cost effective as possible while MINIMIZING data loss. It is a trade off. this is why B and C are similar answers.\nThey do this on purpose and this is a question to loop your head and make you lose time into the test.\nUsing a read replica in another region is expensive so don't think we should count this option.\nOption C will cover the requirements","comment_id":"926343","timestamp":"1687046940.0","poster":"SkyZeroZx"},{"poster":"Jesuisleon","comment_id":"913951","upvote_count":"1","content":"I prefer D to C.\nMy point is not at the data loss differing between C and D.\nI highly suspect whether you can fulfill 2 hrs RTO by method C.\nThis is multinational company with staff all over the world, it implies the database will be huge. I highly suspect the restore work from a huge database could be finished in 2 hrs.\nSo I chose D.","timestamp":"1685833440.0"},{"timestamp":"1685463540.0","poster":"rbm2023","comment_id":"910421","upvote_count":"1","content":"Selected Answer: C\nThe solution MUST be as cost effective as possible while MINIMIZING data loss. It is a trade off. this is why B and C are similar answers. \nThey do this on purpose and this is a question to loop your head and make you lose time into the test. \nUsing a read replica in another region is expensive so don't think we should count this option.\nOption C will cover the requirements"},{"poster":"naiduveerendra","upvote_count":"2","comment_id":"643167","timestamp":"1659755220.0","content":"Selected Answer: D\nIts DDDD"},{"upvote_count":"1","poster":"hilft","content":"D is the best option","timestamp":"1658771280.0","comment_id":"636876"},{"content":"In my opinion, with b/c you'd have to construct the application from base with a snapshot which might take longer than the rto of 2 hours since it's a HUGE multinational company so the probability of the data being a lot is huge so...\nand i'd go for d since since you already have a read replica, it should save more time promoting it that than just starting from a snapshot. and there's the data loss of 60 minutes with b/c","comment_id":"561805","poster":"Alvindo","timestamp":"1646548860.0","upvote_count":"2"},{"comment_id":"540876","upvote_count":"1","timestamp":"1644056640.0","poster":"HellGate","content":"My answer is D.\n\nThere is requirement on data of reducing data loss… which means data need to be backed up near real time.\na 1-day window of 60-minute snapshots have RPO of 1 hour but read replica is near zero RPO."},{"comment_id":"526185","poster":"weurseuk","timestamp":"1642462680.0","upvote_count":"1","content":"D, Rto=2h but rpo: available , so read replica is the best choice"},{"poster":"AzureDP900","timestamp":"1638655980.0","content":"I will go with D","upvote_count":"1","comment_id":"493983"},{"upvote_count":"1","poster":"Kopa","content":"Im going for D","comment_id":"468598","timestamp":"1636284300.0"},{"poster":"nsei","content":"I’ll go with D. Depending on the size of the DB restoring the snapshot within 2 hours cannot be guaranteed","timestamp":"1636236480.0","comment_id":"467183","upvote_count":"1"},{"upvote_count":"2","poster":"StelSen","content":"Well \"C\" is most cost effective. \"D\" handles better in data loss. Let's read the requirements:\n\".....solution MUST be as cost-effective as possible\" - The word MUST leads to Option-C\n\"How SHOULD the Solutions Architect meet the CFOג€™s REQUIREMENTS \" - The word SHOULD and the word \"Requirements leads to Option-C as well\n\"....while minimizing data loss\" - This leads to Option-D, agree. However C also minimize data loss atleast for >60 mins. \nAll these keywords leads to Option-C to me.\nbut he solution must be as cost-effective as possible.\nwhile minimizing data loss","comment_id":"456364","timestamp":"1636233360.0"},{"poster":"student22","comments":[{"comment_id":"455710","content":"Minimize data loss + less costly","poster":"student22","timestamp":"1636189560.0","upvote_count":"1"}],"upvote_count":"2","content":"D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html","timestamp":"1636122000.0","comment_id":"434889"},{"timestamp":"1635974280.0","content":"D. There is a requirement to minimize data loss. So having snapshot of 60minutes involve data loss. D with read replica looks correct","upvote_count":"1","poster":"Shran","comment_id":"419781"},{"content":"The key requirement is - \"minimizing data loss\". How we can make it happen the cheapest way?\nD correct","poster":"DerekKey","comment_id":"413829","upvote_count":"2","timestamp":"1635952980.0"},{"comment_id":"410253","upvote_count":"2","content":"I'll go with D\n\"minimizing data loss\"","poster":"WhyIronMan","timestamp":"1635909840.0"},{"upvote_count":"1","poster":"zolthar_z","comment_id":"370997","timestamp":"1635717780.0","content":"The Answer is D\n\n\"With 1-day window\" is a little confusing... it could be 1 snapshot all nights or 1 snapshot one-time, but at the end, if you took the snapshot at 12 AM and you have a failure at 1 PM you will have a data loss of 12 hours. D over C is for the cost (even the question didn't mention anything related to cost)"},{"poster":"digimaniac","content":"BC are wrong. the snapshot is taken every hour. if there is something wrong, you have to ask international users who did their timesheet during the last hour to redo the timesheet. you will be fired and then get killed.","comment_id":"356758","timestamp":"1635667680.0","upvote_count":"2"},{"content":"It's D","upvote_count":"6","poster":"Waiweng","timestamp":"1635547320.0","comment_id":"345843"},{"content":"I prefer C.\nThe only difference between C and D, is the possibiliy for data loss.\nThe requirement is\n1. must be as cost-effective as possible\n2. minimizing data loss\nit looks like cost the is a must.\nAnd snapshot can be taken incrementally, so I think it won't take that long.","poster":"01037","upvote_count":"1","timestamp":"1635478380.0","comment_id":"337977"},{"timestamp":"1635427500.0","content":"C. I don't think there is cross region read replica in AWS.","poster":"awsnoob","comment_id":"304076","upvote_count":"2","comments":[{"upvote_count":"2","timestamp":"1643629560.0","comment_id":"536934","content":"There is. you can create read replicas of primary db instances in different regions. You just can`t create the standby instances in a Multi-AZ deployment in a \ndifferent region from the primary","poster":"Clandestine60"}]},{"poster":"Kian1","content":"Going with C over D, Snapshot, not more than 2 hours, as cost-effective as possible.","timestamp":"1635381180.0","upvote_count":"2","comment_id":"291002"},{"content":"D: The key is minimizing data loss. With the snapshots copy you can lost up to 60 minutes of data.","poster":"lechuk","timestamp":"1635341100.0","comment_id":"287556","upvote_count":"3"},{"content":"I hope this helps .... Based on this blog, it is cheaper to implement manual snapshot than using a read replica. \nhttps://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/","upvote_count":"2","timestamp":"1635185280.0","poster":"QCO","comment_id":"284220"},{"timestamp":"1635086100.0","upvote_count":"1","content":"C\nThe RTO is 2 hours, which has nothing to do with the RPO which is NOT SPECIFIED. Answer \"C\" builds in a one-hour outage every day and effectively creates a 24hr RPO. Cross-region replication provides data durability and is in alignment with best practices but has NOTHING TO DO with the CIO requirements in the question. The requirement is: \"A cost effective way to ensure no more than 2 hours of application downtime.\" Running a read replica doesn't grant MORE uptime than a snapshot (it does improve performance) if you have to rebuild the application from a CF template in a new region anyway. Anyone choosing \"D\" either didn't read the question closely or doesn't understand very well the difference between a read replica and a snapshot.","comment_id":"282715","poster":"Trap_D0_r"},{"content":"Snapshot copy cannot be an option in here: \n\nDepending on the AWS Regions involved and the amount of data to be copied, a cross-Region snapshot copy can take hours to complete. In some cases, there might be a large number of cross-Region snapshot copy requests from a given source AWS Region. In these cases, Amazon RDS might put new cross-Region copy requests from that source AWS Region into a queue until some in-progress copies complete. No progress information is displayed about copy requests while they are in the queue. Progress information is displayed when the copy starts.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html\n\nI go with D","comments":[{"comments":[{"content":"But the quetion is\nHow should the Solutions Architect meet the CFO s requirements while minimizing data loss? For minimizing data loss the answer is D","comment_id":"309887","upvote_count":"2","timestamp":"1635434700.0","poster":"carlosmds"}],"timestamp":"1635300180.0","upvote_count":"3","comment_id":"287467","content":"Changing to C, 2 hours is RTO and not RPO.\nSnapshots copy is always cheapest DR option","poster":"Ebi"}],"timestamp":"1635083580.0","comment_id":"281618","poster":"Ebi","upvote_count":"3"},{"timestamp":"1634919900.0","comment_id":"269678","poster":"kopper2019","upvote_count":"1","content":"I vote for D, but not sire what this really means \"Configure a 1-day window of 60-minute snapshots of the Amazon RDS Multi-AZ\" ? Any idea it says 2 hours RTO so not sure what is means those 60 min 1 day window","comments":[{"content":"60 minutes data loss. requirement says no data loss. RPO mean how much business can afford to lost the data , RTO is recovery time objective once issue occurred.","comment_id":"493985","timestamp":"1638656220.0","upvote_count":"1","poster":"AzureDP900"}]},{"comment_id":"244983","poster":"petebear55","upvote_count":"2","content":"Guys the Answer is D, Whilst A may also look attractive the question asks for \"most cost effective\" this will neutralise A which would incur a cost by maintaining a clone .... infrastructure in a differing region. .. Further the question emphasises to \"avoid loss of Data\" .. thus the way to do this in RDS database scenarios is by a read replica. ...","timestamp":"1634805660.0"},{"timestamp":"1634801940.0","poster":"T14102020","upvote_count":"2","content":"Correct answer is D. CloudFormation + replica","comment_id":"242911"},{"comments":[{"comment_id":"244987","poster":"petebear55","upvote_count":"1","content":"read rto / rpo before u take exam","timestamp":"1634818680.0"}],"content":"What is meant by 1-day window of 60-minute ? am confused!","timestamp":"1634749980.0","poster":"cloudgc","comment_id":"241669","upvote_count":"1"},{"poster":"Bulti","upvote_count":"2","content":"This is one of those tricky questions. Because the solution should be as cost effective as possible but should provide minimal data loss I am inclined towards selecting D as it surely would result in a minimal data loss compared to the cross-region copying of Snapshots in Option C.","timestamp":"1634593320.0","comment_id":"230684"},{"timestamp":"1634552400.0","poster":"jackdryan","upvote_count":"4","comment_id":"230629","content":"I'll go with D"},{"timestamp":"1634237340.0","content":"I would go for D finally. I thought it C first for cost effective word. but..\n*you must promise 2 hours RTO.\n*solution should be cost effective as possible.\nyou cannot assure under 2hours RTO under the cross region snapshot copy; it can take hours to copying cross region as \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html\nyes, read replica is more expensive than snapshots but it can assure 2 hours RTO. you should check what is the must to be promised and what is the optional.","comment_id":"181481","poster":"proxyolism","upvote_count":"4"},{"poster":"Ganfeng","comment_id":"175062","content":"i would go to D. The question is \"as cost effective AS POSSIBLE\" and Minimising the data lost. it did not spell out the MOST cost effective","upvote_count":"2","timestamp":"1634036340.0"},{"timestamp":"1633711020.0","comments":[{"comment_id":"244988","content":"u are wrong ... its D ... IVE HAD THIS VERIFIED BY aMAZON","comments":[{"timestamp":"1679100060.0","comment_id":"842389","upvote_count":"1","content":"Yes but probably not by the moron who wrote this question.","poster":"atlasga"}],"poster":"petebear55","timestamp":"1634871000.0","upvote_count":"1"}],"upvote_count":"2","comment_id":"157292","poster":"balisongjam","content":"I'll set this to rest. Key words are \"must be as cost effective as possible\" and \"minimize data loss\". It doesnt say no data lossso RPO can be > 0. Clearly the answer is C."},{"content":"D. \"The application must not be down for more than two hours,\" refers to RTO \nminimal data loss is RPO","upvote_count":"2","poster":"Mikey123","comment_id":"153501","timestamp":"1633610100.0"},{"comments":[{"timestamp":"1634912100.0","content":"WRONG ITS D","poster":"petebear55","comment_id":"244989","upvote_count":"1"}],"upvote_count":"1","content":"C is correct, cost effective and recovery in 2 hrs (only consider provision db), minimal data loss no hard requirement possible for up to 4 hrs","comment_id":"150123","timestamp":"1633584180.0","poster":"fullaws"},{"comment_id":"143023","content":"Its C as solution needs cost effective & no data loss.","upvote_count":"1","poster":"Anila_Dhharisi","timestamp":"1633431000.0"},{"content":"answer : C","timestamp":"1633251540.0","comment_id":"132746","poster":"mat2020","upvote_count":"2"},{"poster":"NikkyDicky","comment_id":"132413","timestamp":"1633158540.0","comments":[{"content":"C is the answer. You'll be doing incremental snapshots. This is recommended. Read the RDS Replica promotion process. It recommends a backup before that is done. Also, read about Replica lag which is the time between the master and the replica. AWS states: \"Long-running queries on the primary DB instance that take an equal amount of time to run on the replica DB instance can increase the seconds_behind_master. For example, if you executed a change on the primary DB instance and it takes one hour to execute, by the time that change starts running on the replica, the lag is one hour. Because the change might also take one hour to complete on the replica, by the time the change is complete, the total lag is approximately two hours.\" The answer is definitely C. It's also more expensive and cost is a factor here. You'll not lose more time to copy the snapshot than you will to re-run the query on the replica. It will depend on the update query lengths.","timestamp":"1633386960.0","poster":"xhova","upvote_count":"1","comment_id":"137551","comments":[{"comment_id":"137553","content":"https://aws.amazon.com/premiumsupport/knowledge-center/rds-mysql-high-replica-lag/#:~:text=Because%20the%20change%20might%20also,running%20statements%20to%20reduce%20lag.","upvote_count":"1","timestamp":"1633415220.0","poster":"xhova"}]}],"content":"Good point about cross-region snapshot copy time!\nD is the answer","upvote_count":"1"},{"upvote_count":"6","content":"D. Based on other comments I was initially inclined to go with C due to the 2 hours of RPO. However, documentation states that copying snapshots can take several hours depending on the amount of data and the regions involved.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html","comment_id":"112911","timestamp":"1633158000.0","poster":"tones"},{"comment_id":"110375","poster":"mat2020","timestamp":"1633011900.0","upvote_count":"1","content":"ans D is best choice"},{"content":"key is minimizing data loss, which D accomplishes with a read replica","timestamp":"1632954420.0","poster":"oatif","upvote_count":"1","comment_id":"105696"},{"upvote_count":"1","content":"D, this is a typical pilot light structure and no data lost, almost","comment_id":"105036","timestamp":"1632899040.0","poster":"IPMAN"},{"timestamp":"1632856980.0","upvote_count":"2","content":"The best option is D from DR and prospective, but the questions specifically say cost-effective and RPO of 2 hours. So, I think keeping cost in mind and having 2 hours to bring the system back up, C is the best option.","poster":"meenu2225","comment_id":"102775"},{"comment_id":"87549","content":"Prefer D. Read replica is minimizing the data loss which is requested by the question.","upvote_count":"2","timestamp":"1632793860.0","poster":"[Removed]"},{"content":"Agreed C\nFeature RTO RPO Cost Scope\nAutomated backups Good Better Low Single Region\nManual snapshots Better Good Medium Cross-Region\nRead replicas Best Best High Cross-Region","timestamp":"1632656220.0","poster":"subdas","comment_id":"83992","upvote_count":"5"},{"comment_id":"77225","poster":"juanC1917","upvote_count":"7","content":"C.- Even when read replica can be promoted easily and reduce significantly the data loss we should consider the other significant requirement: \"cost-efective as possible\" with a RPO up to 2 hours, so even if we took the entire 2h if the cost is the most effective the solution would be in compliance. Read replicas in RDS are high expensive and snapshots are cheaper, so as long as we are within 2h RPO the cost inclines to C. see for reference: https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/","timestamp":"1632617280.0"},{"comments":[{"content":"Right, that's why we can promote a read replica ... D seems to be the best choice.","timestamp":"1632580380.0","comments":[{"upvote_count":"1","poster":"Stec1980","content":"Not the most cost effective though, which seems to be more important than the potential 60 minutes of data loss. In which case, the answer is C.","comment_id":"159849","timestamp":"1633957020.0"}],"poster":"virtual","upvote_count":"1","comment_id":"61813"}],"content":"D is most likely correct. you can use read replica in another region to improve disaster recovery: https://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/","comment_id":"53145","poster":"Gorha","timestamp":"1632563520.0","upvote_count":"4"},{"timestamp":"1632367380.0","poster":"amog","comments":[{"poster":"Smart","timestamp":"1632616380.0","content":"I get it - Snapshot is cheaper than Read Replica. However, a multinational company losing timesheet records for all its employee can be more expensive. Also, you have a read replica function for DR - probably have lower instance type & size for it.","comment_id":"76603","comments":[{"poster":"Smart","comments":[{"upvote_count":"3","poster":"sam422","content":"Exam requires answer on Question requirement not on student assumptions, C cannot achieve 2hrs downtime requirement, creating environment on cloudformation, restoring DB, and pointing DNS 2hrs is not achievable","comment_id":"187308","timestamp":"1634307120.0"}],"upvote_count":"2","content":"Revisiting...I am overthinking AGAIN. My logic was: imagine this design went down when 100k employees were about to clock in. The company would have to pay for all its employees including absentees. Thereby, minimizing data loss through read replica would be necessary. This boils down to the cost comparison of paying absentees vs paying for read replica. For this reason, I had initially chosen D. \n\nBut then if the company is okay with 2 hours of RTO, they should be okay with additional 1 hour of RPO ;) . This leads to C.","timestamp":"1632757740.0","comment_id":"84732"}],"upvote_count":"2"}],"upvote_count":"6","content":"There is two requiment need concern\nCost effective as possible and minimize data loss.\nI will pick C. Read replica is using for performance purpose","comment_id":"51126"},{"content":"D is best choice","timestamp":"1632314700.0","upvote_count":"6","comment_id":"16214","poster":"chaudh"},{"upvote_count":"6","comment_id":"12248","poster":"awsec2","timestamp":"1632207480.0","content":"c is right"}],"isMC":true},{"id":"x4hERpRzaNs67e9ewCId","answer":"A","answer_description":"","timestamp":"2021-03-30 15:26:00","exam_id":32,"discussion":[{"content":"C is not the r8 option. What if the DR stikes while copying the DB dumps to S3, it won't be able to fulfil the RPO of 1 hour or less. Becoz the DB copy is sent every 1 hour and also it will take some time to copy the dumps to S3. In that case, the latest copy available will be 2 hrs back. \nAlso, we can eliminate B as it doesn't talk abt AMI backup and D, as well becoz Direct connect, is expensive. \nThus, the r8 answer is A.","upvote_count":"13","comment_id":"506109","timestamp":"1640091240.0","poster":"saptati"},{"comments":[{"poster":"Yahowmy","timestamp":"1635841380.0","comment_id":"391912","upvote_count":"1","content":"There is a requirement of moving to AWS."}],"poster":"Dgix","comment_id":"324302","content":"C. No mention is made of migrating the database, only of preparing for a DR situation.","timestamp":"1632992580.0","upvote_count":"5"},{"comment_id":"1307232","timestamp":"1730790660.0","content":"Selected Answer: A\nA - Minimizes the cost","poster":"student22","upvote_count":"1"},{"poster":"amministrazione","content":"B. Deploy your application on EC2 instances within an Auto Scaling group across multiple availability zones. Asynchronously replicate transactions from your on- premises database to a database instance in AWS across a secure VPN connection.","timestamp":"1723735740.0","upvote_count":"1","comment_id":"1266523"},{"poster":"TravelKo","comment_id":"962328","content":"Selected Answer: A\nOption A minimizes the costs.","upvote_count":"1","timestamp":"1690258920.0"},{"content":"Selected Answer: A\nA - Should be a viable solution. ~ 9 days to transfer DB to the cloud over this internet connectivity = 20Mbps, and do asynchronous replicate pransactions over VPN. In case of DR, - run a CloudFormation template and re-configure stack to use DB. 4 hours RTO and 1 hour RPO seems to be achievable in this case and also minimal costs, as only DB should run in Cloud.\n\nB - no, multiple AZ deployment contradict the costs constrains\nC - no, why would you want to use S3 at all here?\nD - too expensive, doesn't meet the cost constraint.","poster":"kondratyevmn","upvote_count":"1","timestamp":"1689257520.0","comment_id":"950732"},{"content":"A is the correct answer. creating a DR between AWS and on-prem will require some type of connection, which the VPN provides. Also, setting up CF with all of the AWS resources will allow us to have that setup in place in case of an issue","upvote_count":"1","poster":"CloudHandsOn","comment_id":"842767","timestamp":"1679143020.0"},{"comment_id":"749801","upvote_count":"1","poster":"TigerInTheCloud","timestamp":"1671451920.0","content":"Selected Answer: A\nD. Direct connection is too expensive on cost (still need achieving buy-in from the other company executives) and time (2 weeks may not enough)\nC. 20Mbp connection internet connection is an issue \nA and B are similar. A is better on RTO with automatical (CloudFormation) deployment with the pre-installed application.\n\nThe initial transferring 200GB is an issue not addressed (need snowball). However, among the answers, A is the best one."},{"content":"Selected Answer: A\nA and B are similar but B is active-active while A leverages a cloudformation template to spin up and start the environment. This is a key point since one of the requirements is to minimize cost and RTO is 4 hours. With this timeframe you will be comfortably re-create your environment with Cloudformation and save a lot of money since you don't need to keep the EC2 up and running 24/7","upvote_count":"2","timestamp":"1664635800.0","poster":"davideccc","comment_id":"684279"},{"content":"To move 200GB data with 20Mbps it takes ~22 days. 200*8(bits)*1000M/20M/60min*60s..How can a db setup be done in aws in 2 weeks?","comments":[{"upvote_count":"2","poster":"EricZhang","timestamp":"1671496680.0","content":"20Mbps ≈ 2MB per second = 7.2G per hour = 172.8G per day","comment_id":"750357"}],"poster":"kaushik9845","timestamp":"1663385700.0","comment_id":"671230","upvote_count":"1"},{"poster":"fukami_ymir","timestamp":"1659537000.0","upvote_count":"3","content":"Selected Answer: B\nI think it's B. C is bad since backup to S3 required a lot of time and won't fit the RPO at all. A is good but the question wants a temporary solution with lower cost so ELB is not necessary.","comment_id":"641952"},{"upvote_count":"1","timestamp":"1658623020.0","content":"Confused.\nC seems an option with DR,\nA is something more with migration and DR.\nForum seems to go for C","poster":"hilft","comment_id":"635819"},{"comment_id":"591758","upvote_count":"1","timestamp":"1650895020.0","content":"RTO=4 and RPO <=1. So:\nD with sync. replication is wrong because not accurate with RPO and would result in high costing ...\nC (full)back ups the local database: this is not a solution for an RPO of approx. one hour.\nA and B are both async. replication (that is good). The difference is A going with CloudFormation, so my choice is A.","poster":"virtual"},{"timestamp":"1642466100.0","comment_id":"526202","content":"C preserves 1 hr backup data - When DR happens we can use that backup to restore instance. This the only option which maps to RPO of 1hr.\nIn case of A - when DR happens we can use the AMI to setup the instance. We havent got any snapshot of data in this case and we have replicate the full 200GB data - doesnt look good considering the bandwidth availability.","upvote_count":"1","poster":"tkanmani76"},{"comment_id":"501981","upvote_count":"1","poster":"dv1","timestamp":"1639556580.0","content":"Question asks for a short term DR solution UNTIL the AWS Migration is approved by all stakeholders. I will go C."},{"comment_id":"415369","upvote_count":"1","poster":"Madhu654","timestamp":"1636291920.0","comments":[{"poster":"javiems","content":"20 Mbps = 2 MBps approx. 2 x 60 seconds x 60 minutes x 24 hours = 172.800 MBps (172 GB approx)","timestamp":"1638175560.0","comment_id":"489712","upvote_count":"1"},{"content":"Don't forget that the solution should be the most cost-effectif","timestamp":"1678492260.0","upvote_count":"1","comment_id":"835534","poster":"ohasnaoui"}],"content":"A,B are incorrect with a 20Mbps internet connection you can copy as much a 50GB of data within 2 weeks\n\nAnswer is C\nC hourly backup to S3 seems a viable option\n\nD could have been correct if there isn't a 2week time constraint.. Direct Connect connections can take anytime between 2weeks to month to setup"},{"content":"A https://jayendrapatil.com/tag/disaster-recovery/","poster":"ExtHo","comment_id":"331247","upvote_count":"2","timestamp":"1634567520.0"},{"content":"As per the question, RTO is 4 and RPO is 1 hr. How option A confirms the RPO? Where as Option C confirming that.","upvote_count":"2","timestamp":"1633073700.0","poster":"tbolar","comment_id":"327253","comments":[{"poster":"dli","timestamp":"1635994860.0","comment_id":"411319","upvote_count":"1","content":"The database has asynchronous replication all the time. Only the web tier is launched during DR to meet RTO."},{"content":"Maybe.\nBut 200G data in S3, could it meet the requirement of RTO with 20Mbps bandwidth?\n\nI support A, though it should mention backup plan.","upvote_count":"1","poster":"01037","timestamp":"1635446520.0","comment_id":"346718"}]}],"question_id":454,"answer_ET":"A","unix_timestamp":1617110760,"answers_community":["A (67%)","B (33%)"],"topic":"1","isMC":true,"question_images":[],"answer_images":[],"choices":{"A":"Create an EBS backed private AMI which includes a fresh install of your application. Develop a CloudFormation template which includes your AMI and the required EC2, AutoScaling, and ELB resources to support deploying the application across Multiple- Availability-Zones. Asynchronously replicate transactions from your on-premises database to a database instance in AWS across a secure VPN connection.","C":"Create an EBS backed private AMI which includes a fresh install of your application. Setup a script in your data center to backup the local database every 1 hour and to encrypt and copy the resulting file to an S3 bucket using multi-part upload.","B":"Deploy your application on EC2 instances within an Auto Scaling group across multiple availability zones. Asynchronously replicate transactions from your on- premises database to a database instance in AWS across a secure VPN connection.","D":"Install your application on a compute-optimized EC2 instance capable of supporting the application's average load. Synchronously replicate transactions from your on-premises database to a database instance in AWS across a secure Direct Connect connection."},"question_text":"Your company currently has a 2-tier web application running in an on-premises data center. You have experienced several infrastructure failures in the past two months resulting in significant financial losses. Your CIO is strongly agreeing to move the application to AWS. While working on achieving buy-in from the other company executives, he asks you to develop a disaster recovery plan to help improve Business continuity in the short term. He specifies a target Recovery Time\nObjective (RTO) of 4 hours and a Recovery Point Objective (RPO) of 1 hour or less. He also asks you to implement the solution within 2 weeks.\nYour database is 200GB in size and you have a 20Mbps Internet connection. How would you do this while minimizing costs?","url":"https://www.examtopics.com/discussions/amazon/view/48517-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"ItwTpmNgfNHn7zm2Gvxg","choices":{"E":"Create a new portfolio for the services in AWS Service Catalog. Create a new AWS CloudFormation template for each service. Alter the existing templates to use cross-stack references to eliminate passing many parameters to each template. Call each required stack for the application as a nested stack from the new stack. Create a product for each application. Add the service template to the product. Add each new product to the portfolio. Deploy the product from the portfolio to deploy the service with the necessary parameters only to start the deployment.","B":"Create a new portfolio in AWS Service Catalog for each service. Create a product for each existing AWS CloudFormation template required to build the service. Add the products to the portfolio that represents that service in AWS Service Catalog. To deploy the service, select the specific service portfolio and launch the portfolio with the necessary parameters to deploy all templates.","D":"Use AWS Step Functions to define a new service. Create a new AWS CloudFormation template for each service. Alter the existing templates to use cross- stack references to eliminate passing many parameters to each template. Call each required stack for the application as a nested stack from the new service template. Configure AWS Step Functions to call the service template directly. In the AWS Step Functions console, execute the step.","C":"Set up an AWS CodePipeline workflow for each service. For each existing template, choose AWS CloudFormation as a deployment action. Add the AWS CloudFormation template to the deployment action. Ensure that the deployment actions are processed to make sure that dependencies are obeyed. Use configuration files and scripts to share parameters between the stacks. To launch the service, execute the specific template by choosing the name of the service and releasing a change.","A":"Create a new AWS CloudFormation template for each service. Alter the existing templates to use cross-stack references to eliminate passing many parameters to each template. Call each required stack for the application as a nested stack from the new stack. Call the newly created service stack from the AWS CloudFormation console to deploy the specific service with a subset of the parameters previously required."},"topic":"1","question_images":[],"answer":"AE","url":"https://www.examtopics.com/discussions/amazon/view/36365-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A Development team has created a series of AWS CloudFormation templates to help deploy services. They created a template for a network/virtual private cloud\n(VPC) stack, a database stack, a bastion host stack, and a web application-specific stack. Each service requires the deployment of at least:\n✑ A network/VPC stack\n✑ A bastion host stack\n✑ A web application stack\nEach template has multiple input parameters that make it difficult to deploy the services individually from the AWS CloudFormation console. The input parameters from one stack are typically outputs from other stacks. For example, the VPC ID, subnet IDs, and security groups from the network stack may need to be used in the application stack or database stack.\nWhich actions will help reduce both the operational burden and the number of parameters passed into a service deployment? (Choose two.)","discussion":[{"poster":"Bulti","upvote_count":"10","timestamp":"1635016560.0","content":"A&E. E requires few parameters and one action to launch a portfolio to deploy all products, where one product represents one service. Between A ( CloudFormation console to deploy a service) vs D ( Step function to deploy a service) I chose A because I didn't find Step function to be a standard practice to deploy a cloudformation stack unlike CodePipeline which is a standard way to test and deploy cloudformation stack via a codepipeline. However Option B and C fall short because they do not reduce the# of input parameters.","comment_id":"255079"},{"timestamp":"1632264720.0","comment_id":"215618","poster":"NNHAN","comments":[{"comment_id":"224698","content":"they have duplicate content","timestamp":"1632363000.0","poster":"keos","upvote_count":"1"},{"content":"Agree, Looks like the only option to pass parameter around is still using nested cloudformation templates","upvote_count":"1","poster":"Kelvin1477","timestamp":"1633066620.0","comment_id":"236450"}],"upvote_count":"6","content":"A and E"},{"comment_id":"926346","comments":[{"comment_id":"926347","timestamp":"1687047180.0","content":"Therefore, options A and E are more suitable for reducing the operational burden and minimizing the number of parameters passed into a service deployment, as stated in the question.","poster":"SkyZeroZx","upvote_count":"1"}],"timestamp":"1687047180.0","poster":"SkyZeroZx","content":"Selected Answer: AE\nOption B suggests creating a new portfolio in AWS Service Catalog for each service and creating a product for each existing AWS CloudFormation template required to build the service. While AWS Service Catalog can help with managing and provisioning services, it may not directly address the issue of reducing the number of parameters passed into a service deployment.\nOptions A and E, on the other hand, specifically suggest using cross-stack references within AWS CloudFormation templates to eliminate the need for passing many parameters. This approach directly tackles the issue of parameter management and reduces the operational burden by making the templates more modular and self-contained.","upvote_count":"1"},{"timestamp":"1653553260.0","comment_id":"607535","upvote_count":"1","poster":"ASC1","content":"why not B ?"},{"timestamp":"1642808580.0","upvote_count":"1","content":"A and E - Pl refer this article - https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/","poster":"tkanmani76","comment_id":"529479"},{"content":"AE is correct, There is no need of codepipeline.","comment_id":"493988","timestamp":"1638656340.0","poster":"AzureDP900","upvote_count":"1"},{"content":"I will go with A, E","timestamp":"1636295700.0","comment_id":"410262","upvote_count":"4","poster":"WhyIronMan"},{"poster":"MarcChartouny","content":"I got lost since I don't have the advanced DevOps expertise, But my guts tell me to go with D & E. Please any DevOps engineers to assist and give their feedback?","comment_id":"379459","timestamp":"1636163700.0","upvote_count":"1"},{"comment_id":"345844","upvote_count":"4","timestamp":"1636001820.0","content":"It's A , E","poster":"Waiweng"},{"content":"D & E are the correct options that eliminate param passing. All other options still require params and no easy way to implement.","comment_id":"330675","timestamp":"1635865140.0","poster":"SD13","upvote_count":"2"},{"content":"going with AE","upvote_count":"2","poster":"Kian1","timestamp":"1635622860.0","comment_id":"293076"},{"timestamp":"1635617400.0","content":"I will go with AE","comment_id":"264484","poster":"Ebi","upvote_count":"4"},{"timestamp":"1634076600.0","poster":"Britts","comment_id":"250330","content":"why not D & E, step functions can launch a stack? i.e. one step function per service","upvote_count":"1"},{"timestamp":"1633943220.0","upvote_count":"1","poster":"arulrajjayaraj","comment_id":"248192","content":"A & E - A --> AWS CloudFormation console to deploy , E -->ServiceCatalog , Cross Stacks When you need to pass export values to many stacks (VPC Id, etc…) & Nested Stacks\n• Helpful when components must be re-used • Ex: re-use how to properly configure an\nApplication Load Balancer • The nested stack only is important to the higher level stack (it’s not shared)"},{"comment_id":"244842","poster":"T14102020","comments":[{"poster":"Aquavk","timestamp":"1634299260.0","comment_id":"253109","upvote_count":"2","content":"Why not D & E ?"}],"content":"Correct AE. ServiceCatalog + without CodePipeline + without Step Functions","upvote_count":"1","timestamp":"1633912740.0"},{"upvote_count":"2","timestamp":"1633189620.0","poster":"joos","content":"A and E","comment_id":"239135"},{"comment_id":"233537","upvote_count":"1","poster":"alexmena1981","content":"E ok, why not D?","timestamp":"1632659160.0"},{"poster":"jackdryan","comment_id":"232552","content":"I'll go with C,E","comments":[{"timestamp":"1633654980.0","comments":[{"timestamp":"1633769220.0","upvote_count":"3","comment_id":"242962","poster":"darthvoodoo","content":"No it doesn't. You would still need to manage and maintain a script to map the parameters, which is unnecessary. The question doesn't ask which combinations address the requirements. A&E are correct answers."}],"comment_id":"241140","content":"C - will help reduce both the operational burden.","poster":"arulrajjayaraj","upvote_count":"1"}],"upvote_count":"3","timestamp":"1632495780.0"},{"content":"looks like CE","timestamp":"1632126660.0","poster":"keos","upvote_count":"3","comment_id":"214587"}],"timestamp":"2020-11-07 13:11:00","exam_id":32,"answer_images":[],"answer_description":"","isMC":true,"unix_timestamp":1604751060,"answer_ET":"AE","answers_community":["AE (100%)"],"question_id":455}],"exam":{"isBeta":false,"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","isMCOnly":false,"id":32,"numberOfQuestions":1019,"provider":"Amazon","isImplemented":true},"currentPage":91},"__N_SSP":true}