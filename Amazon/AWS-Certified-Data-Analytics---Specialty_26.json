{"pageProps":{"questions":[{"id":"oCSe1CVgNU8FWImsx2I6","answers_community":["AC (100%)"],"timestamp":"2020-08-14 10:19:00","question_images":[],"answer_ET":"AC","topic":"1","question_text":"A large financial company is running its ETL process. Part of this process is to move data from Amazon S3 into an Amazon Redshift cluster. The company wants to use the most cost-efficient method to load the dataset into Amazon Redshift.\nWhich combination of steps would meet these requirements? (Choose two.)","unix_timestamp":1597393140,"choices":{"D":"Use the UNLOAD command to upload data into Amazon Redshift.","B":"Use S3DistCp to load files into Amazon Redshift.","E":"Use Amazon Redshift Spectrum to query files from Amazon S3.","A":"Use the COPY command with the manifest file to load data into Amazon Redshift.","C":"Use temporary staging tables during the loading process."},"discussion":[{"comment_id":"157930","poster":"Priyanka_01","content":"A & C\nCopy command and loading into temp staging tables","timestamp":"1632490680.0","upvote_count":"31"},{"poster":"carol1522","content":"A and c, because the goal is move data from s3 to redshift, and in the E we are not moving.","upvote_count":"14","comment_id":"161790","timestamp":"1632673020.0"},{"timestamp":"1685212200.0","upvote_count":"2","content":"A & C. But If you are going to appear exam in near future - redshift auto copy is now a new no-ETL feature and may replace these options.","poster":"Debi_mishra","comment_id":"908159"},{"content":"AC: I passed the test","timestamp":"1682956920.0","comment_id":"886457","poster":"pk349","upvote_count":"1"},{"upvote_count":"8","content":"Selected Answer: AC\nCorrect answers are A & C.\n\nOption B is wrong as S3DistCp is used to copy data between S3 and HDFS.\n\nOption D is wrong as UNLOAD helps unloading the data from Redshift to S3.\n\nOption E is wrong as Redshift Spectrum does not load the data into Redshift, but the requirement is to load.","comments":[{"upvote_count":"4","timestamp":"1667829060.0","content":"Option A as the COPY command loads data in parallel from Amazon S3, Amazon EMR, Amazon DynamoDB, or multiple data sources on remote hosts. COPY loads large amounts of data much more efficiently than using INSERT statements, and stores the data more effectively as well. Amazon S3 provides eventual consistency for some operations. Thus, it's possible that new data won't be available immediately after the upload, which can result in an incomplete data load or loading stale data. You can manage data consistency by using a manifest file to load data\n\nOption C as you can efficiently update and insert new data by loading your data into a staging table first. Amazon Redshift doesn't support a single merge statement (update or insert, also known as an upsert) to insert and update data from a single data source. However, you can effectively perform a merge operation. To do so, load your data into a staging table and then join the staging table with your target table for an UPDATE statement and an INSERT statement.","poster":"cloudlearnerhere","comment_id":"713074"}],"comment_id":"713073","timestamp":"1667829060.0","poster":"cloudlearnerhere"},{"poster":"dushmantha","content":"Selected Answer: AC\nB is not correct because its used with EMR. D is not correct because UNLOAD is used to put data from Redshift to S3. C seems to be involve lot of work, but E does not allow to move data to Redshift but the organization requires that and A is anyway correct. So I would go with A nd C","comment_id":"651154","timestamp":"1661330460.0","upvote_count":"1"},{"comment_id":"634380","poster":"rocky48","timestamp":"1658381640.0","content":"Selected Answer: AC\nA, C are correct","upvote_count":"1"},{"comment_id":"605255","upvote_count":"1","poster":"Bik000","timestamp":"1653205620.0","content":"Selected Answer: AC\nAnswer is A & C"},{"poster":"jrheen","timestamp":"1651350720.0","comment_id":"595270","upvote_count":"1","content":"Answer - A,C"},{"content":"A and C","upvote_count":"1","timestamp":"1637376480.0","comment_id":"482228","poster":"aws2019"},{"poster":"gunjan4392","content":"A, C are correct","upvote_count":"1","timestamp":"1636159980.0","comment_id":"388421"},{"timestamp":"1635702120.0","poster":"lostsoul07","upvote_count":"2","content":"A,C is the right answer","comment_id":"274375"},{"upvote_count":"10","comments":[{"upvote_count":"1","poster":"Ramshizzle","timestamp":"1655362740.0","comment_id":"617141","content":"Point 5 is also important to note in the article mentioned by Subho_in.\nAlso look at this why to use Staging tables: https://docs.aws.amazon.com/redshift/latest/dg/merge-create-staging-table.html"}],"content":"https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/\nPoint number 1 and 2. Option A and C must be the answer","comment_id":"268231","timestamp":"1635647400.0","poster":"Subho_in"},{"comment_id":"250483","timestamp":"1634379660.0","content":"I disagree with C. Question is about Loading data. Staging tables is about Transformation. It's A and E for me.","poster":"gtourkas","comments":[{"upvote_count":"1","content":"\"The organization want to load the dataset onto Amazon Redshift\". answer E is not moving any data not does help with it","comment_id":"647987","timestamp":"1660725060.0","poster":"APIsche"}],"upvote_count":"1"},{"poster":"jove","timestamp":"1634362740.0","upvote_count":"2","comment_id":"209975","content":"It's asking a \"combination of steps\", so they are A and C.."},{"poster":"sanjaym","timestamp":"1634059800.0","upvote_count":"2","comment_id":"205311","content":"A and C"},{"comment_id":"191926","poster":"syu31svc","content":"A & C for sure; the rest are clearly wrong","upvote_count":"2","timestamp":"1633867800.0"},{"content":"A and C.","upvote_count":"3","timestamp":"1633192020.0","poster":"Paitan","comment_id":"175617"},{"content":"My answer is A, C","upvote_count":"3","comment_id":"169380","timestamp":"1633149120.0","poster":"Nicki1013"},{"timestamp":"1632943080.0","poster":"zeronine","upvote_count":"5","comment_id":"162239","content":"A C - Question is about loading data"}],"url":"https://www.examtopics.com/discussions/amazon/view/28556-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"AC","isMC":true,"question_id":126,"answer_images":[],"exam_id":20,"answer_description":""},{"id":"izFJCrwaemUYZ19KNKXx","question_id":127,"answer":"BD","question_text":"A university intends to use Amazon Kinesis Data Firehose to collect JSON-formatted batches of water quality readings in Amazon S3. The readings are from 50 sensors scattered across a local lake. Students will query the stored data using Amazon Athena to observe changes in a captured metric over time, such as water temperature or acidity. Interest has grown in the study, prompting the university to reconsider how data will be stored.\nWhich data format and partitioning choices will MOST significantly reduce costs? (Choose two.)","question_images":[],"exam_id":20,"answer_description":"","unix_timestamp":1597385820,"url":"https://www.examtopics.com/discussions/amazon/view/28542-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"upvote_count":"44","poster":"Priyanka_01","timestamp":"1632894000.0","comment_id":"157848","content":"D :can save from 30% to 90% on your per-query costs and get better performance by compressing, partitioning, and converting your data into columnar formats.\nB: For partition"},{"content":"B and D are the right answers.\n\nSome background: Snappy compresses the data to help with I/O, it roughly does the same level of compression for both parquet and AVRO. AVRO stores the data in row format and does not compresses the data. However, Parquet is a columnar store (without any additional compression algorithm like snappy applied), it natively compresses the data by 2X to 5X on average. \n\nA) Since Parquet does a better job in compression, this option is incorrect\nB) This is correct since data is partitioned with keys (year, month, day) with medium cardinality. \nC) Even though ORC and Parquet are both columnar storage formats and both supported by Athena, Since no compression is used in this option, we can safely ignore this. \nD) Parquet with Snappy is a better choice than ORC with no compression, so this is correct. \nE) Adding sensor(ID) to the partition creates high cardinality on the partitions and may lead to multiple small files under each partition which will slow down performance. So, B is a better option as you can keep all 50 sensor data in a single file for a day.","poster":"jay1ram2","upvote_count":"30","comment_id":"280219","timestamp":"1635358440.0","comments":[{"poster":"wally_1995","comment_id":"939346","upvote_count":"1","timestamp":"1688153280.0","content":"I found this at this link:\n\nColumns that are used as filters are good candidates for partitioning.\nPartitioning has a cost. As the number of partitions in your table increases, the higher the overhead of retrieving and processing the partition metadata, and the smaller your files. Partitioning too finely can wipe out the initial benefit.\n\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\n\nSo I'd also go with B!"}]},{"timestamp":"1711059960.0","comment_id":"1179703","poster":"Sai3596","content":"In the question it is mentioned that \"Athena to observe changes in a captured metric over time, such as water temperature or acidity.\"\n\nNo signs of using a specific sensor and then observe metrics.\nSo if we introduce sensor in partition and not filter it in the query you are introducing an additional partition to search.\nThe request of the question it makes sense that we use B","upvote_count":"1"},{"comment_id":"971609","poster":"MLCL","timestamp":"1691114100.0","upvote_count":"1","content":"Selected Answer: BD\nBD : Makes the most sense"},{"timestamp":"1682957100.0","comments":[{"content":"i'm also passed the test, but the sensor id increases the cardinality of the dataset... then the best option is to partition the data by year, month, and day, compress and convert JSON to a colunar file, in this case, parquet file.","timestamp":"1703547660.0","comment_id":"1105592","poster":"GCPereira","upvote_count":"1"}],"poster":"pk349","comment_id":"886460","upvote_count":"1","content":"DE: I passed the test"},{"content":"Partitioning by sensor, year, month, and day (option E) would likely increase costs as compared to partitioning by only year, month, and day (option B) because it would create a larger number of smaller partitions. Each partition would contain data from a single sensor for a given date range, resulting in more small files that would need to be scanned by Athena for each query.\n\nSo B is better answer than E","poster":"rags1482","timestamp":"1679787480.0","upvote_count":"2","comment_id":"850553"},{"content":"Selected Answer: DE\npartition by sensor and then by year/month/day make sense, parquet with snappy gives best compressions","upvote_count":"1","timestamp":"1676299560.0","comment_id":"807502","poster":"murali12180"},{"poster":"Gabba","comment_id":"797641","content":"Selected Answer: BD\nB partition strategy better than E. \nD for sure.","upvote_count":"4","timestamp":"1675490940.0"},{"comment_id":"734328","comments":[{"poster":"rocky48","comment_id":"754848","timestamp":"1671884040.0","content":"Adding sensor(ID) to the partition creates high cardinality on the partitions and may lead to multiple small files under each partition which will slow down performance. But the question mentions about saving costs and not performance.","upvote_count":"2"}],"poster":"rocky48","timestamp":"1670061540.0","upvote_count":"2","content":"Selected Answer: DE\nD is an obvious choice. E has the highest potential to save costs also for queries that filter the sensor and the task is to find the solution with the most cost savings"},{"comments":[{"content":"(A. Store the data in Apache Avro format using Snappy compression) Option A includes compression but Parquet with Snappy Compression is better option because Avro stores data in row format per @jay1ram2. Correct me if I am wrong.","upvote_count":"1","poster":"Saneeda","comment_id":"714680","timestamp":"1668006600.0"}],"content":"Selected Answer: BD\nCorrect answers are B & D\n\nOption B as the data can be partitions by year, month, and day as it needs to be analyzed using captured metrics over time and not specific to any sensor.\n\nOption D as columnar data format helps to improve query performance.\n\nOptions A & C are wrong as Avro and ORC without compression would not provide query performance similar to parquet with compression.\n\nOption E is wrong as the data needs to be analyzed as per the metrics and not specific to a particular sensor.","upvote_count":"3","comment_id":"713075","timestamp":"1667829240.0","poster":"cloudlearnerhere"},{"comment_id":"708108","upvote_count":"1","poster":"aefuen1","timestamp":"1667177820.0","content":"Selected Answer: BD\nB and D. They are will query by time, not sensor id."},{"poster":"LukeTran3206","content":"Selected Answer: BD\nIf possible, avoid having a large number of small files – Amazon S3 has a limit of 5500 requests per second. Athena queries share the same limit.\nhttps://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html","upvote_count":"1","timestamp":"1665902700.0","comment_id":"696040"},{"comment_id":"678563","poster":"Arka_01","content":"Selected Answer: DE\nBecause more and optimal number of partitions can be done through the option E. Snappy compression with Parquet format, allows easy integration and maximum storage saving.","upvote_count":"1","timestamp":"1664093040.0"},{"timestamp":"1660737180.0","upvote_count":"1","poster":"ryuhei","content":"Selected Answer: BD\nAnswer is B & D","comment_id":"648058"},{"timestamp":"1658381400.0","poster":"rocky48","content":"Selected Answer: BD\nB and D are the right answers.","comments":[{"poster":"rocky48","comment_id":"734326","timestamp":"1670061420.0","upvote_count":"1","content":"E has the highest potential to save costs also for queries that filter the sensor and the task is to find the solution with the most cost savings."}],"comment_id":"634376","upvote_count":"2"},{"content":"Selected Answer: DE\nThe metrics of temperature and acidity may be varying between different locations of the lake, students may want to see if any issues at a particular location level based on metrics. So its advisable to partition by Sensor","comment_id":"631613","poster":"ru4aws","timestamp":"1657860300.0","upvote_count":"2"},{"comment_id":"626273","upvote_count":"1","timestamp":"1656787020.0","poster":"[Removed]","content":"I vote A & E. Vote for A because \"gather JSON-formatted batches of water quality values in Amazon S3\" is the requirement. We can't compress the Json format file using Parquet or ORC."},{"comment_id":"614850","content":"D columnar format can reduce the dose\nB not E\n1) not mention that they need data by sensor\n2) if add sensor as partition, the files will be split into small ones, which means it will be divided into 50 times, which will increase the cost","poster":"Mystica","upvote_count":"1","timestamp":"1654926960.0"},{"content":"Selected Answer: DE\nFor analytics, columnar formats are better, leaving ORC or Parquet (where Snappy compression is the default). Turning of compression in ORC is not good. -> D\nBetween the two partitions: I'm going with E because this has the highest potential to save costs also for queries that filter the senser and the task is to find the solution with the most cost savings (even though that might reduce query performance if files are too small)\nsee https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","comment_id":"593795","upvote_count":"4","timestamp":"1651147500.0","comments":[{"content":"In Kinesis Data Firehose-S3 delivery, the default prefix is already based on year, month, day, and hour.","poster":"samsanta2012","upvote_count":"1","comments":[{"timestamp":"1655405640.0","content":"From a storage cost-reduction perspective, it does not help in properly partitioning your data. You have to partition your data by a data source identifier (e.g. sensor ID or device ID) and date for better performance.","upvote_count":"1","poster":"samsanta2012","comment_id":"617367"}],"comment_id":"617366","timestamp":"1655405580.0"}],"poster":"Teraxs"},{"timestamp":"1646403660.0","upvote_count":"3","poster":"rb39","comment_id":"560828","content":"Selected Answer: BD\nQueries will check one parameter globally (for all sensors), so no point in having sensorId as part of the partition"},{"upvote_count":"5","content":"D, E\nE because you want to pull data by sensor; loading everything together accross 50 sensors make little sense","timestamp":"1636182240.0","poster":"Dr_Kiko","comment_id":"424538"},{"timestamp":"1635929940.0","comment_id":"390488","poster":"jueueuergen","comments":[{"upvote_count":"1","content":"I agree to. Probably 50 sensors are there to aggregate data for better representation. So by partitioning it by sensor would add extra step","comment_id":"627901","poster":"dushmantha","timestamp":"1657111620.0"}],"upvote_count":"7","content":"Between B and E: At first I thought E, but it says \"scattered across a local lake\". So the 50 sensors refer to different locations, not different metrics. If each sensor reported a different metric it would surely be E. However, since they all seem to measure water metrics, I'm quite sure it's B, because nowhere in the question does it say that queries regarding different locations are relevant."},{"content":"Answer B,D.","poster":"Donell","comments":[{"content":"I will also go for D,E.","upvote_count":"1","poster":"Donell","comment_id":"404694","timestamp":"1636177740.0"}],"timestamp":"1635876180.0","comment_id":"387926","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"I mean between B and E.","poster":"gtourkas","comment_id":"279274","timestamp":"1635344640.0"}],"poster":"gtourkas","timestamp":"1635191880.0","content":"D is one without doubt. Between B and D, I would opt for D since the number of fixed (50) so the effect on date-only restricted predicates wouldn't be that high compared to sensor-specific restricted predicates.","upvote_count":"1","comment_id":"279271"},{"timestamp":"1635070080.0","upvote_count":"2","comment_id":"274376","poster":"lostsoul07","content":"B,D is the right answer"},{"comment_id":"268242","poster":"Subho_in","content":"The data will be fetched to analyze the change over time not a particular time. So the query will touch all the time partitions anyways. And the other query parameter is metric type, so its better to keep the sensor as most significant partition.\nOption DE should be answer.","upvote_count":"2","timestamp":"1634931480.0"},{"content":"The question is interested in cost ONLY, and with Athena you pay by data scanned.\n\nWe don't KNOW the queries that will be run, but if you were to look at the following two examples queries, one to get data for a given day, the other to get all data (over all time) for a sensor:\n\nSelect * from sensor_data\nwhere year = 2020 and month = 01 and day = 01\n\nselect * from sensor_data\nwhere sensor = sensor1\n\nThe first query is still optimised against the partitioning strategy with sensor, year, month, date (we just get all sensors back for the day) and scans the same amount of data as if it were partitioned without the sensor, whereas the second query results in a scan of all data if we only partition by year, month, day.\n\nSo, D and E.","upvote_count":"9","poster":"kempstonjoystick","timestamp":"1634814480.0","comment_id":"267071"},{"comment_id":"262325","upvote_count":"4","content":"Answer : D & E\n\nReference : https://docs.aws.amazon.com/athena/latest/ug/partitions.html \n\nYou can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but loaded one time per day, may partition by a data source identifier and date.","timestamp":"1634731080.0","poster":"Roontha"},{"comment_id":"261703","poster":"CertNerd1234","comments":[{"upvote_count":"1","poster":"Roontha","content":"Agree with CertNerd1234","comment_id":"262326","timestamp":"1634743080.0"}],"timestamp":"1634716680.0","upvote_count":"1","content":"D&E\n“ For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but loaded one time per day, may partition by a data source identifier and date.”\nhttps://docs.aws.amazon.com/athena/latest/ug/partitions.html"},{"poster":"Draco31","timestamp":"1634618340.0","comment_id":"239017","content":"B and D. It will make sense to have temp/year/month/day.. or acidity/year/month/day, so you can choose by metric. But here E is by sensor, it make no sense for me you as will have to read all the sensor on each query to get your information. So the partition is useless","upvote_count":"2"},{"comment_id":"227677","poster":"hans1234","timestamp":"1634526840.0","content":"I think E is more likely over B, because it states values like \"water temperature or acidity\". You do not want to aggregate two different types of sensors together.","upvote_count":"1"},{"comment_id":"224507","poster":"passteque","timestamp":"1634519340.0","content":"=> MOST <=","upvote_count":"1"},{"content":"should be D, E\nThe temperature changes measured and compasion should be based on the same sensor, while it is less meaning for the compasion between different sensors.","poster":"passtest100","comment_id":"221536","timestamp":"1634428200.0","upvote_count":"8"},{"poster":"Skdbc","comment_id":"217608","upvote_count":"2","content":"Still not clear on why not E. if sensor corresponds to a metric then you will need either metric or sensor as part of the partition right?","timestamp":"1634413860.0"},{"comment_id":"205313","upvote_count":"2","timestamp":"1634054880.0","content":"B and D","poster":"sanjaym"},{"upvote_count":"1","content":"No need to include in sensor for partition so E is out\nAthena so Apache Parquet compressed (https://docs.aws.amazon.com/athena/latest/ug/compression-formats.html)\nB and D","timestamp":"1633723680.0","poster":"syu31svc","comment_id":"191935"},{"poster":"Paitan","content":"B and D. Analysis is not done per sensor so no need to include it in the partition.","comment_id":"175619","upvote_count":"6","timestamp":"1633180860.0"},{"comment_id":"163394","poster":"ramozo","upvote_count":"9","content":"D: Right.\nB: Because the analysis is on one metric in the lake not per sensor.","timestamp":"1633125300.0"},{"content":"my answer is DE","timestamp":"1633047660.0","upvote_count":"8","comment_id":"162238","comments":[{"comment_id":"169195","upvote_count":"2","content":"can you exlain why you would choose \"sensor\" as a partition?","comments":[{"poster":"LMax","upvote_count":"4","timestamp":"1634252340.0","content":"Because they want to track changes of metric over time. I believe this only makes sense if you do it for the same sensor.","comment_id":"214591","comments":[{"timestamp":"1634354040.0","upvote_count":"5","content":"Also, sensors are of a different type. So you don't want to search through the data from Temperature sensor, if you are analysing acidity metric change.","comments":[{"comment_id":"339916","timestamp":"1635558480.0","upvote_count":"2","comments":[{"comment_id":"339920","upvote_count":"1","timestamp":"1635779640.0","content":"And also question says metrics would be compared over time. So even if comparison happens between 2 sensors for multiple days - data scan would happen for 2 partitions only. if comparison happens among say all 50 sensors data for a particular day, then B is good. If we have to compare data for 2 days for a sensor, then data scan happens across 100 json files with option B. Same scenario - scan happens on 2 Json files.","poster":"sivajiboss"}],"content":"Good point, temperature sensor and the other would be pH sensor. It makes sense to partition by Sensors first. That way data scan would be limited and thus reducing the cost. If paritioned by Year,month,day.. each day will have 50 json files which will have to be scanned through and if temperature comparison is for multiple days then data scan will happen on no of days * 50 json files. If partitioned by sensor, year,month, day.. then you will be scanning only one partition if comparison happens on one sensor.","poster":"sivajiboss"}],"comment_id":"214593","poster":"LMax"}]}],"timestamp":"1633133400.0","poster":"awssp12345"}],"poster":"zeronine"},{"upvote_count":"3","poster":"mohitmittal94","comments":[{"content":"Why do we have to assume what is not stated? For 50 sensors, it is ideal to partition by sensor first before the dates.","timestamp":"1633976640.0","poster":"kamparia","upvote_count":"3","comment_id":"203265"}],"content":"D is correct as Priyanka_01 explained.\nWhy no E instead of B? \nIt says \"observe changes in a captured metric overtime\", assuming the query would run by each sensor.","timestamp":"1632899220.0","comment_id":"159219"}],"timestamp":"2020-08-14 08:17:00","topic":"1","answer_ET":"BD","choices":{"A":"Store the data in Apache Avro format using Snappy compression.","E":"Partition the data by sensor, year, month, and day.","B":"Partition the data by year, month, and day.","C":"Store the data in Apache ORC format using no compression.","D":"Store the data in Apache Parquet format using Snappy compression."},"answers_community":["BD (62%)","DE (38%)"],"answer_images":[],"isMC":true},{"id":"rVurZrFDdV0FrWozrEnv","choices":{"A":"Run an AWS Glue crawler that connects to one or more data stores, determines the data structures, and writes tables in the Data Catalog.","D":"Create an Apache Hive catalog in Amazon EMR with the table schema definition in Amazon S3, and update the table partition with a scheduled job. Migrate the Hive catalog to the Data Catalog.","B":"Use the AWS Glue console to manually create a table in the Data Catalog and schedule an AWS Lambda function to update the table partitions hourly.","C":"Use the AWS Glue API CreateTable operation to create a table in the Data Catalog. Create an AWS Glue crawler and specify the table as the source."},"isMC":true,"topic":"1","answer":"C","answer_description":"","answers_community":["C (92%)","8%"],"discussion":[{"poster":"Marc34","content":"C.\nhttps://docs.aws.amazon.com/glue/latest/dg/tables-described.html\nIn this section :\nUpdating Manually Created Data Catalog Tables Using Crawlers\n\"The following are other reasons why you might want to manually create catalog tables and specify catalog tables as the crawler source:\n\n You want to choose the catalog table name and not rely on the catalog table naming algorithm.\n\"","comments":[{"timestamp":"1633026240.0","upvote_count":"1","poster":"Phoenyx89","comment_id":"172543","content":"I agree is C. B is wrong because it takes more effort than B using Lambda, instead the Glue Crawler is full automated to update and find new partitions"},{"comment_id":"173032","timestamp":"1633533060.0","poster":"awssp12345","upvote_count":"1","content":"I change my answer to C"},{"content":"However there is no info on scheduling the crawler in C. any thoughts?","poster":"rsn","upvote_count":"1","comment_id":"834227","timestamp":"1678383240.0"}],"timestamp":"1632634800.0","upvote_count":"31","comment_id":"171458"},{"content":"Selected Answer: A\nI think Option A is correct. AWS Glue crawlers automatically scan data in S3, recognize the format and schema, and create metadata tables in the AWS Glue Data Catalog. This eliminates manual schema definition, table creation and meets new partitions with minimal effort. We can specify the database in which the tables are created and control the naming of the tables through the crawler's configuration settings rather than relying on an automated naming algorithm. As new data is added to S3 in hourly, daily, and yearly partitions, running the crawler at regular intervals ensures that new partitions are discovered and added to the respective Data Catalog tables automatically.\nOption C uses the AWS Glue API to create a table, which is more manual than allowing a crawler to discover and manage tables and does not automatically address the ongoing discovery of new partitions.","timestamp":"1708555200.0","comment_id":"1155923","poster":"NarenKA","upvote_count":"1"},{"upvote_count":"1","poster":"pk349","comment_id":"886462","timestamp":"1682957460.0","content":"C: I passed the test"},{"comment_id":"713078","poster":"cloudlearnerhere","timestamp":"1667829540.0","upvote_count":"3","content":"Selected Answer: C\nCorrect answer is C as the AWS Glue API CreateTable operation can be used to create a table in the Data Catalog and the AWS Glue crawler can be created and the table as the source can be specified. This meets the requirement of catalog table names and keeping the table updated with new data.\n\nB is incorrect. Although creating a new catalog table is right, the use of a Lambda function to update the table partitions entails a lot of development work. Remember that it is explicitly stated in the scenario that the solution should be implemented with the least configuration overhead.\nA is incorrect because you must create a new catalog table if you do not want to rely on the catalog table naming algorithm provided by AWS Glue.\nD is incorrect because this solution entails a lot of effort. A better and easier solution is to just create a new table in AWS Glue Data Catalog and set up an AWS Glue crawler."},{"comment_id":"693582","poster":"bp339","upvote_count":"2","content":"Selected Answer: C\nThe following are other reasons why you might want to manually create catalog tables and specify catalog tables as the crawler source:\n\nYou want to choose the catalog table name and not rely on the catalog table naming algorithm.\n\nYou want to prevent new tables from being created in the case where files with a format that could disrupt partition detection are mistakenly saved in the data source path.","timestamp":"1665634020.0"},{"upvote_count":"1","content":"Selected Answer: C\nAnswer is C","poster":"rocky48","timestamp":"1659817080.0","comment_id":"643511"},{"upvote_count":"1","content":"Selected Answer: C\nMy Answer is C","poster":"Bik000","timestamp":"1653132780.0","comment_id":"604864"},{"comment_id":"583427","content":"Selected Answer: C\nYou need to use the API to be able to provide a custom name","timestamp":"1649525100.0","poster":"rb39","upvote_count":"1"},{"upvote_count":"4","poster":"lakediver","comment_id":"506713","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/glue/latest/dg/tables-described.html#update-manual-tables","timestamp":"1640149500.0"},{"comment_id":"486276","timestamp":"1637788920.0","poster":"aws2019","upvote_count":"1","content":"Answer should be C."},{"upvote_count":"1","content":"C as per the linked provided not B \nread \"Updating Manually Created Data Catalog Tables Using Crawlers\"\nhttps://docs.aws.amazon.com/glue/latest/dg/tables-described.html","timestamp":"1635897540.0","poster":"sayed","comment_id":"298460"},{"poster":"lostsoul07","comment_id":"280812","timestamp":"1635647520.0","upvote_count":"2","content":"C is the right answer"},{"upvote_count":"1","comment_id":"223408","content":"I think there might be a miss typo in C, the table needs to be defined as target and not source","poster":"tleflond","comments":[{"timestamp":"1635607080.0","upvote_count":"1","content":"It also mentions this \nThe following are other reasons why you might want to manually create catalog tables and specify catalog tables as the crawler source:\n\n-You want to choose the catalog table name and not rely on the catalog table naming algorithm","comment_id":"234876","poster":"zevzek"},{"upvote_count":"2","content":"No typo I think..\nhttps://docs.aws.amazon.com/glue/latest/dg/tables-described.html\nUpdating Manually Created Data Catalog Tables Using Crawlers:\nTo do this, when you define a crawler, instead of specifying one or more data stores as the source of a crawl, you specify one or more existing Data Catalog tables. The crawler then crawls the data stores specified by the catalog tables. In this case, no new tables are created; instead, your manually created tables are updated.","timestamp":"1635540720.0","poster":"zevzek","comment_id":"234873"}],"timestamp":"1635361140.0"},{"poster":"LMax","comment_id":"214799","content":"C for me.","timestamp":"1635166680.0","upvote_count":"2"},{"comment_id":"205316","upvote_count":"2","content":"Answer should be C.","poster":"sanjaym","timestamp":"1634890980.0"},{"upvote_count":"2","comment_id":"191937","content":"Answer is C","poster":"syu31svc","timestamp":"1634442720.0"},{"timestamp":"1634251860.0","upvote_count":"3","comment_id":"175624","content":"Answer is C.","poster":"Paitan"},{"content":"Agree with C","poster":"singh100","timestamp":"1633900440.0","comment_id":"173299","upvote_count":"2"},{"timestamp":"1632443820.0","comments":[{"upvote_count":"1","comment_id":"169259","timestamp":"1632544320.0","poster":"awssp12345","content":"Agreed."}],"poster":"testtaker3434","content":"I agree with B. Link provided describes the solution","upvote_count":"2","comment_id":"167889"}],"exam_id":20,"question_text":"A healthcare company uses AWS data and analytics tools to collect, ingest, and store electronic health record (EHR) data about its patients. The raw EHR data is stored in Amazon S3 in JSON format partitioned by hour, day, and year and is updated every hour. The company wants to maintain the data catalog and metadata in an AWS Glue Data Catalog to be able to access the data using Amazon Athena or Amazon Redshift Spectrum for analytics.\nWhen defining tables in the Data Catalog, the company has the following requirements:\n✑ Choose the catalog table name and do not rely on the catalog table naming algorithm.\n✑ Keep the table updated with new partitions loaded in the respective S3 bucket prefixes.\nWhich solution meets these requirements with minimal effort?","question_images":[],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/29734-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2020-08-28 01:03:00","answer_ET":"C","unix_timestamp":1598569380,"question_id":128},{"id":"iKssC9CFWaDPIZaU7J2i","url":"https://www.examtopics.com/discussions/amazon/view/29824-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_images":[],"answer_images":[],"timestamp":"2020-08-28 12:25:00","topic":"1","exam_id":20,"unix_timestamp":1598610300,"question_text":"A large university has adopted a strategic goal of increasing diversity among enrolled students. The data analytics team is creating a dashboard with data visualizations to enable stakeholders to view historical trends. All access must be authenticated using Microsoft Active Directory. All data in transit and at rest must be encrypted.\nWhich solution meets these requirements?","isMC":true,"question_id":129,"answer_ET":"B","answers_community":["B (78%)","D (22%)"],"choices":{"B":"Amazon QuickSight Enterprise edition configured to perform identity federation using SAML 2.0 and the default encryption settings.","C":"Amazon QuckSight Standard edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS.","A":"Amazon QuickSight Standard edition configured to perform identity federation using SAML 2.0. and the default encryption settings.","D":"Amazon QuickSight Enterprise edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS."},"discussion":[{"upvote_count":"31","content":"Answer is B\nAuthentication: https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers-setting-up-saml.html#external-identity-providers-config-idp \nEncryption: https://docs.aws.amazon.com/quicksight/latest/user/data-encryption-at-rest.html\n\"All keys associated with Amazon QuickSight are managed by AWS.\" in https://docs.aws.amazon.com/quicksight/latest/user/key-management.html - No way to use customer-provided keys in QuickSight","timestamp":"1634248200.0","comment_id":"233541","comments":[{"comments":[{"upvote_count":"7","poster":"Edwars","content":"A and B aren't both correct, because encryption at rest is only available with Entreprise edition\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-encryption.html\nSo, answer is B","comment_id":"627924","timestamp":"1657115640.0"}],"comment_id":"344286","poster":"chengxu32","upvote_count":"5","content":"\"Single Sign On with SAML or OpenID Connect\" is available in both Standard and Enterprise edition, which means both A and B are correct. Since this is not a multiple choice question, then both A and B are out. Active Directory is only available in Enterprise edition, so the answer is D","timestamp":"1635982620.0"},{"upvote_count":"4","content":"I don't know how much has changed after 20 months since you posted this. But\nAccording to: \nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html\nIf you have an existing directory that you want to use for Amazon QuickSight, you can use Active Directory Connector.\n\nSince it's only available in enterprise, then the only option here to choose is D.\n\nAlso https://docs.aws.amazon.com/quicksight/latest/user/key-management.html gives a full tutorial on how to use a customer provided key as the encryption key","comment_id":"940174","timestamp":"1688229120.0","poster":"wally_1995"}],"poster":"kikakiko"},{"timestamp":"1633710360.0","poster":"GauravM17","comment_id":"177025","upvote_count":"11","content":"All Keys are managed by QuickSight enterprise edition and hence D can not be the answer. I would go with B"},{"comment_id":"1160338","upvote_count":"1","timestamp":"1709022720.0","content":"Selected Answer: B\nAnswer is B. \n\nAn AWS Organization is required to use IAM Identity Center with AD Connector. https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_getting_started.html, https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html\n\nSince the univerasity does not has an AWS Organization and did not mention that they need one. Option B would b a more feasible option","poster":"awsmonster"},{"timestamp":"1708812300.0","poster":"LeoSantos121212121212121","upvote_count":"1","content":"ChatGPT chose answer D.","comment_id":"1158179"},{"upvote_count":"3","content":"Selected Answer: B\nOption D mentioned using AD Connector and configuring Amazon QuickSight to use customer-provided keys imported into AWS Key Management Service (KMS). While using customer-provided keys in AWS KMS for encryption offers additional control over encryption keys, the question does not specify a requirement that necessitates this level of key management. Additionally, the AD Connector is not a feature of Amazon QuickSight; instead, the Enterprise edition supports direct AD integration.\nTherefore, option B is the correct solution, as it leverages the capabilities of Amazon QuickSight Enterprise edition to meet the university’s requirements for Active Directory authentication and data encryption.","timestamp":"1708555560.0","poster":"NarenKA","comment_id":"1155928"},{"poster":"pn12345","timestamp":"1702225560.0","upvote_count":"1","content":"Selected Answer: D\nCorrect answer","comment_id":"1092650"},{"timestamp":"1699582440.0","poster":"LocalHero","content":"Probably B is correct when the problem was created in the past.\nbut now D looks like also correct.(AD connecter looks like very easy) \nEncryption method is not defined in the sentence.\nso B is more correct.It is less effort.ummm","upvote_count":"1","comment_id":"1066889"},{"upvote_count":"1","timestamp":"1693452420.0","comment_id":"994710","content":"Ans :D","poster":"nroopa"},{"upvote_count":"1","content":"Selected Answer: B\nB : SAML 2.0 works with AD, \nEnterprise Edition offers encryption at rest.\nAll data is encrypted in transit by default https://docs.aws.amazon.com/quicksight/latest/user/data-encryption-in-transit.html","comment_id":"971612","poster":"MLCL","timestamp":"1691114580.0"},{"comments":[{"comment_id":"1005975","content":"Sure you pass the test. But this one you got it wrong. The answer is \"D\"","timestamp":"1694541960.0","upvote_count":"5","poster":"r3mo"}],"comment_id":"886463","poster":"pk349","timestamp":"1682957520.0","upvote_count":"1","content":"B: I passed the test"},{"content":"Selected Answer: D\nQuickSight enables you to encrypt your SPICE datasets using the keys you have stored in AWS Key Management Service. This provides you with the tools to audit access to data and satisfy regulatory security requirements. If you need to do so, you have the option to immediately lock down access to your data by revoking access to AWS KMS keys.","comment_id":"885713","upvote_count":"1","timestamp":"1682898780.0","poster":"Mirandaali"},{"content":"D seems to be right. Reason: it is stated that user authentication must be via Microsoft Active Directory. This rules out options A, B and C. A and B mention SAML, C mentions an AD Connector but this is only supported in the Enterprise Edition. Source: https://docs.aws.amazon.com/quicksight/latest/user/directory-integration.html and https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html. And yes it is possible to use CMKs with aws managed KMS, even for SPICE data not just meta data. Source: https://docs.aws.amazon.com/quicksight/latest/user/key-management.html - Using customer-managed keys from AWS KMS with SPICE datasets in Amazon QuickSight","upvote_count":"2","timestamp":"1681382400.0","poster":"tbhtp","comment_id":"869298"},{"comment_id":"850567","content":"D is the right answer\nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html","timestamp":"1679789220.0","poster":"rags1482","upvote_count":"2"},{"comment_id":"843958","upvote_count":"2","content":"Selected Answer: D\nTo create customer-managed keys (CMKs), you use AWS Key Management Service (AWS KMS) in the same AWS account and AWS Region as the Amazon QuickSight SPICE dataset. A QuickSight administrator can then use a CMK to encrypt SPICE datasets and control access.\nhttps://docs.aws.amazon.com/quicksight/latest/user/key-management.html","comments":[{"poster":"akashm99101001com","timestamp":"1679243880.0","upvote_count":"1","content":"Key statement - \"All data in transit and at rest must be encrypted.\" A and B are out","comment_id":"843968"}],"poster":"akashm99101001com","timestamp":"1679243760.0"},{"comment_id":"811882","timestamp":"1676639220.0","upvote_count":"1","poster":"Arjun777","content":"QuickSight enables you to encrypt your SPICE datasets using the keys you have stored in AWS Key Management Service. This provides you with the tools to audit access to data and satisfy regulatory security requirements. If you need to do so, you have the option to immediately lock down access to your data by revoking access to AWS KMS keys. All data access to encrypted datasets in QuickSight SPICE is logged in AWS CloudTrail. Administrators or auditors can trace data access in CloudTrail to identify when and where data was accessed.\n\nTo create customer-managed keys (CMKs), you use AWS Key Management Service (AWS KMS) in the same AWS account and AWS Region as the Amazon QuickSight SPICE dataset. A QuickSight administrator can then use a CMK to encrypt SPICE datasets and control access."},{"comment_id":"799824","upvote_count":"3","content":"Answer should be D. This allow the use of customer managed keys: https://docs.aws.amazon.com/quicksight/latest/user/key-management.html\nAnd easy enabling of AD","timestamp":"1675692600.0","poster":"SorenBendixen"},{"comment_id":"768754","upvote_count":"1","content":"Selected Answer: B\nAnswer is B","poster":"Ody__","timestamp":"1673111160.0"},{"timestamp":"1669715880.0","upvote_count":"1","content":"Selected Answer: B\nhttps://github.com/awsdocs/amazon-quicksight-user-guide/blob/main/doc_source/editions.md\n\nFeatures Standard Edition Enterprise Edition\nConnect to Active Directory ✓\nUse Active Directory groups ✓","poster":"nadavw","comment_id":"730197","comments":[{"poster":"nadavw","comment_id":"730199","content":"Secure data encryption at rest only at enterprise","timestamp":"1669715940.0","upvote_count":"1"}]},{"poster":"cloudlearnerhere","timestamp":"1667829840.0","content":"Selected Answer: B\nB is correct as Amazon QuickSight Enterprise edition integrates with your existing directories, using either Microsoft Active Directory or single sign-on (SSO) using Security Assertion Markup Language (SAML).\n\nA & C are both incorrect because you can only use AD Connector for Amazon QuickSight Enterprise Edition, not on the Standard edition. Encryption at rest is available in Enterprise edition only\n\nD is incorrect because in Amazon QuickSight Enterprise edition, the data at rest in SPICE is encrypted using block-level encryption with AWS-managed keys. You cannot use customer-provided keys that are imported into AWS KMS.","comment_id":"713080","upvote_count":"6"},{"timestamp":"1658445180.0","comment_id":"634888","poster":"rocky48","content":"Selected Answer: B\nAnswer is B","upvote_count":"1"},{"upvote_count":"1","content":"Either B or D","poster":"Bik000","timestamp":"1653205440.0","comment_id":"605254"},{"upvote_count":"1","comment_id":"595280","timestamp":"1651351440.0","poster":"jrheen","content":"Answer - B"},{"upvote_count":"4","poster":"CHRIS12722222","comment_id":"573084","timestamp":"1647966780.0","content":"Guys, SAML 2.0 identity federation can be done with microsoft active directory...B is more appropriate. Quicksight dont work with customer managed keys (imported or not)\n\n\"You can use a third-party identity provider that supports through Security Assertion Markup Language 2.0 (SAML 2.0) to provide a simple onboarding flow for your Amazon QuickSight users. Such identity providers include Microsoft Active Directory Federation Services, Okta, and Ping One Federation Server\".\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers.html"},{"content":"B is my answer\n- At rest in SPICE cannot be done using KMS keys. It only supports AWS-managed keys","timestamp":"1644171300.0","poster":"vkbajoria","upvote_count":"1","comment_id":"541939"},{"poster":"arun004","upvote_count":"1","comment_id":"489514","timestamp":"1638145800.0","content":"Answer is B"},{"upvote_count":"1","timestamp":"1637378460.0","poster":"aws2019","content":"B is the ans","comment_id":"482241"},{"poster":"Donell","content":"Answer B.\nNot D because of the following reason:\nD is incorrect because in Amazon QuickSight Enterprise edition, the data at rest in SPICE is encrypted using block-level encryption with AWS-managed keys. You cannot use customer-provided keys that are imported into AWS KMS.","upvote_count":"1","timestamp":"1636142880.0","comment_id":"387935"},{"content":"I go with B.. \nhttps://docs.aws.amazon.com/quicksight/latest/user/data-encryption-at-rest.html\n\n\"Minimal data necessary to coordinate user identification with your Microsoft Active Directory or identity federation implementation (Federated Single Sign-On (SSO) through Security Assertion Markup Language 2.0 (SAML 2.0)).\"\n\"Encryption for each source that you use for data is controlled by that data source or file system. Amazon QuickSight doesn't store any actual data except metadata and data that you upload into SPICE. In Enterprise edition, data at rest in SPICE is encrypted using block-level encryption with AWS-managed keys. In Standard edition, data at rest in SPICE is securely stored, but not encrypted. \"","poster":"AjithkumarSL","timestamp":"1636103280.0","comment_id":"350674","upvote_count":"5"},{"upvote_count":"5","timestamp":"1635838620.0","poster":"sivajiboss","content":"Answer is D, to authenticate with existing Microsoft Active directory, got to use AD connector\nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html","comment_id":"340522"},{"upvote_count":"4","poster":"sayed","timestamp":"1635487500.0","comment_id":"297929","content":"D \nIf you have an existing directory that you want to use for Amazon QuickSight, you can use Active Directory Connector. This service redirects directory requests to your Active Directory—in another AWS Region or on-premises—without caching any information in the cloud.\nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html\n\nand customer provided keys in KMS means that customer provide the key only but it is managed by AWS\nso i think it is D"},{"content":"B is the right answer","timestamp":"1635209280.0","comment_id":"280817","poster":"lostsoul07","upvote_count":"5"},{"timestamp":"1635062460.0","comment_id":"280209","poster":"jay1ram2","comments":[{"timestamp":"1635392280.0","comment_id":"283612","poster":"Magicroko","upvote_count":"1","content":"Are the customer-provided keys imported into AWS KMS for rest encrypted? \nThen D"}],"upvote_count":"1","content":"I believe the answer options provided here is not Accurate. Only QuickSight Enterprise Edition supports Microsoft AD and Encryption (using AWS Managed KMS Keys). So, if I have to choose with the exact language, I will choose D as it has AD and KMS called out."},{"comment_id":"253028","upvote_count":"4","timestamp":"1634981100.0","content":"Well B is a good answer if authentication using SAML via external IDP. For authentication via Microsoft AD, then D is only the answer\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html","poster":"Par301"},{"poster":"anotherlameaccount","upvote_count":"1","content":"i think that both answers B & D can be a solution to the points: AD auth + Encryption. However, answer D adds some additional complexity to the situation that is not required in the task. Id go for B as well.","comment_id":"252850","timestamp":"1634929380.0"},{"timestamp":"1634563680.0","poster":"lydia_young","upvote_count":"2","content":"I will go with B","comment_id":"252702"},{"poster":"Rajkbasu","upvote_count":"1","content":"Answer - D. \nOption B missing Active directory authentication point.","comment_id":"223870","timestamp":"1634208300.0"},{"upvote_count":"2","timestamp":"1634101020.0","poster":"[Removed]","content":"100% B. D is not possible","comment_id":"217614"},{"upvote_count":"1","timestamp":"1634057820.0","comment_id":"209587","content":"I agree with option B.\nIn both editions of quicksight , all transfers of data encrypted and database connections are secured using SSL and all of the transfers are secured using TLS.\nIn Enterprise edition, data at rest in SPICE is encrypted using block-level encryption with AWS-managed keys.","poster":"vanireddy"},{"timestamp":"1633984680.0","comment_id":"205329","upvote_count":"1","content":"it's B Vs D. Incline toward B because \"In Enterprise edition, data at rest in SPICE is encrypted using block-level encryption with AWS-managed keys\" but not sure customer provided key can be used or not.\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-encryption-at-rest.html","poster":"sanjaym"},{"poster":"syu31svc","timestamp":"1633792320.0","upvote_count":"4","content":"I would take D as the answer\nhttps://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-encryption.html","comment_id":"191939"},{"timestamp":"1633498320.0","upvote_count":"1","content":"Why do we need customer provided keys in KMS? So will go with B.","poster":"Paitan","comment_id":"175657"},{"comment_id":"168482","poster":"ramozo","content":"D. Encryption at rest is only provided by Enterprise Edition\nhttps://docs.aws.amazon.com/quicksight/latest/user/data-encryption.html","comments":[{"poster":"testtaker3434","timestamp":"1632319560.0","comments":[{"comment_id":"169266","timestamp":"1632544980.0","content":"I agree. The question does not mention anything about importing customer-provided keys into AWS KMS so D did not feel right.","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"173037","comments":[{"content":"QuickSight by default encrypts the data at rest in enteprise edition? Do we have even option to manage KMS keys on QuickSight? I believe B is the right answer.\nhttps://docs.aws.amazon.com/quicksight/latest/user/key-management.html","poster":"GauravM17","upvote_count":"1","comment_id":"174361","timestamp":"1633195620.0"}],"content":"I change my answer to D since AD connector is more appropriate in this scenario.","poster":"awssp12345","timestamp":"1632736320.0"}],"poster":"awssp12345"}],"upvote_count":"5","comment_id":"168793","content":"I think B is the answer.. Have you seen thin :\nhttps://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers.html"}],"upvote_count":"1","timestamp":"1632157620.0"},{"comments":[{"content":"Agree with B. \nOnly Enterprise Encryption options apply here and seems like choosing Encryption Key is not an supported for QuickSight.","comment_id":"214800","poster":"LMax","timestamp":"1634067900.0","upvote_count":"1"}],"timestamp":"1632079980.0","content":"I will pick B","comment_id":"168326","poster":"testtaker3434","upvote_count":"2"}],"answer":"B","answer_description":""},{"id":"sKhl5u0gWEZ6SpynwOrx","topic":"1","answers_community":["CDF (100%)"],"exam_id":20,"unix_timestamp":1598569200,"choices":{"D":"Use an S3 bucket in the same Region as Athena.","C":"Compress the objects to reduce the data transfer I/O.","B":"Use an S3 bucket in the same account as Athena.","A":"Add a randomized string to the beginning of the keys in S3 to get more throughput across partitions.","E":"Preprocess the .csv data to JSON to reduce I/O by fetching only the document keys needed by the query.","F":"Preprocess the .csv data to Apache Parquet to reduce I/O by fetching only the data blocks needed for predicates."},"discussion":[{"poster":"carol1522","content":"For me is CDF","timestamp":"1633212960.0","comment_id":"173558","upvote_count":"26","comments":[{"poster":"GauravM17","comment_id":"174371","timestamp":"1633377840.0","content":"Parquet file is by default compressed which is convered under F. The answer should be A,D,F","upvote_count":"4"}]},{"comments":[{"timestamp":"1662528660.0","upvote_count":"3","content":"@Woong Thanks for quoting the excerpt. This makes option \"A\" incorrect.\nSharing the link to this statement for anyone who wish to verify\nhttps://d1.awsstatic.com/whitepapers/AmazonS3BestPractices.pdf\n\"This guidance supersedes any previous guidance on optimizing performance for Amazon S3. For\nexample, previously Amazon S3 performance guidelines recommended randomizing prefix naming\nwith hashed characters to optimize performance for frequent data retrievals. You no longer have to\nrandomize prefix naming for performance, and can use sequential date-based naming for your prefixes\"","comment_id":"662041","poster":"Abep"},{"timestamp":"1634114100.0","upvote_count":"3","content":"That's absolutely right, hence should be C,D,F","comment_id":"181208","poster":"vicks316"}],"comment_id":"178963","content":"A is not best practice any more. [Quoted]previously Amazon S3 performance guidelines recommended randomizing prefix naming with hashed characters to optimize performance for frequent data retrievals. You no longer have to randomize prefix naming for performance, and can use sequential date-based naming for your prefixes. [Unquoted]","timestamp":"1633950120.0","poster":"Woong","upvote_count":"14"},{"content":"The comment about random strings is also in this useful link:\nhttps://docs.aws.amazon.com/whitepapers/latest/s3-optimizing-performance-best-practices/introduction.html","timestamp":"1700298480.0","comment_id":"1073870","upvote_count":"1","poster":"monkeydba"},{"content":"C and F are not doubt easy answers. But I believe A and D both are correct. People quoting randomize prefix not required - are ignoring \"sequential date-based naming\" which is also not mentioned in question and Athen can run longer as S3 list operations will take time without a well distributed prefix.","timestamp":"1685213460.0","poster":"Debi_mishra","upvote_count":"1","comment_id":"908167"},{"content":"CDF: I passed the test","comment_id":"886464","upvote_count":"3","timestamp":"1682957580.0","poster":"pk349"},{"upvote_count":"1","timestamp":"1669777560.0","content":"A,C,F\nSome data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. In this scenario, millions of data points are stored on Amazon S3 and it is recommended to create a random string and add that to the beginning of the object prefixes to increase the read performance for S3 objects.","poster":"henom","comment_id":"731043"},{"timestamp":"1667830020.0","comment_id":"713083","poster":"cloudlearnerhere","content":"Selected Answer: CDF\nCorrect answers are C, D & F\n\nOptions C & F as using compression and columnar data format helps improve query performance and optimize storage\nOption D as using Athena and S3 within the same region would help with query performance and cost.\n\nOption A is wrong as S3 scales automatically now and is not bounded by the restriction.\n\nOption B is wrong as using the same account does not help in optimizing the cost of query performance.\n\nOption E is wrong as using JSON is the same as using CSV files and does help in n optimizing the cost or query performance.","upvote_count":"8"},{"poster":"rocky48","comment_id":"637730","content":"Selected Answer: CDF\nSelected Answer: CDF","upvote_count":"1","timestamp":"1658885640.0"},{"content":"Ans A. can't be correct at all.\nAdding randomized string will make partition size too small to reap the benefits.\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","upvote_count":"1","timestamp":"1655936520.0","comment_id":"620670","poster":"GiveMeEz"},{"content":"Selected Answer: CDF\nC,D,F fulfills are needs","poster":"f4bi4n","timestamp":"1653204720.0","upvote_count":"1","comment_id":"605233"},{"comment_id":"604834","poster":"Bik000","upvote_count":"1","content":"Selected Answer: CDF\nMy Answer is C, D & F","timestamp":"1653131040.0"},{"timestamp":"1637789100.0","content":"For me is CDF","upvote_count":"4","comment_id":"486278","poster":"aws2019"},{"timestamp":"1636270140.0","comment_id":"421024","upvote_count":"1","poster":"mickies9","content":"As a best practice, S3 and Athena should be in the same region and account and columnar based is appropriate for the performance. My answer would be BDF"},{"poster":"jueueuergen","upvote_count":"3","content":"CDF.\nParquet compresses by default, yes, but there is also an \"uncompressed\" option. So C is not redundant.","comment_id":"392601","timestamp":"1636069380.0"},{"timestamp":"1635888720.0","poster":"gunjan4392","content":"I think CDF","upvote_count":"1","comment_id":"388435"},{"content":"I goes with C,D,F.","upvote_count":"1","timestamp":"1635696780.0","poster":"Donell","comment_id":"387939"},{"poster":"DerekKey","comment_id":"351395","content":"C -> is WRONG in my opinion\n1. How do you want to compress Apache Parquet that is already compressed by default? We selected Parquet as file format in F.\n\"For Athena, we recommend using either Apache Parquet or Apache ORC, which compress data by default and are splittable.\"\n2. We still need prefixes but we don't have to randomize them\nYou can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.\nBUT\nYou no longer have to randomize prefix naming for performance and can use sequential date-based naming for your prefixes.\n3. You can not reduce data transfer I/O. I/O represents an entity that sends/receives data, therefore, you can only reduce parameters of I/O e.g. data transfer bandwidth, speed, no of operations (e.g. IOPS) etc.","timestamp":"1635494460.0","upvote_count":"1"},{"upvote_count":"4","content":"C, D, F is the right answer","timestamp":"1635332040.0","poster":"lostsoul07","comment_id":"280819"},{"poster":"Roontha","content":"Answer ; C,D,F\n\nOption A is no longer valid as per AWS documentation","timestamp":"1634821500.0","upvote_count":"2","comment_id":"263618"},{"content":"CDF https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","comment_id":"218852","upvote_count":"4","poster":"Skdbc","timestamp":"1634666940.0"},{"poster":"LMax","content":"B and D are ruled out because the question doesn't say they are not in the same account or region right now.\nE and F are alternatives to each other, where F is better alternative as it is Column Store format that provides better query performance.\nAs in total we need to choose three, I get a list of A, C, F","comment_id":"214603","timestamp":"1634531880.0","upvote_count":"2","comments":[{"comment_id":"214606","content":"Given the updated performance guidelines I would have to agree that A should be replaced. Choosing what to take instead, will go for D. \nSo my choice is C, D, F.","poster":"LMax","timestamp":"1634586240.0","upvote_count":"1"}]},{"upvote_count":"3","comment_id":"210061","poster":"jove","content":"C D F for sure","timestamp":"1634210400.0"},{"timestamp":"1633596000.0","upvote_count":"6","comment_id":"177438","comments":[{"upvote_count":"1","comment_id":"503147","poster":"Ritzritz","content":"“ You no longer have to randomize prefix naming for performance, and can use sequential date-based naming for your prefixes.”","timestamp":"1639682880.0"}],"poster":"Paitan","content":"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\nAs per this updated article Amazon no longer recommends randomizing prefix naming with hashed characters to optimize performance for frequent data retrievals. So that rules out option A I guess. So C,D and F should work. However Parquets are compressed by default, so why do we need C then? Confused..."},{"upvote_count":"8","comments":[{"poster":"awssp12345","timestamp":"1632899580.0","upvote_count":"1","comment_id":"169278","content":"Agreed"},{"poster":"Phoenyx89","upvote_count":"1","content":"Agree with D and F but why A?","timestamp":"1633112580.0","comment_id":"172548"}],"content":"I pick ADF","poster":"testtaker3434","timestamp":"1632180180.0","comment_id":"167887"}],"url":"https://www.examtopics.com/discussions/amazon/view/29732-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2020-08-28 01:00:00","question_images":[],"answer_images":[],"answer_description":"","answer_ET":"CDF","isMC":true,"question_text":"An airline has been collecting metrics on flight activities for analytics. A recently completed proof of concept demonstrates how the company provides insights to data analysts to improve on-time departures. The proof of concept used objects in Amazon S3, which contained the metrics in .csv format, and used Amazon\nAthena for querying the data. As the amount of data increases, the data analyst wants to optimize the storage solution to improve query performance.\nWhich options should the data analyst use to improve performance as the data lake grows? (Choose three.)","answer":"CDF","question_id":130}],"exam":{"id":20,"lastUpdated":"11 Apr 2025","numberOfQuestions":164,"provider":"Amazon","isImplemented":true,"isBeta":false,"name":"AWS Certified Data Analytics - Specialty","isMCOnly":true},"currentPage":26},"__N_SSP":true}