{"pageProps":{"questions":[{"id":"l4JLB2RzCPLSx0ofXgIY","answer_description":"","choices":{"D":"Set the F1 score as the objective metric for a new SageMaker automatic hyperparameter tuning job. Double the maximum training jobs parameter that was used in the previous tuning job.","B":"Set the Area Under the ROC Curve (AUC) as the objective metric for a new SageMaker automatic hyperparameter tuning job. Use the same maximum training jobs parameter that was used in the previous tuning job.","A":"Run a SageMaker incremental training based on the best candidate from the current model's tuning job. Monitor the same metric that was used as the objective metric in the previous tuning, and look for improvements.","C":"Run a SageMaker warm start hyperparameter tuning job based on the current modelâ€™s tuning job. Use the same objective metric that was used in the previous tuning."},"topic":"1","question_id":126,"url":"https://www.examtopics.com/discussions/amazon/view/99688-exam-aws-certified-machine-learning-specialty-topic-1/","answer_images":[],"discussion":[{"content":"Selected Answer: C\nA. NO - Incremental training not supported by XGBoost (https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html)\nB. NO - we don't want to change the objective and restart from scratch\nC. YES - warm start can leverage new data from production for further tuning\nD. NO - we don't want to start from the training from scratch or use F1 score as objective","poster":"loict","upvote_count":"3","comment_id":"1004846","timestamp":"1726061400.0"},{"comment_id":"984745","poster":"Mickey321","upvote_count":"1","timestamp":"1724009460.0","content":"Selected Answer: C\nAnswer C"},{"comment_id":"980937","poster":"kaike_reis","upvote_count":"2","timestamp":"1723651740.0","content":"Given time constraint, I believe that C is the crrect one."},{"upvote_count":"3","timestamp":"1713791640.0","comment_id":"877294","poster":"Ahmedhadi_","content":"Selected Answer: C\nC is the correct answer because it uses the results from past HPO jobs and builds upon them to improve accuracy."},{"poster":"mawsman","comment_id":"872738","upvote_count":"4","timestamp":"1713361500.0","content":"Selected Answer: C\nI go with C - warm start, A is not supported on XGBoost, and other options will start tuning from scratch and might be just as bad as the inital tuning job. We only have 1 day, so more tuning with existing job to inform the new trainging job is the only option here https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html"},{"content":"C is the correct answer.\nYou can't use Incremental training on Xgboost algorithm https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html","upvote_count":"1","comment_id":"857683","poster":"Mllb","timestamp":"1711956840.0","comments":[{"upvote_count":"1","comment_id":"865682","content":"It appears in 2023-April-3","poster":"Mllb","timestamp":"1712681400.0"}]},{"timestamp":"1710341040.0","upvote_count":"3","comment_id":"837990","content":"Selected Answer: B\nSince ROC-AUC is presumed to be one of the best for a binary classification. Hence option B. Option A -- Incremental training is suited wherein the training dataset gets updated frequently.","poster":"SANDEEP_AWS"},{"poster":"drcok87","content":"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html\nhttps://francesca-donadoni.medium.com/training-an-xgboost-model-for-pricing-analysis-using-aws-sagemaker-55d777708e52\n \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html\n\nC","comment_id":"812195","comments":[{"poster":"drcok87","comment_id":"814718","upvote_count":"1","content":"also it cannot be a: \"Only three built-in algorithms currently support incremental training: Object Detection - MXNet, Image Classification - MXNet, and Semantic Segmentation Algorithm.\" from https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html","timestamp":"1708393560.0"}],"upvote_count":"3","timestamp":"1708193460.0"}],"question_images":[],"unix_timestamp":1676657460,"question_text":"A bank wants to use a machine learning (ML) model to predict if users will default on credit card payments. The training data consists of 30,000 labeled records and is evenly balanced between two categories. For the model, an ML specialist selects the Amazon SageMaker built-in XGBoost algorithm and configures a SageMaker automatic hyperparameter optimization job with the Bayesian method. The ML specialist uses the validation accuracy as the objective metric.\n\nWhen the bank implements the solution with this model, the prediction accuracy is 75%. The bank has given the ML specialist 1 day to improve the model in production.\n\nWhich approach is the FASTEST way to improve the model's accuracy?","timestamp":"2023-02-17 19:11:00","isMC":true,"answer_ET":"C","answers_community":["C (79%)","B (21%)"],"exam_id":26,"answer":"C"},{"id":"k597sVa1g9BWKbPmdiRx","answer_description":"","timestamp":"2023-02-10 22:57:00","discussion":[{"poster":"jopaca1216","comment_id":"1010004","upvote_count":"1","content":"B is right.\nIs very simple to create a conversion file JOB in AWS Glue, using just 3 workflow steps.\nWITH NO CODE.. CREATED AUTOMATICALLY BY GLUE (Scala or Python)\n\n(s3 - source data file) --> (Data Mapping) --> (target transformed data file)","timestamp":"1726598760.0"},{"timestamp":"1726061580.0","upvote_count":"1","content":"Selected Answer: B\nA. NO - Crawler is to populate the data catalog\nB. YES - leverage serverless for distributed processing\nC. NO - Altough EMR can run Spark like Glue, it is not serverless \nD. NO - using the PySpark kernel will be single instance (running in the notebook)","poster":"loict","comment_id":"1004852"},{"upvote_count":"2","comment_id":"984755","poster":"Mickey321","content":"Selected Answer: B\nOption B is better than option A because option A uses an AWS Glue crawler to convert the file format. A crawler is a component of AWS Glue that scans your data sources and infers the schema, format, partitioning, and other properties of your data. A crawler can create or update a table in the AWS Glue Data Catalog that points to your data source. However, a crawler cannot change the format of your data source itself. You still need to write a script or use a tool to convert your CSV files to Parquet files.","timestamp":"1724010360.0"},{"content":"Selected Answer: B\nOption B.\nA - Glue crawler creates Glue Data Catalog from S3 buckets. It can be used to query by athena.\nC, D - not serverless and not generally used for etl.","upvote_count":"2","poster":"GiyeonShin","comment_id":"817320","timestamp":"1708563180.0"},{"poster":"AjoseO","timestamp":"1708384920.0","content":"Selected Answer: B\nAWS Glue is a fully-managed ETL service that makes it easy to move data between data stores. AWS Glue can be used to automate the conversion of CSV files to Parquet format with minimal effort. AWS Glue supports reading data from CSV files, transforming the data, and writing the transformed data to Parquet files.\n\nOption A is incorrect because AWS Glue crawler is used to infer the schema of data stored in S3 and create AWS Glue Data Catalog tables.\n\nOption C is incorrect because while Amazon EMR can be used to process large amounts of data and perform data conversions, it requires more operational effort than AWS Glue.\n\nOption D is incorrect because Amazon SageMaker is a machine learning service, and while it can be used for data processing, it is not the best option for simple data format conversion tasks.","upvote_count":"2","comment_id":"814658"},{"comments":[{"comment_id":"809021","timestamp":"1707963480.0","poster":"Jerry84","content":"From you link, A(Glue crawler) Should be correct.","comments":[{"content":"crawler just creates the data catalog (schema), it does not actually converts the data to another format. As per details in that article, you are creating a job where source is schema created by crawler and destination is output s3 where we store formatted data.","poster":"drcok87","timestamp":"1708193940.0","comment_id":"812203","upvote_count":"3"}],"upvote_count":"1"}],"comment_id":"804813","poster":"drcok87","content":"in sagemaker notebook, you'd have to write python code but question is asking for something easy so i choose option b https://blog.searce.com/convert-csv-json-files-to-apache-parquet-using-aws-glue-a760d177b45f","timestamp":"1707602220.0","upvote_count":"2"}],"unix_timestamp":1676066220,"choices":{"C":"Write a script to convert the file format. Run the script on an Amazon EMR cluster.","A":"Use an AWS Glue crawler to convert the file format.","D":"Write a script to convert the file format. Run the script in an Amazon SageMaker notebook.","B":"Write a script to convert the file format. Run the script as an AWS Glue job."},"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/98756-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A data scientist has 20 TB of data in CSV format in an Amazon S3 bucket. The data scientist needs to convert the data to Apache Parquet format.\n\nHow can the data scientist convert the file format with the LEAST amount of effort?","answer_ET":"B","answers_community":["B (100%)"],"isMC":true,"question_id":127,"answer_images":[],"answer":"B","exam_id":26,"topic":"1"},{"id":"9KwnZ2ycDyvPols2EBfW","answers_community":["A (90%)","10%"],"question_text":"A company is building a pipeline that periodically retrains its machine learning (ML) models by using new streaming data from devices. The company's data engineering team wants to build a data ingestion system that has high throughput, durable storage, and scalability. The company can tolerate up to 5 minutes of latency for data ingestion. The company needs a solution that can apply basic data transformation during the ingestion process.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","timestamp":"2023-02-18 15:21:00","unix_timestamp":1676730060,"isMC":true,"question_images":[],"exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/99814-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"A","discussion":[{"content":"Selected Answer: A\nA. YES - Kinesis/Kafka acts as buffer for ingestion, Firehose provides good integration with Lambda (tranformation) & S3 (storage)\nB. NO - no point to save the data twice in S3 (raw and transformed)\nC. NO - since we do single-record transformation Glue/Spark is overkill\nD. NO - since we do single-record transformation Glue/Spark is overkill; further, we can reasonably expect devices to produce Kafka events but deploying a Firehose client API seem complicated","comment_id":"1004867","upvote_count":"1","poster":"loict","timestamp":"1726062180.0"},{"comment_id":"984756","timestamp":"1724010480.0","upvote_count":"1","content":"Selected Answer: A\nanswer A","poster":"Mickey321"},{"content":"Selected Answer: A\nAWS Glue cannot get data from Kinesis Firehose, only from Kinesis Data Stream. It's not D.\nhttps://docs.aws.amazon.com/glue/latest/dg/add-job-streaming.html","comment_id":"948598","poster":"ADVIT","timestamp":"1720663560.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1711621320.0","comment_id":"853004","content":"Selected Answer: D\nIt is D","poster":"DS2021","comments":[{"content":"Glue can't read from Firehose.\n\nIt's A.","upvote_count":"1","comment_id":"882955","timestamp":"1714244520.0","poster":"avland"}]},{"poster":"DS2021","content":"It is C","comments":[{"timestamp":"1712655900.0","comment_id":"865400","content":"Option C uses AWS Glue, which can perform data transformation and load data into S3 buckets. However, Glue may not be the most efficient option for this use case, as it requires setting up a Glue job, which can introduce additional latency.","poster":"ParkXD","comments":[{"content":"Option A uses Amazon Kinesis data stream, which is optimized for high throughput, durable storage, and scalability.","comment_id":"865401","poster":"ParkXD","upvote_count":"1","timestamp":"1712655960.0"}],"upvote_count":"3"}],"timestamp":"1711023120.0","comment_id":"845912","upvote_count":"1"},{"poster":"sevosevo","comment_id":"842620","timestamp":"1710753780.0","upvote_count":"1","content":"Why not C?"},{"content":"Selected Answer: A\nFirehose can take just at a maximum of 5 minutes, then it's the best solution for transformations.","poster":"Valcilio","upvote_count":"2","comment_id":"832334","timestamp":"1709849520.0"},{"timestamp":"1708563600.0","comment_id":"817323","upvote_count":"2","poster":"GiyeonShin","content":"Selected Answer: A\nA general architecture for (near)real - time ingesting & processing data:\nKinesis Data Streams - Kinesis Data Firehose - (If needs etl, lambda) - S3(Redshift, ...)"},{"poster":"AjoseO","timestamp":"1708384860.0","comment_id":"814657","content":"Selected Answer: A\nThis solution provides a highly scalable and efficient way to ingest streaming data from devices with high throughput and durable storage by using Amazon Kinesis Data Streams and Amazon Kinesis Data Firehose. \n\nBy configuring an AWS Lambda function to transform the data during the ingestion process, the solution also applies basic data transformation with low latency. Additionally, Amazon S3 provides highly durable and scalable storage for the transformed data, which can be easily accessed by downstream processes such as machine learning model training.","upvote_count":"2"},{"poster":"wolfsong","upvote_count":"2","comment_id":"813080","content":"A:\nhttps://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html","timestamp":"1708266060.0"}],"topic":"1","answer_images":[],"answer_description":"","choices":{"D":"Configure the devices to send streaming data to an Amazon Kinesis Data Firehose delivery stream. Configure an AWS Glue job that connects to the delivery stream to transform the data and load the output into an Amazon S3 bucket.","A":"Configure the devices to send streaming data to an Amazon Kinesis data stream. Configure an Amazon Kinesis Data Firehose delivery stream to automatically consume the Kinesis data stream, transform the data with an AWS Lambda function, and save the output into an Amazon S3 bucket.","B":"Configure the devices to send streaming data to an Amazon S3 bucket. Configure an AWS Lambda function that is invoked by S3 event notifications to transform the data and load the data into an Amazon Kinesis data stream. Configure an Amazon Kinesis Data Firehose delivery stream to automatically consume the Kinesis data stream and load the output back into the S3 bucket.","C":"Configure the devices to send streaming data to an Amazon S3 bucket. Configure an AWS Glue job that is invoked by S3 event notifications to read the data, transform the data, and load the output into a new S3 bucket."},"question_id":128,"answer":"A"},{"id":"lV3A9aVGeWQdGoPHnSdZ","choices":{"C":"Deploy an Amazon S3 File Gateway in the stores. Update the in-store software to deliver data to the S3 File Gateway. Use a scheduled daily AWS Glue job to transform the data that the S3 File Gateway delivers to Amazon S3.","A":"Create an AWS Lambda function that can transform the incoming records. Enable data transformation on the ingestion Kinesis Data Firehose delivery stream. Use the Lambda function as the invocation target.","B":"Deploy an Amazon EMR cluster that runs Apache Spark and includes the transformation logic. Use Amazon EventBridge (Amazon CloudWatch Events) to schedule an AWS Lambda function to launch the cluster each day and transform the records that accumulate in Amazon S3. Deliver the transformed records to Amazon S3.","D":"Launch a fleet of Amazon EC2 instances that include the transformation logic. Configure the EC2 instances with a daily cron job to transform the records that accumulate in Amazon S3. Deliver the transformed records to Amazon S3."},"topic":"1","question_id":129,"question_text":"A retail company is ingesting purchasing records from its network of 20,000 stores to Amazon S3 by using Amazon Kinesis Data Firehose. The company uses a small, server-based application in each store to send the data to AWS over the internet. The company uses this data to train a machine learning model that is retrained each day. The company's data science team has identified existing attributes on these records that could be combined to create an improved model.\n\nWhich change will create the required transformed records with the LEAST operational overhead?","question_images":[],"discussion":[{"poster":"seifskl","comment_id":"1059552","upvote_count":"2","timestamp":"1730455860.0","content":"Selected Answer: A\nA is correct, \nwhy not C : C require updating software in each of the 20,000 stores, which is operationally intensive. Moreover, the S3 File Gateway is designed for on-premises integration with S3."},{"upvote_count":"1","comment_id":"984757","content":"Selected Answer: A\nAnswer A","poster":"Mickey321","timestamp":"1724010660.0"},{"poster":"Valcilio","timestamp":"1709849820.0","comment_id":"832337","upvote_count":"4","content":"Selected Answer: A\nFirehose can use lambda functions to do data transformations!"},{"timestamp":"1708383900.0","comment_id":"814645","upvote_count":"4","poster":"AjoseO","content":"Selected Answer: A\nA is the best option for this use case. \n\nBy creating an AWS Lambda function that can transform the incoming records and enabling data transformation on the ingestion Kinesis Data Firehose delivery stream, the company can transform the data with minimal operational overhead. \n\nThe Lambda function can be the invocation target for Kinesis Data Firehose, so that data is transformed as it is ingested. \n\nThis approach is serverless and scalable, and it does not require the company to manage any additional infrastructure."},{"timestamp":"1708267020.0","comment_id":"813104","content":"A: Lambda as invocation target just means that the function will invoke in response to the Firehose stream. See following:\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-kinesisfirehose.html\nnote: invocationid in Firehose message event.","poster":"wolfsong","upvote_count":"3"},{"content":"a - seems to be an easy to manage solution however the phrase \"Use the Lambda function as the invocation target.\" confuses me a bit.","upvote_count":"2","timestamp":"1708194420.0","comment_id":"812210","comments":[{"content":"well that is used by Kinesis Data Firehouse..","timestamp":"1717208460.0","comment_id":"911662","upvote_count":"1","poster":"JonSno"}],"poster":"drcok87"}],"url":"https://www.examtopics.com/discussions/amazon/view/99689-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"timestamp":"2023-02-17 19:27:00","unix_timestamp":1676658420,"answer_description":"","answers_community":["A (100%)"],"answer_images":[],"answer_ET":"A","answer":"A","exam_id":26},{"id":"O0MXXY4XXKJvpzQJAsua","question_text":"A sports broadcasting company is planning to introduce subtitles in multiple languages for a live broadcast. The commentary is in English. The company needs the transcriptions to appear on screen in French or Spanish, depending on the broadcasting country. The transcriptions must be able to capture domain-specific terminology, names, and locations based on the commentary context. The company needs a solution that can support options to provide tuning data.\n\nWhich combination of AWS services and features will meet these requirements with the LEAST operational overhead? (Choose two.)","answer_images":[],"question_id":130,"question_images":[],"timestamp":"2023-02-17 19:42:00","unix_timestamp":1676659320,"answers_community":["BE (55%)","AE (45%)"],"answer_ET":"BE","topic":"1","exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/99696-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","discussion":[{"upvote_count":"1","comment_id":"1399163","poster":"ef12052","timestamp":"1742113080.0","content":"Selected Answer: BE\nhttps://docs.aws.amazon.com/transcribe/latest/dg/improving-accuracy.html"},{"content":"Selected Answer: BE\nA: Only specific words can be corrected","timestamp":"1739498160.0","upvote_count":"1","poster":"Carpediem78","comment_id":"1356336"},{"comment_id":"1310322","upvote_count":"1","poster":"MerryLew","content":"Selected Answer: AE\nAE is most straight forward.","timestamp":"1731359040.0"},{"content":"Selected Answer: AE\nIn contrast, Amazon Transcribe with custom vocabularies (option A) and Amazon Translate (option E) provide a simpler, more efficient solution with lower operational overhead, making them better suited for the companyâ€™s needs.","upvote_count":"1","poster":"MultiCloudIronMan","timestamp":"1729522140.0","comment_id":"1301066"},{"upvote_count":"2","content":"Selected Answer: AE\nfrom AWS docs. No doubt. \nB is a solution that can fix more problem than this requirement but no need.","timestamp":"1728104340.0","comment_id":"1293332","poster":"MJSY"},{"poster":"72cc81d","content":"Selected Answer: AE\nIt is clear from AWS docs. No doubt.","comment_id":"1258212","upvote_count":"2","timestamp":"1722343500.0"},{"upvote_count":"1","content":"Selected Answer: BE\ncustom language model will be needed as custom vocab will just help with pronunciation, and requirement clearly states handling domain specific terminologies, which cannot be handled by custom vocab.","comment_id":"1205193","timestamp":"1714587840.0","poster":"rookiee1111"},{"poster":"Denise123","content":"Selected Answer: AE\nOption A - Amazon Transcribe with custom vocabularies, allows you to enhance the transcription accuracy by providing domain-specific terminology, names, and locations. Custom vocabularies enable you to train the transcription model to recognize specific words or phrases commonly used in the context of sports commentary. This would help ensure that the transcriptions accurately capture the specialized terminology and context of the commentary, meeting the requirements of the sports broadcasting company. Additionally, the option mentions supporting options to provide tuning data, which further enhances the flexibility and customization of the solution.","timestamp":"1712755860.0","upvote_count":"3","comment_id":"1193024"},{"upvote_count":"1","poster":"vkbajoria","content":"Selected Answer: AE\nCould someone explain why A is not correct?\n\nAs par AWS documentation: Use custom vocabularies to improve transcription accuracy for one or more specific words. These are generally domain-specific terms, such as brand names and acronyms, proper nouns, and words that Amazon Transcribe isn't rendering correctly.\n\nCustom vocabularies can be used with all supported languages.","timestamp":"1711904940.0","comment_id":"1186907"},{"timestamp":"1709255160.0","content":"Selected Answer: AE\nA - Yes - Transcribe custom vocabs allow domain specific transforms\nB - No - Custom language models are typically used for fine-tuning for specific accents, dialects, or unique speech patterns. This question is about domain specific terminology\nC - No - building and tarining requires more overhead\nD - No - building and tarining requires more overhead\nE - Once transcribed in english, translate can perform laguage transformation","poster":"AIWave","upvote_count":"3","comment_id":"1163143"},{"poster":"loict","content":"Selected Answer: BE\nA. NO - Amazon Transcribe with custom vocabularies does not allow to take into account the broader context (https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary-create-list.html)\nB. YES - Custom language models are designed to improve transcription accuracy for domain-specific speech (https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html)\nC. NO - better to use built-in Translate service than base Seq2Seq\nD. NO - Hugging Face Speech2Text is custom model, use standard Transcribe\nE. YES - we need to translate English","timestamp":"1694440800.0","comment_id":"1004884","upvote_count":"4"},{"poster":"Mickey321","timestamp":"1692472920.0","content":"Selected Answer: BE\ncustom language + Translate","comment_id":"985397","upvote_count":"2"},{"content":"Selected Answer: BE\nAnswer BE","upvote_count":"2","timestamp":"1692388380.0","comment_id":"984759","poster":"Mickey321"},{"comment_id":"963191","timestamp":"1690328280.0","content":"BE : For Specific Language: Custom Language Model","upvote_count":"1","poster":"vbal"},{"poster":"Mllb","timestamp":"1680335040.0","comment_id":"857697","upvote_count":"3","content":"Selected Answer: BE\nComentary context is custom language"},{"comments":[{"upvote_count":"1","poster":"angus","content":"https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html","timestamp":"1678628280.0","comment_id":"837057"}],"poster":"Valcilio","comment_id":"832339","timestamp":"1678227720.0","content":"Selected Answer: BE\nIt's BE, custom languages is for domain-specific speech like terminologies, custom vocabulary is for words liken nouns.","upvote_count":"3"},{"timestamp":"1677483840.0","poster":"GiyeonShin","upvote_count":"1","comments":[{"comment_id":"826453","upvote_count":"2","content":"option A cannot capture \"commentary context\".\nB and E should be correct.","poster":"drcok87","timestamp":"1677726660.0"}],"comment_id":"823354","content":"Selected Answer: AE\nTwo sub-processes are needed: Speech to Text and Text to Text.\nWe can consider Amazon Transcribe for Speech2Text. If we use custom language models or SageMaker, we would need to gather our own data to train or retrain models. for less effort, (a) option is better than (b) option.\nThen, Amazon Translate can be used to translate transcription to other language: (e) option"},{"upvote_count":"2","timestamp":"1676659320.0","content":"b: because The transcriptions must be able to capture domain-specific terminology, names, and locations based on the \"commentary context\"\ne: managed service","poster":"drcok87","comment_id":"812230"}],"answer":"BE","isMC":true,"choices":{"D":"Amazon SageMaker with Hugging Face Speech2Text","E":"Amazon Translate","B":"Amazon Transcribe with custom language models","C":"Amazon SageMaker Seq2Seq","A":"Amazon Transcribe with custom vocabularies"}}],"exam":{"name":"AWS Certified Machine Learning - Specialty","numberOfQuestions":369,"isImplemented":true,"provider":"Amazon","isMCOnly":false,"lastUpdated":"11 Apr 2025","id":26,"isBeta":false},"currentPage":26},"__N_SSP":true}