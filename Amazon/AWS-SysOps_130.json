{"pageProps":{"questions":[{"id":"E5NIyz6mO9n6n0v7LZ63","answers_community":[],"topic":"1","answer_description":"CloudTrail log file integrity validation can be used to check whether a log file was modified, deleted, or unchanged after CloudTrail delivered it","question_images":[],"answer_ET":"B","isMC":true,"question_text":"A company monitors its account activity using AWS CloudTrail, and is concerned that some log files are being tampered with after the logs have been delivered to the account's Amazon S3 bucket.\nMoving forward, how can the SysOps Administrator confirm that the log files have not been modified after being delivered to the S3 bucket.","answer_images":[],"choices":{"C":"Replicate the S3 log bucket across regions, and encrypt log files with S3 managed keys.","D":"Enable S3 server access logging to track requests made to the log bucket for security audits.","B":"Enable log file integrity validation and use digest files to verify the hash value of the log file.","A":"Stream the CloudTrail logs to Amazon CloudWatch Logs to store logs at a secondary location."},"answer":"B","timestamp":"2019-10-26 16:15:00","unix_timestamp":1572099300,"exam_id":36,"url":"https://www.examtopics.com/discussions/amazon/view/7274-exam-aws-sysops-topic-1-question-684-discussion/","question_id":646,"discussion":[{"content":"When you enable log file integrity validation, CloudTrail creates a hash for every log file that it delivers. Every hour, CloudTrail also creates and delivers a file that references the log files for the last hour and contains a hash of each. This file is called a digest file. CloudTrail signs each digest file using the private key of a public and private key pair. After delivery, you can use the public key to validate the digest file. CloudTrail uses different key pairs for each AWS region","timestamp":"1727831820.0","poster":"karmaah","comments":[{"poster":"joyjyothi","upvote_count":"3","timestamp":"1728470100.0","content":"Thank you so much for the details.","comment_id":"129245"}],"upvote_count":"19","comment_id":"26008"},{"comment_id":"17625","content":"B is correct","poster":"saumenP","upvote_count":"14","timestamp":"1726929600.0"},{"content":"B is correct.\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html","upvote_count":"1","timestamp":"1730448240.0","poster":"Cyril_the_Squirl","comment_id":"434324"},{"poster":"Huy","comment_id":"408214","comments":[{"poster":"Cyril_the_Squirl","upvote_count":"1","timestamp":"1730477700.0","comment_id":"434326","content":"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html"}],"timestamp":"1730448120.0","upvote_count":"1","content":"C is correct. VPC Flow Logs is not able to gather operating system log files for analysis. I think C means use EC2Rescure on the new instance to understand the issue, not on the unreachable instances."},{"poster":"RicardoD","timestamp":"1729866060.0","upvote_count":"1","comment_id":"356336","content":"B is the answer"},{"content":"B. Enable log file integrity validation and use digest files to verify the hash value of the log file.","poster":"abhishek_m_86","upvote_count":"3","timestamp":"1729785780.0","comment_id":"274004"},{"comment_id":"243027","upvote_count":"1","poster":"jackdryan","timestamp":"1729458420.0","content":"I'll go with B"},{"content":"B. Enable log file integrity validation and use digest files to verify the hash value of the log file.","upvote_count":"1","poster":"gilbertlelancelo","timestamp":"1729331400.0","comment_id":"209203"},{"content":"B is correct","comment_id":"185742","upvote_count":"1","poster":"waterzhong","timestamp":"1729003800.0"}]},{"id":"qSFB3qrW7bRswpi7rkMJ","answer_ET":"C","timestamp":"2019-10-26 16:21:00","exam_id":36,"choices":{"D":"Use Amazon Inspector to gather operating system log files for analysis.","B":"Use VPC Flow Logs to gather operating system log files for analysis.","C":"Use EC2Rescue to gather operating system log files for analysis.","A":"Use AWS Trusted Advisor to gather operating system log files for analysis."},"url":"https://www.examtopics.com/discussions/amazon/view/7275-exam-aws-sysops-topic-1-question-685-discussion/","topic":"1","isMC":true,"question_id":647,"answer_description":"Reference:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/troubleshoot-remote-desktop-connection-ec2-windows/","unix_timestamp":1572099660,"question_text":"After launching a new Amazon EC2 instance from a Microsoft Windows 2012 Amazon Machine Image (AMI), the SysOps Administrator is unable to connect to the instance using Remote Desktop Protocol (RDP). The instance is also unreachable. As part of troubleshooting, the Administrator deploys a second instance from a different AMI using the same configuration and is able to connect to the instance.\nWhat should be the next logical step in troubleshooting the first instance?","answer":"C","answer_images":[],"question_images":[],"answers_community":["C (100%)"],"discussion":[{"poster":"saumenP","timestamp":"1663669800.0","comment_id":"17627","content":"C is correct","upvote_count":"8"},{"upvote_count":"7","comment_id":"155027","comments":[{"timestamp":"1700047320.0","upvote_count":"1","content":"EC2 Rescue would be used from another instance that you could attach the existing EBS volume to.","poster":"xenodamus","comment_id":"718687"},{"timestamp":"1664515260.0","comments":[{"comment_id":"170633","content":"You need to read the answer carefully my friend: \"Use VPC Flow Logs to gather operating system log files for analysis.\"\nFlow logs is not meant for gathering OS log files. It can only help with:\n- Diagnosing overly restrictive security group rules\n- Monitoring the traffic that is reaching your instance\n- Determining the direction of the traffic to and from the network interfaces\nEC2Rescue, on the other hand, is meant for for gathering OS log files. \nIt's true that you can't access it via RDP, but you can via other means like SSH ...","timestamp":"1665369480.0","upvote_count":"3","comments":[{"poster":"nisoshabangu","comment_id":"197074","upvote_count":"3","content":"Other means like SSH? I don't think so SSH is linux. unless there is an option for serial connection.","timestamp":"1665640140.0"}],"poster":"shammous"}],"upvote_count":"1","content":"Yes when I read the blog, so I also chose B as the option.","poster":"KhatriRocks","comment_id":"162147"}],"timestamp":"1664242440.0","content":"B: You have to use VPC Flow Logs, EC2 rescue will not work as you cannot connect via RDP...\nas per this ref: https://aws.amazon.com/premiumsupport/knowledge-center/ec2rescue-windows-troubleshoot/\nEC2Rescue requires an EC2 Windows instance that meets the following specifications:\nWindows Server 2008 R2 or later\nNET Framework 3.5 SP1 or later installed\nIs accessible from a Remote Desktop Protocol (RDP) connection","poster":"NoCrapEva"},{"content":"Selected Answer: C\nUsing EC2Rescue is an effective way to gather relevant system logs and perform diagnostics on the first instance, allowing the SysOps Administrator to identify and troubleshoot the underlying cause of the connectivity and reachability issues.","upvote_count":"1","comment_id":"951979","poster":"albert_kuo","timestamp":"1721014860.0"},{"content":"C is correct.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/ec2rescue-windows-troubleshoot/","upvote_count":"1","timestamp":"1667791740.0","poster":"Cyril_the_Squirl","comment_id":"434327"},{"comment_id":"408212","upvote_count":"1","poster":"Huy","content":"C is correct. VPC Flow Logs is not able to gather operating system log files for analysis.","timestamp":"1667760180.0"},{"poster":"abhishek_m_86","upvote_count":"1","timestamp":"1667611200.0","content":"C. Use EC2Rescue to gather operating system log files for analysis.\nSeem correct","comment_id":"274006"},{"timestamp":"1666978020.0","upvote_count":"3","content":"C is correct \n\nEC2Rescue for EC2 Windows is a troubleshooting tool that you can run on your Amazon EC2 Windows Server instances. Use the tool to troubleshoot OS-level issues and to collect advanced logs and configuration files for further analysis. The following are some common issues that EC2Rescue can address:\n\nInstance connectivity issues due to firewall, Remote Desktop Protocol (RDP), or network interface configuration.\nOS boot issues due to a blue screen or stop error, a boot loop, or a corrupted registry.\nAny issues that might need advanced log analysis and troubleshooting.","comment_id":"273154","poster":"Chirantan"},{"comment_id":"269775","timestamp":"1666971540.0","content":"c\nhttps://aws.amazon.com/premiumsupport/knowledge-center/troubleshootremote-\ndesktop-connection-ec2-windows/","poster":"shamith","upvote_count":"1"},{"poster":"jerry19","upvote_count":"1","timestamp":"1666940400.0","content":"After reviewing everyone's excellent comments....answer is C.\n\"Then, use EC2Rescue to troubleshoot Amazon EC2 Windows Server instance issues:\nInstance connectivity issues: Use the Diagnose and Rescue feature in Offline instance mode.\nOS boot issues: Use the Restore feature in Offline instance mode.\nAdvanced logs and troubleshooting: Use the Capture logs feature in either Current instance mode or Offline instance mode.\" VPC Flow logs have nothing to do with os log files only interfaces to specific source you are troubleshooting. The info is in the same link everyone keeps referencing.","comment_id":"268228"},{"upvote_count":"1","timestamp":"1666521660.0","content":"EC2Rescue requires an Amazon EC2 Windows instance that:\n\n* Runs on Windows Server 2008 R2 or later\n* Has .NET Framework 3.5 SPI or later installed\n* Is accessible from an RDP connection\nAbove info is from https://aws.amazon.com/premiumsupport/knowledge-center/ec2rescue-windows-troubleshoot/ \n\nSince RDP isn't possible in our scenario, I reckon B is the answer","poster":"NivNZ","comments":[{"poster":"NivNZ","comment_id":"263560","comments":[{"content":"EC2Rescue can be run as SSM Document hence you don't need RDP access.","comment_id":"381855","timestamp":"1667683800.0","upvote_count":"2","poster":"Bigdss"}],"content":"I take it back. C is the answer because VPC flow logs cannot be used to give OS sys log files for analysis.","upvote_count":"3","timestamp":"1666848780.0"}],"comment_id":"255451"},{"comment_id":"245590","upvote_count":"1","content":"https://aws.amazon.com/premiumsupport/knowledge-center/ec2rescue-windows-troubleshoot/\nC should be just fine as AWS already stated EC2Rescue is a way to go for this case.","timestamp":"1666454880.0","poster":"whgustn28"},{"comment_id":"243035","timestamp":"1666327920.0","content":"I'll go with C","upvote_count":"1","poster":"jackdryan"},{"comment_id":"228491","content":"C. Use EC2Rescue to gather operating system log files for analysis.","upvote_count":"1","timestamp":"1666052460.0","poster":"MFDOOM"},{"comment_id":"217395","poster":"mbspark","upvote_count":"1","comments":[{"timestamp":"1666698360.0","upvote_count":"2","poster":"moon_lee","content":"@mbspark - is it? because the \"good\" instance is from a different AMI if im not mistaken.","comment_id":"257328"}],"content":"C Is Correct. Even though you can't RDP and the documentation states that is a requirement, what you CAN do with EC2Rescue is \"offline\" mode. Notice in the question, it tells you that you can deploy a 2nd instance. So you take the volume of the \"bad\" Offline instance, and attach that volume to the \"good\" instance and THEN run EC2Rescue on the volume from the Good instance.","timestamp":"1665987480.0"},{"comment_id":"216933","timestamp":"1665745380.0","poster":"ataraxium","upvote_count":"2","content":"Answer = 'B'\nI would think, since EC2 Rescue requires RDP, but that is unavailable."},{"poster":"waterzhong","upvote_count":"1","timestamp":"1665452280.0","content":"C. Use EC2Rescue to gather operating system log files for analysis.","comment_id":"185743"},{"comments":[{"poster":"MrDEVOPS","timestamp":"1665106440.0","comment_id":"167281","upvote_count":"1","content":"its C :-\nEc2 rescue only supports windows server 2008 or later(which means 2012 too.)"}],"content":"System Requirements\n\nEC2Rescue requires an EC2 Windows instance that meets the following specifications:\n\nWindows Server 2008 R2 or later\nNET Framework 3.5 SP1 or later installed\nIs accessible from a Remote Desktop Protocol (RDP) connection\nNote: EC2Rescue can only be run on Windows Server 2008 R2 or later, but it can also analyze the offline volumes of Windows Server 2008 or later","poster":"KhatriRocks","timestamp":"1664659620.0","upvote_count":"1","comment_id":"162150"},{"poster":"gretch","upvote_count":"1","timestamp":"1664068800.0","content":"https://aws.amazon.com/premiumsupport/knowledge-center/ec2rescue-windows-troubleshoot/","comment_id":"103940"},{"upvote_count":"1","timestamp":"1664049000.0","content":"C is the correct answer","comment_id":"94963","poster":"surewin"},{"comment_id":"80810","content":"ANSWER IS C","timestamp":"1663900980.0","upvote_count":"1","poster":"anishmn10"},{"timestamp":"1663732380.0","upvote_count":"1","poster":"cloud","comment_id":"37598","content":"C. Use EC2Rescue to gather operating system log files for analysis."}]},{"id":"OsRygzAvVCp0c9cu4pso","answer_ET":"A","timestamp":"2019-09-23 22:26:00","exam_id":36,"choices":{"A":"Launch a script that downloads and installs the application using the Amazon EC2 user data.","C":"Use AWS Systems Manager to inject the application into an AMI.","B":"Create a custom API using Amazon API Gateway to call an installation executable from an AWS CloudFormation Template.","D":"Configure AWS CodePipeline to deploy code changes and updates."},"url":"https://www.examtopics.com/discussions/amazon/view/5623-exam-aws-sysops-topic-1-question-686-discussion/","topic":"1","isMC":true,"answer_description":"","question_id":648,"unix_timestamp":1569270360,"answer":"A","question_text":"A custom application must be installed on all Amazon EC2 instances. The application is small, updated frequently and can be installed automatically.\nHow can the application be deployed on new EC2 instances?","answer_images":[],"question_images":[],"answers_community":["A (100%)"],"discussion":[{"content":"I Think A - the question says 'How can the application be deployed on new EC2 instances?',not existing.","comment_id":"32533","upvote_count":"27","timestamp":"1648029540.0","comments":[{"content":"Good point. I was almost tempted to say that D was the right answer. CodePipeline can be triggered if there are updates to the application code, but not when \"new\" EC2 instances are launched. The only way to deploy the application on new instances is through User Data. I agree A is the most appropriate answer in this case.","timestamp":"1648059840.0","poster":"smplysam","upvote_count":"7","comment_id":"40714"}],"poster":"Wmatt"},{"comment_id":"170669","upvote_count":"8","poster":"shammous","timestamp":"1649598960.0","content":"A: Correct (A way to Install apps on new EC2 instances)\nB: Complicated and Out of scope\nC: SSM can be used to patch exiting AMIs or injecting apps into EC2 instances, not AMIs (I think)\nD: Codepipeline deploy code changes and updates to existing EC2 instances using CodeDeploy or AWS Elastic Beanstalk. We are more looking at installing the app on a new EC2 instance which why I would rather choose A as an answer."},{"content":"Selected Answer: A\nUsing Amazon EC2 user data is a simple and efficient way to install a small and frequently updated application during the instance boot process. User data scripts run when the instance launches, and they can include commands to download and install applications automatically.","timestamp":"1734439920.0","comment_id":"1327885","upvote_count":"1","poster":"Aresius"},{"timestamp":"1729204020.0","content":"Almost no one address the 'update frequently ' part of the question, only D can do it automatically, both install and update","comment_id":"1197553","upvote_count":"1","poster":"jyrajan69"},{"upvote_count":"1","timestamp":"1670303700.0","content":"Selected Answer: A\nA - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\n\nThe application is self sustaining ie no intervention needed post install, but pre-install, user data can be used.","comment_id":"612127","poster":"Finger41"},{"timestamp":"1651860000.0","poster":"ahaffar","content":"The question is stating new EC2 instances so A is the best","comment_id":"382333","upvote_count":"1"},{"upvote_count":"2","comment_id":"339527","poster":"Kimle","timestamp":"1651841940.0","content":"I believe D is better than A \nbecause question Say that application is *updated frequently* .. \nyes question mention it's a new instance but that doesn't prevent us from using code deploy after New instance is initialized ."},{"poster":"solotvun","comment_id":"335002","upvote_count":"1","timestamp":"1651790460.0","content":"D would be the only logical answer here."},{"comment_id":"319987","poster":"CountryGent","content":"1. EC2 instances are already deployed(leaves A and D). 2. When efficiency or cost isn't mentioned AWS defaults to looking for the lowest cost. Codepipeline is $1 per pipeline every 30 days. \nC mentions injecting the code into the AMI. The AMI has to be deployed, updated and recreated. This doesn't help the existing deployed instances. \nI go with A.","timestamp":"1651700100.0","upvote_count":"1"},{"poster":"khun","content":"TRICK QUESTION: \"How can the application be deployed on new EC2 instances?\"\nAnswer is A. Launch a script that downloads and installs the application using the Amazon EC2 user data.","comment_id":"304672","timestamp":"1651695480.0","upvote_count":"1"},{"content":"is request is for New EC2 answer should be A","upvote_count":"1","timestamp":"1651616220.0","comment_id":"276378","poster":"Chirantan"},{"content":"A. Launch a script that downloads and installs the application using the Amazon EC2 user data.","upvote_count":"1","comment_id":"274010","poster":"abhishek_m_86","timestamp":"1650868620.0"},{"upvote_count":"1","content":"I agree that A can be used for installing instances.\nHowever, there is another question.\nCan D also be used for installing instances?\nThat's what I am wondering...","comment_id":"272434","poster":"Weekly_diary","timestamp":"1650679740.0"},{"comment_id":"270449","timestamp":"1650540420.0","content":"i Think it should be C --> You can use the AWS Management Console or the AWS CLI to deploy packages to your AWS Systems Manager managed instances by using AWS Systems Manager Distributor. You can currently deploy one version of one package per command. You can install new packages or update existing installations in place. You can choose to deploy a specific version or choose to always deploy the latest version of a package for deployment. We recommend using State Manager to install packages. Using State Manager helps ensure that your instances are always running the most up-to-date version of your package.","poster":"Chirantan","upvote_count":"1","comments":[{"upvote_count":"1","poster":"Chirantan","content":"AWS CodePipeline automates your software release process, allowing you to rapidly release new features to your users. With CodePipeline, you can quickly iterate on feedback and get new features to your users faster.\n\nAutomating your build, test, and release process allows you to quickly and easily test each code change and catch bugs while they are small and simple to fix. You can ensure the quality of your application or infrastructure code by running each change through your staging and release process.","comment_id":"276377","timestamp":"1651473480.0"}]},{"comment_id":"256651","poster":"wannaaws","content":"Isn't user data loaded at instance reboot? If so, for change to take effect every time, an instance reboot will be needed. Otherwise, https://aws.amazon.com/codepipeline/?nc=sn&loc=1, the benefit of CodePipeline\n\"AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. \"","upvote_count":"1","timestamp":"1650416040.0"},{"content":"OK I take my answer back. It is installing user data and consequently A is the correct answer.","poster":"kiev","upvote_count":"1","timestamp":"1650238620.0","comment_id":"253543"},{"content":"I would go with C. We could use System manager to do this","comment_id":"246885","timestamp":"1650113520.0","upvote_count":"1","poster":"kiev"},{"timestamp":"1650102300.0","comment_id":"243039","poster":"jackdryan","upvote_count":"1","content":"I'll go with A"},{"content":"A. Launch a script that downloads and installs the application using the Amazon EC2 user data.","poster":"MFDOOM","timestamp":"1650000660.0","comment_id":"228492","upvote_count":"1"},{"upvote_count":"2","poster":"YouYouYou","content":"he is missing up with our minds \nquestion is how can the application be installed on new ec2 instances NOT again NOT how can the application be updated on EC2 instances it maybe updated later by any other mean so it means all running instances are receiving updates all new instances will have the app installed and then get's updates the same way like every other running EC2 instance.\n\n\nso to achieve this we use user data. my answer is A","timestamp":"1649810280.0","comment_id":"199800"},{"poster":"SONLE","content":"I go with A. Small and fastest way to install the app on new EC2 when launch","timestamp":"1649653140.0","comment_id":"195601","upvote_count":"2"},{"comment_id":"162156","content":"New Instances and NOT existing, since its small, A would be ideal.","timestamp":"1649495280.0","upvote_count":"2","poster":"KhatriRocks"},{"timestamp":"1649437740.0","poster":"JGD","upvote_count":"3","comment_id":"156943","content":"if we think of regular updates as well as for new Ec2 machines, then Answer D is correct.\n\n\nAnswer A could be correct, but the machines needs to be rebooted to execute the script on the server bootup for updates on which the software is already installed. IN prod, we cannot do this in frequent manner. \nAnswer C: Incorrect. We can install the app on newly EC2 instance, how about updates?"},{"comments":[{"timestamp":"1649212020.0","poster":"SHoKMaSTeR","content":"In addition, you don't need to worry about new versions of the application for upgrading the AMI, it is upgraded when the instance is deployed.","upvote_count":"1","comment_id":"145896"}],"comment_id":"145895","timestamp":"1649131320.0","content":"Ans A\nIf you patch an AMI with System Manager, a new AMI ID is generated.\n\"Output\nThe execution returns the new AMI ID as output.\"\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/automation-walk-ami-patching.html\nSo probably you will need to change the AMI ID whenever you deploy. User Data is better in this case.","poster":"SHoKMaSTeR","upvote_count":"1"},{"timestamp":"1649100180.0","upvote_count":"4","poster":"4007","comment_id":"128765","content":"C. Use AWS Systems Manager to inject the application into an AMI. SSM Patch Manager will automatically update an AMI. \"Update an AMI with SSM Patch Manager. This section includes walkthroughs that describe how to patch or update Amazon Machines Images (AMIs).”\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/automation-walk-ami-patching.html"},{"poster":"gretch","timestamp":"1649047380.0","content":"I think D\nhttps://aws.amazon.com/codepipeline/","comment_id":"103954","upvote_count":"2"},{"comment_id":"86689","poster":"inf","timestamp":"1648761180.0","upvote_count":"3","content":"Answer: A\n\"new instances\". it doesn't ask you to consider lifecycle management of an application already installed - for instance there may be other agents or processes to manage that. Add the new code to an S3 bucket, userdata, plus role - new binaries are always installed on EC2 launch"},{"content":"small changes deployable by CC, D is my vote","upvote_count":"1","comment_id":"66338","timestamp":"1648460580.0","poster":"sardarfine1"},{"upvote_count":"1","comment_id":"51242","content":"after reading the question again, it seem C is the right answer. because the app is updated frequently and automaticly installed. you can't do that with user data.","timestamp":"1648243560.0","poster":"amo82"},{"content":"c is not enough, it's only patching the AMI. the requirement is deploying the app into new ec2 instances.","comment_id":"51237","upvote_count":"1","poster":"amo82","timestamp":"1648095000.0"},{"timestamp":"1647861840.0","content":"Answer is D.\nC cannot be correct cause System Manger can deploy changes to EC2 directly, but not to an AMI. A is not correct cause user-data used at launch only.","comment_id":"26136","poster":"white_shadow","upvote_count":"5","comments":[{"content":"agree Answer is D. CICD","comment_id":"66339","poster":"sardarfine1","upvote_count":"1","timestamp":"1648545240.0"},{"comment_id":"51238","content":"you can patch AMI too with SSM.","timestamp":"1648183080.0","poster":"amo82","upvote_count":"2"}]},{"timestamp":"1647782700.0","comment_id":"24498","content":"Should be C: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html","poster":"awsnoob","upvote_count":"2"},{"comment_id":"13032","poster":"mukeshs","timestamp":"1647762360.0","content":"And why not C?","upvote_count":"1"},{"content":"why not D ?","upvote_count":"4","timestamp":"1647748260.0","comment_id":"12341","poster":"TJarriault"}]},{"id":"a9vQJg5cCCeQzGjvLQTC","answer_description":"","question_images":[],"unix_timestamp":1619852760,"isMC":true,"question_text":"A SysOps Administrator noticed that the cache hit ratio for an Amazon CloudFront distribution is less than 10%.\nWhich collection of configuration changes will increase the cache hit ratio for the distribution? (Choose two.)","exam_id":36,"answers_community":["AE (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/51367-exam-aws-sysops-topic-1-question-687-discussion/","discussion":[{"upvote_count":"1","timestamp":"1721015040.0","comment_id":"951980","poster":"albert_kuo","content":"Selected Answer: AE\nForwarding Required Parameters: By ensuring that only required cookies, query strings, and headers are forwarded in the Cache Behavior Settings, unnecessary variations in requests are eliminated. This allows CloudFront to serve more responses from the cache, increasing the cache hit ratio.\n\nIncreasing Time to Live (TTL): Increasing the CloudFront time to live (TTL) settings in the Cache Behavior Settings extends the duration for which CloudFront caches the objects. A longer TTL increases the chances of serving the content from the cache, reducing the number of requests sent to the origin server and improving the cache hit ratio."},{"upvote_count":"2","content":"Correct Answer: A & E\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html#cache-hit-ratio-http-streaming","comment_id":"376922","timestamp":"1667400600.0","poster":"TroyMcLure"},{"content":"A | E are the answers","timestamp":"1664626380.0","upvote_count":"2","poster":"RicardoD","comment_id":"357685"},{"comment_id":"346656","timestamp":"1663958220.0","upvote_count":"2","content":"Answer is A and E\n\nFor A - We can improve your cache hit ratio by caching based on query string parameters, cookie values, request headers\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html\n\nFor E - We can modify TTL to improve your cache hit ratio\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html","poster":"binhdt2611"}],"choices":{"E":"Increase the CloudFront time to live (TTL) settings in the Cache Behavior Settings","B":"Change the Viewer Protocol Policy to use HTTPS only","C":"Configure the distribution to use presigned cookies and URLs to restrict access to the distribution","D":"Enable automatic compression of objects in the Cache Behavior Settings","A":"Ensure that only required cookies, query strings, and headers are forwarded in the Cache Behavior Settings"},"answer_images":[],"answer":"AE","timestamp":"2021-05-01 09:06:00","question_id":649,"topic":"1","answer_ET":"AE"},{"id":"td619bWZs5aHI39OtUMQ","isMC":true,"question_text":"On a weekly basis, the Administrator for a photo sharing website receives an archive of all files users have uploaded the previous week. these file archives can be as large as 10TB in size. For legal reasons, these archives must be saved with no possibility of someone deleting or modifying these archives. Occasionally, there may be a need to view the contents, but it is expected that retrieving them can take three or more hours.\nWhat should the Administrator do with the weekly archive?","unix_timestamp":1569048000,"answer_description":"","discussion":[{"upvote_count":"18","poster":"mukeshs","comment_id":"13048","timestamp":"1679593380.0","content":"It should be B. \nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\n\nVault lock will ensure that the data is never changed - Write once read many."},{"content":"Agree with B","comment_id":"13796","timestamp":"1680166620.0","upvote_count":"5","poster":"kkwang"},{"content":"Selected Answer: B\nVote for B","comment_id":"798736","timestamp":"1722845400.0","upvote_count":"1","poster":"gulu73"},{"poster":"RicardoD","timestamp":"1682822760.0","upvote_count":"2","comment_id":"359349","content":"B is the answer\n\nThe requirement for the files asks for vault lock \" files cannot be modified or deleted\""},{"upvote_count":"2","poster":"abhishek_m_86","comment_id":"274013","content":"B. Upload the archive to the Amazon Glacier with the AWS CLI and enable Vault Lock.\nSeem correct","timestamp":"1682684400.0"},{"content":"B: Vault lock is the key to answer this question","poster":"kenkct","timestamp":"1682633040.0","upvote_count":"1","comment_id":"254590"},{"timestamp":"1682428260.0","poster":"jackdryan","comment_id":"243041","content":"I'll go with B","upvote_count":"1"},{"comment_id":"212258","content":"B. Upload the archive to the Amazon Glacier with the AWS CLI and enable Vault Lock.","upvote_count":"1","poster":"MFDOOM","timestamp":"1682328360.0"},{"timestamp":"1681464960.0","comment_id":"185746","poster":"waterzhong","content":"B - because of the vault lock and the requirement of it not being deleted","upvote_count":"1"},{"comment_id":"82791","poster":"AWS_Noob","upvote_count":"5","content":"B - because of the vault lock and the requirement of it not being deleted","timestamp":"1681383240.0"},{"poster":"LuciEn","timestamp":"1681292940.0","comment_id":"77744","content":"it's B: Upload archives in parts – Using the multipart upload API, you can upload large archives, up to about 40,000 GB (10,000 * 4 GB). \n\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html","upvote_count":"1"},{"timestamp":"1680963660.0","comments":[{"timestamp":"1682443680.0","content":"\"A: The maximum size of an individual file is 5 TB, which is the maximum size of an individual object in S3. If you write a file larger than 5 TB, you will get a \"file too large\" error message and only the first 5 TB of the file will be uploaded.\"\n\nThis is the maximum size of AN INDIVIDUAL File, the question says an archive of FILES\n\nAmazon S3 multipart upload limits\nItem Specification\nMaximum object size 5 TB\nso each file can be upto 5Tb in a archive of FILEs. so this does not go against option A, as the admin recieves a LOT of files, the size of each file was not specified","poster":"tahaRyski","upvote_count":"1","comment_id":"253102"}],"poster":"mvsnogueira","upvote_count":"5","comment_id":"66183","content":"I was between B and D. \nD has a problem, the maximum file size to upload is 5TB. and this option did not mention something about multi file update.\nhttps://aws.amazon.com/storagegateway/faqs/\nQ: What is the maximum size of an individual file?\n\nA: The maximum size of an individual file is 5 TB, which is the maximum size of an individual object in S3. If you write a file larger than 5 TB, you will get a \"file too large\" error message and only the first 5 TB of the file will be uploaded.\n\nAnd question B has a problem too, because they didn't mention multipart upload\nSingle operation - you can upload file from 1 byte to 4 GB\nMultipart upload - sing the multipart upload API, you can upload large archives, up to about 40,000 GB (10,000 * 4 GB) that is 40 TB\nand you enable vault lock\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-archive-mpu.html\n\nBut eliminating others options, I will choose B if I get this question on my exam."},{"comment_id":"24503","poster":"awsnoob","content":"My guess is B, Vault Lock enables WORM and Glacier has long recovery time which is acceptable in this scenario","timestamp":"1680643140.0","upvote_count":"5"},{"content":"B seems to be correct","comment_id":"17636","timestamp":"1680494760.0","poster":"saumenP","upvote_count":"5"},{"content":"A is wrong, should be D, using console, max size is 160GB","poster":"coolboylqy","comment_id":"12009","upvote_count":"2","timestamp":"1679285820.0"}],"exam_id":36,"url":"https://www.examtopics.com/discussions/amazon/view/5540-exam-aws-sysops-topic-1-question-688-discussion/","answer_images":[],"answer":"B","choices":{"A":"Upload the file to Amazon S3 through the AWS Management Console and apply a lifecycle policy to change the storage class to Amazon Glacier.","D":"Create a file gateway attached to a file share on an S3 bucket with the storage class S3 Infrequent Access. Upload the archives via the gateway.","B":"Upload the archive to the Amazon Glacier with the AWS CLI and enable Vault Lock.","C":"Create a Linux EC2 instance with an encrypted Amazon EBS volume and copy each weekly archive file for this instance."},"question_images":[],"topic":"1","answer_ET":"A","answers_community":["B (100%)"],"question_id":650,"timestamp":"2019-09-21 08:40:00"}],"exam":{"id":36,"isMCOnly":false,"numberOfQuestions":928,"isBeta":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","name":"AWS-SysOps","isImplemented":true},"currentPage":130},"__N_SSP":true}