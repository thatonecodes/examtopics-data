{"pageProps":{"questions":[{"id":"iduEdUObf1v7OWlflHEe","topic":"1","timestamp":"2023-05-15 11:08:00","answers_community":["CE (76%)","AE (15%)","10%"],"answer_ET":"CE","answer_images":[],"answer":"CE","answer_description":"","discussion":[{"upvote_count":"10","comment_id":"992093","timestamp":"1709126580.0","content":"Selected Answer: CE\nThe correct answer is (C) and (E).\n\nOption (C): Creating an SCP and attaching it to the root organizational unit (OU) will deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false. This means that any IAM user or root user in any account in the organization will not be able to create an EBS volume without encrypting it.\nOption (E): Specifying the Default EBS volume encryption setting in the Organizations management account will ensure that all new EBS volumes created in any account in the organization are encrypted by default.","poster":"Guru4Cloud"},{"content":"Selected Answer: CE\nCE\nPrevent future issues by creating a SCP and set a default encryption.","upvote_count":"9","timestamp":"1700256780.0","poster":"Axaus","comment_id":"900467"},{"timestamp":"1733658480.0","poster":"Jazz888","upvote_count":"2","comment_id":"1226662","content":"The problem here is we don't know in which account the workload is on. The account in ap-xx-is that the management account or it's a member account?? That will decide to select either A or E. C is certainly correct"},{"comment_id":"1222334","timestamp":"1733007480.0","poster":"NSA_Poker","content":"Selected Answer: CE\n(A) is incorrect bc absent of SCP or the Organizations management account, the scope of the EC2 console is too narrow to be applied to 'any IAM user or root user'.","upvote_count":"2"},{"content":"Selected Answer: AC\nhttps://repost.aws/knowledge-center/ebs-automatic-encryption\nNewly created Amazon EBS volumes aren't encrypted by default. However, you can turn on default encryption for new EBS volumes and snapshot copies that are created within a specified Region. To turn on encryption by default, use the Amazon Elastic Compute Cloud (Amazon EC2) console.","timestamp":"1729962780.0","poster":"venutadi","upvote_count":"2","comment_id":"1202712"},{"comment_id":"1115670","timestamp":"1720336560.0","content":"Selected Answer: AC\nA: will enforce automatic encryption in a account. This will have no effect on employees. Do this in every account.\nB: permission boundary is not appropriate here.\nC: an SCP will force employees to create encrypted volumes in every account.\nD: This would work but is too much maintenance.\nE: Setting EBS volume encryption in the Organizations management account will only have impact on volumes in that account, not on other accounts.","upvote_count":"2","poster":"1rob"},{"timestamp":"1719735600.0","comments":[{"comment_id":"1167423","poster":"dkw2342","timestamp":"1725641940.0","content":"IMO the correct solution is AC:\n\nIn the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key. \n-> This has to be done in every AWS account separately.\n\nCreate an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.\n-> This will just act as a safeguard in case an admin would disable default encryption in the member account, so it should not have any effect on employees who create EBS volumes.\n\nI think an updated question would offer options A and an updated C:\n\nCreate an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:DisableEbsEncryptionByDefault action. \n-> This will prevent disabling default encryption once is has been enabled.","upvote_count":"2"}],"upvote_count":"3","poster":"pentium75","content":"Selected Answer: AE\nThe solution should \"have minimal effect on employees who create EBS volumes\". Thus new volumes should automatically be encrypted. Options B, C and D do NOT automatically encrypt volumes, they simply cause requests to create non-encrypted volumes to fail.","comment_id":"1110447"},{"upvote_count":"1","content":"Wondering if just C would be sufficient?","comment_id":"1001515","poster":"Valder21","timestamp":"1709820420.0"},{"timestamp":"1709699640.0","upvote_count":"4","comment_id":"1000103","poster":"bjexamprep","content":"Seems many people selected E as part of the correct answer. But I didn't find so called Organization level EBS default setting in my Organization management account. I tried setting default EBS encryption setting in my Organization management account, and it didn't apply to the member account. If E cannot guarantee default encryption in all other account, E has no advantage over A. Anyone can explain why E is better than A?"},{"content":"Selected Answer: AE\nOption A: By default, EBS encryption is not enabled for EC2 instances. However, you can set an EBS encryption by default in your AWS account in the Amazon EC2 console. This ensures that every new EBS volume that is created is encrypted.\nOption E: With AWS Organizations, you can centrally set the default EBS encryption for your organization's accounts. This helps in enforcing a consistent encryption policy across your organization.\nOption B, C and D are not correct because while you can use IAM policies or SCPs to restrict the creation of unencrypted EBS volumes, this could potentially impact employees' ability to create necessary resources if not properly configured. They might require additional permissions management, which is not mentioned in the requirements. By setting the EBS encryption by default at the account or organization level (Options A and E), you can ensure all new volumes are encrypted without affecting the ability of employees to create resources.","upvote_count":"3","comment_id":"939900","poster":"novelai_me","timestamp":"1704117840.0"},{"content":"Selected Answer: CE\nSCPs are a great way to enforce policies across an entire AWS Organization, preventing users from creating resources that do not comply with the set policies.\n\nIn AWS Management Console, one can go to EC2 dashboard -> Settings -> Data encryption -> Check \"Always encrypt new EBS volumes\" and choose a default KMS key. This ensures that every new EBS volume created will be encrypted by default, regardless of how it is created.","poster":"Buruguduystunstugudunstuy","upvote_count":"3","timestamp":"1702439820.0","comment_id":"921870"},{"poster":"PRASAD180","comment_id":"907859","content":"1000% CE crt","timestamp":"1701082140.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"903993","content":"Encryption by default allows you to ensure that all new EBS volumes created in your account are always encrypted, even if you don’t specify encrypted=true request parameter.\nhttps://aws.amazon.com/blogs/compute/must-know-best-practices-for-amazon-ebs-encryption/","poster":"[Removed]","timestamp":"1700660040.0"},{"poster":"hiroohiroo","comment_id":"902325","timestamp":"1700453520.0","content":"Selected Answer: CE\nCとEが正しいと考える。","upvote_count":"3"},{"comment_id":"899136","content":"Selected Answer: CE\nCE for me as well","poster":"Efren","timestamp":"1700142420.0","upvote_count":"2"},{"content":"Selected Answer: CE\nSCP that denies the ec2:CreateVolume action when the ec2:Encrypted condition equals false. This will prevent users and service accounts in member accounts from creating unencrypted EBS volumes in the ap-southeast-2 Region.","timestamp":"1700046480.0","poster":"nosense","upvote_count":"2","comment_id":"898150","comments":[{"timestamp":"1719735120.0","poster":"pentium75","content":"Wouldn't this have \"effect on employees who create EBS volumes\", which we are asked to minimize?","comment_id":"1110436","upvote_count":"1"},{"content":"agreed","timestamp":"1700142360.0","upvote_count":"1","comment_id":"899135","poster":"Efren"}]}],"question_text":"A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest.\n\nAn audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes.\n\nWhich combination of steps will meet these requirements? (Choose two.)","question_images":[],"isMC":true,"choices":{"E":"In the Organizations management account, specify the Default EBS volume encryption setting.","D":"Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.","A":"In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.","B":"Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.","C":"Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false."},"unix_timestamp":1684141680,"exam_id":31,"question_id":376,"url":"https://www.examtopics.com/discussions/amazon/view/109268-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"8ImCsNAUMQKDUZvjqJ8J","question_id":377,"answer_images":[],"topic":"1","answers_community":["C (99%)","1%"],"answer_ET":"C","question_images":[],"question_text":"A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is concerned about data transfer charges.\nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges?","isMC":true,"exam_id":31,"discussion":[{"comment_id":"768164","timestamp":"1673052840.0","comments":[{"upvote_count":"9","content":"Very good explanation!","poster":"Bmarodi","timestamp":"1685769780.0","comment_id":"913299"},{"upvote_count":"5","poster":"johne42","content":"https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/","timestamp":"1692704640.0","comment_id":"987406"},{"comment_id":"1072985","content":"Precisely","timestamp":"1700188680.0","poster":"OmegaLambda7XL9","upvote_count":"3"}],"poster":"SilentMilli","upvote_count":"98","content":"Selected Answer: C\nDeploying a gateway VPC endpoint for Amazon S3 is the most cost-effective way for the company to avoid Regional data transfer charges. A gateway VPC endpoint is a network gateway that allows communication between instances in a VPC and a service, such as Amazon S3, without requiring an Internet gateway or a NAT device. Data transfer between the VPC and the service through a gateway VPC endpoint is free of charge, while data transfer between the VPC and the Internet through an Internet gateway or NAT device is subject to data transfer charges. By using a gateway VPC endpoint, the company can reduce its data transfer costs by eliminating the need to transfer data through the NAT gateway to access Amazon S3. This option would provide the required connectivity to Amazon S3 and minimize data transfer charges."},{"poster":"Buruguduystunstugudunstuy","upvote_count":"6","content":"Selected Answer: C\nOption C (correct). Deploy a gateway VPC endpoint for Amazon S3.\n\nA VPC endpoint for Amazon S3 allows you to access Amazon S3 resources within your VPC without using the Internet or a NAT gateway. This means that data transfer between your EC2 instances and S3 will not incur Regional data transfer charges.\n\nOption A (wrong), launching a NAT gateway in each Availability Zone, would not avoid data transfer charges because the NAT gateway would still be used to access S3. \n\nOption B (wrong), replacing the NAT gateway with a NAT instance, would also not avoid data transfer charges as it would still require using the Internet or a NAT gateway to access S3. \n\nOption D (wrong), provisioning an EC2 Dedicated Host, would not affect data transfer charges as it only pertains to the physical host that the EC2 instances are running on and not the data transfer charges for accessing.","timestamp":"1671508680.0","comment_id":"750469"},{"timestamp":"1738000680.0","comment_id":"1347496","poster":"Dharmarajan","upvote_count":"1","content":"Selected Answer: C\nThis one was obvious. with VPC gateway for S3, the traffic will not leave AWS and directly go to S3. Therefore will reduce the cost of outgoing data."},{"timestamp":"1735667820.0","comment_id":"1334956","poster":"satyaammm","upvote_count":"1","content":"Selected Answer: C\nGateway VPC endpoint are most suitable for privately accessing S3 here."},{"timestamp":"1735286700.0","content":"Selected Answer: C\nUsing a gateway VPC endpoint for Amazon S3 is the most cost-effective solution because it enables instances within the VPC to communicate directly with Amazon S3 without incurring data transfer charges via a NAT gateway. Here's how it addresses the problem:\n\nAvoids NAT Gateway Charges for S3 Access:\n\nNAT gateways incur data processing and data transfer charges when instances access S3.\nA gateway VPC endpoint eliminates the need to route traffic through the NAT gateway by enabling direct communication between EC2 instances and S3.\nNo Data Transfer Costs for Intra-Region S3 Access:\n\nData transferred between Amazon S3 and resources in the same AWS Region through a gateway VPC endpoint is free of charge.\nHighly Available and Managed:\n\nGateway VPC endpoints are highly available and fully managed by AWS, requiring no additional operational overhead.","upvote_count":"1","comment_id":"1332300","poster":"MGKYAING"},{"upvote_count":"1","timestamp":"1735160400.0","content":"Selected Answer: C\nCorrect","comment_id":"1331706","poster":"atikla"},{"upvote_count":"1","content":"Selected Answer: C\nC for sure","poster":"ChymKuBoy","timestamp":"1728935280.0","comment_id":"1297804"},{"poster":"PaulGa","comment_id":"1282035","timestamp":"1726046520.0","content":"Selected Answer: C\nAns C - excellent explanation by SilentMilli","upvote_count":"1"},{"content":"Selected Answer: C\nVPC gatwway endpoint is free to use, but only available for S3 and DynamoDB","comment_id":"1265806","timestamp":"1723643640.0","poster":"monkey_aws","upvote_count":"4"},{"timestamp":"1705231560.0","poster":"awsgeek75","upvote_count":"3","content":"Selected Answer: C\nGateway VPC allows direct access to S3 without going through public internet. This is the de-facto way to save cost for S3 to VPC traffic. \nCorrect answer is C","comment_id":"1122485"},{"content":"Avoid regional data transfer charge - VPC endpoint","timestamp":"1700505120.0","comment_id":"1075713","upvote_count":"3","poster":"[Removed]"},{"poster":"Ruffyit","upvote_count":"2","timestamp":"1698371760.0","content":"https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/","comment_id":"1055047"},{"content":"Selected Answer: C\nGateway Endpoint bests suits the requirement","upvote_count":"2","poster":"ACloud_Guru15","comment_id":"1043220","timestamp":"1697269680.0"},{"comment_id":"1010300","timestamp":"1695015660.0","content":"Answer is C: An S3 VPC endpoint provides a way for an S3 request to be routed through to the Amazon S3 service, without having to connect a subnet to an internet gateway. The S3 VPC endpoint is what's known as a gateway endpoint.","upvote_count":"2","poster":"srinivasmn"},{"comment_id":"976042","poster":"Guru4Cloud","upvote_count":"2","timestamp":"1691528160.0","content":"Selected Answer: C\nthe EC2 instances are downloading and uploading images to S3, configuring a gateway VPC endpoint will allow them to access S3 without crossing Availability Zones or regions, eliminating regional data transfer charges"},{"timestamp":"1691122140.0","content":"Selected Answer: C\nGateway VPC endpoints provide reliable connectivity to Amazon S3 without requiring an internet gateway or a NAT device for your VPC.","poster":"TariqKipkemei","upvote_count":"3","comment_id":"971673"},{"timestamp":"1689552180.0","comment_id":"953726","poster":"miki111","content":"Option C is the right answer.","upvote_count":"2"},{"content":"By deploying a gateway VPC endpoint for S3, the company can establish a direct connection between their VPC and S3 without going through the internet gateway or NAT gateway. This enables traffic between the EC2 and S3 to stay within the Amazon network, avoiding Regional data transfer charges.\n\nA suggests launching the NAT gateway in each AZ. While this can help with availability and redundancy, it does not address the issue of data transfer charges, as the traffic would still traverse the NAT gateways and incur data transfer fees.\n\nB suggests replacing the NAT gateway with a NAT instance. However, this solution still involves transferring data between the instances and S3 through the NAT instance, which would result in data transfer charges.\n\nD suggests provisioning an EC2 Dedicated Host to run the EC2. While this can provide dedicated hardware for the instances, it does not directly address the issue of data transfer charges.","poster":"cookieMr","upvote_count":"5","timestamp":"1687335000.0","comment_id":"929185"},{"comment_id":"913298","upvote_count":"1","content":"Selected Answer: C\nOption C is the answer.","timestamp":"1685769600.0","poster":"Bmarodi"},{"content":"Selected Answer: C\nA gateway VPC endpoint is a fully managed service that allows connectivity from a VPC to AWS services such as S3 without the need for a NAT gateway or a public internet gateway. By deploying a Gateway VPC endpoint for Amazon S3, the company can ensure that all S3 traffic remains within the VPC and does not cross the regional boundary. This eliminates regional data transfer charges and provides a more cost-effective solution for the company.","timestamp":"1680270240.0","upvote_count":"2","comment_id":"857129","poster":"linux_admin"},{"poster":"AndyMartinez","upvote_count":"1","timestamp":"1675455000.0","comment_id":"797342","content":"Selected Answer: C\nC - gateway VPC endpoint."},{"comment_id":"761780","content":"'Regional' data transfer isn't clear but I think we have to assume this means the traffic stays in the region.\nThe two options that seem possible are NAT gateway per AZ vs privatelink gateway endpoints per AZ.\nprivatelink/endpoints do have costs (url below)\nprivatelink endpoint / LB costs look lower than NAT gateway costs\nprivatelink doesn't incur inter-AZ data transfer charges (if in the same region) as NAT gateways do which goes towards the key requirement stated\n\ngood writeup here : https://www.vantage.sh/blog/nat-gateway-vpc-endpoint-savings\n\nhttps://aws.amazon.com/privatelink/pricing/\nhttps://aws.amazon.com/vpc/pricing/\nhttps://aws.amazon.com/premiumsupport/knowledge-center/vpc-reduce-nat-gateway-transfer-costs/","poster":"secdaddy","upvote_count":"3","timestamp":"1672389780.0"},{"timestamp":"1671563820.0","comment_id":"751381","content":"Selected Answer: C\nC, privately connects vpc to aws services via privatelink. Doesn't require nat gateway, vpn or direct connect. Data doesn't leave amazon network so there are no data transfer charges\nA, used to enable instances in private subnets to connect to internet or aws services, data transfered is charged \nB, similar to nat gateway\nD, not related to data transfer","poster":"pazabal","upvote_count":"4"},{"comment_id":"749997","content":"Selected Answer: C\nVPC endpoint","poster":"Morinator","upvote_count":"1","timestamp":"1671464580.0"},{"comment_id":"749497","content":"Selected Answer: C\nOption C","poster":"career360guru","upvote_count":"1","timestamp":"1671429060.0"},{"content":"Option is C bcz Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Gateway endpoints do not enable AWS PrivateLink. There is no additional charge for using gateway endpoints","poster":"shyam_yadav","comment_id":"734650","timestamp":"1670094900.0","upvote_count":"3"},{"poster":"Shasha1","upvote_count":"2","timestamp":"1669210740.0","content":"C is correct \nhttps://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html","comment_id":"725169"},{"poster":"Wpcorgan","content":"C is Correct","comment_id":"723540","upvote_count":"1","timestamp":"1669037220.0"},{"upvote_count":"5","content":"Selected Answer: C\nThis link clearly states that \"VPC gateway endpoints allow communication to Amazon S3 and Amazon DynamoDB without incurring data transfer charges within the same Region\". On the other hand NAT gateway incurs additional data processing charges. Hence, C is the correct answer.\nhttps://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/","comment_id":"715785","poster":"justsaysid","timestamp":"1668147540.0"},{"content":"Selected Answer: A\nWhy not A?","comment_id":"712968","poster":"dduque10","comments":[{"upvote_count":"2","timestamp":"1669517460.0","comment_id":"727901","poster":"TuLe","content":"using the NAT gateway you will be charge for data transfer out. When VPC gateway endpoint in place for S3, the service will use internal route inside AWS to send data to S3 -> no charge at all."}],"timestamp":"1667813940.0","upvote_count":"1"},{"content":"Selected Answer: C\nC is the answer","upvote_count":"4","poster":"airraid2010","comment_id":"705521","timestamp":"1666872480.0"},{"comment_id":"697785","content":"If we deploy VPC Gateway Endpoint then data will be transfer through AWS network only.","timestamp":"1666056780.0","upvote_count":"2","poster":"Jahangeer_17","comments":[{"comment_id":"713421","poster":"KADSM","content":"Though will it not incur regional data transfer cost ? Here the question is to avoid regional data transfer costs","timestamp":"1667873400.0","comments":[{"timestamp":"1668923520.0","upvote_count":"2","content":"Here it also says \"The company is concerned about data transfer charges\". They just want to reduce costs hence it is C.","comment_id":"722415","poster":"Wajif"}],"upvote_count":"1"}]},{"upvote_count":"2","poster":"Rachness","comment_id":"695144","content":"Selected Answer: C\nGateway Endpoint","timestamp":"1665802620.0"},{"upvote_count":"3","comment_id":"692490","timestamp":"1665531060.0","poster":"Lilibell","content":"The answer is C"}],"answer_description":"","unix_timestamp":1665531060,"timestamp":"2022-10-12 01:31:00","choices":{"B":"Replace the NAT gateway with a NAT instance.","A":"Launch the NAT gateway in each Availability Zone.","D":"Provision an EC2 Dedicated Host to run the EC2 instances.","C":"Deploy a gateway VPC endpoint for Amazon S3."},"url":"https://www.examtopics.com/discussions/amazon/view/85205-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C"},{"id":"EBDTHzUgzbU0lcQHBTJ6","answer_ET":"D","answer":"D","isMC":true,"exam_id":31,"question_id":378,"question_text":"A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible.\n\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/109269-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-05-15 11:19:00","choices":{"B":"Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.","A":"Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.","C":"Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.","D":"Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint."},"topic":"1","answers_community":["D (86%)","13%"],"unix_timestamp":1684142340,"answer_images":[],"question_images":[],"discussion":[{"comments":[{"comment_id":"1208411","timestamp":"1715176860.0","comments":[{"poster":"MatAlves","comment_id":"1282507","content":"No, the question clearly says \"failover support in most scenarios in less than 40 seconds.\"\nD is the only possible answer.","upvote_count":"2","timestamp":"1726124820.0"}],"upvote_count":"2","poster":"kelmryan1","content":"They want to keep cost low as possible, A is the right answer"}],"timestamp":"1684862880.0","upvote_count":"19","comment_id":"905110","poster":"ogerber","content":"Selected Answer: D\nA - multi-az instance : failover takes between 60-120 sec\nD - multi-az cluster: failover around 35 sec"},{"poster":"Buruguduystunstugudunstuy","comments":[{"timestamp":"1690691700.0","content":"Sorry I'm a bit confused... I thought only Aurora DB Cluster has reader endpoint. Do you by any chance has the link to the doc for RDS Reader Endpoint?","comments":[{"content":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts-connection-management.html#multi-az-db-clusters-concepts-connection-management-endpoints-reader","poster":"lemur88","upvote_count":"6","timestamp":"1692989100.0","comment_id":"990337"}],"upvote_count":"3","comment_id":"966853","poster":"Kiki_Pass"}],"comment_id":"921877","upvote_count":"13","content":"Selected Answer: D\nThe correct answer is:\nD. Use an Amazon RDS Multi-AZ DB cluster deployment. Point the read workload to the reader endpoint.\n\nExplanation:\nThe company wants high availability, automatic failover support in less than 40 seconds, read offloading from the primary instance, and cost-effectiveness.\n\nAnswer D is the best choice for several reasons:\n\n1. Amazon RDS Multi-AZ deployments provide high availability and automatic failover support.\n\n2. In a Multi-AZ DB cluster, Amazon RDS automatically provisions and maintains a standby in a different Availability Zone. If a failure occurs, Amazon RDS performs an automatic failover to the standby, minimizing downtime.\n\n3. The \"Reader endpoint\" for an Amazon RDS DB cluster provides load-balancing support for read-only connections to the DB cluster. Directing read traffic to the reader endpoint helps in offloading read operations from the primary instance.","timestamp":"1686622800.0"},{"upvote_count":"2","content":"to offload read we use read replicas also there is no such thing as reader endpoint in rds, it is only on aurora","comment_id":"1270029","poster":"zikou","timestamp":"1724237160.0"},{"upvote_count":"1","comment_id":"1193575","timestamp":"1712823480.0","poster":"rondelldell","content":"Selected Answer: B\nhttps://aws.amazon.com/rds/features/multi-az/\nAmazon RDS Multi-AZ with two readable standbys"},{"timestamp":"1711584480.0","poster":"hro","comment_id":"1184471","upvote_count":"3","content":"I think the cluster is over-kill - but the company 'wants to use an Amazon RDS ... DB cluster'."},{"comment_id":"1110453","timestamp":"1704018540.0","upvote_count":"7","poster":"pentium75","content":"Selected Answer: D\nA would be cheapest but \"failover times are typically 60–120 seconds\" which does not meet our requirements. We need Multi-AZ DB cluster (not instance). This has a reader endpoint by default, thus no need for additional read replicas (to \"keep costs as low as possible\").\n https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html"},{"comment_id":"1105655","poster":"master9","timestamp":"1703559120.0","upvote_count":"1","content":"Selected Answer: A\nin question, it has mentioned that \"keep costs as low as possible\"\n\nIn a Multi-AZ configuration, the DB instances and EBS storage volumes are deployed across two Availability Zones.\nIt provides high availability and failover support for DB instances.\nThis setup is primarily for disaster recovery.\nIt involves a primary DB instance and a standby replica, which is a copy of the primary DB instance.\nThe standby replica is not accessible directly; instead, it serves as a failover target in case the primary instance fails."},{"timestamp":"1699199760.0","upvote_count":"2","comment_id":"1063094","poster":"potomac","content":"Selected Answer: D\nIt is D.\nA is not correct. Multi-AZ DB instance deployment, which creates a primary instance and a standby instance to provide failover support. However, the standby instance does not serve traffic."},{"content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/database/choose-the-right-amazon-rds-deployment-option-single-az-instance-multi-az-instance-or-multi-az-database-cluster/#:~:text=Unlike%20Multi%2DAZ%20instance%20deployment,different%20AZs%20serving%20read%20traffic.\n\nAccording to this the answer is D\n\"Unlike Multi-AZ instance deployment, where the secondary instance can’t be accessed for read or writes, Multi-AZ DB cluster deployment consists of primary instance running in one AZ serving read-write traffic and two other standby running in two different AZs serving read traffic.\"\n\nYou don't have to create read replicas with cluster deployment so B is out.","comment_id":"1058670","timestamp":"1698746700.0","poster":"maudsha","upvote_count":"2"},{"poster":"kwang312","timestamp":"1694694900.0","comment_id":"1007617","content":"D\nFail-over on Multi-AZ DB instance is 60-120s\nOn Cluster, the time under 35s","upvote_count":"5"},{"upvote_count":"2","timestamp":"1693220880.0","poster":"Guru4Cloud","comment_id":"992082","content":"Selected Answer: D\nD. Use an Amazon RDS Multi-AZ DB cluster deployment. Point the read workload to the reader endpoint"},{"content":"Selected Answer: D\nUse an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.","poster":"Guru4Cloud","timestamp":"1693220400.0","upvote_count":"2","comment_id":"992073"},{"timestamp":"1692978300.0","poster":"Eminenza22","content":"Selected Answer: A\nThe solutions architect should use an Amazon RDS Multi-AZ DB instance deployment. The company can create one read replica and point the read workload to the read replica. Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments.","upvote_count":"1","comment_id":"990182"},{"content":"and d..\n\nMulti-AZ DB clusters typically have lower write latency when compared to Multi-AZ DB instance deployments. They also allow read-only workloads to run on reader DB instances.","comment_id":"939018","upvote_count":"2","poster":"Gooniegoogoo","timestamp":"1688122320.0"},{"content":"Selected Answer: D\nThis is as case where both option A and D can work, but option D gives 2 DB instances for read compared to only 1 given by option A. Costwise they are the same as both options use 3 DB instances.","timestamp":"1686111480.0","comment_id":"916845","upvote_count":"2","poster":"TariqKipkemei"},{"comment_id":"910539","content":"Selected Answer: A\nlowest cost option, and effective with read replica","timestamp":"1685472060.0","poster":"Henrytml","upvote_count":"3"},{"poster":"antropaws","content":"Selected Answer: D\nIt's D. Read well: \"A company wants to use an Amazon RDS for PostgreSQL DB CLUSTER\".","comment_id":"910160","upvote_count":"4","timestamp":"1685443140.0"},{"upvote_count":"3","content":"Selected Answer: D\nA Multi-AZ DB cluster deployment is a semisynchronous, high availability deployment mode of Amazon RDS with two readable standby DB instances. A Multi-AZ DB cluster has a writer DB instance and two reader DB instances in three separate Availability Zones in the same AWS Region.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html\n\nAmazon RDS Multi-AZ with two readable standbys. Automatically fail over in typically under 35 seconds\nhttps://aws.amazon.com/rds/features/multi-az/","timestamp":"1685424540.0","poster":"[Removed]","comment_id":"909913"},{"content":"A Multi-AZ DB cluster deployment is a semisynchronous, high availability deployment mode of Amazon RDS with two readable standby DB instances. A Multi-AZ DB cluster has a writer DB instance and two reader DB instances in three separate Availability Zones in the same AWS Region.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html\n\nAmazon RDS Multi-AZ with two readable standbys. Automatically fail over in typically under 35 seconds\nhttps://aws.amazon.com/rds/features/multi-az/","upvote_count":"2","poster":"[Removed]","comment_id":"909910","timestamp":"1685424480.0"},{"poster":"omoakin","content":"D.\nUse an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.","timestamp":"1685385240.0","upvote_count":"2","comment_id":"909636"},{"poster":"coldgin37","timestamp":"1685124300.0","content":"D - Instance deployment Failover times are typically 60–120 seconds, so a clustered deployment is required for 40sec or less\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html","upvote_count":"3","comment_id":"907536"},{"upvote_count":"4","poster":"elmogy","comment_id":"906785","timestamp":"1685032800.0","content":"Selected Answer: D\nD for two reasons, \n1- Failover times are typically 60–120 seconds in RDS Multi-AZ DB instance deployment.\n2- we can use the secondary DB for read (it can be used on RDS Multi-AZ DB cluster), and that's will \"keep the cost as low as possible\""},{"poster":"Cipi","content":"In both options A and B we have 3 database instances:\n- Option A: 1 instance for read and write, 1 standby instance and 1 additional instance for read\n- Option B: 1 instance for read and write and 2 instances for both read and standby\nThus, option B gives 2 DB instances for read compared to only 1 given by option A and costs seems to be in favor of option B in case we consider on-demand instances (https://aws.amazon.com/rds/postgresql/pricing/?pg=pr&loc=3). So I consider option B is better","timestamp":"1684617480.0","comment_id":"902778","upvote_count":"1"},{"comment_id":"900470","poster":"Axaus","content":"Selected Answer: A\nA.\nIt has to be cost effective. Multi A-Z for availability and 1 read replica.","upvote_count":"1","timestamp":"1684352160.0"},{"poster":"greyrose","comment_id":"898203","content":"Selected Answer: A\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA","timestamp":"1684148940.0","upvote_count":"1"},{"content":"Selected Answer: A\nRDS Multi-AZ DB instance deployment is a highly available and scalable database deployment option that provides automatic failover support in most scenarios in less than 40 seconds.","timestamp":"1684142340.0","poster":"nosense","upvote_count":"2","comment_id":"898152"}],"answer_description":""},{"id":"kw28NAjZzal9pBMb4cen","topic":"1","answers_community":["B (86%)","8%"],"timestamp":"2023-05-15 11:28:00","answer_ET":"B","answer":"B","answer_images":[],"answer_description":"","discussion":[{"content":"Selected Answer: B\nNot A - Transfer Family canj't use EBS\nB - Possible and meets requirement\nNot C - S3 doesn't guarantee \"high IOPS performance\"; also there is no \"public endpoint that allows only trusted IP addresses\" (you can assign a Security Group to a public endpoint but that is not mentioned here)\nNot D - Endpoint would be in private subnet, not accessible from Internet at all","timestamp":"1704019440.0","comment_id":"1110462","upvote_count":"9","poster":"pentium75"},{"upvote_count":"7","timestamp":"1685838300.0","comment_id":"913993","poster":"alexandercamachop","content":"Selected Answer: B\nFirst Serverless - EFS\nSecond it says it is attached to the Linux instances at the same time, only EFS can do that."},{"poster":"FlyingHawk","content":"Selected Answer: B\nD, the description \"VPC endpoint that has internal access in a private subnet\" is technically incorrect and doesn't make sense for AWS Transfer Family.\nAWS Transfer Family actually supports:\n\nPublic endpoints (internet-facing)\nVPC endpoints (private network access)\n\nThe VPC endpoint doesn't have \"internal access\" - it's about how the SFTP service is configured to interact with your VPC network resources.","timestamp":"1733346240.0","comments":[{"upvote_count":"1","content":"A more accurate description for a VPC-based AWS Transfer Family SFTP service would be:\n\nCreate an AWS Transfer Family SFTP service with a VPC endpoint\nPlace the endpoint in a private subnet\nConfigure security groups to allow only trusted IP sources\nAttach the S3 bucket to the SFTP service endpoint\n\nSo while the overall strategy in option D (using S3 with VPC endpoint and security group restrictions) could be valid, the specific wording about \"internal access\" is incorrect.\nThis technical inaccuracy in the description would make D an incorrect answer in a certification or technical assessment.","comment_id":"1322088","poster":"FlyingHawk","timestamp":"1733346420.0"},{"poster":"FlyingHawk","timestamp":"1733348160.0","upvote_count":"1","content":"C public endpoint that allows only trusted IP addresses does not sound technical correct as only the security group can restrict the IP address. the correct description should \" Create an AWS Transfer Family SFTP service in public subnet with security group allows only trusted IP address, create the vpc endpoint (gateway endpoint here for saving the cost) of S3 bucket to allow the SFTP service can access it privately, grant user access to the SFTP service via IAM policies and grant the access of S3 to SFTP service via IAM role","comment_id":"1322100"}],"comment_id":"1322085","upvote_count":"1"},{"comments":[{"upvote_count":"1","timestamp":"1733348880.0","comment_id":"1322107","content":"After read the description of C, I feel the description of \"an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses\" does not sound correct, the endpoint itself cannot restrict the IP address from internet, only the security group. the correct description should be: Create an AWS Transfer Family SFTP service with a public endpoint and security group that allows only trusted IP addresses. create the vpc endpoint (gateway endpoint here for saving the cost) of S3 bucket to allow the SFTP service can access it privately, grant user access to the SFTP service via IAM policies and grant the access of S3 to SFTP service via IAM role","poster":"FlyingHawk"}],"content":"Selected Answer: C\nAmazon S3 provides high throughput and performance suitable for many use cases, including those requiring high IOPS (Input/Output Operations Per Second). S3 is not a block storage solution (like EBS) or a file system (like EFS). While its performance is exceptional for its intended use case (object storage), it may not match the millisecond latency or consistent high IOPS required for transactional databases or other ultra-low latency applications.\nFor applications where sub-millisecond latency or extremely high random IOPS (e.g., 64,000 IOPS) is required, solutions like EBS or EFS would be better.","upvote_count":"1","timestamp":"1733294280.0","poster":"FlyingHawk","comment_id":"1321745"},{"upvote_count":"1","poster":"JA2018","content":"Selected Answer: B\nActually AWS Transfer Family can use S3, so it's a toss-up between Options B & C but I tend to favour Option B for the following reasons (based on the keys in STEM):\n\ncompany runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept traffic from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers.\n1. Requires a serverless option that provides high IOPS performance and highly configurable security. \n\n2. User also wants to maintain control over user permissions","timestamp":"1732278840.0","comment_id":"1316302"},{"content":"Option B best meets the company's requirements by leveraging AWS Transfer Family with an EFS volume, ensuring high availability, security, and performance.","poster":"523db89","upvote_count":"2","timestamp":"1724066220.0","comment_id":"1268598"},{"comment_id":"1065765","upvote_count":"4","poster":"NickGordon","content":"Selected Answer: B\nA is incorrect as EBS is not an option\nC is incorrect as when I select public accessible, I don't see an option I can set up trusted IP address\nD isi incorrect as it is internal.\n\nB, followed the steps and I can set up a sftp in this way","timestamp":"1699461180.0"},{"poster":"potomac","content":"Selected Answer: B\nB\nEFS has lower latency and higher throughput than S3 when accessed from within the same availability zone.","comment_id":"1063097","timestamp":"1699200060.0","upvote_count":"3"},{"comments":[{"content":"Amazon Elastic File System - Serverless, fully elastic file storage:\nhttps://aws.amazon.com/efs/","upvote_count":"5","timestamp":"1697902320.0","comment_id":"1049610","poster":"warp"}],"poster":"thanhnv142","upvote_count":"1","timestamp":"1697808720.0","comment_id":"1048827","content":"C: Because it is server-less. deffinitely not A or B because it utilizes server."},{"poster":"bsbs1234","content":"B, \nA), transfer family does not support EBS\nC,D), S3 has lower IOPS than EFS","upvote_count":"4","comment_id":"1020840","timestamp":"1695995580.0"},{"upvote_count":"2","timestamp":"1693218720.0","poster":"Guru4Cloud","content":"Selected Answer: B\nCreate an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.","comment_id":"992055"},{"upvote_count":"2","poster":"Axeashes","comment_id":"922601","content":"https://aws.amazon.com/blogs/storage/use-ip-whitelisting-to-secure-your-aws-transfer-for-sftp-servers/","timestamp":"1686696900.0"},{"content":"Selected Answer: B\nEFS is best to serve this purpose.","comment_id":"916852","timestamp":"1686112380.0","upvote_count":"2","poster":"TariqKipkemei"},{"poster":"envest","content":"Answer C (from abylead.com)\nTransfer Family offers fully managed serverless support for B2B file transfers via SFTP, AS2, FTPS, & FTP directly in & out of S3 or EFS. For a controlled internet access you can use internet-facing endpts with Transfer SFTP servers & restrict trusted internet sources with VPC's default Sgrp. In addition, S3 Access Points aliases allows you to use S3 bkt names for a unique access control plcy on shared S3 datasets.\nTransfer SFTP & S3: https://aws.amazon.com/blogs/apn/how-to-use-aws-transfer-family-to-replace-and-scale-sftp-servers/\n\nA)Transfer SFTP doesn’t support EBS, not for share data, & not serverless: infeasible.\nB)EFS mounts via ENIs not endpts: infeasible.\nD)pub endpt for internet access is missing: infeasible.","timestamp":"1685447700.0","upvote_count":"4","comment_id":"910200"},{"comment_id":"909638","upvote_count":"1","timestamp":"1685385420.0","content":"BBBBBBBBBBBBBB","poster":"omoakin"},{"poster":"vesen22","content":"Selected Answer: B\nEFS all day","timestamp":"1685350140.0","upvote_count":"2","comment_id":"909216"},{"timestamp":"1685279400.0","comment_id":"908653","poster":"norris81","upvote_count":"2","content":"https://aws.amazon.com/blogs/storage/use-ip-whitelisting-to-secure-your-aws-transfer-for-sftp-servers/ is worth a read"},{"upvote_count":"3","poster":"odjr","content":"Selected Answer: B\nEFS is serverless. There is no reference in S3 about IOPS","comment_id":"908116","timestamp":"1685207400.0"},{"upvote_count":"3","content":"Selected Answer: B\nOption D is incorrect because it suggests using an S3 bucket in a private subnet with a VPC endpoint, which may not meet the requirement of maintaining control over user permissions as effectively as the EFS-based solution.","poster":"willyfoogg","comment_id":"908012","timestamp":"1685193720.0"},{"upvote_count":"1","poster":"anibinaadi","content":"It is D\nRefer https://docs.aws.amazon.com/transfer/latest/userguide/create-server-in-vpc.html for further details.","comment_id":"907968","timestamp":"1685189640.0","comments":[{"upvote_count":"1","poster":"pentium75","content":"In D you create an \"endpoint that has internal access in a private subnet\", how to access that from the Internet?","comment_id":"1110461","timestamp":"1704019200.0"}]},{"comment_id":"906791","timestamp":"1685033580.0","upvote_count":"5","content":"Selected Answer: B\nEFS is serverless and has high IOPS.\nregardless the IOPS, I believe option D is incorrect because it is internal, and the request needs internet access","poster":"elmogy"},{"timestamp":"1684925580.0","poster":"alvinnguyennexcel","upvote_count":"1","comment_id":"905763","content":"Selected Answer: C\nThe reason is that AWS Transfer Family is a serverless option that provides a fully managed service for transferring files over Secure Shell (SSH) File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP). It allows you to use your existing authentication systems and store your data in Amazon S3 or Amazon EFS. It also provides high IOPS performance and highly configurable security option"},{"timestamp":"1684759380.0","comment_id":"904037","upvote_count":"2","poster":"luisgu","content":"Selected Answer: B\nThe question is requiring highly configurable security --> that excludes default S3 encryption, which is SSE-S3 (is not configurable)"},{"timestamp":"1684568940.0","comment_id":"902419","poster":"Rob1L","content":"Selected Answer: C\nOption D is not the best choice for this scenario because the AWS Transfer Family SFTP service, when configured with a VPC endpoint that has internal access in a private subnet, will not be accessible from the internet.","upvote_count":"1"},{"poster":"hiroohiroo","comment_id":"902321","content":"Selected Answer: D\nS3＋VPCエンドポイント","upvote_count":"1","timestamp":"1684548420.0"},{"comment_id":"901835","content":"EFS is a serverless, fully elastic storage as mentioned below\n\nhttps://aws.amazon.com/efs/","comments":[{"content":"Also, S3 is a blob storage service and there aren't any IOPS metric for S3 which inclines more towards EFS","timestamp":"1684493280.0","upvote_count":"2","comment_id":"901843","poster":"y0"}],"poster":"y0","upvote_count":"2","timestamp":"1684492380.0"},{"upvote_count":"2","content":"Should not it B, according to ChatGPT?\nAmazon EFS provides a serverless file storage option with high IOPS performance, which is suitable for the shared storage requirement of the SFTP service.\nThe AWS Transfer Family allows you to create an SFTP service with highly configurable security. By configuring a VPC endpoint with internet-facing access and attaching a security group that allows only trusted IP addresses, you can control access to the SFTP service.\nBy attaching an encrypted Amazon EFS volume to the SFTP service endpoint, you can ensure data at rest is encrypted, meeting the security requirements.\nGranting users access to the SFTP service allows you to maintain control over user permissions, as user accounts are managed as Linux users within the SFTP servers.","comment_id":"900529","comments":[{"comment_id":"901193","timestamp":"1684412460.0","upvote_count":"1","poster":"nosense","content":"Option B is not the correct answer because it does not meet a serverless option"}],"timestamp":"1684361940.0","poster":"cloudenthusiast"},{"comment_id":"899152","content":"Selected Answer: D\nD for me as well","timestamp":"1684238220.0","upvote_count":"1","poster":"Efren"},{"comments":[{"poster":"nosense","content":"changed to b, in the end. because\nIOPS efs - Up to 300,000 s3- Up to 5,500","timestamp":"1684516140.0","upvote_count":"3","comment_id":"902141"}],"content":"Selected Answer: D\nhigh IOPS performance, highly configurable security","comment_id":"898157","timestamp":"1684142880.0","poster":"nosense","upvote_count":"2"}],"question_text":"A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept traffic from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers.\n\nThe company wants a serverless option that provides high IOPS performance and highly configurable security. The company also wants to maintain control over user permissions.\n\nWhich solution will meet these requirements?","question_images":[],"isMC":true,"choices":{"C":"Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.","A":"Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.","D":"Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.","B":"Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service."},"unix_timestamp":1684142880,"exam_id":31,"question_id":379,"url":"https://www.examtopics.com/discussions/amazon/view/109270-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"ReEj7LiRDKFnTAF7N3zu","discussion":[{"content":"asynchronous=SQS, microservices=ECS.\nUse AWS Auto Scaling to adjust the number of ECS services.","poster":"examtopictempacc","comments":[{"content":"good breakdown :)","comment_id":"916858","poster":"TariqKipkemei","timestamp":"1701931080.0","upvote_count":"3"}],"upvote_count":"16","comment_id":"903708","timestamp":"1700634780.0"},{"timestamp":"1701931260.0","content":"Selected Answer: D\nFor once examtopic answer is correct :) haha...\n\nBatch requests/async = Amazon SQS\nMicroservices = Amazon ECS\nWorkload variations = AWS Auto Scaling on Amazon ECS","upvote_count":"11","poster":"TariqKipkemei","comment_id":"916861"},{"comment_id":"1196205","content":"Selected Answer: D\nALB is mentioned in other options to distract you, you dont need ALB for scaling here, we would need ECS autoscaling, they play with that idea in option B a bit however D gets it in a completely optimized way.... A and C both have lambda which for Machine learning models with workloads on heavy side, will not fly","poster":"wizcloudifa","timestamp":"1729023900.0","upvote_count":"2"},{"upvote_count":"3","poster":"Guru4Cloud","comment_id":"992048","timestamp":"1709123100.0","content":"Selected Answer: D\nI go with everyone D."},{"upvote_count":"2","poster":"alexandercamachop","content":"Selected Answer: D\nD, no need for an App Load balancer like C says, no where in the text.\nSQS is needed to ensure all request gets routed properly in a Microservices architecture and also that it waits until its picked up.\nECS with Autoscaling, will scale based on the unknown pattern of usage as mentioned.","timestamp":"1701660480.0","comment_id":"914028"},{"content":"It is D\nRefer https://aws.amazon.com/blogs/containers/amazon-elastic-container-service-ecs-auto-scaling-using-custom-metrics/ for additional information/knowledge.","timestamp":"1701095220.0","comment_id":"907975","poster":"anibinaadi","upvote_count":"2"},{"content":"Selected Answer: D\nbecause it is scalable, reliable, and efficient.\nC does not scale the models automatically","comments":[{"timestamp":"1709272980.0","upvote_count":"1","content":"why C doesn't scale the model? Application Auto Scaling can apply to lambda.","comments":[{"comment_id":"1110466","timestamp":"1719737460.0","content":"How would you \"use Auto Scaling (!) to increase the number of vCPUs (!) for the Lamba functions\"?","upvote_count":"3","poster":"pentium75"},{"upvote_count":"2","poster":"NSA_Poker","comment_id":"1222411","timestamp":"1733023320.0","content":"Auto Scaling doesn't apply to Lambda. As your functions receive more requests, Lambda automatically handles scaling the number of execution environments until you reach your account's concurrency limit."}],"poster":"deechean","comment_id":"995643"}],"upvote_count":"4","comment_id":"898230","poster":"nosense","timestamp":"1700055600.0"}],"exam_id":31,"topic":"1","answer":"D","unix_timestamp":1684150800,"question_id":380,"answers_community":["D (100%)"],"question_images":[],"answer_images":[],"answer_ET":"D","question_text":"A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent.\n\nThe company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time.\n\nWhich design should a solutions architect recommend to meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/109280-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","timestamp":"2023-05-15 13:40:00","isMC":true,"choices":{"B":"Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.","D":"Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.","A":"Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.","C":"Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size."}}],"exam":{"id":31,"isBeta":false,"isMCOnly":true,"provider":"Amazon","isImplemented":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025","numberOfQuestions":1019},"currentPage":76},"__N_SSP":true}