{"pageProps":{"questions":[{"id":"2aXXsiwsKdVvV0Kr4w3p","choices":{"A":"Define an IAM group for each manager with each employee as an IAM user in that group, and use that to limit the access.","C":"Define a key for each manager in AWS KMS and encrypt the data for their employees with their private keys.","B":"Use Amazon Redshift snapshot to create one cluster per manager. Allow the manager to access only their designated clusters.","D":"Define a view that uses the employee’s manager name to filter the records based on current user names."},"exam_id":17,"answer_ET":"A","answer_images":[],"question_images":[],"question_text":"Managers in a company need access to the human resources database that runs on Amazon Redshift, to run reports about their employees. Managers must only see information about their direct reports.\nWhich technique should be used to address this requirement with Amazon Redshift?","discussion":[{"timestamp":"1632158280.0","comment_id":"6665","content":"Answer is D. One of the reasons for not going with A is, max number of IAM groups in an AWS account is 300. So, A is not scalable solution. If the company has more than 300 managers, A won't work.","upvote_count":"8","poster":"mattyb123","comments":[{"timestamp":"1632161100.0","poster":"pra276","upvote_count":"1","comment_id":"7535","comments":[{"upvote_count":"2","timestamp":"1634533140.0","content":"But you can have up-to 5000 IAM users in one AWS account","poster":"BigEv","comment_id":"19821"}],"content":"I would go with A. You can create as many groups as you can."}]},{"content":"All in all, option D is the correct answer\n\n\"The second advantage of views is that you can assign a different set of permissions to the view. A user might be able to query the view, but not the underlying table. Creating the view excluding the sensitive columns (or rows) should be useful in this scenario.\"\nhttp://www.silota.com/blog/rethink-database-views-redshift/\n\n\"For example, the following command enables the user HR both to perform SELECT commands on the employees table and to grant and revoke the same privilege for other users.\ngrant select on table employees to HR with grant option;\"\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html","comment_id":"39877","poster":"ME2000","upvote_count":"5","timestamp":"1635287820.0"},{"timestamp":"1636264080.0","content":"D is probably the best option. A means creating a IAM user for each employee. While we have no details of the industry or the number of employees, in general that doesn't seem a great idea.","poster":"MichRox","comment_id":"133750","upvote_count":"1"},{"upvote_count":"3","comment_id":"76101","content":"Answer : D – \nNot A – Lot of maintenance to create one group per manager.\nNot B – Cost overhead- cost will multiple manifold.\nNot C – Doesn’t make sense. How will RedShift talk to KMS? Who will manage the key-pair for each manager in KMS.\nD – is the correct answer because you can create view to provide row-level access based on the attribute values in the underlying table.","poster":"Bulti","timestamp":"1636170720.0"},{"timestamp":"1636092360.0","poster":"san2020","comment_id":"52355","content":"my selection D","upvote_count":"5"},{"content":"D is the right choice. There is only one option for RLS (row-level security)","poster":"practicioner","timestamp":"1635618600.0","upvote_count":"2","comment_id":"40621"},{"poster":"Raju_k","upvote_count":"1","timestamp":"1635179460.0","comment_id":"22310","content":"I would choose D over A since IAM access let you control access or deny at table level but not at records level as per my understanding."},{"content":"D, using view you can restrict data that is being retrieved from Redshift. It is a common practice in traditional Relational DBs","upvote_count":"1","timestamp":"1634165940.0","poster":"cybe001","comment_id":"19244"},{"upvote_count":"2","timestamp":"1634135280.0","poster":"asadao","comment_id":"18259","content":"I went with A"},{"timestamp":"1634082000.0","poster":"Zire","content":"D is a common approach in this case. A should be incorrect since, e.j.\nCreate view my_employees as select * from employees where manager = db_user_who_is_a _manager. \nOption A is incorrect as per its wording. You don’t create IAM groups for a manager with each employee as an IAM user in that group...\nThis question is not related to security.","upvote_count":"1","comment_id":"14017"},{"content":"create as many groups to control access right, I don't hire pra276 even he has tons of Certs.","comment_id":"13651","poster":"pkfe","upvote_count":"3","timestamp":"1633906740.0"},{"timestamp":"1632528420.0","content":"Whoever says D is answer. Please read about these \nhttps://www.intermix.io/blog/iam-to-generate-temporary-amazon-redshift-passwords/ \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/generating-iam-credentials-steps.html","comments":[{"timestamp":"1633075260.0","content":"I completely understand where you are coming from but the question asks 'address this requirement with Amazon Redshift'. So the simple way to do that in redshift is through views. Also its hinted quite heavily on the acloudguru and aws big data exam prep course about using redshift views.","poster":"mattyb123","comment_id":"7832","upvote_count":"3","comments":[{"comment_id":"8785","timestamp":"1633785060.0","content":"Please view the big data exam preparation course on aws. Views are mentioned https://www.aws.training/Details/Curriculum?id=21332","poster":"mattyb123","upvote_count":"1"}]}],"poster":"pra276","comment_id":"7678","upvote_count":"1"},{"poster":"jlpl","upvote_count":"4","comment_id":"7551","content":"D, redshift view is allowed filter out the user base access","timestamp":"1632162300.0"}],"unix_timestamp":1565672340,"timestamp":"2019-08-13 06:59:00","answer":"A","topic":"1","answers_community":[],"url":"https://www.examtopics.com/discussions/amazon/view/3522-exam-aws-certified-big-data-specialty-topic-1-question-37/","isMC":true,"answer_description":"","question_id":31},{"id":"aRzHIIMK3NnEjG0kBt1p","exam_id":17,"choices":{"D":"Write them to CloudWatch Logs and use an AWS Lambda function to load them into HDFS on an Amazon Elastic MapReduce (EMR) cluster for analysis.","C":"Write them to the local disk and configure the Amazon CloudWatch Logs agent to load the data into CloudWatch Logs and subsequently into Amazon Elasticsearch Service.","B":"Write them to a file on Amazon Simple Storage Service (S3). Write an AWS Lambda function that runs in response to the S3 event to load the events into Amazon Elasticsearch Service for analysis.","A":"Write them directly to a Kinesis Firehose. Configure Kinesis Firehose to load the events into an Amazon Redshift cluster for analysis."},"answer_ET":"B","answer_images":[],"question_text":"A company is building a new application in AWS. The architect needs to design a system to collect application log events. The design should be a repeatable pattern that minimizes data loss if an application instance fails, and keeps a durable copy of a log data for at least 30 days.\nWhat is the simplest architecture that will allow the architect to analyze the logs?","question_images":[],"discussion":[{"poster":"Bulti","timestamp":"1634595540.0","upvote_count":"11","content":"Correct answer is C: \nNot A - because writing logs to RedShift doesn't make sense.\nNot B - because to write logs to S3 you will have to still configure Cloudwatch logs on the server or write code in your application to use S3 SDK to write the log data to S3 directly which is not the best or simplest solution.\nNot D -because EMR is not a storage service.\nOnly option that fits is C with no additional effort except for configuring Cloudwatch log agent to ship the logs to Cloudwatch and then configure CloudWatch to send the logs directly to Elasticsearch which doesn't require Lambda to glue them together.","comment_id":"75387"},{"poster":"ariane_tateishi","content":"For me the right option is B considering that \"Write them to the local disk\" will not be compliance with the requirement because if the application fail the logs will be lost.","comment_id":"368707","upvote_count":"1","timestamp":"1636262040.0"},{"comment_id":"337425","poster":"DerekKey","upvote_count":"2","timestamp":"1636177500.0","content":"C:\nYou can configure a CloudWatch Logs log group to stream data it receives to your Amazon Elasticsearch Service (Amazon ES) cluster in near real-time through a CloudWatch Logs subscription."},{"comment_id":"150163","poster":"Royk2020","upvote_count":"2","timestamp":"1636106220.0","content":"\"Simplest architecture\" .... Answer is C. No code , just configuration"},{"timestamp":"1635554640.0","poster":"abhineet","content":"A is perfect answer actually, kinesis firehose first writes data to s3, this meets saving logs upto 30 days with lifecycle policy on bucket, while redshift can be used for analysis","comment_id":"125116","upvote_count":"1"},{"poster":"kkyong","timestamp":"1635077040.0","upvote_count":"1","comment_id":"99755","content":"B is wrong. If you are a developer ,you will know application can't write log to s3 directly . \napplication must write log to buffer or file ,and then put to s3\nso C is correct answer","comments":[{"content":"good point, but i think \"write them to a file on S3\" leaves room for creating this file first locally and then store it to S3. no need to subsequentially write to an existing S3 object. but yeah, i still think C is correct due to Bulit's comments.","comment_id":"100975","timestamp":"1635264180.0","poster":"Corram","upvote_count":"1"}]},{"poster":"Bulti","content":"to support answer C- Here is from Cloudwatch FAQ\nCloudWatch Logs uses your log data for monitoring; so, no code changes are required. Long term log retention: You can use CloudWatch Logs to store your log data indefinitely in highly durable and cost effective storage without worrying about hard drives running out of space.","upvote_count":"4","comments":[{"poster":"Corram","upvote_count":"1","timestamp":"1635277980.0","comment_id":"100978","content":"to make this even more clear, Cloudwatch Log groups can also be easily streamed to ElasticSearch https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html"},{"upvote_count":"1","poster":"matthew95","comment_id":"118007","timestamp":"1635535380.0","content":"This is C, look at this page: https://www.oreilly.com/library/view/aws-automation-cookbook/9781788394925/30fd87cf-3d67-4363-a95a-5208296d32cb.xhtml"}],"comment_id":"79549","timestamp":"1634649480.0"},{"upvote_count":"1","comments":[{"upvote_count":"1","poster":"Corram","timestamp":"1634974200.0","content":"to me it looks like technically this could work. however, Elasticsearch is really suited for log file analysis and both Kinesis and Redshift can be expensive.","comment_id":"98918"}],"timestamp":"1634524740.0","content":"can anyone pls explain why not A?","poster":"jiedee","comment_id":"70659"},{"upvote_count":"3","comments":[{"poster":"Soona_Paana","upvote_count":"1","content":"Itc C... But a large portion of ur answers are right :p","timestamp":"1635372780.0","comment_id":"114068"}],"comment_id":"52356","content":"my selection B","poster":"san2020","timestamp":"1634486460.0"},{"poster":"ME2000","timestamp":"1634467680.0","comment_id":"39885","upvote_count":"2","comments":[{"content":"Simple way to do it through console. I guess the answer is C.\nhttps://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-aws-integrations.html#es-aws-integrations-cloudwatch-es","timestamp":"1634890200.0","comment_id":"96941","poster":"iamsajal","upvote_count":"1"}],"content":"The big problem with option C is how to load events into ES?\nSo one option left is B"},{"poster":"chaudh","timestamp":"1634097000.0","upvote_count":"2","comment_id":"21528","content":"B is correct"},{"upvote_count":"2","timestamp":"1634075640.0","comment_id":"21422","content":"B is Correct. \n\nLocal disk or EBS durability is 5 9's, while S3 durability is 11 9's.\n\nAs the requirement is to keep a durable copy of the log data, S3 is the the best option.","poster":"viduvivek"},{"comment_id":"19248","upvote_count":"1","content":"I also pick B, S3 is simple and durable and Elasticsearch is for log analysis.","timestamp":"1633413540.0","poster":"cybe001"},{"content":"the \"minimizes data loss if an application instance fails\" phrase hints me to option C.\nOption B looks to simple, but you have to re-invent the services (lambda probably) to transfer the files to S3 and schedule it.","poster":"Zire","timestamp":"1632956100.0","comment_id":"15552","upvote_count":"1"},{"content":"Answer is B","comment_id":"15051","poster":"M2","upvote_count":"2","timestamp":"1632943860.0"},{"content":"mattyb123 are you sure ?","upvote_count":"2","poster":"bigdatalearner","timestamp":"1632893160.0","comment_id":"13818"},{"comments":[{"poster":"exams","comments":[{"poster":"Nik225","timestamp":"1635330060.0","content":"Slightly more durable but not nearly as simple","comment_id":"111056","upvote_count":"1"}],"upvote_count":"2","content":"B. most durable","comment_id":"11538","timestamp":"1632860520.0"}],"upvote_count":"2","timestamp":"1632672960.0","content":"B. Question states minimizes data loss S3 should be used.","comment_id":"11065","poster":"pdach"},{"poster":"mattyb123","comment_id":"9685","upvote_count":"3","comments":[{"comments":[{"timestamp":"1633501680.0","upvote_count":"1","comment_id":"20475","content":"But CloudWatch Logs is...","poster":"d00ku"}],"timestamp":"1633485720.0","poster":"cybe001","content":"Local disk is not durable compared to S3","upvote_count":"1","comment_id":"19249"}],"timestamp":"1632499800.0","content":"Correct answer is C. \n1. Simplest architecture. \n2. No need to write a lambda function\n3. can create a repeatable pattern\n4. Integrates into Elasticsearch\nhttps://aws.amazon.com/cloudwatch/faqs/\nhttps://blog.flux7.com/why-we-recommend-everyone-use-amazon-cloudwatch-logs\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\nhttps://aws.amazon.com/blogs/big-data/viewing-amazon-elasticsearch-service-error-logs/"}],"unix_timestamp":1567657620,"timestamp":"2019-09-05 06:27:00","answer":"B","topic":"1","answers_community":[],"url":"https://www.examtopics.com/discussions/amazon/view/4724-exam-aws-certified-big-data-specialty-topic-1-question-38/","isMC":true,"answer_description":"","question_id":32},{"id":"oEcbbZLHbWPRwF96UYEz","answer_ET":"B","answers_community":[],"answer_images":[],"unix_timestamp":1565780400,"question_text":"An organization uses a custom map reduce application to build monthly reports based on many small data files in an Amazon S3 bucket. The data is submitted from various business units on a frequent but unpredictable schedule. As the dataset continues to grow, it becomes increasingly difficult to process all of the data in one day. The organization has scaled up its Amazon EMR cluster, but other optimizations could improve performance.\nThe organization needs to improve performance with minimal changes to existing processes and applications.\nWhat action should the organization take?","question_id":33,"answer":"B","discussion":[{"comment_id":"38756","timestamp":"1634617440.0","content":"The Answer is D.\n\nS3Distcp has native capability to combine multiple small files into large files and does not require any coding. So, minimal change to existing processes. \n\nhttps://aws.amazon.com/blogs/big-data/seven-tips-for-using-s3distcp-on-amazon-emr-to-move-data-efficiently-between-hdfs-and-amazon-s3/#5 \n\nAlso through a process of elimination, we can exclude \n\nA) DynamoDB is not a good service for large scale data analytics.\nC) Moving from EMR to ElasticSearch requires significant changes to existing processes and applications. \nB) Moving from MR to Spark requires significant changes to existing processes and applications. \nE) Moving from S3 to firehose requires significant changes to existing processes and applications.","comments":[{"upvote_count":"3","timestamp":"1634706240.0","content":"Option E is wrong because of how FH to aggregate data hourly into Amazon S3?","poster":"ME2000","comment_id":"39902"}],"poster":"jay1ram2","upvote_count":"10"},{"timestamp":"1635599760.0","comment_id":"117200","content":"D. Hadoop is optimized for reading a fewer number of large files rather than many small files, whether from S3 or HDFS. You can use S3DistCp to aggregate small files into fewer large files of a size that you choose, which can optimize your analysis for both performance and cost. https://aws.amazon.com/blogs/big-data/seven-tips-for-using-s3distcp-on-amazon-emr-to-move-data-efficiently-between-hdfs-and-amazon-s3/#5","upvote_count":"8","poster":"guruguru"},{"comment_id":"119007","timestamp":"1636184400.0","upvote_count":"1","content":"D is correct. You can aggregate files using DistCp command. Hadoop is optimized for reading a fewer number of large files rather than many small files","poster":"kriscool"},{"upvote_count":"2","comment_id":"75407","poster":"Bulti","content":"The choice is really between B and D. \nWith B- Spark has the capability to improve performance over the existing AMR setup if lets says its using Hive or Pig instead. However it still needs to deal with large number of small data sets to process which is really the issue. So at the end of the day it won't solve the root cause of the issue which is about I/O not about compute. \nWith D- This is the simplest solution that doesn't need the businesses to change the way they ingest those files into the organization today. It's a change that solves the root cause of the problem which is I/O yet keeps the overall process and flow unchanged.\n\nSo the correct answer is D.","timestamp":"1635014340.0"},{"comments":[{"timestamp":"1635262740.0","content":"Hadoop has a default chunk size of 64MB and is not efficient at processing many small files, that's why D actually does help.","upvote_count":"2","poster":"Corram","comment_id":"100970"}],"content":"It is B\n\nWrong for : \nA- is wrong because new function\nC- Lambda with elastic search - why something new \nD- data is already there, its just gorwing the issue is not how to store or move data - but how to process\nE- a big change for all","upvote_count":"2","comment_id":"52793","poster":"ramz123","timestamp":"1634882640.0"},{"poster":"san2020","upvote_count":"3","comment_id":"52357","timestamp":"1634861760.0","content":"my selection D"},{"timestamp":"1634518020.0","content":"Answer us D https://aws.amazon.com/blogs/big-data/seven-tips-for-using-s3distcp-on-amazon-emr-to-move-data-efficiently-between-hdfs-and-amazon-s3/","poster":"marwan","comment_id":"30611","upvote_count":"3"},{"content":"I support B since you can add a custom Jar so I believe you could use your own map-reduce app there","poster":"antoneti","comment_id":"28166","upvote_count":"1","timestamp":"1634327160.0"},{"upvote_count":"1","content":"I would choose D as it is simple solution to improve performance with minimal changes\n to the existing process.","timestamp":"1634288880.0","comment_id":"22316","poster":"Raju_k"},{"comment_id":"21429","timestamp":"1634112120.0","content":"D looks reasonable.\n\nUsing S3DistCp, you can efficiently copy large amounts of data from Amazon S3 into HDFS where it can be processed by subsequent steps in your Amazon EMR cluster.","upvote_count":"4","poster":"viduvivek"},{"poster":"[Removed]","comment_id":"20032","timestamp":"1634000760.0","upvote_count":"1","content":"What is the right answer for this? A or D?"},{"poster":"cybe001","upvote_count":"3","comment_id":"19251","timestamp":"1633489860.0","content":"Option A, require EMR to access DynamoDB to get the index files which needs change in current setup. Using Option D, you can add a S3DistCp step in EMR job which is minimal change. Option D is correct"},{"comment_id":"15052","upvote_count":"3","poster":"M2","timestamp":"1633462500.0","content":"A,E & C are not minimal changes. the question says minimal changes to existing processes and application. Answer should be D or B."},{"upvote_count":"1","comment_id":"11539","poster":"exams","content":"Any thought on B?","timestamp":"1632694020.0"},{"content":"A is correct","comment_id":"7559","poster":"jlpl","upvote_count":"1","timestamp":"1632309780.0"},{"poster":"muhsin","comment_id":"6893","content":"a require some additional services. the question asks with minimal changes.","timestamp":"1632307380.0","upvote_count":"3","comments":[{"content":"Since the app is written for map reduce wouldn't adding spark mean the app would have to be re-written for spark?","upvote_count":"2","poster":"mattyb123","comment_id":"8786","timestamp":"1632687060.0"}]},{"timestamp":"1632107340.0","upvote_count":"1","content":"Thoughts on A?","comment_id":"6853","poster":"mattyb123"}],"timestamp":"2019-08-14 13:00:00","exam_id":17,"answer_description":"","isMC":true,"choices":{"D":"Schedule a daily AWS Data Pipeline process that aggregates content into larger files using S3DistCp.","C":"Use Amazon S3 Event Notifications and AWS Lambda to index each file into an Amazon Elasticsearch Service cluster.","B":"Add Spark to the Amazon EMR cluster and utilize Resilient Distributed Datasets in-memory.","A":"Use Amazon S3 Event Notifications and AWS Lambda to create a quick search file index in DynamoDB.","E":"Have business units submit data via Amazon Kinesis Firehose to aggregate data hourly into Amazon S3."},"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/3586-exam-aws-certified-big-data-specialty-topic-1-question-39/","topic":"1"},{"id":"f0znJjbo8TZ5UofaNhUD","choices":{"A":"When the administrator needs to optimize a large, slowly changing dimension table.","E":"When the administrator needs to take advantage of data locality on a local node for joins and aggregates.","D":"When the administrator needs to balance data distribution and collocation data.","C":"When the administrator needs to optimize the fact table for parity with the number of slices.","B":"When the administrator needs to reduce cross-node traffic."},"url":"https://www.examtopics.com/discussions/amazon/view/3422-exam-aws-certified-big-data-specialty-topic-1-question-4/","timestamp":"2019-08-10 14:06:00","discussion":[{"upvote_count":"7","content":"bde? any thoughts?","timestamp":"1632633660.0","comment_id":"6447","comments":[{"comments":[{"upvote_count":"3","content":"BDE seems good","poster":"exams","comments":[{"poster":"cybe001","timestamp":"1633621500.0","comment_id":"19172","content":"I also choose BDE","upvote_count":"2"}],"comment_id":"11394","timestamp":"1632813780.0"}],"content":"agreed","timestamp":"1632658380.0","comment_id":"6630","poster":"mattyb123","upvote_count":"3"},{"comment_id":"104200","poster":"kkyong","timestamp":"1635581820.0","upvote_count":"1","content":"For slowly changing dimensions of reasonable size, DISTSTYLE ALL is a good choice for the dimension (reasonable size in this case means up to a few million rows, and that the number of rows in the dimension table is fewer than the filtered fact table for a typical join)"}],"poster":"jlpl"},{"comment_id":"76617","timestamp":"1635351420.0","content":"B,D and E are the right choices. Please read the article at the below link in its entirety and watch out for phrases used as options in this question and you will also arrive at B, D and E.\nhttps://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/","poster":"Bulti","upvote_count":"7"},{"comment_id":"199123","upvote_count":"1","poster":"Mahesh22","timestamp":"1635969300.0","content":"ABD is correct. Reference https://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/"},{"timestamp":"1635729960.0","upvote_count":"2","comment_id":"127396","poster":"skytango","content":"B cannot be the right option cause it will not reduce the cross-node traffic. I would choose C,D,E."},{"timestamp":"1635213780.0","poster":"YashBindlish","upvote_count":"1","content":"BCE. why C because fact table's corresponding foreign key as DISTKEY which is a pattern for Key Distribution Type","comment_id":"74472"},{"content":"A is right","timestamp":"1635204780.0","poster":"yuvaraj228","comment_id":"61340","upvote_count":"2"},{"upvote_count":"3","timestamp":"1634989140.0","content":"Selected BDE","poster":"san2020","comment_id":"52141"},{"content":"B,D,E are my options","comment_id":"50570","upvote_count":"2","timestamp":"1634967240.0","poster":"shouvanik"},{"timestamp":"1634962800.0","upvote_count":"1","poster":"practicioner","content":"From my point of view:\nA - is an option for ALL distribution.\nC - is an option for even distribution\n\nBDE - is a right answers","comment_id":"43878"},{"timestamp":"1634587080.0","comment_id":"41791","content":"A,C,D looks correct \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html","upvote_count":"1","poster":"kalpanareddy"},{"comment_id":"38809","poster":"ME2000","timestamp":"1634348640.0","content":"Options B and E are for the outlook of KEY distribution.\nQuestion is for the determining factors for KEY distribution (DISTKEY), So Options A, C and D are the correct answers.\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_designating_distribution_styles.html\nhttps://aws.amazon.com/blogs/big-data/amazon-redshift-engineerings-advanced-table-design-playbook-distribution-styles-and-distribution-keys/","upvote_count":"1"},{"upvote_count":"1","poster":"michelleY","timestamp":"1634317440.0","comment_id":"35015","content":"BCE looks good, key distribution is for fact table in star schema."},{"comment_id":"23954","timestamp":"1634311200.0","content":"BDE seems right","poster":"WWODIN","upvote_count":"2"},{"content":"BCE looks correct to me. D is not because even distribution does the balancing.","comment_id":"14951","comments":[{"content":"No, because collocation is not possible with even distribution. So in order to achieve collocation and balancing, Key-distribution style is needed. Also, C is really a great use case for Even-distribution style, so C is wrong.","comment_id":"100294","poster":"Corram","timestamp":"1635408840.0","upvote_count":"3"}],"upvote_count":"2","timestamp":"1633527840.0","poster":"M2"}],"answer_description":"","answer":"ACD","answer_ET":"ACD","unix_timestamp":1565438760,"question_text":"An administrator needs to design a distribution strategy for a star schema in a Redshift cluster. The administrator needs to determine the optimal distribution style for the tables in the Redshift schema.\nIn which three circumstances would choosing Key-based distribution be most appropriate? (Select three.)","answer_images":[],"isMC":true,"question_images":[],"topic":"1","answers_community":[],"question_id":34,"exam_id":17},{"id":"6NR2jEZwMw78gcSVgbaN","answers_community":[],"choices":{"D":"Configure Lambda to read from fewer shards in parallel.","A":"Add more Lambda functions to improve concurrent batch processing.","C":"Ignore and skip events that are older than 5 minutes and put them to Dead Letter Queue (DLQ).","B":"Reduce the batch size that Lambda is reading from the stream."},"url":"https://www.examtopics.com/discussions/amazon/view/3592-exam-aws-certified-big-data-specialty-topic-1-question-40/","question_text":"An administrator is processing events in near real-time using Kinesis streams and Lambda. Lambda intermittently fails to process batches from one of the shards due to a 5-munite time limit.\nWhat is a possible solution for this problem?","question_images":[],"topic":"1","answer_ET":"D","answer":"D","answer_images":[],"discussion":[{"upvote_count":"6","comments":[{"content":"More resources:\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\nTest with different batch and record sizes so that the polling frequency of each event source is tuned to how quickly your function is able to complete its task. BatchSize controls the maximum number of records that can be sent to your function with each invoke. A larger batch size can often more efficiently absorb the invoke overhead across a larger set of records, increasing your throughput. By default, Lambda invokes your function as soon as records are available in the stream. If the batch it reads from the stream only has one record in it, Lambda only sends one record to the function. To avoid invoking the function with a small number of records, you can tell the event source to buffer records for up to 5 minutes by configuring a batch window. Before invoking the function, Lambda continues to read records from the stream until it has gathered a full batch, or until the batch window expires.","timestamp":"1632487320.0","poster":"mattyb123","comment_id":"9516","upvote_count":"7"}],"content":"I think it is B. with Lambda, we can finetune \nMemory\nBatch-size\nTimeout. There is no option for lambda to tune shards reading. \nhttps://tech.trivago.com/2018/07/13/aws-kinesis-with-lambdas-lessons-learned/","timestamp":"1632137700.0","comment_id":"6866","poster":"muhsin"},{"poster":"muhsin","comments":[{"comment_id":"6901","poster":"mattyb123","comments":[{"upvote_count":"4","comment_id":"11541","comments":[{"timestamp":"1633707360.0","upvote_count":"3","comment_id":"19252","content":"Yes, it is B","poster":"cybe001"}],"poster":"exams","content":"B. Agreed","timestamp":"1633244280.0"}],"upvote_count":"3","timestamp":"1632467340.0","content":"agreed"}],"upvote_count":"6","comment_id":"6894","timestamp":"1632419340.0","content":"each Kinesis shard invoke a separate Lambda."},{"poster":"san2020","comment_id":"52358","timestamp":"1636140240.0","content":"my selection B","upvote_count":"3"},{"poster":"ME2000","timestamp":"1634483340.0","upvote_count":"2","comment_id":"37973","content":"B. is correct\nQuestion asking for \"from one of the shards\"\nhere we go...\nError Handling\nFor function errors, you can also configure the event source mapping to split a failed batch into two batches. Retrying with smaller batches isolates bad records and works around timeout issues.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"},{"content":"I would select B as suggested by muhsin and mattyb123.","poster":"Raju_k","upvote_count":"3","comment_id":"22317","timestamp":"1634400840.0"}],"exam_id":17,"isMC":true,"answer_description":"","question_id":35,"timestamp":"2019-08-14 15:38:00","unix_timestamp":1565789880}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","name":"AWS Certified Big Data - Specialty","numberOfQuestions":85,"id":17,"isImplemented":true,"isBeta":false,"provider":"Amazon"},"currentPage":7},"__N_SSP":true}