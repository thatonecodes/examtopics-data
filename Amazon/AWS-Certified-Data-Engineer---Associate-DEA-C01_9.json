{"pageProps":{"questions":[{"id":"qBXUvi4uADyEChXMpeSJ","timestamp":"2024-08-12 12:48:00","answer":"C","discussion":[{"timestamp":"1723459680.0","content":"Selected Answer: C\nAthena does not recognize exclude patterns that you specify an AWS Glue crawler. For example, if you have an Amazon S3 bucket that contains both .csv and .json files and you exclude the .json files from the crawler, Athena queries both groups of files. To avoid this, place the files that you want to exclude in a different location. \nhttps://docs.aws.amazon.com/athena/latest/ug/troubleshooting-athena.html","upvote_count":"8","comment_id":"1264605","poster":"teo2157"},{"poster":"AdityaB","timestamp":"1728933180.0","content":"If the AWS Glue crawler is configured to exclude .json files, then the AWS Glue Data Catalog will not have any metadata related to those .json files. In this case, the Athena table that uses the Glue Data Catalog would not be aware of the .json files at all, and Athena queries would only process the files that are included in the Glue catalog (e.g., .csv files).","comment_id":"1297793","upvote_count":"1"},{"comment_id":"1282687","upvote_count":"1","content":"Athena will scan both types of files.\n\nAlthough it may be feasible to adjust Athena query to exclude .json, the SHORTEST query times would be via relocating .json files to different path.","poster":"BenLearningDE","timestamp":"1726151880.0"}],"answer_ET":"C","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/145607-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true,"unix_timestamp":1723459680,"answer_images":[],"question_text":"A data engineer is using an AWS Glue crawler to catalog data that is in an Amazon S3 bucket. The S3 bucket contains both .csv and json files. The data engineer configured the crawler to exclude the .json files from the catalog.\n\nWhen the data engineer runs queries in Amazon Athena, the queries also process the excluded .json files. The data engineer wants to resolve this issue. The data engineer needs a solution that will not affect access requirements for the .csv files in the source S3 bucket.\n\nWhich solution will meet this requirement with the SHORTEST query times?","choices":{"C":"Relocate the .json files to a different path within the S3 bucket.","B":"Use the Athena console to ensure the Athena queries also exclude the .json files.","D":"Use S3 bucket policies to block access to the .json files.","A":"Adjust the AWS Glue crawler settings to ensure that the AWS Glue crawler also excludes .json files."},"topic":"1","answers_community":["C (100%)"],"answer_description":"","question_images":[],"question_id":41},{"id":"CEdPbL5xORbecPYTGQVc","topic":"1","answer_images":[],"answer_ET":"D","exam_id":21,"discussion":[{"upvote_count":"2","poster":"AgboolaKun","comment_id":"1308877","timestamp":"1731087780.0","content":"Selected Answer: D\nThe correct answer is D. \n\nHere is why:\n\nThe Lambda function is configured to access the S3 bucket: The data engineer has already set up the Lambda function's execution role to access the S3 bucket. This means that basic S3 access permissions are likely in place.\n\nThe object is encrypted with a KMS key: This is a crucial detail. When an object in S3 is encrypted with a KMS key, any entity trying to read that object needs two sets of permissions: a. Permission to access the S3 bucket and object b. Permission to use the specific KMS key for decryption\n\nThe error occurs when trying to retrieve the content: This suggests that the Lambda function can likely see the object (as it has S3 access) but fails when trying to read its contents.\n\nTo resolve this issue, the data engineer should grant the Lambda function's execution role the required KMS permissions. Specifically, add the 'kms:Decrypt' permission for the KMS key used to encrypt the S3 object."},{"upvote_count":"1","comment_id":"1265759","content":"Selected Answer: D\nAnswer is D","timestamp":"1723639740.0","poster":"aragon_saa"},{"timestamp":"1723637160.0","content":"Selected Answer: D\nOption D: The Lambda function’s execution role does not have the necessary permissions to access the KMS key that can decrypt the S3 object.","comment_id":"1265694","upvote_count":"1","poster":"matt200"}],"url":"https://www.examtopics.com/discussions/amazon/view/145725-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A data engineer set up an AWS Lambda function to read an object that is stored in an Amazon S3 bucket. The object is encrypted by an AWS KMS key.\n\nThe data engineer configured the Lambda function’s execution role to access the S3 bucket. However, the Lambda function encountered an error and failed to retrieve the content of the object.\n\nWhat is the likely cause of the error?","choices":{"A":"The data engineer misconfigured the permissions of the S3 bucket. The Lambda function could not access the object.","B":"The Lambda function is using an outdated SDK version, which caused the read failure.","D":"The Lambda function’s execution role does not have the necessary permissions to access the KMS key that can decrypt the S3 object.","C":"The S3 bucket is located in a different AWS Region than the Region where the data engineer works. Latency issues caused the Lambda function to encounter an error."},"question_images":[],"timestamp":"2024-08-14 14:06:00","unix_timestamp":1723637160,"isMC":true,"answer_description":"","question_id":42,"answers_community":["D (100%)"],"answer":"D"},{"id":"WqruLrifFwoPia9DAT15","exam_id":21,"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/145726-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true,"choices":{"C":"Create an Amazon EMR cluster. Run a pipeline on Amazon EMR that edits the rules for each Data Catalog table. Use an AWS Lambda function to run the EMR pipeline.","D":"Use the AWS Management Console to edit the rules within the Data Catalog.","A":"Create a pipeline in AWS Glue ETL to edit the rules for each of the 1,000 Data Catalog tables. Use an AWS Lambda function to call the corresponding AWS Glue job for each Data Catalog table.","B":"Create an AWS Lambda function that makes an API call to AWS Glue Data Quality to make the edits."},"answer_ET":"B","timestamp":"2024-08-14 14:07:00","unix_timestamp":1723637220,"question_id":43,"answer_description":"","answer_images":[],"question_text":"A data engineer has implemented data quality rules in 1,000 AWS Glue Data Catalog tables. Because of a recent change in business requirements, the data engineer must edit the data quality rules.\n\nHow should the data engineer meet this requirement with the LEAST operational overhead?","discussion":[{"timestamp":"1723639800.0","upvote_count":"1","content":"Selected Answer: B\nAnswer is B","comment_id":"1265760","poster":"aragon_saa"},{"upvote_count":"1","comment_id":"1265697","timestamp":"1723637220.0","poster":"matt200","content":"Selected Answer: B\nOption B: Create an AWS Lambda function that makes an API call to AWS Glue Data Quality to make the edits."}],"answers_community":["B (100%)"],"question_images":[],"topic":"1"},{"id":"6aLpnSlk1kMn3iIUNBsP","answer_images":[],"unix_timestamp":1723637280,"answers_community":["C (89%)","11%"],"discussion":[{"content":"Selected Answer: B\nB\nIt's considered the general default option for a few reasons\n(1) it includes git fetch and\n(2) you won't get a merge conflict if there happens to be a branch C spawned off after A was merged, you won't need to use --force\n(3) related to (2) you're less likely to be clobbering commit history","poster":"simon2133","timestamp":"1740817140.0","upvote_count":"1","comment_id":"1363449"},{"comment_id":"1308876","upvote_count":"4","poster":"AgboolaKun","timestamp":"1731087720.0","content":"Selected Answer: C\nThe correct answer is C.\n\nHere is why:\n\nRebasing Branch B onto the updated master branch ensures that Branch B incorporates all the recent changes from the master branch (including the changes from Branch A that were deployed to production).\n\nIt helps maintain a linear, clean history by placing Branch B's commits on top of the latest master branch commits.\n\nThis approach reduces the likelihood of merge conflicts when the pull request is eventually merged into master.\n\nIt makes the code review process easier as all the changes in the pull request will be relevant and up-to-date.\n\nBy using git rebase master, the developer ensures that Branch B is up-to-date with all changes in the master branch, including those from Branch A, before creating the pull request. This approach helps maintain a clean, linear history and reduces the likelihood of conflicts during the merge process."},{"poster":"mzansikiller","comment_id":"1268216","content":"Rebasing\nIn Git, there are two main ways to integrate changes from one branch into another: the merge and the rebase. In this section you’ll learn what rebasing is, how to do it, why it’s a pretty amazing tool, and in what cases you won’t want to use it.\n\nThe Basic Rebase\nIf you go back to an earlier example from Basic Merging, you can see that you diverged your work and made commits on two different branches.\n\nAnswer C","timestamp":"1724015340.0","upvote_count":"3"},{"comment_id":"1265761","upvote_count":"2","poster":"aragon_saa","timestamp":"1723639800.0","content":"Selected Answer: C\nAnswer is C"},{"comment_id":"1265699","content":"Selected Answer: C\nOption C: git rebase maste","timestamp":"1723637280.0","poster":"matt200","upvote_count":"2"}],"choices":{"A":"git diff branchB master\ngit commit -m","C":"git rebase master","B":"git pull master","D":"git fetch -b master"},"question_text":"Two developers are working on separate application releases. The developers have created feature branches named Branch A and Branch B by using a GitHub repository’s master branch as the source.\n\nThe developer for Branch A deployed code to the production system. The code for Branch B will merge into a master branch in the following week’s scheduled application release.\n\nWhich command should the developer for Branch B run before the developer raises a pull request to the master branch?","question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/145728-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"C","isMC":true,"exam_id":21,"topic":"1","timestamp":"2024-08-14 14:08:00","answer_ET":"C","question_id":44},{"id":"JjReFmb3fkL3YgBB8fPk","question_id":45,"discussion":[{"poster":"teo2157","timestamp":"1723525680.0","comment_id":"1264994","upvote_count":"7","content":"Selected Answer: AB\nTo maximize the speed of queries using a compound sort key in Amazon Redshift, you should structure your queries to take advantage of the order of the columns in the sort key. The most efficient queries will filter or join on the columns in the same order as the sort key. Saying that, the most efficient queries would be:\nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1' \n AND Department_ID = 'dept1' \n AND Role_ID = 'role1';\nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1' \n AND Department_ID = 'dept1'; \nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1';"},{"poster":"antun3ra","timestamp":"1723134540.0","comment_id":"1262608","upvote_count":"6","content":"Selected Answer: BE\nTo maximize the speed of queries by using the compound sort key (Region ID, Department ID, and Role ID) in the Employee table in Amazon Redshift, the queries should align with the order of the columns in the sort key."},{"poster":"minhhnh","upvote_count":"2","timestamp":"1736054940.0","comment_id":"1336645","content":"Selected Answer: BC\nThe filter order in the query is irrelevant to the performance because the sort key itself determines the storage order. So the execution plan is the same"},{"poster":"HagarTheHorrible","upvote_count":"1","comment_id":"1328337","timestamp":"1734509580.0","content":"Selected Answer: AB\nE is not optimal bc of skipping of the second column."},{"content":"Selected Answer: BC\nThe execution plan of these 2 queries should be the same.","poster":"altonh","comment_id":"1324293","upvote_count":"2","timestamp":"1733792340.0"},{"upvote_count":"2","comment_id":"1318213","content":"Selected Answer: BC\nsort key works best with the first column in the sort key and continuing in sequential order","poster":"RockyLeon","timestamp":"1732642080.0"},{"comment_id":"1312623","content":"Selected Answer: AB\nThe order is the key to speed up queries","poster":"michele_scar","upvote_count":"2","timestamp":"1731674580.0"},{"content":"Answer:AB\nA:This query filters by Region ID, which is the first column in the compound sort key. Queries filtering on the leading sort key column(s) will benefit from optimized performance because the data can be quickly located.\n B:\n\nThis query filters by both Region ID (the first column) and Department ID (the second column) in the sort key. This further narrows down the search space, leading to even faster query performance.","poster":"Parandhaman_Margan","comment_id":"1303470","upvote_count":"4","timestamp":"1730000940.0"},{"upvote_count":"4","timestamp":"1729897440.0","comment_id":"1303034","content":"Selected Answer: BC\nI would vote for B and C. I've tested with a compound sort key (3 columns) and even inverting predicate order the explain plan was the same.","poster":"tucobbad"},{"timestamp":"1722924420.0","content":"Selected Answer: BE\nBased on the order of the compound sort key columns.","poster":"Shanmahi","upvote_count":"5","comment_id":"1261491"}],"unix_timestamp":1722924420,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/145119-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A company stores employee data in Amazon Resdshift. A table names Employee uses columns named Region ID, Department ID, and Role ID as a compound sort key.\n\nWhich queries will MOST increase the speed of query by using a compound sort key of the table? (Choose two.)","answers_community":["BE (35%)","AB (32%)","BC (32%)"],"answer_ET":"BE","isMC":true,"topic":"1","answer":"BE","answer_images":[],"timestamp":"2024-08-06 08:07:00","answer_description":"","choices":{"D":"Select *from Employee where Role ID=50;","E":"Select *from Employee where Region ID=’North America’ and Role ID=50;","C":"Select *from Employee where Department ID=20 and Region ID=’North America’;","B":"Select *from Employee where Region ID=’North America’ and Department ID=20;","A":"Select *from Employee where Region ID=’North America’;"},"exam_id":21}],"exam":{"lastUpdated":"11 Apr 2025","isImplemented":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","isBeta":false,"isMCOnly":true,"provider":"Amazon","numberOfQuestions":207,"id":21},"currentPage":9},"__N_SSP":true}