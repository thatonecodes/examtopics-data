{"pageProps":{"questions":[{"id":"1cRGyR9MnfLBVcXLgR50","question_images":[],"timestamp":"2023-07-21 15:17:00","answers_community":["D (100%)"],"exam_id":34,"question_id":301,"url":"https://www.examtopics.com/discussions/amazon/view/115980-exam-aws-certified-sysops-administrator-associate-topic-1/","answer_ET":"D","choices":{"B":"Install the Amazon CloudWatch agent on the on-premises system. Push the log files to a CloudWatch log group. Create an AWS Lambda function that creates more tapes when the \"Not Enough Space\" error appears. Create a metric filter and a metric alarm that launches the Lambda function.","A":"Create an AWS Lambda function that runs on an hourly basis and checks how many tapes have available space. If the available tapes are below a certain threshold, provision more.","C":"Create an additional Tape Gateway with its own set of tapes. Configure Amazon Simple Notification Service (Amazon SNS) to send a notification to the backup engineer if the tapes that are associated with the primary Tape Gateway do not have available space.","D":"Configure tape auto-create on the Tape Gateway. In the auto-create settings, configure a minimum number of tapes, an appropriate barcode prefix, and a tape pool."},"answer_description":"","question_text":"A company has a large on-premises tape backup solution. The company has started to use AWS Storage Gateway. The company created a Tape Gateway to replace the existing on-premises hardware. The company's backup engineer noticed that some of the backup jobs that were supposed to write to AWS failed to run because of a \"Not Enough Space\" error.\n\nThe company does not want these failures to happen again. The company also wants to consistently have enough tape available on AWS.\n\nWhat is the MOST operationally efficient way for a SysOps administrator to meet these requirements?","answer_images":[],"answer":"D","isMC":true,"topic":"1","unix_timestamp":1689945420,"discussion":[{"poster":"Christina666","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/storagegateway/latest/tgw/managing-automatic-tape-creation.html\n\nThe Tape Gateway automatically creates new virtual tapes to maintain the minimum number of available tapes that you configure. It then makes these new tapes available for import by the backup application so that your backup jobs can run without interruption. Automatic tape creation removes the need for custom scripting in addition to the manual process for creating new virtual tapes.","comments":[{"upvote_count":"2","poster":"Christina666","comment_id":"966034","content":"Virtual tapes are uniquely identified by a barcode, and you can add a prefix to the barcode. The prefix is optional, but you can use it to help identify your virtual tapes. The prefix must be uppercase letters (Aâ€“Z) and must be one to four characters long.","timestamp":"1722224160.0"}],"comment_id":"966033","timestamp":"1722224100.0","upvote_count":"5"},{"content":"Selected Answer: D\nThe Tape Gateway automatically creates new virtual tapes to maintain the minimum number of available tapes that you configure. It then makes these new tapes available for import by the backup application so that your backup jobs can run without interruption. Automatic tape creation removes the need for custom scripting in addition to the manual process for creating new virtual tapes.","poster":"lityann","comment_id":"1067230","timestamp":"1731242100.0","upvote_count":"2"},{"poster":"mh8","comment_id":"960518","upvote_count":"2","content":"Selected Answer: D\nD is the most appropriate. \nRef: https://repost.aws/knowledge-center/s3-empty-bucket-lifecycle-rule","timestamp":"1721745540.0"},{"comment_id":"959318","timestamp":"1721632020.0","content":"Options A, B, and C may address the issue, but they introduce additional complexity and may require more maintenance and monitoring effort. Option D is the simplest and most automated approach to ensure the availability of enough tape storage for backup jobs on AWS Storage Gateway.","upvote_count":"2","poster":"trvtrinh"},{"timestamp":"1721567820.0","comment_id":"958529","upvote_count":"3","content":"Selected Answer: D\nAuto-Create function is more efficient for this.","poster":"jas26says"}]},{"id":"hpKD6sa4VzbIq4TG3sKh","question_id":302,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/115983-exam-aws-certified-sysops-administrator-associate-topic-1/","discussion":[{"content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/\n\nDelete expired delete markers or incomplete multipart uploads under the Lifecycle rule actions section","timestamp":"1722037980.0","comment_id":"964231","upvote_count":"5","poster":"mana25"},{"content":"Selected Answer: A\nA. Create an S3 Lifecycle rule on the S3 bucket to delete expired markers or incomplete multipart uploads.\n\nExplanation:\nS3 Lifecycle rules allow you to define actions that Amazon S3 should take on objects in the bucket over time. This includes transitioning objects between storage classes and deleting objects when they meet certain criteria.\n\nTo reduce the number of incomplete multipart upload objects in the S3 bucket, you can create an S3 Lifecycle rule that targets incomplete multipart uploads and specifies a deletion action for them. This will help in automatically cleaning up the incomplete multipart uploads after a certain period.","comment_id":"966035","poster":"Christina666","comments":[{"comment_id":"966036","upvote_count":"5","content":"Option B, requiring users to use the S3 TransferUtility when performing uploads, does not directly address the existing incomplete multipart uploads. It may help in preventing new incomplete uploads, but it won't clean up the existing ones.\n\nOption C, enabling S3 Versioning, won't automatically delete incomplete multipart uploads. Versioning helps you keep multiple versions of objects in S3, but it won't handle the cleanup of incomplete uploads.\n\nOption D, creating an S3 Object Lambda Access Point to delete incomplete multipart uploads, is not a valid solution. S3 Object Lambda Access Points provide a way to modify or transform data as it is being retrieved from an S3 bucket, but it is not designed for deleting objects.","timestamp":"1722224340.0","poster":"Christina666"}],"timestamp":"1722224340.0","upvote_count":"5"},{"timestamp":"1723684680.0","upvote_count":"2","poster":"nizammusasoac02","content":"Selected Answer: A\ndelete expired markers","comment_id":"981202"},{"timestamp":"1721650260.0","comment_id":"959476","poster":"trvtrinh","content":"A is true","upvote_count":"2"},{"upvote_count":"3","timestamp":"1721568000.0","comment_id":"958534","poster":"jas26says","content":"Selected Answer: A\nLifecycle rules can delete incomplete multipart upload files."}],"exam_id":34,"answer_ET":"A","answer_images":[],"timestamp":"2023-07-21 15:20:00","choices":{"A":"Create an S3 Lifecycle rule on the S3 bucket to delete expired markers or incomplete multipart uploads.","B":"Require users that perform uploads of files into Amazon S3 to use the S3 TransferUtility.","C":"Enable S3 Versioning on the S3 bucket that contains the incomplete multipart uploads.","D":"Create an S3 Object Lambda Access Point to delete incomplete multipart uploads."},"question_text":"A SysOps administrator manages a company's Amazon S3 buckets. The SysOps administrator has identified 5 GB of incomplete multipart uploads in an S3 bucket in the company's AWS account. The SysOps administrator needs to reduce the number of incomplete multipart upload objects in the S3 bucket.\n\nWhich solution will meet this requirement?","unix_timestamp":1689945600,"question_images":[],"isMC":true,"answer":"A","answers_community":["A (100%)"],"topic":"1"},{"id":"GYeAIAm9iwIDQslH41Tl","question_images":[],"answer_ET":"B","answer_description":"","isMC":true,"answers_community":["B (92%)","8%"],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/116075-exam-aws-certified-sysops-administrator-associate-topic-1/","choices":{"C":"","A":"","B":"","D":""},"exam_id":34,"question_text":"A SysOps administrator needs to create an Amazon S3 bucket as a resource in an AWS CloudFormation template. The bucket name must be randomly generated, and the bucket must be encrypted. Other resources in the template will reference the bucket.\n\nWhich CloudFormation resource definition should the SysOps administrator use to meet these requirements?","timestamp":"2023-07-22 14:40:00","answer_images":[],"discussion":[{"comment_id":"1225042","timestamp":"1733446020.0","upvote_count":"1","content":"Selected Answer: B\nI would go for B","poster":"Student013657"},{"timestamp":"1723184280.0","comment_id":"1145324","content":"D is wrong because null values on the property is not allowed. Hence B is correct.","poster":"icecool36","upvote_count":"2"},{"poster":"r2c3po","comments":[{"timestamp":"1719649560.0","content":"Make sure to adjust the bucket name or other properties based on your specific requirements.","comment_id":"1108579","upvote_count":"1","poster":"r2c3po"},{"poster":"null0xAF","upvote_count":"1","timestamp":"1722797460.0","comment_id":"1140522","content":"lol How did you pick A and give explanation that uses Sub to insert a unique name. . . Answer A literally states a Bucket name of \"DOC-EXAMPLE-BUCKET\" this is a static name. . . When you want a unique randomly generated name, which you get if you don't specify the bucketname... your explanation makes 0 sense."}],"timestamp":"1719649320.0","upvote_count":"1","comment_id":"1108576","content":"Selected Answer: A\nResources:\n MyS3Bucket:\n Type: AWS::S3::Bucket\n Properties:\n BucketName: !Sub \"my-s3-bucket-${AWS::AccountId}-${AWS::Region}-${AWS::StackName}-${AWS::StackId}\"\n AccessControl: Private # or the desired access control policy\n Encryption:\n ServerSideEncryptionConfiguration:\n - ServerSideEncryptionByDefault:\n SSEAlgorithm: AES256 # or any other encryption algorithm"},{"content":"Selected Answer: B\nI would go for B, for exam purpose, but as already said by ahrentom from beginning 2023 default encryption is enable by default:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html","upvote_count":"2","timestamp":"1711981080.0","comment_id":"1022325","poster":"TwinSpark"},{"upvote_count":"1","comment_id":"1012236","content":"I would go with D, because the default Bucket encrytion is always enabled with SSE-S3.\nIÂ´ve checked this settings in my account.\n\"BucketEncryption\nSpecifies default encryption for a bucket using server-side encryption with Amazon S3-managed keys (SSE-S3), AWS KMS-managed keys (SSE-KMS), or dual-layer server-side encryption with KMS-managed keys (DSSE-KMS). \"\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html","poster":"ahrentom","timestamp":"1710940080.0"},{"content":"Selected Answer: B\nB since it doesnt specify NAME","comment_id":"961709","timestamp":"1706114040.0","upvote_count":"3","poster":"mana25"},{"poster":"mh8","content":"Selected Answer: B\nB is the answer as it doesnt mention any name.","timestamp":"1706028120.0","comment_id":"960523","upvote_count":"3"},{"timestamp":"1706006520.0","comment_id":"960252","upvote_count":"2","content":"Selected Answer: B\nvoting for B","poster":"alexleely"},{"timestamp":"1705976520.0","comment_id":"959926","content":"Should be \"B\" is correct answer\n\nRandom name is the key. \"A\" and \"C\" is specified S3 name, so both is not correct. \"D\" is also not because no encryption policy.","upvote_count":"3","poster":"VeeAWS"},{"poster":"Zotarix","upvote_count":"2","timestamp":"1705964220.0","comment_id":"959825","comments":[{"comment_id":"973554","upvote_count":"1","timestamp":"1707205440.0","content":"BucketName:\nA name for the bucket. If you don't specify a name, AWS CloudFormation generates a unique ID and uses that ID for the bucket name.\nSource: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html","poster":"[Removed]"}],"content":"In C and A the bucket named DOC-EXAMPLE-BUCKET is created and it's wrong because bucket name only allows lowercase letters.\n\"A name for the bucket. If you don't specify a name, AWS CloudFormation generates a unique ID and uses that ID for the bucket name.\" in D the bucket created has a random name but encryption is not configured.\nThe correct answer is B due that no contains the BucketName parameter and the encryption is configured.\nSource: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html"},{"upvote_count":"2","content":"Option A specifies a static bucket name \"DOC-EXAMPLE-BUCKET\", but the requirement is for a randomly generated bucket name.\nOption C specifies only the bucket name, but it doesn't include encryption settings, and the requirement is for the bucket to be encrypted.\nOption D is an incomplete definition; it doesn't specify any properties, including the bucket name and encryption settings.\nOption B is the correct choice as it creates an S3 bucket with a randomly generated name and enables server-side encryption with AES256. The randomly generated bucket name is handled by AWS CloudFormation during the stack creation, and the bucket name will be unique for each stack.","comment_id":"959500","poster":"trvtrinh","timestamp":"1705934400.0"}],"answer":"B","unix_timestamp":1690029600,"question_id":303},{"id":"dCckTVQzDV0qKoZsLqcq","question_text":"A SysOps administrator manages policies for many AWS member accounts in an AWS Organizations structure. Administrators on other teams have access to the account root user credentials of the member accounts. The SysOps administrator must prevent all teams, including their administrators, from using Amazon DynamoDB. The solution must not affect the ability of the teams to access other AWS services.\n\nWhich solution will meet these requirements?","choices":{"B":"Create a service control policy (SCP) in the management account to deny all DynamoDB actions. Apply the SCP to the root of the organization","A":"In all member accounts, configure IAM policies that deny access to all DynamoDB resources for all users, including the root user.","D":"Remove the default service control policy (SCP) in the management account. Create a replacement SCP that includes a single statement that denies all DynamoDB actions.","C":"In all member accounts, configure IAM policies that deny AmazonDynamoDBFullAccess to all users, including the root user."},"topic":"1","timestamp":"2023-07-21 15:30:00","answer_description":"","question_images":[],"discussion":[{"poster":"eboehm","content":"Selected Answer: B\nThe answer is B as you have no idea what other SCP policies could be in place and deleting the entire SCP would be bad practice.","upvote_count":"10","comment_id":"958893","timestamp":"1705875600.0"},{"timestamp":"1706507040.0","poster":"Christina666","comment_id":"966039","upvote_count":"5","content":"Selected Answer: B\nService Control Policies (SCPs) are a feature of AWS Organizations that allow you to set permissions across all member accounts in the organization. When you apply an SCP at the root of the organization, it affects all member accounts within that organization.\n\nIn this scenario, by creating an SCP that denies all DynamoDB actions and applying it to the root of the AWS organization, you effectively block access to Amazon DynamoDB for all users, including the root user, in all member accounts within the organization. This solution prevents any team, including their administrators, from using DynamoDB while still allowing access to other AWS services that are not restricted by the SCP."},{"timestamp":"1730992380.0","content":"Selected Answer: B\nI agree with the explanation of many folks here why B is the correct answer. One thing we must understand is that AWS Organizations attaches an AWS managed SCP named FullAWSAccess to every root, OU and account when it's created. This policy allows all services and actions. Therefore, attaching a new SCP that denies all DynamoDB actions to the root of the organizations makes a lot more sense. This is because we don't care what access other SCPs have granted the Management accounts and OUs, this new SCP explicitly denies DynamoDB related actions. We are good since explicit \"deny\" overrides explicit \"allow\". Please refer to this documentation - https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html for more information","comment_id":"1207886","upvote_count":"1","poster":"AgboolaKun"},{"poster":"trvtrinh","upvote_count":"3","content":"Answer: B\nService Control Policies (SCPs) are used to manage permissions for all members of an AWS Organization. SCPs allow you to set permissions that restrict actions across the entire organization or specific organizational units (OUs).\n\nBy creating an SCP that denies all DynamoDB actions and applying it at the root level of the AWS Organization, you can prevent all member accounts, including their administrators with root user credentials, from using Amazon DynamoDB. This solution will not affect the ability of the teams to access other AWS services, as SCPs are used to control permissions for specific services or actions.","comment_id":"959516","timestamp":"1705935060.0"},{"comments":[{"timestamp":"1705935000.0","comment_id":"959515","upvote_count":"3","poster":"trvtrinh","content":"I think B is true"}],"poster":"jas26says","content":"IÂ´m not sure, but for me itÂ´s something between B and D, but not C.","timestamp":"1705851000.0","comment_id":"958551","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/amazon/view/115989-exam-aws-certified-sysops-administrator-associate-topic-1/","question_id":304,"answer_ET":"B","answers_community":["B (100%)"],"answer":"B","isMC":true,"exam_id":34,"answer_images":[],"unix_timestamp":1689946200},{"id":"3BKtydecAU2LU4gFWe57","question_text":"A company has users that deploy Amazon EC2 instances that have more disk performance capacity than is required. A SysOps administrator needs to review all Amazon Elastic Block Store (Amazon EBS) volumes that are associated with the instances and create cost optimization recommendations based on IOPS and throughput.\n\nWhat should the SysOps administrator do to meet these requirements in the MOST operationally efficient way?","topic":"1","choices":{"B":"Stop the EC2 instances from the EC2 console. Change the EC2 instance type for Amazon EBS-optimized. Start the EC2 instances.","C":"Opt in to AWS Compute Optimizer. Allow sufficient time for metrics to be gathered. Review the Compute Optimizer findings for EBS volumes.","A":"Use the monitoring graphs in the EC2 console to view metrics for EBS volumes. Review the consumed space against the provisioned space on each volume. Identify any volumes that have low utilization.","D":"Install the fio tool onto the EC2 instances and create a .cfg file to approximate the required workloads. Use the benchmark results to gauge whether the provisioned EBS volumes are of the most appropriate type."},"timestamp":"2023-07-21 15:31:00","answer_description":"","question_images":[],"discussion":[{"comment_id":"958552","timestamp":"1721568660.0","content":"Selected Answer: C\nThe answer heres itÂ´s kind of obvious. Compute Optimizer all the way.","poster":"jas26says","upvote_count":"5"},{"upvote_count":"3","comment_id":"966041","timestamp":"1722224760.0","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/storage/cost-optimizing-amazon-ebs-volumes-using-aws-compute-optimizer/","comments":[{"content":"To help you right-size your EBS volumes, AWS Compute Optimizer analyzes your AWS resource utilization data collected by Amazon CloudWatch from the past 14 days (paid option available for up to three months of data) to identify the optimal resource configuration. These recommendations are available for your Amazon Elastic Compute Cloud (EC2) instances, EBS volumes, AWS Auto Scaling groups, AWS Lambda functions, and AWS Fargate.","poster":"Christina666","upvote_count":"5","timestamp":"1722224760.0","comment_id":"966042"}],"poster":"Christina666"},{"upvote_count":"3","timestamp":"1721680860.0","comment_id":"959818","content":"A and D are no operationally efficient because the admin needs to invert time on it. B are the steps to get the issue resolved. Hence, C is the correct answer.","poster":"Zotarix"},{"timestamp":"1721652900.0","poster":"trvtrinh","comment_id":"959520","content":"C is correct","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/amazon/view/115990-exam-aws-certified-sysops-administrator-associate-topic-1/","question_id":305,"answer_ET":"C","answer":"C","answers_community":["C (100%)"],"isMC":true,"answer_images":[],"exam_id":34,"unix_timestamp":1689946260}],"exam":{"isMCOnly":false,"numberOfQuestions":477,"lastUpdated":"11 Apr 2025","name":"AWS Certified SysOps Administrator - Associate","isImplemented":true,"id":34,"isBeta":false,"provider":"Amazon"},"currentPage":61},"__N_SSP":true}