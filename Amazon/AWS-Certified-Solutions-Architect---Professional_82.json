{"pageProps":{"questions":[{"id":"ovmbMlML1FquUY2UAn35","answer_description":"","unix_timestamp":1569263460,"isMC":true,"question_text":"A company has asked a Solutions Architect to design a secure content management solution that can be accessed by API calls by external customer applications.\nThe company requires that a customer administrator must be able to submit an API call and roll back changes to existing files sent to the content management solution, as needed.\nWhat is the MOST secure deployment design that meets all solution requirements?","question_images":[],"exam_id":32,"answers_community":["A (53%)","B (47%)"],"choices":{"D":"Use Amazon S3 for object storage with versioning and enable S3 bucket access logging. Use an IAM role and access policy for each customer application. Encrypt objects using client-side encryption, and distribute an encryption key to all customers when accessing the content management application.","C":"Use Amazon EFS for object storage, using encryption at rest for the Amazon EFS volume and a customer managed key stored in AWS KMS. Use IAM roles and Amazon EFS access policies to specify separate encryption keys for each customer application. Deploy the content management application to store all new versions as new files in Amazon EFS and use a control API to revert a specific file to a previous version.","B":"Use Amazon WorkDocs for object storage. Leverage WorkDocs encryption, user access management, and version control. Use AWS CloudTrail to log all SDK actions and create reports of hourly access by using the Amazon CloudWatch dashboard. Enable a revert function in the SDK based on a static Amazon S3 webpage that shows the output of the CloudWatch dashboard.","A":"Use Amazon S3 for object storage with versioning and bucket access logging enabled, and an IAM role and access policy for each customer application. Encrypt objects using SSE-KMS. Develop the content management application to use a separate AWS KMS key for each customer."},"timestamp":"2019-09-23 20:31:00","answer_images":[],"discussion":[{"comments":[{"comment_id":"16167","timestamp":"1632469620.0","content":"WorkkDocs is not for Object Storage I think","upvote_count":"6","poster":"chaudh"},{"content":"A is the right answer. You are not considering the rollback requirement which is satisfied by S3 versioning.","poster":"shammous","upvote_count":"1","comment_id":"279911","timestamp":"1634955540.0"},{"comment_id":"390252","poster":"Kopa","comments":[{"comment_id":"445806","comments":[{"timestamp":"1636050540.0","comment_id":"445807","content":"my bad the question says it should be done by using API calls","upvote_count":"2","poster":"rlandire"}],"poster":"rlandire","upvote_count":"2","timestamp":"1635886800.0","content":"it looks possible \nhttps://docs.aws.amazon.com/workdocs/latest/userguide/revert-version.html"}],"timestamp":"1635639720.0","content":"How can WorkDocs roll back the docs?","upvote_count":"1"},{"upvote_count":"16","timestamp":"1632940140.0","comment_id":"50044","content":"B sounds good until you get to the last sentance which to me just sounds like nonesense. Use Cloudtrail to log SDK actions to create hourly reports, and use an S3 webpage to enable the revert function in the SDK? Just sounds like a bunch of wrong stuff put in to disqualify this answer.","poster":"AWSPro24"},{"content":"There doesn't appear to be an API available in Workdocs to perform roll back of changes. https://docs.aws.amazon.com/workdocs/latest/APIReference/Welcome.html","poster":"sb333","comments":[{"poster":"AWSPro24","timestamp":"1632922380.0","comment_id":"50040","content":"Check page 116 https://docs.aws.amazon.com/workdocs/latest/APIReference/workdocs-api.pdf . UpdateDocumentVersion Method should do the trick. I'm not a developer but this looks possible.","upvote_count":"5"}],"upvote_count":"3","comment_id":"46386","timestamp":"1632734820.0"}],"upvote_count":"25","poster":"donathon","timestamp":"1632253200.0","content":"B\nAmazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it’s stored centrally on AWS, access it from anywhere on any device. Amazon WorkDocs makes it easy to collaborate with others, and lets you easily share content, provide rich feedback, and collaboratively edit documents. You can use Amazon WorkDocs to retire legacy file share infrastructure by moving file shares to the cloud. Amazon WorkDocs lets you integrate with your existing systems, and offers a rich API so that you can develop your own content-rich applications.\nhttps://aws.amazon.com/workdocs/","comment_id":"13207"},{"comment_id":"12328","timestamp":"1632116220.0","content":"A is my view","poster":"awsec2","upvote_count":"20"},{"content":"Selected Answer: B\nsimilar question in TutorialsDojo , is posible Amazon WorkDocs","timestamp":"1688494560.0","comment_id":"943069","poster":"SkyZeroZx","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nA sounds good version funtion with rollback and KMS for encript","poster":"SkyZeroZx","comment_id":"926263","timestamp":"1687033740.0"},{"timestamp":"1672156980.0","upvote_count":"1","comment_id":"758756","poster":"evargasbrz","content":"Selected Answer: A\nA is the right answer."},{"upvote_count":"5","poster":"Lorrendo","comment_id":"716787","content":"Selected Answer: A\nSSE-KMS = One KMS key per customer (most SECURE)\nRollback = S3 versioning\nAgainst B: 1) Default encryption for all customers (not MOST secuire) 2) The last part of the description sounds made up","timestamp":"1668267600.0"},{"comment_id":"716783","content":"It's A; keyword is MOST SECURE, and with A you have one key per user, in B this is not specified","poster":"Lorrendo","timestamp":"1668267060.0","upvote_count":"2"},{"timestamp":"1667342580.0","content":"Selected Answer: A\ncan not be B - work docs is not content management solution (CMS). work doc is to share documents with other. \nA supports versioning","poster":"AjayPrajapati","comment_id":"709460","upvote_count":"2"},{"upvote_count":"3","content":"The answer is A. B is 100pc incorrect people, trust me. Workdocs is not for object storage. S3 with versioning can be used as CMS. A is 1000pc correct.","poster":"mrgreatness","comment_id":"704144","timestamp":"1666727760.0"},{"comment_id":"685660","poster":"dmscountera","timestamp":"1664813760.0","content":"Selected Answer: B\nBased on all comments","upvote_count":"3"},{"timestamp":"1663128540.0","content":"B - WorkDocs is ready built CMS with API capabilities and easy rollback","comment_id":"668589","upvote_count":"1","poster":"zaypaak"},{"timestamp":"1661957340.0","poster":"Sizuma","comment_id":"655206","upvote_count":"1","content":"A RIGHT"},{"poster":"ssSsEclipse","comment_id":"606116","upvote_count":"1","content":"Selected Answer: A\nA is the best option here","timestamp":"1653312300.0"},{"timestamp":"1645334820.0","poster":"peddyua","comment_id":"551548","upvote_count":"5","content":"Selected Answer: B\nSeems like it's one of those AWS questions where they promote their services, why do you need to develop an application from scratch when they already have a service for it, and it's AWS WorkDocs"},{"timestamp":"1642943100.0","content":"\"A business has hired a Solutions Architect to create a secure content management system that can be accessed through API calls from external customer apps.\" - The first sentence of the question. Check out https://aws.amazon.com/workdocs/?amazon-workdocs-whats-new.sort-by=item.additionalFields.postDateTime&amazon-workdocs-whats-new.sort-order=desc.\n\"Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service.\"\n\"Amazon WorkDocs lets you integrate with your existing systems, and offers a rich API so that you can develop your own content-rich applications.\"\n\"“We started using Amazon WorkDocs to securely store and share files such as queries, datasets, and reports. The ability to view historical versions and \"... I think its B given this. Also most questions typically are looking for us to sell the AWS way of doing things with whatever shiny service they are currently peddling.","comment_id":"530544","upvote_count":"1","poster":"lulz111"},{"upvote_count":"2","content":"Doesn't SSE-KMS generally mean Default AWS Managed Key. SSE-KMS CMK is generally used term to specify customer specific keys???","poster":"vbal","timestamp":"1638748980.0","comment_id":"494744"},{"upvote_count":"1","content":"A right choice","timestamp":"1638638580.0","poster":"AzureDP900","comment_id":"493847"},{"comment_id":"454418","content":"A \n---","timestamp":"1636290960.0","upvote_count":"1","poster":"student22"},{"timestamp":"1636202940.0","comment_id":"450737","content":"It's A","upvote_count":"1","poster":"andylogan"},{"content":"As much as I want to agree that the answer is B, I'm leaning towards A \n\nWorkdocs is content storage. Yes the question is about content management, but based on the answer s provided, it talks about OBJECT storage. So thay being said I will say the answer is \n\nA","timestamp":"1635770580.0","poster":"AWSum1","upvote_count":"2","comment_id":"445413"},{"poster":"Kopa","comment_id":"427993","timestamp":"1635762720.0","upvote_count":"1","content":"Im going with A"},{"upvote_count":"2","content":"I'll go with A","timestamp":"1635740520.0","comment_id":"409558","poster":"WhyIronMan"},{"timestamp":"1635696600.0","upvote_count":"1","poster":"Akhil254","comment_id":"406656","content":"A Correct"},{"upvote_count":"1","content":"The Answer is B, Amazon workdocs is a content management tool and can accomplish all the requirements, https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_constructing.html","comment_id":"368711","timestamp":"1635630840.0","poster":"zolthar_z"},{"content":"A is the answer","upvote_count":"3","timestamp":"1635584520.0","comment_id":"344273","poster":"Waiweng"},{"comments":[{"timestamp":"1635547980.0","poster":"ExtHo","upvote_count":"1","content":"Yes its there :) Use Amazon WorkDocs for document storage and utilize its user access management, version control, and built-in encryption. Integrate the Amazon WorkDocs Content Manager to the external custom applications. Develop a rollback feature to replace the current document version with the previous version from Amazon WorkDocs.","comment_id":"323019"}],"timestamp":"1635518940.0","content":"B\n---\nWorkDocs supports API and, the same question is on Jon Bonso's Mock Exam.","comment_id":"294779","upvote_count":"3","poster":"gpark"},{"poster":"Kian1","upvote_count":"3","comment_id":"289574","content":"I am going with A","timestamp":"1635494760.0"},{"poster":"Ebi","comments":[{"poster":"Ebi","upvote_count":"5","comment_id":"287998","timestamp":"1635186900.0","content":"Changing my answer to A as in this option also there is one KMS key per client"},{"content":"https://docs.aws.amazon.com/workdocs/latest/APIReference/Welcome.html","timestamp":"1645334700.0","upvote_count":"2","comment_id":"551546","poster":"peddyua"}],"timestamp":"1635090000.0","content":"B is not the answer, WorkDoc does not support any API to upload or change the document:\nhttps://docs.aws.amazon.com/workdocs/latest/APIReference/API_UpdateDocument.html\n\nBetween A,D I go with D:\nOption A causes all contents by all accounts encrypted using same key.\nOption D, content of each account will have separate encryption key. KMS still can be used for client side encryption, different KMS keys will be created for each account in the bucket owner's account with proper access to each key per account, they keys are shared with customer's account\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html","upvote_count":"3","comment_id":"283158"},{"poster":"gookseang","comment_id":"275065","upvote_count":"1","content":"A for sure","timestamp":"1634939040.0"},{"poster":"T14102020","upvote_count":"1","content":"Correct answer is A.","comment_id":"242428","timestamp":"1634824980.0"},{"timestamp":"1634811120.0","upvote_count":"3","comment_id":"229442","content":"I'll go with A","poster":"jackdryan"},{"timestamp":"1634792340.0","comment_id":"228792","poster":"Bulti","upvote_count":"3","content":"The correct answer is A. C is not a best practice, D is incorrect since in Customer managed CMK, the customer is expected to provide the encryption key and not be supplied one from AWS. B is a bit vague when it comes to the last sentence around enabling revert function based on the static web page hosted on S3 that contains CloudWatch logs accumulated from CloudTrail. Therefore the correct answer is A which is very secured and meets all the requirements."},{"poster":"lostri","upvote_count":"1","content":"Answer is A","timestamp":"1634775540.0","comment_id":"226673"},{"timestamp":"1634773080.0","content":"I would go with B. AWS Workdocs is a brand new content management solution - I think AWS would want us to select that option","upvote_count":"2","poster":"sk2022","comment_id":"185683"},{"timestamp":"1634772180.0","poster":"ipindado2020","comment_id":"182647","upvote_count":"2","content":"I go for B...\n\nWith A option security design seems to be quite good, but \"Develop the content management application to use a separate AWS KMS key for each customer\" means you have to develop the full content managemet solution on the top of s3...\n\nWith B content management solution is already developed (fully managed) with security already integrated,and just develop a simple page."},{"content":"ANS is A, Requirement is Customer Administrator ability to rollback , with client side encryption, administrator cannot rollback","poster":"sam422","comment_id":"181204","timestamp":"1634749920.0","upvote_count":"1"},{"comments":[{"comment_id":"177180","poster":"Phat","upvote_count":"2","timestamp":"1634699040.0","content":"A is correct."}],"upvote_count":"1","timestamp":"1634535420.0","poster":"fullaws","content":"B is correct, A has the limitation of KMS keys creation.","comment_id":"149472"},{"comment_id":"136939","timestamp":"1634334420.0","upvote_count":"1","poster":"noisonnoiton","content":"A,D acceptable,,"},{"content":"A for sure","comment_id":"134248","poster":"NikkyDicky","upvote_count":"1","timestamp":"1634314860.0"},{"comment_id":"132139","timestamp":"1633926720.0","poster":"inf","upvote_count":"6","content":"Answer: A\nA - correct - SSE-KMS ensures all data is encrypted using customer specific keys, rolback is possible using APIs.\nB - incorrect - Workdocs is file storage, not object storage. The question also says \"rollback... as needed\" - possible using an \"hourly\" report? Plus what detail would Cloudwatch Dashboard collect from CloudTrail - we would need specific details of every file/version uploaded - going back days/months/years. Where is the \"revert\" function in the SDK? \nC - incorrect - EFS is file storage - plus would use the KMS key used for all customer data - not very secure.\nD - incorrect - for CSE, customer should own the key thus no need to distribute the key to the clients. Would also rather send a \"decryption\" key if possible."},{"upvote_count":"1","comment_id":"118696","poster":"chicagomassageseeker","content":"Answer is B. Dont let \"object storage\" word fool you. After a lot of digging I found Amazon workdocs indeed use S3 in the background for files storage. \n\"If you use an Amazon WorkDocs client in Amazon WorkSpaces or an EC2 instance, your endpoint policy must allow full access to Amazon S3\"\ncheck the below linkhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html. Hate how AWS words these questions.","comments":[{"content":"Don't under estimate the wording of the question - Amazon clearly state it is a \"file storage\" solution. They never misdirect when it comes to use of the words object, file and block as storage solutions","upvote_count":"3","comment_id":"132146","timestamp":"1634194980.0","poster":"inf"}],"timestamp":"1633875180.0"},{"comment_id":"115194","content":"B is correct. There is limit on the keys generation which is required for every customer in option A.\nIs there a limit to the number of keys I can create in AWS KMS? You can create up to 10000 CMKs per account per region. As both enabled and disabled CMKs count towards the limit, we recommend deleting disabled keys that you no longer use.","timestamp":"1633803120.0","poster":"pgarg","upvote_count":"1"},{"comment_id":"106139","comments":[{"upvote_count":"2","content":"why would you send an encryption key to customers if using client-side encryption? customer already has the key. Thus A","comment_id":"132143","poster":"inf","timestamp":"1634113140.0"}],"timestamp":"1633637040.0","poster":"VrushaliD","upvote_count":"6","content":"in my opinion its D - client side encryption is more secure .."},{"upvote_count":"1","poster":"Ibranthovic","timestamp":"1633604220.0","content":"why using AWS KMS While D is same as A and using client-side encryption ?","comment_id":"93264"},{"poster":"Smart","content":"WorkDocs is an end-user services. Why use it when accessed by external applications?","comment_id":"76420","timestamp":"1633131360.0","upvote_count":"2"},{"upvote_count":"3","comment_id":"57715","poster":"mandrakenet","timestamp":"1632996480.0","content":"to me is A, WorkDocs is not object storage"},{"comment_id":"49546","timestamp":"1632795000.0","content":"I think it's A\nhttps://acloud.guru/forums/aws-csa-pro-2019/discussion/-LlZLvsLr2qekE_gEnGQ/SAP%20sample%20question,%20S3,%20WorkDoc","poster":"amog","upvote_count":"2"},{"timestamp":"1632685620.0","upvote_count":"1","comment_id":"32782","content":"Customer Administrator? SSE-Encryption possible?","poster":"VMHarry"},{"upvote_count":"2","content":"A makes sense here","timestamp":"1632650460.0","poster":"dojo","comment_id":"30488"},{"content":"I think answer is A. For each customer application, there is a role to access data on S3. I think same role could be an owner for KMS key which would be applied to S3","timestamp":"1632613800.0","upvote_count":"2","poster":"sara_an","comment_id":"23618"},{"content":"I support B\nBecause they want 'a secure content management solution'","comment_id":"17330","timestamp":"1632546420.0","poster":"pudak","upvote_count":"3"},{"timestamp":"1632519960.0","poster":"ahmedghanem","comment_id":"16309","upvote_count":"2","content":"A\nVersioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures."},{"content":"I think A makes more sense due to the Versioning and Rolling back.","comments":[{"content":"WorkDocs also provide Versioning and Rolling back.","comment_id":"17588","poster":"TechGuru","upvote_count":"2","timestamp":"1632603360.0"}],"upvote_count":"5","comment_id":"15691","poster":"Marcos","timestamp":"1632375240.0"}],"topic":"1","answer_ET":"A","question_id":406,"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/5619-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"BwYbCzlQCUYnVwEBxEbH","url":"https://www.examtopics.com/discussions/amazon/view/5159-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2019-09-14 13:08:00","answer_description":"Reference:\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-how-to-use-lambdaedge-and-json-web-tokens-to-enhance-web-application- security/","choices":{"D":"Replicate the setup in each geography and use Network Load Balancers to route traffic to the authentication service running in the closest region to users.","C":"Use Amazon Lambda@Edge attached to the CloudFront viewer request trigger to authenticate and authorize users by maintaining a secure cookie token with a session expiry to improve the user experience in multiple geographies.","B":"Use an Amazon Route 53 weighted routing policy to route traffic to the CloudFront distribution. Use CloudFront cached HTTP methods to improve the user login experience.","A":"Replicate the setup in each new geography and use Amazon Route 53 geo-based routing to route traffic to the AWS Region closest to the users."},"exam_id":32,"answer_ET":"C","answer":"C","question_text":"A company has released a new version of a website to target an audience in Asia and South America. The website's media assets are hosted on Amazon S3 and have an Amazon CloudFront distribution to improve end-user performance. However, users are having a poor login experience, the authentication service is only available in the us-east-1 AWS Region.\nHow can the Solutions Architect improve the login experience and maintain high security and performance with minimal management overhead?","discussion":[{"timestamp":"1632609000.0","content":"C\nThere are several benefits to using Lambda@Edge for authorization operations. First, performance is improved by running the authorization function using Lambda@Edge closest to the viewer, reducing latency and response time to the viewer request. The load on your origin servers is also reduced by offloading CPU-intensive operations such as verification of JSON Web Token (JWT) signatures. Finally, there are security benefits such as filtering out unauthorized requests before they reach your origin infrastructure.\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-how-to-use-lambdaedge-and-json-web-tokens-to-enhance-web-application-security/","poster":"donathon","comment_id":"14040","comments":[{"comment_id":"205650","poster":"LeoChu","timestamp":"1634815920.0","upvote_count":"1","content":"very detail explanation, thank you"},{"content":"Another link specifically about using Lambda@Edge with cookies:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/","upvote_count":"1","poster":"kirrim","timestamp":"1636198680.0","comment_id":"455191"}],"upvote_count":"42"},{"comments":[{"timestamp":"1633530180.0","comment_id":"98607","poster":"JAWS1600","content":"https://medium.com/monstar-lab-bangladesh-engineering/configure-basic-authentication-for-cloudfront-using-lambda-edge-c23ce46216d7\nHere is teh document showing how to use lambda edge for authENTICATION","upvote_count":"4"}],"upvote_count":"6","poster":"JWC","content":"To those proposing C, the issue is with user login/authentication, NOT authorization. I agree Lambda@Edge would help with authorization. I'm not sure how it helps with authentication, which would require some type of directory, preferably close to the user, to validate user credentials. A would accomplish this.","timestamp":"1633327440.0","comment_id":"81492"},{"content":"Selected Answer: C\nBased on all comments","comment_id":"685662","upvote_count":"1","timestamp":"1664813940.0","poster":"dmscountera"},{"comment_id":"598055","poster":"tartarus23","timestamp":"1651915980.0","upvote_count":"1","content":"Selected Answer: C\nC. Lambda@Edge enables modifying and servicing requests to and from the CloudFront so that the authorization process is offloaded to it instead of waiting to reach the AWS servers."},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1638638880.0","comment_id":"493850","content":"c is perfect answer"},{"timestamp":"1637410080.0","poster":"SivaDorai76","comment_id":"482502","content":"https://aws.amazon.com/blogs/networking-and-content-delivery/lambdaedge-design-best-practices/ The link has details on Authentication which can be done using Lambda@Edge and more.","upvote_count":"1"},{"upvote_count":"1","comment_id":"450739","timestamp":"1636057860.0","poster":"andylogan","content":"It's C"},{"content":"I'll go with C","poster":"WhyIronMan","comment_id":"409563","upvote_count":"2","timestamp":"1636026120.0"},{"timestamp":"1635850200.0","poster":"Waiweng","comment_id":"344274","upvote_count":"2","content":"it's C"},{"poster":"kiev","content":"Full House says C is the answer. Very clear explanation from Donathon","timestamp":"1635831540.0","comment_id":"293287","upvote_count":"4"},{"timestamp":"1635666840.0","comment_id":"289575","content":"going with C","poster":"Kian1","upvote_count":"1"},{"upvote_count":"4","comment_id":"283163","poster":"Ebi","timestamp":"1635491940.0","content":"C is the answer"},{"timestamp":"1635415740.0","upvote_count":"1","poster":"sanjaym","comment_id":"266581","content":"C for sure."},{"timestamp":"1635342120.0","content":"Correct answer is C. Lambda@Edge","poster":"T14102020","comment_id":"242431","upvote_count":"1"},{"timestamp":"1635044640.0","poster":"jackdryan","comment_id":"229444","upvote_count":"3","content":"I'll go with C"},{"poster":"Bulti","content":"C is the answer. A Lambda@Edge function can also make network calls to external resources to confirm user credentials. Assuming authentication happens only in us-east-1 region, Lamba@Edge can atleast validate the JWT Token signature to determine if the user can directly access the content if the signature is valid. If the JWT token signature is invalid then it can redirect the user to the Authentication service in us-east-1 thereby improving the performance of the system.","comment_id":"228798","timestamp":"1634944980.0","upvote_count":"3"},{"poster":"AlwaysLearning2020","upvote_count":"1","timestamp":"1634810820.0","comment_id":"191489","content":"https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/"},{"poster":"fullaws","timestamp":"1634380500.0","content":"C is correct","upvote_count":"1","comment_id":"149473"},{"timestamp":"1633932000.0","content":"C acceptable","upvote_count":"1","comment_id":"136941","poster":"noisonnoiton"},{"poster":"NikkyDicky","content":"C make sense","timestamp":"1633852260.0","comment_id":"134251","upvote_count":"1"},{"timestamp":"1633309380.0","content":"C: https://aws.amazon.com/fr/blogs/networking-and-content-delivery/dynamically-route-viewer-requests-to-any-origin-using-lambdaedge/","poster":"Asds","upvote_count":"1","comment_id":"63137"},{"content":"Answer is C\nUsing Lambda to support login on Edge","poster":"amog","timestamp":"1633127640.0","upvote_count":"2","comment_id":"49547"},{"poster":"dojo","upvote_count":"1","content":"C is the answer according to me","comment_id":"30489","timestamp":"1632876420.0"},{"poster":"DJTau","timestamp":"1632851040.0","content":"Correct answer C thnx for analysis folks","comment_id":"14894","upvote_count":"1"},{"comment_id":"14229","poster":"Ibranthovic","content":"We know that we should configure Lambda@Edge in us-east-1\nOn another word, they are telling us to choice Lambda@Edge","upvote_count":"3","timestamp":"1632832020.0"},{"upvote_count":"3","content":"I prefer answer \"C\".\nLambda@edge can be used to support authorization and authentication.","comment_id":"14143","timestamp":"1632677100.0","poster":"Moon"},{"timestamp":"1632245940.0","poster":"Lee","content":"I prefer to choose C. https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html","comment_id":"11573","comments":[{"content":"same. c","comment_id":"11624","poster":"dpvnme","timestamp":"1632311400.0","upvote_count":"2"}],"upvote_count":"2"},{"content":"why not c","timestamp":"1632080340.0","upvote_count":"3","comment_id":"11060","poster":"awsec2"}],"question_images":[],"topic":"1","answer_images":[],"question_id":407,"isMC":true,"answers_community":["C (100%)"],"unix_timestamp":1568459280},{"id":"oeHvwP54oXd1MeO6Bgb5","timestamp":"2019-09-14 13:09:00","answer":"D","answer_ET":"D","question_id":408,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/5160-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","choices":{"B":"The fault is in the third-party environment. Contact the third party that provides the maps and request a fix that will provide better uptime.","A":"The network ACL for one subnet is blocking outbound web traffic. Open the network ACL and prevent administration from making future changes through IAM.","D":"One of the NAT instances failed. Recommend replacing the EC2 NAT instances with a NAT gateway.","C":"One NAT instance has become overloaded. Replace both EC2 NAT instances with a larger-sized instance and make sure to account for growth when making the new instance size."},"unix_timestamp":1568459340,"question_images":[],"answer_images":[],"question_text":"A company has a standard three-tier architecture using two Availability Zones. During the company's off season, users report that the website is not working. The\nSolutions Architect finds that no changes have been made to the environment recently, the website is reachable, and it is possible to log in. However, when the\nSolutions Architect selects the `find a store near you` function, the maps provided on the site by a third-party RESTful API call do not work about 50% of the time after refreshing the page. The outbound API calls are made through Amazon EC2 NAT instances.\nWhat is the MOST likely reason for this failure and how can it be mitigated in the future?","isMC":true,"discussion":[{"poster":"donathon","comments":[{"upvote_count":"5","content":"logic for A seems incorrect , it mentions outbound calls(call to restapi) are restricted via NACL which is possible. The correct reason is that there has no changes made in the environment , so A cannot be the option","poster":"JohnyGaddar","comment_id":"97134","timestamp":"1633134780.0","comments":[{"comment_id":"333257","poster":"sarah_t","content":"A failed instance is not a \"change\" though, as changes are (more or less) intentional.","upvote_count":"2","timestamp":"1635793980.0"}]}],"timestamp":"1632445140.0","content":"D\nA: Network ACL is stateless and hence return traffic must be explicitly allowed by rules. If outbound is not allowed then how can the webpage load 100% of the time?\nB: This is not possible since it only fails 50% of the time which means only 1 AZ is affected.\nC: Unlikely to cause exactly 50% failure. API calls should not be load intensive.\nD: Assuming NAT instance HA is not configured. https://aws.amazon.com/articles/high-availability-for-amazon-vpc-nat-instances-an-example/","upvote_count":"34","comment_id":"13209"},{"timestamp":"1632359820.0","poster":"awsec2","content":"why not d","comment_id":"11061","upvote_count":"8"},{"timestamp":"1711764960.0","comment_id":"1185789","poster":"steed47","upvote_count":"1","content":"Answer is D.\nIn real life, I go for B for sure"},{"content":"Selected Answer: D\nIt should be D, but the answer should say NAT Gateways as we need one NAT gateway in each AZ.","timestamp":"1687034100.0","comment_id":"926269","poster":"SkyZeroZx","upvote_count":"1"},{"comment_id":"906452","poster":"AMEJack","upvote_count":"1","timestamp":"1684996980.0","content":"Selected Answer: D\nIt should be D, but the answer should say NAT Gateways as we need one NAT gateway in each AZ."},{"poster":"dmscountera","upvote_count":"1","timestamp":"1664814000.0","comment_id":"685664","content":"Selected Answer: D\nBased on all comments"},{"poster":"Ni_yot","upvote_count":"2","comment_id":"527879","timestamp":"1642622880.0","content":"D for me. the NAT instances are deployed in 2 AZs so when one fails thats 50% of the routing down. So best to replace the NAT with a Gateway. Why not use a GW in the first place anyways"},{"timestamp":"1639194300.0","upvote_count":"1","poster":"challenger1","content":"My Answer: D","comment_id":"499077"},{"timestamp":"1638639240.0","comment_id":"493856","content":"D is perfect . Thanks donathon for detail explanation !","poster":"AzureDP900","upvote_count":"1"},{"comment_id":"450742","poster":"andylogan","timestamp":"1636047240.0","content":"It's D","upvote_count":"1"},{"poster":"WhyIronMan","timestamp":"1635962520.0","comment_id":"409568","upvote_count":"2","content":"I'll go with D"},{"upvote_count":"2","content":"it's D","poster":"Waiweng","comment_id":"344275","timestamp":"1635945960.0"},{"content":"can't be B as question has also indicated as \"off-peak\" season so no way the NAT instance would be overloaded.","comment_id":"309651","timestamp":"1635592320.0","poster":"Pupu86","upvote_count":"2"},{"timestamp":"1635548700.0","comment_id":"293290","content":"D for me is the answer and the replacement with Nat Gatway will solve any elasticity issue.","upvote_count":"1","poster":"kiev"},{"comment_id":"289578","upvote_count":"2","timestamp":"1635023760.0","poster":"Kian1","content":"D is my choice"},{"timestamp":"1635020100.0","poster":"LoganIsh","comment_id":"287523","upvote_count":"2","content":"D is the right choice to replacing NAT gateway"},{"timestamp":"1634918160.0","poster":"Ebi","comment_id":"283168","upvote_count":"3","content":"Definitely D"},{"comment_id":"267416","poster":"sanjaym","upvote_count":"1","timestamp":"1634904900.0","content":"D for sure."},{"content":"D is Correct. NAT Gateway","upvote_count":"1","timestamp":"1634779500.0","comment_id":"242433","poster":"T14102020"},{"comment_id":"238649","poster":"MeepMeep","content":"DDDDDDDD","timestamp":"1634705640.0","upvote_count":"1"},{"timestamp":"1634666640.0","comment_id":"236628","content":"D is definitely good practice but in the context of this question, there have been no changes. This is also off-season for the company so less traffic I'm assuming. B would be my first step.","upvote_count":"1","poster":"nqobza"},{"upvote_count":"3","timestamp":"1634298180.0","content":"I'll go with D","comment_id":"229445","poster":"jackdryan"},{"content":"D is Correct. The best practice is to use the NAT Gateway over NAT Instances. NAT Instances is legacy.","poster":"Bulti","upvote_count":"2","comment_id":"228801","timestamp":"1634107140.0"},{"timestamp":"1633998000.0","poster":"BigMomma4752","content":"Answer D is correct.","upvote_count":"1","comment_id":"206716"},{"comment_id":"149474","content":"D is correct","upvote_count":"1","timestamp":"1633920900.0","poster":"fullaws"},{"content":"D acceptable","poster":"noisonnoiton","timestamp":"1633803660.0","upvote_count":"1","comment_id":"137709"},{"timestamp":"1633743060.0","content":"D for sure","poster":"NikkyDicky","comment_id":"134252","upvote_count":"1"},{"timestamp":"1633478880.0","content":"I don't think it's A, as the question explicitly states \"The\nSolutions Architect finds that no changes have been made to the environment recently\". So no sence to fix anything as A suggests.\nD seems the most reasonable","upvote_count":"2","comment_id":"103154","poster":"Oleksandr"},{"content":"Should be D","comment_id":"49570","poster":"amog","upvote_count":"5","timestamp":"1632977520.0"},{"poster":"Moon","upvote_count":"6","timestamp":"1632959100.0","comment_id":"14141","content":"Agree on answer \"D\".\nthe issue is 50% failure, means the balancing over 2 AZs is failing on one NAT instance in one AZ.\nThe solution is to replace the NAT instance with fully managed and high available NAT gateway."}],"exam_id":32,"topic":"1"},{"id":"9wne7v3v6yroi8kQEG4n","topic":"1","answer_description":"","question_images":[],"choices":{"A":"Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center.","B":"Configure the Amazon CloudWatch agent on all servers within the local environment and publish metrics to Amazon CloudWatch Logs.","C":"Use AWS Application Discovery Service and enable agentless discovery in the existing virtualization environment.","D":"Enable AWS Application Discovery Service in the AWS Management Console and configure the corporate firewall to allow scans over a VPN."},"timestamp":"2019-09-12 14:56:00","exam_id":32,"unix_timestamp":1568292960,"discussion":[{"comments":[{"upvote_count":"1","poster":"TechGuru","timestamp":"1633081560.0","comment_id":"17595","content":"Awesome explanation donathon!!"}],"content":"A\nA: Agent is needed because network dependencies are needed.\nB: CloudWatch is not suitable.\nC\\D: Agentless is only for VMWare environment where network dependencies are not needed.\nAWS Application Discovery Service collects and presents data to enable enterprise customers to understand the configuration, usage, and behavior of servers in their IT environments. Server data is retained in the Application Discovery Service where it can be tagged and grouped into applications to help organize AWS migration planning. Collected data can be exported for analysis in Excel or other cloud migration analysis tools. \nAWS Application Discovery Service supports agent-based and agentless modes of operation. With the agentless discovery, VMware customers collect VM configuration and performance profiles without deploying the AWS Application Discovery Agent on each host, which accelerates data collection. Customers in a non-VMware environment or that need additional information, like network dependencies and information about running processes, may install the Application Discovery Agent on servers and virtual machines (VMs) to collect data.","timestamp":"1633038840.0","poster":"donathon","upvote_count":"78","comment_id":"13211"},{"upvote_count":"8","poster":"awsec2","content":"A is right one","comment_id":"10805","timestamp":"1632168180.0","comments":[{"timestamp":"1632298380.0","poster":"dpvnme","upvote_count":"2","comment_id":"11628","content":"The question doesn't mention the VMs are VMware so we need agent here. A"}]},{"comment_id":"926270","poster":"SkyZeroZx","upvote_count":"1","content":"Selected Answer: A\nA: Agent is needed because network dependencies are needed.","timestamp":"1687034160.0"},{"comment_id":"716932","poster":"DarthYoda","upvote_count":"1","timestamp":"1668287700.0","content":"Selected Answer: A\nMost definitely A"},{"timestamp":"1667324880.0","content":"Selected Answer: A\nI did not know about AWS Application Discovery Service and my answer was B, because CloudWatch Agent can do this too, but after reviewing the AWS Application Discovery Service Documentation, this service does exactly what is requested. Final Answer A.","comment_id":"709336","poster":"superuser784","upvote_count":"1"},{"timestamp":"1664814120.0","content":"Selected Answer: A\nBased on all comments","upvote_count":"1","comment_id":"685665","poster":"dmscountera"},{"upvote_count":"1","timestamp":"1652742660.0","content":"A ,this is because you need info about each process running on the VMs\nhttps://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html","comment_id":"602716","poster":"user0001"},{"content":"A. Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center.","timestamp":"1639052880.0","upvote_count":"1","poster":"cldy","comment_id":"497712"},{"comment_id":"450743","upvote_count":"1","poster":"andylogan","timestamp":"1636193940.0","content":"It's A - Agentless for VMware"},{"poster":"WhyIronMan","timestamp":"1636191000.0","upvote_count":"1","content":"I'll go with A","comment_id":"409570"},{"upvote_count":"1","comment_id":"344277","poster":"Waiweng","timestamp":"1635912480.0","content":"A is correct"},{"comment_id":"323033","upvote_count":"1","content":"Application Discovery Service offers two ways of performing discovery and collecting data about your on-premises servers:\n\n- Agentless discovery can be performed by deploying the AWS Agentless Discovery Connector (OVA file) through your VMware Center.\n\n- Agent-based discovery can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers.\n\nSo A is correct","timestamp":"1635892800.0","poster":"ExtHo"},{"comment_id":"289581","timestamp":"1635884820.0","content":"going with A","upvote_count":"2","poster":"Kian1"},{"comment_id":"283170","poster":"Ebi","content":"A is answer","timestamp":"1635835020.0","upvote_count":"4"},{"comment_id":"267419","upvote_count":"2","poster":"sanjaym","content":"A for sure","timestamp":"1635659700.0"},{"comment_id":"242438","content":"A is correct answer. Application Discovery Service agent is key","poster":"T14102020","timestamp":"1635455400.0","upvote_count":"1"},{"comment_id":"229446","timestamp":"1635409920.0","upvote_count":"2","poster":"jackdryan","content":"I'll go with A"},{"poster":"Bulti","content":"A is the right answer. For the requirements mentioned, agent based Application Discovery Service is required.","timestamp":"1635133920.0","comment_id":"228803","upvote_count":"2"},{"comment_id":"149476","upvote_count":"1","timestamp":"1635054180.0","poster":"fullaws","content":"A is correct"},{"content":"A acceptable\nneed agent","poster":"noisonnoiton","timestamp":"1634271120.0","comment_id":"137711","upvote_count":"1"},{"content":"A for sure","timestamp":"1634260080.0","upvote_count":"1","comment_id":"134254","poster":"NikkyDicky"},{"content":"One of the easiest Question haha\nA","comment_id":"93269","comments":[{"poster":"chicagomassageseeker","upvote_count":"1","comment_id":"118712","content":"for sure!","timestamp":"1633947300.0"}],"upvote_count":"3","timestamp":"1633164900.0","poster":"Ibranthovic"},{"upvote_count":"3","poster":"ThanhTran","comment_id":"58166","timestamp":"1633155180.0","content":"A and B are possible as both are agent-based. However, A is cost-effective as it's free https://aws.amazon.com/application-discovery/pricing/.\nFor B you have to pay for CloudWatch metrics."},{"timestamp":"1633141560.0","comment_id":"49573","poster":"amog","content":"Should be A","upvote_count":"3"}],"answer":"A","answer_images":[],"answer_ET":"A","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/5117-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A company is migrating to the cloud. It wants to evaluate the configurations of virtual machines in its existing data center environment to ensure that it can size new Amazon EC2 instances accurately. The company wants to collect metrics, such as CPU, memory, and disk utilization, and it needs an inventory of what processes are running on each instance. The company would also like to monitor network connections to map communications between servers.\nWhich would enable the collection of this data MOST cost effectively?","answers_community":["A (100%)"],"question_id":409},{"id":"IGIY1qAqRWLcgMbOcPBd","timestamp":"2019-12-28 04:52:00","answer":"A","answer_ET":"A","question_id":410,"answers_community":["A (71%)","14%","14%"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/10978-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"A":"Backup RDS using automated daily DB backups. Backup the EC2 instances using AMIs and supplement with file-level backup to S3 using traditional enterprise backup software to provide file level restore.","B":"Backup RDS using a Multi-AZ Deployment. Backup the EC2 instances using Amis, and supplement by copying file system data to S3 to provide file level restore.","D":"Backup RDS database to S3 using Oracle RMAN. Backup the EC2 instances using Amis, and supplement with EBS snapshots for individual volume restore.","C":"Backup RDS using automated daily DB backups. Backup the EC2 instances using EBS snapshots and supplement with file-level backups to Amazon Glacier using traditional enterprise backup software to provide file level restore."},"unix_timestamp":1577505120,"question_images":[],"answer_images":[],"question_text":"Your customer wishes to deploy an enterprise application to AWS, which will consist of several web servers, several application servers and a small (50GB)\nOracle database. Information is stored, both in the database and the file systems of the various servers. The backup system must support database recovery whole server and whole disk restores, and individual file restores with a recovery time of no more than two hours. They have chosen to use RDS Oracle as the database.\nWhich backup architecture will meet these requirements?","isMC":true,"discussion":[{"poster":"Mm_meaw","comments":[{"comment_id":"513241","content":"How do we address volume level restore (part of the question)\nBackup the EC2 instances using AMI -what does this mean ?","upvote_count":"1","timestamp":"1640859840.0","poster":"RVivek"},{"comment_id":"211708","timestamp":"1634125320.0","upvote_count":"2","content":"Really? RDS not support RMAN?\ni search and find this document.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.RMAN.html","comments":[{"poster":"ExtHo","upvote_count":"6","content":"@JJu but you missed note in your provided link clearly mentioned \n\"Currently, RMAN restore isn't supported for Amazon RDS for Oracle DB instances.\"","timestamp":"1634970240.0","comment_id":"313759"}],"poster":"JJu"},{"upvote_count":"1","comments":[{"content":"If the question does not specify the Glacier type, we need to assume it's \"classic\" Glacier with 12 hours retrieval time","upvote_count":"1","poster":"davideccc","timestamp":"1664635140.0","comment_id":"684272"}],"timestamp":"1635294240.0","poster":"cldy","comment_id":"324694","content":"why no Glacier? Expedited retrievals in Glacier is within 1-5 minutes."},{"content":"RDS for Oracle supports RMAN: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.RMAN.html","upvote_count":"1","timestamp":"1689245700.0","comment_id":"950579","poster":"kondratyevmn"}],"timestamp":"1632269160.0","upvote_count":"13","comment_id":"34813","content":"- no need for multi-az\n- no glacier\n- rds not support RMAN\nThen answer left is A."},{"comment_id":"1266511","poster":"amministrazione","content":"A. Backup RDS using automated daily DB backups. Backup the EC2 instances using AMIs and supplement with file-level backup to S3 using traditional enterprise backup software to provide file level restore.","timestamp":"1723734300.0","upvote_count":"1"},{"timestamp":"1689245820.0","poster":"kondratyevmn","upvote_count":"1","content":"Selected Answer: D\nD. \n\nRMAN is supported at RDS (for Oracle) -> restore individual files from DB, should meet RTO =2h. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.RMAN.html\nAMIs will provide a quick recovery of the EC2/server.\nEBS snapshots will provide disk recovery functionality.","comment_id":"950581"},{"comment_id":"695846","poster":"ashii007","upvote_count":"2","timestamp":"1665887400.0","content":"AWS RDS oracle supports RMAN \nhttps://aws.amazon.com/blogs/database/automate-amazon-rds-backups-using-the-oracle-rman-utility-and-upload-backup-files-to-amazon-s3/\n\nGiven 2 hour Restore objective, D is a viable option."},{"timestamp":"1664635200.0","content":"Selected Answer: A\nthe only answer that fits all the requirements is A","upvote_count":"1","comment_id":"684273","poster":"davideccc"},{"poster":"emmanuelodenyire","upvote_count":"1","content":"Selected Answer: A\nA fits the scenario","timestamp":"1664544180.0","comment_id":"683580"},{"poster":"TechX","content":"Selected Answer: A\nCan not be C. Even Expedited Galicer has retrival time 1-5 minutes (but only with files are < 250MB). Should be A","comment_id":"624372","upvote_count":"2","timestamp":"1656468840.0"},{"content":"Selected Answer: A\nA - full server backup is done using AMI. snapshots are per volume backup.","comment_id":"596494","upvote_count":"1","timestamp":"1651636980.0","poster":"snakecharmer2"},{"timestamp":"1650642540.0","content":"Selected Answer: C\nC is correct.\nA) Backup the EC2 instances using AMIs is nonsense. The only reasonable way to back up your instance is snapshots using DLM.\nB) Multi-AZ deployment is not from a backup opera at all. It's more of HA strategy.\nD) For using Oracle RMAN the one have to OS level access to DB instance. Which's not the case with RDS.","upvote_count":"1","comment_id":"590095","poster":"bobsmith2000"},{"poster":"Akhil254","upvote_count":"1","content":"A Correct","timestamp":"1636103340.0","comment_id":"406218"},{"comments":[{"timestamp":"1678315560.0","upvote_count":"1","content":"I guess generating an image of the EC2 Instance using AMI and consider it as a Whole Backup for the instance","poster":"ohasnaoui","comment_id":"833420"}],"comment_id":"346707","content":"What does \"Backup the EC2 instances using AMIs means?\nDoes it mean snapshot?\nIf so, I support A.","timestamp":"1636064940.0","upvote_count":"1","poster":"01037"},{"timestamp":"1634831220.0","poster":"bnagaraja9099","upvote_count":"1","comment_id":"283094","content":"I ll go with A. No multi az, no glacier. Question says individual file level back up. So no volumes. Leaves S3 and snapshots as best option"},{"poster":"A_New_Guy","upvote_count":"2","comment_id":"215295","content":"The D is the option correct\n\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/Amazon-RDS-for-Oracle-Now-Supports-Amazon-S3-Integration/","timestamp":"1634293500.0"},{"content":"Answer is A: RDS support automated backup to another region. EC2 backup usin AMI and individual files using S3 seems to be the right option.","comment_id":"170449","upvote_count":"2","poster":"Bulti","timestamp":"1633860600.0"},{"comment_id":"143858","poster":"fullaws","upvote_count":"3","content":"A is correct (automatic daily backup, transaction log), (s3 retrieve time)","timestamp":"1633588500.0"},{"poster":"noisonnoiton","upvote_count":"2","comment_id":"131097","content":"go with A","timestamp":"1633004460.0"},{"timestamp":"1632796800.0","poster":"JAWS1600","content":"A is correct","comment_id":"106994","upvote_count":"2"},{"poster":"KB26","upvote_count":"2","comment_id":"94343","content":"Now you can use RMAN with RDS for Backup, but you can't restore it as per: \nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.RMAN.html","timestamp":"1632774120.0"},{"comment_id":"87349","poster":"qianhaopower","content":"It is D, you can use RMAN on RDS. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.RMAN.html","upvote_count":"4","timestamp":"1632634560.0"},{"comment_id":"49675","timestamp":"1632489840.0","content":"A is Correct!","upvote_count":"3","poster":"BillyC"},{"comments":[{"poster":"sarah1","comment_id":"43463","upvote_count":"1","timestamp":"1632471180.0","content":"Agreed. D also only provides volume level restore (with EBS snapshots), not file level restore (for example to s3)."},{"upvote_count":"1","content":"why no Glacier? Expedited retrievals in Glacier is within 1-5 minutes.","comment_id":"324916","comments":[{"comment_id":"624373","content":"Can not be C. Even Expedited Galicer has retrival time 1-5 minutes (but only with files are < 250MB). Should be A","upvote_count":"1","timestamp":"1656468900.0","poster":"TechX"}],"timestamp":"1635860220.0","poster":"cldy"}],"poster":"amog","comment_id":"37896","upvote_count":"2","content":"Answer is A\nB: Multi-AZ is more of an Disaster recovery solution\nC: Glacier not an option with the 2 hours RTO\nD: Will use RMAN only if Database hosted on EC2 and not when using RDS","timestamp":"1632454500.0"},{"content":"The most important thing to know about Amazon Glacier is that if you want to retrieve files, it takes 3 to 5 hours to complete. So this isn't for backing up and quickly retrieving a file you accidentally deleted and need right away.","upvote_count":"4","poster":"hezll","timestamp":"1632122940.0","comment_id":"33842"},{"comment_id":"33154","content":"Back up EC2 using AMIs is a really bad choice. C is a better option for me","comments":[{"upvote_count":"1","comments":[{"content":"Expedited retrievals in Glacier is within 1-5 minutes.","comments":[],"comment_id":"324693","upvote_count":"1","timestamp":"1635176460.0","poster":"cldy"}],"timestamp":"1634609220.0","poster":"Ajeeshpv","content":"Recovery time cant be more than 2 hours ...so Glacier not possible...so go for A","comment_id":"278426"}],"upvote_count":"2","timestamp":"1632091500.0","poster":"chandler"}],"exam_id":32,"topic":"1"}],"exam":{"id":32,"provider":"Amazon","isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isBeta":false},"currentPage":82},"__N_SSP":true}