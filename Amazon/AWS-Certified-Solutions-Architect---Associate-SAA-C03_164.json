{"pageProps":{"questions":[{"id":"z7FA58bGlTlKaL5sPjvq","answer_description":"","discussion":[{"timestamp":"1710675480.0","upvote_count":"8","poster":"Mikado211","content":"Selected Answer: D\nIn such situation if you had an ALB you would use Cloudfront\nSince you have a NLB you use AWS Global Accelerator\nSo D.","comment_id":"1175765"},{"content":"Selected Answer: D\nShould be D","poster":"asdfcdsxdfc","comment_id":"1166788","timestamp":"1709676060.0","upvote_count":"5"},{"poster":"Scheldon","comment_id":"1228599","content":"Selected Answer: D\nAnswerD\n\nUsage of Global Accelerator should help here.\n\"\nAcceleration for latency-sensitive applications\nMany applications, especially in areas such as gaming, media, mobile apps, ad-tech, and financials, require very low latency for a great user experience. To improve the user experience, Global Accelerator directs user traffic to the application endpoint that is nearest to the client, which reduces internet latency and jitter. Global Accelerator routes traffic to the closest edge location by using Anycast, and then routes it to the closest regional endpoint over the AWS global network. Global Accelerator quickly reacts to changes in network performance to improve your users’ application performance.\n\"\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-benefits-of-migrating.html","timestamp":"1718130420.0","upvote_count":"3"},{"poster":"seetpt","upvote_count":"2","content":"Selected Answer: D\nAgree with D","timestamp":"1709889300.0","comment_id":"1168684"}],"answer_ET":"D","answer":"D","isMC":true,"answer_images":[],"timestamp":"2024-03-05 23:01:00","topic":"1","question_images":[],"choices":{"C":"Create additional NLBs and EC2 instances in other Regions where the company has large customer bases.","D":"Create a standard accelerator in AWS Global Accelerator. Configure the existing NLBs as target endpoints.","B":"Configure Amazon Route 53 to route equally weighted traffic to the NLBs in each Region.","A":"Create Application Load Balancers (ALBs) in each Region to replace the existing NLBs. Register the existing EC2 instances as targets for the ALBs in each Region."},"exam_id":31,"answers_community":["D (100%)"],"question_text":"An online gaming company hosts its platform on Amazon EC2 instances behind Network Load Balancers (NLBs) across multiple AWS Regions. The NLBs can route requests to targets over the internet. The company wants to improve the customer playing experience by reducing end-to-end load time for its global customer base.\n\nWhich solution will meet these requirements?","question_id":816,"unix_timestamp":1709676060,"url":"https://www.examtopics.com/discussions/amazon/view/135267-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"B27HHL1RzPaEqlYYXoXO","answer_ET":"B","answer":"B","choices":{"D":"Configure an Amazon S3 File Gateway for vendors that use legacy applications to upload files to an SMB file share.","B":"Create an AWS Transfer Family endpoint for vendors that use legacy applications.","C":"Configure an Amazon EC2 instance to run an SFTP server. Instruct the vendors that use legacy applications to use the SFTP server to upload data.","A":"Create an AWS Database Migration Service (AWS DMS) instance to replicate data from the storage of the vendors that use legacy applications to Amazon S3. Provide the vendors with the credentials to access the AWS DMS instance."},"answers_community":["B (100%)"],"exam_id":31,"topic":"1","answer_images":[],"question_id":817,"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/135268-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","unix_timestamp":1709676180,"discussion":[{"timestamp":"1709676180.0","upvote_count":"8","poster":"asdfcdsxdfc","content":"Selected Answer: B\nB is correct","comment_id":"1166789"},{"content":"Selected Answer: B\nA - DMS is designed for database migrations, not for file-based SFTP uploads.\nB - AWS Transfer Family is a fully managed service that supports SFTP, FTPS, and FTP protocols.\nC - OK but significant operational overhead.\nD - S3 File Gateway supports SMB and NFS, not SFTP.","upvote_count":"1","comments":[{"content":"also C - Not a managed services.","comment_id":"1358973","timestamp":"1739999580.0","poster":"Dantecito","upvote_count":"1"}],"comment_id":"1332231","timestamp":"1735275780.0","poster":"LeonSauveterre"},{"poster":"Johnoppong101","content":"Selected Answer: B\nAnswer is B","timestamp":"1723784520.0","upvote_count":"1","comment_id":"1266822"},{"comment_id":"1205952","upvote_count":"4","content":"Selected Answer: B\nExplanation:\n\nAWS Transfer Family is a fully managed service that allows you to set up SFTP, FTPS, and FTP endpoints for accessing Amazon S3 and Amazon EFS storage.\nBy creating an AWS Transfer Family endpoint, the company can provide vendors with the familiar SFTP interface to upload data directly to Amazon S3 without requiring them to make any changes to their legacy applications.\nThis solution eliminates the need for the company to manage and maintain additional infrastructure such as EC2 instances or file gateways.\nAWS Transfer Family handles scalability, availability, and security, reducing operational overhead for the company.","timestamp":"1714717560.0","poster":"Sergiuss95"},{"content":"Selected Answer: B\nB is correct","comment_id":"1168686","upvote_count":"2","timestamp":"1709889360.0","poster":"seetpt"}],"question_text":"A company has an on-premises application that uses SFTP to collect financial data from multiple vendors. The company is migrating to the AWS Cloud. The company has created an application that uses Amazon S3 APIs to upload files from vendors.\n\nSome vendors run their systems on legacy applications that do not support S3 APIs. The vendors want to continue to use SFTP-based applications to upload data. The company wants to use managed services for the needs of the vendors that use legacy applications.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","timestamp":"2024-03-05 23:03:00"},{"id":"q6wS92stdCwXqViJ6SSr","answers_community":["C (100%)"],"question_text":"A marketing team wants to build a campaign for an upcoming multi-sport event. The team has news reports from the past five years in PDF format. The team needs a solution to extract insights about the content and the sentiment of the news reports. The solution must use Amazon Textract to process the news reports.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","timestamp":"2024-03-05 23:07:00","url":"https://www.examtopics.com/discussions/amazon/view/135269-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","answer":"C","answer_images":[],"isMC":true,"answer_ET":"C","question_id":818,"discussion":[{"content":"Selected Answer: C\nAnswerC\n\"\nAmazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases.\n\"\nhttps://docs.aws.amazon.com/comprehend/latest/dg/what-is.html","timestamp":"1718130900.0","comment_id":"1228605","poster":"Scheldon","upvote_count":"3"},{"upvote_count":"2","comment_id":"1202002","content":"Selected Answer: C\nSelected Answer: C\namazon comprehend= sentiment analysis","poster":"zinabu","timestamp":"1714050660.0"},{"content":"Selected Answer: C \namazon comprehend= sentiment analysis","timestamp":"1712719500.0","upvote_count":"4","comment_id":"1192645","poster":"zinabu"},{"poster":"alawada","comment_id":"1180457","timestamp":"1711155780.0","content":"Selected Answer: C\nWhenever new PDF files are uploaded to the designated S3 bucket, the Lambda function will be triggered to extract insights using Textract and Comprehend.","upvote_count":"3"},{"comment_id":"1175761","poster":"Mikado211","content":"Selected Answer: C\nWhen you have words like \"sentiment\" in a sentence, it's related to Comprehend\nSo C.","timestamp":"1710675240.0","upvote_count":"2"},{"timestamp":"1709889480.0","poster":"seetpt","upvote_count":"2","content":"Selected Answer: C\nMaybe C?","comment_id":"1168688"},{"poster":"asdfcdsxdfc","upvote_count":"4","content":"Shouldn't it be C?","timestamp":"1709676420.0","comment_id":"1166790"}],"choices":{"A":"Provide the extracted insights to Amazon Athena for analysis. Store the extracted insights and analysis in an Amazon S3 bucket.","D":"Store the extracted insights in an Amazon S3 bucket. Use Amazon QuickSight to visualize and analyze the data.","C":"Provide the extracted insights to Amazon Comprehend for analysis. Save the analysis to an Amazon S3 bucket.","B":"Store the extracted insights in an Amazon DynamoDB table. Use Amazon SageMaker to build a sentiment model."},"question_images":[],"answer_description":"","exam_id":31,"unix_timestamp":1709676420},{"id":"bxPyhdXejsTYQJunk9Lo","answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/135270-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-03-05 23:08:00","question_images":[],"choices":{"D":"Create an AWS Direct Connect connection to the application for data ingestion. Create Amazon Kinesis Data Firehose delivery streams to consume direct PUT operations from the application. Specify the S3 bucket as the destination of the delivery streams.","A":"Create Amazon Kinesis data streams for data ingestion. Create Amazon Kinesis Data Firehose delivery streams to consume the Kinesis data streams. Specify the S3 bucket as the destination of the delivery streams.","C":"Create and configure AWS DataSync agents on the EC2 instances. Configure DataSync tasks to transfer data from the EC2 instances to the S3 bucket.","B":"Create database migration tasks in AWS Database Migration Service (AWS DMS). Specify replication instances of the EC2 instances as the source endpoints. Specify the S3 bucket as the target endpoint. Set the migration type to migrate existing data and replicate ongoing changes."},"answer_ET":"A","question_id":819,"topic":"1","exam_id":31,"discussion":[{"poster":"asdfcdsxdfc","upvote_count":"9","content":"Selected Answer: A\nA is correct","comment_id":"1166791","timestamp":"1709676480.0"},{"comments":[{"poster":"JA2018","comments":[{"timestamp":"1732949340.0","comment_id":"1320102","content":"Key points to remember:\nWhen dealing with real-time data ingestion, Kinesis Data Streams is generally the preferred solution on AWS.\nKinesis Data Firehose can be used to easily stream data from Kinesis to various destinations like S3 buckets.","upvote_count":"1","poster":"JA2018"}],"timestamp":"1732949280.0","content":"Why the other options are incorrect:\nB. AWS Database Migration Service (DMS):\nDMS is primarily used for migrating databases, not for real-time data ingestion from third-party applications.\nC. AWS DataSync:\nWhile DataSync can transfer data to S3, it's not optimized for real-time data ingestion and would likely not be the best choice for this scenario.\nD. AWS Direct Connect:\nDirect Connect is used for dedicated private network connections between your on-premises network and AWS, not for real-time data ingestion from third-party applications.","comment_id":"1320101","upvote_count":"1"}],"poster":"JA2018","timestamp":"1732949280.0","content":"Selected Answer: A\nWhy A?\n\nKinesis Data Streams: This is designed for real-time data ingestion, which is exactly what the scenario requires.\n\nKinesis Data Firehose: This service can then be used to efficiently deliver the ingested data from Kinesis streams to an S3 bucket, fulfilling the need to store raw data in S3","upvote_count":"1","comment_id":"1320100"},{"comment_id":"1300238","timestamp":"1729389300.0","upvote_count":"2","poster":"mk168898","content":"Selected Answer: A\nreal time data => amazon kinesis data stream"},{"poster":"MatAlves","timestamp":"1726812360.0","upvote_count":"2","content":"Selected Answer: A\n- Amazon Kinesis Data Streams: designed for real-time data ingestion. \n- Kinesis Data Firehose: can consume data from Kinesis Data Streams and automatically deliver it to Amazon S3.\n\nA is the answer.","comment_id":"1286678"},{"comment_id":"1243110","upvote_count":"1","content":"Selected Answer: C\neach ec2 needs to proceed data separately","poster":"Mayank0502","timestamp":"1720228020.0"},{"comments":[{"poster":"Sergiuss95","timestamp":"1714718040.0","upvote_count":"5","content":"DataSync is more suitable for transferring data between on-premises storage systems and AWS, rather than ingesting real-time data. Best solution is A","comment_id":"1205956"}],"upvote_count":"3","timestamp":"1712172780.0","poster":"xBUGx","comment_id":"1188870","content":"Selected Answer: C\nA is best solution, but i think the question is saying \"The application needs to ingest real-time data from third-party applications.\" and the application is run on EC2.\nso i think we need a solution that works with the application on EC2 for this question?"},{"timestamp":"1709889540.0","poster":"seetpt","content":"Agree with A","upvote_count":"2","comment_id":"1168690"}],"unix_timestamp":1709676480,"question_text":"A company's application runs on Amazon EC2 instances that are in multiple Availability Zones. The application needs to ingest real-time data from third-party applications.\n\nThe company needs a data ingestion solution that places the ingested raw data in an Amazon S3 bucket.\n\nWhich solution will meet these requirements?","answer_images":[],"answers_community":["A (78%)","C (22%)"],"answer":"A"},{"id":"IbFbLqsSA5WvPdAZDEda","choices":{"B":"Store the large data as objects in an Amazon S3 bucket. In a DynamoDB table, create an item that has an attribute that points to the S3 URL of the data.","D":"Create an AWS Lambda function that uses gzip compression to compress the large objects as they are written to a DynamoDB table.","A":"Create an AWS Lambda function to filter the data that exceeds DynamoDB item size limits. Store the larger data in an Amazon DocumentDB (with MongoDB compatibility) database.","C":"Split all incoming large data into a collection of items that have the same partition key. Write the data to a DynamoDB table in a single operation by using the BatchWriteItem API operation."},"answer":"B","isMC":true,"unix_timestamp":1709731680,"question_text":"A company’s application is receiving data from multiple data sources. The size of the data varies and is expected to increase over time. The current maximum size is 700 KB. The data volume and data size continue to grow as more data sources are added.\n\nThe company decides to use Amazon DynamoDB as the primary database for the application. A solutions architect needs to identify a solution that handles the large data sizes.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/135302-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"answer_ET":"B","exam_id":31,"question_id":820,"timestamp":"2024-03-06 14:28:00","topic":"1","answer_description":"","answers_community":["B (100%)"],"discussion":[{"comment_id":"1167178","timestamp":"1709731680.0","content":"Selected Answer: B\noption B is the most operationally efficient solution for handling large data sizes in Amazon DynamoDB.","poster":"Neung983","upvote_count":"9"},{"comment_id":"1168692","timestamp":"1709889600.0","content":"Selected Answer: B\nB is correct","poster":"seetpt","upvote_count":"5"},{"poster":"LeonSauveterre","upvote_count":"1","comment_id":"1332248","content":"Selected Answer: B\nA - Introducing another database. No need.\nB - Decoupling the large data storage (S3) from the metadata or smaller structured data (DynamoDB) is an AWS-recommended pattern.\nC - OK but this is too complicated.\nD - Compression does not guarantee that the data will always fit within DynamoDB's 400 KB limit.","timestamp":"1735278360.0"},{"poster":"Scheldon","timestamp":"1718132160.0","comment_id":"1228623","upvote_count":"1","content":"Selected Answer: B\nAnswerB\n\nCompresion of data in DynamoDB is a good idea especially for text data link from forum, but to do that we do not need AWS Lambda if I'm not wrong.\nIn other head Storing big object on S3 and seving URL to it in DynamoDB is one of best practices mentioned by Amazon. Hence we do not know what kind of data we are storing in DB and how big objects will be in the future option B looks like the best solution.\nhttps://aws.amazon.com/blogs/database/large-object-storage-strategies-for-amazon-dynamodb/ <<<< Read Option 2\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html"},{"upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html","poster":"Sergiuss95","comment_id":"1205961","timestamp":"1714718640.0"}]}],"exam":{"lastUpdated":"11 Apr 2025","isMCOnly":true,"id":31,"isImplemented":true,"isBeta":false,"name":"AWS Certified Solutions Architect - Associate SAA-C03","numberOfQuestions":1019,"provider":"Amazon"},"currentPage":164},"__N_SSP":true}