{"pageProps":{"questions":[{"id":"SfTDU2jVdyxnWEvOqSts","unix_timestamp":1629621900,"isMC":true,"timestamp":"2021-08-22 10:45:00","answer_ET":"A","discussion":[{"comments":[{"comment_id":"446909","timestamp":"1635213840.0","content":"I meant to put A","poster":"Katapangan55","upvote_count":"1"}],"content":"https://aws.amazon.com/lake-formation/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc\n\n\"You can use Lake Formation to centrally define security, governance, and auditing policies in one place, versus doing these tasks per service, and then enforce those policies for your users across their analytics applications. Your policies are consistently implemented, eliminating the need to manually configure them across security services like AWS Identity and Access Management and AWS Key Management Service, storage services like S3, and analytics and machine learning services like Redshift, Athena, and (in beta) EMR for Apache Spark. This reduces the effort in configuring policies across services and provides consistent enforcement and compliance.\"","comment_id":"446908","upvote_count":"15","poster":"Katapangan55","timestamp":"1635180660.0"},{"poster":"taizo777","timestamp":"1632199740.0","content":"I think A is right answer","comment_id":"429135","upvote_count":"12"},{"poster":"pk349","content":"A: I passed the test","comment_id":"886589","upvote_count":"2","timestamp":"1682965680.0"},{"content":"Selected Answer: A\nCorrect answer is A as AWS Lake Formation helps define security policy-based rules for the users and applications by role centrally and easily with the LEAST operational overhead.","comment_id":"713896","poster":"cloudlearnerhere","timestamp":"1667917380.0","upvote_count":"5"},{"poster":"hary104","timestamp":"1663608480.0","comment_id":"673505","content":"A is the answer. - Multiple BUs, Data Calatlog. - permission handling. - use LKF","upvote_count":"1"},{"comment_id":"650925","upvote_count":"2","timestamp":"1661275260.0","content":"the answer is B. least operational overhead means \"don't set up anything new\". \nAthena has capability to provide access control via IAM (resource-based, action based)\n\nhttps://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonathena.html","poster":"muhsin"},{"content":"Selected Answer: A\nA is right answer","comment_id":"643512","poster":"rocky48","upvote_count":"2","timestamp":"1659817200.0"},{"content":"Selected Answer: A\nAs already explained in this topic. The answer should be A. Using Lake Formation it is much easier to assign data access to roles. Glue can also work, but it is harder. Using IAM I don't think you can achieve fine-grained access.","poster":"Ramshizzle","timestamp":"1655973180.0","upvote_count":"3","comment_id":"620853"},{"comment_id":"604866","content":"Selected Answer: D\nAgree. Answer D is correct","comments":[{"upvote_count":"4","poster":"rav009","timestamp":"1666341000.0","comment_id":"700661","content":"IAM policy can't do column-level security for athena, only table level."}],"poster":"Bik000","upvote_count":"1","timestamp":"1653132900.0"},{"content":"A is the answer for new data lake builder to maintain access to lake.","comment_id":"446343","upvote_count":"1","timestamp":"1635130800.0","poster":"ThomasKalva"},{"timestamp":"1632911280.0","poster":"doobc","comment_id":"441908","comments":[{"upvote_count":"1","poster":"doobc","comment_id":"441913","timestamp":"1633503480.0","content":"A is easier"}],"upvote_count":"1","content":"Why is it A? Not B?"},{"content":"I think B is the correct answer.","upvote_count":"1","comment_id":"441642","comments":[{"comment_id":"442072","content":"Changed A is correct.","poster":"Marcinha","upvote_count":"2","timestamp":"1634577000.0"}],"timestamp":"1632228900.0","poster":"Marcinha"}],"answer":"A","question_images":[],"answer_images":[],"choices":{"D":"Define security policy-based rules for the tables and columns by role in AWS Identity and Access Management (IAM).","C":"Define security policy-based rules for the tables and columns by role in AWS Glue.","A":"Define security policy-based rules for the users and applications by role in AWS Lake Formation.","B":"Define security policy-based rules for the users and applications by role in AWS Identity and Access Management (IAM)."},"topic":"1","question_text":"A company has a data lake on AWS that ingests sources of data from multiple business units and uses Amazon Athena for queries. The storage layer is Amazon\nS3 using the AWS Glue Data Catalog. The company wants to make the data available to its data scientists and business analysts. However, the company first needs to manage data access for Athena based on user roles and responsibilities.\nWhat should the company do to apply these access controls with the LEAST operational overhead?","answers_community":["A (91%)","9%"],"question_id":11,"answer_description":"","exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/60220-exam-aws-certified-data-analytics-specialty-topic-1-question/"},{"id":"lwIUSSUdbgy5kIxDSYzC","timestamp":"2021-08-22 10:51:00","choices":{"D":"Enable default encryption on the Amazon S3 bucket where the logs are stored by using AES-256 encryption. Use Amazon Redshift Spectrum to query the data as required.","C":"Enable default encryption on the Amazon S3 bucket where the logs are stored by using AES-256 encryption. Copy the data into the Amazon Redshift cluster from Amazon S3 on a daily basis. Query the data as required.","B":"Disable encryption on the Amazon Redshift cluster, configure audit logging, and encrypt the Amazon Redshift cluster. Use Amazon Redshift Spectrum to query the data as required.","A":"Encrypt the Amazon S3 bucket where the logs are stored by using AWS Key Management Service (AWS KMS). Copy the data into the Amazon Redshift cluster from Amazon S3 on a daily basis. Query the data as required."},"isMC":true,"answer_description":"","answer_ET":"D","unix_timestamp":1629622260,"topic":"1","discussion":[{"upvote_count":"26","comment_id":"429145","content":"I think D is right answer","timestamp":"1632808440.0","poster":"taizo777"},{"comment_id":"543899","upvote_count":"6","poster":"penelop","content":"Selected Answer: D\nA business owns an Amazon Redshift cluster that is encrypted. The organization just enabled audit logs in Amazon Redshift and wants to guarantee that audit logs are likewise encrypted at rest. The logs are kept for one year. The auditor conducts a monthly audit of the logs.\n\nHow might these needs be met in the MOST cost-effective manner possible?\n\nD is the best answer. We want to have our logs in S3 and be able to query them. Using the S3 encryption is enough for our security requirements. Now, the logs are audited once a month, meaning we need to extract meaningful information from them. We already have a Redshift cluster, so using spectrum is a bliss for this task.","timestamp":"1644422940.0"},{"content":"D: I passed the test","upvote_count":"1","timestamp":"1682965740.0","comment_id":"886591","poster":"pk349"},{"timestamp":"1672072080.0","poster":"Zast","content":"A can't be correct because of the following: \"Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging.\"\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html#db-auditing-logs\n\nHence, D is the right answer.","upvote_count":"2","comment_id":"757653"},{"upvote_count":"3","poster":"cloudlearnerhere","comments":[{"upvote_count":"1","poster":"cloudlearnerhere","comments":[{"upvote_count":"1","timestamp":"1666893120.0","poster":"cloudlearnerhere","content":"Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to execute very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Multiple clusters can concurrently query the same dataset in Amazon S3 without the need to make copies of the data for each cluster.","comment_id":"705794"}],"content":"Amazon Redshift logs information about connections and user activities in your database. These logs help you to monitor the database for security and troubleshooting purposes, a process called database auditing. The logs are stored in Amazon S3 buckets. These provide convenient access with data-security features for users who are responsible for monitoring activities in the database.","comment_id":"705793","timestamp":"1666893120.0"}],"content":"Correct answer is D as S3 default encryption helps data at rest encryption. As the logs are queried once a month, it would be cost-effective to store the data in S3 and have it queried using Redshift Spectrum. \n\nOptions A & C are wrong as loading the data in Redshift would result in the most cost-effective solution.\n\nOption B is wrong as Redshift Audit logs are stored in S3.","comment_id":"705792","timestamp":"1666893120.0"},{"poster":"Sen5476","timestamp":"1656454380.0","comment_id":"624293","upvote_count":"2","content":"Option C. AES 256 - Free, S3 select can query the data directly. Instead KMS additional cost and Spectrum needs cluster, costly."},{"upvote_count":"1","comments":[{"comment_id":"625089","poster":"dushmantha","timestamp":"1656573420.0","upvote_count":"1","content":"Agree to Sen5476. KMS is additional cost so I can go with C"}],"comment_id":"619155","timestamp":"1655716740.0","poster":"dushmantha","content":"Selected Answer: A\nI doubt that D is the most cost effective answer since it involves expensive \"Amazon Redshift Spectrum\". Guys this is an additional cost as given in \"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html\". Its original purpose is to query exabytes of data resides in S3 without loading to Redshift. But copying an audit log will be of no cost compared to that, coz its small in size. So I guess it should be A. We can use copy command to load data and its capable of automatically decrypting data in S3 while copying."},{"comment_id":"612605","timestamp":"1654583700.0","upvote_count":"3","poster":"Ramshizzle","content":"Selected Answer: D\nIt is D! AWS KMS is more expensive then using the default SSE-S3 (using AES-256). Loading the logs into Redshift Cluster is more expensive then querying the logs via Redshift Spectrum."},{"timestamp":"1651116300.0","comment_id":"593495","poster":"MWL","content":"Selected Answer: D\nD. As CHRIS12722222 commented:\n\"Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging.\"","upvote_count":"3"},{"poster":"sbxme","comments":[{"upvote_count":"6","poster":"CHRIS12722222","comment_id":"589865","timestamp":"1650619920.0","content":"This statement is wrong. \n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\n\"Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging.\""}],"upvote_count":"1","content":"Selected Answer: A\nAES-256 not supported","comment_id":"581690","timestamp":"1649232660.0"},{"timestamp":"1648513560.0","upvote_count":"1","content":"Selected Answer: D\nNo need to COPY the data into Redshift. You can use Redshift Spectrum to query the data in S3 since it is used monthly only.","poster":"pidkiller","comment_id":"577169"},{"poster":"moon2351","content":"Selected Answer: D\nAnswer is D","timestamp":"1647564900.0","upvote_count":"2","comment_id":"570144"},{"content":"D\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html","comment_id":"501486","timestamp":"1639497780.0","upvote_count":"3","poster":"Crypt0zknight"},{"timestamp":"1637180400.0","comment_id":"480251","content":"ans is D","poster":"aws2019","upvote_count":"1"},{"content":"D is the correct answer as auditor is only looking to query data once a month. Data can stay on s3.","comment_id":"446345","poster":"ThomasKalva","timestamp":"1635543300.0","upvote_count":"3"}],"answer_images":[],"question_images":[],"answers_community":["D (88%)","12%"],"answer":"D","question_id":12,"url":"https://www.examtopics.com/discussions/amazon/view/60223-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"question_text":"A company has an encrypted Amazon Redshift cluster. The company recently enabled Amazon Redshift audit logs and needs to ensure that the audit logs are also encrypted at rest. The logs are retained for 1 year. The auditor queries the logs once a month.\nWhat is the MOST cost-effective way to meet these requirements?"},{"id":"L7keejV2PBbTp4etIRgK","topic":"1","question_id":13,"exam_id":20,"question_images":[],"answers_community":["A (100%)"],"isMC":true,"discussion":[{"content":"A. \nBD dropped due to row based format.\nChoosing between ORC and Parquet format would be tough since their performance is very close. However, data are supposed to partitioned by date then sorted by source IP, so C dropped.","timestamp":"1632478560.0","comment_id":"159208","comments":[{"timestamp":"1632519300.0","poster":"abhineet","upvote_count":"2","comment_id":"159372","content":"correct"}],"poster":"zanhsieh","upvote_count":"60"},{"upvote_count":"12","poster":"Paitan","content":"ORC and Parquet are ideal here. But the data should be partitioned by Date and sorted on IP and not the other way round. So option A is the right choice.","timestamp":"1633108800.0","comment_id":"175257"},{"upvote_count":"2","timestamp":"1707803160.0","comment_id":"1148904","poster":"kondi2309","content":"Selected Answer: A\nideal choices will be ORC and Parquet, first choice being Apache Parquet, but here we have to consider partition, and partition by Date then sort on IP is best way to store data."},{"timestamp":"1690998360.0","content":"Selected Answer: A\nBetween A and C, if we have more IPs than Dates.\nI would go with A since analysis is performed on a daily schedule and anomalies are detected on a time interval.","comment_id":"970424","poster":"MLCL","upvote_count":"1"},{"poster":"NikkyDicky","comment_id":"968524","upvote_count":"1","timestamp":"1690840920.0","content":"Selected Answer: A\nits an A"},{"poster":"pk349","content":"A: I passed the test","upvote_count":"2","timestamp":"1682946540.0","comment_id":"886269"},{"comment_id":"837695","upvote_count":"1","timestamp":"1678690380.0","content":"Option A: In Apache ORC partitioned by date and sorted by source IP\n\nPartitioning the data by date allows for faster query performance and efficient data retrieval based on time periods.\nSorting the data by source IP enables efficient filtering and joins on that attribute.\nOverall, ORC partitioned by date and sorted by source IP would provide efficient storage and querying of the data.","poster":"anonymous909"},{"comment_id":"817485","poster":"srirnag","content":"Option C is the best. The analysis is done on last 24 hours of data. Hence, sorting by IP may not be ideal.","timestamp":"1677043080.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1667564700.0","content":"Selected Answer: A\nA is the right answer as the company does daily analysis, so it only needs to look at the data generated for a given date\n\nC is wrong as partitioning by source IP is incorrect for this use case, and partitioning by date is optimal.\n\nB & D, Both the above options are not columnar storage formats, they are row-based formats that are not optimal for big data retrievals for complex analytical queries.","comment_id":"711119","poster":"cloudlearnerhere"},{"content":"Selected Answer: A\nA is the right answer","timestamp":"1658380260.0","poster":"rocky48","upvote_count":"1","comment_id":"634368"},{"comment_id":"626916","timestamp":"1656924840.0","content":"Selected Answer: A\nAgree with \"zanhsieh\"","upvote_count":"1","poster":"dushmantha"},{"poster":"Ayaa4","content":"Columnar data is faster such as ORC and Parquet, answer is A","comment_id":"616323","upvote_count":"1","timestamp":"1655227140.0"},{"content":"Selected Answer: A\nAnswer is A","poster":"Bik000","upvote_count":"2","comment_id":"605259","timestamp":"1653205800.0"},{"timestamp":"1651349520.0","content":"Answer : A","upvote_count":"1","poster":"jrheen","comment_id":"595253"},{"poster":"rav009","content":"For previous 24 hours, sorted by date from C is not helpful. Sorted by timestamp makes sense.","upvote_count":"2","timestamp":"1643446920.0","comment_id":"535333"},{"timestamp":"1636173060.0","upvote_count":"5","poster":"Donell","content":"Answer is A. In Apache ORC partitioned by date and sorted by source IP.\nBecause the company analyzes historical logs dating back 2 years and also past 24 hours data.\nHence the Data should be partitioned based on Date and sorted by IP and not the other way around.\nORC is columnar hence preferred data format.","comment_id":"386012"},{"timestamp":"1635997320.0","comment_id":"383526","poster":"Shraddha","upvote_count":"2","content":"B and D = wrong, use columnar format. C = wrong, partition by date so historical data and be separated."},{"comment_id":"359141","content":"The answer is A.","timestamp":"1635956700.0","upvote_count":"2","poster":"leliodesouza"},{"comments":[{"upvote_count":"1","comment_id":"582744","timestamp":"1649403600.0","content":"But you will concentrate on a specific time window to find anomalies, so partioning by date sounds better for me...","poster":"DevoteamAnalytix"}],"poster":"jAWStest","upvote_count":"1","content":"C\nI think the key here is \"logs for abnormality detection and to troubleshoot and resolve user issues\" Partition by source IP helps with troubleshooting and abnormality detection","comment_id":"298089","timestamp":"1635848640.0"},{"content":"A. With partitioned by date, we don't have to scan the unnecessary two-year ago files.","poster":"Exia","comment_id":"285138","timestamp":"1635192120.0","upvote_count":"3"},{"comment_id":"280409","upvote_count":"3","content":"A is the right answer. CSV and JSON are not optimized formats for big data processing. That leaves A and C. C is not correct as it will create too many many partitions and small files and degrade query performance.","poster":"jay1ram2","timestamp":"1635150660.0"},{"comment_id":"274247","poster":"lostsoul07","timestamp":"1634956440.0","upvote_count":"1","content":"A is the right answer"},{"content":"The sort by IP address is a red-herring and is meant to confuse the exam-taker; the priority here is to OPTIMIZE PARTITIONING so that you don't need to scan unnecessary data based on the requirements. \n\nThe correct answer should be A. Say for example you want to analyze only the past 24h worth of data - if you partition by IP address, then you'll have to scan every single partition. But if you partition by date, the query will isolate only in that partition for that date","poster":"mendelthegreat","upvote_count":"4","timestamp":"1634728080.0","comment_id":"255721"},{"poster":"Deep101","comment_id":"255002","upvote_count":"4","content":"A says, sorting by source IP, what good can it bring? I think the answer is C.","timestamp":"1634680920.0"},{"poster":"hans1234","comment_id":"222970","timestamp":"1634526360.0","upvote_count":"3","content":"Must be A! Imagine you want to analyze the last 24 hours, if it is not partitioned by date, you have to touch every single file. That is bad."},{"comment_id":"216829","timestamp":"1634299920.0","content":"A is correct for me","upvote_count":"2","poster":"BillyC"},{"poster":"jove","upvote_count":"3","timestamp":"1634258520.0","comment_id":"205777","content":"It is A.. Data should be partitioned by date and ORC gives a better performance than compressed csv."},{"comments":[{"poster":"syu31svc","upvote_count":"1","comment_id":"194060","timestamp":"1634082960.0","content":"Sorry link is https://aws.amazon.com/premiumsupport/knowledge-center/s3-object-key-naming-pattern/"}],"content":"Answer is A\nFrom link: https://www.examtopics.com/exams/amazon/aws-certified-data-analytics-specialty/view/\nAn object key name that follows the date-based naming convention often looks similar to the following:\n\nawsexamplebucket/HadoopTableName/dt=yyyy-mm-dd/objectname","poster":"syu31svc","upvote_count":"4","comment_id":"191275","timestamp":"1633683960.0"},{"upvote_count":"2","content":"I am undecided between A and C. You must choose a partitioning key to better distribute the data.\n- The answer A does not partition the data well because it would create very large partitions (\"There are about 10 billion events every day\")\n- The C answer partitions the data well. In this way 10 billions of daily data are partitioned by IP and therefore have a higher cardinality.\nSo my answer is C","poster":"bjit","timestamp":"1633608960.0","comments":[{"comment_id":"191861","content":"It's 10 billion 'Events' every day (not 10 billion day). So don't worry about how many event in a specific date. It's confusing I know!","poster":"AWS_Trial","upvote_count":"2","timestamp":"1634025000.0"}],"comment_id":"190112"},{"timestamp":"1633407660.0","poster":"B47U","content":"I change my Mind it's A as they look for anomalies and it should scan across different patterns of IP events in a 24 hour window","comment_id":"190048","upvote_count":"2"},{"content":"Answer is C, key here is daily they have 10 billion data if we partition on Date each partition has 10 billion records, if we partition by Source IP the partition holds less number of records and all the records irrespective of date falls in that partition and are retrieved faster","upvote_count":"1","timestamp":"1633124520.0","poster":"B47U","comment_id":"190042"},{"upvote_count":"2","timestamp":"1633108800.0","comment_id":"177462","poster":"Karan_Sharma","content":"Option: A, as the data should be partitioned on date so that the query scan is optimized and only scans required date partitions rather than scanning the entire data."},{"timestamp":"1633060020.0","poster":"awsdeveloper","upvote_count":"2","content":"Option C. I think, company wanted to analyse user data, which means, company don't want to analyse with other users, and every use has dedicated IP, so partitioned by IP and sorted by date would be correct","comment_id":"171969"},{"comment_id":"162894","poster":"Nicki1013","timestamp":"1632924420.0","content":"I mean A","upvote_count":"3"},{"content":"My answer is C, ORC and Parquet are columnar format can be processed in parallel has better query performance. In this case the company also need to analyze historical logs dating back 2 years, therefore partitioned by date will be good for query in specific time range to limit the data to be load can has better query performance.","comment_id":"162891","upvote_count":"2","timestamp":"1632808440.0","poster":"Nicki1013"},{"comment_id":"159526","timestamp":"1632797100.0","content":"My answer is C - Sort by Date! and partitioned by IP","upvote_count":"1","poster":"zeronine"},{"timestamp":"1632086640.0","upvote_count":"3","poster":"testtaker3434","content":"To me should be C. Apache Parquet which is better than a compressed JSON. Right?","comment_id":"153574"}],"answer":"A","question_text":"A company that produces network devices has millions of users. Data is collected from the devices on an hourly basis and stored in an Amazon S3 data lake.\nThe company runs analyses on the last 24 hours of data flow logs for abnormality detection and to troubleshoot and resolve user issues. The company also analyzes historical logs dating back 2 years to discover patterns and look for improvement opportunities.\nThe data flow logs contain many metrics, such as date, timestamp, source IP, and target IP. There are about 10 billion events every day.\nHow should this data be stored for optimal performance?","url":"https://www.examtopics.com/discussions/amazon/view/27698-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2020-08-09 13:56:00","answer_images":[],"unix_timestamp":1596974160,"answer_ET":"A","answer_description":"","choices":{"B":"In compressed .csv partitioned by date and sorted by source IP","D":"In compressed nested JSON partitioned by source IP and sorted by date","C":"In Apache Parquet partitioned by source IP and sorted by date","A":"In Apache ORC partitioned by date and sorted by source IP"}},{"id":"t7Bo71tU6HxA0RWCNlhd","choices":{"A":"A unique rule name, a query runtime condition, and an AWS Lambda function to resubmit any failed queries in off hours","B":"A queue name, a unique rule name, and a predicate-based stop condition","C":"A unique rule name, one to three predicates, and an action","D":"A workload name, a unique rule name, and a query runtime-based condition"},"answer_images":[],"answer":"C","question_id":14,"url":"https://www.examtopics.com/discussions/amazon/view/60221-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"timestamp":"2021-08-22 10:47:00","question_text":"A data analytics specialist is setting up workload management in manual mode for an Amazon Redshift environment. The data analytics specialist is defining query monitoring rules to manage system performance and user experience of an Amazon Redshift cluster.\nWhich elements must each query monitoring rule include?","discussion":[{"timestamp":"1636216500.0","comment_id":"446353","content":"C is the answer; textbook question\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html","upvote_count":"10","poster":"ThomasKalva"},{"timestamp":"1682965800.0","content":"C: I passed the test","upvote_count":"3","comment_id":"886592","poster":"pk349"},{"content":"Correct answer is C as a query monitoring rule includes a unique rule name, one to three predicates, and an action. \nTo define a query monitoring rule, you specify the following elements: \n1-A rule name \n2-One or more predicates \n3- An action \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html","timestamp":"1666893480.0","comment_id":"705801","poster":"cloudlearnerhere","upvote_count":"2"},{"comment_id":"637727","poster":"rocky48","upvote_count":"1","content":"Selected Answer: C\nC is correct answer","timestamp":"1658885400.0"},{"comment_id":"493359","timestamp":"1638564120.0","content":"C is correct as per:\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html\nEach rule includes up to three conditions, or predicates, and one action.","poster":"damaldon","upvote_count":"4"},{"upvote_count":"1","timestamp":"1634261280.0","comment_id":"441645","poster":"Marcinha","content":"C is correct answer."},{"upvote_count":"3","comment_id":"433683","content":"C is correct answer\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html","timestamp":"1634170620.0","poster":"SamChan"},{"poster":"jyeon222","upvote_count":"1","comment_id":"432280","timestamp":"1633624260.0","content":"help me.. what is answer?"},{"timestamp":"1633412580.0","poster":"taizo777","comment_id":"429147","upvote_count":"2","content":"I agree C is answer"},{"poster":"taizo777","comment_id":"429137","upvote_count":"1","content":"I think A is right answer","timestamp":"1633030680.0","comments":[{"timestamp":"1633489320.0","poster":"taizo777","comment_id":"429160","upvote_count":"1","content":"sorry mistake"}]}],"answers_community":["C (100%)"],"topic":"1","exam_id":20,"question_images":[],"unix_timestamp":1629622020,"answer_ET":"C","answer_description":"Reference:\nhttps://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html"},{"id":"7ATcNssTz3V0nvGlUYcr","url":"https://www.examtopics.com/discussions/amazon/view/60224-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_text":"A market data company aggregates external data sources to create a detailed view of product consumption in different countries. The company wants to sell this data to external parties through a subscription. To achieve this goal, the company needs to make its data securely available to external parties who are also AWS users.\nWhat should the company do to meet these requirements with the LEAST operational overhead?","isMC":true,"answer_description":"","answer_images":[],"timestamp":"2021-08-22 10:52:00","exam_id":20,"discussion":[{"upvote_count":"7","comment_id":"433693","comments":[{"timestamp":"1638639120.0","comments":[{"timestamp":"1640142840.0","comment_id":"506634","content":"Presigned URL can be generated, the assets are not shared using that. it works using subscription model","upvote_count":"1","poster":"lakediver"}],"poster":"TestGangster123","comment_id":"493855","upvote_count":"3","content":"Pressigned URL can be generated https://boto3.amazonaws.com/v1/documentation/api/1.18.43/reference/services/dataexchange.html#DataExchange.Client.generate_presigned_url"}],"content":"Between C or D, I don't think can generate pre-signed URL from Data Exchange...\nI'll choose D, however there're no such thing called AWS Data Exchange sharing wizard\nPerhaps this question is outdated\nHere's the standard flow of Providing data products on AWS Data Exchange\nhttps://docs.aws.amazon.com/es_es/data-exchange/latest/userguide/providing-data-sets.html","timestamp":"1633327680.0","poster":"SamChan"},{"upvote_count":"5","comments":[{"timestamp":"1666893600.0","upvote_count":"1","comments":[{"upvote_count":"2","content":"AWS Data Exchange now gives customers an easy way to set up export jobs upon subscribing to products. Instead of navigating to separate screens, subscribers can use the AWS Data Exchange console to configure export jobs that will begin automatically after their subscription is completed. This functionality reduces friction and time to value for customers. For subscribers just getting started on AWS Data Exchange, setting up data exports to Amazon S3 is a critical first step towards downstream analysis in a variety of AWS services.","poster":"cloudlearnerhere","timestamp":"1666893600.0","comment_id":"705809"}],"comment_id":"705807","poster":"cloudlearnerhere","content":"AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud. Qualified data providers include category-leading brands such as Reuters, who curate data from over 2.2 million unique news stories per year in multiple languages; Change Healthcare, who process and anonymize more than 14 billion healthcare transactions and $1 trillion in claims annually; Dun & Bradstreet, who maintain a database of more than 330 million global business records; and Foursquare, whose location data is derived from 220 million unique consumers and includes more than 60 million global commercial venues."}],"poster":"cloudlearnerhere","timestamp":"1666893600.0","comment_id":"705806","content":"Correct answer is D as AWS Data Exchange makes it easy to find, subscribe to, and use third-party data in the cloud with least operational overhead. \n\nOption A is wrong as S3 Presigned URLs provide access to anyone who has access to the URL. As the customers are already AWS users, the access can be provided securely through Data exchange. \n\nOption B is wrong as the only recommended use case for the bucket ACL is to grant write permission to the S3 Log Delivery group to write access log objects to the bucket. \n\nOption C is wrong as AWS Data Exchange does not support resigned URLs."},{"content":"D: I passed the test","timestamp":"1682965800.0","poster":"pk349","upvote_count":"1","comment_id":"886593"},{"upvote_count":"1","timestamp":"1669799400.0","comment_id":"731258","poster":"dhyuk","content":"i think its A"},{"timestamp":"1658985420.0","poster":"rocky48","content":"Selected Answer: D\nSelected Answer: D","comment_id":"638474","upvote_count":"1"},{"timestamp":"1653207360.0","content":"Selected Answer: D\n... based on a subscription model ... -> Must be Data Exchange Service -> ... also AWS Customer... -> no need for a presigned url > so should be D","poster":"f4bi4n","upvote_count":"2","comment_id":"605283"},{"timestamp":"1647738060.0","upvote_count":"2","comment_id":"571334","poster":"moon2351","content":"Selected Answer: D\nI think Answer is D."},{"comment_id":"556758","content":"C: But I think it's called the \"Publish new product wizard\". See step 5 in https://docs.aws.amazon.com/data-exchange/latest/userguide/publishing-products.html#publish-products","poster":"wolfsong","timestamp":"1645891200.0","upvote_count":"1","comments":[{"upvote_count":"1","poster":"wolfsong","timestamp":"1645891440.0","content":"I mean D","comment_id":"556763"}]},{"upvote_count":"1","poster":"npt","comments":[{"comment_id":"509390","upvote_count":"1","timestamp":"1640487900.0","poster":"npt","content":"Change to D\nPreSignedURL for S3, so it can be A\nBut the question mentions the least operational overhead, so it's D. There is AWS Data Exchange post-subscription wizard, maybe it's the sharing wizard in the answer D\nhttps://aws.amazon.com/about-aws/whats-new/2021/04/aws-data-exchange-launches-post-subscription-wizard-to-configure-revision-exports-to-amazon-s3/"}],"content":"C\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html","timestamp":"1640390700.0","comment_id":"508906"},{"upvote_count":"1","comment_id":"503832","timestamp":"1639763880.0","poster":"arun004","content":"i guess both C and D will work but question says least operational so that i chose D since C need code changes to generate preassigned url for data exchange. pls correct me if i am wrong ."},{"content":"I bend towards D\nhttps://docs.aws.amazon.com/data-exchange/latest/userguide/publishing-products.html\nTo create a data set product on Data Exchange: \nOpen your web browser and go to the AWS Data Exchange console.\nOn the left side navigation pane, under Publish data, choose Owned data sets.\nIn Owned data sets, choose Create data set to open the Data set creation steps wizard***.","poster":"tobsam","upvote_count":"1","timestamp":"1639444140.0","comment_id":"500978"},{"upvote_count":"4","comment_id":"494977","timestamp":"1638778020.0","poster":"lakediver","content":"I will go with A\nData Exchange doesn't support pre-signed URL, neither there is any wizard to share data.\nThis is the way to share data using Data exchange \nhttps://docs.aws.amazon.com/data-exchange/latest/userguide/publishing-products.html#publish-data-product"},{"comment_id":"490408","comments":[{"content":"why you say that?","poster":"vasi_9969","upvote_count":"1","comment_id":"495315","timestamp":"1638811680.0"}],"content":"Answer is A","poster":"Thiya","upvote_count":"3","timestamp":"1638246540.0"},{"content":"In the blog for exchange, they do talk about some wizard but not really some exchange wizard feature. I would still go with D. Although, presigned URLs sounds like a right way to share data with external aws account securely specially when you're selling content. Answer is actually C or D, need more clarifications.","comment_id":"446358","poster":"ThomasKalva","upvote_count":"1","timestamp":"1636060560.0"},{"upvote_count":"1","comment_id":"429146","content":"I think C is right answer","poster":"taizo777","timestamp":"1632332400.0"}],"topic":"1","answer":"D","unix_timestamp":1629622320,"choices":{"D":"Upload the data to AWS Data Exchange for storage. Share the data by using the AWS Data Exchange sharing wizard.","B":"Store the data in Amazon S3. Share the data by using S3 bucket ACLs.","A":"Store the data in Amazon S3. Share the data by using presigned URLs for security.","C":"Upload the data to AWS Data Exchange for storage. Share the data by using presigned URLs for security."},"answers_community":["D (100%)"],"question_id":15,"question_images":[],"answer_ET":"D"}],"exam":{"name":"AWS Certified Data Analytics - Specialty","lastUpdated":"11 Apr 2025","isImplemented":true,"id":20,"isMCOnly":true,"isBeta":false,"numberOfQuestions":164,"provider":"Amazon"},"currentPage":3},"__N_SSP":true}