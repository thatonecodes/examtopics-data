{"pageProps":{"questions":[{"id":"Knrlsy0VInQopsoEbHt4","answer_description":"","answer_images":[],"unix_timestamp":1662198840,"answers_community":["D (100%)"],"question_images":[],"question_text":"A company deploys a new web application. As part of the setup, the company configures AWS WAF to log to Amazon S3 through Amazon Kinesis Data Firehose.\nThe company develops an Amazon Athena query that runs once daily to return AWS WAF log data from the previous 24 hours. The volume of daily logs is constant. However, over time, the same query is taking more time to run.\nA solutions architect needs to design a solution to prevent the query time from continuing to increase. The solution must minimize operational overhead.\nWhich solution will meet these requirements?","answer":"D","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/79738-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":946,"discussion":[{"upvote_count":"1","poster":"masetromain","content":"Selected Answer: D\nD. Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions.\n\nTo prevent the query time from continuing to increase and minimize operational overhead, a solutions architect should modify the Kinesis Data Firehose configuration and the Athena table definition to partition the data by date and time. This will allow Athena to only scan the relevant partitions, reducing the amount of data that needs to be scanned and improving query performance. Additionally, the architect should change the Athena query to view the relevant partitions, ensuring that the query is only scanning the necessary data.","comment_id":"773734","comments":[{"content":"Option A: Create an AWS Lambda function that consolidates each day's AWS WAF logs into one log file, this would reduce the number of files that Athena needs to scan, but it does not address the issue of the increasing query time.\n\nOption B: Reduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket each day, this would reduce the number of files that Athena needs to scan, but it does not address the issue of the increasing query time.\n\nOption C: Update the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and time. Create external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the data source. This option is more complex and requires additional resources and maintenance,","comment_id":"773735","upvote_count":"1","poster":"masetromain","timestamp":"1673543100.0"}],"timestamp":"1673543040.0"},{"comment_id":"753272","poster":"Kende","upvote_count":"1","content":"Selected Answer: D\n\"D\" is the one.","timestamp":"1671712320.0"},{"upvote_count":"3","timestamp":"1662750180.0","comment_id":"664896","content":"Selected Answer: D\nD https://docs.aws.amazon.com/athena/latest/ug/partition-projection-kinesis-firehose-example.html","poster":"AwsBRFan"},{"content":"Selected Answer: D\nIt is D. The solution is to partition the data.","comment_id":"662542","poster":"cale","timestamp":"1662557640.0","upvote_count":"2"},{"timestamp":"1662198840.0","upvote_count":"3","content":"I will go for D\nKey word - minimum operational overhead","poster":"SGES","comment_id":"658307"}],"answer_ET":"D","timestamp":"2022-09-03 11:54:00","isMC":true,"exam_id":32,"choices":{"B":"Reduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket each day.","A":"Create an AWS Lambda function that consolidates each days AWS WAF logs into one log file.","D":"Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions.","C":"Update the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and time. Create external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the data source."}},{"id":"Y2LlayPfkNSSzWyPzp39","isMC":true,"discussion":[{"comment_id":"662555","comments":[{"content":"I meant to say Aurora MySQL Serverless.","timestamp":"1662558300.0","comment_id":"662556","upvote_count":"2","poster":"cale"}],"content":"Selected Answer: ACF\nA, C, F looks right as these are all using managed AWS services i.e. S3, Fargate, Aurora (MySQL).","upvote_count":"9","poster":"cale","timestamp":"1662558180.0"},{"comment_id":"773741","comments":[{"poster":"masetromain","content":"Option F: Replatforming the database to Amazon Aurora MySQL Serverless eliminates the need for server maintenance, as it is a fully-managed service. Aurora Serverless automatically starts, scales, and stops database capacity based on application usage, so the company would not have to worry about provisioning, scaling, or patching the\n\nOption D: Creating an API layer with Amazon API Gateway and rehosting the microservices on Amazon Elastic Container Service (Amazon ECS) containers would work, but Fargate is more serverless way of running containerized applications.\n\nOption E: Replatforming the database to Amazon RDS for MySQL eliminates the need for maintenance on the database servers and allows the company to easily scale the database as needed, but it would not be as serverless as using Aurora Serverless.","upvote_count":"1","comment_id":"773743","timestamp":"1673543820.0"}],"timestamp":"1673543760.0","content":"Selected Answer: ACF\nOption A: Hosting the web application on Amazon S3 with Amazon Cognito identity pools (federated identities) with SAML for authentication and authorization is a good option as it allows the company to use their existing Active Directory for user management, and secure access to the application.\n\nOption C: Creating an API layer with Amazon API Gateway and rehosting the microservices on AWS Fargate containers would allow the company to securely connect to the microservices and eliminates the need for maintenance on the application server.\n\nOption F: Replatforming the database to Amazon Aurora MySQL Serverless eliminates the need for server maintenance, as it is a fully-managed service. Aurora Serverless automatically starts, scales, and stops database capacity based on application usage, so the company would not have to worry about provisioning, scaling, or patching the","poster":"masetromain","upvote_count":"2"},{"timestamp":"1667653980.0","comment_id":"711766","poster":"Ni_yot","upvote_count":"1","content":"Selected Answer: ACF\nYes ACF is right."},{"upvote_count":"2","timestamp":"1663682940.0","content":"Selected Answer: ACF\nAll managed services","comment_id":"674241","poster":"redipa"}],"answer":"ACF","choices":{"F":"Replatform the database to Amazon Aurora MySQL Serverless.","C":"Create an API layer with Amazon API Gateway. Rehost the microservices on AWS Fargate containers.","E":"Replatform the database to Amazon RDS for MySQL.","D":"Create an API layer with Amazon API Gateway. Rehost the microservices on Amazon Elastic Container Service (Amazon ECS) containers.","A":"Host the web application on Amazon S3. Use Amazon Cognito identity pools (federated identities) with SAML for authentication and authorization.","B":"Host the web application on Amazon EC2 with Auto Scaling. Use Amazon Cognito federation and Login with Amazon for authentication and authorization."},"question_text":"A company manages an on-premises JavaScript front-end web application. The application is hosted on two servers secured with a corporate Active Directory.\nThe application calls a set of Java-based microservices on an application server and stores data in a clustered MySQL database. The application is heavily used during the day on weekdays. It is lightly used during the evenings and weekends.\nDaytime traffic to the application has increased rapidly, and reliability has diminished as a result. The company wants to migrate the application to AWS with a solution that eliminates the need for server maintenance, with an API to securely connect to the microservices.\nWhich combination of actions will meet these requirements? (Choose three.)","timestamp":"2022-09-07 15:43:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/80929-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"ACF","unix_timestamp":1662558180,"topic":"1","answers_community":["ACF (100%)"],"exam_id":32,"question_id":947,"answer_images":[],"answer_description":""},{"id":"oeGB6o6JV0vjsaUIRoTk","answers_community":["A (68%)","C (23%)","9%"],"discussion":[{"timestamp":"1729076640.0","comment_id":"1298669","poster":"Sin_Dan","content":"Selected Answer: A\nThe correct answer is A\nS3 intelligent-tiering for cost-effectiveness\nFSx lustre is a requirement, because the application needs high performance computing.","upvote_count":"1"},{"poster":"SkyZeroZx","comment_id":"934844","timestamp":"1687819620.0","content":"Selected Answer: A\nkeyword = Intelligent-Tiering && Lazy Load \nThen A","upvote_count":"2"},{"comment_id":"792004","upvote_count":"2","timestamp":"1675021440.0","poster":"zozza2023","comments":[{"poster":"Sin_Dan","upvote_count":"1","comment_id":"1298667","timestamp":"1729076520.0","content":"S3 Standard is more expensive than Intelligent-tiering\nBatch loading in FSx for lustre is less cost-effective than lazy loading"}],"content":"Selected Answer: C\ncan someone explian why it is not C\nA seems KO for me as inteligent tiering"},{"content":"Selected Answer: A\nThe file system must provide high performance access","comment_id":"734111","upvote_count":"2","poster":"SureNot","timestamp":"1670023740.0"},{"upvote_count":"2","timestamp":"1668045840.0","content":"'A'\n\nExplained well here:\nhttps://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-between-amazon-fsx-for-lustre-and-amazon-s3/","poster":"sjpd10","comment_id":"714866"},{"content":"Where I can't choose A is \"Delete the file system\" at the end... wait... don't we need that?","timestamp":"1665439020.0","poster":"joanneli77","comments":[{"upvote_count":"1","content":"the job runs once a month for 72 hours, Why do you need the file system to be up the whole month? the file system will be created again BEFORE the job start.","poster":"superuser784","timestamp":"1667874240.0","comment_id":"713429"}],"upvote_count":"2","comment_id":"691530"},{"content":"Selected Answer: D\nThere is no mentioned of High Performance Computing with low latency requirement.. thus Amazon FSx for Lustre should out..\n\nInstead another ways to access S3 object is via File Storage Gateway. Just need to install a file storage gateway and points to existing S3 storage... present either NFS or SMB to appl server.","poster":"skywalker","upvote_count":"2","timestamp":"1665279660.0","comment_id":"689836"},{"poster":"dcdcdc3","content":"Selected Answer: A\nA described here\nhttps://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-between-amazon-fsx-for-lustre-and-amazon-s3/\n\nwhy not C? there is no \"batch loading\"","upvote_count":"4","timestamp":"1665196380.0","comment_id":"688989"},{"poster":"JohnPi","timestamp":"1664793000.0","content":"Selected Answer: A\nAmazon FSx will import the objects in our S3 bucket as files, and “lazy-load” the file contents from S3 when we first access the file.","comment_id":"685392","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: C\nWhy not C? It looks more appropriate","comment_id":"682464","poster":"akash_it","timestamp":"1664438700.0"},{"timestamp":"1664272680.0","poster":"Biden","content":"Selected Answer: C\nWhats the need for Intelligent Tiering ? Standard S3 should suffice, hence C","comment_id":"680603","upvote_count":"2"},{"content":"D could not be answer because Storage Gateway generally used for file migration from on-prem to AWS, this is not the requirement here","timestamp":"1662993960.0","poster":"Ally26","comment_id":"667114","upvote_count":"1"},{"poster":"pixepe","content":"Answer is A, In reality, we need to select from A & D only.\n\nA & D comparison:\nA do have S3 Intelligent Tiering (remember it's 200 GB) which should matter, Which D doesn't have - A wins\nHigh Performance due to FSx for Lusture (requirement - file system must provide high performance acces) - A wins\n\nUnsure but File Gateway may be less costly (than FSx lusture) - May be D wins here\n\nAfter above points, Overall A is winner","timestamp":"1662887220.0","comments":[{"timestamp":"1666540860.0","poster":"fdoxxx","upvote_count":"1","comment_id":"702299","content":"200 TB..."}],"comment_id":"665969","upvote_count":"4"},{"timestamp":"1662750660.0","poster":"AwsBRFan","content":"Selected Answer: A\nA seems the right one to me","comment_id":"664904","upvote_count":"4"},{"timestamp":"1662726720.0","poster":"Rocketeer","upvote_count":"3","content":"Need \"The file system must provide high performance access to the needed data ...\" Hence my choice is A","comment_id":"664557"},{"timestamp":"1662573120.0","comments":[{"poster":"Pazhuvil","timestamp":"1662740940.0","content":"But need high performance access for 72 hours each month. So A seems to be reasonable.","upvote_count":"2","comment_id":"664801","comments":[{"content":"file gateway is slower than FSx... - A is a good choice","comment_id":"702301","upvote_count":"1","poster":"fdoxxx","timestamp":"1666540980.0"}]}],"content":"D with file gateway (software appliance) and S3 seems to be the cheapest","poster":"ArreRaja","comment_id":"662769","upvote_count":"2"}],"question_text":"A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file system also runs on several EC2 instances that store 200 TB of data. The application reads and modifies the data on the shared file system and generates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. The compute instances scale in an Auto Scaling group, but the instances that host the shared the system run continuously. The compute and storage instances are all in the same AWS Region.\nA solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access to the needed data for the duration of the 72-hour run.\nWhich solution will provide the LARGEST overall cost reduction while meeting these requirements?","exam_id":32,"isMC":true,"timestamp":"2022-09-07 19:52:00","answer":"A","choices":{"C":"Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.","B":"Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete.","D":"Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete.","A":"Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete."},"question_id":948,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/80991-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","answer_ET":"A","answer_images":[],"unix_timestamp":1662573120,"topic":"1"},{"id":"R7GUGHqP0aGGorfcal7m","isMC":true,"discussion":[{"comment_id":"675627","poster":"gnandam","upvote_count":"6","content":"A: Is right answer - https://aws.amazon.com/about-aws/whats-new/2020/11/announcing-amazon-mq-rabbitmq/\nD: SQS need backend application to be refactored\nB/C : not feasible","timestamp":"1663807500.0"},{"comment_id":"1295062","content":"Selected Answer: A\nSetting up Amazon MQ allows you to use a managed messaging service that supports RabbitMQ protocols, enabling a smooth transition from on-premises to the cloud without changing the messaging architecture significantly.","upvote_count":"1","timestamp":"1728465720.0","poster":"nimbus_00"},{"content":"A : correct\n\nB : The company does not want to make any major changes to the application., so AWS Lambda is wrong.\nC : Install Kuhernetes on a fleet of different EC2 instances is more operational overhead\nD: Amazon Simple Queue Service (Amazon SQS) queue is not suitable for RabbitMQ","timestamp":"1681982580.0","upvote_count":"2","comment_id":"875444","poster":"yama234"},{"timestamp":"1666108260.0","content":"Selected Answer: A\n\"A company is refactoring...\" looks more like a replatforming","poster":"wassb","upvote_count":"2","comment_id":"698332"},{"poster":"JohnPi","timestamp":"1665642540.0","upvote_count":"1","content":"Selected Answer: A\ndoes not want to make any major changes to the application","comment_id":"693664"},{"content":"Selected Answer: A\ndefinitely A","timestamp":"1664960940.0","upvote_count":"2","comment_id":"686734","poster":"JayF88"},{"poster":"akash_it","timestamp":"1664438820.0","upvote_count":"3","comment_id":"682465","content":"Selected Answer: A\nA is correct"},{"content":"A: Amazon MQ is suitable for RabbitMQ","poster":"parayan","upvote_count":"1","comment_id":"682199","timestamp":"1664408100.0"},{"poster":"astalavista1","content":"Selected Answer: D\nIt's A or D but key for me here is LEAST Overhead, so will go for SQS over Amazon MQ. But will the application need a rewrite with MQ chosen?","upvote_count":"1","comments":[{"content":"This would require a rewrite if you choose SQS. Better to choose what's more compatible with on-premises solution, which is Amazon MQ","comment_id":"683855","timestamp":"1664573580.0","upvote_count":"3","poster":"sb333"}],"timestamp":"1663788060.0","comment_id":"675458"}],"answer":"A","choices":{"A":"Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.","D":"Create an AMI of the web server VM Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue, Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.","C":"Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kuhernetes on a fleet of different EC2 instances to host the order-processing backend.","B":"Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend."},"question_text":"A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs.\nRabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?","timestamp":"2022-09-21 21:21:00","question_images":[],"unix_timestamp":1663788060,"url":"https://www.examtopics.com/discussions/amazon/view/83132-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"A","topic":"1","answers_community":["A (90%)","10%"],"exam_id":32,"question_id":949,"answer_images":[],"answer_description":""},{"id":"D9I4HR8foWwP5n3gt5p7","url":"https://www.examtopics.com/discussions/amazon/view/82131-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2022-09-14 12:46:00","answers_community":["C (100%)"],"answer":"C","choices":{"C":"In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.","D":"In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account's security group by using a nested security group reference of \"<transit-account-id>/sg-1a2b3c4d\".","B":"Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected.","A":"Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges."},"question_text":"A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a transit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured between all of the company's global offices and the transit account. The company has AWS Config enabled on all of its accounts.\nThe company's networking team needs to centrally manage a list of internal IP address ranges that belong to the global offices. Developers will reference this list to gain access to their applications securely.\nWhich solution meets these requirements with the LEAST amount of operational overhead?","topic":"1","isMC":true,"question_images":[],"exam_id":32,"unix_timestamp":1663152360,"question_id":950,"answer_ET":"C","answer_images":[],"discussion":[{"comment_id":"674255","poster":"redipa","timestamp":"1663683960.0","content":"Selected Answer: C\nC - Prefix list\nCustomer-managed prefix lists — Sets of IP address ranges that you define and manage. You can share your prefix list with other AWS accounts, enabling those accounts to reference the prefix list in their own resources.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html","upvote_count":"8"},{"upvote_count":"1","poster":"Jonfernz","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/vpc/latest/userguide/sharing-managed-prefix-lists.html","comment_id":"702530","timestamp":"1666567740.0"},{"timestamp":"1663152360.0","upvote_count":"4","poster":"Cloudxie","comment_id":"668902","content":"https://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html"}],"answer_description":""}],"exam":{"isImplemented":true,"provider":"Amazon","isMCOnly":false,"id":32,"numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","isBeta":false,"name":"AWS Certified Solutions Architect - Professional"},"currentPage":190},"__N_SSP":true}