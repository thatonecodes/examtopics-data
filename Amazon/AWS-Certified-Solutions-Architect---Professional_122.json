{"pageProps":{"questions":[{"id":"fEDdvNF4owdpZk7mGPV8","answer":"D","choices":{"A":"Use an SCP in Organizations to implement a deny list of AWS servers. Apply this SCP at the level. For any specific exceptions for an OU, create a new SCP for that OU and add the required AWS services to the allow list.","D":"Use an SCP in Organizations to implement an allow list of AWS services. Apply this SCP at the root level. Remove the default AWS managed SCP from the root level and all OU levels. For any specific exceptions for an OU, modify the SCP attached to that OU, and add the required AWS services to the allow list.","B":"Use an SCP in Organizations to implement a deny list of AWS service. Apply this SCP at the root level and each OU. Remove the default AWS managed SCP from the root level and all OU levels. For any specific exceptions, modify the SCP attached to that OU, and add the required AWS services to the allow list.","C":"Use an SCP in Organizations to implement a deny list of AWS service. Apply this SCP at each OU level. Leave the default AWS managed SCP at the root level. For any specific executions for an OU, create a new SCP for that OU."},"answers_community":["D (56%)","C (40%)","4%"],"isMC":true,"answer_description":"","answer_images":[],"question_images":[],"discussion":[{"comment_id":"255779","upvote_count":"25","content":"Correct answer is C. When you use a Deny list, you cannot explicitly allow access to services at OU or account levels. You need to explicitly deny access to services and that's why the term deny list. By default, all services are explicitly allowed starting at the root level. So you need to explicitly create an SCP at each OU level where you need to implement the control policy of denying access to services. In exceptional circumstances on a use case basis, you need to allow access to the services that already have an allow access from root to this OU level where you are creating an exception. Only C satisfies this criteria. D is not correct because it doesn't create an SCP that allow access at all level from the OU in question upto the root level. So even if you create an SCP that allows access to a service, access won't be granted as it's not been explicitly allowed at all level above this OU.","comments":[{"content":"Here the correct answer must be D.\n\n1 - The allowed rights work with as the intersection of the rights given by SCP at root, OU and IAM Policies. Therefore if you implement on a SCP at OU level a Deny of an AWS Server you then wish to grant, the only option is to Modify your SCP, which rules out answers A and C which recommend you to Create a new SCP\n\n2 - In answers A, B and C it is suggested to Implement an Explicit Deny, and for options B and C, this Deny is at Root Level. It is not possible with this strategy to allow exceptions with this configurations because Explicit Deny takes precedence over Explicit Allow, then Implicit Deny, then Implicit Allow. The only way to address this problem is to set Implicit Deny at the Root Level, so then with our Explicit Allow on SCP at OU Level, it overrides the Implicit Deny, which is what is proposed in Answer D : it is an Allow list of AWS Services not including the restricted AWS Servers which are Implicitly Denied.","timestamp":"1635704400.0","comments":[{"upvote_count":"2","content":"Your explanation is not correct. D is wrong.\n\nUsing Allow List Strategy, to allow a permission, SCPs with allow statement must be added to the account and every OU above it including root. Every SCP in the hierarchy must explicitly allow the APIs you want to use.\nExplicit allow at a lower level of organization hierarchy cannot overwrite the implicit deny at a higher level.","poster":"tomosabc1","timestamp":"1664967060.0","comment_id":"686783"}],"comment_id":"424151","upvote_count":"6","poster":"tekkart"}],"poster":"Bulti","timestamp":"1633556520.0"},{"comments":[{"upvote_count":"2","content":"D would have been the answer if 'These business units may need to use different AWS services' was not required.\n\nWith D we are giving the same AWS Services to all the units.","poster":"cloudgc","comments":[{"timestamp":"1635639840.0","comment_id":"411427","poster":"aws_arn_name","content":"No, D state that \"modify the SCP attached to that OU\" not the root SCP","upvote_count":"4"}],"comment_id":"237845","timestamp":"1633331760.0"}],"content":"Prohibit all AWS servers (should be services i guess) can only be achieved by whitelisting method. This means that you will have to remove the AWS managed SCP from the root. \nWhitelist SCP on the root of your organisation makes sure that any new account will apply these settings. SCP never grants access but can allow you to make use of AWS services. \nWith that baseline set, granting a new set of AWS services in a separate SCP attaching it to the new account in your organisation complies here for the minimal operational overhead.\n\nonly D will statisfy.\n\nOne more negative for C. once you implement a deny on a toplevel. it will override any allow in a child OU. not that it is stated within this question. but with that in mind that it could be the case, whitelisting makes more sense for me.","timestamp":"1633192800.0","poster":"dutchy1988","upvote_count":"22","comment_id":"234866"},{"upvote_count":"1","content":"Selected Answer: C\nAnswer is C. In D, the allow list on the root level implicitly denies all other services. You can try to allow what you want on OU level, it will never overrule the implicit deny on root level. With C, you actually use the default allow from AWS and specify any restrictions on OU level.","comment_id":"997725","poster":"ramonipony","timestamp":"1693754700.0"},{"poster":"walkwolf3","timestamp":"1686360120.0","comment_id":"919725","upvote_count":"1","content":"Both C and D can meet the requirements, but D is more efficient.\n\nFor answer C, SCP needs to be applied on EACH OU level. While for answer D, SCP is applied ONLY on root level.\n\nFrom another angle, which list is longer, deny or allow? For most of accounts, we just give them a few basic accesses. I would say allow list is shorter.\n\nIMHO, answer is D."},{"comment_id":"918224","poster":"claymannain","upvote_count":"1","timestamp":"1686225660.0","content":"C\n\nWhen there is an implicit deny at the root of an AWS organization and an allow at an OU, the allow policy will take precedence. This means that users in the OU will be able to access the resources that are allowed by the policy, even though there is an implicit deny at the root.\n\nFor example, if the root of the organization has an implicit deny policy that prohibits the creation of AWS servers, and an OU has an allow policy that allows the creation of AWS servers, users in the OU will be able to create AWS servers.\n\nIt is important to note that the implicit deny policy at the root will still apply to resources that are not explicitly allowed by the allow policy at the OU. For example, if the root has an implicit deny policy that prohibits the use of the AWS Management Console, and the OU has an allow policy that allows the use of the AWS Management Console for a specific service, users in the OU will still be able to use the AWS Management Console for that service, but they will not be able to use the AWS Management Console for any other services."},{"poster":"Jesuisleon","comment_id":"902735","timestamp":"1684609920.0","upvote_count":"1","content":"Selected Answer: D\nD and C both right , but I think D is better since it requires less effort than C."},{"comment_id":"871960","content":"Selected Answer: C\nCan't be D..\nD says: \"Apply SCP the allow specific list of services to the root, \nThen, For any specific exceptions for an OU, modify the SCP attached to that OU, and add the required AWS services to the allow list.\n\nRead it again... Did you see they mentioned \"add Deny\" to the SCP in OU level? üòÇ","poster":"dev112233xx","timestamp":"1681664400.0","upvote_count":"2"},{"timestamp":"1681656540.0","comment_id":"871857","upvote_count":"1","poster":"dev112233xx","comments":[{"upvote_count":"1","comment_id":"871942","timestamp":"1681663140.0","content":"\"Deny statements require less maintenance, because you don't need to update them when AWS adds new services. Deny statements usually use less space, thus making it easier to stay within the maximum size for SCPs\"\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html","poster":"dev112233xx"}],"content":"Selected Answer: C\nC is correct\nD is totally wrong and can't minimal operational overhead"},{"upvote_count":"1","content":"If a deny list of AWS services is applied at the root level SCP and specific exceptions are allowed at the OU level SCP in the AWS Organizations hierarchy, then the SCP applied at the OU level will take precedence. This means that the AWS services that are allowed in the OU level SCP will be allowed, while the rest of the AWS services will be prohibited as per the deny list applied at the root level SCP.\n\nWhen multiple SCPs are applied, AWS Organizations evaluates them in the order of precedence, which is determined by the level at which the SCP is applied. The SCP applied at the highest level takes precedence over the SCPs applied at lower levels. If a lower-level SCP explicitly allows an action, that action is allowed, even if higher-level SCPs would have otherwise prohibited the action.\n\nIn this case, the allow list of AWS services at the OU level SCP will take precedence over the deny list of AWS services at the root level SCP, allowing the specified AWS services for that OU, while prohibiting all other AWS services.","timestamp":"1675925220.0","comment_id":"802894","poster":"unknownUser22952"},{"poster":"evargasbrz","content":"Selected Answer: D\nYou only have access to services because of the default AWS managed SCP, so if you remove that, you don't need to explicitly deny access.","timestamp":"1672264260.0","comment_id":"760367","upvote_count":"1"},{"timestamp":"1670239620.0","comments":[{"comment_id":"920180","poster":"Jesuisleon","timestamp":"1686414960.0","upvote_count":"1","content":"You have a very strong justification"}],"content":"Selected Answer: D\nC is wrong. \n1.Attach Deny SAP to all OUs.\n2. Attach Allow SCP to the OU.\nThey don't say to Detach Deny SCP from the OU - so explicit Deny will be here and win!","comment_id":"735877","poster":"SureNot","upvote_count":"2"},{"timestamp":"1668382740.0","poster":"Relaxeasy","upvote_count":"1","content":"Selected Answer: C\nC makes more sense","comment_id":"717578"},{"timestamp":"1664967600.0","upvote_count":"1","comment_id":"686799","poster":"tomosabc1","content":"Selected Answer: C\nC is correct. For explanation, please refer to Bulti's answer."},{"timestamp":"1663872720.0","comment_id":"676488","poster":"dcdcdc3","content":"Selected Answer: A\nD Cannot work if SCP is not attached to Every Level of OU including root. C can work but is too much overhead;\nA may have incomplete wording but as is, it is working solution, as the SCP is attached \"at the Level\". In A, it nowhere says to \"attach deny to root level\".\nHere is the whole text for A:\n\"A. Use an SCP in Organizations to implement a deny list of AWS servers. Apply this SCP at the level. For any specific exceptions for an OU, create a new SCP for that OU and add the required AWS services to the allow list.\"\nThe New SCP will not have a Deny for specific service and will have an Allow statement..","upvote_count":"1"},{"comments":[{"upvote_count":"1","poster":"aqiao","comment_id":"660618","timestamp":"1662420240.0","content":"Here is the official statement:\nIf an action is blocked by a Deny statement, then all OUs and accounts affected by that SCP are denied access to that action. An SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level. SCPs can only filter; they never add permissions.\nYou can get the details here :https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html\nSo only D satisfied. Actually there is no need to remove default permission on root OU, like key point 3 said, it will be overrode by explicit allow lists"}],"upvote_count":"3","timestamp":"1662420180.0","comment_id":"660616","content":"Selected Answer: D\nThree key points in SCP:\n1 Explicit deny actions has the highest priority;\n2 Accounts under sub OU inherit the parent OU permissions;\n3 Explicit allow actions overrides default FullAWSAccess on root organizations;\n4 Once a deny actions applied on a some OU, even an explicit allow action added on sub OU, all the accounts directly under this OU and its sub OU have no permission to perform the action.","poster":"aqiao"},{"poster":"gnic","timestamp":"1661773620.0","upvote_count":"3","content":"Selected Answer: D\nit's D","comment_id":"653465"},{"timestamp":"1660487280.0","upvote_count":"2","comment_id":"646778","content":"Selected Answer: D\nMinimal operational overhead compared to C","poster":"Harithareddynn"},{"comment_id":"637320","content":"C.\ni chose C as the first answer","timestamp":"1658829780.0","poster":"CloudHandsOn","upvote_count":"1"},{"timestamp":"1656942480.0","comment_id":"627009","upvote_count":"2","content":"Selected Answer: C\nA - will not work as a deny is on the root level so no specification - there is no way to add the permission back lower in the hierarchy\nB - Same for A\nC - Can work - deny on OU level and leave AWS Full access to all accounts at root (I can only assume we also leave for all OU levels) - for exceptions create a new deny SCP and replace it - however it has an operational overhead as it requires attaching it to every OU and every new OU\nD- cannot work if FullAccess is replaced with specific access SCP it should be applied to all level including OU and account levels (intersection).\n\nOverall all answers are not fully complete but I have to go with C","poster":"Enigmaaaaaa"},{"timestamp":"1652962020.0","upvote_count":"2","poster":"bobsmith2000","comment_id":"603872","content":"Selected Answer: D\nBoth C and D will work.\nBut we need a solution with the LAST operational overhead.\n\nC) There's no shared policy. So every time we must edit the OU SCP\nD) We must specify common resources on the root level with an allow list (allow explicitly with implicit deny for the rest), and then we are able to both deny or allow any specific services: additional allow will be merged with the root SCP, and explicit deny will override the root's allow. So that we make changes only if needed, not every time when create an account"},{"comment_id":"538456","content":"Correct answer is C.\nWe need allow at the root as per the question says \" a variety of services needs to be used by business units\"\nDeny SCP is being applied on all OU's in option C.\nFor any specific service allow for any OU, we can replace or edit the OU SCP. \nIt has the least overhead.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_denylist","poster":"Ishu_awsguy","timestamp":"1643790180.0","upvote_count":"1"},{"comment_id":"507982","content":"C says create new SCP for Exception Allow after it have been explicitly Denied...Doesn't make sense if you already have an Deny it is gonna take precedence over explicit Allow in newly created SCP; Answer is D.","timestamp":"1640273880.0","comments":[{"timestamp":"1643790360.0","poster":"Ishu_awsguy","comment_id":"538457","upvote_count":"1","content":"the SCP is being applied on OU level.\nNo precedence. \nprecedence is for default allow policy. \nand new policy is to be created for any change."}],"poster":"vbal","upvote_count":"1"},{"timestamp":"1640228160.0","upvote_count":"3","comment_id":"507554","content":"Inclined towards CCCCC.\n\nQuestion is asking about minimal operational overhead.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_denylist\n\nThe default configuration of AWS Organizations supports using SCPs as deny lists. Using a deny list strategy, account administrators can delegate all services and actions until you create and attach an SCP that denies a specific service or set of actions. Deny statements require less maintenance, because you don't need to update them when AWS adds new services. Deny statements usually use less space, thus making it easier to stay within the maximum size for SCPs. In a statement where the Effect element has a value of Deny, you can also restrict access to specific resources, or define conditions for when SCPs are in effect.\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html","poster":"Suresh108"},{"upvote_count":"2","content":"C. Use an SCP in Organizations to implement a deny list of AWS service. Apply this SCP at each OU level. Leave the default AWS managed SCP at the root level. For any specific executions for an OU, create a new SCP for that OU.","poster":"cldy","timestamp":"1639051680.0","comment_id":"497696"},{"content":"Selected Answer: C\nC correct","timestamp":"1638394620.0","comment_id":"491957","poster":"AzureDP900","upvote_count":"2"},{"poster":"Bigbearcn","comment_id":"451093","upvote_count":"4","content":"C is correct.\nB is wrong. If you use a deny list and remove the default SCP( which is full access for all services), you cannot do anything.\nD is wrong. If you apply a SCP at root level, it define the max permission and will overwrite the SCP at OU level.","timestamp":"1636296360.0"},{"timestamp":"1636252440.0","poster":"student22","upvote_count":"1","comment_id":"449890","comments":[{"poster":"student22","comments":[{"upvote_count":"2","content":"Not needed. Each level already inherited from the root level, which allows a list of services.","timestamp":"1638780660.0","poster":"tiana528","comment_id":"494994"}],"upvote_count":"1","comment_id":"449891","timestamp":"1636283640.0","content":"D lacks SCPs at each level for allow list."}],"content":"C is the answer."},{"comment_id":"443735","upvote_count":"1","poster":"Cotter","content":"I think D because it is an Allow list of AWS Services not including the restricted AWS Servers which are Implicitly Denied as tekkart 1 commented.","timestamp":"1636028700.0"},{"timestamp":"1635935520.0","upvote_count":"2","comment_id":"443301","poster":"andylogan","content":"It's C"},{"upvote_count":"1","comment_id":"443125","timestamp":"1635929460.0","poster":"nsei","content":"Both C & D will work, but since the question requires access to some, and not ALL, AWS services to be prohibited, I would say C has less operational overhead. For example, as AWS releases new services, we don't need to keep track of them and update the policies accordingly using the approach in C. We can remain focused on the services that we don't want to provide access. \n\nHowever, if the requirement was to prohibit access to ALL AWS SERVIECES by default and only provide access based on use case, then I would go with D as in that case allow strategy would have lower operational overhead."},{"poster":"tgv","content":"CCC\n---\nSo I see that the battle is between C and D.\nI suggest everyone who has doubts about the answer to read carefully how a \"deny list strategy\" and an \"allow list strategy\" works --> https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html\nHaving said that, D is not possible because for the allow list strategy: \"Every SCP in the hierarchy, STARTING AT THE ROOT, must explicitly allow the APIs that you want to be usable in the OUs and accounts below it\".\nThe thing that I don't like for C is: \"create a new SCP for that OU\", but as @cloudgc mentioned, we have to consider that the existing DENY SCP will be removed and the new one is applied.","upvote_count":"5","timestamp":"1635910320.0","comment_id":"434864"},{"timestamp":"1635700320.0","upvote_count":"2","content":"I'll go with D","poster":"WhyIronMan","comment_id":"413285"},{"timestamp":"1635234180.0","comment_id":"382537","upvote_count":"5","poster":"Waiweng","content":"It s C"},{"upvote_count":"1","poster":"vkbajoria","comment_id":"368729","content":"I strongly think it is C. We have Default allow list at the root, Create denied list for all existing OUs. For new accounts and OUs if they have exception, create new denied list. Least amount of work\nD requires more work. If a new exception required, it will have to add that in allow list in the root, that will have impact on all other OUs in the organization","timestamp":"1635212760.0"},{"comment_id":"367808","upvote_count":"2","timestamp":"1635061140.0","poster":"LCC92","content":"only D is correct. In policy evaluation, deny is prioritized, if there is a deny, it would ignore all allow."},{"poster":"Waiweng","timestamp":"1634966760.0","comment_id":"355165","upvote_count":"2","content":"D is best fit"},{"content":"D\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html","comment_id":"349836","timestamp":"1634942580.0","poster":"digimaniac","upvote_count":"2"},{"upvote_count":"1","comment_id":"317247","poster":"KevinZhong","content":"the options are wrong, it should be B but change deny list to allow list","timestamp":"1634750100.0"},{"content":"Changing my mind to C. The reason is that if you apply SCP at the root even future accounts will be affected and we aren't sure if those accounts will want to be part of it.","timestamp":"1634607840.0","comment_id":"305051","poster":"kiev","upvote_count":"3"},{"upvote_count":"3","content":"D is correct. implicit deny is less work than explicit deny which you would need to keep list updated to block new aws service.","comment_id":"303677","timestamp":"1634427780.0","poster":"certainly"},{"content":"I don't think SCP is meant to create an allow list and based on my reasoning D is put.","poster":"kiev","comments":[{"comment_id":"433739","poster":"DerekKey","timestamp":"1635729660.0","upvote_count":"1","content":"By default you get SPC allow"}],"timestamp":"1634064240.0","upvote_count":"1","comment_id":"296503"},{"timestamp":"1633953660.0","poster":"kiev","upvote_count":"3","comment_id":"296498","content":"I am going to go for B. SCP is used to deny and it must be applicable to everyone in the organisation and any list that comes with it will be a deny list and not allow. The answer could as well be D but the think it is B"},{"content":"going with C","upvote_count":"5","poster":"Kian1","comment_id":"293333","timestamp":"1633893720.0"},{"timestamp":"1633764360.0","poster":"Ebi","comment_id":"267605","upvote_count":"7","content":"Answer is C,","comments":[{"poster":"Ebi","content":"C and D both work, but question has asked for most efficient method with minimal overhead, for D the default AWS managed SCP must be removed from both root and each OU which is not efficient","upvote_count":"2","timestamp":"1633809000.0","comment_id":"285156"}]},{"poster":"petebear55","content":"C: I was first inclined towards B ... However have changed my mind to C after seeing this in the answer for B .... \"and each OU\" ... This rules B out as its not best practice.","upvote_count":"2","comment_id":"255177","timestamp":"1633501860.0"},{"poster":"T14102020","content":"Correct C. Without root SCP and delete default SCP","timestamp":"1633431240.0","comment_id":"244909","upvote_count":"2"},{"comments":[{"poster":"rscloud","comment_id":"244832","timestamp":"1633371660.0","content":"to allow an AWS service API at the member account level, you must allow that API at every level between the member account and the root of your organization.","upvote_count":"1"}],"upvote_count":"2","timestamp":"1633358820.0","poster":"rscloud","content":"C is the best fit.\nB- Deny list Strategy make use of \"AWSFullAccess\" SCP that is attached to every OU, Overrides Implicit deny\nD- Allow list on OU SCP doesn't not work until we allow service in root SCP and with that all the accounts will have access to that service\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html","comment_id":"244828"},{"content":"seems D","poster":"gookseang","comment_id":"233561","upvote_count":"2","timestamp":"1633189680.0"},{"comments":[{"content":"Changing to C after reviewing cloudgc analysis","upvote_count":"2","poster":"jackdryan","comment_id":"235068","timestamp":"1633281180.0"}],"content":"I'll go with D","upvote_count":"2","timestamp":"1633176180.0","comment_id":"232578","poster":"jackdryan"},{"content":"C is the answer!!","comment_id":"217491","timestamp":"1632777180.0","poster":"smartassX","upvote_count":"2","comments":[{"upvote_count":"5","content":"C is the only one which atleast make some sense. New SCP specifically created for that OU will have the required services (since it is mentioned as NEW SCP - we have to consider that the existing DENY SCP will be removed and the new one is applied).","poster":"cloudgc","timestamp":"1632893340.0","comment_id":"232519"}]},{"poster":"Gmail78","timestamp":"1632338220.0","content":"I believe the answer is B, you use a deny and then allow case by case Granting an exception to your SCP using a condition\nhttps://aws.amazon.com/blogs/security/how-to-use-service-control-policies-to-set-permission-guardrails-across-accounts-in-your-aws-organization/","upvote_count":"6","comment_id":"214958"},{"timestamp":"1632300600.0","poster":"bbnbnuyh","comment_id":"212442","upvote_count":"5","content":"Changed my answer to D","comments":[{"poster":"cloudgc","content":"Two defect on this - \n1 - If we consider adding the new SCP to that OU alone, it will not work as the root allow-list do not have this service allowed.\n2 - If we allow the service in the root SCP, allow-list then all the OU will have access to the service as the same SCP with the allow-list is applied to the other OUs as well.","upvote_count":"4","comment_id":"232523","timestamp":"1633127760.0","comments":[{"content":"It won't remain an exception as all the OUs will inherit the same permission.","comment_id":"235066","upvote_count":"1","timestamp":"1633271760.0","poster":"jackdryan"},{"content":"I am changing to C.\nWonderful analysis cloudgc. Explicit allow in OU SCP can not override implicit deny at root SCP. For an exception to be made, permission as to be added to allow list in root as well.","comment_id":"235065","upvote_count":"3","timestamp":"1633237380.0","poster":"jackdryan","comments":[{"upvote_count":"1","content":"B.\nC would be wrong. \nDespite creating a new allowed SCP, it will be denied by denied list SCP attached on OU together. It will be denied unless detaching the denied list SCP on OU.","comment_id":"297963","poster":"gpark","timestamp":"1634390040.0"}]}]},{"timestamp":"1632596940.0","poster":"A_New_Guy","comment_id":"215849","upvote_count":"4","content":"An allow list strategy has you remove the AWSFullAccess SCP that is attached by default to every OU and account. This means that no APIs are permitted anywhere unless you explicitly allow them. To allow a service API to operate in an AWS account, you must create your own SCPs and attach them to the account and every OU above it, up to and including the root. Every SCP in the hierarchy, starting at the root, must explicitly allow the APIs that you want to be usable in the OUs and accounts below it. This strategy works because an explicit allow in an SCP overrides an implicit deny. For more information, see Using SCPs as an allow list."}]},{"upvote_count":"1","poster":"bbnbnuyh","comment_id":"212441","timestamp":"1632181320.0","content":"A deny list ‚Äì actions are allowed by default, and you specify what services and actions are prohibited\n\nAn allow list ‚Äì actions are prohibited by default, and you specify what services and actions are allowed"},{"upvote_count":"1","content":"https://thetechtantra.com/scps-whitelisting-and-blacklisting/","poster":"bbnbnuyh","timestamp":"1632117960.0","comment_id":"212361"},{"comment_id":"212356","poster":"bbnbnuyh","content":"\"all accounts\" is the keyword. \nB is the answer","timestamp":"1632079440.0","upvote_count":"2"}],"timestamp":"2020-11-04 00:04:00","answer_ET":"D","unix_timestamp":1604444640,"question_text":"A company has a single AWS master billing account, which is the root of the AWS Organizations hierarchy.\nThe company has multiple AWS accounts within this hierarchy, all organized into organization units (OUs). More OUs and AWS accounts will continue to be created as other parts of the business migrate applications to AWS. These business units may need to use different AWS services. The Security team is implementing the following requirements for all current and future AWS accounts:\n‚úë Control policies must be applied across all accounts to prohibit AWS servers.\n‚úë Exceptions to the control policies are allowed based on valid use cases.\nWhich solution will meet these requirements with minimal optional overhead?","topic":"1","question_id":606,"url":"https://www.examtopics.com/discussions/amazon/view/35974-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32},{"id":"PwxexPxTqmgTReQB3gPP","url":"https://www.examtopics.com/discussions/amazon/view/36073-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":607,"discussion":[{"content":"D and E","comments":[{"content":"A - Uses SNS topics. Will not work.\nB - Uses CloudWatch Alarms. It is required to use a CloudWatch Event/EventBridge rule\nC - Correct.\nD - doesn't have reties to address the situation when the ticketing system is down.\nE - Correct","timestamp":"1634820120.0","poster":"DashL","upvote_count":"4","comment_id":"395250","comments":[{"comment_id":"424168","content":"You are right, but following your logic, it should be A & C because C requires SNS\nC offers a solution for the Ticketing System unavailable. None of the solution based on SQS triggers a solution based on its availability","comments":[{"upvote_count":"1","timestamp":"1635448380.0","comments":[{"poster":"tekkart","timestamp":"1635691740.0","content":"Considering D&E as answers.\n1 - SQS, as Event source mapping for Lambda, where errors such as unavailable ticketing system block processing until errors are solved or items expire. \nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#invocation-async-destinations\n\nWith Dead Letter Queuing option as an alternative solution for on-failure destination :\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\n\n2 - SNS is possible as a destination from Event Source Mapping, having SQS->SNS->Lambda, plus for multiple destination notifications such as email sending would be useful, hence C&E could be feasible assuming this link between SQS and SNS.","upvote_count":"3","comment_id":"424214"}],"poster":"tekkart","comment_id":"424178","content":"But the phrasal of answer A is not OK : \"CW Alarm to invoke the Lambda function\" \n\nWhy need an Event and a queue, the Lambda is already scheduled... unless when the event is \"the ticketing system is available\" not \"the invoking user identity is root\" in question E... E does not address the main concern which is the unavailability of ticketing system"}],"upvote_count":"1","poster":"tekkart","timestamp":"1635423540.0"}]}],"poster":"Rajarshi","comment_id":"216034","timestamp":"1632359640.0","upvote_count":"24"},{"timestamp":"1632417900.0","poster":"beso","comment_id":"218345","content":"B and D, CloudWatch--> SQS--> Lambda-->Ticketing system","upvote_count":"13","comments":[{"upvote_count":"1","comment_id":"277831","timestamp":"1634444100.0","content":"You need EventBridge to trigger root API calls only and then take action. Option B is too broad and doesn't satisfy the requirement of detecting \"API actions performed by the root user\".","poster":"shammous"},{"poster":"Kelvin","timestamp":"1634686380.0","upvote_count":"4","content":"You need CloudWatch Events (aka EventBridge) but not CloudWatch Alarm in this case. So D and E.","comment_id":"339284"}]},{"content":"Selected Answer: DE\nD and E","upvote_count":"1","timestamp":"1672264620.0","comment_id":"760372","poster":"evargasbrz"},{"upvote_count":"1","poster":"hilft","content":"B and D, CloudWatch--> SQS--> Lambda","timestamp":"1658937360.0","comment_id":"638188"},{"poster":"bobsmith2000","timestamp":"1653590940.0","comment_id":"607768","upvote_count":"1","content":"Selected Answer: DE\nRight by the book!"},{"content":"D and E","comment_id":"543381","timestamp":"1644360060.0","upvote_count":"1","poster":"jj22222"},{"upvote_count":"2","timestamp":"1641669540.0","comment_id":"519737","poster":"CloudChef","content":"Selected Answer: DE\nD and E"},{"timestamp":"1641225240.0","comment_id":"515889","content":"D and E is good choice.","upvote_count":"2","poster":"Ni_yot"},{"poster":"cldy","content":"D and E.","timestamp":"1641010200.0","upvote_count":"1","comment_id":"514347"},{"comment_id":"491968","timestamp":"1638395820.0","content":"Selected Answer: DE\nD,E \n\nThe existing system can be modified to use Amazon EventBridge instead of using AWS CloudTrail with Amazon Athena. Eventbridge can be configured with a rule that checks all AWS API calls via CloudTrail. The rule can be configured to look for the usage or the root user account. Eventbridge can then be configured with an Amazon SQS queue as a target that puts a message in the queue waiting to be processed.\nThe Lambda function can then be configured to poll the queue for messages (event-source mapping), process the event synchronously and only return a successful result when the ticketing system has processed the request. The message will be deleted only if the result is successful, allowing for retries.\nThis system will ensure that the important events are not missed when the ticketing system is unavailable.","poster":"AzureDP900","upvote_count":"4"},{"comment_id":"449834","timestamp":"1636241100.0","comments":[{"upvote_count":"1","poster":"kirrim","timestamp":"1636270860.0","comment_id":"462600","content":"+1 for citing documentation on how to make this work\n\nA & B are wrong because CloudWatch Alarms is based on metrics, not an event/action (that's CloudWatch Events)\nC is eliminated because it could have only worked in combo with A, and A is wrong\nD is valid per your links\nE is valid per your links\n\n(Note that you'd probably have to be careful with D that you don't have a Lambda function running for a LONG time trying to reach the API! Might require some extra work here to avoid that)"}],"poster":"niruk","content":"D & E\nEventbridge => https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html\nSQS permissions => https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-use-resource-based.html#eb-sqs-permissions\nSearch for root => https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-logging-monitoring.html also look at the policy.","upvote_count":"3"},{"content":"It's D E \nSince we need CloudWatch Events (aka EventBridge) but not CloudWatch Alarm in this case.","poster":"andylogan","upvote_count":"2","comment_id":"443306","timestamp":"1635843360.0"},{"timestamp":"1635828180.0","content":"DDD EEE\n---","upvote_count":"1","comment_id":"436285","poster":"tgv"},{"comment_id":"435683","upvote_count":"1","poster":"denccc","timestamp":"1635784560.0","content":"it's D and E"},{"upvote_count":"2","comment_id":"433760","content":"A& B - wrong -> CloudWatch alarms base on metrics\nC - wrong - no services in correct answers that write to SNS\nD - correct - Lambda -> SQS\nE - correct - EventBridge = CW Events -> SQS","timestamp":"1635774360.0","poster":"DerekKey"},{"timestamp":"1635762720.0","content":"I believe D and E.","upvote_count":"1","comment_id":"433290","poster":"blackgamer"},{"timestamp":"1635367260.0","poster":"WhyIronMan","upvote_count":"2","comment_id":"413287","content":"I'll go with D,E\n\nSNS does not serve for this purpose"},{"timestamp":"1635233520.0","poster":"Kopa","comment_id":"406933","content":"Im for D & E, E more faster then B.","upvote_count":"1"},{"content":"D is not correct unless you also have B","poster":"zapper1234","upvote_count":"1","comments":[{"timestamp":"1634990220.0","comment_id":"400332","poster":"Gladabhi","content":"You need SQS either with D or B. Right ans - D and E","upvote_count":"1"}],"comment_id":"396160","timestamp":"1634953140.0"},{"upvote_count":"4","poster":"Waiweng","comment_id":"355489","timestamp":"1634772180.0","content":"it's D and E"},{"timestamp":"1634519400.0","content":"going with DE","upvote_count":"2","poster":"Kian1","comment_id":"293340"},{"upvote_count":"1","poster":"rcher","comment_id":"277420","timestamp":"1634139360.0","content":"B E seems ok, inclining towards E cause Cloudtrail (Monitor API Calls) and then publish events to EventBridge."},{"poster":"Ebi","upvote_count":"7","content":"CloudWatch Alarm is not helpful in here, \nI will go with DE","timestamp":"1634100300.0","comment_id":"267610"},{"timestamp":"1634009580.0","poster":"vipgcp","comments":[{"poster":"kirrim","comment_id":"462604","upvote_count":"1","timestamp":"1636300980.0","content":"Agree with your conclusion of D and E, just I don't think B is a valid option for consideration. CloudWatch alarms is based on metrics, rather than an event/action like the root user doing something and it showing up in CloudTrail (that's CloudWatch Events)"}],"upvote_count":"5","content":"D&E (Solution needs combination of 2 and not individual)\nA - SNS is used to notify only and lmbda further will not maintain queue so not a good solution\nB - SQS will maintain queue on both side\nC - not using SNS to discarded\nD - message will be there in SQS if lambda didnt update it. After cooling period another lambda will pick this again - ok\nE - Event bridge will log everything from root for API. Now there is choice between B and E, so E will be much faster then B - ok","comment_id":"256083"},{"content":"Sorry I meant only E does that*","poster":"Bulti","timestamp":"1633983600.0","upvote_count":"1","comment_id":"255796"},{"comment_id":"255794","poster":"Bulti","upvote_count":"1","timestamp":"1633808100.0","content":"D & E. B is incorrect because you cannot publish alarms to SQS. You can only publish events to SQS and only E does not based on the EventBridge rule being defined to look for userIdentity of Root admin when accessing the Service API."},{"comment_id":"255206","upvote_count":"1","timestamp":"1633789020.0","poster":"petebear55","content":"b and e after reading this https://aws.amazon.com/about-aws/whats-new/2019/07/introducing-amazon-eventbridge/ ... However it could just as easily be B and D ... so it will be down to luck in exam. Please note that EVENT BRIDGE is just a new name for Cloudwatch events."},{"content":"Correct BD. CloudWatch--> SQS--> Lambda-->Ticketing system","comment_id":"244913","timestamp":"1633532160.0","poster":"T14102020","upvote_count":"2"},{"content":"B and D","poster":"joos","comment_id":"239297","timestamp":"1633151040.0","comments":[{"comments":[{"content":"Nah...you can't use cloudwatch alarms (as opposed to events) to publish to SQS queue. \nEvent Bridge > SQS > Lambda > Tickets...D&E are correct answers.","timestamp":"1633517160.0","upvote_count":"4","comment_id":"244165","poster":"darthvoodoo","comments":[{"content":"Agreed , Ans DE - Cloudwatch alarm do trigger actions: EC2 action (reboot, stop, terminate, recover), Auto Scaling, SNS (No SQS).But SNS to SQS is possible which is not the options.i.e CloudWatch--> SNS-->SQS","poster":"arulrajjayaraj","timestamp":"1633753500.0","comment_id":"248143","upvote_count":"1"},{"content":"And how will you create a CloudWatch Event rule that matches all APIs invoked by root user? Rules only allow to specify source service and either: specific event type/all events for service/all operations for Cloud Trail/specific operation for Cloud Trail.","comment_id":"330784","poster":"RedKane","upvote_count":"1","timestamp":"1634562900.0"}]}],"poster":"arulrajjayaraj","upvote_count":"2","content":"B & D - Updating the ticketing system is missing in option E","timestamp":"1633325400.0","comment_id":"241114"},{"comment_id":"395868","timestamp":"1634887320.0","poster":"MrCarter","content":"nope, read the explanations provided in the discussion","upvote_count":"1"}],"upvote_count":"2"},{"upvote_count":"3","timestamp":"1632617520.0","content":"I'll go with D,E","comment_id":"232584","poster":"jackdryan"},{"poster":"cpd","timestamp":"1632605040.0","content":"D and E https://docs.aws.amazon.com/eventbridge/latest/userguide/logging-cw-api-calls-eventbridge.html","upvote_count":"5","comment_id":"218845"},{"content":"D for sure, between B and E, E seems to be correct","comment_id":"212914","poster":"liono","upvote_count":"2","timestamp":"1632107040.0"}],"question_text":"A healthcare company runs a production workload on AWS that stores highly sensitive personal information. The security team mandates that, for auditing purposes, any AWS API action using AWS account root user credentials must automatically create a high-priority ticket in the company's ticketing system. The ticketing system has a monthly 3-hour maintenance window when no tickets can be created.\nTo meet security requirements, the company enabled AWS CloudTrail logs and wrote a scheduled AWS Lambda function that uses Amazon Athena to query API actions performed by the root user. The Lambda function submits any actions found to the ticketing system API. During a recent security audit, the security team discovered that several tickets were not created because the ticketing system was unavailable due to planned maintenance.\nWhich combination of steps should a solutions architect take to ensure that the incidents are reported to the ticketing system even during planned maintenance?\n(Choose two.)","answer":"DE","isMC":true,"answer_description":"","answer_images":[],"topic":"1","answer_ET":"DE","choices":{"E":"Create an Amazon EventBridge rule that triggers on all API events where the invoking user identity is root. Configure the EventBridge rule to write the event to an Amazon SQS queue.","D":"Modify the Lambda function to be triggered when there are messages in the Amazon SQS queue and to return successfully when the ticketing system API has processed the request.","B":"Create an Amazon SQS queue to which Amazon CloudWatch alarms will be published. Configure a CloudWatch alarm to publish to the SQS queue.","A":"Create an Amazon SNS topic to which Amazon CloudWatch alarms will be published. Configure a CloudWatch alarm to invoke the Lambda function.","C":"Modify the Lambda function to be triggered by messages published to an Amazon SNS topic. Update the existing application code to retry every 5 minutes if the ticketing system's API endpoint is unavailable."},"unix_timestamp":1604511240,"answers_community":["DE (100%)"],"timestamp":"2020-11-04 18:34:00","exam_id":32,"question_images":[]},{"id":"NjLqJ7BG5aX4yRraEPi2","answer_description":"","discussion":[{"upvote_count":"22","comment_id":"211978","poster":"asldavid","timestamp":"1632158880.0","content":"B. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html"},{"upvote_count":"12","poster":"liono","comment_id":"212916","content":"B is the correct answer. The NAT needs to be in public subnet.\nhttps://stackoverflow.com/questions/48368499/stopped-cannotpullcontainererror-api-error-500","comments":[{"comments":[{"upvote_count":"2","comment_id":"255209","poster":"petebear55","timestamp":"1633393080.0","content":"THINK YOUR RIGHT .. however because they have asked you to choose between public and private in the answers .. knowing aws this leads to one of these being the answer .. so in exam i would put b .. for public"}],"poster":"porlarowl","content":"I understand that \"Configure a NAT GW in the private subnet\" dose not mean Creating a NAT GW. If it means creating a NAT GW, the answer should be B. On the other hand, it means attaching a NAT GW to subnet, the answer should be C. I am not sure, cause I am not a English native speaker.","upvote_count":"2","comment_id":"213176","timestamp":"1632490560.0"},{"comment_id":"225810","content":"yes agree, NAT gw always public facing caused need public IP to communicate with ECR","poster":"Kelvin1477","timestamp":"1633048560.0","upvote_count":"1"}],"timestamp":"1632304920.0"},{"comment_id":"940364","timestamp":"1688255580.0","content":"Selected Answer: B\nyes agree, NAT gw always public facing caused need public IP to communicate with ECR\nSimilar question in tutorials Dojos","poster":"SkyZeroZx","upvote_count":"1"},{"content":"Selected Answer: B\nNAT gateway in the public subnet in the VPC to route requests to the internet.","timestamp":"1673195520.0","upvote_count":"1","poster":"aws0909","comment_id":"769624"},{"timestamp":"1656017160.0","comment_id":"621292","poster":"kangtamo","upvote_count":"1","content":"Selected Answer: B\nAgree with B."},{"comment_id":"491970","content":"B is right \n\nWhen a Fargate task is launched, its elastic network interface requires a route to the internet to pull container\nimages. If you receive an error similar to the following when launching a task, it is because a route to the internet\ndoes not exist:\nCannotPullContainerError: API error (500): Get https://111122223333.dkr.ecr.us-east-1.amazonaws.com/v2/:\nnet/http: request canceled while waiting for connection‚Äù\nTo resolve this issue, you can:\no For tasks in public subnets, specify ENABLED for Auto-assign public IP when launching the task.\no For tasks in private subnets, specify DISABLED for Auto-assign public IP when launching the task, and\nconfigure a NAT gateway in your VPC to route requests to the internet.","poster":"AzureDP900","timestamp":"1638396000.0","upvote_count":"5"},{"poster":"andylogan","content":"It's B","timestamp":"1636048440.0","comment_id":"443313","upvote_count":"1"},{"timestamp":"1635770940.0","content":"Answer is B. NAT gateway should be in the public subnet.","comment_id":"443131","upvote_count":"1","poster":"nsei"},{"timestamp":"1635696480.0","comment_id":"436287","content":"BBB\n---","upvote_count":"1","poster":"tgv"},{"poster":"blackgamer","upvote_count":"2","comment_id":"433292","timestamp":"1635385680.0","content":"B. NAT gateway needs to be in public subnet."},{"content":"I go with C For tasks in public subnets, specify ENABLED for Auto-assign public IP when launching the task","comments":[{"comment_id":"433762","content":"Strange. Read the question again.\n\"The task can only run in a private subnet within the VPC where there is no direct connectivity from outside the system to the application\"","timestamp":"1635609900.0","upvote_count":"1","poster":"DerekKey"}],"timestamp":"1635122820.0","poster":"AndyTokyo608","comment_id":"430797","upvote_count":"1"},{"timestamp":"1634746860.0","content":"I'll go with B","poster":"WhyIronMan","comment_id":"413288","upvote_count":"2"},{"comment_id":"355387","timestamp":"1634726040.0","upvote_count":"3","poster":"Waiweng","content":"it's B"},{"poster":"Waiweng","upvote_count":"2","timestamp":"1634277960.0","comment_id":"355318","content":"it's B"},{"timestamp":"1633959840.0","upvote_count":"2","comment_id":"293341","poster":"Kian1","content":"going with B"},{"upvote_count":"4","poster":"Ebi","timestamp":"1633725120.0","comment_id":"284663","content":"B is my choice"},{"comment_id":"259121","content":"B, NAT GW must in a public subnet in order to work","upvote_count":"1","poster":"kopper2019","comments":[{"timestamp":"1633681140.0","upvote_count":"1","comment_id":"259122","poster":"kopper2019","content":"https://aws.amazon.com/blogs/compute/task-networking-in-aws-fargate/\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-network.html"}],"timestamp":"1633620000.0"},{"upvote_count":"1","timestamp":"1633536780.0","comment_id":"255802","content":"Answer is B. There is no difference between configuring and creating as far as this question is concerned. And we all know that NAT Gateway needs to be created in a Public Subnet. It needs to be accessed from the private subnet via a route table attached to it that routes outbound traffic to the NAT Gateway which is in the public subnet and from there to the internet via the Internet Gateway attached to the VPC.","poster":"Bulti"},{"timestamp":"1633447080.0","poster":"petebear55","content":"B . aws are sh**s ... sent to persecute us poor students of aws !! . they throw red herring questions in like this .. knowing most would go for C .. but this is not the case .. it needs to be PUBLIC so select B ... be aware of this in the exam ... your thinking should go opposite to what your instinct is saying !! .. it is the same in my previous answers when they mention uploading files and mention small or large ... go for large even though instinct says small ... Wizzlabs have a good couple of questions similar to this one and very good explanations from people whom took exam etc ... But for now lets choose B","upvote_count":"2","comment_id":"255212"},{"poster":"T14102020","content":"Correct is B. The NAT needs to be in public subnet","upvote_count":"1","timestamp":"1633370280.0","comment_id":"244923"},{"timestamp":"1633307280.0","content":"B 100% although I would have used the VPC endpoint interface for fargate to pull the image from ECR without the need to traverse the internet...","comments":[{"content":"+1\n\nVPCE seems to be a MUCH better way of maintaining the intent that the container hosts be isolated in a private subnet, with no contact with the outside world.\n\nhttps://aws.amazon.com/blogs/compute/setting-up-aws-privatelink-for-amazon-ecs-and-amazon-ecr/\n\nMaybe this question was written before VPCE for ECR existed?","comment_id":"462614","upvote_count":"1","timestamp":"1636079100.0","poster":"kirrim"}],"upvote_count":"2","comment_id":"244173","poster":"darthvoodoo"},{"content":"I'll go with B","comment_id":"232589","poster":"jackdryan","timestamp":"1633132560.0","upvote_count":"3"},{"comment_id":"217955","content":"B: because NAT is in public subnet and you dont need public IP (the service is in private subnet)","upvote_count":"1","timestamp":"1632885180.0","poster":"pinox1"},{"comment_id":"213252","upvote_count":"1","poster":"bbnbnuyh","timestamp":"1632573840.0","comments":[{"upvote_count":"6","comment_id":"214612","timestamp":"1632789420.0","content":"The NAT needs to be in public subnet","poster":"keos"}],"content":"c it is"},{"comment_id":"211785","content":"A - https://aws.amazon.com/premiumsupport/knowledge-center/ecs-pull-container-api-error-ecr/","upvote_count":"1","comments":[{"timestamp":"1632231480.0","comments":[{"poster":"Amitv2706","timestamp":"1634096640.0","upvote_count":"1","content":"B\nI think it doesnt matter if task is running in a private subnet,\nThe whole purpose of NAT Gateway is to provide internet connectivity to instances/containers running in private subnets.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html","comment_id":"343139"},{"comments":[{"upvote_count":"1","timestamp":"1636271760.0","comment_id":"462621","poster":"kirrim","content":"I don't think you need to attach the private subnet to the NAT GW (or NAT instance) in order to route traffic from the private subnet through it. Just point to it in the routing table of the private subnet. It'll still be available even though it's over in the public subnet.\n\nB is the correct answer\n\nHere is a blog post illustrating the NAT GW solution and then an even better one using a VPCE for ECR:\n\nhttps://aws.amazon.com/blogs/compute/setting-up-aws-privatelink-for-amazon-ecs-and-amazon-ecr/"}],"comment_id":"214971","timestamp":"1632804000.0","upvote_count":"1","poster":"Gmail78","content":"my bad, after reading again the question, this link and the comments I believe is C. Despite you are launching the NAT GW from the public subnet then you need to attach the private subnet, which is more in line with C.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html"}],"poster":"porlarowl","comment_id":"212109","content":"C is correct. \nBecause the task can only run in a private subnet. So we need a NAT GW.","upvote_count":"1"}],"poster":"Gmail78","timestamp":"1632080760.0"}],"answer":"B","answers_community":["B (100%)"],"choices":{"A":"Ensure the task is set to ENABLED for the auto-assign public IP setting when launching the task.","D":"Ensure the network mode is set to bridge in the Fargate task definition.","C":"Ensure the task is set to DISABLED for the auto-assign public IP setting when launching the task. Configure a NAT gateway in the private subnet in the VPC to route requests to the internet.","B":"Ensure the task is set to DISABLED for the auto-assign public IP setting when launching the task. Configure a NAT gateway in the public subnet in the VPC to route requests to the internet."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/35881-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"B","exam_id":32,"timestamp":"2020-11-03 09:59:00","topic":"1","unix_timestamp":1604393940,"answer_images":[],"question_text":"A solutions architect is migrating an existing workload to AWS Fargate. The task can only run in a private subnet within the VPC where there is no direct connectivity from outside the system to the application. When the Fargate task is launched, the task fails with the following error:\nCannotPullContainerError: API error (500): Get https://111122223333.dkr.ecr.us-east-1.amazonaws.com/v2/: net/http: request canceled while waiting for connection\nHow should the solutions architect correct this error?","question_images":[],"question_id":608},{"id":"ZqkThwB7wQUGzIoH38bx","exam_id":32,"answer_description":"","topic":"1","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/36075-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"B","choices":{"C":"Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the robin routing and sticky sessions enabled.","A":"Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.","D":"Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.","B":"Enable Aurora Auto Scaling for Aurora writes. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled."},"answer":"C","question_id":609,"question_text":"A company is running a two-tier web-based application in an on-premises data center. The application user consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server. The application's user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load\nBalancing.\nWhich solution will provide a consistent user experience that will allow the application and database tiers to scale?","answer_images":[],"discussion":[{"comment_id":"218348","upvote_count":"23","poster":"beso","timestamp":"1632564120.0","content":"C, \nAurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances"},{"upvote_count":"1","poster":"Sin_Dan","content":"You cannot horizontally scale the aurora writers! There can be only one Aurora Writer instance per cluster. The only way it can be scaled is vertically. Technically, Aurora writers cannot be scaled automatically and hence the correct answer shall be C.","comment_id":"1298220","timestamp":"1728993960.0"},{"timestamp":"1672264860.0","poster":"evargasbrz","comment_id":"760374","content":"Selected Answer: C\nI'll go with C","upvote_count":"1"},{"upvote_count":"1","timestamp":"1660087380.0","content":"Selected Answer: C\nIt's C","comment_id":"644708","poster":"MarkChoi"},{"timestamp":"1638940920.0","poster":"cldy","upvote_count":"2","content":"C. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the robin routing and sickly sessions enabled.","comment_id":"496545"},{"content":"C is right !","comment_id":"491972","timestamp":"1638396120.0","poster":"AzureDP900","upvote_count":"1"},{"content":"It's C with Aurora Replicas","comment_id":"443315","poster":"andylogan","timestamp":"1636112760.0","upvote_count":"2"},{"comment_id":"443173","poster":"nsei","content":"C is correct","upvote_count":"1","timestamp":"1635942720.0"},{"content":"C\n Auto Scaling for Aurora Replicas + ALB with sticky sessions","poster":"student22","comment_id":"438239","timestamp":"1635859320.0","upvote_count":"3"},{"content":"CCC\n---","upvote_count":"1","poster":"tgv","comment_id":"436289","timestamp":"1635725880.0"},{"comment_id":"413290","content":"I'll go with C","poster":"WhyIronMan","upvote_count":"1","timestamp":"1635486300.0"},{"content":"C also for me","comment_id":"406943","poster":"Kopa","timestamp":"1635106380.0","upvote_count":"1"},{"upvote_count":"4","timestamp":"1634933160.0","poster":"Waiweng","comment_id":"355183","content":"it's C"},{"poster":"alisyech","upvote_count":"2","content":"i choose C","timestamp":"1634794860.0","comment_id":"321743"},{"timestamp":"1634248560.0","comment_id":"277421","upvote_count":"4","poster":"rcher","content":"ALB cause its Web application (Although i can argue that NLB can scale better,just that you need to do SSL termination at the web app)\n\nAurora scale read replica, haven't heard of writes (Correct me if i am wrong)\n\nC then","comments":[{"timestamp":"1636266480.0","poster":"kirrim","comment_id":"462637","content":"The Least Outstanding Requests algo is only supported on the ALB, not the NLB that I could find. So it's definitely C in my mind","upvote_count":"4"}]},{"timestamp":"1634181840.0","content":"C is the correct answer.","upvote_count":"4","comment_id":"259266","poster":"Ebi"},{"upvote_count":"1","content":"Answer is C","comment_id":"259107","timestamp":"1633757520.0","poster":"Bulti"},{"content":"Correct is C.","poster":"T14102020","comment_id":"244927","timestamp":"1633205700.0","upvote_count":"1"},{"comment_id":"232598","timestamp":"1633130700.0","poster":"jackdryan","content":"I'll go with C","upvote_count":"2"},{"poster":"cloudgc","upvote_count":"3","content":"C - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html#Aurora.Integrating.AutoScaling.Add","timestamp":"1633036500.0","comment_id":"232544"},{"poster":"NNHAN","upvote_count":"1","content":"Answer is C","comment_id":"216035","timestamp":"1632530160.0"},{"poster":"Gmail78","timestamp":"1632356280.0","comment_id":"214974","content":"why not C?","upvote_count":"3"},{"comment_id":"212921","timestamp":"1632134040.0","poster":"liono","comments":[{"timestamp":"1632344100.0","content":"writes?","comment_id":"214618","poster":"keos","upvote_count":"1"},{"poster":"MrCarter","comment_id":"395869","timestamp":"1635023460.0","content":"nope ur wrong","upvote_count":"1"}],"upvote_count":"2","content":"B seems to be correct"}],"timestamp":"2020-11-04 18:45:00","unix_timestamp":1604511900,"isMC":true,"question_images":[]},{"id":"Fi0OJXmxqeV7EHtRgvY1","question_images":[],"discussion":[{"poster":"amministrazione","comment_id":"1266675","upvote_count":"1","timestamp":"1723753860.0","content":"C. Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. S3 to host videos with Lifecycle Management to archive original files to Glacier after a few days. CloudFront to serve HLS transcoded videos from S3."},{"comment_id":"625511","content":"Selected Answer: C\nThe answer is C","upvote_count":"3","poster":"klientel","timestamp":"1656638280.0"},{"upvote_count":"4","timestamp":"1656500160.0","content":"The answer is C. For those who think it's B because of the Glacier, take note that it is the ORIGINAL file that is moved to Glacier, not the CONVERTED file.","comment_id":"624566","poster":"altonh"},{"poster":"makpk","comment_id":"615097","upvote_count":"2","timestamp":"1654986300.0","content":"C.\n\nAmazon Elastic Transcoder uses pipelines to manage transcoding jobs. When you create a job, you specify the pipeline that you want to submit the job to. Pipelines are closely tied to an S3 bucket that you specify."},{"timestamp":"1654564920.0","poster":"Anhdd","comments":[{"timestamp":"1655693460.0","content":"After re-read the question, I change to C","upvote_count":"2","comment_id":"618942","poster":"Anhdd"},{"upvote_count":"1","timestamp":"1655704380.0","comment_id":"619003","poster":"Anhdd","content":"my bad, I have mistake between S3 One-Zone IA which is HA (99.5%), not S3 Glacier. C would be better solution here"}],"upvote_count":"1","content":"Selected Answer: B\nB for me\nBecause option C mention S3 glacier which is not really HA (99.5%) while other S3 type is 99.9% and the question request that need maintain HA","comment_id":"612546"},{"comment_id":"597773","upvote_count":"2","poster":"walkwolf3","comments":[{"timestamp":"1666991040.0","upvote_count":"1","content":"You are moving the original video to glacier which is high in size and will increase cost.\nYou will server your users the transcoded video which is still on S3\nSo C is the correct answer for me","poster":"Cal88","comment_id":"706754"}],"content":"B.\n\nFor C, once file is moved in Glacier, it will take time to retrieve.","timestamp":"1651849200.0"},{"comments":[{"timestamp":"1661613600.0","content":"Glacier is for achieving the original videos which are in mp4 format, not the transcoded videos","poster":"Network_1","comment_id":"652611","upvote_count":"1"}],"upvote_count":"2","comment_id":"591948","content":"Why C?\nGlacier doesn't seem to be correct.","poster":"bobsmith2000","timestamp":"1650915660.0"},{"upvote_count":"1","content":"C. Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. S3 to host videos with Lifecycle Management to archive original files to Glacier after a few days. CloudFront to serve HLS transcoded videos from S3.","poster":"cldy","comment_id":"498673","timestamp":"1639145340.0"},{"content":"To serve the video from S3 when they are archieved in Glacier after a few days? It doesn't seem right.","poster":"pt8","comment_id":"352830","upvote_count":"3","timestamp":"1635839520.0"},{"upvote_count":"3","content":"C\nIt's MediaConvert now","poster":"01037","timestamp":"1634998260.0","comment_id":"351052"},{"content":"C form me","comment_id":"340775","upvote_count":"1","poster":"Malcnorth59","timestamp":"1634709000.0"},{"upvote_count":"1","timestamp":"1633551720.0","poster":"ppshein","content":"I will go C","comment_id":"333967"}],"isMC":true,"exam_id":32,"choices":{"B":"Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. CloudFront to serve HLS transcoded videos from EC2.","C":"Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. S3 to host videos with Lifecycle Management to archive original files to Glacier after a few days. CloudFront to serve HLS transcoded videos from S3.","A":"A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. CloudFront to serve HLS transcoded videos from EC2.","D":"A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days. CloudFront to serve HLS transcoded videos from Glacier."},"unix_timestamp":1618233120,"question_id":610,"topic":"1","answer":"C","answer_description":"","answer_ET":"C","answer_images":[],"question_text":"Your website is serving on-demand training videos to your workforce. Videos are uploaded monthly in high resolution MP4 format. Your workforce is distributed globally often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video. Your company has no video transcoding expertise and it required you may need to pay for a consultant.\nHow do you implement the most cost-efficient architecture without compromising high availability and quality of video delivery?","url":"https://www.examtopics.com/discussions/amazon/view/49963-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["C (75%)","B (25%)"],"timestamp":"2021-04-12 15:12:00"}],"exam":{"isBeta":false,"id":32,"isMCOnly":false,"isImplemented":true,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Professional","provider":"Amazon","lastUpdated":"11 Apr 2025"},"currentPage":122},"__N_SSP":true}