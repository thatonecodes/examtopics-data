{"pageProps":{"questions":[{"id":"3FbJGxtX4jwwBVREejCS","url":"https://www.examtopics.com/discussions/amazon/view/146208-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-08-21 06:47:00","unix_timestamp":1724215620,"question_text":"A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application stores data in an Amazon Aurora MySQL DB cluster.\n\nThe company needs to create a disaster recovery (DR) solution. The acceptable recovery time for the DR solution is up to 30 minutes. The DR solution does not need to support customer usage when the primary infrastructure is healthy.\n\nWhich solution will meet these requirements?","isMC":true,"choices":{"C":"Back up the Aurora MySQL DB cluster data by using AWS Backup. Deploy the DR infrastructure in a second AWS Region with an ALB. Update the Auto Scaling group to include EC2 instances from the second Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora MySQL DB cluster in the second Region Restore the data from the backup.","D":"Back up the infrastructure configuration by using AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Set the Auto Scaling group desired capacity to zero. Use Amazon Route 53 to configure active-passive failover. Convert the Aurora MySQL DB cluster to an Aurora global database.","A":"Deploy the DR infrastructure in a second AWS Region with an ALB and an Auto Scaling group. Set the desired capacity and maximum capacity of the Auto Scaling group to a minimum value. Convert the Aurora MySQL DB cluster to an Aurora global database. Configure Amazon Route 53 for an active-passive failover with ALB endpoints.","B":"Deploy the DR infrastructure in a second AWS Region with an ALUpdate the Auto Scaling group to include EC2 instances from the second Region. Use Amazon Route 53 to configure active-active failover. Convert the Aurora MySQL DB cluster to an Aurora global database."},"topic":"1","question_images":[],"answers_community":["A (70%)","D (30%)"],"answer_ET":"A","answer_description":"","answer":"A","answer_images":[],"question_id":971,"exam_id":31,"discussion":[{"timestamp":"1724215620.0","content":"Selected Answer: A\nRTO 30 minute, warm standby.","poster":"chwieobjom","comment_id":"1269825","upvote_count":"12"},{"poster":"spoved","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\nThe DR solution does not need to support customer usage when the primary infrastructure is healthy. -> Pilot Light","timestamp":"1727664060.0","upvote_count":"7","comment_id":"1291360","comments":[{"upvote_count":"2","content":"Backup and restore RTO is up to hours","timestamp":"1729291680.0","comment_id":"1299823","poster":"jingen11"}]},{"upvote_count":"1","poster":"zdi561","comment_id":"1355471","timestamp":"1739339160.0","content":"Selected Answer: D\nA is not right because the max and min are set to minimum which is zero. and the global table use resource."},{"content":"Selected Answer: A\nThe question doesn't ask about the most cost effective solution. A is the option that ensures the lowest recovery time","timestamp":"1736510940.0","comment_id":"1338813","poster":"Salilgen","upvote_count":"3"},{"timestamp":"1736129340.0","upvote_count":"4","content":"Selected Answer: A\nWell just a thought, I hate it when each option looks like a valid one... I think I'll go with A because it seems to me other options are somewhat flawed compared to A.\n\nB - \"The DR solution does not need to support customer usage when the primary infrastructure is healthy.\" So active-active failover is not necessary.\nC - Restoring a database from AWS Backup can take hours to complete, let alone the 30-minute RTO. If I remember it correcly, it can take up to 7 days?\nD - Just like C.\n\nOne more thing about option D, the desired capacity parameter in an Auto Scaling group determines the number of EC2 instances that should be running. By setting it to 0, the group will not launch any EC2 instances during normal operations, so costs are reduced when idle. That's a plus actually.","comment_id":"1336931","poster":"LeonSauveterre"},{"comment_id":"1332650","content":"Selected Answer: A\nThe correct answer is A.\n\nExplanation:\n\nOption A: Correct. This solution sets up disaster recovery infrastructure in a secondary region with minimal running costs by keeping the Auto Scaling group's desired and maximum capacity low. By converting the Aurora MySQL DB cluster to an Aurora global database, the company can provide rapid cross-region replication and promote the standby database to primary if needed, minimizing downtime. Configuring active-passive failover using Route 53 ensures the DR system is only activated when necessary, aligning with the requirement of not supporting customer usage when the primary infrastructure is healthy.","timestamp":"1735337640.0","upvote_count":"3","poster":"Anyio"},{"content":"Selected Answer: A\nRTO 30 min has to be warm standby. \nAWS Backup DOES NOT back up configuration, it backs up DATA","upvote_count":"4","timestamp":"1735333740.0","comment_id":"1332623","poster":"Liongeek"},{"timestamp":"1735121340.0","upvote_count":"1","comment_id":"1331495","content":"Selected Answer: D\npilot light, set auto scaling group desired capacity to 0","poster":"EllenLiu"},{"comment_id":"1323748","timestamp":"1733687700.0","content":"Selected Answer: A\nRestoring Database from AWS back up could take from a few min to several hours. So not D","upvote_count":"3","poster":"Penjerla"},{"timestamp":"1733145840.0","upvote_count":"3","poster":"JA2018","comment_id":"1320968","content":"Selected Answer: A\nI will choose Option A for the following options:\n\n#1 - Active-passive failover: This aligns with the requirement of a DR solution that doesn't need to support customer usage during normal operations, meaning the secondary region only activates when a primary failure occurs.\n\n# 2 - Minimal Auto Scaling group capacity: Setting the desired and maximum capacity to a minimum value ensures minimal cost when the DR infrastructure is not actively used.\n\n# 3 - Aurora Global Database: Converting the Aurora cluster to a global database allows for fast failover to the secondary region, meeting the 30-minute recovery time objective.\n\n#4 - Separate AWS Region: Deploying the DR infrastructure in a different region provides geographic redundancy, crucial for disaster recovery."},{"content":"Selected Answer: D\nD is pilot-light because desired min capacity =0 so ressources are live , services idle, RTO 10s.Ressources are provisionned and scale after even.","upvote_count":"2","poster":"Atdotcom","comment_id":"1308169","timestamp":"1730929680.0"},{"upvote_count":"3","poster":"elmyth","timestamp":"1729355220.0","comment_id":"1300080","content":"Selected Answer: D\nIt can be A, if minimum value = 0. \nIt Can be D, if this approach is implemented before the disaster. \nAnd \"Set the Auto Scaling group desired capacity to zero\" means that it supposed to be done before the disaster."}]},{"id":"kndOrEqwmFfRi3gLstyt","question_images":[],"answer_ET":"D","answer_description":"","answer_images":[],"choices":{"C":"Deploy Amazon EC2 Spot Instances to run the batch jobs. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Flexible Retrieval after 30 days. Set an expiration to delete the data after 2 years.","D":"Deploy Amazon EC2 On-Demand Instances to run the batch jobs. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Deep Archive after 30 days. Set an expiration to delete the data after 2 years.","A":"Migrate the data processing application to Amazon EC2 Spot Instances. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Instant. Retrieval after 30 days. Set an expiration to delete the data after 2 years.","B":"Migrate the data processing application to Amazon EC2 On-Demand Instances. Store the data in Amazon S3 Glacier Instant Retrieval. Move the data to S3 Glacier Deep Archive after 30 days. Set an expiration to delete the data after 2 years."},"isMC":true,"answer":"D","question_text":"A company is migrating its data processing application to the AWS Cloud. The application processes several short-lived batch jobs that cannot be disrupted. Data is generated after each batch job is completed. The data is accessed for 30 days and retained for 2 years.\n\nThe company wants to keep the cost of running the application in the AWS Cloud as low as possible.\n\nWhich solution will meet these requirements?","discussion":[{"timestamp":"1723338360.0","upvote_count":"9","comment_id":"1263688","poster":"nebajp","content":"Selected Answer: D\nD is the correct answer\nfor 30 days - use Amazon S3 standard\n2 years Retaining - Glacier Deep Archive \nCan not be Disrupted - On-Demand Instances"},{"poster":"flaviobrf","content":"Selected Answer: D\nI understand that D is the right anwser","timestamp":"1722967980.0","upvote_count":"5","comment_id":"1261788"},{"comment_id":"1342353","upvote_count":"2","poster":"FlyingHawk","content":"Selected Answer: D\n1. The short-lived batch jobs cannot be disrupted -> on demand. A and C are out \n2. The data is accessed for 30 days and retained for 2 years. means S3 Standard (30 days) -> S3 Glacier -> expire. S3 Glicer instant Retrieval has a minimum storage for 90 days, so B is out.","timestamp":"1737150660.0"},{"comment_id":"1336934","poster":"LeonSauveterre","content":"Selected Answer: D\nB - Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds. So this is wrong because \"The data is accessed for 30 days\".","timestamp":"1736129700.0","upvote_count":"2"},{"timestamp":"1735337760.0","upvote_count":"2","comment_id":"1332651","poster":"Anyio","content":"Selected Answer: D\nThe correct answer is D. Deploy Amazon EC2 On-Demand Instances to run the batch jobs. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Deep Archive after 30 days. Set an expiration to delete the data after 2 years.\n\nExplanation:\n\nOption D: Correct. Since the batch jobs cannot be disrupted, using Amazon EC2 On-Demand Instances ensures the application runs without interruption. Initially storing data in Amazon S3 Standard allows for access during the 30-day period post-processing. Moving the data to Amazon S3 Glacier Deep Archive after 30 days optimizes storage costs for long-term retention of up to 2 years, the lowest cost for infrequent access. Setting an expiration ensures the data is deleted after 2 years, controlling costs and data lifecycle management."},{"content":"Selected Answer: D\nI am always confused about the choice between s3 standard and s3 glacier instant...\nshort period of storage + frequently retrieve ==> s3 standard\nlong period of storage + infrequently retrieve ==> s3 glacier instant\ns3 standard: $0.023 per GB for storage / $0.0004 get, select per 1000 request \ns3 glacier instant: $0.004 per GB for storage /$0.01 get, select per 1000 request \nhttps://aws.amazon.com/s3/pricing/?nc=sn&loc=4","timestamp":"1735122360.0","upvote_count":"3","comment_id":"1331509","poster":"EllenLiu"},{"comments":[{"upvote_count":"1","comment_id":"1342351","content":"S3 Glacier Instant Retrieval has a minimum storage duration of 90 days. If you move data to S3 Glacier Deep Archive before 90 days, you will be charged for the full 90 days of storage, even if the data is deleted or moved earlier. This makes it cost-ineffective for the company's requirement to move data after 30 days.","timestamp":"1737150360.0","poster":"FlyingHawk"}],"upvote_count":"1","comment_id":"1291401","content":"Selected Answer: B\nhttps://aws.amazon.com/about-aws/whats-new/2021/11/amazon-s3-glacier-instant-retrieval-storage-class/\n- The easiest way to store data in S3 Glacier Instant Retrieval is to use the S3 PUT API to upload data directly, or use S3 Lifecycle to transition data from the S3 Standard and S3 Standard-IA storage classes.\n- The company wants to keep the cost of running the application in the AWS Cloud as low as possible\n=> B","timestamp":"1727671320.0","poster":"spoved"},{"poster":"[Removed]","upvote_count":"4","comment_id":"1269529","content":"Selected Answer: D\nD looks right","timestamp":"1724162280.0"},{"poster":"pujithacg8","upvote_count":"2","comment_id":"1264100","timestamp":"1723377540.0","content":"D is correct"},{"content":"Selected Answer: C\nI VOTE C","timestamp":"1722862800.0","comment_id":"1261073","comments":[{"comment_id":"1366465","poster":"tch","timestamp":"1741400340.0","content":"A& C are out! \nAnswer is D\nS3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year.","upvote_count":"1"},{"poster":"officedepotadmin","content":"you voted wrong","upvote_count":"1","timestamp":"1723919100.0","comment_id":"1267812"}],"upvote_count":"1","poster":"siheom"},{"poster":"SR0312","content":"Selected Answer: B\nJob cannot be disrupted - On demand","upvote_count":"5","comment_id":"1260853","timestamp":"1722817800.0"}],"topic":"1","unix_timestamp":1722817800,"timestamp":"2024-08-05 02:30:00","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/145012-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["D (79%)","B (18%)","3%"],"question_id":972},{"id":"nBlkJlsyF4llvC3CHptf","question_images":[],"exam_id":31,"answers_community":["BD (90%)","10%"],"isMC":true,"answer_ET":"BD","answer":"BD","question_id":973,"url":"https://www.examtopics.com/discussions/amazon/view/145777-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"unix_timestamp":1723658280,"discussion":[{"poster":"[Removed]","comment_id":"1268170","upvote_count":"5","content":"Selected Answer: BD\nBD sounds right","timestamp":"1724001300.0"},{"comment_id":"1336935","content":"Selected Answer: BD\nA little memo for me. Hope it'll be helpful for you as well.\n1. VPN Connection: For secure, encrypted traffic between on-premises and AWS over the public internet. ==> Relies on public internet (higher latency)\n2. Direct Connect: For dedicated private network connectivity between on-premises and AWS (like data centers with large workloads). ==> Higher cost, longer setup time.\n3. PrivateLink: For secure private connectivity to AWS services or third-party services (like Salesforce) within AWS. ==> Simple setup but limited to services configured with PrivateLink (not general connectivity).\n4. VPC Peering: For direct connectivity between two AWS VPCs (like multi-account or region scenarios). ==> Does not support transitive connections; not suitable for external services like Salesforce.","upvote_count":"1","timestamp":"1736130480.0","poster":"LeonSauveterre"},{"poster":"MSAM16","comment_id":"1333295","content":"Selected Answer: BD\nBD is correct","timestamp":"1735441020.0","upvote_count":"1"},{"comment_id":"1299827","poster":"jingen11","content":"Selected Answer: BE\nQuestion asked about cheapest. D is not cheap","timestamp":"1729292640.0","upvote_count":"1","comments":[{"upvote_count":"3","poster":"Sergantus","timestamp":"1731637920.0","comment_id":"1312382","content":"Single-digit latency is required"}]},{"timestamp":"1723658280.0","upvote_count":"2","poster":"muhammadahmer36","content":"Selected Answer: BD\nBD is correct","comment_id":"1265904"}],"question_text":"A company needs to design a hybrid network architecture. The company's workloads are currently stored in the AWS Cloud and in on-premises data centers. The workloads require single-digit latencies to communicate. The company uses an AWS Transit Gateway transit gateway to connect multiple VPCs.\n\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)","topic":"1","timestamp":"2024-08-14 19:58:00","answer_description":"","choices":{"B":"Associate an AWS Direct Connect gateway with the transit gateway that is attached to the VPCs.","A":"Establish an AWS Site-to-Site VPN connection to each VPC.","C":"Establish an AWS Site-to-Site VPN connection to an AWS Direct Connect gateway.","D":"Establish an AWS Direct Connect connection. Create a transit virtual interface (VIF) to a Direct Connect gateway.","E":"Associate AWS Site-to-Site VPN connections with the transit gateway that is attached to the VPCs."}},{"id":"Pom4OU6ADntiL3VlHw7q","topic":"1","answer_ET":"A","question_text":"A global ecommerce company runs its critical workloads on AWS. The workloads use an Amazon RDS for PostgreSQL DB instance that is configured for a Multi-AZ deployment.\n\nCustomers have reported application timeouts when the company undergoes database failovers. The company needs a resilient solution to reduce failover time.\n\nWhich solution will meet these requirements?","timestamp":"2024-08-18 19:29:00","answer":"A","unix_timestamp":1724002140,"answer_description":"","question_images":[],"exam_id":31,"question_id":974,"isMC":true,"discussion":[{"content":"Selected Answer: A\nAnswer is A","upvote_count":"2","poster":"aragon_saa","comment_id":"1268225","timestamp":"1724018340.0"},{"poster":"[Removed]","content":"Selected Answer: A\nA. Create an Amazon RDS Proxy. Assign the proxy to the DB instance.\n\nExplanation:\nAmazon RDS Proxy:\n\nRDS Proxy is designed to manage connections to the database more efficiently. It can reduce the impact of failovers on the application by maintaining connections and transparently rerouting them to the standby instance during a failover event.\nBy using RDS Proxy, the failover time is reduced because the proxy minimizes the disruption that occurs when the database fails over, thus reducing application timeouts.","timestamp":"1724002140.0","comment_id":"1268172","upvote_count":"4"}],"answer_images":[],"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/145957-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Enable Performance Insights. Monitor the CPU load to identify the timeouts.","D":"Take regular automatic snapshots. Copy the automatic snapshots to multiple AWS Regions.","B":"Create a read replica for the DB instance. Move the read traffic to the read replica.","A":"Create an Amazon RDS Proxy. Assign the proxy to the DB instance."}},{"id":"T0g0Z6LjFSkX5V4HjhwG","answer_description":"","topic":"1","question_images":[],"isMC":true,"answer_images":[],"answer_ET":"D","question_id":975,"timestamp":"2024-08-11 04:23:00","answers_community":["D (56%)","C (44%)"],"question_text":"A company has multiple Amazon RDS DB instances that run in a development AWS account. All the instances have tags to identify them as development resources. The company needs the development DB instances to run on a schedule only during business hours.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","exam_id":31,"unix_timestamp":1723342980,"answer":"D","choices":{"D":"Create an Amazon EventBridge rule that invokes AWS Lambda functions to start and stop the RDS instances.","A":"Create an Amazon CloudWatch alarm to identify RDS instances that need to be stopped. Create an AWS Lambda function to start and stop the RDS instances.","B":"Create an AWS Trusted Advisor report to identify RDS instances to be started and stopped. Create an AWS Lambda function to start and stop the RDS instances.","C":"Create AWS Systems Manager State Manager associations to start and stop the RDS instances."},"url":"https://www.examtopics.com/discussions/amazon/view/145438-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"content":"Selected Answer: D\nWhen comparing the two:\nOption C is less straightforward because State Manager isn’t designed specifically for start/stop operations on RDS. It would likely require additional complexity (e.g., custom automation documents and potential workarounds) to achieve a scheduled start/stop functionality.\n\nOption D leverages EventBridge’s native scheduling capabilities to invoke Lambda functions, offering a more direct and streamlined approach with the least operational overhead.","timestamp":"1740939000.0","upvote_count":"2","poster":"ieffiong","comment_id":"1364073"},{"poster":"FlyingHawk","timestamp":"1737152340.0","comment_id":"1342358","upvote_count":"3","comments":[{"upvote_count":"1","poster":"FlyingHawk","comment_id":"1342359","timestamp":"1737152640.0","content":"If State Manager does not fully meet your needs, you can also use AWS Lambda with Amazon EventBridge to achieve similar functionality. This approach provides more flexibility but requires additional setup and management\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-stop-and-start-an-amazon-rds-db-instance-using-aws-systems-manager-maintenance-windows.html"}],"content":"Selected Answer: C\nbased on the AWS doc, you can use AWS Systems Manager State Manager associations to start and stop the RDS instances:\n1. https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-ref-rds.html \nAWS-StartRdsInstance\nAWS-StartStopAuroraCluster\nAWS-StopRdsInstance\n2. https://docs.aws.amazon.com/systems-manager/latest/userguide/scheduling-automations-state-manager-associations.html\nD is also workable solution, but it will take more effort to implement."},{"content":"Selected Answer: C\nA - CloudWatch alarms are designed to respond to metric thresholds (like CPU/memory utilization) rather than schedules.\nB - Trusted Advisor provides cost optimization and resource usage recommendations but can't schedule anything.\nD - Works but more complicated than option C. We don't wanna manually compose and then maintain lambda functions right?","comment_id":"1336975","timestamp":"1736141760.0","upvote_count":"1","poster":"LeonSauveterre"},{"comment_id":"1332630","content":"Selected Answer: C\nSetting State Manager with AWS managed Documents (StartEC2Instance, StopEC2Instance, StartRDSInstance and StopRDSInstance) it's easier and faster than creating your own Lambda functions and event bridge rules. \n\nLetter C is the answ.","timestamp":"1735335060.0","upvote_count":"1","poster":"Liongeek"},{"poster":"bujuman","comments":[{"timestamp":"1736513100.0","poster":"Salilgen","upvote_count":"1","content":"You are right: AWS Systems Manager has limitations (https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-stop-and-start-an-amazon-rds-db-instance-using-aws-systems-manager-maintenance-windows.html#automatically-stop-and-start-an-amazon-rds-db-instance-using-aws-systems-manager-maintenance-windows-prereqs). \nAnyway, the question doesn't mention any of them","comment_id":"1338820"},{"poster":"ARV14","timestamp":"1733019960.0","comment_id":"1320419","upvote_count":"2","content":"It should be c as requirement is least operational overhead. Creating 4 lambda functions, event bridge rule, deploying through cloud formation works but adds operational overhead"}],"upvote_count":"3","content":"Selected Answer: D\nAWS Systems Manager State Manage has limitations regarding MSQL and the Db engine is not mentioned. \nCheck the this link to confirm D option : https://aws.amazon.com/blogs/database/save-costs-by-automating-the-start-and-stop-of-amazon-rds-instances-with-aws-lambda-and-amazon-eventbridge/","comment_id":"1312691","timestamp":"1731684660.0"},{"comment_id":"1300781","poster":"KaZimirovich","content":"Selected Answer: D\nWhile AWS Systems Manager State Manager can be used to manage configuration states of AWS resources, it is generally more complex to set up for straightforward use cases like schedule-based starting and stopping of RDS instances compared to using a direct scheduling method through EventBridge.","upvote_count":"4","timestamp":"1729492020.0"},{"comment_id":"1299830","timestamp":"1729293060.0","upvote_count":"1","content":"Selected Answer: C\nStart, restart, or stop managed nodes and Amazon Relational Database Service (Amazon RDS) instances.","poster":"jingen11"},{"comment_id":"1294490","content":"D. Amazon EventBridge allows you to create rules based on a schedule (using cron expressions) to automate tasks. You can set up rules to start the RDS instances at the beginning of business hours and stop them at the end of business hours.\nBy using AWS Lambda in conjunction with EventBridge, you can create functions that handle","timestamp":"1728341220.0","upvote_count":"1","poster":"hharbiordun85"},{"poster":"siheom","comment_id":"1291704","timestamp":"1727742900.0","content":"Selected Answer: D\nVOTE D","upvote_count":"2"},{"timestamp":"1725343920.0","comment_id":"1277324","content":"To meet the requirement of running Amazon RDS DB instances only during business hours with the least operational overhead, the best solution would be:\n\nD. Create an Amazon EventBridge rule that invokes AWS Lambda functions to start and stop the RDS instances. This approach allows you to automate the scheduling of start and stop actions using EventBridge rules, which can trigger Lambda functions based on a cron expression. This setup is straightforward and requires minimal ongoing management","comments":[{"poster":"AbhiBK","content":"Option C, which involves using AWS Systems Manager State Manager to start and stop the RDS instances, is indeed a viable solution. It allows you to automate the process of keeping your RDS instances in a desired state, such as starting and stopping them on a schedule12.\n\nHowever, the reason Option D (using Amazon EventBridge with AWS Lambda) might be preferred for this scenario is due to its simplicity and flexibility. EventBridge rules can be easily configured with cron expressions to trigger Lambda functions, which can start and stop the RDS instances. This setup typically involves fewer steps and less configuration compared to setting up State Manager associations and IAM roles3.\n\nBoth options are valid, but Option D generally offers a more straightforward approach with potentially lower operational overhead","upvote_count":"2","comment_id":"1277325","timestamp":"1725344040.0"}],"upvote_count":"4","poster":"AbhiBK"},{"upvote_count":"2","poster":"dhewa","timestamp":"1724210160.0","comment_id":"1269805","content":"Selected Answer: C\nAWS Systems Manager State Manager allows you to automate the process of starting and stopping RDS instances based on a defined schedule."},{"timestamp":"1723380300.0","comment_id":"1264147","poster":"komorebi","content":"Selected Answer: D\nAnswer is D","upvote_count":"4"},{"timestamp":"1723342980.0","content":"Selected Answer: C\nCorrect Answer us C - it allows you to define and automatically enforce desired configurations for EC2 and RDS.","comment_id":"1263704","upvote_count":"4","poster":"nebajp"}]}],"exam":{"provider":"Amazon","lastUpdated":"11 Apr 2025","isBeta":false,"name":"AWS Certified Solutions Architect - Associate SAA-C03","numberOfQuestions":1019,"isMCOnly":true,"isImplemented":true,"id":31},"currentPage":195},"__N_SSP":true}