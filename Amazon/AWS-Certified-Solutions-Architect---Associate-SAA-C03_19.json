{"pageProps":{"questions":[{"id":"paJLFQ19IhVVyvFo2C1e","unix_timestamp":1668588540,"answer":"A","timestamp":"2022-11-16 09:49:00","answers_community":["A (100%)"],"answer_description":"","topic":"1","question_id":91,"choices":{"B":"Amazon FSx for Windows File Server integrated with Amazon S3","D":"Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume","A":"Amazon FSx for Lustre integrated with Amazon S3","C":"Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)"},"discussion":[{"comments":[{"upvote_count":"5","poster":"HayLLlHuK","content":"yeap, you’re right!","timestamp":"1672772100.0","comment_id":"764940"}],"content":"Selected Answer: A\nIf you see HPC and Linux both in the question.. Pick Amazon FSx for Lustre","poster":"Marge_Simpson","comment_id":"743645","upvote_count":"45","timestamp":"1670911620.0"},{"comment_id":"765716","poster":"aba2s","timestamp":"1672840680.0","upvote_count":"11","content":"Selected Answer: A\nAdditional keywords: make data available for processing by all EC2 instances ==> FSx\n\nIn absence of EFS, it should be FSx. Amazon FSx For Lustre provides a high-performance, parallel file system for hot data"},{"timestamp":"1739028600.0","upvote_count":"1","content":"Selected Answer: A\nLustre is the most suitable when HPC is required.","comment_id":"1353471","poster":"satyaammm"},{"comment_id":"1299627","content":"Selected Answer: A\nAns A - HPC so its FSx and Lustre","timestamp":"1729248360.0","poster":"PaulGa","upvote_count":"3"},{"poster":"MrPCarrot","upvote_count":"3","timestamp":"1706687340.0","comment_id":"1136506","content":"A is the answer because Amazon FSx for Lustre provides a high-performance, scalable file system optimized for compute-intensive workloads like HPC."},{"content":"Selected Answer: A\nLustre is default when HPC is involved. https://aws.amazon.com/fsx/lustre/\n\nB: mentions Windows and no-one asked for it.\nC: S3 Glacier is too slow for HPC\nD: I don't think this is possible, unless I'm mistake, how can you connect a VPC endpoint to EBS without an EC2 kind of instance?","comment_id":"1110821","timestamp":"1704058500.0","upvote_count":"2","poster":"awsgeek75"},{"content":"Selected Answer: A\nHPC workloads running on Linux = Amazon FSx for Lustre","comment_id":"1004477","timestamp":"1694411820.0","poster":"TariqKipkemei","upvote_count":"2"},{"comment_id":"994996","upvote_count":"2","timestamp":"1693476840.0","content":"High performance - Lustre","poster":"Jeyaluxshan"},{"upvote_count":"3","timestamp":"1692277800.0","comment_id":"983654","content":"Selected Answer: A\nThe reasons are:\n\nAmazon FSx for Lustre provides a high-performance, scalable file system optimized for compute-intensive workloads like HPC. It has native integration with Amazon S3.\nData can be copied from on-premises to an S3 bucket, acting as persistent long-term storage.\nThe FSx for Lustre file system can then access the S3 data for high speed processing of datasets and output files.\nFSx for Lustre is designed for the Linux environments used in this HPC workload.","poster":"Guru4Cloud"},{"timestamp":"1687759200.0","upvote_count":"5","poster":"cookieMr","comment_id":"934079","content":"Selected Answer: A\nFSx for Lustre is a high-performance file system optimized for compute-intensive workloads. It provides scalable, parallel access to data and is suitable for HPC applications.\nBy integrating FSx for Lustre with S3, you can easily copy on-premises data to long-term persistent storage in S3, making it available for processing by EC2 instances.\nS3 serves as the durable and highly scalable object storage for storing the output files, allowing for analytics and long-term future use.\n\nOption B, FSx for Windows File Server, is not suitable because the workloads run on Linux, and this option is designed for Windows file sharing.\n\nOption C, S3 Glacier integrated with EBS, is not the best choice as it is a low-cost archival storage service and not optimized for high-performance file system requirements.\n\nOption D, using an S3 bucket with a VPC endpoint integrated with an Amazon EBS General Purpose SSD (gp2) volume, does not provide the required high-performance file system capabilities for HPC workloads."},{"timestamp":"1684909020.0","content":"Selected Answer: A\nOption A is right answer.","poster":"Bmarodi","comment_id":"905534","upvote_count":"1"},{"content":"FSx for Lustre makes it easy and cost-effective to launch and run the popular, high-performance Lustre file system. You use Lustre for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling. \nAmazon Fsx for Lustre is integrated with Amazon S3.","poster":"kerin","comment_id":"818726","upvote_count":"3","timestamp":"1677121860.0"},{"timestamp":"1672756680.0","comment_id":"764754","upvote_count":"2","poster":"SilentMilli","content":"Selected Answer: A\nAmazon FSx for Lustre integrated with Amazon S3"},{"content":"Selected Answer: A\nA is right choice here.","upvote_count":"1","poster":"techhb","timestamp":"1671987480.0","comment_id":"755838"},{"timestamp":"1671306900.0","poster":"career360guru","upvote_count":"2","content":"Selected Answer: A\nOption A is the best high performance storage with integration to S3","comment_id":"748360"},{"upvote_count":"2","content":"Selected Answer: A\nrequirement is File System and workload running on linux. so S3 and FSx for windows is not an option","comment_id":"745496","timestamp":"1671056280.0","poster":"wly_al"},{"timestamp":"1670760660.0","content":"A \nThe Amazon FSx for Lustre service is a fully managed, high-performance file system that makes it easy to move and process large amounts of data quickly and cost-effectively. It provides a fully managed, cloud-native file system with low operational overhead, designed for massively parallel processing and high-performance workloads. The Lustre file system is a popular, open source parallel file system that is well-suited for a variety of applications such as HPC, image processing, AI/ML, media processing, data analytics, and financial modeling, among others. With Amazon FSx for Lustre, you can quickly create and configure new file systems in minutes, and easily scale the size of your file system up or down","poster":"Shasha1","upvote_count":"3","comment_id":"741665"},{"timestamp":"1669134180.0","poster":"Wpcorgan","comment_id":"724460","content":"A is correct","upvote_count":"1"},{"content":"A - for HPC \"Amazon FSx for Lustre\" and long-term persistence \"S3\"","poster":"BENICE","timestamp":"1668606960.0","upvote_count":"2","comment_id":"719704"},{"comment_id":"719565","poster":"rjam","content":"Amazon FSx for Lustre: \n• HPC optimized distributed file system, millions of IOPS\n• Backed by S3","comments":[{"poster":"rjam","upvote_count":"1","comment_id":"719566","content":"Answer A","timestamp":"1668596760.0"}],"timestamp":"1668596760.0","upvote_count":"4"},{"content":"Selected Answer: A\nFxS Lustre integrated with S3","poster":"babaxoxo","upvote_count":"1","comment_id":"719491","timestamp":"1668588540.0"}],"answer_images":[],"answer_ET":"A","question_images":[],"exam_id":31,"question_text":"A company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The company’s HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon EC2 Spot Instances, is short-lived, and generates thousands of output files that are ultimately stored in persistent storage for analytics and long-term future use.\n\nThe company seeks a cloud storage solution that permits the copying of on-premises data to long-term persistent storage to make data available for processing by all EC2 instances. The solution should also be a high performance file system that is integrated with persistent storage to read and write datasets and output files.\n\nWhich combination of AWS services meets these requirements?","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/87634-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"u571ALEp7oNVF0iXJ8UJ","question_images":[],"answer":"A","answers_community":["A (100%)"],"isMC":true,"timestamp":"2022-11-15 14:28:00","question_text":"A company is building a containerized application on premises and decides to move the application to AWS. The application will have thousands of users soon after it is deployed. The company is unsure how to manage the deployment of containers at scale. The company needs to deploy the containerized application in a highly available architecture that minimizes operational overhead.\n\nWhich solution will meet these requirements?","unix_timestamp":1668518880,"discussion":[{"poster":"goatbernard","comment_id":"718746","content":"Selected Answer: A\nAWS Fargate","upvote_count":"17","timestamp":"1668518880.0"},{"comments":[{"upvote_count":"2","comment_id":"1124472","content":"Also, simply speaking, if the company is unsure how to manage deployed containers then Fargate is the only choice.","timestamp":"1705434120.0","poster":"awsgeek75"}],"comment_id":"1110825","poster":"awsgeek75","timestamp":"1704058800.0","upvote_count":"8","content":"Selected Answer: A\nA is minimal overhead. \nB has EC2 overhead\nC EC2 instance overhead + container repository running on EC2 overhead\nD AMII, CloudWatch alarm is overhead++"},{"poster":"satyaammm","timestamp":"1739028780.0","comment_id":"1353475","content":"Selected Answer: A\nAWS Fargate provides least operational overhead.","upvote_count":"1"},{"upvote_count":"2","timestamp":"1729334640.0","content":"Selected Answer: A\nAns A - AWS Fargate - it does it all","comment_id":"1299982","poster":"PaulGa"},{"poster":"lofzee","content":"Selected Answer: A\nfargate is the compute managed for you.. A","timestamp":"1716891000.0","comment_id":"1220131","upvote_count":"2"},{"upvote_count":"4","timestamp":"1701245100.0","poster":"Ruffyit","comment_id":"1083231","content":"ECR+ECS+Fargate = Less overhead"},{"poster":"ACloud_Guru15","content":"Selected Answer: A\nECR+ECS+Fargate = Less overhead","upvote_count":"3","comment_id":"1056624","timestamp":"1698562200.0"},{"content":"Selected Answer: A\nFargate","poster":"Sindokuhlep","upvote_count":"2","comment_id":"1053628","timestamp":"1698229440.0"},{"comment_id":"1004480","content":"Selected Answer: A\nHighly available architecture that minimizes operational overhead = Severless = Elastic Container Registry, Amazon Elastic Container Service with AWS Fargate launch type","upvote_count":"2","poster":"TariqKipkemei","timestamp":"1694412120.0"},{"timestamp":"1692278460.0","upvote_count":"3","comment_id":"983669","poster":"Guru4Cloud","content":"Selected Answer: A\nUsing ECR provides a fully managed container image registry.\nECS with Fargate launch type allows running containers without managing servers or clusters. Fargate will handle scaling and optimization.\nTarget tracking autoscaling will allow automatically adjusting capacity based on demand.\nThe serverless approach with Fargate minimizes operational overhead."},{"upvote_count":"2","poster":"MikeDu","content":"Selected Answer: A\nAWF Fargate should be the best chonice","timestamp":"1691846280.0","comment_id":"979378"},{"timestamp":"1688607720.0","comment_id":"944216","poster":"aadityaravi8","content":"A is the right answers undoubtedly.","upvote_count":"1"},{"comment_id":"934085","poster":"cookieMr","comments":[{"content":"Nice exlanations!","timestamp":"1689223440.0","comment_id":"950330","poster":"Bmarodi","upvote_count":"1"}],"content":"Selected Answer: A\nECR provides a secure and scalable repository to store and manage container images. ECS with the Fargate launch type allows you to run containers without managing the underlying infrastructure, providing a serverless experience. Target tracking in ECS can automatically scale the number of tasks or services based on a target value such as CPU or memory utilization, ensuring that the application can handle increasing demand without manual intervention.\n\nOption B is not the best choice because using the EC2 launch type requires managing and scaling EC2 instances, which increases operational overhead.\n\nOption C is not the optimal solution as it involves managing the container repository on an EC2 instance and manually launching EC2 instances, which adds complexity and operational overhead.\n\nOption D also requires managing EC2 instances, configuring ASGs, and setting up manual scaling rules based on CloudWatch alarms, which is not as efficient or scalable as using Fargate in combination with ECS.","upvote_count":"6","timestamp":"1687759500.0"},{"upvote_count":"2","comment_id":"905555","poster":"Bmarodi","timestamp":"1684910100.0","content":"Selected Answer: A\nECS + Fargate satisfy requirements, hence option A is the best solution."},{"timestamp":"1683970800.0","poster":"studynoplay","content":"Selected Answer: A\nminimize operational overhead = Serverless\nFargate is Serverless","upvote_count":"2","comment_id":"896556"},{"comment_id":"869969","timestamp":"1681448580.0","upvote_count":"1","poster":"NoinNothing","content":"Selected Answer: A\nCorrect is \"A\""},{"comment_id":"862042","upvote_count":"2","timestamp":"1680694260.0","comments":[{"timestamp":"1709332020.0","comment_id":"1163827","poster":"wizcloudifa","upvote_count":"1","content":"that was my doubt too, is Fargate by default highly available? I chose D as option C didnt have a scalability option in it, option D has an autoscaling group in it C doesnt"}],"content":"You can place Fargate launch type all in one AZ, or across multiple AZs.But Option A does not take care of High Availability requirement of question. With Option C we have multi AZ.","poster":"jaswantn"},{"comment_id":"860289","upvote_count":"3","content":"Selected Answer: A\nA\nWhy ?\nBecause fargate provisioned on demand resource","timestamp":"1680549720.0","poster":"SkyZeroZx"},{"comment_id":"759731","content":"Option A\n\nAWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage servers or clusters of Amazon EC2 instances. With Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to choose server types, decide when to scale your clusters, or optimize cluster packing.","poster":"CheckpointMaster","upvote_count":"2","timestamp":"1672224300.0"},{"upvote_count":"1","content":"Selected Answer: A\nOption A","timestamp":"1671307020.0","poster":"career360guru","comment_id":"748362"},{"comment_id":"748116","poster":"alect096","timestamp":"1671287220.0","content":"Selected Answer: A\n\"minimizes operational overhead\" --> Fargate is serverless","upvote_count":"2"},{"poster":"Shasha1","content":"A\nAWS Fargate is a serverless experience for user applications, allowing the user to concentrate on building applications instead of configuring and managing servers. Fargate also automates resource management, allowing users to easily scale their applications in response to demand.","upvote_count":"2","timestamp":"1670760780.0","comment_id":"741667"},{"comment_id":"728001","timestamp":"1669534380.0","content":"Selected Answer: A\nFargate is the only serverless option.","upvote_count":"2","poster":"Phinx"},{"content":"A is correct","timestamp":"1669134240.0","upvote_count":"1","poster":"Wpcorgan","comment_id":"724462"},{"poster":"ds0321","comment_id":"721192","timestamp":"1668763260.0","content":"Selected Answer: A\nAWS Fargate","upvote_count":"2"},{"content":"I think A is the correct option. AWS Farget","comment_id":"719719","poster":"BENICE","timestamp":"1668607680.0","upvote_count":"2"},{"comment_id":"719271","upvote_count":"4","timestamp":"1668564000.0","content":"Selected Answer: A\nA seems right","poster":"mricee9"}],"topic":"1","answer_description":"","question_id":92,"url":"https://www.examtopics.com/discussions/amazon/view/87509-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer_images":[],"choices":{"B":"Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.","D":"Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.","A":"Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.","C":"Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2 instances that are spread across multiple Availability Zones. Monitor the average CPU utilization in Amazon CloudWatch. Launch new EC2 instances as needed."},"answer_ET":"A"},{"id":"rgJVFKJCKneUr3vEM3tQ","answer_images":[],"unix_timestamp":1668522600,"url":"https://www.examtopics.com/discussions/amazon/view/87523-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"question_text":"A company has two applications: a sender application that sends messages with payloads to be processed and a processing application intended to receive the messages with payloads. The company wants to implement an AWS service to handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take up to 2 days to be processed: If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages.\n\nWhich solution meets these requirements and is the MOST operationally efficient?","timestamp":"2022-11-15 15:30:00","answer_ET":"C","answer_description":"","question_id":93,"topic":"1","answer":"C","choices":{"A":"Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the instance. Store, process, and delete the messages, respectively.","B":"Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the processing application with the Kinesis Client Library (KCL).","D":"Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications to process. Integrate the sender application to write to the SNS topic.","C":"Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process."},"discussion":[{"content":"Selected Answer: C\nAmazon SQS supports dead-letter queues (DLQ), which other queues (source queues) can target for messages that can't be processed (consumed) successfully.\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","comment_id":"765743","upvote_count":"13","timestamp":"1672842000.0","poster":"aba2s"},{"comments":[{"content":"Nice exlanations always, thanks a lot!","comment_id":"950335","poster":"Bmarodi","comments":[{"upvote_count":"1","comment_id":"980146","poster":"Bmarodi","content":"Nice explanations always, thanks a lot","timestamp":"1691945640.0"}],"timestamp":"1689224100.0","upvote_count":"1"}],"poster":"cookieMr","timestamp":"1687759740.0","comment_id":"934089","content":"Selected Answer: C\nBy integrating both the sender and processor applications with an SQS, messages can be reliably sent from the sender to the processor application for processing. SQS provides at-least-once delivery, ensuring that messages are not lost in transit. If a message fails to process, it can be retained in the queue and retried without impacting the processing of other messages. Configuring a DLQ allows for the collection of messages that repeatedly fail to process, providing visibility into failed messages for troubleshooting and analysis.\n\nA is not the optimal choice as it involves managing and configuring an EC2 instance running a Redis, which adds operational overhead and maintenance requirements.\n\nB is not the most operationally efficient solution as it introduces additional complexity by using Amazon Kinesis data streams and integrating with the Kinesis Client Library for message processing.\n\nD, using SNS, is not the best fit for the scenario as it is more suitable for pub/sub messaging and broadcasting notifications rather than the specific requirement of message processing between two applications.","upvote_count":"6"},{"comment_id":"1353512","content":"Selected Answer: C\nDead Letter queues are the most suitable here.","upvote_count":"1","poster":"satyaammm","timestamp":"1739033460.0"},{"comment_id":"1299984","poster":"PaulGa","content":"Selected Answer: B\nAns C - Use SQS and dead letter queue. \nNot ideal because max visibility timeout for SQS is 12h and can't extended by user. \nI did consider B: KInesis which is for real-time processing - altho' the question doesn't say its not real-time. Anyway, SQS is basic and fits.","timestamp":"1729335000.0","upvote_count":"2"},{"timestamp":"1709218260.0","comment_id":"1162717","upvote_count":"4","content":"C) This option works. There is a 12h maximum visibility timeout, but:\n\n\"If you don't know how long it takes to process a message, create a heartbeat for your consumer process: Specify the initial visibility timeout (for example, 2 minutes) and then—as long as your consumer still works on the message—keep extending the visibility timeout by 2 minutes every minute.\"\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/working-with-messages.html","poster":"dkw2342"},{"poster":"dkw2342","upvote_count":"2","timestamp":"1709217720.0","content":"None of the options fit. \n\nA) Not operationally efficient \nB) Kinesis is for real-time processing \nD) SNS is not suitable for work queuing. \n\nC) While this may be the \"correct\" answer, it also doesn't really fit the problem statement.\n\nMaximum visibility timeout for SQS is 12h, also can't be extended by the consumer. \n\n\"If your consumer needs longer than 12 hours, consider using Step Functions.\" \n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html","comments":[{"upvote_count":"2","poster":"MatAlves","comment_id":"1274379","content":"It doesn't seem to be the case though:\n\n\" For example, assume that a message spends 1 day in the original queue before it's moved to a dead-letter queue. If the dead-letter queue's retention period is 4 days, the message is deleted from the dead-letter queue after 3 days and the ApproximateAgeOfOldestMessage is 3 days.\"\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html#understanding-message-retention-periods","timestamp":"1724912820.0"}],"comment_id":"1162711"},{"poster":"awsgeek75","comment_id":"1110827","timestamp":"1704058920.0","content":"Selected Answer: C\nC is the only option with dead letter que which meets the requirement of retaining messages that fail to process without impacting other messages.","upvote_count":"3"},{"poster":"TariqKipkemei","timestamp":"1694412300.0","comment_id":"1004483","upvote_count":"4","content":"Selected Answer: C\nImplement an AWS service to handle messages between the two applications = Amazon Simple Queue Service\nIf the messages fail to process, they must be retained = a dead-letter queue"},{"upvote_count":"3","poster":"Guru4Cloud","timestamp":"1692278580.0","content":"Selected Answer: C\nSQS provides a fully managed message queuing service that meets all the requirements:\n\nSQS can handle the sending and processing of 1,000 messages per hour\nMessages can be retained for up to 14 days to allow the full 2 days for processing\nUsing a dead-letter queue will retain failed messages without impacting other processing\nSQS requires minimal operational overhead compared to running your own message queue server","comment_id":"983673"},{"comment_id":"957843","upvote_count":"1","poster":"MutiverseAgent","content":"Selected Answer: B\nAnswer is B), the reason is:\n- Because messages might up to 2 days to be processed. Visibility timeout of SQS is 12 hours, so after 12 hours another consumer might take a message from the queue which is currently being processed.","timestamp":"1689877860.0"},{"content":"Selected Answer: C\nAnswer C, In Question if Keyword have Processing Failed >> SQS","comment_id":"907769","poster":"Jeeva28","upvote_count":"2","timestamp":"1685164080.0"},{"comment_id":"905562","content":"Selected Answer: C\nsolution that meets these requirements and is the MOST operationally efficient will be option C. SQS is buffer between 2 APPs.","upvote_count":"2","poster":"Bmarodi","timestamp":"1684910820.0"},{"timestamp":"1684837980.0","upvote_count":"2","content":"The visibility timeout must not be more than 12 hours. ( For SQS )\n\nJobs may take 2 days to process","comment_id":"904816","poster":"norris81"},{"timestamp":"1683971820.0","content":"Selected Answer: C\noperationally efficient = Serverless\nSQS is serverless","comment_id":"896567","comments":[{"comment_id":"896569","timestamp":"1683971880.0","content":"SNS too is serverless, but it is obvious that it is not the correct answer in this case","upvote_count":"1","poster":"studynoplay"}],"poster":"studynoplay","upvote_count":"2"},{"poster":"apchandana","comment_id":"883201","upvote_count":"3","comments":[{"upvote_count":"2","poster":"nilandd44gg","comment_id":"955711","content":"Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html","timestamp":"1689702420.0"}],"timestamp":"1682652660.0","content":"Selected Answer: C\nmore realistic option is C.\n\nonly problem with this is the limit of the visibility timeout is 12H max. as the second application take 2 days to process, there will be a duplicate of processing messages in the queue. this might complicate things."},{"comment_id":"827102","poster":"vherman","timestamp":"1677774360.0","content":"SQS has a limit 12h for visibility time out","upvote_count":"1"},{"comments":[{"upvote_count":"4","content":"Kinesis stream save data for up to 24 hours, doesn't meet the 2 day requirement.\nKinesis streams don't have fail-safe for failed processing, unlike SQS.\nThe correct answer is C - SQS.","timestamp":"1677523980.0","poster":"UnluckyDucky","comments":[{"comment_id":"883197","upvote_count":"1","content":"this is not a correct statement.\nA data stream is a logical grouping of shards. There are no bounds on the number of shards within a data stream (request a limit increase if you need more). A data stream will retain data for 24 hours by default, or optionally up to 365 days.\nShard\nhttps://aws.amazon.com/kinesis/data-streams/getting-started/","timestamp":"1682652240.0","poster":"apchandana"}],"comment_id":"824043"},{"poster":"LuckyAro","upvote_count":"1","comment_id":"795876","timestamp":"1675325760.0","content":"There's no way for kinesis to know whether the message processing failed."}],"comment_id":"784400","content":"Selected Answer: B\nOption C, using Amazon SQS, is a valid solution that meets the requirements of the company. However, it may not be the most operationally efficient solution because SQS is a managed message queue service that requires additional operational overhead to handle the retention of messages that failed to process. Option B, using Amazon Kinesis Data Streams, is more operationally efficient for this use case because it can handle the retention of messages that failed to process automatically and provides the ability to process and analyze streaming data in real-time.","upvote_count":"1","poster":"bullrem","timestamp":"1674398520.0"},{"timestamp":"1671307140.0","poster":"career360guru","comment_id":"748363","content":"Selected Answer: C\nOption C.","upvote_count":"1"},{"timestamp":"1670138220.0","content":"Selected Answer: C\nThis matches mostly the job of Dead Letter Q: \n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\nvs\nhttps://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html","poster":"ocbn3wby","upvote_count":"5","comment_id":"734903"},{"upvote_count":"1","content":"Selected Answer: C\nOption C is the correct ans","timestamp":"1669659300.0","comment_id":"729503","poster":"Kapello10"},{"poster":"Gabs90","timestamp":"1669645560.0","comment_id":"729216","upvote_count":"2","content":"Selected Answer: C\nC is correct. The B is wrong because the question ask for a way to let the two application to comunicate, so che process is already done"},{"comments":[{"poster":"KADSM","upvote_count":"5","timestamp":"1669651860.0","content":"Kinesis may not be having message retry - there is no way for kinesis to know whether the message processing failed. message can be there till their retention period.","comment_id":"729362"},{"upvote_count":"2","content":"The processing is done at the 2nd application level. \n\nThis seems to be the job of Dead Letter Q","timestamp":"1670138160.0","comment_id":"734902","poster":"ocbn3wby"},{"timestamp":"1672772520.0","poster":"HayLLlHuK","upvote_count":"2","content":"As per question, the processing application will take messages.\n\"The company wants to implement an AWS service to handle messages between the two applications.\"","comment_id":"764945"}],"poster":"TelaO","timestamp":"1669483860.0","content":"Selected Answer: B\nPlease explain by \"B\" is incorrect? How does SQS process data?\n\n\"KCL helps you consume and process data from a Kinesis data stream by taking care of many of the complex tasks associated with distributed computing.\"\n\nhttps://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html","upvote_count":"2","comment_id":"727713"},{"content":"C is correct","upvote_count":"1","poster":"Wpcorgan","comment_id":"724463","timestamp":"1669134360.0"},{"upvote_count":"3","timestamp":"1668622380.0","comment_id":"719890","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","poster":"mabotega"},{"timestamp":"1668608220.0","comment_id":"719727","poster":"BENICE","content":"Option: C\n\"Amazon FSx for Lustre\" ---> Dead Letter Queue","upvote_count":"1"},{"comment_id":"718852","poster":"Nigma","upvote_count":"4","timestamp":"1668522600.0","content":"Ans: C\nhttps://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"}],"answers_community":["C (89%)","11%"],"question_images":[],"isMC":true},{"id":"1SQ0EyuKhkBaRh6A1Dy6","answer_images":[],"unix_timestamp":1668522720,"url":"https://www.examtopics.com/discussions/amazon/view/87524-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a static website. The company’s security policy requires that all website traffic be inspected by AWS WAF.\n\nHow should the solutions architect comply with these requirements?","exam_id":31,"timestamp":"2022-11-15 15:32:00","answer_ET":"D","answer_description":"","question_id":94,"answer":"D","topic":"1","choices":{"B":"Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.","D":"Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution.","A":"Configure an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only.","C":"Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only. Associate AWS WAF to CloudFront."},"discussion":[{"comments":[{"poster":"FNJ1111","comment_id":"761481","content":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/ confirms use of OAI (and option D).","timestamp":"1672347180.0","upvote_count":"14"}],"poster":"Nigma","content":"Answer D. Use an OAI to lockdown CloudFront to S3 origin & enable WAF on CF distribution","comment_id":"718854","upvote_count":"35","timestamp":"1668522720.0"},{"poster":"cookieMr","content":"Selected Answer: B\nBy configuring CloudFront to forward all incoming requests to AWS WAF, the traffic will be inspected by AWS WAF before reaching the S3 origin, complying with the security policy requirement. This approach ensures that all website traffic is inspected by AWS WAF, providing an additional layer of security before accessing the content stored in the S3 origin.\n\nOption A is not the correct choice as configuring an S3 bucket policy to accept requests from the AWS WAF ARN only would bypass the inspection of traffic by AWS WAF. It does not ensure that all website traffic is inspected.\n\nOption C is not the optimal solution as it focuses on controlling access to S3 using a security group. Although it associates AWS WAF with CloudFront, it does not guarantee that all incoming requests are inspected by AWS WAF.\n\nOption D is not the recommended solution as configuring an OAI in CloudFront and restricting access to the S3 bucket does not ensure that all website traffic is inspected by AWS WAF. The OAI is used for restricting direct access to S3 content, but the traffic should still pass through AWS WAF for inspection.","upvote_count":"10","comment_id":"934099","comments":[{"timestamp":"1729560060.0","content":"CloudFront does not \"forward\" requests to AWS WAF. Instead, AWS WAF integrates directly with CloudFront to inspect traffic as it passes through the distribution. There is no manual forwarding process involved.\nso, my Answer is D.","comment_id":"1301350","poster":"Tsige","upvote_count":"2"},{"comment_id":"1171606","timestamp":"1710240240.0","poster":"escalibran","upvote_count":"3","content":"Option B does use the WAF through Cloudfront, but it does not mention anything to prevent direct access to the objects without going through Cloudfront."},{"comment_id":"1066208","timestamp":"1699514460.0","comments":[{"comment_id":"1107313","upvote_count":"4","content":"agreed. Must be D as per above security blog","poster":"SinghJagdeep","timestamp":"1703727960.0"}],"upvote_count":"4","content":"Apparently you can only point to a custom host that is \"not an Amazon Simple Storage Service (Amazon S3) bucket\" (other than for static hosting). https://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/. Answer should be D","poster":"bogobob"}],"timestamp":"1687760280.0"},{"poster":"Vandaman","comment_id":"1359405","upvote_count":"1","timestamp":"1740075180.0","content":"Selected Answer: B\nThe requirement is for the website traffic to be inspected by WAF, not to ensure that the S3 bucket is only accessible through CloudFront. Hence, I say B"},{"poster":"satyaammm","timestamp":"1739033760.0","upvote_count":"1","content":"Selected Answer: D\nUsing OAI for CloudFront is the most suitable option here.","comment_id":"1353514"},{"content":"Selected Answer: D\noption D is correct \nThis is the correct approach because it ensures that the S3 bucket is only accessible through CloudFront, and AWS WAF can inspect all incoming traffic to the CloudFront distribution.\nwhy option B is incorrect \n because AWS WAF is integrated with CloudFront at the edge locations. CloudFront does not forward requests to AWS WAF; instead, AWS WAF inspects the requests as they come into CloudFront","comment_id":"1310542","upvote_count":"3","poster":"jayessh","timestamp":"1731410520.0"},{"poster":"PaulGa","timestamp":"1729335360.0","content":"Selected Answer: B\nAns B - configure CloudFront to forward incoming requests to AWS WAF for inspection before sending to S3. This provides an additional layer of security before accessing the content stored in the S3 origin.\n\nD: not ideal because configuring an OAI in CloudFront and restricting access to the S3 bucket does not guarantee website traffic is inspected by WAF.","upvote_count":"1","comment_id":"1299985"},{"timestamp":"1722992160.0","comment_id":"1261882","content":"Selected Answer: D\nB is incorrect , it misrepresents how AWS WAF works with CloudFront. AWS WAF is not an intermediary service that CloudFront forwards requests to. Instead, AWS WAF is directly integrated with CloudFront as a layer to inspect incoming requests. The correct configuration is to associate AWS WAF with the CloudFront distribution, not to forward requests separately.","poster":"maryam_sh","upvote_count":"5"},{"comment_id":"1243480","timestamp":"1720288620.0","poster":"jatric","content":"Selected Answer: D\nCloud Front allows configuration to enable AWS WAF and restrict direct access to S3 through OAI will meet the requirenments.","upvote_count":"3"},{"content":"Selected Answer: D\nThe requirements indicate that S3 is used to \"store\" a static website, not that it must be configured as a static website (which does not make any sense if it's to be used with CF anyway). Furthermore, the requirements also indicate that all traffic must be inspected by WAF. If you do not setup OAI/OAC, you can potentially bypass CF and access S3 directly. So option B does not satisfy the second requirement.","timestamp":"1719356700.0","comment_id":"1237125","upvote_count":"4","poster":"Duckydoo"},{"poster":"shil_31","content":"Selected Answer: D\nUsing an OAI (Origin Access Identity) restricts access to the S3 bucket, ensuring that only CloudFront can access the content.\nEnabling AWS WAF on the CloudFront distribution allows you to inspect website traffic and filter out malicious requests before they reach your S3 origin.","timestamp":"1717600260.0","upvote_count":"2","comment_id":"1224791"},{"poster":"ManikRoy","comment_id":"1206555","content":"Selected Answer: D\nOAI is required so that S3 bucket is not accessed directly.","timestamp":"1714838820.0","upvote_count":"4"},{"upvote_count":"1","content":"I guess D","timestamp":"1714164540.0","poster":"Hkayne","comment_id":"1202817"},{"timestamp":"1712324700.0","upvote_count":"4","poster":"keehua","content":"Selected Answer: D\nThere are two ways you can serve static websites on AWS S3 origin, either using website endpoints or REST API endpoints. Website endpoints does not support HTTPS. Note that the question does not mention which endpoint is used.\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteEndpoints.html#WebsiteRestEndpointDiff\n\nB is incorrect because we do not 'forward' requests to AWS WAF, we attach WAF on the Cloudfront distribution itself. Could be bad wording of the question.\n\nD is totally valid because it doesn't mention using website endpoints. D also uses OAI to restrict direct access to objects in AWS S3. Although OAI is still viable in this point of time, it is still a legacy method and it is more recommended to use OAC instead.\n\nhttps://repost.aws/knowledge-center/cloudfront-serve-static-website","comment_id":"1189947"},{"upvote_count":"2","comment_id":"1181034","poster":"Uzbekistan","content":"Selected Answer: B\nOption B ensures that all incoming requests to the static website served through Amazon CloudFront are first forwarded to AWS WAF for inspection before the content is requested from the S3 origin. This ensures that all website traffic is inspected by AWS WAF as required by the company's security policy.","timestamp":"1711213320.0"},{"timestamp":"1710321840.0","upvote_count":"1","content":"D is not possible since you cannot set OAC or OAI if S3 bucket is used as static website host","comment_id":"1172399","poster":"drdz13"},{"content":"Selected Answer: D\nWAF is associated to a Cloudfront Distribution","timestamp":"1705507560.0","poster":"bujuman","comment_id":"1125111","upvote_count":"2"},{"upvote_count":"3","comment_id":"1124476","timestamp":"1705434660.0","content":"Selected Answer: D\nA: Doesn't make sense in context with CF.\nB: You configure WAF on CF for HTTP status handling so this may be right be is badly worded\nC: You might as well re-engineer S3 and CloudFront!\nD: The requirement for WAF usage is met with this. Doesn't have to be smart usage, just enabled.","poster":"awsgeek75"},{"comments":[{"poster":"Parul25","upvote_count":"2","content":"A content delivery network is typically deployed before a web application firewall (WAF). Refer to the \"Here’s how the solution works\" section provided in your linked resource.","timestamp":"1706745240.0","comment_id":"1137151"}],"poster":"vip2","comment_id":"1122279","timestamp":"1705209360.0","content":"Selected Answer: D\nsome people use below link as supported point, but when you look into link, AWF is in front of CloudFront from traffic view. So, B is incorrect because 'there is no CloudFront forward requesting to ACL.'\n https://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/","upvote_count":"2"},{"poster":"ale_brd_111","comment_id":"1094632","content":"Selected Answer: B\nB. Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.\n\nThis option ensures that all website traffic passes through AWS WAF for inspection before reaching the S3 origin, complying with the security policy requirements. I appreciate your thorough analysis.","upvote_count":"1","timestamp":"1702392840.0"},{"timestamp":"1699482600.0","poster":"wearrexdzw3123","comment_id":"1066014","content":"Selected Answer: D\nIt's storage, not web endpoint.so It's http://[bucket-name].s3.[region].amazonaws.com ，and\noai can be used","upvote_count":"1"},{"comment_id":"1065210","timestamp":"1699397640.0","poster":"wearrexdzw3123","content":"This resolution doesn't apply to S3 origins that are configured as a website endpoint. For example, AWSDOC-EXAMPLE-BUCKET.s3-website-us-east-1.amazonaws.com.","upvote_count":"1"},{"comment_id":"1051089","comments":[{"poster":"fageroff","upvote_count":"2","comment_id":"1055473","content":"If your origin is an Amazon S3 bucket configured as a website endpoint, you must set it up with CloudFront as a custom origin. That means you can't use OAC (or OAI).","timestamp":"1698403800.0"}],"content":"Selected Answer: D\nWAF is not a destination.\nWAF is attached to something to inspect traffic (ALB, CloudFront etc), so D is the correct answer.","poster":"rlamberti","upvote_count":"6","timestamp":"1698007740.0"},{"upvote_count":"1","comment_id":"1023711","timestamp":"1696317120.0","poster":"Ramdi1","content":"Selected Answer: B\nvoting B because of inspecting traffic"},{"upvote_count":"5","content":"Selected Answer: D\nAnswer D. By configuring an OAI, you restrict direct access to your S3 bucket, ensuring that only CloudFront can access the content in the bucket. This enhances security by preventing direct access to the S3 origin. Enabling AWS WAF on the CloudFront distribution allows you to inspect all incoming traffic through CloudFront before it reaches the S3 origin. This ensures that all website traffic is inspected for security threats as required by the company's security policy.","timestamp":"1695910860.0","poster":"javiems","comment_id":"1019975"},{"timestamp":"1695823440.0","content":"Answer is D. B option doesn't involve S3 or the use of an origin access identity (OAI) to restrict access to the S3 bucket. It's important to ensure that unauthorized users cannot access S3 objects directly.","upvote_count":"3","comment_id":"1018865","poster":"vijaykamal"},{"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\nD","comment_id":"1016116","upvote_count":"2","poster":"JKevin778","timestamp":"1695582300.0"},{"timestamp":"1694921100.0","upvote_count":"2","poster":"BrijMohan08","comment_id":"1009526","content":"Selected Answer: D\nUsing an Origin Access Identity (OAI) allows you to restrict direct access to the S3 bucket and ensure all traffic comes through CloudFront.\n\nAWS WAF can then be enabled on the CloudFront distribution to inspect all incoming traffic.\n\nThe correct answer is D. Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution."},{"content":"Selected Answer: D\nConfigure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution.","poster":"TariqKipkemei","timestamp":"1694412840.0","upvote_count":"2","comment_id":"1004488"},{"comment_id":"986940","upvote_count":"2","timestamp":"1692655920.0","poster":"mtmayer","content":"Selected Answer: D\nD for me."},{"content":"Selected Answer: B\nThis option meets the requirements by:\n\nUsing CloudFront with an S3 origin to store the static website\nConfiguring CloudFront to forward requests to AWS WAF first for inspection before fetching content from S3\nThis allows AWS WAF to inspect all traffic to the website per the security policy","upvote_count":"2","poster":"Guru4Cloud","comment_id":"983676","timestamp":"1692278820.0"},{"content":"Answer D","comment_id":"965836","upvote_count":"2","timestamp":"1690578540.0","poster":"[Removed]"},{"comment_id":"964377","timestamp":"1690435860.0","upvote_count":"3","poster":"Nazmul123","content":"D\n\nCloudFront's Origin Access Identity (OAI) is a special CloudFront user that you can associate with your distribution. By applying an OAI to your S3 bucket, you're able to securely lock down all direct access to your S3 files and require all requests to come through CloudFront.\n\nAmazon Web Application Firewall (WAF) is a security feature that helps protect your resources against common exploits. You can configure AWS WAF directly on your CloudFront distribution to inspect incoming requests to your web application."},{"timestamp":"1689127800.0","poster":"sosda","content":"Selected Answer: D\nWAF associated with cloudfront on creation/distribution. No need to forward request to WAF","comment_id":"949413","upvote_count":"3"},{"content":"Selected Answer: B\nI vote for B!\nOption D is not correct, OAI in CloudFront and restricting access to the S3 bucket does not ensure that all website traffic is inspected by AWS WAF.","upvote_count":"2","poster":"Dhaysindhu","timestamp":"1687941960.0","comment_id":"936366"},{"timestamp":"1686937380.0","poster":"baba365","upvote_count":"1","content":"Answer B: \nIf your origin is an Amazon S3 bucket configured as a website endpoint, you must set it up with CloudFront as a custom origin. That means you can't use OAC (or OAI). However, you can restrict access to a custom origin by setting up custom headers and configuring the origin to require them. For more information, see Restricting access to files on custom origins.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html","comment_id":"925452"},{"upvote_count":"2","timestamp":"1684911660.0","content":"Selected Answer: D\nthe solutions architect that comply with these requirements is option D.\nCF+ S3= OAI/ OAC are the best solutions.","comment_id":"905576","poster":"Bmarodi"},{"poster":"Ankit_EC_ran","upvote_count":"2","timestamp":"1682503380.0","content":"Selected Answer: D\nUse an OAI to have access only from CloudFront to S3 origin & enable WAF on CF distribution","comment_id":"881414"},{"comments":[{"poster":"apchandana","content":"actually speaking; you are able to enable WAF in cloudfront. there is nothing called fording.","upvote_count":"3","timestamp":"1682665980.0","comment_id":"883299"}],"poster":"Robrobtutu","timestamp":"1681828560.0","comment_id":"873733","upvote_count":"1","content":"Selected Answer: B\nI'm voting B because the traffic flows from the user to CloudFront, then from CloudFront to AWS WAF, and then back to CloudFront before being sent to the S3 origin.\n\nRegarding answer D, from what I can tell when you use OAI (or OAC) you don't use WAF, and the question specifically asks for us to use WAF."},{"upvote_count":"1","poster":"[Removed]","content":"B. Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.\n\nOption B is the best solution for enforcing AWS WAF protection for a static website hosted on Amazon S3 through Amazon CloudFront. This involves configuring Amazon CloudFront to forward incoming requests to AWS WAF before requesting content from the S3 origin to ensure that all website traffic is inspected by AWS WAF.","timestamp":"1681642080.0","comment_id":"871667"},{"timestamp":"1681111500.0","comment_id":"866087","content":"ANSWER- B :CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI). We recommend using OAC because it supports:\n\nAll Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022\n\nAmazon S3 server-side encryption with AWS KMS (SSE-KMS)\n\nDynamic requests (PUT and DELETE) to Amazon S3\n\nOAI doesn't work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios.","upvote_count":"1","poster":"TECHNOWARRIOR"},{"upvote_count":"2","comment_id":"850791","timestamp":"1679814660.0","poster":"MaxMa","content":"To option B, If OAI is not used, how about the direct traffic to S3 be inspect by WAF?"},{"poster":"nbisdfsd","upvote_count":"2","content":"Selected Answer: B\nD. Is wrong because \"..specifically, OAI doesn't support:\n\nAmazon S3 buckets in all AWS Regions, including opt-in Regions\"\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html","timestamp":"1679525460.0","comment_id":"847582"},{"content":"According to chat gpt \n\nTo comply with the security policy that requires all website traffic to be inspected by AWS WAF, the solutions architect should configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin. Therefore, option B is the correct answer.\n\nOption A is not sufficient because it only restricts access to the S3 bucket, but it does not ensure that all website traffic is inspected by AWS WAF.\n\nOption C is also not sufficient because it only allows Amazon CloudFront IP addresses to access Amazon S3, but it does not ensure that all website traffic is inspected by AWS WAF.\n\nOption D is partially correct because it uses an origin access identity (OAI) to restrict access to the S3 bucket, but it does not mention configuring Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin. Therefore, it is not the best answer.","poster":"Bogeyman1984","upvote_count":"3","timestamp":"1678380960.0","comment_id":"834196"},{"comment_id":"833473","poster":"MaxMa","content":"Selected Answer: D\nWith option B, the question is if the WAF can be intergrated with the S3?","timestamp":"1678321560.0","upvote_count":"2"},{"poster":"Akademik6","content":"Selected Answer: D\nThe Answer is D.","upvote_count":"1","comment_id":"832820","timestamp":"1678271700.0"},{"poster":"Ttomm","upvote_count":"3","content":"it should be D. refer at section \"Securing Your Content\"\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/","timestamp":"1676964420.0","comment_id":"816321"},{"upvote_count":"6","timestamp":"1675700880.0","comment_id":"799955","content":"For people who chose B as the right Answer, look at this link : https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\n\n\"When you create a web ACL, you can specify one or more CloudFront distributions that you want AWS WAF to inspect. AWS WAF starts to inspect and manage web requests for those distributions based on the criteria that you identify in the web ACL\"\n\nYou don't configure Cloudfront to redirect traffic to WAF. You just create an ACL and points to the Cloudfront distribution.\n\nSo D is the best solution to secure and integrate Cloudfront with S3 and WAF.\n\nFrom one side it protects your S3 Content by allowing user requests to access only the OAI.\nAnd from other side it enable WAF to control traffic before reaching Cloudfront by creating a WAF Rule or ACL (Not redirecting Cloudfront traffic to WAF which as a solution architect you cannot do)","poster":"CaoMengde09"},{"upvote_count":"2","timestamp":"1675644780.0","content":"Selected Answer: B\nexplicitly explains the rationale for war forwarding-- new feature\nhttps://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/","comment_id":"799294","poster":"tinkeringpuncturing"},{"comment_id":"787222","upvote_count":"3","poster":"Training4aBetterLife","content":"Selected Answer: B\nThis can be done by selecting \"Yes\" for \"Viewer Protocol Policy\" when creating or updating the CloudFront distribution and selecting \"AWS WAF\" for \"Origin Protocol Policy.\" This will ensure that all traffic to the website is inspected by AWS WAF before being served by CloudFront.\n\nOption D is incorrect because configuring Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket and enabling AWS WAF on the distribution will not allow AWS WAF to inspect website traffic BEFORE it is served by CloudFront and S3.","timestamp":"1674614940.0"},{"poster":"bullrem","upvote_count":"1","timestamp":"1674336000.0","comment_id":"783736","content":"Selected Answer: B\nThis allows the website traffic to be inspected by AWS WAF before being served by CloudFront and S3."},{"poster":"mj61","upvote_count":"1","content":"Option B is the best solution as it specifies that Cloudfront should forward ALL incoming requests to AWS WAF before requesting content from S3 origin. This way, all the incoming traffic to the website will be inspected by AWS WAF and only the traffic that meets the security rules will be allowed to access the content stored in the S3 bucket.","timestamp":"1673729100.0","comment_id":"775913"},{"comment_id":"764791","upvote_count":"2","poster":"SilentMilli","content":"Selected Answer: D\nKey word: origin access identity (OAI)","timestamp":"1672758840.0"},{"comment_id":"763430","content":"D = correct answer\nNot sure why people are picking B. Traffic is inspected first by the WAF if conditions are met the Cloudfront responds to requests either to request content or deny from the S3 this would then be based on OAI.","upvote_count":"2","poster":"Mindvision","timestamp":"1672621920.0"},{"comment_id":"763343","upvote_count":"1","poster":"dan80","comments":[{"comment_id":"766429","upvote_count":"2","comments":[{"content":"Ignore that, I'm thinking B too now","upvote_count":"1","timestamp":"1672909980.0","comment_id":"766432","poster":"JayBee65"}],"content":"The URL shows that B is wrong! You do not 'Configure Amazon CloudFront to forward all incoming requests to AWS WAF' but instead 'When you create a web ACL, you can specify one or more CloudFront distributions that you want AWS WAF to inspect' - see https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html","timestamp":"1672909920.0","poster":"JayBee65"}],"content":"B - https://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/","timestamp":"1672607820.0"},{"comment_id":"755866","timestamp":"1671989580.0","content":"Selected Answer: D\nAnswer is D only\nhttps://blog.shikisoft.com/restrict-amazon-s3-bucket-access-on-cloudfront/","poster":"techhb","upvote_count":"2"},{"timestamp":"1671731760.0","content":"Selected Answer: B\nThe correct answer is B. Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.\n\nTo comply with the security policy, the solutions architect should configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin. This will allow AWS WAF to inspect all website traffic before it is served by CloudFront and S3.","upvote_count":"4","comments":[{"comment_id":"753546","timestamp":"1671731760.0","poster":"Buruguduystunstugudunstuy","upvote_count":"2","content":"Option A is incorrect because configuring an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only will not allow CloudFront to forward incoming requests to AWS WAF.\n\nOption C is incorrect because configuring a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only and associating AWS WAF to CloudFront will not allow AWS WAF to inspect website traffic before it is served by CloudFront and S3.\n\nOption D is incorrect because configuring Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket and enabling AWS WAF on the distribution will not allow AWS WAF to inspect website traffic before it is served by CloudFront and S3."},{"comment_id":"753547","timestamp":"1671731940.0","content":"This can be done by selecting \"Yes\" for \"Viewer Protocol Policy\" when creating or updating the CloudFront distribution and selecting \"AWS WAF\" for \"Origin Protocol Policy.\" This will ensure that all traffic to the website is inspected by AWS WAF before being served by CloudFront.","upvote_count":"2","poster":"Buruguduystunstugudunstuy"}],"poster":"Buruguduystunstugudunstuy","comment_id":"753544"},{"comment_id":"749063","upvote_count":"1","poster":"bammy","content":"The answer is B","timestamp":"1671383400.0"},{"content":"Selected Answer: D\nAnswer is D","timestamp":"1671307260.0","poster":"career360guru","comment_id":"748366","upvote_count":"1"},{"comment_id":"741705","content":"B \nwe can use create a new web distribution using the appropriate S3 origin in CloudFront console. as follows: Select “Create WAF Web ACL” from the Restriction Rules section.\nChoose an existing or create a new WAF web access control list (ACL) with necessary settings, such as rule activation and blocking criteria.\nSelect “Forward All Requests to WAF” in the Distribution Settings.\nProvide the IP address of the WAF, and select “Allow” in the Origin Protocol Policy.\nSet “Compress Objects Automatically” to Yes, and click “Create Distribution.”\nFurther reference:\n\nhttps://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/","upvote_count":"4","poster":"Shasha1","timestamp":"1670763300.0"},{"content":"D is correct","comment_id":"724465","timestamp":"1669134480.0","upvote_count":"1","poster":"Wpcorgan"},{"comment_id":"724020","poster":"TonyghostR05","timestamp":"1669081500.0","content":"D using OAI","upvote_count":"2"},{"upvote_count":"4","poster":"handyplazt","content":"Selected Answer: D\nAnswer D\nnot B because it is not supported to forward S3 requests to WAF","comment_id":"723401","timestamp":"1669027020.0"},{"timestamp":"1668978480.0","comment_id":"723008","content":"Selected Answer: B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","upvote_count":"1","poster":"EKA_CloudGod"},{"content":"Selected Answer: D\nhttps://www.examtopics.com/discussions/amazon/view/35639-exam-aws-certified-solutions-architect-associate-saa-c02/","upvote_count":"3","poster":"sdasdawa","comment_id":"718900","timestamp":"1668525420.0"}],"question_images":[],"answers_community":["D (69%)","B (31%)"],"isMC":true},{"id":"l9dIxpPQEdx40rtOkFIZ","answer":"D","unix_timestamp":1668522420,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/87522-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","isMC":true,"answer_images":[],"discussion":[{"comment_id":"753553","timestamp":"1671732120.0","upvote_count":"11","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: D\nThe most effective and efficient solution would be Option D (Use Amazon CloudFront with the S3 bucket as its origin.) \n\nAmazon CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content, such as HTML pages, images, and videos. By using CloudFront, the HTML pages will be served to users from the edge location that is closest to them, resulting in faster delivery and a better user experience. CloudFront can also handle the high traffic and large number of requests expected for the global event, ensuring that the HTML pages are available and accessible to users around the world."},{"upvote_count":"5","content":"Selected Answer: D\nCloudFront is well-suited for efficiently serving static HTML pages to users around the world. By using itwith the S3 as its origin, the static HTML pages can be cached and distributed globally to edge locations, reducing latency and improving performance for users accessing the pages from different regions. This solution ensures efficient and effective delivery of the daily reports to millions of users worldwide, providing a scalable and high-performance solution for the global event.\n\nA would allow temporary access to the files, but it does not address the scalability and performance requirements of serving millions of views globally.\n\nB is not necessary for this scenario as the goal is to distribute the static HTML pages efficiently to users worldwide, not replicate the files across multiple Regions.\n\nC is primarily used for routing DNS traffic based on the geographic location of users, but it does not provide the caching and content delivery capabilities required for this use case.","timestamp":"1687760460.0","comment_id":"934101","poster":"cookieMr"},{"content":"Selected Answer: D\nCloudFront is the most suitable option here.","comment_id":"1353516","upvote_count":"1","timestamp":"1739033820.0","poster":"satyaammm"},{"timestamp":"1729936980.0","poster":"PaulGa","comment_id":"1303217","content":"Selected Answer: D\nAns D - CloudFront content delivery netwk is designed for the job: speeds delivery of static/ dynamic content (HTML, images, vids). Pages will be served from the closest edge location = faster delivery; improved UX; managed high vol traffic; high vol requests","upvote_count":"2"},{"comment_id":"1004489","upvote_count":"2","poster":"TariqKipkemei","timestamp":"1694412900.0","content":"Selected Answer: D\nGlobal users = Amazon CloudFront"},{"comment_id":"983679","poster":"Guru4Cloud","content":"Selected Answer: D\nCloudFront is the best solution for this use case because:\n\nCloudFront is a content delivery network (CDN) that caches content at edge locations around the world. This brings content closer to users for fast performance.\nFor high traffic global events with millions of viewers, a CDN is necessary for effective distribution.\nUsing the S3 bucket as the origin, CloudFront can fetch the files once and cache them globally.","timestamp":"1692278940.0","upvote_count":"2"},{"upvote_count":"1","poster":"career360guru","timestamp":"1671307380.0","content":"Selected Answer: D\nOption D","comment_id":"748367"},{"poster":"k1kavi1","content":"Selected Answer: D\nAgreed","comment_id":"746065","timestamp":"1671107520.0","upvote_count":"1"},{"timestamp":"1670820720.0","content":"answer is D agree with Shasha1","upvote_count":"1","comment_id":"742411","poster":"Sahilbhai"},{"comment_id":"741686","timestamp":"1670762100.0","content":"D\nCloudFront is a content delivery network (CDN) offered by Amazon Web Services (AWS). It functions as a reverse proxy service that caches web content across AWS's global data centers, improving loading speeds and reducing the strain on origin servers. CloudFront can be used to efficiently deliver large amounts of static or dynamic content anywhere in the world.","poster":"Shasha1","upvote_count":"3"},{"upvote_count":"2","content":"D is correct","timestamp":"1669134480.0","comment_id":"724467","poster":"Wpcorgan"},{"comment_id":"718862","content":"D\n\nStatic content on S3 and hence Cloudfront is the best way","timestamp":"1668522900.0","poster":"Nigma","upvote_count":"2"},{"timestamp":"1668522420.0","poster":"Pamban","upvote_count":"2","content":"Selected Answer: D\nD is the correct answer","comment_id":"718848"}],"choices":{"A":"Generate presigned URLs for the files.","B":"Use cross-Region replication to all Regions.","C":"Use the geoproximity feature of Amazon Route 53.","D":"Use Amazon CloudFront with the S3 bucket as its origin."},"question_text":"Organizers for a global event want to put daily reports online as static HTML pages. The pages are expected to generate millions of views from users around the world. The files are stored in an Amazon S3 bucket. A solutions architect has been asked to design an efficient and effective solution.\n\nWhich action should the solutions architect take to accomplish this?","exam_id":31,"question_images":[],"answer_ET":"D","timestamp":"2022-11-15 15:27:00","question_id":95,"answers_community":["D (100%)"]}],"exam":{"isBeta":false,"name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isMCOnly":true,"provider":"Amazon","isImplemented":true,"id":31},"currentPage":19},"__N_SSP":true}