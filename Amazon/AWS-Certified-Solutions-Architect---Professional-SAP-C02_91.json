{"pageProps":{"questions":[{"id":"GeGnzfnjywD4T1BJOMt5","topic":"1","unix_timestamp":1719518940,"answer_description":"","discussion":[{"comment_id":"1321340","upvote_count":"1","timestamp":"1733224080.0","poster":"nimbus_00","content":"Selected Answer: D\nhttps://stackoverflow.com/questions/70184420/can-jars-be-uploaded-successfully-to-aws-elastic-beanstalk-from-the-aws-web-ui"},{"timestamp":"1731808980.0","poster":"AzureDP900","upvote_count":"1","content":"D is right\nElastic Beanstalk: AWS Elastic Beanstalk is a managed service that automates the deployment, scaling, and management of web applications on EC2 instances. It provides auto-scaling capabilities, which can handle increased traffic without manual intervention.\nMulti-AZ deployment: Deploying Tomcat servers in multiple Availability Zones (AZs) ensures high availability and reduces downtime in case of failures or outages.\nRDS database instance: Using an RDS PostgreSQL database instance allows for easy scaling and management of your application's data, making it well-suited for increased traffic.\nCloudFront and ALB: Deploying Amazon CloudFront (CDN) and Application Load Balancer (ALB) helps distribute traffic across multiple regions and instances, ensuring a scalable and high-performance architecture.","comment_id":"1313345"},{"timestamp":"1720027500.0","content":"Selected Answer: D\nAnswer D\nElastic Beanstalk: Provides an easy and managed way to deploy and scale web applications. It handles the deployment, capacity provisioning, load balancing, and auto-scaling automatically.\n\nAmazon RDS for PostgreSQL: Manages the database operations, providing automated backups, patching, and scaling, which reduces operational overhead.\n\nCloudFront and Application Load Balancer: Ensure that the application can handle increased traffic efficiently, distributing the load across multiple Availability Zones and providing low latency.","poster":"AhmedSalem","comment_id":"1241560","upvote_count":"4"},{"poster":"kupo777","timestamp":"1719732240.0","upvote_count":"1","comment_id":"1239552","content":"D\nThe option with the least overhead is the use of AWS Elastic Beanstalk Tomcat."},{"timestamp":"1719518940.0","content":"Selected Answer: D\nUpload the .jar straightforward to AWS Elastic Beanstalk. Answer D","poster":"mifune","upvote_count":"3","comment_id":"1238379"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/142998-exam-aws-certified-solutions-architect-professional-sap-c02/","question_id":451,"exam_id":33,"answer":"D","timestamp":"2024-06-27 22:09:00","question_text":"A company plans to migrate a legacy on-premises application to AWS. The application is a Java web application that runs on Apache Tomcat with a PostgreSQL database.\n\nThe company does not have access to the source code but can deploy the application Java Archive (JAR) files. The application has increased traffic at the end of each month.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"D","answers_community":["D (100%)"],"isMC":true,"choices":{"C":"Refactor the Java application into Python-based containers. Use AWS Lambda functions for the application logic. Store application data in Amazon DynamoDB global tables. Use AWS Storage Gateway and Lambda concurrency to scale for increased traffic.","D":"Use AWS Elastic Beanstalk to deploy the Tomcat servers with auto scaling in multiple Availability Zones. Store application data in an Amazon RDS for PostgreSQL database. Deploy Amazon CloudFront and an Application Load Balancer to scale for increased traffic.","B":"Provision Amazon Elastic Kubernetes Service (Amazon EKS) in an Auto Scaling group across multiple AWS Regions. Deploy Tomcat and PostgreSQL in the container images. Use a Network Load Balancer to scale for increased traffic.","A":"Launch Amazon EC2 instances in multiple Availability Zones. Deploy Tomcat and PostgreSQL to all the instances by using Amazon Elastic File System (Amazon EFS) mount points. Use AWS Step Functions to deploy additional EC2 instances to scale for increased traffic."},"answer_images":[]},{"id":"bLDBIlf3JVfesJgJJoVO","isMC":true,"choices":{"E":"Migrate the MongoDB cluster to Amazon DocumentDB (with MongoDB compatibility).","B":"Create an AWS Lambda function. Program the Lambda function to connect to the IoT devices. process the data, and write the data to the data store. Configure a Lambda layer to temporarily store messages for processing.","C":"Configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Create an ingress controller on the EKS cluster to serve the reports.","A":"Create AWS Step Functions state machines with AUS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Configure an Amazon CloudFront distribution that has an S3 origin to serve the reports","F":"Migrate the MongoDB cluster to Amazon EC2 instances.","D":"Connect the IoT devices to AWS IoT Core to publish messages. Create an AWS IoT rule that runs when a message is received. Configure the rule to call an AWS Lambda function. Program the Lambda function to parse, transform, and store device message data to the data store."},"unix_timestamp":1719519240,"url":"https://www.examtopics.com/discussions/amazon/view/142999-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"ADE","answer":"ADE","answer_images":[],"question_text":"A company is migrating its on-premises IoT platform to AWS. The platform consists of the following components:\n\n• A MongoDB cluster as a data store for all collected and processed IoT data.\n• An application that uses Message Queuing Telemetry Transport (MQTT) to connect to IoT devices every 5 minutes to collect data.\n• An application that runs jobs periodically to generate reports from the IoT data. The jobs take 120-600 seconds to finish running.\n• A web application that runs on a web server. End users use the web application to generate reports that are accessible to the general public.\n\nThe company needs to migrate the platform to AWS to reduce operational overhead while maintaining performance.\n\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)","topic":"1","timestamp":"2024-06-27 22:14:00","exam_id":33,"question_images":[],"answers_community":["ADE (100%)"],"discussion":[{"upvote_count":"7","content":"Selected Answer: ADE\nStep Functions and Lambda for Report Generation (A):\n\nAWS Step Functions and Lambda can manage the periodic jobs to generate reports with minimal operational overhead. By using Amazon S3 for storage and Amazon CloudFront for distribution, the solution provides scalability and reliability with minimal management.\nAWS IoT Core for MQTT Messaging (D):\n\nAWS IoT Core is a managed service that simplifies the connection and management of IoT devices. Using IoT rules and Lambda functions ensures efficient message processing and data storage with minimal overhead.\nAmazon DocumentDB for MongoDB Compatibility (E):\n\nAmazon DocumentDB is a managed database service compatible with MongoDB, which reduces the operational burden of managing a MongoDB cluster while maintaining performance and scalability.","poster":"awsaz","timestamp":"1719598680.0","comment_id":"1238832"},{"poster":"vip2","content":"Selected Answer: ADE\nA, D and E meet all requirements clearly","upvote_count":"1","comment_id":"1243223","timestamp":"1720246500.0"},{"upvote_count":"2","content":"Selected Answer: ADE\nA - for preparing the reports and writing them to Amazon S3 | D - for handling connections from IoT devices | E - supporting MongoDB workloads.","comment_id":"1238381","timestamp":"1719519240.0","poster":"mifune"}],"answer_description":"","question_id":452},{"id":"Oo5NMP9R5v6mEf9ZVzgX","question_id":453,"answer":"D","isMC":true,"timestamp":"2024-06-27 22:19:00","discussion":[{"content":"Selected Answer: D\nThe most cost-effective solution to limit cost and usage for the API Gateway API with minimal code changes is:\n\nD. Create an API Gateway API Key and usage plan. Define throttling limits and quotas in the usage plan. Associate the usage plan with the Production stage and the API key. Share the API key with the external development team.\n\nHere's why this approach is most cost-effective:\n\nAPI Key and Usage Plan: This restricts access to the API only for the development team using the provided API key. The usage plan allows defining throttling limits (maximum requests per unit time) and quotas (total requests allowed) for the API key. This controls resource utilization and costs.\nMinimal Code Changes: No modifications are required to the existing Lambda functions, reducing development effort.","comment_id":"1252859","timestamp":"1721628900.0","upvote_count":"5","poster":"Chakanetsa"},{"content":"Selected Answer: D\neffective way to control API consumption = API KEY + Usage Plan\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html","timestamp":"1733224680.0","comment_id":"1321343","upvote_count":"1","poster":"nimbus_00"},{"poster":"gfhbox0083","timestamp":"1720791120.0","content":"Selected Answer: D\nD, for sure.\nAPI Gateway Usage Plans allow you to set throttling limits and quotas on API keys. This directly controls the number of requests per second and per day that the external development team can make. It helps in managing costs by limiting the amount of Lambda invocations triggered by API requests.","upvote_count":"1","comment_id":"1246783"},{"comment_id":"1243221","timestamp":"1720246260.0","upvote_count":"1","content":"Selected Answer: D\nUser plan is to define who and how much for API usage","poster":"vip2"},{"poster":"awsaz","upvote_count":"4","comment_id":"1238834","content":"Selected Answer: D\nCreating an API key and a usage plan allows you to control and limit the usage of the API. The usage plan lets you define throttling limits (requests per second) and quotas (total requests per day or month).\nBy associating the usage plan with the Production stage and the API key, you can enforce these limits on the external development team, ensuring that the API usage stays within the desired boundaries.\nThis approach directly addresses the concern of sudden increases in usage and helps control costs without requiring any changes to the existing Lambda functions or the overall architecture","timestamp":"1719599100.0"},{"timestamp":"1719519540.0","poster":"mifune","content":"Selected Answer: D\nThe \"usage plan\" is the key here for me to access the API within the defined limits.","comment_id":"1238384","upvote_count":"1"}],"exam_id":33,"question_images":[],"answer_images":[],"question_text":"A company creates an Amazon API Gateway API and shares the API with an external development team. The API uses AWS Lambda functions and is deployed to a stage that is named Production.\n\nThe external development team is the sole consumer of the API. The API experiences sudden increases of usage at specific times, leading to concerns about increased costs. The company needs to limit cost and usage without reworking the Lambda functions.\n\nWhich solution will meet these requirements MOST cost-effectively?","choices":{"A":"Configure the API to send requests to Amazon Simple Queue Service (Amazon SQS) queues instead of directly to the Lambda functions. Update the Lambda functions to consume messages from the queues and to process the requests. Set up the queues to invoke the Lambda functions when new messages arrive.","D":"Create an API Gateway API Key and usage plan. Define throttling limits and quotas in the usage plan. Associate the usage plan with the Production stage and the API key. Share the API key with the external development team.","C":"Create an API Gateway API key and an AWS WAF Regional web ACL. Associate the web ACL with the Production stage. Add a rate-based rule to the web ACL. In the rule, specify the rate limit and a custom request aggregation that uses the X-API-Key header. Share the API key with the external development team.","B":"Configure provisioned concurrency for each Lambda function. Use AWS Application Auto Scaling to register the Lambda functions as targets. Set up scaling schedules to increase and decrease capacity to match changes in API usage."},"answers_community":["D (100%)"],"answer_description":"","answer_ET":"D","topic":"1","unix_timestamp":1719519540,"url":"https://www.examtopics.com/discussions/amazon/view/143000-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"hK534zGnZ72l6hM0ZI2o","question_images":[],"unix_timestamp":1719519900,"answers_community":["C (61%)","A (39%)"],"topic":"1","answer_images":[],"question_id":454,"choices":{"B":"Create an AWS Lambda function to update an Amazon Elastic File System (Amazon EFS) file share with the pricing file each time the file is updated. Update the ticketing service to use Amazon EFS to access the pricing file.","D":"Create an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS Multi-Attach to attach the volume to every EC2 instance. When a new EC2 instance launches, configure the new instance to update the pricing file on the EBS volume. Update the ticketing service to point to the new local source.","A":"Create an AWS Lambda function to update an Amazon DynamoDB table with new prices each time the pricing file is updated. Update the ticketing service to use DynramoDB to look up pricing","C":"Load Mountpoint for Amazon S3 onto the AMI of the EC2 instances. Configure Mountpoint for Amazon S3 to mount the S3 bucket that contains the pricing file. Update the ticketing service to point to the mount point and path to access the $3 object,"},"discussion":[{"timestamp":"1719599460.0","poster":"awsaz","content":"Selected Answer: C\nMountpoint for Amazon S3: This solution allows the EC2 instances to directly access the S3 bucket as if it were a local file system. This ensures that the instances always access the latest version of the pricing file without having to download it each time.\nCost-Effective: This approach avoids the need to constantly download and store the file on each instance, which can save on both S3 GET requests and local storage costs.\nSimplicity: By mounting the S3 bucket, you ensure that all instances are using the most current file without additional logic or processes to manage file updates.","comment_id":"1238836","upvote_count":"7"},{"poster":"mifune","content":"Selected Answer: A\nDynamoDB in this scenario looks cheaper than EFS. Answer A","timestamp":"1719519900.0","upvote_count":"5","comment_id":"1238385"},{"upvote_count":"1","content":"Selected Answer: C\nOption C (Mountpoint for S3) (best solution):\nMountpoint for Amazon S3 allows the EC2 instances to directly access the latest version of the pricing file stored in S3 without repeatedly downloading it. Each EC2 instance will always read the most up-to-date file directly from S3, eliminating the risk of outdated information. This solution is cost-effective as it involves minimal overhead, does not incur unnecessary data transfer or operational complexity, and requires minimal application modification.","timestamp":"1743776100.0","poster":"eesa","comment_id":"1469745"},{"upvote_count":"2","timestamp":"1733225160.0","comment_id":"1321345","poster":"nimbus_00","content":"Selected Answer: C\n\"Mountpoint for Amazon S3 is available only for Linux operating systems. You can use Mountpoint to access S3 objects in all storage classes except S3 Glacier Flexible Retrieval, S3 Glacier Deep Archive, S3 Intelligent-Tiering Archive Access Tier, and S3 Intelligent-Tiering Deep Archive Access Tier.\"\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mountpoint.html"},{"upvote_count":"2","content":"Selected Answer: C\nBy leveraging the strong consistency guarantees, cost-effectiveness, and simplicity of Mountpoint for Amazon S3, Option C provides the most appropriate and cost-effective solution for ensuring the EC2 instances in the Auto Scaling group always have access to the latest pricing information, resolving the outdated pricing data problem.\n\nThe other options have drawbacks or are less cost-effective: \nOption A: Using DynamoDB may not be cost-effective for storing and accessing a large, frequently updated pricing file with several thousand line items.\n\nOption B: While Amazon EFS is viable, it introduces additional infrastructure and potential costs compared to directly accessing the pricing file from the S3 bucket using Mountpoint for Amazon S3.\n\nOption D: Using an Amazon EBS volume with Multi-Attach would require updating the pricing file on the volume whenever a new instance launches, which is less efficient and more prone to errors than directly accessing the file from the S3 bucket.","timestamp":"1731810480.0","poster":"0b43291","comment_id":"1313352"},{"content":"Option C is most cost effective, but the question has ambiguity where it tells customer could be wrongly charged, more details should be provided on the same to understand if wrong charging is critical or not. If wrong charging is critical and needs low latency and more reliability on the queried data then its option A","comment_id":"1302770","timestamp":"1729832220.0","poster":"Danm86","upvote_count":"1"},{"timestamp":"1729430520.0","comment_id":"1300474","poster":"pk0619","content":"Selected Answer: C\nmost cost effective","upvote_count":"1"},{"poster":"chris_spencer","comments":[{"timestamp":"1733688780.0","comment_id":"1323756","upvote_count":"1","poster":"alexbraila","content":"The way I understand it, the question is not about S3 consistency. The problem is that an EC2 instance downloads the file from S3 upon launch and never updates it during its lifetime. Hence A, B and C would all solve this problem (but not D, as the file in EBS would only be updated when a new instance is launched), but the most cost-effective is C"}],"content":"none of them makes sense... if an S3 object is uploaded it is strongly consist since end 2020, eventual consistency is a matter of the past. So it doesn't matter if the lambda function get the trigger after upload and transfer the information to dynamodb (A) or EFS(b), or the ec2 instance get the object via blocklevel file access (C) or EBS (D). The consistency is being provided by the source system which is S3, so nothing helps here. From the cost perspective is C the cheapest","comment_id":"1298229","upvote_count":"1","timestamp":"1728995400.0"},{"comment_id":"1296948","poster":"JoeTromundo","timestamp":"1728828960.0","content":"Selected Answer: C\nMountpoint for Amazon S3 allows EC2 instances to treat an S3 bucket like a file system. This solution ensures that the EC2 instances always have access to the latest version of the pricing file, as the file is directly accessed from S3. You avoid downloading the file every time and reduce the risk of using outdated pricing data.\nS3 Consistency: Amazon S3 provides strong read-after-write consistency, so any update to the pricing file in S3 will be immediately visible to all EC2 instances accessing the file via the mount point.\nCost Efficiency: By using Mountpoint for Amazon S3, you leverage S3's cost-effective storage and avoid additional infrastructure like DynamoDB or Elastic File System (EFS). This solution does not require copying data to another storage system, minimizing overhead.","upvote_count":"2"},{"poster":"wbedair","upvote_count":"2","timestamp":"1726724640.0","comment_id":"1286169","content":"Selected Answer: C\nthe question is asking about cost effectiveness so why choose A to add additional service like Dynamodb . I will go for option C"},{"timestamp":"1726458600.0","poster":"liuliangzhou","upvote_count":"2","comments":[{"comment_id":"1323753","upvote_count":"1","poster":"alexbraila","timestamp":"1733688480.0","content":"I would argue that option C does solve the problem of outdated pricing data. If the ticketing service looks up the price in the local file upon every request and since the local file links to the up to date file in S3, it looks right to me"}],"comment_id":"1284444","content":"Selected Answer: A\nA. DynamoDB provides fast data access and query capabilities, suitable for frequently read but infrequently updated data.\nB. EFS may not be suitable for frequent small file updates, and its cost may be higher than using DynamoDB.\nC. This solution can directly read pricing files from S3, but it does not solve the problem of outdated pricing data being used by old instances even after the pricing files are updated.\nD. EBS is not good at Multi Attach to multiple EC2 instances, and it can increase complexity and cost."},{"upvote_count":"4","content":"Selected Answer: A\nOption A is the correct answer.","timestamp":"1723389960.0","poster":"DS2023","comment_id":"1264206"},{"comment_id":"1254184","timestamp":"1721803860.0","content":"There is no need to move away from S3","upvote_count":"2","poster":"mns0173"}],"exam_id":33,"isMC":true,"answer_description":"","question_text":"An entertainment company hosts a ticketing service on a fleet of Linux Amazon EC2 instances that are in an Auto Scaling group. The ticketing service uses a pricing file. The pricing file is stored in an Amazon S3 bucket that has S3 Standard storage. A central pricing solution that is hosted by a third party updates the pricing file.\n\nThe pricing file is updated every 1-15 minutes and has several thousand line items. The pricing file is downloaded to each EC2 instance when the instance launches.\n\nThe EC2 instances occasionally use outdated pricing information that can result in incorrect charges for customers.\n\nWhich solution will resolve this problem MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/143001-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"C","answer":"C","timestamp":"2024-06-27 22:25:00"},{"id":"oIONAKioxkvApPHrQGMU","exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/143002-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1719520080,"answer_ET":"B","choices":{"B":"Create an AWS Service Catalog product from the environment template. Add a launch constraint to the product with the existing role. Give users in the QA department permission to use AWS Service Catalog APIs only. Train users to launch the template from the AWS Service Catalog console.","A":"Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to assume the manager’s role and add a policy that restricts the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console.","C":"Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to use CloudFormation and S3 APIs, with conditions that restrict the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console.","D":"Create an AWS Elastic Beanstalk application from the environment template. Give users in the QA department permission to use Elastic Beanstalk permissions only. Train users to launch Elastic Beanstalk environments with the Elastic Beanstalk CLI, passing the existing role to the environment as a service role."},"answer_images":[],"topic":"1","timestamp":"2024-06-27 22:28:00","answer":"B","discussion":[{"content":"Selected Answer: B\nB is the answer","comment_id":"1238841","timestamp":"1719599880.0","poster":"awsaz","upvote_count":"6"},{"poster":"0b43291","timestamp":"1731810960.0","content":"Selected Answer: B\nBy using AWS Service Catalog, you can leverage its built-in features for self-service, launch constraints, and restricted permissions, making it the most appropriate solution for allowing testers to launch their own environments while limiting their access to only the necessary resources and actions.\n\nThe other options have drawbacks or do not fully address the requirements:\nOption A: Granting users permission to assume the manager's role and restricting permissions through policies can be complex to manage and may still grant broader permissions than desired.\n\nOption C: Granting users direct permission to use CloudFormation and S3 APIs, even with conditions, may still provide more access than necessary and increase the risk of unintended actions.\n\nOption D: While Elastic Beanstalk can be used to launch environments, it may not provide the same level of control and customization as a CloudFormation template. Additionally, granting Elastic Beanstalk permissions may still provide more access than necessary.","comment_id":"1313353","upvote_count":"1"},{"upvote_count":"1","timestamp":"1719520080.0","poster":"mifune","content":"Selected Answer: B\nService Catalog, answer B","comment_id":"1238386"}],"question_text":"A company has an application that uses Amazon EC2 instances in an Auto Scaling group. The quality assurance (QA) department needs to launch a large number of short-lived environments to test the application. The application environments are currently launched by the manager of the department using an AWS CloudFormation template. To launch the stack, the manager uses a role with permission to use CloudFormation, EC2, and Auto Scaling APIs. The manager wants to allow testers to launch their own environments, but does not want to grant broad permissions to each user.\n\nWhich set up would achieve these goals?","isMC":true,"question_id":455,"answers_community":["B (100%)"],"question_images":[],"answer_description":""}],"exam":{"lastUpdated":"11 Apr 2025","id":33,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isBeta":false,"provider":"Amazon","isImplemented":true,"numberOfQuestions":529,"isMCOnly":true},"currentPage":91},"__N_SSP":true}