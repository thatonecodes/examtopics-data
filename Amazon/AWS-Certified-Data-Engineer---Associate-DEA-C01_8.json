{"pageProps":{"questions":[{"id":"uVdTM49xE1hTJEPm2gBE","question_id":36,"timestamp":"2024-08-09 12:09:00","exam_id":21,"answer_images":[],"discussion":[{"poster":"PashoQ","comment_id":"1285644","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html\nregister the identity provider with Amazon Redshift, using SQL statements, which set authentication parameters that are unique to the identity provider.","upvote_count":"7","timestamp":"1726648380.0"},{"poster":"komorebi","comments":[{"poster":"Salam9","content":"Incorrect. Amazon Redshift provides native IdP federation. Therefore, you can use your third-party IdP for authentication and permission management. To use this feature, you need to register the IdP with Amazon Redshift, not with the individual clusters.","upvote_count":"1","comment_id":"1346624","timestamp":"1737833700.0"}],"comment_id":"1263251","content":"Selected Answer: A\nAnswer is A","timestamp":"1723243740.0","upvote_count":"6"},{"comment_id":"1349639","content":"Selected Answer: A\nSince the question is asking for \"The first step\", the correct answer is A.\n\"First, you register Amazon Redshift as a third-party application with your identity provider, requesting the necessary API permissions\"\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html","upvote_count":"1","timestamp":"1738349340.0","poster":"solopez_111"},{"poster":"YUICH","comments":[{"poster":"YUICH","content":"sorry my answer is A","timestamp":"1738231620.0","upvote_count":"2","comment_id":"1348999"}],"timestamp":"1738231560.0","comment_id":"1348998","upvote_count":"3","content":"Selected Answer: B\nWhy Option (A) is Correct\nRedshift Uses SAML at the Cluster Level\nTo enable single sign-on with a SAML 2.0–compatible IdP (for example, Okta or Azure AD) for Redshift Query Editor, you register the IdP by uploading its SAML metadata in the Amazon Redshift console. This is done at the cluster configuration or security level—not “within” the database engine itself.\n\nOption (B): “Within Amazon Redshift”\nThere is no direct command such as CREATE IDENTITY PROVIDER inside Redshift SQL. Federating a third-party IdP requires configuring the cluster to trust that IdP’s SAML metadata. That is done via the AWS console or CLI at the cluster level, not by running commands inside the database."},{"poster":"BigMrT","timestamp":"1736106960.0","comment_id":"1336861","content":"Selected Answer: A\nRedshift does not support directly registering the IdP \"within\" the service. The registration must be done through the cluster configuration settings.","upvote_count":"1"},{"content":"Selected Answer: B\no complete the preliminary setup between the identity provider and Amazon Redshift, you perform a couple of steps: First, you register Amazon Redshift as a third-party application with your identity provider, requesting the necessary API permissions. Then you create users and groups in the identity provider. Last, you register the identity provider with Amazon Redshift, using SQL statements, which set authentication parameters that are unique to the identity provider. As part of registering the identity provider with Redshift, you assign a namespace to make sure users and roles are grouped correctly.","comment_id":"1327274","poster":"paali","upvote_count":"2","timestamp":"1734345000.0"},{"timestamp":"1732638480.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html","upvote_count":"3","comment_id":"1318179","poster":"RockyLeon"},{"content":"To enable users to authenticate into the Amazon Redshift query editor using a third-party identity provider (IdP), the data engineer must first register that IdP within the configuration settings of the Redshift cluster itself.\n\nAmazon Redshift natively supports integrating with external identity providers to manage user authentication. By registering the third-party IdP directly in the Redshift cluster settings, it establishes the trust relationship needed for Redshift to rely on that IdP for authenticating users when they log into the query editor. Answer A","upvote_count":"5","timestamp":"1724017800.0","poster":"mzansikiller","comment_id":"1268221"}],"answers_community":["B (68%)","A (32%)"],"answer_ET":"B","unix_timestamp":1723198140,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/145294-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","choices":{"C":"Register the third-party IdP as an identity provider for AVS Secrets Manager. Configure Amazon Redshift to use Secrets Manager to manage user credentials.","B":"Register the third-party IdP as an identity provider from within Amazon Redshift.","D":"Register the third-party IdP as an identity provider for AWS Certificate Manager (ACM). Configure Amazon Redshift to use ACM to manage user credentials.","A":"Register the third-party IdP as an identity provider in the configuration settings of the Redshift cluster."},"question_images":[],"answer_description":"","question_text":"A company has implemented a lake house architecture in Amazon Redshift. The company needs to give users the ability to authenticate into Redshift query editor by using a third-party identity provider (IdP).\n\nA data engineer must set up the authentication mechanism.\n\nWhat is the first step the data engineer should take to meet this requirement?","answer":"B"},{"id":"1a338l71iO2SKBFAXhNl","unix_timestamp":1722922380,"isMC":true,"exam_id":21,"answer_description":"","question_images":[],"topic":"1","question_text":"A company currently uses a provisioned Amazon EMR cluster that includes general purpose Amazon EC2 instances. The EMR cluster uses EMR managed scaling between one to five task nodes for the company’s long-running Apache Spark extract, transform, and load (ETL) job. The company runs the ETL job every day.\n\nWhen the company runs the ETL job, the EMR cluster quickly scales up to five nodes. The EMR cluster often reaches maximum CPU usage, but the memory usage remains under 30%.\n\nThe company wants to modify the EMR cluster configuration to reduce the EMR costs to run the daily ETL job.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_ET":"C","answer":"C","answers_community":["C (100%)"],"discussion":[{"timestamp":"1731081960.0","upvote_count":"1","comment_id":"1308862","content":"Selected Answer: C\nC is the correct answer. \n\nHere is why:\nCompute optimized Amazon EC2 instances are less expensive per CPU core than general purpose instances, making them the better choice for workloads that require high processing power, as they prioritize CPU cores over memory, resulting in a lower cost per vCPU compared to general purpose instances.","poster":"AgboolaKun"},{"upvote_count":"4","timestamp":"1723052460.0","content":"Selected Answer: C\ncurrent situation shows that the EMR cluster is reaching maximum CPU usage, but memory usage remains low (under 30%). This indicates that the workload is CPU-bound rather than memory-bound.","poster":"antun3ra","comment_id":"1262188"},{"comment_id":"1261467","content":"Selected Answer: C\nSince the ETL job reaches maximum CPU usage but not memory usage, switching from general-purpose instances to compute-optimized instances (such as C5 or C6g instances) can provide better performance per dollar for CPU-bound workloads.","poster":"Shanmahi","timestamp":"1722922380.0","upvote_count":"4"}],"timestamp":"2024-08-06 07:33:00","choices":{"B":"Change the task node type from general purpose EC2 instances to memory optimized EC2 instances.","C":"Switch the task node type from general purpose Re instances to compute optimized EC2 instances.","A":"Increase the maximum number of task nodes for EMR managed scaling to 10.","D":"Reduce the scaling cooldown period for the provisioned EMR cluster."},"url":"https://www.examtopics.com/discussions/amazon/view/145106-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":37,"answer_images":[]},{"id":"49QDnPfaUjMeGru4qTz4","answers_community":["A (100%)"],"choices":{"C":"Use Apache Spark’s DataFrame dropDuplicates() API to eliminate duplicates. Write the data to the Redshift tables.","A":"Modify the AWS Glue job to copy the rows into a staging Redshift table. Add SQL commands to update the existing rows with new values from the staging Redshift table.","B":"Modify the AWS Glue job to load the previously inserted data into a MySQL database. Perform an upsert operation in the MySQL database. Copy the results to the Amazon Redshift tables.","D":"Use the AWS Glue ResolveChoice built-in transform to select the value of the column from the most recent record."},"unix_timestamp":1722922800,"answer_description":"","discussion":[{"poster":"Shanmahi","upvote_count":"7","comment_id":"1261470","content":"Selected Answer: A\nTwo step approach involving creating a staging table, followed by using Redshift's merge statement to update the target table from staging table and finally truncate/housekeep the staging table.","timestamp":"1722922800.0"}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/145107-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","isMC":true,"answer":"A","question_images":[],"question_text":"A company uploads .csv files to an Amazon S3 bucket. The company’s data platform team has set up an AWS Glue crawler to perform data discovery and to create the tables and schemas.\n\nAn AWS Glue job writes processed data from the tables to an Amazon Redshift database. The AWS Glue job handles column mapping and creates the Amazon Redshift tables in the Redshift database appropriately.\n\nIf the company reruns the AWS Glue job for any reason, duplicate records are introduced into the Amazon Redshift tables. The company needs a solution that will update the Redshift tables without duplicates.\n\nWhich solution will meet these requirements?","answer_ET":"A","timestamp":"2024-08-06 07:40:00","exam_id":21,"question_id":38,"answer_images":[]},{"id":"T7rfRHxj8NtTPis414k4","isMC":true,"topic":"1","answer_description":"","question_images":[],"unix_timestamp":1722808440,"exam_id":21,"discussion":[{"upvote_count":"5","content":"D?\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_Loading-data-from-S3.html","comment_id":"1260824","timestamp":"1722808440.0","poster":"canace"},{"upvote_count":"1","timestamp":"1723623660.0","comment_id":"1265568","content":"Selected Answer: D\nthis is D","poster":"cas_tori"},{"content":"Selected Answer: D\nA single COPY command automatically parallelizes the load operation across all nodes in the Redshift cluster. This ensures optimal use of cluster resources.","upvote_count":"3","comment_id":"1262189","poster":"antun3ra","timestamp":"1723052700.0"},{"poster":"phkhadse","content":"A - Using multiple COPY commands allows parallel loading of data, which maximizes throughput.","timestamp":"1722988920.0","comment_id":"1261871","upvote_count":"2"},{"upvote_count":"2","timestamp":"1722923040.0","content":"Selected Answer: D\nAgree with canace; Redshift's copy command uses MPP architecture to read and load in parallel from files into DWH.","poster":"Shanmahi","comment_id":"1261474"}],"question_id":39,"question_text":"A company is using Amazon Redshift to build a data warehouse solution. The company is loading hundreds of files into a fact table that is in a Redshift cluster.\n\nThe company wants the data warehouse solution to achieve the greatest possible throughput. The solution must use cluster resources optimally when the company loads data into the fact table.\n\nWhich solution will meet these requirements?","answer_images":[],"timestamp":"2024-08-04 23:54:00","choices":{"B":"Use S3DistCp to load multiple files into Hadoop Distributed File System (HDFS). Use an HDFS connector to ingest the data into the Redshift cluster.","C":"Use a number of INSERT statements equal to the number of Redshift cluster nodes. Load the data in parallel into each node.","A":"Use multiple COPY commands to load the data into the Redshift cluster.","D":"Use a single COPY command to load the data into the Redshift cluster."},"url":"https://www.examtopics.com/discussions/amazon/view/145007-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"D","answer_ET":"D","answers_community":["D (100%)"]},{"id":"x4CMK1CDTJYduX0iE8pr","timestamp":"2024-08-06 07:52:00","choices":{"B":"Train and use the AWS Glue PySpark Filter class in the ETL job.","D":"Train and use the AWS Lake Formation FindMatches transform in the ETL job.","C":"Partition tables and use the ETL job to partition the data on a unique identifier.","A":"Use Amazon Macie pattern matching as part of the ETL job."},"isMC":true,"exam_id":21,"unix_timestamp":1722923520,"answer_images":[],"topic":"1","question_text":"A company ingests data from multiple data sources and stores the data in an Amazon S3 bucket. An AWS Glue extract, transform, and load (ETL) job transforms the data and writes the transformed data to an Amazon S3 based data lake. The company uses Amazon Athena to query the data that is in the data lake.\n\nThe company needs to identify matching records even when the records do not have a common unique identifier.\n\nWhich solution will meet this requirement?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/145116-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"D","answer":"D","question_id":40,"answers_community":["D (100%)"],"discussion":[{"upvote_count":"6","content":"Selected Answer: D\nAWS Lake Formation provides machine learning capabilities to create custom transforms to cleanse your data. There is currently one available transform named FindMatches. The FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. This will not require writing any code or knowing how machine learning works.","timestamp":"1722923520.0","poster":"Shanmahi","comment_id":"1261482"},{"comment_id":"1318193","upvote_count":"1","content":"Selected Answer: D\nCorrect answer is D","timestamp":"1732639800.0","poster":"RockyLeon"},{"timestamp":"1727312640.0","comment_id":"1289237","poster":"LR2023","content":"Selected Answer: D\n","upvote_count":"1"}],"answer_description":""}],"exam":{"id":21,"name":"AWS Certified Data Engineer - Associate DEA-C01","numberOfQuestions":207,"provider":"Amazon","lastUpdated":"11 Apr 2025","isMCOnly":true,"isImplemented":true,"isBeta":false},"currentPage":8},"__N_SSP":true}