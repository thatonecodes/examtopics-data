{"pageProps":{"questions":[{"id":"klXAgwfJd09pYcwgVa9U","answers_community":["B (95%)","4%"],"timestamp":"2022-10-10 17:02:00","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/85037-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"question_id":321,"answer_description":"","discussion":[{"content":"Selected Answer: B\nHow can Session Manager benefit my organization? \nAns: No open inbound ports and no need to manage bastion hosts or SSH keys\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html","comment_id":"706211","poster":"BoboChow","upvote_count":"24","comments":[{"comments":[{"timestamp":"1669910280.0","upvote_count":"10","poster":"JayBee65","content":"Session Manager provides support for Windows, Linux, and macOS from a single tool","comment_id":"732795"},{"poster":"sohailn","content":"session manager works with linux, windows, and mac too","upvote_count":"4","timestamp":"1691553360.0","comment_id":"976219"},{"content":"\"Cross-platform support for Windows, Linux, and macOS\"\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html","timestamp":"1687537620.0","poster":"TienHuynh","comment_id":"931770","upvote_count":"3"}],"poster":"Nightducky","content":"Do you know what from the question is it Windows or Linux EC2. I think not so how you want to do SSH session for Windows?\nAnswer is C","comment_id":"718647","upvote_count":"3","timestamp":"1668505260.0"}],"timestamp":"1666939500.0"},{"poster":"cookieMr","timestamp":"1687089720.0","content":"Selected Answer: B\nOption A provides direct access to the terminal interface of each instance, but it may not be practical for administration purposes and can be cumbersome to manage, especially for multiple instances.\n\nOption C adds operational overhead and introduces additional infrastructure that needs to be managed, monitored, and secured. It also requires SSH key management and maintenance.\n\nOption D is complex and may not be necessary for remote administration. It also requires administrators to connect from their local on-premises machines, which adds complexity and potential security risks.\n\nTherefore, option B is the recommended solution as it provides secure, auditable, and repeatable remote access using IAM roles and AWS Systems Manager Session Manager, with minimal operational overhead.","comment_id":"926666","upvote_count":"9"},{"content":"Selected Answer: A\nlooks correct","poster":"Kazmin","timestamp":"1743104520.0","comment_id":"1411008","upvote_count":"1"},{"comment_id":"1338917","upvote_count":"1","content":"Selected Answer: B\nA Opção B é a solução mais simples, segura e de menor sobrecarga operacional. O uso do AWS Systems Manager Session Manager elimina a necessidade de abrir portas, gerenciar chaves SSH ou configurar infraestrutura adicional, alinhando-se ao AWS Well-Architected Framework.","timestamp":"1736533740.0","poster":"Rcosmos"},{"comment_id":"1334727","timestamp":"1735638660.0","upvote_count":"1","content":"Selected Answer: B\nCreatin an IAM Role is the most suitable as others do not meet the criteria of less operational overhead.","poster":"satyaammm"},{"poster":"safa_123","content":"B. Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.\n\nExplanation:\nMulti-Region KMS Key: AWS KMS supports multi-Region keys, which can be replicated across Regions, enabling encryption and decryption in multiple Regions using the \"same\" KMS key (though technically it's a replica). This meets the requirement of using the same key in both Regions without additional management overhead for separate keys.\n\nClient-side encryption: Configuring the application to use the KMS key with client-side encryption ensures that the data is encrypted before it is sent to Amazon S3, and decrypted when retrieved.","upvote_count":"2","timestamp":"1730710320.0","comment_id":"1306842"},{"upvote_count":"1","timestamp":"1725820500.0","poster":"zied007","content":"Selected Answer: B\nAnswer is B","comment_id":"1280535"},{"upvote_count":"4","poster":"ManikRoy","timestamp":"1713955620.0","content":"Selected Answer: B\nSession Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.","comment_id":"1201267"},{"poster":"awsgeek75","timestamp":"1705187040.0","upvote_count":"2","content":"Selected Answer: B\nA: Serial console is for device direct connection to peripherals and monitor boot etc.\nC: Workable solution but a lot of overhead\nD: Too much overhead for everyone\nB: Managed product for this purpose so least overhead.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html","comment_id":"1122111"},{"comment_id":"1121653","content":"Selected Answer: B\nAnswer-B","upvote_count":"1","poster":"A_jaa","timestamp":"1705149780.0"},{"upvote_count":"4","comments":[{"poster":"AWSStudyBuddy","comment_id":"1048120","upvote_count":"5","content":"Without requiring direct SSH connection, securely access and control EC2 instances with AWS Systems Manager Session Manager.\n\nLeast Operational Overhead: An effective and fully managed method of managing instances.\n\nWell-Architected Framework: Complies with performance, security, and reliability best practices from AWS.\n\nCons of alternative options:\n\n\n\nOption A: The automation and flexibility required for secure administration at scale are not provided by using the EC2 serial terminal directly.\n\nOption C: There is more operational overhead and complexity when a bastion host is deployed.\n\nOption D: For secure instance administration, setting up an AWS Site-to-Site VPN connection is too difficult and not the optimal approach.\n\nIn conclusion, Option B is suggested as the best option given the given circumstances.","timestamp":"1697733780.0"}],"timestamp":"1697733660.0","content":"I go with option B. Here's why--- IAM Roles: Without SSH keys or shared passwords, securely provide access to EC2 instances and AWS services.","poster":"AWSStudyBuddy","comment_id":"1048117"},{"upvote_count":"3","poster":"Guru4Cloud","content":"Selected Answer: B\nThis solution meets all of the requirements with the LEAST operational overhead. It is repeatable, uses native AWS services, and follows the AWS Well-Architected Framework.\n\nRepeatable: The process of attaching an IAM role to an EC2 instance and using Systems Manager Session Manager to establish a remote SSH session is repeatable. This can be easily automated, so that new instances can be provisioned and administrators can connect to them securely without any manual intervention.","comment_id":"975983","timestamp":"1691524260.0"},{"poster":"TariqKipkemei","upvote_count":"3","comment_id":"970744","timestamp":"1691037780.0","content":"Selected Answer: B\nWith AWS Systems Manager Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs). You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI). It provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html#:~:text=RSS-,Session%20Manager,-is%20a%20fully"},{"content":"Selected Answer: B\nKeyword \"access and administer the instances remotely and securely\" See \"AWS Systems Manager Session Manager at \" https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html .","upvote_count":"2","poster":"james2033","comment_id":"956384","timestamp":"1689752280.0"},{"poster":"miki111","upvote_count":"1","comment_id":"953631","timestamp":"1689539700.0","content":"Option B is the right answer for this."},{"upvote_count":"2","timestamp":"1687537680.0","comment_id":"931772","poster":"TienHuynh","content":"Selected Answer: B\n+Centralized access control to managed nodes using IAM policies\n+No open inbound ports and no need to manage bastion hosts or SSH keys\n+Cross-platform support for Windows, Linux, and macOS"},{"upvote_count":"1","content":"Selected Answer: B\nThe choice for me is the option B.","comment_id":"912839","timestamp":"1685711520.0","poster":"Bmarodi"},{"poster":"cheese929","comment_id":"863640","upvote_count":"1","timestamp":"1680854340.0","content":"Selected Answer: B\nB is correct and has the least overhead."},{"comment_id":"857106","poster":"linux_admin","content":"Selected Answer: B\nAWS Systems Manager Session Manager is a fully managed service that provides secure and auditable instance management without the need for bastion hosts, VPNs, or SSH keys. It provides secure and auditable access to EC2 instances and eliminates the need for managing and securing SSH keys.","timestamp":"1680268500.0","upvote_count":"1"},{"content":"Selected Answer: B\nI selected B) as \"open inbound ports, maintain bastion hosts, or manage SSH keys\" https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html However Session Manager comes with pretty robust list of prerequisites to put in place (SSM Agent and connectivity to SSM endpoints). On the other side A) come with basically no prerequisites, but it is only for Linux and we do not have info about OSs, so we should assume Windows as well.","comment_id":"849474","upvote_count":"1","timestamp":"1679677020.0","poster":"PaoloRoma"},{"comment_id":"830434","upvote_count":"2","timestamp":"1678060020.0","poster":"nour","content":"Selected Answer: B\nThe keyword that makes option B follows the AWS Well-Architected Framework is \"IAM role.\" IAM roles provide fine-grained access control and are a recommended best practice in the AWS Well-Architected Framework. By attaching the appropriate IAM role to each instance and using AWS Systems Manager Session Manager to establish a remote SSH session, the solution is using IAM roles to control access and follows a recommended best practice."},{"content":"Answer is B ~ Chat GPT\nTo meet the requirements with the least operational overhead, the company can use the AWS Systems Manager Session Manager. It is a native AWS service that enables secure and auditable access to instances without the need for remote public IP addresses, inbound security group rules, or Bastion hosts. With AWS Systems Manager Session Manager, the company can establish a secure and auditable session to the EC2 instances and perform administrative tasks without the need for additional operational overhead.","poster":"Shaw605","upvote_count":"1","comment_id":"800808","timestamp":"1675769580.0"},{"upvote_count":"1","comment_id":"800806","content":"Answer is B ~ (Chat GPT)\nA company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.\nWhich solution will meet these requirements with the LEAST operational overhead?","timestamp":"1675769460.0","poster":"Shaw605"},{"upvote_count":"1","timestamp":"1673891100.0","poster":"Pranav_523","comment_id":"778038","content":"Selected Answer: B\ncorrect answer is B"},{"timestamp":"1673045460.0","content":"Selected Answer: B\nOption B. Attaching the appropriate IAM role to each existing instance and new instance and using AWS Systems Manager Session Manager to establish a remote SSH session would meet the requirements with the least operational overhead. This approach allows for secure remote access to the instances without the need to manage additional infrastructure or maintain a separate connection to the instances. It also allows for the use of native AWS services and follows the AWS Well-Architected Framework.","upvote_count":"1","comment_id":"768129","poster":"SilentMilli"},{"upvote_count":"1","comment_id":"764161","content":"Selected Answer: B\nhttps://dev.to/aws-builders/aws-systems-manager-session-manager-implementation-f9a#:~:text=Session%20Manager%20is%20a%20fully%20managed%20AWS%20Systems,ports%2C%20maintain%20bastion%20hosts%2C%20or%20manage%20SSH%20keys.","timestamp":"1672706760.0","poster":"techhb"},{"comment_id":"757460","timestamp":"1672060860.0","content":"EC2 = IAM role","poster":"Zerotn3","upvote_count":"2"},{"comment_id":"751287","timestamp":"1671557820.0","content":"Selected Answer: B\nadminister the instances remotely and securely: \nEC2 serial console (option A) not intended for regular administration. \noption B allows administrators to remotely access and administer the instances securely without the need for additional infrastructure or maintenance. \noption C requires additional infrastructure and maintenance\noption D can be a complex and time-consuming process.","poster":"pazabal","upvote_count":"1"},{"upvote_count":"1","comments":[{"content":"AWS Systems Manager Session Manager is a native AWS service that allows you to remotely and securely access the command line interface of your Amazon EC2 instances, on-premises servers, and virtual machines (VMs) running in other clouds, without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. With Session Manager, you can establish a secure, auditable connection to your instances using the AWS Management Console, the AWS CLI, or the AWS SDKs.\n\nUsing the EC2 serial console to directly access the terminal interface of each instance for administration would not be a repeatable process and would not follow the AWS Well-Architected Framework.","upvote_count":"2","timestamp":"1671506520.0","poster":"Buruguduystunstugudunstuy","comment_id":"750442","comments":[{"timestamp":"1671506520.0","content":"Creating an administrative SSH key pair and loading the public key into each EC2 instance would require you to manage and rotate the keys, which would increase the operational overhead. Additionally, deploying a bastion host in a public subnet to provide a tunnel for administration of each instance would also increase the operational overhead and potentially introduce security risks.\n\nEstablishing an AWS Site-to-Site VPN connection and instructing administrators to use their local on-premises machines to connect directly to the instances using SSH keys across the VPN tunnel would also increase the operational overhead and potentially introduce security risks.","comment_id":"750443","poster":"Buruguduystunstugudunstuy","upvote_count":"2"}]}],"comment_id":"750441","poster":"Buruguduystunstugudunstuy","timestamp":"1671506460.0","content":"Selected Answer: B\nThe correct answer is B: Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.\n\nTo remotely and securely access and administer the Amazon EC2 instances in the company's AWS account, you should attach the appropriate IAM role to each existing instance and new instance. This will allow the instances to access the required AWS services and resources. Then, you can use AWS Systems Manager Session Manager to establish a remote SSH session to each instance."},{"content":"Selected Answer: B\nOption B - AWS best practice for remote SSH access to EC2","comment_id":"749565","upvote_count":"1","poster":"career360guru","timestamp":"1671433140.0"},{"poster":"Shasha1","timestamp":"1670925000.0","content":"B \nthe question with the least operational overhead, you can attach the appropriate IAM role to each existing instance and new instance. This will allow you to use AWS Systems Manager Session Manager to establish a remote SSH session to each instance without the need to manage SSH keys. Option C is not correct, it is because, it requires you to manage SSH keys, which can be time-consuming and error-prone.","upvote_count":"1","comment_id":"743848"},{"upvote_count":"2","poster":"JohnnyBG","comment_id":"736666","timestamp":"1670317860.0","content":"Selected Answer: B\nB, No doubt about it"},{"content":"B is correct for me","comment_id":"723521","poster":"Wpcorgan","timestamp":"1669036020.0","upvote_count":"1"},{"comment_id":"719125","timestamp":"1668545940.0","poster":"xeun88","content":"B is the right answer","upvote_count":"1"},{"comments":[{"comment_id":"732793","poster":"JayBee65","timestamp":"1669910220.0","content":"Session Manager provides support for Windows, Linux, and macOS from a single tool, so B","upvote_count":"2"},{"poster":"A_New_Guy","upvote_count":"1","content":"SSH Only work in Linux ;)","comment_id":"743412","timestamp":"1670887380.0"},{"comments":[{"upvote_count":"2","timestamp":"1668599580.0","comment_id":"719587","comments":[{"content":"B is correct","timestamp":"1668599640.0","upvote_count":"1","poster":"17Master","comment_id":"719588"}],"content":"Is correct C - https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html ------>\nCross-platform support for Windows, Linux, and macOS\n\nSession Manager provides support for Windows, Linux, and macOS from a single tool. For example, you don't need to use an SSH client for Linux and macOS managed nodes or an RDP connection for Windows Server managed nodes.","poster":"17Master"}],"content":"only works for *Linux*","comment_id":"715913","poster":"Keld","upvote_count":"1","timestamp":"1668159060.0"},{"content":"bro you can not be serious. AWS provides the SSH option to connect from anywhere to your EC2 instances, it doesn't matter the OS of those. Try it by yourself if you don't think is true","poster":"David_Ang","timestamp":"1695819420.0","comment_id":"1018809","upvote_count":"1"}],"content":"Selected Answer: C\nThe answer is C, there is no indication of which type of EC2 Windows/Linux.\nSSH only works for Windows","upvote_count":"1","timestamp":"1668159000.0","poster":"Keld","comment_id":"715912"},{"content":"The answer is C, they mentioned that it must be native service, option B is not a service, it is one of the option to connect to instances.","poster":"ManoAni","comments":[],"comment_id":"703313","upvote_count":"1","timestamp":"1666639980.0"},{"content":"Selected Answer: B\n> works with native AWS services\nthink they want you to use AWS service so B seems to make the most sense.\nand also least operational overhead","comments":[{"comment_id":"703175","content":"in real world scenario C seems to be most common solution thought..","poster":"Six_Fingered_Jose","upvote_count":"1","timestamp":"1666626540.0"}],"comment_id":"703174","upvote_count":"1","poster":"Six_Fingered_Jose","timestamp":"1666626480.0"},{"upvote_count":"2","comment_id":"695134","timestamp":"1665800940.0","content":"Selected Answer: B\nB for least operational overhead\n\nbut bastion is for best practice","poster":"ninjawrz"},{"poster":"Sinaneos","timestamp":"1665675120.0","content":"Selected Answer: B\nIt's either B or C, but B requires less operational overhead (just attaching the IAM role), so I'd pick B","comment_id":"694041","upvote_count":"3"},{"comment_id":"692800","content":"Selected Answer: C\ngood answer is C ! Bastion is all time the good answer for this case.\n\nhttps://aws.amazon.com/fr/quickstart/architecture/linux-bastion/","timestamp":"1665559020.0","poster":"masetromain","comments":[{"upvote_count":"1","timestamp":"1668212940.0","comment_id":"716395","content":"B is correct with less overhead in the question","poster":"AbhiJo"}],"upvote_count":"2"},{"timestamp":"1665555960.0","poster":"tubtab","comment_id":"692761","upvote_count":"1","content":"Selected Answer: B\nI think b is the correct answer"},{"poster":"TaiTran1994","comment_id":"691276","content":"I think C is the correct answer","timestamp":"1665414120.0","comments":[{"timestamp":"1665530100.0","content":"they say the least operational overhead so B","upvote_count":"3","poster":"Lilibell","comment_id":"692481"}],"upvote_count":"1"}],"unix_timestamp":1665414120,"answer":"B","answer_ET":"B","choices":{"A":"Use the EC2 serial console to directly access the terminal interface of each instance for administration.","B":"Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.","D":"Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel.","C":"Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance."},"question_text":"A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.\nWhich solution will meet these requirements with the LEAST operational overhead?","isMC":true,"exam_id":31,"answer_images":[]},{"id":"Dhk40VLwkQqfgPamUlui","answers_community":["C (100%)"],"answer":"C","timestamp":"2023-03-10 14:26:00","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/102134-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"unix_timestamp":1678454760,"choices":{"A":"Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.","C":"Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.","D":"Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.","B":"Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance."},"answer_description":"","exam_id":31,"question_images":[],"discussion":[{"content":"Selected Answer: C\n\"The company needs a managed solution that minimizes operational maintenance\"\n\nWatch out for NAT instances vs NAT Gateways.\n\nAs the company needs a managed solution that minimizes operational maintenance - NAT Gateway is a public subnet is the answer.","upvote_count":"9","comment_id":"837012","timestamp":"1694515200.0","poster":"UnluckyDucky"},{"comment_id":"904708","content":"C\nNat gateway can't deploy in a private subnet.","poster":"lucdt4","upvote_count":"5","timestamp":"1700733300.0"},{"poster":"von_himmlen","content":"C\nhttps://docs.aws.amazon.com/appstream2/latest/developerguide/managing-network-internet-NAT-gateway.html\n...and a NAT gateway in a public subnet.","upvote_count":"2","timestamp":"1731145320.0","comment_id":"1208743"},{"content":"Selected Answer: C\nThis meets the requirements for a managed, low maintenance solution for private subnets to access the internet:\n\nNAT gateway provides automatic scaling, high availability, and fully managed service without admin overhead.\nPlacing the NAT gateway in a public subnet with proper routes allows private instances to use it for internet access.\nMinimal operational maintenance compared to NAT instances.","comments":[{"poster":"Guru4Cloud","upvote_count":"4","comment_id":"996322","timestamp":"1709322960.0","content":"No good:\nNAT instances (A, B) require more hands-on management.\n\nPlacing a NAT gateway in a private subnet (D) would not allow internet access."}],"upvote_count":"3","poster":"Guru4Cloud","timestamp":"1709322900.0","comment_id":"996321"},{"timestamp":"1700290080.0","comment_id":"900729","upvote_count":"2","content":"Selected Answer: C\nminimizes operational maintenance = NGW","poster":"TariqKipkemei"},{"timestamp":"1695169800.0","upvote_count":"3","comment_id":"844440","content":"Selected Answer: C\nC..provision NGW in Public Subnet","poster":"WherecanIstart"},{"comment_id":"838070","poster":"cegama543","upvote_count":"2","timestamp":"1694615340.0","content":"Selected Answer: C\nccccc is the best"},{"timestamp":"1694345160.0","poster":"[Removed]","upvote_count":"3","content":"Selected Answer: C\nccccccccc","comment_id":"835024"}],"topic":"1","question_id":322,"answer_ET":"C","question_text":"A company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance.\n\nWhich solution meets these requirements?"},{"id":"thJSGSPm7qNeuRUbChJ9","discussion":[{"upvote_count":"16","content":"Selected Answer: CD\nhttps://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html#:~:text=encrypted%20Amazon%20EBS%20volumes%20without%20using%20a%20launch%20template%2C%20encrypt%20all%20new%20Amazon%20EBS%20volumes%20created%20in%20your%20account.","comment_id":"843174","timestamp":"1679174400.0","poster":"asoli","comments":[{"timestamp":"1711037880.0","content":"If you want to encrypt Amazon EBS volumes for your nodes, you can deploy the nodes using a launch template. To deploy managed nodes with encrypted Amazon EBS volumes without using a launch template, encrypt all new Amazon EBS volumes created in your account. For more information, see Encryption by default in the Amazon EC2 User Guide for Linux Instances.","poster":"bujuman","comment_id":"1179399","upvote_count":"3"}]},{"comment_id":"872554","comments":[{"comment_id":"1220468","upvote_count":"2","poster":"NSA_Poker","content":"(C) is correct; the EBS volumes of other applications in the region will not be affected bc an IAM role will limit the encryption key to the EKS cluster.","timestamp":"1716919020.0"}],"content":"Selected Answer: BD\nQuickly rule out A (which plugin? > overhead) and E because of bad practice\n\nAmong B,C,D: B and C are functionally similar > choice must be between B or C, D is fixed \n\nBetween B and C: C is out since it set default for all EBS volume in the region, which is more than required and even wrong, say what if other EBS volumes of other applications in the region have different requirement?","poster":"imvb88","upvote_count":"12","timestamp":"1681724280.0"},{"poster":"babayomi","timestamp":"1729793100.0","content":"CD\nThis options are wrongly stated in my understanding. B is wrong because you can not encrypt an unencrypted created ebs volume. You need to take a snapshot of the volume, encrypt the snapshot ,creat new ebs volume from snapshot. You can also enable encryption during creation. So the statement is wrong. The next available option can only be C, except that in that account all ebs volumes created would be encrypted, this is also questionable. Because if another person create a new ebs volumes it's automatically encrypted.","comment_id":"1302592","upvote_count":"2"},{"timestamp":"1724227320.0","content":"Selected Answer: CD\nA and E are obvious nos. D is a shoo-in.\nThe difference between B&C is basicually EBS encrption by default vs encrption. Encryption by default is by region, and encrypt everything in that region going forward, versus simple encryption is volume by volume, C is less operational overhead. Check doc & chatGPT.","comment_id":"1269955","upvote_count":"2","poster":"scaredSquirrel"},{"timestamp":"1708479720.0","upvote_count":"7","comment_id":"1155143","poster":"jjcode","content":"this one is going on my skip list","comments":[{"timestamp":"1711079040.0","content":"Don't it came for me in my exam today xd","comment_id":"1179827","comments":[{"poster":"JA2018","upvote_count":"1","comment_id":"1315401","timestamp":"1732121220.0","content":"Hi Mahmouddddddddd, can you share what were your chosen answers?"}],"upvote_count":"4","poster":"Mahmouddddddddd"}]},{"upvote_count":"2","comment_id":"1144022","timestamp":"1707363060.0","content":"If question is giving a requirement related to a particular case and asking to encrypt all data at rest; it is clear that encryption is for this case only and not for other projects in entire region. so option B is more appropriate along with option D.","poster":"jaswantn"},{"content":"Selected Answer: CD\nIt says: 'The company must encrypt ALL data at rest', so there is nothing wrong with 'enabling EBS encryption by default' . C & D","poster":"frmrkc","comments":[{"upvote_count":"1","poster":"LeonSauveterre","comment_id":"1320068","timestamp":"1732942980.0","content":"Exactly. Option B is out of the question. Not to mention option C barely has any operational overhead."}],"comment_id":"1138705","upvote_count":"5","timestamp":"1706890380.0"},{"upvote_count":"3","poster":"upliftinghut","timestamp":"1704818220.0","comment_id":"1117685","content":"Selected Answer: BD\nB&D are correct. C is wrong because when you turn on encryption by defaul, AWS uses its own key while the requirement is using Customer key. \n\nDetail is here: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encryption-by-default"},{"comment_id":"1110266","timestamp":"1703999460.0","poster":"pentium75","comments":[{"content":"\"The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS).\"\n\nI am just a bit concerned that the question does not put any limits on not encrypting all the EBS by default in the account. Both B and C can work. C is a hack but it is definitely LEAST operational overhead. Also, we don't know if there are other services or not that may be impacted. What do you think?","upvote_count":"2","comment_id":"1113224","timestamp":"1704325140.0","poster":"awsgeek75"}],"content":"Selected Answer: BD\nNot A (avoid 3rd party plugins when there are native services)\nNot C (\"encryption by default\" would impact other services)\nNot E (Keys belong in KMS, not in EKS cluster)","upvote_count":"3"},{"content":"Selected Answer: CD\nEBS encryption is set regionally. AWS account is global but it does not mean EBS encryption is enable by default at account level. default EBS encryption is a regional setting within your AWS account. Enabling it in a specific region ensures that all new EBS volumes created in that region are encrypted by default, using either the default AWS managed key or a customer managed key that you specify.","timestamp":"1702732560.0","comment_id":"1098191","poster":"Marco_St","comments":[{"timestamp":"1703999100.0","comment_id":"1110258","upvote_count":"2","content":"\"Enabling it in a specific region ensures that all new EBS volumes created in that region are encrypted by default\" which is not what we want. We want to encrypt the EBS volumes used by this EKS cluster, NOT \"all new EBS volumes created in that region.\"","poster":"pentium75"}],"upvote_count":"2"},{"content":"Selected Answer: CD\nIF you need to encrypt an unencrypted volume,\n• Create an EBS snapshot of the volume\n• Encrypt the EBS snapshot ( using copy )\n• Create new EBS volume from the snapshot ( the volume will also be encrypted )\n so it has an operational overhead.\n\nSo assuming they won't use this account for anything else we can use C. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.","upvote_count":"1","comment_id":"1058454","comments":[{"poster":"pentium75","upvote_count":"2","comment_id":"1110260","content":"\"Assuming they won't use this account for anything else\" how could we assume that?","timestamp":"1703999160.0"}],"timestamp":"1698728340.0","poster":"maudsha"},{"comment_id":"1046520","poster":"TariqKipkemei","timestamp":"1697600400.0","content":"Selected Answer: CD\nOption D is required wither way.\nTechnically both option B and C would work, but with B you would have to enable encryption node by node, while with option C provides a onetime action of enabling encryption on all nodes.\nThe requirement is the option with LEAST operational overhead.","upvote_count":"3","comments":[{"comment_id":"1110261","timestamp":"1703999220.0","content":"B created some deployment work, but NOT \"operational (!) overhead\" once it's deployed. C enables encryption by default for all new EBS volumes which is not what we want.","poster":"pentium75","upvote_count":"2"}]},{"content":"Selected Answer: CD\nThese options allow EBS encryption with the customer managed KMS key with minimal operational overhead:\n\nC) Setting the KMS key as the regional EBS encryption default automatically encrypts new EKS node EBS volumes.\n\nD) The IAM role grants the EKS nodes access to use the key for encryption/decryption operations.","upvote_count":"1","timestamp":"1693590120.0","comment_id":"996310","poster":"Guru4Cloud"},{"comment_id":"947506","poster":"jaydesai8","upvote_count":"1","timestamp":"1688929740.0","comments":[],"content":"Selected Answer: CD\nC - enable EBS encryption by default in a region -https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\n\nD - Provides key access permission just to the EKS cluster without changing broader IAM permissions"},{"upvote_count":"2","comments":[{"comments":[{"content":"Still C is wrong because \"encryption by default\" is not what we want.","upvote_count":"2","poster":"pentium75","timestamp":"1703999280.0","comment_id":"1110263"}],"comment_id":"930407","timestamp":"1687431600.0","content":"Not accurate: \"Encryption by default is a Region-specific setting\": https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encryption-by-default","upvote_count":"4","poster":"antropaws"}],"content":"Selected Answer: BD\nI was in doubt between B and C.\nYou can't \"Enable EBS encryption by default in the AWS Region\". Enable EBS encryption by default is only possible at Account level, not Region.\nB is the right option once you can enable encryption on the EBS volume with KMS and custom KMS.","poster":"pedroso","comment_id":"919942","timestamp":"1686390660.0"},{"comment_id":"914919","upvote_count":"5","content":"Selected Answer: CD\nIt's C and D. I tried it in my AWS console.\nC seems to have fewer operations ahead compared to B.","timestamp":"1685910840.0","poster":"jayce5"},{"timestamp":"1684486800.0","content":"B and C. \nUnless the key policy explicitly allows it, you cannot use IAM policies to allow access to a KMS key. Without permission from the key policy, IAM policies that allow permissions have no effect.","poster":"nauman001","comment_id":"901790","upvote_count":"1"},{"comment_id":"886232","content":"Selected Answer: BD\nB. Manually enable encryption on the intended EBS volumes after ensuring no default changes. Requires manually enabling encryption on the nodes but ensures minimum impact.\nD. Create an IAM role with access to the key to associate with the EKS cluster. This provides key access permission just to the EKS cluster without changing broader IAM permissions.","comments":[{"poster":"kruasan","timestamp":"1682944800.0","upvote_count":"3","content":"A. Using a custom plugin requires installing, managing and troubleshooting the plugin. Significant operational overhead. \nC. Modifying the default region encryption could impact other resources with different needs. Should be avoided if possible.\nE. Managing Kubernetes secrets for key access requires operations within the EKS cluster. Additional operational complexity.","comment_id":"886233"}],"timestamp":"1682944800.0","upvote_count":"3","poster":"kruasan"},{"content":"Selected Answer: BC\nB&C B&C B&C B&C B&C B&C B&C B&C B&C","upvote_count":"1","comments":[{"content":"Why enable encryption for individual volumes PLUS enable encryption by default?","comment_id":"1110265","upvote_count":"2","poster":"pentium75","timestamp":"1703999340.0"}],"poster":"neosis91","comment_id":"876164","timestamp":"1682047380.0"},{"upvote_count":"2","timestamp":"1681596480.0","content":"Selected Answer: BD\nB. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.\n\nD. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.\n\nExplanation:\n\nOption B is the simplest and most direct way to enable encryption for the EBS volumes associated with the EKS cluster. After the EKS cluster is created, you can manually locate the EBS volumes and enable encryption using the customer managed key through the AWS Management Console, AWS CLI, or SDKs.\n\nOption D involves creating an IAM role with a policy that grants permission to the customer managed key, and then associating that role with the EKS cluster. This allows the EKS cluster to have the necessary permissions to access the customer managed key for encrypting and decrypting data on the EBS volumes. This approach is more automated and can be easily managed through IAM, which provides centralized control and reduces operational overhead.","comment_id":"871352","poster":"ssha2"},{"comment_id":"858866","upvote_count":"1","poster":"kraken21","timestamp":"1680441540.0","content":"Selected Answer: CD\n\"The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service\" : All data leans towards option CD. Least operational overhead."},{"content":"Selected Answer: BD\nOption C is not necessary as enabling EBS encryption by default will apply to all EBS volumes in the region, not just those associated with the EKS cluster. Additionally, it does not specify the use of a customer managed key.","upvote_count":"3","comment_id":"850049","poster":"Russs99","timestamp":"1679741880.0","comments":[{"timestamp":"1681341540.0","poster":"tommmoe","upvote_count":"2","content":"How is it B? Option C is best practice, you can definitely specify a CMK within KMS when setting the default encryption. Please test it out yourself","comment_id":"868896"}]},{"comment_id":"848611","upvote_count":"1","content":"Selected Answer: BC\nOption A is incorrect because it suggests using a Kubernetes plugin, which may increase operational overhead.\n\nOption D is incorrect because it suggests creating an IAM role and associating it with the EKS cluster, which is not necessary for this scenario.\n\nOption E is incorrect because it suggests storing the customer managed key as a Kubernetes secret, which is not the best practice for managing sensitive data such as encryption keys.","comments":[{"comment_id":"861061","timestamp":"1680613380.0","poster":"maver144","content":"\"Option D is incorrect because it suggests creating an IAM role and associating it with the EKS cluster, which is not necessary for this scenario.\" \n\nThen your EKS cluster would not be able to access encrypted EBS volumes.","upvote_count":"1"}],"poster":"Rob1L","timestamp":"1679600220.0"},{"poster":"UnluckyDucky","content":"Selected Answer: BD\nB & D Do exactly what's required in a very simple way with the least overhead.\n\nOptions C affects all EBS volumes in the region which is absolutely not necessary here.","upvote_count":"5","comment_id":"843057","timestamp":"1679164020.0"},{"upvote_count":"1","timestamp":"1678947300.0","content":"Selected Answer: CD\nWas thinking about CD vs CE, but CD least ovearhead","poster":"Maximus007","comment_id":"840588"},{"poster":"Karlos99","comment_id":"836951","timestamp":"1678621200.0","upvote_count":"3","content":"Selected Answer: CD\nLeast overhead"},{"upvote_count":"2","comment_id":"835028","timestamp":"1678455000.0","poster":"[Removed]","content":"Selected Answer: BD\nbdbdbdbdbd"}],"answers_community":["CD (53%)","BD (45%)","3%"],"question_text":"A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media streaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS).\n\nWhich combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)","question_images":[],"exam_id":31,"isMC":true,"answer_ET":"CD","url":"https://www.examtopics.com/discussions/amazon/view/102135-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"B":"After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.","D":"Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.","C":"Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.","A":"Use a Kubernetes plugin that uses the customer managed key to perform data encryption.","E":"Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes."},"unix_timestamp":1678455000,"answer_images":[],"question_id":323,"answer_description":"","topic":"1","timestamp":"2023-03-10 14:30:00","answer":"CD"},{"id":"slwAmwNaw4DOjNDQmuy9","discussion":[{"poster":"Wayne23Fang","upvote_count":"15","comment_id":"1003315","timestamp":"1694270220.0","content":"Selected Answer: B\nAmazon prefers people to move from Oracle to its own services like DynamoDB and S3."},{"timestamp":"1678621440.0","poster":"Karlos99","content":"Selected Answer: D\nThe company wants a solution that is highly available and scalable","comment_id":"836956","upvote_count":"9","comments":[{"comment_id":"1328513","content":"another thing is the design of dynamoDB, \"Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.\" geographic code can not be the only key for any image","poster":"EllenLiu","upvote_count":"1","timestamp":"1734525840.0"},{"comment_id":"858109","poster":"[Removed]","content":"But DynamoDB is also highly available and scalable https://aws.amazon.com/dynamodb/faqs/#:~:text=DynamoDB%20automatically%20scales%20throughput%20capacity,high%20availability%20and%20data%20durability.","comments":[{"poster":"pbpally","timestamp":"1683666840.0","content":"Yes but has a size limit at 400kb so theoretically it could store images but it's not a plausible solution.","comments":[{"comments":[{"upvote_count":"5","content":"but would it be easy and cost-effective to migrate Oracle (relational db) to (Dynamodb)NoSQL?","timestamp":"1688930040.0","comment_id":"947509","poster":"jaydesai8","comments":[{"content":"Yes because it's a single table with two records, for which Oracle or any relation database has been a bad choice in the first place.","poster":"pentium75","upvote_count":"5","timestamp":"1703999700.0","comment_id":"1110267"}]}],"poster":"ruqui","timestamp":"1685192220.0","upvote_count":"8","content":"It doesn't matter the size limit of DynamoDB!!!! The images are saved in S3 buckets. Right answer is B","comment_id":"907997"}],"upvote_count":"2","comment_id":"893436"}],"timestamp":"1680366360.0","upvote_count":"5"}]},{"content":"Selected Answer: B\nDynamoDB with its HA and built-in scalability. The nature of the table also resonates with NoSQL than SQL DB such as Oracle. Only 1 table so migration is just a script from Oracle to DynamoDB\n\nD is workable but more expensive with Oracle licenses and other setups for HA and scalability","comments":[{"upvote_count":"3","content":"HA & built-in scalability of Amazon DynamoDB : https://aws.amazon.com/dynamodb/features/#:~:text=Amazon%20DynamoDB%20is%20a%20fully,for%20the%20most%20demanding%20applications.","comment_id":"1130706","poster":"upliftinghut","timestamp":"1706107140.0"}],"timestamp":"1706107140.0","poster":"upliftinghut","upvote_count":"3","comment_id":"1130704"},{"comment_id":"1113227","timestamp":"1704325800.0","upvote_count":"4","poster":"awsgeek75","content":"Selected Answer: B\nA puts images in Oracle, not a good idea\nC DAX is not going to help with images\nD It is doable but RDS on multi AZ does not give you more performance or write scalability. It gives more availability and read scalability which is not required here.\nB works as Geographic code is the key in DynamoDB and S3 image URL is the data so DynamoDB can handle tens of thousands such record and S3 can scale for writing"},{"timestamp":"1704000480.0","content":"Selected Answer: B\nThey are currently using Oracle, but only for one simple table with a single key-value pair. This is a typical use case for a NoSQL database like DynamoDB (and whoever decided to use Oracle for this in the first place should be fired). Oracle is expensive as hell, so options A and D might work but are surely not cost-effective. C won't work because the images are too big for the database. Leaves B which would be the ideal solution and meet the availability and scalability requirements.","upvote_count":"6","comment_id":"1110275","poster":"pentium75"},{"upvote_count":"3","timestamp":"1698497160.0","poster":"wsdasdasdqwdaw","comment_id":"1056204","content":"For D - Oracle is not cheap as well. RDS with Oracle vs DynamoDB, I would go for pure AWS provided option. In each exam there is a lot of marketing => B"},{"poster":"jubolano","timestamp":"1698457200.0","comment_id":"1055938","upvote_count":"2","content":"Selected Answer: D\nCost effective, D","comments":[{"upvote_count":"2","poster":"awsgeek75","content":"How is Oracle more cost effective than other options?","timestamp":"1705601460.0","comment_id":"1126104"}]},{"timestamp":"1697617080.0","comments":[{"content":"Key in sTEM states that the customer's current Oracle DB table is setup with a single key-value pair. \n\nA typical use case for a NoSQL database like DynamoDB.","poster":"JA2018","timestamp":"1732121580.0","comment_id":"1315404","upvote_count":"1"}],"poster":"wsdasdasdqwdaw","content":"B or D, but the question is MOST cost-effectively DynamoDB is more expensive than RDS, I am going for D","comment_id":"1046742","upvote_count":"3"},{"content":"Selected Answer: B\nAnswer is B, DynamoDB is Highly available and scalable","upvote_count":"2","poster":"gouranga45","comment_id":"1019227","timestamp":"1695849180.0"},{"upvote_count":"2","comment_id":"1014396","content":"A single table in a relational db can have items that are related ? e.g. ‘select * from Faculty where department_id in (10, 20) and dept_name = AWS’.\nIn the sql query example above, * means all and Faculty is name of the table.","poster":"baba365","timestamp":"1695405960.0"},{"content":"Selected Answer: B\nB option offers a cost-effective solution for storing and accessing high-resolution GIS images during natural disasters. Storing the images in Amazon S3 buckets provides scalable and durable storage, while using Amazon DynamoDB allows for quick and efficient retrieval of images based on geographic codes. This solution leverages the strengths of both S3 and DynamoDB to meet the requirements of high availability, scalability, and cost-effectiveness.","timestamp":"1693232220.0","comment_id":"992277","upvote_count":"2","poster":"Eminenza22"},{"poster":"cd93","comment_id":"983180","content":"Selected Answer: B\nWhat were the company thinking using the most expensive DB on the planet FOR ONE SINGLE TABLE???\nMigrate a single table from SQL to NoSQL should be easy enough I guess...","timestamp":"1692237720.0","upvote_count":"3"},{"comments":[{"timestamp":"1703999760.0","comment_id":"1110268","upvote_count":"2","content":"But relational DB does not make sense for the use case. It's a single table.","poster":"pentium75"}],"comment_id":"964139","timestamp":"1690399740.0","poster":"vini15","upvote_count":"3","content":"Should be D.\nthe question says company wants to migrate oracle to AWS. Oracle is a relational db hence RDS makes more sense whereas Dynamodb is non relational db."},{"upvote_count":"6","comment_id":"956718","content":"I hate these questions:) I can’t choose between B and D","timestamp":"1689775860.0","poster":"iBanan"},{"poster":"ces_9999","timestamp":"1689179400.0","upvote_count":"6","comments":[{"comments":[{"comment_id":"966534","content":"Actually now that I think about it , B sounds ok as well. Company just need to use SCT and that would be more cost effective.","poster":"Kp88","timestamp":"1690649940.0","upvote_count":"2"}],"upvote_count":"2","comment_id":"966532","timestamp":"1690649760.0","content":"You can't do migration of Oracle to Dynmodb without SCT. I am not the DB guy but since its saying oracle I would go with D otherwise B makes more sense if a company is starting out from scratch.","poster":"Kp88"}],"comment_id":"950003","content":"Guys the answer is B the oracle database only has one table without any relationships so why we should use a relational database in the first place, second we are storing the images in S3 not in the database why not use this alongside dynamo"},{"comment_id":"926447","comments":[{"timestamp":"1704000120.0","comment_id":"1110270","upvote_count":"2","poster":"pentium75","content":"Yeah, per my understanding that doesn't implicate that the destination must be an Oracle database."}],"content":"Selected Answer: D\n\"A company wants to migrate an Oracle database to AWS\"","timestamp":"1687066200.0","poster":"joehong","upvote_count":"2"},{"poster":"secdgs","upvote_count":"2","comment_id":"924701","timestamp":"1686878340.0","content":"D: Wrorng\n if you caluate License Oracle Database, It is not cost-effectively. Multi-AZ is not scalable and if you set scalable, you need more license for Oracle database."},{"poster":"secdgs","content":"Selected Answer: B\nD. wrong because RDS with multi-AZ not autoscale and guarantee database performance when \"natural disaster occurs, tens of thousands of images get updated every few minutes\"","comment_id":"921936","timestamp":"1686631800.0","upvote_count":"4"},{"comment_id":"920968","poster":"Dun6","timestamp":"1686515880.0","content":"Selected Answer: B\nThe images are stored in S3. It is the metadata of the object that is stored in DynamoDB which is obviously less than 400kb. DynamoDB key-value pair","upvote_count":"3"},{"comments":[],"timestamp":"1686474240.0","content":"Selected Answer: D\nI voted for D, highly available and scalable","poster":"MostafaWardany","upvote_count":"1","comment_id":"920526"},{"upvote_count":"4","poster":"KMohsoe","content":"Selected Answer: D\nMy option is D.\nWhy choose B? \"_\"","timestamp":"1684990320.0","comment_id":"906366"},{"upvote_count":"3","timestamp":"1684386480.0","comments":[{"comment_id":"921941","poster":"secdgs","comments":[{"content":"Agreed, the single table would work on dynamo DB. I retract my option. \nSwitching to option B.","timestamp":"1697600820.0","comment_id":"1046523","upvote_count":"3","poster":"TariqKipkemei"}],"upvote_count":"4","timestamp":"1686632760.0","content":"If you change to store image on S3, you need change code. And DB is only 1 table, SQL or NoSQL is not much difference because no table relationships."},{"timestamp":"1704000180.0","content":"And \"read scalability\" is not what is asked for, it's about write scalability","poster":"pentium75","upvote_count":"2","comment_id":"1110271"}],"content":"Selected Answer: D\nwhy would you want to change an SQL DB into a NoSQL DB.it involves code changes and rewrite of the stored procedures. For me D is the best option. You get read scalability with two readable standby DB instances by deploying the Multi-AZ DB cluster.","comment_id":"900756","poster":"TariqKipkemei"},{"poster":"kruasan","content":"Selected Answer: B\nThis uses:\n- S3 for inexpensive, scalable image storage\n- DynamoDB as an index, which can scale seamlessly and cost-effectively\n- No expensive database storage/compute required","timestamp":"1682945220.0","upvote_count":"3","comment_id":"886239"},{"poster":"SkyZeroZx","content":"Selected Answer: D\nA company wants to migrate an Oracle database to AWS. \n\nANS D","timestamp":"1682632740.0","upvote_count":"3","comment_id":"883069"},{"comment_id":"875652","upvote_count":"4","comments":[{"upvote_count":"3","timestamp":"1683666900.0","content":"Yes, 100%.","comment_id":"893437","poster":"pbpally"},{"timestamp":"1704000240.0","poster":"pentium75","upvote_count":"2","content":"DynamoDB is SQL-based too. In this use case there's only a single table. The SQL queries for options B and D would be 100 % identical.","comment_id":"1110272"}],"timestamp":"1681997580.0","poster":"kels1","content":"Guys, \"A company wants to migrate an Oracle database to AWS\" Isn't Oracle SQL based? \nSo doesn't it mean that DynamoDB is rolled out?"},{"content":"Selected Answer: B\nSimple use case, highly available, and scalable -> Choose DynamoDB over RDS in terms of cost.","poster":"jayce5","comment_id":"865258","timestamp":"1681010580.0","upvote_count":"3"},{"content":"Selected Answer: B\nB, because its a KEY-VALUE scenario","upvote_count":"4","poster":"MLCL","comment_id":"844596","timestamp":"1679295000.0"},{"timestamp":"1678947540.0","content":"Selected Answer: B\nAccording to ChatGPT","upvote_count":"2","comment_id":"840591","poster":"Maximus007"},{"timestamp":"1678687200.0","content":"Selected Answer: B\nOption B is the right answer . You cannot store high resolution images in DynamoDB due to its limitation - Maximum size of an item is 400KB","comments":[{"poster":"Nel8","comment_id":"845042","timestamp":"1679327520.0","content":"You said that DynamoDB has limitation and maximum size of an item is 400 KB. But the scenario stated \"contains millions of geographic information systems (GIS) images that are high resolution and are identified by a geographic code\", so the answer must not option B, right? As high resolution images could be more than 400 KB of size. So, DynamoDB is not the right answer here.... I go for option D.","comments":[{"comment_id":"850395","timestamp":"1679772120.0","content":"In DynamoDB you will store the geographic code and the URL, not the image so it will be less than 400Kb. You will provide tens of thousands request every few minutes, I think DynamoDB will work better than Oracle DDBB","poster":"MssP","upvote_count":"4"}],"upvote_count":"1"},{"content":"And what about that they are using Oracle DB? Is not it easier to move to RDS which will be behaving in similar way which will not keep images but only associated codes and S3 urls.\nIn my opinion it is more cost-effective to do it with RDS.","comments":[{"content":"Option D","poster":"Michal_L_95","timestamp":"1678807080.0","comment_id":"838971","upvote_count":"1"}],"timestamp":"1678807020.0","comment_id":"838970","upvote_count":"2","poster":"Michal_L_95"}],"comment_id":"837671","upvote_count":"4","poster":"KAUS2"},{"comment_id":"835042","timestamp":"1678455540.0","upvote_count":"2","content":"Selected Answer: B\nbbbbbbbbbb","poster":"[Removed]"}],"choices":{"A":"Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.","D":"Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.","C":"Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB Accelerator (DAX) during times of high load.","B":"Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value."},"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/102136-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer_images":[],"answers_community":["B (71%)","D (29%)"],"question_text":"A company wants to migrate an Oracle database to AWS. The database consists of a single table that contains millions of geographic information systems (GIS) images that are high resolution and are identified by a geographic code.\n\nWhen a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has a single image or row that is associated with it. The company wants a solution that is highly available and scalable during such events.\n\nWhich solution meets these requirements MOST cost-effectively?","question_id":324,"isMC":true,"topic":"1","timestamp":"2023-03-10 14:39:00","answer_ET":"B","unix_timestamp":1678455540,"question_images":[],"answer_description":""},{"id":"UnL1l2aNAFr71PmYHe9C","timestamp":"2023-03-10 14:43:00","answer_ET":"D","exam_id":31,"discussion":[{"comments":[{"comment_id":"895550","upvote_count":"2","poster":"ealpuche","content":"You are missing: <<The data must be available with minimal delay for up to 1 year. After one year, the data must be retained for archival purposes.>> You are secure that data after 1 year is not accessible anymore.","timestamp":"1699759620.0"}],"content":"Selected Answer: D\nAccess patterns is given, therefore D is the most logical answer.\n\nIntelligent tiering is for random, unpredictable access.","upvote_count":"13","comment_id":"837072","poster":"UnluckyDucky","timestamp":"1694519580.0"},{"poster":"TariqKipkemei","upvote_count":"5","content":"Selected Answer: D\nFirst 30 days data accessed every morning = S3 Standard\nBeyond 30 days data accessed quarterly = S3 Standard-Infrequent Access \nBeyond 1 year data retained = S3 Glacier Deep Archive","comment_id":"900766","timestamp":"1700292000.0"},{"poster":"jjcode","content":"I dont get how its A\n\n1. Each morning, the company uses the data from the previous 30 days \n2. Four times each year, the company uses the data from the previous 12 months to perform analysis and train other ML models\n3. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained for archival purposes\n\nThe data ingestion happens 4 times a year, that means that after the initial 30 days it still needs to be pulled 3 more times, why would you put the data in standard infrequent if you were going to use it 3 more times and speed is a requirement? Makes more sense to put it in S3 standard, or intelligent then straight to glacier.","comment_id":"1150751","upvote_count":"2","timestamp":"1723687020.0"},{"timestamp":"1721825340.0","comment_id":"1130724","upvote_count":"2","content":"Selected Answer: D\nClear access pattern. data in Standard-Infrequent Access is for data requires rapid access when needed","poster":"upliftinghut"},{"upvote_count":"4","comment_id":"1113229","content":"Selected Answer: D\nA and B, Intelligent Tiering cannot be configured. It is managed by AWS.\nC SIA does not allow immediate access for \"each morning\"\nD is best for 30 day standard access, SIA after 30 days and archive after 1 year","poster":"awsgeek75","timestamp":"1720043700.0"},{"timestamp":"1719718860.0","upvote_count":"2","comment_id":"1110283","poster":"pentium75","content":"Selected Answer: D\nSee reasoning below, just accidentally voted A"},{"content":"Selected Answer: A\nThe data is used every day (typical use case for Standard) for 30 days, for the remaining 12 months it is used 3 or 4 times (typical use case for IA), after 12 months it is not used at all but must be kept (typical use case for Glacier Deep Archive).","upvote_count":"2","poster":"pentium75","comment_id":"1110278","timestamp":"1719718500.0","comments":[{"comment_id":"1110279","upvote_count":"4","poster":"pentium75","content":"Sorry, D!!!!!!!!! Not A!!!! D!","timestamp":"1719718500.0"}]},{"poster":"Guru4Cloud","comment_id":"996235","upvote_count":"3","timestamp":"1709316720.0","content":"Selected Answer: D\nThis option optimizes costs while meeting the data access requirements:\n\nStore new data in S3 Standard for first 30 days of frequent access\nTransition to S3 Standard-IA after 30 days for infrequent access up to 1 year\nArchive to Glacier Deep Archive after 1 year for long-term archival"},{"upvote_count":"1","content":"Selected Answer: A\nOption A meets the requirements most cost-effectively. The S3 Intelligent-Tiering storage class provides automatic tiering of objects between the S3 Standard and S3 Standard-Infrequent Access (S3 Standard-IA) tiers based on changing access patterns, which helps optimize costs. The S3 Lifecycle policy can be used to transition objects to S3 Glacier Deep Archive after 1 year for archival purposes. This solution also meets the requirement for minimal delay in accessing data for up to 1 year. Option B is not cost-effective because it does not include the transition of data to S3 Glacier Deep Archive after 1 year. Option C is not the best solution because S3 Standard-IA is not designed for long-term archival purposes and incurs higher storage costs. Option D is also not the most cost-effective solution as it transitions objects to the S3 Standard-IA tier after 30 days, which is unnecessary for the requirement to retrain the suite of ML models each morning using data from the previous 30 days.","comment_id":"895547","comments":[{"upvote_count":"3","timestamp":"1719718440.0","content":"I can't follow. The data is used every day (typical use case for Standard) for 30 days, for the remaining 12 months it is used 3 or 4 times (typical use case for IA), after 12 months it is not used at all but must be kept (typical use case for Glacier Deep Archive).","comment_id":"1110277","poster":"pentium75"}],"poster":"ealpuche","timestamp":"1699759500.0"},{"upvote_count":"2","timestamp":"1694569740.0","content":"Selected Answer: D\nAgree with UnluckyDucky , the correct option is D","poster":"KAUS2","comment_id":"837607"},{"timestamp":"1694559540.0","comment_id":"837503","upvote_count":"3","poster":"fkie4","content":"Selected Answer: D\nShould be D. see this:\nhttps://www.examtopics.com/discussions/amazon/view/68947-exam-aws-certified-solutions-architect-associate-saa-c02/"},{"comments":[],"content":"Selected Answer: B\nBbbbbbbbb","timestamp":"1694448480.0","upvote_count":"1","poster":"Nithin1119","comment_id":"836366"},{"comment_id":"835050","upvote_count":"4","comments":[{"timestamp":"1694346180.0","upvote_count":"9","comment_id":"835051","poster":"[Removed]","content":"D because:\n- First 30 days- data access every morning ( predictable and frequently) – S3 standard\n- After 30 days, accessed 4 times a year – S3 infrequently access\n- Data preserved- S3 Gllacier Deep Archive"}],"content":"Selected Answer: D\nddddddd","poster":"[Removed]","timestamp":"1694346180.0"}],"question_id":325,"answers_community":["D (90%)","7%"],"question_text":"A company has an application that collects data from IoT sensors on automobiles. The data is streamed and stored in Amazon S3 through Amazon Kinesis Data Firehose. The data produces trillions of S3 objects each year. Each morning, the company uses the data from the previous 30 days to retrain a suite of machine learning (ML) models.\n\nFour times each year, the company uses the data from the previous 12 months to perform analysis and train other ML models. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained for archival purposes.\n\nWhich storage solution meets these requirements MOST cost-effectively?","unix_timestamp":1678455780,"answer_description":"","question_images":[],"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/102137-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.","A":"Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.","D":"Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year.","B":"Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically move objects to S3 Glacier Deep Archive after 1 year."},"isMC":true,"answer":"D"}],"exam":{"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Associate SAA-C03","provider":"Amazon","isBeta":false,"numberOfQuestions":1019,"isMCOnly":true,"isImplemented":true,"id":31},"currentPage":65},"__N_SSP":true}