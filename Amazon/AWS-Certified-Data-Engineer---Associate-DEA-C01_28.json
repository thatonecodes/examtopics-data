{"pageProps":{"questions":[{"id":"4r5fcqVbLrOjfQLtuux9","question_images":[],"unix_timestamp":1707222240,"answer_images":[],"topic":"1","answer_description":"","discussion":[{"timestamp":"1728771300.0","content":"Selected Answer: D\nCross-Account Delivery: Kinesis Data Streams in the security account ensures the logs reside in the designated security-focused environment.\nCloudWatch Logs Integration: Granting CloudWatch Logs permissions to put records into the Kinesis Data Stream directly establishes a streamlined and secure data flow from the production account.\nFiltering Controls: The subscription filter in the production account provides precise control over which log events are sent to the security account.","poster":"Christina666","upvote_count":"6","comment_id":"1194563"},{"poster":"Salam9","comment_id":"1346223","upvote_count":"2","timestamp":"1737749460.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters-AccountLevel.html#DestinationKinesisExample-AccountLevel"},{"content":"1. **Cross-Account Access:**\n - AWS Documentation: [Cross-Account Access]\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html \n - This documentation provides detailed instructions on how to set up cross-account access using IAM roles and trust policies, which is essential for allowing CloudWatch Logs in one AWS account to put data into a Kinesis Data Stream in another AWS account.\n\n2. **Configuring CloudWatch Logs Subscription Filters:**\n - AWS Documentation: [Subscription Filters for Amazon CloudWatch Logs]\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html \n - This documentation explains how to create subscription filters for CloudWatch Logs, which enable you to route log data to various destinations, including Kinesis Data Streams. Placing the subscription filter in the production AWS account ensures that only the relevant security logs are sent to the Kinesis Data Stream in the security AWS account.","upvote_count":"2","timestamp":"1726935960.0","poster":"certplan","comment_id":"1179496"},{"upvote_count":"2","timestamp":"1726036560.0","poster":"GiorgioGss","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions-Kinesis.html","comment_id":"1170894"},{"comment_id":"1142142","upvote_count":"1","timestamp":"1722939840.0","poster":"Aesthet","content":"Both ChatGPT and me agree with anser D"}],"answer":"D","exam_id":21,"answer_ET":"D","isMC":true,"choices":{"C":"Create a destination data stream in the production AWS account. In the production AWS account, create an IAM role that has cross-account permissions to Kinesis Data Streams in the security AWS account.","A":"Create a destination data stream in the production AWS account. In the security AWS account, create an IAM role that has cross-account permissions to Kinesis Data Streams in the production AWS account.","D":"Create a destination data stream in the security AWS account. Create an IAM role and a trust policy to grant CloudWatch Logs the permission to put data into the stream. Create a subscription filter in the production AWS account.","B":"Create a destination data stream in the security AWS account. Create an IAM role and a trust policy to grant CloudWatch Logs the permission to put data into the stream. Create a subscription filter in the security AWS account."},"timestamp":"2024-02-06 13:24:00","url":"https://www.examtopics.com/discussions/amazon/view/133056-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["D (100%)"],"question_text":"A company has a production AWS account that runs company workloads. The company's security team created a security AWS account to store and analyze security logs from the production AWS account. The security logs in the production AWS account are stored in Amazon CloudWatch Logs.\nThe company needs to use Amazon Kinesis Data Streams to deliver the security logs to the security AWS account.\nWhich solution will meet these requirements?","question_id":136},{"id":"KLgXUAqR0p6UmCGuHpBf","answer_ET":"C","question_images":[],"question_text":"A company uses Amazon S3 to store semi-structured data in a transactional data lake. Some of the data files are small, but other data files are tens of terabytes.\nA data engineer must perform a change data capture (CDC) operation to identify changed data from the data source. The data source sends a full snapshot as a JSON file every day and ingests the changed data into the data lake.\nWhich solution will capture the changed data MOST cost-effectively?","discussion":[{"comment_id":"1170898","timestamp":"1710146520.0","upvote_count":"7","poster":"GiorgioGss","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/big-data/implement-a-cdc-based-upsert-in-a-data-lake-using-apache-iceberg-and-aws-glue/"},{"content":"Selected Answer: A\nGenerally, AWS questions never give preference to the others solution than an AWS service so even if C could be better the answer is A","comment_id":"1339524","poster":"plutonash","timestamp":"1736689020.0","upvote_count":"1"},{"comment_id":"1262346","content":"https://aws.amazon.com/blogs/big-data/choosing-an-open-table-format-for-your-transactional-data-lake-on-aws/","poster":"influxy","upvote_count":"1","timestamp":"1723098960.0"},{"content":"Selected Answer: C\nIll go with Delta or something like that. is C","poster":"FunkyFresco","timestamp":"1716727800.0","upvote_count":"2","comment_id":"1218982"},{"content":"Relative to cost, here are docs for the reason for option C:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html \nhttps://aws.amazon.com/blogs/big-data/ \nhttps://docs.aws.amazon.com/glue/latest/dg/welcome.html \nhttps://docs.aws.amazon.com/emr/ \n\nHere are docs for reasons the others are not correct:\nhttps://aws.amazon.com/lambda/pricing/ \nhttps://aws.amazon.com/rds/pricing/\nhttps://aws.amazon.com/dms/pricing/","upvote_count":"2","comment_id":"1179805","timestamp":"1711076100.0","poster":"certplan"},{"comment_id":"1163736","timestamp":"1709324700.0","content":"Answ. D\nYou can migrate data from any MySQL-compatible database (MySQL, MariaDB, or Amazon Aurora MySQL) using AWS Database Migration Service. \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html","poster":"damaldon","upvote_count":"1","comments":[{"poster":"GiorgioGss","upvote_count":"6","comment_id":"1170897","timestamp":"1710146460.0","content":"\"other data files are tens of terabytes\" - good luck with DMS on that :) I think it's C"}]},{"poster":"[Removed]","upvote_count":"4","timestamp":"1705799580.0","comments":[{"upvote_count":"4","timestamp":"1707827940.0","poster":"Houyon","content":"If all files were small I believe it would be a great idea. However, you wouldn't be able to compare heavy files with lambda due to its memory/capacity and runtime constraints","comment_id":"1149157"}],"comment_id":"1127563","content":"Selected Answer: C\nThis is a tricky one. Although option A seems like the best choice since it uses an AWS service, I believe using Delta/Iceberg APIs would be easier than writing custom code on Lambda"}],"exam_id":21,"unix_timestamp":1705799580,"answers_community":["C (93%)","7%"],"answer_images":[],"timestamp":"2024-01-21 02:13:00","question_id":137,"answer":"C","isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/131705-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"D":"Ingest the data into an Amazon Aurora MySQL DB instance that runs Aurora Serverless. Use AWS Database Migration Service (AWS DMS) to write the changed data to the data lake.","C":"Use an open source data lake format to merge the data source with the S3 data lake to insert the new data and update the existing data.","A":"Create an AWS Lambda function to identify the changes between the previous data and the current data. Configure the Lambda function to ingest the changes into the data lake.","B":"Ingest the data into Amazon RDS for MySQL. Use AWS Database Migration Service (AWS DMS) to write the changed data to the data lake."},"answer_description":""},{"id":"4ajoskCfp3cQI0udeXFx","question_images":[],"exam_id":21,"choices":{"B":"Bucket the data based on a column that the data have in common in a WHERE clause of the user query.","A":"Create an AWS Glue partition index. Enable partition filtering.","E":"Use the Amazon EMR S3DistCP utility to combine smaller objects in the S3 bucket into larger objects.","C":"Use Athena partition projection based on the S3 bucket prefix.","D":"Transform the data that is in the S3 bucket to Apache Parquet format."},"answer":"AC","answer_description":"","isMC":true,"answer_ET":"AC","discussion":[{"poster":"rralucard_","content":"Selected Answer: AC\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nOptimizing Partition Processing using partition projection\nProcessing partition information can be a bottleneck for Athena queries when you have a very large number of partitions and aren’t using AWS Glue partition indexing. You can use partition projection in Athena to speed up query processing of highly partitioned tables and automate partition management. Partition projection helps minimize this overhead by allowing you to query partitions by calculating partition information rather than retrieving it from a metastore. It eliminates the need to add partitions’ metadata to the AWS Glue table.","timestamp":"1722488100.0","upvote_count":"7","comment_id":"1137343"},{"content":"Selected Answer: AC\nBucketing not address the problem of having a large number of partitions in the metadata, which is the root cause of the query planning bottleneck.\nConverting to a columnar format like Apache Parquet will not directly reduce the overhead associated with managing a large number of partitions.\nCombining small objects will not mitigate the planning overhead that comes from a large number of partitions in the data catalog. Hence A and C","comment_id":"1355458","upvote_count":"2","poster":"Mahidbdwh","timestamp":"1739337360.0"},{"content":"Selected Answer: AE\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","poster":"SMALLAM","timestamp":"1736714880.0","upvote_count":"1","comment_id":"1339666"},{"upvote_count":"1","timestamp":"1733793180.0","content":"Selected Answer: AC\nCreating an AWS Glue partition index and enabling partition filtering can significantly improve query performance when dealing with large datasets with many partitions. The partition index allows Athena to quickly identify the relevant partitions for a query, reducing the time spent scanning unnecessary data. Partition filtering further optimizes the query by only scanning the partitions that match the filter conditions.\nAthena partition projection based on the S3 bucket prefix is another effective technique to improve query performance. By leveraging the bucket prefix structure, Athena can prune partitions that are not relevant to the query, reducing the amount of data that needs to be scanned and processed. This approach is particularly useful when the data is organized in a hierarchical structure within the S3 bucket.","comment_id":"1227563","poster":"pypelyncar"},{"poster":"VerRi","upvote_count":"1","timestamp":"1732023600.0","comment_id":"1213759","content":"Selected Answer: AC\nD is not correct because the issue is related to partitioning."},{"content":"Selected Answer: AC\nI guess A / C, beucase we faced with - query plans performance bottleneck, so indexing should be improved","poster":"HunkyBunky","comment_id":"1204844","upvote_count":"1","timestamp":"1730440260.0"},{"poster":"khchan123","comment_id":"1203041","timestamp":"1730028000.0","content":"A. Creating an AWS Glue partition index and enabling partition filtering can help improve query performance by allowing Athena to prune unnecessary partitions from the query plan. This can reduce the number of partitions that need to be scanned, resulting in faster query planning times.\n\nC. Athena partition projection allows you to define a partition scheme based on the S3 bucket prefix. This can help reduce the number of partitions that need to be scanned, as Athena can use the prefix to determine which partitions are relevant to the query. This can also help improve query performance and reduce planning times.","upvote_count":"2"},{"timestamp":"1728844200.0","upvote_count":"1","comment_id":"1195093","content":"The right answer is BD","poster":"okechi"},{"poster":"Christina666","comment_id":"1194568","content":"Selected Answer: AD\nA. Create an AWS Glue partition index. Enable partition filtering.\nTargeted Optimization: Partition indexes within the Glue Data Catalog help Athena efficiently identify the relevant partitions, significantly reducing query planning time. Partition filtering further refines the search during query execution.\nD. Transform the data that is in the S3 bucket to Apache Parquet format.\nEfficient Columnar Format: Parquet's columnar storage and built-in metadata often allow Athena to skip over large portions of data irrelevant to the query, leading to faster query planning and execution.","timestamp":"1728772020.0","upvote_count":"3"},{"timestamp":"1727147880.0","content":"Selected Answer: AC\nKeyword: Athena query planning time\n\nSee explanation in the link:\nhttps://www.myexamcollection.com/Data-Engineer-Associate-vce-questions.htm\n\nB & D are related to analytical queries performance, not about \"query planning\" performance.","upvote_count":"4","comment_id":"1181325","poster":"fceb2c1"},{"upvote_count":"2","content":"Just finished the exam and I went with AD. I agree with GiorgioGss, but the reason why I picked A over C was becaues the table is already using Glue catalog.\nIf we use the indexes, there's no reason to use C as we already have the partitions indexed. \nNo reason to pick B if we have C selected.\nThus I picked D with this to optimize the query e.g. if I'm only selecting a subset of the columns.","timestamp":"1726314300.0","comment_id":"1173449","poster":"ottarg"},{"upvote_count":"1","comment_id":"1171090","content":"Strange questions.... it can be ABCD","poster":"GiorgioGss","timestamp":"1726060620.0"},{"comment_id":"1137345","content":"If your table stored in an AWS Glue Data Catalog has tens and hundreds of thousands and millions of partitions, you can enable partition indexes on the table. With partition indexes, only the metadata for the partition value in the query’s filter is retrieved from the catalog instead of retrieving all the partitions’ metadata. The result is faster queries for such highly partitioned tables. The following table compares query runtimes between a partitioned table with no partition indexing and with partition indexing. The table contains approximately 100,000 partitions and uncompressed text data. The orders table is partitioned by the o_custkey column.","poster":"rralucard_","timestamp":"1722488160.0","upvote_count":"1"},{"poster":"[Removed]","comment_id":"1127567","content":"Selected Answer: BD\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","upvote_count":"2","timestamp":"1721517900.0"}],"question_id":138,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/131708-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["AC (73%)","14%","9%"],"question_text":"A data engineer runs Amazon Athena queries on data that is in an Amazon S3 bucket. The Athena queries use AWS Glue Data Catalog as a metadata table.\nThe data engineer notices that the Athena query plans are experiencing a performance bottleneck. The data engineer determines that the cause of the performance bottleneck is the large number of partitions that are in the S3 bucket. The data engineer must resolve the performance bottleneck and reduce Athena query planning time.\nWhich solutions will meet these requirements? (Choose two.)","timestamp":"2024-01-21 02:25:00","unix_timestamp":1705800300,"answer_images":[]},{"id":"uQvw7iigkdZEguu48hkD","answer":"D","question_text":"A data engineer must manage the ingestion of real-time streaming data into AWS. The data engineer wants to perform real-time analytics on the incoming streaming data by using time-based aggregations over a window of up to 30 minutes. The data engineer needs a solution that is highly fault tolerant.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"D","question_id":139,"question_images":[],"answer_description":"","answer_images":[],"exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/132739-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","unix_timestamp":1706971200,"isMC":true,"answers_community":["D (100%)"],"timestamp":"2024-02-03 15:40:00","choices":{"D":"Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to analyze the data by using multiple types of aggregations to perform time-based analytics over a window of up to 30 minutes.","C":"Use an AWS Lambda function that includes both the business and the analytics logic to perform aggregations for a tumbling window of up to 30 minutes, based on the event timestamp.","A":"Use an AWS Lambda function that includes both the business and the analytics logic to perform time-based aggregations over a window of up to 30 minutes for the data in Amazon Kinesis Data Streams.","B":"Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to analyze the data that might occasionally contain duplicates by using multiple types of aggregations."},"discussion":[{"comment_id":"1139312","poster":"rralucard_","timestamp":"1706971200.0","content":"Selected Answer: D\nD. Amazon Managed Service for Apache Flink for Time-Based Analytics over 30 Minutes: This option correctly identifies the use of Amazon Managed Service for Apache Flink for performing time-based analytics over a window of up to 30 minutes. Apache Flink is adept at handling such scenarios, providing capabilities for complex event processing, time-windowed aggregations, and maintaining state over time. This option would offer high fault tolerance and minimal operational overhead due to the managed nature of the service.","upvote_count":"7"},{"content":"Selected Answer: D\nLambda can not be used because it max processing limit of time is 15 min and remaining two option related to flink and using flink we can perfrom time-series and window size aggregation","upvote_count":"2","timestamp":"1737528420.0","poster":"div_div","comment_id":"1344641"},{"comment_id":"1255929","upvote_count":"2","content":"This link is not AWS documents but I think you guys can take a look.\nhttps://amandeep-singh-johar.medium.com/real-time-stream-processing-with-apache-flink-153992840f16","timestamp":"1722041280.0","poster":"Linuslin"},{"content":"Selected Answer: D\nShow the Docs","timestamp":"1715865120.0","poster":"Just_Ninja","comment_id":"1212424","upvote_count":"2"},{"timestamp":"1714974840.0","poster":"DevoteamAnalytix","upvote_count":"1","comment_id":"1207204","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/managed-flink/latest/java/how-operators.html#how-operators-agg"},{"poster":"harrura","upvote_count":"2","timestamp":"1711815780.0","comment_id":"1186186","content":"this is crazy, the answers by bot are wrong, please don't rely on them. please care to open discussions and look for reasoning"}],"topic":"1"},{"id":"pV6eYyZ7owMPeqbYAtvw","answers_community":["C (100%)"],"question_id":140,"choices":{"D":"Use AWS DataSync to create new gp3 volumes. Transfer the data from the original gp2 volumes to the new gp3 volumes.","C":"Change the volume type of the existing gp2 volumes to gp3. Enter new values for volume size, IOPS, and throughput.","B":"Create new gp3 volumes. Gradually transfer the data to the new gp3 volumes. When the transfer is complete, mount the new gp3 volumes to the EC2 instances to replace the gp2 volumes.","A":"Create snapshots of the gp2 volumes. Create new gp3 volumes from the snapshots. Attach the new gp3 volumes to the EC2 instances."},"question_images":[],"unix_timestamp":1707022260,"answer_description":"","answer_ET":"C","question_text":"A company is planning to upgrade its Amazon Elastic Block Store (Amazon EBS) General Purpose SSD storage from gp2 to gp3. The company wants to prevent any interruptions in its Amazon EC2 instances that will cause data loss during the migration to the upgraded storage.\nWhich solution will meet these requirements with the LEAST operational overhead?","timestamp":"2024-02-04 05:51:00","topic":"1","answer_images":[],"answer":"C","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/132762-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","discussion":[{"timestamp":"1726724820.0","poster":"GiorgioGss","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/","upvote_count":"6","comment_id":"1177062"},{"comment_id":"1181328","poster":"fceb2c1","content":"Selected Answer: C\nOption C: Check section under \"To modify an Amazon EBS volume using the AWS Management Console“ in GiorgioGss's link\nAmazon EBS Elastic Volumes enable you to modify your volume type from gp2 to gp3 without detaching volumes or restarting instances (requirements for modification), which means that there are no interruptions to your applications during modification.","upvote_count":"6","timestamp":"1727148240.0"},{"timestamp":"1737923400.0","comment_id":"1347105","poster":"lcsantos99","content":"Selected Answer: C\nthe correct answer is C\n\nhttps://aws.amazon.com/pt/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/","upvote_count":"1"},{"timestamp":"1722739860.0","comment_id":"1139792","content":"Selected Answer: C\nOption C is the most straightforward and efficient approach to upgrading from gp2 to gp3 EBS volumes, providing an in-place upgrade path with minimal operational overhead and no interruption in service.","poster":"rralucard_","upvote_count":"2"}],"isMC":true}],"exam":{"name":"AWS Certified Data Engineer - Associate DEA-C01","lastUpdated":"11 Apr 2025","id":21,"isImplemented":true,"isMCOnly":true,"provider":"Amazon","isBeta":false,"numberOfQuestions":207},"currentPage":28},"__N_SSP":true}