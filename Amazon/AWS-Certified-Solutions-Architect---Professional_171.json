{"pageProps":{"questions":[{"id":"2pWfnyjhBnh43DDCN8n4","question_images":[],"answer_ET":"D","choices":{"C":"Create a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling group's launch template to the new AMI. Run an Amazon EC2 Auto Scaling instance refresh operation.","B":"Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete, create a new AMI and configure the Auto Scaling group's launch template to use the new AMI for new launches. Resume Amazon EC2 Auto Scaling operations.","A":"Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code the Lambda function to associate the EC2 instances with the CodeDeploy deployment group.","D":"Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group's launch template to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling group instead of the EC2 instances."},"answer_images":[],"unix_timestamp":1650664500,"exam_id":32,"question_text":"A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to deploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.\nWhen the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and associates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.\nWhat should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/74176-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","discussion":[{"content":"Selected Answer: D\ncode deployment group with ASG.","comment_id":"591076","poster":"Bigbearcn","upvote_count":"6","timestamp":"1650806100.0"},{"poster":"shailurtm2001","upvote_count":"5","content":"Correct answer D https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html","comment_id":"590255","timestamp":"1650664500.0"},{"upvote_count":"1","poster":"evargasbrz","timestamp":"1672770000.0","content":"Selected Answer: D\nI'll go with D","comment_id":"764926"},{"upvote_count":"2","poster":"sindra","comment_id":"706499","content":"Selected Answer: D\nvote D","timestamp":"1666964040.0"},{"timestamp":"1661949600.0","poster":"gnic","comment_id":"655076","content":"Selected Answer: D\nD is more logical","upvote_count":"3"}],"timestamp":"2022-04-22 23:55:00","answer":"D","topic":"1","question_id":851,"answers_community":["D (100%)"],"isMC":true},{"id":"yUgKwkuhKsvBCcl7rsyf","topic":"1","exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/73981-exam-aws-certified-solutions-architect-professional-topic-1/","discussion":[{"content":"C is correct","timestamp":"1650515280.0","upvote_count":"9","comment_id":"589083","poster":"mostafasookar"},{"upvote_count":"4","content":"Selected Answer: C\nCluster = Close together for high performance, networking etc.\nPartition = Spread so they don't share the same underlying hardware\nSpread = Similar to above, but alot stricter.","poster":"janvandermerwer","timestamp":"1668063600.0","comment_id":"714988"},{"timestamp":"1663502820.0","content":"Selected Answer: C\nThe cluster placement group is indicated for high troughput and low network latency, while the partition placement group is used to avoid hw failures separating the instances in groups running over different hardware.","comment_id":"672315","upvote_count":"4","poster":"joancarles"},{"upvote_count":"3","content":"Selected Answer: A\ndistributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","comment_id":"669487","poster":"Sathish1412","timestamp":"1663218120.0"},{"comments":[{"comment_id":"642891","upvote_count":"1","poster":"gondohwe","content":"yes from correct indeed..a Cluster placement group allows intsances to be grouped together minimizing latency between them","timestamp":"1659693360.0"}],"upvote_count":"3","comment_id":"632969","poster":"Enigmaaaaaa","timestamp":"1658139960.0","content":"Selected Answer: C\nC - memory instance + cluster to have lowest possible network latency."}],"unix_timestamp":1650515280,"answer":"C","question_images":[],"isMC":true,"answer_description":"","answers_community":["C (79%)","A (21%)"],"answer_images":[],"question_id":852,"answer_ET":"C","choices":{"D":"Launch compute optimized EC2 instances in a spread placement group.","B":"Launch compute optimized EC2 instances in a partition placement group.","A":"Launch memory optimized EC2 instances in a partition placement group.","C":"Launch memory optimized EC2 instances in a cluster placement group"},"question_text":"A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleet consists of a primary node and eight worker nodes. The primary node is responsible for monitoring cluster health, accepting user requests, distributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions.\nThe company requires the lowest possible networking latency to achieve maximum performance.\nWhich solution will meet these requirements?","timestamp":"2022-04-21 06:28:00"},{"id":"7WW6Osrq6umpop5Z2RZ5","question_id":853,"discussion":[{"comment_id":"590260","poster":"shailurtm2001","upvote_count":"9","content":"AD. https://aws.amazon.com/premiumsupport/knowledge-center/s3-upload-large-files/","timestamp":"1650665220.0"},{"poster":"Enigmaaaaaa","timestamp":"1658140260.0","comment_id":"632970","upvote_count":"3","content":"Selected Answer: AD\nAD\nS3 TA + Multipart"},{"upvote_count":"2","comment_id":"602494","timestamp":"1652689320.0","content":"Selected Answer: AD\nTransfer Accelerator + Multi-part uploads for files more 500MB","poster":"bobsmith2000"},{"comment_id":"592656","content":"BC: Only for Australia users, so not file size issue, A3 accelerator is configured for bucket, not from app clinet","poster":"cooldeity","comments":[{"timestamp":"1659153360.0","content":"Bucket is in the US.\nThe application uploads a variety of files to an Amazon S3 bucket located in the us-east-1 region","comment_id":"639463","poster":"cen007","upvote_count":"1"}],"timestamp":"1651002240.0","upvote_count":"4"}],"exam_id":32,"question_text":"A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-east-1 Region. The files range in size from 1 GB to 10 GB.\nUsers who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload for these users. A solutions architect must improve the app's performance for these uploads.\nWhich solutions will meet these requirements? (Choose two.)","answers_community":["AD (100%)"],"answer_images":[],"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/74177-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"AD","answer":"AD","unix_timestamp":1650665220,"timestamp":"2022-04-23 00:07:00","question_images":[],"choices":{"A":"Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.","E":"Modify the app to add random prefixes to the files before uploading.","B":"Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket.","D":"Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.","C":"Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region."},"topic":"1"},{"id":"OHLvSaa3YzgSMEr64vX0","unix_timestamp":1617625800,"topic":"1","answer":"B","answer_description":"Reference:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html","isMC":true,"question_id":854,"answers_community":["B (100%)"],"answer_images":[],"exam_id":32,"timestamp":"2021-04-05 14:30:00","choices":{"B":"Create an IAM role for EC2 that allows list access to objects in the S3 bucket; launch the Instance with the role, and retrieve the role's credentials from the EC2 instance metadata.","A":"Use the AWS account access keys; the application retrieves the credentials from the source code of the application.","C":"Create an IAM user for the application with permissions that allow list access to the S3 bucket; the application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the Application user.","D":"Create an IAM user for the application with permissions that allow list access to the S3 bucket; launch the instance as the IAM user, and retrieve the IAM user's credentials from the EC2 instance user data."},"url":"https://www.examtopics.com/discussions/amazon/view/49196-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"You have an application running on an EC2 instance which will allow users to download files from a private S3 bucket using a pre-signed URL. Before generating the URL, the application should verify the existence of the file in S3.\nHow should the application use AWS credentials to access the S3 bucket securely?","answer_ET":"B","question_images":[],"discussion":[{"timestamp":"1728450360.0","comment_id":"1294964","content":"IAM Role for EC2: By assigning an IAM role to the EC2 instance, you provide the necessary permissions for the application to access the S3 bucket without hardcoding credentials.","poster":"nimbus_00","upvote_count":"1"},{"comment_id":"1266965","content":"B. Create an IAM role for EC2 that allows list access to objects in the S3 bucket; launch the Instance with the role, and retrieve the role's credentials from the EC2 instance metadata.","timestamp":"1723803840.0","poster":"amministrazione","upvote_count":"1"},{"poster":"SkyZeroZx","upvote_count":"1","comment_id":"927904","content":"Selected Answer: B\nB.\nthe keyword here is IAM role and metadata.","timestamp":"1687206900.0"},{"poster":"hilft","upvote_count":"2","timestamp":"1658622060.0","comment_id":"635814","content":"B. \nthe keyword here is IAM role and metadata. userdata is irrelevant"},{"content":"Answer is B","upvote_count":"1","comment_id":"443002","poster":"kashi1983","timestamp":"1635780120.0"},{"content":"B for sure","timestamp":"1632220980.0","upvote_count":"1","comment_id":"366139","poster":"01037"},{"timestamp":"1632121500.0","poster":"cldy","comment_id":"328655","upvote_count":"3","content":"B.\nIAM role for EC2 to access objects In the S3 bucket."}]},{"id":"sn5Ud33Vsu0Vo5dikasP","question_text":"A company is migrating an application to the AWS Cloud. The application runs in an on-premises data center and writes thousands of images into a mounted NFS file system each night. After the company migrates the application, the company will host the application on an Amazon EC2 instance with a mounted Amazon\nElastic File System (Amazon EFS) file system.\nThe company has established an AWS Direct Connect connection to AWS. Before the migration cutover, a solutions architect must build a process that will replicate the newly created on-premises images to the EFS file system.\nWhat is the MOST operationally efficient way to replicate the images?","url":"https://www.examtopics.com/discussions/amazon/view/74180-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["D (59%)","C (41%)"],"exam_id":32,"question_id":855,"isMC":true,"unix_timestamp":1650667080,"choices":{"A":"Configure a periodic process to run the aws s3 sync command from the on-premises file system to Amazon S3. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.","C":"Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an S3 bucket by using public VIF. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.","D":"Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every 24 hours.","B":"Deploy an AWS Storage Gateway file gateway with an NFS mount point. Mount the file gateway file system on the on-premises server. Configure a process to periodically copy the images to the mount point."},"discussion":[{"poster":"shailurtm2001","upvote_count":"15","content":"D https://aws.amazon.com/datasync/faqs/","comment_id":"590269","comments":[{"timestamp":"1653570720.0","content":"Search for \"VPC endpoints\" or \"AWS PrivateLink\"","poster":"bobsmith2000","upvote_count":"2","comment_id":"607668"},{"upvote_count":"1","poster":"vn_thanhtung","timestamp":"1693225140.0","comment_id":"992144","comments":[{"upvote_count":"1","timestamp":"1693225260.0","poster":"vn_thanhtung","comment_id":"992149","content":"https://docs.aws.amazon.com/datasync/latest/userguide/datasync-in-vpc.html"}],"content":"https://repost.aws/knowledge-center/datasync-cross-activate-agent\nAnswer is D"},{"upvote_count":"2","timestamp":"1667964660.0","comment_id":"714255","content":"https://docs.aws.amazon.com/efs/latest/ug/efs-vpc-endpoints.html\n\nVPC endpoints for EFS are only for the control plane APIs, not data transfer.","comments":[{"poster":"Byrney","content":"Q: How does AWS DataSync access my Amazon EFS file system?\n\nA: AWS DataSync accesses your Amazon EFS file system using the NFS protocol. The DataSync service mounts your file system from within your VPC from Elastic Network Interfaces (ENIs) managed by the DataSync service. DataSync fully manages the creation, use, and deletion of these ENIs on your behalf. You can choose to mount your EFS file system using a mount target or an EFS Access Point.","comment_id":"714256","upvote_count":"2","timestamp":"1667964720.0"}],"poster":"Byrney"},{"timestamp":"1667576460.0","comment_id":"711248","comments":[{"timestamp":"1673779680.0","content":"I agree, but I think it meant for that only, wording could have been better","upvote_count":"1","poster":"Arunava1","comment_id":"776456"}],"content":"D would have been the best solution if it were using VPC endpoint for \"DataSync\". But it is using VPC endpoint for EFS, which won't work. C is the answer.","poster":"kzqc","upvote_count":"1"}],"timestamp":"1650667080.0"},{"comment_id":"863579","timestamp":"1680851340.0","upvote_count":"1","poster":"velikivelicu","content":"Selected Answer: D\nA private VIF to directly sent data to EFS using DataSync is the most efficient solution, so D is the correct answer\nSending data through a public VIF to S3 and then copying it to EFS is inefficient, so C is incorrect"},{"content":"OPTION C:\n\nOne way to replicate the images from the on-premises NFS file system to the Amazon EFS file system after the migration cutover is to use an AWS Lambda function to process event notifications from Amazon S3. The Lambda function can be triggered when new images are added to an S3 bucket, and it can then copy the images from S3 to the EFS file system.\nThis will be an operationally efficient way to replicate the images as it is an event-driven, serverless architecture that can automatically scale to handle the number of images being written each night. And also S3 bucket can automatically trigger the lambda function when any new image is added to the bucket. This will be efficient and cost effective as well and will not require any manual intervention","poster":"Heer","comment_id":"791291","timestamp":"1674955380.0","upvote_count":"1"},{"content":"D : \"What is the MOST operationally efficient way to replicate the images?\" C works but we dont need to go roundabout way and involve S3 here. we are actually connecting datasync service through the privatelink not the EFS. the phrase \" vpc endpoint for EFS\" is just to create confusion, actaully it does mean we are connecting datasync service through it and then datasync service can run on EFS","timestamp":"1673779620.0","poster":"Arunava1","comment_id":"776454","upvote_count":"2"},{"timestamp":"1673207340.0","comment_id":"769775","upvote_count":"1","content":"Correct C.","poster":"ggrodskiy"},{"poster":"evargasbrz","comment_id":"764928","upvote_count":"1","timestamp":"1672770600.0","content":"Selected Answer: C\nI'll go with C"},{"upvote_count":"1","poster":"SureNot","comment_id":"715625","content":"Selected Answer: C\nC - event notifications is the most operationally efficient way","timestamp":"1668124800.0"},{"timestamp":"1668114000.0","content":"D for me","upvote_count":"1","comment_id":"715563","poster":"mrgreatness"},{"comment_id":"715561","content":"https://www.youtube.com/watch?v=_snUm9g5jG0","upvote_count":"1","timestamp":"1668113700.0","poster":"mrgreatness"},{"poster":"Byrney","comment_id":"714257","content":"Selected Answer: C\nAnswer is C. D won't work as VPC Endpoints for EFS are to the control/management APIs, not for data transfer","timestamp":"1667964840.0","upvote_count":"1"},{"poster":"kzqc","timestamp":"1667575920.0","upvote_count":"2","content":"Selected Answer: C\nD is WRONG! DataSync agent can ONLY use VPC endpoint for \"DataSync\", not VPC endpoint for \"EFS\".","comment_id":"711241"},{"content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/storage/transferring-files-from-on-premises-to-aws-and-back-without-leaving-your-vpc-using-aws-datasync/","comment_id":"692662","poster":"JohnPi","upvote_count":"1","timestamp":"1665551580.0"},{"poster":"joancarles","comment_id":"672307","timestamp":"1663501800.0","upvote_count":"2","content":"Selected Answer: D\nThe only option that comments the need of a task schedule it's the option D, also if you have a DX, you can use a vpc endpoint for the transmission, so it's correct:\n\nhttps://aws.amazon.com/datasync/faqs/?nc1=h_ls"},{"upvote_count":"1","timestamp":"1660592820.0","content":"Option is D. Using the AWS direct, you can only connect over private VIF. https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html\n\nOnce you are connected to the VPC, then accessing EFS is only possible by private LINK and EFS.","poster":"AWSProfessionalPractioner","comment_id":"647340"},{"content":"I agree - D. How do I use datasync to migrate data to AWS? Configure DataSync to make an initial copy of your entire dataset, and schedule subsequent incremental transfers of changing data until the final cut-over from on-premises to AWS","poster":"Ni_yot","upvote_count":"1","timestamp":"1659375300.0","comment_id":"640787"},{"comment_id":"637039","timestamp":"1658797860.0","content":"Selected Answer: D\nD.\nhttps://docs.aws.amazon.com/efs/latest/ug/gs-step-four-sync-files.html\nNow that you have created a functioning Amazon EFS file system, you can use AWS DataSync to transfer files from an existing file system to Amazon EFS. AWS DataSync is a data transfer service that simplifies, automates, and accelerates moving and replicating data between on-premises storage systems and AWS storage services over the internet or AWS Direct Connect. AWS DataSync can transfer your file data, and also file system metadata such as ownership, timestamps, and access permissions.\n\nCan option C work? \nYes, but that's a lot of unnecessary action. Remember, the question says \"What is the MOST OPTIMAL method for replicating the images?\"","upvote_count":"4","poster":"cen007"},{"upvote_count":"1","content":"vote for D","timestamp":"1657196640.0","comment_id":"628345","poster":"aandc"},{"poster":"kangtamo","timestamp":"1656333480.0","content":"Selected Answer: C\nAgree with C: S3.","upvote_count":"1","comment_id":"623270"},{"content":"Selected Answer: D\nYou can use VPC endpoints to ensure data transferred between your AWS DataSync agent, either deployed on-premises or in-cloud, doesn't traverse the public internet or need public IP addresses. Using VPC endpoints increases the security of your data by keeping network traffic within your Amazon Virtual Private Cloud (Amazon VPC). VPC endpoints for DataSync are powered by AWS PrivateLink, a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services.","comment_id":"616171","upvote_count":"2","timestamp":"1655205240.0","poster":"Chuky64"},{"poster":"Anhdd","upvote_count":"1","comment_id":"613659","content":"Selected Answer: C\nI do support C because of best practice of S3 if store images which are static files","timestamp":"1654756800.0"},{"poster":"James_Zheng","upvote_count":"1","timestamp":"1652236860.0","content":"The best practice transferring organizations' internal data is to go through PrivateLink for S3; Refer to: https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html\nThe correct answer should be D...","comment_id":"599845"},{"upvote_count":"1","comment_id":"597721","content":"Selected Answer: C\nIt's C","timestamp":"1651838940.0","poster":"Ryannn"},{"poster":"Bigbearcn","timestamp":"1650804300.0","comments":[{"timestamp":"1650853200.0","content":"change to D.","comment_id":"591342","upvote_count":"2","poster":"Bigbearcn"}],"comment_id":"591053","content":"Selected Answer: C\ndatasync to EFS directly. It's C.","upvote_count":"1"}],"answer_ET":"D","answer_images":[],"answer":"D","answer_description":"","topic":"1","question_images":[],"timestamp":"2022-04-23 00:38:00"}],"exam":{"provider":"Amazon","isImplemented":true,"lastUpdated":"11 Apr 2025","id":32,"isMCOnly":false,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Professional","isBeta":false},"currentPage":171},"__N_SSP":true}