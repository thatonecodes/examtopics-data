{"pageProps":{"questions":[{"id":"zIn1t982FzDPKnbFAnUm","answers_community":["B (95%)","5%"],"topic":"1","question_text":"A company runs a web application that extends across multiple Availability Zones. The company uses an Application Load Balancer (ALB) for routing, AWS Fargate for the application, and Amazon Aurora for the application data. The company uses AWS CloudFormation templates to deploy the application. The company stores all Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository in the same AWS account and AWS Region.\n\nA DevOps engineer needs to establish a disaster recovery (DR) process in another Region. The solution must meet an RPO of 8 hours and an RTO of 2 hours. The company sometimes needs more than 2 hours to build the Docker images from the Dockerfile.\n\nWhich solution will meet the RTO and RPO requirements MOST cost-effectively?","question_images":[],"answer_images":[],"answer":"B","discussion":[{"upvote_count":"6","poster":"thanhnv142","comment_id":"1148257","timestamp":"1707750960.0","content":"Selected Answer: B\nB is correct:\nA: < build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region>: This process might take longer than 2 hours, which does not meet the RTO \nB: correct\nC: <AWS Lambda function to take an hourly snapshot of the Aurora database>: lambda has a maximum running time of 15 minutes. Cannot do an hourly task\nD: No mention about the docker image for the ECS"},{"comment_id":"1412479","content":"Selected Answer: B\nThe best option in this case is Option B because it:\n\nEnsures both Aurora database and Docker images are replicated to the DR Region, meeting the RPO requirement of 8 hours.\nEliminates the need to rebuild Docker images in the DR Region, thus ensuring the RTO of 2 hours is met, which is crucial during the disaster recovery process.\nProvides a straightforward, cost-effective solution with minimal operational overhead by leveraging AWS native features like cross-Region replication for both Aurora and ECR.","poster":"Srikantha","upvote_count":"1","timestamp":"1743339000.0"},{"comment_id":"1325129","timestamp":"1733938800.0","upvote_count":"2","poster":"spring21","content":"Selected Answer: B\nYes, Amazon Aurora supports cross-Region automated backups:"},{"poster":"teo2157","upvote_count":"1","content":"Selected Answer: C\nB is not correct as Amazon Aurora doesn´t support cross-Region automated backups: https://repost.aws/questions/QUPm9L8amQTTKkz1RXU7EqWA/unable-to-enable-cross-region-automated-backups-in-rds\nHave anyone that has responded B tried to enable cross-Region automated backups in an Aurora RDS?","comment_id":"1324541","timestamp":"1733835900.0"},{"comment_id":"1194591","content":"Selected Answer: B\nanswer is B","timestamp":"1712969100.0","upvote_count":"3","poster":"dkp"},{"upvote_count":"4","content":"Selected Answer: B\nB: Least operational overhead, lower cost and meets RPO and RTO","timestamp":"1710581460.0","comment_id":"1174880","poster":"DanShone"},{"poster":"fdoxxx","content":"Selected Answer: B\nB provides a cost-effective solution that meets the RTO and RPO","upvote_count":"4","timestamp":"1709238180.0","comment_id":"1162985"},{"upvote_count":"2","comment_id":"1145648","timestamp":"1707497460.0","content":"I also go with B","poster":"LeoSantos121212121212121"},{"upvote_count":"2","poster":"Chelseajcole","content":"B. Most cost effective","timestamp":"1707329700.0","comment_id":"1143639"}],"url":"https://www.examtopics.com/discussions/amazon/view/133300-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"C":"Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Use Amazon EventBridge to schedule an AWS Lambda function to take an hourly snapshot of the Aurora database and of the most recent Docker image in the ECR repository. Copy the snapshot and the Docker image to the DR Region. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region.","B":"Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Configure Aurora automated backup Cross-Region Replication. Configure ECR Cross-Region Replication. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.","D":"Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Deploy a second application CloudFormation stack in the DR Region. Reconfigure Aurora to be a global database. Update both CloudFormation stacks when a new application release in the current Region is needed. In case of DR, update the application DNS records to point to the new ALB.","A":"Copy the CloudFormation templates and the Dockerfile to an Amazon S3 bucket in the DR Region. Use AWS Backup to configure automated Aurora cross-Region hourly snapshots. In case of DR, build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region. Use the CloudFormation template that has the most recent Aurora snapshot and the Docker image from the ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB."},"unix_timestamp":1707329700,"timestamp":"2024-02-07 19:15:00","answer_description":"","question_id":116,"exam_id":23,"isMC":true,"answer_ET":"B"},{"id":"uaHpNGKBtVMJZ5avYcKt","question_text":"A company's application runs on Amazon EC2 instances. The application writes to a log file that records the username, date, time, and source IP address of the login. The log is published to a log group in Amazon CloudWatch Logs.\n\nThe company is performing a root cause analysis for an event that occurred on the previous day. The company needs to know the number of logins for a specific user from the past 7 days.\n\nWhich solution will provide this information?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/133301-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_ET":"C","choices":{"D":"Create a CloudWatch dashboard. Add a number widget that has a filter pattern that counts the number of logins for the username over the past 7 days directly from the log group.","A":"Create a CloudWatch Logs metric filter on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.","C":"Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.","B":"Create a CloudWatch Logs subscription on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days."},"answer_description":"","answer_images":[],"timestamp":"2024-02-07 19:25:00","answer":"C","isMC":true,"discussion":[{"comment_id":"1149430","poster":"Ramdi1","content":"Selected Answer: C\nC - CloudWatch Logs Insights: This service allows you to run ad-hoc queries against your log data without creating additional infrastructure like metrics or subscriptions.\nAggregation function: Functions like count() can specifically calculate the number of occurrences based on specific criteria.\nFiltering by username and timeframe: The query can be tailored to include the specific username and filter for entries within the past 7 days.","timestamp":"1707846600.0","comments":[{"content":"A. Metric filter: It can count occurrences, but requires additional metric creation and subscription, introducing complexity.\nB. Subscription: Similar to metric filter, requires creating an additional subscription and pushing data elsewhere.\nD. Dashboard widget: Limited in its capabilities, might not allow complex filtering and aggregation needed for this specific analysis.\n \nTherefore, CloudWatch Logs Insights offers the most direct and flexible solution for analyzing the desired login data within the given timeframe and user criteria.","timestamp":"1707846600.0","upvote_count":"1","comment_id":"1149431","poster":"Ramdi1"}],"upvote_count":"5"},{"upvote_count":"5","timestamp":"1712970240.0","poster":"dkp","content":"Selected Answer: C\nanswer C","comment_id":"1194597"},{"poster":"heff_bezos","content":"Selected Answer: C\nThere is no cloudwatch metric that logs user logins","timestamp":"1727325960.0","comment_id":"1289285","upvote_count":"2"},{"upvote_count":"1","content":"D\nFor A you would have to setup the cloudwatch metric filters beforehand as you won't be able to analyze past logs without the setup","timestamp":"1723298280.0","poster":"itzrahulyadav","comment_id":"1263533"},{"timestamp":"1712473260.0","upvote_count":"4","content":"Selected Answer: C\nThe solution that will provide the number of logins for a specific user from the past 7 days is Option C: Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.","comment_id":"1190816","poster":"mumumu"},{"timestamp":"1710320040.0","poster":"Shasha1","content":"C \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_AnalyzeLogData_AggregationQuery.html","comment_id":"1172390","upvote_count":"2"},{"content":"Selected Answer: C\nC is the most suitable solution for obtaining the required information","comment_id":"1162995","poster":"fdoxxx","upvote_count":"5","timestamp":"1709238960.0"},{"content":"Selected Answer: C\n, C. Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group. is the best solution. This method directly addresses the need to analyze log data for a specific pattern (user logins) and aggregate the counts over a specified period, which is exactly what's needed for a root cause analysis of the event.","timestamp":"1708189440.0","poster":"kyuhuck","comment_id":"1152711","upvote_count":"5"},{"comment_id":"1148261","timestamp":"1707751320.0","poster":"thanhnv142","content":"Selected Answer: A\nA is correct: CloudWatch Logs metric filter can filter out relevant logs and count\nB: irrelevant, this is for sharing logs to other sources\nC: This can accomplish the task. However, loudWatch Logs metric filter offers us the same function with less cost. Using CloudWatch Logs Insights query incur more costs. This feature is primarily used for data analysis\nD: irrelevant","upvote_count":"1"},{"poster":"Chelseajcole","upvote_count":"1","content":"C. It must be CloudWatch Logs Insights query","comment_id":"1143650","timestamp":"1707330300.0"}],"exam_id":23,"unix_timestamp":1707330300,"answers_community":["C (96%)","4%"],"topic":"1","question_id":117},{"id":"GrvCknGaNnUOAQm3N5sC","choices":{"D":"Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Department=Marketing tag.","B":"Change the current single tag group to include the Department=Marketing, Environment=production, and Name=ApplicationA tags.","A":"Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Name=ApplicationA tag.","C":"Add another single tag group that includes only the Department=Marketing tag. Keep the Environment=Production and Name=ApplicationA tags with the current single tag group."},"topic":"1","question_id":118,"timestamp":"2024-02-07 19:28:00","exam_id":23,"answer_images":[],"discussion":[{"poster":"thanhnv142","content":"Selected Answer: A\nA is correct: All tags in one tag group are OR operator. Tags are in multiple tag groups are AND operator","upvote_count":"9","timestamp":"1707751920.0","comment_id":"1148268"},{"timestamp":"1720466640.0","comment_id":"1244532","comments":[{"upvote_count":"2","poster":"Gomer","content":"A:(YES) Block ApplicationA deployments on EC2B because it would now have to have both Production AND ApplicationA tags (AND Operator between tag groups)\nB: (NO) Allow ApplicationA deployments on EC2B because it has the Production tag (OR Operator within tag group)\nC: (NO) Allow ApplicationA deployments on EC2B because it has the Production and Marketing tags (OR Operator within tag group, AND Operator between tag groups)\nD: (NO) Allow ApplicationA deployments on EC2B because it has the Production and Marketing tags (OR Operator within tag group, AND Operator between tag groups)","timestamp":"1720466640.0","comment_id":"1244533"}],"upvote_count":"6","content":"Selected Answer: A\nI Found this question largely incomprehensible untill I understood the following concepts and mapped out everything\n- Single tag group: Any instance identified by at least one tag in the group is included in the deployment group. (OR Operator within individual tag groups)\n- Multiple tag groups: Instances that are identified by at least one tag in each of the tag groups are included. (AND Operator between multiple tag groups)\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html","poster":"Gomer"},{"content":"Selected Answer: B\nOption B is the most effective solution, as it specifies that the deployment group should only target instances that have all three tags (Department=Marketing, Environment=Production, and Name=ApplicationA). This ensures that only instances specifically meant for ApplicationA are selected for the deployment, preventing unwanted instances (like those with Name=ApplicationB) from receiving the application.","poster":"Srikantha","comment_id":"1412480","upvote_count":"1","timestamp":"1743339300.0"},{"comment_id":"1331169","timestamp":"1735054920.0","content":"Selected Answer: A\nThis is a very tricky question, even ChatGPT can't get it right, a good one!\n\nMake sure to go through this AWS doc first and fully understand it.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html\n\nFirst thing first, why ApplicationA was installed on the new EC2 instance?\nReason is <single tag group, multiple tags> was used. The <Environment> and <Name> tags are both defined within this tag group. So as long as the EC2 instance has one tag matched, ApplicationA would be deployed. In this case, the new EC2 instance has <Environment=Production> tag, so ApplicationA got deployed.","upvote_count":"2","comments":[{"poster":"youonebe","comment_id":"1331170","timestamp":"1735054920.0","content":"Now we look at the choices one by one:\nA. [Correct]This updates the current tag group, add a new tag group. Which means BOTH tags need to match before ApplicationA gets deployed. For the new EC2 instance, because Name=ApplicationB, hence preventing the mistake from happening. \n\nB. [Wrong] This will be very similar to the original configuration, as long as one of the 3 tags matches, ApplicationA will be deployed. This does not solve the problem.\n\nC. [Wrong] The company still needs to install ApplicationA to existing EC2 instances, adding\"Department=Marketing will break the current workflow.\n\nD. [Wrong] The company still needs to install ApplicationA to existing EC2 instances, adding Department=Marketing will break the current workflow.","upvote_count":"1"}],"poster":"youonebe"},{"timestamp":"1710581160.0","comment_id":"1174879","poster":"DanShone","content":"Selected Answer: A\nA: A single tag group can only contain 1 tag, so multiple Single tag groups will be needed.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html","upvote_count":"5"},{"poster":"fdoxxx","comment_id":"1162989","upvote_count":"1","timestamp":"1709238540.0","content":"Selected Answer: B\nOption B is the most appropriate solution for meeting the requirements:\n\nChanging the current single tag group to include the specific tags (Department=Marketing, Environment=Production, and Name=ApplicationA) ensures that only instances with these specific tags are identified for the deployment of ApplicationA.\nThe other options are not suitable for achieving the desired outcome"},{"upvote_count":"5","comments":[{"content":"Reasons why other options won't work:\n \nB: Including Department=Marketing in the current tag group wouldn't change anything - all instances in Production with ApplicationA tag would be targeted.\nC: Adding a Department=Marketing tag group alone still leaves the original tag group targeting the Marketing instance due to the Name=ApplicationA tag.\nD: Removing Name=ApplicationA from the initial tag group removes a necessary criterion, potentially deploying ApplicationA to all Production instances regardless of their Name tag.\n \nTherefore, solution A provides the most precise and effective way to exclude the Marketing instance from ApplicationA deployment while maintaining the desired targeting for other instances.","comment_id":"1149439","poster":"Ramdi1","upvote_count":"1","timestamp":"1707846960.0"}],"content":"Selected Answer: A\nA - Original configuration: The single tag group with Environment=Production and Name=ApplicationA tags targets any instance with both tags, resulting in ApplicationA being deployed on the new Marketing instance despite its different Name tag.\nSolution A:\nChanging the current tag group to \"Environment=Production\" ensures only instances in the Production environment are considered.\nAdding a separate tag group with \"Name=ApplicationA\" specifically targets instances meant for that application.\nThe Marketing instance with Department=Marketing tag doesn't match the new criteria and avoids unintended ApplicationA installation.","timestamp":"1707846960.0","poster":"Ramdi1","comment_id":"1149438"},{"poster":"Chelseajcole","timestamp":"1707330480.0","comment_id":"1143653","upvote_count":"1","content":"A. need to applicationA tag"}],"question_text":"A company has an AWS CodeDeploy application. The application has a deployment group that uses a single tag group to identify instances for the deployment of Application. The single tag group configuration identifies instances that have Environment=Production and Name=ApplicationA tags for the deployment of ApplicationA.\n\nThe company launches an additional Amazon EC2 instance with Department=Marketing, Environment=Production, and Name=ApplicationB tags. On the next CodeDeploy deployment of Application, the additional instance has ApplicationA installed on it. A DevOps engineer needs to configure the existing deployment group to prevent ApplicationA from being installed on the additional instance.\n\nWhich solution will meet these requirements?","answer_ET":"A","answer_description":"","isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/133302-exam-aws-certified-devops-engineer-professional-dop-c02/","answers_community":["A (93%)","7%"],"unix_timestamp":1707330480,"answer":"A"},{"id":"IeC8n59IQUy6i37jHBpI","question_text":"A company is launching an application that stores raw data in an Amazon S3 bucket. Three applications need to access the data to generate reports. The data must be redacted differently for each application before the applications can access the data.\n\nWhich solution will meet these requirements?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/133303-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_ET":"D","choices":{"C":"For each application, create an S3 access point that uses the raw data's S3 bucket as the destination. Create an AWS Lambda function that is invoked by object creation events in the raw data's S3 bucket. Program the Lambda function to redact data for each application. Store the data in each application's S3 access point. Configure each application to consume data from its own S3 access point.","B":"Create an Amazon Kinesis data stream. Create an AWS Lambda function that is invoked by object creation events in the raw data’s S3 bucket. Program the Lambda function to redact data for each application. Publish the data on the Kinesis data stream. Configure each application to consume data from the Kinesis data stream.","A":"Create an S3 bucket for each application. Configure S3 Same-Region Replication (SRR) from the raw data's S3 bucket to each application's S3 bucket. Configure each application to consume data from its own S3 bucket.","D":"Create an S3 access point that uses the raw data’s S3 bucket as the destination. For each application, create an S3 Object Lambda access point that uses the S3 access point. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved. Configure each application to consume data from its own S3 Object Lambda access point"},"answer_description":"","answer_images":[],"timestamp":"2024-02-07 19:31:00","answer":"D","isMC":true,"discussion":[{"upvote_count":"3","content":"Selected Answer: D\nkeywords: S3 Object Lambda access point to redact data","timestamp":"1722774000.0","poster":"jamesf","comment_id":"1260657"},{"content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html","upvote_count":"3","comment_id":"1202688","poster":"c3518fc","timestamp":"1714147920.0"},{"comment_id":"1194604","content":"Selected Answer: D\nanswer D","timestamp":"1712972820.0","poster":"dkp","upvote_count":"3"},{"poster":"anj_k","comment_id":"1175591","timestamp":"1710651840.0","upvote_count":"4","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html"},{"content":"Selected Answer: C\nS3 Object Lambda access point is not suitable for generating reports. Generally, creating a report requires an aggregate process, which is expensive. Since reports are expected to be viewed multiple times, it is inefficient to pay for Lambda processing time and CPU costs each time they are viewed. \nTo adopt D, CloudFront should be added to the front.\nhttps://aws.amazon.com/jp/blogs/aws/new-use-amazon-s3-object-lambda-with-amazon-cloudfront-to-tailor-content-for-end-users/","poster":"dzn","comment_id":"1167976","timestamp":"1709814600.0","upvote_count":"2"},{"comment_id":"1163001","content":"Selected Answer: D\nD, using S3 Object Lambda access points, is the most appropriate solution for the requirements:\nS3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data at the time of retrieval. In this scenario, you can create an S3 access point that uses the raw data's S3 bucket as the destination. For each application, create a separate S3 Object Lambda access point that uses the S3 access point as the source. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved.\nThis solution ensures that each application can access the data with its own redaction rules, and the redaction is applied dynamically at the time of retrieval.","poster":"fdoxxx","timestamp":"1709239200.0","upvote_count":"4"},{"upvote_count":"3","content":"Selected Answer: D\nSingle source of truth: This solution maintains a single copy of the raw data in the original S3 bucket, avoiding data duplication and associated costs.\nFine-grained redaction: Each application has its own S3 Object Lambda access point, allowing independent Lambda functions to redact data according to specific needs. This ensures targeted redaction without creating multiple S3 buckets with potentially inefficient data copies.\nEfficient access: Applications access the data through their respective S3 Object Lambda access points, incurring the redaction processing only when data is retrieved, improving cost-effectiveness compared to upfront redaction approaches.","timestamp":"1707847380.0","poster":"Ramdi1","comments":[{"timestamp":"1707847380.0","comment_id":"1149447","poster":"Ramdi1","content":"A: Duplicates data for each application, increasing storage costs and maintenance overhead.\nB: While Kinesis Data Streams can handle large data volumes, it adds an extra layer of complexity and latency compared to direct S3 access with redaction.\nC: Still requires upfront redaction for each application's specific needs, potentially duplicating redacted data across S3 access points.","upvote_count":"1"}],"comment_id":"1149446"},{"upvote_count":"3","timestamp":"1707752160.0","comment_id":"1148273","poster":"thanhnv142","content":"Selected Answer: D\nD is correct: S3 access point is actually S3 lambda access point, which is option D\nA and B: too expensive\nC: is not correct"},{"upvote_count":"2","timestamp":"1707330660.0","comment_id":"1143656","content":"D. S3 Bucker endpoint plus Lambda","poster":"Chelseajcole"}],"exam_id":23,"unix_timestamp":1707330660,"answers_community":["D (92%)","8%"],"topic":"1","question_id":119},{"id":"Z1XARehjkETHvA1gLde6","choices":{"B":"Use AWS Control Tower with a multi-account environment. Configure and enable proactive AWS Control Tower controls on all OUs with CloudFormation hooks.","A":"Use AWS Organizations. Attach an SCP that denies the s3:PutObject permission if the request does not include an x-amz-server-side-encryption header that requests server-side encryption with AWS KMS keys (SSE-KMS).","C":"Use AWS Control Tower with a multi-account environment. Configure and enable detective AWS Control Tower controls on all OUs with CloudFormation hooks.","D":"Use AWS Organizations. Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. Deploy the rule. Create and apply an SCP to prevent users from stopping and deleting AWS Config across all AWS accounts,"},"isMC":true,"question_id":120,"topic":"1","answer":"B","answer_images":[],"exam_id":23,"timestamp":"2024-02-07 19:32:00","question_text":"A company uses AWS Control Tower and AWS CloudFormation to manage its AWS accounts and to create AWS resources. The company requires all Amazon S3 buckets to be encrypted with AWS Key Management Service (AWS KMS) when the S3 buckets are created in a CloudFormation stack.\n\nWhich solution will meet this requirement?","url":"https://www.examtopics.com/discussions/amazon/view/133304-exam-aws-certified-devops-engineer-professional-dop-c02/","unix_timestamp":1707330720,"answers_community":["B (97%)","3%"],"answer_ET":"B","answer_description":"","discussion":[{"upvote_count":"5","timestamp":"1707847860.0","poster":"Ramdi1","comments":[{"comment_id":"1149453","upvote_count":"1","content":"The other options have limitations:\n \n \nA: While SCPs enforce policies, they react to actions instead of proactively preventing them. Additionally, denying s3:PutObject might be too restrictive as it can impact other legitimate operations.\nC: Detective controls monitor and report on existing resources, not preventing non-compliant creations.\nD: Config and SCPs combined address encryption checks and user limitations, but lack the direct integration with CloudFormation stacks crucial for enforcing during creation.","poster":"Ramdi1","timestamp":"1707847920.0"}],"comment_id":"1149452","content":"Selected Answer: B\nProactive controls: Proactive controls are preventative measures that block actions violating defined policies before they occur. This ensures encryption gets applied automatically during S3 bucket creation within CloudFormation stacks.\nCloudFormation hooks: Hooks enable Control Tower to intercept and enforce policies on CloudFormation stack operations, making it ideal for enforcing encryption during resource creation.\nMulti-account environment: Since the requirement applies across all accounts, Control Tower's multi-account capabilities ensure consistent policy enforcement throughout the organization."},{"content":"Selected Answer: B\nB is correct: <AWS Control Tower> means we need to use the proactive control \nA: SCP s3:PutObject permission only deny action related to put object to S3, not when creating it\nB: Detective controls used only for monitoring\nC: correct\nD: This option can achive the goal of the question. However, it is way more complicated than B.","timestamp":"1707752460.0","poster":"thanhnv142","comment_id":"1148279","upvote_count":"5"},{"comment_id":"1260661","poster":"jamesf","upvote_count":"3","content":"Selected Answer: B\nkeywords: proactive","timestamp":"1722829680.0"},{"comment_id":"1244572","timestamp":"1720475040.0","content":"Selected Answer: B\nHere's the Control Tower proactive control:\n\"[CT.S3.PR.10] Require an Amazon S3 bucket to have server-side encryption configured using an AWS KMS key\"\nhttps://docs.aws.amazon.com/controltower/latest/controlreference/s3-rules.html#ct-s3-pr-10-description","upvote_count":"4","poster":"Gomer"},{"content":"Selected Answer: B\nClearly answer is B , here is article that explains the same.\nhttps://aws.amazon.com/blogs/mt/how-aws-control-tower-users-can-proactively-verify-compliance-in-aws-cloudformation-stacks/\n\nAnswer D with config rule also fits the bill (if no control tower), but since we have Control tower managing the accounts already its better to make use of the features that Control tower leverages","timestamp":"1718427120.0","upvote_count":"2","poster":"Venki_dev","comment_id":"1230771"},{"poster":"dkp","timestamp":"1712973600.0","upvote_count":"3","content":"Selected Answer: B\nAnswer B","comment_id":"1194609"},{"timestamp":"1712480700.0","poster":"fdoxxx","upvote_count":"3","comment_id":"1190869","content":"Selected Answer: B\nB is better than D..."},{"upvote_count":"3","comment_id":"1177591","content":"Selected Answer: B\nB, 100%","poster":"ogerber","timestamp":"1710871380.0"},{"upvote_count":"1","comment_id":"1164636","comments":[],"content":"Selected Answer: D\nD provides a solution that leverages AWS Organizations and AWS Config to enforce the requirement for AWS KMS encryption on all S3 buckets created through CloudFormation:\nAWS Config Organizational Rule: Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. This rule helps ensure that the encryption requirement is enforced.\nOptions A, B, and C do not directly address the requirement for AWS KMS encryption on S3 buckets created through CloudFormation:\nOption A mentions using an SCP but focuses on denying s3:PutObject without the required encryption header. However, this approach doesn't ensure that the encryption is enforced through AWS KMS.\nOptions B and C mention using AWS Control Tower with proactive or detective controls, but they don't specifically address the encryption requirement for S3 buckets.","poster":"fdoxxx","timestamp":"1709457540.0"},{"upvote_count":"1","timestamp":"1707330720.0","poster":"Chelseajcole","content":"Maybe D","comments":[{"poster":"Chelseajcole","upvote_count":"1","comment_id":"1143659","content":"Because of AWS Config","timestamp":"1707330840.0"}],"comment_id":"1143657"}],"question_images":[]}],"exam":{"provider":"Amazon","isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":23,"isImplemented":true,"numberOfQuestions":355,"name":"AWS Certified DevOps Engineer - Professional DOP-C02"},"currentPage":24},"__N_SSP":true}