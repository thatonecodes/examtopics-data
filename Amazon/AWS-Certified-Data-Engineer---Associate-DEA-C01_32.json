{"pageProps":{"questions":[{"id":"BjhseyipXH7EkMgyx54t","url":"https://www.examtopics.com/discussions/amazon/view/132677-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","question_images":[],"answers_community":["B (94%)","6%"],"timestamp":"2024-02-02 10:51:00","unix_timestamp":1706867460,"choices":{"A":"Use Amazon Aurora as the data catalog. Create AWS Lambda functions that will connect to the data catalog. Configure the Lambda functions to gather the metadata information from multiple sources and to update the Aurora data catalog. Schedule the Lambda functions to run periodically.","C":"Use Amazon DynamoDB as the data catalog. Create AWS Lambda functions that will connect to the data catalog. Configure the Lambda functions to gather the metadata information from multiple sources and to update the DynamoDB data catalog. Schedule the Lambda functions to run periodically.","B":"Use the AWS Glue Data Catalog as the central metadata repository. Use AWS Glue crawlers to connect to multiple data stores and to update the Data Catalog with metadata changes. Schedule the crawlers to run periodically to update the metadata catalog.","D":"Use the AWS Glue Data Catalog as the central metadata repository. Extract the schema for Amazon RDS and Amazon Redshift sources, and build the Data Catalog. Use AWS Glue crawlers for data that is in Amazon S3 to infer the schema and to automatically update the Data Catalog."},"discussion":[{"poster":"pypelyncar","upvote_count":"7","comment_id":"1227602","timestamp":"1733801100.0","content":"Selected Answer: B\nThe AWS Glue Data Catalog is a purpose-built, fully managed service designed to serve as a central metadata repository for your data sources. It provides a unified view of your data across various sources, including structured databases (like Amazon RDS and Amazon Redshift) and semi-structured data formats (like JSON and XML files in Amazon S3)."},{"upvote_count":"3","content":"Selected Answer: B\nglue data catalog with crawlers","poster":"valuedate","comment_id":"1217547","timestamp":"1732464060.0"},{"upvote_count":"1","comments":[{"content":"Sorry there is no Aurora Data Catalog :)","poster":"Just_Ninja","comment_id":"1212499","upvote_count":"1","timestamp":"1731781380.0"},{"timestamp":"1733035800.0","poster":"tgv","comment_id":"1222508","upvote_count":"3","content":"Even though you picked A."}],"content":"Selected Answer: A\nB is the obvious answer","poster":"hnk","comment_id":"1210757","timestamp":"1731490800.0"},{"content":"Selected Answer: B\nA,C out for obvious reason \nD out because it involves manual schema extract","comment_id":"1177162","timestamp":"1726730580.0","poster":"GiorgioGss","upvote_count":"4"},{"timestamp":"1722585060.0","content":"Selected Answer: B\nOption B, using the AWS Glue Data Catalog with AWS Glue Crawlers, is the best solution to meet the requirements with the least operational overhead. It provides a fully managed, integrated solution for cataloging both structured and semistructured data across various AWS data stores without the need for extensive manual configuration or custom coding.","poster":"rralucard_","upvote_count":"3","comment_id":"1138398"}],"isMC":true,"exam_id":21,"answer_images":[],"answer":"B","answer_description":"","question_id":156,"answer_ET":"B","question_text":"A company needs to set up a data catalog and metadata management for data sources that run in the AWS Cloud. The company will use the data catalog to maintain the metadata of all the objects that are in a set of data stores. The data stores include structured sources such as Amazon RDS and Amazon Redshift. The data stores also include semistructured sources such as JSON files and .xml files that are stored in Amazon S3.\nThe company needs a solution that will update the data catalog on a regular basis. The solution also must detect changes to the source metadata.\nWhich solution will meet these requirements with the LEAST operational overhead?"},{"id":"zGwxDru3gJkGE71xfScM","answers_community":["C (89%)","6%"],"timestamp":"2024-02-02 10:54:00","question_images":[],"topic":"1","question_id":157,"choices":{"B":"Divide the table into two tables. Provision each table with half of the provisioned capacity of the original table. Spread queries evenly across both tables.","A":"Increase the provisioned capacity to the maximum capacity that is currently present during peak load times.","C":"Use AWS Application Auto Scaling to schedule higher provisioned capacity for peak usage times. Schedule lower capacity during off-peak times.","D":"Change the capacity mode from provisioned to on-demand. Configure the table to scale up and scale down based on the load on the table."},"unix_timestamp":1706867640,"discussion":[{"timestamp":"1706867640.0","content":"Selected Answer: C\nOption C, using AWS Application Auto Scaling to schedule higher provisioned capacity for peak usage times and lower capacity during off-peak times, is the most cost-effective solution for the described scenario. It allows the company to align their DynamoDB capacity costs with actual usage patterns, scaling up only when needed and scaling down during low-usage periods.","poster":"rralucard_","comment_id":"1138401","upvote_count":"5"},{"content":"Selected Answer: C\nMy guess is C as it stands for Cat","upvote_count":"1","timestamp":"1741079940.0","poster":"Rakiko","comment_id":"1364827"},{"content":"C\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html\n\nDynamoDB auto scaling modifies provisioned throughput settings only when the actual workload stays elevated or depressed for a sustained period of several minutes. This means that provisioned capacity is probably best for you if you have relatively predictable application traffic, run applications whose traffic is consistent, and ramps up or down gradually.","upvote_count":"1","timestamp":"1720946220.0","poster":"sdas1","comment_id":"1247697"},{"timestamp":"1717982940.0","upvote_count":"4","poster":"pypelyncar","content":"Selected Answer: C\napp autoscalling allows you to dynamically adjust provisioned capacity based on usage patterns. You only pay for the capacity you utilize, reducing costs compared to keeping a high, fixed capacity throughout the week","comment_id":"1227605"},{"timestamp":"1712982180.0","comment_id":"1194643","upvote_count":"3","poster":"Christina666","content":"Selected Answer: C\nD. Change the capacity mode from provisioned to on-demand... On-demand mode is great for unpredictable workloads. In your case, with predictable patterns, you'd likely pay more with on-demand than with a well-managed, scheduled, provisioned mode.","comments":[{"timestamp":"1716881340.0","content":"I agree with you, on-demand tends to be picked when you don't know the workload. While in this scenario they know, so technically the Auto Scaling solution would be much cheaper here.","poster":"tgv","comment_id":"1220058","upvote_count":"1"}]},{"poster":"lucas_rfsb","comment_id":"1187753","timestamp":"1712017560.0","content":"Selected Answer: D\nAs I understand, should be D","comments":[{"comment_id":"1187756","upvote_count":"3","poster":"lucas_rfsb","timestamp":"1712017860.0","content":"But C is also a good choice. Maybe because it is predictable, I'm now intending to choose C"}],"upvote_count":"1"},{"timestamp":"1711270020.0","content":"Selected Answer: C\nObviously better than B because of peak scaling","comment_id":"1181393","upvote_count":"3","poster":"FuriouZ"},{"timestamp":"1711093380.0","comments":[{"comments":[{"content":"Well, as your comment says:\n\nD - on-demand capacity mode is probably best when you have new tables with unknown workloads, unpredictable application traffic and also if you only want to pay exactly for what you use.\n\nThat's not the case, they know exactly when they are expecting an increasing. So the most cost-effective solution is C - Auto Scaling.","comment_id":"1220059","timestamp":"1716881460.0","poster":"tgv","upvote_count":"1"}],"upvote_count":"1","content":"selected answer should be D","comment_id":"1179932","timestamp":"1711093380.0","poster":"jpmadan"}],"poster":"jpmadan","content":"Selected Answer: B\nD\nExcerpts from documentation: \nThis means that provisioned capacity is probably best for you if you have relatively predictable application traffic, run applications whose traffic is consistent, and ramps up or down gradually.\nWhereas on-demand capacity mode is probably best when you have new tables with unknown workloads, unpredictable application traffic and also if you only want to pay exactly for what you use. The on-demand pricing model is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when under-provisioned capacity would impact the user experience.\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html","comment_id":"1179931","upvote_count":"1"},{"timestamp":"1709753340.0","comment_id":"1167439","poster":"damaldon","upvote_count":"2","content":"C.\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/services-that-can-integrate-dynamodb.html"}],"answer_images":[],"question_text":"A company stores data from an application in an Amazon DynamoDB table that operates in provisioned capacity mode. The workloads of the application have predictable throughput load on a regular schedule. Every Monday, there is an immediate increase in activity early in the morning. The application has very low usage during weekends.\nThe company must ensure that the application performs consistently during peak usage times.\nWhich solution will meet these requirements in the MOST cost-effective way?","exam_id":21,"answer":"C","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/132678-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"C","isMC":true},{"id":"d9ebTm9Cl0ZMgEngvQ3O","question_images":[],"topic":"1","answers_community":["B (82%)","A (18%)"],"answer_ET":"B","choices":{"A":"Use AWS Database Migration Service (AWS DMS) to migrate the Hive metastore into Amazon S3. Configure AWS Glue Data Catalog to scan Amazon S3 to produce the data catalog.","C":"Configure an external Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use Amazon Aurora MySQL to store the company's data catalog.","D":"Configure a new Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use the new metastore as the company's data catalog.","B":"Configure a Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use AWS Glue Data Catalog to store the company's data catalog as an external data catalog."},"question_text":"A company is planning to migrate on-premises Apache Hadoop clusters to Amazon EMR. The company also needs to migrate a data catalog into a persistent storage solution.\nThe company currently stores the data catalog in an on-premises Apache Hive metastore on the Hadoop clusters. The company requires a serverless solution to migrate the data catalog.\nWhich solution will meet these requirements MOST cost-effectively?","timestamp":"2024-02-02 11:00:00","exam_id":21,"answer_images":[],"unix_timestamp":1706868000,"answer_description":"","answer":"B","discussion":[{"upvote_count":"1","content":"Selected Answer: B\nA and D can be discarded because of added steps. This link provides documentation for this exact use case : https://aws.amazon.com/blogs/big-data/migrate-and-deploy-your-apache-hive-metastore-on-amazon-emr/\nC is also discarded because of the serverless key word, although Aurora can be serverless it is not specified in the choice.","comment_id":"1309418","poster":"Asmunk","timestamp":"1731238740.0"},{"poster":"Christina666","content":"Selected Answer: B\nServerless and Cost-Efficient: AWS Glue Data Catalog offers a serverless metadata repository, reducing operational overhead and making it cost-effective. Using it as an external data catalog means you don't have to manage additional database infrastructure.\nSeamless Migration: Migrating your existing Hive metastore to Amazon EMR ensures compatibility with your current Hadoop setup. EMR is designed to run Hadoop workloads, facilitating this process.\nFlexibility: An external data catalog in AWS Glue offers flexibility and separation of concerns. Your metastore remains managed by EMR for your Hadoop workloads, while Glue provides a centralized catalog for broader AWS data sources.","timestamp":"1712982300.0","comment_id":"1194644","upvote_count":"2"},{"comment_id":"1190356","upvote_count":"1","timestamp":"1712402280.0","poster":"nyaopoko","content":"B is answer!\nBy leveraging AWS Glue Data Catalog as an external data catalog and migrating the existing Hive metastore into Amazon EMR, the company can achieve a serverless, persistent, and cost-effective solution for storing and managing their data catalog."},{"content":"Selected Answer: B\nB. https://aws.amazon.com/jp/blogs/big-data/migrate-and-deploy-your-apache-hive-metastore-on-amazon-emr/","timestamp":"1712381700.0","poster":"arvehisa","comment_id":"1190247","upvote_count":"2"},{"upvote_count":"2","poster":"lucas_rfsb","content":"Selected Answer: A\nI will go with A. Besides DMS is typical for migration, it's the only choice which explicitly concerns about how the migration itself will be made. Other choices would demand a script or GLUE ETL job if you will. But this logic of migration was never put","comment_id":"1187761","timestamp":"1712018520.0"},{"timestamp":"1711380240.0","content":"I will go with A","comment_id":"1182594","upvote_count":"2","poster":"LeoSantos121212121212121"},{"comment_id":"1179930","poster":"jpmadan","upvote_count":"1","timestamp":"1711093320.0","content":"Selected Answer: B\nserverless catalog in AWS == glue"},{"timestamp":"1709753760.0","poster":"damaldon","content":"B.\nSet up an AWS Glue ETL job which extracts metadata from your Hive metastore (MySQL) and loads it into your AWS Glue Data Catalog. This method requires an AWS Glue connection to the Hive metastore as a JDBC source. An ETL script is provided to extract metadata from the Hive metastore and write it to AWS Glue Data Catalog.\nhttps://github.com/aws-samples/aws-glue-samples/blob/master/utilities/Hive_metastore_migration/README.md","comment_id":"1167445","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/big-data/migrate-and-deploy-your-apache-hive-metastore-on-amazon-emr/ Option B is likely the most suitable. Migrating the Hive metastore into Amazon EMR and using AWS Glue Data Catalog as an external catalog provides a balance between leveraging the scalable and managed services of AWS (like EMR and Glue Data Catalog) and ensuring a smooth transition from the on-premises setup. This approach leverages the serverless nature of AWS Glue Data Catalog, minimizing operational overhead and potentially reducing costs compared to managing database servers.","timestamp":"1706868000.0","poster":"rralucard_","upvote_count":"3","comment_id":"1138404"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/132680-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":158},{"id":"v2gTHTPupqiov1tlO8FR","exam_id":21,"answer_images":[],"answer_description":"","choices":{"C":"Upgrade the reserved node from ra3.4xlarge to ra3.16xlarge.","A":"Change the sort key to be the data column that is most often used in a WHERE clause of the SQL SELECT statement.","D":"Change the primary key to be the data column that is most often used in a WHERE clause of the SQL SELECT statement.","B":"Change the distribution key to the table column that has the largest dimension."},"answer_ET":"B","answer":"B","unix_timestamp":1706868420,"question_id":159,"discussion":[{"upvote_count":"7","comment_id":"1138409","poster":"rralucard_","timestamp":"1722586020.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html\nOption B, changing the distribution key, is the most effective solution to balance the load more evenly across all five compute nodes. Selecting an appropriate distribution key that aligns with the query patterns and data characteristics can result in a more uniform distribution of data and workloads, thus reducing the likelihood of one node being overutilized while others are underutilized."},{"upvote_count":"2","content":"Selected Answer: B\nIn a Redshift cluster with key distribution, data is distributed across compute nodes based on the values of the distribution key. An uneven distribution can lead to skewed workloads on specific nodes.\nBy choosing the table column with the largest dimension (most distinct values) as the distribution key, you ensure a more even spread of data across all nodes. This balances the processing load on each node when queries access that column.","comment_id":"1228214","timestamp":"1733891700.0","poster":"pypelyncar"},{"poster":"khchan123","timestamp":"1730102400.0","upvote_count":"2","comment_id":"1203386","content":"Selected Answer: B\nThe correct solution is B. Change the distribution key to the table column that has the largest dimension. This will help to distribute the data more evenly across the nodes, reducing the load on the heavily utilized node."},{"comments":[{"upvote_count":"1","timestamp":"1733036220.0","comment_id":"1222510","poster":"tgv","content":"The sort key determines the order of data storage and can improve query performance for specific queries, but it does not directly affect the distribution of data across nodes. Therefore, this will not address the uneven CPU load issue."}],"comment_id":"1194645","upvote_count":"1","timestamp":"1728793980.0","poster":"Christina666","content":"Selected Answer: A\nGemini result:\nUnderstanding the Problem:\n\nThe scenario describes a Redshift cluster with uneven load distribution. This indicates potential issues with either the distribution style or the sort key.\n\nKey Distribution:\n\nThe problem states that the cluster uses key distribution, meaning a specific column is designated as the distribution key. Data rows with matching distribution key values are placed on the same node.\n\nSort Key:\n\nA sort key determines the order in which data is physically stored within a table's blocks on a node. A well-chosen sort key can significantly optimize query performance, especially when queries often filter by that column."},{"comment_id":"1167457","poster":"damaldon","timestamp":"1725645480.0","upvote_count":"2","content":"B.\nWith \"Key distribution\". The rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns. This way, matching values from the common columns are physically stored together. \nhttps://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html"}],"question_text":"A company uses an Amazon Redshift provisioned cluster as its database. The Redshift cluster has five reserved ra3.4xlarge nodes and uses key distribution.\nA data engineer notices that one of the nodes frequently has a CPU load over 90%. SQL Queries that run on the node are queued. The other four nodes usually have a CPU load under 15% during daily operations.\nThe data engineer wants to maintain the current number of compute nodes. The data engineer also wants to balance the load more evenly across all five compute nodes.\nWhich solution will meet these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/132681-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","question_images":[],"timestamp":"2024-02-02 11:07:00","answers_community":["B (92%)","8%"],"isMC":true},{"id":"3XaG6pEqqVrKGQC61bLP","exam_id":21,"answer_images":[],"answer_description":"","choices":{"B":"Create an Amazon Redshift provisioned cluster. Create an Amazon Redshift Spectrum database for the analytics department to explore the data that is in Amazon S3. Create Redshift stored procedures to load the data into Amazon Redshift.","A":"Create an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create a new AWS Glue workload to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless.","D":"Create an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create AWS Lambda user defined functions (UDFs) by using the Amazon Redshift Data API. Create an AWS Step Functions job to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless.","C":"Create an Amazon Athena workgroup. Explore the data that is in Amazon S3 by using Apache Spark through Athena. Provide the Athena workgroup schema and tables to the analytics department."},"answer_ET":"A","answer":"A","unix_timestamp":1706870580,"discussion":[{"content":"Selected Answer: A\nOption A, creating an AWS Glue Data Catalog with Glue Schema Registry and orchestrating data ingestion into Amazon Redshift Serverless using AWS Glue, appears to be the most cost-effective and suitable solution. It offers a serverless approach to manage the evolving data schema of the IoT data and efficiently supports data analytics needs without the overhead of managing a provisioned database cluster or complex orchestration setups.","timestamp":"1706870580.0","comments":[{"timestamp":"1712402460.0","content":"Selected Answer: A\nAmazon Redshift Serverless is a serverless option for Amazon Redshift, which means you don't have to provision and manage clusters. This makes it a cost-effective choice for the analytics department's use case.","upvote_count":"2","poster":"nyaopoko","comment_id":"1190357"}],"poster":"rralucard_","comment_id":"1138433","upvote_count":"9"},{"comment_id":"1213874","content":"Selected Answer: A\nAthena is not able to create new data catalog","poster":"VerRi","timestamp":"1716135420.0","upvote_count":"1"},{"poster":"sdas1","timestamp":"1715093520.0","upvote_count":"1","comment_id":"1207917","content":"Option C\n\nCost-effectiveness: Amazon Athena allows you to query data directly from Amazon S3 without the need for any infrastructure setup or management. You pay only for the queries you run, making it cost-effective, especially for sporadic or exploratory analysis.\nFlexibility: Since the data structure can change with IoT device upgrades, using Athena allows for flexibility in querying and analyzing the data regardless of its structure. You don't need to define a fixed schema upfront, enabling you to adapt to changes seamlessly.\nApache Spark Support: Athena supports querying data using Apache Spark, which is powerful for processing and analyzing large datasets. This capability ensures that the analytics department can leverage Spark for more advanced analytics if needed.\nhttps://www.youtube.com/watch?v=Q93NZJBFSWw"},{"upvote_count":"2","poster":"khchan123","comments":[{"timestamp":"1733395020.0","comment_id":"1322324","poster":"altonh","content":"However, combined with Notebook, Athena+Spark can be a powerful tool for analytics.","upvote_count":"1"}],"comment_id":"1203387","timestamp":"1714284180.0","content":"Selected Answer: A\nThe correct solution is A. Create an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create a new AWS Glue workload to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless.\n\nOption C (Amazon Athena and Apache Spark) is suitable for ad-hoc querying and exploration but may not be the best choice for the analytics department's ongoing data analysis needs, as Athena is designed for interactive querying rather than complex data transformations."},{"upvote_count":"4","timestamp":"1713349140.0","poster":"chris_spencer","content":"Selected Answer: A\nThe objective is to create a data catalog that includes the IoT data and AWS Glue Data Catalog is the best option for this requirement.\nhttps://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html\n\nC is incorrect. While Athena makes it easy to read from S3 using SQL, it does not crawl the data source and create a data catalog.","comment_id":"1197154"},{"poster":"Christina666","timestamp":"1712991840.0","upvote_count":"2","comment_id":"1194717","content":"Selected Answer: C\nWhy Option C is the Most Cost-Effective\n\nServerless and Pay-as-you-go: Athena is a serverless query service, meaning you only pay for the queries the analytics department runs. No need to provision and manage always-running clusters.\nFlexible Schema Handling: Athena works well with semi-structured data like JSON and can handle schema evolution on the fly. This is perfect for the scenario where IoT data structures might change.\nSpark Integration: Integrating Apache Spark with Athena provides rich capabilities for data processing and transformation.\nEase of Use for Analytics: Athena's familiar SQL-like interface and ability to directly query S3 data make it convenient for the analytics department."},{"content":"Selected Answer: C\nOptions A, B, and D involve setting up additional infrastructure (e.g., AWS Glue, Redshift clusters, Lambda functions) which may incur unnecessary costs and complexity for the given requirements. Option C, on the other hand, utilizes a serverless and scalable solution directly querying data in S3, making it the most cost-effective choice.","comment_id":"1187769","timestamp":"1712019480.0","poster":"lucas_rfsb","upvote_count":"2"}],"question_id":160,"url":"https://www.examtopics.com/discussions/amazon/view/132683-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A security company stores IoT data that is in JSON format in an Amazon S3 bucket. The data structure can change when the company upgrades the IoT devices. The company wants to create a data catalog that includes the IoT data. The company's analytics department will use the data catalog to index the data.\nWhich solution will meet these requirements MOST cost-effectively?","topic":"1","question_images":[],"timestamp":"2024-02-02 11:43:00","answers_community":["A (80%)","C (20%)"],"isMC":true}],"exam":{"isImplemented":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","lastUpdated":"11 Apr 2025","numberOfQuestions":207,"id":21,"isBeta":false,"isMCOnly":true,"provider":"Amazon"},"currentPage":32},"__N_SSP":true}