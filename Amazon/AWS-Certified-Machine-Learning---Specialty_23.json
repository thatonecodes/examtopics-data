{"pageProps":{"questions":[{"id":"YyMvX7kNACmOlBAjfDkl","topic":"1","answer_images":[],"exam_id":26,"timestamp":"2022-11-28 18:30:00","isMC":true,"discussion":[{"timestamp":"1702271220.0","content":"Selected Answer: AE\nKNN can be used for dimensionality reduction through NCA (https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html#)","upvote_count":"10","poster":"Peeking","comment_id":"741422"},{"timestamp":"1705104120.0","upvote_count":"7","content":"Selected Answer: AE\nA. Correct\nB. Incorrect. MDS is Non-linear dimensionality reduction method. https://towardsdatascience.com/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b\nC. Incorrect. This is a classification problem instead of Regression.\nD. Incorrect. K-means is for Clustering(Unsupervised learning).\nE. Correct.","poster":"Jerry84","comment_id":"773959","comments":[{"timestamp":"1723388100.0","upvote_count":"2","comment_id":"978735","poster":"kaike_reis","content":"Why \"Non-linear dimensionality reduction method\" is a problem? We can add non linear features as x^2 to a model to improve performance."},{"content":"E : https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html","upvote_count":"2","poster":"drcok87","timestamp":"1708131360.0","comment_id":"811267"}]},{"content":"B is wrong as Multidimensional Scaling sits under the Unsupervised branch of Machine Learning algorithms","upvote_count":"1","comment_id":"1041068","timestamp":"1728676380.0","poster":"Pt8442"},{"comment_id":"1002542","timestamp":"1725803400.0","upvote_count":"1","content":"Selected Answer: AE\nA. YES - F1 score is low. Reducing feature count could improve F1 score.\nB. NO - MDS is for visualization\nC. NO - regressor is to predict a numerical value, we want classification\nD. NO - K-means is clustering, we want classification \nE. YES - k-Means could work if a linear model is not best","poster":"loict"},{"poster":"Mickey321","timestamp":"1724503200.0","content":"Selected Answer: AE\nA & E are correct","comment_id":"989199","upvote_count":"1"},{"content":"Selected Answer: AB\nA and B are correct.\nBut I understand E being correct as well.","timestamp":"1723387800.0","poster":"kaike_reis","comment_id":"978733","upvote_count":"1"},{"timestamp":"1712073360.0","content":"Selected Answer: AE\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html","poster":"Mllb","upvote_count":"6","comment_id":"859001"},{"upvote_count":"2","poster":"dunhill","content":"I think the answer is AB. k-means and k-nearest neighbors are not for reduce dimension.","comment_id":"729468","timestamp":"1701192600.0","comments":[{"upvote_count":"2","content":"k-nn does. https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html","poster":"cox1960","comment_id":"885199","timestamp":"1714481040.0"}]}],"question_text":"A manufacturing company needs to identify returned smartphones that have been damaged by moisture. The company has an automated process that produces 2,000 diagnostic values for each phone. The database contains more than five million phone evaluations. The evaluation process is consistent, and there are no missing values in the data. A machine learning (ML) specialist has trained an Amazon SageMaker linear learner ML model to classify phones as moisture damaged or not moisture damaged by using all available features. The model's F1 score is 0.6.\n\nWhich changes in model training would MOST likely improve the model's F1 score? (Choose two.)","answers_community":["AE (96%)","4%"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/89151-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1669656600,"question_images":[],"choices":{"B":"Continue to use the SageMaker linear learner algorithm. Reduce the number of features with the scikit-learn multi-dimensional scaling (MDS) algorithm.","A":"Continue to use the SageMaker linear learner algorithm. Reduce the number of features with the SageMaker principal component analysis (PCA) algorithm.","D":"Use the SageMaker k-means algorithm with k of less than 1,000 to train the model.","C":"Continue to use the SageMaker linear learner algorithm. Set the predictor type to regressor.","E":"Use the SageMaker k-nearest neighbors (k-NN) algorithm. Set a dimension reduction target of less than 1,000 to train the model."},"answer":"AE","answer_ET":"AE","question_id":111},{"id":"JmRj9LRQpN4KeKRLdryv","answers_community":["B (100%)"],"topic":"1","choices":{"B":"Build a collaborative filtering recommendation engine with Apache Spark ML on Amazon EMR.","C":"Build a model-based filtering recommendation engine with Apache Spark ML on Amazon EMR","A":"Build a content-based filtering recommendation engine with Apache Spark ML on Amazon EMR","D":"Build a combinative filtering recommendation engine with Apache Spark ML on Amazon EMR"},"url":"https://www.examtopics.com/discussions/amazon/view/11248-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":112,"question_images":[],"exam_id":26,"unix_timestamp":1577952300,"timestamp":"2020-01-02 09:05:00","answer_images":[],"answer_ET":"B","answer":"B","question_text":"A Machine Learning Specialist is designing a system for improving sales for a company. The objective is to use the large amount of information the company has on users' behavior and product preferences to predict which products users would like based on the users' similarity to other users.\nWhat should the Specialist do to meet this objective?","answer_description":"","isMC":true,"discussion":[{"content":"B\nsee https://en.wikipedia.org/wiki/Collaborative_filtering#Model-based","comment_id":"34475","upvote_count":"21","timestamp":"1632255180.0","poster":"mlyu"},{"upvote_count":"13","poster":"kalyanvarma","comment_id":"252388","content":"Content-based filtering relies on similarities between features of items, whereas colloborative-based filtering relies on preferences from other users and how they respond to similar items.","timestamp":"1636282440.0"},{"upvote_count":"2","timestamp":"1728917280.0","content":"Answer is B : Build a collaborative filtering recommendation engine with Apache Spark ML on Amazon EMR.\nCollaborative filtering focuses on user behavior and preferences therefore it is perfect for predicting products based on user similarities.","comment_id":"1297694","poster":"Manju_Bn"},{"content":"Selected Answer: B\nB. Build a collaborative filtering recommendation engine with Apache Spark ML on Amazon EMR.\n\nCollaborative filtering is a technique used to recommend products to users based on their similarity to other users. It is a widely used method for building recommendation engines. Apache Spark ML is a distributed machine learning library that provides scalable implementations of collaborative filtering algorithms. Amazon EMR is a managed cluster platform that provides easy access to Apache Spark and other distributed computing frameworks.","poster":"AjoseO","upvote_count":"1","timestamp":"1727163720.0","comment_id":"815961"},{"comment_id":"799815","timestamp":"1727163720.0","content":"Selected Answer: B\nBuild a collaborative filtering recommendation engine with Apache Spark ML on Amazon EMR. ( TRUE )\n\nCollaborative filtering is a commonly used method for recommendation systems that aims to predict the preferences of a user based on the behavior of similar users. In the case described, the objective is to use users' behavior and product preferences to predict which products they want, making collaborative filtering a good fit. \n\nApache Spark ML is a machine learning library that provides scalable, efficient algorithms for building recommendation systems, while Amazon EMR provides a cloud-based platform for running Spark applications.\nYou can find more detail in https://www.udemy.com/course/aws-certified-machine-learning-specialty-2023","upvote_count":"2","poster":"solution123"},{"content":"Selected Answer: B\ncollaborative filtering","upvote_count":"1","timestamp":"1726662060.0","poster":"ychaabane","comment_id":"1285716"},{"comment_id":"1167580","timestamp":"1709773380.0","upvote_count":"1","poster":"james2033","content":"Selected Answer: B\n'Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users.' \n\nSource: https://realpython.com/build-recommendation-engine-collaborative-filtering/#what-is-collaborative-filtering"},{"timestamp":"1694586000.0","poster":"loict","upvote_count":"4","comment_id":"1006256","content":"Selected Answer: B\nA. NO - content-based filtering looks at similarities with items the user already looked at, not activities of other users\nB. YES - state of the art\nC. NO - too generic terms, everything is a model\nD. NO - combinative filtering does not exist"},{"content":"Selected Answer: B\nCollaborative filtering is a technique used by recommendation engines to make predictions about the interests of a user by collecting preferences or taste information from many users. The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have Bâ€™s opinion on a different issue than that of a randomly chosen person.","upvote_count":"2","comment_id":"972989","poster":"Mickey321","timestamp":"1691237340.0"},{"comment_id":"970229","content":"Selected Answer: B\nB. Build a collaborative filtering recommendation engine with Apache Spark ML on Amazon EMR.","timestamp":"1690983300.0","poster":"Mickey321","upvote_count":"1"},{"content":"Selected Answer: B\nI think it should be b","poster":"Venkatesh_Babu","upvote_count":"1","timestamp":"1690207080.0","comment_id":"961664"},{"content":"Selected Answer: B\nContent-based recommendations rely on product similarity. If a user likes a product, products that are similar to that one will be recommended. Collaborative recommendations are based on user similarity. If you and other users have given similar reviews to a range of products, the model assumes it is likely that other products those other people have liked but that you haven't purchased should be a good recommendation for you.","comment_id":"846448","timestamp":"1679441400.0","upvote_count":"4","poster":"brunokiyoshi"},{"poster":"dreswardev","timestamp":"1672521000.0","upvote_count":"1","content":"feature engineering is required, use model based","comment_id":"762940"},{"content":"Selected Answer: B\nAnswer is \"B\"","timestamp":"1663459200.0","comment_id":"671863","poster":"ryuhei","upvote_count":"1"},{"poster":"roytruong","content":"go for B","comment_id":"98653","upvote_count":"2","timestamp":"1635330780.0"},{"upvote_count":"6","poster":"cybe001","content":"B is correct\nhttps://aws.amazon.com/blogs/big-data/building-a-recommendation-engine-with-spark-ml-on-amazon-emr-using-zeppelin/","timestamp":"1635046680.0","comment_id":"37736"}]},{"id":"20ChY4GZLOkoh92htDnK","timestamp":"2019-12-06 11:47:00","choices":{"B":"Produce a set of synonyms for every word using Amazon Mechanical Turk.","C":"Create word embedding vectors that store edit distance with every other word.","D":"Download word embeddings pre-trained on a large corpus.","A":"Create one-hot word encoding vectors."},"exam_id":26,"answer_ET":"D","question_images":[],"unix_timestamp":1575629220,"answer":"D","discussion":[{"timestamp":"1633353780.0","upvote_count":"31","comment_id":"35239","content":"the solution is word embedding. As it is a interactive online dictionary, we need pre-trained word embedding thus the answer is D. In addition, there is no mention that the online dictonary is unique and does not have a pre-trained word embedding.\nThus I strongly feel the answer is D","poster":"JayK"},{"content":"D is correct. It is not a specialized dictionary so use the existing word corpus to train the model","timestamp":"1634282220.0","poster":"cybe001","upvote_count":"16","comment_id":"37843"},{"comment_id":"1358018","upvote_count":"1","content":"Selected Answer: D\nD. Download word embeddings pre-trained on a large corpus.\nReason :\nFor a nearest neighbor model that finds words used in similar contexts, word embeddings are the best choice. Pre-trained word embeddings capture semantic relationships and contextual similarity between words based on a large text corpus (e.g., Wikipedia, Common Crawl).\n\nThe Specialist should:\n\nUse pre-trained word embeddings like Word2Vec, GloVe, or FastText.\nLoad the embeddings into the model for efficient similarity comparisons.\nUse a nearest neighbor search algorithm (e.g., FAISS, k-d tree, Annoy) to quickly find similar words.","poster":"JonSno","timestamp":"1739832900.0"},{"content":"Selected Answer: D\nD. Download word embeddings pre-trained on a large corpus.\n\nWord embeddings are a type of dense representation of words, which encode semantic meaning in a vector form. These embeddings are typically pre-trained on a large corpus of text data, such as a large set of books, news articles, or web pages, and capture the context in which words are used. Word embeddings can be used as features for a nearest neighbor model, which can be used to find words used in similar contexts.\n\nDownloading pre-trained word embeddings is a good way to get started quickly and leverage the strengths of these representations, which have been optimized on a large amount of data. This is likely to result in more accurate and reliable features than other options like one-hot encoding, edit distance, or using Amazon Mechanical Turk to produce synonyms.","timestamp":"1727164800.0","comment_id":"803234","poster":"AjoseO","upvote_count":"6"},{"comment_id":"1006381","content":"Selected Answer: D\nA. NO - one-hot encoding is a very early featurization stage\nB. NO - we don't want human labelling\nC. NO - too costly to do from scratch\nD. YES - leverage exiting training; the word embeddings will provide vectors than be used to measure distance in the downstream nearest neighbor model","upvote_count":"3","poster":"loict","timestamp":"1727164800.0"},{"poster":"game_changer","comment_id":"1106809","timestamp":"1727164800.0","content":"Selected Answer: D\nPre-trained word embeddings, such as Word2Vec, GloVe, or FastText, capture the semantic and contextual meaning of words based on a large corpus of text data. By downloading pre-trained word embeddings, the Specialist can leverage the semantic relationships between words to provide meaningful word features for the nearest neighbor model powering the widget. Utilizing pre-trained word embeddings allows the model to understand and display words used in similar contexts effectively.","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: D\ncorrect D ay tupoy","comment_id":"1066990","timestamp":"1699599780.0","poster":"elvin_ml_qayiran25091992razor"},{"poster":"sonoluminescence","content":"Selected Answer: D\nwords that are used in similar contexts will have vectors that are close in the embedding space","comment_id":"1056296","upvote_count":"1","timestamp":"1698507360.0"},{"poster":"Mickey321","comment_id":"973142","upvote_count":"1","content":"Selected Answer: D\nD is correct","timestamp":"1691249760.0"},{"poster":"DavidRou","comment_id":"968188","content":"I also believe that D is the correct answer. No reason to create word embeddings from scratch","upvote_count":"1","timestamp":"1690813860.0"},{"poster":"ortamina","timestamp":"1689253380.0","upvote_count":"1","comment_id":"950683","content":"Selected Answer: D\n1. One-hot encoding will blow up the feature space - it is not recommended for a high cardinality problem domain.\n\n2. One still needs to train the word features on large bodies of text to map context to each word"},{"content":"12-sep exam","timestamp":"1663009560.0","comment_id":"667391","upvote_count":"1","poster":"Shailendraa"},{"poster":"helpaws","timestamp":"1661456760.0","content":"Selected Answer: D\nDDDDDDDDDDDDD","comment_id":"651925","upvote_count":"3"},{"content":"D for sure","poster":"engomaradel","timestamp":"1636110420.0","upvote_count":"2","comment_id":"241494"},{"upvote_count":"3","timestamp":"1636084320.0","comment_id":"212169","poster":"yeetusdeleetus","content":"Definitely D."},{"poster":"weslleylc","upvote_count":"1","comment_id":"202051","timestamp":"1635717780.0","content":"A)It requires that document text be cleaned and prepared such that each word is one-hot encoded.\nRef:https://machinelearningmastery.com/what-are-word-embeddings/"},{"timestamp":"1635635160.0","upvote_count":"2","comment_id":"165075","poster":"syu31svc","content":"I don't see how one-hot encoding works; I would say D 100%\nB & C are definitely wrong"},{"comment_id":"149790","comments":[{"comment_id":"149792","content":"Appears D is the correct answer....","poster":"GeeBeeEl","upvote_count":"2","timestamp":"1635393120.0"}],"poster":"GeeBeeEl","content":"took a look at https://medium.com/@athif.shaffy/one-hot-encoding-of-text-b69124bef0a7 it appears this is useful at the point of running the model. See: https://arxiv.org/abs/1705.08488 Word embeddings are dense, low-dimensional vector representations of words that are commonly used as input features in a variety of natural language processing (NLP) tasks [1]. In contrast to symbolic one-hot or hierarchical clusteringâ€“based representations, real-valued embedding vectors easily reflect varying degrees of similarity between words, and significantly reduce sparsity in linear algebra operations.","upvote_count":"1","timestamp":"1634973660.0"},{"comment_id":"137527","content":"I think should be A, we need to provide features for next step","comments":[{"comment_id":"433756","timestamp":"1636302960.0","upvote_count":"1","content":"learn what one-hot means. each and every word will be unique, how will you work with that data? D is the answer","poster":"Dr_Kiko"}],"poster":"Achievement","upvote_count":"1","timestamp":"1634596020.0"},{"timestamp":"1634568120.0","comment_id":"109576","poster":"andreylh","content":"Why not A since it is required to pass words as features to a nearest neighbor model?","upvote_count":"3"},{"upvote_count":"3","content":"absolutely D","timestamp":"1634356020.0","poster":"roytruong","comment_id":"98689"},{"poster":"AKT","timestamp":"1634345100.0","upvote_count":"2","content":"answer is D","comment_id":"58360"},{"upvote_count":"5","timestamp":"1632439680.0","comment_id":"34366","comments":[{"comments":[{"timestamp":"1632596820.0","comment_id":"35149","upvote_count":"2","comments":[{"upvote_count":"3","content":"C is irrelevant as it only concern about the \"edit distance\", not the meaning of the word for \"used in similar context\"\n\nCorrect answer should be D","timestamp":"1635979680.0","poster":"HaiHN","comment_id":"202882","comments":[{"timestamp":"1636271820.0","comment_id":"278138","upvote_count":"4","poster":"cloud_trail","content":"Correct. Edit distance or Levenshtein Distance concerns spelling and is used by spell checkers. Has nothing to do with context. D is the obvious answer. There's no need to create your own embedding when you can just download pre-trained ones."}]}],"poster":"WWODIN","content":"sorry again, should be C or D, but more towards D"}],"upvote_count":"2","timestamp":"1632513960.0","poster":"WWODIN","comment_id":"34984","content":"sorry, seems A is a necessary first step"}],"poster":"WWODIN","content":"Seems D?"},{"upvote_count":"2","poster":"rsimham","content":"C sounds to be right for me, not sure.\nhttps://medium.com/@athif.shaffy/one-hot-encoding-of-text-b69124bef0a7","comment_id":"28054","timestamp":"1632395820.0"},{"comment_id":"27175","upvote_count":"4","poster":"vetal","content":"Why not D?","timestamp":"1632108300.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/9825-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"answers_community":["D (100%)"],"question_text":"An interactive online dictionary wants to add a widget that displays words used in similar contexts. A Machine Learning Specialist is asked to provide word features for the downstream nearest neighbor model powering the widget.\nWhat should the Specialist do to meet these requirements?","topic":"1","answer_images":[],"question_id":113,"answer_description":""},{"id":"Dm8wNt1Gn4IAQlJhfHQK","discussion":[{"comments":[{"comment_id":"730885","poster":"hichemck","timestamp":"1701294840.0","comments":[{"timestamp":"1723388220.0","poster":"kaike_reis","comment_id":"978737","upvote_count":"3","content":"C is the most wrong solution."},{"poster":"Peeking","upvote_count":"3","content":"Autopilot is is not autoscaling in AWS. Autopilot is for model training, Autoscaling is during inference.","timestamp":"1702271640.0","comment_id":"741428"}],"upvote_count":"1","content":"false. its C. in the link you shared under High ModelLatency, it states \"If an endpoint is overused, it might cause higher model latency. You can add Auto scaling to an endpoint to dynamically increase and decrease the number of instances available for an instance.\""}],"content":"Selected Answer: B\nIt's B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-endpoint-latency/","upvote_count":"10","timestamp":"1701070440.0","poster":"tsangckl","comment_id":"728004"},{"timestamp":"1726053540.0","upvote_count":"2","content":"Selected Answer: B\nA. NO - that is image processing so more CPU would only provide incremental improvement\nB. YES - that is image processing so GPU would provide a step change; supported by the built-in algorithm\nC. NO - Autopilot is for training, not inference\nD. NO - usually inference uses little memory","poster":"loict","comment_id":"1004668"},{"upvote_count":"1","comment_id":"970012","content":"Selected Answer: B\nAttach an Amazon Elastic Inference ml.eia2.medium accelerator to the endpoint instance. Amazon Elastic Inference allows users to attach low-cost GPU-powered acceleration to Amazon EC2 and SageMaker instances or Amazon ECS tasks, to reduce the cost of running deep learning inference by up to 75%","poster":"Mickey321","timestamp":"1722592500.0"},{"comment_id":"885145","timestamp":"1714476000.0","content":"B is not correct anymore. \nAfter April 15, 2023, new customers will not be able to launch instances with Amazon EI accelerators in Amazon SageMaker, Amazon ECS, or Amazon EC2. (https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html)","comments":[{"content":"Changes in exams apply 6 months after the change as been applied (oct 2023)","poster":"robotgeek","comments":[{"upvote_count":"2","content":"Freak!","comment_id":"971102","poster":"ccpmad","timestamp":"1722687960.0"}],"timestamp":"1722147900.0","comment_id":"965320","upvote_count":"2"}],"upvote_count":"1","poster":"Maaayaaa"},{"comments":[{"content":"bro, isn't this exactly the question is asking for?","upvote_count":"2","poster":"ZSun","timestamp":"1714666680.0","comment_id":"887663"}],"content":"Selected Answer: B\nIt's B.\nBurstable instances only solves when lot of users are making inferences at the same time","timestamp":"1712074260.0","upvote_count":"2","poster":"Mllb","comment_id":"859022"},{"poster":"AjoseO","timestamp":"1708390980.0","upvote_count":"2","content":"Selected Answer: B\nThe ModelLatency metric shows that the model inference time is causing the latency issue. Amazon Elastic Inference is designed to speed up the inference process of a machine learning model without needing to deploy the model on a more powerful instance. By attaching an Elastic Inference accelerator to the endpoint instance, the ML specialist can offload the compute-intensive parts of the inference process to the accelerator, resulting in faster inference times and lower latency.","comment_id":"814706"},{"comment_id":"775209","content":"B - https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-endpoint-latency/","upvote_count":"1","poster":"It626","timestamp":"1705222680.0"},{"timestamp":"1702271580.0","upvote_count":"3","content":"Selected Answer: B\nElastic Inference accelerator ( and AutoScaling but there is no autoscaling in the option). Be aware that Autopilot is is not autoscaling.","poster":"Peeking","comment_id":"741425"}],"timestamp":"2022-11-27 08:34:00","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/88924-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A company is building a machine learning (ML) model to classify images of plants. An ML specialist has trained the model using the Amazon SageMaker built-in Image Classification algorithm. The model is hosted using a SageMaker endpoint on an ml.m5.xlarge instance for real-time inference. When used by researchers in the field, the inference has greater latency than is acceptable. The latency gets worse when multiple researchers perform inference at the same time on their devices. Using Amazon CloudWatch metrics, the ML specialist notices that the ModelLatency metric shows a high value and is responsible for most of the response latency.\n\nThe ML specialist needs to fix the performance issue so that researchers can experience less latency when performing inference from their devices.\n\nWhich action should the ML specialist take to meet this requirement?","isMC":true,"unix_timestamp":1669534440,"topic":"1","choices":{"A":"Change the endpoint instance to an ml.t3 burstable instance with the same vCPU number as the ml.m5.xlarge instance has.","C":"Enable Amazon SageMaker Autopilot to automatically tune performance of the model.","B":"Attach an Amazon Elastic Inference ml.eia2.medium accelerator to the endpoint instance.","D":"Change the endpoint instance to use a memory optimized ML instance."},"answer_images":[],"answer_ET":"B","question_images":[],"answer_description":"","exam_id":26,"question_id":114,"answers_community":["B (100%)"]},{"id":"HF91pEODzaMhhpvrtxQ4","question_images":[],"answer_description":"","isMC":true,"timestamp":"2022-11-29 16:51:00","topic":"1","exam_id":26,"question_id":115,"answer":"C","discussion":[{"poster":"loict","content":"Selected Answer: C\nA. NO - Must use SageMaker Debugger for visibility into model insights \nB. NO - Hyperparameters will most likely influence model accuracy but not response time\nC. YES - SageMaker Debugger is the right tool for model insights; filter (or \"kernels\") slides in CNN to identify specific features\nD. NO - SageMaker Model Monitor is for model performance","upvote_count":"3","timestamp":"1726054020.0","comment_id":"1004686"},{"timestamp":"1723986420.0","content":"Selected Answer: C\nPruning is a technique that reduces the complexity of convolutional neural networks (CNNs) by removing unimportant filters or neurons. This can lead to faster inference times and lower memory consumption, which are desirable for self-driving applications. Pruning can be done by ranking the filters based on some criteria, such as the norm of the weights, the activation outputs, or the Taylor expansion of the loss function123.","poster":"Mickey321","upvote_count":"1","comment_id":"984509"},{"timestamp":"1723388400.0","poster":"kaike_reis","comment_id":"978741","comments":[{"comment_id":"1007701","upvote_count":"3","poster":"teka112233","timestamp":"1726321320.0","content":"you are very right, about how awesome ChatGPT, but since we find it's answers over here, so some colleagues are trying to help us in proving why these could be the right answers without wasting our time to prove it.\nAll the names over here are without any way of connection and most of the names are fictitious, so when the leave their answers, we don't know them but still we know the right answers with the right proof."}],"upvote_count":"3","content":"Selected Answer: C\nChatGPT is an awesome tool, but please ML colleagues: study!"},{"comment_id":"970016","content":"Selected Answer: C\nThe company should use solution C. Use SageMaker Debugger for visibility into the training weights, gradients, biases, and activation outputs. Compute the filter ranks based on this information. Apply pruning to remove the low-ranking filters. Set the new weights. Run a new training job with the pruned model.","poster":"Mickey321","timestamp":"1722592980.0","upvote_count":"1"},{"poster":"Tony_1406","comment_id":"884473","timestamp":"1714405680.0","content":"Selected Answer: C\nSame example here:\nhttps://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/","upvote_count":"2"},{"content":"Selected Answer: B\nTo reduce the time required for performing inferences in autonomous cars, the automotive company should use SageMaker Debugger for visibility into the training weights, gradients, biases, and activation outputs. They can adjust the model hyperparameters and look for lower inference times. They can also use SageMaker Model Monitor for visibility into the ModelLatency metric and OverheadLatency metric of the model after the model is deployed. However, option C, which suggests computing the filter ranks based on the training outputs and applying pruning to remove the low-ranking filters, is not applicable for transfer learning models since the layers in the pre-trained model are already trained and cannot be changed. Therefore, the correct solution is B.","timestamp":"1714108740.0","poster":"Gaby999","comments":[{"comment_id":"972158","timestamp":"1722775560.0","content":"better not use chatgpt without knowing something of AWS, it will trick you","upvote_count":"3","poster":"ccpmad"}],"upvote_count":"1","comment_id":"881137"},{"upvote_count":"2","poster":"Valcilio","comment_id":"833919","timestamp":"1709985180.0","content":"Selected Answer: C\nEven if a better machine could help, the problem is about the model, not about the general or the machine in specific."},{"upvote_count":"4","poster":"AjoseO","content":"Selected Answer: C\nUsing SageMaker Debugger, the company can monitor the training process and evaluate the performance of the model by computing filter ranks based on information like weights, gradients, biases, and activation outputs. \n\nAfter identifying the low-ranking filters, the company can apply pruning to remove them and set new weights. \n\nBy doing so, the company can reduce the model size and improve the inference time. Finally, a new training job with the pruned model can be run to verify the performance improvements\n\nNot D because Model Monitor is a tool for monitoring the performance of deployed models, and it does not provide any direct feedback or insights into the model training process or ways to improve model inference time. Therefore, while Model Monitor can be useful for monitoring the performance of deployed models, it is not the best choice for evaluating and improving the performance of the models during the training phase, which is what the question is asking for.","timestamp":"1708390740.0","comment_id":"814705"},{"comment_id":"812953","upvote_count":"1","content":"It's between C and D. But I think it's C. \nC. https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/ Everything is there.\nD: https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-endpoint-latency/ Here it says use Cloudwatch to view ModelLatency and OverheadLatency, not Model Monitor. I think Model Monitor is just for model performance i.e. drift, bias, accuracy etc.","timestamp":"1708260540.0","poster":"wolfsong"},{"upvote_count":"1","poster":"expertguru","comment_id":"772742","content":"The answer I guess D per below , they should have said Sagemaker model monitor using cloud watch\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html","timestamp":"1704992580.0"},{"upvote_count":"4","comment_id":"755118","content":"C, \nhttps://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/","timestamp":"1703448300.0","poster":"jim20541"},{"content":"I would say 'D as a more generic approach than C. The problem can be caused not just filters.","poster":"Alphacentavra","upvote_count":"1","comment_id":"752639","timestamp":"1703183520.0"},{"poster":"BoroJohn","comment_id":"748219","timestamp":"1702831920.0","upvote_count":"2","content":"The answer is \"c\" as the question is asking for evaluate and improve the performance of the models? \"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-visualization.html\""},{"upvote_count":"2","comment_id":"730583","poster":"dunhill","content":"I think the answer is D.","timestamp":"1701273060.0"}],"question_text":"An automotive company is using computer vision in its autonomous cars. The company has trained its models successfully by using transfer learning from a convolutional neural network (CNN). The models are trained with PyTorch through the use of the Amazon SageMaker SDK. The company wants to reduce the time that is required for performing inferences, given the low latency that is required for self-driving.\n\nWhich solution should the company use to evaluate and improve the performance of the models?","answers_community":["C (100%)"],"answer_images":[],"unix_timestamp":1669737060,"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/89262-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"B":"Use SageMaker Debugger for visibility into the training weights, gradients, biases, and activation outputs. Adjust the model hyperparameters, and look for lower inference times. Run a new training job.","C":"Use SageMaker Debugger for visibility into the training weights, gradients, biases, and activation outputs. Compute the filter ranks based on this information. Apply pruning to remove the low-ranking filters. Set the new weights. Run a new training job with the pruned model.","A":"Use Amazon CloudWatch algorithm metrics for visibility into the SageMaker training weights, gradients, biases, and activation outputs. Compute the filter ranks based on this information. Apply pruning to remove the low-ranking filters. Set the new weights. Run a new training job with the pruned model.","D":"Use SageMaker Model Monitor for visibility into the ModelLatency metric and OverheadLatency metric of the model after the model is deployed. Adjust the model hyperparameters, and look for lower inference times. Run a new training job."}}],"exam":{"isBeta":false,"provider":"Amazon","isMCOnly":false,"lastUpdated":"11 Apr 2025","id":26,"isImplemented":true,"numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty"},"currentPage":23},"__N_SSP":true}