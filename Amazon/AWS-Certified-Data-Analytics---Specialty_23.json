{"pageProps":{"questions":[{"id":"4chIpqdop1daedix0Sot","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/27705-exam-aws-certified-data-analytics-specialty-topic-1-question/","unix_timestamp":1596975360,"answer_ET":"B","isMC":true,"exam_id":20,"choices":{"B":"Update the sensors code to use the PutRecord/PutRecords call from the Kinesis Data Streams API with the AWS SDK for Java. Use Kinesis Data Analytics to enrich the data based on a company-developed anomaly detection SQL script. Direct the output of KDA application to a Kinesis Data Firehose delivery stream, enable the data transformation feature to flatten the JSON file, and set the Kinesis Data Firehose destination to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.","C":"Set the RecordMaxBufferedTime property of the KPL to \"0\" to disable the buffering on the sensor side. Connect for each stream a dedicated Kinesis Data Firehose delivery stream and enable the data transformation feature to flatten the JSON file before sending it to an Amazon S3 bucket. Load the S3 data into an Amazon Redshift cluster.","A":"Set the RecordMaxBufferedTime property of the KPL to \"גˆ’1\" to disable the buffering on the sensor side. Use Kinesis Data Analytics to enrich the data based on a company-developed anomaly detection SQL script. Push the enriched data to a fleet of Kinesis data streams and enable the data transformation feature to flatten the JSON file. Instantiate a dense storage Amazon Redshift cluster and use it as the destination for the Kinesis Data Firehose delivery stream.","D":"Update the sensors code to use the PutRecord/PutRecords call from the Kinesis Data Streams API with the AWS SDK for Java. Use AWS Glue to fetch and process data from the stream using the Kinesis Client Library (KCL). Instantiate an Amazon Elasticsearch Service cluster and use AWS Lambda to directly push data into it."},"answers_community":["B (100%)"],"topic":"1","discussion":[{"timestamp":"1632225480.0","poster":"Priyanka_01","upvote_count":"21","content":"B?\nhttps://aws.amazon.com/blogs/big-data/perform-near-real-time-analytics-on-streaming-data-with-amazon-kinesis-and-amazon-elasticsearch-service/","comments":[{"upvote_count":"1","poster":"GCPereira","content":"agreed","timestamp":"1704998460.0","comment_id":"1120073"}],"comment_id":"158464"},{"upvote_count":"13","poster":"awssp12345","comments":[{"upvote_count":"14","timestamp":"1633060500.0","poster":"awssp12345","content":"I agree the answer is B.","comment_id":"168553"}],"content":"When Not to Use the KPL :\n\nThe KPL can incur an additional processing delay of up to RecordMaxBufferedTime within the library (user-configurable). Larger values of RecordMaxBufferedTime results in higher packing efficiencies and better performance. Applications that cannot tolerate this additional delay may need to use the AWS SDK directly. For more information about using the AWS SDK with Kinesis Data Streams, see Developing Producers Using the Amazon Kinesis Data Streams API with the AWS SDK for Java. For more information about RecordMaxBufferedTime and other user-configurable properties of the KPL, see Configuring the Kinesis Producer Library. \n\nhttps://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html","comment_id":"168534","timestamp":"1632702720.0"},{"timestamp":"1682949720.0","poster":"pk349","content":"B: I passed the test","comment_id":"886347","upvote_count":"2"},{"content":"Selected Answer: B\nCorrect answer is B. Using PutRecord/PutRecords would send the data synchronously to Kinesis Data Streams. Kinesis Data Analytics can be detect anomalies and the data can be pushed to Kinesis Data Firehose with transformation to Elasticsearch for analysis.\nOption A is wrong as Kinesis data streams does not provide data transformation feature.\n\nOption C is wrong as copying data to S3 and loading to Redshift would not make it near-real time.\n\nOption D is wrong as using Glue is ideal for batch jobs and not for near-real time analytics.","poster":"cloudlearnerhere","comment_id":"712244","timestamp":"1667731560.0","upvote_count":"7"},{"poster":"dushmantha","timestamp":"1659433620.0","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1659433680.0","comment_id":"641198","poster":"dushmantha","content":"D can be eleminated coz, KCL can't read from SDK producer"}],"content":"Selected Answer: B\nLow latency retrival can be achieved with DynamoDB, Redis and OpenSearch. I guess that would be enough to select the answer.","comment_id":"641197"},{"timestamp":"1658451780.0","poster":"rocky48","upvote_count":"1","comment_id":"634920","content":"Selected Answer: B\nAnswer - B"},{"content":"Selected Answer: B\nnear realtime should be opensearch","comment_id":"614875","poster":"treeli","timestamp":"1654932180.0","upvote_count":"1"},{"comment_id":"605249","poster":"Bik000","timestamp":"1653205260.0","upvote_count":"1","content":"Selected Answer: B\nAnswer is B"},{"comment_id":"595298","poster":"jrheen","timestamp":"1651353240.0","content":"Answer - B","upvote_count":"1"},{"timestamp":"1646670300.0","content":"B - near-realtime analytics keyword, only ES can provide that from the set of options","poster":"rb39","upvote_count":"2","comment_id":"562763"},{"poster":"aws2019","content":"B it is","upvote_count":"1","comment_id":"482249","timestamp":"1637380320.0"},{"poster":"Donell","content":"Answer B","timestamp":"1636267200.0","upvote_count":"1","comment_id":"387114"},{"timestamp":"1635872400.0","comment_id":"349655","poster":"AjithkumarSL","upvote_count":"1","content":"Looks like Answer is B.. \n\nhttps://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html\n\nWhen Not to Use the KPL\nThe KPL can incur an additional processing delay of up to RecordMaxBufferedTime within the library (user-configurable). Larger values of RecordMaxBufferedTime results in higher packing efficiencies and better performance. Applications that cannot tolerate this additional delay may need to use the AWS SDK directly."},{"content":"B is the answer","comment_id":"316564","poster":"umatrilok","upvote_count":"1","timestamp":"1635552000.0"},{"poster":"lostsoul07","comment_id":"274336","content":"B is the right answer","upvote_count":"2","timestamp":"1635522900.0"},{"upvote_count":"1","content":"The only thing about B is why transform to CSV since putting to Elastic Search ?","timestamp":"1635223560.0","poster":"gtourkas","comment_id":"249879"},{"timestamp":"1634685240.0","poster":"freaky","content":"Answer B\nA and C dropped because RedShift is not meant for \"near-real-time\" analysis. Also, it would require some kind on Visualization on top of it to do the analysis. \nDropped D because KDS in itself cannot transform data.","upvote_count":"2","comment_id":"249457"},{"poster":"Katana19","comments":[{"content":"I think that asynchrony in the context of this exam means buffering, not time decoupling and it needs to go away since it might lead to false positives in anomaly detection. This is also the reason it recommends setting RecordMaxBufferedTime to 0/-1.","upvote_count":"1","comment_id":"249877","poster":"gtourkas","timestamp":"1634912340.0"}],"content":"The key word is ASYNCHRONOUSLY ! only KPL can handle asynchronous streaming !","upvote_count":"4","comment_id":"235233","timestamp":"1634663640.0"},{"comments":[{"comment_id":"218759","content":"i take that back. real time analysis sounds more like an Elastic search use case. I go with B","upvote_count":"1","poster":"Skdbc","timestamp":"1634525400.0"}],"content":"I am leaning towards C due to the fault tolerance capabilities of KPL. Issue is not with delay it is more with missing records.","upvote_count":"1","comment_id":"217489","timestamp":"1634507400.0","poster":"Skdbc"},{"timestamp":"1634192760.0","upvote_count":"1","content":"100% B.\nC & D doesn't make any sense. KDS cannot transform data, it is possible in Kinesis Firehose only. so final answer should be B","comment_id":"205085","poster":"sanjaym"},{"poster":"syu31svc","upvote_count":"2","comment_id":"194159","timestamp":"1633952220.0","content":"https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-sdk.html\nAnswer is B to me"},{"poster":"Paitan","timestamp":"1633831320.0","content":"Option B for me.","upvote_count":"7","comment_id":"175574"},{"content":"My answer is B","upvote_count":"6","poster":"zeronine","comment_id":"162242","timestamp":"1632384840.0","comments":[{"comment_id":"166659","timestamp":"1632406320.0","poster":"ali_baba_acs","content":"same for me","upvote_count":"2"}]},{"comment_id":"159883","poster":"zanhsieh","content":"C\nA dropped because RecordMaxBufferedTime cannot be set below 0. BD dropped because KPL already could achieve the requirement.\nhttps://github.com/awslabs/amazon-kinesis-producer/blob/943bc701038936ebe284041768740e284156f86c/java/amazon-kinesis-producer/src/main/java/com/amazonaws/services/kinesis/producer/KinesisProducerConfiguration.java#L1232","comments":[{"upvote_count":"2","content":"JSON in redshift ?","comment_id":"167096","poster":"ali_baba_acs","timestamp":"1632689400.0"}],"timestamp":"1632351660.0","upvote_count":"3"},{"upvote_count":"2","timestamp":"1632187740.0","content":"It should be between B and D, right? \n\nThe \"RecordMaxBufferedTime property\" the documentation only says that you should NOT set lower than 60s otherwise you will have problems. It doens mentions nothing about -1 or 0.","poster":"testtaker3434","comment_id":"153583"}],"question_text":"A company wants to improve user satisfaction for its smart home system by adding more features to its recommendation engine. Each sensor asynchronously pushes its nested JSON data into Amazon Kinesis Data Streams using the Kinesis Producer Library (KPL) in Java. Statistics from a set of failed sensors showed that, when a sensor is malfunctioning, its recorded data is not always sent to the cloud.\nThe company needs a solution that offers near-real-time analytics on the data from the most updated sensors.\nWhich solution enables the company to meet these requirements?","answer_images":[],"answer_description":"","question_id":111,"timestamp":"2020-08-09 14:16:00","answer":"B"},{"id":"5R6JcEhDnNq6ECNmOQMt","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/29208-exam-aws-certified-data-analytics-specialty-topic-1-question/","unix_timestamp":1597996380,"answer_ET":"A","isMC":true,"exam_id":20,"choices":{"B":"Use Amazon QuickSight with Amazon S3 as the data source. Use heat maps as the visual type.","D":"Use Amazon QuickSight with Amazon S3 as the data source. Use pivot tables as the visual type.","A":"Use Amazon QuickSight with Amazon Athena as the data source. Use heat maps as the visual type.","C":"Use Amazon QuickSight with Amazon Athena as the data source. Use pivot tables as the visual type."},"answers_community":["A (83%)","C (17%)"],"topic":"1","discussion":[{"upvote_count":"34","poster":"ramozo","comment_id":"162780","content":"It is A. QuickSight does not support S3 files with parquet format, Athena does it. For visualization is better a graph than a pivot table. \nhttps://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html\nhttps://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html","comments":[{"timestamp":"1633827660.0","content":"100% agree with A","comment_id":"180577","poster":"vicks316","upvote_count":"1"},{"comment_id":"186505","content":"Why not B. Why cant S3 be the datasource for Quicksight","comments":[{"content":"Sorry just noticed that you’ve mentioned Quicksight doesn’t support files in S3 in parquet format","upvote_count":"3","comment_id":"186574","timestamp":"1634111040.0","poster":"AjNapa"},{"upvote_count":"2","timestamp":"1634202300.0","comment_id":"191672","content":"I agree. It should be B\nhttps://aws.amazon.com/quicksight/\nhttps://d1.awsstatic.com/r2018/h/QuickSight%20Q/Provide%20interactive%20dashboards_QuickSight.7efb01bbe9a6b6592a821bccff04f463647afd82.png","poster":"sam202033"}],"timestamp":"1633889040.0","upvote_count":"2","poster":"AjNapa"},{"content":"You can use any of the following relational data stores as data sources for Amazon QuickSight:\n\nAmazon Athena\n\nAmazon Aurora\n\nAmazon Redshift\n\nAmazon Redshift Spectrum\n\nAmazon S3\n\nAmazon S3 Analytics","upvote_count":"2","comment_id":"191668","timestamp":"1634121900.0","comments":[{"upvote_count":"1","content":"Agree with you. QuickSight supports S3. the data is already on S3. I don't see need for Anthena in this scenario.","comments":[{"timestamp":"1635103500.0","poster":"Draco31","content":"No. https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\nParquet files not supported by Quicksight, must use Athena.","upvote_count":"9","comment_id":"238102"}],"timestamp":"1634481300.0","comment_id":"192571","poster":"AWS_Trial"}],"poster":"sam202033"},{"comment_id":"505888","upvote_count":"4","timestamp":"1640072280.0","content":"I am inclined towards C\nHeat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to further analyze data on the visual, for example by changing column sort order or applying aggregate functions across rows or columns.\nhttps://docs.aws.amazon.com/quicksight/latest/user/heat-map.html","poster":"lakediver"},{"content":"Heat maps are used to show correlation which is not the case here. With a pivot table you easily sort the column and see which one is higher/lower.","timestamp":"1632559140.0","comment_id":"164375","poster":"testtaker3434","upvote_count":"7","comments":[{"comment_id":"165998","content":"\"Visualization with least amount of effort\".\nhttps://docs.aws.amazon.com/quicksight/latest/user/heat-map.html","poster":"ramozo","upvote_count":"5","timestamp":"1632768480.0"},{"comments":[{"timestamp":"1700842020.0","comment_id":"1079421","poster":"dp_learner","upvote_count":"1","content":"which basically makes*"}],"comment_id":"1079420","upvote_count":"1","poster":"dp_learner","timestamp":"1700842020.0","content":"i would generally agree with you but as i read https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html. apparently in quicksight you can add a measure for the heat map with basically makes it a colorful pivot table from my understanding"}]},{"comment_id":"1120075","poster":"GCPereira","timestamp":"1704998700.0","upvote_count":"1","content":"full agreed"}],"timestamp":"1632443580.0"},{"upvote_count":"10","poster":"testtaker3434","timestamp":"1632551280.0","content":"Agree wih C. Thoughts?","comment_id":"163074","comments":[{"content":"Totally agree :-)","poster":"Paitan","comment_id":"175581","timestamp":"1633411800.0","upvote_count":"2"}]},{"timestamp":"1697345640.0","comment_id":"1043866","content":"A or C? For me both are valid\n- QuickSight does not support S3 files with parquet format. \n- AWS says here https://docs.aws.amazon.com/quicksight/latest/user/pivot-table.html \"Heat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to analyze data on the visual.\"","poster":"gofavad926","upvote_count":"1"},{"upvote_count":"1","timestamp":"1693218960.0","content":"Ans Is C . Athena as the data is in Parquet format and Pivot table as it was to identify sub-organization is the strongest performer in each country whereas heat map will provide only correlation","poster":"nroopa","comment_id":"992058"},{"timestamp":"1682949780.0","poster":"pk349","upvote_count":"2","content":"B: I passed the test","comment_id":"886352"},{"comment_id":"877097","timestamp":"1682146800.0","content":"Answer is A\nSince Parquet is mentioned then Athena.\nVisuals means heat map, Pivot table is just a table with Data","poster":"anjuvinayan","upvote_count":"1"},{"content":"Selected Answer: A\nThe correct answer is A. Here’s why:\n\nA is correct because Amazon QuickSight can be used with Amazon Athena as the data source to visualize sales data stored in Amazon S3 in Parquet format. Heat maps can be used as the visual type to quickly identify which sub-organization is the strongest performer in each country. Heat maps are a great way to visualize data that is organized in a grid format, such as sales data.\n\nC is incorrect because pivot tables are not the best visual type to use for this scenario. Pivot tables are great for summarizing and analyzing large amounts of data, but they are not the best way to visualize data.","upvote_count":"2","comment_id":"843773","timestamp":"1679232480.0","poster":"akashm99101001com"},{"poster":"cloudlearnerhere","upvote_count":"5","content":"Selected Answer: A\nCorrect answer is A as Quicksight can use Athena as data source to query the data in S3. Heat Maps would provide the required visual representation.\n\nHeat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers, because the use of color makes these easier to spot. Use a pivot table if you want to analyze data on the visual.\n\nOption C is wrong as Heat Map would provide the required visual and would be preferred over Pivot tables as there is not need to analyze the data.\n\nOptions B & D are wrong as Quicksight supports S3 as a data source, however it does not work directly with parquet file format.","comment_id":"712245","timestamp":"1667731920.0"},{"timestamp":"1664091120.0","upvote_count":"1","comment_id":"678531","poster":"Arka_01","content":"Selected Answer: A\nQuickly identification is possible by HeatMap color coding feature. Pivot table is unnecessary here. Athena is required as data is stored in Parquet format."},{"poster":"he11ow0rId","timestamp":"1663979220.0","comment_id":"677528","upvote_count":"1","content":"Selected Answer: C\nc, heat maps are for correlation ("},{"upvote_count":"1","content":"Selected Answer: C\nheat maps are for correlation (P.S.: heat maps do not use a world map) -> Pivot tables is correct here.\n\nquick-sight cannot directly query parquet files.","poster":"redwan123","comment_id":"651297","timestamp":"1661349600.0"},{"timestamp":"1658805600.0","upvote_count":"1","comment_id":"637094","content":"Selected Answer: A\nSelected Answer: A","poster":"rocky48"},{"comment_id":"622232","upvote_count":"1","poster":"GiveMeEz","timestamp":"1656182520.0","content":"Ans is B.\nHeat map allows one simple chart to show the pattern for the whole thing easily.\n\nQuickSight can use S3 as data source.\nCreating a dataset using Amazon S3 files - Amazon QuickSight\nhttps://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-s3.html"},{"comment_id":"608222","upvote_count":"3","content":"Ans is C. Heat map is to show co-relation and identify outliers. Here the requirement is very straightforward which is to identify best performer in each region. Hence Pivot table.","timestamp":"1653702420.0","poster":"certificationJunkie"},{"comment_id":"605231","timestamp":"1653204540.0","content":"Selected Answer: A\nMy Answer is A","upvote_count":"1","poster":"Bik000"},{"poster":"Engy2020","content":"swiftly determine which sub-organization is the best performance in each country\n\nC: is the answer , because (in each country)you should use pivot table","timestamp":"1647499440.0","upvote_count":"3","comment_id":"569486"},{"timestamp":"1645808100.0","content":"It's C: Question says you need to \"swiftly\" determine best performers. I can't imagine doing that with a heatmap as some of the squares may have similar color. Question doesn't even say how many sub-organizations there are? 50 ? 100? Doing a segment sorted pivot table will give you a swift read of highest performer per country.","comment_id":"556106","upvote_count":"3","poster":"wolfsong"},{"comment_id":"510178","content":"A\nA and C are quite confused, recommend to use a QuickSight with \"People Overview analysis\" dataset and try to draw heat map vs pivot table. In the pivot table, you might need to sort the \"Sales\" column, so heat map chart takes less amount off effort","upvote_count":"1","timestamp":"1640596380.0","poster":"npt"},{"timestamp":"1638597840.0","content":"I think it should be C\nQS doesn't support S3 files with parquet, only Athena. which eliminates B and D\nHeatmaps are not relevant because they show a measure for the intersection of two dimensions, with color-coding to easily differentiate where values fall in the range , which eliminates A\nHence C","upvote_count":"2","comment_id":"493521","poster":"lakeswimmer"},{"poster":"aws2019","content":"A is the right answer","comment_id":"482938","timestamp":"1637453820.0","upvote_count":"1"},{"upvote_count":"2","content":"Its A, Quicksight supports only below formats not parquet\nhttps://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\nYou can use files in Amazon S3 or on your local (on-premises) network as data sources. QuickSight supports files in the following formats:\nCSV and TSV – Comma-delimited and tab-delimited text files\nELF and CLF – Extended and common log format files\nJSON – Flat or semistructured data files\nXLSX – Microsoft Excel files","comment_id":"393114","timestamp":"1636192680.0","poster":"pkad44"},{"poster":"jueueuergen","comment_id":"392653","upvote_count":"1","content":"I recommend that anyone who is unsure whether it's A or C actually uses QuickSight for just 2 minutes to try it themselves (e.g. using the People Overview default dataset with Dimensions: Business Function versus Region; Metric: Tenure).\nThe answer is absolutely clear and the majority here is wrong.","timestamp":"1636162020.0"},{"upvote_count":"1","poster":"Donell","timestamp":"1636124700.0","content":"Answer is A.\nQuicksight does not support parquet format directly.\nQuickSight supports files in the following formats:\nCSV and TSV – Comma-delimited and tab-delimited text files\nELF and CLF – Extended and common log format files\nJSON – Flat or semistructured data files\nXLSX – Microsoft Excel files","comment_id":"387122"},{"upvote_count":"1","timestamp":"1635922380.0","content":"A should be the answer","poster":"gunjan4392","comment_id":"386696"},{"poster":"umatrilok","timestamp":"1635812280.0","content":"Answer is A, As QuickSight doesn't read parquet formatted file and Athena does. Heat maps are used for Anomaly identification and trend/pattern identification.","upvote_count":"1","comment_id":"316571"},{"upvote_count":"3","timestamp":"1635588960.0","comment_id":"297494","content":"A\nQuicksight can not read parquet from S3 so eliminate B and D as per the link\nhttps://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html \n\nheat map is the correct visual to use as per the below \n\"For example, the following heat map shows which products are most used by the customers in these countries, measured by a simple count\" as i think is like our situation \nhttps://docs.aws.amazon.com/quicksight/latest/user/heat-map.html","poster":"sayed"},{"content":"A is the right answer","upvote_count":"1","timestamp":"1635552600.0","comment_id":"274338","poster":"lostsoul07"},{"content":"A. As heat map used for correlation .","upvote_count":"1","timestamp":"1635409980.0","comment_id":"267234","poster":"SA_206"},{"timestamp":"1635316140.0","content":"C. S3 cannot connect to Parquet files. Athena is needed. Heat Map is not the right visualization choice for such metrics. Pivot table can easily show information sr leadership is looking for ie, performance for each sub-division and roll up at multiple levels.","upvote_count":"3","poster":"saabji","comment_id":"252061"},{"poster":"hans1234","timestamp":"1635080760.0","comment_id":"237255","content":"I am not sure with A: If you have two very close competitors, you can not see a difference on the heatmap and you needd the numbers of a pivot table to make sure who is best performer.","upvote_count":"2"},{"comment_id":"230382","timestamp":"1634810760.0","content":"I think its heatmap since we can find with the darker color for the stronger performer rather than numbers\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/pivot-table.html","poster":"DhivakarSathya","upvote_count":"2"},{"upvote_count":"1","comment_id":"217493","content":"A parquet is not supported directly in manifest file (option B). So you have to go through Athena. Visual indicator is important hence use heat map.","poster":"Skdbc","timestamp":"1634543580.0"},{"comment_id":"205090","poster":"sanjaym","timestamp":"1634518020.0","content":"100% A. \"least amount of effort\" forces to go for heat maps. Quicksight can read directly from S3 but not Parquet format. so answer is A.\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\nYou can use files in Amazon S3 or on your local (on-premises) network as data sources. QuickSight supports files in the following formats:\n\nCSV and TSV – Comma-delimited and tab-delimited text files\n\nELF and CLF – Extended and common log format files\n\nJSON – Flat or semistructured data files\n\nXLSX – Microsoft Excel files","upvote_count":"1"},{"poster":"syu31svc","content":"I think B would suffice as the question states least amount of effort","comments":[{"upvote_count":"1","comment_id":"193334","timestamp":"1634504160.0","content":"Sorry is A\nYou can use files in Amazon S3 or on your local network as data sources for Amazon\nQuickSight. Amazon QuickSight supports files in the following formats:\nCSV/TSV – Delimited text files\nELF/CLF – Extended and common log format files\nJSON – Flat or semistructured data files\nXLSX – Microsoft Excel files","poster":"syu31svc"}],"upvote_count":"1","comment_id":"191886","timestamp":"1634316660.0"},{"poster":"GauravM17","comment_id":"176963","timestamp":"1633758120.0","upvote_count":"3","content":"I am changing my stance. In this Organization and Country to 2 dimensios and we need to evaluate the performance. This is case where 2 dimensions with 1 measure and hence Heat map should be the way to go. A is something I would go with."},{"content":"Definitely option C.","comment_id":"175576","poster":"Paitan","upvote_count":"1","timestamp":"1633090020.0","comments":[{"timestamp":"1633814640.0","comment_id":"177419","upvote_count":"2","content":"On second thoughts if they just want to have a quick glance at the top performer per country, Heat Map will be just fine. With pivot table they may have to sort the data and also its a tabular interpretation of data and not really a visual graph.","poster":"Paitan"}]},{"comment_id":"174207","poster":"GauravM17","content":"It should be C?","upvote_count":"1","timestamp":"1632928800.0"},{"poster":"awssp12345","comment_id":"168720","comments":[{"poster":"DerekKey","comment_id":"350775","content":"Dimension 1 - country\nDimension 2 - sub-organization\nValue - color","comments":[{"poster":"c_shweng","timestamp":"1650340440.0","comment_id":"587969","comments":[{"content":"Agreed !!!! Answer = C","upvote_count":"1","poster":"CHRIS12722222","timestamp":"1650536520.0","comment_id":"589255"}],"content":"Value - color -> How about \"each of which offers its goods and services?\" You need to aggregate columns to get the \"Value,\" so I think the answer should be C.","upvote_count":"1"}],"timestamp":"1635863040.0","upvote_count":"1"}],"upvote_count":"1","content":"Since a heatmap measures for the intersection of two dimensions, can someone please explain me how that will applicable in this example?\nI think the answer is C.","timestamp":"1632825900.0"},{"content":"It is A. For correlation you use Scatter Plots. For distribution of one variable you use Heat Maps. \"..Use heat maps to show a measure for the intersection of two dimensions, with color-coding to easily differentiate where values fall in the range..\"\nhttps://docs.aws.amazon.com/quicksight/latest/user/heat-map.html","timestamp":"1632630960.0","upvote_count":"5","poster":"ramozo","comment_id":"165996"}],"question_text":"A global company has different sub-organizations, and each sub-organization sells its products and services in various countries. The company's senior leadership wants to quickly identify which sub-organization is the strongest performer in each country. All sales data is stored in Amazon S3 in Parquet format.\nWhich approach can provide the visuals that senior leadership requested with the least amount of effort?","answer_images":[],"answer_description":"","question_id":112,"answer":"A","timestamp":"2020-08-21 09:53:00"},{"id":"2Fm9llDCOafpfkJtQZMz","question_images":[],"isMC":true,"discussion":[{"upvote_count":"12","poster":"rb39","content":"Selected Answer: B\nOpenSearch to scan all text","timestamp":"1650627060.0","comment_id":"589936"},{"content":"Answer is C\nStore the metadata in Redshift. Metadata is extracted using company provided ML program.","upvote_count":"1","timestamp":"1701216060.0","comment_id":"1083025","poster":"skb0071"},{"upvote_count":"1","comment_id":"907802","timestamp":"1685168460.0","content":"B is the correct answer. Keywords to look for in question - \"Performance\" and \"search using text\". D can be correct only if there is no text based search requirement.","poster":"Debi_mishra"},{"content":"B: I passed the test","comment_id":"886355","upvote_count":"1","timestamp":"1682949840.0","poster":"pk349"},{"comment_id":"843780","timestamp":"1679233260.0","upvote_count":"3","content":"Selected Answer: B\nOption A is incorrect because object tags are not searchable and cannot be used to query the data. S3 Select can be used to retrieve the files based on the applicant name and application date, but object tags cannot be used to store metadata.\nOption B is correct because Amazon OpenSearch Service (Amazon Elasticsearch Service) can be used to index the metadata and the Amazon S3 location of the image file. Data analysts can use OpenSearch Dashboards (Kibana) to submit queries to the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.\nOption C is incorrect because Amazon Redshift is not designed for storing large binary objects such as images. It is a data warehousing solution that is optimized for querying structured data.\nOption D is incorrect because Apache Parquet files are not optimized for querying unstructured data such as images. Amazon Athena can be used to submit custom queries, but it is not optimized for querying large binary objects.","poster":"akashm99101001com"},{"upvote_count":"3","timestamp":"1678154400.0","poster":"rags1482","content":"D is the right answer\n\nin Option B there is no direct method provided in this option to download the image file(s) associated with the search results.","comment_id":"831480"},{"comment_id":"712254","content":"Selected Answer: B\nCorrect answer is B as the metadata can be indexed with the S3 file location in ElasticSearch to provide a quick search and allow the users to download the file as well.\n\nhttps://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/\n\nOption A is wrong as using S3 Select would impact query performance.\n\nOption C is wrong as it would have a huge cost impact without improving query performance much.\n\nOption D is wrong as using Athena would impact query performance.","poster":"cloudlearnerhere","upvote_count":"4","timestamp":"1667733000.0"},{"poster":"JHJHJHJHJ","comment_id":"680002","timestamp":"1664212920.0","content":"Answer A: Validated using Jon bosco paid dumps","upvote_count":"2","comments":[{"poster":"JoellaLi","upvote_count":"1","timestamp":"1666180680.0","comment_id":"698985","content":"But why A?"}]},{"content":"Selected Answer: B\nCost control is secondary to query performance - This is the key here. Though D also can do the work, but it will be slower than option B.","upvote_count":"2","poster":"Arka_01","comment_id":"678533","timestamp":"1664091240.0"},{"comments":[{"content":"and 'Cost control is secondary to query performance.'","upvote_count":"2","timestamp":"1661384700.0","comment_id":"651508","poster":"Gavin_Y"}],"comment_id":"647399","poster":"rrshah83","upvote_count":"3","content":"Selected Answer: D\nParquet format improves performance. None of the other options talk about performance improvement.","timestamp":"1660608360.0"}],"question_id":113,"answer_images":[],"choices":{"D":"Store the metadata and the Amazon S3 location of the image files in an Apache Parquet file in Amazon S3, and define a table in the AWS Glue Data Catalog. Allow data analysts to use Amazon Athena to submit custom queries.","B":"Index the metadata and the Amazon S3 location of the image file in Amazon OpenSearch Service (Amazon Elasticsearch Service). Allow the data analysts to use OpenSearch Dashboards (Kibana) to submit queries to the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster.","C":"Store the metadata and the Amazon S3 location of the image file in an Amazon Redshift table. Allow the data analysts to run ad-hoc queries on the table.","A":"For each image, use object tags to add the metadata. Use Amazon S3 Select to retrieve the files based on the applicant name and application date."},"unix_timestamp":1650627060,"question_text":"A company has 1 million scanned documents stored as image files in Amazon S3. The documents contain typewritten application forms with information including the applicant first name, applicant last name, application date, application type, and application text. The company has developed a machine learning algorithm to extract the metadata values from the scanned documents. The company wants to allow internal data analysts to analyze and find applications using the applicant name, application date, or application text. The original images should also be downloadable. Cost control is secondary to query performance.\nWhich solution organizes the images and metadata to drive insights while meeting the requirements?","url":"https://www.examtopics.com/discussions/amazon/view/74121-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2022-04-22 13:31:00","topic":"1","answer_ET":"B","answer":"B","answer_description":"","answers_community":["B (88%)","13%"],"exam_id":20},{"id":"B3wLQ7UDtBzw3DpNLd9b","topic":"1","unix_timestamp":1597660500,"timestamp":"2020-08-17 12:35:00","question_images":[],"answer":"A","question_text":"A mobile gaming company wants to capture data from its gaming app and make the data available for analysis immediately. The data record size will be approximately 20 KB. The company is concerned about achieving optimal throughput from each device. Additionally, the company wants to develop a data stream processing application with dedicated throughput for each consumer.\nWhich solution would achieve this goal?","answer_images":[],"answers_community":["A (100%)"],"isMC":true,"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/28829-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"discussion":[{"upvote_count":"30","comment_id":"159889","poster":"zanhsieh","comments":[{"upvote_count":"2","comment_id":"193126","content":"A seems correct","timestamp":"1634885400.0","poster":"Mishra123"}],"content":"A\nDedicated throughput equals enhanced fan-out. So BCD dropped.\nhttps://docs.aws.amazon.com/streams/latest/dev/enhanced-consumers.html","timestamp":"1632954540.0"},{"timestamp":"1634045700.0","upvote_count":"6","content":"A: Developing Custom Consumers with Dedicated Throughput (Enhanced Fan-Out)","poster":"korcaptain","comment_id":"172280"},{"upvote_count":"2","comment_id":"886358","content":"A: I passed the test","poster":"pk349","timestamp":"1682949960.0"},{"timestamp":"1667734920.0","poster":"cloudlearnerhere","upvote_count":"4","comment_id":"712264","content":"Selected Answer: A\nCorrect answer is A as Kinesis Data Streams with Enhanced Fanout provides dedicated throughput for each consumer.\n\nIn Amazon Kinesis Data Streams, you can build consumers that use a feature called enhanced fan-out. This feature enables consumers to receive records from a stream with throughput of up to 2 MB of data per second per shard. This throughput is dedicated, which means that consumers that use enhanced fan-out don't have to contend with other consumers that are receiving data from the stream. Kinesis Data Streams pushes data records from the stream to consumers that use enhanced fan-out. Therefore, these consumers don't need to poll for data.\n\n\nOption B is wrong as there is no option to open support case to enable dedicated throughput.\n\n\nOption C is wrong as Kinesis Data Firehose does not support enhanced fan-out feature.\n\n\nD is incorrect. An Auto Scaling group of EC2 instances will not provide dedicated throughput for the consumers. You have to enable the enhanced fan-out feature in Amazon Kinesis Data Streams."},{"comment_id":"678535","poster":"Arka_01","upvote_count":"1","content":"Selected Answer: A\n\"dedicated throughput for each consumer\" - this is the key statement here.","timestamp":"1664091360.0"},{"poster":"rocky48","upvote_count":"1","comment_id":"638491","timestamp":"1658986560.0","content":"Selected Answer: A\nSelected Answer: A"},{"comment_id":"604845","upvote_count":"1","timestamp":"1653131760.0","content":"Selected Answer: A\nAnswer is A","poster":"Bik000"},{"timestamp":"1636215060.0","upvote_count":"4","poster":"Naresh_Dulam","comment_id":"283804","content":"Have the app call the PutRecords API to send data to Amazon Kinesis Data Streams. Use the enhanced fan-out feature while consuming the data. ==> PutRecords aggregate while sending data to Kinesis Data Streams to increase producer throghuput and Enhanced fan-out increase consumer through put\nB. Have the app call the PutRecordBatch API to send data to Amazon Kinesis Data Firehose. Submit a support case to enable dedicated throughput on the account. ==> Question is about stream processing using producer and consumer. \nC. Have the app use Amazon Kinesis Producer Library (KPL) to send data to Kinesis Data Firehose. Use the enhanced fan-out feature while consuming the data. ==> Enhanced fanout feature is not part of Firehose\nD. Have the app call the PutRecords API to send data to Amazon Kinesis Data Streams. Host the stream-processing application on Amazon EC2 with Auto Scaling. ==> We don't need EC@"},{"timestamp":"1636209240.0","comment_id":"274341","content":"A is the right answer","upvote_count":"1","poster":"lostsoul07"},{"upvote_count":"1","comment_id":"238248","poster":"Draco31","timestamp":"1635999780.0","content":"A.\nWhatever how you consume your data (EC2 in ASG or not), the stream must have Enhanced fan out."},{"poster":"sanjaym","timestamp":"1635452940.0","content":"A for sure.","upvote_count":"1","comment_id":"205199"},{"upvote_count":"1","poster":"sam202033","comment_id":"191663","timestamp":"1634825280.0","content":"Answer is A\n\nThere is no limit for the enhanced fan out . Check this link..\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\n\nQ: Is there a limit on the number of consumers using enhanced fan-out on a given stream?\n\nThere is a default limit of 20 consumers using enhanced fan-out per data stream. If you need more than 20, please submit a limit increase request though AWS support. Keep in mind that you can have more than 20 total consumers reading from a stream by having 20 consumers using enhanced fan-out and other consumers not using enhanced fan-out at the same time."},{"poster":"manish9363","comment_id":"188812","content":"How A is even possible ! enhanced fan out has limitation of 20 consumers.\n\nI will go with D","upvote_count":"1","timestamp":"1634728920.0"},{"comment_id":"181430","poster":"giocal","upvote_count":"1","comments":[{"timestamp":"1635939600.0","upvote_count":"3","content":"I think the gaming devices are producers not consumers in the given use case.","comment_id":"212759","poster":"jove"},{"upvote_count":"1","timestamp":"1666182120.0","comment_id":"699014","content":"\"A consumer is an application that processes all data from a Kinesis data stream. \"","poster":"JoellaLi"}],"content":"Enhanced fan out has a limit of 20 consumer per stream (not shard, stream). In a case of thousand or maybe millions devices I don't think is a valid solution.\nI'll probably go with D here.","timestamp":"1634523300.0"},{"content":"Option A is the right choice.","upvote_count":"2","comment_id":"175586","poster":"Paitan","timestamp":"1634275380.0"},{"poster":"Nicki1013","upvote_count":"1","timestamp":"1633851240.0","comment_id":"169365","content":"A is correct"}],"question_id":114,"answer_description":"","choices":{"C":"Have the app use Amazon Kinesis Producer Library (KPL) to send data to Kinesis Data Firehose. Use the enhanced fan-out feature while consuming the data.","B":"Have the app call the PutRecordBatch API to send data to Amazon Kinesis Data Firehose. Submit a support case to enable dedicated throughput on the account.","A":"Have the app call the PutRecords API to send data to Amazon Kinesis Data Streams. Use the enhanced fan-out feature while consuming the data.","D":"Have the app call the PutRecords API to send data to Amazon Kinesis Data Streams. Host the stream-processing application on Amazon EC2 with Auto Scaling."}},{"id":"xQZE1SZAFBzTEf2NDt1y","question_images":[],"answers_community":["B (100%)"],"timestamp":"2020-08-25 12:10:00","question_id":115,"answer_description":"","answer":"B","choices":{"C":"Store the last 24 months of data in Amazon S3 and query it using Amazon Redshift Spectrum. Configure Amazon QuickSight with Amazon Redshift Spectrum as the data source.","D":"Store the last 2 months of data in Amazon Redshift and the rest of the months in Amazon S3. Use a long-running Amazon EMR with Apache Spark cluster to query the data as needed. Configure Amazon QuickSight with Amazon EMR as the data source.","A":"Store the last 24 months of data in Amazon Redshift. Configure Amazon QuickSight with Amazon Redshift as the data source.","B":"Store the last 2 months of data in Amazon Redshift and the rest of the months in Amazon S3. Set up an external schema and table for Amazon Redshift Spectrum. Configure Amazon QuickSight with Amazon Redshift as the data source."},"answer_ET":"B","answer_images":[],"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/29549-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"timestamp":"1632288300.0","comment_id":"166437","poster":"ramozo","comments":[{"poster":"Katana19","comments":[{"poster":"Gavin_Y","comment_id":"651631","upvote_count":"2","content":"it's mentioned 'The sales team also wants to view the data as soon as it reaches the reporting backend', so I think there require perfomance","timestamp":"1661399160.0"}],"comment_id":"235254","upvote_count":"5","timestamp":"1633418520.0","content":"they didnt require performance, the required cost-effectiveness !! 2 months of data means 200TB... a redshift cluster of 200TB is not cheap !!!"}],"upvote_count":"41","content":"For me is B. Redshift offers better performance for querying and analyzing latest 2 months of data and in combination with Spectrum for non-frequent queries on 24 months of data."},{"content":"https://aws.amazon.com/premiumsupport/knowledge-center/redshift-spectrum-query-charges/\n\n\"Load the data in S3 and use Redshift Spectrum if the data is infrequently accessed.\"\nIn this case, the operations data is accessed hourly; this is not infrequent. I think even with the statement of cost effective, the answer is B. IF all the monthly data of 100TB is scanned hourly as part of those reports (and there's nothing in the question to say it isn't), then the cost becomes 100TB * $5 * ~720 hours in a month, which is $360,000 per month! The storage costs for 200TB of Redshift data is $5000 a month.","comment_id":"261825","upvote_count":"15","timestamp":"1634088000.0","poster":"kempstonjoystick"},{"timestamp":"1682950020.0","upvote_count":"1","content":"B: I passed the test","poster":"pk349","comment_id":"886360"},{"comment_id":"838975","content":"Selected Answer: B\nOption B seems to be the best solution for this scenario. It suggests storing the last 2 months of data in Amazon Redshift and the rest of the months in Amazon S3. Setting up an external schema and table for Amazon Redshift Spectrum will allow for querying data stored in Amazon S3. Additionally, configuring Amazon QuickSight with Amazon Redshift as the data source will allow for creating reports and dashboards for the data.\n\nThis solution is cost-effective because it uses Amazon S3 to store the majority of the data, which is cheaper than storing it all in Amazon Redshift. Also, it leverages Amazon Redshift Spectrum, which allows for querying data in Amazon S3 using a standard SQL interface without needing to move the data into Amazon Redshift. Finally, storing only two months of data in Amazon Redshift will minimize storage costs in Redshift while still allowing for fast query performance for the most recent data.","poster":"AwsNewPeople","timestamp":"1678807560.0","upvote_count":"1"},{"poster":"cloudlearnerhere","comment_id":"712379","timestamp":"1667745960.0","upvote_count":"4","content":"Selected Answer: B\nCorrect answer is B as the base requirements are cost and performance, keeping data in Redshift for 2 months would allow data analysis for current and previous month. Holding data for 24 months in S3 would provide a cost-effective option.\n\nOption A is wrong as holding 24 months data in Redshift is not cost-effective.\n\nOption C is wrong as storing 24 months of data in S3 would not provide performance.\n\nOption D is wrong as using a long-running EMR cluster is not cost-effective."},{"timestamp":"1664091480.0","upvote_count":"1","poster":"Arka_01","content":"Selected Answer: B\n\"as cost- effective as possible\" - this is the key statement here. So we need fast retrieval and query performance on last 2 months data, and infrequent querying capability for last 24 months of data. So B is the correct answer.","comment_id":"678536"},{"upvote_count":"1","comment_id":"641181","timestamp":"1659431220.0","content":"Selected Answer: B\nI would choose \"B\", although I had doubts to choose \"C\". The main reason for the switch is that, its not a very good use case of using Redshift Spectrum without using Redshift for any part of the job, I don't know if its possible. Ideally Redshift suppose to query hot data and Redshift Spectrum supposed to extend the querying capability to exabytes of data","poster":"dushmantha"},{"content":"Selected Answer: B\nB is the right answer.","poster":"rocky48","comment_id":"634918","timestamp":"1658451660.0","upvote_count":"1"},{"content":"Selected Answer: B\nB should be Correct","comment_id":"605251","poster":"Bik000","timestamp":"1653205260.0","upvote_count":"1"},{"content":"Answer - B","upvote_count":"1","poster":"jrheen","timestamp":"1651352820.0","comment_id":"595292"},{"content":"Agree B is the best and most cost effective option","upvote_count":"1","timestamp":"1646190720.0","comment_id":"559154","poster":"Blueocean"},{"comment_id":"534330","content":"B is the correct answer","upvote_count":"1","timestamp":"1643339640.0","poster":"GoKhe"},{"upvote_count":"1","poster":"lixin2402","timestamp":"1637527440.0","content":"Definitely, B is the right one. The cost was already being cut in half. EMR long-running instance is not cheap.","comment_id":"483654"},{"poster":"aws2019","timestamp":"1637504820.0","upvote_count":"1","content":"B is the right answer","comment_id":"483340"},{"poster":"goutes","timestamp":"1637007840.0","content":"RD Spectrum can query exabytes of unstructured data in S3 without loading. It even supports gzip and snappy. So option C is correct.","comment_id":"478975","upvote_count":"3"},{"content":"I think answer B is correct because of the hourly scanning costs.\nHowever, I think we don't have enough information:\n - what are the usage patterns? is only a small subset of columns required? -> less scanning\n - what compression factor is possible? Spectrum seems to support compressed data, whereas data in Redshift seems to be uncompressed\n[Compression factor] * [column selection] can easily decrease the amount of data that needs to be scanned by a factor of 100x, possibly even 1000x or more.\n\nFinally, the phrasing \"The sales team also wants to view the data as soon as it reaches the reporting backend.\" could go either way if you ask me - Spectrum doesn't introduce a lag because data is loaded lazily, however it leads to slower queries compared to Redshift.","comment_id":"389687","upvote_count":"3","poster":"jueueuergen","timestamp":"1635565560.0"},{"upvote_count":"1","comment_id":"389159","timestamp":"1635145320.0","poster":"Huy","content":"B. One more thing that make C wrong is Spectrum only runs within a Redshift Cluster. Therefore you are both charged by the cluster and the data scanned."},{"poster":"Donell","comment_id":"387151","upvote_count":"1","timestamp":"1634622300.0","content":"Answer B"},{"timestamp":"1634551620.0","comment_id":"316808","poster":"umatrilok","upvote_count":"1","content":"B is the answer"},{"upvote_count":"3","timestamp":"1634544720.0","content":"B is the right answer","comment_id":"274344","poster":"lostsoul07"},{"timestamp":"1633748640.0","content":"Configuring Quicksight with Spectrum as the data source is not possible of course. This eliminates C imo.","comments":[{"content":"Please ignore my statement. Spectrum is listed as a data source. I would go with C.","upvote_count":"2","timestamp":"1633960020.0","poster":"gtourkas","comment_id":"250438"}],"comment_id":"250434","poster":"gtourkas","upvote_count":"1"},{"comment_id":"238910","poster":"liyungho","upvote_count":"2","content":"Although the requirement is most cost-effective, I think they also consider performance because of this sentence -- \"The sales team also wants to view the data as soon as it reaches the reporting backend.\", so I will go for B.","timestamp":"1633577580.0","comments":[{"timestamp":"1634585700.0","upvote_count":"1","comment_id":"350824","content":"This eliminates EMR","poster":"DerekKey"}]},{"timestamp":"1632953340.0","poster":"sparun","upvote_count":"5","comment_id":"208275","content":"Answer is B"},{"timestamp":"1632895260.0","comment_id":"205228","poster":"sanjaym","upvote_count":"2","content":"It should be B"},{"comment_id":"191893","timestamp":"1632839640.0","upvote_count":"1","content":"I would take C as the answer since the question mentions cost-effective","poster":"syu31svc"},{"poster":"Paitan","upvote_count":"3","content":"The answer is B.","timestamp":"1632798420.0","comment_id":"175588"},{"poster":"GauravM17","content":"I believe it is B","timestamp":"1632747120.0","upvote_count":"2","comment_id":"174224"},{"comments":[{"poster":"ali_baba_acs","timestamp":"1632564720.0","content":"Answer can't be C, the answer is B. You can't managed this volume of data without performance degradation on your redshift Cluster.","upvote_count":"5","comments":[{"content":"\"The company is looking for a solution that is as cost- effective as possible.\"\nThat can only be C. Performance is not important. They dont write this without a reason. Otherwise they would only say \"cost effective\"","poster":"hans1234","upvote_count":"2","timestamp":"1633336140.0","comment_id":"227614"},{"comment_id":"238262","poster":"Draco31","upvote_count":"1","content":"Wrong.\nhttps://aws.amazon.com/fr/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/\nAs Spectrum is serverless and managed, it scaled automatically and it's stated in the previous link that performance can also be higher than local sotrage\nI'm going for C as it's clearly stated \"most cost effective\"; it can be more clear","comments":[{"poster":"kempstonjoystick","upvote_count":"3","content":"If you're looking at cost effective - Spectrum costs are $5 per TB scanned. In this case, the monthly data is 100TB, and there are hourly queries against it. If that's across the whole 100TB data, that's 100TB * $5 * ~720 hours a month, which is clearly not cost effective compared to standing up a 200TB redshift cluster at $5000 a month for storage.","timestamp":"1634189460.0","comment_id":"271128"}],"timestamp":"1633535040.0"}],"comment_id":"166450"}],"comment_id":"165885","timestamp":"1632272880.0","poster":"sparkKarthik","upvote_count":"1","content":"Answer C. Any thoughts?"}],"unix_timestamp":1598350200,"exam_id":20,"question_text":"A marketing company wants to improve its reporting and business intelligence capabilities. During the planning phase, the company interviewed the relevant stakeholders and discovered that:\n✑ The operations team reports are run hourly for the current month's data.\n✑ The sales team wants to use multiple Amazon QuickSight dashboards to show a rolling view of the last 30 days based on several categories. The sales team also wants to view the data as soon as it reaches the reporting backend.\n✑ The finance team's reports are run daily for last month's data and once a month for the last 24 months of data.\nCurrently, there is 400 TB of data in the system with an expected additional 100 TB added every month. The company is looking for a solution that is as cost- effective as possible.\nWhich solution meets the company's requirements?"}],"exam":{"numberOfQuestions":164,"name":"AWS Certified Data Analytics - Specialty","id":20,"isImplemented":true,"provider":"Amazon","isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true},"currentPage":23},"__N_SSP":true}