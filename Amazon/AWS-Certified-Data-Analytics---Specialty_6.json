{"pageProps":{"questions":[{"id":"rOaSDEa3OdLsSWCOReAc","exam_id":20,"question_id":26,"answer_ET":"C","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/64639-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"timestamp":"1633707420.0","upvote_count":"14","poster":"srinivasa","comments":[{"comment_id":"741826","poster":"nadavw","timestamp":"1670771880.0","content":"A is wrong as it's required to persist the data to S3, which can't be directly from KDS","upvote_count":"2","comments":[{"poster":"nadavw","upvote_count":"1","comment_id":"778946","timestamp":"1673961300.0","content":"The native integration of Amazon Connect and Amazon Kinesis to upload the Contact Trace Records (CTR) to Amazon Simple Storage Service (Amazon S3) bucket in near-real-time\n\nhttps://aws.amazon.com/blogs/contact-center/analyzing-amazon-connect-usage-with-agent-desktop-and-streaming-data/#:~:text=native%20integration%20of%20Amazon%20Connect%20and%20Amazon%20Kinesis%20to%20upload%20the%20Contact%20Trace%20Records%20(CTR)%20to%20Amazon%20Simple%20Storage%20Service%20(Amazon%20S3)%20bucket%20in%20near%2Dreal%2Dtime"}]}],"content":"Answer: C\nhttps://aws.amazon.com/blogs/apn/building-secure-and-private-data-flows-between-aws-and-salesforce-using-amazon-appflow/;https://aws.amazon.com/connect/faqs/","comment_id":"467214"},{"timestamp":"1640491920.0","content":"Both A and C are right\nAmazon Connect can stream metrics and agent event data to Amazon Kinesis Data Stream or Amazon Kinesis Data Firehose\nhttps://aws.amazon.com/connect/faqs/\nBut the question asks for the least operational overhead, so Firehose is more suitable for the data pipeline as its further sources and sinks","upvote_count":"7","poster":"npt","comment_id":"509412"},{"poster":"LocalHero","upvote_count":"1","timestamp":"1699275660.0","comment_id":"1063853","content":"Amazon Connect has data streaming feature.\nso Amazon Connect can stream Firehose and Streams.\nWhich is more unmanaged tool?\nIt is Firehose.\nC is correct."},{"poster":"pk349","comment_id":"886611","timestamp":"1682966340.0","content":"C: I passed the test","upvote_count":"3"},{"timestamp":"1679238300.0","upvote_count":"3","poster":"AwsNewPeople","content":"Selected Answer: C\nThe most efficient way to collect data in the data lake with the least operational overhead is option C: Use Amazon Kinesis Data Firehose to ingest Amazon Connect data and Amazon AppFlow to ingest Salesforce data.\n\nAmazon Kinesis Data Firehose is a fully managed service that can reliably capture and load streaming data into Amazon S3 with no operational overhead. It can also transform the incoming data before storing it in S3.\n\nAmazon AppFlow is a fully managed integration service that can securely transfer data between different SaaS applications, including Salesforce and S3. It supports incremental data transfers and can also transform the data during the transfer process.\n\nUsing Kinesis Data Firehose for Amazon Connect data and AppFlow for Salesforce data provides a simple and scalable solution that requires minimal operational overhead, allowing the data engineering team to focus on other tasks.","comment_id":"843852"},{"upvote_count":"2","content":"Selected Answer: C\nCorrect answer is C as the most efficient way is to use Amazon Connect integration with Kinesis Data Firehose to stream data directly to S3 and AppFlow to ingest data from Salesforce data to S3.\n\nOption A is also correct, although it's valid, it is not the most efficient solution.","poster":"rocky48","timestamp":"1671057360.0","comment_id":"745508"},{"content":"Correct answer is C as the most efficient way is to use Amazon Connect integration with Kinesis Data Firehose to stream data directly to S3 and AppFlow to ingest data from Salesforce data to S3. \n\nOption A is wrong as although it's valid, it is not the most efficient solution.\n\nOptions B & D are wrong as Kinesis does not integrate directly with Salesforce and AppFlow is a better solution. \n\nhttps://aws.amazon.com/blogs/apn/building-secure-and-private-data-flows-between-aws-and-salesforce-using-amazon-appflow/\n\nhttps://aws.amazon.com/connect/faqs/","comment_id":"705869","timestamp":"1666898580.0","poster":"cloudlearnerhere","upvote_count":"2"},{"poster":"Anudeep96","content":"Answer: C \nBoth A and C are correct but C provides a reduced operational overhead as it is fully managed","upvote_count":"2","comments":[{"upvote_count":"1","content":"This is correct answer.","comment_id":"1063854","timestamp":"1699275720.0","poster":"LocalHero"}],"comment_id":"688179","timestamp":"1665103020.0"},{"upvote_count":"1","poster":"he11ow0rId","comment_id":"669446","content":"Selected Answer: C\nc to use KDF","timestamp":"1663212420.0"},{"poster":"Hruday","comment_id":"649883","upvote_count":"2","timestamp":"1661106180.0","content":"Selected Answer: C\nSounds like C for s3 destinations"},{"timestamp":"1660763940.0","comment_id":"648195","upvote_count":"2","content":"C, since the kinesis data stream can not write to S3 directly.","poster":"Kodoma"},{"timestamp":"1659817920.0","comments":[{"upvote_count":"1","timestamp":"1671057240.0","poster":"rocky48","content":"Ah nevermind, One is Firehose, the other is Data Streams","comment_id":"745506"}],"poster":"rocky48","comment_id":"643520","upvote_count":"1","content":"Selected Answer: A\nBoth A and C are the same option. \nAdmins, please look into the missing answer option."},{"content":"Selected Answer: A\nA is the answer https://aws.amazon.com/quickstart/connect/data-streaming/","upvote_count":"1","timestamp":"1656577320.0","comments":[{"comment_id":"741828","timestamp":"1670772000.0","upvote_count":"2","poster":"nadavw","content":"does KDS persist to S3 directly?"}],"comment_id":"625121","poster":"ritears41"},{"poster":"GiveMeEz","comment_id":"621992","timestamp":"1656138660.0","content":"A.\nAmazon Connect feeds to Kinesis Data Stream. The other 3 answer is incorrectly using Firehose.\nhttps://aws.amazon.com/quickstart/connect/data-streaming/","upvote_count":"2"},{"upvote_count":"1","content":"C is the Answer.","comment_id":"491025","poster":"Thiya","timestamp":"1638310500.0"}],"unix_timestamp":1635130680,"topic":"1","answer_images":[],"choices":{"C":"Use Amazon Kinesis Data Firehose to ingest Amazon Connect data and Amazon AppFlow to ingest Salesforce data.","D":"Use Amazon AppFlow to ingest Amazon Connect data and Amazon Kinesis Data Firehose to ingest Salesforce data.","A":"Use Amazon Kinesis Data Streams to ingest Amazon Connect data and Amazon AppFlow to ingest Salesforce data.","B":"Use Amazon Kinesis Data Firehose to ingest Amazon Connect data and Amazon Kinesis Data Streams to ingest Salesforce data."},"isMC":true,"answers_community":["C (80%)","A (20%)"],"answer":"C","question_text":"A manufacturing company uses Amazon Connect to manage its contact center and Salesforce to manage its customer relationship management (CRM) data. The data engineering team must build a pipeline to ingest data from the contact center and CRM system into a data lake that is built on Amazon S3.\nWhat is the MOST efficient way to collect data in the data lake with the LEAST operational overhead?","timestamp":"2021-10-25 04:58:00","question_images":[]},{"id":"gooVY4potfBSjf5QPDYt","question_text":"A manufacturing company wants to create an operational analytics dashboard to visualize metrics from equipment in near-real time. The company uses Amazon\nKinesis Data Streams to stream the data to other applications. The dashboard must automatically refresh every 5 seconds. A data analytics specialist must design a solution that requires the least possible implementation effort.\nWhich solution meets these requirements?","timestamp":"2021-10-25 04:59:00","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/64640-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"C","choices":{"B":"Use Apache Spark Streaming on Amazon EMR to read the data in near-real time. Develop a custom application for the dashboard by using D3.js.","A":"Use Amazon Kinesis Data Firehose to store the data in Amazon S3. Use Amazon QuickSight to build the dashboard.","C":"Use Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Visualize the data by using an OpenSearch Dashboards (Kibana).","D":"Use AWS Glue streaming ETL to store the data in Amazon S3. Use Amazon QuickSight to build the dashboard."},"unix_timestamp":1635130740,"discussion":[{"poster":"Thiya","timestamp":"1638025920.0","upvote_count":"18","content":"Whenever the requirement states it is Operational Analytics, I consider using ES. Also, here QuickSight cannot refresh data for every 5 secs automatically, which in case of ELK is possible. So, Answer is C.","comment_id":"488233"},{"timestamp":"1704277860.0","comment_id":"1112665","content":"Selected Answer: D\nA,C is wrong because \"You can configure the values for OpenSearch buffer size (1 MB to 100 MB) or buffer interval (60 to 900 seconds)\", the requirement is \"5 seconds\" to refresh data.\nhttps://aws.amazon.com/tw/kinesis/data-firehose/faqs/\nB is wrong since EMR requires more effort.\nhence D.","upvote_count":"1","poster":"zzhangi0520"},{"comment_id":"1112663","upvote_count":"1","content":"A,C is wrong because \"You can configure the values for OpenSearch buffer size (1 MB to 100 MB) or buffer interval (60 to 900 seconds)\", the requirement is \"5 seconds\" to refresh data.\nhttps://aws.amazon.com/tw/kinesis/data-firehose/faqs/\nB is wrong since EMR requires more effort.\nhence D.","poster":"zzhangi0520","timestamp":"1704277800.0"},{"timestamp":"1682966400.0","content":"C: I passed the test","poster":"pk349","comment_id":"886612","upvote_count":"2"},{"content":"Selected Answer: C\nThe solution that meets the requirements and requires the least possible implementation effort is option C: Use Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Visualize the data by using an OpenSearch Dashboards (Kibana).\n\nAmazon Kinesis Data Firehose can be used to stream data to an Amazon Elasticsearch Service cluster with minimal configuration. Amazon OpenSearch Service is an open-source, distributed search and analytics engine that is fully compatible with Elasticsearch. OpenSearch Dashboards (Kibana) can be used to visualize and explore the data in near-real time, with automatic refreshing every 5 seconds.\n\nThis solution eliminates the need for a custom application or custom coding, and it requires minimal configuration and maintenance effort. It also provides a scalable, fully managed, and cost-effective way to store, analyze, and visualize streaming data.","poster":"AwsNewPeople","comment_id":"843856","timestamp":"1679238600.0","upvote_count":"1"},{"upvote_count":"4","content":"What is the need of refreshing the dashboard for every 5seconds if Firehose can deliver for every 60 seconds? Go with D(don't use quicksight spice, query the data directly from quicksight) or go with B(but more dev effort)","poster":"gopi_data_guy","timestamp":"1674908700.0","comment_id":"790585"},{"upvote_count":"1","content":"Selected Answer: C\nAnswer is C. Simple to deploy.","timestamp":"1658295480.0","poster":"rocky48","comment_id":"633864"},{"content":"Selected Answer: C\nIt is C. Simple to deploy = not a custom Java application. QuickSight can't be refreshed fast enough.","poster":"Ramshizzle","comment_id":"616599","upvote_count":"2","timestamp":"1655278380.0"},{"content":"Selected Answer: B\nAnswer B is correct","comment_id":"608244","timestamp":"1653708120.0","upvote_count":"1","poster":"Bik000"},{"content":"It should be C. There is no need to use JavaScript libraries to generate a dashboard as the end user is not public using a webpage but data analysts within the organization. Firehose, ElasticSearch and Kibana are able to handle this requirement very well. And there is no custom code development required either.","poster":"certificationJunkie","timestamp":"1653120420.0","comment_id":"604753","upvote_count":"2"},{"poster":"MWL","timestamp":"1651733280.0","comment_id":"597163","upvote_count":"2","content":"Selected Answer: C\nIn the requirement, it want to refresh the dashboard every 5 seconds, but not need to process data in 5 seconds."},{"comment_id":"593544","poster":"MWL","timestamp":"1651123620.0","upvote_count":"1","content":"Selected Answer: B\nI vote for B.\nFor C, it use kinesis anadata firehose. But the data firehose has a buffer with a minimum 60 seconds or 1 MB. We are not sure about the message size and frequence, so the deliver interval maybe much more than 5 seconds(it is required in the question). So B will not match the requirement.\nFor B, the spark in a EMR can process data in realtime. It will develop application using d3.js for visualization. But we still need a storage for the realtime data, which is output of spark stream, and used by d3.js to get and display. Although C isn't clear enough and needs much efford, but it match the requirement.","comments":[{"upvote_count":"1","content":"Some typo in my comments.\nBut I want to change my vote. In the requirement, it want to refresh the dashboard every 5 seconds, but not need to process data in 5 seconds. \nSo I changed to C.","comment_id":"597162","timestamp":"1651733220.0","poster":"MWL"}]},{"comment_id":"570986","timestamp":"1647684300.0","upvote_count":"1","content":"Selected Answer: C\nQuickSight can only be updated every 15 minutes. So the answer is C.","poster":"moon2351"},{"comment_id":"559130","content":"B and C are close. As simple to deploy rules out D3.js as it's not simple and quicksight is not meant for real-time (or near real-time) refresh! hence answer is C","poster":"PravinT","upvote_count":"4","timestamp":"1646185680.0"},{"comment_id":"557603","poster":"Agn3001","timestamp":"1645993800.0","upvote_count":"2","content":"Kinesis firehose can write to destination RES R - REdshift , E - elastic search , S - S3 . when it can write directly to elastic search why write to S3 and then connect to elastic search. So the answer is C directly write to elastic search and use kibana to refresh every 5 secs and reporting display."},{"comment_id":"557212","upvote_count":"2","content":"I think it's B because Quicksight can't refresh data every 5sec and Firehose has a minila batch size of 60sec so the only remaining option is B even if it requires custom code","poster":"simo40010","timestamp":"1645952760.0"},{"upvote_count":"4","content":"B and C are close.\nBut B is not simple to deploy.\nso C.","poster":"rav009","timestamp":"1643444580.0","comment_id":"535310"},{"comment_id":"480333","timestamp":"1637193300.0","content":"Option A & D Cannot be due to refresh rate \nhttps://docs.aws.amazon.com/quicksight/latest/user/refreshing-imported-data.html\nOption B - not due to custom application \nOnly left with Option C - Correct","upvote_count":"4","poster":"ali98"},{"poster":"aws2019","timestamp":"1637182560.0","comments":[{"comment_id":"485434","content":"QuickSight Can't refresh every 5 sec.","poster":"ali98","upvote_count":"2","timestamp":"1637703480.0"}],"upvote_count":"2","content":"why not D ?\n\nquestion is asking for most \"feasible solution\" -> less operational overhead.\n\nGlue now supports spark streaming ETL -> s3 -> quicksight.\n\nAssuming KDF has 60 sec buffer time this is the best managed option.","comment_id":"480267"},{"comments":[{"timestamp":"1637399100.0","content":"Only problem with B is custom application.","upvote_count":"1","poster":"ali98","comment_id":"482364"}],"comment_id":"478779","timestamp":"1636990800.0","poster":"Fazil_Cp","upvote_count":"2","content":"I will vote for B. That's the only option which provides the refresh in 5 secs latency and they have also mentioned kinesis data streams is used for other applications, so why cant we consume using spark streaming from kinesis ?. The min buffer interval for firehose to open search is above 5 secs. please share your thoughts."},{"timestamp":"1636940580.0","comment_id":"478415","content":"I think it is C because kibana lets you choose the refresh frequency to 5 sec.","upvote_count":"3","poster":"goutes"},{"poster":"goutes","upvote_count":"2","comment_id":"478414","timestamp":"1636940040.0","content":"is answer A or C ?"},{"comment_id":"478333","poster":"doubeguy","content":"Answer: C\nhttps://docs.aws.amazon.com/quicksight/latest/user/refreshing-imported-data.html\nRefreshing a Dataset on a Schedule\nFor Standard or Enterprise editions, you can choose Daily, Weekly, or Monthly.\nFor Enterprise edition only, you can choose Hourly.","timestamp":"1636922340.0","upvote_count":"3"},{"comment_id":"475234","upvote_count":"1","poster":"Chints01","content":"Answer should be A.\nhttps://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/build-operational-analytics-pipeline-on-AWS-modern-data-architecture.pdf?did=wp_card&trk=wp_card","comments":[{"content":"i correct myself, the answer should be C given quicksight cannot refresh at 5 second interval while for kibana, the default refresh interval is 5 seconds","upvote_count":"1","timestamp":"1637873340.0","comment_id":"486929","poster":"Chints01"},{"comment_id":"479088","poster":"DMK2021","upvote_count":"1","content":"Your example proves C not A. As noted above QuickSight can't work with S3 on auto refreshed schedule.","timestamp":"1637021520.0"}],"timestamp":"1636523040.0"},{"poster":"srinivasa","comments":[{"timestamp":"1637021640.0","comment_id":"479090","poster":"DMK2021","content":"Agree but this is Outdated question. ElasticService --> OpenSearch Service and Kebana --> OpenSearch Dashboards\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html","upvote_count":"3"}],"upvote_count":"2","comment_id":"467215","content":"Answer: C","timestamp":"1635129300.0"}],"question_id":27,"question_images":[],"isMC":true,"answers_community":["C (70%)","B (20%)","10%"],"answer_ET":"C","topic":"1","answer_description":"","exam_id":20},{"id":"rqZdmHPkLkk0XFmC8RY1","question_text":"A data analyst is designing an Amazon QuickSight dashboard using centralized sales data that resides in Amazon Redshift. The dashboard must be restricted so that a salesperson in Sydney, Australia, can see only the Australia view and that a salesperson in New York can see only United States (US) data.\nWhat should the data analyst do to ensure the appropriate data security is in place?","isMC":true,"question_images":[],"answer_description":"","topic":"1","choices":{"C":"Deploy QuickSight Enterprise edition to implement row-level security (RLS) to the sales table.","D":"Deploy QuickSight Enterprise edition and set up different VPC security groups for Australia and the US.","A":"Place the data sources for Australia and the US into separate SPICE capacity pools.","B":"Set up an Amazon Redshift VPC security group for Australia and the US."},"discussion":[{"upvote_count":"8","timestamp":"1637723580.0","poster":"mottyy","content":"Should be C, standard exmaple given in AWS website\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","comment_id":"485575"},{"timestamp":"1682966460.0","upvote_count":"3","comment_id":"886614","poster":"pk349","content":"C: I passed the test"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","timestamp":"1678626960.0","upvote_count":"1","poster":"rsn","comment_id":"837037"},{"content":"Selected Answer: C\nShould be C","comment_id":"759118","poster":"minhld","upvote_count":"2","timestamp":"1672179960.0"},{"poster":"cloudlearnerhere","content":"Correct answer is C as QuickSight Enterprise edition provides row-level security (RLS) which can be configured for the sales table to restrict access. \nTo do this, you create a query or file that has one column named UserName, GroupName, or both. Or you can create a query or file that has one column named UserARN, GroupARN, or both. You can think of this as adding a rule for that user or group. Then you can add one column to the query or file for each field that you want to grant or restrict access to. For each user or group name that you add, you add the values for each field. You can use NULL (no value) to mean all values.\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","upvote_count":"3","comment_id":"705873","timestamp":"1666899300.0"},{"content":"Selected Answer: C\nSelected Answer: C","poster":"rocky48","comment_id":"638471","upvote_count":"1","timestamp":"1658985120.0"},{"comment_id":"604842","upvote_count":"1","content":"Selected Answer: C\nMy Answer is C","poster":"Bik000","timestamp":"1653131640.0"},{"poster":"ali98","upvote_count":"2","content":"Answer : C","timestamp":"1637703300.0","comment_id":"485433"},{"content":"Answer is C \nIn the Enterprise edition of Amazon QuickSight, you can restrict access to a dataset by configuring row-level security (RLS) on it. You can do this before or after you have shared the dataset. When you share a dataset with RLS with dataset owners, they can still see all the data. When you share it with readers, however, they can only see the data restricted by the permission dataset rules. By adding row-level security, you can further control their access.\nhttps://docs.aws.amazon.com/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","timestamp":"1637048220.0","upvote_count":"4","comment_id":"479225","poster":"polooor"},{"timestamp":"1634318100.0","content":"Answer: C","upvote_count":"3","comment_id":"467216","poster":"srinivasa"}],"answer_ET":"C","unix_timestamp":1635130860,"timestamp":"2021-10-25 05:01:00","answer_images":[],"question_id":28,"url":"https://www.examtopics.com/discussions/amazon/view/64641-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"answer":"C","answers_community":["C (100%)"]},{"id":"ENm2plZqHkNgjvQAHusH","question_text":"A company wants to run analytics on its Elastic Load Balancing logs stored in Amazon S3. A data analyst needs to be able to query all data from a desired year, month, or day. The data analyst should also be able to query a subset of the columns. The company requires minimal operational overhead and the most cost- effective solution.\nWhich approach meets these requirements for optimizing and querying the log data?","topic":"1","isMC":true,"answer_ET":"D","answers_community":["D (100%)"],"exam_id":20,"answer_images":[],"question_id":29,"answer_description":"","timestamp":"2021-10-25 05:02:00","unix_timestamp":1635130920,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/64642-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"poster":"srinivasa","content":"Answer: D","timestamp":"1632315300.0","upvote_count":"20","comment_id":"467217"},{"upvote_count":"15","timestamp":"1638284580.0","poster":"Thiya","comment_id":"490777","content":"A - .csv format is not optimal\nB - long running EMR is not cost-effective and has operational over-head of cluster management\nC - Again EMR with running Redshift Cluster is not cost-effective\nSo, Answer is Option D - low-cost and no operational over-head. Data Scanning cost by Athena can be minimized by partition pruning and subset of columns from parquet file."},{"comment_id":"886615","poster":"pk349","content":"D: I passed the test","timestamp":"1682966520.0","upvote_count":"3"},{"poster":"CleverMonkey092","content":"answer is d","upvote_count":"1","timestamp":"1679821020.0","comment_id":"850846"},{"comment_id":"783842","timestamp":"1674349560.0","content":"Selected Answer: D\nAgree D","upvote_count":"2","poster":"Mirandaali"},{"timestamp":"1666899420.0","content":"Correct answer is D as the Glue job can be used to transform and partition the logs files. Athena can be used to query the day. Glue and Athena are cost-effective with low operational overhead. Parquet data format can help query on a subset of the columns \n\nOption A is wrong as the CSV format is not the optimal format for storing and querying data using Athena.\n\nOption B is wrong as the long-running Amazon EMR cluster would not be a cost-effective option.\n\nOption C is wrong as using EMR and Redshift would not be as cost-effective or reduce operational cost as compared to Athena and Glue. \n\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","poster":"cloudlearnerhere","comment_id":"705875","upvote_count":"3"},{"upvote_count":"1","poster":"rocky48","content":"Selected Answer: D\nSelected Answer: D","timestamp":"1658986740.0","comment_id":"638494"},{"timestamp":"1656065160.0","comment_id":"621557","content":"Selected Answer: D\nD as my other comment explains","upvote_count":"1","poster":"Ramshizzle"},{"comment_id":"621556","poster":"Ramshizzle","content":"Answer D is obvious. \nA: we don't want csv. we want ORC or Parquet\nB: Long-running EMR cluster is expensive. Storing the data in HDFS is expensive\nC: Can work. But Using Redshift Spectrum is only logical if we want to combine the data with other data in Redshift. \nD: This is optimal. Glue works well. Parquet with Y/M/D partitions is optimal. Athena to query the data is perfect!","upvote_count":"2","timestamp":"1656065100.0"},{"upvote_count":"3","comment_id":"489392","poster":"awsmani","timestamp":"1638129480.0","content":"Very tricky question, Option C has some operational over-head but D is AWS glue is server less, considering that I might go for D"}],"choices":{"C":"Launch a transient Amazon EMR cluster nightly to transform new log files into Apache ORC format and partition by year, month, and day. Use Amazon Redshift Spectrum to query the data.","D":"Use an AWS Glue job nightly to transform new log files into Apache Parquet format and partition by year, month, and day. Use AWS Glue crawlers to detect new partitions. Use Amazon Athena to query data.","B":"Launch a long-running Amazon EMR cluster that continuously transforms new log files from Amazon S3 into its Hadoop Distributed File System (HDFS) storage and partitions by year, month, and day. Use Apache Presto to query the optimized format.","A":"Use an AWS Glue job nightly to transform new log files into .csv format and partition by year, month, and day. Use AWS Glue crawlers to detect new partitions. Use Amazon Athena to query data."},"answer":"D"},{"id":"73ieAMsT2kfF6UQKEZhI","timestamp":"2021-10-25 05:11:00","isMC":true,"question_images":[],"answers_community":["C (80%)","D (20%)"],"answer_ET":"C","answer":"C","topic":"1","choices":{"A":"Store the most recent 4 months of data in the Amazon Redshift cluster. Use Amazon Redshift Spectrum to query data in the data lake. Use S3 lifecycle management rules to store data from the previous 12 months in Amazon S3 Glacier storage.","B":"Leverage DS2 nodes for the Amazon Redshift cluster. Migrate all data from Amazon S3 to Amazon Redshift. Decommission the data lake.","C":"Store the most recent 4 months of data in the Amazon Redshift cluster. Use Amazon Redshift Spectrum to query data in the data lake. Ensure the S3 Standard storage class is in use with objects in the data lake.","D":"Store the most recent 4 months of data in the Amazon Redshift cluster. Use Amazon Redshift federated queries to join cluster data with the data lake to reduce costs. Ensure the S3 Standard storage class is in use with objects in the data lake."},"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/64643-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":30,"question_text":"An education provider's learning management system (LMS) is hosted in a 100 TB data lake that is built on Amazon S3. The provider's LMS supports hundreds of schools. The provider wants to build an advanced analytics reporting platform using Amazon Redshift to handle complex queries with optimal performance.\nSystem users will query the most recent 4 months of data 95% of the time while 5% of the queries will leverage data from the previous 12 months.\nWhich solution meets these requirements in the MOST cost-effective way?","discussion":[{"poster":"srinivasa","comment_id":"467218","content":"Answer: C","timestamp":"1632877680.0","upvote_count":"16"},{"timestamp":"1666899660.0","content":"Correct answer is C as only the last 4 months of data is required for 95%, the data can be stored in the Redshift cluster. The data covering the 12 months can be moved to S3 and queried using Redshift Spectrum. A mix of S3 and Redshift would provide the most cost-effective option. \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\n\nOption A is wrong as Redshift Spectrum cannot be used to query S3 Glacier storage data.\n\nOption B is wrong as using Redshift for all the data is not cost-effective.\n\nOption D is wrong as although Redshift federated queries would work, however, for 5% it would be cost-effective to query S3 directly instead of joining data from cluster and S3.","poster":"cloudlearnerhere","upvote_count":"9","comment_id":"705880"},{"timestamp":"1708693200.0","comment_id":"1157130","poster":"chinmayj213","content":"Point A cannot be answer because 5% user/query will be frequently used and Glacier need 90 to 180 days period. Point D : Federated query make sense when multiple data source because it required lots of authentication and authorization process , But here we have only S3 , so we can go with Point C","upvote_count":"2"},{"comment_id":"1076288","poster":"roymunson","comments":[{"upvote_count":"1","poster":"roymunson","comment_id":"1076292","timestamp":"1700570580.0","content":"Ahh now I got it: They want tu use the glacier to store the data from the previous 12 months."}],"timestamp":"1700570400.0","content":"I don't get it:\n\nThe question says:\n\"System users will query the most recent 4 months of data 95% of the time while 5% of the queries will leverage data from the previous 12 months.\"\n\n95% using data from the previous 4 month\n5 % using data from the previous 12 month\n0 % using data that is older than 12 month. \n\nSo why not archive them?","upvote_count":"1"},{"timestamp":"1682966580.0","upvote_count":"1","poster":"pk349","comment_id":"886617","content":"C: I passed the test"},{"upvote_count":"4","comment_id":"808798","timestamp":"1676405640.0","content":"Option C suggests storing the most recent 4 months of data in the Amazon Redshift cluster and using Amazon Redshift Spectrum to query data in the data lake, while ensuring that the S3 Standard storage class is in use with objects in the data lake. While this approach could work, it may not be the most cost-effective way to meet the requirements, because storing data in the Amazon Redshift cluster can be more expensive than storing data in S3. Additionally, by storing all data in the data lake, you may be able to use other data analysis services to query the data, which can be more cost-effective than using Amazon Redshift. Therefore, Option A, which leverages Amazon Redshift Spectrum to query the data in the data lake and uses S3 lifecycle management rules to move data from the previous 12 months to Amazon S3 Glacier, is likely a more cost-effective solution.","poster":"Arjun777"},{"upvote_count":"1","comment_id":"637114","timestamp":"1658806680.0","poster":"rocky48","content":"Selected Answer: C\nSelected Answer: C"},{"upvote_count":"1","timestamp":"1657380060.0","comment_id":"629241","poster":"Alekx42","content":"Selected Answer: C\nC is the answer. If you have to join old data coming from S3 with new data coming from the Redshift cluster, you can do that. It is described here: https://catalog.us-east-1.prod.workshops.aws/workshops/e5548031-3004-49ad-89be-a13e8cd616f6/en-US/perform-analytics-on-your-data/join-and-query-data-with-redshift-spectrum"},{"upvote_count":"1","comment_id":"620066","content":"Not D. https://docs.aws.amazon.com/redshift/latest/dg/federated-limitations.html","poster":"GiveMeEz","timestamp":"1655855940.0"},{"upvote_count":"1","content":"Selected Answer: C\nAnswer C should be correct","timestamp":"1653710400.0","poster":"Bik000","comment_id":"608249"},{"poster":"certificationJunkie","timestamp":"1653191460.0","content":"C. There is no mention of joining old and new data. Hence no need for federated queries.","comments":[{"upvote_count":"1","poster":"certificationJunkie","comment_id":"608232","timestamp":"1653703800.0","content":"federated queries are for databases and not specific to s3. Here requirement is s3 so spectrum should work okay."}],"comment_id":"605124","upvote_count":"1"},{"comment_id":"602952","poster":"Shammy45","timestamp":"1652795880.0","upvote_count":"1","content":"Selected Answer: C\nIts C , textbook case for spectrum"},{"comment_id":"597962","upvote_count":"1","content":"Selected Answer: D\nI think D is correct.\nUsing federated queries to combine redshift and Spectrum (from S3) will be more cost effective.","timestamp":"1651891920.0","poster":"MWL"},{"upvote_count":"2","poster":"yogen","comments":[{"comment_id":"512155","poster":"yogen","content":"I correct myself for misreading data from last 12 months to be in glacier, answer is C","timestamp":"1640781660.0","upvote_count":"2"}],"comment_id":"509179","timestamp":"1640448600.0","content":"A - Data from past 12 months is not required so glacier is most cost effective, Redshift spectrum to be used for querying data 4 - 12 months older, and upto 4 months old data is to queried from Redshift DB"},{"comment_id":"495431","poster":"damaldon","upvote_count":"3","timestamp":"1638827100.0","content":"The question says \"efficiently handle complicated queries\", A. recommend S3 lifecycle, but I don´t think Glacier be efficient, even Expedited retrieval (1-5) minutes. I will go for C."},{"comment_id":"494658","upvote_count":"1","content":"ali98 on point. Answer is C","poster":"tobsam","timestamp":"1638737520.0"},{"poster":"awsmani","upvote_count":"4","timestamp":"1638130080.0","comment_id":"489398","content":"Isn't D? The 5% of the user should be able to query both. moving the 4 months data to redshift makes sense and at the same time for 12 months data you need to query Redshift + Data lake data so in that federated queries can help to do that.\n\nIn option C if they have provided Redshift Spectrum queries S3 as an external also then it make sense to choose C, But I do not read that way."},{"comment_id":"481044","poster":"Chints01","comments":[{"comment_id":"483890","poster":"ali98","content":"Link you provided - > Data access to S3 Glacier requires data retrieval in the range of minutes (if using expedited retrieval) and this can’t be matched with the ability to query data. + S3 Glacier Select allows you to query on data directly in S3 Glacier, but it only supports uncompressed CSV files.","upvote_count":"7","timestamp":"1637560800.0"},{"upvote_count":"1","content":"No, Redshift Spectrum cannot be used to query S3 Glacier storage data.\nhttps://repost.aws/questions/QUlNlRlmQSTLCd0th3P3mPSg/supported-s3-storage-classes-for-redshift-spectrum","poster":"liar_p","comment_id":"985710","timestamp":"1692526560.0"}],"upvote_count":"2","content":"answer is A. the ask from the question is to have a cost effective solution hence s3 storage class has to be glacier and not standard. In case someone has a confusion if redshift spectrum support storage classes other than standard, the answer is yes and below is the reference.\n\nhttps://aws.amazon.com/blogs/big-data/part-1-building-a-cost-efficient-petabyte-scale-lake-house-with-amazon-s3-lifecycle-rules-and-amazon-redshift-spectrum/","timestamp":"1637269740.0"},{"upvote_count":"4","poster":"ali98","timestamp":"1636861380.0","content":"Answer: C","comment_id":"477920"}],"exam_id":20,"unix_timestamp":1635131460}],"exam":{"id":20,"isBeta":false,"provider":"Amazon","name":"AWS Certified Data Analytics - Specialty","isImplemented":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":164,"isMCOnly":true},"currentPage":6},"__N_SSP":true}