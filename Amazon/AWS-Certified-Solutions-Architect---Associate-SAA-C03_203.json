{"pageProps":{"questions":[{"id":"URzcsxgm7uw35rBmbqD2","topic":"1","answer_description":"","exam_id":31,"discussion":[{"content":"Selected Answer: C\nThe correct Answer is C. Amazon SQS is a fully managed message queuing service that allows you to decouple and scale applications by buffering the incoming data. This would ensure that data is stored in a reliable, scalable queue until the web application is ready to process it.\nSQS provides high availability and fault tolerance, and guarantees that no data will be lost, as the messages remain in the queue until they are processed.\nThe web application can poll the SQS queue for new messages and process them at its own pace, preventing the application from being overwhelmed by large volumes of tracker data.\nOperational overhead is minimal because SQS is a fully managed service. The application only needs to poll the queue for messages, and there is no need to manage infrastructure.","upvote_count":"3","comment_id":"1298387","timestamp":"1729016100.0","poster":"Bwhizzy"},{"content":"Answer: C\nAmazon SQS provides a reliable and scalable way to decouple the components of the application. By using a queue, the incoming tracker data can be buffered, ensuring that if the web application is overwhelmed with data, the messages will still be retained in the queue until they can be processed.","comment_id":"1294869","poster":"hharbiordun85","timestamp":"1728422520.0","upvote_count":"3"}],"unix_timestamp":1728422520,"answer_ET":"C","isMC":true,"answer":"C","question_text":"A company uses GPS trackers to document the migration patterns of thousands of sea turtles. The trackers check every 5 minutes to see if a turtle has moved more than 100 yards (91.4 meters). If a turtle has moved, its tracker sends the new coordinates to a web application running on three Amazon EC2 instances that are in multiple Availability Zones in one AWS Region.\n\nRecently, the web application was overwhelmed while processing an unexpected volume of tracker data. Data was lost with no way to replay the events. A solutions architect must prevent this problem from happening again and needs a solution with the least operational overhead.\n\nWhat should the solutions architect do to meet these requirements?","question_images":[],"timestamp":"2024-10-08 23:22:00","answer_images":[],"choices":{"B":"Create an Amazon API Gateway endpoint to handle transmitted location coordinates. Use an AWS Lambda function to process each item concurrently.","C":"Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incoming data. Configure the application to poll for new messages for processing.","D":"Create an Amazon DynamoDB table to store transmitted location coordinates. Configure the application to query the table for new data for processing. Use TTL to remove data that has been processed.","A":"Create an Amazon S3 bucket to store the data. Configure the application to scan for new data in the bucket for processing."},"question_id":1011,"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/148885-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"fz6w7Gu8YsKzF8PNoJYu","topic":"1","discussion":[{"timestamp":"1729017420.0","poster":"Bwhizzy","upvote_count":"6","content":"Selected Answer: B\nThe Correct Answer is B.\nExplanation:\nVPC and Private Subnets: By placing the RDS cluster in private subnets, you ensure that the RDS cluster is not publicly accessible from the internet. This significantly improves security as the database is only accessible through secure channels, not directly from the public internet.\n\nAWS Site-to-Site VPN: Using a Site-to-Site VPN establishes a secure, encrypted connection between the on-premises office and the AWS environment. This provides secure access to the RDS cluster without exposing it to the internet, ensuring that the developers can only access the cluster when connected to the office network.\n\nCustomer Gateway: The customer gateway is configured in the company's office to handle the VPN connection, providing secure connectivity for the desktop client to the RDS cluster when the development team is in the office.","comment_id":"1298398"},{"timestamp":"1728509520.0","poster":"blehbleh","upvote_count":"6","content":"Selected Answer: B\nThis is B site to site von adds additional security. We are going for more secure.","comment_id":"1295292"},{"comment_id":"1293324","poster":"kbgsgsgs","content":"Selected Answer: C\nThe goal is to limit the team to only being in the office to be in the RDS cluster, so wouldn't checking IP ranges based on the office network rather than bringing up the internet be better suited to what you really need?","upvote_count":"3","timestamp":"1728100260.0","comments":[{"timestamp":"1728322500.0","content":"But if they are in private subnets, how do they connect? Can't over public internet. And there's no connection between their office and the VPC. Needs more info I think.","poster":"trongod05","upvote_count":"2","comment_id":"1294415"}]}],"choices":{"D":"Create a VPC and two public subnets. Create the RDS cluster in the public subnets. Create a cluster user for each developer. Use RDS security groups to allow the users to access the cluster.","A":"Create a VPC and two public subnets. Create the RDS cluster in the public subnets. Use AWS Site-to-Site VPN with a customer gateway in the company's office.","C":"Create a VPC and two private subnets. Create the RDS cluster in the private subnets. Use RDS security groups to allow the company's office IP ranges to access the cluster.","B":"Create a VPC and two private subnets. Create the RDS cluster in the private subnets. Use AWS Site-to-Site VPN with a customer gateway in the company's office."},"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/148461-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-09-30 20:33:00","question_id":1012,"answer":"B","question_text":"A company's software development team needs an Amazon RDS Multi-AZ cluster. The RDS cluster will serve as a backend for a desktop client that is deployed on premises. The desktop client requires direct connectivity to the RDS cluster.\n\nThe company must give the development team the ability to connect to the cluster by using the client when the team is in the office.\n\nWhich solution provides the required connectivity MOST securely?","answers_community":["B (80%)","C (20%)"],"answer_ET":"B","unix_timestamp":1727721180,"answer_description":"","exam_id":31,"answer_images":[]},{"id":"FNDAhOwKrukzXRUDEbkl","topic":"1","question_id":1013,"url":"https://www.examtopics.com/discussions/amazon/view/148462-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"content":"Selected Answer: C\nData transfer within the zone is free, but cross-zone is not free: \n https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/","upvote_count":"1","timestamp":"1737245220.0","comment_id":"1342805","poster":"FlyingHawk"},{"comment_id":"1319280","upvote_count":"3","content":"Selected Answer: C\nK. After research it seems C make sense since it is saying data transfer between EC2 and data transfter between two EC2 in AZ is free but not between AZ's.\n\nNo cost within AZ: Data transfer between EC2 instances in the same AZ is free. \nCross-AZ charges: If you transfer data between instances in different Availability Zones, you will be charged for the inter-zone data transfer. \nRegion-based pricing: While data transfer within an AZ is free, data transfer across different regions may also incur charges","poster":"ckhemani","timestamp":"1732802880.0"},{"upvote_count":"2","comment_id":"1319274","poster":"ckhemani","timestamp":"1732802640.0","content":"Selected Answer: B\nS3 is repliacted only to 3 AZ's and you can't define AZ while creating the Bucket. If this Question would have said about Directory which is One Zone express where you can define the AZ then it makes sense to select option C. \n\nOption B makes sense to configure EC2 in same region since EC2/S3 transfer data between same region is free too."},{"poster":"78b9037","upvote_count":"2","content":"Selected Answer: C\nTwo reasons: \n1. EC2 instances in the same AZ can communicate with each other at no cost. \n2. Data transfer between EC2 instances and S3 within the same AZ is free\nhttps://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/","timestamp":"1732713840.0","comment_id":"1318676"},{"content":"Selected Answer: C\nC","upvote_count":"2","timestamp":"1729330620.0","comment_id":"1299970","poster":"sOI852POL"},{"timestamp":"1728296760.0","content":"Respuesta correcta C :","poster":"viejito","comment_id":"1294201","upvote_count":"2"},{"content":"Selected Answer: C\nAnswer is C","poster":"aragon_saa","upvote_count":"2","comment_id":"1293429","timestamp":"1728123360.0"}],"answer":"C","unix_timestamp":1727722440,"choices":{"A":"Place all the EC2 instances in an Auto Scaling group.","C":"Place all the EC2 instances in the same Availability Zone.","B":"Place all the EC2 instances in the same AWS Region.","D":"Place all the EC2 instances in private subnets in multiple Availability Zones."},"answer_ET":"C","answers_community":["C (83%)","B (17%)"],"exam_id":31,"isMC":true,"answer_description":"","timestamp":"2024-09-30 20:54:00","answer_images":[],"question_text":"A solutions architect is creating an application that will handle batch processing of large amounts of data. The input data will be held in Amazon S3 and the output data will be stored in a different S3 bucket. For processing, the application will transfer the data over the network between multiple Amazon EC2 instances.\n\nWhat should the solutions architect do to reduce the overall data transfer costs?","question_images":[]},{"id":"MDt7PeM6VUPwMT2IXYJO","topic":"1","question_id":1014,"url":"https://www.examtopics.com/discussions/amazon/view/148463-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"A","discussion":[{"poster":"LeonSauveterre","content":"Selected Answer: A\nA - AWS KMS + Secrets Manager support custom rotation intervals, such as 14 days, with built-in integration for Aurora DB clusters.\nB, C, D - Manual implementation of rotation? This is really hard.","timestamp":"1736398380.0","comment_id":"1338189","upvote_count":"1"},{"content":"respuesta correcta A . Credenciales , rotación automática =AWS Secrets Manager .","upvote_count":"2","poster":"viejito","timestamp":"1728297060.0","comment_id":"1294203"},{"upvote_count":"2","poster":"aragon_saa","timestamp":"1728123360.0","comment_id":"1293428","content":"Selected Answer: A\nAnswer is A"}],"unix_timestamp":1727723040,"choices":{"A":"Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.","B":"Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.","D":"Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.","C":"Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file."},"answer_ET":"A","answers_community":["A (100%)"],"exam_id":31,"isMC":true,"answer_description":"","timestamp":"2024-09-30 21:04:00","question_text":"A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company's IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days.\n\nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?","answer_images":[],"question_images":[]},{"id":"FTBz9a1aKHmkgu4cBS4M","topic":"1","discussion":[{"timestamp":"1728100740.0","upvote_count":"7","comment_id":"1293326","poster":"kbgsgsgs","content":"Selected Answer: B\nS3 Intelligent-Tiering is cost-effective for storing large amounts of video content, and since Lambda doesn't work, shouldn't we consider serverless?"},{"timestamp":"1736400300.0","content":"Selected Answer: B\nA - Video processing can take up to 20 minutes. So lambda is not suitable here.\nB - Fargate allows running containers without managing EC2 instances, scaling automatically based on demand. Besides, S3 Intelligent-Tiering is cost-effective for storing large video files.\nC - I gotta say, the SQS is a really good choice here! It's just that S3 Standard is more expensive than S3 Intelligent-Tiering for large files, and that EC2 (even with Auto Scaling) requires managing the underlying infrastructure.\nD - EKS is more complex to set up but the real issue is that RDS in a single Availability Zone introduces a single point of failure, so scalability and reliability can't be met. Also, S3 Glacier Deep Archive is designed for long-term archival storage with high latency and not suitable for frequently accessed video content.","poster":"LeonSauveterre","comment_id":"1338192","upvote_count":"4"},{"content":"Selected Answer: B\nit is a tough question. \nterabyte-sized videos are difficult to process. we need to split the videos into chucks either with fargate or EC2. \nseems DB is essential in this scenario as I have no idea how to get the videos out from s3 without metadata which should be stored into DB. So I prefer B. SQS is used for peak clipping during burst periods.","comments":[{"upvote_count":"1","comment_id":"1331831","poster":"EllenLiu","timestamp":"1735199400.0","content":"this the whole work flow for a ideal solution:\n1.Upload: Users upload videos to CloudFront with origin S3.\n2.Trigger: S3 sends an event notification to SQS.\n3.EC2 instances poll the SQS queue and download and process Video files from S3\n4.The processed video is uploaded to another S3 bucket, metadata is saved into Aurora"}],"timestamp":"1735198440.0","poster":"EllenLiu","upvote_count":"1","comment_id":"1331827"},{"poster":"78b9037","content":"Selected Answer: B\nB and C are both possible but i decided for B because:\n- Fargate provides serverless containers that scale automatically\n- No infrastructure management is required\n- Aurora provides high availability for metadata\n- S3 Intelligent-Tiering optimizes storage costs automatically\n- It is not tile limited like Lambda\nOn the other hand C requires more management overhead it is less cost-effective than containerized solutions, and need to manage EC2 instances manually","upvote_count":"2","comment_id":"1318684","timestamp":"1732714800.0"},{"comment_id":"1297067","timestamp":"1728843420.0","upvote_count":"2","poster":"XXXXXlNN","comments":[{"timestamp":"1728843960.0","content":"Additionally, C also introduced ALB service in the picture, that increases the cost as well.","upvote_count":"2","comment_id":"1297069","poster":"XXXXXlNN"}],"content":"This is a tough one I would say. I wish B and C can combine. but I vote for B.\n\nFor B, it is more cost efficiency focus rather than decoupling the whole process for improving overall reliability.\nFor C, the use of SQS is perferct solution for the downside of option B. But EC2 comes in picture which increases the cost and operational complexity.\n\nHow to pick then? lets go back to the question and see what it focuses - cost or operational compllexity and stability? It looks like it leans more on focusing scalibility and cost-efficiency. In that case, I would go for B because fargate provides cost-efficiency and store just metadata in DB and the rest data in S3 also provides a lower cost and improves its performance."},{"upvote_count":"1","timestamp":"1728509460.0","poster":"blehbleh","comment_id":"1295291","content":"Selected Answer: C\nC makes the most sense out of the options and given requirements."},{"content":"Answer: C\nThe question did not mention the application need to be containerized , i will choose C","upvote_count":"1","comment_id":"1294880","poster":"hharbiordun85","comments":[{"content":"But it mentioned \"rebuilding its infrastructure\", opening the door to a re-engineering process in which micro services can be applied","comment_id":"1343120","upvote_count":"1","poster":"GOTJ","timestamp":"1737308580.0"}],"timestamp":"1728424380.0"},{"content":"Selected Answer: C\nI personally think C, I could be wrong.","timestamp":"1727813760.0","upvote_count":"2","comment_id":"1292104","poster":"blehbleh"}],"choices":{"B":"Use Amazon Elastic Container Service (Amazon ECS) and AWS Fargate to implement microservices to process videos. Store video metadata in Amazon Aurora. Store video content in Amazon S3 Intelligent-Tiering.","D":"Deploy a containerized video processing application on Amazon Elastic Kubernetes Service (Amazon EKS) on Amazon EC2. Store video metadata in Amazon RDS in a single Availability Zone. Store video content in Amazon S3 Glacier Deep Archive.","A":"Use AWS Lambda functions to process videos. Store video metadata in Amazon DynamoDB. Store video content in Amazon S3 Intelligent-Tiering.","C":"Use Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) to process videos. Store video content in Amazon S3 Standard. Use Amazon Simple Queue Service (Amazon SQS) for queuing and to decouple processing tasks."},"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/148465-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-09-30 21:15:00","question_id":1015,"answer":"B","answers_community":["B (82%)","C (18%)"],"question_text":"A streaming media company is rebuilding its infrastructure to accommodate increasing demand for video content that users consume daily.\n\nThe company needs to process terabyte-sized videos to block some content in the videos. Video processing can take up to 20 minutes.\n\nThe company needs a solution that will scale with demand and remain cost-effective.\n\nWhich solution will meet these requirements?","answer_ET":"B","unix_timestamp":1727723700,"answer_description":"","exam_id":31,"answer_images":[]}],"exam":{"name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025","isMCOnly":true,"numberOfQuestions":1019,"isBeta":false,"provider":"Amazon","isImplemented":true,"id":31},"currentPage":203},"__N_SSP":true}