{"pageProps":{"questions":[{"id":"AsZZ2p9UIu4GVAzlElJA","discussion":[{"comments":[{"poster":"MarcChartouny","comment_id":"378043","content":"Nemer in Arabic means 'Tiger'... And it seems you are a real AWS Tiger Man!! #Guru_Level","timestamp":"1635533520.0","upvote_count":"11"}],"upvote_count":"36","timestamp":"1632564000.0","comment_id":"154258","poster":"Nemer","content":"D. Between executionRoleArn (option C) and taskRoleArn (D), only the latter is used to interact with DynamoDB. The former is used to download images or write logs to Cloudwatch. \n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html\n\nStatus 400 with DynamoDB. Here,probably an authn failure due to someone messing up the role.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodes"},{"upvote_count":"1","timestamp":"1642899960.0","poster":"RVivek","comment_id":"530180","content":"D is the answer"},{"comment_id":"498336","timestamp":"1639116900.0","upvote_count":"2","poster":"GeniusMikeLiu","content":"Selected Answer: D\nhttps://sysadmins.co.za/difference-with-ecs-task-and-execution-iam-roles-on-aws/"},{"timestamp":"1638750720.0","upvote_count":"2","comment_id":"494760","poster":"AzureDP900","content":"I will go with D, This question is part of Neal Davis practice tests."},{"comment_id":"484896","poster":"backfringe","content":"I go with D","timestamp":"1637659140.0","upvote_count":"1"},{"timestamp":"1637394240.0","upvote_count":"3","comment_id":"482330","poster":"acloudguru","content":"Selected Answer: D\nC is only for agent related tasks such as cloudwatch, secret manager ,ECR, while this is 400 error, must be something wrong between DynamoDB, so such role should be D."},{"timestamp":"1636014600.0","content":"I'll go with D","upvote_count":"3","comment_id":"411749","poster":"WhyIronMan"},{"comment_id":"398889","content":"D IS CORRECT","timestamp":"1635857940.0","poster":"tuananhngo","upvote_count":"3"},{"upvote_count":"4","comment_id":"350039","content":"it's D","timestamp":"1635264660.0","poster":"Waiweng"},{"poster":"blackgamer","content":"D is the answer.","timestamp":"1634958780.0","comment_id":"348185","upvote_count":"2"},{"content":"agree D.","timestamp":"1634846220.0","poster":"certainly","comment_id":"295538","upvote_count":"1"},{"comment_id":"292475","poster":"Kian1","upvote_count":"2","content":"going with D","timestamp":"1634841840.0"},{"timestamp":"1634738760.0","content":"D - Task role modification","comment_id":"281182","poster":"Firststack","upvote_count":"2"},{"content":"I will go with D","comment_id":"276810","upvote_count":"4","timestamp":"1634569080.0","poster":"Ebi"},{"poster":"Bulti","comment_id":"253062","upvote_count":"1","content":"D is the correct answer.","timestamp":"1634566440.0"},{"timestamp":"1633576920.0","content":"Correct is D. Task Role","comment_id":"244010","poster":"T14102020","upvote_count":"1"},{"comment_id":"231613","poster":"jackdryan","upvote_count":"2","timestamp":"1633454400.0","content":"I'll go with D"},{"comment_id":"199185","upvote_count":"1","timestamp":"1633188180.0","content":"Option D","poster":"Paitan"},{"timestamp":"1632656400.0","comment_id":"156997","poster":"Anila_Dhharisi","upvote_count":"3","content":"For errors in Dynamodb showing 400 could be because of resource not in active state. To update the table in dynamo the resource should be in active state not in updating state.\nNow according the options, C is correct. ECS does the execution based on ECSTaskExecution Role. The error looks like \" \"InvalidParameterException: Unable to assume the service linked role. Please verify that the ECS service linked role exists.\\n\\tstatus code: 400, request id\". So for this kind of errors we need to execute this command in your AWS CLI aws iam create-service-linked-role --aws-service-name ecs.amazonaws.com, then use the newly created role's Arn within the ExecutionRoleArn property of your AWS::ECS::TaskDefinition. So answer is C.","comments":[{"poster":"balflearchen","content":"400 error code means authentication failure,\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html","upvote_count":"1","timestamp":"1641300780.0","comment_id":"516620"},{"poster":"TK2019","upvote_count":"5","content":"D is correct , the tasks executes by taskRoleArn which needs the permission . Refer the below stakoverflow resolution as well .\nhttps://stackoverflow.com/questions/48999472/difference-between-aws-elastic-container-services-ecs-executionrole-and-taskr","timestamp":"1632780180.0","comment_id":"162918"}]}],"url":"https://www.examtopics.com/discussions/amazon/view/27791-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true,"choices":{"A":"The ECS service was deleted.","B":"The ECS configuration does not contain an Auto Scaling group.","D":"The ECS task role was modified.","C":"The ECS instance task execution IAM role was modified."},"answer_ET":"D","question_text":"A fleet of Amazon ECS instances is used to poll an Amazon SQS queue and update items in an Amazon DynamoDB database. Items in the table are not being updated, and the SQS queue is filling up. Amazon CloudWatch Logs are showing consistent 400 errors when attempting to update the table. The provisioned write capacity units are appropriately configured, and no throttling is occurring.\nWhat is the LIKELY cause of the failure?","answer_images":[],"unix_timestamp":1596991080,"topic":"1","question_images":[],"answers_community":["D (100%)"],"timestamp":"2020-08-09 18:38:00","answer_description":"","question_id":536,"answer":"D","exam_id":32},{"id":"eMZZSyMUAm0TnP0otzVI","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/27886-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":537,"answer_ET":"A","topic":"1","discussion":[{"timestamp":"1632443400.0","content":"Its A. The alerts clearly indicate the problem was caused by sudden spike in traffic. Autoscaling on DDB didnt work because the suddenness of the spike, which is why you need to scale out the DDB before the traffic spike comes in rather than wait for the actual spike to trigger the scaling","upvote_count":"19","comments":[{"content":"It makes sense to auto scale dynamodb when cpu utilisation is being spiked, rather than predicting the spike time","upvote_count":"2","comment_id":"184008","poster":"sam422","timestamp":"1632472560.0"},{"upvote_count":"1","comment_id":"333898","poster":"sarah_t","timestamp":"1635503820.0","content":"This https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html points to C, not A","comments":[{"upvote_count":"2","timestamp":"1635591480.0","poster":"sarah_t","comment_id":"333899","content":"However, after reading this https://aws.amazon.com/about-aws/whats-new/2017/11/scheduled-scaling-now-available-for-application-auto-scaling/ I am probably going with A..."}]}],"comment_id":"175455","poster":"hailiang"},{"upvote_count":"9","content":"Ans: C\nAlthough it had auto scaling enabled in Dynamodb, it did not scale quick enough. Dynamodb's auto scaling relies on cloudwatch alarms and it takes at least a minute to trigger each scaling based on the 70% utilisation target. This was explained in the GetRecords.IteratorAgeMilliseconds matrix from Kinesis that lambda was not getting records from Kinesis quick enough. \nhttps://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html\n\nSince the spikes were huge and it hit the provisioned WCU during that time before auto-scaling could kick in. It resulted in ProvisionedThroughputExceededException from Dynamodb. As a result, it took a few rounds (a few mins) to scale to the desired utilisation target. \nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\n\nSo, the solution is to lower the utilisation target and let it scale ASAP.","timestamp":"1632237480.0","poster":"b3llman","comment_id":"168028"},{"poster":"sumaju","upvote_count":"1","timestamp":"1701420540.0","content":"Selected Answer: D\nBased on this article, I would go by D.\n\nIf you have only one consumer application, it is always possible to read at least two times faster than the put rate. That’s because you can write up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second (including partition keys). Each open shard can support up to 5 transactions per second for reads, up to a maximum total data read rate of 2 MB per second. Note that each read (GetRecords call) gets a batch of records. The size of the data returned by GetRecords varies depending on the utilization of the shard. The maximum size of data that GetRecords can return is 10 MB. If a call returns that limit, subsequent calls made within the next 5 seconds throw ProvisionedThroughputExceededException.\n\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","comment_id":"1085074"},{"content":"Selected Answer: A\nA is right and D is wrong.\nAt first I chose D but after reading this link:https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html\na high value of GetRecords.IteratorAgeMilliseconds means \n\"a consumer is not keeping up with the stream because it is not processing records fast enough\" It clearly indicates the consumer side problem which is dynamodb side. so increasing shard just makes it worse.","upvote_count":"1","timestamp":"1684609140.0","poster":"Jesuisleon","comment_id":"902729"},{"upvote_count":"1","poster":"evargasbrz","timestamp":"1672248480.0","content":"Selected Answer: D\nRegarding this: https://aws.amazon.com/pt/premiumsupport/knowledge-center/kinesis-data-streams-iteratorage-metric/\n\nTo address the following root cause \"RecordsProcessed\": \nYou can also check the overall throughput of the Kinesis data stream by monitoring the CloudWatch metrics IncomingBytes and IncomingRecords. For more information about KCL and custom CloudWatch metrics, see Monitoring the Kinesis Client Library with Amazon CloudWatch. However, if the processing time cannot be reduced, then consider upscaling the Kinesis stream by increasing the number of shards.\n\nso D looks good.","comment_id":"760148"},{"upvote_count":"2","timestamp":"1665122400.0","poster":"JohnPi","content":"DynamoDB auto scaling modifies provisioned throughput settings only when the actual workload stays elevated (or depressed) for a sustained period of several minutes. The Application Auto Scaling target tracking algorithm seeks to keep the target utilization at or near your chosen value over the long term.\n\nSudden, short-duration spikes of activity are accommodated by the table's built-in burst capacity.","comment_id":"688360"},{"poster":"AwsBRFan","upvote_count":"2","timestamp":"1663851060.0","content":"Selected Answer: A\nSince issue can be related to consumers, then changing to A","comment_id":"676140"},{"comment_id":"676117","content":"Selected Answer: D\nhttps://aws.amazon.com/pt/premiumsupport/knowledge-center/kinesis-data-streams-iteratorage-metric/\n\n\"However, if the processing time cannot be reduced, then consider upscaling the Kinesis stream by increasing the number of shards.\"","timestamp":"1663849980.0","poster":"AwsBRFan","upvote_count":"2"},{"content":"Selected Answer: A\nA. Use Application Auto Scaling to set a scaling schedule to scale out write capacity on the DynamoDB table during predictable load spikes.","upvote_count":"2","comment_id":"577481","poster":"jj22222","timestamp":"1648549560.0"},{"timestamp":"1641954120.0","upvote_count":"1","content":"Selected Answer: A\nI think it's A","comment_id":"521874","poster":"limeboi18"},{"comment_id":"508191","content":"Option A.\nThis is a case of piling records for processing. Kinesis GetRecords.IteratorAgeMilliseconds increasing indicates that records are being processed slowly and this higlights the risk of records expiring. ProvisionedThroughputExceededException indicates request rate is too high. AWS API Doc says - Reduce the frequency of requests and use exponential backoff so they can be processed. To ensure the records are processed quickly during surge times which is known ahead write capacity should be increased.","poster":"tkanmani76","timestamp":"1640301960.0","upvote_count":"2","comments":[{"timestamp":"1640302800.0","content":"Related information - When Kinesis Producer is writing to KDS - the capacity is determined by the number of shards ( provisioned mode where the load is known). AWS supports on-demand mode where the shards are scaled up/down. Each shard for writing is able to handle 1MB/Sec. So if we need to increase write we need to increase the shards. This is not relevant in our case as the data is getting written and Lambda is able to read from the shards.","comment_id":"508196","poster":"tkanmani76","upvote_count":"2"}]},{"content":"A is right answer based on traffic surge that often surpasses five times the average load","timestamp":"1638751020.0","upvote_count":"1","poster":"AzureDP900","comment_id":"494764"},{"timestamp":"1636168980.0","poster":"kirrim","comment_id":"461277","upvote_count":"5","content":"You can tell the issue is with DynamoDB because Lambda is reporting a ProvisionedThroughputExceededException, which is part of the DynamoDB SDK that Lambda code is using, indicating DynamoDB cannot keep up. So you know you're dealing with A or C. The root of the problem is that even though DynamoDB is set up for autoscaling, it takes a few minutes for it to happen. Merely adjusting the auto scaling policy thresholds can't change that fact, it's still going to take a while to scale up. If the traffic was a slow ramp up, you might be able to get away with C, but this is a sudden flood that happens twice per day. Since this is very predictable and on a schedule, the easiest method is to schedule the scale-up to happen in advance of the flood hitting. (A)\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-streams-iteratorage-metric/\nhttps://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/ProvisionedThroughputExceededException.html\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html"},{"poster":"tgv","upvote_count":"2","comment_id":"435739","timestamp":"1636131360.0","content":"AAA\n---"},{"upvote_count":"1","poster":"WhyIronMan","timestamp":"1636061520.0","content":"I'll go with A","comment_id":"411761"},{"poster":"Kopa","comment_id":"402713","content":"Im for A, it happens on scheduled time so why not choose schedule automatic scale...","upvote_count":"2","timestamp":"1636052880.0"},{"poster":"Waiweng","upvote_count":"3","timestamp":"1636028160.0","content":"it's A","comment_id":"350064"},{"comment_id":"348190","poster":"blackgamer","upvote_count":"1","timestamp":"1635912300.0","content":"My answer is A."},{"upvote_count":"3","content":"A\nFirst you need to figure out where the congestion is. It is between Lamda and Dynamo DB. Then, you need to understand the Autoscaling of Dynamo DB. it only react after a few min of sustained spike. Adjusting the target down actually wont do anything. In reality, Dynamo DB has Using Burst Capacity which can handle 5 min burst.","poster":"digimaniac","timestamp":"1635687000.0","comment_id":"347715"},{"upvote_count":"1","content":"A is the Answer\n\"The first is predictable traffic, which means the scheduled actions. An example of predictable traffic is when your Kinesis Data Stream endpoint sees growing traffic in specific time window. In this case, you can make sure that an Application Auto Scaling scheduled action increases the number of Kinesis Data Stream shards to meet the demand. For instance, you might increase the number of shards at 12:00 p.m. and decrease them at 8:00 p.m.\"\nhttps://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/#aws-comment-trigger-5929:~:text=The%20first%20is%20predictable%20traffic%2C%20which,and%20decrease%20them%20at%208%3A00%20p.m.","poster":"AJBA","comment_id":"302501","timestamp":"1635473220.0"},{"upvote_count":"1","comment_id":"295533","timestamp":"1635004740.0","content":"A. https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html","poster":"certainly"},{"poster":"Joaster","comment_id":"292628","timestamp":"1634988540.0","upvote_count":"2","content":"D.https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html"},{"comment_id":"292477","poster":"Kian1","upvote_count":"2","timestamp":"1634834220.0","content":"going with C"},{"timestamp":"1634801160.0","content":"Issue is with DynamoDB table and not Kinesis, so answer is either A or C.\nAs the spike has a pattern there is no point to lower threshold to 20% instead we can have schedule scale for write throughput, so I will go with A","comment_id":"276836","poster":"Ebi","upvote_count":"8","comments":[{"upvote_count":"5","content":"\"The scaling policy also contains a target utilization—the percentage of consumed provisioned throughput at a point in time. Application Auto Scaling uses a target tracking algorithm to adjust the provisioned throughput of the table (or index) upward or downward in response to actual workloads, so that the actual capacity utilization remains at or near your target utilization.\"\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\n\nThis explains why setting target utilization to 20% is not a good idea, specially for the cases the spike in load has a pattern.","poster":"Ebi","comment_id":"286561","timestamp":"1634824980.0"}]},{"comments":[{"timestamp":"1635639420.0","upvote_count":"2","comment_id":"333903","content":"Turns out, yes: https://aws.amazon.com/about-aws/whats-new/2017/11/scheduled-scaling-now-available-for-application-auto-scaling/ \nApplication Auto Scaling is a service that can scale a number of services, including DynamoDB.","poster":"sarah_t"}],"content":"My answer is C. \nA - Says Use \"Application Auto Scaling to set a scaling schedule to scale out write capacity \". Why would I use app autoscaling for DynamoDB to scale out? seems wrong to me","poster":"SD13","timestamp":"1634672340.0","upvote_count":"2","comment_id":"263525"},{"poster":"01037","timestamp":"1634647920.0","upvote_count":"2","comment_id":"263204","content":"A.\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html\n\nBoth A and C work, but 20% too sensitive."},{"content":"Answer is A. You need scheduled autoscaling for predictable workloads which i the case here.","poster":"Bulti","comments":[{"comment_id":"269357","content":"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html","poster":"Bulti","timestamp":"1634779680.0","upvote_count":"1"}],"timestamp":"1634635440.0","upvote_count":"3","comment_id":"253067"},{"content":"Will go with D, as dynamo has autoscaling enabled already. For kinesis streams even Read also participates in the Throughput configuration. So yes, time for handling resharding.","comments":[{"poster":"Britts","timestamp":"1634183700.0","comment_id":"250750","content":"My bad. It would have been ReadProvisionedThroughputExceeded in that case. Yup changed to C, i.e. aggressive autoscaling.","upvote_count":"2"}],"upvote_count":"2","timestamp":"1634156040.0","comment_id":"250747","poster":"Britts"},{"poster":"srinivasa","comment_id":"245121","upvote_count":"1","content":"Why not A ? When we know that spikes are occurring at a specified time. \nAlso, why would you need 20% target utilization during off-peak hours ?\nA seems to be logical to me.","timestamp":"1633874700.0"},{"upvote_count":"2","comments":[{"comment_id":"244017","upvote_count":"1","content":"Correct is A. Why 20% and not 30% or 40% for C?","timestamp":"1633796940.0","poster":"T14102020"}],"comment_id":"244016","content":"Correct is A. Why 20% and not 30% or 40% for D?","timestamp":"1633350480.0","poster":"T14102020"},{"content":"I think A is better answer , my friend choose C","comments":[{"timestamp":"1633034340.0","content":"On a second thought, I am changing to A.","comments":[{"poster":"ali98","upvote_count":"1","comment_id":"235304","comments":[{"content":"sorry its valid. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","poster":"ali98","timestamp":"1633113480.0","comment_id":"235308","upvote_count":"2"}],"timestamp":"1633035720.0","content":"is \"Application Auto Scaling \" valid term for DynamoDB ?"}],"poster":"jackdryan","upvote_count":"2","comment_id":"234452"}],"poster":"gookseang","timestamp":"1632847800.0","upvote_count":"1","comment_id":"233052"},{"content":"I'll go with C","upvote_count":"1","poster":"jackdryan","timestamp":"1632719940.0","comment_id":"231618"},{"comment_id":"209591","timestamp":"1632620220.0","content":"A. If the spike time is predictable, then the setup can cater for a huge capacity during those spike time.","poster":"CYL","upvote_count":"2"},{"poster":"Momon","upvote_count":"1","timestamp":"1632305820.0","comment_id":"168340","content":"C https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html"},{"upvote_count":"1","timestamp":"1632188400.0","poster":"Anila_Dhharisi","content":"This seems there is an issue between the Kinesis and Lambda and not with Lambda and Dynamodb. So the best way is to increase the shards. So Option D is right answer.","comment_id":"157003"},{"content":"D. DynamoDB table has auto scaling enabled -> the problem is not with DB, but with Kinesis shards (1MB/s or 1000 messages/s at write per shard). Beyond those limits, PutRecord throws ProvisionedThroughputExceededException. \nNeed to do Shard splitting to increase #shards -> increase capacity.\n\nhttps://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html","upvote_count":"3","poster":"Nemer","comments":[{"upvote_count":"3","timestamp":"1632404100.0","poster":"Nemer","content":"Err. Change to C after reading a couple of excellent comments. We have kinesis streams -> Lambda -> dynDB. It is Lambda writing the throughput exceeded, meaning that the db didn't scale fast enough.","comment_id":"170038"},{"poster":"LunchTime","content":"C is correct\nNemer’s analysis is not correct as Lambda is writing the ProvisionedThrougputExceededException and not Kinesis. Lambda writing the ProvisionedThrougputExceededException exception means DynamoDB has exceeded its maximum allowed provisioned throughput for a table or for one or more global secondary indexes (https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html). \nAlso, GetRecords.IteratorAgeMilliseconds is the age of the last record in the most recent GetRecords call made against a Kinesis stream. Age is the difference between the current time and when the last record of the GetRecords call was written to the stream. (https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html). In other words, a large value for this metric means that the records are waiting a long time to be read by the Lamda function. \nThis means that the issue is not with Kinesis but with Lambda having trouble getting the records into DynamoDB. Therefore, D is not correct.\n\nI agree with b3llman’s analysis. C is correct","upvote_count":"8","timestamp":"1632395580.0","comments":[{"content":"I agree mostly with LunchTime; however scaling at 20% is too sensitive. DynamoDB auto scaling allows to scale based on schedule, since we know the spike is consistent during start and end of each day, better choice is A","comment_id":"205890","upvote_count":"2","poster":"cpd","timestamp":"1632516540.0"}],"comment_id":"168634"}],"timestamp":"1632073620.0","comment_id":"154330"}],"answer_description":"","question_text":"A mobile gaming application publishes data continuously to Amazon Kinesis Data Streams. An AWS Lambda function processes records from the data stream and writes to an Amazon DynamoDB table. The DynamoDB table has an auto scaling policy enabled with the target utilization set to 70%.\nFor several minutes at the start and end of each day, there is a spike in traffic that often exceeds five times the normal load. The company notices the\nGetRecords.IteratorAgeMilliseconds metric of the Kinesis data stream temporarily spikes to over a minute for several minutes. The AWS Lambda function writes\nProvisionedThroughputExceededException messages to Amazon CloudWatch Logs during these times, and some records are redirected to the dead letter queue.\nNo exceptions are thrown by the Kinesis producer on the gaming application.\nWhat change should the company make to resolve this issue?","answers_community":["A (60%)","D (40%)"],"question_images":[],"timestamp":"2020-08-10 10:40:00","exam_id":32,"isMC":true,"unix_timestamp":1597048800,"answer":"A","choices":{"D":"Increase the number of shards in the Kinesis data stream to increase throughput capacity.","A":"Use Application Auto Scaling to set a scaling schedule to scale out write capacity on the DynamoDB table during predictable load spikes.","C":"Reduce the DynamoDB table auto scaling policy's target utilization to 20% to more quickly respond to load spikes.","B":"Use Amazon CloudWatch Events to monitor the dead letter queue and invoke a Lambda function to automatically retry failed records."}},{"id":"1tiRVCzzXIPIEGJmnX1h","answers_community":["C (100%)"],"answer_images":[],"question_text":"A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. The company requires that only authenticated users are allowed to post content. The application generates a presigned URL that is used to upload objects through a browser interface. Most users are reporting slow upload times for objects larger than 100 MB.\nWhat can a Solutions Architect do to improve the performance of these uploads while ensuring only authenticated users are allowed to post content?","answer_description":"","isMC":true,"answer":"C","topic":"1","question_images":[],"discussion":[{"timestamp":"1632719700.0","poster":"Nemer","comment_id":"154368","upvote_count":"17","content":"C. S3 Transfer Acceleration + multipart upload for performance, presigned URLs for access."},{"content":"Selected Answer: C\nC. S3 Transfer Acceleration + multipart upload for performance, presigned URLs for access.","poster":"SkyZeroZx","timestamp":"1688314680.0","upvote_count":"1","comment_id":"941051"},{"timestamp":"1639030920.0","content":"C. Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API.","poster":"cldy","comment_id":"497381","upvote_count":"1"},{"timestamp":"1638751200.0","content":"S3 Transfer Acceleration is right choice, I will go with C.","poster":"AzureDP900","comment_id":"494765","upvote_count":"1"},{"poster":"moon2351","upvote_count":"1","timestamp":"1636200180.0","comment_id":"448515","content":"Answer is C"},{"upvote_count":"1","timestamp":"1635856560.0","content":"I'll go with C","comment_id":"411765","poster":"WhyIronMan"},{"poster":"Chibuzo1","upvote_count":"4","comment_id":"353385","content":"The answer is C.\n. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.","timestamp":"1635778860.0"},{"upvote_count":"3","comment_id":"350069","poster":"Waiweng","timestamp":"1635414120.0","content":"it's C"},{"content":"ofc going with C","timestamp":"1635379140.0","comment_id":"292480","upvote_count":"2","poster":"Kian1"},{"poster":"Ebi","timestamp":"1635201900.0","content":"I go with C","upvote_count":"3","comment_id":"277537"},{"comment_id":"253082","poster":"Bulti","timestamp":"1635038160.0","content":"The question is about uploading the object faster not about retrieving uploaded objects faster and hence the answer is C. When using CloudFront to upload objects with S3 as origin the request goes through the Edge servers but doesn't use the S3 Transfer acceleration feature to accelerate the upload. Uploading speeds from slow to fast - direct S3-> Cloudfront to S3-> S3 transfer acceleration","upvote_count":"3"},{"timestamp":"1634818620.0","content":"No brainer. C","upvote_count":"1","comment_id":"250745","poster":"Britts"},{"comment_id":"244023","poster":"T14102020","timestamp":"1634502240.0","upvote_count":"1","content":"Correct is C. S3 Transfer Acceleration"},{"upvote_count":"2","content":"I'll go with C","comment_id":"231619","timestamp":"1634448600.0","poster":"jackdryan"},{"content":"C. Multipart upload and S3 transfer acceleration to handle the upload challenge. Presigned URL to ensure only the right users can do the upload.","poster":"CYL","comment_id":"209592","upvote_count":"2","timestamp":"1634307720.0"},{"comment_id":"199191","content":"C is the right option","poster":"Paitan","timestamp":"1634019960.0","upvote_count":"1"},{"content":"D could be the answer, but POST and PUT methods are not supported as cache in cloudfront\nhttps://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CachedMethods.html","timestamp":"1633636680.0","poster":"kanavpeer","comments":[{"poster":"sam422","comment_id":"184013","timestamp":"1633996020.0","upvote_count":"1","content":"Issue is with S3 upload right , I didn't see a cache issue?"}],"upvote_count":"4","comment_id":"168953"},{"upvote_count":"2","comment_id":"157006","timestamp":"1633563900.0","content":"C is right answer","poster":"Anila_Dhharisi"},{"upvote_count":"2","content":"C - Speed up uploading = S3 Transfer Acceleration","timestamp":"1633203300.0","poster":"zeronine","comment_id":"155338"}],"choices":{"D":"Configure an Amazon CloudFront distribution for the destination S3 bucket. Enable PUT and POST methods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity (OAI). Give the OAI user s3:PutObject permissions in the bucket policy. Have the browser interface upload objects using the CloudFront distribution.","B":"Set up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload API objects.","C":"Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API.","A":"Set up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects."},"answer_ET":"C","timestamp":"2020-08-10 11:13:00","exam_id":32,"question_id":538,"unix_timestamp":1597050780,"url":"https://www.examtopics.com/discussions/amazon/view/27896-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"K5wJxoVQuvUEA2Cgq3o1","unix_timestamp":1597052940,"answers_community":["B (100%)"],"discussion":[{"poster":"Nemer","content":"B seems about right: blue/green deployments to minimize downtime (as opposed to in-place deployments) + deployments can be rolled back automatically or manually with CodeDeploy.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html","upvote_count":"18","timestamp":"1632123420.0","comment_id":"154397"},{"comment_id":"168066","timestamp":"1632338220.0","content":"Ans: A\nSince the requirement is \"as quickly as possible with minimal downtime\". Blue/green is not as quick and the question didn't ask for zero downtime.","comments":[{"timestamp":"1632425460.0","upvote_count":"1","content":"Any links?","poster":"angelsrp","comment_id":"207082"},{"upvote_count":"3","comments":[{"comment_id":"580140","timestamp":"1648969260.0","content":"You are missing the question's requirements.","upvote_count":"1","poster":"sashsz"}],"timestamp":"1634018940.0","comment_id":"253627","content":"Your missing the point of the question \"push another code update.\" is NOT a ROLLBACK .... Its important to READ the question in the exam ... B is right as it is very fluid ..","poster":"petebear55"}],"poster":"b3llman","upvote_count":"5"},{"upvote_count":"1","content":"Selected Answer: B\nB seems about right: blue/green deployments to minimize downtime (as opposed to in-place deployments) + deployments can be rolled back automatically or manually with CodeDeploy.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html","comment_id":"941053","timestamp":"1688314740.0","poster":"SkyZeroZx"},{"content":"It should be B :\nhttps://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/bluegreen-deployments.html","comment_id":"874215","timestamp":"1681867440.0","upvote_count":"1","poster":"Dehradoon"},{"timestamp":"1661698200.0","poster":"kadev","upvote_count":"1","comment_id":"652989","content":"\"push another code update\" i dont like that => need to rollback to latest stable version => B"},{"content":"B. Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy.","upvote_count":"1","poster":"cldy","comment_id":"497391","timestamp":"1639031460.0"},{"upvote_count":"3","timestamp":"1638751320.0","comment_id":"494767","content":"I will go with B, Blue/Green is fast to rollback.","poster":"AzureDP900"},{"content":"I'll go with B","poster":"WhyIronMan","comment_id":"411769","upvote_count":"1","timestamp":"1636112460.0"},{"comment_id":"350071","content":"it's B","timestamp":"1635813780.0","upvote_count":"3","poster":"Waiweng"},{"comment_id":"330611","poster":"KnightVictor","content":"No brainer. going with B","upvote_count":"1","timestamp":"1634981160.0"},{"comment_id":"292483","upvote_count":"2","timestamp":"1634916480.0","poster":"Kian1","content":"going with B"},{"timestamp":"1634857440.0","content":"B Blue/Green","poster":"Firststack","comment_id":"281187","upvote_count":"3"},{"timestamp":"1634569620.0","comment_id":"277538","content":"I go with B","upvote_count":"4","poster":"Ebi"},{"poster":"kopper2019","content":"B as well","timestamp":"1634487180.0","comment_id":"265289","upvote_count":"1"},{"content":"B is the right answer as it provides the least downtime option.","timestamp":"1633350180.0","upvote_count":"1","poster":"Bulti","comment_id":"253107"},{"timestamp":"1633024680.0","upvote_count":"1","comment_id":"244024","content":"Correct is B. blue/green deployments","poster":"T14102020"},{"comment_id":"231622","timestamp":"1632776160.0","upvote_count":"2","poster":"jackdryan","content":"I'll go with B"},{"timestamp":"1632598980.0","comment_id":"209595","upvote_count":"1","content":"B. Use blue/green deployment to minimize downtime. The rest of the options do not allow for low downtime during deployment.","poster":"CYL"},{"upvote_count":"2","timestamp":"1632126420.0","comment_id":"157009","poster":"Anila_Dhharisi","content":"B is right answer. Yes Blue/Green deployments has minimal downtime when compared to in-place and can be rollback automatically."}],"url":"https://www.examtopics.com/discussions/amazon/view/27899-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true,"choices":{"A":"Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for in-place deployment. Monitor the newly deployed code, and, if there are any issues, push another code update.","C":"Configure CodePipeline with a deploy stage using AWS CloudFormation to create a pipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, push another code update.","B":"Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy.","D":"Configure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deployments. Monitor the newly deployed code, and, if there are any issues, push another code update."},"topic":"1","question_text":"A company's CISO has asked a Solutions Architect to re-engineer the company's current CI/CD practices to make sure patch deployments to its applications can happen as quickly as possible with minimal downtime if vulnerabilities are discovered. The company must also be able to quickly roll back a change in case of errors.\nThe web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer. The company is currently using GitHub to host the application source code, and has configured an AWS CodeBuild project to build the application. The company also intends to use AWS CodePipeline to trigger builds from GitHub commits using the existing CodeBuild project.\nWhat CI/CD configuration meets all of the requirements?","answer_images":[],"answer_description":"","question_images":[],"exam_id":32,"answer_ET":"B","timestamp":"2020-08-10 11:49:00","question_id":539,"answer":"B"},{"id":"1YmHmeGhspi1vacbHWFG","answer_images":[],"topic":"1","choices":{"B":"Store the data in multiple S3 buckets.","E":"Store the data using Apache Hive partitioning in Amazon S3 using a key that includes a date, such as dt=2019-02.","A":"Store each object in Amazon S3 with a random string at the front of each key.","D":"Store the data in Amazon S3 in objects that are smaller than 10 MB.","C":"Store the data in Amazon S3 in a columnar format, such as Apache Parquet or Apache ORC."},"timestamp":"2020-08-10 12:10:00","question_images":[],"exam_id":32,"discussion":[{"poster":"Nemer","timestamp":"1632194820.0","upvote_count":"25","comment_id":"154414","content":"C & E: Optimal performance with Athena is achieved with columnar storage and partitioning the data. \nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"},{"upvote_count":"1","poster":"SkyZeroZx","comment_id":"941057","timestamp":"1688314920.0","content":"Selected Answer: CE\nC, E is correct . This question is part of Neal Davis practice test"},{"upvote_count":"1","timestamp":"1668314700.0","content":"Selected Answer: CE\nC: \"Use an efficient file format such as parquet or ORC – To dramatically reduce query running time and costs, use compressed Parquet or ORC files to store your data. To convert your existing dataset to those formats in Athena, you can use CTAS. For more information, see Using CTAS and INSERT INTO for ETL and data analysis.\"\nhttps://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html\n\nE: \"By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost.\" + \"Athena can use Apache Hive style partitions\" \nhttps://docs.aws.amazon.com/athena/latest/ug/partitions.html","comment_id":"717092","poster":"et22s"},{"comment_id":"513372","content":"C and E.","upvote_count":"1","poster":"cldy","timestamp":"1640868900.0"},{"comment_id":"499322","poster":"cldy","timestamp":"1639218060.0","content":"C. Store the data in Amazon S3 in a columnar format, such as Apache Parquet or Apache ORC.\nE. Store the data using Apache Hive partitioning in Amazon S3 using a key that includes a date, such as dt=2019-02.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1639177800.0","poster":"challenger1","comment_id":"498995","content":"Selected Answer: CE\nMy Answer: C & E"},{"upvote_count":"2","comment_id":"494768","timestamp":"1638751500.0","poster":"AzureDP900","content":"C, E is correct . This question is part of Neal Davis practice test"},{"timestamp":"1638244080.0","upvote_count":"1","content":"Selected Answer: CE\nC & E: Optimal performance with Athena is achieved with columnar storage and partitioning the data.\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","poster":"acloudguru","comment_id":"490398"},{"upvote_count":"1","poster":"moon2351","content":"CE is correct","comment_id":"448517","timestamp":"1636265700.0"},{"content":"I'll go with C,E","poster":"WhyIronMan","upvote_count":"2","comment_id":"411770","timestamp":"1636262820.0"},{"upvote_count":"1","timestamp":"1636182600.0","comment_id":"362533","comments":[{"timestamp":"1641102840.0","upvote_count":"1","content":"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\nThis will clarify why E.","comment_id":"514848","poster":"tkanmani76"}],"poster":"oscargee","content":"B & C: Athena is used for S3 query only. In question they mentioned Athena not HIVE, so don't chose E."},{"timestamp":"1636138200.0","comment_id":"350075","upvote_count":"4","poster":"Waiweng","content":"it's C&E"},{"upvote_count":"1","comment_id":"348434","timestamp":"1635153900.0","content":"C & E. Don't confuse Apache Hive bucketing with AWS S3 Bucket.","poster":"blackgamer"},{"poster":"kiev","upvote_count":"1","content":"Full House with CE","timestamp":"1634956320.0","comment_id":"295081"},{"upvote_count":"2","timestamp":"1634848980.0","comment_id":"292485","content":"going with CE","poster":"Kian1"},{"timestamp":"1634834220.0","comment_id":"281189","poster":"Firststack","content":"C & E is correct","upvote_count":"2"},{"timestamp":"1634752980.0","comment_id":"277547","poster":"Ebi","upvote_count":"4","content":"Definitely C,E are correct answers"},{"poster":"Bulti","content":"Answer is C & E- Optimize Columnar data store + Partition to improve performance.","comment_id":"253126","timestamp":"1633579380.0","upvote_count":"2"},{"timestamp":"1633560900.0","comment_id":"244032","poster":"T14102020","upvote_count":"1","content":"Correct CE. Columnar storage and Hive partition"},{"timestamp":"1633537740.0","poster":"PAUGURU","comment_id":"236270","content":"C,E\nDocument on S3- Hive data partitioning on Athena here:\nhttps://docs.aws.amazon.com/athena/latest/ug/partitions.html","upvote_count":"2"},{"content":"I'll go with C,E","timestamp":"1633444680.0","comment_id":"231623","upvote_count":"3","poster":"jackdryan"},{"timestamp":"1633188840.0","comment_id":"209597","content":"C with columnar storage like parquet for compression and access efficiency. E partitioning will help to reduce number of files scanned for queries that are time-based.","poster":"CYL","upvote_count":"2"},{"comment_id":"157010","poster":"Anila_Dhharisi","content":"C,E is right answer.","timestamp":"1632892980.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"155336","content":"C & E - Optimize columnar data store generation + Partition","timestamp":"1632261660.0","poster":"zeronine"}],"question_text":"A company wants to analyze log data using date ranges with a custom application running on AWS. The application generates about 10 GB of data every day, which is expected to grow. A Solutions Architect is tasked with storing the data in Amazon S3 and using Amazon Athena to analyze the data.\nWhich combination of steps will ensure optimal performance as the data grows? (Choose two.)","unix_timestamp":1597054200,"url":"https://www.examtopics.com/discussions/amazon/view/27902-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","answer_ET":"CE","isMC":true,"answers_community":["CE (100%)"],"question_id":540,"answer":"CE"}],"exam":{"name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"isImplemented":true,"provider":"Amazon","numberOfQuestions":1019,"isBeta":false,"id":32,"lastUpdated":"11 Apr 2025"},"currentPage":108},"__N_SSP":true}