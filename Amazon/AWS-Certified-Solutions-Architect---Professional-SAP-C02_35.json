{"pageProps":{"questions":[{"id":"wU77HUPjceCtmuVH7t44","exam_id":33,"unix_timestamp":1687466820,"answer":"C","timestamp":"2023-06-22 22:47:00","answer_description":"","topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/113022-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. Run queries on the Amazon Redshift data to determine database activities on the Aurora database.","A":"Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch Service cluster for further analysis.","B":"Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program the Lambda function to decrypt the messages from EventBridge and to publish all database activity to Amazon S3 for further analysis.","C":"Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis."},"answer_ET":"C","answers_community":["C (100%)"],"question_id":171,"discussion":[{"content":"Selected Answer: C\nC achieves the Goal.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Monitoring.html","poster":"elanelans","timestamp":"1687505880.0","upvote_count":"7","comment_id":"931321"},{"upvote_count":"1","poster":"JoeTromundo","timestamp":"1728400380.0","content":"Selected Answer: C\nFor those who think the correct option is B: \"The Aurora DB cluster sends activities to an Amazon Kinesis data stream in near real time.\" It does NOT send to EventBridge.\nhttps://docs.aws.amazon.com/pt_br/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.html","comment_id":"1294756"},{"content":"C seems correct. but why not B?","upvote_count":"1","comment_id":"1113302","timestamp":"1704335460.0","poster":"thotwielder"},{"content":"B is ans :\nHere's why this solution is the most suitable:\n\nDirect integration: Database activity streams natively integrate with EventBridge, streamlining the process of capturing and routing events.\nRich event filtering: EventBridge offers powerful filtering capabilities, allowing the database team to selectively monitor specific events or patterns of interest.\nFlexible delivery: EventBridge can trigger various targets, including Lambda functions, which provide the ability to process and store events in S3 for further analysis.\nServerless architecture: Lambda functions eliminate the need to manage servers, reducing operational overhead and scaling automatically to handle event volume.\nCost-effective storage: S3 offers durable and cost-effective storage for long-term analysis of database activity logs.","upvote_count":"2","comment_id":"1091181","comments":[{"comment_id":"1268291","timestamp":"1724030400.0","poster":"helloworldabc","content":"just C","upvote_count":"1"}],"poster":"duriselvan","timestamp":"1702052040.0"},{"poster":"career360guru","comment_id":"1077984","timestamp":"1700705100.0","upvote_count":"1","content":"Selected Answer: C\nOption C"},{"poster":"ggrodskiy","timestamp":"1690050480.0","content":"Correct C.","upvote_count":"1","comment_id":"959772"},{"comment_id":"945204","upvote_count":"1","timestamp":"1688698860.0","poster":"NikkyDicky","content":"Selected Answer: C\nits a C"},{"comment_id":"934816","upvote_count":"2","content":"C - Correct. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Monitoring.html","timestamp":"1687816800.0","poster":"SmileyCloud"},{"timestamp":"1687719660.0","upvote_count":"1","comment_id":"933834","content":"Selected Answer: C\nC achieves the Goal.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Monitoring.html","poster":"SkyZeroZx"},{"poster":"shree2023","timestamp":"1687578060.0","upvote_count":"1","comment_id":"932136","content":"Selected Answer: C\nC indeed"},{"comment_id":"932092","timestamp":"1687571220.0","poster":"gd1","content":"Selected Answer: C\nGPT: Option A and D are incorrect because AWS DMS's Change Data Capture (CDC) functionality captures changes made at the database level, not data activity.","upvote_count":"4"},{"upvote_count":"1","timestamp":"1687503840.0","poster":"MoussaNoussa","content":"C is the right answer","comment_id":"931295"},{"comment_id":"931020","upvote_count":"1","content":"Selected Answer: C\nI go with C","poster":"bhanus","timestamp":"1687466820.0"}],"question_text":"A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor all data activity on all the databases.\n\nWhich solution will achieve this goal?","isMC":true,"answer_images":[]},{"id":"SGdbsDl7RFKwHwWhFkbn","answer":"C","timestamp":"2023-06-22 23:01:00","question_id":172,"exam_id":33,"answer_images":[],"discussion":[{"timestamp":"1687467660.0","upvote_count":"11","content":"Selected Answer: C\nC . From the question, app is running on memory-optimized instances (r6g.16xlarge) but only utilizing about one quarter of the CPU and memory. So cost-effective to use smaller instances (r6g.4xlarge), which provide a quarter of r6g.16xlarge instances.","poster":"bhanus","comment_id":"931043"},{"upvote_count":"7","content":"Selected Answer: C\n16large = 64CPU, \n4Large = 16 CPU\n8Large = 32 CPU\n¼ usage of 64 = 16CPU\n¼ of 12 EC2 = 3 instance, so C is a better choice.","poster":"nexus2020","timestamp":"1687639920.0","comment_id":"932920"},{"comments":[{"upvote_count":"1","poster":"helloworldabc","content":"just C","comment_id":"1268292","timestamp":"1724030520.0"}],"comment_id":"1133758","content":"In regards with Efficiency vs. Headroom: I would choose D over C because there will be less headroom during peak loads.","poster":"rajkanch","timestamp":"1706406960.0","upvote_count":"1"},{"content":"SORRY c ANS\nr6g.8xlarge\n\nUpfront cost\n0.00 USD\nMonthly cost\n1,248.01 USD\nTotal 12 months cost\n14,976.12 USD\n\n r6g.4xlarge \n\n624.00 USD\n\nTotal 12 months cost\n7,488.00 USD\nhttps://calculator.aws/#/estimate","poster":"duriselvan","timestamp":"1702617540.0","upvote_count":"1","comment_id":"1097050"},{"upvote_count":"2","timestamp":"1702616400.0","content":"I would suggest that option B is the most cost-effective solution that meets the requirements. It uses m6g.4xlarge instances, which are general purpose instances powered by Arm-based AWS Graviton2 processors. These instances offer a balance of compute, memory, and networking resources, and deliver up to 40% better price performance than comparable current generation x86-based instances5 This option can also reduce the number of instances needed to meet the demand, as each m6g.4xlarge instance has 16 vCPUs and 64 GiB of memory, which is equivalent to one quarter of the resources of an r6g.16xlarge instance. This option can also leverage the existing Network Load Balancer and CloudWatch metrics to monitor and distribute the traffic across the instances.","poster":"duriselvan","comment_id":"1097048"},{"poster":"duriselvan","comment_id":"1095377","upvote_count":"1","timestamp":"1702463160.0","content":"Option D, using r6g.8xlarge instances with a minimum capacity of 2, a desired capacity of 2, and a maximum capacity of 6, is the most cost-effective solution for this scenario. Here's why:\n\nCost reduction: Lower instance size and smaller fleet size significantly reduce cost compared to the current configuration.\nBalanced memory and cost: R6g.8xlarge still provides sufficient memory for current demand while being cheaper than r6g.16xlarge.\nScalability for peak demand: Doubling the capacity up to 6 instances can cater to potential player spikes while remaining within a controlled budget."},{"content":"Selected Answer: C\nOption C","comment_id":"1077987","timestamp":"1700705640.0","poster":"career360guru","upvote_count":"1"},{"timestamp":"1700374020.0","upvote_count":"1","poster":"severlight","comment_id":"1074468","content":"Selected Answer: C\nsee Maria2023's answer"},{"poster":"chico2023","comment_id":"977030","upvote_count":"1","content":"Selected Answer: C\nInitially I was thinking on how the ASG would handle the spikes knowing that each r6g.4xlarge might have troubles handle the load, but the question is to handle the demand in the most cost-effective way.\n\nIn terms of cost, Maria2023 and Nexus2020 made a point that can't be beaten here.\n\nI am still thinking on the load, but if there is something I am learning with these questions is that many of them won't give you enough to make a REAL informed decision, so you should go with your best judgement.","timestamp":"1691615880.0"},{"timestamp":"1690050000.0","comment_id":"959769","upvote_count":"1","content":"Correct C.","poster":"ggrodskiy"},{"poster":"NikkyDicky","upvote_count":"1","content":"Selected Answer: C\nC I guess. weird question","timestamp":"1688748000.0","comment_id":"945870"},{"upvote_count":"1","timestamp":"1687816980.0","content":"Selected Answer: C\nC makes most sense.","poster":"SmileyCloud","comment_id":"934818"},{"comment_id":"933314","timestamp":"1687674540.0","upvote_count":"4","poster":"Maria2023","content":"Selected Answer: C\n1 r6g.4xlarge - $0.8064/h\n1 r6g.8xlarge - $1.6128/h\nDuring peak times both C and D will cost 9.6768/h\nHowever, during non-peak times, C will cost less - 2.4192/h vs 3.2256\nPlus that I think D will be a bit underutilized most of the times if the trends remain the same","comments":[{"timestamp":"1733284020.0","content":"but the auto scaling group have to maintain maximum capability to meet the current setup = ¼ of (12 * r6g.16xlarge) = (3 * r6g.16xlarge) or (12 * r6g.4xlarge). Does comparison on \"non-peak\" make any sense?","comment_id":"1321649","poster":"LuongTo","upvote_count":"1"}]},{"timestamp":"1687578480.0","upvote_count":"1","poster":"shree2023","comment_id":"932137","content":"Selected Answer: C\nMemory optimized and cost optimized"},{"timestamp":"1687572540.0","poster":"Alabi","content":"Selected Answer: D\nThe company initially deployed 12 r6g.16xlarge instances but found that the consumption was much lower than expected. To optimize cost, it is necessary to scale down the instance type while still meeting the demand.\n\nOption D suggests configuring the Auto Scaling group to use r6g.8xlarge instances, which have less memory capacity compared to r6g.16xlarge instances. With a minimum capacity of 2, desired capacity of 2, and maximum capacity of 6, the Auto Scaling group will scale up or down based on CPU and memory utilization.","comment_id":"932105","upvote_count":"1"},{"comment_id":"932094","poster":"gd1","timestamp":"1687571700.0","upvote_count":"1","content":"Selected Answer: C\nThe requirements state that the current set of instances (r6g.16xlarge - memory optimized) are only using about a quarter of the available CPU and memory. Therefore, a smaller instance size would be more cost-effective while still meeting the demand. In this case, the r6g.4xlarge instances would be appropriate, as they are a quarter of the size of the currently used instances (r6g.16xlarge)."}],"isMC":true,"answers_community":["C (97%)","3%"],"topic":"1","question_text":"An entertainment company recently launched a new game. To ensure a good experience for players during the launch period, the company deployed a static quantity of 12 r6g.16xlarge (memory optimized) Amazon EC2 instances behind a Network Load Balancer. The company's operations team used the Amazon CloudWatch agent and a custom metric to include memory utilization in its monitoring strategy.\n\nAnalysis of the CloudWatch metrics from the launch period showed consumption at about one quarter of the CPU and memory that the company expected. Initial demand for the game has subsided and has become more variable. The company decides to use an Auto Scaling group that monitors the CPU and memory consumption to dynamically scale the instance fleet. A solutions architect needs to configure the Auto Scaling group to meet demand in the most cost-effective way.\n\nWhich solution will meet these requirements?","choices":{"C":"Configure the Auto Scaling group to deploy r6g.4xlarge (memory optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.","D":"Configure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances. Configure a minimum capacity of 2, a desired capacity of 2, and a maximum capacity of 6.","B":"Configure the Auto Scaling group to deploy m6g.4xlarge (general purpose) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.","A":"Configure the Auto Scaling group to deploy c6g.4xlarge (compute optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12."},"url":"https://www.examtopics.com/discussions/amazon/view/113023-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1687467660,"answer_description":"","question_images":[],"answer_ET":"C"},{"id":"yGpCaEjylG2zYdi1eQtM","discussion":[{"poster":"chico2023","comment_id":"977041","content":"Selected Answer: D\nIt's D. Purchase Savings Plans in Cost Explorer is not for DynamoDB. At least not today.","upvote_count":"10","timestamp":"1691616540.0"},{"content":"Selected Answer: D\nRepeated lookups = DAX\nAvoid bursts = Provisioned Capacity","poster":"MRL110","comment_id":"967072","upvote_count":"9","timestamp":"1690718820.0"},{"upvote_count":"1","poster":"LeoSantos121212121212121","timestamp":"1742832840.0","comment_id":"1409714","content":"Selected Answer: A\nDAX is not ideal for batch writes"},{"poster":"Dgix","content":"Selected Answer: A\nDAX is more expensive than Elasticache. Therefore, A.","upvote_count":"1","comment_id":"1177540","timestamp":"1710866100.0","comments":[{"poster":"helloworldabc","content":"just D","upvote_count":"1","comment_id":"1268294","timestamp":"1724030580.0"}]},{"content":"DynamoDB Accelerator (DAX): This in-memory cache reduces read latency and improves read throughput for frequently accessed data. Since the application has burst read activity and repeatedly accesses a limited set of keys, DAX can significantly improve performance and reduce costs associated with read throughput on DynamoDB.\nProvisioned capacity mode: While on-demand capacity mode eliminates the need for upfront planning, it can be costly for applications with predictable workloads. Provisioned capacity allows for better cost optimization and predictability by specifying the minimum and maximum capacity required throughout the day.\nDynamoDB auto scaling: This feature automatically adjusts provisioned capacity based on actual usage patterns. This ensures that the table has sufficient capacity during peak hours while avoiding wasted resources during off-peak periods, further reducing costs.","comment_id":"1091552","upvote_count":"8","poster":"duriselvan","timestamp":"1702110600.0"},{"comment_id":"1077990","upvote_count":"1","poster":"career360guru","content":"Selected Answer: D\noption D","timestamp":"1700705940.0"},{"upvote_count":"1","poster":"Just_Ninja","content":"Selected Answer: B\nDid you read the question?\nTo reduce costs you can use DAX.\nhttps://aws.amazon.com/dynamodb/dax/\nHere is nothing in the question about saving plans or else.","timestamp":"1690427820.0","comment_id":"964297","comments":[{"upvote_count":"1","comment_id":"964334","content":"I now switch to D, because it's an expeded workload.","timestamp":"1690432740.0","poster":"Just_Ninja","comments":[{"upvote_count":"2","comment_id":"967362","poster":"rxhan","timestamp":"1690739040.0","content":"looool"}]}]},{"comment_id":"959768","timestamp":"1690049640.0","upvote_count":"1","content":"Correct D.","poster":"ggrodskiy"},{"comment_id":"953399","content":"Selected Answer: C\nThe D looks like a perfect solution. But the question is only asking to reduce the cost, so I would like to choose C instead.","upvote_count":"3","comments":[{"upvote_count":"1","poster":"goodard","content":"Saving plans are not applicable to dynamodb. https://aws.amazon.com/savingsplans/","comment_id":"1256249","timestamp":"1722074940.0"},{"poster":"rxhan","upvote_count":"1","comment_id":"967364","content":"what about caching?","timestamp":"1690739100.0"}],"poster":"achillessatan","timestamp":"1689518580.0"},{"timestamp":"1689117600.0","comment_id":"949350","poster":"rrrrrrrrrr1","content":"Isn't DAX extremely expensive? Weird question.","upvote_count":"1"},{"comment_id":"945874","poster":"NikkyDicky","timestamp":"1688748120.0","upvote_count":"1","content":"Selected Answer: D\nits a D"},{"comment_id":"944122","timestamp":"1688591400.0","poster":"Christina666","content":"Selected Answer: D\nDAX + Provision Capactiy + Auto Scaling meets the need","upvote_count":"1"},{"timestamp":"1687817400.0","content":"Selected Answer: D\nSavings plan is for EC2, B and C are out. A is for read boost. D is correct.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","upvote_count":"4","comment_id":"934821","poster":"SmileyCloud"},{"content":"Selected Answer: D\nDynamoDB Accelerator (DAX) is an in-memory caching service provided by AWS that is specifically designed to enhance the performance of Amazon DynamoDB. It acts as a caching layer between your application and DynamoDB, reducing the need to directly access the DynamoDB service for frequently accessed data.\n\nD!","comment_id":"932929","timestamp":"1687640580.0","upvote_count":"1","poster":"nexus2020"},{"content":"Selected Answer: D\nDAX + Provision Capactiy + Auto Scaling meets the need","timestamp":"1687578780.0","comment_id":"932139","upvote_count":"2","poster":"shree2023"},{"content":"Selected Answer: D\nDeploying DynamoDB Accelerator (DAX) will help in caching read activity, which can reduce the read cost because DAX is a fully managed, highly available, in-memory cache for DynamoDB that can improve the read performance by up to 10 times, even at millions of requests per second.\n\nThe use of provisioned capacity mode allows you to set the capacity for your table to handle expected workloads, and the table's capacity will not scale up and down based on traffic patterns, which could potentially reduce cost when compared to on-demand capacity mode if your usage is predictable.","upvote_count":"1","poster":"gd1","comment_id":"932098","timestamp":"1687571940.0"},{"upvote_count":"2","timestamp":"1687498980.0","content":"Selected Answer: D\nhttps://www.examtopics.com/discussions/amazon/view/80440-exam-aws-certified-solutions-architect-professional-topic-1/","poster":"elanelans","comment_id":"931236"},{"comment_id":"931055","content":"Selected Answer: D\nD provisioned capacity mode. \nAs per charGPT company is currently using on-demand capacity mode. On-demand capacity mode is priced higher than provisioned capacity mode because it automatically accommodates your workload's capacity needs based on the volume of reads and writes your application performs. For workloads with predictable capacity needs, provisioned capacity mode can be more cost effective.","upvote_count":"1","poster":"bhanus","timestamp":"1687468200.0"}],"exam_id":33,"answer_images":[],"question_images":[],"question_id":173,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/113025-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"choices":{"B":"Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer.","C":"Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer.","A":"Deploy an Amazon ElastiCache cluster in front of the DynamoDB table","D":"Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling."},"answers_community":["D (85%)","Other"],"question_text":"A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity mode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts throughout the day. and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.\n\nWhich strategy should a solutions architect recommend to meet this requirement?","answer":"D","unix_timestamp":1687468200,"timestamp":"2023-06-22 23:10:00","answer_description":"","answer_ET":"D"},{"id":"tKegCrCG8yUUwqUNV9XX","topic":"1","question_images":[],"unix_timestamp":1687581900,"choices":{"B":"Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances.","C":"Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.","A":"Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.","D":"Check the security group for the logging service running on EC2 instances to ensure it allows ingress from the clients.","E":"Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets."},"exam_id":33,"answers_community":["AC (63%)","BD (21%)","Other"],"isMC":true,"answer_ET":"AC","question_id":174,"question_text":"A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between the client services and the logging service.\n\nIn each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.\n\nWhich combination of steps should a solutions architect take to resolve this issue? (Choose two.)","answer_images":[],"answer":"AC","discussion":[{"timestamp":"1712576280.0","content":"Selected Answer: AC\nWhen you associate a Network Load Balancer with an endpoint service, the Network Load Balancer forwards requests to the registered target. The requests are forwarded as if the target was registered by IP address. In this case, the source IP addresses are the private IP addresses of the load balancer nodes. If you have access to the Amazon VPC endpoint service, then verify that:\n\n The Inbound security group rules of the Network Load Balancer’s targets allow communication from the private IP address of the Network Load Balancer nodes\n The rules within the network ACL associated with the Network Load Balancer’s targets allow communication from the private IP address of the Network Load Balancer nodes\n\nhttps://repost.aws/knowledge-center/security-network-acl-vpc-endpoint","poster":"magmichal05","comment_id":"1027922","upvote_count":"14"},{"content":"Selected Answer: AC\nA and C.\nThe flow is:\nApplication -> NLB -> Logging Monitor Tool.\nSo we need to check NACL of NLB subnets (in and out from applications client and in and out to EC2 subnet) and Security group (Statefull, so only ingress) of EC2 Instances of Logging Monitor Tool.","poster":"red_panda","upvote_count":"5","comment_id":"1207763","timestamp":"1730971260.0"},{"content":"Selected Answer: CE\nTo resolve connectivity issues between clients using VPC endpoints and the logging service:\nNLB Security Group (Option E):\nThe NLB must allow traffic from the subnets where the client's interface endpoints reside. Since clients connect via PrivateLink, the NLB’s security group must permit ingress from the CIDR blocks of the client’s interface endpoint subnets.\nEC2 Security Group (Option C):\nThe EC2 instances hosting the logging service must allow traffic from the NLB’s subnets. The NLB forwards traffic to the EC2 instances, and their security group must permit ingress from the NLB’s subnet CIDRs (or the NLB’s security group).","timestamp":"1743957120.0","upvote_count":"1","poster":"Longc","comment_id":"1558310"},{"comment_id":"1399561","content":"Selected Answer: BE\nB.- Network ACLs operate at the subnet level and could be blocking traffic between:\n\n The interface endpoints (created in each AWS account) and the logging service's subnets.\n The logging service subnets and the interface endpoint subnets.\n\nAWS PrivateLink uses interface endpoints, and the NACL must allow inbound/outbound traffic between the interface endpoint subnets and the EC2 instances running the logging service.\n\nE.-The interface endpoint in each AWS account connects to the NLB.\nIf the NLB security group does not allow ingress from the interface endpoint subnets, traffic from the clients will be dropped.","upvote_count":"1","poster":"eesa","timestamp":"1742198880.0"},{"comment_id":"1208910","poster":"titi_r","timestamp":"1731171840.0","upvote_count":"2","content":"Selected Answer: AC\nA and C.\nhttps://repost.aws/knowledge-center/security-network-acl-vpc-endpoint"},{"upvote_count":"2","poster":"BrijMohan08","comment_id":"1204260","content":"Selected Answer: BD\nB. Network Access Control Lists (NACLs) act as a firewall at the subnet level. To ensure communication between the interface endpoint subnets and the logging service subnets running on EC2 instances, the NACLs attached to both subnets should be configured to allow the necessary traffic.\n\nD. Security groups act as virtual firewalls at the instance level. To allow clients to submit logs to the logging service running on EC2 instances, the security group associated with the EC2 instances should be configured to allow ingress traffic from the clients' IP addresses or security groups.","comments":[{"upvote_count":"1","comment_id":"1361767","timestamp":"1740550500.0","poster":"altonh","content":"The EC2 will not receive the interface endpoint IP but the NLB's IP instead."}],"timestamp":"1730256960.0"},{"poster":"chelbsik","comment_id":"1145583","content":"Selected Answer: CE\nCE: we only need to allow access from client -> NLB -> application","timestamp":"1723208520.0","upvote_count":"3"},{"comment_id":"1113329","comments":[{"comment_id":"1204293","content":"The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets.","poster":"7f6aef3","timestamp":"1730262540.0","upvote_count":"1"}],"poster":"Mehrannn","upvote_count":"1","content":"Selected Answer: BD\nB&D are correct answers. Rational:\n\nEC2s and NLB are both in one subnet, so the NACL is associated with one subnet and there is no NACL which controls EC2 and NLB communication --> A is not Valid, C is not Valid.\n\nSecurity groups are attached to EC2s --> E is not Valid","timestamp":"1720058940.0"},{"comment_id":"1098345","content":"guys .pls B,E ans \ne:-\nThe Inbound security group rules of the Network Load Balancer’s targets allow communication from the private IP address of the Network Load Balancer nodes","timestamp":"1718550720.0","upvote_count":"1","poster":"duriselvan"},{"timestamp":"1717858440.0","upvote_count":"2","poster":"duriselvan","comment_id":"1091206","content":"CE is ans\nThe clients are trying to connect to the logging service through the NLB.\nThe NLB needs to forward the requests to the EC2 instances running the logging service.\nTherefore, both the NLB and the EC2 instances need to have security group rules allowing inbound traffic from each other's subnets."},{"comment_id":"1089497","timestamp":"1717681260.0","upvote_count":"3","poster":"ayadmawla","content":"Selected Answer: AC\nLink below seems to confirm it. The focus is on the Provider VPC so the question wasn't really that clear. \n\nhttps://repost.aws/knowledge-center/security-network-acl-vpc-endpoint"},{"poster":"career360guru","timestamp":"1716424560.0","content":"Selected Answer: AC\nA and C","comment_id":"1078003","upvote_count":"1"},{"poster":"severlight","comment_id":"1074494","content":"Selected Answer: AC\nsee magmichal05's answer","upvote_count":"1","timestamp":"1716095340.0"},{"comment_id":"1047224","content":"Selected Answer: BE\nB is pretty clear plus E is valid as well since AWS has introduced support for associating security groups with Network Load Balancers (NLBs).","timestamp":"1713474180.0","upvote_count":"1","poster":"dpatra"},{"timestamp":"1713210960.0","content":"Selected Answer: AC\nAC - NLB needs to be allowed to the instances otherwise targets are unhealthy","comment_id":"1044422","poster":"Certified101","upvote_count":"1"},{"poster":"cmoreira","comment_id":"997311","content":"Selected Answer: AC\nAC\n3rd point on https://docs.aws.amazon.com/vpc/latest/privatelink/create-endpoint-service.html#considerations-endpoint-services","upvote_count":"3","timestamp":"1709448300.0"},{"comment_id":"984619","timestamp":"1708277340.0","poster":"vjp_training","content":"Selected Answer: AC\nhttps://www.examtopics.com/discussions/amazon/view/36058-exam-aws-certified-solutions-architect-professional-topic-1/","upvote_count":"4"},{"comment_id":"964300","upvote_count":"4","poster":"Just_Ninja","timestamp":"1706333340.0","content":"Selected Answer: BC\nB and C.\nThe NLB is places in the destination Account. That means the EC2 logging instance get traffic from the NLB.\nSo the source for the Logging EC2 instance must be the NLB.\nhttps://aws.amazon.com/de/blogs/architecture/building-saas-services-for-aws-customers-with-privatelink/\nOld but not outdated"},{"upvote_count":"4","comment_id":"956956","content":"Selected Answer: AC\nWhen service consumers send traffic to a service through an interface endpoint, the source IP addresses provided to the application are the private IP addresses of the load balancer nodes, not the IP addresses of the service consumers.\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/create-endpoint-service.html","timestamp":"1705710780.0","poster":"emupsx1"},{"content":"Foe those selecting options B), D) over A), C). Please note Consumer & Service providers are in different VPCs that are not peered (connected through PrivateLink) & can have overlapping IPs also. You'll not be able to reference SGs across VPCs not peered & even by private IPs which can be overlapping.\n\nA) - We can reference IP of the Network LB with the subnet of EC2s via NACL, though it's allowed by default within VPC unless we want to make this more restrictive.\n\nC) - Network LB itself does not have a SG, but the option states allowing the IP range of CIDR associated with Network LB subnet in the SG associated with the EC2 instances, which is a valid option.\n\nIMO, options B) & D) are feasible only if hundreds of AWS accounts (client services) lie in the same VPC as the logging service, which the question does not seem to state.","timestamp":"1705470240.0","upvote_count":"4","comment_id":"953803","poster":"study_aws1"},{"comment_id":"947710","content":"Selected Answer: AC\nIt's actually AC. \nLogging service will receive traffic from NLB, not from the clients directly. That architecture (PrivateLink endpoint service) allows you to have overlapping CIDR block between client and service provider.","poster":"shacky","upvote_count":"2","timestamp":"1704867360.0"},{"upvote_count":"2","poster":"NikkyDicky","timestamp":"1704653700.0","content":"Selected Answer: BD\nits BD\nno SG for NLB","comments":[{"comment_id":"958641","timestamp":"1705858980.0","upvote_count":"1","poster":"NikkyDicky","content":"after reading newer comments, I'm switching to AC"}],"comment_id":"945879"},{"comment_id":"944126","comments":[{"upvote_count":"1","timestamp":"1719739140.0","comment_id":"1109640","poster":"carpa_jo","content":"Nowadays NLB supports security groups (at least since August 2023):\nhttps://aws.amazon.com/about-aws/whats-new/2023/08/network-load-balancer-supports-security-groups/"},{"content":"You can associate a security group to you NLB. Please find link below:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html\nBased on this, the answer should be B and E.","comment_id":"983856","upvote_count":"2","timestamp":"1708197600.0","poster":"finesse_999"}],"timestamp":"1704496440.0","upvote_count":"1","content":"Selected Answer: BD\nNLB no security groups\n\nAWS Network Load Balancer does not support security groups today. You can use Amazon VPC NACLs, AWS Network Firewall, and/or a marketplace firewall with AWS Gateway Load Balancer to provide various levels of protection for your NLB. You can also use security groups on your targets if client IP preservation is enabled (see more here about when client IP preservation is supported)\n\nhttps://repost.aws/questions/QUuueXAi20QuisbkOhinnbzQ/aws-nlb-security-group","poster":"Christina666"},{"poster":"SmileyCloud","upvote_count":"4","timestamp":"1703636700.0","comment_id":"934834","content":"Selected Answer: BD\nB and D. C is out, you don't configure ingress from NLB subnets, same with E, NLBs dont have security groups. A - same concept. NLB is transparent unlike classic ELB and ALB."},{"comment_id":"932944","upvote_count":"2","timestamp":"1703460300.0","poster":"nexus2020","content":"Selected Answer: BD\nSender (Client 's interface endpoint) --> private link --> Receiver (NLB --> EC2 of Logging service)\n\nso the NACL and Security group are all on the receiver side, therefore it should include the sender (client) ip.\n\nso B & D"},{"timestamp":"1703412240.0","poster":"PhuocT","content":"I think it B and D","upvote_count":"1","comment_id":"932333"},{"content":"Selected Answer: BD\nB&D seems right","upvote_count":"1","poster":"shree2023","timestamp":"1703400300.0","comment_id":"932171"}],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/113152-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-06-24 06:45:00"},{"id":"XHiGREBgO8aimoeF52bN","isMC":true,"question_id":175,"url":"https://www.examtopics.com/discussions/amazon/view/113153-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1687582860,"timestamp":"2023-06-24 07:01:00","answer":"B","question_images":[],"question_text":"A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed frequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS keys (SSE-KMS).\n\nA solutions architect reviews the company’s monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of requests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"answer_description":"","exam_id":33,"choices":{"B":"Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3.","A":"Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C.","D":"Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive.","C":"Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM."},"discussion":[{"poster":"gd1","comment_id":"932564","upvote_count":"15","timestamp":"1703430840.0","content":"Selected Answer: B\nThis option switches the encryption method from using AWS Key Management Service (AWS KMS) to using server-side encryption with S3 managed keys (SSE-S3). This change can significantly reduce costs because AWS KMS charges per API request, while SSE-S3 does not have additional charges per API request beyond the S3 usage."},{"content":"Selected Answer: B\n100% B","timestamp":"1726754340.0","poster":"Oznerol96_","upvote_count":"1","comment_id":"1177517"},{"comment_id":"1102238","upvote_count":"4","timestamp":"1718948160.0","poster":"GoKhe","content":"Bucket key would have been an option here but it is not in the answers."},{"content":"Selected Answer: B\nOption B","poster":"career360guru","upvote_count":"1","comment_id":"1078006","timestamp":"1716424740.0"},{"timestamp":"1709030100.0","upvote_count":"2","poster":"shizhan","content":"B\nhttps://aws.amazon.com/about-aws/whats-new/2020/12/amazon-s3-bucket-keys-reduce-the-costs-of-server-side-encryption-with-aws-key-management-service-sse-kms/","comment_id":"991301"},{"content":"Selected Answer: B\nB...\nBecause SSE-S3 has no additional costs.\nSSE-C cost per month 0,00040 USD per GB encrypted Data on Top","timestamp":"1706334060.0","poster":"Just_Ninja","comment_id":"964302","upvote_count":"2"},{"content":"Correct B.","timestamp":"1705952820.0","upvote_count":"2","poster":"ggrodskiy","comment_id":"959741"},{"comment_id":"946558","timestamp":"1704731340.0","upvote_count":"1","content":"Selected Answer: B\nthis is B","poster":"nicecurls"},{"timestamp":"1704654540.0","upvote_count":"1","comment_id":"945884","content":"Selected Answer: B\nB for sure","poster":"NikkyDicky"},{"upvote_count":"1","poster":"SmileyCloud","content":"Selected Answer: B\nNone of this is correct. https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html, but let's go with B.","comment_id":"934837","timestamp":"1703637240.0"},{"upvote_count":"1","content":"Selected Answer: B\nI would actually expect an option with a bucket key as a possible answer since that's the purpose of it. From the available choices, I choose B.","poster":"Maria2023","comment_id":"933339","timestamp":"1703496420.0"},{"upvote_count":"4","comment_id":"932502","content":"Selected Answer: B\nBy choosing option B, you can switch the encryption type from SSE-KMS to SSE-S3, which eliminates the need for AWS KMS requests, thereby reducing the associated costs. This solution requires minimal changes to the application and avoids additional operational overhead.","poster":"Alabi","timestamp":"1703424840.0"},{"comment_id":"932465","upvote_count":"3","content":"Selected Answer: B\nThe goal here is to reduce the cost related to the usage of AWS KMS keys for server-side encryption. Using SSE-S3, which uses Amazon S3 managed keys for server-side encryption, would eliminate the additional cost related to KMS key usage while still maintaining a high level of security. Amazon S3 handles key management, which also reduces operational overhead. S3 Batch Operations can be used to efficiently copy the existing objects to the new bucket.","poster":"i_am_robot","timestamp":"1703422560.0"},{"content":"B, SSE-S3 does not incur additional costs.","comment_id":"932341","poster":"PhuocT","upvote_count":"2","timestamp":"1703412780.0"},{"poster":"shree2023","comment_id":"932180","content":"Selected Answer: B\nB is the least operational overhead","timestamp":"1703401260.0","upvote_count":"1"}],"answer_ET":"B","answers_community":["B (100%)"],"topic":"1"}],"exam":{"numberOfQuestions":529,"lastUpdated":"11 Apr 2025","isBeta":false,"isImplemented":true,"id":33,"name":"AWS Certified Solutions Architect - Professional SAP-C02","provider":"Amazon","isMCOnly":true},"currentPage":35},"__N_SSP":true}