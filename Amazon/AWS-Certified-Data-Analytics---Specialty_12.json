{"pageProps":{"questions":[{"id":"RCVLUpFzkXY10RbvWW7V","timestamp":"2022-04-25 01:58:00","url":"https://www.examtopics.com/discussions/amazon/view/74396-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_images":[],"question_images":[],"isMC":true,"question_id":56,"answer_description":"","topic":"1","exam_id":20,"answer_ET":"C","question_text":"A company has 10-15 ׀¢׀’ of uncompressed .csv files in Amazon S3. The company is evaluating Amazon Athena as a one-time query engine. The company wants to transform the data to optimize query runtime and storage costs.\nWhich option for data format and compression meets these requirements?","unix_timestamp":1650844680,"answer":"C","discussion":[{"upvote_count":"8","poster":"[Removed]","comment_id":"591306","timestamp":"1650844680.0","content":"Selected Answer: C\nApache parquet"},{"comment_id":"886673","poster":"pk349","content":"C: I passed the test","timestamp":"1682970060.0","upvote_count":"1"},{"comment_id":"786139","timestamp":"1674530460.0","poster":"Mirandaali","upvote_count":"1","content":"Selected Answer: C\nC it is"},{"comment_id":"643500","upvote_count":"4","timestamp":"1659816060.0","content":"Selected Answer: C\nParquet with snappy compression suits the requirement.","poster":"rocky48"}],"choices":{"D":"Apache Avro compressed with LZO","A":"CSV compressed with zip","C":"Apache Parquet compressed with Snappy","B":"JSON compressed with bzip2"},"answers_community":["C (100%)"]},{"id":"5t8DvliYVGVxxa0USXwB","discussion":[{"timestamp":"1632202140.0","comment_id":"160191","comments":[{"comments":[{"timestamp":"1632525120.0","comment_id":"176980","content":"with a Create External Table as Select... I suppose","upvote_count":"1","comments":[{"poster":"DerekKey","content":"Wrong \"Create External Table as Select\" is Redshift command not Spectrum. Spectrum is used to query external as Read Only.","comment_id":"356000","timestamp":"1635585300.0","upvote_count":"1"}],"poster":"Phoenyx89"},{"comments":[{"poster":"Manue","comment_id":"254095","timestamp":"1634707740.0","content":"I think Jh2501 is not challenging Spectrum capability to create external tables, but how/why to create an external table on data which is stored in Redshift and not in S3. So, if there is not a typo in the question, C is a doubtful answer, and A could be the right one.","upvote_count":"5"}],"upvote_count":"3","content":"If you are confused, check https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html","poster":"GeeBeeEl","timestamp":"1633227780.0","comment_id":"178889"},{"poster":"JBAWA","content":"To define an external table in Amazon Redshift, use the CREATE EXTERNAL TABLE command","comment_id":"700485","timestamp":"1666323480.0","upvote_count":"1"}],"content":"Agree C. However, one thing I am still confused about - how can Spectrum create external table for the call centre data whereas it doesn't get stored on S3?","upvote_count":"7","poster":"Jh2501","timestamp":"1632483420.0","comment_id":"175038"},{"timestamp":"1672898220.0","poster":"Merrick","content":"https://aws.amazon.com/ko/premiumsupport/knowledge-center/redshift-spectrum-external-table/","comment_id":"766273","upvote_count":"1"}],"content":"I would go for C, Spectrum is serverless as well. Ques also asks for minimum development effort. For option A, you need to develop Lambda and Glue code.","upvote_count":"39","poster":"abhineet"},{"timestamp":"1634141760.0","comments":[{"comment_id":"255742","poster":"mendelthegreat","upvote_count":"28","content":"1. Redshift Spectrum is a compute layer that sits between S3 and Redshift, so it will not add more load to Redshift \n\n2. What the case is saying is that because Redshift is already under heavy load, we shouldn't load the .CSV data from S3 into redshift, so an external table would be better in Redshift Spectrum \n\n3. The best use case for Redshift Spectrum, as described in the question, is to JOIN data in Redshift with another external data source, which in this case is S3, without having the need to bring everything into Redshift. \n\nC is the undeniable correct answer here","timestamp":"1634953080.0"}],"content":"C doesn't make sense. Call center data is already stored in Redshift. What would be the purpose of creating an external table for the call center data? Also C suggests to perform the join with Redshift which is already under a heavy load.","comment_id":"210769","poster":"jove","upvote_count":"19"},{"timestamp":"1709857740.0","upvote_count":"1","poster":"Kam006","comment_id":"1168454","content":"C is the correct answer. I do agree that table creation is part of RS and external table created to access the NON RS data sources (e.g. S3). External tables allow you to query data in S3 using the same SELECT syntax as with other Amazon Redshift tables. Here, the question says RS is already overloaded, hence we should not load the data in to RS. RS spectrum will join the S3 data along with RS data which is also serverless and minimal development efforts"},{"content":"Selected Answer: C\nC is correct as it is the one with minimal effort as no data is moved.","timestamp":"1700506260.0","comment_id":"1075753","upvote_count":"2","poster":"jerkane"},{"poster":"markstudy","content":"Selected Answer: B\nI would pick B:\n\nA: Lambda is limited to 15 minutes of execution time, might not be enough to unload.\nC: The call center data is already in Redshift, the missing data is the airport data.\n\nBest possible option seems to be B: Unload redshift data and develop something to merge/join data, so redshift doesn't want to run the queries and merge.","upvote_count":"1","comment_id":"1063859","timestamp":"1699275780.0"},{"comment_id":"989735","timestamp":"1692945600.0","content":"Selected Answer: A\nReason why C is incorrect is as it mentions Creating an external table using Amazon Redshift Spectrum for the call center data ( which is already in Redshift) and performing the join with Amazon Redshift (not sure how to join on redshift as the data is already in Redshift) so i guess this incorrect until it is a type regarding the call center data. So my Option will be A","upvote_count":"1","poster":"nroopa"},{"content":"Selected Answer: C\ngoing with C","timestamp":"1690906020.0","comment_id":"969188","poster":"NikkyDicky","upvote_count":"1"},{"upvote_count":"2","poster":"Hyperdanny","content":"I would pick B:\n\nA: Lambda is limited to 15 minutes of execution time, might not be enough to unload. \nC: The call center data is already in Redshift, the missing data is the airport data.\n\nBest possible option seems to be B: Unload redshift data and develop something to merge/join data, so redshift doesn't want to run the queries and merge.","timestamp":"1683727320.0","comment_id":"894056"},{"timestamp":"1683446040.0","poster":"Cloudbert","upvote_count":"1","comment_id":"891232","content":"Selected Answer: A\nC is wrong. To use the CREATE External Table command the data has TO BE IN S3. \"To define an external table in Amazon Redshift, use the CREATE EXTERNAL TABLE command. The external table statement defines the table columns, the format of your data files, and the location of your data in Amazon S3.\" The solution also requires to put as little burden on redshift as possible. Lambda and Glue are serverless and by choosing solution 1 we offload the burden from Redshift completely. Solution A must be correct."},{"poster":"Debi_mishra","upvote_count":"2","content":"I will say none of the answers are exactly correct without additional information such as call centre data structure and volume. D is not correct as its not serverless. C is not correct - you cant create external table when data is in redshift and also it will put load on redshift. A aij ok only if data is small else lambda will timeout Again B can be a problem if data is large as it will put load on redshift.","timestamp":"1683400680.0","comment_id":"890968"},{"comment_id":"886276","content":"C: I passed the test","upvote_count":"1","poster":"pk349","timestamp":"1682946840.0"},{"timestamp":"1677948360.0","poster":"itsme1","content":"Selected Answer: C\n\"Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster.\"\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\n\nC: Only caveat being that external table is not created in redshift-spectrum\n\nA: UNLOAD is faster as oppose to Lambda, however, it also burdens redshift.","upvote_count":"1","comment_id":"829144"},{"timestamp":"1676544660.0","content":"Selected Answer: C\nUse Redshift Spectrum for it!","comment_id":"810561","upvote_count":"1","poster":"[Removed]"},{"upvote_count":"2","comment_id":"741135","poster":"silvaa360","content":"Selected Answer: C\nI think that there is a typo here, instead of call center data it should be airline data. I had the same question in another paid question dump and the answer was the same as C), but will try to see if there is the same typo in there.","timestamp":"1670692860.0"},{"poster":"nadavw","upvote_count":"1","content":"Selected Answer: B\nB seems to be a valid approach, taking into consideration that the load shouldn't be on RedShift, The Glue can export the data from RedShift and run (Spark serverless). \nThis blog explains it (ignore the daatbrew which is just a UI above the architecture):\nhttps://aws.amazon.com/blogs/big-data/data-preparation-using-amazon-redshift-with-aws-glue-databrew/","comment_id":"719434","timestamp":"1668584040.0"},{"content":"Selected Answer: C\nC is the correct answer \n\nA is wrong Although this is a possible solution, it requires a lot of development overhead to build Glue ETL scripts for joining the Redshift and S3 data. A better solution here is to use Amazon Redshift Spectrum","timestamp":"1667575020.0","poster":"cloudlearnerhere","upvote_count":"2","comment_id":"711237"},{"timestamp":"1666641720.0","comment_id":"703333","content":"Selected Answer: C\nC looks simple enough.","upvote_count":"1","poster":"rocky48"},{"comment_id":"702857","content":"Selected Answer: C\nminimal effort is the key!","upvote_count":"1","timestamp":"1666601580.0","poster":"Erso"},{"poster":"rav009","comment_id":"683338","content":"Selected Answer: C\nC very obvious","timestamp":"1664524500.0","upvote_count":"1"},{"content":"Selected Answer: C\nA is incorrect, as we do not need a lambda function to unload data from RedShift to S3. Plus building a lambda function will increase manual effort. C is the best answer here.","upvote_count":"2","poster":"Arka_01","timestamp":"1664080380.0","comment_id":"678416"},{"timestamp":"1662567720.0","poster":"YouYouYou","comment_id":"662691","comments":[{"timestamp":"1663042680.0","poster":"rocky48","upvote_count":"2","comment_id":"667645","content":"1. Redshift Spectrum is a compute layer that sits between S3 and Redshift, so it will not add more load to Redshift\n\n2. What the case is saying is that because Redshift is already under heavy load, we shouldn't load the .CSV data from S3 into redshift, so an external table would be better in Redshift Spectrum\n\n3. The best use case for Redshift Spectrum, as described in the question, is to JOIN data in Redshift with another external data source, which in this case is S3, without having the need to bring everything into Redshift.\n\n4. Redshift Spectrum is serverless and there’s nothing to provision or manage. You just pay for the resources you consume for the duration of your Redshift Spectrum query (https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/#:~:text=Like%20Amazon%20Athena%2C%20Redshift%20Spectrum,of%20your%20Redshift%20Spectrum%20query)\n\nI feel, C might be the correct answer."},{"timestamp":"1669191780.0","upvote_count":"1","poster":"reach2ashish","comment_id":"724993","content":"are you sure, how big is the callcenter data ? because what if the load does not finish in 15 min. That reason alone is enough to rule out lambda. Copy Data on Lambda is a BIG NO. which is why we use glue for ETL."},{"comment_id":"803845","poster":"itsme1","upvote_count":"1","timestamp":"1675990260.0","content":"external table is needed for the data in S3, which is required by redshift spectrum to run query"}],"content":"Selected Answer: A\nunload script is literally 3 lines look here\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD_command_examples.html\njoin is similar to the below example you don't have to write any script\nhttps://www.youtube.com/watch?v=32dy6NYEwuo\n\nAnswer A is the best answer\n\nnow doubts about answer C you don't need to create external table for data that is already inside of redshift this is very obvious plus the question asks for minimal efforts and development activity which is met by using A \nso A is the correct answer no doubt.","upvote_count":"1"},{"poster":"rav009","content":"A.\nC is wrong: Redshift Spectrum is actually using the the nodes of Redshift, it will use the CPU and memory of Redshift nodes, how can you say it does not put load to redshift?","upvote_count":"1","timestamp":"1659682380.0","comment_id":"642797","comments":[{"timestamp":"1669191900.0","poster":"reach2ashish","comment_id":"724994","content":"how can you be sure the dataload will complete in 15 min. no information how big is the callcenter data","upvote_count":"1"}]},{"poster":"maitis","content":"Selected Answer: C\nserverless and is not putting load on redshift","upvote_count":"1","timestamp":"1659498480.0","comment_id":"641553"},{"upvote_count":"1","timestamp":"1659276300.0","content":"Selected Answer: A\nthe primary objective is not put load on Redshift. B and D are obviusly out. C is good answer but it will put all load on Redshift though it is simplest one. So A is right one","comment_id":"640134","poster":"arboles"},{"poster":"rocky48","timestamp":"1658454840.0","content":"Answer-C","upvote_count":"1","comment_id":"634939"},{"content":"A looks correct. You need to get the data out of Redshift as you don't want to put more pressure on it to join dataset with external tables and internal tables for the required output. Hence, use unload to get data into S3 using Lambda. Then use serverless Glue to do whatever you want to do in your ETL","poster":"certificationJunkie","timestamp":"1653180420.0","comment_id":"605061","upvote_count":"1"},{"poster":"Shammy45","comment_id":"602894","upvote_count":"3","timestamp":"1652786640.0","content":"Selected Answer: C\nIts a textbox question, Spectrum is serverless and connects with Redshift"},{"poster":"MWL","upvote_count":"1","comment_id":"597655","timestamp":"1651830000.0","content":"I think it is a type error in question."},{"content":"Answer-C","upvote_count":"2","comment_id":"595313","timestamp":"1651356000.0","poster":"jrheen"},{"timestamp":"1647121620.0","content":"Selected Answer: C\nC ticks the box","upvote_count":"2","poster":"jmensah60","comment_id":"566408"},{"content":"Selected Answer: A\nC is not the answer as it says \"Create an external table using Amazon Redshift Spectrum for the call center data\". Call center data is already in redshift. It is the airline data that's in S3. If C said, create external table for airline data, then that would have been sure shot correct answer.","upvote_count":"4","timestamp":"1646758980.0","poster":"910","comments":[{"poster":"910","timestamp":"1646759040.0","upvote_count":"2","comment_id":"563415","content":"Correct answer >>>>B.\n\nhttps://github.com/aws-samples/aws-etl-microservice-redshift-datalake/blob/master/README.md"}],"comment_id":"563414"},{"timestamp":"1645965960.0","upvote_count":"2","content":"I think C has a typo . if C was \"Create an external table using Amazon Redshift Spectrum for the airlines data (in S3) and perform the join with Amazon Redshift.\" in that case C will be the answer. But with the current options, I vote for B . B and A are the same, A uses lambda to unload the data and B uses Glue, and I think for huge dataset, lambda will fail in the UNLOAD action.","poster":"simo40010","comment_id":"557321"},{"upvote_count":"2","content":"B\nExport to s3 , redshift unload triggered via glue python. The use the glue job to process airline with call center data.\nC: doesn't seem correct or there is typo. Even if spectrum is used all airline data will come to redshit nodes and then will be joined with redshift call center data, this will require lot of redshift resource.","comment_id":"517724","timestamp":"1641407880.0","poster":"Poomi"},{"content":"A\nC is wrong because call center data is already in Redshift, so there is no need to create external table for this data, unless the answer has a typo, it should be creating external table for csv files in S3.\nBetween A & B, B does not say export to where, and Glue Python shell job does not support job bookmark. We don't know how it exports, it might be complicated than Redshift UNLOAD command\nA is right because \"As a part of daily process\", it means each day we only need to unload the data for 1 day, so the Lambda function with 15 mins is enough. In addition, unload is a proper word than export","upvote_count":"1","comment_id":"507473","poster":"npt","timestamp":"1640223240.0","comments":[{"upvote_count":"1","comment_id":"511981","poster":"npt","comments":[{"upvote_count":"1","content":"So confused, change the answer to A again\n\"the solution should take less effort and development activities\"\nI also found an example of using Lambda function to invoke the UNLOAD command, this approach is easier than EXPORT from the Python shell job","timestamp":"1640917500.0","comment_id":"513827","poster":"npt"}],"content":"Change my mind to B\nWe are not sure how long the UNLOAD command will take.\nBelow link demonstrates the way to export data from Redshift to S3 by Glue jobs\nhttps://github.com/aws-samples/aws-etl-microservice-redshift-datalake/blob/master/README.md","timestamp":"1640767860.0"}]},{"poster":"aws2019","upvote_count":"1","content":"Answer C","timestamp":"1637417340.0","comment_id":"482591"},{"content":"I think is c correct. https://aws.amazon.com/pt/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/\nAmazon Redshift Spectrum is a sophisticated serverless compute service. The native Amazon Redshift cluster makes the invocation to Amazon Redshift Spectrum when the SQL query requests data from an external table stored in Amazon S3. You can handle multiple requests in parallel by using Amazon Redshift Spectrum on external tables to scan, filter, aggregate, and return rows from Amazon S3 into the Amazon Redshift cluster. All these operations are performed outside of Amazon Redshift, which reduces the computational load on the Amazon Redshift cluster and improves concurrency. In addition, Amazon Redshift Spectrum scales intelligently. Based on the demands of your queries, Amazon Redshift Spectrum can potentially use thousands of instances to take advantage of massively parallel processing (MPP).","comment_id":"434159","poster":"Marcinha","upvote_count":"1","timestamp":"1636303680.0"},{"upvote_count":"2","timestamp":"1636161600.0","poster":"Gekko","comment_id":"405522","content":"A...\nHi, the question says the following requirement:\n- perform a daily batch process\n- The solution must be managed, serverless.. and minimize the load on the existing Redshift cluster.\n- Minimal development effort.\n\nD is not correct because EMR is not serverless\nC is not correct because we must minimize the load on the existing Redshift cluster and the external table should not be created for the call center data, maybe for csv data. Furthermore, this solution is not a batch job.\nB is not correct because \"Python shell in AWS Glue\" is much labor configuring parameters in glue & coding than doing directly with Lambda. Furthermore, shell job can't use job bookmarks.\nA is correct because Lambda is serverless and requires much less effort than Python shell."},{"comment_id":"387709","upvote_count":"3","content":"Let me see. A is wrong because upload all DB to S3 and using lambda function here is not optimal solution. Lambda just run in 15mins and has limited memory. Development effort is bigger.\nC is clearly wrong with \"Spectrum for the call center data\" not the .csv. Spectrum only work with S3 as datasource.\nD is wrong. It might be OK but costly and can impact current load of Redshift and also development effort is bigger.\n\nI go with B because when I can use my python script to export the data I want based on the join condition in the CSV not the whole DB, store it in python sell spark (memory). This is not perfect but still minimize the load on Redshift. This seems to be balanced solution.","poster":"Huy","timestamp":"1636128900.0"},{"comment_id":"386241","timestamp":"1635740160.0","content":"Answer C. Create an external table using Amazon Redshift Spectrum for the call center data and perform the join with Amazon Redshift.","poster":"Donell","upvote_count":"1"},{"poster":"Shraddha","upvote_count":"2","content":"This is a free score question. Remember Redshift Spectrum is serverless. So answer is C","comment_id":"383538","timestamp":"1635620880.0"},{"upvote_count":"4","content":"Answer is B. \nC is not the answer as it says \"Create an external table using Amazon Redshift Spectrum for the call center data\". Call center data is already in redshift. It is the airline data that's in S3. If C said, create external table for airline data, then that would have been sure shot correct answer. \nNow in this case, best answer seems to be B.","timestamp":"1635299400.0","comment_id":"334383","poster":"SuperSundra"},{"comment_id":"285173","content":"B.\n\nA. Lambda's maximum running time is 900s.\nC. Redshift Spectrum is used to link data stored in S3.\nD. EMR is not a serverless service and is complicated","poster":"Exia","upvote_count":"4","timestamp":"1635273120.0"},{"poster":"Pruthvi","timestamp":"1635178020.0","content":"If C doesn't have a typo its the best answer - Spectrum is ideal for this scenario. \nB works as well - its possible to execute redshift command from glue using python shell - thus allowing it to move data out of redshift and use with glue. Glue is also ideal for daily jobs. \nhttps://github.com/aws-samples/amazon-redshift-commands-using-aws-glue","comment_id":"277528","upvote_count":"3"},{"upvote_count":"2","poster":"lostsoul07","comment_id":"274253","content":"C is the right answer","timestamp":"1635151320.0"},{"comment_id":"255588","upvote_count":"3","timestamp":"1634913960.0","poster":"Manue","content":"I have doubts between A and C. I think the key point of this question is \"as part of a dally batch process.\". This makes me feel like it should be Glue, since Redshift is not designed for periodic ETL. I do not feel confortable with using Lambda to move data from Redshift to Amazon, but I think it is the most accurate answer.\nOn the other hand, C is inconsistent when asking to create an external table on call centre data which is already stored in Redshift. If this is not a typo, it definetly discards C as a possible answer."},{"poster":"hoty","content":"C doesnt qualify because load on the redshift remain same with spectrum because spectrum is used for query in S3","comments":[{"timestamp":"1635609720.0","poster":"DerekKey","content":"Wrong. Spectrum has external processing to Redshift. It doesn't impact Redshift perfomrance.","comment_id":"356003","upvote_count":"1"}],"upvote_count":"1","comment_id":"247004","timestamp":"1634693340.0"},{"poster":"SumaD2020","content":"C is the right answer to avoid the development effort.","comment_id":"243104","timestamp":"1634590020.0","upvote_count":"2"},{"timestamp":"1634541060.0","comment_id":"234985","poster":"hans1234","content":"\"The Amazon Redshift cluster is already under a heavy load. The solution must be managed, serverless, well- functioning, and minimize the load on the existing Amazon Redshift cluster.\"\n\nThis hints to NOT C and therefore its A.","upvote_count":"2"},{"poster":"BillyC","content":"C is correct !","comment_id":"216821","timestamp":"1634532900.0","upvote_count":"2"},{"content":"Answer is C. Check Simplifying ETL pipelines in the link - https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/","poster":"dfaola","comment_id":"214600","upvote_count":"2","timestamp":"1634355960.0"},{"poster":"sanjaym","content":"Answer should be C.","upvote_count":"1","comment_id":"204676","timestamp":"1633838760.0"},{"poster":"syu31svc","upvote_count":"1","timestamp":"1633807620.0","comment_id":"192833","content":"Redshift Spectrum allows the ability to query and join across Redshift and S3. Also it is managed, serverless and well-functioning and does not load the existing Redshift cluster.\nAnswer is C"},{"upvote_count":"6","comment_id":"179101","content":"I think the answer is C, but something is wrong ... the call center data already In Redshit I think they should create a spectrum table for the airline data","comments":[{"upvote_count":"4","poster":"Paitan","content":"You are right. Maybe its a typo :-)","comment_id":"179658","timestamp":"1633475460.0"}],"timestamp":"1633244340.0","poster":"KoMo"},{"timestamp":"1632537720.0","poster":"Karan_Sharma","upvote_count":"1","content":"Option C, Keys in question are minimal load on Redshift and serverless with minimum dev effort. Having the redshift table unloaded will add to the existing heavy load that the cluster has. Having and external table created will not invoke any additional storage and will enhance the query performance. GLue/lamba is also serverless but it will require more development effort than creating an external table.","comment_id":"177482"},{"comment_id":"175267","timestamp":"1632491280.0","upvote_count":"2","poster":"Paitan","content":"Redshift Spectrum and external table for sure."},{"poster":"GauravM17","comments":[{"comments":[{"comment_id":"175035","timestamp":"1632451680.0","upvote_count":"2","content":"Spectrum should be serverless. \nhttps://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/","poster":"Jh2501"},{"timestamp":"1633992300.0","comment_id":"205903","poster":"jove","upvote_count":"2","content":"According to AWS, Spectrum is serverless : https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/#aws-comment-trigger-2634:~:text=Like%20Amazon%20Athena%2C%20Redshift%20Spectrum%20is%20serverless"}],"upvote_count":"4","content":"spectrum isn't serverless. It uses your cluster resource and the redshift cluster is already on heavy load","poster":"bigollo","timestamp":"1632326580.0","comment_id":"174808"}],"timestamp":"1632211920.0","content":"I believe C is the answer. Spectrum is serverless and fits well on this use case","upvote_count":"4","comment_id":"174080"},{"comments":[{"comment_id":"259918","upvote_count":"2","comments":[{"comment_id":"340017","timestamp":"1635403980.0","upvote_count":"1","content":"wrong. External table should be created for airline data, not call center data. Answer C is wrong here. It should be B.","poster":"sivajiboss"}],"content":"Incorrect. C is a better answer with RedShift Spectrum. RedShift Spectrum is a serverless solution that can run on-top of your existing RedShift cluster (without leveraging its existing resources)","poster":"Glendon","timestamp":"1634953560.0"}],"upvote_count":"4","content":"Answer: A. Reducing load on existing Redshit, and using serverless (Lambda and Glue) are the keys here.","poster":"singh100","comment_id":"159268","timestamp":"1632173220.0"}],"answers_community":["C (69%)","A (24%)","7%"],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/28727-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"answer_images":[],"isMC":true,"answer_description":"","answer":"C","unix_timestamp":1597585080,"answer_ET":"C","question_text":"An airline has .csv-formatted data stored in Amazon S3 with an AWS Glue Data Catalog. Data analysts want to join this data with call center data stored in\nAmazon Redshift as part of a dally batch process. The Amazon Redshift cluster is already under a heavy load. The solution must be managed, serverless, well- functioning, and minimize the load on the existing Amazon Redshift cluster. The solution should also require minimal effort and development activity.\nWhich solution meets these requirements?","topic":"1","timestamp":"2020-08-16 15:38:00","question_id":57,"choices":{"B":"Export the call center data from Amazon Redshift using a Python shell in AWS Glue. Perform the join with AWS Glue ETL scripts.","D":"Export the call center data from Amazon Redshift to Amazon EMR using Apache Sqoop. Perform the join with Apache Hive.","A":"Unload the call center data from Amazon Redshift to Amazon S3 using an AWS Lambda function. Perform the join with AWS Glue ETL scripts.","C":"Create an external table using Amazon Redshift Spectrum for the call center data and perform the join with Amazon Redshift."}},{"id":"rQTvCyCIDbym3sAbVxiV","discussion":[{"upvote_count":"7","timestamp":"1651059540.0","comment_id":"593112","content":"Selected Answer: B\nAmazon Redshift workload management (WLM) enables users to flexibly manage priorities within workloads so that short, fast-running queries won't get stuck in queues behind long-running queries.\n\nfrom https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html","poster":"Teraxs"},{"timestamp":"1682970120.0","upvote_count":"1","poster":"pk349","content":"B: I passed the test","comment_id":"886675"},{"content":"Selected Answer: B\nAnswer should be B","comment_id":"633854","upvote_count":"2","timestamp":"1658294580.0","poster":"rocky48"},{"timestamp":"1658052780.0","comment_id":"632529","poster":"arboles","upvote_count":"1","content":"Selected Answer: B\nfor sure"},{"upvote_count":"1","poster":"Ramshizzle","timestamp":"1655275920.0","comment_id":"616573","content":"Selected Answer: B\nB is the correct answer. It is the Redshift method for queueing queries."},{"comment_id":"608243","upvote_count":"1","timestamp":"1653708000.0","poster":"Bik000","content":"Selected Answer: B\nI think Answer should be B"},{"timestamp":"1651347600.0","content":"B- Correct","comment_id":"595236","poster":"jrheen","upvote_count":"1"},{"comment_id":"589271","content":"Selected Answer: B\nConfigure automatic workload management (WLM) from the Amazon Redshift console.","poster":"rb39","upvote_count":"4","timestamp":"1650538560.0"}],"choices":{"B":"Configure automatic workload management (WLM) from the Amazon Redshift console.","D":"Run the VACUUM command for all tables in the database.","C":"Create Amazon Simple Queue Service (Amazon SQS) queues with different priorities. Assign queries to a queue based on priority.","A":"Create partitions in the tables queried in ad-hoc queries."},"answer_description":"","question_text":"A company uses Amazon Redshift to store its data. The reporting team runs ad-hoc queries to generate reports from the Amazon Redshift database. The reporting team recently started to experience inconsistencies in report generation. Ad-hoc queries used to generate reports that would typically take minutes to run can take hours to run. A data analytics specialist debugging the issue finds that ad-hoc queries are stuck in the queue behind long-running queries.\nHow should the data analytics specialist resolve the issue?","url":"https://www.examtopics.com/discussions/amazon/view/74010-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1","unix_timestamp":1650538560,"answer_ET":"B","timestamp":"2022-04-21 12:56:00","exam_id":20,"question_id":58,"answers_community":["B (100%)"],"answer":"B","question_images":[],"isMC":true,"answer_images":[]},{"id":"4i6ge4hzdIDtFuhG6KQQ","url":"https://www.examtopics.com/discussions/amazon/view/74297-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_images":[],"isMC":true,"answer_ET":"C","choices":{"B":"Ingest the data into Amazon DynamoDB by using an Amazon API Gateway API as a DynamoDB proxy. Use an AWS Step Functions workflow to create a transient Amazon EMR cluster every hour and process the new data from DynamoDB. Output the processed data to Amazon Redshift to run analytics calculations. Archive the data from Amazon Redshift after 1 year.","C":"Ingest the data into Amazon Kinesis Data Streams by using an Amazon API Gateway API as a Kinesis proxy. Run Amazon Kinesis Data Analytics on the stream data. Output the processed data into Amazon S3 by using Amazon Kinesis Data Firehose. Use Amazon Athena to run analytics calculations. Use S3 Lifecycle rules to transition objects to S3 Glacier after 1 year.","D":"Write the data from the application into Amazon S3 by using Amazon Kinesis Data Firehose. Use Amazon Athena to run the analytics on the data in Amazon S3. Use S3 Lifecycle rules to transition objects to S3 Glacier after 1 year.","A":"Use Amazon Cognito to write the data from the application to Amazon DynamoDB. Use an AWS Step Functions workflow to create a transient Amazon EMR cluster every hour and process the new data from DynamoDB. Output the processed data to Amazon Redshift for analytics. Archive the data from Amazon Redshift after 1 year."},"topic":"1","question_id":59,"answer_description":"","answers_community":["C (72%)","D (28%)"],"timestamp":"2022-04-24 10:17:00","question_images":[],"discussion":[{"timestamp":"1670837520.0","comment_id":"742606","content":"Selected Answer: C\nNear/Real-time analytics on live data is always done via KDA. Once the data is stored it cannot be seen as live data anymore.\nArguments regarding operational overhead are relevant and will also mix things up for me, but I think here we need KDA.","upvote_count":"11","poster":"silvaa360"},{"poster":"MWL","timestamp":"1651107720.0","content":"Selected Answer: C\nI am not clear about C or D.\nFor C: it use API gateway as a proxy. ALthough it can, as mentioned beow by \"Teraxs\". But Is it a good idea to use API gateway for streaming data? For it's pricing, it will cost about for every million message. If the company has millions of customers, uploading activity stream every second. The cost will be huge.\nFor D: The question says they need analyze live data. So it needs kinesis analystics. But D doesn't mention about that. And answer D use kinesis firehose, it is also not near real-time.\nBut, anyway, C will meet the requirement, but D cannot. So I choose C.","comment_id":"593447","upvote_count":"7"},{"content":"FSDFASDF","poster":"Garavirod","timestamp":"1707859860.0","upvote_count":"1","comment_id":"1149602"},{"comment_id":"1063512","content":"D is not correct.\nD cant solve realtime anallitics.","upvote_count":"1","timestamp":"1699246680.0","poster":"LocalHero"},{"content":"I vote for D. D has the least operational overhead and fulfill the purpose. C need to manage KDS shard, API gateway, etc.","timestamp":"1690267860.0","upvote_count":"2","poster":"penguins2","comment_id":"962415"},{"upvote_count":"3","content":"Selected Answer: C\n\"the processed data must be stored\", so it is C. D stores the raw data directly to S3","poster":"ccpmad","timestamp":"1688723340.0","comment_id":"945542"},{"comment_id":"942346","upvote_count":"2","timestamp":"1688438460.0","poster":"wally_1995","content":"Stored PROCESSED data, not raw data, hence C instead of D!"},{"timestamp":"1682970180.0","comment_id":"886678","poster":"pk349","upvote_count":"2","content":"D: I passed the test"},{"upvote_count":"2","timestamp":"1679180280.0","poster":"rsn","content":"Selected Answer: C\nI will go with C. Though D has least operational overhead, Athena can't be considered as the solution for Analytics","comment_id":"843230"},{"upvote_count":"2","content":"D sounds right","poster":"hughnguyen","comment_id":"789079","timestamp":"1674763320.0"},{"timestamp":"1673168220.0","content":"Selected Answer: D\nD has the least operational overhead","comment_id":"769221","upvote_count":"3","poster":"Ody__"},{"upvote_count":"1","poster":"b33f","timestamp":"1667957220.0","content":"Selected Answer: C\nI vote for C. \n\nBecause the company needs to perform analytics on LIVE data, I don't think D can be the answer. D analyzes the data after it is stored in S3 and does not perform analytics on live data.","comment_id":"714208"},{"comment_id":"700130","content":"C because it clearly says the company wants to ingest and perform analytics near-real time then the processed that is stored. So its KDA for near-real time analtics, although KDS is real-time, but it can also deliver near-real time.","upvote_count":"1","poster":"MultiCloudIronMan","timestamp":"1666284660.0"},{"comment_id":"689814","timestamp":"1665278340.0","poster":"jazzok","upvote_count":"2","content":"The key to selecting C instead of D is: \"the processed data must be stored…\". In C, the data has been processed through KDA, then saved in S3. But in D, data is from the application to S3 directly, not being processed yet, Athena comes later. So I go with C."},{"timestamp":"1663180020.0","content":"Selected Answer: D\nfor the purpose of the exam, near-real time is generally KDF. Have to agree that arguments can be made for C","upvote_count":"1","poster":"he11ow0rId","comment_id":"669258"},{"comment_id":"633836","comments":[{"upvote_count":"1","comment_id":"633837","content":"Assuming \"LEAST amount of operational overhead\", D seems to be the best option.","poster":"rocky48","timestamp":"1658292060.0"}],"poster":"rocky48","content":"Selected Answer: D\nOption D","timestamp":"1658291700.0","upvote_count":"3"},{"content":"Selected Answer: D\nFor near real-time and with least operational overhead D is better answer","comment_id":"632521","timestamp":"1658051040.0","upvote_count":"2","poster":"arboles"},{"poster":"nawabhasan","upvote_count":"2","comment_id":"631763","timestamp":"1657891200.0","content":"Selected Answer: C\nData has to be submitted from a mobile device. So the specific application should write data through the API to Amazon kinesis Data Stream and then to the Firhose. So C is the right answer. Sending data from Kinesis Agent is also possible and you can get into the near real time and can select but for this use case answer D will be wrong"},{"timestamp":"1655630820.0","upvote_count":"3","content":"Selected Answer: D\nSince the application can be \"near real time\" it is safe to use Firehose instead of Kinesis Streams. And its less complicated and scalable leading to less operational overhead.","comment_id":"618604","poster":"dushmantha"},{"content":"Selected Answer: C\none of the requirements is an analysis of real-time data. For this we need Kinesis Analytics.","poster":"Ramshizzle","comment_id":"612593","timestamp":"1654580160.0","comments":[{"comments":[{"poster":"rocky48","comment_id":"660706","content":"Answer should satisfy LEAST Operational Overhead, D has the least amount of components to satisfy this requirement. Option C has these components :-\nKinesis Data Streams + Amazon Gateway API + Kinesis Data Analytics + S3 + Data Firehose + Athena.","upvote_count":"2","timestamp":"1662430080.0"}],"upvote_count":"1","timestamp":"1662429840.0","comment_id":"660701","content":"Its not real-time, its mentioned as live data in near real-time.","poster":"rocky48"}],"upvote_count":"4"},{"poster":"certificationJunkie","upvote_count":"2","comments":[{"poster":"dushmantha","timestamp":"1655630700.0","upvote_count":"1","content":"That is not true. We can connect our applications through Kinesis Agent in linux machine to Fireshorse.\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-name.html#:~:text=Kinesis%20Agent%20(linux)","comment_id":"618603"},{"content":"We can configure kinesis firehose in kinesis agent of machine. so the application can write data into firehose with kinesis data streams.","comment_id":"613709","timestamp":"1654765200.0","upvote_count":"2","poster":"imukesh"},{"content":"https://docs.aws.amazon.com/firehose/latest/dev/writing-with-sdk.html","comment_id":"845744","timestamp":"1679392320.0","upvote_count":"1","poster":"akashm99101001com"}],"content":"It can't be D. How would you connect Firehose directly with your application? It has to go via Kinesis Datastream and Kinesis Analytics first.","timestamp":"1653102060.0","comment_id":"604697"},{"comment_id":"593745","poster":"finnliang","timestamp":"1651141560.0","upvote_count":"3","content":"Selected Answer: C\nvote for c"},{"comment_id":"593075","upvote_count":"2","timestamp":"1651056240.0","poster":"Teraxs","comments":[{"content":"Correction, C is probably right: there is documentation for C https://docs.aws.amazon.com/de_de/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html\nAlso, near real-time analytics is required which is not done in D.","poster":"Teraxs","timestamp":"1651056540.0","upvote_count":"6","comment_id":"593078"}],"content":"A and B are not near real-time, C assumes an API Gateway Kinesis Proxy that I couldn't find in the documentation. D sounds reasonable, Firehose also has less operation overhead as Data Streams and data can be put into Firehose via e.g. an the AWS SDK from the application https://docs.aws.amazon.com/firehose/latest/dev/basic-write.html"},{"upvote_count":"1","timestamp":"1651023960.0","poster":"Vietcloud","comment_id":"592758","content":"vote for D"},{"content":"I opt for D. \nNear-real-time refers to Firehose. And its a ELT process not a ETL process so no requirement for Kinesis Analytics.","upvote_count":"1","poster":"VJ_RV","timestamp":"1651013220.0","comment_id":"592702"},{"upvote_count":"1","content":"Selected Answer: C\nData ingestion, data processing , store the data and keep it for long term","timestamp":"1650936420.0","comment_id":"592037","poster":"[Removed]"},{"timestamp":"1650932640.0","content":"Selected Answer: C\nC is right","comment_id":"592021","poster":"AWSRanger","upvote_count":"2"},{"timestamp":"1650788220.0","content":"Selected Answer: D\nB & C ruled out as API gateway not designed for ingesting streams of data.\nWould opt for D.","upvote_count":"2","poster":"astalavista1","comment_id":"590935"}],"answer":"C","question_text":"A company provides an incentive to users who are physically active. The company wants to determine how active the users are by using an application on their mobile devices to track the number of steps they take each day. The company needs to ingest and perform near-real-time analytics on live data. The processed data must be stored and must remain available for 1 year for analytics purposes.\nWhich solution will meet these requirements with the LEAST operational overhead?","unix_timestamp":1650788220,"exam_id":20},{"id":"Qvp7SFhJ1qHyBvKzn2nc","isMC":true,"discussion":[{"comments":[{"poster":"rocky48","content":"requirement => LOWEST latency from message ingestion to delivery ??\nHow will Option D be better over B ?","timestamp":"1671452940.0","comments":[{"comment_id":"749815","upvote_count":"1","timestamp":"1671453060.0","poster":"rocky48","content":"1 or 20 Fan-out, it'll still be the same latency from message ingestion to delivery. Right ??"}],"comment_id":"749813","upvote_count":"1"}],"comment_id":"742632","timestamp":"1670838960.0","content":"I would pick D over B.\n\n1) The question is asking for the LEAST latency. It doesn't matter if we will have high costs, setup or complexity.\n2) What we will be the benefit of having 1 lambda (aka 1 consumer) with EFO? The EFO exists exatly to be possible to have multiple consumers without one affecting each other. 3) The parallelism factor will increase throughput, but we will still have 1 consumer getting data in parallel from the same shard, it is not like we will have multiple lambda executions writting to different HTTP endpoints.\n4) With 1 lambda, we will need to write to the HTTP sequentially, like http1.write, http2.write, etc.\n5) To finalize, I think that EFO with one consumer is irrelevant, because you will get 2MB/s even without EFO.\n\nI would go for D all the way. \n\nPlease, if there is people going around this, just share your thoughts.","poster":"silvaa360","upvote_count":"12"},{"comment_id":"846263","timestamp":"1679425500.0","poster":"mawsman","upvote_count":"8","content":"Selected Answer: B\nEnhanced fan-out is an Amazon Kinesis Data Streams feature that enables consumers to receive records from a data stream with dedicated throughput of up to 2 MB of data per second per shard. When using Lambda as an enhanced fan-out consumer, you can use the Event Source Mapping Parallelization Factor to have one Lambda pull from one shard concurrently with up to 10 parallel invocations per consumer. A consumer that uses enhanced fan-out doesn't have to contend with other consumers that are receiving data from the stream. our data cap is up to 500kb, meaning we have the ability to pull at least 4 messages per shard per second. The 10 parralel invocations would mean that we can call up to 10 shards in parallel and thus consume a minimum of 40 messages at the maximum message size as defined in the question. Thus, one lambda enhanced fanout consumer should be enough to route to 20 destinations concurrently and 20 lambdas would be waaay too many, hence B","comments":[{"upvote_count":"1","comment_id":"1040746","poster":"rlnd2000","content":"What about ordering? With option B will be very difficult to maintain the order in case of a failure, timeout I think D is a better option","timestamp":"1697034180.0","comments":[{"upvote_count":"1","content":"When using Lambda as an enhanced fan-out consumer, you can use the Event Source Mapping Parallelization Factor to have one Lambda pull from one shard concurrently with up to 10 parallel invocations per consumer. Each parallelized invocation contains messages with the same partition key (HotelCode) and maintains order . https://aws.amazon.com/vi/blogs/architecture/field-notes-how-to-scale-opentravel-messaging-architecture-with-amazon-kinesis-data-streams/","timestamp":"1712498760.0","poster":"TheEnquirer","comment_id":"1191002"}]}]},{"content":"Selected Answer: D\nBing said answer is D :)","timestamp":"1711352520.0","upvote_count":"2","poster":"tsangckl","comment_id":"1182285"},{"upvote_count":"1","timestamp":"1700645100.0","poster":"roymunson","comment_id":"1077169","content":"I've heard that the enhanced fan out feature only works for http/2."},{"timestamp":"1699460640.0","poster":"michalf84","comment_id":"1065759","content":"I am confused with lambda payload limit that is 256kb thhat is greater than message size","upvote_count":"1"},{"timestamp":"1697034480.0","content":"Selected Answer: D\nIn my opinion this two requirements make me select D\n\"The performance of a single destination HTTP endpoint should not impact the performance of the delivery for other destinations.\"\n\nThis requirement underscores the need for a design that isolates the message delivery paths to different endpoints. If a single Lambda function were responsible for all destinations, any slow response or failure at one endpoint could cause delays or retries that would affect the entire function, impacting the delivery performance for other endpoints. This is undesirable because the behavior or performance of one endpoint should not influence the others and Message Ordering, Each function maintains the order for its own endpoint, reducing complexity and the risk of out-of-order delivery that might arise when one function handles multiple endpoints.","comment_id":"1040752","upvote_count":"2","poster":"rlnd2000"},{"comment_id":"942368","content":"B does not address the requirement: The performance of a single destination HTTP endpoint should not impact the performance of the delivery for other destinations. As one single lambda would send http put requests one after the other.\n\n20 independent destinations = 20 consumers\n\nso I say D","poster":"wally_1995","timestamp":"1688441220.0","comments":[{"comment_id":"942374","timestamp":"1688441400.0","content":"Also with one lambda you get 10 concurrent lambdas to consume from the same shard,\nwith 20 lambdas you get 200 concurrent lambdas for the entire stream this would process the stream way faster and reduce latency. Question does not mention costs.","upvote_count":"1","poster":"wally_1995"}],"upvote_count":"2"},{"poster":"pk349","comment_id":"886679","timestamp":"1682970180.0","upvote_count":"2","content":"B: I passed the test"},{"comment_id":"843328","poster":"rsn","timestamp":"1679188740.0","upvote_count":"7","content":"Selected Answer: D\nThe key requirement is \"The performance of a single destination HTTP endpoint should not impact the performance of the delivery for other destinations.\""},{"timestamp":"1678302240.0","comment_id":"833292","poster":"Ashas","content":"Answer: D\nI think it's D because\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/aws-lambda-supports-kinesis-data-streams-enhanced-fan-out-and-http2/\n\nenhanced-fan-out with one lambda doesn't makes sense and logically incorrect statement.","upvote_count":"2"},{"poster":"Arjun777","timestamp":"1674338040.0","comment_id":"783766","content":"KDS for near real time ? for low size .. KDS is for real time and min 10GB of data streaming ?? \nconsidering its near real time - shouldnt the answer be firehose ?","upvote_count":"1"},{"content":"Selected Answer: B\nThe question is: does 20 HTTP Endpoint destinations considered as one consumer or 20 consumers? \n\nFan-out lambda can support up to 5 consumers so 20 HTTPS endpoints consider 20 consumers, you will need to establish at least 4 Fan-out lambda functions, which rule out B.\n\nHowever, these 20 HTTP endpoints should consider as one consumer, they are going to the same place and this place has 20 endpoints. So one Fan-out should be good enough instead of 20.\nLink: https://aws.amazon.com/blogs/compute/increasing-real-time-stream-processing-performance-with-amazon-kinesis-data-streams-enhanced-fan-out-and-aws-lambda/","timestamp":"1673549340.0","poster":"Chelseajcole","comment_id":"773794","upvote_count":"4"},{"upvote_count":"3","comment_id":"740680","content":"Ans- B\nLambda enhanced fan-out consumers\nEnhanced fan-out consumers can increase the per shard read consumption throughput through event-based consumption, reduce latency with parallelization, and support error handling. The enhanced fan-out consumer increases the read capacity of consumers from a shared 2 MB per second, to a dedicated 2 MB per second for each consumer.\n\nWhen using Lambda as an enhanced fan-out consumer, you can use the Event Source Mapping Parallelization Factor to have one Lambda pull from one shard concurrently with up to 10 parallel invocations per consumer. Each parallelized invocation contains messages with the same partition key (HotelCode) and maintains order. The invocations complete each message before processing with the next parallel invocation.\n\n\nFigure 8: Lambda fan-out consumers with parallel invocations, maintaining order","timestamp":"1670646720.0","poster":"henom"},{"poster":"rudramadhu","comments":[{"comment_id":"849443","poster":"akashm99101001com","content":"The blog has a comparison that uses 3 lambda function in EFO.\n\nComparing methods\nTo demonstrate the advantage of Kinesis Data Streams enhanced fan-out, I built an application with a single shard stream. It has three Lambda functions connected using the standard method and three Lambda functions connected using enhanced fan-out for consumers. I created roughly 76 KB of dummy data and inserted it into the stream at 1,000 records per second. After four seconds, I stopped the process, leaving a total of 4,000 records to be processed.\n\nMeans we need 20 EOF, max limit for EOF is 20 so that satisfies the max\nhttps://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html","upvote_count":"2","timestamp":"1679672760.0"}],"content":"Answer B - https://aws.amazon.com/blogs/compute/increasing-real-time-stream-processing-performance-with-amazon-kinesis-data-streams-enhanced-fan-out-and-aws-lambda/\nrefer \"Enhanced fan-out with Lambda functions\"","timestamp":"1660197960.0","upvote_count":"4","comment_id":"645291"},{"poster":"arboles","timestamp":"1659274680.0","content":"I have concerns regarding B, 1 lamda function processing data and sending to 20 HTTP endpoints seems bit complicated and fan out for 1 lambda seems none necessary I think. From the other side in D haveing 20 lambdas that send to each point puts unnecesary load on kinesis, I think good case would be to have severl lambdas , for example 5 , each for 4 endpoints","comment_id":"640122","upvote_count":"3"},{"upvote_count":"2","comment_id":"634938","timestamp":"1658454540.0","content":"Selected Answer: B\nAnswer-B","poster":"rocky48"},{"comment_id":"626539","timestamp":"1656849660.0","content":"Why would you want to use enhanced fan out consumer in answer B? there is only a single consumer and there are no benefits in using this feature.\nIn answer D, using 20 lambda functions is a waste, but with enhanced consumer you can be sure that you will have not throttling problems.","poster":"Alekx42","upvote_count":"1"},{"comment_id":"595311","poster":"jrheen","content":"Answer-B","upvote_count":"1","timestamp":"1651355760.0"},{"poster":"bamosk","content":"B\nWhen using Lambda as an enhanced fan-out consumer, you can use the Event Source Mapping Parallelization Factor to have one Lambda pull from one shard concurrently with up to 10 parallel invocations per consumer.\n\nhttps://aws.amazon.com/vi/blogs/architecture/field-notes-how-to-scale-opentravel-messaging-architecture-with-amazon-kinesis-data-streams/","comment_id":"592447","timestamp":"1650978480.0","upvote_count":"6","comments":[{"content":"If you are struggling with this question - as B and D are very similar - then this link from bamosk is essential to read. As this use case is EXACTLY the same as the question. And it is all detailed out. There is a 20 consumer hard limit also to note on fanout per shard.","poster":"np2021","comment_id":"842777","timestamp":"1679143740.0","upvote_count":"2"}]},{"poster":"chp2022","comment_id":"591876","timestamp":"1650907140.0","upvote_count":"2","content":"I chose B, I'm not sure according to the link provided I'm convinced only a single lambda function. Is there a reason we should need 20?"},{"timestamp":"1650728820.0","upvote_count":"5","content":"D looks good","poster":"CHRIS12722222","comment_id":"590700","comments":[{"content":"Changed to B. One function invoked sequentially 20 times makes more sense","upvote_count":"3","timestamp":"1651156440.0","poster":"CHRIS12722222","comment_id":"593879"}]}],"exam_id":20,"question_images":[],"topic":"1","answer_ET":"B","unix_timestamp":1650728820,"url":"https://www.examtopics.com/discussions/amazon/view/74253-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":60,"choices":{"B":"Create an Amazon Kinesis data stream, and ingest the data for each source into the stream. Create a single enhanced fan-out AWS Lambda function to read these messages and send the messages to each destination endpoint. Register the function as an enhanced fan-out consumer.","D":"Create an Amazon Kinesis data stream, and ingest the data for each source into the stream. Create 20 enhanced fan-out AWS Lambda functions to read these messages and send the messages to each destination endpoint. Register the 20 functions as enhanced fan-out consumers.","C":"Create an Amazon Kinesis Data Firehose delivery stream, and ingest the data for each source into the stream. Configure Kinesis Data Firehose to deliver the data to an Amazon S3 bucket. Invoke an AWS Lambda function with an S3 event notification to read these messages and send the messages to each destination endpoint.","A":"Create an Amazon Kinesis data stream, and ingest the data for each source into the stream. Create 30 AWS Lambda functions to read these messages and send the messages to each destination endpoint."},"timestamp":"2022-04-23 17:47:00","answer":"B","question_text":"A company needs to implement a near-real-time messaging system for hotel inventory. The messages are collected from 1,000 data sources and contain hotel inventory data. The data is then processed and distributed to 20 HTTP endpoint destinations. The range of data size for messages is 2-500 KB.\nThe messages must be delivered to each destination in order. The performance of a single destination HTTP endpoint should not impact the performance of the delivery for other destinations.\nWhich solution meets these requirements with the LOWEST latency from message ingestion to delivery?","answers_community":["B (56%)","D (44%)"],"answer_description":"","answer_images":[]}],"exam":{"name":"AWS Certified Data Analytics - Specialty","isImplemented":true,"numberOfQuestions":164,"provider":"Amazon","isBeta":false,"id":20,"isMCOnly":true,"lastUpdated":"11 Apr 2025"},"currentPage":12},"__N_SSP":true}