{"pageProps":{"questions":[{"id":"x1RTXUZoKJxiGBaotgmK","answers_community":["D (69%)","B (31%)"],"unix_timestamp":1651178040,"question_id":81,"discussion":[{"timestamp":"1653333780.0","comments":[{"content":"“Generally, you use load_run with no arguments to track metrics, parameters, and artifacts within a SageMaker training or processing job script.”\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/experiments-create.html","poster":"Jerry84","comment_id":"777310","upvote_count":"2","timestamp":"1673842920.0"},{"content":"Run PySpark script in SageMaker processing job\nhttps://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html","upvote_count":"1","poster":"Jerry84","comment_id":"816356","timestamp":"1676967900.0"}],"upvote_count":"12","poster":"dolorez","comment_id":"606294","content":"Selected Answer: D\nwhile I agree that Sagemaker Experiments is the way to go, it only supports Training, Processing, and Transform jobs, so the right answer is to run the job as a processing job, hence D not B\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/experiments-create.html#:~:text=CreateTrainingJob-,Processing,-Processor.run"},{"content":"B - https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html","upvote_count":"7","poster":"bluer1","timestamp":"1651178040.0","comment_id":"594041","comments":[{"comment_id":"618095","content":"But It doesn't describe glue job.","poster":"KlaudYu","upvote_count":"4","timestamp":"1655529900.0"}]},{"content":"Selected Answer: B\nPyspark -> AWS Glue","comment_id":"1229614","timestamp":"1718252160.0","poster":"salim1905","upvote_count":"1","comments":[{"timestamp":"1742739660.0","content":"https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor -> D","poster":"ef12052","comment_id":"1402316","upvote_count":"1"}]},{"poster":"3eb0542","timestamp":"1713808920.0","upvote_count":"1","content":"Selected Answer: B\nAWS Glue is a fully managed extract, transform, and load (ETL) service that is purpose-built for processing large datasets and executing PySpark scripts. It's more aligned with the task of running a PySpark script with complex window aggregation operations to prepare data for training and testing","comment_id":"1200329"},{"content":"D https://sagemaker-experiments.readthedocs.io/en/latest/tracker.html","comment_id":"1066092","poster":"sanjosh","timestamp":"1699502640.0","upvote_count":"2"},{"poster":"Mickey321","upvote_count":"2","timestamp":"1690754160.0","content":"Selected Answer: D\nA PySpark script can be run as a SageMaker processing job by using the SparkProcessor class.\nA SageMaker processing job can use Amazon SageMaker Experiments to track the input parameters, output metrics, and artifacts of each run.\nA SageMaker processing job can also use Amazon SageMaker Debugger to capture tensors and analyze the training behavior, but this is more useful for deep learning models than for data preparation tasks.\nRunning the script as an AWS Glue job would not allow the ML specialist to use Amazon SageMaker Experiments or Amazon SageMaker Debugger, as these features are specific to SageMaker.","comment_id":"967542"},{"poster":"ADVIT","content":"D: SageMaker Experiments automatically tracks the inputs, parameters, configurations, and results of your iterations as runs.","comment_id":"944275","upvote_count":"1","timestamp":"1688613360.0"},{"content":"Selected Answer: D\nThe PySpark script defined above is passed via via the submit_app parameter\nhttps://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_processing/spark_distributed_data_processing/sagemaker-spark-processing.ipynb","comment_id":"897638","upvote_count":"1","timestamp":"1684077360.0","poster":"dkx"},{"comments":[{"content":"what is the difference between key metrics and key parameteres? why we care about key metrics, because we can compare the key metrics of different parametes and then find impact of the number of features. \nso the key is \"glue\" or \"SageMaker processing\"","upvote_count":"2","poster":"ZSun","comment_id":"874690","timestamp":"1681910220.0"}],"poster":"Mllb","upvote_count":"1","comment_id":"859287","content":"Selected Answer: B\nKey metrics is the \"key\". Then D is not a correct answer","timestamp":"1680463680.0"},{"comment_id":"843664","timestamp":"1679224080.0","content":"Selected Answer: D\nD looks the right answer","poster":"blanco750","upvote_count":"2"},{"upvote_count":"1","timestamp":"1678692600.0","comment_id":"837723","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html ---- Use SageMaker Experiments to view, manage, analyze, and compare both custom experiments that you programmatically create and experiments automatically created from SageMaker jobs.","comments":[{"content":"\"SageMaker jobs\" not \"Glue job\", it is D!","upvote_count":"2","poster":"ZSun","timestamp":"1683410160.0","comment_id":"891043"}],"poster":"SANDEEP_AWS"},{"content":"B: Glue job goes with window aggregation operations","timestamp":"1674632580.0","upvote_count":"1","poster":"jhonivy","comment_id":"787364"},{"content":"Selected Answer: D\nhttps://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/spark_distributed_data_processing/sagemaker-spark-processing.html","upvote_count":"3","poster":"aScientist","comment_id":"713980","timestamp":"1667925360.0"},{"comment_id":"621863","poster":"ovokpus","upvote_count":"5","content":"Selected Answer: B\nhere:\n\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-glue-now-supports-connecting-amazon-sagemaker-notebooks-to-development-endpoints/#:~:text=AWS%20Glue%20now%20supports%20connecting%20Amazon%20SageMaker%20notebooks%20to%20development%20endpoints,-Posted%20On%3A%20Oct&text=You%20can%20now%20create%20an,an%20AWS%20Glue%20development%20endpoint.","timestamp":"1656104220.0"}],"topic":"1","choices":{"B":"Add an Amazon SageMaker Experiments tracker to the script to capture key metrics. Run the script as an AWS Glue job.","D":"Add an Amazon SageMaker Experiments tracker to the script to capture key parameters. Run the script as a SageMaker processing job.","C":"Add an Amazon SageMaker Debugger hook to the script to capture key parameters. Run the script as a SageMaker processing job.","A":"Add an Amazon SageMaker Debugger hook to the script to capture key metrics. Run the script as an AWS Glue job."},"exam_id":26,"question_text":"A machine learning (ML) specialist wants to create a data preparation job that uses a PySpark script with complex window aggregation operations to create data for training and testing. The ML specialist needs to evaluate the impact of the number of features and the sample count on model performance.\nWhich approach should the ML specialist use to determine the ideal data transformations for the model?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/74810-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","timestamp":"2022-04-28 22:34:00","isMC":true,"answer":"D","answer_ET":"D","question_images":[]},{"id":"OeqiFUf04gcTYv9w9NZg","unix_timestamp":1651223520,"isMC":true,"question_id":82,"answers_community":["D (100%)"],"question_text":"A data scientist has a dataset of machine part images stored in Amazon Elastic File System (Amazon EFS). The data scientist needs to use Amazon SageMaker to create and train an image classification machine learning model based on this dataset. Because of budget and time constraints, management wants the data scientist to create and train a model with the least number of steps and integration work required.\nHow should the data scientist meet these requirements?","choices":{"D":"Run a SageMaker training job with an EFS file system as the data source.","C":"Mount the EFS file system to an Amazon EC2 instance and use the AWS CLI to copy the data to an Amazon S3 bucket. Run the SageMaker training job with Amazon S3 as the data source.","B":"Launch a transient Amazon EMR cluster. Configure steps to mount the EFS file system and copy the data to an Amazon S3 bucket by using S3DistCp. Run the SageMaker training job with Amazon S3 as the data source.","A":"Mount the EFS file system to a SageMaker notebook and run a script that copies the data to an Amazon FSx for Lustre file system. Run the SageMaker training job with the FSx for Lustre file system as the data source."},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/74877-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer_description":"","answer_ET":"D","exam_id":26,"discussion":[{"timestamp":"1667034720.0","content":"Selected Answer: D\nShould be D according to the following article\nhttps://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/","poster":"cron0001","upvote_count":"12","comment_id":"594345"},{"comments":[{"upvote_count":"1","poster":"confusedyeti69","content":"boobies","comment_id":"1177247","timestamp":"1726737720.0"}],"timestamp":"1709735700.0","content":"Selected Answer: D\nSageMaker Noteboob instances can take input data directly from below,\n1. AWS S3\n2. Elastic File System (EFS)\n3. FSx for Lustre file system\n\nSince the question is only regarding less coding effort and does not concern high availability or high performance, Option D would be good","poster":"Sharath1783","comment_id":"1000601","upvote_count":"5"},{"poster":"AIWave","comment_id":"1159062","upvote_count":"2","content":"Selected Answer: D\nUsing Amazon SageMaker for training, you can utilize an Amazon EFS as your data source as long as the data already resides in Amazon EFS before starting the training job. This option requires least integration work.","timestamp":"1724608020.0"},{"content":"My God, the answer is not D!!!\n\nUsing EFS for Lustre reduces the start-up time by eliminating the data download step of the training process and leveraging the various performance and throughput benefits of the file system to execute the training job faster.\n\nSo, A IS the correct !!!","timestamp":"1710630480.0","poster":"jopaca1216","upvote_count":"1","comments":[{"content":"\"with the least number of steps and integration work required\"","timestamp":"1716621240.0","comment_id":"1079869","poster":"thuyeinaung","upvote_count":"1"}],"comment_id":"1009324"},{"upvote_count":"1","comment_id":"1005835","poster":"teka112233","timestamp":"1710259560.0","content":"Selected Answer: D\nthe management wants the data scientist to create and train a model with the least number of steps and integration work required, (this is the keyword) so there is no need to include more things than sagemaker and EFS which make option D is the most suitable"},{"timestamp":"1706733180.0","content":"Selected Answer: D\nThis option allows the data scientist to use the existing dataset in EFS without copying or moving it to another storage service. It also minimizes the number of steps and integration work required, as SageMaker supports EFS as a data source for training jobs. This option is also cost-effective and time-efficient, as it avoids additional charges and delays associated with data transfer and storage.","comment_id":"968408","upvote_count":"1","poster":"Mickey321"},{"timestamp":"1696275600.0","poster":"Mllb","content":"Selected Answer: D\nLess effort, then D","comment_id":"859298","upvote_count":"3"},{"content":"Selected Answer: D\n“When you create a training job, you specify the location of a training dataset and an input mode for accessing the dataset. For data location, Amazon SageMaker supports Amazon Simple Storage Service (Amazon S3), Amazon Elastic File System (Amazon EFS), and Amazon FSx for Lustre. ”\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html","poster":"Jerry84","comment_id":"777319","upvote_count":"2","timestamp":"1689475020.0"},{"upvote_count":"3","poster":"srikanth923","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/machine-learning/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations/","timestamp":"1681897140.0","comment_id":"698880"},{"timestamp":"1667839260.0","comment_id":"598153","upvote_count":"4","comments":[{"poster":"jhonivy","timestamp":"1690265460.0","comments":[{"comment_id":"810734","poster":"wolfsong","content":"A is not the right answer. It's D.\n- A requires this setup: EFS -> Lustre -> Sagemaker. \n- D requires this setup: EFS -> Sagemaker\nIt's obviously not A.","timestamp":"1692187740.0","upvote_count":"2"}],"comment_id":"787389","upvote_count":"1","content":"Time constraints. A is the right answer for this question"}],"content":"Amazon SageMaker now supports Amazon Elastic File System (Amazon EFS) and Amazon FSx for Lustre file systems as data sources for training machine learning models on SageMaker. then why not select D ??","poster":"ckkobe24"}],"answer":"D","question_images":[],"timestamp":"2022-04-29 11:12:00"},{"id":"VggLy7IR7F77mWT3payG","question_text":"A retail company uses a machine learning (ML) model for daily sales forecasting. The company's brand manager reports that the model has provided inaccurate results for the past 3 weeks.\nAt the end of each day, an AWS Glue job consolidates the input data that is used for the forecasting with the actual daily sales data and the predictions of the model. The AWS Glue job stores the data in Amazon S3. The company's ML team is using an Amazon SageMaker Studio notebook to gain an understanding about the source of the model's inaccuracies.\nWhat should the ML team do on the SageMaker Studio notebook to visualize the model's degradation MOST accurately?","question_id":83,"answer":"C","answer_images":[],"choices":{"D":"Create a scatter plot of daily sales versus model error for the last 3 weeks. In addition, create a scatter plot of daily sales versus model error from before that period.","B":"Create a histogram of the model errors over the last 3 weeks. In addition, create a histogram of the model errors from before that period.","A":"Create a histogram of the daily sales over the last 3 weeks. In addition, create a histogram of the daily sales from before that period.","C":"Create a line chart with the weekly mean absolute error (MAE) of the model."},"exam_id":26,"answers_community":["C (44%)","D (32%)","B (24%)"],"isMC":true,"topic":"1","discussion":[{"comments":[{"content":"why? i think it's D","poster":"ef12052","comment_id":"1514841","upvote_count":"1","timestamp":"1743830160.0"}],"content":"C is the correct answer.","timestamp":"1653514980.0","comment_id":"607399","upvote_count":"13","poster":"exam_prep"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/forecast/latest/dg/predictor-monitoring-results.html","poster":"ef12052","timestamp":"1743510120.0","comment_id":"1417419","comments":[],"upvote_count":"1"},{"timestamp":"1743380580.0","content":"Selected Answer: D\nC. Creating a line chart of weekly Mean Absolute Error (MAE)\n→ This can show changes in model performance over time, but it does not reveal whether errors are concentrated in specific sales ranges.\n\nD. Creating a scatter plot of daily sales vs. model errors for the past three weeks and the period before that\n→ This allows for a direct visual analysis of the relationship between actual sales and prediction errors.\n→ It helps identify whether errors are more significant in specific sales ranges and effectively compares model performance before and after degradation.","upvote_count":"1","comment_id":"1413857","poster":"Carpediem78"},{"comments":[{"content":"Although, following the docs it will be C:\nhttps://docs.aws.amazon.com/forecast/latest/dg/predictor-monitoring-results.html","comment_id":"1241063","timestamp":"1719959220.0","upvote_count":"2","poster":"learningcloud1"}],"comment_id":"1240645","timestamp":"1719910920.0","content":"Selected Answer: B\nMost accurately.\nC is a tempting answer. You will see the model degradation over time. You could see if it's slowly getting worse or was it sudden. \n\nB is more accurate. You will only have two histograms to compare, but you will easily see which direction the error move: Are we over or underestimating.\n\nIn practice you would use both. I'm terms of the exam - most accurate - most added information, B gives more information than C. It's a preference though.","upvote_count":"1","poster":"learningcloud1"},{"poster":"f3a4b7c","comment_id":"1220818","content":"Selected Answer: D\nD is the correct answer","upvote_count":"1","timestamp":"1716978300.0"},{"comment_id":"1200332","poster":"3eb0542","timestamp":"1713809640.0","content":"Selected Answer: D\nWeekly MAE aggregates the error metrics over a larger time window, which can mask fluctuations and specific patterns in the model's performance on a daily basis. In situations where there are sudden changes or degradation in the model's accuracy within a week, this visualization might not capture those nuances effectively.","upvote_count":"2"},{"timestamp":"1708892520.0","comment_id":"1159146","poster":"AIWave","upvote_count":"3","content":"Selected Answer: D\nDegradation over time: line or scatter plot (options C, D)\nC is aggregrate weekly view and doesn't give any additional details.\nD compares the model's errors during 3 week period to the errors from before that period giving an accurate picture of anomalies"},{"comments":[{"timestamp":"1710933840.0","upvote_count":"1","content":"Claude 3 Sonnet:\nBased on the evidence from AWS documentation and best practices, Option B: Create a histogram of the model errors over the last 3 weeks. In addition, create a histogram of the model errors from before that period, is the most accurate approach for the ML team to visualize the model's degradation.\n\nHistograms of model errors directly visualize the distribution and patterns of the model's inaccuracies, which is crucial for understanding the source of the problem. By comparing the error distributions before and after the 3-week period, the ML team can identify any significant shifts or changes that may indicate the cause of the model's degradation.\n\nThis approach aligns with AWS best practices for model monitoring and visualization, as recommended by the Amazon SageMaker Model Monitor documentation. It provides a clear and focused visualization of the model's performance, enabling the ML team to gain insights and take appropriate actions to address the inaccuracies.","poster":"F1Fan","comment_id":"1178194"}],"comment_id":"1153534","timestamp":"1708285320.0","upvote_count":"1","poster":"3eb0542","content":"Selected Answer: D\nGPT:\nTo accurately visualize the degradation of the model over time and understand the source of inaccuracies, the ML team should focus on comparing the model's performance before and after the reported period of inaccuracies. The most appropriate option is:\n\nD. Create a scatter plot of daily sales versus model error for the last 3 weeks. In addition, create a scatter plot of daily sales versus model error from before that period."},{"comment_id":"1144310","poster":"kyuhuck","content":"Selected Answer: B\nThe best option to visualize the model's degradation most accurately would be to compare the model's errors over the relevant periods. This directly addresses the issue of model accuracy and allows for a clear comparison of model performance before and after the reported period of inaccuracy.\n\nTherefore, the most appropriate approach would be:\n\nB. Create a histogram of the model errors over the last 3 weeks. In addition, create a histogram of the model errors from before that period.\n\nThis approach will allow the ML team to see if the distribution of errors has changed recently, indicating a degradation in model performance.","upvote_count":"2","timestamp":"1707383460.0"},{"content":"Is there a reason to create weekly MAE plot, if the prediction is made on daily granularity?","poster":"DimLam","timestamp":"1698755460.0","upvote_count":"3","comment_id":"1058770"},{"poster":"teka112233","content":"Selected Answer: C\nthis is the key sentence :\nAt the end of each day, an AWS Glue job consolidates the input data that is used for the forecasting with the actual daily sales data and the predictions of the model\nand that is exactly what MAE do:\n mean absolute error (MAE) is a statistical measure of the difference between two continuous variables. It is calculated as the average of the absolute differences between the predicted and actual values\nso the answer is C","upvote_count":"1","comment_id":"1005902","timestamp":"1694535180.0"},{"upvote_count":"1","poster":"loict","timestamp":"1694149800.0","comment_id":"1002134","content":"Selected Answer: C\nA. NO - Daily sales histogram does not help to see model error\nB. NO - Histogram of the model errors is good, but no point to have one for the first 3 weeks and another for older data\nC. YES - one chart of model errors is perfect\nD. NO - no point to have 2 charts again"},{"comment_id":"977797","timestamp":"1691680260.0","upvote_count":"1","poster":"kaike_reis","content":"Selected Answer: C\nC is correct."},{"comment_id":"968422","poster":"Mickey321","timestamp":"1690829340.0","content":"Selected Answer: B\noption B with histograms of model errors for the specific time periods is the most accurate and appropriate visualization to understand the model's degradation and identify the reasons behind the inaccuracies in the daily sales forecasting.","upvote_count":"3"},{"upvote_count":"3","content":"Selected Answer: C\nC is the answer, line plots are good solutions for time series analysis.","timestamp":"1678128720.0","poster":"Valcilio","comment_id":"831157"},{"content":"Selected Answer: C\nC is correct. We could view the \"Degradation\" as a trend.\nLine charts are usually very helpful to show if there is any trend in the data over the period of time under analysis. \n\nHistogram is normally used to visualizing distributions in your data.","timestamp":"1673844900.0","poster":"Jerry84","upvote_count":"4","comment_id":"777324"},{"content":"Should be A because it is daily forecasting and histograms before and after will show the comparable degradation.","poster":"DD4","upvote_count":"1","comments":[{"comment_id":"742640","content":"It only states to plot daily sales.. how should that help with the error?\nYou need a plot of the actual and predicted values or or the errors - def not A","poster":"venimus_vidimus_vicimus","timestamp":"1670839080.0","upvote_count":"1"}],"comment_id":"654046","timestamp":"1661862420.0"},{"poster":"weixing","upvote_count":"4","timestamp":"1654481160.0","comment_id":"612117","comments":[{"upvote_count":"2","timestamp":"1676556960.0","content":"No, it's neither A or B as they use a histogram which would plot the distribution of errors. It will not tell you anything about how the model degrades over time, as the histogram will have no time component. You need a line chart for this. So it's C.","poster":"wolfsong","comment_id":"810740"}],"content":"B, I guess"}],"url":"https://www.examtopics.com/discussions/amazon/view/76292-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1653514980,"question_images":[],"answer_ET":"C","timestamp":"2022-05-25 23:43:00","answer_description":""},{"id":"KhaFZyGoLzbuWfGc6Vof","question_text":"An ecommerce company sends a weekly email newsletter to all of its customers. Management has hired a team of writers to create additional targeted content. A data scientist needs to identify five customer segments based on age, income, and location. The customers' current segmentation is unknown. The data scientist previously built an XGBoost model to predict the likelihood of a customer responding to an email based on age, income, and location.\nWhy does the XGBoost model NOT meet the current requirements, and how can this be fixed?","exam_id":26,"choices":{"B":"The XGBoost model provides a true/false binary output. Increase the number of classes the XGBoost model predicts to five classes to predict a segment.","A":"The XGBoost model provides a true/false binary output. Apply principal component analysis (PCA) with five feature dimensions to predict a segment.","D":"The XGBoost model is a supervised machine learning algorithm. Train a k-means model with K = 5 on the same dataset to predict a segment.","C":"The XGBoost model is a supervised machine learning algorithm. Train a k-Nearest-Neighbors (kNN) model with K = 5 on the same dataset to predict a segment."},"discussion":[{"comments":[{"content":"well, both are used for customer segmentation Knn & kmeans but kmeans is for unsupervised learning and knn is for supervised learning. since we have the data it's better to use supervised learning in this case. Ref: https://rstudio-pubs-static.s3.amazonaws.com/599866_59be74824ca7482ba99dbc8466dc36a0.html#:~:text=The%20difference%20between%20the%20two,to%20predict%20the%20unlabelled%20data.","comment_id":"605771","upvote_count":"4","timestamp":"1669176240.0","poster":"Omijh"}],"upvote_count":"16","poster":"spaceexplorer","comment_id":"594630","timestamp":"1667066400.0","content":"Selected Answer: D\nAnswer is D! K-means used for customer segmentation"},{"poster":"tgaos","upvote_count":"11","content":"The answer is D.\n1. \"The current segmentation of consumers is unclear.\" so it is unsupervised learning. \n2. Then K-means is for unsupervised learning.","timestamp":"1669453440.0","comment_id":"607506"},{"comment_id":"1159149","content":"Selected Answer: D\nTypical clustering problem - use K means","timestamp":"1724610300.0","poster":"AIWave","upvote_count":"1"},{"upvote_count":"1","comment_id":"1004471","timestamp":"1710142980.0","content":"Selected Answer: D\nKNN is used to solve missing data in regression/supervised problems. Since the question says unknown segmentation, it is an unsupervised problem and K-Means is the right choice. So Option D it is.","poster":"Sharath1783"},{"comment_id":"977800","timestamp":"1707585240.0","poster":"kaike_reis","upvote_count":"1","content":"D is the correct\nC is wrong because kNN stills a supervised algorithm"},{"poster":"Mickey321","comment_id":"969144","content":"Selected Answer: D\nThe XGBoost model is a supervised machine learning algorithm, which means it requires labeled data to learn from. However, the customers’ current segmentation is unknown, so there are no labels to train or evaluate the model. The data scientist needs an unsupervised machine learning algorithm, which can discover patterns and clusters in unlabeled data. A k-means model is an example of an unsupervised machine learning algorithm that can partition the data into K groups based on similarity. By setting K = 5, the data scientist can obtain five customer segments based on age, income, and location.","upvote_count":"1","timestamp":"1706805360.0"},{"upvote_count":"1","timestamp":"1686429420.0","comment_id":"741281","poster":"Peeking","content":"Selected Answer: D\nKNN has no k parameter in its input. C is not the answer.","comments":[{"poster":"drcok87","timestamp":"1691579880.0","content":"in K-means also there is no input parameter \"K\". What i mean to say here is in knn the k is nothing but \"kNN classifier identifies the class of a data point using the majority voting principle. If k is set to 5, the classes of 5 nearest points are examined.\"","upvote_count":"1","comment_id":"803253"}]},{"content":"D\nThe key work is that the classification is \"unclear\", therefore k-means","upvote_count":"3","comment_id":"637465","poster":"matteocal","timestamp":"1674750540.0"}],"unix_timestamp":1651255200,"timestamp":"2022-04-29 20:00:00","url":"https://www.examtopics.com/discussions/amazon/view/74922-exam-aws-certified-machine-learning-specialty-topic-1/","answers_community":["D (100%)"],"answer_images":[],"question_id":84,"topic":"1","question_images":[],"answer":"D","answer_description":"","isMC":true,"answer_ET":"D"},{"id":"IgI19R8zTBHBrW2sAmSg","answers_community":["A (95%)","5%"],"unix_timestamp":1651346460,"answer":"A","topic":"1","question_id":85,"choices":{"C":"Use a label encoder for the categorical fields in the dataset. Perform L1 regularization on the financial fields in the dataset. Apply L2 regularization to the data.","B":"Use tokenization of the categorical fields in the dataset. Perform binning on the financial fields in the dataset. Remove the outliers in the data by using the z- score.","A":"Use a one-hot encoder for the categorical fields in the dataset. Perform standardization on the financial fields in the dataset. Apply L1 regularization to the data.","D":"Use a logarithm transformation on the categorical fields in the dataset. Perform binning on the financial fields in the dataset. Use imputation to populate missing values in the dataset."},"question_images":[],"question_text":"A global financial company is using machine learning to automate its loan approval process. The company has a dataset of customer information. The dataset contains some categorical fields, such as customer location by city and housing status. The dataset also includes financial fields in different units, such as account balances in US dollars and monthly interest in US cents.\nThe company's data scientists are using a gradient boosting regression model to infer the credit score for each customer. The model has a training accuracy of\n99% and a testing accuracy of 75%. The data scientists want to improve the model's testing accuracy.\nWhich process will improve the testing accuracy the MOST?","url":"https://www.examtopics.com/discussions/amazon/view/74994-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"A","answer_images":[],"exam_id":26,"isMC":true,"answer_description":"","timestamp":"2022-04-30 21:21:00","discussion":[{"poster":"ckkobe24","comment_id":"598054","content":"Selected Answer: A\nagree it's A for me","timestamp":"1667820720.0","upvote_count":"15"},{"upvote_count":"11","poster":"spaceexplorer","timestamp":"1667164860.0","content":"A: it's overfitting so regularization is needed, need apply scaling on financial data fields as it's for regression problem; one hot encoding for city of the house field.","comment_id":"595229"},{"timestamp":"1723101240.0","content":"Selected Answer: A\nOption A is the most likely to improve the testing accuracy the most effectively because it uses appropriate preprocessing techniques for both categorical and numerical data and applies a regularization technique that can help in reducing overfitting, thereby potentially improving the model's generalization to unseen data.","comment_id":"1144311","poster":"kyuhuck","upvote_count":"2"},{"upvote_count":"1","comment_id":"1047810","poster":"DimLam","timestamp":"1713523320.0","comments":[{"comment_id":"1047817","content":"B also looks suspicious.","upvote_count":"1","timestamp":"1713523740.0","poster":"DimLam"}],"content":"Selected Answer: B\nI will go with B. \n(A) suggests applying regularization to the data. It doesn't make sense. \n\n(B) answer is well framed. At least it doesn't use the wrong formulation."},{"poster":"Mickey321","comment_id":"969149","content":"Selected Answer: A\nUse a one-hot encoder for the categorical fields in the dataset. Perform standardization on the financial fields in the dataset. Apply L1 regularization to the data.","upvote_count":"1","timestamp":"1706805840.0"},{"comment_id":"884115","content":"Selected Answer: A\nAgree with A, but I think the answer is slightly inaccurate. L1 regularization within the model and to the loss function. As a result, some features will be removed in the model. The answer suggest L1 regularization is applied to the dataset directly.","timestamp":"1698570480.0","upvote_count":"1","poster":"Tony_1406"},{"timestamp":"1692477240.0","comment_id":"814629","upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1713523380.0","comment_id":"1047813","content":"How do you apply regularization to Data and not to the model params?","poster":"DimLam"}],"content":"Selected Answer: A\nOption A is the most appropriate approach to improve the testing accuracy of the model.\n\nOne-hot encoding can effectively represent categorical variables in a numeric format that is suitable for machine learning models. Standardizing the financial fields can make the data more comparable and improve the model's performance. L1 regularization can help in feature selection and avoid overfitting by reducing the number of features.","poster":"AjoseO"},{"poster":"Peeking","upvote_count":"1","timestamp":"1686429720.0","content":"Why are most of the chosen answers by ExamTopics mostly obviously wrong? There is nothing like tokenisation of categorical variable and B should be obviously wrong.","comments":[{"content":"When they were published (firtsly, they steal them by photo/camera) they didn't have chatgpt to see the answers, and of course, they don't have any ML specialist or time to resolve them.","upvote_count":"1","poster":"ccpmad","timestamp":"1706888880.0","comment_id":"970243"}],"comment_id":"741284"},{"comment_id":"667365","content":"12-sep exam","timestamp":"1678653720.0","poster":"Shailendraa","upvote_count":"4"}]}],"exam":{"isBeta":false,"numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty","id":26,"provider":"Amazon","isMCOnly":false,"isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":17},"__N_SSP":true}