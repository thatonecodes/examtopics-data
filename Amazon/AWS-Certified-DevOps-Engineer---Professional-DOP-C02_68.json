{"pageProps":{"questions":[{"id":"BQvX5SHB0gz0nTiLUfOd","question_id":336,"answer_images":[],"discussion":[{"content":"Selected Answer: BD\nAmazon Kinesis Data Firehose simplifies the process of loading streaming data into S3 and provides automatic scaling, buffering, and retries.","poster":"2pk","comment_id":"898096","upvote_count":"6","timestamp":"1684134420.0"},{"timestamp":"1722559320.0","upvote_count":"1","comment_id":"1259615","poster":"jamesf","content":"Selected Answer: BD\nB - keywords: continue stream but not one time task\nD - keywords: S3 Glacier"},{"comment_id":"1256699","content":"Selected Answer: BD\nvote B and D. \nI initially thought the C is better than B, because Amazon Kinesis Data Firehose is primarily used for time-sensitive tasks, which is not suitable for this case, But when I read the C. I found the Directly streaming logs from cloudwatch log to s3 is not a feature provided by Cloudwatch. \nSo, I will go with B and D.","timestamp":"1722161940.0","upvote_count":"3","poster":"ericphl"},{"poster":"Gomer","upvote_count":"3","comment_id":"1229621","content":"Selected Answer: BD\nYou can absolutly directly \"export log data from your log groups to an Amazon S3 bucket\"\nHowever, this is a one time export, and NOT an ongoing stream.\nIf you want to steam continuously you have to use \"subscription filter with Kinesis Data Streams, Lambda, or Firehose.\"\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\nhttps://dev.to/aws-builders/automate-export-of-cloudwatch-logs-to-s3-bucket-using-lambda-with-eventbridge-trigger-2ieg","timestamp":"1718253720.0"},{"upvote_count":"1","comment_id":"1212920","timestamp":"1715954160.0","poster":"zijo","content":"Looks like creating subscription filters in AWS cloudwatch logs, there are only limited destination options. There is no S3 as a direct destination. You have to either create Elasticsearch or Kinesis or Kinesis Firehose or Lambda subscription filters. Given the choices we have, we need to pick B & D"},{"comment_id":"1205382","upvote_count":"1","poster":"Jay_2pt0_1","timestamp":"1714638660.0","content":"Selected Answer: CD\nC & D for the reasons that thanhnv142 mentioned.","comments":[{"comment_id":"1210806","timestamp":"1715595060.0","content":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\nPls check link. You can use a subscription filter with Kinesis Data Streams, Lambda, or Firehose. Logs that are sent to a receiving service through a subscription filter are base64 encoded and compressed with the gzip format. correct is B and D","upvote_count":"2","poster":"vn_thanhtung"}]},{"upvote_count":"2","content":"CD，The question does not mention trying to switch to S3 in real time. C is more cost-effective.\nhttps://docs.aws.amazon.com/zh_cn/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html","comment_id":"1160431","poster":"Heyang","timestamp":"1709029740.0"},{"timestamp":"1708838160.0","poster":"dzn","upvote_count":"3","content":"Selected Answer: BD\nAmazon S3 Glacier is a secure, durable, and very low-cost cloud storage service that can be used for data archiving and long-term backup.","comment_id":"1158399"},{"poster":"jojom19980","content":"Selected Answer: BD\nYou can use a subscription filter with Kinesis Data Streams, Lambda, or Firehose.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html","upvote_count":"2","comment_id":"1153723","timestamp":"1708320120.0"},{"poster":"thanhnv142","content":"Selected Answer: CD\nC and D is correct: <archive the logs to an Amazon S3 bucket> means we need to transport logs from CloudWatch Logs to S3. CloudWatch Logs can directly transport log data to S3. Logs are rarely accessed after 90 days means we need S3 bucket lifecycle policy","timestamp":"1706893380.0","comments":[{"content":"A: AWS Glue is used primarily to integrate data from multiple data sources (up to 70) for data analysis. Of course it works well with one data source only (CW logs in this case). But it costs a lot of money and using it with only one data source is a waste of corporate budget. Should not use this. \nB: Amazon Kinesis Data Firehose is primarily used for time-sensitive tasks, such as video streaming. It is very powerful that it can handle data in near realtime. However, this premium feature comes with a big expense. We only need to archive data, not video streaming it. \nE: we need to transition it to S3 Glacier not Reduced Redundancy after 90 days","poster":"thanhnv142","upvote_count":"1","timestamp":"1706893380.0","comment_id":"1138737"}],"comment_id":"1138736","upvote_count":"4"},{"comment_id":"946848","timestamp":"1688869980.0","poster":"habros","upvote_count":"4","content":"Selected Answer: BD\nB to shift logs out using Kinesis Firehose to S3. Then D to set S3 bucket storage class to Glacier Flexible."},{"upvote_count":"4","timestamp":"1687964520.0","comment_id":"936798","content":"Selected Answer: BD\nC would make sense, but subscription filters don't go to S3 directly\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html","comments":[{"comment_id":"976215","poster":"Aja1","content":"Option C is incorrect because streaming all logs to an S3 bucket is not a good solution for archiving logs","timestamp":"1691552880.0","upvote_count":"2"}],"poster":"YXXt55"},{"comment_id":"929028","upvote_count":"1","content":"Selected Answer: BD\nBD is right","poster":"haazybanj","timestamp":"1687320060.0"},{"content":"Selected Answer: BD","upvote_count":"1","timestamp":"1684237860.0","comment_id":"899145","poster":"OrganizedChaos25"}],"unix_timestamp":1684134420,"answer_description":"","question_text":"A company manages an application that stores logs in Amazon CloudWatch Logs. The company wants to archive the logs to an Amazon S3 bucket. Logs are rarely accessed after 90 days and must be retained for 10 years.\n\nWhich combination of steps should a DevOps engineer take to meet these requirements? (Choose two.)","topic":"1","choices":{"D":"Configure the S3 bucket lifecycle policy to transition logs to S3 Glacier after 90 days and to expire logs after 3.650 days.","C":"Configure a CloudWatch Logs subscription filter to stream all logs to an S3 bucket.","A":"Configure a CloudWatch Logs subscription filter to use AWS Glue to transfer all logs to an S3 bucket.","B":"Configure a CloudWatch Logs subscription filter to use Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.","E":"Configure the S3 bucket lifecycle policy to transition logs to Reduced Redundancy after 90 days and to expire logs after 3.650 days."},"timestamp":"2023-05-15 09:07:00","answer":"BD","answer_ET":"BD","isMC":true,"question_images":[],"answers_community":["BD (84%)","CD (16%)"],"exam_id":23,"url":"https://www.examtopics.com/discussions/amazon/view/109258-exam-aws-certified-devops-engineer-professional-dop-c02/"},{"id":"uodwOfzw1OCwgEHwCvhz","discussion":[{"timestamp":"1722649500.0","poster":"thanhnv142","content":"Selected Answer: BCF\nBCF is my choice","upvote_count":"2","comment_id":"1138988"},{"upvote_count":"2","content":"BCF are correct:\nA is not correct: <needs to create the infrastructure as code (IaC)> means we prefer AWS SAM over ACF. ACF is used to deploy AWS instances, not for IaC\nD is wrong: no mention of AWS SAM\nE is wrong: <Amazon CloudWatch composite alarm for all the Lambda functions>, but we need alarm for each lambda func, not one alarm for all of them","timestamp":"1722649440.0","poster":"thanhnv142","comment_id":"1138986"},{"timestamp":"1719714780.0","upvote_count":"2","comment_id":"1109404","poster":"sarlos","content":"it should be BDF because code deploy can be configured for canary\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"},{"poster":"HugoFM","upvote_count":"3","content":"Selected Answer: BCF\nBCF, E is not correct you need to monitor each lambda to do a rollback of a particular deploy","timestamp":"1716951900.0","comment_id":"1083118"},{"comment_id":"1080992","content":"BCF is right","timestamp":"1716744840.0","poster":"AzureDP900","upvote_count":"1"},{"content":"Answer is BCF.\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html","upvote_count":"1","timestamp":"1714479060.0","comment_id":"1057850","poster":"rlf"},{"timestamp":"1713718260.0","content":"Can someone please explain why not A and D? seem the same of B C without using SAM","poster":"sivre","comment_id":"1049679","upvote_count":"3"},{"comments":[{"poster":"hotblooded","content":"Why we cannot use code deploy for canary there are already few deployment percentage for codedeploy BDF is correct","timestamp":"1722316020.0","comment_id":"1135567","upvote_count":"1"}],"comment_id":"1005699","upvote_count":"2","poster":"RVivek","timestamp":"1710251100.0","content":"Selected Answer: BCF\nA is wrong beacuse of AWS::Lambda::Function\nB - Can work\nC: is correct\nD: SAM or Lambda deployment in Codedeploy cannot be canary deployment. canaray deployment should be included in the Lambda code as mentioned in option B.\nE: Composite Alram is not required. if any Lambda fails , it should generate alarm\nF: works\nBasically select B from AB which is for lambda coding, Select C from CD for deploying and F from EF for monitoring and alerting"},{"content":"Selected Answer: BCF\nLeaning to BCF. Lambda errors are standard although BCE is possible too.\n\nFor server less it is giveaway question. Stick with SAM if possible","poster":"habros","comment_id":"946851","timestamp":"1704775080.0","upvote_count":"2","comments":[{"comment_id":"946852","poster":"habros","content":"I’ll stick with BCF still. Composite alarms does not apply in this context. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html","timestamp":"1704775200.0","upvote_count":"2"}]},{"content":"Selected Answer: BCF\nBCF is correct.","comment_id":"946711","poster":"sb333","upvote_count":"2","timestamp":"1704750060.0"},{"content":"Selected Answer: BCF\nBC and F","comment_id":"943514","timestamp":"1704452880.0","upvote_count":"3","poster":"Blueee"},{"upvote_count":"3","poster":"Manny20","comment_id":"938504","content":"Composite Alarm requires underlying metric alarms which requires one CloudWatch alarm for each lambda functions and then tie them back to a composite alarm. So BC and F makes sense.","timestamp":"1703885640.0"},{"upvote_count":"1","content":"Selected Answer: BCE\nI think BCE makes more sense.","comment_id":"933510","timestamp":"1703510280.0","poster":"FunkyFresco"},{"timestamp":"1702797420.0","content":"Selected Answer: BCE\nF creates an Amazon CloudWatch alarm for each Lambda function. However, it is not necessary to create an alarm for each Lambda function. A single composite alarm can be used to monitor all the Lambda functions.","comments":[],"poster":"ducluanxutrieu","comment_id":"925779","upvote_count":"1"},{"poster":"OrganizedChaos25","content":"BCF are correct","timestamp":"1700143140.0","comment_id":"899156","upvote_count":"1"},{"upvote_count":"1","content":"Cannot deploy the canary deployemnt in a pipeline for lambda creation, it has to be created in lambda resource file.","timestamp":"1700040600.0","comment_id":"898109","poster":"2pk"},{"upvote_count":"4","timestamp":"1700040420.0","poster":"2pk","comment_id":"898103","content":"Selected Answer: BCF\nBCF correct"}],"unix_timestamp":1684135620,"exam_id":23,"answer_description":"","timestamp":"2023-05-15 09:27:00","url":"https://www.examtopics.com/discussions/amazon/view/109261-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_images":[],"isMC":true,"answers_community":["BCF (90%)","10%"],"question_text":"A company is developing a new application. The application uses AWS Lambda functions for its compute tier. The company must use a canary deployment for any changes to the Lambda functions. Automated rollback must occur if any failures are reported.\n\nThe company’s DevOps team needs to create the infrastructure as code (IaC) and the CI/CD pipeline for this solution.\n\nWhich combination of steps will meet these requirements? (Choose three.)","question_id":337,"question_images":[],"answer_ET":"BCF","answer":"BCF","choices":{"B":"Create an AWS Serverless Application Model (AWS SAM) template for the application. Define each Lambda function in the template by using the AWS::Serverless::Function resource type. For each function, include configurations for the AutoPublishAlias property and the DeploymentPreference property. Configure the deployment configuration type to LambdaCanary10Percent10Minutes.","F":"Create an Amazon CloudWatch alarm for each Lambda function. Configure the alarms to enter the ALARM state if any errors are detected. Configure an evaluation period, dimensions for each Lambda function and version, and the namespace as AWS/Lambda on the Errors metric.","D":"Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeDeploy deployment group that is configured for canary deployments with a DeploymentPreference type of Canary10Percent10Minutes. Upload the AWS CloudFormation template and source code to the CodeCommit repository. In the CodeCommit repository, create an appspec.yml file that includes the commands to deploy the CloudFormation template.","A":"Create an AWS CloudFormation template for the application. Define each Lambda function in the template by using the AWS::Lambda::Function resource type. In the template, include a version for the Lambda function by using the AWS::Lambda::Version resource type. Declare the CodeSha256 property. Configure an AWS::Lambda::Alias resource that references the latest version of the Lambda function.","C":"Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeBuild project to deploy the AWS Serverless Application Model (AWS SAM) template. Upload the template and source code to the CodeCommit repository. In the CodeCommit repository, create a buildspec.yml file that includes the commands to build and deploy the SAM application.","E":"Create an Amazon CloudWatch composite alarm for all the Lambda functions. Configure an evaluation period and dimensions for Lambda. Configure the alarm to enter the ALARM state if any errors are detected or if there is insufficient data."},"topic":"1"},{"id":"y8107VGzi0lG3DLVQJ2f","answer":"AD","question_text":"A DevOps engineer is deploying a new version of a company’s application in an AWS CodeDeploy deployment group associated with its Amazon EC2 instances. After some time, the deployment fails. The engineer realizes that all the events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.\n\nWhat are valid reasons for this failure? (Choose two.)","question_id":338,"answers_community":["AD (92%)","8%"],"timestamp":"2023-05-09 15:56:00","answer_ET":"AD","answer_description":"","choices":{"A":"The networking configuration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the CodeDeploy endpoint cannot be reached.","D":"An instance profile with proper permissions was not attached to the target EC2 instances.","E":"The appspec.yml file was not included in the application revision.","C":"The target EC2 instances were not properly registered with the CodeDeploy endpoint.","B":"The IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint."},"topic":"1","question_images":[],"unix_timestamp":1683640560,"isMC":true,"exam_id":23,"discussion":[{"poster":"haazybanj","upvote_count":"11","content":"Selected Answer: AD\nA.\n\nExplanation: For CodeDeploy to work, the EC2 instances need to reach the CodeDeploy endpoint to download the deployment artifacts. If the networking configuration of the EC2 instances does not allow them to access the internet via a NAT gateway or internet gateway, they won't be able to reach the CodeDeploy endpoint, leading to deployment failure.\nD\n\nExplanation: When EC2 instances are part of a CodeDeploy deployment group, they need to have an associated IAM instance profile with the necessary permissions to interact with CodeDeploy and download the deployment artifacts. If the instance profile with proper permissions is not attached to the target EC2 instances, the deployment will fail as the instances won't have the required permissions to complete the deployment process.","comment_id":"962906","timestamp":"1690303800.0"},{"content":"Selected Answer: AD\nAD\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html\nSearch with: Troubleshooting all lifecycle events skipped errors","upvote_count":"7","poster":"rhinozD","comments":[{"poster":"bnagaraja9099","timestamp":"1702611540.0","content":"C is correct. \nthe first reason for skipped errors on the link. \n The CodeDeploy agent might not be installed or running on the instance. To determine if the CodeDeploy agent is running:","upvote_count":"1","comments":[{"timestamp":"1710161640.0","content":"No registration required, once agent is installed it should be sufficient. However permissions and network connectivity to S3 or code deploy would be must. Since that takes priority, A&D should be right.","comment_id":"1171022","poster":"sejar","upvote_count":"1"}],"comment_id":"1097024"}],"comment_id":"924241","timestamp":"1686836820.0"},{"content":"My question is \"skipped\" situation doesn't sound like a network error. There is no 4xx error or fail status","poster":"YucelFuat","timestamp":"1725617520.0","comment_id":"1279520","upvote_count":"1"},{"comment_id":"1212948","upvote_count":"1","poster":"zijo","content":"The user needs to create a service role and attach the AWSCodeDeployRole policy to it to grant the correct permissions for CodeDeploy to access EC2 instances. The role chosen should allow access to start and stop EC2 instances.\nIf the IAM role used by CodeDeploy doesn't have the necessary permissions to access the deployment artifacts or interact with the EC2 instances, the deployment may be skipped.\nSo it is not the IAM permissions of the user invoking the CodeDeploy.","timestamp":"1715960760.0"},{"timestamp":"1706933100.0","comment_id":"1138993","content":"Selected Answer: AD\nA and D are correct: the deployment process might be skipped because of codedeploy agent\nA: no connection means skipped deployment\nD: insufficient permission means skipped deployment","upvote_count":"1","poster":"thanhnv142"},{"poster":"khchan123","comment_id":"1125099","upvote_count":"1","content":"Selected Answer: AD\nA and D. See https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events","timestamp":"1705506720.0"},{"content":"Do you really have to have internet connectivity to use CodeDeploy? Why not use VPC endpoint in such cases? I go for CD.","timestamp":"1704164220.0","comment_id":"1111557","upvote_count":"1","poster":"3a29cc4"},{"content":"Selected Answer: CD\nSome of the other options could cause a deployment to fail, but not specifically result in a \"Skipped\" status:\n\nA Networking issues may prevent the deployment from reaching instances, but this would likely cause the deployment to fail, not be skipped.\nB Lack of permissions for the IAM user would cause the deployment job itself to fail authorization.\nE Missing appspec.yml would cause validation errors prior to the deployment attempt.\n\nAnyone has diffrent views?","comment_id":"1092418","timestamp":"1702204140.0","upvote_count":"2","comments":[{"content":"oh yeah;\nwhy;\nC -> CodeDeploy needs to be able to communicate with the instances in order to deploy revisions to them. If the instances are not registered, CodeDeploy will skip deploying to them.\nD - > i think everyone know that point. i guess dont need explaintion. \nPeace :)","upvote_count":"1","timestamp":"1702204320.0","comment_id":"1092419","poster":"yorkicurke"}],"poster":"yorkicurke"},{"timestamp":"1694520900.0","content":"Selected Answer: AD\nI agree with rhinozD","upvote_count":"1","poster":"RVivek","comment_id":"1005728"},{"poster":"Blueee","upvote_count":"1","timestamp":"1688548080.0","comment_id":"943515","content":"Selected Answer: AD\nAD is correct"},{"comment_id":"897266","content":"Its AD","timestamp":"1684042080.0","upvote_count":"1","poster":"devnv"},{"content":"Selected Answer: AD\nAD is correct","upvote_count":"1","comment_id":"893142","poster":"ParagSanyashiv","timestamp":"1683640560.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/108810-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_images":[]},{"id":"EnOqxhQa7qFA4z1pRQI5","answer_images":[],"discussion":[{"content":"Selected Answer: C\nC is correct: <automate the process that the security team uses to provide the AMI IDs to the development teams> and <MOST scalable solution> means we need a pipeline (imange builder) to build AMI and to automate sharing\nA and B: no mention of EC2 Imange builder, which is better than codepipeline in building Ec2 image\nD: They have to do this manually","comment_id":"1138995","timestamp":"1706933940.0","poster":"thanhnv142","upvote_count":"6"},{"timestamp":"1731714360.0","comment_id":"1312854","poster":"ad3fdb1","content":"A question to answer of option C - is it able to update the System Manager Parameter Store automatically? Option A seems able to do it automatically, right?","upvote_count":"1"},{"upvote_count":"2","comment_id":"1117584","timestamp":"1704811980.0","content":"C is the best option","poster":"yuliaqwerty"},{"timestamp":"1698805620.0","upvote_count":"2","content":"Answer is C.\nhttps://aws.amazon.com/ko/blogs/compute/tracking-the-latest-server-images-in-amazon-ec2-image-builder-pipelines/","comment_id":"1059318","poster":"rlf"},{"timestamp":"1688871180.0","poster":"habros","comment_id":"946856","upvote_count":"2","content":"Selected Answer: C\nUse SSM Parameter Store or Secret Manager as the lookup K/V store for all the related AMIs. ANother way is also for security team to constantly update and share the images cross-account and grant them KMS keys to the encrypted AMIs. (not in question)"},{"poster":"devnv","comment_id":"897267","upvote_count":"2","content":"C is correct","timestamp":"1684042320.0"},{"upvote_count":"4","timestamp":"1683640680.0","poster":"ParagSanyashiv","content":"Selected Answer: C\nC make more sense","comment_id":"893146"}],"choices":{"C":"Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to place the AMI ARNs as parameters in AWS Systems Manager Parameter Store. Instruct the developers to specify a parameter of type SSM in their CloudFormation stack to obtain the most recent AMI ARNs from Parameter Store.","B":"Direct the security team to use a CloudFormation stack to create an AWS CodePipeline pipeline that builds new AMIs and places the latest AMI ARNs in an encrypted Amazon S3 object as part of the pipeline output. Instruct the developers to use a cross-stack reference within their own CloudFormation template to obtain the S3 object location and the most recent AMI ARNs.","D":"Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to create an Amazon Simple Notification Service (Amazon SNS) topic so that every development team can receive notifications. When the development teams receive a notification, instruct them to write an AWS Lambda function that will update their CloudFormation stack with the most recent AMI ARNs.","A":"Direct the security team to use CloudFormation to create new versions of the AMIs and to list the AMI ARNs in an encrypted Amazon S3 object as part of the stack’s Outputs section. Instruct the developers to use a cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs."},"answers_community":["C (100%)"],"question_text":"A company has a guideline that every Amazon EC2 instance must be launched from an AMI that the company’s security team produces. Every month, the security team sends an email message with the latest approved AMIs to all the development teams.\n\nThe development teams use AWS CloudFormation to deploy their applications. When developers launch a new service, they have to search their email for the latest AMIs that the security department sent. A DevOps engineer wants to automate the process that the security team uses to provide the AMI IDs to the development teams.\n\nWhat is the MOST scalable solution that meets these requirements?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/108811-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_ET":"C","unix_timestamp":1683640680,"timestamp":"2023-05-09 15:58:00","isMC":true,"exam_id":23,"answer_description":"","question_images":[],"answer":"C","question_id":339},{"id":"NS0nJDnKsX7Lx9QT6fVf","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/109204-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_images":[],"isMC":true,"discussion":[{"poster":"rhinozD","content":"Selected Answer: C\nC is the answer\nrefer this: https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-allowtraffic-no-logs","comment_id":"924247","timestamp":"1702655640.0","upvote_count":"17"},{"content":"Selected Answer: C\nC is correct: <deployment fails during the AllowTraffic lifecycle event> means there are problems with ALB. \nA: no mention of the ALB\nB: The user who init the deployment does not need necessary permission\nD: If agent was not installed, it would fail from the start","comment_id":"1138996","timestamp":"1722651780.0","upvote_count":"5","poster":"thanhnv142"},{"upvote_count":"2","poster":"OrganizedChaos25","content":"C is the answer","timestamp":"1700145540.0","comment_id":"899185"},{"timestamp":"1699947360.0","comment_id":"897271","upvote_count":"1","content":"C is the correct answer","poster":"devnv"}],"question_images":[],"timestamp":"2023-05-14 07:36:00","exam_id":23,"answers_community":["C (100%)"],"question_text":"An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOps engineer is using AWS CodeDeploy to release a new version. The deployment fails during the AllowTraffic lifecycle event, but a cause for the failure is not indicated in the deployment logs.\n\nWhat would cause this?","choices":{"A":"The appspec.yml file contains an invalid script that runs in the AllowTraffic lifecycle hook.","B":"The user who initiated the deployment does not have the necessary permissions to interact with the ALB.","C":"The health checks specified for the ALB target group are misconfigured.","D":"The CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group."},"answer_ET":"C","unix_timestamp":1684042560,"question_id":340,"answer":"C","answer_description":""}],"exam":{"isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":23,"numberOfQuestions":355,"provider":"Amazon","isMCOnly":true,"name":"AWS Certified DevOps Engineer - Professional DOP-C02"},"currentPage":68},"__N_SSP":true}