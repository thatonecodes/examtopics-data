{"pageProps":{"questions":[{"id":"8bl34NfIcIlR2geCwHXI","topic":"1","answer":"B","answer_description":"","unix_timestamp":1683318000,"question_id":341,"timestamp":"2023-05-05 22:20:00","url":"https://www.examtopics.com/discussions/amazon/view/108587-exam-aws-certified-devops-engineer-professional-dop-c02/","answers_community":["B (80%)","D (20%)"],"exam_id":23,"choices":{"B":"Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names for communication between microservices.","A":"Create a new AWS account in AWS Organizations. Create a VPC in this account, and use AWS Resource Access Manager to share the private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2 instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.","D":"Create a new AWS account in AWS Organizations. Create a transit gateway in this account, and use AWS Resource Access Manager to share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use the NLB DNS names for communication between microservices.","C":"Create a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between microservices."},"answer_ET":"B","question_text":"A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations.\n\nEach service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices communicate with each other across the public internet. The company’s security team has issued a new guideline that all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet.\n\nA DevOps engineer must implement a solution that fulfills these obligations and minimizes the number of changes for each service team.\n\nWhich solution will meet these requirements?","discussion":[{"comment_id":"943526","upvote_count":"8","timestamp":"1688548680.0","content":"Selected Answer: B\nB is correct because all 20 services team in different separate AWS accounts are using the same CIDR block, which means they are overlapping CIDR. \n\nD state that to update the route tables of each VPC to use the transit gateway but they are all having the same CIDR block so this cannot proceed, as shared by Arnaud92 link the pre-requisite of using the transit gateway is \"No-overlapping CIDR block between VPCs.\"","poster":"Blueee"},{"content":"Selected Answer: B\nPrivateLink = HTTPS connection","timestamp":"1731502800.0","comment_id":"1311256","upvote_count":"1","poster":"Saudis"},{"comment_id":"1215141","timestamp":"1716313380.0","upvote_count":"2","poster":"zijo","content":"B is the answer\nWhen VPCs have overlapping CIDR blocks, AWS PrivateLink still ensures secure and private connectivity by using Interface Endpoints (ENIs) and Network Load Balancers (NLBs) to route traffic, bypassing the need for direct IP routing between the VPCs."},{"timestamp":"1706935380.0","content":"B is correct: <all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet> means privatelink\nA and C: no mention of privatelink\nD: Using transite gateway. But this solution need IP to route traffic and cannot be used for overlapped VPC CIDR block (every team uses 192.168.0.0/22)","comment_id":"1139004","poster":"thanhnv142","upvote_count":"2"},{"poster":"zolthar_z","content":"Selected Answer: B\nAnswer is B, Transit gateway can't route overlapping networks, the solution for this is privatelink: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-privatelink.html","timestamp":"1700763360.0","comment_id":"1078634","upvote_count":"4"},{"comment_id":"1005750","timestamp":"1694522400.0","content":"Selected Answer: D\nThanks to rhinozD. Please check the side by sode comparision at the bottom of this page https://tomgregory.com/cross-account-vpc-access-in-aws","poster":"RVivek","upvote_count":"2","comments":[{"poster":"[Removed]","timestamp":"1695585060.0","upvote_count":"5","content":"In that same document you shared it says:\nNo-overlapping CIDR block between VPCs possible for Transit Gateway.\nSo it cannot be D.","comment_id":"1016144"}]},{"poster":"ixdb","content":"B is right.,","timestamp":"1692098100.0","upvote_count":"2","comment_id":"981569"},{"poster":"Just_Ninja","content":"Selected Answer: B\nB. is the right Solution! \nDue to AWS's Transit Gateway not supporting same CIDRs (https://aws.amazon.com › transit-gateway › faqs), the most viable solution is the deployment of a Network Load Balancer (NLB) in each VPC. However, it's crucial to note that NLB operates similar to a NAT Gateway, allowing only incoming requests. After an incoming request is accepted, the NLB can then provide a response.","upvote_count":"3","timestamp":"1689165720.0","comment_id":"949782"},{"upvote_count":"1","comment_id":"949210","poster":"SVGoogle89","content":"AWS Transit Gateway doesn’t support routing between Amazon VPCs with identical CIDRs. If you attach a new Amazon VPC that has a CIDR which is identical to an already attached Amazon VPC, AWS Transit Gateway will not propagate the new Amazon VPC route into the AWS Transit Gateway route table.","timestamp":"1689097320.0"},{"upvote_count":"1","comment_id":"946859","content":"I’ll lean towards B. For D, transit gateway is really expensive and does get the job done. There is also a need for NAT gateway as by default all AWS API traffic passes through the public internet. Hence, PrivateLink endpoints are for.","poster":"habros","timestamp":"1688871900.0"},{"upvote_count":"1","poster":"FunkyFresco","content":"Selected Answer: D\nI go with option D. It makes more sense to me.","timestamp":"1687638840.0","comment_id":"932907"},{"comment_id":"927305","poster":"allen_devops","timestamp":"1687163700.0","upvote_count":"3","content":"I think the correct answer is B. Please note all service team is using the same cidr block for their vpc. It's impossible to add them in the same network mesh using vpc peering and transit gateway."},{"timestamp":"1686031740.0","poster":"Arnaud92","comments":[{"comment_id":"924254","upvote_count":"2","content":"Please read the \"Side-by-side comparison\" part at the end of the post.\nD is wrong.\nB is correct.","poster":"rhinozD","timestamp":"1686837660.0"}],"content":"Selected Answer: D\nsee https://tomgregory.com/cross-account-vpc-access-in-aws/ , Option 3\nThe use of a central hub reduce the complexity for 20 accounts\nneed an additional account to avoid cidr block collision, in the link they put the transit gateway in one of existing account","comment_id":"915944","upvote_count":"2"},{"timestamp":"1685969820.0","poster":"youonebe","comment_id":"915454","content":"Answer is D.\nOption B is incorrect because it requires creating a Network Load Balancer in each of the microservice VPCs and using AWS PrivateLink to create VPC endpoints. This would result in a lot of configuration changes for each service team and increased complexity.","upvote_count":"1"},{"comment_id":"897277","content":"B is the right answer","upvote_count":"2","timestamp":"1684042800.0","poster":"devnv"},{"upvote_count":"4","timestamp":"1683706380.0","comment_id":"893742","content":"Selected Answer: B\nB is correct","poster":"ParagSanyashiv"},{"timestamp":"1683514200.0","comment_id":"891761","content":"Option D is correct to me.","upvote_count":"3","poster":"PhuocT"},{"poster":"kassem77","content":"Option D is the correct solution to meet the requirements.","upvote_count":"3","comments":[{"content":"see https://tomgregory.com/cross-account-vpc-access-in-aws/ , Option 3\nThe use of a central hub reduce the complexity for 20 accounts","timestamp":"1685864040.0","upvote_count":"1","comments":[{"upvote_count":"1","timestamp":"1686031680.0","comment_id":"915943","poster":"Arnaud92","content":"D is correct , central hub reduce complexity , need an additional account to avoid cidr block collision, in the link they put the transit gateway in one of existing account"}],"comment_id":"914238","poster":"Arnaud92"}],"comment_id":"890310","timestamp":"1683318000.0"}],"isMC":true,"answer_images":[],"question_images":[]},{"id":"HuBwZNBeaGHNgmTH7zEO","question_text":"An Amazon EC2 instance is running in a VPC and needs to download an object from a restricted Amazon S3 bucket. When the DevOps engineer tries to download the object, an AccessDenied error is received.\n\nWhat are the possible causes for this error? (Choose two.)","timestamp":"2023-05-14 07:44:00","topic":"1","answer":"BD","answers_community":["BD (100%)"],"answer_images":[],"question_id":342,"exam_id":23,"answer_ET":"BD","url":"https://www.examtopics.com/discussions/amazon/view/109205-exam-aws-certified-devops-engineer-professional-dop-c02/","discussion":[{"upvote_count":"3","poster":"yuliaqwerty","timestamp":"1720530060.0","comment_id":"1117590","content":"I think B and D"},{"poster":"Jamshif01","timestamp":"1719329520.0","content":"ACCESS DENIED - you got it","comment_id":"1105433","upvote_count":"2"},{"timestamp":"1710254820.0","content":"Selected Answer: BD\nIMHO it is BD","upvote_count":"3","comment_id":"1005755","poster":"RVivek"},{"comment_id":"970182","content":"Selected Answer: BD\nBD\nNot an error though. Misconfiguration.","poster":"vherman","timestamp":"1706885520.0","upvote_count":"4"},{"upvote_count":"2","poster":"FunkyFresco","comment_id":"933516","content":"Selected Answer: BD\nB and D for sure.","timestamp":"1703510640.0"},{"timestamp":"1700145780.0","content":"BD are the answers I got","comment_id":"899192","poster":"OrganizedChaos25","upvote_count":"1"},{"content":"BD are correct","comment_id":"897279","timestamp":"1699947840.0","upvote_count":"1","poster":"devnv"}],"answer_description":"","question_images":[],"isMC":true,"choices":{"A":"The S3 bucket default encryption is enabled.","E":"S3 Versioning is enabled.","C":"The object has been moved to S3 Glacier.","B":"There is an error in the S3 bucket policy.","D":"There is an error in the IAM role configuration."},"unix_timestamp":1684043040},{"id":"8kWrXVEdig08DSw4tN37","isMC":true,"answer_images":[],"unix_timestamp":1684043160,"answer":"A","topic":"1","answer_description":"","exam_id":23,"answers_community":["A (100%)"],"discussion":[{"content":"Selected Answer: A\nI’ll use config management tool as well. In this case Opsworks (Chef/Puppet).","upvote_count":"6","comment_id":"946861","poster":"habros","comments":[{"poster":"Exto1124","comment_id":"1254518","timestamp":"1721836740.0","upvote_count":"1","content":"But how the files content (get actual nodes list) is updated in that case?"}],"timestamp":"1688872200.0"},{"poster":"thanhnv142","upvote_count":"5","content":"Selected Answer: A\nA is correct: <wants to use a grid system> means opswork stacks\nB, C and D: no mention of opswork stack","timestamp":"1706948580.0","comment_id":"1139082"},{"timestamp":"1734723900.0","upvote_count":"2","poster":"youonebe","content":"Selected Answer: A\nAWS OpsWorks services have reached end of life and have been disabled for both new and existing customers. Will this question surface in the exam?\n\n\nhttps://aws.amazon.com/blogs/mt/migrate-your-aws-opsworks-stacks-to-aws-systems-manager/","comment_id":"1329653"},{"timestamp":"1729557300.0","poster":"hayjaykay","content":"D. \nThis approach ensures that your nodes.config file is kept up-to-date with minimal manual intervention. The script dynamically adjusts the cluster configuration by reflecting changes in the security group, making the process seamless. Efficient and automated—just the way it should be!","comment_id":"1301344","upvote_count":"1"},{"upvote_count":"1","poster":"Cloudxie","comment_id":"1276538","timestamp":"1725265560.0","content":"D is the best"},{"comment_id":"1014399","timestamp":"1695406380.0","upvote_count":"2","content":"Selected Answer: A\n1\nThe best solution to meet the company's requirements is to use AWS OpsWorks Stacks to layer the server nodes of the cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event.","poster":"Dushank"},{"timestamp":"1686838620.0","upvote_count":"2","poster":"rhinozD","content":"Selected Answer: A\nA is correct.\nThis event occurs on all of the stack's instances when one of the following occurs:\nAn instance enters or leaves the online state.\nYou associate an Elastic IP address with an instance or disassociate one from an instance.\nYou attach an Elastic Load Balancing load balancer to a layer, or detach one from a layer.\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html","comment_id":"924269"},{"comment_id":"897282","poster":"devnv","upvote_count":"2","timestamp":"1684043160.0","content":"A is correct"}],"answer_ET":"A","timestamp":"2023-05-14 07:46:00","question_images":[],"choices":{"C":"Create an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the file’s most recent members. Upload the new file to the S3 bucket.","A":"Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event.","B":"Put the file nodes.config in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services.","D":"Create a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster."},"question_text":"A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated, listing the IP addresses of the current node members of that cluster.\n\nThe company wants to automate the task of adding new nodes to a cluster.\n\nWhat can a DevOps engineer do to meet these requirements?","question_id":343,"url":"https://www.examtopics.com/discussions/amazon/view/109206-exam-aws-certified-devops-engineer-professional-dop-c02/"},{"id":"Sk7z55CQUXiqJTCFLRoI","answer":"BD","answer_description":"","exam_id":23,"question_text":"A DevOps engineer is working on a data archival project that requires the migration of on-premises data to an Amazon S3 bucket. The DevOps engineer develops a script that incrementally archives on-premises data that is older than 1 month to Amazon S3. Data that is transferred to Amazon S3 is deleted from the on-premises location. The script uses the S3 PutObject operation.\n\nDuring a code review, the DevOps engineer notices that the script does not verify whether the data was successfully copied to Amazon S3. The DevOps engineer must update the script to ensure that data is not corrupted during transmission. The script must use MD5 checksums to verify data integrity before the on-premises data is deleted.\n\nWhich solutions for the script will meet these requirements? (Choose two.)","answer_ET":"BD","timestamp":"2023-05-14 07:49:00","isMC":true,"question_id":344,"url":"https://www.examtopics.com/discussions/amazon/view/109207-exam-aws-certified-devops-engineer-professional-dop-c02/","discussion":[{"content":"Selected Answer: BD\nB. Explanation: When using the S3 PutObject operation, you can include the MD5 checksum of the object in the Content-MD5 parameter of the request. Amazon S3 will calculate the MD5 checksum of the object and compare it to the provided checksum. If the checksums do not match, Amazon S3 will return an error response, indicating that the data integrity check failed. This way, you can ensure that the data was successfully copied to Amazon S3 without corruption.\n\nD. Explanation: When you use the S3 PutObject operation, it returns an ETag in the response, which is the MD5 checksum of the object that was stored in Amazon S3. After performing the upload, you can check the returned ETag against the MD5 checksum you have locally calculated. If they match, it means the data was transferred successfully without corruption. If they don't match, it indicates a data integrity issue, and you can take appropriate actions.","upvote_count":"11","comment_id":"962910","poster":"haazybanj","timestamp":"1706208960.0"},{"poster":"dzn","timestamp":"1724640960.0","upvote_count":"3","content":"Selected Answer: BD\nIf the object was created by a PutObject, PostObject, or Copy operation, or via the AWS Management Console, and the object is either plain text or encrypted with server-side encryption using the Amazon S3 managed key ( SSE-S3), the object's ETag is the MD5 digest of the object data.","comment_id":"1159411"},{"comment_id":"1139086","content":"Selected Answer: BD\nB and D are correct: <verify whether the data was successfully copied to Amazon S3> means we need to check <operation call’s return status> code. <use MD5 checksums to verify data integrity> means we need to check ETag\nA: no mention of ETag\nC and E: no mention of ETag or return status code","upvote_count":"3","poster":"thanhnv142","timestamp":"1722666600.0"},{"content":"Selected Answer: BD\nBD\nrefer this link: https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html","upvote_count":"4","comment_id":"924274","poster":"rhinozD","timestamp":"1702657440.0"},{"comment_id":"897284","content":"BD are correct","upvote_count":"3","poster":"devnv","timestamp":"1699948140.0"}],"choices":{"C":"Include the checksum digest within the tagging parameter as a URL query parameter.","B":"Include the MD5 checksum within the Content-MD5 parameter. Check the operation call’s return status to find out if an error was returned.","E":"Include the checksum digest within the Metadata parameter as a name-value pair. After upload, use the S3 HeadObject operation to retrieve metadata from the object.","D":"Check the returned response for the ETag. Compare the returned ETag against the MD5 checksum.","A":"Check the returned response for the VersionId. Compare the returned VersionId against the MD5 checksum."},"topic":"1","unix_timestamp":1684043340,"answers_community":["BD (100%)"],"question_images":[],"answer_images":[]},{"id":"otzvhR1hHqc830nFi5ZA","question_images":[],"topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/105235-exam-aws-certified-devops-engineer-professional-dop-c02/","choices":{"B":"Configure an Amazon EventBridge event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an Amazon Simple Notification Service (Amazon SNS) topic.","A":"Configure AWS KMS to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.","D":"Configure AWS Security Hub to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.","C":"Develop an AWS Config custom rule that publishes to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old."},"question_text":"A company uses AWS Key Management Service (AWS KMS) keys and manual key rotation to meet regulatory compliance requirements. The security team wants to be notified when any keys have not been rotated after 90 days.\nWhich solution will accomplish this?","timestamp":"2023-04-05 02:32:00","question_id":345,"answer":"C","exam_id":23,"answer_description":"","answer_ET":"C","unix_timestamp":1680654720,"answers_community":["C (100%)"],"discussion":[{"poster":"thanhnv142","content":"C is correct\nA is not because KMS does not provide this function","comment_id":"1133447","timestamp":"1722086820.0","upvote_count":"5"},{"content":"Answer C. AWS Config","upvote_count":"3","poster":"yuliaqwerty","timestamp":"1720356600.0","comment_id":"1115928"},{"poster":"habros","content":"Selected Answer: C\nC. Config rules notifies.","timestamp":"1704381840.0","upvote_count":"3","comment_id":"942803"},{"content":"Selected Answer: C\nAre these questions really came from DOP-C02?","upvote_count":"3","poster":"Toptip","comment_id":"936928","timestamp":"1703789700.0"},{"timestamp":"1702037100.0","comment_id":"918097","content":"Selected Answer: C\nC makes sense. it should be a custom rule. Rule \"access-keys-rotated\" checks for access keys, not KMS keys.","upvote_count":"2","poster":"madperro"},{"timestamp":"1697389320.0","comment_id":"871136","content":"C it is","upvote_count":"1","poster":"alce2020"},{"comment_id":"863848","poster":"ele","timestamp":"1696681320.0","upvote_count":"1","content":"Selected Answer: C\ncustom config: C"},{"content":"Selected Answer: C\nLooks like C, actually there is a managed rule for this:\nhttps://docs.aws.amazon.com/config/latest/developerguide/access-keys-rotated.html\nanyway trusted advisor cannot be used as there is no such check, also KMS does not have this action, security hub is not conducting any active checks just react to events","upvote_count":"4","poster":"asfsdfsdf","comments":[{"poster":"zijo","comment_id":"1156606","upvote_count":"1","content":"IAM Access Key & KMS key are different. The managed rule is for IAM Access key","timestamp":"1724343240.0"},{"upvote_count":"3","content":"access key?","poster":"s50600822","timestamp":"1705167840.0","comment_id":"950809"}],"timestamp":"1696577880.0","comment_id":"862749"},{"timestamp":"1696535820.0","content":"Selected Answer: C\nC for me. A there no such functionality, B i checked trusted advisor there is no such kms days, d is aggregator for config, guardduty. So you need config for D","upvote_count":"2","comment_id":"862441","poster":"Dimidrol"},{"comments":[{"content":"When you enable a control in Security hub it will automatically create a Config. There are 4 KMS related controls in security hub but none of them is about the rotation age. In this case you need to create a custom Config.","timestamp":"1708412160.0","upvote_count":"1","comment_id":"985552","poster":"beanxyz"},{"timestamp":"1701974340.0","comment_id":"917414","content":"• Option D is not the correct answer because AWS Security Hub is primarily focused on aggregating and managing security findings, and it does not have a specific feature to monitor the age of AWS KMS keys.","poster":"Manny20","upvote_count":"2"}],"poster":"lqpO_Oqpl","comment_id":"861638","content":"Tell me Why not D..","upvote_count":"1","timestamp":"1696465920.0"}],"isMC":true}],"exam":{"isImplemented":true,"numberOfQuestions":355,"isBeta":false,"provider":"Amazon","isMCOnly":true,"name":"AWS Certified DevOps Engineer - Professional DOP-C02","lastUpdated":"11 Apr 2025","id":23},"currentPage":69},"__N_SSP":true}