{"pageProps":{"questions":[{"id":"6YXt5rJq9rgE4eg3nbOi","question_images":[],"choices":{"D":"Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB to encrypt data in transit.","A":"Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest.","C":"Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.","B":"Use the AWS root account to log in to the AWS Management Console. Upload the company’s encryption certificates. While in the root account, select the option to turn on encryption for all data at rest and in transit for the account."},"question_id":171,"answer_ET":"C","answers_community":["C (100%)"],"unix_timestamp":1673724420,"answer":"C","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/95325-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-01-14 20:27:00","discussion":[{"content":"Selected Answer: C\nAWS KMS can be used to encrypt the EBS and Aurora database storage at rest.\nACM can be used to obtain an SSL/TLS certificate and attach it to the ALB. This encrypts the data in transit between the clients and the ALB.\n\nA is incorrect because it suggests using ACM to encrypt the EBS, which is not the correct service for encrypting EBS.\n\nB is incorrect because relying on the AWS root account and selecting an option in the AWS Management Console to enable encryption for all data at rest and in transit is not a valid approach.\n\nD is incorrect because BitLocker is not a suitable solution for encrypting data in AWS services. It is primarily used for encrypting data on Windows-based operating systems. Additionally, importing TLS certificate keys to AWS KMS and attaching them to the ALB is not the recommended approach for encrypting data in transit.","upvote_count":"11","comment_id":"936622","poster":"cookieMr","timestamp":"1703772480.0"},{"content":"Got this question in exam today","poster":"Awsbeginner87","upvote_count":"4","timestamp":"1728489540.0","comment_id":"1192395"},{"content":"To encrypt data at rest, AWS Key Management Service (AWS KMS) can be used to encrypt EBS volumes and Aurora database storage.\n\nTo encrypt data in transit, an AWS Certificate Manager (ACM) certificate can be attached to the Application Load Balancer (ALB) to enable HTTPS and TLS encryption.","poster":"Ruffyit","comment_id":"1077152","upvote_count":"1","timestamp":"1716361200.0"},{"timestamp":"1711352220.0","upvote_count":"2","comment_id":"1016478","poster":"TariqKipkemei","content":"Selected Answer: C\nUse AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit"},{"timestamp":"1710177900.0","content":"Selected Answer: C\nC is the best answer.\n\nTo encrypt data at rest, AWS Key Management Service (AWS KMS) can be used to encrypt EBS volumes and Aurora database storage.\n\nTo encrypt data in transit, an AWS Certificate Manager (ACM) certificate can be attached to the Application Load Balancer (ALB) to enable HTTPS and TLS encryption.","comment_id":"1004940","upvote_count":"3","poster":"Guru4Cloud"},{"timestamp":"1702330020.0","content":"Selected Answer: C\nOption C it's correct","comment_id":"920937","poster":"MAMADOUG","upvote_count":"2"},{"poster":"Bmarodi","timestamp":"1701368220.0","content":"Selected Answer: C\nOption C fulfills the requirements.","upvote_count":"2","comment_id":"910419"},{"poster":"techhb","timestamp":"1689595500.0","upvote_count":"4","content":"Selected Answer: C\nC is correct ,A REVERSES the work ofeach service.","comment_id":"778996"},{"comment_id":"777867","content":"Selected Answer: C\nC is correct!","poster":"Aninina","upvote_count":"4","timestamp":"1689514140.0"},{"content":"Selected Answer: C\nc is correct answer","timestamp":"1689355620.0","comment_id":"775864","poster":"mhmt4438","upvote_count":"3"}],"isMC":true,"question_text":"A company is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit.\n\nWhich solution will meet these requirements?","exam_id":31,"answer_images":[],"answer_description":""},{"id":"6Sf9UqkC0fYulh5O2o31","isMC":true,"choices":{"D":"Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a compute optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select the largest tables.","B":"Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.","C":"Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.","A":"Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping to select all tables."},"answer_ET":"C","answer_images":[],"discussion":[{"upvote_count":"16","comment_id":"797909","poster":"aakashkumar1999","content":"Selected Answer: C\nC : because we need SCT to convert from Oracle to PostgreSQL, and we need memory optimized machine for databases not compute optimized.","timestamp":"1691143200.0","comments":[{"upvote_count":"8","comments":[{"upvote_count":"20","comment_id":"1004937","content":"A memory-optimized replication instance is recommended because the database has a high number of reads and writes. Memory-optimized instances are designed to deliver fast performance for workloads that process large data sets in memory.","comments":[{"upvote_count":"2","poster":"hissein","content":"thank you","comment_id":"1021553","timestamp":"1711814640.0"}],"poster":"Guru4Cloud","timestamp":"1710177420.0"},{"poster":"pentium75","content":"Maybe it doesn't matter, but in D we create a table mapping only for \"the largest tables\" while obviously we need \"all tables\" as in C.","upvote_count":"8","comment_id":"1107467","timestamp":"1719546720.0"}],"poster":"hissein","content":"why it is memory optimized and not compute optimized machine ?","comment_id":"1001610","timestamp":"1709825760.0"}]},{"comment_id":"1155462","upvote_count":"2","timestamp":"1724232240.0","content":"B. Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.\n\nThis approach leverages AWS DataSync for the initial data migration, which is optimized for high-speed transfer of large amounts of data. Then, AWS DMS is used to create a replication task with full load plus change data capture (CDC), ensuring ongoing synchronization between the on-premises Oracle database and Amazon Aurora PostgreSQL. By selecting all tables, the migration process ensures that all applications can continue to read from and write to the database without interruption during the migration period.","poster":"TechStuff"},{"poster":"MrPCarrot","comment_id":"1149911","upvote_count":"1","content":"Answer is C - AWS Schema Conversion Tool (AWS SCT) supports heterogeneous database migrations by automatically converting the source database schema and a majority of the custom code to a format compatible with the target database.","timestamp":"1723613040.0"},{"content":"Selected Answer: C\nHas to be SCT + DMS for all the tables so C is the choice. Why do you need SCT? Read this:\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-data-from-an-on-premises-oracle-database-to-aurora-postgresql.html","poster":"awsgeek75","comment_id":"1125205","comments":[{"poster":"awsgeek75","timestamp":"1721233020.0","content":"AB-> DataSync has nothing to do with DB Migration (https://aws.amazon.com/datasync/)\nD: Only migrates the largest table\n\nI think these questions are more for seeing how much of AWS product catalogue can you remember efficiently with associated features. Afterall AWS SA is also a salesperson for the right product! I now look at product cheat-sheets to look them up at work.","comment_id":"1125207","upvote_count":"2"}],"timestamp":"1721232720.0","upvote_count":"2"},{"content":"Selected Answer: C\nOracle -> PostgreSQL, we need SCT, thus A and B are out.\nD maps only \"the largest tables\" but we need all tables","timestamp":"1719546840.0","upvote_count":"3","comment_id":"1107470","poster":"pentium75"},{"upvote_count":"4","content":"Selected Answer: C\nAnother reason to rule out D is because it states “a table mapping to select the largest tables”, whereas selecting all tables (as stated in option C) in the table mapping is necessary to ensure a comprehensive migration.","poster":"ansagr","comment_id":"1092891","timestamp":"1718049060.0"},{"content":"because we need SCT to convert from Oracle to PostgreSQL, and we need memory optimized machine for databases not compute optimized.\nA memory-optimized replication instance is recommended because the database has a high number of reads and writes. Memory-optimized instances are designed to deliver fast performance for workloads that process large data sets in memory.","comment_id":"1077160","timestamp":"1716361920.0","upvote_count":"2","poster":"Ruffyit"},{"upvote_count":"2","poster":"Po_chih","comment_id":"1024519","timestamp":"1712212440.0","content":"Selected Answer: C\nbecause we need SCT to convert from Oracle to PostgreSQL, and we need memory optimized machine for databases not compute optimized.\nhttps://repost.aws/zh-Hant/knowledge-center/dms-optimize-aws-sct-performance"},{"comment_id":"1016502","content":"Selected Answer: C\nOracle database to Amazon Aurora PostgreSQL = AWS Schema Conversion Tool \nHigh number of reads and writes = memory optimized replication instance","upvote_count":"3","poster":"TariqKipkemei","timestamp":"1711353600.0"},{"comment_id":"1004934","timestamp":"1710177000.0","poster":"Guru4Cloud","content":"Selected Answer: C\nA memory-optimized replication instance is recommended because the database has a high number of reads and writes. Memory-optimized instances are designed to deliver fast performance for workloads that process large data sets in memory.","upvote_count":"2"},{"content":"Selected Answer: C\nDataSync is for file-level synch, so A and B can be excluded. C is better than D because memory-optimized instances are recommended to handle the high number of reads and writes","comment_id":"986418","poster":"_d1rk_","timestamp":"1708519980.0","upvote_count":"2"},{"comment_id":"985057","poster":"ukivanlamlpi","timestamp":"1708337520.0","comments":[{"poster":"pentium75","comment_id":"1107468","timestamp":"1719546780.0","content":"Oracle -> PostgreSQL requires SCT","upvote_count":"1"}],"upvote_count":"2","content":"Selected Answer: A\nwhy not a? only capture the change is sufficient"},{"comment_id":"941261","poster":"Mmmmmmkkkk","timestamp":"1704239340.0","content":"Bbbbbb","upvote_count":"1"},{"poster":"cookieMr","comment_id":"936628","content":"Selected Answer: C\nThe AWS SCT is used to convert the schema and code of the Oracle database to be compatible with Aurora PostgreSQL. AWS DMS is utilized to migrate the data from the Oracle database to Aurora PostgreSQL. Using a memory-optimized replication instance is recommended to handle the high number of reads and writes during the migration process.\nBy creating a full load plus CDC replication task, the initial data migration is performed, and ongoing changes in the Oracle database are continuously captured and applied to the Aurora PostgreSQL database. Selecting all tables for table mapping ensures that all the applications writing to the same tables are migrated.\n\nOption A & B are incorrect because using AWS DataSync alone is not sufficient for database migration and data synchronization.\n\nOption D is incorrect because using a compute optimized replication instance is not the most suitable choice for handling the high number of reads and writes.","upvote_count":"3","timestamp":"1703772840.0"},{"timestamp":"1701047280.0","comment_id":"907618","content":"BBBBBBBBBBBBB","upvote_count":"2","poster":"omoakin"},{"content":"B chatgpt","poster":"SimiTik","upvote_count":"2","comment_id":"872281","timestamp":"1697507280.0"},{"comment_id":"813591","content":"DMS+SCT for Oracle to Aurora PostgreSQL migration\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-oracle-database-to-aurora-postgresql-using-aws-dms-and-aws-sct.html","timestamp":"1692396480.0","upvote_count":"2","poster":"KZM"},{"poster":"icurfer","timestamp":"1690721640.0","content":"https://aws.amazon.com/ko/premiumsupport/knowledge-center/dms-memory-optimization/","comment_id":"792942","upvote_count":"1"},{"poster":"dark_firzen","content":"Selected Answer: C\nIt has to be either C or D because it requires Schema Conversion Tool to convert Oracle database to Amazon Aurora PostgreSQL. C would be the better choice here because it replicates a memory optimized instance, which is recommended for databases. Also, the database must be kept in sync, so they require mapping to select all tables.","comment_id":"791032","timestamp":"1690567140.0","upvote_count":"3"},{"comment_id":"787065","poster":"bullrem","content":"A or C are both valid options. Both options involve using AWS DataSync for the initial migration, and then using AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task for ongoing data synchronization.\nOption A: Uses a memory optimized replication instance.\nOption C: Uses a compute optimized replication instance.\n\nOption A is a better choice for migrations where the data is more complex and may require more memory.\nOption C is a better choice for migrations that require more processing power.\nIt is also depend on the size of the data, the complexity of the data, and the resources available in the target Aurora cluster.","upvote_count":"1","timestamp":"1690233120.0"},{"timestamp":"1690172760.0","content":"Why would you not use the schema conversion tool, which is designed specifically to covert form one db engine to another. It can convert Oracle to Aurora PostgreSQL, see https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Welcome.html. Then it is a choice of C or D. Since you want to move all tables C makes more sense that D. \nA and B are wrong since DataSync deals with data not databases, see https://aws.amazon.com/datasync/faqs/.","upvote_count":"4","poster":"JayBee65","comment_id":"786205"},{"upvote_count":"1","comments":[{"content":"Changing my answer to C as you need schema conversion from Oracle the PostgreSQL","poster":"brownest","comment_id":"787777","timestamp":"1690290660.0","upvote_count":"2"}],"poster":"brownest","content":"Selected Answer: A\nInitial migration is full using DataSync and on-going replication is through CDC for the changes. The full load was already performed so no need to do it again as with Answer B.","timestamp":"1690033380.0","comment_id":"784454"},{"content":"Correct answer is C","timestamp":"1689661260.0","poster":"TapasGhosh","comment_id":"779746","upvote_count":"2"},{"poster":"wmp7039","timestamp":"1689507060.0","upvote_count":"1","comment_id":"777708","content":"Selected Answer: A\nA is correct. Initial migration is full using DataSync and on-going replication is through CDC Task - https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html","comments":[]},{"timestamp":"1689397140.0","content":"B. Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.\n\nAWS DataSync can be used for the initial migration of the data, it can transfer large amount of data quickly and securely over the network. AWS Database Migration Service (AWS DMS) can be used to replicate changes made to the data in the source database to the target database. A full load plus CDC replication task allows for the initial migration of the data and then continuously replicate any changes made to the data in the source database to the target database. This will ensure that the data is kept in sync across both databases throughout the migration process. Selecting all tables in the table mapping will ensure that all data is replicated, as the migration process will be done in several steps, it will be important to make sure that all data is kept in sync.","poster":"forzadejan","upvote_count":"3","comment_id":"776221"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.Types.html","timestamp":"1689392640.0","comment_id":"776186","poster":"venice1234","upvote_count":"1"},{"upvote_count":"3","poster":"mhmt4438","comment_id":"775866","timestamp":"1689355740.0","content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/46704-exam-aws-certified-solutions-architect-associate-saa-c02/"}],"question_text":"A company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several applications that write to the same tables. The applications need to be migrated one by one with a month in between each migration. Management has expressed concerns that the database has a high number of reads and writes. The data must be kept in sync across both databases throughout the migration.\n\nWhat should a solutions architect recommend?","topic":"1","answers_community":["C (92%)","8%"],"url":"https://www.examtopics.com/discussions/amazon/view/95326-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C","answer_description":"","question_images":[],"unix_timestamp":1673724540,"question_id":172,"exam_id":31,"timestamp":"2023-01-14 20:29:00"},{"id":"2a7TIdOoZM1btnNN0K5R","choices":{"C":"Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group for the application layer. Move the database to a memory optimized instance type to store and serve users’ images.","A":"Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move the database to an Amazon DynamoDB table. Use Amazon S3 to store and serve users’ images.","D":"Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users’ images.","B":"Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS DB instance with multiple read replicas to serve users’ images."},"answer":"D","answer_ET":"D","unix_timestamp":1673599980,"isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/94990-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","discussion":[{"comment_id":"790806","content":"Selected Answer: B\nB and D very similar with D being the 'best' solution but it is not the one that requires the least amount of development changes as the application would need to be changed to store images in S3 instead of DB","upvote_count":"15","timestamp":"1690554840.0","comments":[{"content":"B is wrong because single \"RDS DB instance\" is not HA.\n\nNo one says that the images are currently stored in S3. Also the requirement is \"least amount of change [not \"no change\"] to the application\".","timestamp":"1719547080.0","poster":"pentium75","upvote_count":"8","comment_id":"1107474"}],"poster":"PDR"},{"comment_id":"775802","content":"Selected Answer: D\nfor \"Highly available\": Multi-AZ &\nfor \"least amount of changes to the application\": Elastic Beanstalk automatically\nhandles the deployment, from capacity provisioning, load balancing, auto-scaling to\napplication health monitoring","poster":"Aninina","upvote_count":"12","timestamp":"1689350580.0"},{"content":"Selected Answer: D\nWhen we are talking about sharing static content and options have S3 why would I select any other option.","poster":"SirDNS","upvote_count":"1","timestamp":"1742666520.0","comment_id":"1401997"},{"timestamp":"1739963700.0","content":"Selected Answer: D\nD is the most suitable option here.","comment_id":"1358677","upvote_count":"1","poster":"satyaammm"},{"timestamp":"1719842940.0","content":"Selected Answer: D\nA: Requires changing EC2 application to Lambda. Seems like a big change\nB: RDS DB is not best option for serve images and also single instance isn't HA\nC: Memory optimised instance is not HA\nD: Multi-AZ EBS is lift and shift for EC2 front-end and app later. RDS Multi AZ is HA. S3 for static images is best performance/scalability/availability.","comment_id":"1111338","upvote_count":"4","poster":"awsgeek75"},{"comment_id":"1092909","timestamp":"1718050200.0","upvote_count":"2","poster":"ansagr","content":"Selected Answer: D\nUsing Amazon RDS for serving images might not be the optimal solution, as RDS is more suitable for storing structured data in a relational database rather than BLOBs like images. Storing and serving images can be more efficiently handled by object storage services like Amazon S3."},{"timestamp":"1716362940.0","comment_id":"1077174","poster":"Ruffyit","upvote_count":"2","content":"Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users’ images"},{"poster":"rlamberti","content":"Option B - DB is not a good option to store images. Read replicas won't improve HA for write, only scalates reading IO. Therefore no true HA achieved.\nD is the goal for me.","comment_id":"1051858","timestamp":"1713877740.0","upvote_count":"2"},{"timestamp":"1711353900.0","upvote_count":"3","poster":"TariqKipkemei","content":"Selected Answer: D\nUse load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users’ images","comment_id":"1016507"},{"poster":"Guru4Cloud","content":"Selected Answer: D\nUse Elastic Beanstalk load-balanced environments for the web and app tiers. This provides auto scaling and high availability with minimal effort.\nMove the database to RDS Multi-AZ. This handles scaling reads and storage, and provides HA with automated failover.\nUse S3 for serving user images. S3 is highly scalable and durable storage.\nThe application code remains unchanged using this approach.","comment_id":"1004933","upvote_count":"4","timestamp":"1710176820.0"},{"content":"Selected Answer: A\nAWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.\n\nI don't quite understand why people choose D.","comment_id":"937468","poster":"Mia2009687","timestamp":"1703830080.0","upvote_count":"1"},{"comment_id":"936636","content":"Selected Answer: D\nBy using load-balanced Multi-AZ AWS EBS, you achieve scalability and high availability for both layers without requiring significant changes to the application. Moving the DB to an RDS Multi-AZ DB ensures high availability and automatic failover. Storing and serving users' images through S3 provides a scalable and highly available solution.\n\nA is incorrect because using S3 for the front-end layer and Lambda for the application layer would require significant changes to the application architecture. Moving the DB to DynamoDB would require rewriting the DB-related code.\n\nB is incorrect because using load-balanced Multi-AZ AWS EBS environments and an RDS DB with read replicas for serving images would be a more suitable solution. RDS with read replicas can handle the image-serving workload more efficiently than using S3 for this purpose.\n\nC is incorrect because using S3 for the front-end layer and an ASG of EC2 for the application layer would require modifying the application architecture. Storing and serving images from a memory-optimized EC2 type may not be the most efficient and scalable approach compared to using S3.","upvote_count":"5","timestamp":"1703773320.0","poster":"cookieMr"},{"comment_id":"926855","poster":"markw92","content":"\"least amount of change to the application.\" - A has lots of changes, completely revamping the application and lots of new pieces. D is closest with only addition of s3 to store images which is right move. You do not want images to store in any database anyway.","comments":[{"upvote_count":"2","timestamp":"1708390140.0","poster":"aaroncelestin","content":"Thats what I was thinking, but the question doesn't mention anything about storing users' images anywhere. Are we supposed to just assume that they wanted to store the images in a DB even though that is a bad idea?","comment_id":"985457"}],"timestamp":"1702931820.0","upvote_count":"4"},{"timestamp":"1701370560.0","content":"Selected Answer: D\nOption D meets the requirements.","poster":"Bmarodi","comment_id":"910448","upvote_count":"2"},{"timestamp":"1695177600.0","poster":"Grace83","upvote_count":"3","comment_id":"844537","content":"D is correct"},{"poster":"focus_23","upvote_count":"3","comment_id":"790480","timestamp":"1690530360.0","content":"Selected Answer: D\nRDS multi AZ."},{"content":"Selected Answer: D\nD is correct as application changes needs to me minimal","poster":"wmp7039","timestamp":"1689507300.0","upvote_count":"3","comment_id":"777709"},{"upvote_count":"3","timestamp":"1689415080.0","comment_id":"776497","content":"Selected Answer: D\nCorrect answer is D","poster":"mhmt4438"},{"content":"Selected Answer: D\nhttps://www.examtopics.com/discussions/amazon/view/24840-exam-aws-certified-solutions-architect-associate-saa-c02/\n\nPlease ExamTopics, review your own answers","upvote_count":"5","timestamp":"1689231180.0","poster":"Morinator","comment_id":"774263"}],"timestamp":"2023-01-13 09:53:00","exam_id":31,"answer_images":[],"question_id":173,"answers_community":["D (75%)","B (23%)","2%"],"answer_description":"","question_text":"A company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the front-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL database. A solutions architect must design a scalable and highly available solution that requires the least amount of change to the application.\n\nWhich solution meets these requirements?"},{"id":"AC2kE701Y33Af22pz57q","question_images":[],"question_text":"An application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-B. Both VPCs are in separate AWS accounts. The network administrator needs to design a solution to configure secure access to EC2 instance in VPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth concerns.\n\nWhich solution will meet these requirements?","answer_images":[],"question_id":174,"exam_id":31,"choices":{"A":"Set up a VPC peering connection between VPC-A and VPC-B.","C":"Attach a virtual private gateway to VPC-B and set up routing from VPC-A.","B":"Set up VPC gateway endpoints for the EC2 instance running in VPC-B.","D":"Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A."},"topic":"1","answers_community":["A (97%)","3%"],"answer_description":"","discussion":[{"timestamp":"1674971520.0","content":"Selected Answer: A\nAWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","poster":"LuckyAro","comment_id":"791429","upvote_count":"19"},{"comment_id":"936651","content":"Selected Answer: A\nA VPC peering connection allows secure communication between instances in different VPCs using private IP addresses without the need for internet gateways, VPN connections, or NAT devices. By setting it up, the application running in VPC-A can directly access the EC2 in VPC-B without going through the public internet or any single point of failure.\n\nB is incorrect because VPC gateway endpoints are used for accessing S3 or DynamoDB from a VPC without going over the internet. They are not designed for establishing connectivity between EC2 instances in different VPCs.\n\nC is incorrect because it would require configuring a VPN connection between the VPCs. This would introduce additional complexity and potential single points of failure.\n\nD is incorrect because creating a private VIF and adding routes would be applicable for establishing a direct connection between on-premises infrastructure and VPC-B using Direct Connect, but it is not suitable for the scenario of communication between EC2 instances in separate VPCs within different AWS accounts.","poster":"cookieMr","timestamp":"1687955280.0","upvote_count":"18"},{"poster":"satyaammm","content":"Selected Answer: A\nVPC peering is the most suitable here.","upvote_count":"1","comment_id":"1358679","timestamp":"1739963760.0"},{"comment_id":"1175728","timestamp":"1710671700.0","upvote_count":"3","poster":"Faridtnx","content":"Selected Answer: A\nYou can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. Peering within the same AZ is free of charge."},{"poster":"lostmagnet001","content":"Selected Answer: A\nI get a little confused about B and A but, because, with a VPC endpoint in B it will work too access from A.","upvote_count":"2","timestamp":"1707061260.0","comments":[{"content":"vpc gw endpoint just dynamo and s3","upvote_count":"2","timestamp":"1726186680.0","comment_id":"1282902","poster":"GPFT"}],"comment_id":"1140305"},{"timestamp":"1703743620.0","comment_id":"1107480","poster":"pentium75","content":"Selected Answer: A\nB is wrong because \"VPC gateway endpoint\" is for S3 or DynamoDB, not EC2\nC is overkill, would require a second gateway in VPC-A, not be HA and have limited bandwidth\nD is wrong because VIF is for Direct Connect, has nothing to do with VPC-to-VPC communication","upvote_count":"5"},{"comment_id":"1077188","poster":"Ruffyit","upvote_count":"2","content":"AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","timestamp":"1700646240.0"},{"upvote_count":"2","comment_id":"1004897","poster":"Guru4Cloud","content":"Selected Answer: A\nA. Set up a VPC peering connection between VPC-A and VPC-B","timestamp":"1694442180.0"},{"upvote_count":"1","timestamp":"1689576960.0","content":"https://www.bing.com/search?pglt=41&q=can+we+do+VPC+peering+across+AWS+accounts&cvid=48a8ceecc85a429c9ddd698b01055890&aqs=edge..69i57j0l8j69i11004.10897j0j1&FORM=ANNAB1&PC=LCTS","poster":"MNotABot","comment_id":"953904"},{"comments":[{"comment_id":"926372","timestamp":"1687052340.0","content":"No, VPC Peering can use across account. \n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","upvote_count":"4","poster":"im6h"}],"comment_id":"923313","poster":"Anmol_1010","timestamp":"1686754140.0","upvote_count":"1","content":"D, VPC PEERINGVIS IN SAME ACCOUNT"},{"timestamp":"1685143080.0","upvote_count":"2","poster":"omoakin","content":"DDDDDDDDDDDDDD","comments":[{"comment_id":"907620","timestamp":"1685143140.0","upvote_count":"1","poster":"omoakin","content":"This is the only viable solution\nCreate a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A"}],"comment_id":"907619"},{"comment_id":"900004","content":"Selected Answer: A\n\"You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account.\"\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","upvote_count":"5","timestamp":"1684320180.0","poster":"michellemeloc"},{"timestamp":"1674924060.0","content":"Selected Answer: A\ncorrect answer is A and as mentioned by JayBee65 below, key reason being that solution should not have a single point of failure and bandwidth restrictions\n\nthe following paragraph is taken from the AWS docs page linked below that backs this up\n\"AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.\"\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","poster":"PDR","comment_id":"790814","upvote_count":"3"},{"comment_id":"781795","comments":[{"poster":"pentium75","content":"B is about a Gateway endpoint, which can be used to connect to S3 or DynamoDB, NOT to another EC2 instance.","upvote_count":"2","comment_id":"1107478","timestamp":"1703743380.0"},{"upvote_count":"4","poster":"JayBee65","content":"Your logic is correct but security is not a requirement here - the requirements are \"The connectivity should not have a single point of failure or bandwidth concerns.\" A VPC gateway endpoint\" would form a single point of failure, so B is incorrect, (and C and D are incorrect for the same reason, they create single points of failure).","comment_id":"786209","timestamp":"1674541920.0"}],"timestamp":"1674181380.0","upvote_count":"2","poster":"LuckyAro","content":"Selected Answer: B\nA VPC endpoint gateway to the EC2 Instance is more specific and more secure than forming a VPC peering that exposes the whole of the VPC infrastructure just for one connection."},{"comment_id":"776498","poster":"mhmt4438","timestamp":"1673784060.0","upvote_count":"3","content":"Selected Answer: A\nCorrect answer is A"},{"upvote_count":"2","comment_id":"775805","content":"Selected Answer: A\nVPC peering allows resources in different VPCs to communicate with each other as if they were within the same network. This solution would establish a direct network route between VPC-A and VPC-B, eliminating the need for a single point of failure or bandwidth concerns.","timestamp":"1673719500.0","poster":"Aninina"},{"poster":"waiyiu9981","content":"Selected Answer: A\nhttps://www.examtopics.com/discussions/amazon/view/27763-exam-aws-certified-solutions-architect-associate-saa-c02/","timestamp":"1673670300.0","comment_id":"775083","upvote_count":"5"}],"url":"https://www.examtopics.com/discussions/amazon/view/95144-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"A","timestamp":"2023-01-14 05:25:00","answer_ET":"A","unix_timestamp":1673670300,"isMC":true},{"id":"0GSIPgy673fZadrPPKF9","question_images":[],"choices":{"C":"Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.","A":"Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.","B":"Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.","D":"Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data with Amazon Athena. Use Amazon EventBridge to schedule an Athena query. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded."},"question_id":175,"answer_ET":"C","answers_community":["C (95%)","5%"],"unix_timestamp":1673603100,"topic":"1","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/94996-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-01-13 10:45:00","discussion":[{"timestamp":"1705255800.0","content":"Selected Answer: C\nAWS Budgets allows you to create budgets for your AWS accounts and set alerts when usage exceeds a certain threshold. By creating a budget for each account, specifying the period as monthly and the scope as EC2 instances, you can effectively track the EC2 usage for each account and be notified when a threshold is exceeded. This solution is the most cost-effective option as it does not require additional resources such as Amazon Athena or Amazon EventBridge.","comment_id":"775808","poster":"Aninina","upvote_count":"14"},{"comment_id":"783941","poster":"alexleely","timestamp":"1705902480.0","content":"C: AWS Budgets allows you to set a budget for costs and usage for your accounts and you can set alerts when the budget threshold is exceeded in real-time which meets the requirement.\n\nWhy not B: B would be the most cost-effective if the requirements didn't ask for real-time notification. You would not incur additional costs for the daily or monthly reports and the notifications. But doesn't provide real-time alerts.","upvote_count":"5"},{"comment_id":"1358680","poster":"satyaammm","upvote_count":"1","timestamp":"1739964000.0","content":"Selected Answer: C\nAWS budgets allow for creating budgets and hence are the most suitable here."},{"timestamp":"1732354440.0","upvote_count":"1","poster":"Ruffyit","comment_id":"1078294","content":"AWS Budgets allows you to create budgets for your AWS accounts and set alerts when usage exceeds a certain threshold. By creating a budget for each account, specifying the period as monthly and the scope as EC2 instances, you can effectively track the EC2 usage for each account and be notified when a threshold is exceeded. This solution is the most cost-effective option as it does not require additional resources such as Amazon Athena or Amazon EventBridge."},{"poster":"vijaykamal","upvote_count":"4","timestamp":"1727610180.0","content":"Selected Answer: C\nOption A and Option B suggest using Cost Explorer to create reports and send notifications. While Cost Explorer is useful for analyzing costs, it does not provide the real-time alerting capability that AWS Budgets offers.\n\nOption D suggests using AWS Cost and Usage Reports integrated with Amazon Athena and Amazon EventBridge, which can be a more complex and potentially costlier solution compared to AWS Budgets for this specific use case. It's also more suitable for fine-grained, custom analytics rather than straightforward threshold-based alerts.","comment_id":"1020722"},{"timestamp":"1727246820.0","upvote_count":"3","poster":"TariqKipkemei","content":"Selected Answer: C\nAWS Budgets was designed to handle this scenario.","comment_id":"1016538"},{"comment_id":"965655","timestamp":"1722181320.0","poster":"Undisputed","upvote_count":"2","content":"Selected Answer: C\nUse AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded."},{"content":"Selected Answer: C\nBy creating a cost budget for each account, specifying the period as monthly and scoping it to EC2, you can track and monitor the costs associated with EC2 specifically. Set an alert threshold in the budget, which will trigger a notification when the specified threshold is exceeded. Configure an SNS to receive the notification, which can be subscribed to by the company to receive immediate alerts.\n\nA and B are not the most cost-effective solutions as they involve using Cost Explorer to create reports, which may not provide real-time notifications when the threshold is exceeded. Additionally, A. suggests using a daily report, while B. suggests using a monthly report, which may not provide the desired level of granularity for immediate notifications.\n\nD involves using Cost and Usage Reports with Athena and EventBridge. This solution provides more flexibility and data analysis capabilities, it is more complex and may incur additional costs for using Athena and generating hourly reports.","poster":"cookieMr","comment_id":"936657","timestamp":"1719578220.0","upvote_count":"3"},{"comment_id":"818470","content":"Selected Answer: D\nI go with D. It says \"as soon as\", \"daily\" reports seems to be a bit longer time frame to wait in my opinion.","timestamp":"1708638240.0","poster":"Samuel03","comments":[{"poster":"Bofi","timestamp":"1709649960.0","content":"Athena can only be use in s3, that is enough to discard D","comment_id":"829982","upvote_count":"2"},{"timestamp":"1708638300.0","upvote_count":"4","content":"Actually, I take that back. It clearly says \"Cost effective.\"","comment_id":"818472","poster":"Samuel03"}],"upvote_count":"2"},{"upvote_count":"3","poster":"mp165","content":"Selected Answer: C\nAgree...C","comment_id":"778125","timestamp":"1705431000.0"},{"upvote_count":"2","timestamp":"1705320360.0","content":"Selected Answer: C\nAnswer is C","comment_id":"776502","poster":"mhmt4438"},{"poster":"venice1234","content":"Selected Answer: C\nhttps://aws.amazon.com/getting-started/hands-on/control-your-costs-free-tier-budgets/","timestamp":"1705298160.0","comment_id":"776192","upvote_count":"2"},{"upvote_count":"3","timestamp":"1705139100.0","comment_id":"774302","poster":"Morinator","content":"Selected Answer: C\nAWS budget IMO, it's done for it"}],"isMC":true,"question_text":"A company wants to experiment with individual AWS accounts for its engineer team. The company wants to be notified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each account.\n\nWhat should a solutions architect do to meet this requirement MOST cost-effectively?","exam_id":31,"answer_images":[],"answer_description":""}],"exam":{"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"provider":"Amazon","isImplemented":true,"isBeta":false,"name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31},"currentPage":35},"__N_SSP":true}