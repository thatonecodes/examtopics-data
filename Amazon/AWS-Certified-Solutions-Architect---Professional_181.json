{"pageProps":{"questions":[{"id":"CqZUop2Z6UDAZ4JTDY5q","unix_timestamp":1662446100,"answer_description":"","isMC":true,"question_text":"A company is running an application on Amazon EC2 instances in three environments: development, testing, and production. The company uses AMIs to deploy the EC2 instances. The company builds the AMIs by using custom deployment scripts and infrastructure orchestration tools for each release in each environment.\nThe company is receiving errors in its deployment process. Errors appear during operating system package downloads and during application code installation from a third-party Git hosting service. The company needs deployments to become more reliable across all environments.\nWhich combination of steps will meet these requirements? (Choose three.)","answers_community":["ACF (86%)","14%"],"exam_id":32,"answer":"ACF","topic":"1","answer_ET":"ACF","question_id":901,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/80521-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"B":"Produce multiple EC2 AMIs, one for each environment, for each release.","C":"Produce one EC2 AMI for each release for use across all environments.","F":"Replace the custom scripts and tools with EC2 Image Builder. Update the deployment process to use AWS CloudFormation.","D":"Mirror the application code to a third-party Git repository that uses Amazon S3 storage. Use the repository for deployment.","E":"Replace the custom scripts and tools with AWS CodeBuild. Update the infrastructure deployment process to use EC2 Image Builder.","A":"Mirror the application code to an AWS CodeCommit Git repository. Use the repository to build EC2 AMIs."},"answer_images":[],"timestamp":"2022-09-06 08:35:00","discussion":[{"comments":[{"poster":"blitzzzz","comment_id":"698788","content":"but why. Is it better for production to use a stable and fixed version?","upvote_count":"1","timestamp":"1666165560.0","comments":[{"comment_id":"900312","content":"you have a stable AMI for prod but for testing and dev AMIs are not the same.\n\nPrefer ABF --> AMI ofr each env and release","timestamp":"1684338060.0","poster":"MikelH93","upvote_count":"1"}]},{"upvote_count":"1","poster":"crerem","timestamp":"1667123040.0","comment_id":"707745","content":"why not doing a diffrent AMI for production, devel an testing ?"}],"comment_id":"660879","upvote_count":"9","content":"Selected Answer: ACF\nHere is a good reference material: https://aws.amazon.com/blogs/mt/create-immutable-servers-using-ec2-image-builder-aws-codepipeline/. My answer is A, C, F because AMIs should be same (immutable) across environments and use Image Builder into CloudFormation.","timestamp":"1662446100.0","poster":"cale"},{"content":"Selected Answer: ABF\nABF makes more sense.. you need to create AMI for each ENV+Release","poster":"dev112233xx","timestamp":"1682705940.0","upvote_count":"2","comment_id":"883815"},{"timestamp":"1680330720.0","content":"Selected Answer: ACF\nUsing the same image in different environments only works with some additional customization after deployment.\nHaving to specific images per environment does only work for simple cases in a simple way. Most cases you end up creating an EC2 configure customizations and create an Ami from it. Also you want to have something to fix small issues. You do not want to go for prod and risk a downtime when something goes south. So you always want to have something to quickly workaround an issue.\nIts always quick response to issues via workaround in combination with thorough work in developement. You do not want to keep workarounds, you just want(and need) to buy developement time to do it thorough.\nA and C we do thorough work. And now F with Cloudformation we can do some final customizations - only what is necesary . \nSo What you would do is put everything that is common on the AMI.","poster":"hobokabobo","comment_id":"857634","upvote_count":"1"},{"comment_id":"824440","upvote_count":"2","timestamp":"1677564480.0","poster":"TajSidKazi","content":"ABE\nA. Mirroring the application code to an AWS CodeCommit Git repository provides a centralized location to manage the code and ensure consistency across environments. Building EC2 AMIs from the CodeCommit repository ensures that the same codebase is used for all deployments.\n\nB. Producing multiple EC2 AMIs, one for each environment, for each release ensures that each environment has its own dedicated AMI that is customized for its specific requirements. This reduces the likelihood of issues occurring due to differences in environment configurations.\n\nE. Replacing the custom scripts and tools with AWS CodeBuild and using EC2 Image Builder for infrastructure deployment provides a consistent and reliable process for building and deploying AMIs. This reduces the likelihood of errors occurring during the deployment process."},{"timestamp":"1668377520.0","content":"Selected Answer: ACF\nACF ok","poster":"Relaxeasy","upvote_count":"1","comment_id":"717542"},{"content":"Selected Answer: ACF\nB - Not best practice and introduces potential issues.\nD - Seems overly complex, rather than using native AWS services \"CodeCommit\"\nE -- ? On the fence, part 2 update infrastructure deployment to use ec2 image builder seems to be inaccurate.","comment_id":"715090","poster":"janvandermerwer","upvote_count":"1","timestamp":"1668072480.0"},{"poster":"Ni_yot","content":"Agree ACF","comment_id":"671355","timestamp":"1663403460.0","upvote_count":"2"}]},{"id":"xJvGliQJCzYpCO4wBcVo","question_id":902,"answer_ET":"B","question_text":"A flood monitoring agency has deployed more than 10,000 water-level monitoring Sensors. Sensors send continuous data updates, and each update is less than\n1 MB in size. The agency has a fleet of on-premises application servers. These servers receive updates from the sensors, convert the raw data into a human readable format, and write the results to an on-premises relational database server. Data analysts then use simple SQL queries to monitor the data.\nThe agency wants to increase overall application availability and reduce the effort that is required to perform maintenance tasks. These maintenance tasks, which include updates and patches to the application servers, cause downtime. While an application server is down, data is lost from sensors because the remaining servers cannot handle the entire workload.\nThe agency wants a solution that optimizes operational overhead and costs. A solutions architect recommends the use of AWS IoT Core to collect the sensor data.\nWhat else should the solutions architect recommend to meet these requirements?","discussion":[{"content":"Selected Answer: B\n\"The agency wants to increase overall application availability and reduce the effort that is required to perform maintenance tasks\" -> B","timestamp":"1663673400.0","poster":"pinhead900","comment_id":"674088","upvote_count":"7"},{"poster":"spencer_sharp","comment_id":"1185951","timestamp":"1711785360.0","content":"Why option D is wrong?","upvote_count":"1"},{"poster":"dcdcdc3","content":"Selected Answer: B\nThe closest I could find. Not Lambda, rather Glue there but still\n\nhttps://aws.amazon.com/blogs/big-data/analyzing-apache-parquet-optimized-data-using-amazon-kinesis-data-firehose-amazon-athena-and-amazon-redshift/","upvote_count":"2","timestamp":"1665107220.0","comment_id":"688206"},{"upvote_count":"1","content":"I like B","poster":"Trump2022","comment_id":"680757","timestamp":"1664282460.0"},{"comment_id":"676547","poster":"gnandam","timestamp":"1663877880.0","upvote_count":"1","content":"B-\nApache Parquet is a incredibly versatile open source columnar storage format. It is 2x faster to unload and takes up 6x less storage in Amazon S3 as compared to text formats. It also allows you to save the Parquet files in Amazon S3 as an open format with all data transformation and enrichment carried out in Amazon Redshift.\nAmazon Athena can be used for object metadata\nParquet is a self-describing format and the schema or structure is embedded in the data itself therefore it is not possible to track the data changes in the file. To track the changes, you can use Amazon Athena to track object metadata across Parquet files as it provides an API for metadata."},{"comments":[{"comment_id":"664490","poster":"cale","comments":[],"content":"Option B does not satisfy these requirements though:\n1. convert the raw data into a human readable format, and \n2. write the results to an on-premises relational database server.","upvote_count":"1","timestamp":"1662724860.0"},{"timestamp":"1662725100.0","comment_id":"664494","upvote_count":"1","content":"I actually like option B and it is how I will do it but those two requirements (at least how I interpret them as requirements) are throwing me off a bit. It's just one of those questions that is tricky but you actually know what to do in real life.","poster":"cale"}],"poster":"SGES","content":"B - better realistic in my opinion","timestamp":"1662519840.0","comment_id":"661894","upvote_count":"1"},{"content":"Selected Answer: A\nI will go with A because it satisfies the requirements.","upvote_count":"2","timestamp":"1662447420.0","poster":"cale","comment_id":"660907"}],"url":"https://www.examtopics.com/discussions/amazon/view/80528-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"answers_community":["B (82%)","A (18%)"],"topic":"1","question_images":[],"answer_description":"","isMC":true,"answer_images":[],"unix_timestamp":1662447420,"timestamp":"2022-09-06 08:57:00","choices":{"B":"Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to Apache Parquet format, and save it to an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena.","C":"Send the sensor data to an Amazon Kinesis Data Analytics application to convert the data to .csv format and store it in an Amazon S3 bucket. Import the data into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance.","D":"Send the sensor data to an Amazon Kinesis Data Analytics application to convert the data to Apache Parquet format and store it in an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena.","A":"Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to .csv format, and insert it into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance."},"answer":"B"},{"id":"dRJxiPrIMP77VLl9mnhG","question_id":903,"url":"https://www.examtopics.com/discussions/amazon/view/78770-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"timestamp":"2022-08-31 21:50:00","isMC":true,"answers_community":["D (100%)"],"choices":{"D":"Create an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project's account.","C":"Create and purchase a t3.small EC2 Reserved Instance for each developer in us-east-2. Assign each developer a specific EC2 instance with their name as the tag.","A":"Create a new developer account. Move all EC2 instances, users, and assets into us-east-2. Add the account to the company's organization in AWS Organizations. Enforce a tagging policy that denotes Region affinity.","B":"Create an SCP that denies the launch of all EC2 instances except t3.small EC2 instances in us-east-2. Attach the SCP to the project's account."},"answer_description":"","topic":"1","unix_timestamp":1661975400,"discussion":[{"comment_id":"655441","timestamp":"1661975400.0","content":"Selected Answer: D\nIt's D.\nQuestion says \"The project's account cannot be part of the company's organization in AWS Organizations\", so how we can use SCP?????","poster":"gnic","upvote_count":"16","comments":[{"timestamp":"1702974240.0","upvote_count":"1","comment_id":"1100434","poster":"sumaju","content":"SCP can be attached at account level."}]},{"content":"Selected Answer: D\n\"The project's account CANNOT be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT\"\nA = Not applicable\nB = SCP requires organisations\nC = High admin overhead - won't allow scaling out.\nD= Not ideal, but will do the job for a standalone account.","poster":"janvandermerwer","upvote_count":"3","comment_id":"714779","timestamp":"1668029580.0"},{"comment_id":"686749","timestamp":"1664962680.0","upvote_count":"3","poster":"firstabed","content":"D D D"}],"exam_id":32,"question_text":"A company has a project that is launching Amazon EC2 instances that are larger than required. The project's account cannot be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT. The company wants to allow only the launch of t3.small\nEC2 instances by developers in the project's account. These EC2 instances must be restricted to the us-east-2 Region.\nWhat should a solutions architect do to meet these requirements?","answer_ET":"D","answer_images":[],"answer":"D"},{"id":"9PfvZOourXweLARkvQNI","url":"https://www.examtopics.com/discussions/amazon/view/80799-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"B":"During an agreed upon maintenance window, disconnect users from the file system. In the Amazon FSx console, update the storage capacity of the file system. Enter an absolute value of 3 TiB. Reconnect users to the file system.","D":"Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule a shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions, and create a new, smaller FSx for Windows File Server file system. Adjust the DNS alias after the copy job is completed. Delete the original file system.","A":"During an agreed upon maintenance window, use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new, smaller FSx for Windows File Server file system. Adjust the DNS alias after the restore is completed. Delete the original file system.","C":"Deploy an AWS DataSyne agent onto a new Amazon EC2 instance. Create a DataSync task. Configure the existing file system as the source location. Configure a new, smaller FSx for Windows File Server file system as the target location. Schedule the task. Adjust the DNS alias after the task is completed. Delete the original file system."},"exam_id":32,"unix_timestamp":1662520980,"question_images":[],"timestamp":"2022-09-07 05:23:00","answer_description":"","discussion":[{"upvote_count":"1","content":"Selected Answer: C\nFSx – Solution Architecture\nDecrease FSx Volume Size\n• If you take a backup, you can only restore to a same size\n• You can only increase the amount of storage capacity for a file system; you cannot decrease storage capacity.\n• Instead, create a new FSx (smaller), use DataSync to sync data and then migrate your app over","comment_id":"824230","timestamp":"1677540660.0","poster":"[Removed]"},{"timestamp":"1664981700.0","poster":"joanneli77","upvote_count":"2","comment_id":"686976","content":"Selected Answer: C\nCan't restore to larger or smaller file system. Can't reduce file system size. C seems a bit convoluted but it's correct."},{"upvote_count":"2","content":"Selected Answer: C\nAmazon FSx for Windows File Server - You can only increase the amount of storage capacity for a file system; you cannot decrease storage capacity.","timestamp":"1664770620.0","comment_id":"685194","poster":"JohnPi"},{"comments":[{"comment_id":"708616","upvote_count":"1","poster":"Rocketeer","timestamp":"1667238840.0","content":"You can only increase storage but cannot decrease it. Hence C."}],"comment_id":"682862","upvote_count":"2","poster":"jg0081","content":"Q: Can I change my file system’s storage capacity and throughput capacity?\n\nA: Yes, you can increase the storage capacity, and increase or decrease the throughput capacity of your file system – while continuing to use it – at any time by clicking “Update storage\" or \"Update throughput” in the Amazon FSx Console, or by calling “update-file-system” in the AWS CLI/API and specifying the desired level.","timestamp":"1664469720.0"},{"timestamp":"1663935240.0","comment_id":"677103","upvote_count":"3","content":"Answer is C:\n\nBasic steps for migrating files using DataSync\nTo transfer files from a source location to a destination location using DataSync, take the following basic steps:\n\nDownload and deploy an agent in your environment and activate it.\n\nCreate and configure a source and destination location.\n\nCreate and configure a task.\n\nRun the task to transfer files from the source to the destination.","poster":"gnandam"},{"timestamp":"1663777080.0","comment_id":"675305","poster":"redipa","content":"It's not A, you can't restore to a smaller file system:\n\nNote\nYou can only restore your backup to a file system of the same deployment type and storage capacity as the original. \n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/using-backups.html#restoring-backups","upvote_count":"4"},{"comment_id":"666859","content":"Selected Answer: C\nThere would be minimal downtime with C. See https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-to-fsx-datasync.html","timestamp":"1662981720.0","poster":"jabilrn","upvote_count":"3"},{"poster":"SGES","upvote_count":"1","timestamp":"1662520980.0","content":"A - agreed","comment_id":"661913"}],"topic":"1","answer_ET":"C","question_id":904,"answers_community":["C (100%)"],"isMC":true,"answer_images":[],"question_text":"A company is using Amazon WorkSpaces to provide access to its corporate applications across multiple global locations. User profile data is stored on an\nAmazon FSx for Windows File Server file system that is configured with a DNS alias. The file system is linked to an existing Active Directory service.\nRecently, the company added a new application that unexpectedly caused user profiles to grow significantly. The company increased the FSx for Windows File\nServer file system size from 3 TiB to 6 TiB to prevent any issues. A few days later, the company made changes to the application's configuration. The user profile storage usage decreased significantly, leaving a large amount of free space on the file system. A solutions architect needs to reduce the size of the file system to avoid unnecessary costs.\nWhat should the solutions architect do to achieve this goal?","answer":"C"},{"id":"wdxRpa4k8b1uDqUqAN3R","discussion":[{"comment_id":"686982","timestamp":"1664981940.0","poster":"joanneli77","content":"Selected Answer: A\nIt can't exceed 900 RCUs, so \"just below 900 RCUs\" is \"at capacity\". Change to auto-scaling. If you were at 896-899, wouldn't you say \"woah, that's too close!\" or would you say \"Must be lambda time-outs?\" Best case, you'd still address dynamoDB even if it WAS lambda time-outs.","upvote_count":"10"},{"poster":"pixepe","comment_id":"665996","upvote_count":"7","timestamp":"1662890100.0","content":"Amazon CloudWatch metrics show that the peak usage of the DynamoDB table is just below 900 RCUs. => There is NO Issue w.r.t DynamoDB\n\nSo, A,B and D filtered out.\n\nANSWER is B"},{"comment_id":"1248541","poster":"WhyIronMan","timestamp":"1721071200.0","upvote_count":"1","content":"A) chat gpt is right after verifying the message"},{"content":"Correct A.","comment_id":"950698","timestamp":"1689255540.0","poster":"ggrodskiy","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nIts about what is most \"cost effective\". So question asks what has most impact on costs?\nI would argue its A because while peak is above 900 its way below most of the time ( RCU overprovisioned). \nThe may give cost savings while also solving the issue.","comment_id":"860316","poster":"hobokabobo","timestamp":"1680552600.0"},{"upvote_count":"2","timestamp":"1676020740.0","poster":"davidy2020","content":"ChatGPT said: The most cost-effective solution to resolve the issue is option A: Configure the DynamoDB table's read capacity to use auto scaling with default parameters. By enabling auto scaling, the DynamoDB table can automatically adjust its read capacity based on the actual usage, which helps ensure that the table has sufficient read capacity to handle the traffic during peak hours without incurring unnecessary costs during off-peak hours. The other options may alleviate the issue, but they come at the cost of increased provisioned capacity, higher Lambda function timeouts, and additional resources to implement the data replication. By using auto scaling, the company can cost-effectively ensure that their system is able to handle the traffic during peak hours.","comments":[{"comment_id":"1269494","timestamp":"1724159640.0","content":"You will find that chtgbt finds these questions on the web and looks at the answer that is marketed correct. This is not always the correct answer. AI is only as good as the information it finds.","poster":"devilman222","upvote_count":"1"}],"comment_id":"804163"},{"poster":"evargasbrz","timestamp":"1672830660.0","content":"Selected Answer: A\nI'll go with A\n\"Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling.\"\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","upvote_count":"1","comment_id":"765535"},{"upvote_count":"2","comment_id":"763750","content":"Selected Answer: B\n\"B\" is the one.","poster":"Kende","timestamp":"1672666860.0"},{"poster":"alxjandroleiva","comment_id":"708836","upvote_count":"2","content":"Selected Answer: B\nRCU is not a problem","timestamp":"1667276940.0"},{"comment_id":"703191","poster":"redipa","content":"The answer is definitely not A. The key words are \"with default parameters\" \nDefault auto-scaling read parameters have a maximum of 10. If the table is already using 900 RCUs, this would severely lower the resources. 900 -> 10","upvote_count":"4","timestamp":"1666627980.0"},{"timestamp":"1665462960.0","comment_id":"691729","poster":"skywalker","content":"Selected Answer: A\nCustomers are reporting timeout errors and slow performance.. Increasing timemout will caused it even slower. \n\nThus going for A. At least it provide some read performance enhancement.","upvote_count":"6","comments":[{"timestamp":"1665463260.0","upvote_count":"2","content":"DynamoDB auto scaling modifies provisioned throughput settings only when the actual workload stays elevated (or depressed) for a sustained period of several minutes. The Application Auto Scaling target tracking algorithm seeks to keep the target utilization at or near your chosen value over the long term.\n\n\nThus no harm turn on AutoScaling if the workload is steady and require some bursting in few min.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","poster":"skywalker","comment_id":"691736"}]},{"comment_id":"660961","upvote_count":"6","content":"Selected Answer: B\nI think it's B.","timestamp":"1662450180.0","poster":"cale"}],"question_images":[],"exam_id":32,"unix_timestamp":1662450180,"question_id":905,"answers_community":["A (64%)","B (36%)"],"answer_images":[],"timestamp":"2022-09-06 09:43:00","choices":{"A":"Configure the DynamoDB table's read capacity to use auto scaling with default parameters.","B":"Increase the timeout of all the Lambda functions that read from the DynamoDB table.","D":"Increase the DynamoDB table's provisioned read capacity to 1,400 RCUs.","C":"Use DynamoDB Streams to replicate data to a new table. Configure all the Lambda functions to read from the new table."},"url":"https://www.examtopics.com/discussions/amazon/view/80542-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1","question_text":"A company has an online shop that uses an Amazon API Gateway API, AWS Lambda functions, and an Amazon DynamoDB table provisioned with 900 RCUs.\nThe API Gateway API receives requests from customers, and the Lambda functions handle the requests. Some of the Lambda functions read data from the\nDynamoDB table.\nDuring peak hours, customers are reporting timeout errors and slow performance. An investigation reveals that the Lambda functions that read the DynamoDB table occasionally time out. Amazon CloudWatch metrics show that the peak usage of the DynamoDB table is just below 900 RCUs.\nWhich solution will resolve this issue MOST cost-effectively?","answer_description":"","isMC":true,"answer_ET":"A","answer":"A"}],"exam":{"isBeta":false,"isImplemented":true,"isMCOnly":false,"name":"AWS Certified Solutions Architect - Professional","id":32,"numberOfQuestions":1019,"provider":"Amazon","lastUpdated":"11 Apr 2025"},"currentPage":181},"__N_SSP":true}