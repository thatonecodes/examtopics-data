{"pageProps":{"questions":[{"id":"Fbyb16JeHLB0QJVVad5x","question_images":[],"question_text":"A retail company is about to migrate its online and mobile store to AWS. The company's CEO has strategic plans to grow the brand globally. A Database\nSpecialist has been challenged to provide predictable read and write database performance with minimal operational overhead.\nWhat should the Database Specialist do to meet these requirements?","timestamp":"2021-12-24 21:26:00","question_id":301,"unix_timestamp":1640377560,"answers_community":["A (100%)"],"answer_images":[],"answer_ET":"A","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/68554-exam-aws-certified-database-specialty-topic-1-question-46/","discussion":[{"upvote_count":"8","timestamp":"1641757740.0","comment_id":"520429","poster":"awsmonster","content":"A\nhttps://aws.amazon.com/dynamodb/features/\nWith global tables, your globally distributed applications can access data locally in the selected regions to get single-digit millisecond read and write performance. \n\nNot Aurora Global Database, as per this link: https://aws.amazon.com/rds/aurora/global-database/?nc1=h_ls . Aurora Global Database lets you easily scale database reads across the world and place your applications close to your users."},{"content":"A. Use Amazon DynamoDB global tables to synchronize transactions","upvote_count":"1","poster":"NishithShah","comment_id":"1096010","timestamp":"1702524240.0"},{"content":"A is best choice https://aws.amazon.com/dynamodb/features/","timestamp":"1697666400.0","comment_id":"1047257","poster":"nahunaws","upvote_count":"1"},{"poster":"redman50","upvote_count":"1","comment_id":"846922","content":"Using Amazon DynamoDB global tables to synchronize transactions - is a viable option, but it's more suited for a NoSQL database. It can provide predictable read and write performance, but with DynamoDB, there are limitations regarding query and transactional capabilities.","timestamp":"1679480340.0"},{"upvote_count":"4","content":"Selected Answer: A\nA is correct\nB is incorrect because aurora global db only allow 'write' on primary region","poster":"im_not_robot","timestamp":"1675747620.0","comment_id":"800574"},{"content":"Selected Answer: A\nA. Use Amazon DynamoDB global tables","upvote_count":"3","poster":"novice_expert","timestamp":"1651406220.0","comment_id":"595585"},{"timestamp":"1641990840.0","upvote_count":"2","content":"Ans: A","poster":"awsmonster","comment_id":"522148"},{"comment_id":"508857","upvote_count":"3","timestamp":"1640377560.0","poster":"jove","content":"Selected Answer: A\nI go with option A"}],"isMC":true,"exam_id":22,"answer_description":"","choices":{"D":"Use Amazon DynamoDB Streams to replicate all DynamoDB transactions and sync them","A":"Use Amazon DynamoDB global tables to synchronize transactions","B":"Use Amazon EMR to copy the orders table data across Regions","C":"Use Amazon Aurora Global Database to synchronize all transactions"},"topic":"1"},{"id":"PKeMjtO7urDhPbGxQphD","question_id":302,"url":"https://www.examtopics.com/discussions/amazon/view/26022-exam-aws-certified-database-specialty-topic-1-question-47/","answer":"B","answers_community":["B (100%)"],"exam_id":22,"unix_timestamp":1595048100,"question_text":"A company is closing one of its remote data centers. This site runs a 100 TB on-premises data warehouse solution. The company plans to use the AWS Schema\nConversion Tool (AWS SCT) and AWS DMS for the migration to AWS. The site network bandwidth is 500 Mbps. A Database Specialist wants to migrate the on- premises data using Amazon S3 as the data lake and Amazon Redshift as the data warehouse. This move must take place during a 2-week period when source systems are shut down for maintenance. The data should stay encrypted at rest and in transit.\nWhich approach has the least risk and the highest likelihood of a successful data transfer?","choices":{"C":"Leverage AWS SCT and apply the converted schema to Amazon Redshift. Once complete, use a fleet of 10 TB dedicated encrypted drives using the AWS Import/Export feature to copy data from on-premises to Amazon S3 with AWS KMS encryption. Use AWS Glue to load the data to Amazon redshift.","B":"Leverage AWS SCT and apply the converted schema to Amazon Redshift. Start an AWS DMS task with two AWS Snowball Edge devices to copy data from on-premises to Amazon S3 with AWS KMS encryption. Use AWS DMS to finish copying data to Amazon Redshift.","D":"Set up a VPN tunnel for encrypting data over the network from the data center to AWS. Leverage a native database export feature to export the data and compress the files. Use the aws S3 cp multi-port upload command to upload these files to Amazon S3 with AWS KMS encryption. Once complete, load the data to Amazon Redshift using AWS Glue.","A":"Set up a VPN tunnel for encrypting data over the network from the data center to AWS. Leverage AWS SCT and apply the converted schema to Amazon Redshift. Once complete, start an AWS DMS task to move the data from the source to Amazon S3. Use AWS Glue to load the data from Amazon S3 to Amazon Redshift."},"topic":"1","isMC":true,"timestamp":"2020-07-18 06:55:00","question_images":[],"answer_ET":"B","answer_images":[],"answer_description":"","discussion":[{"comments":[{"poster":"BillyMadison","upvote_count":"2","timestamp":"1632207180.0","comment_id":"141230","content":"Going with B as well due to your compelling link.","comments":[{"comment_id":"141232","poster":"BillyMadison","timestamp":"1632942840.0","upvote_count":"4","content":"Found this link that agrees with B \nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html\n\"Large-scale data migrations can include many terabytes of information, and can be slowed by network performance and by the sheer amount of data that has to be moved. AWS Snowball Edge is an AWS service you can use to transfer data to the cloud at faster-than-network speeds using an AWS-owned appliance.\"\n\"When you use AWS SCT and an AWS Snowball Edge device, you migrate your data in two stages. First, you use the AWS SCT to process the data locally and then move that data to the AWS Snowball Edge device. You then send the device to AWS using the AWS Snowball Edge process, and then AWS automatically loads the data into an Amazon S3 bucket. Next, when the data is available on Amazon S3, you use AWS SCT to migrate the data to Amazon Redshift.\""}]},{"comment_id":"157649","content":"B is incorrect. \n1. You just need one snowball edge device because one device can hold up to 80 TB of data. \n2. The lastest step for B is \"Use AWS DMS to finish copying data to Amazon Redshift.\" but from the AWS docs need \"use AWS SCT to migrate the data to Amazon Redshift.\"","upvote_count":"2","comments":[{"poster":"BillyMadison","comment_id":"161576","upvote_count":"9","timestamp":"1634651280.0","content":"There is 100 TB of data that needs to be transferred. Since a snowball can only hold 80, it makes sense that there would be a need for 2 of them. Hence B."},{"timestamp":"1646515560.0","upvote_count":"2","comment_id":"561660","content":"Wrong. you need TWO snowball edge devices since out of 100TB only 80TB is usable. This is a standard question in tutotrialsdojo as well - very respectable site.","poster":"RotterDam"}],"poster":"szmulder","timestamp":"1633534080.0"}],"poster":"pan24","content":"Ans: B\nhttps://aws.amazon.com/blogs/database/new-aws-dms-and-aws-snowball-integration-enables-mass-database-migrations-and-migrations-of-large-databases/","comment_id":"140112","upvote_count":"19","timestamp":"1632070080.0"},{"upvote_count":"5","comments":[{"timestamp":"1711977720.0","poster":"58a2d17","content":"That is why you need snowball. Network speed won't do it .","upvote_count":"1","comment_id":"1187458"},{"upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"1081260","timestamp":"1701068040.0","content":"Got it. Lowercase \"b\" is bits while uppercase \"B\" is bytes. 500 mbps is 500/8 MB/s.","poster":"jitesh_k"}],"poster":"jitesh_k","timestamp":"1701067920.0","content":"How is 500MBPS equal to 62.5 MB/sec?","comment_id":"1081259"}],"timestamp":"1635827280.0","comment_id":"293647","poster":"Exia","content":"Although snowball is a petabyte-scale data transport solution, 100TB database migration through a 500 Mbps network is impossible in 2 weeks.\nbandwidth: 500 Mbps = 62.5 MB/s\ncapacity: 100 TB = 100 * 10**6 = 100000000 MB\ntime consume: 100000000 / 62.5 MB/s = 1600000.0 s = \n1600000 / 60 / 60 / 24 = 18.5 days"},{"comment_id":"597941","timestamp":"1651889340.0","content":"Selected Answer: B\nWith the given bandwidth any other option will be completed in more than 20 days.","poster":"KaranGandhi30","upvote_count":"4"},{"timestamp":"1651176180.0","upvote_count":"1","comment_id":"594029","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/database/new-aws-dms-and-aws-snowball-integration-enables-mass-database-migrations-and-migrations-of-large-databases/","poster":"novice_expert"},{"timestamp":"1640207400.0","upvote_count":"1","content":"Option B.. With SCT data extraction agent you can extract the data as well as the schema. Snowball Edge devices are safe.","poster":"jove","comment_id":"507384"},{"timestamp":"1636168620.0","poster":"LMax","content":"Agree with B","comment_id":"314912","upvote_count":"1"},{"content":"Ans: B","timestamp":"1635845700.0","upvote_count":"1","comment_id":"298023","poster":"myutran"},{"upvote_count":"1","comment_id":"253257","content":"B is the right choice here","timestamp":"1635634380.0","poster":"JobinAkaJoe"},{"comment_id":"212383","content":"B. Snow ball will be the solution to transfer 100TB, since existing 500Mbps bandwidth is not enough","poster":"Ashoks","upvote_count":"2","timestamp":"1635553080.0"},{"comment_id":"168224","timestamp":"1635404520.0","upvote_count":"1","content":"For me B here","poster":"AWSCert2020"},{"upvote_count":"1","poster":"szmulder","content":"A. Answer: It's possible but copy 100TB via internet is not reliable\n\nB. Answer: \"Use AWS DMS to finish copying data to Amazon Redshift.\" but from the AWS docs need \"use AWS SCT to migrate the data to Amazon Redshift.\"\n\nC. Answer: Correct\n\nD. Answer: It's possible but it's missing the step AWS SCT and apply the converted schema to Amazon Redshift.","comment_id":"157655","timestamp":"1634225460.0","comments":[{"content":"didn't know you can ship fleets of drives to AWS.","poster":"[Removed]","comment_id":"160994","upvote_count":"2","timestamp":"1634601420.0"},{"content":"sorry, change my mind, B is correct.","poster":"szmulder","timestamp":"1635548880.0","comment_id":"174222","upvote_count":"1"}]},{"upvote_count":"1","poster":"badrik","timestamp":"1633388280.0","comment_id":"149738","content":"B is right option"},{"timestamp":"1633003860.0","content":"B is correct for me","comment_id":"145812","poster":"BillyC","upvote_count":"1"}]},{"id":"pkIw84NfBmCcvDVFB2bi","topic":"1","question_id":303,"timestamp":"2020-07-22 17:54:00","isMC":true,"answer_images":[],"answer_description":"","exam_id":22,"url":"https://www.examtopics.com/discussions/amazon/view/26418-exam-aws-certified-database-specialty-topic-1-question-48/","discussion":[{"comments":[{"poster":"khchan123","timestamp":"1653028860.0","upvote_count":"4","content":"I think it is C. The explanation from vicks316 is irrelevant. From this link -> \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html\nUsing the Max LOB size (K) option with a value greater than 63KB impacts the performance of a full load configured to run in limited LOB mode. During a full load, DMS allocates memory by multiplying the Max LOB size (k) value by the Commit rate, and the product is multiplied by the number of LOB columns. When DMS can’t pre-allocate that memory, DMS starts consuming SWAP memory, and that impacts performance of a full load. So, if you experience performance issues when using limited LOB mode, consider decreasing the commit rate until you achieve an acceptable level of performance. You can also consider using inline LOB mode for supported endpoints once you understand your LOB distribution for the table.","comment_id":"604259"},{"upvote_count":"2","content":"This doesn't mean we need two tasks. It just explains how LOBs are copied to target in any scenario. First all columns except LOBs then the LOB.","poster":"JobinAkaJoe","timestamp":"1634425980.0","comment_id":"253258"}],"content":"C sounds correct since as per link https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LOBS, \"AWS DMS migrates LOB data in two phases:\n1. AWS DMS creates a new row in the target table and populates the row with all data except the associated LOB value.\n2.AWS DMS updates the row in the target table with the LOB data.\"\nThis means that we would need two tasks, one per phase and use limited LOB mode for best performance.","upvote_count":"17","comment_id":"200887","timestamp":"1633732500.0","poster":"vicks316"},{"comments":[{"content":"the link says it not a hard limit 100MB, it is recommended.","poster":"Sathish_dbs","upvote_count":"1","timestamp":"1696513680.0","comment_id":"1025729"},{"poster":"Shunpin","timestamp":"1640692320.0","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html","comment_id":"511056","upvote_count":"4"}],"poster":"Shunpin","upvote_count":"14","content":"Selected Answer: B\nLimited LOB maxsize is 100MB. This question is about LOB average 350MB. Full LOB is better option","comment_id":"511055","timestamp":"1640692260.0"},{"comment_id":"1121338","timestamp":"1705126800.0","upvote_count":"1","poster":"MultiAZ","content":"Selected Answer: C\nI believe the right answer is C\nAs for B, 500 chunk size is way too big and will lead to many failures."},{"poster":"rrshah83","timestamp":"1703605440.0","content":"Selected Answer: C\nLimited LOB with max size of 500MB makes migration fast.","comment_id":"1106149","upvote_count":"1"},{"upvote_count":"1","poster":"jitesh_k","timestamp":"1701082020.0","content":"Full LOB mode has no information about chunk size.\nFull LOB mode – In full LOB mode AWS DMS migrates all LOBs from source to target regardless of size. In this configuration, AWS DMS has no information about the maximum size of LOBs to expect. Thus, LOBs are migrated one at a time, piece by piece. Full LOB mode can be quite slow.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html\n\nEither C or D - but C seems more optimal.","comment_id":"1081423"},{"upvote_count":"1","poster":"Sathish_dbs","content":"the question is confusing, but the lesson here full lob -> incur performance issue, limited lob - smooth, hopefully the same question doesn't repeat, if so then let it come with some additional clarity on performance, either way 500MB is huge and we expect performance issue, this is seems to be meaningless question","comment_id":"1025732","timestamp":"1696513860.0"},{"poster":"roymunson","comment_id":"1019831","timestamp":"1695903600.0","upvote_count":"2","content":"Selected Answer: B\nIt can not be C because of \n\"Limited LOB mode – In limited LOB mode, you set a maximum LOB size for DMS to accept. That enables DMS to pre-allocate memory and load the LOB data in bulk. LOBs that exceed the maximum LOB size are truncated, and a warning is issued to the log file. In limited LOB mode, you can gain significant performance over full LOB mode. We recommend that you use limited LOB mode whenever possible. The maximum recommended value is 102400 KB (100 MB).\"\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html"},{"content":"To optimize for large size LOB, we should use multiple tasks => eliminate A & D.\nB: we can't configure such large LOB chunk size of 500 MB. Please note that \"chunk size\" is different from \"max LOB size\".\nSo I select C but can anyone explain what benefit of task 2 without LOBs?","upvote_count":"1","poster":"cuibap","comment_id":"954817","timestamp":"1689645660.0"},{"upvote_count":"2","poster":"rn30","content":"B looks to be right\nTask1 with LOB Tables:\nUse the full LOB mode in AWS DMS to ensure efficient streaming of LOB data during replication.\nSet the LOB chunk size to match the maximum LOB size in Oracle (500 MB). This ensures optimal data transfer and minimizes network overhead.\nTask2 without LOBs:\nMigrate the remaining tables that do not contain LOBs separately in a task without LOB-specific configurations.","timestamp":"1689171420.0","comment_id":"949881"},{"upvote_count":"2","poster":"saikarthikeya777","content":"Selected Answer: C\nfull lob is quite slow ! to optimize we need to use limited lob even if 500mb is not recommended size . Maybe i am super confused","timestamp":"1687948380.0","comment_id":"936493"},{"content":"B is correct. Option C could have been correct however, please note the word \"recommended size\" for Limited LOB mode which is 100MB. Agreed that 100 MB is not the max limit for limited LOB however, anything above 100MB in limited LOB is not recommended which makes full LOB with 2 different tasks a better option.","comment_id":"843775","timestamp":"1679232600.0","upvote_count":"2","poster":"backbencher2022"},{"poster":"anantarb","content":"Selected Answer: B\nThe Max recommended LOB size is 100MB in limited LOB mode. So can't be C. B is correct. \nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html","comment_id":"787094","timestamp":"1674604740.0","upvote_count":"3"},{"comment_id":"779869","poster":"teo2157","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LOBS. Limited LOB mode migrates all LOB values up to a user-specified size limit (default is 32 KB). LOB values larger than the size limit must be manually migrated. Limited LOB mode, the default for all migration tasks, typically provides the best performance. However, ensure that the Max LOB size parameter setting is correct. Set this parameter to the largest LOB size for all your tables.","timestamp":"1674037980.0","upvote_count":"1"},{"timestamp":"1673613360.0","upvote_count":"1","comment_id":"774460","content":"Selected Answer: C\nC is the answer - https://aws.amazon.com/premiumsupport/knowledge-center/dms-improve-speed-lob-data/","poster":"parle101"},{"upvote_count":"2","timestamp":"1671535920.0","comment_id":"750788","content":"Selected Answer: C\nC. is the answer. Faster = 2 task Limited Lob and No Lob\n\nFull LOB mode – Migrate complete LOBs regardless of size. AWS DMS migrates LOBs piecewise in chunks controlled by the Max LOB Size parameter. This mode is slower than using limited LOB mode.\nLimited LOB mode – Truncate LOBs to the value specified by the Max LOB Size parameter. This mode is faster than using full LOB mode.","poster":"khun"},{"comment_id":"743713","content":"Selected Answer: C\nC is correct, 100MB is the maximum recommended value, not a limitation.\nB is wrong, because 500MB is too huge for chunk setting, connection error would be encountered.\n\nWhen a task is configured to use full LOB mode, AWS DMS retrieves LOBs in pieces. The LOB chunk size (K) option determines the size of each piece. When setting this option, pay particular attention to the maximum packet size allowed by your network configuration. If the LOB chunk size exceeds your maximum allowed packet size, you might see disconnect errors. The recommended value for LobChunkSize is 64 kilobytes. Increasing the value for LobChunkSize above 64 kilobytes can cause task failures.","timestamp":"1670917260.0","poster":"shuo82","upvote_count":"2"},{"timestamp":"1666574940.0","poster":"rags1482","upvote_count":"1","comment_id":"702584","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LOBS\n\nAnswer : C"},{"content":"i think its C , since all the data will be migrated when we give 500 has the max lob size and nothing is truncated , and also Limited LOB mode is faster than Full Lob mode","upvote_count":"1","poster":"niteshdba","timestamp":"1664441100.0","comment_id":"682486"},{"comment_id":"632785","timestamp":"1658108160.0","content":"Limited LOB mode migrates all LOB values up to a user-specified size limit (default is 32 KB). LOB values larger than the size limit must be manually migrated. Limited LOB mode, the default for all migration tasks, typically provides the best performance. However, ensure that the Max LOB size parameter setting is correct. Set this parameter to the largest LOB size for all your tables.\n\nFull LOB mode migrates all LOB data in your tables, regardless of size. Full LOB mode provides the convenience of moving all LOB data in your tables, but the process can have a significant impact on performance.\n\nFor some database engines, such as PostgreSQL, Amazon DMS treats JSON data types like LOBs. Make sure that if you chose Limited LOB mode, the Max LOB size option is set to a value that doesn't cause the JSON data to be truncated.\nOracle to PostgreSQL --> Limited Lob Mode ( Max Lob Size is set to Max Lob in DB ) \nC is best choice","upvote_count":"1","poster":"db2luwdba"},{"content":"Using multiple job also improves performance in DMS \nSo C is more viable option , Limited LOB with multiple tasks\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.Performance\nUse multiple tasks\nSometimes using multiple tasks for a single migration can improve performance. If you have sets of tables that don't participate in common transactions, you might be able to divide your migration into multiple tasks. Transactional consistency is maintained within a task, so it's important that tables in separate tasks don't participate in common transactions. Also, each task independently reads the transaction stream, so be careful not to put too much stress on the source database.\n\nYou can use multiple tasks to create separate streams of replication. By doing this, you can parallelize the reads on the source, the processes on the replication instance, and the writes to the target database.","upvote_count":"1","poster":"sachin","timestamp":"1657448940.0","comment_id":"629522"},{"comments":[{"upvote_count":"2","comment_id":"633643","poster":"rlnd2000","timestamp":"1658243880.0","content":"I don't think selecting in the test any option that is not recommended is a good Idea, AWS say ... We recommend that you use limited LOB mode whenever possible. The maximum recommended value is 102400 KB (100 MB)...\nfor me that is enough :) Thanks @novice_expert"}],"comment_id":"595616","poster":"novice_expert","timestamp":"1651412340.0","upvote_count":"3","content":"Selected Answer: B\nA& D out for LOB should be in separate task\n\nnot C because Limited LOB mode, maximum LOB size= 100MB\n\nso use Full LOB mode, chunk size 500 MB and task 2 without LOB\n\nB. Create two tasks: task1 with LOB tables using full LOB mode with a LOB chunk size of 500 MB and task2 without LOBs\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html"},{"timestamp":"1649555880.0","content":"B seems to be the answer, With limited LOB mode the max size accepted is 100MB and here the average is 350 MB. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html","poster":"BOYKA","comment_id":"583550","upvote_count":"2"},{"poster":"jove","comment_id":"510467","timestamp":"1640623080.0","upvote_count":"1","content":"Selected Answer: C\nLimited LOB + multiple task"},{"comment_id":"426311","upvote_count":"2","comments":[{"comment_id":"426313","timestamp":"1635459120.0","poster":"ChauPhan","upvote_count":"1","comments":[{"content":"ok....","poster":"guru_ji","upvote_count":"1","comment_id":"455057","timestamp":"1636236060.0"}],"content":"Because we knew the maximum LOB size is 500MB, we can choose Limited LIB with Max LOB size parameter setting to optimize the performance. If we don't know it, we have to set Full LOB mode to make sure no data left behind (if LOB > 500MB)."},{"comment_id":"430242","timestamp":"1635900120.0","poster":"guru_ji","upvote_count":"1","content":"I agree with you.\nBut why C is not correct, can you explain ?"},{"timestamp":"1637539620.0","comment_id":"483743","poster":"johnconnor","content":"it is asking you for the best way to improve performance, D is simpler but C would improve performance, hence I'd choose C","upvote_count":"1"}],"poster":"ChauPhan","timestamp":"1635243420.0","content":"I choose D because it is performance\nUsing limited LOB mode\nAWS DMS uses two methods that balance performance and convenience when your migration contains LOB values:\n\n1. Limited LOB mode migrates all LOB values up to a user-specified size limit (default is 32 KB). LOB values larger than the size limit must be manually migrated. Limited LOB mode, the default for all migration tasks, typically provides the best performance. However, ensure that the Max LOB size parameter setting is correct. Set this parameter to the largest LOB size for all your tables.\n2. Full LOB mode migrates all LOB data in your tables, regardless of size. Full LOB mode provides the convenience of moving all LOB data in your tables, but the process can have a significant impact on performance."},{"poster":"TonyGe","content":"Choose between C&D, D is correct:\nWith option C,1st task with 500 LOB limited is fine. But 2nd task with no LOB limited? I think this will migrate all the data set, how can we separate large data and small data into different task?","timestamp":"1635102300.0","comment_id":"415793","upvote_count":"1"},{"poster":"Dip11","comment_id":"364850","timestamp":"1635000960.0","upvote_count":"1","content":"C and D both are possible. Looks like C is efficient but requires more work to segregate tables. D is simple. I would go for D in such cases."},{"content":"C. Answer","timestamp":"1634918400.0","poster":"shantest1","upvote_count":"1","comment_id":"328003"},{"timestamp":"1634701680.0","comments":[{"timestamp":"1643789280.0","poster":"thelad","upvote_count":"1","comment_id":"538448","content":"Using this link you also find the following text \n\"Using Limited LOB mode improves performance, but before you run the task, you must identify the maximum LOB size of the data on the source. Then, you must specify the Max LOB size parameter when using Limited LOB mode\"\nTherefore answer is C"}],"upvote_count":"3","poster":"giter","content":"Definitely B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dms-improve-speed-lob-data/#:~:text=When%20using%20Full%20LOB%20mode,LOBs%20regardless%20of%20their%20size.&text=If%20you%20have%20LOBs%20that,to%20migrate%20these%20tables%20alone.\n\nIf you have LOBs that are larger than a few megabytes, then you can create a separate AWS DMS task with Full LOB mode. It's a best practice to create the separate task on a new replication instance to migrate these tables alone.","comment_id":"327458"},{"poster":"LMax","upvote_count":"3","timestamp":"1634645820.0","comments":[{"timestamp":"1634878680.0","poster":"shantest1","comments":[{"content":"Have you focused on wording ?\n\"So if you already know the maximum lob size in your source database, then you can definitely use the limited lob mode with the particular maximum lob size.\"","upvote_count":"2","timestamp":"1635008100.0","poster":"ExtHo","comment_id":"401419"}],"upvote_count":"1","content":"They also say\nLimited LOB mode – truncates each LOB to Max LOB size (is faster)\nHence Answer is C:","comment_id":"328002"}],"content":"Based on info from Udemy course from Maarek & Riyaz I would choose B.","comment_id":"314929"},{"comment_id":"253261","upvote_count":"2","content":"C is my choice","timestamp":"1634481960.0","poster":"JobinAkaJoe"},{"poster":"Ashoks","upvote_count":"3","timestamp":"1633908480.0","comment_id":"238231","content":"yes, C is the answer. Two phase tasks and limited LOB mode since knowing max lob size is 500M"},{"timestamp":"1633593000.0","content":"As we have the maximum size for LOB object so the answer either C or D\nI think C is correct, It's better to have many task and in this case haveing task 1 for LOB with limited LOB and task 2 for the rest tables \nin task 2 we can execlude the table that has LOB object \nthis secnario will give better performance","comment_id":"197384","poster":"halol","upvote_count":"2"},{"comment_id":"168230","upvote_count":"2","content":"D Here, the following link explain very well the process: https://aws.amazon.com/premiumsupport/knowledge-center/dms-improve-speed-lob-data/","poster":"AWSCert2020","timestamp":"1633564740.0"},{"comment_id":"156311","timestamp":"1633001940.0","poster":"Ebi","upvote_count":"3","content":"I will go with D"},{"upvote_count":"3","comment_id":"150022","poster":"BillyMadison","content":"Switching my answer to D because of https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LOBS Full lob mode has performance negatives. \n\"Limited LOB mode migrates all LOB values up to a user-specified size limit (default is 32 KB). LOB values larger than the size limit must be manually migrated. Limited LOB mode, the default for all migration tasks, typically provides the best performance. However you need to ensure that the Max LOB size parameter setting is correct. This parameter should be set to the largest LOB size for all your tables.\nFull LOB mode migrates all LOB data in your tables, regardless of size. Full LOB mode provides the convenience of moving all LOB data in your tables, but the process can have a significant impact on performance.\"","comments":[{"upvote_count":"2","content":"So why its not recommended to have 2 tasks like C?","comments":[{"poster":"szmulder","upvote_count":"1","timestamp":"1633416120.0","comment_id":"157738","content":"I think Use multiple tasks need to split the db to different tables group. For LOB object is not apply."}],"comment_id":"151338","poster":"Dicky","timestamp":"1632993480.0"},{"poster":"guru_ji","comment_id":"430237","upvote_count":"1","timestamp":"1635592620.0","content":"I agree with D.\nBut, why C is not correct, can anyone explain ??"}],"timestamp":"1632773340.0"},{"content":"and according to https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.LOBS\nanswer seems to be D.","upvote_count":"2","timestamp":"1632692940.0","comment_id":"141761","poster":"SaulGoodman"},{"comment_id":"141750","timestamp":"1632481140.0","upvote_count":"1","content":"answer seems correct.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dms-improve-speed-lob-data/","poster":"SaulGoodman"},{"upvote_count":"1","timestamp":"1632195960.0","comment_id":"141246","content":"B or C. Going with C for now since that is what the answer says and the max size is 500 just like the question. \nAccording to this slide deck, it makes sense to have two tasks. \nhttps://www.slideshare.net/AmazonWebServices/migrate-from-oracle-to-aurora-postgresql-best-practices-design-patterns-setup-gpsct406-aws-reinvent-2018","poster":"BillyMadison"}],"unix_timestamp":1595433240,"answers_community":["B (67%)","C (33%)"],"question_images":[],"question_text":"A company is looking to migrate a 1 TB Oracle database from on-premises to an Amazon Aurora PostgreSQL DB cluster. The company's Database Specialist discovered that the Oracle database is storing 100 GB of large binary objects (LOBs) across multiple tables. The Oracle database has a maximum LOB size of\n500 MB with an average LOB size of 350 MB. The Database Specialist has chosen AWS DMS to migrate the data with the largest replication instances.\nHow should the Database Specialist optimize the database migration using AWS DMS?","answer_ET":"B","choices":{"A":"Create a single task using full LOB mode with a LOB chunk size of 500 MB to migrate the data and LOBs together","B":"Create two tasks: task1 with LOB tables using full LOB mode with a LOB chunk size of 500 MB and task2 without LOBs","C":"Create two tasks: task1 with LOB tables using limited LOB mode with a maximum LOB size of 500 MB and task 2 without LOBs","D":"Create a single task using limited LOB mode with a maximum LOB size of 500 MB to migrate data and LOBs together"},"answer":"B"},{"id":"1tbMWqO5YI9AS2taxLmI","answer_ET":"BC","discussion":[{"upvote_count":"16","poster":"Nots","content":"I'll go B and C.\nThe following items need to be reconfigured after restoring the DynamoDB table.\n--AutoScaling policy\n--IAM policy\n--CloudWatch settings\n--Tags\n--Stream settings\n--TTL","comments":[{"upvote_count":"3","poster":"johnconnor","content":"You are right, it is explained here -> https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html","comment_id":"468369","timestamp":"1635459840.0"}],"comment_id":"468065","timestamp":"1635338820.0"},{"upvote_count":"6","comment_id":"561715","timestamp":"1646527020.0","content":"Selected Answer: BC\nThe following are not restored and would need to be configured again:\n- IAM and Autoscaling Policies\n- Cloudwatch Triggers and Alarms\n- TTL and Streams\n- Tag","poster":"RotterDam"},{"upvote_count":"4","poster":"ken_test1234","comment_id":"853274","timestamp":"1680009060.0","content":"Selected Answer: BC\nThe answer is BC because this is what included in backup https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateBackup.html"},{"comment_id":"594122","timestamp":"1651191960.0","content":"Selected Answer: BC\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CreateBackup.html\n\nYou must manually set up the following on the restored table:\nAuto scaling policies\nAWS Identity and Access Management (IAM) policies\nAmazon CloudWatch metrics and alarms\nTags\nStream settings\nTime to Live (TTL) settings","upvote_count":"5","poster":"novice_expert"},{"content":"Selected Answer: BC\nOption B, C","comment_id":"501342","upvote_count":"3","timestamp":"1639484700.0","poster":"GMartinelli"}],"url":"https://www.examtopics.com/discussions/amazon/view/64810-exam-aws-certified-database-specialty-topic-1-question-49/","unix_timestamp":1635255420,"exam_id":22,"choices":{"D":"Encrypt the table from the AWS Management Console or use the update-table command","C":"Define the TTL settings","E":"Set the provisioned read and write capacity","A":"Re-create global secondary indexes in the new table","B":"Define IAM policies for access to the new table"},"answers_community":["BC (100%)"],"answer_images":[],"timestamp":"2021-10-26 15:37:00","answer_description":"","topic":"1","question_id":304,"question_images":[],"answer":"BC","question_text":"A Database Specialist is designing a disaster recovery strategy for a production Amazon DynamoDB table. The table uses provisioned read/write capacity mode, global secondary indexes, and time to live (TTL). The Database Specialist has restored the latest backup to a new table.\nTo prepare the new table with identical settings, which steps should be performed? (Choose two.)","isMC":true},{"id":"KEesP6QcK3IL35efJAfy","timestamp":"2020-07-07 17:39:00","answer":"C","question_images":[],"isMC":true,"unix_timestamp":1594136340,"question_text":"A company is concerned about the cost of a large-scale, transactional application using Amazon DynamoDB that only needs to store data for 2 days before it is deleted. In looking at the tables, a Database Specialist notices that much of the data is months old, and goes back to when the application was first deployed.\nWhat can the Database Specialist do to reduce the overall cost?","exam_id":22,"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/25030-exam-aws-certified-database-specialty-topic-1-question-5/","answer_ET":"C","question_id":305,"discussion":[{"comment_id":"129089","timestamp":"1632094080.0","poster":"chicagomassageseeker","upvote_count":"13","content":"Answer C. Enable TTL on a new attribute."},{"comment_id":"1007918","content":"Selected Answer: C\nC. Create a new attribute in each table to track the expiration time and enable time to live (TTL) on each table.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","upvote_count":"2","poster":"Pranava_GCP","timestamp":"1694723400.0"},{"comment_id":"996107","upvote_count":"1","content":"Selected Answer: C\nEnable TTL on a new attribute.","timestamp":"1693575780.0","poster":"cat_of_meerkat"},{"comment_id":"852834","upvote_count":"1","content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-before-you-start.html correct answer C","timestamp":"1679982960.0","poster":"ken_test1234"},{"content":"Selected Answer: C\nAnswer C. Enable TTL on a new attribute.","comment_id":"813901","upvote_count":"1","timestamp":"1676798640.0","poster":"pintu143"},{"timestamp":"1674672000.0","content":"Selected Answer: C\nAnswer:C","poster":"renfdo","upvote_count":"2","comment_id":"788018"},{"poster":"SteveMartin9","upvote_count":"2","timestamp":"1673958420.0","content":"Selected Answer: C\nAuthor from the Udemy.com practice test says C is the correct answer.","comment_id":"778903"},{"timestamp":"1671033540.0","content":"Can someone please explain why A was a better solution for cost?","upvote_count":"1","comment_id":"745246","poster":"lollyj"},{"upvote_count":"4","content":"Selected Answer: C\nAnswer:C","poster":"ryuhei","comment_id":"620954","timestamp":"1655985900.0"},{"content":"Selected Answer: C\nC. Create a new attribute in each table to track the expiration time and enable time to live (TTL) on each table.","timestamp":"1651346340.0","comment_id":"595227","upvote_count":"3","poster":"novice_expert"},{"timestamp":"1636074780.0","content":"C: TTL is used in dynamo DB","upvote_count":"1","comment_id":"439488","poster":"Anuragdba"},{"comment_id":"434485","content":"its C, Dynamo DB TTL attribute for auto deletion","upvote_count":"1","poster":"aws4myself","timestamp":"1635993780.0"},{"content":"C ==>> Correct Answer.","timestamp":"1635931740.0","poster":"guru_ji","upvote_count":"1","comment_id":"425810"},{"comment_id":"395246","comments":[{"upvote_count":"2","content":"Super, I guess you are an aws product developer/architect. Nice answers, dont mislead people.","timestamp":"1635958200.0","poster":"aws4myself","comment_id":"432438"},{"content":"I don't think your solution helps to minimize the total cost of ownership, the delete queries will scan the tables we will need to pay for that, and we will need to pay for Glue Jobs executions and aws says \"...TTL is provided at no extra cost as a means to reduce stored data volumes..\"","comment_id":"630611","poster":"rlnd2000","timestamp":"1657640700.0","upvote_count":"2"}],"content":"My opinion:\n\nYou can include AWS glue to write a Deletion job to transform the data.\n\nTTL might be bound by the timefactor in the document","timestamp":"1635338580.0","upvote_count":"2","poster":"Ninjamonkey8812"},{"comment_id":"361508","timestamp":"1635239820.0","content":"Yes C is correct","poster":"Dip11","upvote_count":"1"},{"timestamp":"1633064220.0","content":"For sure Answer C","comment_id":"314123","upvote_count":"1","poster":"LMax"},{"timestamp":"1632962280.0","poster":"myutran","content":"Ans: C","upvote_count":"1","comment_id":"296291"},{"comment_id":"137097","content":"This is c - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","poster":"bigaws","upvote_count":"4","timestamp":"1632672720.0"},{"comment_id":"134143","upvote_count":"2","poster":"BillyC","timestamp":"1632313440.0","content":"C its correct"}],"topic":"1","choices":{"A":"Create a new attribute in each table to track the expiration time and create an AWS Glue transformation to delete entries more than 2 days old.","D":"Create an Amazon CloudWatch Events event to export the data to Amazon S3 daily using AWS Data Pipeline and then truncate the Amazon DynamoDB table.","B":"Create a new attribute in each table to track the expiration time and enable DynamoDB Streams on each table.","C":"Create a new attribute in each table to track the expiration time and enable time to live (TTL) on each table."},"answers_community":["C (100%)"]}],"exam":{"lastUpdated":"11 Apr 2025","provider":"Amazon","isMCOnly":false,"name":"AWS Certified Database - Specialty","numberOfQuestions":359,"isBeta":false,"isImplemented":true,"id":22},"currentPage":61},"__N_SSP":true}