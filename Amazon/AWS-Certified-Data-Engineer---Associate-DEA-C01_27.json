{"pageProps":{"questions":[{"id":"MKrVF9K3aA6KlbZET9Dx","unix_timestamp":1705799640,"answer_images":[],"choices":{"D":"Use Amazon Kinesis Data Streams to access and integrate third-party datasets from Amazon Elastic Container Registry (Amazon ECR).","C":"Use Amazon Kinesis Data Streams to access and integrate third-party datasets from AWS CodeCommit repositories.","B":"Use API calls to access and integrate third-party datasets from AWS DataSync.","A":"Use API calls to access and integrate third-party datasets from AWS Data Exchange."},"discussion":[{"content":"Selected Answer: A\nAWS DataSync is primarily used for data transfer services designed to simplify, automate, and accelerate moving data between on-premises storage systems and AWS storage services, as well as between different AWS storage services. Its primary role is not for accessing third-party datasets but for efficiently transferring large volumes of data.\nIn contrast, AWS Data Exchange is designed specifically for discovering and subscribing to third-party data in the cloud, providing direct API access to these datasets, which aligns perfectly with the company's need to integrate this data into their recommendation systems with minimal overhead.","comment_id":"1198355","poster":"KelvinPun","upvote_count":"13","timestamp":"1726862580.0"},{"poster":"ttpro1995","content":"Selected Answer: A\nYeah, AWS want people to buy data from their marketplace. So, ... you know.","comment_id":"1330879","upvote_count":"3","timestamp":"1734972480.0"},{"upvote_count":"1","content":"Should be AWS Data Exchange.","comment_id":"1288853","poster":"Shubham1989","timestamp":"1727233560.0"},{"upvote_count":"1","comment_id":"1250160","timestamp":"1721282820.0","poster":"lunachi4","content":"Selected Answer: A\nI will go with A. Kinesis Data Stram is more operational overhead."},{"upvote_count":"1","poster":"Manohar24","content":"Selected Answer: A\nA is correct","timestamp":"1720676160.0","comment_id":"1245911"},{"content":"A is correct, DataSync doesn't really rely on API calls.","comment_id":"1223805","poster":"sudohogan","upvote_count":"2","timestamp":"1717449120.0"},{"comment_id":"1223303","content":"Selected Answer: A\nA is correct answer.","poster":"nanaw770","timestamp":"1717367940.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1716389700.0","comment_id":"1215808","poster":"0060594","content":"Selected Answer: A\nAWS DataExchange"},{"upvote_count":"1","content":"Selected Answer: A\noptions B, C, and D involve using Amazon Kinesis Data Streams or other services that may not be directly suited for integrating third-party datasets from external sources like AWS Data Exchange. These options might require additional configurations, data processing steps, or infrastructure management, resulting in higher operational overhead compared to directly leveraging AWS Data Exchange's capabilities through API calls (Option A).","comment_id":"1208679","poster":"k350Secops","timestamp":"1715225400.0"},{"timestamp":"1710746700.0","comment_id":"1176326","poster":"hsnin","content":"Selected Answer: A\nAWS Data Exchange is a service that makes it easy to share and manage data permissions from other organizations","upvote_count":"4"},{"poster":"kj07","comment_id":"1172533","content":"I will go with A.","timestamp":"1710332280.0","upvote_count":"1"},{"upvote_count":"2","poster":"Josa2","content":"Selected Answer: B\nThere is no info or guarantee this third-party dataset is available in AWS to be part of a data-share, hence the more assertive answer is B","timestamp":"1710063600.0","comment_id":"1170191"},{"content":"Selected Answer: A\nA for me. \"You can also discover and subscribe to new third-party data sets available through AWS Data Exchange\" \nhttps://docs.aws.amazon.com/data-exchange/latest/userguide/what-is.html","comment_id":"1167771","upvote_count":"3","poster":"GiorgioGss","timestamp":"1709798580.0"},{"poster":"ceramem","comment_id":"1148917","content":"A\nData exchange is primarily designed for this purpose.","timestamp":"1707805440.0","upvote_count":"2"},{"upvote_count":"3","comment_id":"1137864","timestamp":"1706816100.0","content":"A\nData exchange is primarily designed for this purpose.","poster":"TonyStark0122"},{"comment_id":"1137262","timestamp":"1706761140.0","upvote_count":"3","poster":"lalitjhawar","content":"A\nData Exchange is the AWS official third-party datasets repository: https://aws.amazon.com/data-exchange"}],"answer_ET":"A","question_text":"A media company wants to improve a system that recommends media content to customer based on user behavior and preferences. To improve the recommendation system, the company needs to incorporate insights from third-party datasets into the company's existing analytics platform.\nThe company wants to minimize the effort and time required to incorporate third-party datasets.\nWhich solution will meet these requirements with the LEAST operational overhead?","exam_id":21,"answer_description":"","isMC":true,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/131706-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["A (93%)","7%"],"topic":"1","timestamp":"2024-01-21 02:14:00","question_id":131,"answer":"A"},{"id":"anmqEXJWq0fWcXuASMXo","answers_community":["B (85%)","A (15%)"],"answer_description":"","discussion":[{"upvote_count":"6","comment_id":"1140101","poster":"rralucard_","content":"Selected Answer: B\nOption B, writing an AWS Glue ETL job with the FindMatches ML transform, is likely to meet the requirements with the least operational overhead. This solution leverages a managed service (AWS Glue) and incorporates a built-in ML transform specifically designed for deduplication, thus minimizing the need for manual setup, maintenance, and machine learning expertise.","timestamp":"1707051360.0"},{"poster":"_JP_","comment_id":"1328213","content":"Selected Answer: A\nI disagree with B. That option requires additional effort just to train the ML model with labeled data. Option A is as simple as to use the robust pandas library","timestamp":"1734479760.0","upvote_count":"2"},{"timestamp":"1722836340.0","comment_id":"1260940","content":"Selected Answer: B\n100 % B","upvote_count":"1","poster":"V0811"},{"comment_id":"1170879","upvote_count":"4","timestamp":"1710144480.0","poster":"GiorgioGss","content":"Selected Answer: B\nB. https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html\n\"Find matches\nFinds duplicate records in the source data. You teach this machine learning transform by labeling example datasets to indicate which rows match. The machine learning transform learns which rows should be matches the more you teach it with example labeled data.\""},{"upvote_count":"1","timestamp":"1707222060.0","comment_id":"1142138","poster":"Aesthet","content":"Remove duplicates from already migrated data - probably D.\nRemove duplicates from data before migration - A is preferable."}],"unix_timestamp":1707051360,"choices":{"D":"Write an AWS Glue extract, transform, and load (ETL) job. Import the Python dedupe library. Use the dedupe library to perform data deduplication.","C":"Write a custom extract, transform, and load (ETL) job in Python. Import the Python dedupe library. Use the dedupe library to perform data deduplication.","B":"Write an AWS Glue extract, transform, and load (ETL) job. Use the FindMatches machine learning (ML) transform to transform the data to perform data deduplication.","A":"Write a custom extract, transform, and load (ETL) job in Python. Use the DataFrame.drop_duplicates() function by importing the Pandas library to perform data deduplication."},"question_images":[],"answer_images":[],"answer_ET":"B","exam_id":21,"topic":"1","question_id":132,"timestamp":"2024-02-04 13:56:00","question_text":"A company is migrating a legacy application to an Amazon S3 based data lake. A data engineer reviewed data that is associated with the legacy application. The data engineer found that the legacy data contained some duplicate information.\nThe data engineer must identify and remove duplicate information from the legacy application data.\nWhich solution will meet these requirements with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/132774-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"B","isMC":true},{"id":"UUGBzJe20I95UfeKQPcG","answer_ET":"BC","answer_images":[],"discussion":[{"content":"Selected Answer: BC\nhttps://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html","comment_id":"1170883","timestamp":"1710144720.0","poster":"GiorgioGss","upvote_count":"6"},{"upvote_count":"5","poster":"rralucard_","content":"Selected Answer: BC\nB. Use a columnar storage file format: This is an excellent approach. Columnar storage formats like Parquet and ORC are highly recommended for use with Redshift Spectrum. They store data in columns, which allows Spectrum to scan only the needed columns for a query, significantly improving query performance and reducing the amount of data scanned.\n\nC. Partition the data based on the most common query predicates: Partitioning data in S3 based on commonly used query predicates (like date, region, etc.) allows Redshift Spectrum to skip large portions of data that are irrelevant to a particular query. This can lead to substantial performance improvements, especially for large datasets.","timestamp":"1706969100.0","comment_id":"1139271"},{"content":"Selected Answer: BC\nPartioning helps filter the data and columnar storage is optimised for analytical (OLAP) queries","poster":"andrologin","upvote_count":"1","comment_id":"1245258","timestamp":"1720583280.0"},{"comment_id":"1227404","poster":"pypelyncar","upvote_count":"3","content":"Selected Answer: BC\nRedshift Spectrum is optimized for querying data stored in columnar formats like Parquet or ORC.\n These formats store each data column separately, allowing Redshift Spectrum to only scan the relevant columns for a specific query, significantly improving performance compared to row-oriented formats\nPartitioning organizes data files in S3 based on specific column values (e.g., date,\n region). When your queries filter or join data based on these partitioning columns (common query predicates), Redshift Spectrum can quickly locate the relevant data files, minimizing the amount of data scanned and accelerating query execution","timestamp":"1717947240.0"},{"comment_id":"1207703","timestamp":"1715053920.0","poster":"d8945a1","upvote_count":"1","content":"Selected Answer: BC\nhttps://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/"},{"upvote_count":"1","comment_id":"1179397","timestamp":"1711037880.0","poster":"certplan","content":"2. **Partitioning**:\n AWS documentation for Amazon Redshift Spectrum highlights the importance of partitioning data based on commonly used query predicates to improve query performance. By partitioning data, Redshift Spectrum can prune unnecessary partitions during query execution, reducing the amount of data scanned and improving overall query performance. This guidance can be found in the AWS documentation for Amazon Redshift Spectrum under \"Using Partitioning to Improve Query Performance\": https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum-partitioning.html"}],"question_id":133,"timestamp":"2024-02-03 15:05:00","answers_community":["BC (100%)"],"answer_description":"","topic":"1","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/132737-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"unix_timestamp":1706969100,"question_text":"A company is building an analytics solution. The solution uses Amazon S3 for data lake storage and Amazon Redshift for a data warehouse. The company wants to use Amazon Redshift Spectrum to query the data that is in Amazon S3.\nWhich actions will provide the FASTEST queries? (Choose two.)","answer":"BC","choices":{"A":"Use gzip compression to compress individual files to sizes that are between 1 GB and 5 GB.","B":"Use a columnar storage file format.","D":"Split the data into files that are less than 10 KB.","E":"Use file formats that are not splittable.","C":"Partition the data based on the most common query predicates."}},{"id":"I07dgTyOtcTFYZgTuwet","answer_ET":"CD","question_images":[],"isMC":true,"topic":"1","answer":"CD","discussion":[{"comment_id":"1185183","timestamp":"1711680000.0","content":"Selected Answer: CD\nThis solution only modifies the inbound rules of the security group of the DB instance, but it does not modify the outbound rules of the security group of the Lambda function. Additionally, this solution does not facilitate a private connection from the Lambda function to the DB instance, hence, the Lambda function would still need to use the public internet to access the DB instance. Therefore, this option does not fulfill the requirements.","upvote_count":"6","poster":"Alagong"},{"timestamp":"1738161720.0","upvote_count":"1","content":"Selected Answer: BD\nI would go with B & D. As C would have operational overhead in my opinion.","poster":"rr01","comment_id":"1348602"},{"content":"Selected Answer: BC\nD is wrong. It is a bad security practice for a DB to share SG with the client.\nC is correct compared to the other opinions (A & E).","timestamp":"1733313060.0","poster":"altonh","comment_id":"1321836","upvote_count":"1"},{"upvote_count":"1","timestamp":"1728291300.0","content":"Selected Answer: BD\nB & D \nC is wrong\nWhile you want the Lambda function to access the RDS instance privately, it does not need to run in the same subnet. As long as both are in the same VPC, the Lambda function can connect.","poster":"proserv","comment_id":"1294149"},{"poster":"tgv","timestamp":"1717307760.0","upvote_count":"2","content":"Selected Answer: CD\nI will go with C and D on this one, because in my opinion B is not correctly phrased. \n\n The correct way to phrase it would be something like:\n\nUpdate the security group of the RDS instance to allow inbound traffic on the database port (3306) only from the security group associated with the Lambda function.","comment_id":"1223034"},{"comments":[{"content":"Moreover, running a Lambda function within a subnet does not inherently ensure private connectivity to the RDS instance. Additional networking configurations would still be needed to allow the Lambda function to access the RDS instance securely, such as configuring the appropriate security groups and potentially adjusting network ACLs.\nHence C can't be the answer","upvote_count":"2","comment_id":"1206803","poster":"sdas1","timestamp":"1714897140.0"}],"upvote_count":"2","timestamp":"1714897080.0","comment_id":"1206802","content":"While placing the Lambda function in the same subnet as the DB instance would technically allow them to communicate privately within the same network, it introduces additional complexity and operational overhead. Lambda functions typically run in AWS-managed VPCs, and configuring them to run in a specific subnet might require manual intervention and ongoing maintenance.","poster":"sdas1"},{"upvote_count":"3","comment_id":"1203747","content":"Selected Answer: BD\nbbb ddd","poster":"Snape","timestamp":"1714350180.0"},{"timestamp":"1712151360.0","poster":"lucas_rfsb","upvote_count":"1","comment_id":"1188693","content":"Selected Answer: CD\nI would go with CD, since it's less operational effort, in my opinion"},{"poster":"arvehisa","upvote_count":"4","comment_id":"1186505","content":"Selected Answer: CD\nB: need update security group. and there there may be other application need to access db except for lambda function\nD: it works and reuse security group which has less operational overhead","timestamp":"1711847040.0"},{"poster":"harrura","comment_id":"1186157","content":"A is not an option as it exposes the data to public\nB is not an option as we don't want the lambda to be the only entity accessing the db, there can be many other apps. doing this is not scalable","timestamp":"1711811940.0","upvote_count":"2"},{"poster":"certplan","upvote_count":"1","content":"B. - While updating the security group of the DB instance to allow only Lambda function invocations on the database port may seem like a viable solution, it's not the most efficient approach. This option overlooks the need for the Lambda function to be able to communicate securely with the DB instance within the same VPC/subnet.\n - Reference: [Amazon RDS documentation on security groups](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithSecurityGroups.html)","timestamp":"1711040280.0","comment_id":"1179443"},{"content":"- AWS Lambda supports VPC configurations, allowing you to run Lambda functions within your own VPC. This enables private connectivity between Lambda functions and resources within the VPC, such as RDS DB instances.\nReference AWS Lambda documentation on VPC configurations: [AWS Lambda VPC Settings]https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html \n\n- AWS security groups provide a flexible and scalable way to control traffic to your instances or resources. By attaching the same security group to both the Lambda function and the RDS DB instance, you can ensure they share the same set of rules for inbound and outbound traffic.\n- Self-referencing rules within security groups enable instances within the same security group to communicate with each other over specified ports.\n- Reference AWS documentation on security groups and self-referencing rules: [Security Groups for Your VPC]https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html","timestamp":"1711039800.0","comment_id":"1179433","poster":"certplan","upvote_count":"1"},{"timestamp":"1711039560.0","upvote_count":"4","content":"So, there coudl be a justified argument for the following:\n\nC. Configure the Lambda function to run in the same subnet that the DB instance uses:\nBy running the Lambda function in the same subnet as the RDS DB instance, you enable them to communicate privately within the same network, eliminating the need for public internet access and reducing operational overhead.\n\nD. Attach the same security group to the Lambda function and the DB instance. Include a self-referencing rule that allows access through the database port:\nBy attaching the same security group to both the Lambda function and the RDS DB instance, and including a self-referencing rule that allows access through the database port, you ensure secure communication between them within the same VPC without exposing the database to the public internet. This approach minimizes operational overhead by centralizing security management and simplifying access control.","poster":"certplan","comment_id":"1179432"},{"timestamp":"1711038540.0","poster":"certplan","content":"Here's how you would implement this:\n\n1. **Attach the same security group to both the Lambda function and the RDS DB instance**: Ensure that both resources are associated with the same security group.\n\n2. **Create an inbound rule in the security group**: Configure the security group to allow inbound traffic on the database port (e.g., 3306 for MySQL) from the security group itself.\n\nFor example, if the security group ID is sg-1234567890 and the database port is 3306, the inbound rule would look something like this:\n\nType: Custom TCP Rule\nProtocol: TCP\nPort Range: 3306 (or the port your database uses)\nSource: sg-1234567890 (the security group ID itself)\n\n\nThis rule allows the Lambda function, which is also part of the same security group, to communicate with the RDS DB instance through the specified port. It effectively creates a loopback or self-referencing rule within the security group, allowing internal communication between resources while maintaining security boundaries.","upvote_count":"1","comment_id":"1179415"},{"content":"The phrase \"Include a self-referencing rule that allows access through the database port\" refers to configuring the security group associated with the resources (in this case, the Lambda function and the RDS DB instance) to allow inbound traffic from the resources themselves on a specific port, typically the port used for database communication.\n\nIn AWS security groups, a self-referencing rule means allowing traffic from the security group itself. This setup is often used to facilitate communication between resources within the same security group or VPC without needing to specify individual IP addresses.","poster":"certplan","comment_id":"1179413","comments":[{"poster":"samadal","timestamp":"1724107860.0","upvote_count":"1","comment_id":"1268960","content":"Thank you. You always help me solve my problems."}],"timestamp":"1711038480.0","upvote_count":"2"},{"poster":"GiorgioGss","content":"Selected Answer: BC\nWhen you want Lambda to \"privately\" connect to a resource (RDS in this case) that sits inside a VPC, then you deploy Lambda inside VPC. = C\nThen you attach a proper IAM role to lambda. \nThen, to be more secure you open the RDS security group only on the specific port:\nMySQL/Aurora MySQL: 3306\nSQL Server: 1433\nPostgreSQL: 5432\nOracle: 1521","upvote_count":"1","comment_id":"1170884","timestamp":"1710145200.0"},{"upvote_count":"1","poster":"BartoszGolebiowski24","comment_id":"1147314","timestamp":"1707664320.0","content":"what does \"Include a self-referencing rule that allows access through the database port.\" mean?"},{"comment_id":"1139272","timestamp":"1706969280.0","upvote_count":"1","poster":"rralucard_","content":"Selected Answer: BC\nB. Update the security group of the DB instance to allow only Lambda function invocations on the database port: Modifying the security group of the RDS instance to allow incoming connections on the database port (e.g., port 3306 for MySQL, 5432 for PostgreSQL) from the Lambda function is a crucial step. This ensures that the RDS instance can accept connections from the Lambda function.\n\nC. Configure the Lambda function to run in the same subnet that the DB instance uses: Placing the Lambda function in the same VPC and subnet as the RDS instance ensures private connectivity. AWS Lambda needs to be configured with a VPC configuration that includes the subnet(s) and security group(s) that allow access to the RDS instance."}],"timestamp":"2024-02-03 15:08:00","choices":{"D":"Attach the same security group to the Lambda function and the DB instance. Include a self-referencing rule that allows access through the database port.","B":"Update the security group of the DB instance to allow only Lambda function invocations on the database port.","A":"Turn on the public access setting for the DB instance.","C":"Configure the Lambda function to run in the same subnet that the DB instance uses.","E":"Update the network ACL of the private subnet to include a self-referencing rule that allows access through the database port."},"answer_images":[],"question_id":134,"answers_community":["CD (62%)","BD (24%)","14%"],"unix_timestamp":1706969280,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/132738-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A company uses Amazon RDS to store transactional data. The company runs an RDS DB instance in a private subnet. A developer wrote an AWS Lambda function with default settings to insert, update, or delete data in the DB instance.\nThe developer needs to give the Lambda function the ability to connect to the DB instance privately without using the public internet.\nWhich combination of steps will meet this requirement with the LEAST operational overhead? (Choose two.)","exam_id":21},{"id":"BF47CG7McNndhYmimi0r","question_id":135,"discussion":[{"content":"Selected Answer: D\nD. Create an AWS Lambda function. Ensure that the function is warm by scheduling an Amazon EventBridge rule to invoke the Lambda function every 5 minutes by using mock events.","timestamp":"1742051880.0","poster":"MephiboshethGumani","upvote_count":"2","comment_id":"1398908"},{"content":"I AM THINKING B. Dont u think provisionning concurrency add additional cost even when the function is not in active use, which is unnecessary for an occasionally invoked function.","upvote_count":"1","comment_id":"1306087","timestamp":"1730535240.0","poster":"royalrum"},{"timestamp":"1717948140.0","poster":"pypelyncar","content":"Selected Answer: B\nB and D are both ok. Still, since it says LEAST operational overhead, then keep it simple. B then.","comment_id":"1227412","upvote_count":"4"},{"timestamp":"1714468980.0","poster":"HunkyBunky","upvote_count":"2","comment_id":"1204449","content":"Selected Answer: B\nB - simple and clear"},{"content":"Selected Answer: B\nI would go in B","comment_id":"1187366","upvote_count":"1","timestamp":"1711969260.0","poster":"lucas_rfsb"},{"poster":"GiorgioGss","upvote_count":"1","content":"Selected Answer: B\nAlthough D seems a good choice but the questions asks for \"LEAST operational overhead\" will result in B","comment_id":"1170888","timestamp":"1710145800.0"},{"timestamp":"1709324280.0","comment_id":"1163730","content":"Answ. B\nYou can create a web API with an HTTP endpoint for your Lambda function by using Amazon API Gateway. API Gateway provides tools for creating and documenting web APIs that route HTTP requests to Lambda functions. You can secure access to your API with authentication and authorization controls. Your APIs can serve traffic over the internet or can be accessible only within your VPC.\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html","poster":"damaldon","upvote_count":"1"},{"timestamp":"1706769060.0","poster":"rralucard_","upvote_count":"2","content":"Selected Answer: B\nB.\nAWS Lambda functions can be easily integrated with Amazon API Gateway to create RESTful APIs. This integration allows API Gateway to directly invoke the Lambda function when the API endpoint is hit.","comment_id":"1137328"}],"exam_id":21,"question_text":"A company has a frontend ReactJS website that uses Amazon API Gateway to invoke REST APIs. The APIs perform the functionality of the website. A data engineer needs to write a Python script that can be occasionally invoked through API Gateway. The code must return results to API Gateway.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"B","answer_description":"","answer_images":[],"answers_community":["B (83%)","D (17%)"],"isMC":true,"timestamp":"2024-02-01 07:31:00","choices":{"B":"Create an AWS Lambda Python function with provisioned concurrency.","C":"Deploy a custom Python script that can integrate with API Gateway on Amazon Elastic Kubernetes Service (Amazon EKS).","A":"Deploy a custom Python script on an Amazon Elastic Container Service (Amazon ECS) cluster.","D":"Create an AWS Lambda function. Ensure that the function is warm by scheduling an Amazon EventBridge rule to invoke the Lambda function every 5 minutes by using mock events."},"unix_timestamp":1706769060,"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/132630-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"B"}],"exam":{"provider":"Amazon","isImplemented":true,"numberOfQuestions":207,"id":21,"name":"AWS Certified Data Engineer - Associate DEA-C01","isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true},"currentPage":27},"__N_SSP":true}