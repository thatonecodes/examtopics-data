{"pageProps":{"questions":[{"id":"9GlPReiLGKQGsZCcai7W","topic":"1","choices":{"D":"Partition the data by source.","A":"Convert the .csv files to Apache Parquet.","B":"Convert the .csv files to Apache Avro.","E":"Compress the .csv files.","C":"Partition the data by campaign."},"isMC":true,"question_id":151,"answer_images":[],"unix_timestamp":1619871300,"answers_community":["AC (90%)","10%"],"question_images":[],"answer_description":"","timestamp":"2021-05-01 14:15:00","question_text":"A marketing company is storing its campaign response data in Amazon S3. A consistent set of sources has generated the data for each campaign. The data is saved into Amazon S3 as .csv files. A business analyst will use Amazon Athena to analyze each campaign's data. The company needs the cost of ongoing data analysis with Athena to be minimized.\nWhich combination of actions should a data analytics specialist take to meet these requirements? (Choose two.)","exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/51407-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"AC","answer":"AC","discussion":[{"content":"A,C\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nRefer: \nPartition your data\nOptimize columnar data store generation","timestamp":"1634327760.0","poster":"AjithkumarSL","comments":[{"upvote_count":"5","timestamp":"1635979260.0","content":"How about compression? Compression take higher chance for optimizing however A, C is correct because Apache Parquet has data compressed as default.","comment_id":"389731","poster":"Huy"}],"upvote_count":"33","comment_id":"350025"},{"comment_id":"886495","timestamp":"1682958720.0","content":"AC: I passed the test","poster":"pk349","upvote_count":"1"},{"comment_id":"850055","timestamp":"1679742240.0","content":"A: for reducing data size\nC: since the user will query the data by campaign","upvote_count":"1","poster":"CleverMonkey092"},{"upvote_count":"3","poster":"cloudlearnerhere","comments":[{"timestamp":"1666870140.0","comment_id":"705483","poster":"cloudlearnerhere","upvote_count":"2","content":"Apache Parquet and Apache ORC are popular columnar data stores. They provide features that store data efficiently by employing column-wise compression, different encoding, compression based on data type, and predicate pushdown. They are also splittable. Generally, better compression ratios or skipping blocks of data means reading fewer bytes from Amazon S3, leading to better query performance."}],"comment_id":"705480","content":"Correct answers are A & C as it is recommended to partition the data as per the requirement and use columnar data store like parquet which compresses the data as well as allows splitting. \n\nCorrect answers are A & C as it is recommended to partition the data as per the requirement and use columnar data store like parquet which compresses the data as well as allows splitting. \n\n\nOption D is wrong as partitioning of the data should be as per the requirement which currently is queried as per the campaigns.\n\nOptions B & E are wrong as compressing the .csv files or using Avro would not provide as many benefits as parquet files.","timestamp":"1666870080.0"},{"upvote_count":"1","comment_id":"634763","poster":"Raje14k","content":"its A & C.","timestamp":"1658428680.0"},{"upvote_count":"1","poster":"rocky48","content":"Selected Answer: AC\nA: Because columnar format helps to improve performance. Parquet is splittable and compresses by default hence option e is already taken care here.\nC: Partitioning improves performance. Here since the bulk of the analysis activity is dependent on campaign hence this will be ideal for partitioning (limited partitions and low cardinality)","timestamp":"1658289060.0","comment_id":"633818"},{"content":"Selected Answer: AC\nAC is correct.","poster":"dushmantha","timestamp":"1656745740.0","comment_id":"625985","upvote_count":"1"},{"poster":"Ob1KN0B","upvote_count":"2","timestamp":"1653840420.0","content":"Selected Answer: AC\nA: Because columnar format helps to improve performance. Parquet is splittable and compresses by default hence option e is already taken care here.\nC: Partitioning improves performance. Here since the bulk of the analysis activity is dependent on campaign hence this will be ideal for partitioning (limited partitions and low cardinality)","comment_id":"608827"},{"comment_id":"608242","timestamp":"1653707700.0","upvote_count":"1","content":"Selected Answer: BC\nAnswer should be B & C","poster":"Bik000"},{"poster":"YahiaAglan74","content":"Selected Answer: AC\nAC is the correct answer","upvote_count":"1","timestamp":"1653428220.0","comment_id":"606913"},{"content":"C,E seems to be correct answer. Since all sources are consistent, it is better to partition based on campaign. Also, compression means less data will be scanned which will save costs. Why would you change format to either Avro or Parquet? You would need to build an etl job using Glue etc. to do this which would add to the cost.","comment_id":"601826","poster":"certificationJunkie","upvote_count":"1","timestamp":"1652570100.0"},{"content":"A and C\n\nas the data analyst will query per campaign, then it is a good partition key\nAlso, columnar data types such as Parquet are better for analysis than row-based such as Avro.","timestamp":"1648324440.0","poster":"pidkiller","upvote_count":"1","comment_id":"575753"},{"content":"A and C it is","upvote_count":"1","poster":"PravinT","comment_id":"558542","timestamp":"1646104860.0"},{"content":"Selected Answer: AC\nA & C is correct","timestamp":"1638737460.0","poster":"Ajithkt","comment_id":"494657","upvote_count":"4"},{"content":"A and C","timestamp":"1636926720.0","poster":"aws2019","comment_id":"478356","upvote_count":"1"},{"comment_id":"367851","timestamp":"1634599740.0","upvote_count":"2","poster":"Monika14Sharma","content":"Correct answer is A&C"},{"comment_id":"348775","timestamp":"1633228920.0","upvote_count":"2","poster":"VikG12","content":"A,C it is."},{"poster":"bermo","comment_id":"346924","upvote_count":"1","content":"A&C for me","timestamp":"1632144840.0"}]},{"id":"BRCnZ5PB7DeiveBkkNcc","question_id":152,"url":"https://www.examtopics.com/discussions/amazon/view/51706-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_description":"","question_images":[],"choices":{"C":"Create an Amazon Athena table with CREATE TABLE AS SELECT (CTAS) to ensure data is refreshed from underlying queries against the raw dataset. Create an AWS Glue Data Catalog to manage the Hive metadata over the CTAS table. Create an Amazon EMR cluster and use the metadata in the AWS Glue Data Catalog to run Hive processing queries in Amazon EMR.","A":"Create an AWS Glue Data Catalog to manage the Hive metadata. Create an AWS Glue crawler over Amazon S3 that runs when data is refreshed to ensure that data changes are updated. Create an Amazon EMR cluster and use the metadata in the AWS Glue Data Catalog to run Hive processing queries in Amazon EMR.","D":"Use an S3 Select query to ensure that the data is properly updated. Create an AWS Glue Data Catalog to manage the Hive metadata over the S3 Select table. Create an Amazon EMR cluster and use the metadata in the AWS Glue Data Catalog to run Hive processing queries in Amazon EMR.","B":"Create an AWS Glue Data Catalog to manage the Hive metadata. Create an Amazon EMR cluster with consistent view enabled. Run emrfs sync before each analytics step to ensure data changes are updated. Create an EMR cluster and use the metadata in the AWS Glue Data Catalog to run Hive processing queries in Amazon EMR."},"answer_ET":"A","unix_timestamp":1620057360,"discussion":[{"content":"looks like 'A' it is.","timestamp":"1632512340.0","comment_id":"348777","comments":[{"timestamp":"1634862180.0","poster":"Dr_Kiko","content":"wrong; it says schema is stable so you dont need to jerk crawlers every time\nB","comment_id":"423913","comments":[{"content":"B is wrong, they did not even mention the usage of S3 dude","timestamp":"1643385300.0","comment_id":"534823","poster":"Ipc01","upvote_count":"1"}],"upvote_count":"3"}],"upvote_count":"25","poster":"VikG12"},{"content":"Answer should be A, \n1. Consistent View is no more required\n2. Though schema is stable in this running Glue Crawler is one of the way to get the partition metadata updated","comment_id":"490407","upvote_count":"5","poster":"Thiya","timestamp":"1638246120.0"},{"comment_id":"940857","upvote_count":"2","content":"I think this question is just outdated. For what the question says, B should be the answer. A is too broad, as it doesn't mention that it needs something to trigger the crawler (like a Lambda). And it states that the schema is stable, so no need to run crawler all the time a file is updated. B doesn't need to mention s3 because the \"enable consistent view\" already means that! It's out dated because Amazon got rid off the consistent view from EMR. But looking 3 years back, B would be the perfect answer.","poster":"wally_1995","timestamp":"1688300580.0"},{"timestamp":"1682958780.0","poster":"pk349","comment_id":"886496","upvote_count":"1","content":"A: I passed the test"},{"timestamp":"1658885460.0","poster":"rocky48","upvote_count":"1","comment_id":"637728","content":"Selected Answer: A\nAnswer is A"},{"comment_id":"475140","upvote_count":"2","timestamp":"1636503420.0","poster":"Fazil_Cp","content":"I think answer is A. \nIt can also be that the question is bit outdated , as now S3 has strong read after write consistency , EMRFS consistent view might not make sense now."},{"poster":"carlosrochacardoso","timestamp":"1636027500.0","comment_id":"426249","upvote_count":"3","content":"I think it's A\nYou no longer need to use EMRFS Consistent View as Amazon S3 supports strong read-after-write Consistency. See Strong read-after-write consistency. This works with all Amazon EMR versions.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-consistent-view.html"},{"upvote_count":"4","poster":"uninit","content":"I believe it is B. \nThe key point is \"It is vital that the data from the reports and associated analytics is completely up to date based on the data in Amazon S3.\" \nEMRFS Consistent view allows EMR clusters to check for list and read-after-write consistency for Amazon S3 objects written by or synced with EMRFS. \nIf you directly delete objects from Amazon S3 that are tracked in EMRFS metadata, EMRFS treats the object as inconsistent and throws an exception after it has exhausted retries. Use EMRFS to delete objects in Amazon S3 that are tracked using consistent view. Alternatively, you can use the emrfs command line to purge metadata entries for objects that have been directly deleted, or you can sync the consistent view with Amazon S3 immediately after you delete the objects.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-consistent-view.html\n\nTo run Glue Crawlers when S3 is refreshed needs S3 Trigger Lambda functions that run the crawler. It seems too much of an overhead especially when EMRFS consistent view / sync command can maintain consistency with already provisioned EMR cluster.","timestamp":"1635011400.0","comment_id":"425419"},{"content":"Glue crawler is responsible for the schema definition , which is mentioned as stable in this case .A glue ETL job could have moved the data .Having said that , I go for B","poster":"kc1982","timestamp":"1634640480.0","comment_id":"417086","upvote_count":"4"},{"content":"B is correct. Because schema is stable, you don't need to run Glue crawler again. Moreover, data stored in S3 therefore EMRFS is needed and with consistent view, data is updated.","poster":"Huy","timestamp":"1633669020.0","upvote_count":"1","comments":[{"content":"Sorry, may be A is correct. When you use AWS Glue Data Catalog as the metastore for Hive, no need to configure EMRFS.","comment_id":"391780","poster":"Huy","timestamp":"1634164020.0","upvote_count":"1"}],"comment_id":"389733"},{"comment_id":"367853","content":"Correct Answer is A","upvote_count":"1","poster":"Monika14Sharma","timestamp":"1632955620.0"},{"comment_id":"355992","timestamp":"1632518880.0","content":"Agree with A","upvote_count":"1","poster":"AjithkumarSL"}],"answers_community":["A (100%)"],"answer_images":[],"isMC":true,"topic":"1","answer":"A","exam_id":20,"question_text":"An online retail company is migrating its reporting system to AWS. The company's legacy system runs data processing on online transactions using a complex series of nested Apache Hive queries. Transactional data is exported from the online system to the reporting system several times a day. Schemas in the files are stable between updates.\nA data analyst wants to quickly migrate the data processing to AWS, so any code changes should be minimized. To keep storage costs low, the data analyst decides to store the data in Amazon S3. It is vital that the data from the reports and associated analytics is completely up to date based on the data in Amazon S3.\nWhich solution meets these requirements?","timestamp":"2021-05-03 17:56:00"},{"id":"UI0DaptOaVPlplOzUMAU","timestamp":"2022-04-20 19:54:00","question_text":"A media company is using Amazon QuickSight dashboards to visualize its national sales data. The dashboard is using a dataset with these fields: ID, date, time_zone, city, state, country, longitude, latitude, sales_volume, and number_of_items.\nTo modify ongoing campaigns, the company wants an interactive and intuitive visualization of which states across the country recorded a significantly lower sales volume compared to the national average.\nWhich addition to the company's QuickSight dashboard will meet this requirement?","answer_description":"","exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/73937-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"A","answer_images":[],"unix_timestamp":1650477240,"choices":{"C":"A drill-down layer for state-level sales volume data.","D":"A drill through to other dashboards containing state-level sales volume data.","A":"A geospatial color-coded chart of sales volume data across the country.","B":"A pivot table of sales volume data summed up at the state level."},"question_images":[],"topic":"1","question_id":153,"isMC":true,"discussion":[{"upvote_count":"15","poster":"G_C_P","timestamp":"1650477240.0","content":"A provides needful","comment_id":"588875"},{"content":"A: I passed the test","poster":"pk349","timestamp":"1682958840.0","upvote_count":"1","comment_id":"886498"},{"timestamp":"1679863620.0","comment_id":"851485","content":"A pivot table of sales volume data summed up at the state level will meet the requirement of an interactive and intuitive visualization of which states across the country recorded a significantly lower sales volume compared to the national average. The pivot table can be sorted by the sales volume column and filtered to show only the states with lower sales volume than the national average, making it easy to identify the problem areas. A geospatial color-coded chart (option A) could also be useful, but it may not be as intuitive for identifying the states with lower sales volume.","poster":"rags1482","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nI think it is C, since with A you are not comparing the states against the national average","poster":"Alex8","timestamp":"1674647460.0","comment_id":"787570"},{"upvote_count":"4","comment_id":"768099","content":"Selected Answer: A\nA \"intuitive visualization\"","timestamp":"1673041860.0","poster":"Ody__"},{"comment_id":"705489","content":"Answer is A\nData visualization depends on the story you want to tell. Maps are great at visualizing your geographic data by location. The data on a map is often displayed in a colored area map or a bubble map. If the location is not part of the story, a map could be messy. With a table, you can display a large number of precise measures and dimensions. You can quickly look up or compare individual values while also showing grand totals. However, given the amount of data, tables take longer to digest.\nHeat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers because color makes these easier to spot. Use a pivot table if you're going to further analyze data on the visual, for example, by changing column sort order or applying aggregate functions across rows or columns.","poster":"cloudlearnerhere","upvote_count":"4","timestamp":"1666870620.0"},{"poster":"Hussben","comment_id":"705323","upvote_count":"3","content":"Selected Answer: A\nA is more intuitive in this context","timestamp":"1666854900.0"},{"poster":"lygf","timestamp":"1664253840.0","comment_id":"680393","upvote_count":"2","content":"Selected Answer: A\nThe question ask for \" interactive and intuitive visualization\".\nBoth A and B works, but with a Pivot table it's hard to identify the states with the largest drop. You have to compare the numbers across the entire table. Not good"},{"upvote_count":"1","timestamp":"1663209240.0","content":"Selected Answer: A\nA is correct","poster":"he11ow0rId","comment_id":"669431"},{"content":"The answer is A.","comment_id":"646188","poster":"kondi2309","timestamp":"1660375560.0","upvote_count":"1"},{"comment_id":"634382","poster":"rocky48","timestamp":"1658381700.0","upvote_count":"2","content":"Selected Answer: A\nSelected Answer: A"},{"upvote_count":"2","comment_id":"631616","timestamp":"1657861860.0","comments":[{"upvote_count":"1","comment_id":"786401","content":"it says \"interactive and intuitive visualization\"","timestamp":"1674555000.0","poster":"hughnguyen"}],"content":"Selected Answer: B\nThey just want State names with low total sales. there is no need to show those on a map","poster":"ru4aws"},{"timestamp":"1657057500.0","poster":"Wesley27","comment_id":"627621","content":"Selected Answer: C\nI support C.\ndrill-down meets the need for interactive; state-level sales volume data meets the need for display of which states achieved considerably lower sales volumes than the national average","upvote_count":"1"},{"upvote_count":"2","timestamp":"1655363100.0","comment_id":"617143","poster":"Ramshizzle","content":"Selected Answer: A\nI think both a Pivot Table and a geo-spatial chart would be nice in this case. However, because the geo-spatial chart would work because we have lat/lon coordinates, and because I feel like these are considered more comprehensible by the exam-makers, I will choose A."},{"poster":"certificationJunkie","comment_id":"608035","content":"A will only show the volume for each state and location of the state in the map. How does it compare against the national average? I think ans is D.","upvote_count":"1","timestamp":"1653647760.0"},{"content":"We only need to compare the sales volume for each state. So there is no need for pivot table here as no requirement for slice and dice of data. GeoSpatial graph will do the job nicely","upvote_count":"2","poster":"certificationJunkie","comment_id":"604879","timestamp":"1653134220.0"},{"poster":"MWL","comment_id":"597505","timestamp":"1651800360.0","content":"Selected Answer: B\nI think it should be B. Using pivot table, we can display sales by city, and sum up with state level. We can use \"collapse\", \"sort\" to get an \"interactive and comprehensive\" display.","upvote_count":"2"},{"content":"Answer - A","comment_id":"595273","upvote_count":"2","poster":"jrheen","timestamp":"1651350840.0"},{"comment_id":"591405","content":"A is the answer","upvote_count":"2","timestamp":"1650868380.0","poster":"poppypanda"}],"answer":"A","answers_community":["A (70%)","B (20%)","10%"]},{"id":"fC9hEDId1vm3XFYlNqII","timestamp":"2020-08-09 13:57:00","answer_ET":"A","unix_timestamp":1596974220,"url":"https://www.examtopics.com/discussions/amazon/view/27699-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"timestamp":"1632698340.0","upvote_count":"53","poster":"Prodip","content":"Option A; We have implemented this to save cost .","comment_id":"154788"},{"upvote_count":"20","timestamp":"1633197420.0","poster":"awssp12345","comment_id":"163967","content":"B is not correct because snapshotting will save costs but not solve problem of cluster being undersized\nC is not correct because - CTAS is not used to move data to S3 via spectrum. CTAS Creates a new table based on a query. The owner of this table is the user that issues the command. \nD is incorrect because EMR cannot be used as Data Warehouse solution And they do not need interactive query with Athena.\nA is correct because that exactly specifies how to move data to Redshift spectrum and reduce cluster space: https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html"},{"comment_id":"1148901","timestamp":"1707802620.0","poster":"kondi2309","content":"Selected Answer: A\nDef A, to save cost and less admin.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1690840560.0","comment_id":"968519","poster":"NikkyDicky","content":"Selected Answer: A\nits an A"},{"timestamp":"1682946360.0","content":"A: I passed the test","comment_id":"886264","upvote_count":"1","poster":"pk349"},{"upvote_count":"2","content":"A. The Udemy course by Stephane Maarek and Frank Kane has a really similar question in the practice exam.","comment_id":"856774","poster":"Aina","timestamp":"1680250380.0"},{"poster":"cloudlearnerhere","comment_id":"711111","timestamp":"1667563920.0","upvote_count":"4","content":"Selected Answer: A\nCorrect answer is A as the AWS Glue job can be used to offload the data older than 13 months from Redshift to S3. 13 months data can be queried from Redshift, while 7 years data in S3 can be queried using Redshift Spectrum.\n\n\n\nOption B is wrong as this would increase the cost further and would not scale far.\n\nOption C is wrong as CTAS is not used to move data to S3 via the spectrum. CTAS creates a new table based on a query. The owner of this table is the user that issues the command.\n\nOption D is wrong as EMR would increase the administrative effort as compared to Redshift."},{"content":"I am wondering why in the portal the correct ans is given as B. who validated and gives the right ans here?","comment_id":"705577","timestamp":"1666876500.0","poster":"Rejju","upvote_count":"2"},{"comment_id":"660628","poster":"Abep","timestamp":"1662422220.0","content":"Selected Answer: A\nAnswer is A\nhttps://d1.awsstatic.com/whitepapers/amazon-redshift-cost-optimization.pdf","upvote_count":"3"},{"timestamp":"1658454300.0","comment_id":"634933","upvote_count":"1","poster":"rocky48","content":"Selected Answer: A\nAnswer-A"},{"poster":"Bik000","timestamp":"1653205140.0","content":"Selected Answer: A\nAnswer should be A","comment_id":"605243","upvote_count":"1"},{"comment_id":"595305","content":"Answer-A","timestamp":"1651355040.0","upvote_count":"1","poster":"jrheen"},{"timestamp":"1647102300.0","poster":"jmensah60","upvote_count":"3","content":"Selected Answer: A\nA ticks all the boxes","comment_id":"566263"},{"upvote_count":"1","timestamp":"1637416260.0","comment_id":"482574","poster":"aws2019","content":"A is the right answer"},{"content":"B = wrong, this will not solve either cost or scale problem. C = wrong, to create table on S3 you use CREATE EXTERNAL TABLE not CTAS, also this does not remove older data. D = wrong, nonsense.","poster":"Shraddha","upvote_count":"1","timestamp":"1636292220.0","comment_id":"383503"},{"poster":"leliodesouza","timestamp":"1634784900.0","content":"The answer is A.","upvote_count":"2","comment_id":"359122"},{"comment_id":"348273","timestamp":"1634684160.0","upvote_count":"1","content":"When reading the Post : https://aws.amazon.com/blogs/big-data/amazon-redshift-dense-compute-dc2-nodes-deliver-twice-the-performance-as-dc1-at-the-same-price/, Option B Makes More sense.. any thoughts..","poster":"AjithkumarSL","comments":[{"comment_id":"364715","content":"It's not cost effective..","timestamp":"1635178680.0","poster":"asg76","upvote_count":"1"}]},{"timestamp":"1634513100.0","content":"A is the right answer","upvote_count":"3","comment_id":"274242","poster":"lostsoul07"},{"poster":"Sai12","upvote_count":"4","comment_id":"245756","content":"What is the question actually telling us?\n\n1) has been collecting IoT sensor data from devices on its factory floor for a year - Performance is not an issue\n2) the cluster will be undersized in less than 4 months - The current cluster size is too small for long term but its good for cicra 16 months (12 + 4)\n3) the cluster will be undersized in less than 4 months - The current cluster can hold 13 months of data \n4) most queries only reference the most recent 13 months of data - All we need is 13 months of data in the cluster\n5) there are also quarterly reports that need to query all the data generated from the past 7 years - Long term clearly means 7 years and beyond\n\nBased on the above A is clearly the answer. For B you would need to create a cluster of a size that can handle 7 years worth of data and this will impact cost.","timestamp":"1634488200.0"},{"comment_id":"216831","timestamp":"1633976640.0","content":"I will go for A","upvote_count":"2","poster":"BillyC"},{"timestamp":"1633651380.0","content":"The key part of the question is:\n\"The data analyst has indicated that most queries only reference the most recent 13 months of data, yet there are also quarterly reports that need to query all the data generated from the past 7 years\"\nOption A would fulfill this requirement","upvote_count":"1","comment_id":"191268","poster":"syu31svc"},{"comment_id":"175253","poster":"Paitan","content":"A for sure.","timestamp":"1633519620.0","upvote_count":"2"},{"upvote_count":"2","content":"my answer is A","timestamp":"1632777420.0","comment_id":"159524","poster":"zeronine"},{"content":"B is definitely wrong. A seems correct. C is a also an option but you can't \"move\" records, required insert and delete like option A.","upvote_count":"2","poster":"abhineet","comment_id":"157434","timestamp":"1632755220.0"},{"timestamp":"1632117120.0","content":"Keytphrase here is \"performance of a long-term solution\". So although option B seems a overkill (you could do option A), for long term that would be the option.","upvote_count":"5","poster":"testtaker3434","comment_id":"153575"}],"answers_community":["A (100%)"],"question_id":154,"isMC":true,"answer_images":[],"topic":"1","choices":{"A":"Create a daily job in AWS Glue to UNLOAD records older than 13 months to Amazon S3 and delete those records from Amazon Redshift. Create an external table in Amazon Redshift to point to the S3 location. Use Amazon Redshift Spectrum to join to data that is older than 13 months.","C":"Execute a CREATE TABLE AS SELECT (CTAS) statement to move records that are older than 13 months to quarterly partitioned data in Amazon Redshift Spectrum backed by Amazon S3.","B":"Take a snapshot of the Amazon Redshift cluster. Restore the cluster to a new cluster using dense storage nodes with additional storage capacity.","D":"Unload all the tables in Amazon Redshift to an Amazon S3 bucket using S3 Intelligent-Tiering. Use AWS Glue to crawl the S3 bucket location to create external tables in an AWS Glue Data Catalog. Create an Amazon EMR cluster using Auto Scaling for any daily analytics needs, and use Amazon Athena for the quarterly reports, with both using the same AWS Glue Data Catalog."},"question_text":"A manufacturing company has been collecting IoT sensor data from devices on its factory floor for a year and is storing the data in Amazon Redshift for daily analysis. A data analyst has determined that, at an expected ingestion rate of about 2 TB per day, the cluster will be undersized in less than 4 months. A long-term solution is needed. The data analyst has indicated that most queries only reference the most recent 13 months of data, yet there are also quarterly reports that need to query all the data generated from the past 7 years. The chief technology officer (CTO) is concerned about the costs, administrative effort, and performance of a long-term solution.\nWhich solution should the data analyst use to meet these requirements?","answer":"A","question_images":[],"answer_description":"","exam_id":20},{"id":"JNSafPdCPVwoHzkfCLSo","timestamp":"2021-05-03 18:01:00","answer_ET":"D","unix_timestamp":1620057660,"url":"https://www.examtopics.com/discussions/amazon/view/51708-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"comment_id":"358369","poster":"asg76","timestamp":"1633867740.0","upvote_count":"30","content":"The question says least operational overhead...so it should be D"},{"poster":"Donell","comments":[{"timestamp":"1636104300.0","comment_id":"423905","poster":"Dr_Kiko","content":"that blog post literally says \"In this post, I describe a solution for transforming and moving data from an on-premises data store to Amazon S3 using AWS Glue\"","upvote_count":"5"}],"upvote_count":"18","timestamp":"1635631380.0","content":"Answer is D\n\nAWS Glue can communicate with an on-premises data store over VPN or DX connectivity. An AWS Glue crawler uses an S3 or JDBC connection to catalog the data source, and the AWS Glue ETL job uses S3 or JDBC connections as a source or target data store.\nhttps://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/","comment_id":"398341"},{"upvote_count":"2","timestamp":"1703501520.0","content":"Selected Answer: B\nD is wrong, the requirement is \"move the data to a data lake\". D doesn't store the source data, but only \"save the result to Amazon S3\".","comment_id":"1105187","poster":"zzhangi0520"},{"timestamp":"1691218140.0","comment_id":"972752","content":"Selected Answer: B\nI would go with B because of the requirement for minimum overhead.\nD is also correct but needs more work and services involved, we are using DMS for a data migration scenario and datapipeline for a transformation and movement scenario, it makes sense.","upvote_count":"2","poster":"MLCL"},{"upvote_count":"1","timestamp":"1691103180.0","poster":"whenthan","comment_id":"971508","content":"Selected Answer: D\nAWS Glue can also connect to a variety of on-premises JDBC data stores such as PostgreSQL, MySQL, Oracle, Microsoft SQL Server, and MariaDB.\n\nAWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target."},{"timestamp":"1688480460.0","poster":"ccpmad","comment_id":"942858","content":"pk349 user is stupid. In all questions posts X: I passed the test. Stupid.","upvote_count":"9"},{"content":"Correct answer is D as AWS Glue can be used to extract from the on-premises data store using JDBC connection, transform data and store the data in S3 with the least operational overhead. \nCorrect answer is D as AWS Glue can be used to extract from the on-premises data store using JDBC connection, transform data and store the data in S3 with the least operational overhead. \n\nOption A is wrong as using a customized batch upload process would add to the operational overhead.\n\nOption B is wrong as creating an intermediate RDS instance with Data Pipeline jobs added to the operational overhead.\n\nOption C is wrong as creating a Redshift cluster would add to the operational overhead.","timestamp":"1666870860.0","comment_id":"705492","upvote_count":"2","poster":"cloudlearnerhere"},{"upvote_count":"1","comment_id":"705331","content":"Selected Answer: D\nI think B is not correct. If the goal is to migrate this database to RDS, then is B. But we want to move it to S3, AWS Glue can do ETL. It is less overhead","timestamp":"1666855620.0","poster":"Hussben"},{"upvote_count":"1","timestamp":"1658291280.0","content":"Selected Answer: D\nAnswer is D","poster":"rocky48","comment_id":"633830"},{"poster":"dushmantha","content":"Selected Answer: B\nI think most of us missing the point that we should understand what each solution is designed for. Clearly this is a database migration task so in that case I would defiinetely use DMS, coz its optimized for such tasks. Considering that I would say this is not and ETL task so that Glue ETL isn't an option. Moreover Glue ETL uses \"serverless spark platform\" and therefore will be expensive for sure. Therefore I go with option B.","upvote_count":"4","timestamp":"1655628780.0","comment_id":"618596"},{"comment_id":"607968","content":"C is the correct answer. For any form of analytics, RedShift is a preferred choice. Athena use case is for AdHoc query.","upvote_count":"3","timestamp":"1653636420.0","comments":[{"poster":"allanm","comment_id":"672118","content":"Redshift and redshift spectrum adds significantly more operational overhead!","timestamp":"1663488900.0","upvote_count":"1"}],"poster":"certificationJunkie"},{"upvote_count":"4","poster":"RSSRAO","comment_id":"546403","content":"Selected Answer: D\nExplanation\nCorrect answer is D as AWS Glue can be used to extract from the on-premises data store using JDBC connection, transform data and store the data in S3 with the least operational overhead.\nRefer AWS documentation - Glue Analyze On-premises Data Store (https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/)\nAWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3,\ndata stores in a VPC, or on-premises JDBC data stores as a target.\nOption A is wrong as using a customized batch upload process would add to the operational overhead.\nOption B is wrong as creating an intermediate RDS instance with Data Pipeline jobs added to the operational overhead.\nOption C is wrong as creating a Redshift cluster would add to the operational overhead.","timestamp":"1644749820.0"},{"comment_id":"543870","content":"Selected Answer: D\nWe want to reduce operational costs and overhead.\n\nA - Sounds good but the beginning is not right, we can do better than customized batch.\nB - That's too much work, we are not interested in keeping the PSQL Engine, so if we can remove that's step from the equation we are good to go.\nC - We directly get the table from the On-prem DB with Glue and move it directly to S3. That's good, but we are adding spectrum to the mix, which is a more expensive solution.\nD - Yes, we are moving directly from on-prem to S3, but using athena to query the data. This is the response.","upvote_count":"4","timestamp":"1644420420.0","poster":"penelop"},{"poster":"TerrancePythonJava","content":"Selected Answer: B\nI believe answer is 'B'","comment_id":"540691","timestamp":"1644016200.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1637179200.0","poster":"aws2019","comment_id":"480244","content":"It is close, but I am leaning towards \"D\""},{"timestamp":"1635393840.0","comment_id":"393589","poster":"Donell","content":"I believe answer is D.\nAWS Glue can communicate with an on-premises data store over VPN or DX connectivity.\n\nAn AWS Glue crawler uses an S3 or JDBC connection to catalog the data source, and the AWS Glue ETL job uses S3 or JDBC connections as a source or target data store.\n\nhttps://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/","upvote_count":"3"},{"upvote_count":"2","timestamp":"1635063900.0","comment_id":"371502","content":"The question asks to achieve the goals with \"least operational overhead\".\nAnswer is D....B cannot be the answer as DMS require EC2 instance to be created for the replication task, this is an operational overhead. Same for A, customized batch process will be an operational overhead. C is not an option as you need to maintain a redshift cluster.","poster":"asg76"},{"timestamp":"1634970300.0","upvote_count":"2","comment_id":"367857","content":"It should be D for least operational overhead.","poster":"Monika14Sharma"},{"poster":"curioustester","upvote_count":"3","content":"C is the answer.\nAs per https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/#:~:text=AWS%20Glue%20can%20communicate%20with,over%20VPN%20or%20DX%20connectivity.&text=An%20AWS%20Glue%20crawler%20uses,source%20or%20target%20data%20store. Glue can connect to the on-prem database and store the data in S3.\n\nSince it is historical data needed for Analytics, the enriched data should be in Redshift.","timestamp":"1633497480.0","comments":[{"poster":"Rob_q","timestamp":"1647950820.0","comment_id":"572899","upvote_count":"1","content":"Redshift is pretty expensive. I was between both, but I think Athena is more suitable as an answer."}],"comment_id":"355876"},{"timestamp":"1633428660.0","upvote_count":"1","poster":"afantict","comment_id":"353626","content":"I think D is correct."},{"timestamp":"1633304100.0","comment_id":"348787","content":"Should be 'B'","poster":"VikG12","upvote_count":"8"}],"answers_community":["D (55%)","B (45%)"],"question_id":155,"isMC":true,"answer_images":[],"topic":"1","choices":{"C":"Configure an AWS Glue crawler to use a JDBC connection to catalog the data in the on-premises database. Use an AWS Glue job to enrich the data and save the result to Amazon S3 in Apache Parquet format. Create an Amazon Redshift cluster and use Amazon Redshift Spectrum to query the data.","A":"Upload the data from the on-premises PostgreSQL database to Amazon S3 by using a customized batch upload process. Use the AWS Glue crawler to catalog the data in Amazon S3. Use an AWS Glue job to enrich and store the result in a separate S3 bucket in Apache Parquet format. Use Amazon Athena to query the data.","B":"Create an Amazon RDS for PostgreSQL database and use AWS Database Migration Service (AWS DMS) to migrate the data into Amazon RDS. Use AWS Data Pipeline to copy and enrich the data from the Amazon RDS for PostgreSQL table and move the data to Amazon S3. Use Amazon Athena to query the data.","D":"Configure an AWS Glue crawler to use a JDBC connection to catalog the data in the on-premises database. Use an AWS Glue job to enrich the data and save the result to Amazon S3 in Apache Parquet format. Use Amazon Athena to query the data."},"question_text":"A company hosts an on-premises PostgreSQL database that contains historical data. An internal legacy application uses the database for read-only activities. The company's business team wants to move the data to a data lake in Amazon S3 as soon as possible and enrich the data for analytics.\nThe company has set up an AWS Direct Connect connection between its VPC and its on-premises network. A data analytics specialist must design a solution that achieves the business team's goals with the least operational overhead.\nWhich solution meets these requirements?","answer":"D","question_images":[],"answer_description":"","exam_id":20}],"exam":{"provider":"Amazon","lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":164,"name":"AWS Certified Data Analytics - Specialty","isMCOnly":true,"isImplemented":true,"id":20},"currentPage":31},"__N_SSP":true}