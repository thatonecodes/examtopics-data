{"pageProps":{"questions":[{"id":"9XbZqOkjxh1MSNjAgqIz","answer_description":"","question_text":"An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance.\n\nA solutions architect needs to minimize the amount of operational effort that is needed for the job to run.\n\nWhich solution meets these requirements?","unix_timestamp":1678463040,"question_images":[],"answer":"C","question_id":351,"answer_ET":"C","answers_community":["C (100%)"],"timestamp":"2023-03-10 16:44:00","url":"https://www.examtopics.com/discussions/amazon/view/102165-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"B":"Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.","C":"Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.","A":"Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.","D":"Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job."},"discussion":[{"poster":"ktulu2602","comment_id":"835816","timestamp":"1694420160.0","upvote_count":"25","content":"Selected Answer: C\nThe requirement is to run a daily scheduled job to aggregate and filter sales records for analytics in the most efficient way possible. Based on the requirement, we can eliminate option A and B since they use AWS Lambda which has a limit of 15 minutes of execution time, which may not be sufficient for a job that can take up to an hour to complete.\n\nBetween options C and D, option C is the better choice since it uses AWS Fargate which is a serverless compute engine for containers that eliminates the need to manage the underlying EC2 instances, making it a low operational effort solution. Additionally, Fargate also provides instant scale-up and scale-down capabilities to run the scheduled job as per the requirement.\n\nTherefore, the correct answer is:\n\nC. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job."},{"comment_id":"872711","poster":"imvb88","upvote_count":"5","timestamp":"1697548020.0","content":"Selected Answer: C\n\"1-hour job\" -> A, B out since max duration for Lambda is 15 min\n\nBetween C and D, \"minimize operational effort\" means Fargate -> C"},{"comment_id":"1114818","poster":"awsgeek75","upvote_count":"4","timestamp":"1720209660.0","content":"Selected Answer: C\nA&B are out due to Lambda 15 min limits\nC is less operationally complex than D so C is the right answer. Fargate is managed ECS cluster whereas EC2 based ECS will require more config overhead."},{"content":"Selected Answer: C\nC. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job","comment_id":"994253","upvote_count":"2","timestamp":"1709228220.0","poster":"Guru4Cloud"},{"timestamp":"1700806920.0","poster":"TariqKipkemei","upvote_count":"4","comment_id":"905467","content":"Selected Answer: C\nThe best option is C.\n'The job can take up to an hour to complete' rules out lambda functions as they only execute up to 15 mins. Hence option A and B are out.\n'The CPU and memory usage of the job are constant and are known in advance' rules out the need for autoscaling. Hence option D is out."},{"poster":"klayytech","comment_id":"849978","content":"Selected Answer: C\nThe solution that meets the requirements with the least operational overhead is to create a **Regional AWS WAF web ACL with a rate-based rule** and associate the web ACL with the API Gateway stage. This solution will protect the application from HTTP flood attacks by monitoring incoming requests and blocking requests from IP addresses that exceed the predefined rate. \n\nAmazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint is also a good solution but it requires more operational overhead than the previous solution. \n\nUsing Amazon CloudWatch metrics to monitor the Count metric and alerting the security team when the predefined rate is reached is not a solution that can protect against HTTP flood attacks. \n\nCreating an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours is not a solution that can protect against HTTP flood attacks.","upvote_count":"1","timestamp":"1695628560.0"},{"timestamp":"1695628200.0","comment_id":"849976","content":"Selected Answer: C\nThe solution that meets these requirements is C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job. This solution will minimize the amount of operational effort that is needed for the job to run.\n\nAWS Lambda which has a limit of 15 minutes of execution time,","upvote_count":"2","poster":"klayytech"}],"exam_id":31,"isMC":true,"topic":"1","answer_images":[]},{"id":"PfaL1f3Yv2dJS4bZdyjx","answers_community":["C (100%)"],"choices":{"D":"Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.","A":"Use Amazon S3 multi-part upload functionality to transfer the files over HTTPS.","B":"Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.","C":"Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3."},"unix_timestamp":1678463100,"answer_description":"","question_text":"A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company’s internet connection can support an upload speed of 100 Mbps.\n\nWhich solution meets these requirements MOST cost-effectively?","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/102166-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C","discussion":[{"timestamp":"1712812200.0","upvote_count":"13","poster":"shanwford","comment_id":"866967","content":"Selected Answer: C\nWith the existing data link the transfer takes ~ 600 days in the best case. Thus, (A) and (B) are not applicable. Solution (D) could meet the target with a transfer time of 6 days, but the lead time for the direct connect deployment can take weeks! Thus, (C) is the only valid solution."},{"comment_id":"994224","poster":"Guru4Cloud","content":"Selected Answer: C\nUse the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.","timestamp":"1725029280.0","upvote_count":"2"},{"timestamp":"1716525240.0","poster":"TariqKipkemei","comment_id":"905470","content":"Selected Answer: C\nC is the best option considering the time and bandwidth limitations","upvote_count":"2"},{"content":"Selected Answer: C\nWe need the admin in here to tell us how they plan on this being achieved over connection with such a slow connection lol.\nIt's C, folks.","timestamp":"1715275620.0","poster":"pbpally","comment_id":"893320","upvote_count":"3"},{"comment_id":"836154","content":"Selected Answer: C\nBest option is to use multiple AWS Snowball Edge Storage Optimized devices. Option \"C\" is the correct one.","timestamp":"1710168060.0","poster":"KAUS2","upvote_count":"2"},{"poster":"ktulu2602","timestamp":"1710152340.0","comments":[{"comment_id":"835820","content":"Or provisioning time in the D case","poster":"ktulu2602","timestamp":"1710152340.0","upvote_count":"2"}],"comment_id":"835818","upvote_count":"1","content":"Selected Answer: C\nAll others are limited by the bandwidth limit"},{"poster":"KZM","timestamp":"1710127620.0","content":"It is C. Snowball (from Snow Family).","comment_id":"835612","upvote_count":"2"},{"content":"Selected Answer: C\nC. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.\n\nThe best option is to use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices and use the devices to transfer the data to Amazon S3. Snowball Edge is a petabyte-scale data transfer device that can help transfer large amounts of data securely and quickly. Using Snowball Edge can be the most cost-effective solution for transferring large amounts of data over long distances and can help meet the requirement of transferring 600 TB of data within two weeks.","upvote_count":"4","comment_id":"835386","timestamp":"1710101280.0","poster":"cegama543"}],"topic":"1","answer_ET":"C","timestamp":"2023-03-10 16:45:00","answer_images":[],"question_id":352,"exam_id":31},{"id":"zvdriTEr3UBv0D0mJCsN","question_text":"A financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP flood attacks might take the application offline.\n\nA solutions architect must design a solution to protect the application from this type of attack.\n\nWhich solution meets these requirements with the LEAST operational overhead?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/102167-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"answer_description":"","timestamp":"2023-03-10 16:49:00","answer":"B","question_images":[],"unix_timestamp":1678463340,"answer_ET":"B","answers_community":["B (100%)"],"discussion":[{"comment_id":"994081","content":"Selected Answer: B\nRegional AWS WAF web ACL is a managed web application firewall that can be used to protect your API Gateway API from a variety of attacks, including HTTP flood attacks.\nRate-based rule is a type of rule that can be used to limit the number of requests that can be made from a single IP address within a specified period of time.\nAPI Gateway stage is a logical grouping of API resources that can be used to control access to your API.","timestamp":"1725020880.0","upvote_count":"10","poster":"Guru4Cloud"},{"poster":"elearningtakai","upvote_count":"6","timestamp":"1711855380.0","comment_id":"856567","content":"Selected Answer: B\nA rate-based rule in AWS WAF allows the security team to configure thresholds that trigger rate-based rules, which enable AWS WAF to track the rate of requests for a specified time period and then block them automatically when the threshold is exceeded. This provides the ability to prevent HTTP flood attacks with minimal operational overhead."},{"upvote_count":"2","timestamp":"1716525540.0","comment_id":"905471","content":"Selected Answer: B\nAnswer is B","poster":"TariqKipkemei"},{"content":"B os correct","comment_id":"863878","timestamp":"1712494140.0","upvote_count":"2","poster":"maxicalypse"},{"upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html","comment_id":"844622","poster":"kampatra","timestamp":"1710920340.0"},{"timestamp":"1710085740.0","poster":"[Removed]","content":"Selected Answer: B\nbbbbbbbb","upvote_count":"4","comment_id":"835181"}],"choices":{"D":"Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predefined rate.","B":"Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.","A":"Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.","C":"Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predefined rate is reached."},"isMC":true,"question_id":353,"topic":"1"},{"id":"IJ0VNMSQSKcZJxVPGS9E","question_text":"An application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet.\nWhich solution will provide private network connectivity to Amazon S3?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/84980-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","exam_id":31,"timestamp":"2022-10-10 11:27:00","answer":"A","answer_ET":"A","unix_timestamp":1665394020,"question_images":[],"answers_community":["A (99%)","1%"],"discussion":[{"comment_id":"860509","upvote_count":"70","timestamp":"1726903140.0","poster":"PhucVuu","content":"Selected Answer: A\nKeywords:\n- EC2 in VPC\n- EC2 instance needs to access the S3 bucket without connectivity to the internet\n\nA: Correct - Gateway VPC endpoint can connect to S3 bucket privately without additional cost\nB: Incorrect - You can set up interface VPC endpoint for CloudWatch Logs for private network from EC2 to CloudWatch. But from CloudWatch to S3 bucket: Log data can take up to 12 hours to become available for export and the requirement only need EC2 to S3\nC: Incorrect - Create an instance profile just grant access but not help EC2 connect to S3 privately\nD: Incorrect - API Gateway like the proxy which receive network from out site and it forward request to AWS Lambda, Amazon EC2, Elastic Load Balancing products such as Application Load Balancers or Classic Load Balancers, Amazon DynamoDB, Amazon Kinesis, or any publicly available HTTPS-based endpoint. But not S3","comments":[{"content":"Option C involves creating an instance profile on the EC2 instance to allow S3 access. While this option could potentially work, it would not provide private network connectivity to S3, as the EC2 instance would still need to access S3 over the internet.\n\nOption D involves creating an Amazon API Gateway API with a private link to access the S3 endpoint. This option would not provide private network connectivity to S3, as the API Gateway API is not a network interface that can be used to privately connect to S3.\n\nOverall, Option A is the correct solution for providing private network connectivity to Amazon S3 from an EC2 instance in a VPC.","timestamp":"1729487220.0","upvote_count":"2","comment_id":"1300761","poster":"rxwcl"},{"poster":"Austinlorenzmccoy","content":"Thank you so much","comment_id":"1090114","timestamp":"1701941340.0","upvote_count":"1"}]},{"upvote_count":"32","content":"Selected Answer: A\nVPC endpoint allows you to connect to AWS services using a private network instead of using the public Internet","timestamp":"1665394020.0","comment_id":"690928","poster":"D2w"},{"timestamp":"1742419680.0","content":"Selected Answer: A\nThe correct answer is A because a gateway endpoint allows us to securely and cost efficiently access either an S3 or Amazon DynamoDB database within our VPC","comment_id":"1400752","upvote_count":"1","poster":"melvis8"},{"poster":"Bl_12","content":"Selected Answer: D\nAPI gateway","comment_id":"1359294","upvote_count":"1","timestamp":"1740062400.0"},{"comment_id":"1347854","content":"Selected Answer: A\nIf accessing S3/DynamoDB privately: Use a Gateway VPC Endpoint.\nIf accessing any other AWS service privately: Use an Interface Endpoint (via PrivateLink).\nIf accessing third-party SaaS applications or services in another account/VPC: Use PrivateLink.","timestamp":"1738062900.0","poster":"Aayush_786","upvote_count":"2"},{"poster":"Chumi","content":"Selected Answer: A\noption A vpc endpoint can easily connect to an S3 bucket privately with little or zero cost accrued.","upvote_count":"1","timestamp":"1738058940.0","comment_id":"1347826"},{"poster":"Mrigraj12","upvote_count":"1","comment_id":"1345879","timestamp":"1737696420.0","content":"Selected Answer: A\nCreate gateway endpoint to access s3 bucket so as the ec2 will not require to go over the internet to access s3 bucket and the process will be fast and cheap also!"},{"comment_id":"1331608","poster":"MGKYAING","content":"Selected Answer: A\nA Gateway VPC Endpoint is designed to provide private network connectivity between resources in a VPC (such as EC2 instances) and services like Amazon S3 or DynamoDB without requiring an internet gateway, NAT gateway, or NAT instance.\nWhen a gateway VPC endpoint is set up for S3:\nTraffic between the EC2 instance and the S3 bucket stays within the AWS private network.\nThis ensures secure, cost-efficient, and private access to the S3 bucket without requiring public internet connectivity.","upvote_count":"2","timestamp":"1735139040.0"},{"poster":"cookieMr","comment_id":"926116","upvote_count":"4","content":"Selected Answer: A\nHere's why Option A is the correct choice:\n\nGateway VPC Endpoint: A gateway VPC endpoint allows you to privately connect your VPC to supported AWS services. By creating a gateway VPC endpoint for S3, you can establish a private connection between your VPC and the S3 service without requiring internet connectivity.\n\nPrivate network connectivity: The gateway VPC endpoint for S3 enables your EC2 instance within the VPC to access the S3 bucket over the private network, ensuring secure and direct communication between the EC2 instance and S3.\n\nNo internet connectivity required: Since the requirement is to access the S3 bucket without internet connectivity, the gateway VPC endpoint provides a private and direct connection to S3 without needing to route traffic through the internet.\n\nMinimal operational complexity: Setting up a gateway VPC endpoint is a straightforward process. It involves creating the endpoint and configuring the appropriate routing in the VPC. This solution minimizes operational complexity while providing the required private network connectivity.","timestamp":"1726903200.0"},{"timestamp":"1726903140.0","poster":"Ruffyit","upvote_count":"4","content":"Keywords:\n- EC2 in VPC\n- EC2 instance needs to access the S3 bucket without connectivity to the internet\nVPC endpoint allows you to connect to AWS services using a private network instead of using the public Internet.\n\nWith a gateway endpoint, you can access Amazon S3 from your VPC, without requiring an internet gateway or NAT device for your VPC, and with no additional cost. However, gateway endpoints do not allow access from on-premises networks, from peered VPCs in other AWS Regions, or through a transit gateway.","comment_id":"1053729"},{"content":"Selected Answer: A\nA. Correct answer. The easiest way to get private network connectivity in S3 is using VPC gateway endpoint. This service is free, and it is integrated natively with S3.\nB. Amazon CloudWatch Logs only collects and monitors logs but natively has not mechanisms to use private connection.\nC. Instance profiles are used to assign IAM roles to an EC2 instance, but it is not related to network connectivity.\nD. API Gateway like the proxy which receive network from out site and it forward request to AWS Lambda, Amazon EC2, Elastic Load Balancing products such as Application Load Balancers or Classic Load Balancers, Amazon DynamoDB, Amazon Kinesis, or any publicly available HTTPS-based endpoint. But not S3.","poster":"Andreshere","upvote_count":"6","comment_id":"1116814","timestamp":"1726903140.0"},{"comments":[{"content":"Option C involves creating an instance profile on the EC2 instance to allow S3 access. While this option could potentially work, it would not provide private network connectivity to S3, as the EC2 instance would still need to access S3 over the internet.\n\nOption D involves creating an Amazon API Gateway API with a private link to access the S3 endpoint. This option would not provide private network connectivity to S3, as the API Gateway API is not a network interface that can be used to privately connect to S3.\n\nOverall, Option A is the correct solution for providing private network connectivity to Amazon S3 from an EC2 instance in a VPC.","timestamp":"1671550500.0","poster":"Buruguduystunstugudunstuy","upvote_count":"1","comment_id":"751129"}],"timestamp":"1726903140.0","poster":"Buruguduystunstugudunstuy","upvote_count":"3","comment_id":"751127","content":"Selected Answer: A\n***CORRECT ANSWER***\nThe correct solution that will provide private network connectivity to Amazon S3 is Option A: Create a gateway VPC endpoint to the S3 bucket.\n\n***EXPLANATION***\nOption A involves creating a gateway VPC endpoint, which is a network interface in a VPC that allows you to privately connect to a service over the Amazon network. You can create a gateway VPC endpoint for Amazon S3, which will allow the EC2 instance in the VPC to access the S3 bucket without connectivity to the internet.\n\nOption B involves streaming the logs to Amazon CloudWatch Logs and then exporting the logs to the S3 bucket. This option would not provide private network connectivity to S3, as the logs would need to be exported over the internet."},{"content":"A. You should create VPC endpoint and link to S3 endpoint to transfer internally in AWS without internet","comment_id":"1264497","timestamp":"1723441260.0","upvote_count":"1","poster":"Chiaki35"},{"comment_id":"1263383","content":"Ans A. VPC = Virtual Private Cloud, so its already private... so just create another end point...","timestamp":"1723278060.0","poster":"PaulGa","upvote_count":"1"},{"comment_id":"1229424","poster":"Awsgrinder94","timestamp":"1718218380.0","content":"Selected Answer: A\nA for sure","upvote_count":"1"},{"content":"Selected Answer: A\nGateway VPC endpoint will provide private network connectivity to Amazon S3","timestamp":"1716985440.0","upvote_count":"1","comment_id":"1220925","poster":"Ishu_"},{"timestamp":"1713346620.0","comment_id":"1197130","poster":"Muavia","upvote_count":"1","content":"Option A is corect when you need a establish a conection between EC2 and S3 then gateway and VPc is the best choice"},{"timestamp":"1710387300.0","comment_id":"1173066","content":"Answer -A \nVPC endpoints are created to access any AWS services privately without going to internet.","poster":"48cd959","upvote_count":"2"},{"timestamp":"1710333540.0","comment_id":"1172545","poster":"TilTil","content":"Selected Answer: A\nVPC Endpoint is the TOP Notch choice, allows services to connect via private could. VPC literally means virtual private cloud. Best choice","upvote_count":"1"},{"comment_id":"1149547","timestamp":"1707855360.0","content":"Selected Answer: A\nEasiest way to avoid internet traffic is to use VPC endpoints to let services communicate with each other","poster":"ldruizsan","upvote_count":"2"},{"content":"Selected Answer: A\ngateway VPC to S3 ensures data stays within AWS","timestamp":"1705170300.0","poster":"awsgeek75","comment_id":"1121950","upvote_count":"1"},{"poster":"A_jaa","comment_id":"1121602","timestamp":"1705148400.0","content":"Selected Answer: A\nAnswer-A","upvote_count":"1"},{"poster":"Ruffyit","timestamp":"1700213460.0","content":"VPC endpoint allows you to connect to AWS services using a private network instead of using the public Internet","comment_id":"1073149","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: A\nCreate VPC endpoint is a private way to connect to AWS services without internet.","timestamp":"1697044140.0","poster":"namtp","comment_id":"1040943"},{"poster":"RNess","content":"Selected Answer: A\nVPC endpoint is the best way to connect in private","upvote_count":"1","comment_id":"998619","timestamp":"1693838640.0"},{"poster":"Bmarodi","comment_id":"985151","upvote_count":"1","content":"Selected Answer: A\nWith a gateway endpoint, you can access Amazon S3 from your VPC, without requiring an internet gateway or NAT device for your VPC, and with no additional cost. However, gateway endpoints do not allow access from on-premises networks, from peered VPCs in other AWS Regions, or through a transit gateway.","timestamp":"1692444900.0"},{"comment_id":"964327","timestamp":"1690432320.0","content":"Selected Answer: A\nA VPC endpoint enables customers to privately connect to supported AWS services and VPC endpoint services powered by AWS PrivateLink.","poster":"TariqKipkemei","upvote_count":"2"},{"comment_id":"954297","timestamp":"1689605580.0","poster":"Guru4Cloud","content":"Selected Answer: A\nThe answer is A. Create a gateway VPC endpoint to the S3 bucket.\n\nA gateway VPC endpoint is a private way to connect to AWS services without using the internet. This is the best solution for the given scenario because it will allow the EC2 instance to access the S3 bucket without any internet connectivity","upvote_count":"1"},{"poster":"james2033","upvote_count":"1","comment_id":"952024","timestamp":"1689399540.0","content":"Selected Answer: A\nKeyword (1) EC2 in a VPC. (2)EC2 instance need access S3 bucket WITHOUT internet. Therefore, A is correct answer: Create a gateway VPC endpoint to S3 bucket."},{"content":"Option A MET THE REQUIREMENT","comment_id":"949949","poster":"miki111","timestamp":"1689176220.0","upvote_count":"1"},{"content":"Selected Answer: A\nA is right answer.","poster":"Bmarodi","timestamp":"1685622840.0","comment_id":"912098","upvote_count":"1"},{"upvote_count":"1","comment_id":"897311","timestamp":"1684045560.0","poster":"cheese929","content":"Selected Answer: A\nA is correct"},{"timestamp":"1680415260.0","content":"Selected Answer: A\nOption B) not provide private network connectivity to S3.\nOption C) not provide private network connectivity to S3.\nOption D) API Gateway with a private link provide private network connectivity between a VPC and an HTTP(S) endpoint, not S3.","poster":"channn","upvote_count":"2","comment_id":"858559"},{"poster":"linux_admin","timestamp":"1680194880.0","content":"Selected Answer: A\nOption A proposes creating a VPC endpoint for Amazon S3. A VPC endpoint enables private connectivity between the VPC and S3 without using an internet gateway or NAT device. This would provide the EC2 instance with private network connectivity to the S3 bucket.","comment_id":"855975","upvote_count":"2"},{"timestamp":"1680192480.0","upvote_count":"1","comment_id":"855941","poster":"dee_pandey","content":"Could someone send me a pdf of this dump please? Thank you so much in advance!"},{"content":"Selected Answer: A\nA bạn ơi :)","timestamp":"1679554560.0","comment_id":"847893","poster":"tienhoboss","upvote_count":"1"},{"timestamp":"1679374080.0","upvote_count":"1","poster":"iamRohanKaushik","comment_id":"845532","content":"Selected Answer: A\nAnswer is A, but was confused with C, instance role will route through internet."},{"timestamp":"1677526620.0","poster":"Folayinka","content":"A VPC endpoint allows you to connect from the VPC to other AWS services outside of the VPC without the use of the internet.","comment_id":"824083","upvote_count":"1"},{"content":"Selected Answer: A\nVPC endpoint enables creation of a private connection between VPC to supported AWS services and VPC endpoint services powered by PrivateLink using its private IP address. Traffic between VPC and AWS service does not leave the Amazon network.","comment_id":"819185","upvote_count":"1","timestamp":"1677155700.0","poster":"bilel500"},{"comment_id":"807019","upvote_count":"1","timestamp":"1676258880.0","poster":"buiducvu","content":"Selected Answer: A\nA is correct, VPC endpoint is a connection between your VPC and an AWS"},{"content":"Selected Answer: A\nVPC endpoint allows you to connect to AWS services using a private network instead of using the public Internet","timestamp":"1676000880.0","comment_id":"803944","poster":"dvoaviarison","upvote_count":"1"},{"comment_id":"775082","upvote_count":"1","timestamp":"1673670300.0","poster":"vishwa10","content":"A is correct"},{"upvote_count":"3","poster":"SilentMilli","comment_id":"767812","content":"Selected Answer: A\nA gateway VPC endpoint is a connection between your VPC and an AWS service that enables private connectivity to the service. A gateway VPC endpoint for S3 allows the EC2 instance to access the S3 bucket without requiring internet connectivity.","timestamp":"1673016840.0"},{"poster":"KZM","upvote_count":"1","content":"You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints (using AWS PrivateLink).","comment_id":"757059","timestamp":"1672029420.0"},{"upvote_count":"1","poster":"BENICE","timestamp":"1671508200.0","comment_id":"750461","content":"A is correct answer"},{"timestamp":"1671431040.0","content":"Selected Answer: A\nA gateway endpoint targets specific IP routes in an Amazon VPC route table, in the form of a prefix-list, used for traffic destined to Amazon DynamoDB or Amazon S3. Gateway endpoints do not enable AWS PrivateLink.","comment_id":"749528","poster":"psr83","upvote_count":"1"},{"comment_id":"747137","poster":"Myxa","content":"Correct Answer: A","upvote_count":"1","timestamp":"1671192060.0"},{"upvote_count":"1","comment_id":"746309","timestamp":"1671121020.0","poster":"NikaCZ","content":"A VPC endpoint allows to connect AWS services and and you don't need to use public network."},{"poster":"sanjay3x1","content":"A is correct","timestamp":"1670581020.0","upvote_count":"1","comment_id":"739986"},{"content":"Selected Answer: A\nA is correct","comment_id":"734969","poster":"javitech83","upvote_count":"1","timestamp":"1670146320.0"},{"comment_id":"724969","upvote_count":"1","timestamp":"1669188900.0","content":"Selected Answer: A\nTo provide connectivity the answer is \"A\". To authorize the connection we can use the instance profile.","poster":"Drekorig"},{"timestamp":"1669125660.0","upvote_count":"1","comment_id":"724362","poster":"cheese929","content":"Selected Answer: A\nA is right."},{"upvote_count":"1","poster":"Wpcorgan","content":"A is correct","timestamp":"1669033200.0","comment_id":"723460"},{"upvote_count":"1","timestamp":"1668892620.0","poster":"grzeev","content":"Selected Answer: A\nA","comment_id":"722257"},{"comment_id":"711938","content":"It's A, Private endpoints within your VPC that allow AWS services to privately connect to resources within your VPC without traversing the public internet (cheaper)","upvote_count":"2","timestamp":"1667671560.0","poster":"pm2229"},{"content":"Selected Answer: A\nYou can access Amazon S3 from your VPC using gateway VPC endpoints. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to Amazon S3.\nYou can create a policy that restricts access to specific IP address ranges by using the aws:VpcSourceIp condition key.","comment_id":"711250","upvote_count":"3","poster":"Naneyerocky","timestamp":"1667577000.0"},{"content":"Selected Answer: A\nA for sure","timestamp":"1667256900.0","comment_id":"708763","poster":"Adrianjavier","upvote_count":"1"},{"upvote_count":"1","timestamp":"1666994160.0","comment_id":"706772","poster":"17Master","content":"Selected Answer: A\nans is A"},{"upvote_count":"1","comment_id":"698495","timestamp":"1666123680.0","poster":"bilel500","content":"Selected Answer: A\nAnswer is A."},{"poster":"queen101","timestamp":"1665946200.0","comment_id":"696441","upvote_count":"1","content":"AAAAAAAAAAAA"},{"poster":"BoboChow","timestamp":"1665541740.0","comment_id":"692570","upvote_count":"1","content":"Selected Answer: A\nobviously it's A"},{"comment_id":"691639","upvote_count":"2","poster":"Mikedodo","content":"Selected Answer: A\nReduce Cost and Increase Security with Amazon VPC Endpoints","timestamp":"1665451560.0"}],"choices":{"C":"Create an instance profile on Amazon EC2 to allow S3 access.","A":"Create a gateway VPC endpoint to the S3 bucket.","D":"Create an Amazon API Gateway API with a private link to access the S3 endpoint.","B":"Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket."},"isMC":true,"topic":"1","question_id":354},{"id":"ISA5cSM3pt14WQTgYGWI","answer_images":[],"unix_timestamp":1665530640,"question_images":[],"timestamp":"2022-10-12 01:24:00","topic":"1","question_id":355,"question_text":"A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.\nThe company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.\nWhat is the MOST operationally efficient solution that meets these requirements?","answers_community":["A (87%)","13%"],"url":"https://www.examtopics.com/discussions/amazon/view/85204-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"A","choices":{"C":"Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.","B":"Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.","A":"Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.","D":"Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue."},"isMC":true,"exam_id":31,"answer_description":"","answer_ET":"A","discussion":[{"comment_id":"694050","content":"Selected Answer: A\nDefinitely A, it's the most operationally efficient compared to D, which requires a lot of code and infrastructure to maintain. A is mostly managed (firehose is fully managed and S3 lifecycles are also managed)","comments":[{"poster":"Kelvin_ke","content":"what about the 30 days minimum requirement to transition to S3 glacier?","comments":[{"content":"This constraint is related to moving from Standard to IA/IA-One Zone only. Nothing to do with Glacier","upvote_count":"5","timestamp":"1696907280.0","poster":"caffee","comment_id":"865951"},{"comments":[{"upvote_count":"1","comment_id":"1400916","timestamp":"1742451840.0","content":"the 30 day requirement seem only apply to IA but the article did not mention any limitation on glacier","poster":"jerryl"},{"comments":[{"poster":"Suvam90","content":"No , It's not correct , We can change the storage class in day 0 also using lifecycle policy , I implemented in my project, 30 days is just an example.","upvote_count":"5","timestamp":"1708228800.0","comment_id":"984126"}],"comment_id":"907275","content":"the current article doesn't enable the current option, minimum days are 30","timestamp":"1701000600.0","upvote_count":"1","poster":"ErnShm"}],"comment_id":"746927","content":"You can directly migrate from S3 standard to glacier without waiting\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html","timestamp":"1686894720.0","upvote_count":"5","poster":"studis"},{"timestamp":"1700122320.0","content":"GLACIER IS 7 DAYS REQUIREMENT NOT 30","comment_id":"898913","poster":"Abrar2022","upvote_count":"6"}],"upvote_count":"8","timestamp":"1686067320.0","comment_id":"737097"}],"timestamp":"1681400160.0","upvote_count":"41","poster":"Sinaneos"},{"comment_id":"697021","poster":"123jhl0","upvote_count":"21","timestamp":"1681714800.0","content":"Selected Answer: A\nOnly A makes sense operationally.\nIf you think D, just consider what is needed to move the message from SQS to S3... you are polling daily 14 TB to take out 1 TB... that's no operationally efficient at all."},{"upvote_count":"1","poster":"satyaammm","timestamp":"1735639440.0","content":"Selected Answer: A\nUsing Kinesis Data Firehouse with S3 Life Configuration policies is the most suitable option as we are looking for less operational overhead.","comment_id":"1334731"},{"timestamp":"1721940360.0","content":"I understand A is the most operationally efficient option of all but I can't wrap my head around the fact that objects must have a minimum of 30 days before they can transition or expire from Amazon S3. This means that for the first 30 days after an item is created, you cannot transition or remove it. So, how option A can be the best fit?","comment_id":"1132124","poster":"Parul25","upvote_count":"1","comments":[{"comments":[{"poster":"krx59456zslszcom","upvote_count":"1","content":"But the docs are also mentioning a table with minimum storage durations beyond the S3 Standard-IA \n\"[...]for a list of minimum storage duration for all storage class, see Comparing the Amazon S3 storage classes.\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-compare","timestamp":"1735829760.0","comment_id":"1335614"}],"comment_id":"1149074","poster":"cheroh_tots","upvote_count":"3","timestamp":"1723536240.0","content":"The same 30-day minimum applies when you specify a transition from S3 Standard-IA storage to S3 One Zone-IA.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"}]},{"content":"Selected Answer: A\nBCD: Additional infra which company doesn't want\nA: Firehose for ingestion and delivery to S3. Lifecycle for managing archive. Fully managed and operationally easy solution","timestamp":"1720905300.0","upvote_count":"2","comment_id":"1122121","poster":"awsgeek75"},{"content":"Selected Answer: A\nAnswer-A","comment_id":"1121657","timestamp":"1720867500.0","poster":"A_jaa","upvote_count":"1"},{"comment_id":"1091398","poster":"jjcode","upvote_count":"4","content":"so many words...","timestamp":"1717891500.0"},{"poster":"OmegaLambda7XL9","timestamp":"1715904780.0","content":"That was an easy A. Kinesis Firehose can load data directly to S3 which makes it the most operationally efficient","upvote_count":"2","comment_id":"1072970"},{"comment_id":"1054742","poster":"Ruffyit","upvote_count":"2","timestamp":"1714147560.0","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"},{"timestamp":"1712618340.0","content":"Selected Answer: A\nKey: MOST operationally efficient solution","upvote_count":"2","comment_id":"1028350","poster":"tom_cruise"},{"timestamp":"1711553040.0","content":"Selected Answer: A\n\"A\" is simply correct because kinesis firehouse is made for this, SQS standard is not going to support 500 million alerts 2KB each (1 TB) this service is made for requests that are lighter.","comment_id":"1018829","poster":"David_Ang","upvote_count":"2"},{"poster":"Ak9kumar","comment_id":"1016698","timestamp":"1711368660.0","upvote_count":"1","content":"I picked A. Appeared to be right answer."},{"poster":"chandu7024","comment_id":"1009714","upvote_count":"1","timestamp":"1710676140.0","content":"Should be A"},{"content":"Selected Answer: A\nThe MOST operationally efficient option is A.","timestamp":"1707026280.0","poster":"TariqKipkemei","upvote_count":"1","comment_id":"971661"},{"upvote_count":"1","comment_id":"956380","content":"Selected Answer: A\nKeyword \"Amazon S3 Glacier\" (A).","poster":"james2033","timestamp":"1705656540.0"},{"content":"Option A is the right answer for this.","timestamp":"1705445340.0","comment_id":"953639","upvote_count":"1","poster":"miki111"},{"timestamp":"1702912440.0","upvote_count":"11","content":"Selected Answer: A\nB suggests launching EC2 instances to ingest and store the alerts, which introduces additional infrastructure management overhead and may not be as cost-effective and scalable as using managed services like Kinesis Data Firehose and S3.\n\nC involves delivering the alerts to an Amazon OpenSearch Service cluster and manually managing snapshots and data deletion. This introduces additional complexity and manual overhead compared to the simpler solution of using Kinesis Data Firehose and S3.\n\nD suggests using SQS to ingest the alerts, but it does not provide the same level of data persistence and durability as storing the alerts directly in S3. Additionally, it requires manual processing and copying of messages to S3, which adds operational complexity.\n\nTherefore, A provides the most operationally efficient solution that meets the company's requirements by leveraging Kinesis Data Firehose to ingest the alerts, storing them in an S3 bucket, and using an S3 Lifecycle configuration to transition data to S3 Glacier for long-term archival, all without the need for managing additional infrastructure.","comment_id":"926711","poster":"cookieMr"},{"poster":"Abrar2022","content":"Focus on keywords: Amazon Kinesis Data Firehose delivery stream to ingest the alerts. S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.","comment_id":"898914","upvote_count":"4","timestamp":"1700122380.0"},{"comment_id":"861331","upvote_count":"2","content":"Selected Answer: D\nD is the correct answer. Check the link below\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html","comments":[{"poster":"pentium75","comment_id":"1105041","content":"I can't see anything in that article that would speak for D or rule out A. It mentions a 30-day delay before moving to Glacier flexible retrieval, but that is not mentioned here. Also there is a 30-day minimum storage limit, but that is about Glacier (you have to pay for Glacier at least for 30 days), that has nothing to do with when you can move it there.","timestamp":"1719287520.0","upvote_count":"2"}],"poster":"XenonDemon","timestamp":"1696440900.0"},{"comment_id":"857121","content":"Selected Answer: A\nAmazon Kinesis Data Firehose is a fully managed service that can capture, transform, and deliver streaming data into storage systems or analytics tools, making it an ideal solution for ingesting and storing status alerts. In this solution, the Kinesis Data Firehose delivery stream ingests the alerts and delivers them to an S3 bucket, which is a cost-effective storage solution. An S3 Lifecycle configuration is set up to transition the data to Amazon S3 Glacier after 14 days to minimize storage costs.","timestamp":"1696080720.0","upvote_count":"3","poster":"linux_admin"},{"comment_id":"832014","poster":"bilel500","upvote_count":"2","content":"Selected Answer: A\nThe correct answer is A: Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.","timestamp":"1694090400.0"},{"timestamp":"1691659260.0","content":"This question was tricky but after some reading my choice went from D to A. Which is Operationally efficient.","upvote_count":"2","comment_id":"804237","poster":"Ello2023"},{"upvote_count":"3","poster":"jannymacna","content":"A. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.\n\nThis solution meets the company's requirements to minimize costs and not manage additional infrastructure while providing high availability. Kinesis Data Firehose is a fully managed service that can automatically ingest streaming data and load it into Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service. By configuring the Firehose to deliver the alerts to an S3 bucket, the company can take advantage of S3's high durability and availability. An S3 Lifecycle configuration can be set up to automatically transition data that is older than 14 days to Amazon S3 Glacier, an extremely low-cost storage class for infrequently accessed data.","timestamp":"1689220560.0","comment_id":"774148"},{"timestamp":"1688677140.0","comment_id":"768132","upvote_count":"3","poster":"SilentMilli","content":"Selected Answer: A\nCreating an Amazon Kinesis Data Firehose delivery stream to ingest the alerts and configuring it to deliver the alerts to an Amazon S3 bucket is the most operationally efficient solution that meets the requirements. Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as S3, Redshift, Elasticsearch Service, and Splunk. It can automatically scale to handle the volume and throughput of the alerts, and it can also batch, compress, and encrypt the data as it is delivered to S3. By configuring a Lifecycle policy on the S3 bucket, the company can automatically transition data to Amazon S3 Glacier after 14 days, allowing the company to store the data for longer periods of time at a lower cost. This solution requires minimal management and provides high availability, making it the most operationally efficient choice."},{"upvote_count":"2","comments":[{"upvote_count":"2","poster":"secdaddy","comment_id":"761707","content":"\"A record can be as large as 1,000 KB.\" and the diagrams shown in this URL support A as the answer.\nhttps://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html","timestamp":"1688098200.0"},{"content":"Option A:\nThinking about this a more as Low operational overhead primary requirement option A will be better option but it will have higher Latency compared to using Kinesis Data Stream.","poster":"career360guru","comment_id":"757992","timestamp":"1687820340.0","upvote_count":"1"}],"timestamp":"1687819440.0","poster":"career360guru","content":"Selected Answer: D\nA is not a right answer is Kinesis Firehose is not the right service to Ingest small 2KB events. Minimum Message Size for Kinesis Firehose is 5MB. Kinesis Data Stream is the right service for this but as that is not given as option here, SQS with 14 Day retention is right answer.","comment_id":"757983"},{"comment_id":"757475","poster":"Zerotn3","timestamp":"1687779360.0","content":"any data older than 14 days => can not D ! => A correct.","upvote_count":"1"},{"timestamp":"1687278600.0","poster":"pazabal","content":"Selected Answer: A\nA, MOST operationally efficient solution = Kinesis Data Firehose, since it's a fully managed solution\nB, more costly and more opp overhead compared to kinesis data firehose\nC, not most cost-effective solution since it's data that's not actively being queried or analyzed after 14 days\nD, designed for messaging rather than storage","comment_id":"751335","upvote_count":"1"},{"poster":"Buruguduystunstugudunstuy","comments":[{"timestamp":"1687225740.0","comment_id":"750460","poster":"Buruguduystunstugudunstuy","content":"To meet the requirements of the company, you can create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts generated by the edge devices. You can then configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. This will provide a highly available solution that does not require the company to manage additional infrastructure.\n\nTo keep 14 days of data available for immediate analysis and archive any data older than 14 days, you can set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days. This will allow the company to store the data for long-term retention at a lower cost than storing it in S3.","upvote_count":"1"}],"timestamp":"1687225740.0","content":"Selected Answer: A\nThe correct answer is A: Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.\n\nAmazon Kinesis Data Firehose is a fully managed service that makes it easy to load streaming data into data stores and analytics tools. It can continuously capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling real-time analytics with existing business intelligence tools and dashboards you're already using.","comment_id":"750459","upvote_count":"1"},{"content":"Selected Answer: A\nA of course","poster":"Morinator","upvote_count":"1","timestamp":"1687181400.0","comment_id":"749985"},{"poster":"career360guru","upvote_count":"1","content":"Selected Answer: A\nOption A","timestamp":"1687146300.0","comment_id":"749493"},{"content":"Selected Answer: D\nD as B is client-side encryption","timestamp":"1686546420.0","upvote_count":"2","comment_id":"742494","poster":"unbornfroyo"},{"comment_id":"732159","timestamp":"1685576340.0","content":"If we can't move data from standard s3 to glacier before 30 days, as described here:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\nThen A is wrong.","upvote_count":"2","poster":"DasCert"},{"poster":"Wpcorgan","comment_id":"723526","upvote_count":"1","timestamp":"1684667460.0","content":"A is correct"},{"comment_id":"695818","poster":"Incognito013","upvote_count":"1","timestamp":"1681607040.0","content":"A\n\nStroring the data in S3 and assign a policy to transfer the data to Glacier after 14 days"},{"comments":[{"comment_id":"728875","timestamp":"1685254740.0","content":"Looks like a lot of contributors are forgetting that one cannot transition S3 objects that are less than 30 days old.\nD is most appropriate.","upvote_count":"4","comments":[{"upvote_count":"1","comment_id":"744432","timestamp":"1686679320.0","poster":"Qjb8m9h","content":"Yes you can, \nMinimum Days for Transition from S3 Standard or S3 Standard-IA to S3 Standard-IA or S3 One Zone-IA No where did they mention S3 glacier flexible or deep archive.\nUsing S3 Lifecycle configuration, you can transition objects to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes for archiving. When you choose the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage class, your objects remain in Amazon S3. You cannot access them directly through the separate Amazon S3 Glacier service\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"},{"upvote_count":"2","content":"You can create S3 => Buckets => Management => “Create Lifecycle Rule” to move objects to diff S3 class with any days you want to !!! & I tested working.\nSo A is 100% Correct !!!","comment_id":"737291","timestamp":"1686088140.0","poster":"Jiang_aws1"}],"poster":"Harry_New"},{"timestamp":"1681831680.0","content":"Nope, can't get immediate access to any data you want with SQS. Additionally, if you do somehow you have to stop calling the delete message API call for 14 hours, and then...","poster":"yd_h","comment_id":"698321","upvote_count":"1"},{"comment_id":"697450","timestamp":"1681739640.0","upvote_count":"2","poster":"ArielSchivo","content":"This is great, but Kinesis and S3 are managed services, so it should be option A as that one is the most operational."},{"timestamp":"1687331760.0","poster":"QueTeddyJR","content":"What of high availability which translate to almost real time availability","upvote_count":"1","comment_id":"752057"},{"upvote_count":"1","timestamp":"1686411120.0","comment_id":"741141","poster":"wh1t4k3r","content":"D will significantly increase operational overhead."}],"content":"Selected Answer: D\nIn most of the questions, first check the answers that are feasible and then check for the Well-Architected pillar emphasis in the question and hints pointing to it in solving Qs of SAA\nD: SQS vs Kinesis \nBoth do support retention period 14 days , max record size [256 KB and 1MB ] and \n Total Data produced is 1TB/day \n\nIn Question there is \"store the alerts for future analysis\" \"highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure\"\"MOST operationally efficient solution \" \nNo requirement for real time and ordered processing. Also need for LEAST OPERATIONAL head. In Case of Kinesis one has to be watchful of shards capacity so no scope for Autoscaling like SQS and Cost Basis. No need for multi-consumers only one place to store S3. SQS- fully serverless\nSo I think its SQS . Incase there are even multi-consumers still consider SQS-SNS model.","comment_id":"695494","upvote_count":"8","poster":"KVK16","timestamp":"1681571700.0"},{"content":"Selected Answer: A\nThis should be A","poster":"ninjawrz","upvote_count":"2","comment_id":"695139","timestamp":"1681526100.0"},{"poster":"tubtab","comment_id":"693863","content":"Selected Answer: D\nddddddddd","upvote_count":"1","timestamp":"1681386960.0"},{"comments":[{"comment_id":"693503","timestamp":"1681349580.0","comments":[{"poster":"Udoyen","comment_id":"731394","content":"Is \"deliver\" not storing?","upvote_count":"1","timestamp":"1685438760.0"}],"upvote_count":"1","content":"option a does not have anything to do with storing data","poster":"mattlai"},{"upvote_count":"2","comment_id":"740944","poster":"Wajif","content":"Ingest and deliver = Firehose\nStore = S3.\nStoring is S3's job.","timestamp":"1686390780.0"}],"comment_id":"693274","poster":"logicalbin","timestamp":"1681319640.0","upvote_count":"1","content":"A cannot be the right answer.\nReason - Fireshose cannot be used to store data. Only Kinesis Data Streams has that ability as it focuses on ingesting and storing data streams while Kinesis Data Firehose focuses on delivering data streams to select destinations.\nSQS on the other hand can store data upto 14 days.\nSo option D is correct."},{"poster":"Lilibell","upvote_count":"5","timestamp":"1681255440.0","comment_id":"692486","content":"The answer is A"}]}],"exam":{"isBeta":false,"lastUpdated":"11 Apr 2025","id":31,"isImplemented":true,"isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","numberOfQuestions":1019,"provider":"Amazon"},"currentPage":71},"__N_SSP":true}