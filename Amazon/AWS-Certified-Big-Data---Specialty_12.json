{"pageProps":{"questions":[{"id":"t3Fqwj8SCz6EkkkFKGyL","topic":"1","question_text":"An administrator needs to design a strategy for the schema in a Redshift cluster. The administrator needs to determine the optimal distribution style for the tables in the Redshift schema.\nIn which two circumstances would choosing EVEN distribution be most appropriate? (Choose two.)","discussion":[{"content":"I think it should be A & D. B is describing Key style","timestamp":"1632336240.0","comment_id":"6059","poster":"mattyb123","upvote_count":"17"},{"upvote_count":"1","timestamp":"1636298820.0","comment_id":"95287","poster":"Debi_mishra","content":"A &D . Not sure how B is correct"},{"upvote_count":"2","poster":"san2020","timestamp":"1635664800.0","content":"my selection A,D","comment_id":"52146"},{"upvote_count":"1","content":"A,D is correct \nreference: https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html","comment_id":"41796","timestamp":"1635571920.0","poster":"kalpanareddy"},{"content":"A & D--EVEN distribution is appropriate when a table does not participate in joins or when there is not a clear choice between KEY distribution and ALL distribution.","poster":"kalpanareddy","upvote_count":"1","comment_id":"41795","timestamp":"1635180600.0"},{"poster":"jay1ram2","content":"A&D. This site contains lot of bad default answers. Thanks for these discussions. They provide much-needed corrections.","timestamp":"1634872980.0","comment_id":"37899","upvote_count":"1"},{"upvote_count":"1","content":"A,D for sure.","poster":"michelleY","comment_id":"35017","timestamp":"1634247900.0"},{"comment_id":"23959","content":"AD seems right","timestamp":"1634013240.0","upvote_count":"1","poster":"WWODIN"},{"poster":"viduvivek","content":"A & D are correct.\n\nAmazon Redshift EVEN distribution : \nThe leader node distributes the rows across the slices in a round-robin fashion, regardless of the values in any particular column. \nEVEN distribution is appropriate when a table does not participate in joins or when there is not a clear choice between KEY distribution and ALL distribution.\n\nRefer : https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html","upvote_count":"2","timestamp":"1633816560.0","comment_id":"19482"},{"comment_id":"14955","upvote_count":"1","content":"A & D is correct as the rest 2 doesnt apply to Even distribution","timestamp":"1633765380.0","poster":"M2"},{"poster":"bigdatalearner","comment_id":"13687","upvote_count":"1","content":"A and D are the right answers for this question . because when you are not sure about data you can choose even and when no more joins are required","timestamp":"1633297380.0"},{"upvote_count":"1","comment_id":"11332","content":"A & D plase!","comments":[{"poster":"exams","timestamp":"1633154880.0","comment_id":"11396","upvote_count":"1","content":"I also think AD"}],"poster":"gonda","timestamp":"1633149180.0"},{"comment_id":"6633","content":"a,d is the correct answer","timestamp":"1633114440.0","upvote_count":"1","poster":"Jialu"},{"timestamp":"1632913680.0","upvote_count":"3","comment_id":"6305","poster":"jlpl","content":"a d are right"}],"choices":{"C":"When data transfer between nodes must be eliminated.","A":"When the tables are highly denormalized and do NOT participate in frequent joins.","D":"When a new table has been loaded and it is unclear how it will be joined to dimension.","B":"When data must be grouped based on a specific key on a defined slice."},"question_images":[],"timestamp":"2019-08-06 09:53:00","unix_timestamp":1565077980,"question_id":56,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/3261-exam-aws-certified-big-data-specialty-topic-1-question-6/","answers_community":[],"answer_ET":"BD","answer":"BD","answer_description":"","exam_id":17,"answer_images":[]},{"id":"Bjm1Sawm44C2LfnKNYfQ","question_text":"A company that provides economics data dashboards needs to be able to develop software to display rich, interactive, data-driven graphics that run in web browsers and leverages the full stack of web standards\n(HTML, SVG, and CSS).\nWhich technology provides the most appropriate support for this requirements?","answer":"A","choices":{"D":"Hue","B":"IPython/Jupyter","A":"D3.js","C":"R Studio"},"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/5360-exam-aws-certified-big-data-specialty-topic-1-question-60/","question_id":57,"answer_images":[],"timestamp":"2019-09-18 08:24:00","answers_community":[],"discussion":[{"upvote_count":"5","content":"A is the Answer.\n\nRefer : https://d3js.org/.\n\nD3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3â€™s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.","comment_id":"21983","timestamp":"1633706280.0","poster":"viduvivek"},{"upvote_count":"5","poster":"san2020","content":"my selection A","timestamp":"1634067900.0","comment_id":"52379"},{"content":"Should be D","upvote_count":"1","comment_id":"18459","timestamp":"1632849180.0","poster":"WWODIN","comments":[{"upvote_count":"2","timestamp":"1635427260.0","content":"From Hue's website: Hue is an open source SQL Assistant for Databases & Data Warehouses. \n\nHue's visualization power is at best limited if at all existing. A is more appropriate here.","poster":"Corram","comment_id":"101113"}]},{"comment_id":"11566","upvote_count":"4","timestamp":"1632456660.0","poster":"exams","content":"A it is"}],"answer_description":"Reference: https://sa.udacity.com/course/data-visualization-and-d3js--ud507","exam_id":17,"unix_timestamp":1568787840,"isMC":true,"answer_ET":"A"},{"id":"FWUtaYIjIiHIYx1iujrG","topic":"1","timestamp":"2019-07-15 10:08:00","unix_timestamp":1563178080,"question_text":"A company hosts a portfolio of e-commerce websites across the Oregon, N. Virginia, Ireland, and Sydney\nAWS regions. Each site keeps log files that capture user behavior. The company has built an application that generates batches of product recommendations with collaborative filtering in Oregon. Oregon was selected because the flagship site is hosted there and provides the largest collection of data to train machine learning models against. The other regions do NOT have enough historic data to train accurate machine learning models.\nWhich set of data processing steps improves recommendations for each region?","answers_community":[],"answer_images":[],"isMC":true,"answer":"D","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/2499-exam-aws-certified-big-data-specialty-topic-1-question-61/","answer_ET":"D","discussion":[{"comment_id":"34165","poster":"I_heart_shuffle_girls","upvote_count":"11","timestamp":"1633814040.0","content":"I feel like we went down a rabbit hole here. We are looking for what improves recommendations for each region. The company seems to want to build regional models so B would not improve Oregon's recommendations, C seems rather off base to me, and D agains goes for consolidating logs. I feel the answer is A as you can transfer logs from Oregon to other regions which will then give them enough data to begin training their models for their regions."},{"upvote_count":"1","content":"B. Because other regions don't have enough data to train the their own model, then funnel them all to a single region to train one model to apply to all to improve their recommendation.\nCloudWatch cross region is only available from Nov 2019, but this question is available 9 months ago, which is Sep, 2019. Therefore, D is wrong. https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/","poster":"guruguru","comment_id":"117931","timestamp":"1636107540.0"},{"poster":"freedomeox","timestamp":"1635934500.0","comment_id":"106616","content":"I will go for C. imo the key of improving the RM engine is to give other regions the access to Oregon logs.","upvote_count":"1"},{"poster":"MultiCloudGuru","timestamp":"1635532680.0","upvote_count":"1","comment_id":"95023","content":"Answer is D \nRefer : https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/"},{"upvote_count":"1","comment_id":"88916","content":"the answer is D \nhttps://aws.amazon.com/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/","poster":"chandrakatasani","timestamp":"1635101280.0"},{"upvote_count":"2","timestamp":"1634918160.0","content":"I think A is the right answer. None of the option clearly talk about replicating the user behavior log captured in Oregon to all the other region to be able to retain the model there and thereby improve the prediction in those regions except for A. Although A might look a bit more involved and less efficient it is the only way to successfully improve the recommendation in other regions.","poster":"Bulti","comment_id":"78154"},{"comment_id":"75827","timestamp":"1634134260.0","content":"Two keywords in this question are \"data manipulation\" and \"interactive\". Only Option B- Pig with Tachyon satisfies both requirements. Pig with Tachyon will be able to handle very high throughput working with large datasets and also be able to manipulate the data. Also You can execute Pig commands interactively or in batch mode. To use Pig interactively, create an SSH connection to the master node and submit commands using the Grunt shell. Refer to this link-> https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-pig.html","poster":"Bulti","upvote_count":"2","comments":[{"upvote_count":"3","timestamp":"1634763480.0","comment_id":"77483","poster":"Bulti","content":"This is a comment against next question."},{"timestamp":"1636263660.0","upvote_count":"1","content":"This is not what the question is about.\nWhich set of data processing steps improves recommendations for each region","comment_id":"337518","poster":"DerekKey"}]},{"timestamp":"1634059200.0","upvote_count":"1","content":"Option B- Doesn't make sense as it doesn't talk about how to improve the recommendation in each region.\nOption D: CloudWatch can consolidate logs into a log group within the same region and not replicate these logs ( from Oregon for e.g.) into other regions of interest.\nSo its between Option A and Option C. I would go with Option C over Option A because it involves using AWS services to replicate the logs from one region to another using a relay mechanism as opposed to having to write code in your ecommerce application to replicate the logs across all regions. Once the logs are replicated using the relay mechanism from one region to the next , the application can use these logs in its respective region to improve recommendation.","poster":"Bulti","comment_id":"75769"},{"content":"my selection D","timestamp":"1634027880.0","comment_id":"52380","upvote_count":"4","poster":"san2020"},{"timestamp":"1634004000.0","comment_id":"38259","content":"Oregon was selected because the flagship site is hosted there and provides the largest collection of data to train machine learning models against.\nit means ML model is already and the application is ready.\nThe other regions do NOT have enough historic data to train accurate machine learning models.\nSo the other sides are going to use the application based on the Oregon ML model and for that they need...\nD. \nUse the CloudWatch Logs agent to consolidate logs into a single CloudWatch Logs group.","poster":"ME2000","upvote_count":"2"},{"timestamp":"1633925220.0","content":"The application built can consume log files..and generates recommendations only. It does not mention that the application can write log files...for that you need Kinesis.","upvote_count":"1","poster":"PK1234","comment_id":"36103"},{"timestamp":"1633896900.0","poster":"PK1234","comment_id":"36099","upvote_count":"1","content":"The recommendations of a cold place like oregon (in nov) will be different than sydney (summer in nov). So tehre cannot eb any consolidation. As per elimination, only C seems correct...."},{"upvote_count":"2","poster":"s3an","content":"the question is \"recommendation for EACH region\", nothing says aggregate across regions. D seems like a good answer","timestamp":"1633619760.0","comment_id":"24621"},{"poster":"d00ku","content":"From AWS: \"You can aggregate the metrics for AWS resources across multiple resources. Amazon CloudWatch can't aggregate data across Regions. Metrics are completely separate between Regions.\"\nsooo... B?","upvote_count":"2","comment_id":"20478","timestamp":"1633367220.0"},{"timestamp":"1633091760.0","poster":"BigEv","upvote_count":"1","comments":[{"comments":[{"upvote_count":"1","content":"actually Cloudwatch can support cross regions monitoring as of Nov 8,2019\nhttps://aws.amazon.com/tw/about-aws/whats-new/2019/11/amazon-cloudwatch-launches-cross-account-cross-region-dashboards/","timestamp":"1634040900.0","comment_id":"70349","poster":"Mountie"}],"upvote_count":"3","comment_id":"20406","timestamp":"1633221300.0","poster":"BigEv","content":"Actually, I think @Hitu is correct. Cloudwatch agent could not aggregate cross-region log into one group. After reading this article, I am voting for B now.\nAny comments gents?\n\nLogs are generated regionally by AWS services so the best practice is to funnel all regional logs into one region in order to analyze the data across regions. There are three options to centralize your AWS logs.\nUse CloudWatch for your centralized log collection and then push them to a log analysis solution via Lambda or Kinesis.\nSend all logs directly to S3 and further process them with Lambda functions.\nConfigure agents like Beats on EC2 instances and FunctionBeat on Lambdas to push logs to a logging solution.\nhttps://coralogix.com/log-analytics-blog/aws-centralized-logging-guide/"}],"content":"Check this out.\nSeems like D is the closet answer\n\nhttps://aws.amazon.com/solutions/centralized-logging/","comment_id":"20389"},{"timestamp":"1632931380.0","upvote_count":"1","comment_id":"11602","poster":"pkfe","content":"Centralized Log Management with AWS CloudWatch.\nLog groups are used to classify log streams together\nhttps://cloudacademy.com/blog/centralized-log-management-with-aws-cloudwatch-part-1-of-3/\nso D"},{"upvote_count":"3","comment_id":"11568","poster":"exams","timestamp":"1632734340.0","content":"I support D"},{"poster":"Hitu","comment_id":"10677","timestamp":"1632688200.0","content":"any thoughts on B? Getting historical data would be easier with S3 plus the replication cadences to collecting all the logs at a centralized S3 location. Moreover, can Cloudwatch agent aggregate logs across regions?","upvote_count":"4"}],"question_id":58,"exam_id":17,"choices":{"B":"Use Amazon S3 bucket replication to consolidate log entries and build a single model in Oregon.","A":"Use the e-commerce application in Oregon to write replica log files in each other region.","D":"Use the CloudWatch Logs agent to consolidate logs into a single CloudWatch Logs group.","C":"Use Kinesis as a buffer for web logs and replicate logs to the Kinesis stream of a neighboring region."},"question_images":[]},{"id":"Qzz0AkfZuFP0KpQlYqNd","question_id":59,"isMC":true,"answer_description":"","topic":"1","timestamp":"2019-08-12 09:20:00","answer":"C","answer_images":[],"question_images":[],"unix_timestamp":1565594400,"choices":{"B":"Apache Pig with Tachyon","A":"Oozie","C":"Apache Hive","D":"Presto"},"exam_id":17,"discussion":[{"content":"Is there any question where we don't choose Presto? It is as if AWS is trying to beat it into our heads lol","poster":"AdamSmith","timestamp":"1634898060.0","upvote_count":"5","comment_id":"59699"},{"comment_id":"133566","upvote_count":"2","content":"B. \"to manipulate the data\". Pig is for ETL so you can transform the data. Tachyon is an in memory engine so you get quick results.\nNowadays, you would want to use Spark for this task.","poster":"askaron","timestamp":"1635694080.0"},{"comment_id":"109995","timestamp":"1635428760.0","poster":"jkoffee","upvote_count":"1","content":"D fit best !\nhttps://aws.amazon.com/fr/big-data/what-is-presto/"},{"timestamp":"1635275340.0","upvote_count":"2","comment_id":"75828","poster":"Bulti","content":"On second thought I think the answer is D because the ask is to just query the data interactively and then later use the results from the query to manipulate the data. So since \"interactive\" is the only requirement I think Presto fits the best."},{"content":"C. data size is PB, Hive is better. Hive is optimized for query throughput, while Presto is optimized for latency. Presto has a limitation on the maximum amount of memory that each task in a query can store, so if a query requires a large amount of memory, the query simply fails. Such error handling logic (or a lack thereof) is acceptable for interactive queries; however, for daily/weekly reports that must run reliably, it is ill-suited. For such tasks, Hive is a better alternative.","poster":"susan8840","comment_id":"74074","timestamp":"1635197520.0","upvote_count":"2"},{"content":"my selection D","timestamp":"1634748420.0","poster":"san2020","upvote_count":"3","comment_id":"52381"},{"poster":"practicioner","comment_id":"44983","timestamp":"1634589420.0","upvote_count":"1","content":"\"Presto is an open source, distributed SQL query engine designed for fast, interactive queries on data in HDFS\"\nI think this is D, but without \"interactive\" hive could be better choice"},{"content":"Answer C\n\"A data engineer needs to run multiple interactive queries to manipulate the data.\"\n\nHive is optimized for query throughput, while Presto is optimized for latency.\nHive translates SQL queries into multiple stages of MapReduce and it is powerful enough to handle huge numbers of jobs.\n\nhttps://blog.treasuredata.com/blog/2015/03/20/presto-versus-hive/","comment_id":"38262","timestamp":"1634371680.0","upvote_count":"1","comments":[{"timestamp":"1634599740.0","comment_id":"47391","poster":"sam3787","content":"the 'interactive' queries might point to Presto only","upvote_count":"2"}],"poster":"ME2000"},{"content":"Presto is used in production at very large scale at many well-known organizations. Youâ€™ll find it used at Facebook, Airbnb, Netflix, Atlassian, Nasdaq, and many more. Facebookâ€™s implementation of Presto is used by over a thousand employees, who run more than 30,000 queries, processing one petabyte of data daily.\nD it is....","upvote_count":"4","poster":"PK1234","comment_id":"36114","timestamp":"1634132640.0"},{"poster":"BigEv","timestamp":"1633818120.0","content":"D\nPresto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes.","upvote_count":"2","comment_id":"20408"},{"upvote_count":"2","poster":"pra276","comment_id":"7683","content":"Answer is D:","timestamp":"1633622160.0"},{"upvote_count":"1","comment_id":"6593","content":"I thought c due to multiple interactive queries as presto can only do one at a time.","timestamp":"1632293580.0","comments":[{"timestamp":"1633017300.0","content":"I have this service confused with Athena","comments":[{"poster":"Jialu","content":"Athena built on Presto w/ SQL Support","timestamp":"1633513020.0","comment_id":"6770","upvote_count":"1"}],"poster":"mattyb123","upvote_count":"1","comment_id":"6597"}],"poster":"mattyb123"},{"content":"The correct answer may is D","comment_id":"6585","comments":[{"comment_id":"6596","content":"I think your right D","upvote_count":"1","poster":"mattyb123","timestamp":"1632706080.0"}],"upvote_count":"4","poster":"Jialu","timestamp":"1632163860.0"}],"answers_community":[],"question_text":"There are thousands of text files on Amazon S3. The total size of the files is 1 PB. The files contain retail order information for the past 2 years. A data engineer needs to run multiple interactive queries to manipulate the data. The Data Engineer has AWS access to spin up an Amazon EMR cluster. The data engineer needs to use an application on the cluster to process this data and return the results in interactive time frame.\nWhich application on the cluster should the data engineer use?","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/3494-exam-aws-certified-big-data-specialty-topic-1-question-62/"},{"id":"HRa2sB5oXq8DQWe8F3Hm","timestamp":"2019-08-11 00:48:00","question_id":60,"answer_ET":"C","discussion":[{"poster":"Bulti","upvote_count":"13","timestamp":"1635024660.0","comment_id":"75836","content":"Not A - Because Spark Streams or any Spark component for that matter cannot read directly from KFH. Also not real-time solution.\nNot C - Not a real-time solution although doable if real-time is not an issue.\nNot D- It doesn't make sense to write the original row data one at a time to S3 when its possible to configure Kinesis stream destination as S3 and split data into multiple files organized by date as prefix.\nB- Is the right answer. Spark Streams can read directly from Kinesis streams and so can a Lambda function which will then insert each record into KFH to be delivered to S3.","comments":[{"content":"The data engineer needs to fulfill a corporate mandate to keep ALL raw messages as they are received as a top priority.","poster":"notcloudguru","upvote_count":"1","timestamp":"1635747240.0","comment_id":"120906"}]},{"content":"A is wrong - Spark Streaming can only read from Kinesis Data Streams\nB doesn't make sense - Kinesis Data Firehose has direct integration with Kinesis Data Streams\nhttps://docs.aws.amazon.com/firehose/latest/dev/writing-with-kinesis-streams.html\nC doesn't make sense - Lambda to integrate Firehose with Data Streams. I don't belive people design such crazy things\nD is correct and looks best compared to the others","comment_id":"337563","timestamp":"1636175280.0","poster":"DerekKey","upvote_count":"1"},{"comment_id":"117953","comments":[{"content":"I agree with D. While we are not degrading any performance in copying the stream message twice, But the no.of services(Firehouse,Lambda) we are bringing. All this is for saving RAW data to S3. Not justifying\n\nSo I believe D is correct answer","comment_id":"123511","poster":"mbabu48","timestamp":"1636007760.0","upvote_count":"1"}],"poster":"guruguru","timestamp":"1635434580.0","content":"D.\nReal-time message ruled out A and C.\nFor B, my concern is that streaming data in parallel to both Lambda and Spark, will that reduce the performance of KDS, or requires more shards? Hence, I pick D.","upvote_count":"1"},{"timestamp":"1635397320.0","content":"Option D. \"The data engineer needs to fulfill a corporate mandate to keep ALL raw messages as they are received as a top priority\" It does mention anything regarding batching, so its ok to write single rows to S3","upvote_count":"1","poster":"esalas0691","comment_id":"114022"},{"content":"It has to be B.","comment_id":"113959","timestamp":"1635388140.0","upvote_count":"1","poster":"Sree_"},{"content":"It could have been B, but we don't need a lambda to push messages, send the stream straight to FH. Because of this, going with D to keep the solution simple.","upvote_count":"3","poster":"srirampc","comment_id":"82591","timestamp":"1635071040.0"},{"poster":"srirampc","comment_id":"82590","timestamp":"1635054720.0","upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"99235","content":"i like Bultis point arguing against D (storing row-wise to S3 appears awful), but your point on B was also what i thought. so weird...","poster":"Corram","timestamp":"1635282780.0"}],"content":"It could have been B, but we don't need a lambda to push messages, send the stream straight to FH. Because of this, going with D to keep the solution simple."},{"poster":"san2020","content":"my selection B","upvote_count":"2","timestamp":"1634908260.0","comment_id":"52382"},{"upvote_count":"2","timestamp":"1633942920.0","comment_id":"41064","comments":[{"content":"After researching I changed for B. Why? Because we should collect batch for storing in S3 (FH is good for it) instead of storing single row events (it will be awful)","timestamp":"1634200440.0","poster":"practicioner","comment_id":"45845","upvote_count":"4"}],"content":"I vote for D. It looks simple and working solution","poster":"practicioner"},{"content":"Answer C\nSpark Streaming can't pull messages from Firehose or Streams, so options A, B and D invalid.\nhttps://docs.aws.amazon.com/solutions/latest/real-time-analytics-spark-streaming/architecture.html","poster":"ME2000","comment_id":"38271","timestamp":"1633763940.0","upvote_count":"1","comments":[{"comments":[{"upvote_count":"1","poster":"sergio1312","comment_id":"42670","comments":[{"upvote_count":"2","timestamp":"1634156520.0","comment_id":"43428","content":"I didn't mention about FH. Amazon Kinesis Streams is using for real-time and spark streaming have integration with it","poster":"practicioner"}],"timestamp":"1633960380.0","content":"Amazon Kinesis Firehose is not real-time"}],"comment_id":"41063","content":"https://spark.apache.org/docs/2.3.0/streaming-kinesis-integration.html\nSpark streaming can pull messages from Kynesis, but it cann't dot it from firehose","poster":"practicioner","upvote_count":"1","timestamp":"1633886940.0"}]},{"poster":"PK1234","content":", Apache Spark can be over-burdened with file operations if it is processing a large number of small files versus fewer larger files. Each of these files has its own overhead of a few milliseconds for opening, reading metadata information, and closing. This overhead of file operations on these large numbers of files results in slow processing. This blog post shows how to use Amazon Kinesis Data Firehose to merge many small messages into larger messages for delivery to Amazon S3. This results in faster processing with Amazon EMR running Spark.\nOption C.","upvote_count":"2","comment_id":"36106","timestamp":"1633752000.0"},{"content":"Nobody has thoughts on A? can spark stream pull out message from kFH directly. A lambda function is necessary for pulling data from KFH to feed spark stream?","timestamp":"1633577100.0","poster":"shwang","comment_id":"30833","upvote_count":"1"},{"comment_id":"30756","timestamp":"1633574700.0","poster":"SamP","upvote_count":"1","content":"I think D is correct. B looks overkill."},{"poster":"Percival","content":"In context, this case doesn't need managed service (FH + fee) in gathering & processing. Cz FH needs buffering time for efficient bulk transfer.. For real time processing.. (not real-time gathering..) it doesn't need FH yet (in gathering & processing.) But in back up to S3.. FH is better.","upvote_count":"2","comment_id":"30216","timestamp":"1633480440.0"},{"poster":"cybe001","timestamp":"1633332900.0","content":"I choose D, kinesis stream can be used to first store the raw data in s3 and then analyze the data in real time.","upvote_count":"2","comment_id":"19399"},{"comment_id":"14820","timestamp":"1633187520.0","content":"funny, nobody bore to search KCL checkpoint table","poster":"pkfe","upvote_count":"1"},{"comments":[{"poster":"pra276","content":"Sorry. The question is \"The companys data engineer needs to collect and process records in real time for analysis using Spark\" Real time for processing using spark so the answer in this case is C. Anyone have any other thoughts?","comments":[{"comments":[{"timestamp":"1633078980.0","comment_id":"7780","poster":"pra276","upvote_count":"2","content":"Agreed. I miss read. Its really confusing"}],"poster":"mattyb123","content":"Only thinking B as when i sat the test previously i used the current answers within this guide and scored very poorly in the collection section. Cause of this i am under the impression C is incorrect and the most likely answer is B for kinesis data streams being realtime and FH having the 60 second delay.","comment_id":"7719","timestamp":"1632931560.0","upvote_count":"5"}],"comment_id":"7583","timestamp":"1632899580.0","upvote_count":"3"}],"timestamp":"1632859920.0","poster":"pra276","content":"FH cannot be an answer it is not a realtime. B is correct","upvote_count":"2","comment_id":"7581"},{"content":"Any thoughts on the additional Kinesis answers? FH has a delay so its truly not realtime","timestamp":"1632162720.0","poster":"mattyb123","comment_id":"6475","comments":[{"upvote_count":"7","content":"B may the correct answer since the records need be in real time for analysis","poster":"Jialu","timestamp":"1632293520.0","comment_id":"6772","comments":[{"comments":[{"timestamp":"1632705480.0","upvote_count":"4","content":"Sounds like a similar usage pattern to: https://aws.amazon.com/blogs/big-data/optimizing-downstream-data-processing-with-amazon-kinesis-data-firehose-and-amazon-emr-running-apache-spark/","comment_id":"6903","poster":"mattyb123"}],"timestamp":"1632343920.0","content":"I agree B","poster":"mattyb123","upvote_count":"4","comment_id":"6801"}]}],"upvote_count":"2"}],"answer_images":[],"isMC":true,"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/3435-exam-aws-certified-big-data-specialty-topic-1-question-63/","unix_timestamp":1565477280,"choices":{"D":"Publish messages to Amazon Kinesis Streams, pull messages off with Spark Streaming, and write row data to Amazon Simple Storage Service (S3) before and after processing.","C":"Publish messages to Amazon Kinesis Firehose backed by Amazon Simple Storage Service (S3). Use AWS Lambda to pull messages from Firehose to Streams for processing with Spark Streaming.","B":"Publish messages to Amazon Kinesis Streams. Pull messages off Streams with Spark Streaming in parallel to AWS Lambda pushing messages from Streams to Firehose backed by Amazon Simple Storage Service (S3).","A":"Publish messages to Amazon Kinesis Firehose backed by Amazon Simple Storage Service (S3). Pull messages off Firehose with Spark Streaming in parallel to persistence to Amazon S3."},"answers_community":[],"question_images":[],"exam_id":17,"answer_description":"","question_text":"A media advertising company handles a large number of real-time messages sourced from over 200 websites.\nThe companys data engineer needs to collect and process records in real time for analysis using Spark\nStreaming on Amazon Elastic MapReduce (EMR). The data engineer needs to fulfill a corporate mandate to keep ALL raw messages as they are received as a top priority.\nWhich Amazon Kinesis configuration meets these requirements?","topic":"1"}],"exam":{"name":"AWS Certified Big Data - Specialty","isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":85,"id":17,"isMCOnly":true,"provider":"Amazon"},"currentPage":12},"__N_SSP":true}