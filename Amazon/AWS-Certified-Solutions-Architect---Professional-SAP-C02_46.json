{"pageProps":{"questions":[{"id":"jt7r5QNiVxmwhqqrrQdu","question_text":"A company is building an application that will run on an AWS Lambda function. Hundreds of customers will use the application. The company wants to give each customer a quota of requests for a specific time period. The quotas must match customer usage patterns. Some customers must receive a higher quota for a shorter time period.\n\nWhich solution will meet these requirements?","unix_timestamp":1698485940,"exam_id":33,"discussion":[{"timestamp":"1702043640.0","content":"Selected Answer: A\nREST APIs and HTTP APIs are both RESTful API products. REST APIs support more features than HTTP APIs, while HTTP APIs are designed with minimal features so that they can be offered at a lower price. Choose REST APIs if you need features such as API keys, per-client throttling, request validation, AWS WAF integration, or private API endpoints. Choose HTTP APIs if you don't need the features included with REST APIs.","upvote_count":"10","poster":"ayadmawla","comment_id":"1091124"},{"timestamp":"1700354580.0","comment_id":"1074378","upvote_count":"5","poster":"career360guru","content":"Selected Answer: A\nOption A answer is little confusing because it talks about Quota but not about Throttle limits. Option B mentions route-level throttling that is also not correct. Route-level throttling can not be applied at per user basis. \nSo option A is right answer."},{"content":"A\nThis approach allows you to create a separate API Gateway instance for each customer, which enables you to implement quotas at the API level.\nBy using a proxy integration, you can invoke the Lambda function on behalf of each customer, while still enforcing their individual quota limits.\nConfiguring an API Gateway usage plan and creating an API key for each user within a customer's organization allows you to manage and enforce quotas for each user separately.\nThis solution is particularly well-suited for managing multiple customers with different quota requirements. By using API Gateway, you can create separate APIs for each customer, which enables fine-grained control over quota enforcement at the client level.","poster":"AzureDP900","comment_id":"1313851","upvote_count":"1","timestamp":"1731904740.0"},{"poster":"Andres123456","comment_id":"1066333","upvote_count":"2","timestamp":"1699525260.0","content":"Selected Answer: A\nOption A\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html"},{"timestamp":"1698680340.0","content":"Selected Answer: B\nIn order to achieve \"Some customers must receive a higher quota for a shorter time period.\", throttling should be set with rate and burst can be set using Throttling","comment_id":"1057981","poster":"Sab","upvote_count":"3","comments":[{"poster":"albert_kuo","upvote_count":"1","comment_id":"1366728","content":"Amazon API Gateway HTTP API does not support usage plan","timestamp":"1741484340.0"}]},{"upvote_count":"4","comment_id":"1056926","content":"Selected Answer: A\nOption A\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html","poster":"gonzales","timestamp":"1698593700.0"},{"timestamp":"1698582900.0","upvote_count":"1","comment_id":"1056808","content":"Selected Answer: A\nIts A, you dont need route level throttling","poster":"KungLjao"},{"timestamp":"1698540900.0","comment_id":"1056525","content":"Option B\nroute-level throttling for each usage plan","upvote_count":"1","poster":"Jun_W"},{"upvote_count":"1","poster":"AM_aws","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html \n HTTP API doesn't include USAGE feature.","comment_id":"1056387","timestamp":"1698519300.0"},{"upvote_count":"1","content":"Option A\nCreate REST API with Proxy Integration and for each customer set the usage plan and Create API Key.\nhttps://medium.com/geekculture/api-key-and-usage-plan-integration-with-aws-api-gateway-2d07bbb9a2a4","comment_id":"1056095","timestamp":"1698485940.0","poster":"airgead"}],"answers_community":["A (88%)","12%"],"answer_images":[],"topic":"1","question_id":226,"answer_ET":"A","timestamp":"2023-10-28 11:39:00","answer_description":"","answer":"A","isMC":true,"choices":{"C":"Create a Lambda function alias for each customer. Include a concurrency limit with an appropriate request quota. Create a Lambda function URL for each function alias. Share the Lambda function URL for each alias with the relevant customer.","B":"Create an Amazon API Gateway HTTP API with a proxy integration to invoke the Lambda function. For each customer configure an API Gateway usage plan that includes an appropriate request quota Configure route-level throttling for each usage plan. Create an API Key from the usage plan for each user that the customer needs.","D":"Create an Application Load Balancer (ALB) in a VPC. Configure the Lambda function as a target for the ALB. Configure an AWS WAF web ACL for the ALB. For each customer configure a rale-based rule that includes an appropriate request quota.","A":"Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs."},"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/124801-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"yK0D3QoMpEcAebLt9psI","url":"https://www.examtopics.com/discussions/amazon/view/124802-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company is planning to migrate its on-premises VMware cluster of 120 VMs to AWS. The VMs have many different operating systems and many custom software packages installed. The company also has an on-premises NFS server that is 10 TB in size. The company has set up a 10 Gbps AWS Direct Connect connection to AWS for the migration.\n\nWhich solution will complete the migration to AWS in the LEAST amount of time?","question_id":227,"answer_description":"","answer":"B","answers_community":["B (94%)","6%"],"topic":"1","choices":{"B":"Configure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMS. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection.","C":"Recreate the VMs on AWS as Amazon EC2 instances. Install all the required software packages. Create an Amazon FSx for Lustre file system. Configure AWS DataSync to copy the NFS server data to the FSx for Lustre file system over the Direct Connect connection.","A":"Export the on-premises VMs and copy them to an Amazon S3 bucket. Use VM Import/Export to create AMIs from the VM images that are stored in Amazon S3. Order an AWS Snowball Edge device. Copy the NFS server data to the device. Restore the NFS server data to an Amazon EC2 instance that has NFS configured.","D":"Order two AWS Snowball Edge devices. Copy the VMs and the NFS server data to the devices. Run VM Import/Export after the data from the devices is loaded to an Amazon S3 bucket. Create an Amazon Elastic File System (Amazon EFS) file system. Copy the NFS server data from Amazon S3 to the EFS file system."},"unix_timestamp":1698486780,"timestamp":"2023-10-28 11:53:00","answer_ET":"B","isMC":true,"exam_id":33,"answer_images":[],"discussion":[{"timestamp":"1698984180.0","comment_id":"1061066","upvote_count":"8","poster":"Ustad","content":"Selected Answer: B\nI'll go with B as 10G direct connection is faster than enough for workload not so big.\nEFS and DataSync are feasible as well."},{"poster":"saptati","upvote_count":"1","comment_id":"1339624","timestamp":"1736703180.0","content":"Selected Answer: B\nB is correct. Let's assume, the total data to transfer is 22,240 GB (120 VMs × 100 GB [each estimated average size] = 12,000 GB and 10,240 GB for NFS data). Using a 10 Gbps Direct Connect link, the theoretical transfer speed is 1.25 GB/s, and accounting for 80% efficiency due to overhead, the effective speed is 1.0 GB/s. The estimated transfer time is 22,240 GB ÷ 1.0 GB/s = 22,240 seconds, or about 6.18 hours. Therefore, the migration can be completed in approximately 6 hours and 10 minutes, leveraging high-speed connectivity and efficient AWS migration services. \nA, C, and D are incorrect: A and D involve longer migration times due to delays from physical shipment and multiple processing steps, while C is incorrect because a manual process would take significantly longer than automated migration tools."},{"comment_id":"1313849","upvote_count":"1","content":"Option B is the best solution because it:\n• Uses AWS Application Migration Service (also known as Storage Gateway), which is designed for high-speed and low-latency data transfer between on-premises infrastructure and AWS.\n• Replicates the VMs with all their software packages installed, ensuring a complete migration with no additional configuration required.\n• Creates an Amazon EFS file system to store the NFS server data, which is a highly available and scalable file storage service that can be easily scaled up or down as needed.\n• Uses AWS DataSync to copy the NFS server data from the on-premises infrastructure to the EFS file system over the Direct Connect connection.\nThis approach ensures a complete and fast migration of all VMs and data to AWS, with minimal downtime and configuration required.","timestamp":"1731904620.0","poster":"AzureDP900"},{"timestamp":"1712951340.0","content":"Selected Answer: D\nAns D.\nThe questions states \"The VMs have many different operating systems and many custom software packages installed\". However, the AWS AMS supports only limited and commonly used OSes and apps.\n\nQ: What operating systems and applications are supported by AWS Application Migration Service?\nAWS Application Migration Service allows you to migrate physical, virtual, and cloud source servers to AWS for a variety of supported operating systems (OS). AWS Application Migration Service supports commonly used applications such as SAP, Oracle, and Microsoft SQL Server.\n\nhttps://aws.amazon.com/application-migration-service/faqs/#:~:text=AWS%20Application%20Migration%20Service%20allows,Oracle%2C%20and%20Microsoft%20SQL%20Server.","comments":[{"comment_id":"1266203","upvote_count":"1","content":"just B","timestamp":"1723697160.0","poster":"helloworldabc"}],"upvote_count":"1","comment_id":"1194515","poster":"titi_r"},{"poster":"Dgix","comment_id":"1179614","content":"Selected Answer: B\nA is too time-consuming.\nB is viable\nC is viable, but overengineered as the DC connection has enough capacity.","comments":[{"poster":"Dgix","content":"Typo:\nC is too time-consuming\nD is viable, but overengineered as the DC connection has enough capacity.","upvote_count":"1","comment_id":"1179615","timestamp":"1711052760.0"}],"upvote_count":"1","timestamp":"1711052700.0"},{"upvote_count":"1","timestamp":"1706325840.0","poster":"ftaws","comment_id":"1133026","content":"10Gbps = 1.25GB/s = 4.5TB/H"},{"upvote_count":"2","poster":"career360guru","timestamp":"1700354820.0","content":"B is the right answer","comment_id":"1074381"},{"poster":"Andres123456","comment_id":"1066336","content":"Selected Answer: B\nOption B.\n10 Gbps AWS Direct Connect connection","timestamp":"1699525440.0","upvote_count":"4"},{"comments":[{"comment_id":"1109956","poster":"carpa_jo","timestamp":"1703967660.0","content":"More like 2.5 hrs, but still a lot faster than shippings Snowballs back and forth...","upvote_count":"2"}],"poster":"KungLjao","timestamp":"1698587580.0","upvote_count":"3","comment_id":"1056856","content":"Selected Answer: B\nB, 13mins to transfer 10tb"},{"comment_id":"1056528","poster":"Jun_W","content":"Option B.\n10 Gbps AWS Direct Connect connection","upvote_count":"4","timestamp":"1698541140.0"},{"upvote_count":"2","poster":"airgead","timestamp":"1698486780.0","comment_id":"1056101","content":"Option D is the correct answer by using Snowball Edge each have 80TB capacity.\nA - Does not make sense to use only 1 Snowball Edge, also NFS to NFS server in EC2 it is not correct! Use AWS EFS\nB - Using Replication will be slow, there is not parellasim especially with additional NFS data transfer\nC - Install required software, as it is custom software, it may be time consuming os 120 VMs"}],"question_images":[]},{"id":"UjyLRW5j4sRb2hIyMObT","answer_ET":"B","question_id":228,"isMC":true,"topic":"1","answers_community":["B (85%)","Other"],"unix_timestamp":1698487560,"answer_images":[],"answer_description":"","timestamp":"2023-10-28 12:06:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/124803-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"answer":"B","discussion":[{"comment_id":"1056952","poster":"gonzales","timestamp":"1698595980.0","content":"Selected Answer: B\nPlease have a look at: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\nsteps:\n- Get a public key-private key pair\n- Create a field-level encryption profile\n- Create a field-level encryption configuration\n- Link to a cache behavior\n\nAn RSA key pair includes a private and a public key (asymmetric)","upvote_count":"15"},{"comment_id":"1074442","poster":"career360guru","content":"Selected Answer: B\nYou need to RSA key for Field levell Encryption and not KMS Symmetric Key so B is the right answer","upvote_count":"6","timestamp":"1700367840.0"},{"poster":"TomTom","timestamp":"1733117220.0","content":"Selected Answer: C\nWhy not C?\nBy using KMS and Lambda@Edge, you can ensure that sensitive data is encrypted both at rest and in transit, providing a robust and secure solution for your distributed application.","upvote_count":"1","comment_id":"1320790"},{"comment_id":"1313847","upvote_count":"1","content":"Here's why option B works:\n\n\n\nCreating an RSA key pair allows you to generate a public-private key pair, where the private key is used for decryption and the public key is used for encryption.\n\nUploading the public key to CloudFront allows the distribution to use this key to encrypt sensitive data as it moves through the application.\n\nUsing field-level encryption with CloudFront and Amazon S3 ensures that only the data-handling microservice (which has access to the private key) can decrypt the sensitive data.","timestamp":"1731904500.0","poster":"AzureDP900"},{"timestamp":"1727500560.0","comment_id":"1290513","upvote_count":"2","poster":"Chakanetsa","content":"Selected Answer: B\nThe following steps provide an overview of setting up field-level encryption. For specific steps, see Set up field-level encryption.\n-Get a public key-private key pair.\n-Create a field-level encryption profile\n-Create a field-level encryption configuration.\n-Link to a cache behavior."},{"timestamp":"1717273200.0","content":"Option B - field-level encryption requires public/private key pair","upvote_count":"3","poster":"9f02c8d","comment_id":"1222887"},{"timestamp":"1711974960.0","poster":"VerRi","upvote_count":"1","content":"Selected Answer: B\nfield-level encryption in CloudFront uses asymmetric encryption with RSA key","comment_id":"1187415"},{"comments":[{"content":"B is correct, Not A","poster":"Russs99","upvote_count":"2","timestamp":"1706142900.0","comment_id":"1131205"}],"timestamp":"1706142720.0","content":"Selected Answer: A\nCloudFront only supports field-level encryption with symmetric KMS keys, not with RSA keys. in this specific scenario, Option A would be the correct answer because it leverages the native capabilities of CloudFront and meets the requirement of centralized key management for decrypting sensitive data.","comment_id":"1131201","upvote_count":"4","poster":"Russs99"},{"upvote_count":"3","comment_id":"1063745","timestamp":"1699267680.0","content":"Selected Answer: B\nALGORITHM: CloudFront uses RSA/ECB/OAEPWithSHA-256AndMGF1Padding as the algorithm for encrypting, so you must use the same algorithm to decrypt the data.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-decrypt","poster":"cachac"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html","poster":"KungLjao","timestamp":"1698583620.0","upvote_count":"2","comment_id":"1056816"},{"content":"Answer: A\nuse field-level encryption with AWS Key Management Service (KMS) so that it can encrypt when send through Cloud Distribution and only the specific microservice with access to the appropriate KMS key can decrypt it.\nRSA does not work as Microservice data handling cannot decrypt it.\nIt does not require Lambda @Edge to perform to encrypt the data, just Associate the KMS key and the configuration with the CloudFront cache behavio","comment_id":"1056106","timestamp":"1698487560.0","upvote_count":"1","poster":"airgead"}],"question_text":"An online survey company runs its application in the AWS Cloud. The application is distributed and consists of microservices that run in an automatically scaled Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster is a target for an Application Load Balancer (ALB). The ALB is a custom origin for an Amazon CloudFront distribution.\n\nThe company has a survey that contains sensitive data. The sensitive data must be encrypted when it moves through the application. The application's data-handling microservice is the only microservice that should be able to decrypt the data\n\nWhich solution will meet these requirements?","choices":{"A":"Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a field-level encryption profile and a configuration. Associate the KMS key and the configuration with the CloudFront cache behavior.","D":"Create an RSA key pair that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the private key of the RSA key pair to encrypt the sensitive data.","C":"Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the KMS key to encrypt the sensitive data.","B":"Create an RSA key pair that is dedicated to the data-handing microservice. Upload the public key to the CloudFront distribution. Create a field-level encryption profile and a configuration. Add the configuration to the CloudFront cache behavior."}},{"id":"h0z7YYLRf2nwGeU1bA2V","exam_id":33,"topic":"1","answer_ET":"B","choices":{"D":"Create a private hosted zone. Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute for the VPC. Deactivate the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=AmazonProvidedDNS.","A":"Create a private hosted zone. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=10.24.34.2.","B":"Create a private hosted zone Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=AmazonProvidedDNS. Associate the new DHCP options set with the VPC.","C":"Deactivate the enableDnsSupport attribute for the VPActivate the enableDnsHostnames attribute for the VPCreate a new VPC DHCP options set, and configure doman-name-servers=10.24.34.2. Associate the new DHCP options set with the VPC."},"timestamp":"2023-10-30 13:56:00","question_text":"A solutions architect is determining the DNS strategy for an existing VPC. The VPC is provisioned to use the 10.24.34.0/24 CIDR block. The VPC also uses Amazon Route 53 Resolver for DNS. New requirements mandate that DNS queries must use private hosted zones. Additionally instances that have public IP addresses must receive corresponding public hostnames\n\nWhich solution will meet these requirements to ensure that the domain names are correctly resolved within the VPC?","isMC":true,"discussion":[{"content":"Selected Answer: B\nBoth settings need to be enabled to allow assigning of public DNS names and use of Amazon DNS, see https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#AmazonDNS","upvote_count":"6","comment_id":"1057690","timestamp":"1698670560.0","poster":"s61"},{"content":"Selected Answer: B\nA is wrong because the question says it use AWS DNS rather than 10.24.34.2 custom DNS server.\nC is wrong because same reason with A.\nD is wrong because we need to actvate DnsSupport and DnsHostnames.\n\nPlease correct me if I am wrong.","upvote_count":"5","comment_id":"1122312","timestamp":"1705215240.0","poster":"JMAN1"},{"poster":"kajiyatta","upvote_count":"1","timestamp":"1733387940.0","content":"Selected Answer: B\nA,D incorrect = Can't update DHCP option.\nC incorrect = Not create private hosted zone.","comment_id":"1322280"},{"poster":"AzureDP900","timestamp":"1731904380.0","comment_id":"1313845","content":"Option B is correct\n\nTo meet the requirements, you need to:\n\nUse private hosted zones for DNS queries.\n\nAssign public hostnames to instances with public IP addresses.\n\n\n\nCreating a private hosted zone (Option B) meets these requirements by providing a private DNS resolution service within the VPC.\n\nAssociating the private hosted zone with the VPC ensures that DNS queries are resolved using this zone.\n\nActivating enableDnsSupport and enableDnsHostnames attributes for the VPC allows instances to use the private hosted zone for DNS lookups and assigns public hostnames to instances with public IP addresses, respectively.\n\nCreating a new VPC DHCP options set with domain-name-servers=AmazonProvidedDNS ensures that instances receive the correct DNS server information.","upvote_count":"1"},{"upvote_count":"4","poster":"career360guru","comment_id":"1074448","timestamp":"1700369280.0","content":"Selected Answer: B\nEnable both the dns options."},{"content":"Selected Answer: B\nB is the best answer","comment_id":"1073294","upvote_count":"2","timestamp":"1700223360.0","poster":"nublit"},{"timestamp":"1699934880.0","poster":"bustedd","upvote_count":"2","content":"B enables both settings","comment_id":"1069966"}],"url":"https://www.examtopics.com/discussions/amazon/view/124950-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1698670560,"answer_images":[],"answers_community":["B (100%)"],"answer":"B","answer_description":"","question_id":229,"question_images":[]},{"id":"eqNrKDqwsXR7rHt7Xdev","question_text":"A data analytics company has an Amazon Redshift cluster that consists of several reserved nodes. The cluster is experiencing unexpected bursts of usage because a team of employees is compiling a deep audit analysis report. The queries to generate the report are complex read queries and are CPU intensive.\n\nBusiness requirements dictate that the cluster must be able to service read and write queries at all times. A solutions architect must devise a solution that accommodates the bursts of usage.\n\nWhich solution meets these requirements MOST cost-effectively?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/124804-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"unix_timestamp":1698488580,"choices":{"D":"Turn on the Concurrency Scaling feature for the Amazon Redshift cluster.","C":"Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elastic resize operation when the cluster’s CPU metrics in Amazon CloudWatch reach 80%.","A":"Provision an Amazon EMR cluster Offload the complex data processing tasks.","B":"Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classic resize operation when the cluster’s CPU metrics in Amazon CloudWatch reach 80%."},"answer":"D","answer_description":"","topic":"1","discussion":[{"comment_id":"1313844","poster":"AzureDP900","timestamp":"1731904200.0","upvote_count":"1","content":"D is right\nConcurrency Scaling is a feature in Amazon Redshift that allows you to scale your cluster's concurrency level up or down based on usage, without having to manually resize the cluster.\nBy turning on Concurrency Scaling, the cluster will automatically scale up when CPU utilization reaches a certain threshold (in this case, 80%), and then scale back down when usage returns to normal. This allows you to meet unexpected bursts of usage while keeping costs under control.\nThis solution meets the business requirements by ensuring that read and write queries can be serviced at all times, while also being cost-effective."},{"poster":"alexandercamachop","timestamp":"1708901580.0","comment_id":"1159246","content":"Selected Answer: D\n“ With the Concurrency Scaling feature, you can support thousands of concurrent users and concurrent queries, with consistently fast query performance. When you turn on concurrency scaling, Amazon Redshift automatically adds additional cluster capacity to process an increase in both read and write queries.” \nhttps://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html","upvote_count":"3"},{"timestamp":"1700398200.0","comment_id":"1074617","content":"Answer C","upvote_count":"1","poster":"cypkir","comments":[{"comment_id":"1266206","timestamp":"1723697520.0","poster":"helloworldabc","content":"just D","upvote_count":"2"}]},{"timestamp":"1700369460.0","upvote_count":"2","comment_id":"1074449","poster":"career360guru","content":"Selected Answer: D\nBest option is D"},{"poster":"nublit","timestamp":"1700223600.0","comment_id":"1073298","upvote_count":"2","content":"Selected Answer: D\nThe most cost-effective solution for addressing bursts of usage and accommodating complex queries in Amazon Redshift is to turn on the Concurrency Scaling feature for the Amazon Redshift cluster."},{"timestamp":"1698985140.0","poster":"Ustad","content":"Selected Answer: D\nSimply D","comment_id":"1061069","upvote_count":"1"},{"poster":"AM_aws","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/big-data/scale-amazon-redshift-to-meet-high-throughput-query-requirements/#:~:text=Use%20concurrency%20scaling%20to%20dynamically,data%20warehouse%20using%20Amazon%20Redshift.","comment_id":"1056469","upvote_count":"4","timestamp":"1698531480.0"},{"comment_id":"1056113","timestamp":"1698488580.0","content":"Answer: D\nThe most cost-effective solution for addressing bursts of usage and accommodating complex queries in Amazon Redshift is to turn on the Concurrency Scaling feature for the Amazon Redshift cluster.","upvote_count":"4","poster":"airgead"}],"timestamp":"2023-10-28 12:23:00","answers_community":["D (100%)"],"exam_id":33,"answer_images":[],"question_id":230,"answer_ET":"D"}],"exam":{"name":"AWS Certified Solutions Architect - Professional SAP-C02","isBeta":false,"id":33,"isImplemented":true,"lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":529,"isMCOnly":true},"currentPage":46},"__N_SSP":true}