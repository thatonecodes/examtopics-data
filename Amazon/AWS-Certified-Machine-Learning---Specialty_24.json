{"pageProps":{"questions":[{"id":"T0CV9efZDTNz9pwIdu6H","choices":{"C":"Use SageMaker script mode, and use train.py unchanged. Point the SageMaker training invocation to the local path of the data without reformatting the training data.","D":"Use SageMaker script mode, and use train.py unchanged. Put the TFRecord data into an Amazon S3 bucket. Point the SageMaker training invocation to the S3 bucket without reformatting the training data.","B":"Rewrite the train.py script to add a section that converts TFRecord data to protobuf format. Point the SageMaker training invocation to the local path of the data. Ingest the protobuf data instead of the TFRecord data.","A":"Put the TFRecord data into an Amazon S3 bucket. Use AWS Glue or AWS Lambda to reformat the data to protobuf format and store the data in a second S3 bucket. Point the SageMaker training invocation to the second S3 bucket."},"answers_community":["D (100%)"],"timestamp":"2022-11-28 00:00:00","answer_images":[],"unix_timestamp":1669590000,"topic":"1","answer_description":"","question_images":[],"question_text":"A company's machine learning (ML) specialist is designing a scalable data storage solution for Amazon SageMaker. The company has an existing TensorFlow-based model that uses a train.py script. The model relies on static training data that is currently stored in TFRecord format.\n\nWhat should the ML specialist do to provide the training data to SageMaker with the LEAST development overhead?","answer":"D","isMC":true,"discussion":[{"content":"Selected Answer: D\nShould be D. TFRecord could be uploaded to S3 directly and be used as SageMaker's data source.\nhttps://sagemaker-examples.readthedocs.io/en/latest/sagemaker_batch_transform/working_with_tfrecords/working-with-tfrecords.html#Upload-dataset-to-S3","timestamp":"1701169200.0","poster":"VinceCar","comment_id":"728992","upvote_count":"5","comments":[{"comment_id":"777528","upvote_count":"1","comments":[{"content":"then how is local path a \"scalable data storage solution\" ?\n\nanswer is D","timestamp":"1708351980.0","upvote_count":"2","poster":"drcok87","comment_id":"814154"}],"timestamp":"1705399560.0","content":"Then why not C","poster":"BTRYING"}]},{"timestamp":"1701243180.0","upvote_count":"5","comment_id":"730055","poster":"Amit11011996","content":"Selected Answer: D\nIt has to option D."},{"content":"Selected Answer: D\nA. NO - SageMaker can use TFRecods as-is in S3\nB. NO - SageMaker can use TFRecods as-is in S3\nC. NO - SageMaker must use S3 as input, it cannot read your local data\nD. YES - SageMaker can use TFRecods as-is in S3","timestamp":"1726054440.0","comment_id":"1004697","poster":"loict","upvote_count":"2"},{"poster":"Mickey321","comment_id":"984510","content":"Selected Answer: D\nSageMaker script mode allows you to use your existing TensorFlow training scripts without any modifications. You can use the same TFRecord data format that your model expects, and point the SageMaker training invocation to the S3 bucket where the data is stored. SageMaker will automatically download the data to the local path of the training instance and pass it as an argument to your train.py script. You don’t need to reformat the data to protobuf format or rewrite your script to convert the data12.","timestamp":"1723986600.0","upvote_count":"3"},{"comment_id":"970052","content":"Selected Answer: D\nThis option allows the ML specialist to use the existing train.py script and TFRecord data without any changes, minimizing development overhead. By using SageMaker script mode, the specialist can run the existing TensorFlow script as-is, and by pointing the SageMaker training invocation to the S3 bucket containing the TFRecord data, the specialist can provide the training data to SageMaker without reformatting it.","upvote_count":"1","timestamp":"1722595440.0","poster":"Mickey321"},{"content":"Selected Answer: D\nThis option leverages SageMaker's built-in support for the TensorFlow framework and script mode. The existing train.py script can be used without any modifications. SageMaker will automatically download the training data from the specified S3 location to the instance running the training job. \n\nThis option saves development time by avoiding the need to rewrite the train.py script or reformat the training data.","upvote_count":"3","timestamp":"1708390440.0","comment_id":"814703","poster":"AjoseO"}],"exam_id":26,"answer_ET":"D","question_id":116,"url":"https://www.examtopics.com/discussions/amazon/view/89051-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"r26L9aU3klv6XwO2GySr","answers_community":["D (100%)"],"discussion":[{"poster":"Peeking","upvote_count":"8","comment_id":"741478","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html\n\nManaged spot training can optimize the cost of training models up to 90% over on-demand instances. SageMaker manages the Spot interruptions on your behalf.\n\n\"Spot instances can be interrupted, causing jobs to take longer to start or finish. You can configure your managed spot training job to use checkpoints. SageMaker copies checkpoint data from a local path to Amazon S3. When the job is restarted, SageMaker copies the data from Amazon S3 back into the local path. The training job can then resume from the last checkpoint instead of restarting.\"","timestamp":"1702276620.0"},{"upvote_count":"6","timestamp":"1701242940.0","poster":"Amit11011996","comments":[{"upvote_count":"4","content":"agree. managed spot training is also cost effective","poster":"hichemck","comment_id":"730757","timestamp":"1701284520.0"}],"comment_id":"730053","content":"Selected Answer: D\nIt has to be D.\nWith Spot training we can reduce the cost and save the model weights with checkpoint enabled."},{"timestamp":"1726054680.0","poster":"loict","upvote_count":"1","content":"Selected Answer: D\nA. NO - D is simpler\nB. NO - D is simpler\nC. NO - D is simpler \nD. YES - works out-of-the-box","comment_id":"1004712"},{"poster":"Mickey321","upvote_count":"1","content":"Selected Answer: D\nManaged spot training in Amazon SageMaker uses Amazon EC2 Spot instances to run training jobs, which can optimize the cost of training models by up to 90% over on-demand instances 1. SageMaker manages the Spot interruptions on the company’s behalf 1. By enabling checkpointing, the company can ensure that if a Spot instance is interrupted, the training job can resume from the last checkpoint instead of restarting, avoiding loss of work and model retraining 1","timestamp":"1722595680.0","comment_id":"970060"},{"timestamp":"1714109220.0","upvote_count":"1","comment_id":"881142","content":"Selected Answer: D\nUse managed spot training in Amazon SageMaker. Launch the training jobs with checkpointing enabled. \n\nManaged spot training in Amazon SageMaker can help minimize operational overhead and cost by using spot instances to perform the training. This can significantly reduce the cost of training, while still achieving the same accuracy. SageMaker provides built-in checkpointing capability, which allows saving model weights and progress to Amazon S3 periodically. This ensures that even if the spot instances are terminated, the training can resume from the last saved checkpoint. Additionally, SageMaker provides a managed service, so the ecommerce company does not need to worry about managing the infrastructure, and can focus on building and tuning their model.","poster":"Gaby999"},{"poster":"Gaby999","comment_id":"881139","content":"Selected Answer: D\nThe ML specialist should choose option D, which provides the training data to SageMaker with the least development overhead. This option involves putting the TFRecord data into an Amazon S3 bucket and pointing the SageMaker training invocation to the S3 bucket without reformatting the training data. Using SageMaker script mode is a convenient way to execute training scripts without any modification. Since the training script train.py already works with TFRecord data, it can be used as is without any changes. By storing the data in S3 and accessing it from there, the specialist can take advantage of SageMaker's built-in data distribution and parallelization capabilities, which can significantly speed up training. Rewriting the train.py script or using additional services like AWS Glue or Lambda would add unnecessary complexity and increase development overhead.","timestamp":"1714108980.0","upvote_count":"1"},{"comment_id":"814702","timestamp":"1708390320.0","content":"Selected Answer: D\nManaged spot training in Amazon SageMaker provides a cost-effective way to run large machine learning workloads. \n\nWith managed spot training, the training jobs are executed using Amazon EC2 Spot instances, which can significantly reduce the cost of training. \n\nAdditionally, by launching training jobs with checkpointing enabled, the work done up to the last checkpoint is saved to Amazon S3. This ensures that the training job can be resumed from the last checkpoint in case of instance failure or termination. This minimizes the risk of data loss and avoids the need for retraining the model from scratch. Using Amazon SageMaker also reduces the operational overhead required to set up and manage the training environment.","poster":"AjoseO","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/89213-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"D","exam_id":26,"timestamp":"2022-11-29 08:29:00","topic":"1","answer_ET":"D","isMC":true,"question_id":117,"question_images":[],"unix_timestamp":1669706940,"question_text":"An ecommerce company wants to train a large image classification model with 10,000 classes. The company runs multiple model training iterations and needs to minimize operational overhead and cost. The company also needs to avoid loss of work and model retraining.\n\nWhich solution will meet these requirements?","choices":{"C":"Use AWS Lambda to run the training jobs. Save model weights to Amazon S3.","A":"Create the training jobs as AWS Batch jobs that use Amazon EC2 Spot Instances in a managed compute environment.","B":"Use Amazon EC2 Spot Instances to run the training jobs. Use a Spot Instance interruption notice to save a snapshot of the model to Amazon S3 before an instance is terminated.","D":"Use managed spot training in Amazon SageMaker. Launch the training jobs with checkpointing enabled."},"answer_images":[],"answer_description":""},{"id":"64NX2Eo8f886bJPPrGFi","timestamp":"2022-11-26 11:02:00","url":"https://www.examtopics.com/discussions/amazon/view/88838-exam-aws-certified-machine-learning-specialty-topic-1/","answer_images":[],"question_id":118,"topic":"1","answer_ET":"A","choices":{"A":"Use Amazon SageMaker Model Monitor to create a data quality baseline. Confirm that the emit_metrics option is set to Enabled in the baseline constraints file. Set up an Amazon CloudWatch alarm for the metric.","C":"Use Amazon SageMaker Debugger to create rules to capture feature values Set up an Amazon CloudWatch alarm for the rules.","B":"Use Amazon SageMaker Model Monitor to create a model quality baseline. Confirm that the emit_metrics option is set to Enabled in the baseline constraints file. Set up an Amazon CloudWatch alarm for the metric.","D":"Use Amazon CloudWatch to monitor Amazon SageMaker endpoints. Analyze logs in Amazon CloudWatch Logs to check for data drift."},"unix_timestamp":1669456920,"isMC":true,"answers_community":["A (94%)","6%"],"answer_description":"","question_images":[],"question_text":"A retail company uses a machine learning (ML) model for daily sales forecasting. The model has provided inaccurate results for the past 3 weeks. At the end of each day, an AWS Glue job consolidates the input data that is used for the forecasting with the actual daily sales data and the predictions of the model. The AWS Glue job stores the data in Amazon S3.\n\nThe company's ML team determines that the inaccuracies are occurring because of a change in the value distributions of the model features. The ML team must implement a solution that will detect when this type of change occurs in the future.\n\nWhich solution will meet these requirements with the LEAST amount of operational overhead?","answer":"A","discussion":[{"comment_id":"730762","upvote_count":"10","timestamp":"1701284760.0","content":"Selected Answer: A\nA is correct.\n\"If the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, the model begins to lose accuracy in its predictions. Amazon SageMaker Model Monitor uses rules to detect data drift and alerts you when it happens.\"\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html","poster":"hichemck"},{"upvote_count":"1","content":"Selected Answer: A\nA. YES - Model Monitor can validate distribution of input data\nB. NO - a model quality baseline is for model performance eg. precision, F1 score, etc.\nC. NO - Model Monitor is the right tool\nD. NO - Model Monitor is the right tool","timestamp":"1726054980.0","comment_id":"1004718","poster":"loict"},{"comment_id":"978744","upvote_count":"1","content":"Selected Answer: A\nit's a problem of monitoring data distributions.","poster":"kaike_reis","timestamp":"1723388820.0"},{"poster":"Mickey321","timestamp":"1722885900.0","upvote_count":"1","comment_id":"973324","content":"Selected Answer: A\nThe reason for this choice is that Amazon SageMaker Model Monitor is a feature of Amazon SageMaker that allows you to monitor and analyze your machine learning models in production. Model Monitor can automatically detect data drift and other data quality issues by comparing your live data with a baseline dataset that you provide1. Model Monitor can also emit metrics and alerts when it detects violations of the constraints that you define or that it suggests based on the baseline2."},{"comment_id":"814700","upvote_count":"3","timestamp":"1708390200.0","content":"Selected Answer: A\nA is correct.\n\nThe best solution to meet the requirements is to use Amazon SageMaker Model Monitor to create a data quality baseline. The ML team can set up a data quality baseline to detect when the input data to the model has drifted significantly from the historical distribution of the data. When data drift occurs, the Model Monitor emits a metric that can trigger an alarm in Amazon CloudWatch. The ML team can use this alarm to investigate and take corrective action.\n\nOption B is incorrect because model quality baseline monitors model performance, not the input data quality.\n\nOption C is incorrect because Amazon SageMaker Debugger is used to debug machine learning models and to identify problems with model training, not data quality.\n\nOption D is incorrect because Amazon CloudWatch does not provide any features to monitor data drift in the input data used for the machine learning model.","poster":"AjoseO"},{"timestamp":"1708352640.0","comment_id":"814163","upvote_count":"1","content":"Selected Answer: A\nData monitor\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html\n Properties of independent variables changes due to seasonality, customer preferences \nModel monitor\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality.html\n Concept of what is spam email, changes over time\n\"The company's ML team determines that the inaccuracies are occurring because of a change in the value distributions of the model features.” They know model features that is data for model input is changing so we monitor data\nhttps://pair-code.github.io/what-if-tool/learn/tutorials/features-overview/\n\na","poster":"drcok87"},{"upvote_count":"1","timestamp":"1701274260.0","content":"I think the answer is B. Data quality can be monitored via model monitor model quality baseline.","comment_id":"730606","poster":"dunhill"},{"upvote_count":"1","timestamp":"1701174360.0","comment_id":"729076","poster":"VinceCar","comments":[{"timestamp":"1723388760.0","content":"model features = data","poster":"kaike_reis","comment_id":"978743","upvote_count":"1"}],"content":"Selected Answer: B\nB. Since it's \"a change in the value distributions of the model features\"."},{"content":"What is the difference of ans A and B?","upvote_count":"1","poster":"tsangckl","timestamp":"1700992920.0","comments":[{"poster":"pass3in3mon","timestamp":"1701274020.0","upvote_count":"1","comment_id":"730600","content":"model quality baseline vs data quality baseline"}],"comment_id":"727431"}],"exam_id":26},{"id":"bOSQ7fr0rWqgdvlwHj75","question_id":119,"discussion":[{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html","comment_id":"761346","timestamp":"1672337640.0","upvote_count":"9","poster":"rrshah83"},{"poster":"VinceCar","comment_id":"734816","comments":[{"timestamp":"1697650020.0","upvote_count":"2","comment_id":"1047106","content":"SageMaker Autopilot is designed to automatically build, train, and tune the best machine learning model based on a dataset, without the user needing to choose an algorithm. It's not designed to be used with custom container images.","poster":"wendaz"},{"poster":"Alexkats_87","content":"It seems that Autopilot doesn't support image data (image classification), so B will be incorrect in this case https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-datasets-problem-types.html#autopilot-datasets","timestamp":"1674509520.0","comment_id":"785857","upvote_count":"3"}],"timestamp":"1670124120.0","content":"Selected Answer: B\nAutopilot is the faster. \"Amazon SageMaker Autopilot experiments are now up to 2x faster in Hyperparameter Optimization training mode\" . Refer to \n\nhttps://aws.amazon.com/about-aws/whats-new/2022/11/amazon-sagemaker-autopilot-experiments-2x-faster-hyperparameter-optimization-training-mode/?nc1=h_ls","upvote_count":"8"},{"comment_id":"1409948","timestamp":"1742890980.0","content":"Selected Answer: D\nThis is the MOST efficient approach. SageMaker's automatic model tuning (HPO) is designed to efficiently search the hyperparameter space and find the best model. By using it for both the custom container image and the built-in algorithm, the ML specialist can directly compare the performance of the two approaches in the least amount of time.","upvote_count":"2","poster":"Togy"},{"timestamp":"1735174980.0","upvote_count":"3","comment_id":"1331742","content":"Selected Answer: C\nince AMT works seamlessly with both the built-in image classification algorithm and custom container images, answer D (\"tune both models at the same time\") might seem feasible at first glance. However, each HPO job is independent—you cannot run a single AMT job for multiple algorithms or containers simultaneously. This limitation makes C the more appropriate choice for managing and comparing experiments systematically","poster":"587df71"},{"upvote_count":"1","content":"C\nQuestion asks \"determine whether HPO with the SageMaker built-in image classification algorithm will produce a better model than the model produced by HPO with the custom container image\" meaning experiment both option and then determine which is better.\n\"All ML experiments and HPO jobs must be invoked from scripts inside SageMaker Studio notebooks\"\n\nSagemake experiments provides more capability \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html","timestamp":"1730633580.0","comment_id":"1306494","poster":"spinatram"},{"content":"SageMaker's automatic model tuning (also known as hyperparameter optimization, or HPO) is designed to find the best hyperparameters for your model by running multiple training jobs with different hyperparameter configurations.\nIt supports both built-in algorithms and custom container images, making it a versatile tool for this task.","upvote_count":"1","timestamp":"1730489760.0","poster":"ifmx3","comment_id":"1305972"},{"content":"Selected Answer: D\nOption D\nNot C bcos using SageMaker Experiments to manage multiple training jobs adds an extra layer of management complexity. While it helps in tracking experiments, it does not inherently speed up the HPO process compared to running them concurrently.\nIn summary, Option D provides the most efficient and straightforward approach to determine the best model by leveraging SageMaker’s automatic model tuning capabilities to run HPO on both models simultaneously","timestamp":"1730060160.0","upvote_count":"1","comment_id":"1303733","poster":"MultiCloudIronMan"},{"upvote_count":"2","comment_id":"1229797","content":"Selected Answer: D\nOption D stands out as the most effective approach because it leverages SageMaker's automatic model tuning capabilities for both the custom container image and the built-in image classification algorithm. This ensures:","poster":"sheetalconect","timestamp":"1718282220.0"},{"comment_id":"1216337","content":"Selected Answer: D\nD\nThe question is talking about how to do HPO using AWS Sagemaker for a model in custom image. \nExperiment is not to do HPO because you need to input parameter manually.\nSo D","upvote_count":"1","timestamp":"1716448560.0","poster":"rav009"},{"timestamp":"1714585440.0","content":"Selected Answer: C\nAmazon sagemaker experiment is ideal for this.","upvote_count":"2","poster":"rookiee1111","comment_id":"1205184"},{"upvote_count":"1","timestamp":"1711545780.0","poster":"F1Fan","content":"Selected Answer: D\nD: By using SageMaker's automatic model tuning capability to tune both the custom container image model and the built-in image classification algorithm model simultaneously, it leverages the parallel processing capabilities of SageMaker. This approach allows for efficient utilization of compute resources and can potentially complete the tuning process for both models in a shorter amount of time compared to running separate tuning jobs sequentially.\n\nAdditionally, option D aligns with the requirement of invoking all ML experiments and HPO jobs from scripts inside SageMaker Studio notebooks, as SageMaker's automatic model tuning can be initiated and managed through notebook scripts. \n\nWhile options B and C could potentially work, option D provides the most direct and efficient path to meeting the requirements in the least amount of time by leveraging SageMaker's parallel processing capabilities and avoiding potential development overhead or limitations associated with other approaches.","comment_id":"1184128"},{"content":"Selected Answer: D\nThe best option to meet the requirements in the least amount of time is D. Use the automatic model tuning capability of SageMaker to tune the models of the custom container image and the built-in image classification algorithm at the same time. This approach directly utilizes SageMaker's built-in capabilities for HPO, applies to both custom containers and built-in algorithms, and avoids the inefficiencies associated with local mode or manual management of experiments. It's important to note that while the tuning jobs would not literally run \"at the same time\" in a single operation, this option represents the most efficient use of SageMaker's capabilities for both scenarios.","upvote_count":"1","timestamp":"1707397020.0","comment_id":"1144459","poster":"kyuhuck"},{"comment_id":"1088494","upvote_count":"1","timestamp":"1701781200.0","poster":"[Removed]","content":"Selected Answer: C\nShould be C. We are looking at comparing 2 models here, where Sagemaker Experiments fits the bill.\n\nD is out because \"Amazon SageMaker automatic model tuning (AMT), also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset.\" https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html"},{"poster":"akgarg00","upvote_count":"2","comment_id":"1083721","timestamp":"1701282840.0","content":"Selected Answer: D\nD can be done easily https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html\n\"You can use SageMaker AMT with built-in algorithms, custom algorithms, or SageMaker pre-built containers for machine learning frameworks.\""},{"comment_id":"1050274","content":"Selected Answer: D\nI will go with D\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html","upvote_count":"1","timestamp":"1697956980.0","poster":"DimLam"},{"timestamp":"1697062980.0","comment_id":"1041145","poster":"backbencher2022","upvote_count":"1","comments":[{"poster":"wendaz","upvote_count":"2","comment_id":"1047107","timestamp":"1697650200.0","content":"SageMaker Autopilot is designed to automatically build, train, and tune the best machine learning model based on a dataset, without the user needing to choose an algorithm. It's not designed to be used with custom container images."}],"content":"Selected Answer: B\nWill go with B and Autopilot supports Image classification as per this link - https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html\n\nAutopilot currently supports the following problem types:\n\nRegression, binary, and multiclass classification with tabular data formatted as CSV or Parquet files in which each column contains a feature with a specific data type and each row contains an observation. The column data types accepted include numerical, categorical, text, and time series that consists of strings of comma-separated numbers.\n\nText classification with data formatted as CSV or Parquet files in which a column provides the sentences to be classified, while another column should provide the corresponding class label.\n\nImage classification with images formats such as PNG, JPEG or a combination of both.\n\nTime-series forecasting with time-series data formatted as CSV or as Parquet files."},{"content":"Selected Answer: D\nA. NO - try AMT (=Automatic Model Tuning) before using custom HPO scripts; further, no reason to use the local mode\nB. NO - Autopilot is not for HPO only, it will also select a model etc.\nC. NO - requires manual parameter setting for each experiments\nD. YES - AMT (=Automatic Model Tuning) work with custom containers","comment_id":"1004736","poster":"loict","upvote_count":"4","timestamp":"1694433600.0"},{"content":"Answer B in my opinion. Key is autopilot in least amount of time and early stopping to switch over","poster":"chet100","comment_id":"993361","upvote_count":"1","timestamp":"1693330800.0"},{"comment_id":"993002","upvote_count":"1","timestamp":"1693306620.0","content":"Selected Answer: C\nChanging to option C","poster":"Mickey321"},{"upvote_count":"2","timestamp":"1693283940.0","poster":"Mickey321","comment_id":"992774","content":"Selected Answer: D\nGoing for Option D"},{"poster":"Mickey321","upvote_count":"3","timestamp":"1690979460.0","content":"Selected Answer: D\nIt’s a bit confusing but leaning towards D\nThis option allows the ML specialist to use the automatic model tuning capability of SageMaker to tune both models simultaneously, saving time compared to tuning them sequentially. By selecting the model with the best objective metric value, the ML specialist can determine whether HPO with the SageMaker built-in image classification algorithm produces a better model than HPO with the custom container image. All of this can be done from scripts inside SageMaker Studio notebooks, meeting the requirements in the least amount of time.","comment_id":"970154"},{"content":"Selected Answer: C\nExperiment to compare the runs.\n\nB is not feasible as you cannot use it to \"tune\" your custom model. Auto Pilot is meant to test your model with lots of other algorithms that, by the way, are not much \"suitable\" for image input.","comment_id":"886660","poster":"WilianCB","timestamp":"1682969520.0","upvote_count":"1"},{"timestamp":"1682916600.0","poster":"cox1960","upvote_count":"1","comment_id":"885875","content":"Selected Answer: C\nwe have 2 algorithms already and to experiment. Autopilot selects algorithms."},{"comment_id":"879534","content":"Option D:\n\nWhat can be easier than calling Hyperparameter Job for both custom container and built-in algorithm using the same command inside SageMaker Studio, probably nothing. So, let's look into proofs why this is possible. Remember, we have to find the possible solution which can be achieved in the LEAST amount of development time.\n\n1. It's possible to Tune Multiple Algorithms with Hyperparameter Optimization to Find the Best Model. We only need to provide job settings that apply to all of the algorithms to be tested with list of training definitions.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/multiple-algorithm-hpo.html\n\n2. You can use SageMaker AMT with built-in algorithms, custom algorithms, or SageMaker pre-built containers for machine learning frameworks. (Ctrl + F). \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html","poster":"injoho","comments":[{"upvote_count":"3","poster":"injoho","comment_id":"879536","timestamp":"1682354220.0","content":"Eliminating other options:\n\nA - Performing custom HPO is possible but time-consuming.\nB - Autopilot is a possible, great, and in terms of development time very fast solution. But pay attention, we are asked to compare two specific algorithms (custom algorithm and built-in image classification algorithm) and not run Autopilot which will evaluate on its own list of algorithms which one is better/worse. We want to compare our two algorithms!\nC - Manually run multiple jobs and log experiments into SageMaker Experiments for custom algorithm and then compare with AMT Job for built-in algorithm seems like a lot of redundant work compared to solution D."}],"upvote_count":"2","timestamp":"1682354100.0"},{"timestamp":"1680445320.0","comments":[{"poster":"Mllb","upvote_count":"1","timestamp":"1680445320.0","comment_id":"858912","content":"Autopilot can work with images","comments":[{"timestamp":"1680445380.0","comment_id":"858913","poster":"Mllb","content":"Sorry, can't","upvote_count":"1"}]}],"poster":"Mllb","upvote_count":"2","content":"Selected Answer: D\nOption D is the most effective in time because applies automatic model tuning (AMT) for all (built-in and custom)","comment_id":"858911"},{"content":"Selected Answer: C\nImage classification is not a supported problem type. https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-datasets-problem-types.html\nC should be the answer as Experiments can be triggered from Notebook and support HPO","poster":"pan_b","comment_id":"837769","upvote_count":"2","timestamp":"1678696560.0"},{"timestamp":"1678225620.0","comment_id":"832316","content":"Selected Answer: C\nIt's C, even if you can transform the images to parquet or csv, it's will take some time turning this process more slowly than just use the other options.","poster":"Valcilio","upvote_count":"1"},{"comment_id":"826199","content":"Selected Answer: C\nAutopilot used for picking model for you, this is comparing built in vs custom, it is experiment job","poster":"Chelseajcole","upvote_count":"2","timestamp":"1677697500.0"},{"timestamp":"1677207900.0","poster":"Siyuan_Zhu","upvote_count":"2","comment_id":"820062","content":"Selected Answer: B\nB\nYou can easily convert image files to parquet files to be trained. I think B is correct.\nC -- Does not mention the tuning and comparison of the built-in model, therefore it does not meet the requirements"},{"upvote_count":"3","comment_id":"814696","poster":"AjoseO","timestamp":"1676853660.0","comments":[{"content":"Autopilot doesn't support image data, only csv and parquet. So can't be B. See https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html","poster":"wolfsong","comment_id":"816742","timestamp":"1676993520.0","upvote_count":"1"}],"content":"Selected Answer: B\nB is the best option. \n\nSageMaker Autopilot provides a fully managed automated machine learning solution that automates the end-to-end machine learning process. It automatically tunes the model's hyperparameters and creates candidate models that match the given objective metric. In this case, the ML specialist can use Autopilot to tune the model of the custom container image and then use the automatic model tuning capability of SageMaker to tune the model of the built-in image classification algorithm. They can then compare the objective metric values of the resulting models of the SageMaker Autopilot AutoML job and the automatic model tuning job and select the model with the best objective metric value. \n\nThis approach is likely to be the least time-consuming because it allows for parallelized experimentation and optimization across two models."},{"timestamp":"1676037600.0","content":"auto pilot is for someone who is not familiar with ML to use this tool for gaining insights into their problem; this service will automatically choose the solution i.e., ML algorithm to solve the problem.\nIn this case however, the team has already trained a model that too with a custom container; these folks are experts in what they do and just look for HPO, another term for HPO is automatic model tuning https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html\n\nc","comments":[{"timestamp":"1676596860.0","poster":"drcok87","upvote_count":"4","comment_id":"811277","content":"Refer to the phrase \"You can use SageMaker AMT with built-in algorithms, custom algorithms, or SageMaker pre-built containers for machine learning frameworks.\" on https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html\n\nI knew its Automatic Model Tuning for the question, but seems like this can be used for both built-in and custom algorithm/containers. \n\nI'm changing my answer to D"}],"upvote_count":"2","comment_id":"804385","poster":"drcok87"},{"comment_id":"748199","timestamp":"1671294000.0","upvote_count":"4","content":"The answer is \"c\" https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html Amazon SageMaker Autopilot doesn't support Image Classification problem. It supports only binary classification or regression. https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-datasets-problem-types.html","poster":"BoroJohn"},{"content":"I think the answer is C.","timestamp":"1669738620.0","upvote_count":"4","comments":[{"upvote_count":"2","poster":"dunhill","content":"Per the link that VinceCar provided, B might be right. I am not very sure, but I agree that view point.","timestamp":"1670347980.0","comment_id":"737057"}],"poster":"dunhill","comment_id":"730613"}],"choices":{"C":"Use SageMaker Experiments to run and manage multiple training jobs and tune the model of the custom container image. Use the automatic model tuning capability of SageMaker to tune the model of the built-in image classification algorithm. Select the model with the best objective metric value.","D":"Use the automatic model tuning capability of SageMaker to tune the models of the custom container image and the built-in image classification algorithm at the same time. Select the model with the best objective metric value.","B":"Use SageMaker Autopilot to tune the model of the custom container image. Use the automatic model tuning capability of SageMaker with early stopping enabled to tune the model of the built-in image classification algorithm. Compare the objective metric values of the resulting models of the SageMaker AutopilotAutoML job and the automatic model tuning job. Select the model with the best objective metric value.","A":"Prepare a custom HPO script that runs multiple training jobs in SageMaker Studio in local mode to tune the model of the custom container image. Use the automatic model tuning capability of SageMaker with early stopping enabled to tune the model of the built-in image classification algorithm. Select the model with the best objective metric value."},"answers_community":["C (39%)","D (37%)","B (24%)"],"timestamp":"2022-11-29 17:17:00","question_images":[],"isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/89266-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"answer_description":"","unix_timestamp":1669738620,"answer":"C","question_text":"A machine learning (ML) specialist has prepared and used a custom container image with Amazon SageMaker to train an image classification model. The ML specialist is performing hyperparameter optimization (HPO) with this custom container image to produce a higher quality image classifier.\n\nThe ML specialist needs to determine whether HPO with the SageMaker built-in image classification algorithm will produce a better model than the model produced by HPO with the custom container image. All ML experiments and HPO jobs must be invoked from scripts inside SageMaker Studio notebooks.\n\nHow can the ML specialist meet these requirements in the LEAST amount of time?","topic":"1","answer_ET":"C"},{"id":"KqMa49v6YRbraSWFZkwK","url":"https://www.examtopics.com/discussions/amazon/view/89069-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A company wants to deliver digital car management services to its customers. The company plans to analyze data to predict the likelihood of users changing cars. The company has 10 TB of data that is stored in an Amazon Redshift cluster. The company's data engineering team is using Amazon SageMaker Studio for data analysis and model development. Only a subset of the data is relevant for developing the machine learning models. The data engineering team needs a secure and cost-effective way to export the data to a data repository in Amazon S3 for model development.\n\nWhich solutions will meet these requirements? (Choose two.)","answer_description":"","answer":"CE","answer_ET":"CE","topic":"1","answer_images":[],"answers_community":["CE (81%)","AE (19%)"],"unix_timestamp":1669639920,"exam_id":26,"choices":{"A":"Launch multiple medium-sized instances in a distributed SageMaker Processing job. Use the prebuilt Docker images for Apache Spark to query and plot the relevant data and to export the relevant data from Amazon Redshift to Amazon S3.","E":"Use SageMaker Data Wrangler to query and plot the relevant data and to export the relevant data from Amazon Redshift to Amazon S3.","B":"Launch multiple medium-sized notebook instances with a PySpark kernel in distributed mode. Download the data from Amazon Redshift to the notebook cluster. Query and plot the relevant data. Export the relevant data from the notebook cluster to Amazon S3.","C":"Use AWS Secrets Manager to store the Amazon Redshift credentials. From a SageMaker Studio notebook, use the stored credentials to connect to Amazon Redshift with a Python adapter. Use the Python client to query the relevant data and to export the relevant data from Amazon Redshift to Amazon S3.","D":"Use AWS Secrets Manager to store the Amazon Redshift credentials. Launch a SageMaker extra-large notebook instance with block storage that is slightly larger than 10 TB. Use the stored credentials to connect to Amazon Redshift with a Python adapter. Download, query, and plot the relevant data. Export the relevant data from the local notebook drive to Amazon S3."},"discussion":[{"timestamp":"1675410660.0","upvote_count":"7","poster":"solution123","content":"Selected Answer: CE\nCE\nOption A: Launching multiple medium-sized instances in a distributed SageMaker Processing job and using the prebuilt Docker images for Apache Spark to query and plot the relevant data is a possible solution, but it may not be the most cost-effective solution as it requires spinning up multiple instances.\nOption B: Launching multiple medium-sized notebook instances with a PySpark kernel in distributed mode is another solution, but it may not be the most secure solution as the data would be stored on the instances and not in a centralized data repository.\nOption D: Using AWS Secrets Manager to store the Amazon Redshift credentials and launching a SageMaker extra-large notebook instance is a solution, but the block storage requirement that is slightly larger than 10 TB could be costly and may not be necessary.","comment_id":"796849"},{"poster":"VinceCar","timestamp":"1669639920.0","upvote_count":"5","comment_id":"729097","content":"Selected Answer: CE\nC and E. No secure control is in option A."},{"content":"Selected Answer: CE\nOption A can do it as well but could be expensive and not as easy as option E","comments":[{"poster":"MultiCloudIronMan","timestamp":"1730061060.0","content":"Changed my mind to AC because Data Wrangler may struggle with 10TB. By using distributed SageMaker Processing jobs with Apache Spark and securely managing credentials with AWS Secrets Manager, the data engineering team can efficiently and securely export the relevant data from Amazon Redshift to Amazon S3","comment_id":"1303737","upvote_count":"1"}],"comment_id":"1301060","upvote_count":"3","timestamp":"1729520820.0","poster":"MultiCloudIronMan"},{"upvote_count":"4","content":"Selected Answer: AE\nAs soon as I see, Download andpython client, I am worried about speed and efficiency. So I would say A and E","timestamp":"1701193140.0","poster":"endeesa","comment_id":"1082787"},{"content":"Selected Answer: CE\nA. NO - SageMaker Processing job is a self-contained feature using the sagemaker.processing API; it does not rely on invoking Spark directly \nB. NO - you want to identify the relevant slice of data without having to download everything first\nC. YES - minimize data movement \nD. NO - you want to identify the relevant slice of data without having to download everything first\nE. YES - built-in tool specifically designed for that use case","comment_id":"1004756","poster":"loict","timestamp":"1694434440.0","upvote_count":"1"},{"poster":"Mickey321","comment_id":"984606","timestamp":"1692371220.0","upvote_count":"1","content":"Selected Answer: CE\nE for sure but was a bit confused on A or C but based on the link would go for C \n https://aws.amazon.com/blogs/big-data/using-the-amazon-redshift-data-api-to-interact-from-an-amazon-sagemaker-jupyter-notebook/"},{"content":"e is obvious choice:\nc https://aws.amazon.com/blogs/big-data/using-the-amazon-redshift-data-api-to-interact-from-an-amazon-sagemaker-jupyter-notebook/","poster":"drcok87","upvote_count":"2","comment_id":"804403","timestamp":"1676038560.0"},{"poster":"BoroJohn","content":"C & E seems right - https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html","comment_id":"745547","upvote_count":"3","timestamp":"1671063000.0"}],"question_id":120,"question_images":[],"timestamp":"2022-11-28 13:52:00","isMC":true}],"exam":{"provider":"Amazon","isBeta":false,"numberOfQuestions":369,"isMCOnly":false,"id":26,"isImplemented":true,"name":"AWS Certified Machine Learning - Specialty","lastUpdated":"11 Apr 2025"},"currentPage":24},"__N_SSP":true}