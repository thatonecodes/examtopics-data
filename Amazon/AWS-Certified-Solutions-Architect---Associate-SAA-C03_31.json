{"pageProps":{"questions":[{"id":"73f0iYx0ZGX6c5bsR2x2","question_images":[],"answers_community":["B (91%)","7%"],"answer_ET":"B","answer_images":[],"unix_timestamp":1673616360,"question_id":151,"question_text":"A company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects and for all objects that are added to the S3 bucket in the future.\n\nWhich solution will meet these requirements with the LEAST amount of effort?","choices":{"A":"Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all existing objects to temporary local storage. Upload the objects to the new S3 bucket.","C":"Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on the S3 bucket to use server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Turn on versioning for the S3 bucket.","D":"Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket’s objects. Sort by the encryption field. Select each unencrypted object. Use the Modify button to apply default encryption settings to every unencrypted object in the S3 bucket.","B":"Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects."},"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/95040-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"discussion":[{"timestamp":"1689365220.0","comments":[{"poster":"Parsons","upvote_count":"10","content":"Useful ref link: https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/","timestamp":"1689365640.0","comment_id":"775970"}],"upvote_count":"17","content":"Selected Answer: B\nStep 1: S3 inventory to get object list\nStep 2 (If needed): Use S3 Select to filter\nStep 3: S3 object operations to encrypt the unencrypted objects.\n\nOn the going object use default encryption.","poster":"Parsons","comment_id":"775965"},{"timestamp":"1703761440.0","content":"Selected Answer: B\nBy enabling default encryption settings on the S3, all newly added objects will be automatically encrypted. To encrypt the existing objects, the S3 Inventory feature can be used to generate a list of unencrypted objects. Then, an S3 Batch Operations job can be executed to copy those objects while applying encryption.\n\nA. This solution involves creating a new S3 and manually downloading and uploading all existing objects. It requires significant effort and time to transfer millions of objects, making it a less efficient solution.\n\nC. While enabling SSE with AWS KMS is a valid approach to encrypt objects in an S3, it does not address the requirement of encrypting existing objects. It only applies encryption to new objects added to the bucket.\n\nD. Manually modifying each object in the S3 to apply default encryption settings is a labor-intensive and error-prone process. It would require individually selecting and modifying each unencrypted object, which is impractical for a large number of objects.","comment_id":"936385","poster":"cookieMr","upvote_count":"15"},{"content":"Selected Answer: B\nOption B uses AWS tools like S3 Default Encryption, Inventory, and Batch Operations to enable encryption for both existing and future objects with minimal effort and automation, making it the best solution.\n\nWhy not the other options?\nA. Create a new S3 bucket and migrate objects:\nThis involves downloading and re-uploading all objects, which is highly manual, time-consuming, and prone to errors. It also incurs additional data transfer costs.\n\nC. Use SSE-KMS and turn on versioning:\nWhile enabling SSE-KMS for future objects is valid, this does not encrypt the existing objects unless explicitly copied or rewritten, requiring additional manual effort.\n\nD. Browse and modify objects manually:\nManually selecting and encrypting each object is impractical for a bucket with millions of objects, as it is labor-intensive and time-consuming.","timestamp":"1737923100.0","upvote_count":"1","poster":"iamroyalty_k","comment_id":"1347100"},{"comment_id":"1220310","timestamp":"1732809300.0","poster":"lofzee","content":"Selected Answer: B\nto be fair all these options take a hell a lot of work to do but i think the least amount of effort is B.\nhttps://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/","upvote_count":"4"},{"poster":"pentium75","comment_id":"1106836","upvote_count":"2","content":"Selected Answer: B\nA - Extreme amount of effort\nB - Should work\nC - SSE-KMS is not \"least amount of effort\" compared to SSE-S3; Turning versioning is not required to achieve the result but on the contrary, it will cause the non-encrypted files to remain as old versions even if you encrypt them in the future.\nD - Even more effort as A","comments":[{"timestamp":"1721577120.0","comment_id":"1128025","content":"B doesnt look like least amount of effort","poster":"foha2012","upvote_count":"1"},{"content":".csv for millions of objects ?? C looks simpler.","poster":"foha2012","upvote_count":"2","comment_id":"1128024","timestamp":"1721576820.0"}],"timestamp":"1719487200.0"},{"timestamp":"1694703360.0","poster":"CapJackSparrow","content":"Selected Answer: B\nB...\n\nhttps://catalog.us-east-1.prod.workshops.aws/workshops/05f16f1a-0bbf-45a7-a304-4fcd7fca3d1f/en-US/s3-track/module-2\n\n\nYou're welcome","upvote_count":"4","comment_id":"839054"},{"content":"Selected Answer: B\nAmazon S3 now configures default encryption on all existing unencrypted buckets to apply server-side encryption with S3 managed keys (SSE-S3) as the base level of encryption for new objects uploaded to these buckets. Objects that are already in an existing unencrypted bucket won't be automatically encrypted.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/default-encryption-faq.html","timestamp":"1692030120.0","poster":"bdp123","upvote_count":"4","comment_id":"808673"},{"comment_id":"808149","poster":"Yelizaveta","upvote_count":"2","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-copy-example-bucket-key.html","timestamp":"1691987700.0"},{"poster":"aakashkumar1999","timestamp":"1691127060.0","upvote_count":"2","comment_id":"797700","content":"Selected Answer: B\nB is the correct answer"},{"poster":"Val182","comment_id":"796928","upvote_count":"2","timestamp":"1691047200.0","content":"Selected Answer: B\nB 100%\nhttps://spin.atomicobject.com/2020/09/15/aws-s3-encrypt-existing-objects/"},{"poster":"LuckyAro","upvote_count":"1","comments":[{"timestamp":"1719486780.0","comment_id":"1106820","upvote_count":"2","poster":"pentium75","content":"Downloading and uploading \"millions of objects\" is surely not \"least amount of effort\", thus does not meet the requirements."}],"comment_id":"791200","timestamp":"1690579920.0","content":"Selected Answer: A\nWhy is no one discussing A ? I think A can also achieve the required results. B is the most appropriate answer though."},{"comment_id":"785642","comments":[{"content":"Versioning:\n\nWhen you overwrite an S3 object, it results in a new object version in the bucket. However, this will not remove the old unencrypted versions of the object. If you do not delete the old version of your newly encrypted objects, you will be charged for the storage of both versions of the objects. \n\nS3 Lifecycle \n\nIf you want to remove these unencrypted versions, use S3 Lifecycle to expire previous versions of objects. When you add a Lifecycle configuration to a bucket, the configuration rules apply to both existing objects and objects added later. C is missing this step, which I believe is what makes B the better choice. B includes the functionality of encrypting the old unencrypted objects via Batch Operations, whereas, Versioning does not address the old unencrypted objects.","timestamp":"1690124880.0","comment_id":"785644","poster":"Training4aBetterLife","upvote_count":"2"}],"upvote_count":"4","poster":"Training4aBetterLife","content":"Selected Answer: B\nS3 provides a single control to automatically encrypt all new objects in a bucket with SSE-S3 or SSE-KMS. Unfortunately, these controls only affect new objects. If your bucket already contains millions of unencrypted objects, then turning on automatic encryption does not make your bucket secure as the unencrypted objects remain.\n\nFor S3 buckets with a large number of objects (millions to billions), use Amazon S3 Inventory to get a list of the unencrypted objects, and Amazon S3 Batch Operations to encrypt the large number of old, unencrypted files.","timestamp":"1690124880.0"},{"comments":[{"comment_id":"785637","poster":"Training4aBetterLife","timestamp":"1690124760.0","upvote_count":"1","content":"Versioning:\n\nWhen you overwrite an S3 object, it results in a new object version in the bucket. However, this will not remove the old unencrypted versions of the object. If you do not delete the old version of your newly encrypted objects, you will be charged for the storage of both versions of the objects. \n\nS3 Lifecycle \n\nIf you want to remove these unencrypted versions, use S3 Lifecycle to expire previous versions of objects. When you add a Lifecycle configuration to a bucket, the configuration rules apply to both existing objects and objects added later. C is missing this step, which I believe is what makes B the better choice. B includes the functionality of encrypting the old unencrypted objects via Batch Operations, whereas, Versioning does not address the old unencrypted objects.","comments":[{"content":"Please remove duplicate response as I was meaning to submit a voting comment.","upvote_count":"1","poster":"Training4aBetterLife","timestamp":"1690124940.0","comment_id":"785646"}]}],"content":"S3 provides a single control to automatically encrypt all new objects in a bucket with SSE-S3 or SSE-KMS. Unfortunately, these controls only affect new objects. If your bucket already contains millions of unencrypted objects, then turning on automatic encryption does not make your bucket secure as the unencrypted objects remain. \n\nFor S3 buckets with a large number of objects (millions to billions), use Amazon S3 Inventory to get a list of the unencrypted objects, and Amazon S3 Batch Operations to encrypt the large number of old, unencrypted files.","poster":"Training4aBetterLife","comment_id":"785636","upvote_count":"2","timestamp":"1690124760.0"},{"poster":"John_Zhuang","comment_id":"783178","timestamp":"1689928680.0","content":"Selected Answer: B\nC is wrong. Even though you turn on the SSE-KMS with a new key, the existing objects are still yet to be encrypted. They still need to be manually encrypted by AWS batch","upvote_count":"3","comments":[{"comment_id":"1106827","poster":"pentium75","timestamp":"1719486960.0","content":"And as in C you \"turn on versioning\", the old, unencrypted objects will be kept.","upvote_count":"2"}]},{"poster":"LuckyAro","timestamp":"1689663540.0","content":"Selected Answer: B\nhttps://spin.atomicobject.com/2020/09/15/aws-s3-encrypt-existing-objects/","comment_id":"779777","upvote_count":"2"},{"comments":[{"upvote_count":"1","content":"Why? This does not include a step to encrypt existing objects, and by turning on versioning you will keep the unencrypted versions forever.","timestamp":"1719487020.0","poster":"pentium75","comment_id":"1106829"}],"content":"Selected Answer: C\nC is the answer","comment_id":"777806","poster":"Aninina","upvote_count":"2","timestamp":"1689512100.0"},{"upvote_count":"2","comment_id":"777724","timestamp":"1689507960.0","content":"Selected Answer: B\nAgree with Parsons","poster":"techhb"},{"timestamp":"1689495360.0","poster":"Lilibell","comments":[{"timestamp":"1719486840.0","upvote_count":"1","content":"Huh? \"future encryption of objects = versioning\"??????","poster":"pentium75","comment_id":"1106822"}],"upvote_count":"1","content":"the answer is C\nalso, the questions require future encryption of the objects is the S3 bucket = VERSIONING","comment_id":"777538"},{"comment_id":"776301","poster":"swolfgang","comments":[],"upvote_count":"1","timestamp":"1689404460.0","content":"Selected Answer: C\ncould not open default encripton for exist bucket,so need to use KMS"},{"timestamp":"1689347760.0","upvote_count":"1","comment_id":"775769","poster":"mhmt4438","content":"Selected Answer: C\nThe correct answer is C"},{"timestamp":"1689247560.0","poster":"Morinator","upvote_count":"1","content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/93042-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"774528"}],"exam_id":31,"timestamp":"2023-01-13 14:26:00","answer_description":"","answer":"B"},{"id":"CquXDuscnJ6MByzo9kWk","choices":{"A":"Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.","B":"Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.","D":"Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region.","C":"Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot."},"answer_images":[],"answer":"A","question_id":152,"answer_description":"","answer_ET":"A","exam_id":31,"question_text":"A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy.\n\nWhat should a solutions architect do to meet these requirements?","topic":"1","discussion":[{"timestamp":"1673735040.0","comments":[{"comment_id":"775974","content":"Ref link: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html","poster":"Parsons","upvote_count":"6","timestamp":"1673735040.0"}],"upvote_count":"31","content":"Selected Answer: A\nA is correct.\n- \"The solution does not need to handle the load when the primary infrastructure is healthy.\" => Should use Route 53 Active-Passive ==> Exclude B, C\n- D is incorrect because \"Create an Aurora second primary instance in the second Region.\", we need to create an Aurora Replica enough.","comment_id":"775973","poster":"Parsons"},{"timestamp":"1689768240.0","content":"Selected Answer: A\nAnything that is not instant recovery is active - passive.\nIn active -passive we have :\n1. Aws Backup(least op overhead) - RTO/RPO = hours\n2. Pilot Light ( Basic Infra is already deployed, but needs to be fully implemented) -RTO/RPO = 10's of minutes.\n3. Warm Standby- (Basic infra + runs small loads ( might need to add auto scaling) -RTO/RPO= minutes \n4. ( ACTIVE -ACTIVE ) : Multi AZ option : instant \n\nhere we can tolerate 30 mins \nhence B,D are incorrect. AWS backup is in hours, hence D is incorrect . \ntherefore A","poster":"diabloexodia","comment_id":"956590","comments":[{"poster":"pentium75","timestamp":"1704618180.0","content":"A does not create the infrastructure in the DR region though.","upvote_count":"2","comment_id":"1115661"}],"upvote_count":"23"},{"content":"Selected Answer: A\nThe Aurora Global Database would be ideal for a production system requiring near-instantaneous failover, however, it was not in the options. Since the option D may not meet the 30-minute tolerance, I choose option A- Aurora Replica-based disaster recovery although replication is more designed for reading traffic, it can still be used for failover based on this doc https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html.","poster":"FlyingHawk","timestamp":"1732599840.0","comment_id":"1317923","upvote_count":"2"},{"content":"Selected Answer: D\nD is right","comment_id":"1271491","timestamp":"1724464500.0","poster":"ensbrvsnss","upvote_count":"1"},{"content":"Selected Answer: D\nconfused with A and D but D looks more promising when it says doesn't need to handle the load when primary infrastructure is healthy","comment_id":"1244677","timestamp":"1720499160.0","poster":"jatric","upvote_count":"2"},{"comment_id":"1220315","content":"Selected Answer: D\nI went for D as the wording of A is weird....\nD seems most plausible","poster":"lofzee","timestamp":"1716905040.0","upvote_count":"2"},{"comment_id":"1215579","upvote_count":"1","poster":"Jazz888","timestamp":"1716367200.0","content":"A\nFor those you are choosing D, I have a question for you. How do you guarantee the provisioning of resources will take less than 30 min through AWS Backup?"},{"content":"By excluding other options you can choose A but this option is incomplete as it doesn't mention deploying/recovering the application in secondary region.","poster":"ManikRoy","upvote_count":"1","comment_id":"1207011","timestamp":"1714933560.0"},{"comment_id":"1148095","upvote_count":"2","poster":"MrPCarrot","content":"A is perfect - Active-Passive Failover: Use this failover configuration when you want a primary group of resources to be available the majority of the time and you want a secondary group of resources to be on standby in case all of the primary resources become unavailable.","timestamp":"1707740760.0"},{"content":"A is perfect","poster":"MrPCarrot","upvote_count":"1","comment_id":"1148086","timestamp":"1707740400.0"},{"upvote_count":"2","poster":"farnamjam","comment_id":"1126507","content":"Selected Answer: A\nHere's why the other options aren't as suitable:\n\nB. Active-active failover: Incur higher costs due to running both infrastructures simultaneously and introduces complexity in managing traffic distribution.\nC. Restoring from snapshot: Could take longer than 30 minutes to recover, exceeding the company's downtime tolerance.\nD. AWS Backup: Dependent on backup and restore times, potentially exceeding the 30-minute recovery window.","timestamp":"1705648920.0"},{"content":"the question doesnt state Aurora MySQL but you can set up cross-region replicas for Aurora MySQL, but not PostgreSQL. the question only says Amazon Aurora, so its left a bit open as Aurora is either MySql or PostgreSQL. this is without using Global Database.\ntbh i think this question and answers are well off... it is a dump after all.","poster":"lofzee","comment_id":"1220320","timestamp":"1716905220.0","upvote_count":"2"},{"poster":"awsgeek75","content":"Selected Answer: D\nI agreed with D as the requirements of 30 min downtime and potential data loss and no load consideration when primary instance is healthy. It makes D more feasible than A. Aurora-Replica is normally used for active-active failovers. Be frugal!","upvote_count":"4","timestamp":"1704120720.0","comment_id":"1111277"},{"comments":[{"timestamp":"1705495680.0","comment_id":"1124970","upvote_count":"2","content":"I really hope the language is better in the exams. Option A is like \"do what it takes it to make the solution work\".... well then by default it is right until the second part makes it wrong. smh!","poster":"awsgeek75"}],"poster":"Jeffab","comment_id":"1055110","timestamp":"1698381720.0","upvote_count":"13","content":"If this is the quality of the questions in exam, then we are all screwed! I don't think any options are correct. A proabably the most correct, but a big flaw. \"Deploy the application with the required infrastructure elements in place.\" Deploy to where? Fair enough if you assume another region/AZ, but it's not stated and only Aurora replica is mentioned, not the Web/app servers etc."},{"upvote_count":"6","content":"Selected Answer: A\n'Can tolerate up to 30 minutes of downtime and potential data loss' rules out any option with 'active-active'. Leaves D and A. D is convoluted. Leaving A.","poster":"TariqKipkemei","comment_id":"1011962","timestamp":"1695189960.0"},{"timestamp":"1687943280.0","content":"Selected Answer: A\nA. involves deploying the application and infrastructure elements in the primary Region. An Aurora Replica is created in a second Region to serve as the standby database. Route 53 is configured with active-passive failover, directing traffic to the primary Region by default. In the event of a disaster, Route 53 can automatically redirect traffic to the standby Region, minimizing downtime. Data loss may occur up to the point of the last replication to the standby Region, which can be within the defined tolerance of 30 minutes.\n\nOption B, is not necessary in this case as the solution does not need to handle the load when the primary infrastructure is healthy, and it may involve higher complexity and costs.\n\nOption C, may introduce additional complexity and potential data loss, as the standby database might not be up-to-date with the primary database.\n\nOption D, may be suitable for backup and recovery scenarios but may not provide the required failover and downtime tolerance specified in the requirements.","comment_id":"936393","poster":"cookieMr","upvote_count":"3"},{"timestamp":"1685944620.0","content":"Selected Answer: D\nI vote D, because option A is not highly available. In option A, you can't configure active-passive failover because you haven't created a backup infrastructure.","comment_id":"915099","poster":"antropaws","upvote_count":"2"},{"comment_id":"856499","content":"Selected Answer: A\nIt is a cross region DR strategy. You need a read replica and Application in another region to have a realistic DR option. The read replica will take few minutes to to promoted/Active and the application is available. Option D lacks clarity on application and Backups can take time to restore.","poster":"kraken21","timestamp":"1680224460.0","upvote_count":"3"},{"timestamp":"1676358720.0","upvote_count":"2","comment_id":"808167","poster":"Yelizaveta","content":"Selected Answer: A\nDepending on the Regions involved and the amount of data to be copied, a cross-Region snapshot copy can take hours to complete and will be a factor to consider for the RPO requirements. You need to take this into account when you estimate the RPO of this DR strategy.\n\nIf you have strict RTO and RPO requirements, you should consider a different DR strategy, such as Amazon Aurora Global Database .\nhttps://aws.amazon.com/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/"},{"comments":[{"poster":"ChrisG1454","upvote_count":"2","content":"Consider Answer B.\nIt is suggesting a Pilot Light DR strategy.\nhttps://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html","timestamp":"1676624280.0","comments":[{"timestamp":"1677808020.0","comments":[{"poster":"ChrisG1454","content":"The key sentence is \n\"a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss\"\nTake a look at the visualization in the URL provided. Pilot light = 30 minutes.","upvote_count":"2","timestamp":"1677897180.0","comment_id":"828517"}],"content":"I will Vote B and i initially thought it Pilot Light however after 2nd read, it seem it more like warm standby. Option D looks more like back up and Restore strategy and it will take more than 30 minutes to get it done. C is wrong, snapshot takes longer time to restore","upvote_count":"1","poster":"Bofi","comment_id":"827591"}],"comment_id":"811661"}],"comment_id":"801968","poster":"JiyuKim","timestamp":"1675857300.0","upvote_count":"4","content":"Selected Answer: D\nThe solution does not need to handle the load when the primary infrastructure is healthy. -> Amazon Route 53 active-passive failover -> A,D\nThe company can tolerate up to 30 minutes of downtime and potential data loss -> backup -> D\nyou don't have to use read replicas if you can tolerate downtime and data loss."},{"comment_id":"797702","poster":"aakashkumar1999","content":"Selected Answer: D\nI am confused within A and D but I think D is the answer because this seems to be a cost related problem, a replica is kind of a standby and you can promote to be the main db anytime without any much downtime, but here it says it can withstand 30 mins of downtime so we can just keep a backup of the instance and then create a DB whenever required from the backup, hence less cost","timestamp":"1675496100.0","upvote_count":"10"},{"upvote_count":"1","comment_id":"777808","poster":"Aninina","content":"Selected Answer: A\nA is correct","timestamp":"1673880960.0"},{"upvote_count":"1","content":"Selected Answer: A\naaaaaaaa","timestamp":"1673738940.0","poster":"gunmin","comment_id":"776022"},{"timestamp":"1673717220.0","poster":"mhmt4438","comment_id":"775779","upvote_count":"1","content":"Selected Answer: A\nanswer is d"},{"upvote_count":"1","poster":"alanp","comment_id":"774593","content":"Ans is A","timestamp":"1673621340.0"},{"timestamp":"1673607660.0","upvote_count":"1","content":"Selected Answer: A\nA is correct answer.\nhttps://www.examtopics.com/discussions/amazon/view/81439-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"bamishr","comment_id":"774356"},{"content":"Selected Answer: D\nhttps://www.examtopics.com/discussions/amazon/view/81439-exam-aws-certified-solutions-architect-associate-saa-c02/","comment_id":"774353","upvote_count":"1","poster":"bamishr","timestamp":"1673607600.0"}],"question_images":[],"timestamp":"2023-01-13 00:00:00","answers_community":["A (71%)","D (29%)"],"url":"https://www.examtopics.com/discussions/amazon/view/95015-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1673564400,"isMC":true},{"id":"OpakEAvBD9Qoq3vCR5Tk","isMC":true,"exam_id":31,"unix_timestamp":1673621400,"url":"https://www.examtopics.com/discussions/amazon/view/95056-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-01-13 15:50:00","answer":"AE","answers_community":["AE (74%)","14%","8%"],"answer_images":[],"answer_description":"","topic":"1","question_id":153,"choices":{"B":"Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.","C":"Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.","A":"Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.","D":"Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.","E":"Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0."},"discussion":[{"content":"Selected Answer: AE\nA, E is perfect the combination. To be more precise, We should add outbound with \"outbound TCP port 32768-65535 to destination 0.0.0.0/0.\" as an ephemeral port due to the stateless of NACL.","poster":"Parsons","comments":[{"comment_id":"971235","timestamp":"1691075580.0","content":"What is the main reason that you are using the TCP port 32768-65535> In the question, it doesn't ask you any requirement about it.","upvote_count":"5","poster":"oguzbeliren"},{"content":"i Think AD because acl is stateless we must open the port outbound and inbound , in option c we only open 443 on inbound","timestamp":"1694895180.0","poster":"MohammadTofic8787","comment_id":"1009291","upvote_count":"3"},{"comment_id":"1009293","content":"i Think AD because acl is stateless we must open the port outbound and inbound , in option E we only open 443 on inbound","upvote_count":"3","poster":"MohammadTofic8787","timestamp":"1694895360.0"}],"comment_id":"775975","upvote_count":"20","timestamp":"1673735280.0"},{"content":"Selected Answer: E\nFor me it's grammatically unclear whether \"port 443\" and \"port 32768-65535\" in answers D and E are referring to the source or destination ports of the outbound traffic. If source ports then it would be D. If destination ports (which seems more likely) then it's E.\n\n\"On Windows, the ephemeral port range is usually from 49152 to 65535.\nOn Linux, it is often from 32768 to 61000.\"\n\nThus 32768-65535 would cover both Windows and Linux.","timestamp":"1703684400.0","comment_id":"1106849","poster":"pentium75","upvote_count":"10"},{"comment_id":"1293418","poster":"Omariox","timestamp":"1728121740.0","content":"Selected Answer: AD\nOption A: Creating a security group that allows inbound traffic on TCP port 443 from all sources (0.0.0.0/0) ensures that the web server can accept incoming HTTPS requests.\n\nOption D: Updating the network ACL to allow inbound traffic on TCP port 443 from all sources (0.0.0.0/0) allows the requests to reach the EC2 instance. Additionally, it is necessary to allow outbound traffic on TCP port 443 to enable responses to clients, which is crucial for HTTPS communication.","upvote_count":"2"},{"content":"Higher priority NACL to allow inbound and outbound traffic on 443 with take the precedence over default blocked NACL","comment_id":"1285667","poster":"srinibas.velumuri","upvote_count":"1","timestamp":"1726652940.0"},{"upvote_count":"1","comment_id":"1253796","timestamp":"1721754360.0","poster":"ChinthaGurumurthi","content":"Selected Answer: AD\nAD\nHow can E be the answer. How can we assure that the port range is definitely from the given port range in the option E?"},{"upvote_count":"4","timestamp":"1716905460.0","poster":"lofzee","content":"Selected Answer: AE\nSecurity group only needs inbound rules.\nACL needs inbound and outbound.. Outbound traffic is going to be dynamic ports. Answer is A and E","comment_id":"1220322"},{"content":"AE \n\nSecurity group is a stateful resource and can understand to allow traffic from source 0.0.0.0/0 with port 443 but ACL is stateless so traffic that is allowed inside the network we must configure the same to go outside the network as well.","upvote_count":"2","poster":"sidharthwader","timestamp":"1709859960.0","comment_id":"1168472"},{"upvote_count":"5","poster":"awsgeek75","comment_id":"1111285","timestamp":"1704121260.0","content":"Selected Answer: AE\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-basics\n\"NACLs are stateless, which means that information about previously sent or received traffic is not saved. If, for example, you create a NACL rule to allow specific inbound traffic to a subnet, responses to that traffic are not automatically allowed. This is in contrast to how security groups work. Security groups are stateful, which means that information about previously sent or received traffic is saved. If, for example, a security group allows inbound traffic to an EC2 instance, responses are automatically allowed regardless of outbound security group rules.\"\nA fulfils the security group requirement\nE is the only option that explicitly covers outbound traffic and ports. \nD covers outbound destination but given that all traffic is blocked (as per the question) this won't work"},{"poster":"[Removed]","upvote_count":"1","comments":[{"content":"But NACLs are stateless.\"The default network ACL has been modified to block all traffic\"; if you don't allow any outbound traffic then the web server won't be able to reply to clients.","timestamp":"1703683920.0","comment_id":"1106842","upvote_count":"2","poster":"pentium75"}],"timestamp":"1700353140.0","content":"Selected Answer: AC\nFor typical web server scenarios, such as serving content over HTTPS (port 443), you generally do not need to explicitly open outbound ports in the network ACL (NACL) for the return traffic.","comment_id":"1074368"},{"content":"Selected Answer: AE\nACL is stateless. you have to define both inbound and outbound rules.","poster":"TariqKipkemei","upvote_count":"3","timestamp":"1695190200.0","comment_id":"1011966"},{"upvote_count":"1","content":"i Think AD because acl is stateless we must open the port outbound and inbound , in option D we only open 443 on inbound","comment_id":"1009294","timestamp":"1694895420.0","poster":"MohammadTofic8787","comments":[{"poster":"MohammadTofic8787","content":"please admin delete this , sorry","timestamp":"1694903340.0","upvote_count":"1","comment_id":"1009371"}]},{"poster":"Guru4Cloud","timestamp":"1694450460.0","content":"Selected Answer: AE\nA, E is perfect the combination. To be more precise, We should add outbound with \"outbound TCP port 32768-65535 to destination 0.0.0.0/0.\" as an ephemeral port due to the stateless of NACL.","comment_id":"1004994","upvote_count":"4"},{"upvote_count":"2","content":"Selected Answer: AE\nAE is the best answer here, but in reality, E is not good enough. Here, it says that the client chooses the ephemeral port, and it can start from 1024. Only Linux clients have the range starting at 32768 https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports Unless the destination advertises the ephemeral ports, which I don't think is the case","comments":[],"poster":"beginnercloud","timestamp":"1693203660.0","comment_id":"991850"},{"timestamp":"1690013160.0","poster":"Thornessen","content":"Selected Answer: AE\nAE is the best answer here, but in reality, E is not good enough.\n\nHere, it says that the client chooses the ephemeral port, and it can start from 1024. Only Linux clients have the range starting at 32768 https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports\n\nUnless the destination advertises the ephemeral ports, which I don't think is the case","upvote_count":"3","comment_id":"959343"},{"comment_id":"911726","upvote_count":"2","content":"32768-65535 ports Allows outbound IPv4 responses to clients on the internet (for example, serving webpages to people visiting the web servers in the subnet).","timestamp":"1685594760.0","poster":"Abrar2022"},{"poster":"WherecanIstart","upvote_count":"3","timestamp":"1678683000.0","comment_id":"837634","content":"Selected Answer: AE\nNACL blocks outgoing traffic since it is infact stateless..Option E allows outbound traffic from ephemeral ports going outside of the VPC back to the web."},{"upvote_count":"2","poster":"Brak","comment_id":"830737","content":"It can't be C, since the current NACL blocks all traffic, including outbound. Need to allow outbound traffic through the NACL.\nBut E is a bad answer, since ephemeral ports start at 1024, not 32768.","timestamp":"1678100400.0"},{"poster":"neosis91","comment_id":"802994","upvote_count":"5","content":"Selected Answer: AC\nA and C not E\nOption E states to allow incoming TCP ports on 443 and outgoing on 32768-65535 to all IP addresses (0.0.0.0/0). This option only allows outgoing ports and does not guarantee that incoming connections on 443 will be allowed. It does not meet the requirement of making the web server accessible on port 443 from anywhere. Therefore, option C which states to allow incoming TCP ports on 443 from all IP addresses is the best answer to meet the requirements.","comments":[{"poster":"slackbot","comment_id":"989972","upvote_count":"2","timestamp":"1692961080.0","content":"seems like either you did not read what you wrote \"Option E states to allow incoming TCP ports on 443 and outgoing on 32768-65535 to all IP addresses (0.0.0.0/0).\" (because first part of the sentence allows incoming 443) or you do not understand how ACLs work - they are STATELESS, which means, you need to allow both IN and OUT, not just IN like SGs which are stateful. if they were the same - what would be the purpose of the ACLs?"},{"timestamp":"1677114720.0","poster":"Deepak_k","upvote_count":"3","comment_id":"818618","content":"Answer : AE - Incoming traffic on port 443 but sever can use any port to reply back."},{"content":"It seems there are lots of questions that ask for minimum requirements, and often times adding 'things' to the solution are not correct. I am not sure about this question and I would pick C. E adds ambiguity. What if you only needed to open ports for Lambda? That would be a different set of ports. I think E adds some assumptions into the question. I think opening some ports for some assumptions and keeping ports closed for other assumptions is not correct. The best assumption is to assume they are asking how to open ports for 443","comment_id":"985765","timestamp":"1692534060.0","comments":[{"poster":"slackbot","upvote_count":"3","comment_id":"989973","content":"E still guarantees something will work. C definitely means - nothing will work, because you are not allowing egress traffic at all","timestamp":"1692961140.0"}],"upvote_count":"1","poster":"JoeGuan"}],"timestamp":"1675932180.0"},{"poster":"Aninina","comment_id":"777812","content":"Selected Answer: AE\nAE correct","timestamp":"1673881020.0","upvote_count":"4"},{"content":"Selected Answer: AE\nA & E , E as NACL is stateless.","timestamp":"1673877360.0","poster":"techhb","comment_id":"777732","upvote_count":"3"},{"poster":"AHUI","upvote_count":"2","content":"AE:\nhttps://www.examtopics.com/discussions/amazon/view/29767-exam-aws-certified-solutions-architect-associate-saa-c02/","timestamp":"1673719080.0","comment_id":"775797"},{"upvote_count":"2","comment_id":"775781","poster":"mhmt4438","content":"Selected Answer: AE\nhttps://www.examtopics.com/discussions/amazon/view/29767-exam-aws-certified-solutions-architect-associate-saa-c02/","timestamp":"1673717520.0"},{"timestamp":"1673688360.0","poster":"kbaruu","content":"Selected Answer: AE\nA E is correct","comment_id":"775218","upvote_count":"2"},{"timestamp":"1673621400.0","upvote_count":"2","content":"Ans AE","comment_id":"774594","poster":"alanp"}],"question_images":[],"answer_ET":"AE","question_text":"A company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443.\n\nWhich combination of steps will accomplish this task? (Choose two.)"},{"id":"1N3cfwUvMX4deo1LK0R1","isMC":true,"answer":"D","answer_description":"","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/95162-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"choices":{"A":"Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.","D":"Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.","B":"Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.","C":"Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning."},"answer_ET":"D","discussion":[{"poster":"Parsons","upvote_count":"38","content":"Selected Answer: D\nD is the correct answer.\n\n \"in-memory tasks\" => need the \"R\" EC2 instance type to archive memory optimization. So we are concerned about C & D. \nBecause EC2 instances don't have built-in memory metrics to CW by default. As a result, we have to install the CW agent to archive the purpose.","comment_id":"775983","timestamp":"1673735820.0"},{"poster":"Babba","content":"Selected Answer: D\nIt's D, EC2 do not provide by default memory metrics to CloudWatch and require the CloudWatch Agent to be installed on the monitored instances : https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-memory-metrics-ec2/","timestamp":"1673684460.0","comment_id":"775184","upvote_count":"13"},{"comment_id":"1357268","content":"Selected Answer: D\nAnswer is most likely \"D\" because of the following ...\n- T3 EC2 instances > low cost burstable general purpose instance type that provide a baseline level of CPU performance with the ability to burst CPU usage\n- M5 EC2 instance > M5 instances offer a balance of compute, memory, and networking resources for a broad range of workload.\n- R5 EC2 instances > Amazon EC2 R5 instances are the next generation of memory optimized instances for the Amazon Elastic Compute Cloud. R5 instances are well suited for memory intensive applications such as high-performance databases etc \n\nM5 EC2 instance is already in use & having performance issues so there is no point in provisioning a lower cost , general purpose instance type like T3 EC2.\nSo as per the given Answer option choice the best one is Answer D","poster":"surajkrishnamurthy","upvote_count":"1","timestamp":"1739715060.0"},{"timestamp":"1720499760.0","comment_id":"1244685","poster":"jatric","upvote_count":"3","content":"Selected Answer: A\nhow future capacity planning and just do verticall scalling will improve the performance. Question doesn't specify if these EC2 are behind auto scalling so it means they are not. out of all A seems more close to the solution"},{"poster":"a7md0","content":"Selected Answer: B\nB will reduce operational overhead and better solution than keep changing the family. Also, I don't think the exam will require you to remember instance families like M5 and R5","upvote_count":"2","comment_id":"1235354","timestamp":"1719055320.0"},{"upvote_count":"2","poster":"lofzee","comment_id":"1220324","content":"Selected Answer: D\nD .....","timestamp":"1716905520.0"},{"poster":"Guru4Cloud","timestamp":"1694450340.0","comment_id":"1004993","upvote_count":"7","content":"Selected Answer: D\nR5 instances are better optimized for the in-memory workload than M5.\nAuto Scaling alone doesn't handle stateful applications well, manual capacity adjustments would still be needed.\nCustom latency metrics give better visibility than built-in metrics for capacity planning."},{"poster":"cookieMr","comment_id":"936412","timestamp":"1687944000.0","upvote_count":"6","content":"Selected Answer: D\nBy replacing the M5 instances with R5 instances, which are optimized for memory-intensive workloads, the application can benefit from increased memory capacity and performance.\n\nIn addition, deploying the CloudWatch agent on the EC2 instances allows for the generation of custom application latency metrics, which can provide valuable insights into the application's performance.\n\nThis solution addresses the performance issues efficiently by leveraging the appropriate instance types and collecting custom application metrics for better monitoring and future capacity planning.\n\nA. Replacing with T3 instances may not provide enough memory capacity for in-memory tasks.\n\nB. Manually increasing the capacity of the ASG does not directly address the performance issues.\n\nC. Relying solely on built-in EC2 memory metrics may not provide enough granularity for optimizing in-memory tasks.\n\nThe most efficient solution is to modify the CloudFormation templates, replace with R5 instances, and deploy the CloudWatch agent for custom metrics."},{"poster":"Bmarodi","comment_id":"907133","timestamp":"1685084040.0","content":"Selected Answer: D\nOption D is the correct answer.","upvote_count":"2"},{"content":"will go for C","poster":"BABU97","upvote_count":"1","comment_id":"856693","timestamp":"1680246600.0"},{"content":"Selected Answer: D\nWould go with D","poster":"Aninina","upvote_count":"2","timestamp":"1673881080.0","comment_id":"777813"},{"poster":"mhmt4438","content":"Selected Answer: D\nı think D","comment_id":"775786","upvote_count":"2","timestamp":"1673718240.0"}],"question_id":154,"question_images":[],"unix_timestamp":1673684460,"answers_community":["D (94%)","4%"],"topic":"1","question_text":"A company’s application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application.\n\nWhich solution will resolve these issues in the MOST operationally efficient way?","timestamp":"2023-01-14 09:21:00"},{"id":"4UWoXH1n9WqBrJoGIoRn","answer":"B","choices":{"C":"S3 Standard-Infrequent Access (S3 Standard-IA)","B":"S3 Intelligent-Tiering","D":"S3 One Zone-Infrequent Access (S3 One Zone-IA)","A":"S3 Standard"},"discussion":[{"comment_id":"696276","timestamp":"1665929400.0","upvote_count":"46","poster":"123jhl0","content":"Selected Answer: B\n\"unpredictable pattern\" - always go for Intelligent Tiering of S3\nIt also meets the resiliency requirement: \"S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive redundantly store objects on multiple devices across a minimum of three Availability Zones in an AWS Region\" https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html"},{"poster":"Buruguduystunstugudunstuy","upvote_count":"19","comments":[{"poster":"Buruguduystunstugudunstuy","comment_id":"759036","content":"Option A, S3 Standard, is not a good choice because it does not offer the cost optimization of S3 Intelligent-Tiering.\n\nOption C, S3 Standard-Infrequent Access (S3 Standard-IA), is not a good choice because it is optimized for infrequently accessed objects and does not offer the cost optimization of S3 Intelligent-Tiering.\n\nOption D, S3 One Zone-Infrequent Access (S3 One Zone-IA), is not a good choice because it is not resilient to the loss of an Availability Zone. It stores objects in a single Availability Zone, making it less durable than other storage classes.","upvote_count":"8","timestamp":"1672172760.0"}],"timestamp":"1672172760.0","comment_id":"759035","content":"Selected Answer: B\nThe storage option that meets these requirements is B: S3 Intelligent-Tiering.\n\nAmazon S3 Intelligent Tiering is a storage class that automatically moves data to the most cost-effective storage tier based on access patterns. It can store objects in two access tiers: the frequent access tier and the infrequent access tier. The frequent access tier is optimized for frequently accessed objects and is charged at the same rate as S3 Standard. The infrequent access tier is optimized for objects that are not accessed frequently and are charged at a lower rate than S3 Standard.\n\nS3 Intelligent Tiering is a good choice for storing media files that are accessed frequently and infrequently in an unpredictable pattern because it automatically moves data to the most cost-effective storage tier based on access patterns, minimizing storage and retrieval costs. It is also resilient to the loss of an Availability Zone because it stores objects in multiple Availability Zones within a region."},{"upvote_count":"1","content":"Selected Answer: B\nI am going to choose B. \nS3 Intelligent-Tiering - Perfect use case when you don't know the frequency of access or irregular patterns of usage.","timestamp":"1733945220.0","poster":"Gizmo2022","comment_id":"1325198"},{"poster":"Tjazz04","timestamp":"1733942460.0","comment_id":"1325172","content":"Selected Answer: B\nKey phrase: Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern.\nAlways use S3 Intelligent-Tiering for unpredictable access patterns.","upvote_count":"1"},{"upvote_count":"2","poster":"PaulGa","content":"Selected Answer: B\nAns B - Intelligent Tiering: cost effective, optimised by access frequency","comment_id":"1265331","timestamp":"1723578960.0"},{"upvote_count":"3","content":"Selected Answer: B\nS3 intelligent tiering support 3 layer, frequent access, infrequent access and rarely access data.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html","comment_id":"1253064","poster":"DavidNgTan","timestamp":"1721650260.0"},{"timestamp":"1716481860.0","upvote_count":"3","content":"Unpredictable pattern equals intelligent tiering","comment_id":"1216742","poster":"lofzee"},{"content":"unpredictable pattern == S3 IA (Intellgent Tiering, not Infrequent access though)","timestamp":"1712020440.0","poster":"JohnZh","upvote_count":"3","comment_id":"1187776"},{"content":"Selected Answer: B\nUnpredictable pattern = Intelligent tiering","poster":"awsgeek75","upvote_count":"3","comment_id":"1122028","timestamp":"1705177680.0"},{"upvote_count":"1","poster":"A_jaa","timestamp":"1705149420.0","comment_id":"1121631","content":"Selected Answer: B\nAnswer-B"},{"timestamp":"1703168160.0","comment_id":"1102599","poster":"bujuman","upvote_count":"1","content":"Selected Answer: B\nThe right answer due to \"unpredictable pattern\""},{"content":"Selected Answer: B\nB because intelligent tiering is what we choose when we don’t have a pattern","comment_id":"1091404","upvote_count":"2","poster":"ddement0r","timestamp":"1702088700.0"},{"content":"Amazon S3 Intelligent Tiering is a storage class that automatically moves data to the most cost-effective storage tier based on access patterns. It can store objects in two access tiers: the frequent access tier and the infrequent access tier. The frequent access tier is optimized for frequently accessed objects and is charged at the same rate as S3 Standard. The infrequent access tier is optimized for objects that are not accessed frequently and are charged at a lower rate than S3 Standard.","poster":"Ruffyit","timestamp":"1698314400.0","upvote_count":"2","comment_id":"1054440"},{"content":"(B) The question mentions that some files are accessed frequently while others are rarely accessed, and the pattern is unpredictable.\nThis makes S3 Intelligent-Tiering a good fit because it automatically moves data between different access tiers based on how frequently they are accessed, optimizing costs.\nIntelligent-Tiering is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.\nB meets the requirements","comment_id":"1025498","timestamp":"1696497660.0","upvote_count":"1","poster":"awsleffe"},{"timestamp":"1694318700.0","comment_id":"1003666","upvote_count":"1","poster":"reema908516","content":"Selected Answer: B\nAmazon S3 Intelligent Tiering is a storage class that automatically moves data to the most cost-effective storage tier based on access patterns."},{"poster":"benacert","content":"Unpredictable pattern, intelligent tiering will handle that.\nB - is the answer..","upvote_count":"1","comment_id":"997968","timestamp":"1693776300.0"},{"upvote_count":"1","poster":"TariqKipkemei","timestamp":"1690949100.0","content":"Files are accessed in an unpredictable pattern, must minimize the costs of storing and retrieving the media files = S3 Intelligent-Tiering.","comment_id":"969640"},{"comment_id":"958559","poster":"Guru4Cloud","content":"Selected Answer: B\nS3 Intelligent-Tiering: This storage class is designed to optimize costs by automatically moving objects between two access tiers based on their usage patterns. It uses frequent access and infrequent access tiers. The frequently accessed objects stay in the frequent access tier, while the objects that are not accessed frequently are moved to the infrequent access tier. Intelligent-Tiering maintains high availability across AZs, just like S3 Standard, but it also helps reduce costs by moving data to the lower-cost tier when appropriate.","timestamp":"1689946500.0","upvote_count":"1"},{"poster":"miki111","timestamp":"1689532440.0","content":"Option B is the right answer for this.","comment_id":"953560","upvote_count":"1"},{"poster":"james2033","comments":[{"content":"S3 Intelligent-Tiering is designed for data with changing or unknown access patterns, while S3 Standard-IA is designed for long-lived, infrequently accessed data [1]. S3 Intelligent-Tiering automatically reduces your storage costs on a granular object level by automatically moving data to the most cost-effective access tier based on access frequency, without performance impact, retrieval fees, or operational overhead [2]. However, it's important to note that by using S3 Intelligent-Tiering, you need to pay for a small object monitoring fee to keep track of access patterns to your data [3].","comment_id":"957986","poster":"james2033","comments":[{"upvote_count":"1","comment_id":"957987","poster":"james2033","content":"[1] S3 Intelligent Tiering: How it Helps to Optimize Storage Costs? https://www.stormit.cloud/blog/s3-intelligent-tiering-storage-class/\n[2] Object Storage Classes – Amazon S3. https://aws.amazon.com/s3/storage-classes/\n[3] S3 Standard vs Intelligent Tiering – What’s the difference? https://www.beabetterdev.com/2021/10/16/s3-standard-vs-intelligent-tiering/","timestamp":"1689902760.0"}],"upvote_count":"1","timestamp":"1689902760.0"}],"upvote_count":"1","content":"Selected Answer: B\n\"S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive are all designed to sustain data in the event of the loss of an entire Amazon S3 Availability Zone.\" source: https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html","timestamp":"1689450480.0","comment_id":"952651"},{"upvote_count":"3","content":"Selected Answer: B\nS3 Intelligent-Tiering is designed to optimize costs by automatically moving objects between two access tiers: frequent access and infrequent access. It uses machine learning algorithms to analyze access patterns and determine the most appropriate tier for each object.\n\nIn the given scenario, where some media files are accessed frequently while others are rarely accessed in an unpredictable pattern, S3 Intelligent-Tiering can be a suitable choice. It automatically adjusts the storage tier based on the access patterns, ensuring that frequently accessed files remain in the frequent access tier for fast retrieval, while rarely accessed files are moved to the infrequent access tier for cost savings.\n\nCompared to S3 Standard-IA, S3 Intelligent-Tiering provides more granular cost optimization and may be more suitable if the access patterns of the media files fluctuate over time. However, it's worth noting that S3 Intelligent-Tiering may have slightly higher storage costs compared to S3 Standard-IA due to the added flexibility and automation it offers.","comment_id":"926572","poster":"cookieMr","timestamp":"1687078920.0"},{"poster":"Abrar2022","timestamp":"1684042380.0","upvote_count":"1","content":"B - for unpredictable patterns use intelligent tiering","comment_id":"897268"},{"timestamp":"1682965920.0","upvote_count":"2","poster":"Rahulbit34","comment_id":"886596","content":"B - \"UNPREDICTABLE pattern\" is the key"},{"poster":"PhucVuu","upvote_count":"4","timestamp":"1680853980.0","comment_id":"863636","content":"Selected Answer: B\nKeywords:\n- Must be resilient to the loss of an Availability Zone.\n- files are accessed FREQUENTLY while other files are RARELY accessed in an UNPREDICTABLE pattern.\n\nA - Incorrect: S3 Standard is not cost effective for rarely access files\nB - Correct: S3 Intelligent-Tiering is good for file which frequently or rarely accessed in an unpredictable pattern. Intelligent-Tiering will help us analyze the pattern and move rarely access files to storage which has lower cost.\nC - Incorrect: Standard-Infrequent Access is not cost effective for frequently access files\nD - Incorrect: One Zone-Infrequent Access is not resilient to the loss of an Availability Zone"},{"upvote_count":"1","timestamp":"1680419820.0","comment_id":"858591","poster":"channn","content":"Selected Answer: B\nKey words: in an unpredictable pattern."},{"upvote_count":"1","comment_id":"842717","poster":"cheese929","content":"Selected Answer: B\nS3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns, independent of object size or retention period","timestamp":"1679139660.0"},{"upvote_count":"1","content":"Selected Answer: B\nS3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive are all designed to sustain data in the event of the loss of an entire Amazon S3 Availability Zone.","comment_id":"826100","poster":"bilel500","timestamp":"1677690420.0"},{"content":"Selected Answer: B\nB is correct","poster":"Rishi1","comment_id":"789345","upvote_count":"1","timestamp":"1674797760.0"},{"upvote_count":"1","content":"C. S3 Standard-Infrequent Access (S3 Standard-IA)\n\nS3 Standard-IA is designed for infrequently accessed data, which is a good fit for the media files that are rarely accessed in an unpredictable pattern. S3 Standard-IA is also cross-Region replicated, providing resilience to the loss of an Availability Zone. Additionally, S3 Standard-IA has a lower storage and retrieval cost compared to S3 Standard and S3 Intelligent-Tiering, which makes it a cost-effective option for storing infrequently accessed data.","comment_id":"774090","poster":"jannymacna","timestamp":"1673583240.0"},{"poster":"vinhle","timestamp":"1673150760.0","comment_id":"769076","content":"B is clearly","upvote_count":"1"},{"upvote_count":"2","poster":"pazabal","content":"Selected Answer: B\nunpredictable pattern = Intelligent Tiering","timestamp":"1671492120.0","comment_id":"750315"},{"poster":"333666999","content":"Selected Answer: B\nS3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive are all designed to sustain data in the event of the loss of an entire Amazon S3 Availability Zone.","timestamp":"1670521440.0","upvote_count":"1","comment_id":"739313"},{"upvote_count":"1","comment_id":"736575","poster":"AlaN652","timestamp":"1670308860.0","content":"Selected Answer: B\nSince there are files which will be accessed frequently and others infrequently"},{"poster":"9014","upvote_count":"2","content":"Selected Answer: B\n\"unpredictable pattern\" - remember the keyword and always go for Intelligent Tiering of S3","comment_id":"735079","timestamp":"1670157540.0"},{"content":"B is correct","timestamp":"1669035180.0","poster":"Wpcorgan","comment_id":"723495","upvote_count":"1"},{"comment_id":"715384","poster":"AbhiJo","timestamp":"1668097740.0","upvote_count":"2","content":"B is correct, C is incorrect because of requirement for frequent access as well"},{"upvote_count":"1","timestamp":"1667921940.0","poster":"xeun88","comment_id":"713929","content":"Since it said some data a access frequently and some are unpredictable, i will go for B."},{"content":"Selected Answer: B\nans is correct B","upvote_count":"2","poster":"17Master","timestamp":"1667168160.0","comment_id":"708066"},{"upvote_count":"2","poster":"GameDad09","content":"Selected Answer: B\nB is the correct one.","timestamp":"1666037400.0","comment_id":"697672"},{"timestamp":"1665948660.0","poster":"queen101","upvote_count":"1","comment_id":"696467","content":"BBBBBBBBBBB"},{"poster":"ninjawrz","upvote_count":"1","timestamp":"1665793320.0","comment_id":"695066","content":"B. S3 Intelligent-Tiering for unpredictable or vague usecase"},{"comment_id":"692631","upvote_count":"1","content":"Selected Answer: B\nsure for B","poster":"BoboChow","timestamp":"1665549360.0"},{"content":"Selected Answer: B\nB\nhttps://aws.amazon.com/getting-started/hands-on/getting-started-using-amazon-s3-intelligent-tiering/?nc1=h_ls","upvote_count":"3","poster":"galbimandu","timestamp":"1665457140.0","comment_id":"691670"},{"comment_id":"690627","content":"B is correct.","timestamp":"1665364560.0","poster":"tt79","upvote_count":"1"}],"question_id":155,"timestamp":"2022-10-10 03:16:00","question_text":"A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and retrieving the media files.\nWhich storage option meets these requirements?","answer_description":"","isMC":true,"answer_images":[],"exam_id":31,"answer_ET":"B","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/84943-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1665364560,"answers_community":["B (100%)"],"topic":"1"}],"exam":{"provider":"Amazon","isMCOnly":true,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isBeta":false,"id":31,"isImplemented":true,"lastUpdated":"11 Apr 2025"},"currentPage":31},"__N_SSP":true}