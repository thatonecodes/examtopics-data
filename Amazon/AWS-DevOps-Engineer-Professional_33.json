{"pageProps":{"questions":[{"id":"epqXTPORWIjCzOC3WOoN","discussion":[{"content":"Answer is C\nB and D wrong because of the \"intrinsic functions in the Parameters section\"\nA: what is the use of the count input parameters ? and UpdateChangeSet? you need create and execute","timestamp":"1632790740.0","comment_id":"24702","comments":[{"upvote_count":"5","content":"C is correct.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html","poster":"un","comment_id":"34197","timestamp":"1633037640.0"}],"upvote_count":"32","poster":"HazemYousry"},{"poster":"xlFireman","timestamp":"1634934480.0","upvote_count":"14","content":"A - Incorrect since StackSets are not needed here. No mention of multi-region/multi-account deployments (although correct usage of intrinsic functions and refs to resource values)\nB - If you reference the template URL, you wouldn't need to use the intrinsic function Fn:import. You would reference the output of the template URL function i.e. TemplateName.Outputs.VariableName\nC - Correct\nD - You do not use Fn:import within the Parameters section. It is solely used within the Resources section to reference stack outputs within nested stacks or other previously deployed stacks.\n\nErgo, answer is C.","comment_id":"220691"},{"poster":"okm1997_2","upvote_count":"2","content":"Selected Answer: C\nC is correct","timestamp":"1677938100.0","comment_id":"829015"},{"comments":[{"poster":"saeidp","comment_id":"823243","timestamp":"1677476580.0","upvote_count":"1","content":"Based on kopper2019 explanation C is correct.\nStackset is also wrong in A"}],"poster":"saeidp","content":"The only answer that has no issue is A\n Fn::ImportValue is only used in cross stack reference\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html","upvote_count":"1","timestamp":"1676847180.0","comment_id":"814638"},{"content":"The correct answer is A. There has been a lot of confusion here because the question is not clear on whether each dev env is created in a different region/ account. However other answers are technically incorrect as per the AWS documentation. B and C are incorrect as fn:ImportValue doesn't work with nested stack and only works with cross-stack references. Answer D is incorrect as fn:ImportValue cannot be used in the Parameter section of a CF template. Therefore through method of elimination the correct answer is A.","poster":"Bulti","comment_id":"778399","upvote_count":"3","timestamp":"1673913720.0","comments":[{"comment_id":"787351","comments":[{"upvote_count":"1","timestamp":"1674631500.0","content":"So C seems bf right.","poster":"kopper2019","comment_id":"787352"}],"upvote_count":"3","poster":"kopper2019","content":"Option C doesn’t mention importing the values between nested stacks. It mentions importing values in nested stack from VPC stack.","timestamp":"1674631500.0"}]},{"upvote_count":"4","comment_id":"652962","poster":"vagobago","content":"Selected Answer: C\nAnswer C seems to be the most suitable one. Why A is marked to be the right one - no idea. \"CloudFormation StackSets\" within the answer - fits to multi-account scenarios - in the question is no word about multi-account","timestamp":"1661694360.0"},{"comment_id":"379567","poster":"Elie777","content":"For cross-stack references, use Fn::ImportValue to import a value from another template. For nested stacks, use Fn::Ref and Fn::GetAtt to reference the value in your current template.\nSo It can' be C","timestamp":"1636292760.0","upvote_count":"1"},{"comment_id":"322133","timestamp":"1635946260.0","upvote_count":"1","poster":"aws_Tamilan","content":"ANs: C"},{"comment_id":"230256","upvote_count":"3","content":"A is right with stackset and all others are wrong.","poster":"rlf","timestamp":"1635795360.0"},{"poster":"jackdryan","comment_id":"223326","upvote_count":"4","timestamp":"1635317820.0","content":"I'll go with C"},{"poster":"ChauPhan","timestamp":"1634816400.0","comment_id":"207653","upvote_count":"1","content":"C is the correct one. \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html\nCF of network exports the VPC, subnet or needed information\nCF of application imports the above information to its stack and UpdateChangeSet/ ExecuteChangeSet"},{"poster":"nqobza","content":"The correct answer is C.","comment_id":"176485","timestamp":"1634667120.0","upvote_count":"1"},{"timestamp":"1633653900.0","comments":[{"timestamp":"1634819940.0","poster":"ChauPhan","comment_id":"207658","content":"A might be correct if doesn't have sentence \"using the Count input parameter to indicate the number of environments needed\", there is not needed. What we need is the VPC ID and subnet from Network CF.","upvote_count":"1"},{"timestamp":"1634589480.0","comment_id":"102650","upvote_count":"2","content":"Option C says: Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values.\nI believe it means that in the resource blocks inside a nested stack you should use Fn::ImportValue function. Which is correct.","poster":"demon42"}],"content":"A is the correct answer.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html#stacksets-concepts-stackset\nC is definitely wrong, and it's why:\nFn::ImportValue is used for cross-stacks reference. i.e. You create a VPC in stack B and export the VPC so another stacks can reference it using Fn::ImportValue.\nTo pass values from nested stack to parent stack, you should use Fn::GetAttr in the parent stack","comment_id":"96891","poster":"PeppaPig","upvote_count":"1"},{"upvote_count":"1","poster":"kawara","content":"C is the right answer","timestamp":"1633635300.0","comment_id":"72018"},{"upvote_count":"1","timestamp":"1633584540.0","poster":"Ebi","content":"C is the answer","comment_id":"62703"},{"comment_id":"62223","timestamp":"1633400220.0","upvote_count":"1","poster":"yassu","content":"C is correct"},{"poster":"toshko85","timestamp":"1633308180.0","upvote_count":"1","content":"voting for C","comment_id":"58739"},{"content":"It is C","upvote_count":"1","comment_id":"56753","poster":"xaocho","timestamp":"1633280340.0"},{"poster":"ele","comments":[{"timestamp":"1633195440.0","poster":"ele","upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1633397220.0","comment_id":"60884","poster":"hendry","content":"ele，what are you saying? A or C?"}],"content":"Agree, Answer is C. In A the UpdateChangeSet doesnt exist, also the Count.","comment_id":"53653"}],"timestamp":"1633116480.0","comment_id":"53064","upvote_count":"3","content":"B, D wrong because of “Use nested stacks to define common infrastructure components.” – common infrastructure cannot be defined in a nested stack. Also, wrong for “use TemplateURL to reference the Networking team's template” – it’s stack name, not template URL. \nC wrong because of “Use Fn:ImportValue intrinsic functions in the Parameters section of the master template to retrieve Virtual Private Cloud (VPC) and subnet values.” – master template is the one that defines the infrastructure. It should hace ExportValue. \nCorrect answer is A"},{"poster":"Arragon","upvote_count":"3","comment_id":"22921","timestamp":"1632663360.0","content":"The right answer should be D. \nA talks about stacksets which need multiple AWS accounts. \nB uses the Fn:ImportValue only on the master template. But this intrinsic function is created to import the exported values from other stacks. So this would be useless. \nC misses the step of defining the recourses for the environments."},{"comment_id":"22370","timestamp":"1632276180.0","content":"I think B","upvote_count":"2","poster":"BeastX"},{"poster":"marwan","timestamp":"1632097320.0","comment_id":"21331","upvote_count":"1","content":"Yes I would go with A"}],"exam_id":35,"choices":{"C":"Use nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.","D":"Use Fn::ImportValue intrinsic functions in the Parameters section of the master template to retrieve Virtual Private Cloud (VPC) and subnet values. Define the development resources in the order they need to be created in the CloudFormation nested stacks. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.","A":"Use Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of environments needed. use the UpdateStackSet command to update existing development environments.","B":"Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the Networking team's template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the master template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments."},"url":"https://www.examtopics.com/discussions/amazon/view/8121-exam-aws-devops-engineer-professional-topic-1-question-56/","question_images":[],"answer":"C","timestamp":"2019-11-13 20:27:00","answer_description":"","question_id":161,"unix_timestamp":1573673220,"question_text":"A rapidly growing company wants to scale for Developer demand for AWS development environments. Development environments are created manually in the\nAWS Management Console. The Networking team uses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for the\nAmazon VPC and all subnets. The development environments have common standards, such as Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables.\nTo keep up with the demand, the DevOps Engineer wants to automate the creation of development environments. Because the infrastructure required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will be used to create a template for the development environments.\nWhich approach will meet these requirements and quickly provide consistent AWS environments for Developers?","topic":"1","answer_ET":"C","answers_community":["C (100%)"],"answer_images":[],"isMC":true},{"id":"Ell4T5HHbRTR8255C3kH","discussion":[{"comment_id":"326681","comments":[{"timestamp":"1694931120.0","content":"alarm goes off means the \"alarm is triggered\".","upvote_count":"1","poster":"RVivek","comment_id":"1009613"},{"comment_id":"1061099","timestamp":"1698989640.0","content":"The reason that D is incorrect is because there is no \"memory consumption metric\" by default in cloudwatch. You will need to use cloudwatch agent to send system-level metrics to create a custom metric.","poster":"DZ_Ben","upvote_count":"2"}],"timestamp":"1633044840.0","content":"I'll go with A, E\n\nB is wrong because it don't attack the problem\nC is wrong because changing the target group health checks from HTTP to TCP will not help\nD is wrong because of \"notifications when the alarm goes off\".","poster":"WhyIronMan","upvote_count":"25"},{"comment_id":"314624","content":"I will go with A,E","upvote_count":"9","timestamp":"1632957120.0","poster":"Jordanro"},{"content":"A & E is the right answer.","timestamp":"1673914080.0","poster":"Bulti","comment_id":"778402","upvote_count":"1"},{"poster":"PepsNick","timestamp":"1673348460.0","comment_id":"771314","content":"Selected Answer: AE\nAE: they are looking for a combination of actions. DE is not possible just from that because they are two similar actions","upvote_count":"1"},{"poster":"mvsnogueira2022","comment_id":"688985","upvote_count":"2","content":"A and E\nC - Is not possible. Only protocols available for health check are HTTP and HTTPS\nB - Increase the time will not solve","timestamp":"1665195780.0"},{"upvote_count":"3","content":"A company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS Region.\nA solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs.\nHow can the solutions architect meet this requirement?\n\nA. Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through It.\nB. Deploy a NAT gateway into a public subnet and attach an end point policy that allows access to the S3 buckets.\nC. Deploy the application Into a public subnet and allow it to route through an internet gateway to access the S3 Buckets\nD. Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets.","timestamp":"1657691700.0","poster":"niklocastro","comment_id":"630788"},{"timestamp":"1657658820.0","upvote_count":"1","content":"Selected Answer: AE\nAE is correct.","comment_id":"630683","poster":"tefdsfdsvasdf"},{"upvote_count":"4","content":"Selected Answer: AE\nAE, E is obvious, A is because ASG by default monitor EC2 health check instead of LB, think that's the point of the question.","comment_id":"544824","timestamp":"1644522420.0","poster":"blueorca"},{"comment_id":"385472","poster":"RLai","content":"Ans: A, E","upvote_count":"1","timestamp":"1634282940.0"},{"upvote_count":"1","content":"ans: B, E\n\nThe Requirement is \"Monitoring and notifications\", replacement helps nothing by outofmemory","comment_id":"361420","timestamp":"1633420680.0","poster":"feelgreat"},{"comment_id":"330308","timestamp":"1633317840.0","poster":"devopp","content":"left with A,B as only sensible options.\nsinceC doesnt solve problem\nand D & E are invalid as no plain ASG (or EC2I) metrics on memory utilization (need Custom metric but no mention of that).","upvote_count":"2","comments":[{"poster":"shammous","content":"I thought, like you, that a custom metric is needed for memory usage, but I came across this: \"By default, AWS gives you visibility into metrics like CPU load logs, network latency, request volume, etc., but not EC2 memory usage. For other metrics like EC2 memory usage, you’ll have to install and configure a CloudWatch agent on the instance...\". For more info, check the \"mem_xxxx\" metrics here: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html","upvote_count":"3","comments":[{"comment_id":"464953","poster":"justfmm","upvote_count":"1","timestamp":"1635111420.0","content":"Cloudwatch agent have memory used metric.\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html"}],"comment_id":"449903","timestamp":"1634771100.0"}]}],"question_images":[],"answer_ET":"AE","answer_description":"","choices":{"D":"Enable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notifications when the alarm goes off.","E":"Use the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm when the memory utilization is high and associate an Amazon SNS topic to receive a notification.","A":"Change the Auto Scaling configuration to replace the instances when they fail the load balancer's health checks.","C":"Change the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.","B":"Change the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks."},"answer_images":[],"isMC":true,"question_id":162,"url":"https://www.examtopics.com/discussions/amazon/view/47704-exam-aws-devops-engineer-professional-topic-1-question-57/","timestamp":"2021-03-19 07:36:00","answer":"AE","topic":"1","exam_id":35,"unix_timestamp":1616135760,"answers_community":["AE (100%)"],"question_text":"A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to respond to user requests. The EC2 instances are also failing target group HTTP health checks.\nUpon inspection, the engineer notices the application process was not running in any EC2 instances. There are a significant number of out of memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application memory leak. Monitoring and notifications should be enabled to alert when there is an issue.\nWhich combination of actions will meet these requirements? (Choose two.)"},{"id":"GYVnzCAC26l9OSXdbkMi","answer_description":"","question_text":"A company wants to migrate a legacy application to AWS and develop a deployment pipeline that uses AWS services only. A DevOps engineer is migrating all of the application code from a Git repository to AWS CodeCommit while preserving the history of the repository. The DevOps engineer has set all the permissions within CodeCommit, installed the Git client and the AWS CLI on a local computer, and is ready to migrate the repository.\nWhich actions will follow?","choices":{"A":"Create the CodeCommit repository using the AWS CLI. Clone the Git repository directly to CodeCommit using the AWS CLI. Validate that the files were migrated, and publish the CodeCommit repository.","D":"Create the CodeCommit repository using the AWS Management Console or the AWS CLI. Clone the Git repository with a mirror argument to the local computer and push the repository to CodeCommit. Validate that the files were migrated, and share the CodeCommit repository.","C":"Create the CodeCommit repository using the AWS Management Console. Use the console to clone the Git repository into the CodeCommit repository. Validate that the files were migrated, and publish the CodeCommit repository.","B":"Create the CodeCommit repository using the AWS Management Console. Clone both the Git and CodeCommit repositories to the local computer. Copy the files from the Git repository to the CodeCommit repository on the local computer. Commit the CodeCommit repository. Validate that the files were migrated, and share the CodeCommit repository."},"unix_timestamp":1615671300,"exam_id":35,"answer_ET":"D","answer_images":[],"answers_community":["D (100%)"],"timestamp":"2021-03-13 22:35:00","discussion":[{"timestamp":"1633351500.0","upvote_count":"18","poster":"rscloud","content":"D\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/how-to-migrate-repository-existing.html","comment_id":"310004"},{"poster":"WhyIronMan","timestamp":"1633780800.0","upvote_count":"9","content":"I'll got with D\n\nReference: https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-migrate-repository-existing.html#how-to-migrate-existing-clone","comment_id":"325820","comments":[{"poster":"StelSen","timestamp":"1635519300.0","upvote_count":"1","content":"The link is very useful to understand. Thanks a lot","comment_id":"428467"}]},{"content":"Selected Answer: D\nIn short : install git locally->clone repo to your machine->via console create codecommit->push from your machine to codecommit. I thought there would be more streamlined process , such as go to console, create and indicate from which remote repo to clone to codecommit.","timestamp":"1738860420.0","upvote_count":"1","comment_id":"1352521","poster":"d3717d5"},{"poster":"Piccaso","timestamp":"1676404140.0","comment_id":"808779","upvote_count":"4","content":"Selected Answer: D\nWhy is A suggested as \"correct answer\" ?"},{"upvote_count":"2","comment_id":"778464","poster":"Bulti","timestamp":"1673921520.0","content":"Answer D use the --mirror option when cloning git repo"},{"poster":"ryuhei","upvote_count":"2","comment_id":"702089","timestamp":"1666522080.0","content":"Selected Answer: D\nAnswer is ”D” ！"},{"timestamp":"1644763560.0","comment_id":"546507","poster":"blueorca","content":"Selected Answer: D\nD is the correct answer","upvote_count":"2"},{"comment_id":"441738","content":"D -\n1 - create a repo\n2 - clone the repo (mirror)\n3 - push\n4 - validate","timestamp":"1635964380.0","poster":"govindrk","upvote_count":"4"},{"timestamp":"1635247140.0","content":"Ans is D","upvote_count":"2","comment_id":"376082","poster":"RLai"},{"comment_id":"376067","timestamp":"1634558220.0","poster":"RLai","upvote_count":"1","content":"Answer: D"},{"content":"ans: D","comment_id":"310279","upvote_count":"4","poster":"Rajarshi","timestamp":"1633446060.0"}],"answer":"D","topic":"1","question_id":163,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/46940-exam-aws-devops-engineer-professional-topic-1-question-58/","question_images":[]},{"id":"v1Wu36kzAAR4GMLHiWdo","timestamp":"2021-03-21 15:13:00","choices":{"D":"Create a test action after the build action. Use a Jenkins server on Amazon EC2 to perform the required tests and mark the action as successful if the tests pass. Create a manual approval action that uses Amazon SQS to notify the team and add a deploy action to deploy the application to the next stage.","A":"Create a manual approval action after the build action of the pipeline. Use Amazon SNS to inform the team of the stage being triggered. Next, add a test action using CodeBuild to perform the required tests. At the end of the pipeline, add a deploy action to deploy the application to the next stage.","C":"Create a new pipeline that uses a source action that gets the code from the same repository as the first pipeline. Add a deploy action to deploy the code to a test environment. Use a test action using AWS Lambda to test the deployment. Add a manual approval action by using Amazon SNS to notify the team, and add a deploy action to deploy the application to the next stage.","B":"Create a test action after the CodeBuild build of the pipeline. Configure the action to use CodeBuild to perform the required tests. If these tests are successful, mark the action as successful. Add a manual approval action that uses Amazon SNS to notify the team, and add a deploy action to deploy the application to the next stage."},"discussion":[{"timestamp":"1633561020.0","comment_id":"323449","upvote_count":"8","poster":"Jordanro","content":"I will go with B."},{"upvote_count":"5","poster":"sb333","comment_id":"328284","timestamp":"1635895680.0","content":"The answer is B."},{"comment_id":"786972","poster":"damians106","content":"Selected Answer: B\nAnswer is B\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline-add-test.html\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html","timestamp":"1674594600.0","upvote_count":"1"},{"content":"Answer is B without a doubt.","upvote_count":"1","timestamp":"1673921700.0","poster":"Bulti","comment_id":"778468"},{"comment_id":"318934","content":"I will got with A","comments":[{"poster":"WhyIronMan","timestamp":"1635561360.0","content":"The manual approval must be after Tests and before deploy, not after build","upvote_count":"5","comment_id":"326942"}],"upvote_count":"1","timestamp":"1633326660.0","poster":"Spavanko"},{"comments":[{"content":"I'll go with B \n\nbecause its the simplest way with lower costs","timestamp":"1635359460.0","poster":"WhyIronMan","upvote_count":"5","comment_id":"326941"}],"comment_id":"316383","content":"Reference: https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline-add-test.html","upvote_count":"2","timestamp":"1632862800.0","poster":"WhyIronMan"}],"exam_id":35,"isMC":true,"question_text":"A company is using AWS to deploy an application. The development team must automate the deployments. The team has created an AWS CodePipeline pipeline to deploy the application to Amazon EC2 instances using AWS CodeDeploy after it has been built using AWS CodeBuild.\nThe team wants to add automated testing to the pipeline to confirm that the application is healthy before deploying the code to the EC2 instances. The team also requires a manual approval action before the application is deployed, even if the tests are successful. The testing and approval must be accomplished at the lowest costs, using the simplest management solution.\nWhich solution will meet these requirements?","answer_images":[],"answers_community":["B (100%)"],"question_id":164,"answer":"B","answer_ET":"B","topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/47875-exam-aws-devops-engineer-professional-topic-1-question-59/","unix_timestamp":1616335980,"answer_description":""},{"id":"NZTDU4aANwVy3XzPtoe0","question_text":"A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks. The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and process. The company wants to reduce the complexity of the release and maintenance of these applications.\nThe company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code, a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure.\nWhat should a DevOps engineer do to meet these requirements?","choices":{"D":"Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic Container Service (Amazon ECS) on infrastructure that AWS Fargate manages.","C":"Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time to create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 fleets by using these AMIs.","B":"Create one AWS CodeCommit repository for each of the applications Use AWS CodeBuild to build the applications one at a time. Use AWS CodeDeploy to deploy the applications to one centralized application server.","A":"Create one AWS CodeCommit repository for all applications. Put each application's code in different branch. Merge the branches, and use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server."},"isMC":true,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/78510-exam-aws-devops-engineer-professional-topic-1-question-6/","exam_id":35,"answer_description":"","discussion":[{"timestamp":"1732882320.0","poster":"Simba84","content":"Selected Answer: D\nD is correct Using AWS CodeCommit, CodeBuild, ECR, and ECS with Fargate provides a modern, scalable, and fully managed solution that meets these requirements.","comment_id":"1319703","upvote_count":"1"},{"upvote_count":"1","timestamp":"1705283220.0","poster":"ycfreeman","content":"B is correct, D is over-engineered","comment_id":"1123002"},{"poster":"frizzolo","content":"Selected Answer: D\nB is wrong, D is correct imo","timestamp":"1690188600.0","comment_id":"961354","upvote_count":"1"},{"upvote_count":"3","poster":"frizzolo","timestamp":"1690186740.0","comment_id":"961325","content":"B says to deploy multiple application to a single Application server which is insane, if one server goes down all the applications go down."},{"poster":"dicaptain","upvote_count":"1","content":"Selected Answer: D\nD is more logical","timestamp":"1687185840.0","comment_id":"927611"},{"content":"Selected Answer: D\nD, because of \"as few maintenance tasks as possible on the underlying infrastructure\". Fargate does that better than \"one centralized application server\"","poster":"George34","upvote_count":"1","timestamp":"1684815900.0","comment_id":"904542"},{"poster":"ParagSanyashiv","upvote_count":"1","timestamp":"1683955440.0","content":"Selected Answer: D\nD makes more sense","comment_id":"896413"},{"comment_id":"844308","content":"Selected Answer: D\nD is the answer","upvote_count":"2","timestamp":"1679266020.0","poster":"hp298"},{"poster":"Tika01","upvote_count":"1","content":"Option D is the best solution for meeting the company's requirements. By creating a separate AWS CodeCommit repository for each application, the company can have centralized control of source code for each application. AWS CodeBuild can be used to build a Docker image for each application, which can then be stored in Amazon ECR. AWS CodeDeploy can be used to deploy the applications to Amazon ECS, which will be managed by AWS Fargate. This provides a consistent and automatic delivery pipeline for each application, and reduces maintenance tasks on the underlying infrastructure. Option A and B do not address the requirement of having a consistent and automatic delivery pipeline, while Option C requires manual management of Amazon EC2 fleets using StackSets.","timestamp":"1679028840.0","comment_id":"841590"},{"poster":"amier","timestamp":"1678677720.0","comment_id":"837588","upvote_count":"1","content":"Answer is D:\nB is wrong because of this: Use AWS CodeBuild to build the applications one at a time.\nbecause we can do it parallel way for saving time."},{"comment_id":"835417","content":"Selected Answer: D\nmultiple applications in a single application server will be an over head to maintain and deploy changes. I will go with D","timestamp":"1678480560.0","poster":"krr_aws","upvote_count":"1"},{"timestamp":"1677680820.0","poster":"sasa33_p","upvote_count":"1","content":"Selected Answer: D\nWhy is B? If you want to deploy on different, should docker be a better choice?","comment_id":"825963"},{"upvote_count":"1","comment_id":"808445","timestamp":"1676383800.0","content":"Selected Answer: D\nWhy is B suggested as \"correct answer\" ?","poster":"Piccaso"},{"poster":"Hamza5","upvote_count":"2","comment_id":"793342","timestamp":"1675113120.0","content":"D will be the answer as we need Docker in this scenario."},{"comments":[{"upvote_count":"1","comment_id":"796988","content":"I thought D is the solution. However, the revealed solution is B","poster":"Piccaso","timestamp":"1675424280.0"}],"timestamp":"1672759800.0","poster":"ericzaj","content":"Selected Answer: D\nThere is a requirement for \"as few maintenance tasks as possible on the underlying infrastructure.\". Fargate is serverless. This is the best answer.","comment_id":"764814","upvote_count":"3"},{"timestamp":"1671947580.0","upvote_count":"1","comment_id":"755426","poster":"Bulti","content":"Answer is D."},{"comment_id":"747109","timestamp":"1671190740.0","upvote_count":"1","content":"Selected Answer: D\niS B , you need docker for all different OS and Fargate toreduce the management of ifrastructure","poster":"ceros399"},{"poster":"ericzaj","timestamp":"1670066820.0","content":"Selected Answer: D\nB does not look correct. B states CodeDeploy to one application server. Requirements are to move multiple application types running on multiple instance types. D is best answer.","upvote_count":"2","comment_id":"734388"},{"content":"Selected Answer: D\nD since is different coding languages and different tehcnologies behind that.","poster":"Xenavis","timestamp":"1669207920.0","upvote_count":"2","comment_id":"725126"},{"poster":"katekarin","content":"Selected Answer: D\nOption D because Fargate requires less maintenance than EC2 instances.","comment_id":"719688","timestamp":"1668605820.0","upvote_count":"2"},{"timestamp":"1668581640.0","poster":"jlb","comment_id":"719401","content":"D : Docker is needed because various OS and language ared used","upvote_count":"1"},{"content":"B would have been a beautiful answer but \"central application server\" spoilt it for applications developed with different programming languages","upvote_count":"1","poster":"flavins","comment_id":"716639","timestamp":"1668248340.0"},{"content":"Selected Answer: D\nD -> It covers, source code control, build and continuous deployment along with Maintenance tasks using fargate","poster":"developer_404","timestamp":"1667541960.0","upvote_count":"1","comment_id":"710928"},{"upvote_count":"1","poster":"nzin4x","comment_id":"690819","content":"Selected Answer: D\nD is correct","timestamp":"1665387060.0"},{"content":"D is more suitable.","timestamp":"1665129240.0","upvote_count":"2","poster":"RogerMarsh","comment_id":"688441","comments":[{"comment_id":"690224","upvote_count":"2","poster":"robotgeek","timestamp":"1665322260.0","content":"The main point when choosing between C and D is the line \"as few maintenance tasks as possible on the underlying infrastructure\", clearly they want fargate, they don't care about their money, Bezos will"}]},{"upvote_count":"2","poster":"bestoneguy","comment_id":"680728","content":"Selected Answer: D\nB is wrong since it is multiple OS, languages, framework, so cannot deploy to 1 server. Best response is D","timestamp":"1664280600.0"},{"poster":"dangdoan","timestamp":"1662434400.0","content":"D. multiple languages and frameworks . So need docker","upvote_count":"1","comment_id":"660753"},{"poster":"ohcn","upvote_count":"2","comment_id":"655006","content":"Selected Answer: D\nD fullfils all requirements","timestamp":"1661943000.0"},{"timestamp":"1661921760.0","content":"D make sense","comment_id":"654772","upvote_count":"1","poster":"Manh"},{"content":"I will go with D managed Fargate and centralised","poster":"Brain4","timestamp":"1661913540.0","upvote_count":"2","comment_id":"654635"},{"timestamp":"1661897760.0","comment_id":"654463","content":"D. Fulfills all criteria since there is no mention about cost savings.","upvote_count":"2","poster":"colinquek"},{"upvote_count":"4","content":"Since the scenario requires support for different OS, the best response is D.","poster":"bigdood","timestamp":"1661894820.0","comment_id":"654434"}],"unix_timestamp":1661894820,"question_id":165,"answer_images":[],"answer_ET":"D","topic":"1","timestamp":"2022-08-30 23:27:00","question_images":[],"answer":"D"}],"exam":{"id":35,"isBeta":false,"isMCOnly":false,"isImplemented":true,"provider":"Amazon","name":"AWS DevOps Engineer Professional","lastUpdated":"11 Apr 2025","numberOfQuestions":208},"currentPage":33},"__N_SSP":true}