{"pageProps":{"questions":[{"id":"20Gwy4z6RyNAwkok7pEe","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/10080-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"question_text":"An online reseller has a large, multi-column dataset with one column missing 30% of its data. A Machine Learning Specialist believes that certain columns in the dataset could be used to reconstruct the missing data.\nWhich reconstruction approach should the Specialist use to preserve the integrity of the dataset?","question_id":311,"discussion":[{"timestamp":"1664579220.0","upvote_count":"26","poster":"rsimham","content":"C looks correct since multiple imputation can be performed based on the related variable as given in the question","comment_id":"28372"},{"timestamp":"1667306640.0","upvote_count":"8","comment_id":"263397","poster":"harmanbirstudy","content":"Multiple Imputation by Chained Equations or MICE, as per udemy this is always the best answer of all"},{"poster":"sonoluminescence","timestamp":"1730215200.0","content":"Why not D:\n\nDoesn't Account for Relationships:\nMean substitution doesn't take into account the potential relationships between variables. In the scenario you provided, it's believed that other columns could help in reconstructing the missing data. Using only the mean of the missing column doesn't leverage this potential inter-column relationship.\n\nAssumption of Missing Completely at Random (MCAR):\nMean substitution often operates under the assumption that the data is Missing Completely at Random (MCAR). In reality, data might be missing for a reason, and that reason might relate to other observed variables. Using mean substitution in such cases can introduce biases.","upvote_count":"2","comment_id":"1056913"},{"content":"Selected Answer: C\nA. NO - Listwise deletion is just dropping rows\nB. NO - does not reconstruct the data based on other fields\nC. YES - by definition\nD. NO - does not reconstruct the data based on other fields","upvote_count":"2","poster":"loict","comment_id":"1006654","timestamp":"1726233000.0"},{"upvote_count":"1","poster":"DavidRou","timestamp":"1726131060.0","comment_id":"1005560","content":"Selected Answer: C\nMICE is the algorithm to choose here"},{"upvote_count":"1","comment_id":"992442","poster":"Mickey321","content":"Selected Answer: C\nOption C","timestamp":"1724865360.0"},{"timestamp":"1707566220.0","poster":"AjoseO","content":"Selected Answer: C\nMultiple imputation is a statistical technique for handling missing data that involves generating multiple versions of the dataset with missing values filled in, and then combining the results to produce a single, complete dataset. \n\nThis approach takes into account the relationship between variables in the dataset, and uses statistical models to predict missing values based on the information in other columns. This helps to preserve the integrity of the dataset by avoiding the introduction of bias or systematic error into the results.","comment_id":"804263","upvote_count":"5"},{"content":"I am trying to understand why Mean Substitution is not the solution. Imputation typically uses the mean if the missing data is random, implying the substitution is not biased.","poster":"[Removed]","comment_id":"482137","comments":[{"upvote_count":"3","timestamp":"1709756460.0","content":"Mean substitution is limited to the current column. In this case, the requirement is to impute missing data from other columns","comment_id":"831232","poster":"cpal012"},{"poster":"rhuanca","upvote_count":"1","content":"Reason is if you replace 30% of the missing values , likely you will bias the variable.","comment_id":"605536","timestamp":"1684762800.0"}],"upvote_count":"2","timestamp":"1668896520.0"},{"timestamp":"1667112360.0","content":"If it's handling missing data then imputation comes into play\nAnswer is C 100%","poster":"syu31svc","comment_id":"169061","upvote_count":"1"},{"timestamp":"1667034240.0","content":"https://www.countants.com/blogs/heres-how-you-can-configure-automatic-imputation-of-missing-data/ C","comment_id":"121786","upvote_count":"1","poster":"Wira"},{"comment_id":"89708","timestamp":"1665438720.0","poster":"roytruong","content":"it's C","upvote_count":"1"},{"comment_id":"73490","poster":"dhs227","timestamp":"1665087060.0","upvote_count":"2","content":"A common strategy used to impute missing values is to replace missing values with the mean or median value. It is important to understand your data before choosing a strategy for replacing missing values. https://docs.aws.amazon.com/machine-learning/latest/dg/feature-processing.html"}],"answers_community":["C (100%)"],"isMC":true,"answer":"C","topic":"1","answer_images":[],"choices":{"D":"Mean substitution","A":"Listwise deletion","B":"Last observation carried forward","C":"Multiple imputation"},"answer_ET":"C","question_images":[],"unix_timestamp":1575942540,"timestamp":"2019-12-10 02:49:00"},{"id":"aXg4BWkjv2rPMdpUijZK","choices":{"D":"Create VPC peering with Amazon VPC hosting Amazon SageMaker.","C":"Create Amazon SageMaker VPC interface endpoints within the corporate VPC.","B":"Route Amazon SageMaker traffic through an on-premises network.","A":"Create a NAT gateway within the corporate VPC."},"answers_community":["C (100%)"],"question_id":312,"url":"https://www.examtopics.com/discussions/amazon/view/8370-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A company is setting up an Amazon SageMaker environment. The corporate data security policy does not allow communication over the internet.\nHow can the company enable the Amazon SageMaker service without enabling direct internet access to Amazon SageMaker notebook instances?","answer_images":[],"answer_description":"","timestamp":"2019-11-17 00:59:00","discussion":[{"poster":"DonaldCMLIN","comment_id":"22076","comments":[{"timestamp":"1647914160.0","content":"Not sure if C is correct in this particular scenario.\nFrom https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf\nPage 202 of the SageMaker Guide has:\nIf you allowed access to resources from your VPC, enable direct internet access. For Direct\ninternet access, choose Enable. Without internet access, you can't train or host models from\nnotebooks on this notebook instance unless your VPC has a NAT gateway and your security\ngroup allows outbound connect","upvote_count":"2","comment_id":"28373","comments":[{"content":"There are two possible solutions, but the safer solution and easier is trough VPC endpoints. \n \nYou can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network. And there is not problem that the notebooks does not have public internet. Because Amazon SageMaker notebook instances support Amazon Virtual Private Cloud (Amazon VPC) interface\nendpoints that are powered by AWS PrivateLink. Each VPC endpoint is represented by one or more\nElastic Network Interfaces (ENIs) with private IP addresses in your VPC subnets. Each VPC endpoint is represented by one or more\nElastic Network Interfaces (ENIs) with private IP addresses in your VPC subnets...\nso the Answer is C.","upvote_count":"5","timestamp":"1649208780.0","poster":"Selectron","comment_id":"94284"},{"content":"A may the right answer","upvote_count":"1","timestamp":"1648030680.0","poster":"rsimham","comment_id":"28374"}],"poster":"rsimham"}],"timestamp":"1647798060.0","upvote_count":"46","content":"NAT CLOUD GO OUT TO THE INTERNET, IT STILL CANNOT PREVENT DOWNLOAD MALICIOUS BY YOURSELF.\n\nTHE RIGHT ANSWER IS C.\nC.INTERFACE VPC ENDPOINT \n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf (516)\nhttps://docs.aws.amazon.com/zh_tw/vpc/latest/userguide/vpc-endpoints.html"},{"comment_id":"47357","poster":"tap123","content":"C is correct. \"The VPC interface endpoint connects your VPC directly to the Amazon SageMaker API or Runtime without an internet gateway, **NAT** device, VPN connection, or AWS Direct Connect connection.\" https://docs.aws.amazon.com/sagemaker/latest/dg/interface-vpc-endpoint.html","timestamp":"1648597440.0","upvote_count":"16"},{"comment_id":"1358056","upvote_count":"1","poster":"JonSno","content":"Selected Answer: C\nExplanation:\nThe company's data security policy does not allow internet access, so the solution must allow Amazon SageMaker to function privately within the VPC without internet access.\n\nVPC Interface Endpoints (AWS PrivateLink) for SageMaker allow services to communicate privately over the AWS network, without requiring an Internet Gateway (IGW) or NAT Gateway.\n\nExplanation:\nThe company's data security policy does not allow internet access, so the solution must allow Amazon SageMaker to function privately within the VPC without internet access.\n\nVPC Interface Endpoints (AWS PrivateLink) for SageMaker allow services to communicate privately over the AWS network, without requiring an Internet Gateway (IGW) or NAT Gateway.","timestamp":"1739839740.0"},{"timestamp":"1723017900.0","content":"The answer is C. \n- If you want to allow internet access, you must use a NAT gateway with access to the internet, for example through an internet gateway.\n- If you don't want to allow internet access, create interface VPC endpoints (AWS PrivateLink) to allow Studio Classic to access the following services with the corresponding service names. You must also associate the security groups for your VPC with these endpoints.\nThis is exactly what's written in the ref. doc given in the answer section of the question. (Check page Security and Permissions 1120- 1121)\nhttps://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf","comment_id":"1143193","poster":"Denise123","upvote_count":"1"},{"content":"C.\n\nTo enable Amazon SageMaker service without enabling direct internet access to Amazon SageMaker notebook instances, while adhering to a corporate data security policy that restricts internet communication, the company can:\n\nC. Create Amazon SageMaker VPC interface endpoints within the corporate VPC.\n\nThis option involves setting up VPC (Virtual Private Cloud) interface endpoints for Amazon SageMaker within the corporate VPC (Virtual Private Cloud). This is done using AWS PrivateLink, which allows private connectivity between AWS services using private IP addresses. By creating VPC interface endpoints, the traffic between the corporate VPC and Amazon SageMaker does not traverse the public internet, thereby meeting the corporate data security requirements.","comment_id":"1115834","upvote_count":"1","poster":"phdykd","timestamp":"1720349640.0"},{"timestamp":"1714397700.0","comment_id":"1056927","content":"Selected Answer: C\nA would allow instances in a private subnet to initiate outbound internet traffic. This is against the requirement of no direct internet access.","poster":"sonoluminescence","upvote_count":"2"},{"upvote_count":"2","comment_id":"999458","timestamp":"1709647080.0","poster":"Sharath1783","content":"Selected Answer: C\nNAT means data will go to internet. C is the right choice."},{"content":"Selected Answer: C\nOption c","poster":"Mickey321","timestamp":"1709147400.0","upvote_count":"1","comment_id":"992434"},{"upvote_count":"1","content":"Only C, endpoints.","timestamp":"1703811000.0","comment_id":"937207","poster":"ADVIT"},{"upvote_count":"1","content":"C is correct, NAT allow outband traffic pass through internet.","comment_id":"832985","poster":"jackzhao","timestamp":"1694171760.0"},{"comments":[{"poster":"Nadia0012","content":"To disable direct internet access, under Direct Internet access, simply choose Disable – use VPC only , and select the Create notebook instance button at the bottom. You are ready to go.\nfrom: https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/#:~:text=To%20disable%20direct%20internet%20access%2C%20under%20Direct%20Internet%20access%2C%20simply,running%2C%20without%20direct%20internet%20access.","timestamp":"1694082300.0","upvote_count":"1","comment_id":"831865"},{"comment_id":"831856","upvote_count":"1","content":"If you want to allow internet access, you must use a example through an internet gateway. If you don't want to allow internet access, NAT gateway with access to the internet, for create interface VPC endpoints (AWS PrivateLink) to allow Studio to access the following services with the corresponding service names. You must also associate the security groups for your VPC with these endpoints.","poster":"Nadia0012","timestamp":"1694081940.0"}],"content":"Selected Answer: C\nTo prevent SageMaker from providing internet access to your Studio notebooks, you can disable internet access by specifying the VPC only network access type when you the onboard to Studio or call CreateDomain API. As a result, you won't be able to run a Studio notebook unless your VPC has an interface endpoint to the SageMaker API and runtime, or a NAT gateway with internet access, and your security groups allow outbound connections.","timestamp":"1694081940.0","comment_id":"831854","poster":"Nadia0012","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: C\nA VPC interface endpoint is a private connection between a VPC and Amazon SageMaker that is powered by AWS PrivateLink. With a VPC interface endpoint, traffic between the VPC and Amazon SageMaker never leaves the Amazon network.","comment_id":"804267","timestamp":"1691661480.0","poster":"AjoseO"},{"content":"Selected Answer: C\nPage 3438 of https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf","poster":"Ob1KN0B","comment_id":"655310","upvote_count":"2","timestamp":"1677611640.0"},{"poster":"ovokpus","content":"Selected Answer: C\nVPC Interface endpoints","comment_id":"622508","upvote_count":"3","timestamp":"1672061040.0"},{"upvote_count":"5","poster":"gcpwhiz","timestamp":"1651832700.0","comment_id":"336929","content":"If the question just had the last sentence, the answer would be A or C, per this page:https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html. \"To disable direct internet access, you can specify a VPC for your notebook instance. By doing so, you prevent SageMaker from providing internet access to your notebook instance. As a result, the notebook instance won't be able to train or host models unless your VPC has an interface endpoint (PrivateLink) or a NAT gateway, and your security groups allow outbound connections.\"\n\nHOWEVER, the question has more context that internet access is not allowed by the corporate policy. (\"When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.\") Therefore, the answer must be ONLY C."},{"timestamp":"1651814940.0","comment_id":"282322","poster":"scuzzy2010","content":"Answer is C. From https://docs.aws.amazon.com/sagemaker/latest/dg/interface-vpc-endpoint.html -> \n\"The VPC interface endpoint connects your VPC directly to the SageMaker API or Runtime without an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. The instances in your VPC don't need public IP addresses to communicate with the SageMaker API or Runtime.\"","upvote_count":"3"},{"timestamp":"1651505640.0","comment_id":"280322","content":"I see a lot of people employing pretzel logic to try to explain why they should be using NAT. The question states no internet communication. Period. No internet means no NAT. Answer is C.","upvote_count":"5","poster":"cloud_trail"},{"timestamp":"1651216140.0","comment_id":"211302","upvote_count":"2","poster":"Omar_Cascudo","content":"The point here is that SM notebooks need internet access to download updates and some open data. Although C is ok, it won't allow SM notebook to download such data. NAT GW will allow this action. I go with A."},{"content":"Notebooks are Internet-enabled by default, If disabled, your VPC needs an interface endpoint (PrivateLink) or NAT Gateway, and allow outbound connections, for training and hosting to work. A doesn't have an outbound connection allowed the correct answer must be C.","poster":"weslleylc","upvote_count":"3","timestamp":"1650928740.0","comment_id":"201649"},{"upvote_count":"1","timestamp":"1650783300.0","comment_id":"169064","poster":"syu31svc","content":"Answer is C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/interface-vpc-endpoint.html"},{"upvote_count":"6","timestamp":"1650767700.0","poster":"jonclem","comment_id":"141077","content":"I'd be inclined to say A myself. Having Architected and built environments using PrivateLink (VPC Endpoints) the fundamental reason behind them is to keep your infra accessible in a private capacity. \n\nSo, say you are connecting from corp office to the Cloud and don't want the end-user exposing any data over the public internet, you would utilise an Endpoint connection. \n\n@Donald... take your finger off CAPS dude ! You can make your comment without \"shouting\" !"},{"comment_id":"107259","poster":"ardisch","upvote_count":"1","timestamp":"1650426120.0","content":"The right answer is C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf (927)"},{"content":"Both A & C are correct","comments":[{"content":"Notebooks are internet enabled by default. I fdisabled, VPC needs an interface enpoint or NAT gateway, and allow outbound connections, for training and hosting to work.","comment_id":"105614","poster":"kumarvn","upvote_count":"1","timestamp":"1650187680.0"}],"upvote_count":"1","poster":"kumarvn","comment_id":"105606","timestamp":"1650046260.0"},{"content":"Has to be C","comment_id":"102661","timestamp":"1649646780.0","upvote_count":"2","poster":"C10ud9"},{"content":"C is correct, NAT used when company need to access internet from VPC, VPC interface endpoint allow instances in VPC access to the other AWS service, see: https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html","upvote_count":"1","poster":"roytruong","comment_id":"98760","timestamp":"1649578500.0"},{"upvote_count":"3","content":"Answer should be A\nTo have VPC without Direct Internet access.\nNotebook needs to be in \n either be in a private subnet with a NAT or to access the internet back through a virtual private gateway. \nIf you just use VPC with Interface Gateway\nin that case Traffic to Interface VPC Endpoints will still go through your VPC. \nBUT Traffic to Gateway VPC Endpoints like Amazon S3 and DynamoDB will go through the public internet\nAND \nSageMaker provides a network interface that allows the notebook to communicate with the internet through a VPC managed by Amazon SageMaker(Public Internet)","comment_id":"95227","poster":"aws_razor","timestamp":"1649392380.0"},{"comment_id":"73492","content":"The correct answer HAS TO BE C\n\nIt says no communication over internet, period. So NAT is out.","poster":"dhs227","upvote_count":"2","timestamp":"1649146980.0"},{"timestamp":"1648879080.0","upvote_count":"3","content":"The question doesn't mention Hosting so NAT seems not suitable. C is better.","comment_id":"51109","poster":"Phong"},{"poster":"devsean","timestamp":"1648814100.0","upvote_count":"3","comments":[{"timestamp":"1648951380.0","upvote_count":"1","comment_id":"68244","content":"but you need to allow outbound connections when using NAT.","poster":"georgeZ"}],"comment_id":"50231","content":"A is correct. \n\"Without internet access, you can't train or host models from\nnotebooks on this notebook instance unless your VPC has a NAT gateway and your security.\"\ngroup allows outbound connections\n\nFrom here on page 212 (pdf page 220), bullet \"g\":\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf"},{"poster":"WWODIN","comment_id":"35313","upvote_count":"2","content":"Seems C is necessary step 1\nA is need if sagemaker need external internal access. But if all it need is training channel in S3, S3 vpc endpoint should be enough.\n\nSo if 1 answer only, C\n2 answers selection: C & A","timestamp":"1648489080.0"},{"comments":[{"upvote_count":"1","content":"The question states: \"The corporate data security policy does not allow communication over the internet.\" That is UNEQUIVOCAL. No internet. No if's and's or but's about it.","timestamp":"1651537620.0","poster":"cloud_trail","comment_id":"281512"},{"poster":"BigEv","upvote_count":"10","timestamp":"1648518240.0","comment_id":"43553","content":"What does this mean \"The corporate data security policy does not allow communication over the internet\" ?"}],"timestamp":"1648048860.0","poster":"JayK","upvote_count":"2","comment_id":"35291","content":"Answer is A. The question is not saying that the Amazon Sagemaker notebook should not have internet access. All it says that communication OVER the internet needs to be secure"}],"topic":"1","exam_id":26,"isMC":true,"answer_ET":"C","unix_timestamp":1573948740,"answer":"C","question_images":[]},{"id":"sKt8GFApeDHYKQx3OWQy","question_images":[],"answers_community":["B (91%)","5%"],"choices":{"C":"Initialize the model with random weights in all layers and replace the last fully connected layer.","B":"Initialize the model with pre-trained weights in all layers and replace the last fully connected layer.","D":"Initialize the model with pre-trained weights in all layers including the last fully connected layer.","A":"Initialize the model with random weights in all layers including the last fully connected layer."},"answer_description":"","answer_ET":"B","discussion":[{"content":"Ans B sounds correct","upvote_count":"28","comment_id":"28377","poster":"rsimham","timestamp":"1632901080.0"},{"comment_id":"804272","timestamp":"1676030580.0","content":"Selected Answer: B\nIn transfer learning, a pre-trained model is used as a starting point to train a new model on a different task, typically using a smaller dataset. The pre-trained model contains weights that have been learned from a large amount of data on a related task, and these weights can be leveraged to train the new model more efficiently.\n\nTo re-train the model with the custom data, the Specialist should initialize the model with pre-trained weights in all layers, as these weights can provide a good starting point for the new task. The Specialist should then replace the last fully connected layer, which is responsible for making the final predictions, as this layer will likely need to be modified to reflect the new task. By keeping the pre-trained weights in the other layers, the Specialist can take advantage of the knowledge learned from the previous task, and potentially speed up the training process.","poster":"AjoseO","upvote_count":"9"},{"comment_id":"1358058","poster":"JonSno","timestamp":"1739839860.0","upvote_count":"1","content":"Selected Answer: B\nExplanation:\nThe Machine Learning Specialist wants to use transfer learning with an existing model trained on general object images and fine-tune it for vehicle make and model classification. The best approach is:\n\nUse pre-trained weights from the existing model for feature extraction.\nReplace the last fully connected (FC) layer to match the number of vehicle classes.\nFine-tune the new model on the vehicle dataset.\nWhy This Works?\nLower training time: The model has already learned useful features from general objects (e.g., edges, shapes).\nImproves accuracy: Instead of training from scratch, transfer learning leverages knowledge from large datasets (e.g., ImageNet).\nAvoids catastrophic forgetting: Reusing pre-trained weights preserves learned low- and mid-level features while adapting the last layer for new classes."},{"poster":"itsme1","content":"Selected Answer: D\nTransfer learning helps accelerate the training and at this point, model has yet to learn from the new data. So, all layers including the fully-connected by replaced. Eventually, the training will update the fully-connected layer. The question is about initialization, so we should initialize the fully-connected layers too.","comment_id":"1281291","upvote_count":"1","timestamp":"1725921780.0"},{"content":"Selected Answer: B\nA. NO - random weights does not allow transfer learning\nB. YES - the last layer gives the final classes, we want to have new classes\nC. NO - random weights does not allow transfer learning\nD. NO - the last layer gives the final classes, we want to have new classes","timestamp":"1694610840.0","comment_id":"1006656","upvote_count":"2","poster":"loict"},{"comment_id":"992421","timestamp":"1693242180.0","content":"Selected Answer: B\nOption B","upvote_count":"1","poster":"Mickey321"},{"poster":"kaike_reis","upvote_count":"2","timestamp":"1690827300.0","content":"Selected Answer: B\nFor Transfer Learning, A and C are incorrect because we restart the model. The correct is letter B","comment_id":"968386"},{"timestamp":"1687345020.0","comment_id":"929373","content":"B. The reason is, fine-tuning a model means to use the weights/biases trained before. also no matter which strategy you go for in transfer learning (fine-tuning or feature extraction) you always replace the last or last few layers.","poster":"SRB1337","upvote_count":"3"},{"timestamp":"1686291060.0","upvote_count":"1","poster":"mirik","content":"Selected Answer: C\nThe task is to \" to re-train it with the custom data\". That means, it is not transfer learning anymore. The \"transfer learning\" is just a title to make a question tricky.\nSo, in this case we should randomize the weights and retrain whole model from scratch on custom user's images only.\nThe correct answer is C.","comment_id":"918963","comments":[{"poster":"FloKo","timestamp":"1690777320.0","comment_id":"967746","upvote_count":"1","content":"I think retraining revers in this context to the training on the custom data that the expert as already conducted before thinking about transfer learning."}]},{"content":"The task is to \" to re-train it with the custom data\". That means, it is not transfer learning anymore. The \"transfer learning\" is just a title to make a question tricky.\nSo, in this case we should randomize the weights and retrain whole model from scratch on custom user's images only.\nThe correct answer is C.","comment_id":"918961","poster":"mirik","timestamp":"1686291000.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1670549880.0","content":"Selected Answer: B\nThe fully connected layer will need to be trained from scratch to incorporate the features of his domain problem (Car models)","poster":"Peeking","comment_id":"739656"},{"upvote_count":"2","content":"12-sep exam","poster":"Shailendraa","comment_id":"667388","timestamp":"1663009440.0"},{"timestamp":"1658343600.0","comment_id":"634206","upvote_count":"1","content":"D is the best - here is why\nQuestion is not to design a final production with deep lense - it is to use it as a dev platform to comeup with a edge ML vs. dump load all to S3 -which is very wasteful! AWS did not mae deeplense as a toy for devs! it is meant to help companies experiment with edge ML And then copy and reuse the open hardware platfom","poster":"chrisdavidi"},{"content":"Selected Answer: B\none of the method to implement transfer learning","poster":"ckkobe24","comment_id":"599662","timestamp":"1652196960.0","upvote_count":"2"},{"upvote_count":"1","poster":"DzR","comment_id":"264738","timestamp":"1635000480.0","content":"I will go with B, we are mainly concerned with the output layer for us to get the desired results, hence we need to replace it."},{"upvote_count":"2","poster":"bobdylan1","content":"B is correct","timestamp":"1634934600.0","comment_id":"254552"},{"upvote_count":"2","content":"Actually, it should be NONE of IT!.... it should be like B with exception that 20-40% top layers should be retrained :) -- this is classic transfer learning setup, so B is the answer here.","poster":"sebtac","comment_id":"174069","timestamp":"1634527260.0"},{"upvote_count":"4","timestamp":"1634450640.0","content":"Since it is transfer learning where you retain knowledge from a solved problem, weights are to be pre-trained. So A and C are wrong. Between B and D, D keeps the last layer but that is not what you want since the question mentions a change of general objects to more specific types. So answer is B","comment_id":"169074","poster":"syu31svc"},{"upvote_count":"1","comment_id":"152745","poster":"scuzzy2010","comments":[{"comment_id":"212451","poster":"yeetusdeleetus","upvote_count":"1","timestamp":"1634897160.0","content":"Transfer learning doesn't work like that. All images are 'somewhat' similar, so a pretrained neural network that was trained on different types of images is still better than random weights. B."},{"content":"If you do option C, you're not doing transfer learning at all. You might just as well start over with your own architecture.","poster":"cloud_trail","upvote_count":"2","comment_id":"278547","timestamp":"1635190440.0"},{"upvote_count":"1","comment_id":"1064952","poster":"akgarg00","timestamp":"1699371660.0","content":"A deep learning model takes lot of time and resources. So, if you have similar images, it is better to start with pre-trained weights as they are somewhat nearer to actual weights and onus of classification can rest on last feed forward layer."}],"content":"I feel the answer is \"C\" because it was not trained on images of \"similar\" objects, it was trained on images of \"general\" objects. If it were trained on similar objects, then answer would be \"B\".","timestamp":"1634298600.0"},{"poster":"tff","upvote_count":"3","content":"https://www.hackerearth.com/practice/machine-learning/transfer-learning/transfer-learning-intro/tutorial/\nanswer is B.\nRead the scenario:\nThe target dataset is large and similar to the base training dataset.\nSince the target dataset is large, we have more confidence that we won’t overfit if we try to fine-tune through the full network. Therefore, we:\n\n1.Remove the last fully connected layer and replace with the layer matching the number of classes in the target dataset;\n2.Randomly initialize the weights in the new fully connected layer;\n3.Initialize the rest of the weights using the pre-trained weights, i.e., unfreeze the layers of the pre-trained network;\n4.Retrain the entire neural network;\nSo, first look at 3, and then at 1.","comment_id":"125670","timestamp":"1633999140.0"},{"upvote_count":"1","content":"From what I read in https://kharshit.github.io/blog/2018/08/10/transfer-learning\nChoice seems to be C.\nCase III: Large dataset, Similar data\n\nremove the last fully connected layer and replace with a layer matching the number of classes in the new data set\nrandomly initialize the weights in the new fully connected layer\ninitialize the rest of the weights using the pre-trained weights\nre-train the entire neural network\nCase IV: Large dataset, Different data\n\nremove the last fully connected layer and replace with a layer matching the number of classes in the new data set\nretrain the network from scratch with randomly initialized weights\nalternatively, you could just use the same strategy as the “large and similar” data case","comment_id":"125181","poster":"fw","timestamp":"1633704000.0"},{"timestamp":"1633599420.0","content":"B is correct, replace only the last layer in transfer learning","comment_id":"102663","upvote_count":"2","poster":"C10ud9"},{"content":"B is right","upvote_count":"2","timestamp":"1633227420.0","comment_id":"89711","poster":"roytruong"}],"answer":"B","timestamp":"2019-12-10 02:58:00","question_text":"A Machine Learning Specialist is training a model to identify the make and model of vehicles in images. The Specialist wants to use transfer learning and an existing model trained on images of general objects. The Specialist collated a large custom dataset of pictures containing different vehicle makes and models.\nWhat should the Specialist do to initialize the model to re-train it with the custom data?","exam_id":26,"isMC":true,"unix_timestamp":1575943080,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/10081-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":313,"topic":"1"},{"id":"U5ibTKr7HgEpL91Yn3f3","answer_images":[],"answer_description":"","answers_community":["A (77%)","C (15%)","8%"],"unix_timestamp":1573952880,"topic":"1","question_images":[],"question_text":"An office security agency conducted a successful pilot using 100 cameras installed at key locations within the main office. Images from the cameras were uploaded to Amazon S3 and tagged using Amazon Rekognition, and the results were stored in Amazon ES. The agency is now looking to expand the pilot into a full production system using thousands of video cameras in its office locations globally. The goal is to identify activities performed by non-employees in real time\nWhich solution should the agency consider?","exam_id":26,"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/8374-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":314,"timestamp":"2019-11-17 02:08:00","choices":{"B":"Use a proxy server at each local office and for each camera, and stream the RTSP feed to a unique Amazon Kinesis Video Streams video stream. On each stream, use Amazon Rekognition Image to detect faces from a collection of known employees and alert when non-employees are detected.","D":"Install AWS DeepLens cameras and use the DeepLens_Kinesis_Video module to stream video to Amazon Kinesis Video Streams for each camera. On each stream, run an AWS Lambda function to capture image fragments and then call Amazon Rekognition Image to detect faces from a collection of known employees, and alert when non-employees are detected.","A":"Use a proxy server at each local office and for each camera, and stream the RTSP feed to a unique Amazon Kinesis Video Streams video stream. On each stream, use Amazon Rekognition Video and create a stream processor to detect faces from a collection of known employees, and alert when non-employees are detected.","C":"Install AWS DeepLens cameras and use the DeepLens_Kinesis_Video module to stream video to Amazon Kinesis Video Streams for each camera. On each stream, use Amazon Rekognition Video and create a stream processor to detect faces from a collection on each stream, and alert when non-employees are detected."},"answer":"A","discussion":[{"content":"Answer is \"A\". C and D are out as DeepLens is not offered as a commercial product. It is purely for developers to experiment with. \nFrom https://aws.amazon.com/deeplens/device-terms-of-use/\n\" (i) you may use the AWS DeepLens Device for personal, educational, evaluation, development, and testing purposes, and not to process your production workloads;\"\n\nA is correct as it's will analyse live video streams instead of images. \nFrom https://aws.amazon.com/rekognition/video-features/\n\"Amazon Rekognition Video can identify known people in a video by searching against a private repository of face images. \"","comment_id":"152751","poster":"scuzzy2010","timestamp":"1650221820.0","upvote_count":"42","comments":[{"timestamp":"1706732400.0","comment_id":"968392","poster":"kaike_reis","content":"Agree as well, besides that: (D) uses Rekognition with Image mode, which is wrong for this case.","upvote_count":"1"},{"timestamp":"1651828500.0","upvote_count":"2","content":"Agreed","comment_id":"441325","poster":"Mezaji"}]},{"comments":[{"comment_id":"40502","timestamp":"1648065960.0","poster":"cybe001","comments":[{"poster":"scuzzy2010","timestamp":"1649134260.0","upvote_count":"5","content":"DeepLens is for developers only, it is not available as a commercial product.","comment_id":"152747"}],"content":"C is the correct answer. We could use A, since it is for security service, DeepLens allows to notify the security (through aws lamda) immediately when it sees non employee at the office location. So C is more appropriate for the problem than A.","upvote_count":"6"},{"upvote_count":"8","timestamp":"1648109280.0","comment_id":"64315","content":"A bit off topic but yeah, how could you justify using deep lens for production. Cameras have viewing angles, weather proofing, network connectivity issues (Wifi only), infra red for low lighting conditions, no power over ethernet? Using Deeplens would be laughable for a full production system.","poster":"sdsfsdsf"}],"poster":"WWODIN","content":"Why not A? \nDeepLens is for development purpose and much more expensive than just a camera.\nThey are referring to 1000 camera in production scale?","timestamp":"1648065660.0","comment_id":"35315","upvote_count":"12"},{"upvote_count":"1","comment_id":"1358061","poster":"JonSno","content":"Selected Answer: A\nAns - A -- Proxy Server + Kinesis Video Streams + Rekognition Video\n\nThe goal is to scale from 100 cameras to thousands and perform real-time detection of non-employees in office locations globally. The best approach is to use Amazon Kinesis Video Streams + Amazon Rekognition Video for real-time face detection.","timestamp":"1739840100.0"},{"upvote_count":"3","comment_id":"1143246","timestamp":"1723020180.0","content":"The correct answer is D. \nVery tricky one but re-read the 2nd sentence in the question; \n“Images from the cameras were uploaded to Amazon S3 and tagged using Amazon Rekognition, and the results were stored in Amazon ES.” \nSo, we have ‘images’ as training data, not videos. This is why it can not be option C - where it says to use Amazon Recognition Video. The only option mentioning Amazon Recognition Image is the option D.\n\nAlso check: https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html\n“...For example, each time a person arrives at your residence, your door camera can upload a photo of the visitor to Amazon S3. This triggers a Lambda function that uses Amazon Rekognition API operations to identify your guest. You can run analysis directly on images that are stored in Amazon S3 without having to load or move the data.”","poster":"Denise123"},{"poster":"phdykd","upvote_count":"1","content":"A is answer","timestamp":"1720350420.0","comment_id":"1115847"},{"timestamp":"1716312480.0","upvote_count":"2","poster":"sukye","comment_id":"1076585","content":"Selected Answer: A\nA not B: Use Amazon Rekognition Video instead of Amazon Rekognition Image in this case."},{"content":"Selected Answer: A\nA is correct!","upvote_count":"1","comment_id":"1067021","poster":"elvin_ml_qayiran25091992razor","timestamp":"1715319000.0"},{"upvote_count":"1","timestamp":"1714398540.0","content":"Selected Answer: A\nDeepLens is overkill for mass systems","comment_id":"1056934","poster":"sonoluminescence"},{"content":"Selected Answer: C\nA. NO - thousands of cameras would choke network bandwidth\nB. NO - thousands of cameras would choke network bandwidth\nC. YES - DeepLens is made for edge computing; it might be EOL / Not commercially available, but if they did not want you to use DeepLens the question would not have come in the first place\nD. NO - use Amazon Rekognition Video directly instead of Amazon Rekognition Image","poster":"loict","timestamp":"1710402360.0","comment_id":"1007246","upvote_count":"2"},{"content":"Why A and not B? Can someone please explain it?","timestamp":"1710247260.0","comment_id":"1005652","poster":"DavidRou","upvote_count":"1"},{"upvote_count":"1","comment_id":"992418","content":"Selected Answer: A\nOption A","poster":"Mickey321","timestamp":"1709146920.0"},{"content":"From Chat GPT\nThe solution that the agency should consider is option A: Use a proxy server at each local office and for each camera, and stream the RTSP feed to a unique Amazon Kinesis Video Streams video stream. On each stream, use Amazon Rekognition Video and create a stream processor to detect faces from a collection of known employees and alert when non-employees are detected.\n\nBy using a proxy server at each local office and streaming the RTSP feed to individual Amazon Kinesis Video Streams video streams, the agency can efficiently handle the large number of video cameras in different office locations. Using Amazon Rekognition Video, the agency can create a stream processor to detect faces from a collection of known employees. This allows for real-time identification of non-employees based on facial recognition. Alerts can then be generated when non-employees are detected, ensuring that the agency is able to identify and respond to potential security threats in real-time.","poster":"strike3test","upvote_count":"2","timestamp":"1708063680.0","comment_id":"982163"},{"comment_id":"929381","timestamp":"1703164020.0","upvote_count":"1","content":"I initially thought it is C but looks like A makes more sense here.","poster":"nilmans"},{"upvote_count":"2","poster":"jyrajan69","timestamp":"1701229860.0","content":"The DeepLens Service will reach EOL at the end of Jan 2024, so more than likely that this question will not be asked in the exam","comment_id":"908966"},{"upvote_count":"1","content":"Selected Answer: D\nD is the answer now, DeepLens is used for situations like this!","comments":[{"upvote_count":"2","poster":"cpal012","content":"Maybe, its EOL Jan 2024","timestamp":"1695660240.0","comment_id":"850371"}],"poster":"Valcilio","comment_id":"832772","timestamp":"1694158260.0"},{"poster":"expertguru","content":"Think big picture - you tested something (let say code python) and ready to implement into prod will you move python code or java code! Here in this particular case, they tested with actual video camera and they did not say deeplense so answer is A! For knowledge sake if they say in real exam it is tested with deeplense ---then ideal solution should be model inference happening at deeplense itself with search against existing employees and send back model inference when it detect new faces who are not employees back to cloud may be S3","upvote_count":"2","comment_id":"768854","timestamp":"1688750640.0"},{"comment_id":"601424","timestamp":"1668406980.0","poster":"Sivadharan","upvote_count":"4","content":"Selected Answer: A\nAnswer is \"A\".\nAs mentioned in below user comment, DeepLens is not offered as a commercial product.\nhttps://aws.amazon.com/deeplens/device-terms-of-use/"},{"content":"Answer is C based on this exact same article answer: https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video.html \nRekognition stream processor doesn't need Lambda.","comment_id":"521139","upvote_count":"4","timestamp":"1657479840.0","poster":"geekgirl007"},{"upvote_count":"2","timestamp":"1654747620.0","poster":"pp13","content":"It cannot be option C & D as it is given in terms n conditions of using deeplens that its not for production purpose","comment_id":"497373"},{"upvote_count":"1","comment_id":"490478","timestamp":"1653886740.0","content":"Is it D? By process of elimination, it has to be C & D. C is eliminated as it's to detect faces, and not really detect the faces of known employees","poster":"anttan"},{"upvote_count":"1","poster":"Huy","comment_id":"399753","timestamp":"1651674660.0","content":"A. https://aws.amazon.com/blogs/machine-learning/easily-perform-facial-analysis-on-live-feeds-by-creating-a-serverless-video-analytics-environment-with-amazon-rekognition-video-and-amazon-kinesis-video-streams/"},{"comment_id":"353565","poster":"StelSen","timestamp":"1651509660.0","upvote_count":"2","content":"I will go with 'A'. Because setting up DeepLens will cost S$304.59. Whereas setting up proxy server using Raspberry PI (production eqivalent) will take around S$50 + Camera may be $50. So overall solution is for A might be $100. Company already did a pilot with some existing camera. So why intro DeepLens now. Use cost effective solution."},{"upvote_count":"4","content":"Its C!\nhttps://docs.aws.amazon.com/rekognition/latest/dg/streaming-video.html","timestamp":"1651387380.0","comment_id":"320398","poster":"srinu3054"},{"content":"D is correct. Because you DEFINITELY need the Lambda Function to \"CALL\" the Amazon Rekognition to start the job (detect faces)!","timestamp":"1651183560.0","upvote_count":"2","comment_id":"298635","comments":[{"comment_id":"316753","upvote_count":"2","timestamp":"1651292400.0","poster":"achiko","content":"C. You don't need Lambda between video stream and rekognition"}],"poster":"SophieSu"},{"upvote_count":"3","content":"D is the correct answer. The point is that the lambda function is necessary before the Amazon Rekognition. See \"https://docs.aws.amazon.com/rekognition/latest/dg/stored-video-lambda.html\". \"You can use Lambda functions with Amazon Rekognition Video operations. For example, the following diagram shows a website that uses a Lambda function to automatically start analysis of a video when it's uploaded to an Amazon S3 bucket. When the Lambda function is triggered, it calls StartLabelDetection to start detecting labels in the uploaded video. For information about using Lambda to process event notifications from an Amazon S3 bucket, see Using AWS Lambda with Amazon S3 Events.\"","timestamp":"1650878220.0","comment_id":"298633","poster":"SophieSu"},{"comment_id":"295050","poster":"Aashi22","content":"Cofusion b/w option C & D. Answer should be option D as per link https://aws.amazon.com/blogs/machine-learning/video-analytics-in-the-cloud-and-at-the-edge-with-aws-deeplens-and-kinesis-video-streams/","upvote_count":"2","timestamp":"1650747240.0"},{"poster":"scuzzy2010","comment_id":"282324","content":"Answer is \"A\". C and D are out because DeepLens is NOT a commercial product, it's sold only to developers. \nA mentions 'stream processor' , and as mentioned here is specifically used to detect and recognise faces in streaming video -> (https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateStreamProcessor.html) \"Amazon Rekognition stream processor that you can use to detect and recognize faces in a streaming video.\"","upvote_count":"4","timestamp":"1650684840.0"},{"content":"Option D. I think that the goal for the company is to do the facial recognition at the edge, which is what DeepLens does. The pilot program cameras were not ML enabled so they had to upload all all images from all cameras to S3 and then use Rekognition. Not practical with 1000s of \"dumb\" cameras. DL can do object detection and the call Rekognition to identify non-employees and the send alerts.","poster":"cloud_trail","comment_id":"278556","timestamp":"1650675120.0","upvote_count":"1"},{"content":"Deep lens cannot be used at large scale so it cannot be C or D\n--Between A and B , A mentions Amazon Rekognition Video which is used for detection on video data.\n-- Hence ANSWER is A","poster":"harmanbirstudy","upvote_count":"3","timestamp":"1650489180.0","comment_id":"263411"},{"content":"The pilot program is done by using image, I wonder what would happen all of the sudden the solution is now done by using video, imagine the cost incurred by processing and storing these info, so D looks more sensible","timestamp":"1650403440.0","upvote_count":"3","comment_id":"194987","poster":"Potato_Noodle"},{"timestamp":"1650308040.0","content":"Regarding the official page for deeplans, deeplans has capability for face and object detection. This event could trigger the lambda function which will call built-in Amazon Recognition model trained on pictures of employees. \nI think that D is the right answer.","upvote_count":"1","poster":"oMARKOo","comment_id":"190872"},{"timestamp":"1650242880.0","comment_id":"176943","upvote_count":"1","poster":"Th3Dud3","content":"Hmmm. >>Use a proxy server at each local office and for each camera<< one server per camera? Wow!"},{"timestamp":"1649102100.0","comments":[{"timestamp":"1649339340.0","content":"Especially if it's not even sold for production use. It is purely a developers tool.","upvote_count":"1","poster":"scuzzy2010","comment_id":"152748"}],"content":"Installing thousands of AWS DeepLens cameras globally is laughable. C & D can be immediately discredited.","upvote_count":"1","comment_id":"134657","poster":"ml_sexpert"},{"comment_id":"102664","upvote_count":"3","content":"Agree C","poster":"C10ud9","timestamp":"1648978740.0"},{"upvote_count":"1","timestamp":"1648268160.0","content":"The answer HAS TO BE C. \nA and B are eliminated to begin with. Deploying thousands of camera globally while managing who knows how many proxy servers at each location is just ridiculous.\nBetween C and D, you know better. Fully managed service is better in this use case.","poster":"dhs227","comment_id":"73495"},{"poster":"vetal","upvote_count":"1","comment_id":"28747","timestamp":"1647969120.0","content":"Amazon recognition can detect faces, labels, etc but not performed activities. On the other hand, amazon recognition video can easily detect activities and even track the person."},{"timestamp":"1647728400.0","poster":"DonaldCMLIN","comment_id":"22082","comments":[{"poster":"Antriksh","upvote_count":"1","comment_id":"108598","content":"Correct. Answer is C. Additional hypothesis is how are we going to track activities based on frames/images. So clearly answer D is wrong.","timestamp":"1649008860.0"},{"comment_id":"28378","upvote_count":"3","timestamp":"1647788700.0","poster":"rsimham","content":"Agree with C"}],"upvote_count":"8","content":"AMAZON REKOGNITION VIDEO USES AMAZON KINESIS VIDEO STREAMS TO RECEIVE AND PROCESS A VIDEO STREAM. THE ANALYSIS RESULTS ARE OUTPUT FROM AMAZON REKOGNITION VIDEO TO A KINESIS DATA STREAM AND THEN READ BY YOUR CLIENT APPLICATION. AMAZON REKOGNITION VIDEO PROVIDES A STREAM PROCESSOR (CREATESTREAMPROCESSOR) THAT YOU CAN USE TO START AND MANAGE THE ANALYSIS OF STREAMING VIDEO.\n\nTHE ANSWER SHOULD BE C."}],"isMC":true},{"id":"cqB74Q0KdfcI883Y4pYN","exam_id":26,"discussion":[{"timestamp":"1632460980.0","poster":"vetal","comment_id":"27158","comments":[{"timestamp":"1632641640.0","upvote_count":"12","content":"should be D","poster":"WWODIN","comment_id":"34980"},{"upvote_count":"8","content":"Should be D.\nUse Glue to do ETL to Hash the card number","timestamp":"1634785440.0","comment_id":"137859","poster":"zzeng"},{"poster":"Antriksh","comment_id":"108319","content":"Answer would be D","timestamp":"1634044920.0","upvote_count":"9"}],"content":"Why not D? When the data encrypted on S3 and SageMaker uses the same AWS KMS key it can use encrypted data there.","upvote_count":"36"},{"poster":"cybe001","comment_id":"37744","content":"D is correct","upvote_count":"8","timestamp":"1632844740.0"},{"poster":"Ganshank","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/big-data/detect-and-process-sensitive-data-using-aws-glue-studio/\nAWS Glue can be used for detecting and processing sensitive data.","timestamp":"1740530880.0","comment_id":"1361641","upvote_count":"2"},{"content":"Selected Answer: D\nUse AWS KMS for encryption and AWS Glue to redact credit card numbers \nReasoning:\nAWS KMS (Key Management Service) encrypts data at rest in Amazon S3 and during processing in Amazon SageMaker.\nAWS Glue can be used to redact sensitive data before processing, ensuring that credit card numbers are removed from datasets before being used for ML.\nComplies with PCI DSS requirements for handling payment information securely.","poster":"JonSno","upvote_count":"1","comment_id":"1357047","timestamp":"1739653560.0"},{"timestamp":"1727163900.0","comment_id":"973029","poster":"Mickey321","upvote_count":"2","content":"Selected Answer: D\nThe reason for this choice is that AWS KMS is a service that allows you to easily create and manage encryption keys and control the use of encryption across a wide range of AWS services and in your applications1. By using AWS KMS, you can encrypt the data on Amazon S3, which is a durable, scalable, and secure object storage service2, and on Amazon SageMaker, which is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly3. This way, you can protect the data at rest and in transit."},{"timestamp":"1694586960.0","poster":"loict","upvote_count":"2","comment_id":"1006271","content":"Selected Answer: D\nA. NO - no need for custom encryption\nB. NO - IAM Policies are not to encrypt\nC. NO - launch configuration is not to encrypt\nD. YES"},{"upvote_count":"1","poster":"Venkatesh_Babu","comment_id":"961668","content":"Selected Answer: D\nI think d is correct","timestamp":"1690207260.0"},{"poster":"Valcilio","upvote_count":"4","comment_id":"832660","content":"Selected Answer: D\nIt's D, KMS key can be used for encrypting the data at rest!","timestamp":"1678262700.0"},{"upvote_count":"3","content":"Selected Answer: D\nagreed with D","timestamp":"1669426800.0","comment_id":"727198","poster":"ystotest"},{"upvote_count":"1","comment_id":"381632","timestamp":"1636145460.0","poster":"jerto97","content":"IMHO, the problem with the question is that it is not clear whether the credit card number is used in the model. In that case discarding is never a good option. Hashing should be a safe option to keep it in the learning path"},{"comment_id":"277517","poster":"cloud_trail","content":"It's gotta be D but C is a clever fake answer. Use PCA to reduce the length of the credit card number? That's a clever joke, as if reducing the length of a character string is the same as reducing dimensionality in a feature set.","upvote_count":"3","timestamp":"1635023580.0"},{"content":"Can Glue do redaction?","comments":[{"poster":"cloud_trail","comment_id":"277998","content":"Just have the Glue job remove the credit card column.","timestamp":"1636105380.0","upvote_count":"1"}],"timestamp":"1635018420.0","comment_id":"271971","upvote_count":"1","poster":"cnethers"},{"upvote_count":"1","content":"Encryption on AWS can be done using KMS so D is the answer","comment_id":"164322","timestamp":"1634964240.0","poster":"syu31svc"},{"poster":"roytruong","upvote_count":"1","content":"D is correct","comment_id":"98662","timestamp":"1633957500.0"},{"poster":"PRC","upvote_count":"4","content":"D..KMS fully managed and other options are too whacky..","comment_id":"65509","timestamp":"1633398120.0"},{"upvote_count":"1","content":"D is correct","comment_id":"58341","timestamp":"1633396080.0","poster":"AKT"},{"content":"Ans D is correct","timestamp":"1633106520.0","comment_id":"52847","upvote_count":"2","poster":"bhavesh0124"},{"content":"is this really a viable reference? so misleading.","poster":"satya_alapati","upvote_count":"2","comment_id":"48658","timestamp":"1632965160.0"},{"content":"I think it is D as using PCA to reduce the length of credit card numbers does not seem viable","poster":"JayK","upvote_count":"6","comment_id":"35222","timestamp":"1632673800.0"}],"question_images":[],"answer":"D","answers_community":["D (100%)"],"topic":"1","answer_ET":"D","question_id":315,"answer_images":[],"timestamp":"2019-12-06 10:45:00","question_text":"A Data Engineer needs to build a model using a dataset containing customer credit card information\nHow can the Data Engineer ensure the data remains encrypted and the credit card information is secure?","unix_timestamp":1575625500,"answer_description":"","choices":{"B":"Use an IAM policy to encrypt the data on the Amazon S3 bucket and Amazon Kinesis to automatically discard credit card numbers and insert fake credit card numbers.","A":"Use a custom encryption algorithm to encrypt the data and store the data on an Amazon SageMaker instance in a VPC. Use the SageMaker DeepAR algorithm to randomize the credit card numbers.","C":"Use an Amazon SageMaker launch configuration to encrypt the data once it is copied to the SageMaker instance in a VPC. Use the SageMaker principal component analysis (PCA) algorithm to reduce the length of the credit card numbers.","D":"Use AWS KMS to encrypt the data on Amazon S3 and Amazon SageMaker, and redact the credit card numbers from the customer data with AWS Glue."},"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/9818-exam-aws-certified-machine-learning-specialty-topic-1/"}],"exam":{"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":false,"name":"AWS Certified Machine Learning - Specialty","id":26,"numberOfQuestions":369,"provider":"Amazon","isBeta":false},"currentPage":63},"__N_SSP":true}