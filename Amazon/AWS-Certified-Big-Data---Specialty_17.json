{"pageProps":{"questions":[{"id":"6h0ms6FKAoAN90RAU3nl","question_id":81,"question_images":[],"exam_id":17,"answer_description":"","discussion":[{"comment_id":"60885","timestamp":"1635388440.0","upvote_count":"8","poster":"Kuang","content":"It is D.\nThe key is \"multiple transient EMR clusters to access the same tables concurrently\".\nFor External RDS metastore, it is not recommended to write concurrently.(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-external.html)\nFor Glue, it have 1 to 10 concurrent access.(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html)"},{"comment_id":"369080","content":"D. Should be the right answer, because Amazon recommends use this configuration when you require a persistent metastore or a metastore shared by different clusters, services, applications, or AWS account. \nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html","poster":"ariane_tateishi","upvote_count":"1","timestamp":"1636103880.0"},{"upvote_count":"1","content":"answer is D. Glue crawlers can update tables when schema changes, thus requiring the LEAST operational effort.","timestamp":"1636081680.0","comment_id":"81708","poster":"srirampc"},{"content":"Answer is D https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html","timestamp":"1635682980.0","upvote_count":"2","poster":"YashBindlish","comment_id":"74763"},{"timestamp":"1635050520.0","content":"my selection B","comment_id":"52391","upvote_count":"2","poster":"san2020"},{"upvote_count":"1","content":"D is correct; the question is about \"which solution will expose the Hive metastore, ... defined on the long-running cluster\"","poster":"richardxyz","timestamp":"1634375040.0","comment_id":"48888"},{"upvote_count":"4","timestamp":"1633863000.0","comments":[{"poster":"ME2000","upvote_count":"1","timestamp":"1633906740.0","comment_id":"38390","comments":[{"upvote_count":"1","poster":"sam3787","comment_id":"47398","content":"thanks. the metadata for S3 external tables is still defined on long running clusters. So shouldn't that point to using Glue? (D as option)","timestamp":"1634125260.0"}],"content":"Here we go...\nConfiguring an External Metastore for Hive\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html\nUsing the AWS Glue Data Catalog as the Metastore for Hive\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html\n(Important point: If another cluster needs to access the table, it fails unless it has adequate permissions to the cluster that created the table. Furthermore, because HDFS storage is transient, if the cluster terminates, the table data is lost, and the table must be recreated)\nUsing an External MySQL Database or Amazon Aurora\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-external.html\n(Your Hive cluster runs using the metastore located in Amazon RDS. Launch all additional Hive clusters that share this metastore by specifying the metastore location.)"}],"content":"well long-running cluster is already operating Hive catalog on in MySQL DB on Master Node so I think moving existing database to RDS and switching is more easy than using Glue Crawler, so B","comment_id":"30546","poster":"yuriy_ber"},{"content":"D is least operational effort","upvote_count":"1","timestamp":"1633857660.0","poster":"cybe001","comment_id":"19418"},{"comment_id":"13808","content":"B and D both are correct but looks like Glue will be easily configurable so it would be D","poster":"bigdatalearner","timestamp":"1633798140.0","upvote_count":"1"},{"poster":"mattyb123","content":"Isn't it D. LEAST operational effort.","timestamp":"1632405780.0","comment_id":"8286","upvote_count":"1","comments":[{"comment_id":"8645","upvote_count":"2","poster":"mattyb123","content":"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html. Using Amazon EMR version 5.8.0 or later, you can configure Hive to use the AWS Glue Data Catalog as its metastore. We recommend this configuration when you require a persistent metastore or a metastore shared by different clusters, services, applications, or AWS accounts.","comments":[{"upvote_count":"1","poster":"mattyb123","comments":[{"content":"Thoughts anyone?","upvote_count":"1","comment_id":"8789","poster":"mattyb123","timestamp":"1632678540.0","comments":[{"comment_id":"8859","comments":[{"poster":"mattyb123","content":"Looks like D is correct. Storing external data on S3.\nhttps://docs.aws.amazon.com/glue/latest/dg/add-crawler.html. \nWhen you define an Amazon S3 data store to crawl, you can choose whether to crawl a path in your account or another account. The output of the crawler is one or more metadata tables defined in the AWS Glue Data Catalog. A table is created for one or more files found in your data store. If all the Amazon S3 files in a folder have the same schema, the crawler creates one table. Also, if the Amazon S3 object is partitioned, only one metadata table is created.","timestamp":"1633421460.0","upvote_count":"1","comment_id":"8933"},{"content":"explains here how. https://github.com/aws-samples/aws-glue-samples/tree/master/utilities/Hive_metastore_migration","comment_id":"8938","upvote_count":"1","timestamp":"1633724100.0","poster":"mattyb123"}],"poster":"jlpl","content":"Make sense to seleted \"D\"\n but again, I have not try 'handon' create a AWS Glue yet.","upvote_count":"1","timestamp":"1633313460.0"}]}],"comment_id":"8650","timestamp":"1632638940.0","content":"Only reason it could be B. Is due to glue needing to be setup as the metadata store before launching the EMR cluster. Anyone else have some thoughts on this?"}],"timestamp":"1632628200.0"}]}],"url":"https://www.examtopics.com/discussions/amazon/view/4074-exam-aws-certified-big-data-specialty-topic-2-question-5/","answers_community":[],"answer_images":[],"answer_ET":"B","choices":{"A":"Export Hive metastore information to Amazon DynamoDB hive-site classification to point to the Amazon DynamoDB table.","B":"Export Hive metastore information to a MySQL table on Amazon RDS and configure the Amazon EMR hive-site classification to point to the Amazon RDS database.","C":"Launch an Amazon EC2 instance, install and configure Apache Derby, and export the Hive metastore information to derby.","D":"Create and configure an AWS Glue Data Catalog as a Hive metastore for Amazon EMR."},"unix_timestamp":1566789060,"question_text":"An organization is currently using an Amazon EMR long-running cluster with the latest Amazon EMR release for analytic jobs and is storing data as external tables on Amazon S3.\nThe company needs to launch multiple transient EMR clusters to access the same tables concurrently, but the metadata about the Amazon S3 external tables are defined and stored on the long-running cluster.\nWhich solution will expose the Hive metastore with the LEAST operational effort?","answer":"B","timestamp":"2019-08-26 05:11:00","isMC":true,"topic":"2"},{"id":"YJllWtLHrsZMJEkPIYlh","answer":"AE","answers_community":[],"unix_timestamp":1567041600,"question_images":[],"exam_id":17,"timestamp":"2019-08-29 03:20:00","isMC":true,"answer_ET":"AE","discussion":[{"poster":"AdamSmith","timestamp":"1633352040.0","upvote_count":"6","comment_id":"47212","content":"There is no throttling on Lambda so it is due to the number of shards (throughput limit of 2MB/shard) = increasing number of shards E is correct.\n\nGiven that Lambda is only processing .5m records, the number of shards to be increased would be at least 20 folds. Which is likely to cause throttling on Lambda, thus increasing the batch size and RAM amount for Lambda to prepare for this would make sense. => A\n\nOther answers don't make sense anyway."},{"timestamp":"1635817320.0","comment_id":"369084","content":"Why not C, E?","poster":"ariane_tateishi","upvote_count":"1"},{"comment_id":"122301","poster":"alopazo","timestamp":"1634781540.0","upvote_count":"1","content":"A, E\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\n...To increase the speed at which your function processes records, add shards to your data stream...."},{"content":"A- Because Lambda’s capacity is underutilized so increasing the batch size will enable the function to process more records in a single invocation. However it will also need more memory to process the increased # of records.\nE- The reason why Lambda is processing only 450 records per day even though the Kinesis Streams is collecting 10 to 12 million records per day is due to lower Kinesis throughput. To increase the throughput ( Bytes/sec read from Kinesis stream) we would need to increase the # of shards.","comment_id":"75887","poster":"Bulti","timestamp":"1634337360.0","upvote_count":"3"},{"poster":"san2020","timestamp":"1633933620.0","content":"my selection AE","upvote_count":"3","comment_id":"52392"},{"poster":"mattyb123","comments":[{"timestamp":"1632403980.0","comment_id":"10524","poster":"alotofjeff","upvote_count":"1","comments":[{"comment_id":"29233","poster":"siva_aws","upvote_count":"2","content":"A - Lambda has no throttle - so increasing batch size will better utilize the memory allocation.\nE - Increase the shards,so more throughput for Lambda consumption - thus utilize Lambda effectively.","timestamp":"1633273380.0"}],"content":"don't get it, the post says increase batch size and lambda memory can let each invocation handle more data to reduce invocation rate, but in the question has specifically indicate there is no throttle."}],"content":"Correct. As mentioned previously via this link \nhttps://tech.trivago.com/2018/07/13/aws-kinesis-with-lambdas-lessons-learned/","comment_id":"8791","upvote_count":"2","timestamp":"1632158160.0"}],"answer_description":"","topic":"2","url":"https://www.examtopics.com/discussions/amazon/view/4281-exam-aws-certified-big-data-specialty-topic-2-question-6/","choices":{"C":"Create multiple Lambda functions that will consume the same Amazon Kinesis stream.","A":"Increase the BatchSize value on the EventSource, and increase the memory allocated to the Lambda function.","B":"Decrease the BatchSize value on the EventSource, and increase the memory allocated to the Lambda function.","D":"Increase the number of vCores allocated for the Lambda function.","E":"Increase the number of shards on the Amazon Kinesis stream."},"question_text":"An organization is using Amazon Kinesis Data Streams to collect data generated from thousands of temperature devices and is using AWS Lambda to process the data. Devices generate 10 to 12 million records every day, but Lambda is processing only around 450 thousand records. Amazon CloudWatch indicates that throttling on Lambda is not occurring.\nWhat should be done to ensure that all data is processed? (Choose two.)","answer_images":[],"question_id":82},{"id":"fi752XfBeHdLLOmf24D5","question_id":83,"discussion":[{"comments":[{"timestamp":"1635832260.0","upvote_count":"1","content":"Change my mind, I think B is correct, historical trend is the key of this answer","comment_id":"115624","poster":"matthew95"}],"comment_id":"110487","upvote_count":"1","content":"It should be C, \n\"With KPI charts, you can present a single aggregated value from a measure, and also comparisons against another measure or over a time period\"\nhttps://aws.amazon.com/blogs/big-data/amazon-quicksight-spring-announcement-kpi-charts-export-to-csv-ad-connector-and-more/","poster":"matthew95","timestamp":"1635673380.0"},{"poster":"axlrose","timestamp":"1635438900.0","comment_id":"66138","upvote_count":"3","content":"B is the right answer.\nC is just one number that changes every minute. You cannot see the historical trend."},{"upvote_count":"3","content":"my selection B","poster":"san2020","timestamp":"1634862180.0","comment_id":"52393"},{"content":"Isn't C just B with much much less information and no time sesries means no information regarding trend?","comment_id":"47220","poster":"AdamSmith","timestamp":"1634167260.0","upvote_count":"1"},{"poster":"mattyb123","timestamp":"1633149300.0","comment_id":"8287","upvote_count":"1","content":"Wouldn't this be C? Would make the dashboard display the MOST useful.","comments":[{"upvote_count":"3","timestamp":"1633736520.0","content":"Use a KPI to visualize a comparison between a key value and its target value. A KPI displays a value comparison, the two values being compared, and a progress bar. For example, the following KPI shows how closely revenue is meeting its forecast.\nhttps://docs.aws.amazon.com/quicksight/latest/user/kpi.html","comment_id":"8790","poster":"mattyb123","comments":[{"content":"will choose C, too. A and B have same problem that the beginning of each minute will show misleading info.","upvote_count":"1","timestamp":"1633783080.0","poster":"alotofjeff","comment_id":"10528"},{"timestamp":"1633869120.0","poster":"apertus","upvote_count":"2","comment_id":"11122","content":"Differnces between current number of visitors and historic number cannot indicate system problem, we also need the trends of the current number of vistors"}]}]}],"unix_timestamp":1566789180,"url":"https://www.examtopics.com/discussions/amazon/view/4075-exam-aws-certified-big-data-specialty-topic-2-question-7/","answers_community":[],"question_images":[],"question_text":"An Operations team continuously monitors the number of visitors to a website to identify any potential system problems. The number of website visitors varies throughout the day. The site is more popular in the middle of the day and less popular at night.\nWhich type of dashboard display would be the MOST useful to allow staff to quickly and correctly identify system problems?","isMC":true,"timestamp":"2019-08-26 05:13:00","choices":{"D":"A scatter plot showing today's website visitors on the X-axis and the historical average number of website visitors on the Y-axis.","B":"An overlay line chart showing today's website visitors at one-minute intervals and also the historical average number of website visitors.","A":"A vertical stacked bar chart showing today's website visitors and the historical average number of website visitors.","C":"A single KPI metric showing the statistical variance between the current number of website visitors and the historical number of website visitors for the current time of day."},"answer_ET":"B","answer":"B","exam_id":17,"answer_description":"","topic":"2","answer_images":[]},{"id":"BeWaFLcdROf1c8LyyK7N","discussion":[{"poster":"VB","upvote_count":"13","timestamp":"1632473100.0","comment_id":"12078","content":"C is correct ... \"Q: Does Athena support other BI Tools and SQL Clients?\nYes. Amazon Athena comes with an ODBC and JDBC driver that you can use with other business intelligence tools and SQL clients. Learn more about using an ODBC or JDBC driver with Athena.\". This satisfies the requirement."},{"content":"C is LEAST operational overhead. Others may require existing BI tool to change","comment_id":"74093","timestamp":"1636143480.0","upvote_count":"4","poster":"susan8840"},{"content":"my selection C","upvote_count":"3","timestamp":"1636087080.0","poster":"san2020","comment_id":"52394"},{"content":"C is LEAST operational overhead","comment_id":"20290","poster":"harry_123","upvote_count":"4","timestamp":"1634784180.0"},{"comments":[{"upvote_count":"1","poster":"shwang","content":"Vote D as well","comment_id":"31088","timestamp":"1635148020.0"},{"upvote_count":"3","poster":"Corram","comment_id":"99514","content":"Cause imagine you were a manager, and your employee would create a transient EMR cluster + a Amazon Redshift database just for some log file viewing in a BI tool, you'd be quite pissed with him too ;) admittedly, technically it would work tho","timestamp":"1636258740.0"}],"upvote_count":"1","comment_id":"19428","poster":"cybe001","content":"Why not D, you can use JDBC to connect to Redshift","timestamp":"1632881400.0"}],"question_images":[],"answer_description":"","topic":"2","answers_community":[],"question_id":84,"isMC":true,"answer":"C","unix_timestamp":1569121320,"timestamp":"2019-09-22 05:02:00","exam_id":17,"answer_images":[],"question_text":"An organization would like to run analytics on their Elastic Load Balancing logs stored in Amazon S3 and join this data with other tables in Amazon S3. The users are currently using a BI tool connecting with JDBC and would like to keep using this BI tool.\nWhich solution would result in the LEAST operational overhead?","choices":{"B":"Launch a long-running Amazon EMR cluster that continuously downloads and transforms new files from Amazon S3 into its HDFS storage. Use Presto to expose the data through JDBC.","D":"Launch a transient Amazon EMR cluster every night that transforms new log files and loads them into Amazon Redshift.","C":"Trigger a Lambda function when a new log file is added to the bucket to transform and move it to another bucket with an optimized data structure. Use Amazon Athena to query the optimized bucket.","A":"Trigger a Lambda function when a new log file is added to the bucket to transform and load it into Amazon Redshift. Run the VACUUM command on the Amazon Redshift cluster every night."},"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/5556-exam-aws-certified-big-data-specialty-topic-2-question-8/"},{"id":"NSeSvTbQJkoAYhAO0KNs","choices":{"D":"Attach more consumers to the Kinesis stream to process records in parallel, improving the performance on the stream. B","B":"Increase the number of shards on the Kinesis stream to allow for more throughput to meet the peak spikes in traffic.","A":"Create multiple Amazon Kinesis streams for page requests to increase the concurrency of the clickstream.","C":"Modify the application to use on the Kinesis Producer Library to aggregate requests before sending them to the Kinesis stream."},"answer_description":"Reference:\nhttps://aws.amazon.com/kinesis/data-streams/faqs/","discussion":[{"comment_id":"21545","poster":"yuriy_ber","timestamp":"1632837420.0","content":"I'm also stumbled upon this questions - it looks very obvious that they are not aggregating because they use PutRecord API. Furthermore they have only peak spikes, so if they increase number of shards they would have constantly higher costs for only occasional spikes. It's definitely C, additionally it would also be possible to implement compression using KPL.","upvote_count":"8"},{"content":"But is B a most cost-effective way?.. the price we pay depends on the shards .. can it be C?","upvote_count":"6","timestamp":"1632216420.0","poster":"VB","comment_id":"13175"},{"upvote_count":"1","comment_id":"338042","content":"C - this is how you avoid ProvisionedThroughputExcededException","timestamp":"1636227960.0","poster":"DerekKey"},{"comment_id":"110498","content":"It should be C, because batching increase throughput and decrease cost","timestamp":"1636165320.0","upvote_count":"2","poster":"matthew95"},{"comment_id":"103801","poster":"k115","upvote_count":"2","timestamp":"1636159500.0","content":"C is the right answer"},{"timestamp":"1635747600.0","comment_id":"94592","upvote_count":"1","content":"B or C?","poster":"winset"},{"poster":"emailtorajivk","content":"The request rate for the stream is too high, or the requested data is too large for the available throughput. Reduce the frequency or size of your requests. For more information,\nhttps://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html\nSo using the producer library will decrease the frequency","timestamp":"1635326580.0","upvote_count":"1","comment_id":"90643"},{"comment_id":"75896","poster":"Bulti","content":"Answer is C: \nBatching (both turned on by default) increase throughput, decrease\ncost:\n•Collect Records and Write to multiple shards in the same PutRecords API call\n•Aggregate increased latency\n•Capability to store multiple records in one record (go over 1000 records per second limit)\n•Increase payload size and improve throughput (maximize 1MB/s limit)","timestamp":"1634886240.0","upvote_count":"4"},{"content":"agreed B. input data increased so to increase shards.\nYour data blob, partition key, and data stream name are required parameters of a PutRecord or PutRecords call. The size of your data blob (before Base64 encoding) and partition key will be counted against the data throughput of your Amazon Kinesis data stream, which is determined by the number of shards within the data stream.","upvote_count":"1","timestamp":"1634496960.0","poster":"susan8840","comment_id":"74095"},{"poster":"san2020","upvote_count":"6","content":"my selection C","timestamp":"1634460600.0","comment_id":"52395"},{"content":"C ! https://docs.aws.amazon.com/streams/latest/dev/kinesis-kpl-concepts.html","timestamp":"1634373540.0","comment_id":"51812","poster":"aewis","upvote_count":"1"},{"timestamp":"1634197440.0","upvote_count":"3","content":"C is correct; KPL supports Aggregation, storing multiple records within a single Kinesis Data Streams record and with this feature, we can go beyond 1000 records per second per shard.","comment_id":"49206","poster":"richardxyz"},{"content":"I will go with B\nhttps://aws.amazon.com/kinesis/data-streams/faqs/","comment_id":"43456","timestamp":"1633884000.0","upvote_count":"1","poster":"kalpanareddy","comments":[{"timestamp":"1634061240.0","poster":"RamNelluru","comment_id":"44330","content":"B may not work because partition key is page name. Even if you increase the number of shards still there may be a hot partition because of single page sending more puts. Since this information is not provided, C may be right answer.","upvote_count":"3"}]},{"timestamp":"1633472340.0","upvote_count":"1","content":"Due to frequent checkpointing its giving the errors. When the data will be aggregated the checkpointing will get reduced and in turn solve the problem in the most effective way.","poster":"am7","comment_id":"33903"},{"content":"C I think. https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html\n\"The KPL is an easy-to-use, highly configurable library that helps you write to a Kinesis data stream. It acts as an intermediary between your producer application code and the Kinesis Data Streams API actions. The KPL performs the following primary tasks:\n\nWrites to one or more Kinesis data streams with an automatic and configurable retry mechanism\n\nCollects records and uses PutRecords to write multiple records to multiple shards per request\n\nAggregates user records to increase payload size and improve throughput\".\n\nPlease, anyone on this thread who passed this exam already? That way we can all be on same page with such, to know which answers are correct","comment_id":"24267","upvote_count":"2","timestamp":"1632963240.0","poster":"s3an"},{"upvote_count":"3","content":"My choice is C. \nWhile B is a way to resolve the throughput issue, aggregation proposed by C would be more cost-effective.","poster":"Zire","comment_id":"14318","timestamp":"1632486720.0","comments":[{"comment_id":"19430","poster":"cybe001","content":"You also get provisionedthroughputexceeded for volume of data also. So aggregation won't solve the issue. Answer is B","timestamp":"1632787560.0","comments":[{"poster":"d00ku","comment_id":"20488","timestamp":"1632836580.0","content":"Aggregation solves volume issues, Batching solves throughput issues.. seems C.","upvote_count":"3"}],"upvote_count":"4"}]},{"timestamp":"1632116880.0","content":"B ? Thoughts","comment_id":"8870","poster":"jlpl","comments":[{"poster":"mattyb123","timestamp":"1632171300.0","content":"Correct. \nhttps://aws.amazon.com/kinesis/data-streams/faqs/\nQ: What happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream?\nThe capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, retry by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the data stream’s input data rate and the occurrence of ProvisionedThroughputExceeded exceptions.","comment_id":"8910","comments":[{"comment_id":"38407","poster":"ME2000","upvote_count":"1","comments":[{"content":"C is correct.\nProvisionedThroughputExceededException can be caused either by too large data volume or too many requests. Since PutRecord API is used, each record gets sent on its own, making too many requests highly probable. Thus, C should help and it is obviously more cost effective than B.","timestamp":"1636039620.0","poster":"Corram","upvote_count":"5","comment_id":"101745"}],"content":"B is the answer\nMore on \"ProvisionedThroughputExceededException\"...\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html\nhttps://any-api.com/amazonaws_com/kinesis/docs/Definitions/ProvisionedThroughputExceededException","timestamp":"1633570320.0"}],"upvote_count":"5"}],"upvote_count":"2"}],"question_text":"An organization has added a clickstream to their website to analyze traffic. The website is sending each page request with the PutRecord API call to an Amazon\nKinesis stream by using the page name as the partition key. During peak spikes in website traffic, a support engineer notices many events in the application logs.\nProvisionedThroughputExcededException\nWhat should be done to resolve the issue in the MOST cost-effective way?","answer_ET":"Explanation","question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/4320-exam-aws-certified-big-data-specialty-topic-2-question-9/","answer_images":[],"answers_community":[],"answer":"Explanation","topic":"2","timestamp":"2019-08-29 16:42:00","unix_timestamp":1567089720,"question_id":85,"exam_id":17}],"exam":{"provider":"Amazon","id":17,"isMCOnly":true,"isImplemented":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":85,"isBeta":false,"name":"AWS Certified Big Data - Specialty"},"currentPage":17},"__N_SSP":true}