{"pageProps":{"questions":[{"id":"6IAe8TbS7zKg2x7nNzht","choices":{"C":"Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create a transit gateway and a DX gateway in a central network account. Create a transit virtual interface for each DX interface and associate them with the DX gateway. Create a gateway association between the DX gateway and the transit gateway.","D":"Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a transit gateway in a central network account and associate it with the virtual private gateways. Create a transit virtual interface on each DX connection and attach the interface to the transit gateway.","B":"Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a DX gateway in a central network account and associate it with the virtual private gateways. Create a public virtual interface on each DX connection and associate the interface with the DX gateway.","A":"Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on-premises location. Create private virtual interfaces on each connection for each AWS account VPC. Associate the private virtual interface with a virtual private gateway attached to each VPC."},"discussion":[{"content":"A - no, there is no connection between VPCs.\nB - no, bcz DX gateway doesn't support routing from one VPN to another ( https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html )\nC - right answer. https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html\nD - no, you can not connect Direct Connect to the Transit gateway without Direct Connect gateway in the middle.","poster":"aws_master","comment_id":"317500","comments":[{"upvote_count":"8","comment_id":"409908","poster":"Tony_W","content":"One more thing I think I see wrong with B is at the end of the answer. It says to create a PUBLIC Virtual interface on each DX connection. Wouldnt that be a private virtual interface making this answer wrong from the start? Plus transit gateways can do peering. So C would work just fine.","timestamp":"1635786600.0"},{"timestamp":"1686491220.0","upvote_count":"2","content":"Your explication for B is wrong. I don't see VPN in the question. B is wrong because you need private VIFs over DX connection to connect to VPC. Public VIFs can only connect to public services like S3 etc.","comment_id":"920729","poster":"Jesuisleon"}],"upvote_count":"29","timestamp":"1634057700.0"},{"comment_id":"625557","timestamp":"1656646320.0","poster":"aandc","content":"C: All AWS application workloads must be connected to one another -> transit-gateway","upvote_count":"6"},{"timestamp":"1688163660.0","upvote_count":"1","comment_id":"939437","poster":"SkyZeroZx","content":"Selected Answer: C\nC\nRef : https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html"},{"upvote_count":"3","poster":"dev112233xx","content":"Selected Answer: C\nC not doubts:\nhttps://www.edge-cloud.net/content/uploads/2019/12/AWS-Interconnect.png","comment_id":"873517","timestamp":"1681815420.0"},{"timestamp":"1665598140.0","comment_id":"693307","content":"Selected Answer: C\nc is the right one","upvote_count":"1","poster":"davideccc"},{"content":"Selected Answer: C\nill go for C","upvote_count":"1","comment_id":"685708","poster":"Ell89","timestamp":"1664818140.0"},{"comment_id":"608139","content":"C. \nThe solution involves the following components:\n\nA transit gateway that has VPC attachments.\n\nA Direct Connect gateway.\n\nAn association between the Direct Connect gateway and the transit gateway.\n\nA transit virtual interface that is attached to the Direct Connect gateway.\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html","poster":"user89","timestamp":"1653666840.0","upvote_count":"2"},{"content":"Selected Answer: B\nThe exact scenario is detailed here: https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html","timestamp":"1652188560.0","comments":[{"poster":"Jonfernz","content":"That link actually suggests C as the answer, bruv.","timestamp":"1654732500.0","comment_id":"613542","upvote_count":"1"},{"upvote_count":"1","timestamp":"1652868900.0","poster":"LiamNg","content":"Agree, the answer should be B. The workload are in separate accounts.\nRef to\"Virtual private gateway associations across accounts\" in MuskyWilkins link","comments":[{"comment_id":"628295","poster":"Enigmaaaaaa","content":"Where do you see in the link \"ublic virtual interface \"?","upvote_count":"2","timestamp":"1657186200.0"}],"comment_id":"603222"},{"poster":"dev112233xx","comment_id":"873516","upvote_count":"1","content":"Why public VIF is needed? did you think about it? üòÖ","timestamp":"1681815300.0"}],"upvote_count":"3","comment_id":"599602","poster":"MuskyWilkins"},{"upvote_count":"1","comment_id":"580138","comments":[{"content":"DX Gateway is required when you want to use a TGW (with Transit VIF per DX connection), even if only for a single Region.","upvote_count":"1","comment_id":"691597","timestamp":"1665446340.0","poster":"sb333"}],"poster":"jyrajan69","timestamp":"1648969140.0","content":"There must be a reason for saying its confined to one region, so DX Gateway is not required. So left with A or D, and D only option that allows for comm between VPC, so the answer is D"},{"upvote_count":"2","content":"Selected Answer: C\nC. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create a transit gateway and a DX gateway in a central network account. Create a transit virtual interface for each DX interface and associate them with the DX gateway. Create a gateway association between the DX gateway and the transit gateway.","comment_id":"537559","poster":"jj22222","timestamp":"1643676060.0"},{"poster":"jj22222","comment_id":"534201","content":"Selected Answer: C\nC looks right","timestamp":"1643328060.0","upvote_count":"2"},{"timestamp":"1641005820.0","poster":"Derrick888","upvote_count":"2","comment_id":"514331","content":"Selected Answer: C\nC. for sure"},{"content":"C. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create a transit gateway and a DX gateway in a central network account. Create a transit virtual interface for each DX interface and associate them with the DX gateway. Create a gateway association between the DX gateway and the transit gateway.","poster":"cldy","upvote_count":"1","comment_id":"499204","timestamp":"1639211400.0"},{"poster":"AzureDP900","content":"It is C","upvote_count":"1","comment_id":"497058","timestamp":"1638986940.0"},{"poster":"andylogan","comment_id":"447483","content":"It's C","timestamp":"1636223580.0","upvote_count":"1"},{"timestamp":"1636059480.0","comment_id":"438491","poster":"Kopa","upvote_count":"1","content":"Im for C. TransitGateway makes sense."},{"poster":"tgv","upvote_count":"1","content":"CCC\n---","timestamp":"1636035240.0","comment_id":"435340"},{"timestamp":"1635920400.0","poster":"blackgamer","content":"C for me","comment_id":"434281","upvote_count":"1"},{"content":"D for me. C is incorrect. A transit virtual interface should be used to access one or more Amazon VPC Transit Gateways associated with Direct Connect gateways and not DX","comment_id":"417755","timestamp":"1635870120.0","poster":"ericmartinez22","upvote_count":"1"},{"content":"I'll go with C","timestamp":"1635854880.0","comment_id":"414069","upvote_count":"3","poster":"WhyIronMan"},{"comment_id":"396239","upvote_count":"2","content":"Q. Can I send traffic from one VPC associated with a Direct Connect gateway to another VPC associated to the same Direct Connect gateway?\n\nNo, Direct Connect gateway only supports routing traffic from Direct Connect VIFs to VGW (associated with VPC). In order to send traffic between 2 VPCs, you would configure a VPC peering connection, the same as you do today.\n\nAnswer is C","poster":"MrCarter","timestamp":"1635590100.0"},{"content":"C is incomplete as the question clearly ask for connectivity between AWS accounts and the answer gives only DX with Transit Gateway. Between B and D I will choose B as not the best(transit gateway preferred) but works https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html","poster":"Gladabhi","timestamp":"1635570900.0","comments":[{"comment_id":"400780","timestamp":"1635690000.0","content":"Transit Gateway is what makes VPC to VPC connectivity. Its the core purpose of TX GW.","poster":"vimgoru24","upvote_count":"1"}],"upvote_count":"1","comment_id":"392163"},{"poster":"vkbajoria","timestamp":"1635182700.0","comments":[{"comments":[{"poster":"victordun","comments":[{"timestamp":"1635496200.0","comment_id":"369061","poster":"vkbajoria","upvote_count":"3","content":"Absolutely I take that back. Must be Highly available. Answer is C"}],"timestamp":"1635432480.0","content":"Requirement --> It is critical for the AWS workloads to maintain connectivity to the legacy system, D only has a single DX partner VS two DX partners","upvote_count":"4","comment_id":"365819"}],"upvote_count":"1","content":"The key is \"a single geographic region\". C is right answer if multiple geographic region","poster":"vkbajoria","comment_id":"363923","timestamp":"1635273720.0"}],"upvote_count":"1","comment_id":"363922","content":"It's D for me\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway-vpn.html"},{"upvote_count":"2","comment_id":"358650","poster":"Waiweng","content":"it's C","timestamp":"1635005460.0"},{"upvote_count":"6","content":"I go with C https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html","timestamp":"1634948340.0","poster":"tvs","comment_id":"355466","comments":[{"content":"awesome link","comment_id":"396240","timestamp":"1635677520.0","poster":"MrCarter","upvote_count":"1"}]},{"content":"C\nTransit virtual interface: A transit virtual interface should be used to access one or more Amazon VPC Transit Gateways associated with Direct Connect gateways. You can use transit virtual interfaces with 1/2/5/10 Gbps AWS Direct Connect connections. For information about Direct Connect gateway configurations, see Direct Connect gateways.","upvote_count":"2","comment_id":"330090","poster":"AJBA","timestamp":"1634568840.0"},{"content":"Use DX gateway only when you need cross-regional sharing of DX. In this case, there isn‚Äôt a need for DX gateway to be in the picture.\nhttps://www.megaport.com/blog/aws-vgw-vs-dgw-vs-tgw/\n\nAnswer is D (transit gateway is required for VPC - VPC communication (multi accounts/single region)","timestamp":"1634464980.0","upvote_count":"2","poster":"Pupu86","comment_id":"329938"},{"upvote_count":"1","comment_id":"319665","timestamp":"1634090520.0","comments":[{"timestamp":"1634176260.0","upvote_count":"1","comments":[{"comment_id":"337664","timestamp":"1634915520.0","upvote_count":"1","content":"typoed. C is correct. \nC. two DX partners for each on-premises location \nD. a DX partner for each on-premises location","poster":"certainly"}],"comment_id":"319670","content":"Also since this is mission critical application it is best practice to have dual DX connection via diff partner/ISP","poster":"certainly"}],"poster":"certainly","content":"D. only Transit Gateway can support transitive routing between VPCs"},{"poster":"SD13","timestamp":"1633258860.0","upvote_count":"2","content":"C for me, since we need to associate DX gateway to transit gateway for full connectivity between 2 on-prem locations and AWS VPCs.","comment_id":"316324","comments":[{"content":"You don't need to DXG as the question says with in region so just TGW should be good enough.","poster":"kalyan_krishna742020","comment_id":"316704","timestamp":"1633624260.0","upvote_count":"1"}]},{"comment_id":"315559","comments":[{"content":"I choose C. \nDX Gateway is missing from D\n\nCheck figure 9 and read about the best practise\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html","poster":"trap","comment_id":"315850","upvote_count":"3","comments":[{"content":"As a best practice, you should have at least two connections at two different Direct Connect locations for maximum redundancy, a total of four connections. You create one VIF per connection for a total of four private VIF‚Äôs and four transit VIFs.","poster":"trap","upvote_count":"2","comment_id":"315860","timestamp":"1633071240.0"}],"timestamp":"1632436920.0"}],"poster":"awsnoob","content":"Should be D\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html","timestamp":"1632321060.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"313835","poster":"nitinz","timestamp":"1632229260.0","content":"me too D"},{"comments":[{"timestamp":"1634259060.0","poster":"M_Asep","comments":[{"timestamp":"1635888660.0","upvote_count":"1","content":"D is the right answer. C is incorrect. A transit virtual interface is used to access one or more Amazon VPC Transit Gateways associated with Direct Connect gateway and not Direct Connect.","comment_id":"417789","poster":"ericmartinez22"}],"comment_id":"325958","content":"D seems incorrect because its used only one DX provider","upvote_count":"2"}],"timestamp":"1632163140.0","comment_id":"312550","content":"I think D","upvote_count":"1","poster":"wasabidev"}],"timestamp":"2021-03-16 18:12:00","answer_images":[],"topic":"1","answers_community":["C (80%)","B (20%)"],"unix_timestamp":1615914720,"answer":"C","answer_ET":"C","question_id":696,"answer_description":"","exam_id":32,"url":"https://www.examtopics.com/discussions/amazon/view/47371-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A company is configuring connectivity to a multi-account AWS environment to support application workloads that serve users in a single geographic region. The workloads depend on a highly available, on-premises legacy system deployed across two locations. It is critical for the AWS workloads to maintain connectivity to the legacy system, and a minimum of 5 Gbps of bandwidth is required. All application workloads within AWS must have connectivity with one another.\nWhich solution will meet these requirements?","question_images":[],"isMC":true},{"id":"gM5RWmVwNNdehcXV9uxN","question_text":"A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts.\nA solutions architect uses the IAM user Support1 from the master account to create a new member account with finance1@example.com as the email address.\nWhat should the solutions architect do to create IAM users in the new member account?","topic":"1","discussion":[{"timestamp":"1632454140.0","comment_id":"312543","content":"B.\nA wrong because \"When you create an account, AWS Organizations initially assigns a long (64 characters), complex, randomly generated password to the root user. You can't retrieve this initial password. To access the account as the root user for the first time, you must go through the process for password recovery. \"","poster":"wasabidev","comments":[{"poster":"RVivek","upvote_count":"2","timestamp":"1644656520.0","comment_id":"545723","content":"Good explanation. Yes. B is the answer. Aditional inforatonhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-member-account-access/"}],"upvote_count":"16"},{"poster":"vkbajoria","comment_id":"364419","timestamp":"1634639940.0","content":"it is B for me. It makes more sense","upvote_count":"6"},{"poster":"dev112233xx","upvote_count":"1","timestamp":"1681815600.0","content":"B\nwhen the master creates a new member account then can switch to it without cross-account trust role (because it will be created automatically)","comment_id":"873518"},{"timestamp":"1668058800.0","upvote_count":"1","comment_id":"714944","content":"Selected Answer: B\nB for sure - Have had to use this option regularly at work.\nWould still recommend resetting the password of the root account however.","poster":"janvandermerwer"},{"poster":"dcdcdc3","comment_id":"680087","content":"Not A because an email with password is never sent when creating the account form Orgs","upvote_count":"1","timestamp":"1664219580.0"},{"poster":"dcdcdc3","upvote_count":"2","content":"Selected Answer: B\nB.\nAs added value I would always want to reset the root password once, set mfa and vault both. If not, email admins can get themselves access in the future.","comment_id":"680086","timestamp":"1664219520.0"},{"poster":"Student1950","comment_id":"526566","timestamp":"1642507140.0","content":"A is the correct answer.\nReason: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html\nWhen an invited account joins your organization, you do not automatically have full administrator control over the account, unlike created accounts. If you want the management account to have full administrative control over an invited member account, you must create the OrganizationAccountAccessRole IAM role in the member account and grant permission to the management account to assume the role.\nWhen you create an account in your organization instead of inviting an existing account to join, AWS Organizations automatically creates an IAM role (named OrganizationAccountAccessRole by default) that you can use to grant users in the management account administrator access to the created account.","comments":[{"comment_id":"713466","upvote_count":"1","poster":"Byrney","timestamp":"1667880300.0","content":"This part is exactly what option B is: \"When you create an account in your organization instead of inviting an existing account to join, AWS Organizations automatically creates an IAM role (named OrganizationAccountAccessRole by default) that you can use to grant users in the management account administrator access to the created account.\""}],"upvote_count":"1"},{"upvote_count":"2","comment_id":"497062","poster":"AzureDP900","timestamp":"1638987180.0","content":"OrganizationAccountAccessRole is keyword here. B is right"},{"upvote_count":"2","comment_id":"497061","poster":"AzureDP900","timestamp":"1638987120.0","content":"B is right answer"},{"upvote_count":"1","poster":"cldy","comment_id":"496008","content":"B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.","timestamp":"1638881640.0"},{"poster":"acloudguru","timestamp":"1637483760.0","upvote_count":"2","content":"Selected Answer: B\nWhen you create a new member account, Organizations sets an initial password for that account that can't be retrieved. To access the account as the root user for the first time, follow these instructions to reset the initial password.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-member-account-access/","comment_id":"483117"},{"comment_id":"480866","upvote_count":"3","content":"The correct option is A. Please see read the below link for the reference. \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html","poster":"Smartphone","timestamp":"1637253900.0"},{"timestamp":"1636237620.0","comment_id":"447486","content":"It's B","poster":"andylogan","upvote_count":"1"},{"poster":"Kopa","content":"Im for B, in Neal Davis we practice this often on labs","comment_id":"438492","upvote_count":"3","timestamp":"1636001640.0"},{"content":"BBB\n---","upvote_count":"1","comment_id":"436605","timestamp":"1635945120.0","poster":"tgv"},{"upvote_count":"1","comment_id":"434283","content":"Answer is B","timestamp":"1635869520.0","poster":"blackgamer"},{"timestamp":"1635288720.0","upvote_count":"2","content":"I'll go with B\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-member-account-access/","poster":"WhyIronMan","comment_id":"414074"},{"timestamp":"1634031540.0","poster":"apmpm","content":"It has to be A","comments":[{"content":"When you create a new member account, Organizations sets an initial password for that account that can't be retrieved. To access the account as the root user for the first time, follow these instructions to reset the initial password.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-member-account-access/","timestamp":"1635432720.0","poster":"WhyIronMan","comment_id":"414075","upvote_count":"1"}],"comment_id":"363598","upvote_count":"3"},{"content":"it's B","upvote_count":"5","poster":"Waiweng","timestamp":"1633782420.0","comment_id":"358658"},{"comment_id":"357763","content":"B is wrong\nYou cannot switch roles if you sign in as the AWS account root user. You can switch roles when you sign in as an IAM user.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-console.html","poster":"viet1991","comments":[{"upvote_count":"7","content":"B doesnt mention root sign-in, its proposing to login to master account.","timestamp":"1634699280.0","poster":"victordun","comment_id":"365829"},{"poster":"WhyIronMan","content":"In the AWS Organizations console, member accounts appear under the Accounts tab. Note the account number, email address, and IAM role name of the member account that you want to access. You can access the member account using either the IAM role or the root user credentials.","timestamp":"1635665580.0","upvote_count":"1","comment_id":"414076"}],"timestamp":"1633696260.0","upvote_count":"1"},{"poster":"Jaypdv","comment_id":"338009","timestamp":"1633154100.0","upvote_count":"4","content":"B. \nIt would not work if the Solutions Architect was connected as the root account (cannot switch role), but the text of the question makes it relatively clear that he's not. Besides, none of the other answers make sense."},{"comment_id":"330564","poster":"ExtHo","timestamp":"1632987780.0","upvote_count":"3","content":"A. It's a best practice to use the root user only to create IAM users, groups, and roles but missing When you create a new member account, Organizations sets an initial password for that account that can't be retrieved. To access the account as the root user for the first time you have to reset password.\n\nB. If you are signed in with root user credentials, you can't switch roles. You must be signed in as an IAM user or role here solutions architect uses the IAM user Support1 from the master account to create a new member account. Seems B is closer to correct.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-member-account-access/"},{"content":"B:\nEither A and B could work.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_access-cross-account-role\nHowever we don't know if finance1@example.com has a policy/role that grants it the required permissions to make A work.","timestamp":"1632857460.0","upvote_count":"2","poster":"kejam","comment_id":"322958"},{"upvote_count":"3","content":"I think its B","timestamp":"1632756780.0","poster":"eji","comment_id":"318583"},{"poster":"Nguyenhau","timestamp":"1632131040.0","upvote_count":"3","content":"i go with B","comment_id":"310586"}],"url":"https://www.examtopics.com/discussions/amazon/view/47059-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"timestamp":"2021-03-14 15:18:00","answer_description":"","question_images":[],"answer_images":[],"unix_timestamp":1615731480,"choices":{"B":"From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.","D":"Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.","C":"Go to the AWS Management Console sign-in page. Choose ◊í‚Ç¨Sign in using root account credentials.◊í‚Ç¨ Sign in by using the email address finance1@example.com and the master account's root password. Set up the IAM users as required.","A":"Sign in to the AWS Management Console with AWS account root user credentials by using the 64-character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required."},"answers_community":["B (100%)"],"answer":"B","answer_ET":"B","isMC":true,"question_id":697},{"id":"SLLhrN67Wt8UdanwQe9Y","exam_id":32,"answer_images":[],"question_id":698,"url":"https://www.examtopics.com/discussions/amazon/view/46616-exam-aws-certified-solutions-architect-professional-topic-1/","choices":{"D":"Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to execute an AWS Lambda function to process each file and store the results in a DynamoDB global table in multiple Regions. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.","C":"Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to push S3 file paths into Amazon EventBridge (Amazon CloudWatch Events). Use an AWS Lambda function to poll EventBridge (CloudWatch Events) to process each file and store the results in a DynamoDB table in each Region. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.","B":"Create an S3 CreateObject event notification to copy the file to Amazon Elastic File System (Amazon EFS). Use AWS DataSync to sync the files between EFS volumes in multiple Regions. Use an AWS Lambda function to process the EFS files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 buckets with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.","A":"Create an S3 CreateObject event notification to copy the file to Amazon Elastic Block Store (Amazon EBS). Use AWS DataSync to sync the files between EBS volumes in multiple Regions. Use an Amazon EC2 Auto Scaling group in multiple Regions to attach the EBS volumes. Process the files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 bucket with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year."},"answers_community":["D (100%)"],"answer":"D","timestamp":"2021-03-12 03:47:00","discussion":[{"content":"D is my choice. S3 CRR, C3 create triggers lambda, stores results in Dynamo DB Global, S3IA and lifecycle to delete after 1 year","timestamp":"1632228420.0","comment_id":"311177","upvote_count":"27","poster":"lostre"},{"timestamp":"1633298280.0","upvote_count":"8","poster":"Waiweng","comment_id":"358663","content":"it's D"},{"content":"I think it's D, and If it's not D then I'll get it wrong in the exam.","upvote_count":"1","comment_id":"615484","poster":"CloudHell","timestamp":"1655064900.0"},{"content":"D. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to execute an AWS Lambda function to process each file and store the results in a DynamoDB global table in multiple Regions. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.","upvote_count":"2","poster":"cldy","comment_id":"497330","timestamp":"1639025280.0"},{"poster":"AzureDP900","comment_id":"497066","timestamp":"1638987480.0","upvote_count":"1","content":"D is right answer. We don't need files after 1 year."},{"timestamp":"1637579280.0","upvote_count":"1","content":"Selected Answer: D\nD is my choice. S3 CRR, C3 create triggers lambda, stores results in Dynamo DB Global, S3IA and lifecycle to delete after 1 year","poster":"acloudguru","comment_id":"484110"},{"comment_id":"447489","content":"It's B","poster":"andylogan","timestamp":"1635753000.0","upvote_count":"1","comments":[{"poster":"andylogan","comment_id":"447490","upvote_count":"1","timestamp":"1635776280.0","content":"typo It's D"}]},{"comment_id":"439781","poster":"DerekKey","timestamp":"1635509400.0","comments":[{"comment_id":"450442","upvote_count":"2","poster":"Viper57","comments":[{"poster":"Viper57","comment_id":"459589","content":"Cross region replication also supports replicating existing objects.\n\nhttps://aws.amazon.com/blogs/storage/replicating-existing-objects-between-s3-buckets/","timestamp":"1636234320.0","upvote_count":"1"}],"timestamp":"1636002960.0","content":"By your own logic question B is also wrong because it relies on a 'CreateObject' event. This assumes new objects will be created in the S3 bucket which also means D can be correct."}],"upvote_count":"1","content":"Have you noticed the requirement:\n\"original files ..... available in multiple AWS Regions\"\nHow will you make it working with C & D?\n\"Copy the files to an S3 bucket in another Region by using cross-Region replication\"\n\"Configure both S3 buckets\"\n--> Therefore C&D should be WRONG\nThe only solution that will work is B."},{"comment_id":"438496","poster":"Kopa","content":"Im going for D.","timestamp":"1635145680.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"436606","content":"DDD\n---","poster":"tgv","timestamp":"1635058980.0"},{"comment_id":"434285","content":"It is D","upvote_count":"1","timestamp":"1634397420.0","poster":"blackgamer"},{"timestamp":"1634094300.0","comment_id":"414079","content":"I'll go with D","poster":"WhyIronMan","upvote_count":"1"},{"content":"A is nonsense, D is the way to go","timestamp":"1633792080.0","upvote_count":"1","poster":"vimgoru24","comment_id":"400786"},{"poster":"KnightVictor","content":"D is correct. \nDefault answer A is incorrect. Glacier cannot give data back immediately","comment_id":"354223","upvote_count":"3","timestamp":"1633041660.0"},{"timestamp":"1632892080.0","poster":"champcloud","content":"Going with D","comment_id":"321518","upvote_count":"2"},{"poster":"nitinz","upvote_count":"5","comment_id":"313839","content":"D is correct","timestamp":"1632643380.0"},{"comment_id":"312519","content":"I think D is better than A","timestamp":"1632572760.0","upvote_count":"6","poster":"wasabidev"},{"timestamp":"1632188040.0","upvote_count":"1","comment_id":"308519","content":"DataSync can not copy to ebs, efs is correct\nhttps://aws.amazon.com/datasync/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc","poster":"doris0306"}],"topic":"1","isMC":true,"unix_timestamp":1615517220,"question_text":"A company is designing a data processing platform to process a large number of files in an Amazon S3 bucket and store the results in Amazon DynamoDB.\nThese files will be processed once and must be retained for 1 year. The company wants to ensure that the original files and resulting data are highly available in multiple AWS Regions.\nWhich solution will meet these requirements?","question_images":[],"answer_ET":"A","answer_description":""},{"id":"bLleLq0ASdqDSqgXlG1G","answers_community":["CE (67%)","DE (33%)"],"url":"https://www.examtopics.com/discussions/amazon/view/52807-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"answer_description":"","answer":"CE","topic":"1","question_images":[],"exam_id":32,"choices":{"E":"AWS reserves the first four and the last IP address in each subnet's CIDR block so you do not have enough addresses left to launch all of the new EC2 instances","C":"The ELB has scaled-up, adding more instances to handle the traffic spike, reducing the number of available private IP addresses for new instance launches","B":"The Internet Gateway (IGW) of your VPC has scaled-up, adding more instances to handle the traffic spike, reducing the number of available private IP addresses for new instance launches","D":"AWS reserves one IP address in each subnet's CIDR block for Route53 so you do not have enough addresses left to launch all of the new EC2 instances","A":"AWS reserves the first and the last private IP address in each subnet's CIDR block so you do not have enough addresses left to launch all of the new EC2 instances"},"question_text":"You have deployed a three-tier web application in a VPC with a CIDR block of 10.0.0.0/28. You initially deploy two web servers, two application servers, two database servers and one NAT instance tor a total of seven EC2 instances. The web, application and database servers are deployed across two availability zones\n(AZs). You also deploy an ELB in front of the two web servers, and use Route53 for DNS Web (raffle gradually increases in the first few days following the deployment, so you attempt to double the number of instances in each tier of the application to handle the new load unfortunately some of these new instances fail to launch.\nWhich of the following could be the root caused? (Choose two.)","discussion":[{"timestamp":"1723755060.0","comment_id":"1266687","content":"C. The ELB has scaled-up, adding more instances to handle the traffic spike, reducing the number of available private IP addresses for new instance launches\nE. AWS reserves the first four and the last IP address in each subnet's CIDR block so you do not have enough addresses left to launch all of the new EC2 instances","poster":"amministrazione","upvote_count":"1"},{"upvote_count":"1","comments":[{"poster":"Zoro483","upvote_count":"1","comment_id":"1305288","timestamp":"1730349600.0","content":"Availability Zone subnets\n\nYou must select at least two Availability Zone subnets. The following restrictions apply:\n\nEach subnet must be from a different Availability Zone.\n\nTo ensure that your load balancer can scale properly, verify that each Availability Zone subnet for your load balancer has a CIDR block with at least a /27 bitmask (for example, 10.0.0.0/27) and at least eight free IP addresses per subnet. These eight IP addresses are required to allow the load balancer to scale out if needed. Your load balancer uses these IP addresses to establish connections with the targets. Without them your Application Load Balancer could experience difficulties with node replacement attempts, causing it to enter a failed state.\n\nNote: If an Application Load Balancers subnet runs out of usable IP addresses while attempting to scale, the Application Load Balancer will run with insufficient capacity. During this time old nodes will continue to serve traffic, but the stalled scaling attempt may cause 5xx errors or timeouts when attempting to establish a connection."}],"timestamp":"1716583680.0","comment_id":"1217812","poster":"a6a3d55","content":"Why C? ELB don‚Äôt scale up it is the auto scaling group that do‚Ä¶"},{"poster":"JPA210","comment_id":"1145734","timestamp":"1707507120.0","content":"Selected Answer: CE\nThere is very good explanations below for this choice.","upvote_count":"1"},{"upvote_count":"1","comment_id":"755686","poster":"HassanYoussef","content":"The right answer i recommend to be ( C& E):\n\nThe Route 53 is not scaling itself to handle the traffic it act for routing the traffic like AWS routing tables in the VPC but for the DNS, so the best answer would be C not D.\n\nAWS docs:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html\nhttps://aws.amazon.com/route53/faqs/","timestamp":"1671976440.0"},{"content":"Selected Answer: CE\nThe question has some issues. Possibly some memory mistake. The /28 is the smallest CIDR size for the AWS VPC subnet, if VPC with /28 CIDR, it cannot be deployed into two AZs, which means at least two subnets (not to mention three-tier structure)\n\nRead the answers, only C and E are the right statements. Others are wrong:\nA. AWS reserves more IPs in a subnet than the first and last IP, +1 IP for Route, +2 for DNS, and +3 reserved for future use (as stated in answer E);\nB: There is no IGW scaling up (at least not a concern for users), and IGW does not consume IP in VPC IP space.\nD: Only mentioned reserving +2 IP which is for DNS, but miss others. Same mistake as A\n\nSomeone in this discussion has doubts about C. When ELB scales up, it does take more IPs. If you cannot deploy a load balancer into a subnet with available IPs less than 8, as AWS expects the scaleup will consume more IPs.","timestamp":"1671800400.0","comment_id":"754216","upvote_count":"2","poster":"TigerInTheCloud"},{"comment_id":"626432","poster":"aandc","content":"Selected Answer: CE\nit's CE","upvote_count":"3","timestamp":"1656826980.0"},{"content":"Selected Answer: DE\nDE as it‚Äôs only correct answers!!!\n\nWRONG Question!!!\nx2 AZs usage require x2 Subnets, x1 for each AZ.\nBut, Subnet can‚Äôt be /29!!!\n‚ÄúIPv4 block sizes must be between a /16 netmask and /28 netmask‚Äù\nSo only one Subnet and AZ is used.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html","comments":[{"upvote_count":"2","content":"https://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html\n‚Äú\nFor example, in a subnet with CIDR block 10.0.0.0/24, the following five IP addresses are reserved:\n10.0.0.0: Network address.\n10.0.0.1: Reserved by AWS for the VPC router.\n10.0.0.2: Reserved by AWS. The IP address of the DNS server is the base of the VPC network range plus two. For VPCs with multiple CIDR blocks, the IP address of the DNS server is located in the primary CIDR. We also reserve the base of each subnet range plus two for all CIDR blocks in the VPC. \nAmazon DNS server = is an Amazon Route 53 Resolver server.\n10.0.0.3: Reserved by AWS for future use.\n10.0.0.255: Network broadcast address. We do not support broadcast in a VPC, therefore we reserve this address.\n‚Äú\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/network-load-balancers.html\n‚Äú\nThe load balancer has one IP address per enabled Availability Zone.\n‚Äú","timestamp":"1650265440.0","poster":"Alexey79","comment_id":"587514","comments":[{"timestamp":"1650265440.0","comment_id":"587515","upvote_count":"3","poster":"Alexey79","content":"/28 is x16 IPs - x5 Reserved IPs = 11 available\n11 - x2 Web Servers - x2 Application Servers - x2 db - x1 NAT Intance = 4 available\n4 - x1 ELB IPs per x1 AZ = 3 available\n\nWhy NOT B:\nIGW has no IP and scaling up will not consume IP Address.\n\nWhy NOT C:\n‚ÄúELB ‚Ä¶ scaled-up‚Äù \nScaling EC2 instances in Subnet doesn‚Äôt take more IP Addresses of the ELB in AZ."}]}],"poster":"Alexey79","upvote_count":"3","timestamp":"1650265440.0","comment_id":"587513"},{"timestamp":"1650020820.0","poster":"tartarus23","comment_id":"586302","upvote_count":"1","content":"CE\nThe first four IP addresses and the last IP address in each subnet CIDR block are not available for your use, and they cannot be assigned to a resource, such as an EC2 instance."},{"comment_id":"553410","poster":"jyrajan69","timestamp":"1645500660.0","content":"10.0.0.0/28, means that 28 IP reserved for Network and 4 IP for the host, hence 2^4 which is 16. So from that 5 are reserved by AWS, hence only 11 IP's available for use. E is one definite answer. 7 of the IP's are in use, so only 4 are available now. B is IGW which is your access to the Internet. The IGW does not have any IP Address assigned to it, but can do NAT. So based on tht only possible is C. Answer C and E.","upvote_count":"3"},{"upvote_count":"1","content":"B or C‚Ä¶\n\nBoth of them have feature of scaling out but does ELB consume more IPs than IGW when spike loads?","timestamp":"1643954100.0","poster":"HellGate","comment_id":"540217"},{"poster":"lucesarano","upvote_count":"2","content":"C,E are correct indeed, but please specify the CIDR notation for the people that may not be used to it.\n\n10.0.0.0/28 has 14 ips available. Precisely,\nfrom 10.0.0.1\nto 10.0.0.14\n\n4 ips are reserved so availables ips are from 10.0.0.1 to 10.0.0.10\n\nknown fixed ips by reqs are 7 ips\nvariable ips are dictated by the Balancers, at least 1.\nso at least 8 ips are busy, making only 2 ips for scaling up.\n\nHence, C, E.","timestamp":"1641583680.0","comment_id":"519177"},{"upvote_count":"4","content":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html#vpc-sizing-ipv4","comment_id":"429594","poster":"nwk","timestamp":"1634024160.0"},{"timestamp":"1633233720.0","upvote_count":"2","content":"Yes CE","poster":"01037","comment_id":"360308"},{"content":"CE is correct","timestamp":"1633135320.0","comment_id":"358052","poster":"alfa2","upvote_count":"3"}],"answer_ET":"CE","unix_timestamp":1621096860,"isMC":true,"timestamp":"2021-05-15 18:41:00","question_id":699},{"id":"pIttCEXVfRxUee5Ap1xt","answer":"A","answer_description":"","question_text":"A company is running an Apache Hadoop cluster on Amazon EC2 instances. The Hadoop cluster stores approximately 100 TB of data for weekly operational reports and allows occasional access for data scientists to retrieve data. The company needs to reduce the cost and operational complexity for storing and serving this data.\nWhich solution meets these requirements in the MOST cost-effective manner?","topic":"1","answers_community":["A (69%)","C (31%)"],"isMC":true,"choices":{"A":"Move the Hadoop cluster from EC2 instances to Amazon EMR. Allow data access patterns to remain the same.","B":"Write a script that resizes the EC2 instances to a smaller instance type during downtime and resizes the instances to a larger instance type before the reports are created.","D":"Migrate the data to Amazon DynamoDB and modify the reports to fetch data from DynamoDB. Allow the data scientists to access the data directly in DynamoDB.","C":"Move the data to Amazon S3 and use Amazon Athena to query the data for reports. Allow the data scientists to access the data directly in Amazon S3."},"question_id":700,"answer_ET":"A","timestamp":"2021-03-13 12:28:00","question_images":[],"exam_id":32,"answer_images":[],"unix_timestamp":1615634880,"discussion":[{"upvote_count":"29","poster":"kejam","comment_id":"322963","timestamp":"1632696300.0","comments":[{"timestamp":"1636072320.0","poster":"AWSum1","upvote_count":"5","comment_id":"470824","content":"Great explanation. I suppose the deliberately put in EMR to confuse you into thinking it solves the Hadoop problem"}],"content":"C: S3 and Athena. \"The company needs to reduce the cost and operational complexity for storing and serving this data. Which solution meets these requirements in the MOST cost-effective manner?\" EMR storage is ephemeral. The company has 100TB that need to persist, they would have to use EMRFS to backup to S3 anyway.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage.html"},{"upvote_count":"8","timestamp":"1632148740.0","content":"A EMR helps creating Hadoop clusters to analyse vase amount of Data","comment_id":"309653","comments":[{"comment_id":"414081","poster":"WhyIronMan","timestamp":"1635217140.0","content":"but is not cost effective","upvote_count":"3"}],"poster":"doris0306"},{"poster":"rbm2023","comment_id":"909030","timestamp":"1685333460.0","upvote_count":"1","content":"Selected Answer: A\nAthena would not replace a map reduce for data analysis. you might reduce costs but you are not applying the right tool for a current solution."},{"poster":"romiao106","content":"Selected Answer: A\nwith 100 TB not actually sure s3 + athena would work","comment_id":"877690","upvote_count":"1","timestamp":"1682206920.0"},{"content":"Selected Answer: A\nA - EMR is cheaper than S3+Athena for such huge storage Athena will cost $5 per query per TB of data scanned (5x100TB = $500 per full query):\nhttps://aws.amazon.com/athena/pricing/","poster":"dev112233xx","comment_id":"873528","upvote_count":"1","timestamp":"1681817280.0"},{"poster":"aws0909","timestamp":"1676246940.0","comment_id":"806935","upvote_count":"2","content":"Selected Answer: C\nCost Effective solution is S3 and Athena"},{"comment_id":"763710","upvote_count":"1","poster":"evargasbrz","content":"Selected Answer: A\nI'll go with A","timestamp":"1672662240.0"},{"comment_id":"693317","content":"Selected Answer: C\nathena + S3 is definitely the cheaper option here","upvote_count":"1","timestamp":"1665599100.0","poster":"davideccc"},{"content":"Selected Answer: C\nMove the data to Amazon S3 and use Amazon Athena to query the data for reports. Allow the data scientists to access the data directly in Amazon S3.","timestamp":"1665340320.0","upvote_count":"1","poster":"JohnPi","comment_id":"690451"},{"poster":"dcdcdc3","timestamp":"1664293500.0","comment_id":"680925","upvote_count":"2","content":"Selected Answer: A\nPer the below article, EMR is way cheaper than ec2. I would choose A as I am not sure if the structure of hte data can be queried by Athena in cost-effective way\n\nhttps://blogs.perficient.com/2016/05/19/two-choices-1-amazon-emr-or-2-hadoop-on-ec2/"},{"content":"I will choose A AWS EMR because Amazon EMR makes it simple and cost effective to run highly distributed processing frameworks such as Hadoop, Spark, and Presto when compared to on-premises\nhttps://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html","timestamp":"1663148880.0","comment_id":"668853","upvote_count":"1","poster":"chase12345"},{"comment_id":"651608","upvote_count":"2","content":"C is the answer because EMR is not a cheap option.","timestamp":"1661395500.0","poster":"AYANtheGLADIATOR"},{"poster":"MarkChoi","timestamp":"1658446200.0","comment_id":"634900","content":"Selected Answer: A\n100TB??\nIs it possible to use Athena?\nI'll go with A","upvote_count":"3"},{"poster":"AzureDP900","comment_id":"497067","content":"I agree with C as right answer.","upvote_count":"1","timestamp":"1638987660.0"},{"timestamp":"1638792480.0","comment_id":"495106","upvote_count":"2","poster":"cldy","content":"C. Move the data to Amazon S3 and use Amazon Athena to query the data for reports. Allow the data scientists to access the data directly in Amazon S3."},{"timestamp":"1635965820.0","comment_id":"447553","content":"It's C","poster":"andylogan","upvote_count":"1"},{"content":"100TB\nEBS - 8.109$\nS3 - 2.355$\nYou have saved 5.752$\nThis amount can be used for Athen. BTW. we don't know indexes, amount of data that is scanned. What we know is that tit will be: \"occasional access for data scientists to retrieve data\"\n\nI am choosing C as CORRECT answer","poster":"DerekKey","timestamp":"1635918480.0","upvote_count":"2","comment_id":"439801"},{"upvote_count":"1","content":"C over A because it is most cost effective.","poster":"blackgamer","comment_id":"434292","timestamp":"1635562620.0"},{"comment_id":"428337","upvote_count":"1","poster":"denccc","content":"It's C","timestamp":"1635520980.0"},{"content":"Anthena is 5.00 per TB of data scanned. \nScenario A: \n10 queries on 5TB of uncompressed data stored in an S3 bucket. 250.00\n10 queries on 100TB of uncompressed data ... 5000.00 for just 10 queries.\nSo long story short, reshift. lol But since they are asking for the MOST cost-effective, I would have to say A. Even though I do see issues with it. And yes, you would back end this with S3 but it just assumes you know that.","timestamp":"1634988660.0","comments":[{"comment_id":"439802","poster":"DerekKey","content":"You are wrong. Check my answer.","timestamp":"1635934920.0","upvote_count":"2"}],"upvote_count":"2","comment_id":"409893","poster":"Tony_W"},{"comments":[{"upvote_count":"1","poster":"DerekKey","timestamp":"1635679920.0","content":"This is not most cost-effective solution","comment_id":"439794"}],"comment_id":"400794","poster":"vimgoru24","timestamp":"1634979600.0","upvote_count":"1","content":"The proper answer should be: move the stuff to Redshift and use it to query / generate reports."},{"poster":"vimgoru24","comment_id":"400792","content":"All of them nonsense. A does address only part of the problem, and C would fail performance-wise in real world - try query 100 TB with Athena.","timestamp":"1633814400.0","upvote_count":"5"},{"timestamp":"1633709160.0","comment_id":"358673","poster":"Waiweng","content":"it's C","upvote_count":"4"},{"poster":"ppshein","timestamp":"1633304100.0","content":"A is the best because not sure data scientists are able to use query in Athena.","comment_id":"347717","upvote_count":"2"},{"timestamp":"1633250340.0","comment_id":"329988","poster":"Pupu86","upvote_count":"4","content":"At first I would have chosen A but digging deeper, C is a better choice.\nHere‚Äôs a dial-down summary of HDFS vs S3 as big data storage options - comparisons.\nhttps://www.xplenty.com/blog/storing-apache-hadoop-data-cloud-hdfs-vs-s3/"},{"poster":"SD13","content":"Correct option C: Seems to be the most cost-effective option. Not choosing A since it would increase cost not reduce it.","comment_id":"328694","upvote_count":"3","timestamp":"1633205760.0"},{"poster":"alexbg88","comment_id":"323722","upvote_count":"3","timestamp":"1633061760.0","content":"Agree with C"},{"content":"A is correct","comment_id":"313840","comments":[{"comment_id":"414080","content":"it's not cost effective","upvote_count":"2","poster":"WhyIronMan","timestamp":"1635134820.0"}],"timestamp":"1632608100.0","poster":"nitinz","upvote_count":"5"}],"url":"https://www.examtopics.com/discussions/amazon/view/46870-exam-aws-certified-solutions-architect-professional-topic-1/"}],"exam":{"isBeta":false,"numberOfQuestions":1019,"provider":"Amazon","isImplemented":true,"id":32,"name":"AWS Certified Solutions Architect - Professional","isMCOnly":false,"lastUpdated":"11 Apr 2025"},"currentPage":140},"__N_SSP":true}