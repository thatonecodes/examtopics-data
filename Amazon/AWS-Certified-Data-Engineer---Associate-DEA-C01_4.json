{"pageProps":{"questions":[{"id":"l31aqioTkRWQAivtyPGS","answer_ET":"A","topic":"1","choices":{"D":"Set up an AWS DMS replication instance in Account_A in eu-east-1.","A":"Set up an AWS DMS replication instance in Account_B in eu-west-1.","B":"Set up an AWS DMS replication instance in Account_B in eu-east-1.","C":"Set up an AWS DMS replication instance in a new AWS account in eu-west-1."},"question_text":"A company wants to migrate data from an Amazon RDS for PostgreSQL DB instance in the eu-east-1 Region of an AWS account named Account_A. The company will migrate the data to an Amazon Redshift cluster in the eu-west-1 Region of an AWS account named Account_B.\n\nWhich solution will give AWS Database Migration Service (AWS DMS) the ability to replicate data between two data stores?","answer":"A","answer_description":"","exam_id":21,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/143054-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"unix_timestamp":1719645060,"discussion":[{"timestamp":"1721276820.0","content":"Selected Answer: A\nRedshift needs to be in the same region as the replication instance see docs:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites","poster":"andrologin","upvote_count":"5","comment_id":"1250100"},{"timestamp":"1724282580.0","upvote_count":"1","poster":"samadal","comment_id":"1270420","content":"Selected Answer: D\nWhen you use WS DMS to migrate data between different AWS Regions or accounts, you must remember the following:\n\nThe replication instance must be created in the same Region as the source database.\nThe target endpoint must be created in the Region where the target data store is located.\nYou must set up the required IAM roles and permissions to enable DMS to access the source and target resources."},{"comment_id":"1243393","timestamp":"1720274160.0","upvote_count":"4","content":"Selected Answer: A\nRedshift has to be in the same region as the DMS\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites","poster":"lool"},{"comment_id":"1241057","upvote_count":"1","timestamp":"1719958320.0","poster":"bakarys","content":"Selected Answer: A\nTo enable AWS Database Migration Service (AWS DMS) to replicate data between two data stores in different AWS Regions, you should choose option A. Hereâ€™s why:\n\nOption A: Set up an AWS DMS replication instance in Account_B in eu-west-1. This approach allows you to configure replication between the Amazon RDS for PostgreSQL DB instance in eu-east-1 and the Amazon Redshift cluster in eu-west-1. By using AWS DMS, you can efficiently migrate data across Regions while minimizing downtime and ensuring data consistency"},{"comment_id":"1240868","timestamp":"1719934200.0","poster":"sdas1","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html","upvote_count":"1"},{"poster":"sdas1","content":"The correct solution to replicate data between the Amazon RDS for PostgreSQL DB instance in Account_A (eu-east-1) and the Amazon Redshift cluster in Account_B (eu-west-1) using AWS Database Migration Service (AWS DMS) is:\n\nA. Set up an AWS DMS replication instance in Account_B in eu-west-1.","comment_id":"1240867","timestamp":"1719933840.0","upvote_count":"1"},{"content":"Selected Answer: B\nB - becuase AWS DMS must be in a same region as AWS redshift cluster\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites","poster":"HunkyBunky","comment_id":"1240842","timestamp":"1719930240.0","upvote_count":"2"},{"timestamp":"1719645060.0","upvote_count":"1","poster":"Bmaster","content":"My Choice is D","comment_id":"1239095"}],"answers_community":["A (77%)","B (15%)","8%"],"timestamp":"2024-06-29 09:11:00","question_id":16,"isMC":true},{"id":"7uOuU4R8Vo5senzokcIJ","choices":{"B":"Load all the data files in parallel into Amazon Aurora. Run an AWS Glue job to load the data into Amazon Redshift.","C":"Use an AWS Give job to copy all the data files into one folder. Use a COPY command to load the data into Amazon Redshift.","D":"Create a manifest file that contains the data file locations. Use a COPY command to load the data into Amazon Redshift.","A":"Use a provisioned Amazon EMR cluster to copy all the data files into one folder. Use a COPY command to load the data into Amazon Redshift."},"isMC":true,"exam_id":21,"timestamp":"2024-06-29 09:34:00","url":"https://www.examtopics.com/discussions/amazon/view/143056-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["D (100%)"],"answer_ET":"D","question_images":[],"unix_timestamp":1719646440,"question_id":17,"answer_images":[],"answer_description":"","discussion":[{"content":"Selected Answer: D\nD is the right answer based on the docs in this page https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html","poster":"andrologin","upvote_count":"1","timestamp":"1721361900.0","comment_id":"1250810"},{"upvote_count":"4","content":"Selected Answer: D\nOnly D makes sense\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html","timestamp":"1719930420.0","comment_id":"1240844","poster":"HunkyBunky"},{"upvote_count":"1","poster":"Bmaster","content":"D is good\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html\nhttps://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html","timestamp":"1719646440.0","comment_id":"1239108"}],"topic":"1","answer":"D","question_text":"A company uses Amazon S3 as a data lake. The company sets up a data warehouse by using a multi-node Amazon Redshift cluster. The company organizes the data files in the data lake based on the data source of each data file.\n\nThe company loads all the data files into one table in the Redshift cluster by using a separate COPY command for each data file location. This approach takes a long time to load all the data files into the table. The company must increase the speed of the data ingestion. The company does not want to increase the cost of the process.\n\nWhich solution will meet these requirements?"},{"id":"0sdQYGEysOQtxIPpwl2p","isMC":true,"question_text":"A company plans to use Amazon Kinesis Data Firehose to store data in Amazon S3. The source data consists of 2 MB .csv files. The company must convert the .csv files to JSON format. The company must store the files in Apache Parquet format.\n\nWhich solution will meet these requirements with the LEAST development effort?","answer_description":"","discussion":[{"upvote_count":"8","comments":[{"content":"why do you need lambda in the middle, per you link Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3...my choice is B","comments":[{"comment_id":"1268228","content":"https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nthere is need is need to invoke Lambda","poster":"mzansikiller","upvote_count":"2","comments":[{"content":"It is stated that \" If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first\"","comment_id":"1300544","upvote_count":"2","timestamp":"1729437900.0","poster":"Eleftheriia"}],"timestamp":"1724019480.0"}],"poster":"LR2023","upvote_count":"2","timestamp":"1721236800.0","comment_id":"1249839"}],"comment_id":"1249430","timestamp":"1721197440.0","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html","poster":"qwertyuio"},{"upvote_count":"6","content":"Answer D\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nAmazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information, see Transform data in Amazon Data Firehose.","poster":"mzansikiller","comment_id":"1252331","timestamp":"1721552040.0"},{"timestamp":"1743008640.0","content":"Selected Answer: B\nsimplest and most efficient - Firehose to convert to JSON and store in Parquet - no need for Lambda function","upvote_count":"1","poster":"JimOGrady","comment_id":"1410487"},{"upvote_count":"1","timestamp":"1742125320.0","comment_id":"1399220","poster":"saurwt","content":"Selected Answer: D\nAmazon Kinesis Data Firehose does not natively support CSV to JSON conversion. However, it does support JSON to Parquet conversion.\n\nGiven that, the best approach with the least development effort is:\n\nD. Use Kinesis Data Firehose to invoke an AWS Lambda function that transforms the .csv files to JSON. Use Kinesis Data Firehose to store the files in Parquet format."},{"timestamp":"1741697820.0","poster":"Ramdi1","upvote_count":"1","comment_id":"1387416","content":"Selected Answer: D\nKinesis Data Firehose natively supports data format conversion to Parquet, reducing development effort.\nAWS Lambda is needed only for the CSV to JSON conversion, as Firehose does not support direct CSV to JSON transformation.\nFirehose then automatically converts JSON to Parquet and stores it in S3, minimizing custom code."},{"comment_id":"1346595","content":"Selected Answer: B\nhttps://aws.amazon.com/ar/about-aws/whats-new/2016/12/amazon-kinesis-firehose-can-now-prepare-and-transform-streaming-data-before-loading-it-to-data-stores/","upvote_count":"1","poster":"Salam9","timestamp":"1737829980.0"},{"upvote_count":"1","content":"Selected Answer: C\nLambda handles both the CSV-to-JSON and JSON-to-Parquet transformations before Firehose stores the data in Amazon S3","comment_id":"1338135","poster":"kailu","timestamp":"1736382120.0"},{"poster":"zoneout","upvote_count":"1","comment_id":"1331276","content":"Selected Answer: D\nIf you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first and then you can use Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC.","timestamp":"1735083120.0"},{"content":"Selected Answer: C\nI would go with C. D is close but Kinesis Data Firehose does not really store files in Parquet format.","upvote_count":"1","poster":"kailu","timestamp":"1734804720.0","comment_id":"1330152"},{"poster":"michele_scar","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nYou need firstly a JSON (using Lambda) to be able using Kinesis to store it in Parquet","upvote_count":"1","comment_id":"1312016","timestamp":"1731592500.0"},{"comment_id":"1301609","poster":"rsmf","timestamp":"1729605540.0","upvote_count":"2","content":"Selected Answer: D\nFirehose can't convert csv to json.\n\nSo, that's D"},{"comment_id":"1285242","upvote_count":"2","timestamp":"1726578420.0","poster":"PashoQ","content":"Selected Answer: D\nIf you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information"},{"comment_id":"1268230","upvote_count":"3","content":"Selected Answer: D\nAmazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information, see Transform source data in Amazon Data Firehose.\nAnswer D","poster":"mzansikiller","timestamp":"1724019660.0"},{"content":"Selected Answer: B\nKinesis Data Firehose: It has built-in support for data transformation and format conversion. It can directly convert incoming data from .csv to JSON format and then further convert the data to Apache Parquet format before storing it in Amazon S3.\n\nMinimal Development Effort: This option requires the least development effort because Kinesis Data Firehose handles both the transformation (from .csv to JSON) and the format conversion (to Parquet) natively. No additional AWS Lambda functions or custom code are needed.","poster":"Shanmahi","comment_id":"1263612","upvote_count":"2","timestamp":"1723320180.0"},{"comment_id":"1259922","content":"Selected Answer: B\nB. Why? Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3.\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\nWith that LEAST development effort, why do we need to use Lambda additionally? :D","comments":[{"content":"read to understand:\n\nAmazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information, see Transform source data in Amazon Data Firehose.","comment_id":"1268229","timestamp":"1724019600.0","upvote_count":"1","poster":"mzansikiller"}],"poster":"MinTheRanger","timestamp":"1722607080.0","upvote_count":"4"},{"content":"Option D - Need to convert the inout data from .csv to JSON first. Firehose can't do that without the help of a lambda function in this case. After firehose can convert to .parquet and deliver it to s3","upvote_count":"3","poster":"valuedate","comment_id":"1254188","timestamp":"1721804340.0"},{"content":"Selected Answer: B\nB - least development efforts","timestamp":"1719930540.0","poster":"HunkyBunky","upvote_count":"2","comment_id":"1240848"},{"upvote_count":"4","comment_id":"1239459","poster":"Alagong","timestamp":"1719707760.0","content":"Selected Answer: B\nBy using the built-in transformation and format conversion features of Kinesis Data Firehose, you achieve the desired result with minimal custom development, thereby meeting the requirements efficiently and cost-effectively."},{"content":"D is good\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html","comments":[{"comment_id":"1239864","content":"\" If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information\"","poster":"Bmaster","upvote_count":"3","timestamp":"1719784680.0"}],"comment_id":"1239110","timestamp":"1719646500.0","poster":"Bmaster","upvote_count":"1"}],"choices":{"C":"Use Kinesis Data Firehose to invoke an AWS Lambda function that transforms the .csv files to JSON and stores the files in Parquet format.","A":"Use Kinesis Data Firehose to convert the .csv files to JSON. Use an AWS Lambda function to store the files in Parquet format.","D":"Use Kinesis Data Firehose to invoke an AWS Lambda function that transforms the .csv files to JSON. Use Kinesis Data Firehose to store the files in Parquet format.","B":"Use Kinesis Data Firehose to convert the .csv files to JSON and to store the files in Parquet format."},"timestamp":"2024-06-29 09:35:00","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/143057-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"D","unix_timestamp":1719646500,"question_id":18,"answer_ET":"D","topic":"1","answers_community":["D (54%)","B (40%)","6%"],"question_images":[],"answer_images":[]},{"id":"JyFfEYxpglpOUutU8ppS","question_images":[],"topic":"1","answer_ET":"C","answers_community":["C (100%)"],"answer_images":[],"answer_description":"","question_id":19,"unix_timestamp":1719646620,"answer":"C","discussion":[{"upvote_count":"1","timestamp":"1742116980.0","comment_id":"1399179","content":"Selected Answer: C\nC is correct","poster":"Palee"},{"comment_id":"1244465","content":"Selected Answer: C\nA company is using an AWS Transfer Family server to migrate data from an on-premises environment to AWS. Company policy mandates the use of TLS 1.2 or above to encrypt the data in transit.\n\nWhich solution will meet these requirements?\n\nA. Generate new SSH keys for the Transfer Family server. Make the old keys and the new keys available for use.\nB. Update the security group rules for the on-premises network to allow only connections that use TLS 1.2 or above.\nC. Update the security policy of the Transfer Family server to specify a minimum protocol version of TLS 1.2\nD. Install an SSL certificate on the Transfer Family server to encrypt data transfers by using TLS 1.2.","poster":"Ja13","timestamp":"1720456140.0","upvote_count":"1"},{"comment_id":"1240850","poster":"HunkyBunky","upvote_count":"2","content":"Selected Answer: C\nOnly C is good","timestamp":"1719930720.0"},{"content":"C is correct\n\nhttps://docs.aws.amazon.com/transfer/latest/userguide/security-policies.html","timestamp":"1719646620.0","upvote_count":"4","comment_id":"1239111","poster":"Bmaster"}],"choices":{"C":"Update the security policy of the Transfer Family server to specify a minimum protocol version of TLS 1.2","B":"Update the security group rules for the on-premises network to allow only connections that use TLS 1.2 or above.","D":"Install an SSL certificate on the Transfer Family server to encrypt data transfers by using TLS 1.2.","A":"Generate new SSH keys for the Transfer Family server. Make the old keys and the new keys available for use."},"question_text":"A company is using an AWS Transfer Family server to migrate data from an on-premises environment to AWS. Company policy mandates the use of TLS 1.2 or above to encrypt the data in transit.\n\nWhich solution will meet these requirements?","isMC":true,"timestamp":"2024-06-29 09:37:00","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/143058-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"emo1vlLPmP006nGuf541","question_text":"A company wants to migrate an application and an on-premises Apache Kafka server to AWS. The application processes incremental updates that an on-premises Oracle database sends to the Kafka server. The company wants to use the replatform migration strategy instead of the refactor strategy.\n\nWhich solution will meet these requirements with the LEAST management overhead?","unix_timestamp":1719647280,"timestamp":"2024-06-29 09:48:00","question_id":20,"choices":{"D":"Amazon Managed Streaming for Apache Kafka (Amazon MSK) Serverless","C":"Amazon Kinesis Data Firehose","A":"Amazon Kinesis Data Streams","B":"Amazon Managed Streaming for Apache Kafka (Amazon MSK) provisioned cluster"},"answer_description":"","answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/143060-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"D","exam_id":21,"answer_images":[],"isMC":true,"topic":"1","question_images":[],"answers_community":["D (100%)"],"discussion":[{"comment_id":"1241156","upvote_count":"4","poster":"HunkyBunky","content":"Selected Answer: D\nD - becase this is lift-and-shift migration and serveless - because LEAST management overhead","timestamp":"1719981780.0"},{"content":"D is good","comments":[{"content":"C. Amazon Kinesis Data Firehose: This service delivers real-time data to other AWS destinations, but it's not a direct replacement for Kafka and requires additional configuration for replicating data streams.\nD. Amazon Managed Streaming for Apache Kafka (Amazon MSK) Serverless: This is the best fit as it provides a fully managed Kafka experience with automatic scaling and eliminates the need to manage servers or infrastructure. This aligns perfectly with the replatform strategy and minimizes management overhead.","poster":"Bmaster","timestamp":"1719647340.0","comment_id":"1239117","upvote_count":"1"}],"timestamp":"1719647280.0","upvote_count":"2","comment_id":"1239115","poster":"Bmaster"}]}],"exam":{"id":21,"isImplemented":true,"provider":"Amazon","name":"AWS Certified Data Engineer - Associate DEA-C01","numberOfQuestions":207,"lastUpdated":"11 Apr 2025","isBeta":false,"isMCOnly":true},"currentPage":4},"__N_SSP":true}