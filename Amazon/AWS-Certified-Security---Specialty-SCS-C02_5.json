{"pageProps":{"questions":[{"id":"Xx02vu21SV8j5SMu2o8G","exam_id":30,"isMC":true,"topic":"1","answer":"B","unix_timestamp":1707829020,"question_id":21,"answers_community":["B (100%)"],"choices":{"D":"Create a role and enforce multi-factor authentication in the role trust policy. Instruct users to run the sts assume-role CLI command and pass --serial-number and --token-code parameters. Store the resulting values in environment variables. Add sts:AssumeRole to NotAction in the policy.","C":"Implement federated API/CLI access using SAML 2.0, then configure the identity provider to enforce multi-factor authentication.","B":"Instruct users to run the aws sts get-session-token CLI command and pass the multi-factor authentication --serial-number and -token-code parameters. Use these resulting values to make API/CLI calls.","A":"Change the value of aws:MultiFactorAuthPresent to true."},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/133742-exam-aws-certified-security-specialty-scs-c02-topic-1/","question_text":"An AWS account administrator created an IAM group and applied the following managed policy to require that each individual user authenticate using multi-factor authentication:\n\n//IMG//\n\n\nAfter implementing the policy, the administrator receives reports that users are unable to perform Amazon EC2 commands using the AWS CLI.\n\nWhat should the administrator do to resolve this problem while still enforcing multi-factor authentication?","timestamp":"2024-02-13 13:57:00","answer_description":"","question_images":["https://img.examtopics.com/aws-certified-security-specialty-scs-c02/image35.png"],"answer_ET":"B","discussion":[{"upvote_count":"1","poster":"molerowan","content":"Selected Answer: B\naws sts get-session-token with MFA parameters creates temporary credentials that include MFA verification\nThese temporary credentials (access key, secret key, and session token) can be used for subsequent CLI calls\nThe AWS CLI will then recognize these requests as being made with MFA authentication, satisfying the policy condition","timestamp":"1741844820.0","comment_id":"1388197"},{"upvote_count":"1","timestamp":"1724409120.0","poster":"nublit","content":"Selected Answer: B\nB for sure","comment_id":"1157108"},{"poster":"awssecuritynewbie","upvote_count":"1","comment_id":"1152921","content":"Selected Answer: B\nB for sure they required to enable CLI","timestamp":"1723930200.0"},{"poster":"sarcactus","content":"Selected Answer: B\nI agree with MikeRach comment.","upvote_count":"3","timestamp":"1723808040.0","comment_id":"1152016"},{"comment_id":"1149170","content":"B\nhttps://www.examtopics.com/discussions/amazon/view/47596-exam-aws-certified-security-specialty-topic-1-question-225/","poster":"MikeRach","upvote_count":"1","timestamp":"1723546620.0"}]},{"id":"t5ux9ckdYoh3W0EPjnKO","question_images":[],"answer_description":"","timestamp":"2024-02-15 03:54:00","question_text":"A company is developing a mechanism that will help data scientists use Amazon SageMaker to read, process, and output data to an Amazon S3 bucket. Data scientists will have access to a dedicated S3 prefix for each of their projects. The company will implement bucket policies that use the dedicated S3 prefixes to restrict access to the S3 objects. The projects can last up to 60 days.\n\nThe company's security team mandates that data cannot remain in the S3 bucket after the end of the projects that use the data.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_images":[],"topic":"1","answer_ET":"C","question_id":22,"discussion":[{"timestamp":"1726154520.0","content":"anyone given exam in this week or last week can anyone confirm is this question is good for practise or required more question ?","poster":"rishikeshshukla7777","upvote_count":"5","comment_id":"1171859"},{"content":"Selected Answer: C\nC makes the most sense","poster":"ion_gee","timestamp":"1728269700.0","upvote_count":"1","comment_id":"1190731"},{"timestamp":"1727712780.0","comment_id":"1186241","poster":"ale_brd_111","upvote_count":"1","content":"Selected Answer: C\nno brainer C."},{"comment_id":"1171326","upvote_count":"1","poster":"Gzuis","content":"my vote is for C","timestamp":"1726084920.0"},{"comment_id":"1157110","poster":"nublit","timestamp":"1724409240.0","upvote_count":"1","content":"Selected Answer: C\nC for sure"},{"comment_id":"1153740","timestamp":"1724042040.0","upvote_count":"1","poster":"sarcactus","content":"Selected Answer: C\nC is the correct one.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configuration-examples.html#lifecycle-config-conceptual-ex3"},{"content":"C is the answer","upvote_count":"2","comment_id":"1153158","timestamp":"1723966800.0","poster":"anasbakla"},{"poster":"anasbakla","comment_id":"1153156","content":"Me too","upvote_count":"1","timestamp":"1723966740.0"},{"upvote_count":"1","poster":"awssecuritynewbie","timestamp":"1723930500.0","content":"Selected Answer: C\nC makes sense you need to remove the data after 60 day so lifecycle will do that.","comment_id":"1152925"},{"comment_id":"1150718","poster":"jabilrn","upvote_count":"1","content":"C for me","timestamp":"1723683240.0"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/133924-exam-aws-certified-security-specialty-scs-c02-topic-1/","choices":{"A":"Create an AWS Lambda function to identify and delete objects in the S3 bucket that have not been accessed for 60 days. Create an Amazon EventBridge scheduled rule that runs every day to invoke the Lambda function.","C":"Create an S3 Lifecycle configuration for each S3 bucket prefix for each project. Set the S3 Lifecycle configurations to expire objects after 60 days.","B":"Create a new S3 bucket. Configure the new S3 bucket to use S3 Intelligent-Tiering. Copy the objects to the new S3 bucket.","D":"Create an AWS Lambda function to delete objects that have not been accessed for 60 days. Create an S3 event notification for S3 Intelligent-Tiering automatic archival events to invoke the Lambda function."},"exam_id":30,"answers_community":["C (100%)"],"unix_timestamp":1707965640,"answer":"C"},{"id":"4nKhLnPJLcPHviTAraXA","exam_id":30,"isMC":true,"topic":"1","answer":"C","question_id":23,"unix_timestamp":1715604840,"answers_community":["C (100%)"],"choices":{"B":"Enable the PublicAccessBlock configuration on the S3 bucket. Configure an SCP to deny the s3:GetObject action for the OU that contains the AWS account.","C":"Enable the PublicAccessBlock configuration on the S3 bucket. Configure an SCP to deny the s3:PutPublicAccessBlock action for the OU that contains the AWS account.","D":"Configure the S3 bucket to use S3 Object Lock in governance mode. Configure an SCP to deny the s3:PutPublicAccessBlock action for the OU that contains the AWS account.","A":"Configure the S3 bucket to use an AWS Key Management Service (AWS KMS) key. Encrypt all objects in the S3 bucket by creating a bucket policy that enforces encryption. Configure an SCP to deny the s3:GetObject action for the OU that contains the AWS account."},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/140556-exam-aws-certified-security-specialty-scs-c02-topic-1/","question_text":"A company has AWS accounts that are in an organization in AWS Organizations. An Amazon S3 bucket in one of the accounts is publicly accessible.\n\nA security engineer must change the configuration so that the S3 bucket is no longer publicly accessible. The security engineer also must ensure that the S3 bucket cannot be made publicly accessible in the future.\n\nWhich solution will meet these requirements?","timestamp":"2024-05-13 14:54:00","answer_description":"","question_images":[],"answer_ET":"C","discussion":[{"content":"Selected Answer: C\nFor those who chose B, how can it be correct? an SCP denying s3:GetObject will deny access to the objects regardless if it is from the public or within the account... B will render the bucket inaccessible.","timestamp":"1737667200.0","upvote_count":"1","poster":"nznzwell","comment_id":"1345683"},{"poster":"helloworldabc","content":"just B","upvote_count":"1","comment_id":"1285500","timestamp":"1726616820.0"},{"timestamp":"1719540960.0","comments":[{"upvote_count":"1","poster":"Davidng88","comment_id":"1281307","content":"By configuring an SCP to deny the s3:GetObject action prevents accessing S3 objects from public, but deny the s3:PutPublicAccessBlock action, prevent any changes to the disable PublicAccessBlock settings, ensuring that the bucket remains private in future.","timestamp":"1725926640.0"}],"comment_id":"1238452","upvote_count":"1","content":"Answer is B \nEnabling the PublicAccessBlock configuration on the S3 bucket prevents public access.\nAdditionally, configuring an SCP (Service Control Policy) to deny the s3:GetObject action for the organizational unit (OU) containing the AWS account ensures that the bucket remains private.","poster":"Olaunfazed"},{"content":"why not B?","poster":"sema2232","timestamp":"1718176980.0","upvote_count":"1","comment_id":"1228879"},{"poster":"aescudero51","content":"Selected Answer: C\nEnable PublicAccessBlock Configuration:\nhttps://aws.amazon.com/s3/features/block-public-access/?nc1=h_ls\n\nConfigure an SCP (Service Control Policy):\nAn SCP is a policy that you can attach to an AWS Organization, organizational unit (OU), or an account. It acts as a guardrail to control permissions across accounts. In your case, you want to deny the s3:PutPublicAccessBlock action for the OU containing your AWS account.\nGo to the AWS Organizations console.\nNavigate to the OU that contains your account.\nCreate a new SCP or edit an existing one.\nAdd a statement that denies the s3:PutPublicAccessBlock action for the relevant S3 buckets.\nAttach the SCP to the OU.\nEnsure that your AWS account is part of the OU.","comment_id":"1216056","timestamp":"1716418740.0","upvote_count":"2"},{"upvote_count":"1","poster":"Zek","content":"C \nEnable the PublicAccessBlock & use SCP to deny the s3:PutPublicAccessBlock action","timestamp":"1715604840.0","comment_id":"1210868"}]},{"id":"5Dc3WLp4NtUXhT7a8Dnr","question_images":[],"answers_community":["ABC (75%)","ACF (25%)"],"answer_ET":"ABC","answer_description":"","unix_timestamp":1696374600,"url":"https://www.examtopics.com/discussions/amazon/view/122296-exam-aws-certified-security-specialty-scs-c02-topic-1/","timestamp":"2023-10-04 01:10:00","isMC":true,"choices":{"A":"Ensure that the Amazon CloudWatch agent is installed on all the EC2 instances that the Auto Scaling groups launch. Generate a CloudWatch agent configuration file to forward the required logs to Amazon CloudWatch Logs.","B":"Set the log retention for desired log groups to 7 years.","F":"Configure an Amazon S3 Lifecycle policy on the target S3 bucket to expire objects after 7 years.","C":"Attach an IAM role to the launch configuration or launch template that the Auto Scaling groups use. Configure the role to provide the necessary permissions to forward logs to Amazon CloudWatch Logs.","D":"Attach an IAM role to the launch configuration or launch template that the Auto Scaling groups use. Configure the role to provide the necessary permissions to forward logs to Amazon S3.","E":"Ensure that a log forwarding application is installed on all the EC2 instances that the Auto Scaling groups launch. Configure the log forwarding application to periodically bundle the logs and forward the logs to Amazon S3."},"question_text":"An international company has established a new business entity in South Korea. The company also has established a new AWS account to contain the workload for the South Korean region. The company has set up the workload in the new account in the ap-northeast-2 Region. The workload consists of three Auto Scaling groups of Amazon EC2 instances. All workloads that operate in this Region must keep system logs and application logs for 7 years.\nA security engineer must implement a solution to ensure that no logging data is lost for each instance during scaling activities. The solution also must keep the logs for only the required period of 7 years.\nWhich combination of steps should the security engineer take to meet these requirements? (Choose three.)","answer":"ABC","answer_images":[],"discussion":[{"timestamp":"1724262720.0","content":"Selected Answer: ABC\nABC adjust more to the question. No need of external applications to send logs.","poster":"FunkyFresco","upvote_count":"2","comment_id":"1270318"},{"content":"ABC IS CORRECT OPTION","timestamp":"1723737660.0","upvote_count":"1","poster":"nischal77777","comment_id":"1266548"},{"timestamp":"1714114020.0","poster":"liuyomz","content":"Selected Answer: ABC\nABC is the most straighfoward and elegant solution here","upvote_count":"1","comment_id":"1202419"},{"timestamp":"1707513000.0","poster":"Raphaello","content":"Selected Answer: ABC\nABC.\nEC2 (with a role allowing sending events) >> CloudWatch agent >> CloudWatch Logs >> CloudWatch Logs retention period","comment_id":"1145798","upvote_count":"2"},{"comments":[{"comment_id":"1089523","timestamp":"1701879060.0","upvote_count":"4","poster":"mav3r1ck","comments":[{"poster":"Daniel76","comment_id":"1110957","content":"Agree it should not be ACF but ABC. ACF does not explain how cloudwatch log end s up in s3. (seems that it requires a lambda function to automate)\nIn fact, if S3 is the chosen path then it can only be DEF for consistency. But this combination assume that s3 bucket policy has been configured and the log forwarding application configured can reliably send all logs data without losing any.","upvote_count":"4","timestamp":"1704081180.0"}],"content":"I would go for ACF if the asked is \"COST-EFFECTIVE\" solution. But leaning to ABF, as Cloudwatch logs support up to 10yrs of retention as well. Feel free to disagree if you think I'm wrong. https://docs.aws.amazon.com/managedservices/latest/userguide/log-customize-retention.html"}],"upvote_count":"3","poster":"Daniel76","timestamp":"1700295000.0","comment_id":"1073854","content":"Selected Answer: ACF\nAs the log data for 7 years will be expensive, use AWS S3 Lifecycle Management to transfer data to lower cost storage class will be more cost effective solution.\nhttps://medium.com/avmconsulting-blog/aws-s3-lifecycle-management-1ed2f67c3b73"},{"poster":"Karamen","timestamp":"1698839280.0","comment_id":"1059666","upvote_count":"4","content":"ABC\n\nthere isn't good option to forwarding log from EC2 to S3 bucket."},{"comment_id":"1056596","poster":"lalee2","content":"Selected Answer: ABC\nCloudWatch agent -> CloudWatch Logs, IAM role to launch template -> CloudWatch Logs","upvote_count":"1","timestamp":"1698557640.0"},{"upvote_count":"2","comment_id":"1054691","content":"Selected Answer: ABC\nABC it is.","timestamp":"1698332880.0","poster":"pupsik"},{"content":"A, B and C","timestamp":"1698120960.0","poster":"KR693","upvote_count":"1","comment_id":"1052480"},{"poster":"100fold","comment_id":"1047453","content":"Selected Answer: ABC\nAnswer ABC","upvote_count":"1","timestamp":"1697681220.0"},{"comment_id":"1024272","content":"https://www.examtopics.com/discussions/amazon/view/89514-exam-aws-certified-security-specialty-topic-1-question-451/","upvote_count":"2","timestamp":"1696374600.0","poster":"aragon_saa"}],"topic":"1","exam_id":30,"question_id":24},{"id":"O1uEJgLKo5SEPjLwu2fb","answers_community":["AC (100%)"],"answer":"AC","discussion":[{"poster":"sema2232","comments":[{"comment_id":"1285501","timestamp":"1726616880.0","content":"just AC","poster":"helloworldabc","upvote_count":"1"}],"content":"why not B?","timestamp":"1718177100.0","comment_id":"1228880","upvote_count":"1"},{"content":"Selected Answer: AC\nA/C\nB - You don't use ACM for encryption, it's KMS\nD - You can't encrypt an existing cluster, you need to snapshot, then encrypt with KMS\nE - Same as B","poster":"aescudero51","upvote_count":"4","comment_id":"1216452","timestamp":"1716459780.0"},{"content":"A,C - Agree","timestamp":"1715605020.0","upvote_count":"1","poster":"Zek","comment_id":"1210869"},{"upvote_count":"2","content":"Selected Answer: AC\nAC . You can not encrypt ebs with ACM.","poster":"danish1234","timestamp":"1715416380.0","comment_id":"1209658"},{"content":"AC . All other options are joke.","upvote_count":"1","timestamp":"1715416320.0","poster":"danish1234","comment_id":"1209657"}],"question_id":25,"answer_description":"","topic":"1","exam_id":30,"question_images":[],"question_text":"A company is designing a new application stack. The design includes web servers and backend servers that are hosted on Amazon EC2 instances. The design also includes an Amazon Aurora MySQL DB cluster.\n\nThe EC2 instances are in an Auto Scaling group that uses launch templates. The EC2 instances for the web layer and the backend layer are backed by Amazon Elastic Block Store (Amazon EBS) volumes. No layers are encrypted at rest A security engineer needs to implement encryption at rest.\n\nWhich combination of steps will meet these requirements? (Choose two.)","answer_images":[],"choices":{"B":"Modify the launch templates for the web layer and the backend layer to add AWS Certificate Manager (ACM) encryption for the attached EBS volumes. Use an Auto Scaling group instance refresh.","C":"Create a new AWS Key Management Service (AWS KMS) encrypted DB cluster from a snapshot of the existing DB cluster.","A":"Modify EBS default encryption settings in the target AWS Region to enable encryption. Use an Auto Scaling group instance refresh.","D":"Apply AWS Key Management Service (AWS KMS) encryption to the existing DB cluster.","E":"Apply AWS Certificate Manager (ACM) encryption to the existing DB cluster."},"unix_timestamp":1715416320,"timestamp":"2024-05-11 10:32:00","url":"https://www.examtopics.com/discussions/amazon/view/140323-exam-aws-certified-security-specialty-scs-c02-topic-1/","answer_ET":"AC","isMC":true}],"exam":{"isBeta":false,"isMCOnly":true,"isImplemented":true,"id":30,"provider":"Amazon","name":"AWS Certified Security - Specialty SCS-C02","numberOfQuestions":288,"lastUpdated":"11 Apr 2025"},"currentPage":5},"__N_SSP":true}