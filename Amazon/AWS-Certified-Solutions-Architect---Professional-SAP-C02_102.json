{"pageProps":{"questions":[{"id":"N9uwTTAdS8APNsjA6bTV","answer_images":[],"question_images":[],"unix_timestamp":1673714940,"exam_id":33,"answer_ET":"B","answers_community":["B (86%)","13%"],"isMC":true,"timestamp":"2023-01-14 17:49:00","discussion":[{"content":"Selected Answer: B\nB is correct.\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html\nSTARTTLS supports ports 25, 587, and 2587\nTLSWRAPPER supports ports 465 and 2465","poster":"scuzzy2010","upvote_count":"18","comments":[{"upvote_count":"5","comment_id":"828237","content":"FYI Amazon SES supports STARTTLS encryption over port 587, which is the recommended port for email transmission. But existing port 25 can be configured too as in this case as the migration came from SMTP port 25","poster":"God_Is_Love","timestamp":"1677867420.0"}],"timestamp":"1677202680.0","comment_id":"819996"},{"poster":"Untamables","upvote_count":"9","content":"Selected Answer: B\nIn this scenario, you should use Amazon SES SMTP interface to send emails because the application can use SMTP only.\nhttps://docs.aws.amazon.com/ses/latest/dg/send-email-smtp.html\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-credentials.html\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html","comment_id":"790752","timestamp":"1674920700.0"},{"content":"B. Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.","timestamp":"1725095820.0","comment_id":"1275534","upvote_count":"1","poster":"amministrazione"},{"upvote_count":"2","comment_id":"1149427","timestamp":"1707846420.0","poster":"8608f25","content":"Selected Answer: B\nHere's why option B is the correct choice:\nSTARTTLS Support: Amazon SES supports STARTTLS, a protocol command used to upgrade an existing insecure connection to a secure connection using TLS (Transport Layer Security). This is crucial since the legacy SMTP server does not support TLS, and STARTTLS can be used to initiate a secure connection.\nSMTP Credentials: Amazon SES requires authentication to send emails through its SMTP interface. This is achieved by using SMTP credentials, which are different from AWS access keys. SMTP credentials can be obtained from the Amazon SES console and are used to authenticate with the Amazon SES SMTP endpoint.\nOperational Simplicity: This approach allows the application to continue using SMTP for sending emails, which aligns with the application's existing capabilities. By using STARTTLS, the application can upgrade its connection to Amazon SES to a secure one, ensuring compliance with security best practices without significant changes to the application's email sending functionality."},{"upvote_count":"2","timestamp":"1706847000.0","comment_id":"1138091","content":"Selected Answer: A\nTerrible Q. All answers are wrong.\nA is wrong because you cannot send emails through SES SMTP using SMTP credentials derived from temporary STS tokens (ie IAM roles). Must use an IAM user access keys to derive creds.\nB is wrong because the question imposes a constraint that prevents us from selecting an answer that requires upgrading or modifying the application itself. Could you just offload SMTP STARTTLS/AUTH to the local sendmail/postfix daemon? Maybe, if it were Linux, but what if it's Windows? Cygwin? WSL?\nC & D - wrong, for a similar rationale as B. \n\nBut the question designer OBVIOUSLY doesn't know that IAM roles can't be used for SES SMTP auth, because these questions are written by inexperienced, unqualified people who are not themselves architects or engineers.","poster":"LazyAutonomy","comments":[{"comment_id":"1138092","content":"To be fair, the question says this:\n\n\"The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.\"\n\nThe question doesn't say the application cannot handle STARTTLS or SMTP AUTH. In theory, if an application claims to support SMTP, then it should support all features of SMTP, which includes STARTTLS and AUTH. It only says the legacy SMTP server cannot handle TLS. So I suppose perhaps B is correct after all :-)","upvote_count":"3","poster":"LazyAutonomy","timestamp":"1706847180.0"}]},{"comment_id":"1123175","content":"Selected Answer: B\nA = this sends email via SES API while application can use SMTP only\nB = correct\nC = this sends email via SES API while application can use SMTP only\nD = this sends email via SES SDK (API) while application can use SMTP only","poster":"ninomfr64","timestamp":"1705306560.0","comments":[{"timestamp":"1705329000.0","upvote_count":"2","poster":"ninomfr64","content":"Need to correct my comment on A. This is a TLS Wrapper (A) vs STARTTLS (B), where STARTTLS allows initiating an encrypted connection by first establishing an unencrypted connection. While TLS Wrapper is a means of initiating an encrypted connection without first establishing an unencrypted connection (it's the client's responsibility to connect to the endpoint using TLS, and to continue using TLS for the entire conversation). As our app con only work with SMTP we should go for B","comment_id":"1123439"}],"upvote_count":"2"},{"comment_id":"1080607","upvote_count":"1","content":"Selected Answer: B\nThe correct answer is B.\nA: We are unable to obtain authentication information.\nC,D: Does not meet SMTP requirements.\n\nB: This is the correct procedure.\n\nhttps://repost.aws/knowledge-center/ses-set-up-connect-smtp\nhttps://docs.aws.amazon.com/ses/latest/dg/security-protocols.html","timestamp":"1700999820.0","poster":"edder"},{"comment_id":"1022943","content":"Selected Answer: B\nHere's why option B is the correct choice:\n\nSMTP Protocol: The legacy SMTP server uses the SMTP protocol, and Amazon SES provides an SMTP interface for sending email, which is suitable for your application.\n\nSTARTTLS: Using STARTTLS ensures that your communication with Amazon SES is encrypted, which is a best practice for secure email transmission.\n\nSMTP Credentials: Amazon SES SMTP credentials are required to authenticate your application with Amazon SES when sending emails. These credentials include an SMTP username and password.","upvote_count":"2","poster":"totten","timestamp":"1696236540.0","comments":[{"comment_id":"1022944","timestamp":"1696236600.0","poster":"totten","upvote_count":"4","content":"Option A mentions TLS Wrapper, which isn't a standard approach when using Amazon SES for sending email. Amazon SES supports STARTTLS for secure communication.\n\nOption C suggests using the SES API, which is a valid approach but requires code modifications to use the SES API instead of SMTP. Since your application can only use SMTP, this option might involve significant code changes.\n\nOption D mentions using AWS SDKs and IAM users, which is more suitable for programmatic access to SES but not for legacy SMTP applications that can only send via SMTP.\n\nTherefore, Option B is the most appropriate choice for configuring your application to send email messages from Amazon SES while preserving the SMTP protocol and ensuring secure communication."}]},{"comment_id":"976311","poster":"CuteRunRun","upvote_count":"1","timestamp":"1691561880.0","content":"Selected Answer: A\nI selecte A"},{"content":"Selected Answer: B\nIt's B - to preserve SMTP protocol","poster":"NikkyDicky","timestamp":"1688328000.0","comment_id":"941201","upvote_count":"1"},{"poster":"SkyZeroZx","comment_id":"926791","timestamp":"1687104960.0","upvote_count":"1","content":"Selected Answer: B\nB because is \"legacy\" app then use properties to set SMTP \nkeyword === Obtain Amazon SES SMTP credentials"},{"content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/big-data/query-and-visualize-aws-cost-and-usage-data-using-amazon-athena-and-amazon-quicksight/","upvote_count":"1","poster":"F_Eldin","comment_id":"894588","timestamp":"1683781920.0"},{"timestamp":"1683517800.0","content":"Selected Answer: B\nOption A states that the company would require to do more changes in the application than a replatform migration strategy where we are supposed to migrate the application with minimal changes. In Option A using the TLS wrapper would require an additional layer of software (stunnel) to be installed and configured on the EC2 instance, which may introduce additional complexity and management overhead.\nIn option B, we need to configure the application to connect to SES using STARTLS using SMTP credentials, since the legacy SMTP server does not support TLS encryption. This would require minimal change to the application.","upvote_count":"3","comment_id":"891784","poster":"rbm2023"},{"upvote_count":"2","poster":"Cassa","comment_id":"870939","content":"Selected Answer: B\nTo set up a STARTTLS connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 25, 587, or 2587, issues an EHLO command, and waits for the server to announce that it supports the STARTTLS SMTP extension. The client then issues the STARTTLS command, initiating TLS negotiation. When negotiation is complete, the client issues an EHLO command over the new encrypted connection, and the SMTP session proceeds normally\n\nTo set up a TLS Wrapper connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 465 or 2465. The server presents its certificate, the client issues an EHLO command, and the SMTP session proceeds normally.","timestamp":"1681564020.0"},{"content":"Selected Answer: B\nB. Configure the application to connect to Amazon SES by using STARTTLS.","poster":"mfsec","comment_id":"850981","timestamp":"1679833080.0","upvote_count":"1"},{"comment_id":"847360","timestamp":"1679508240.0","poster":"Dimidrol","upvote_count":"3","content":"Selected Answer: B\nB , https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html"},{"comment_id":"844306","comments":[{"content":"https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html \nsays:\n\"Amazon Elastic Compute Cloud (Amazon EC2) throttles email traffic over port 25 by default. To avoid timeouts when sending email through the SMTP endpoint from EC2,submit a Request to Remove Email Sending Limitations\"\n\nAnd the question explicitly says:\n\"The company has lifted the SES limits.\"","timestamp":"1686007140.0","comment_id":"915769","upvote_count":"2","poster":"F_Eldin"}],"upvote_count":"2","content":"Selected Answer: A\nB is wrong because STARTTLS uses port 25 and EC2 instances can’t send outbound traffic through port 25 (you must ask AWS to allow port 25)","timestamp":"1679265540.0","poster":"dev112233xx"},{"timestamp":"1678541700.0","content":"Selected Answer: B\nThe key to this question imo is the sentence \"The application can use SMTP only\". \nSo we cannot go for encryption. \nImo there is no TLS wrapper for Mail that supports authentication which is needed for SES, one needs a proxiing mailserver for that(need support for auth and encryption, rewriting mail).\nWith Starttls SMTP protocol is supported by AWS and the legacy application can send the mail to AWS just as it did to to the legacy mailserver. \n(Of course: a unix machine has not just one application but a lot of little apps like cron, at ... and low traffic mailserver consumes like no resources, so in real world every unix machine should have a small local smtp, eg a postfix configured to forward all traffic from every tool app, system ... to ses but that real world option is not provided as possible answer: so B.)","poster":"hobokabobo","comments":[{"poster":"hobokabobo","upvote_count":"1","comment_id":"836119","timestamp":"1678543740.0","content":"you may look at https://www.stunnel.org/, if find a way to make auth work with ses: well then go for A. Afaik it is not possible - but happy to learn if there is a way."},{"timestamp":"1678544940.0","content":"also have a look at\nhttps://hector.dev/2015/01/17/sending-e-mail-via-amazon-ses-over-smtp-with-iam-roles/\nUsing iam roles does not realy work with smpt auth.(I didn't get it to work and it seems no one else either)","upvote_count":"1","poster":"hobokabobo","comment_id":"836138"}],"upvote_count":"2","comment_id":"836040"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html\nFor new apps, C should be correct. But here, Its re-platforming strategy migrating from SMTP to SES\nSTARTTLS vs TLS Wrapper is being tested here. (A or B) But A sounds 25 port communication which the existing app uses. So B should be correct","comment_id":"828234","poster":"God_Is_Love","timestamp":"1677867300.0","upvote_count":"3"},{"comment_id":"793752","content":"Selected Answer: B\nIt's B, becuse D is discarded since \"The application can use SMTP only.\"","timestamp":"1675148820.0","poster":"Musk","upvote_count":"1"},{"upvote_count":"1","timestamp":"1675148700.0","content":"I doubt between B and D. Both seem correct to me.","poster":"Musk","comment_id":"793750"},{"poster":"boomx","content":"Selected Answer: B\nB\nSTARTTLS works over 25, less change. Also SES SMTP interface needs SMTP credentials\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-credentials.html","timestamp":"1674742500.0","upvote_count":"3","comment_id":"788794"},{"content":"Selected Answer: A\nThe correct answer is option A: \"Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.\"\n\nOption B is incorrect as it suggests to use SMTP with STARTTLS to connect to Amazon SES, which is a less secure method than using a secure wrapper such as TLS Wrapper. Option B also suggests using long-term SMTP credentials which could be a security concern.\n\nOption C is incorrect as it suggests to use the SES API to send email messages, which is not necessary as the application can only use SMTP.\n\nOption D is incorrect as it suggests to use AWS SDKs to send email messages, which is not necessary as the application can only use SMTP. Also, it suggests to use IAM user for Amazon SES which is also a security concern as it will involve long-term credentials as well.","comment_id":"778285","comments":[{"poster":"hobokabobo","content":"A) what tls wrapper are you talking about? \nB) \"Starttls is less secure\": SES AWS Mailservers support Starttls anf you have no way of reconfigure the AWS severs. \n(With Starttls the *server* accepts unencrypted and encrypted incomming smtp mail. The client just connects with smpt encrypted or not, the server will accept both. ...)","upvote_count":"2","comments":[{"content":"lol the answers are copied from chatgpt.","upvote_count":"3","poster":"BabaP","timestamp":"1685698140.0","comment_id":"912685"}],"comment_id":"836026","timestamp":"1678540860.0"}],"poster":"masetromain","timestamp":"1673903880.0","upvote_count":"2"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html","upvote_count":"1","poster":"Atila50","timestamp":"1673832900.0","comment_id":"777245"},{"upvote_count":"3","comment_id":"777108","content":"A is correct. \nhttps://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html\nTLS support TLS Wrapper or STARTTLS.\nB: use STARTTLE but use credential, should use role which is in A","poster":"zhangyu20000","timestamp":"1673820720.0","comments":[{"poster":"zejou1","timestamp":"1679102460.0","upvote_count":"1","content":"no - you are assuming the type of SMTP server, it has to be programmatically configured because the \"application can use SMTP only\".\n\nSES only support TLS and they already created validated the SES domain. So, if you follow this: https://docs.aws.amazon.com/ses/latest/dg/send-using-smtp-programmatically.html\n\nyou have to configure the app to use starttls to upgrade it, code in your SMTP creds to authenticate, and bang- send email","comment_id":"842404"}]},{"poster":"masetromain","comment_id":"775745","content":"Selected Answer: B\nThe correct answer is B. Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.\n\nThis solution allows the application to use the Simple Mail Transfer Protocol (SMTP) protocol to send email messages, which the application requires. Using STARTTLS enables the use of Transport Layer Security (TLS) encryption to secure the connection between the application and Amazon SES. Obtaining the Amazon SES SMTP credentials and using them to authenticate with Amazon SES allows the application to use Amazon SES to send email messages.","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"775747","content":"Option A is incorrect because it uses an IAM role, rather than SMTP credentials, to authenticate with Amazon SES.\nOption C is incorrect because it uses the SES API to send email messages, which the application may not support.\nOption D is incorrect because it uses an IAM user, rather than SMTP credentials, to authenticate with Amazon SES.","poster":"masetromain","timestamp":"1673714940.0"}],"timestamp":"1673714940.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/95299-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","topic":"1","answer":"B","question_id":506,"choices":{"A":"Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.","D":"Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use the access keys to authenticate with Amazon SES.","C":"Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES.","B":"Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES."},"question_text":"A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the migrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends outbound email messages to the company’s customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.\n\nThe company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has created and validated the SES domain. The company has lifted the SES limits.\n\nWhat should the company do to modify the application to send email messages from Amazon SES?"},{"id":"0A0HruLQJbQilRG3dgL2","timestamp":"2023-01-14 18:33:00","answer":"A","discussion":[{"upvote_count":"14","timestamp":"1673717580.0","comment_id":"775782","content":"Selected Answer: A\nThe correct solution is A.\n\nCreating an AWS Cost and Usage Report for the organization and defining tags and cost categories in the report will allow for detailed cost reporting for the different companies that have been consolidated into one organization. By creating a table in Amazon Athena and an Amazon QuickSight dataset based on the Athena table, the finance team will be able to easily query and generate reports on the costs for all the companies. The dataset can then be shared with the finance team for them to use for their reporting needs.\n\nOption B is not correct because it does not provide a way to query and generate reports on the costs for all the companies. \n\nOption C is not correct because it only provides spending information from the AWS Price List Query API and does not provide detailed cost reporting for the different companies. \n\nOption D is not correct because it only uses the AWS Price List Query API and does not provide a way to query and generate reports on the costs for all the companies.","poster":"masetromain"},{"poster":"moota","upvote_count":"7","content":"Selected Answer: A\nI can customize reporting in Cost Explorer but cannot find how to do templates.","comment_id":"804987","timestamp":"1676090220.0"},{"upvote_count":"1","poster":"amministrazione","timestamp":"1725095820.0","content":"A. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team.","comment_id":"1275535"},{"comment_id":"1123445","upvote_count":"3","timestamp":"1705329540.0","poster":"ninomfr64","content":"Selected Answer: A\nA = correct\nB = there isn't specialized templates in AWS Cost Explorer. It provides default reports, but also enables you to change the filters and constraints used to create the reports. You can save the reports that you made as a bookmark, download the CSV file, or save them as a report\nC & D = AWS Price List provides a catalog of the products and prices for AWS services that you can purchase on AWS, not cost of your resources"},{"comment_id":"976346","poster":"CuteRunRun","content":"Selected Answer: A\nI prefer A","timestamp":"1691565780.0","upvote_count":"1"},{"poster":"NikkyDicky","timestamp":"1688328120.0","upvote_count":"1","comment_id":"941203","content":"Selected Answer: A\nits n A"},{"comment_id":"929016","upvote_count":"2","timestamp":"1687318560.0","content":"Selected Answer: A\nI vote A mostly because there is no template option in Cost Explorer and A is the only other option which covers the scenario","poster":"Maria2023"},{"upvote_count":"3","poster":"F_Eldin","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/big-data/query-and-visualize-aws-cost-and-usage-data-using-amazon-athena-and-amazon-quicksight/","timestamp":"1683782040.0","comment_id":"894589"},{"timestamp":"1679833140.0","content":"Selected Answer: A\nA. Create an AWS Cost and Usage Report for the organization.","upvote_count":"1","comment_id":"850982","poster":"mfsec"},{"timestamp":"1673821320.0","upvote_count":"2","poster":"zhangyu20000","comment_id":"777115","content":"A is correct\nB: no such template for cost exporer\nCD: Price List Query API is for list price, not for usage"}],"isMC":true,"exam_id":33,"answer_ET":"A","unix_timestamp":1673717580,"question_images":[],"question_id":507,"url":"https://www.examtopics.com/discussions/amazon/view/95305-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_description":"","question_text":"A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method. The acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found it difficult to generate a cost report that contains meaningful groups for all the teams.\n\nThe acquiring company’s finance team needs a solution to report on costs for all the companies through a self-managed application.\n\nWhich solution will meet these requirements?","answers_community":["A (100%)"],"topic":"1","answer_images":[],"choices":{"A":"Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team.","D":"Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.","B":"Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.","C":"Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the finance team."}},{"id":"qh5w0X5xvDY1Xa6M61gn","answer":"B","choices":{"D":"Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.","A":"Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.","B":"Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application’s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.","C":"Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3."},"isMC":true,"timestamp":"2022-12-10 18:52:00","url":"https://www.examtopics.com/discussions/amazon/view/90943-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","answer_description":"","answer_ET":"B","question_text":"A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application’s data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.\nThe company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company does not have a large enough budget for an active-active strategy.\nWhat should a solutions architect recommend to meet these requirements?","question_images":[],"answer_images":[],"answers_community":["B (100%)"],"unix_timestamp":1670694720,"question_id":508,"exam_id":33,"discussion":[{"upvote_count":"19","poster":"masetromain","content":"Selected Answer: B\nI go with B\nhttps://docs.amazonaws.cn/en_us/Route53/latest/DeveloperGuide/welcome-health-checks.html","comment_id":"741154","timestamp":"1670694720.0","comments":[{"comment_id":"774692","timestamp":"1673629080.0","poster":"masetromain","upvote_count":"17","content":"B is correct, because it meets the company's requirements for reducing RTO to less than 15 minutes and not having a large budget for an active-active strategy.\n\nIn this solution, the company creates an AWS Lambda function in the backup region which promotes the read replica and modifies the Auto Scaling group values. Route 53 is configured with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. The Route 53 record is also updated with a failover policy that routes traffic to the ALB in the backup region when a health check failure occurs. This way, when the primary region goes down, the failover policy triggers and traffic is directed to the backup region, ensuring a quick recovery time."}]},{"poster":"TariqKipkemei","comment_id":"1304866","timestamp":"1730266080.0","content":"Selected Answer: B\nOption B is the least costly active-passive strategy","upvote_count":"1"},{"poster":"Untamables","upvote_count":"4","comment_id":"758588","content":"Selected Answer: B\nI Vote B.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\n\nOption A, C and D are wrong. The latency-based routing and endopoint weights should be used for active/active strategy.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoints-endpoint-weights.html","timestamp":"1727061600.0"},{"poster":"higashikumi","comment_id":"827941","content":"The best option to meet the requirements and reduce RTO to less than 15 minutes is to choose option B.\n\nOption B involves creating an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values. Additionally, Route 53 can be configured with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. The application's Route 53 record can be updated with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.\n\nThis option is cost-effective as it does not require an active-active strategy, and it uses AWS services to minimize the RTO. The Lambda function can be invoked to promote the read replica in the backup region, and the Auto Scaling group values can be updated to launch EC2 instances in the backup region. Furthermore, the Route 53 health check feature can be used to monitor the web application and initiate the failover process.","timestamp":"1727061600.0","upvote_count":"1"},{"comment_id":"1100175","timestamp":"1727061600.0","upvote_count":"3","content":"Selected Answer: B\nOption A - This option will not work as needed: The client will get errors when the closest region is the application's backup region\n\nOption B - This option implements an active-passive strategy as needed: When the health check fails, Route 53 will resolve to the backup region and the Lambda function will ensure the backup region has resources to function\n\nOption C - This option implements an active-active strategy\n\nOption D - This option will not work as needed: The client will get errors 50% of the time","poster":"atirado"},{"timestamp":"1726104360.0","comment_id":"1282394","poster":"GreyBox1","upvote_count":"1","content":"Selected Answer: B\nB is right."},{"poster":"amministrazione","content":"B. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application’s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.","upvote_count":"1","comment_id":"1275435","timestamp":"1725090420.0"},{"timestamp":"1718328180.0","upvote_count":"1","poster":"Bereket","comment_id":"1230173","content":"Selected Answer: B\nCorrect answer B"},{"poster":"gofavad926","upvote_count":"1","comment_id":"1175072","content":"Selected Answer: B\nB for sure","timestamp":"1710604560.0"},{"timestamp":"1706128260.0","comment_id":"1131107","upvote_count":"1","content":"This explains Lambda promoting backup read replica in other region - https://medium.com/ankercloud-engineering/aws-lambda-promoting-rds-read-replica-on-cross-region-using-aws-lambda-113db758869","poster":"Vaibs099"},{"comments":[{"upvote_count":"1","poster":"rhinozD","comment_id":"1153010","timestamp":"1708225440.0","content":"What about RDS failover?\nYou need lambda to promote read replica."}],"poster":"ftaws","content":"why we need Lambda Function ? Is it enough a Route 53 failover policy ?","timestamp":"1705144200.0","comment_id":"1121536","upvote_count":"1"},{"upvote_count":"1","comment_id":"1091804","poster":"ninomfr64","content":"Selected Answer: B\nThe problem is not detecting the right answer, but reading quickly enough trough all the words in the question!","timestamp":"1702125840.0"},{"comment_id":"1078253","upvote_count":"1","content":"Selected Answer: B\nB satisfies all the requirements.","timestamp":"1700729340.0","poster":"jainparag1"},{"content":"Selected Answer: B\nHealth check is a metric, hence alarms can be executed, and alarms are integrated with SNS, SNS integrated with lambda. This sounds weird, but it will work.","upvote_count":"1","comment_id":"1067776","timestamp":"1699700160.0","poster":"severlight"},{"poster":"ansgohar","content":"Selected Answer: B\nB. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application’s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.","upvote_count":"2","timestamp":"1695893580.0","comment_id":"1019668"},{"upvote_count":"1","timestamp":"1693109940.0","poster":"dimitry_khan_arc","comment_id":"991203","content":"Selected Answer: B\nHealth check+SNS. This does not need to have active-active which satisfy the rquirement."},{"poster":"NikkyDicky","comment_id":"934822","upvote_count":"1","content":"it's a B again","timestamp":"1687817460.0"},{"timestamp":"1687617300.0","upvote_count":"1","poster":"Parimal1983","comment_id":"932657","content":"Selected Answer: B\nAs company can not afford with active active configuration and with lambda data layer can be promoted to primary"},{"content":"Selected Answer: B\nSNS + Health check","poster":"SkyZeroZx","comment_id":"919769","timestamp":"1686367560.0","upvote_count":"2"},{"content":"Selected Answer: B\nSNS + Health check","timestamp":"1679978640.0","comment_id":"852778","upvote_count":"1","poster":"mfsec"},{"comment_id":"831719","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html","poster":"kiran15789","upvote_count":"1","timestamp":"1678182600.0"},{"poster":"Sarutobi","content":"Selected Answer: B\nIt would be interesting to see if this actually works. SNS is a regional service, in the last outage of the Virginia Region, we lost SNS completely.","upvote_count":"2","timestamp":"1676304480.0","comments":[{"comment_id":"858576","comments":[{"content":"That is a good point, but how do you need to do some health-API integration? How does SNS in one region know about failure in another? What if your application was not a complete regional outage, or only a service in that region failed? I know this is no longer the initial question :) .","poster":"Sarutobi","comments":[{"poster":"frfavoreto","content":"First of all, SNS in one region doesn't need to know anything about the other region. In the backup region, SNS receives a message from Route53 that triggers a Lambda Function, this is simple as that. \n\nSecondly, you need to implement proper health checks in your frontend web server in order to return a 5xx or 4xx error codes to the probes coming from Route53. If anything is wrong (database, high latency or even the web server itself), Route53 notices the error code/timeout and immediately triggers the failover solution with SNS messaging. Route53 doesn't need to care about what exactly went wrong, just by receiving any unexpected results from the health checks it triggers the failover region.","upvote_count":"1","comment_id":"1012558","timestamp":"1695234360.0"}],"upvote_count":"1","comment_id":"870999","timestamp":"1681569720.0"}],"timestamp":"1680417120.0","poster":"frfavoreto","content":"The SNS topic is in the backup region, not the primary. If you have an issue with the backup region at the same time there is not much you can do as your entire architecture is affected.","upvote_count":"2"}],"comment_id":"807566"},{"poster":"aws0909","comment_id":"796179","content":"I will go with option B as it reduces the RTO","upvote_count":"1","timestamp":"1675352400.0"},{"content":"Selected Answer: B\nA: no health check\nC: active active\nD: Equal weight?","upvote_count":"3","poster":"Yihong","comment_id":"794958","timestamp":"1675242660.0"},{"upvote_count":"3","timestamp":"1671774960.0","poster":"ptpho","content":"I go with B\n5xx is incorrectly method to cover the case of the main site completely down\nIts not act-act loading so R53 should not load traffic between 2 ALBs","comment_id":"753893"}]},{"id":"2aFCLmcMyCgaqPLj7GQX","isMC":true,"discussion":[{"comment_id":"775793","upvote_count":"20","poster":"masetromain","content":"Selected Answer: CE\nC and E are the correct answers.\n\nOption C: Leveraging Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data would help to resolve the issues with the API servers being consistently overloaded. By using Kinesis, the data can be ingested and processed in real-time, allowing the API servers to handle the increased load. Using Lambda to process the data can also help to improve the overall performance and scalability of the platform.\n\nOption E: Re-architecting the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance would help to resolve the issues with high write latency. DynamoDB is a NoSQL database that is designed for high performance and scalability, making it a good fit for this use case. Additionally, DynamoDB supports auto-scaling, which can help to ensure that the database can handle the expected growth in the number of sensors.","comments":[{"poster":"SuperP43","upvote_count":"9","comment_id":"825234","timestamp":"1677615180.0","content":"I disagree with option E. Re-architecting the database tier from RDS to DynamoDB is not possible. RDS is a SQL database, and DynamoDB is a NoSQL database. \n\nThe correct one should be C and B","comments":[{"comment_id":"1170880","content":"That is why it says to \"Re-architect the DB tier\".","poster":"ajeeshb","timestamp":"1710144480.0","upvote_count":"5"},{"timestamp":"1686134400.0","comments":[{"timestamp":"1686134460.0","comment_id":"917118","poster":"tromyunpak","upvote_count":"1","content":"also rds proxy is not used (sorry typo) to handle write operations properly"}],"comment_id":"917116","upvote_count":"2","poster":"tromyunpak","content":"if it was read operations yes but the issue is write latency. also rds proxy is used to handle the write operations"},{"upvote_count":"2","poster":"kamaro","content":"I agree with you.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html\nAurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.\n\nAurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take advantage of that fast distributed storage. The underlying storage grows automatically as needed. An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB).","comments":[{"content":"Naw, you can migrate: https://aws.amazon.com/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/\n\nPlus, with DynamoDB it scales, don't need to add read replica complexity and it also supports IoT out of the box - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.WhyDynamoDB.html\nThis is for IoT sensors that send data and I don't need to store forever so, DynamoDB for this use case is better and cheaper allowing scale","comments":[{"upvote_count":"1","timestamp":"1683835620.0","comment_id":"895390","poster":"Sarutobi","content":"I think this is the big point in this question and that DynamoDB is being position by AWS for IoT very hard. Although is technically possible to migrate with DMS from SQL to DynamoDB, is hard, but harder yet is the change of model inside the application or service."}],"comment_id":"842417","timestamp":"1679104080.0","poster":"zejou1","upvote_count":"2"}],"comment_id":"834714","timestamp":"1678432440.0"},{"timestamp":"1692164100.0","poster":"Gmail78","content":"not the best but not impossible https://aws.amazon.com/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/","upvote_count":"2","comment_id":"982214"}]},{"upvote_count":"1","content":"While options C and E may also provide some benefits, they may not address the underlying issues with the overloaded API servers and high write latency in the database. Therefore, options B and D are the best combination for resolving the issues and enabling growth as new sensors are provisioned.","poster":"OCHT","timestamp":"1680700920.0","comment_id":"862133"},{"timestamp":"1673718900.0","upvote_count":"3","content":"Option A, Resizing the MySQL General Purpose SSD storage to 6 TB to improve the volume’s IOPS will not solve the problem, as the problem is not just related to storage size but also high write latency.\n\nOption B, Re-architecting the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and adding read replicas would help to improve the read performance, but it won't help in reducing write latency.\n\nOption D, Using AWS X-Ray to analyze and debug application issues and adding more API servers to match the load, would help in identifying the problem and resolving it, but it will not help in reducing the load on the servers.","poster":"masetromain","comment_id":"775794"}],"timestamp":"1673718900.0"},{"comment_id":"1421855","timestamp":"1743582840.0","upvote_count":"1","poster":"Paul123456789","content":"Selected Answer: CE\nA. will not fix the problem\nB. read replicas will not fix the high write latency\nD. is for debugging, not a solution\nThis make it C and E"},{"upvote_count":"1","content":"Selected Answer: BC\nWrite performance will be improved by switch RDS to Aurora. RDS to Aurora is smooth transition without too much on the application side.\nAnswer E will application side not just backend DB.","poster":"hhiguita","timestamp":"1742761860.0","comment_id":"1405767"},{"timestamp":"1741865520.0","comment_id":"1388270","poster":"29fb203","upvote_count":"1","content":"Selected Answer: BC\nB. Re-architect the database tier to use Amazon Aurora and add read replicas\n Aurora automatically scales storage up to 128 TB without manual resizing.\n Faster writes and lower read latency than standard RDS MySQL.\n\nC. Use Amazon Kinesis Data Streams and AWS Lambda for ingestion and processing\nDecouples IoT data ingestion from database writes\n Kinesis Data Streams ingests large volumes of sensor data without overloading API servers.\n Scales automatically with the number of sensors.\nNot E becvause DynamoDB is NOSql and doesn't support MySQL."},{"poster":"bhanus","timestamp":"1735509600.0","upvote_count":"1","comment_id":"1333752","content":"Selected Answer: BC\nB. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.\n\nAmazon Aurora is a managed database service compatible with MySQL, designed for high performance and scalability.\nAurora provides better write performance and supports read replicas to handle increased read traffic as the platform grows. This will address the high write latency issue and enable horizontal scaling.\nC. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\n\nUsing Amazon Kinesis Data Streams for data ingestion offloads traffic from the API servers, reducing their load and improving scalability.\nAWS Lambda can process the raw data in real time and pass it to the database or other systems, providing a cost-effective and scalable solution for data processing."},{"timestamp":"1733853180.0","comment_id":"1324697","upvote_count":"1","poster":"Heman31in","content":"Selected Answer: CE\nBy combining C (Kinesis + Lambda) with E (DynamoDB), you're preparing the platform to handle exponential growth in sensor data while ensuring high availability, scalability, and low latency for both data processing and storage. This solution directly addresses the need for a robust, future-proof architecture capable of supporting massive data volumes without bottlenecks, making it well-suited for the IoT platform's growth."},{"comment_id":"1323622","poster":"wem","content":"Selected Answer: BC\nE would require a shift from relational to a no-sql table - what if there are multiple tables?","timestamp":"1733673180.0","upvote_count":"1"},{"content":"Selected Answer: CE\nC is straightforward.\n\nI go for E rather than B, because db shows heavy write latency, not limit. Replacing with Aurora will speed up thing up until a limit. \nGoal is to deal with it once and for all","comment_id":"1317703","poster":"konieczny69","upvote_count":"2","timestamp":"1732561140.0"},{"comment_id":"1314308","upvote_count":"2","content":"Selected Answer: BC\nBy combining options B and C, the company can address the current performance and scalability issues while enabling future growth as more sensors are deployed. Amazon Aurora provides a scalable and high-performance relational database, while Kinesis Data Streams and Lambda offer a serverless and cost-effective solution for ingesting and processing the raw data streams.\n\nOption A may provide temporary relief by increasing IOPS, but it doesn't address the scalability and performance limitations of RDS MySQL. \nOption D can help identify application issues but doesn't solve the underlying database problems. \nOption E is not ideal as DynamoDB is a NoSQL database, and the existing application is likely designed for a relational database like MySQL or Aurora, requiring significant changes to the application code and data modeling.","poster":"0b43291","timestamp":"1731976500.0"},{"upvote_count":"1","comment_id":"1275536","timestamp":"1725095880.0","content":"C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\nE. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.","poster":"amministrazione"},{"content":"Selected Answer: CE\nWhat discards B is \"Add read replicas\", the problem is writing the new data in the DB, adding Read replicas will increase the cost and this is not what question requests \"maintain cost\"","upvote_count":"2","timestamp":"1721412720.0","comment_id":"1251376","poster":"zolthar_z"},{"content":"Selected Answer: BC\nWrite performance will be improved by switch RDS to Aurora. RDS to Aurora is smooth transition without too much on the application side. \nAnswer E will application side not just backend DB.","upvote_count":"2","timestamp":"1718672880.0","poster":"Helpnosense","comment_id":"1232178"},{"upvote_count":"3","content":"Selected Answer: CE\nOption CE and BC. The only reason I choose E over B because said SO. Per AWS, DynamoDB is suitable for IoT ( Sensor data and log ingestion) \n\nhttps://docs.aws.amazon.com/whitepapers/latest/best-practices-for-migrating-from-rdbms-to-dynamodb/suitable-workloads.html","timestamp":"1712149320.0","comment_id":"1188673","poster":"TonytheTiger"},{"timestamp":"1710680400.0","upvote_count":"1","content":"Selected Answer: CE\nCE, kinesis + lambda & Dynamodb","poster":"gofavad926","comment_id":"1175835"},{"upvote_count":"2","timestamp":"1709480220.0","poster":"a54b16f","content":"Selected Answer: BC\nSwitching from RDS mysql to aurora will improve performance, by up to 10 times, which could solve the write issue. Switching from relationship database to nosql is not practical, need re-engineering whole application. plus, the performance improvement of nosql are around data read, not data write ( creating/updating indexes is a huge effort)","comment_id":"1164889"},{"upvote_count":"2","timestamp":"1707851940.0","comment_id":"1149507","content":"Selected Answer: BC\n* B. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.\n\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. Aurora provides several benefits over standard RDS MySQL, including better performance, scalability, and availability. It automatically grows storage as needed, up to 128 TB, potentially providing better write performance. Aurora also supports up to 15 read replicas with very low replication latency, improving read performance significantly and reducing the load on the primary database instance.","comments":[{"comment_id":"1149508","timestamp":"1707851940.0","content":"* C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\nBy using Amazon Kinesis Data Streams, the company can collect, process, and analyze real-time, streaming data so that it can react to new information quickly. This service allows for the ingestion of a large amount of data generated by IoT sensors. AWS Lambda can then be used to process this data in real-time, which can help to offload the work from the API servers, reducing their load. This setup can scale automatically with the number of incoming data records, providing a more efficient and cost-effective solution to handle growth.","poster":"8608f25","upvote_count":"1"}],"poster":"8608f25"},{"timestamp":"1705329720.0","poster":"ninomfr64","comment_id":"1123450","content":"Selected Answer: CE\nA = does not fix permanently (who knows that 6TB is enough?)\nB = going from RDS to Aurora will not fix the issue\nC = correct\nD = this could work, but it is not cost efficient (more EC2 instance along the line)\nE = correct","upvote_count":"2"},{"timestamp":"1699452180.0","content":"Selected Answer: CE\nThe requirement is to resolve high write latency while Aurora is a good fit for structured datasets but option B has indicated read replica as a direction for resolution against a question seeking resolution in write latency. So Option B is definitely out.","poster":"Pupu86","upvote_count":"1","comment_id":"1065676"},{"timestamp":"1692281520.0","poster":"duriselvan","upvote_count":"1","content":"Amazon RDS FeaturesAmazon RDS supports multiple database engines, including Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server, and PostgreSQL.Amazon RDS allows you to scale your database instances’ storage size and performance.Amazon RDS makes it easy to set up, operate, and scale a relational database in the cloud.Amazon RDS provides a cost-effective way to manage relational databases in the cloud.DynamoDB FeaturesPrimarily, DynamoDB features flexibility, scalability, and performance. It offers high availability out of the box with no need for setup or configuration. DynamoDB automatically replicates your data across multiple Availability Zones within a Region to give you fault tolerance and high availability.","comment_id":"983722"},{"content":"CE \nhttps://aws.amazon.com/dynamodb/iot/","upvote_count":"1","timestamp":"1691849280.0","poster":"rizzu2023","comment_id":"979397"},{"comment_id":"962991","content":"b-c-b-c-b-c-b-c","upvote_count":"1","poster":"easytoo","timestamp":"1690308960.0"},{"comment_id":"941205","upvote_count":"1","poster":"NikkyDicky","content":"Selected Answer: CE\nCE for sure. classic IoT use case","timestamp":"1688328420.0"},{"upvote_count":"1","content":"Selected Answer: BD\nRds MySQL to Aurora to scale automatically and stay relational","timestamp":"1687291620.0","poster":"Asds","comment_id":"928767"},{"upvote_count":"2","content":"Selected Answer: BC\nOptions A, D, and E are not the most suitable choices for resolving the issues and enabling growth while keeping the platform cost-efficient in this scenario:\n\nA. Resizing the MySQL General Purpose SSD storage to 6 TB might increase the volume's IOPS, but it won't address the underlying scalability and performance issues caused by the growing number of sensors and high write latency.\n\nD. While using AWS X-Ray for analyzing and debugging application issues can help optimize performance, it alone won't be sufficient to handle the increased workload caused by the growing number of sensors.\n\nE. Re-architecting the database tier to use Amazon DynamoDB instead of RDS MySQL would require significant changes to the application and might not be cost-efficient, considering the already established use of RDS MySQL. DynamoDB is a NoSQL database and requires a different data modeling approach compared to a relational database like MySQL.","timestamp":"1687264440.0","poster":"HussamShokr","comment_id":"928455"},{"poster":"bcx","upvote_count":"1","content":"Selected Answer: CE\nC: Kinesis Data Stream, scalable large volume ingestion. Process with Lambda, also scalable.\nE: Use DynamoDB, Aurora, replicas, etc are not meant for this class of applications. You would have to increase more and more capacity and will be too expensive. At one time it may not be enough.","timestamp":"1687174200.0","comment_id":"927415"},{"comment_id":"917694","content":"Selected Answer: CE\nA is wrong. for gp2, 3 iops per gb and when you increase your ebs from 4tb to 6tb, you increase your iops from 1,2000 to 1,6000( not 1,8000 because the max iops for gp2 is 1,6000, see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html ),\nthe key word in the question is \"resolve the issues permanently\" this method can't resolve the problem permanently.\nB is wrong because the scenario is mainly focus on writing to db, so read replica really can't help too much.\nD is wrong since the bottleneck is at DB \"RDS metrics show high write latency\"","timestamp":"1686184620.0","upvote_count":"2","poster":"Jesuisleon"},{"content":"Selected Answer: CE\nC&E for me...\nIf you choose B&E (Kinesis+Lambda to Ingest Aurora database) you will need also to add to the solution RDS Proxy, since Lambda will keep opening DB connections and this will impact the DB performance and probably the cost","poster":"dev112233xx","upvote_count":"1","comment_id":"905005","timestamp":"1684853100.0"},{"poster":"rbm2023","timestamp":"1683519900.0","content":"Selected Answer: CE\nwe really should consider the COST for adopting Dynamo, currently the expected data is 4TB which would be much more expensive than migrating to Aurora, still, adding READ replicas does not necessarily helps the issue. So this question is completely tricky.\ncombinations of B and C or C and E should be acceptable in my view.\nvoting for C and E anyway only because the read replicas in aurora would not fix the write issues","upvote_count":"2","comment_id":"891798"},{"timestamp":"1683363600.0","upvote_count":"4","poster":"petervu","content":"Selected Answer: BC\nWe can’t simply change database from SQL to noSQL so I think E is not appropriate. BC should be better choice.","comment_id":"890621"},{"upvote_count":"1","comments":[{"comment_id":"864735","content":"My mistake. The correct answers are B and C.\n\nB. Re-architecting the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and adding read replicas can improve performance and scalability. Amazon Aurora is a MySQL-compatible database that can deliver up to five times the performance of MySQL without requiring changes to most of your existing applications. Adding read replicas can help offload read traffic from the primary instance and improve read performance.\n\nC. Leveraging Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data can help reduce the load on the API servers and improve performance. Kinesis Data Streams can capture, store, and process large amounts of data in real-time, while AWS Lambda can automatically scale to process the data as it arrives.\n\nThese two options together will resolve the issues permanently and enable growth as new sensors are provisioned while keeping this platform cost-efficient.","timestamp":"1680962640.0","upvote_count":"3","poster":"OCHT"}],"comment_id":"862136","poster":"OCHT","content":"Selected Answer: BD\nB. Amazon Aurora is a high-performance and cost-effective alternative to using RDS MySQL. It is specifically designed for the cloud and is optimized for high concurrency and low latency. By migrating to Aurora, the company can take advantage of its ability to automatically scale resources to meet demand, while providing fast and consistent performance. Additionally, Aurora supports read replicas, which can help distribute the workload and improve query performance.\n\nD. AWS X-Ray can help identify issues with the application and API servers. By analyzing X-Ray traces, the company can identify which API endpoints are experiencing high latency and prioritize optimization efforts accordingly. Additionally, adding more API servers can help distribute the load and improve performance. This approach allows for more granular scaling of the application tier, as opposed to simply adding more resources to the RDS instance, which may not fully address the underlying issues.","timestamp":"1680700980.0"},{"timestamp":"1679833500.0","content":"Selected Answer: CE\nCE is the best choice. DynamoDB is the better choice for the IoT sensors growth.","poster":"mfsec","upvote_count":"2","comment_id":"850986"},{"timestamp":"1679267040.0","content":"Selected Answer: BD\nBD fit the requirement. (Cost efficient)\nCE are the opposite of the requirement, DynamoDB and Kinesis more expensive than Aurora for large scale apps .. even DynamoDB alone more expensive than Aurora for large scale apps","poster":"dev112233xx","comment_id":"844319","upvote_count":"1","comments":[{"comment_id":"844321","timestamp":"1679267280.0","upvote_count":"1","poster":"dev112233xx","content":"I forgot to mention that DynamoDB is noSQL database, and requires also a big refactor in the NodeJS app. Does not make sense here to choose DynamoDB over Aurora (MySQL).."}]},{"timestamp":"1679003280.0","comment_id":"841376","poster":"Damijo","content":"Selected Answer: BC\nhttps://www.examtopics.com/discussions/amazon/view/5011-exam-aws-certified-solutions-architect-professional-topic-1/ DynamoDB or other NoSQL options are not the solutions when organizations need to store predictable, structured data. In that case, Amazon Aurora is the best solution with high scalability and best performances.","upvote_count":"4"},{"poster":"Dimidrol","content":"Selected Answer: AC\nA C for me.","upvote_count":"4","comment_id":"838356","timestamp":"1678744800.0"},{"timestamp":"1678522200.0","content":"B and D is correct","comment_id":"835732","upvote_count":"2","poster":"limjieson"},{"upvote_count":"2","content":"CE are correct","comment_id":"777119","timestamp":"1673821440.0","poster":"zhangyu20000"}],"timestamp":"2023-01-14 18:55:00","answers_community":["CE (59%)","BC (31%)","6%"],"topic":"1","question_text":"A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company’s Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.\n\nThe number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.\n\nWhich of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)","answer":"CE","question_id":509,"url":"https://www.examtopics.com/discussions/amazon/view/95308-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"CE","answer_description":"","question_images":[],"exam_id":33,"answer_images":[],"choices":{"D":"Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.","C":"Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.","E":"Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.","A":"Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume’s IOPS.","B":"Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas."},"unix_timestamp":1673718900},{"id":"79tRkkmBPl6dRSgWcj7W","answer_images":[],"isMC":true,"unix_timestamp":1673719200,"question_id":510,"choices":{"A":"Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.","C":"Change the API Gateway Regional endpoints to edge-optimized endpoints.","D":"Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.","B":"Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.","E":"Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database."},"answer_description":"","timestamp":"2023-01-14 19:00:00","discussion":[{"timestamp":"1673719200.0","upvote_count":"23","content":"Selected Answer: AC\nA and C are correct answers.\nA. Enable S3 Transfer Acceleration on the S3 bucket and ensure that the web application uses the Transfer Acceleration signed URLs will accelerate the uploads of documents to S3 bucket, this will help to reduce the latency for users outside of Europe.\nC. Change the API Gateway Regional endpoints to edge-optimized endpoints will help the company to improve the latency by caching the responses of the API Gateway closer to the users.","comments":[{"upvote_count":"1","comment_id":"1194461","timestamp":"1712944860.0","content":"A is wrong because why would you enable transfer acceleration when transfer acceleration uses the cloudfront distribution system. it makes no sense","poster":"e4bc18e","comments":[{"content":"S3 Global acceleration is used to upload files, so, the users can upload faster the documents in any part of the world","poster":"zolthar_z","comment_id":"1251381","upvote_count":"1","timestamp":"1721413020.0"}]},{"comment_id":"927420","poster":"bcx","comments":[{"comments":[{"timestamp":"1705498560.0","upvote_count":"2","content":"\"web app uses CloudFront distribution for delivery with Amazon S3 as the origin\" and \"Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket\" these 2 sentences let me think that users are not uploading via CluodFront into the S3 bucket at its origin, rather docs are uploaded from the Lambda","poster":"ninomfr64","comment_id":"1125000"}],"upvote_count":"2","comment_id":"1001832","timestamp":"1694111100.0","poster":"Sab","content":"Users of S3 are not lambda, lambda is used only for writing to serverless database. Also, Aurora serverless global database only writes in one cluster and the other region cluster are used only for reads. So no matter from which location you upload, the metadata will be written to cluster in Central Europe . If it was Global DynnamoDB table then it could have helped to reduce latency."}],"upvote_count":"3","content":"A is wrong because the users of S3 are the lambda functions, not the end user. \"The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.\"","timestamp":"1687174800.0"},{"timestamp":"1673719200.0","upvote_count":"5","poster":"masetromain","comment_id":"775800","comments":[{"upvote_count":"2","timestamp":"1673719200.0","content":"https://www.examtopics.com/discussions/amazon/view/69470-exam-aws-certified-solutions-architect-professional-topic-1/","comment_id":"775801","poster":"masetromain"},{"timestamp":"1700734440.0","upvote_count":"1","content":"Complexity is not evidence against option D.\nFurthermore, option D is correct because the question statement also suggests that costs can be incurred.\nOn the other hand, A is not a method to eliminate geographical factors.","poster":"Japanese1","comment_id":"1078319","comments":[{"content":"D does not mention how to route traffic","poster":"hussainbaloch1002","comment_id":"1336403","timestamp":"1736001420.0","upvote_count":"1"}]}],"content":"B. Creating an accelerator in AWS Global Accelerator and attaching it to the CloudFront distribution will not help in this scenario as it only helps to route the traffic to the optimal endpoint based on the location of the user.\nD. Provisioning the entire stack in two other locations that are spread across the world and using global databases on the Aurora Serverless cluster will help to reduce the latency but it would be more complex to implement and manage.\nE. Adding an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database will not help in this scenario because it is only used to improve connection management and load balancing for Amazon RDS databases, but not for Aurora Serverless databases."}],"poster":"masetromain","comment_id":"775798"},{"poster":"caputmundi666","upvote_count":"1","comment_id":"1420826","content":"Selected Answer: CD\nCD - for me. I don't understand why S3 Transfer Acceleration is better than D sincethe transfer from lambda is already on AWS's backbone.","timestamp":"1743572160.0"},{"comment_id":"1399873","upvote_count":"1","poster":"ParamD","timestamp":"1742251920.0","content":"Selected Answer: AC\nA is correct because it is with signed url option, lambda will facilitate signed url generation and file will be uploaded directly to S3 with transfer acceleration"},{"timestamp":"1721412960.0","upvote_count":"1","poster":"zolthar_z","comment_id":"1251380","content":"Selected Answer: AC\nAC will improve latency using AWS edge locations worldwide, adding 2 locations will only benefit those 2 locations"},{"comment_id":"1243709","poster":"gfhbox0083","upvote_count":"2","content":"A, C for sure.\nB is wrong; AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.","timestamp":"1720331160.0"},{"timestamp":"1712313060.0","content":"Selected Answer: AC\nA and C for me are the correct answers.\nD is not so usefull as we are recreating the entire stack and increase a lot the costs. As first approach, A and C are the most appropriate","poster":"red_panda","upvote_count":"2","comment_id":"1189863"},{"content":"Selected Answer: AC\nAurora serverless does not support global database. search DB instance class requirements in https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-getting-started.html","comment_id":"1182280","timestamp":"1711351980.0","upvote_count":"3","poster":"failexamonly","comments":[{"poster":"bacharbhouri","comment_id":"1218182","timestamp":"1716630720.0","content":"it does in V2.\n\n[] https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html#aurora-serverless-v2.advantages : Using Aurora Serverless v2 - Advantages of Aurora Serverless v2","upvote_count":"1"}]},{"content":"Selected Answer: AC\nBy elimination: B is pointless, as CF already does geo proximity. D is impossible as global DBs aren't supported by Aurora Serverless. E doesn't really help.\n\nRemaining: A and C, which are sensible and will do the trick.","timestamp":"1711041240.0","upvote_count":"2","comment_id":"1179457","poster":"Dgix"},{"timestamp":"1710681240.0","content":"Selected Answer: AC\nAC, s3 transfer acceleration + edge-optimised api gateway","comment_id":"1175841","upvote_count":"2","poster":"gofavad926"},{"comment_id":"1125023","poster":"ninomfr64","timestamp":"1705500060.0","comments":[{"upvote_count":"1","timestamp":"1709326620.0","comment_id":"1163751","content":"Looks like D is wrong because you don't use global databases on the Aurora Serverless cluster. That is just not a feature given by Aurora Serverless (even v2). However, it does support using Aurora Serverless in global databases. \"The secondary clusters\" in the link below is a reference to Aurora Global Database.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.how-it-works.html#aurora-serverless.ha:~:text=You%20can%20use%20Aurora%20Serverless%20v2%20capacity%20in%20the%20secondary%20clusters%20so%20they%27re%20ready%20to%20take%20over%20during%20disaster%20recovery.","poster":"djeong95","comments":[{"timestamp":"1734462480.0","upvote_count":"1","comment_id":"1328107","poster":"grumpysloth","content":"\"Aurora Serverless v2 supports all manner of database workloads. Examples include development and test environments, websites, and applications that have infrequent, intermittent, or unpredictable workloads to the most demanding, business critical applications that require high scale and high availability. It supports the full breadth of Aurora features, including global database, Multi-AZ deployments, and read replicas. Aurora Serverless v2 is available for the Amazon Aurora MySQL-Compatible Edition and PostgreSQL-Compatible Edition.\""},{"upvote_count":"1","poster":"djeong95","comment_id":"1165730","timestamp":"1709564220.0","content":"In addition, we are more likely to get latency from Lambda functions loading documents into S3 from API Gateway calls than we are from Lambda functions loading metadata into Aurora Serverless DB. \n\nhttps://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/"}]}],"upvote_count":"2","content":"Selected Answer: CD\nThis is tricky. Here is my take having in mind that the question is \"The company must improve latency outside of Europe\",.\n\nA = Transfer Acceleration improves upload/downlad time, but we have already CloudFront that can also be used to speedup upload. This will not further improve. Also I don't know how to combine TA with CF\nB = This will not help and also I don't know how to combine GA with CF\nC = correct\nD = correct\nE = RDS Proxy do not improve latency"},{"timestamp":"1703746920.0","comments":[{"upvote_count":"1","poster":"jpa8300","timestamp":"1704299820.0","content":"Yes there is, https://stackoverflow.com/questions/37437782/aws-transfer-acceleration-with-pre-signed-urls-using-javascript-sdk","comment_id":"1112937","comments":[{"poster":"JMAN1","comment_id":"1121524","content":"Sorry. I was wrong. Answer is A C.\nserverless does not support global database and RDS proxy.","comments":[{"comments":[],"timestamp":"1705143120.0","upvote_count":"1","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations","comment_id":"1121525","poster":"JMAN1"}],"upvote_count":"1","timestamp":"1705143060.0"}]}],"upvote_count":"3","comment_id":"1107524","content":"Selected Answer: CD\nTricky Tricky.\n\nA. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs. -> Wrong. No such thing like TA signed URLs.\nB. Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution. -> Wrong GA does not support CF.\nC. Change the API Gateway Regional endpoints to edge-optimized endpoints.\nD. Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.\nE. Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database. -> Wrong. It is not related with latency.","poster":"JMAN1"},{"content":"Selected Answer: AC\nA and C makes sense.\nA is clear as what masetromain has explained. \nC, An edge-optimized API endpoint typically routes requests to the nearest CloudFront Point of Presence (POP). It certaintly improve the latency of traffic originating from Europe as the traffic will now be directed to the nearest POP instead of the origin API Gateway.","timestamp":"1702469340.0","upvote_count":"2","comment_id":"1095436","poster":"[Removed]"},{"content":"Selected Answer: AC\nsee Sab answer","upvote_count":"1","poster":"severlight","comment_id":"1070096","timestamp":"1699946160.0"},{"upvote_count":"1","poster":"wookchan","content":"\"The company must improve latency outside of Europe.\"\nThen in where are you going to provision an additional stack? It only says \"outside of Europe.\"\nUSA? Asia? Where?\nYou have to consider an overall latency. \nI'll go for AC","comment_id":"1027842","timestamp":"1696756800.0"},{"poster":"AMohanty","timestamp":"1694261340.0","comment_id":"1003176","upvote_count":"1","content":"AD\nIssue is minimize latency for \"users uploading documents\"\nIts NOT an issue with the latency of website being delivered to the users.\n\nGlobal Accelerator - Is used to decrease latency in having the user request delivered using AWS backbone network to the point of Origin\nBut it doesnt accelerate delivery of uploaded files into S3 .... so A is a better option.\n\nRDS Proxy is used to decrease the time in establishing the DB connectivity ... It keeps few DB connections on warm-by condition. Option D doesn't help in reducing cross-Region latency\n\nAPI Gateway edge point will reduce the latency in serving the website closer to ur location. But here question is about uploading document.\n\nAurora Serverless Global - can be used for uploading meta-data reducing latency time."},{"timestamp":"1693898820.0","upvote_count":"2","poster":"uC6rW1aB","content":"Selected Answer: AC\nOn a global scale, and particularly for users outside of Europe, the API Gateway and S3 access operations are the most likely components to introduce significant latency.\nFor the API Gateway, changing from regional endpoints to edge-optimized endpoints would bring API calls closer to global users.\nFor S3, enabling Transfer Acceleration would speed up the uploading and downloading of files.\nTherefore, based on the provided system overview, these two components are the most likely areas needing optimization to reduce latency.","comment_id":"999216"},{"upvote_count":"2","comment_id":"991859","content":"Selected Answer: CD\neven though option D is complex, it would decrease the latency outside eu region.","timestamp":"1693204380.0","poster":"Gabehcoud"},{"upvote_count":"1","content":"S3 Transfer Acceleration primarily improves upload speeds to an S3 bucket and doesn't significantly affect the latency of the web application itself.","timestamp":"1692030000.0","poster":"nharaz","comment_id":"980948"},{"timestamp":"1691663040.0","content":"Selected Answer: CD\nI prefer CD. \nA is not right. You already get a cloudfront, what is the acceleration used for.","upvote_count":"1","comment_id":"977515","comments":[{"poster":"longns","upvote_count":"1","content":"in S3 hosted web application you upload directly to s3 using s3 url","comments":[{"timestamp":"1705499280.0","comment_id":"1125008","poster":"ninomfr64","upvote_count":"1","content":"You can (and should) avoid enabling S3 web site hosting working with CloudFront. You can simply configure the S3 as the distribution origin"}],"timestamp":"1696135320.0","comment_id":"1022001"}],"poster":"CuteRunRun"},{"poster":"chico2023","comment_id":"973569","content":"Selected Answer: AC\nAnswer: A and C (over C and D which I also am inclined to).\nFor me, the lack of a really clear direction like \"What solution will provide the best latency improvement in a cost effective way\", for example, opens the debate into two possible ways.\n\nI personally like the idea presented in C and D, but if I want to improve latency for users outside Europe, initially I would try to perform A and C. Simply because I am not sure which regions I am going to use. I know that it says \"Provision the entire stack in two other locations that are spread across the world.\" But where, exactly? One in Sao Paulo and the other in Cape Town? How much will it improve for users in Auckland, if that's the case?\n\nThere this great blog explaining S3 Transfer Acceleration with signed URLs and how they can improve latency. Have a look: https://www.blendedsoftware.com/articles/how-to-accelerate-file-uploads-with-aws-s-3/","upvote_count":"4","timestamp":"1691303040.0"},{"upvote_count":"2","comment_id":"963026","timestamp":"1690311720.0","poster":"easytoo","content":"c-d-c-d-c-d-c-d\nEnabling S3 transfer acceleration and using Global Accelerator may help but are more targeted to optimizing S3 and CloudFront performance specifically. RDS proxies can help but do not address the broader issue of latency outside the eu-central-1 region. Spreading the stack across regions and using Aurora global databases will provide the most comprehensive latency improvements."},{"content":"Selected Answer: CD\nC: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html#apigateway-definition-edge-optimized-api-endpoint\nD: If the users are globally distributed, it would be beneficial to provision the entire stack in other regions...","poster":"aviathor","timestamp":"1690267320.0","upvote_count":"2","comment_id":"962410"},{"upvote_count":"1","timestamp":"1690267260.0","content":"C: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html#apigateway-definition-edge-optimized-api-endpoint\nD: If the users are globally distributed, it would be beneficial to provision the entire stack in other regions...","poster":"aviathor","comment_id":"962409"},{"upvote_count":"1","poster":"khksoma","content":"A nd C\nhttps://www.blendedsoftware.com/articles/how-to-accelerate-file-uploads-with-aws-s-3","comment_id":"957177","timestamp":"1689831840.0"},{"content":"C. Yes, because an edge-optimized API endpoint is best for geographically distributed clients. API requests are routed to the nearest CloudFront Point of Presence (POP). \n\nD. Yes, because this question is NOT about cost-effectiveness, it is about a POC with its largest customer that seemingly went poorly, as they MUST improve latency outside of Europe.\n\nA. No, because this only addresses part of the latency issue, S3 transfer speeds, but does not address the largest customer's geographic distance from the entire stack.\nB. No, because because Endpoints for can be NLB, ALB, EC2 and Elastic IPs, but not CloudFront\nE. No, because adding an Amazon RDS proxy between the Lambda functions would help with connection pooling, and does not match any other option","poster":"dkx","comment_id":"952679","upvote_count":"1","timestamp":"1689453360.0"},{"comments":[{"comment_id":"952165","upvote_count":"5","poster":"lxrdm","content":"Global accelerator does not support CloudFront\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoints.html","timestamp":"1689409260.0"}],"comment_id":"944823","upvote_count":"1","poster":"hexie","timestamp":"1688660160.0","content":"Selected Answer: BC\nWhy people are going for A if Transfer Acceleration is mainly to improve the upload and download speeds for objects in an S3 bucket? I'm sick of these ChatGPT answers tho.\nDude, in the given scenario, the requirement is to IMPROVE LATENCY outside of Europe, specifically for the web application and API Gateway endpoints.\nCreating an accelerator with AWS Global Accelerator and attaching it to CloudFront, traffic will be routed through AWS global network, optimizing latency for users outside of Europe as requested :)"},{"poster":"nicecurls","upvote_count":"1","comment_id":"942703","content":"Selected Answer: CD\nA is wrong!","timestamp":"1688471280.0"},{"timestamp":"1688329080.0","comments":[{"timestamp":"1690267020.0","comment_id":"962405","content":"E is \"totally supported\" but won't do much good with the latency...","poster":"aviathor","upvote_count":"1"}],"upvote_count":"1","poster":"NikkyDicky","comment_id":"941213","content":"Selected Answer: CE\nC -= no brainer\nE is totally supported (https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html)\nA not applicable (upload is not direct to s3), B make no sense. D is a dumb idea"},{"comment_id":"940877","poster":"hexie","content":"Selected Answer: CD\nIm going with C and D\n\nIm sorry, but I cant get what do A option mean by \"Transfer Acceleration signed URLs\". Its a tricky option because it starts with S3 Transfer Acceleration.\n\nD is right because If the users are globally distributed, it would be beneficial to provision the entire stack in other regions","upvote_count":"3","timestamp":"1688301960.0"},{"poster":"Parimal1983","content":"Selected Answer: CD\nQuestion asked to reduce latency. So with option C and D overall latency can be reduced. With option A, says signed URL which is wrong, instead if it would have mentioned Transfer Accelerator Endpoint then might be make more sense.","upvote_count":"2","timestamp":"1687852020.0","comments":[{"poster":"Parimal1983","upvote_count":"1","content":"Moreover, setting up environment in 2 region will also helpful. Lambda is regional, using Aurora global database, improve performance and reduce latency ultimately.","comment_id":"935119","timestamp":"1687852140.0"}],"comment_id":"935116"},{"content":"Selected Answer: CD\nits cd","comment_id":"910422","timestamp":"1685463600.0","upvote_count":"2","poster":"johnballs221"},{"timestamp":"1684854780.0","upvote_count":"4","poster":"dev112233xx","comment_id":"905028","comments":[{"poster":"NikkyDicky","content":"wrong. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html: \"you can use RDS Proxy with Aurora Serverless v2 clusters, but not with Aurora Serverless v1 clusters.\"","timestamp":"1688328840.0","upvote_count":"1","comment_id":"941211"}],"content":"Selected Answer: CD\nA is wrong! can someone explain what they mean by \"Ensure that the web application uses the Transfer Acceleration signed URLs\"??? it's definitely a trap ... most of you will pick this answer just because of this \"S3 Transfer Acceleration\" but the second part is a wrong. That's why you always need to read every word of the answer...\nB: is also wrong, can't use Global Accelerator with CF\nE: is also wrong!. RDS Proxy doesn't support Aurora Serverless"},{"poster":"F_Eldin","comment_id":"894614","timestamp":"1683784560.0","comments":[{"timestamp":"1684244220.0","content":"B is wrong, you can't integrate aws global accelerator with CloudFront, they are two different aws services","poster":"Jesuisleon","comment_id":"899241","upvote_count":"2"}],"content":"Selected Answer: BC\nB: AWS Global Accelerator utilizes the Amazon global network, allowing you to improve the performance of your applications by lowering first byte latency (the round trip time for a packet to go from a client to your endpoint and back again) and jitter (the variation of latency), and increasing throughput (amount of data transferred in a second) as compared to the public internet. https://aws.amazon.com/global-accelerator/faqs/\nC: An edge-optimized API endpoint is best for geographically distributed clients. API requests are routed to the nearest CloudFront Point of Presence (POP). This is the default endpoint type for API Gateway REST APIs. https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint-types.html","upvote_count":"1"},{"timestamp":"1682382240.0","upvote_count":"2","content":"Selected Answer: BD\nCost is not a concern. B and D are the best options here.","poster":"Yowie351","comment_id":"879790"},{"content":"Selected Answer: AC\nTricky question, Amazon's FAQ for S3 recommends using CloudFront instead of S3 Acceleration when the file is less than 1GB. There is no info about the size of the document, but I think they are typically less than 1GB. So maybe D is better than A here.","poster":"Sarutobi","comment_id":"874737","upvote_count":"2","timestamp":"1681912140.0"},{"comment_id":"850990","content":"Selected Answer: AC\nAC are the best combo","timestamp":"1679833680.0","upvote_count":"2","poster":"mfsec"},{"comment_id":"836454","upvote_count":"2","poster":"hobokabobo","content":"Selected Answer: CD\nA) well yes, maybe but we do already have cloudfront in front - so questionable if acceleration has a big impact on performance.\nB) Not possible. Afaik you cannot attach global accelerator to cloudfront.\nC) this is possible and should five some improvements for api access.\nD) its more infrastructure but it would indeed improve performance.\nE)This is AFAIK not possible.","timestamp":"1678563660.0","comments":[{"comment_id":"836457","content":"It may be that this is an outdated question when there was no support for D(serverless v1 <-> serverless v2). In which case I indeed would go with AC instead.","timestamp":"1678564020.0","upvote_count":"1","poster":"hobokabobo"}]},{"poster":"God_Is_Love","comments":[{"poster":"God_Is_Love","upvote_count":"2","timestamp":"1677959580.0","content":"Global accelerator vs CloudFront\nAWS Global Accelerator is designed for non-HTTP(S) traffic and is used when you have a globally distributed set of endpoints that need to be accessed by clients around the world. CloudFront is designed for HTTP(S) traffic and is used when you have web content that needs to be delivered to users around the world","comment_id":"829340"}],"upvote_count":"3","comment_id":"829298","timestamp":"1677957420.0","content":"Selected Answer: AC\nD is wrong because question asked for improvement. nor re-architect.\nB is not apt here although it improves networking performance because use case here is for S3 uploads.\nE is wrong because there is no need of RDS proxy which provides DB connection pooling."},{"timestamp":"1676130180.0","poster":"moota","content":"Selected Answer: AC\nI agree with C vs D because the backend behind the API Gateway return metadata, which are cacheable with an edge-optimized API Gateway.","comment_id":"805356","upvote_count":"2"},{"poster":"Musk","comment_id":"794658","upvote_count":"3","timestamp":"1675202820.0","content":"Selected Answer: BD\nThinking better bout the ones that in combintion would help best, I finally vote B and D."},{"timestamp":"1675202220.0","upvote_count":"2","comment_id":"794646","poster":"Musk","content":"Selected Answer: CD\nI think D will help more to reduce latency than A. Compleexity or cost is not a factor in the request."},{"content":"Selected Answer: AC\nwell explained by masetromain :)","comments":[{"comment_id":"888234","timestamp":"1683096840.0","comments":[{"upvote_count":"4","timestamp":"1684244340.0","content":"I really sick of it, his answers are not helpful all from chatgpt!","comment_id":"899245","poster":"Jesuisleon"}],"content":"completely","upvote_count":"2","poster":"momo3321"},{"upvote_count":"1","content":"It's really disgusting. Because of that person, I can't be sure of the correct answer.","poster":"k8s_Seoul","timestamp":"1694053800.0","comment_id":"1001113"},{"comment_id":"828432","timestamp":"1677878280.0","content":"His answers are from chatGPT :)","upvote_count":"6","poster":"Pete697989"}],"timestamp":"1675113600.0","upvote_count":"3","poster":"zozza2023","comment_id":"793355"}],"exam_id":33,"url":"https://www.examtopics.com/discussions/amazon/view/95309-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","answer_ET":"AC","answer":"AC","answers_community":["AC (61%)","CD (30%)","6%"],"question_text":"A company is building an electronic document management system in which users upload their documents. The application stack is entirely serverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.\nThe company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of Europe.\n\nWhich combination of actions will meet these requirements? (Choose two.)","question_images":[]}],"exam":{"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional SAP-C02","provider":"Amazon","isImplemented":true,"isMCOnly":true,"id":33,"numberOfQuestions":529,"isBeta":false},"currentPage":102},"__N_SSP":true}