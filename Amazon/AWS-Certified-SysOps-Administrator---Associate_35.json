{"pageProps":{"questions":[{"id":"stCsIGCJ59ZHhzOwPKEy","discussion":[{"poster":"Christina666","content":"Selected Answer: B\nB..........In KMS policy, grant permission to all related accounts","comment_id":"962310","upvote_count":"3","timestamp":"1721879580.0"},{"content":"Selected Answer: B\nB is correct","timestamp":"1711275360.0","comment_id":"849192","poster":"hpipit","upvote_count":"2"},{"poster":"csG13","comment_id":"833159","timestamp":"1709914980.0","upvote_count":"3","content":"Selected Answer: B\nThe correct answer is B. You must first update the key policy use to encrypt the volume, then share it with other accounts."},{"comment_id":"829236","poster":"braveheart22","upvote_count":"4","timestamp":"1709575980.0","content":"B is the answer from my point of view\nhttps://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/"}],"choices":{"A":"Write a script to download the encrypted snapshot, decrypt it using the AWS KMS encryption key used to encrypt the snapshot, then create a new volume in each account.","B":"Update the key policy to grant permission to the AWS KMS encryption key used to encrypt the snapshot with all relevant accounts, then share the snapshot with those accounts.","D":"Create a new unencrypted RDS instance from the encrypted snapshot, connect to the instance using SSH/RDP, export the database contents into a file, then share this file with the other accounts.","C":"Create an Amazon EC2 instance based on the snapshot, then save the instance's Amazon EBS volume as a snapshot and share it with the other accounts. Require each account owner to create a new volume from that snapshot and encrypt it."},"url":"https://www.examtopics.com/discussions/amazon/view/101526-exam-aws-certified-sysops-administrator-associate-topic-1/","answer_ET":"B","answer_description":"","isMC":true,"answer":"B","question_id":171,"unix_timestamp":1677953580,"answers_community":["B (100%)"],"exam_id":34,"answer_images":[],"question_images":[],"timestamp":"2023-03-04 19:13:00","question_text":"A SysOps administrator is building a process for sharing Amazon RDS database snapshots between different accounts associated with different business units within the same company. All data must be encrypted at rest.\n\nHow should the administrator implement this process?","topic":"1"},{"id":"bZxBxLsuUSLVApLmqvvX","discussion":[{"timestamp":"1690257420.0","upvote_count":"5","content":"Selected Answer: C\nhttps://repost.aws/knowledge-center/connect-s3-vpc-endpoint#:~:text=An%20outbound%20rule%20allowing%20traffic%20to%20the%20ID%20of%20the%20prefix%20list%20associated%20with%20the%20gateway%20VPC%20endpoint.","comment_id":"962312","poster":"Christina666"},{"comment_id":"833166","poster":"csG13","timestamp":"1678293360.0","content":"Selected Answer: C\nIt’s C - subnet route table must have an entry pointing to the VPC gateway prefix.","upvote_count":"5"},{"poster":"numark","comment_id":"1320304","content":"Selected Answer: C\nWNhay Not D>>While this could be part of a broader solution to ensure that the S3 bucket accepts uploads from the instances in the private subnet, this step alone does not solve the connectivity issue. The EC2 instance must be able to reach the S3 bucket, which is a matter of network configuration, not bucket policy.","upvote_count":"1","timestamp":"1732985340.0"},{"upvote_count":"2","poster":"VerRi","content":"Selected Answer: C\nIt should be C","timestamp":"1721731080.0","comment_id":"1253607"},{"upvote_count":"2","content":"Selected Answer: C\nC makes the most sense here.","timestamp":"1721343420.0","comment_id":"1250721","poster":"bae0fd3"},{"content":"Selected Answer: C\nWhen an Amazon S3 gateway endpoint is configured in a VPC, the private subnets within the VPC need to have their route tables updated to route the S3 traffic to the gateway endpoint, instead of the internet.\nWithout the appropriate route table updates, the EC2 instance in the private subnet will not be able to communicate with the S3 bucket, even if the instance has the necessary S3 permissions.\nOption A is incorrect because the issue is not related to the IAM role permissions, but rather the routing configuration.\nOption B is incorrect because allowing outbound traffic to 0.0.0.0/0 on port 80 is not necessary to solve this problem. The issue is with the routing to the S3 gateway endpoint, not the internet access.\nOption D is incorrect because the S3 bucket policy is not the issue here. The problem is with the routing, not the permissions on the S3 bucket.","timestamp":"1717712340.0","poster":"Student013657","comment_id":"1225801","upvote_count":"2"},{"timestamp":"1716066540.0","comment_id":"1213467","upvote_count":"1","poster":"noircesar25","content":"Why Option C is Essential:\nDirecting Traffic to S3: Without the correct route in the subnet’s route table, the instance cannot send traffic to S3 because it doesn’t know that it should use the gateway endpoint.\nUsing the Gateway Endpoint: The S3 gateway endpoint enables access to S3 without internet. For this to work, the route table must have a route for the S3 prefix list pointing to the endpoint.\nWhy Option D Alone is Insufficient:\nAccess Control vs. Network Path:\nS3 Bucket Policy: Controls who (or what) can access the bucket and perform specific actions (e.g., s3:PutObject). While necessary for access control, it does not configure the network path needed for the EC2 instance to reach S3.\nNetwork Configuration: Ensures that there is a valid route for the traffic from the EC2 instance to S3."},{"poster":"pekalyok","comment_id":"1208631","content":"Selected Answer: C\nTo resolve the issue where a user cannot upload a file to an Amazon S3 bucket from an Amazon EC2 instance in a private subnet without outbound internet access, even with an S3 gateway endpoint configured, you should: \nC. Update the EC2 subnet route table to include the S3 prefix list destination routes to the S3 gateway endpoint. The primary step needed here is to ensure that the route table associated with the private subnet where the EC2 instance resides correctly routes S3 traffic to the S3 gateway endpoint, enabling secure, private connectivity to S3.","upvote_count":"2","timestamp":"1715218620.0"},{"content":"Selected Answer: C\nWHY NOT A or D. the issue is related to network connectivity, not permissions.","upvote_count":"3","comments":[{"poster":"tgv","content":"not necessarily. the question says the user cannot upload. the first thing I would check is obviously the bucket policy. being in a private subnet and using a S3 endpoint doesn't rely much on the network connectivity, does it?","timestamp":"1713009000.0","comment_id":"1194927","upvote_count":"2"}],"poster":"Koshi202","timestamp":"1712623440.0","comment_id":"1191882"},{"poster":"March2023","timestamp":"1709918220.0","content":"Selected Answer: C\nThe answer is C: \nEach subnet route table must have a route that sends traffic destined for the service to the gateway endpoint using the prefix list for the service\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html","comment_id":"1168989","upvote_count":"2"},{"poster":"TareDHakim","timestamp":"1704516480.0","content":"Selected Answer: A\nA - you need access to allow upload Put object.\n\nwhy not D ? well this is a potential cause, however, when you create S3 Gateway Endpoint you can associate your subnets and that creates a route automatically. \nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html#create-gateway-endpoint-s3:~:text=For%20Route%20tables%2C%20select%20the%20route%20tables%20to%20be%20used%20by%20the%20endpoint.%20We%20automatically%20add%20a%20route%20that%20points%20traffic%20destined%20for%20the%20service%20to%20the%20endpoint%20network%20interface.\n\nWhy not C ? Once traffic is routed via the S3 Gateway endpoint, then the private subnet CIDR is irrelevant.","comment_id":"1114976","upvote_count":"2"},{"poster":"konieczny69","content":"Why not A?\nIts actually A and C.","comment_id":"1098353","timestamp":"1702748100.0","upvote_count":"1"},{"upvote_count":"3","comment_id":"1086038","content":"Selected Answer: D\nWhen you create a gateway endpoint, you select the VPC route tables for the subnets that you enable. The following route is automatically added to each route table that you select. The destination is a prefix list for the service owned by AWS and the target is the gateway endpoint.","poster":"Hudescu","timestamp":"1701511440.0"},{"content":"Selected Answer: D\nI tackled a similar issue in work this week and I still couldn't confidently answer this.\nI click-ops a dummy network my findings:\nroutes are by default, VPC -> local\nprefix list would apply to a SG.\nEndpoint policy by default allows absolutely everything.\nBy default an s3 bucket has no policy.","timestamp":"1700215560.0","comment_id":"1073190","upvote_count":"3","poster":"DeaconStJohn"},{"poster":"callspace","timestamp":"1698129960.0","upvote_count":"3","comment_id":"1052614","content":"Selected Answer: D\nIn the question As the SysOps admin already configures an Amazon S3 gateway endpoint in a VPC then Updating the EC2 subnet route table to include the S3 prefix list destination routes to the S3 gateway endpoint is not required. So just Update the S3 bucket policy to allow s3:PutObject access from the private subnet CIDR block."},{"poster":"TwinSpark","content":"Selected Answer: D\n\"For Route tables, select the route tables to be used by the endpoint. We automatically add a route that points traffic destined for the service to the endpoint network interface.\"\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html#associate-route-tables-s3\nSo route should be already enable by default, Change need to bedone in s3 bucket policy","timestamp":"1695667740.0","upvote_count":"4","comment_id":"1017138"},{"poster":"jipark","comment_id":"984462","upvote_count":"4","content":"Selected Answer: D\nwhy not C : route cannot solve issue (security or policy grant needed)","timestamp":"1692360180.0"},{"upvote_count":"4","comment_id":"829243","content":"C is the way here.","timestamp":"1677953880.0","poster":"braveheart22"}],"choices":{"A":"Update the EC2 instance role policy to include s3:PutObject access to the target S3 bucket.","B":"Update the EC2 security group to allow outbound traffic to 0.0.0.0/0 for port 80.","D":"Update the S3 bucket policy to allow s3:PutObject access from the private subnet CIDR block.","C":"Update the EC2 subnet route table to include the S3 prefix list destination routes to the S3 gateway endpoint."},"url":"https://www.examtopics.com/discussions/amazon/view/101527-exam-aws-certified-sysops-administrator-associate-topic-1/","answer_ET":"C","answer_description":"","isMC":true,"question_id":172,"answer":"C","unix_timestamp":1677953880,"answers_community":["C (56%)","D (40%)","5%"],"exam_id":34,"answer_images":[],"question_images":[],"timestamp":"2023-03-04 19:18:00","question_text":"A SysOps administrator configures an Amazon S3 gateway endpoint in a VPC. The private subnets inside the VPC do not have outbound internet access. User logs in to an Amazon EC2 instance in one of the private subnets and cannot upload a file to an Amazon S3 bucket in the same AWS Region.\n\nWhich solution will solve this problem?","topic":"1"},{"id":"dWsxMM9MBQbV4nIdWoX1","timestamp":"2023-03-08 17:44:00","answers_community":["CE (100%)"],"answer":"CE","isMC":true,"answer_ET":"CE","exam_id":34,"choices":{"A":"Create multiple AWS Direct Connect connections between AWS and branch offices in Europe and Australia for file uploads into the destination S3 bucket.","B":"Create multiple AWS Site-to-Site VPN connections between AWS and branch offices in Europe and Australia for file uploads into the destination S3 bucket.","E":"Use multipart uploads for file uploads into the destination S3 bucket from the branch offices in Europe and Australia.","C":"Use Amazon S3 Transfer Acceleration for file uploads into the destination S3 bucket.","D":"Use AWS Global Accelerator for file uploads into the destination S3 bucket from the branch offices in Europe and Australia."},"question_id":173,"unix_timestamp":1678293840,"url":"https://www.examtopics.com/discussions/amazon/view/101908-exam-aws-certified-sysops-administrator-associate-topic-1/","question_images":[],"answer_images":[],"discussion":[{"upvote_count":"9","poster":"csG13","comments":[{"upvote_count":"4","content":"C, D, E works for acceleration.\nwhy not D : but not should be 'Global'","timestamp":"1708265400.0","comment_id":"984470","poster":"jipark"}],"timestamp":"1694184240.0","comment_id":"833169","content":"C & E - are the most cost effective solutions to upload objects to S3. In particular, S3 transfer acceleration will only charge you if the upload is indeed accelerated. Multi-part upload is recommended by AWS when object’s size exceeds 100MB. Also, it’s recommended to maximise the use of available bandwidth."},{"comment_id":"1114984","upvote_count":"2","timestamp":"1720236180.0","content":"Selected Answer: CE\nAmazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects.","poster":"TareDHakim"},{"timestamp":"1709198400.0","comment_id":"994884","content":"Selected Answer: CE\nGlobal Accelerator service does not work with S3\n\nSo its C and E","poster":"AdriBFK","upvote_count":"4"}],"question_text":"A company uses Amazon S3 to aggregate raw video footage from various media teams across the US. The company recently expanded into new geographies in Europe and Australia. The technical teams located in Europe and Australia reported delays when uploading large video files into the destination S3 bucket in the United States.\n\nWhat are the MOST cost effective ways to increase upload speeds into the S3 bucket? (Choose two.)","topic":"1","answer_description":""},{"id":"36zQXoXYLKjkEgKyB5au","question_text":"A SysOps administrator is helping a development team deploy an application to AWS. The AWS CloudFormation template includes an Amazon Linux EC2 instance, an Amazon Aurora DB cluster, and a hardcoded database password that must be rotated every 90 days.\n\nWhat is the MOST secure way to manage the database password?","url":"https://www.examtopics.com/discussions/amazon/view/101910-exam-aws-certified-sysops-administrator-associate-topic-1/","unix_timestamp":1678294980,"timestamp":"2023-03-08 18:03:00","answers_community":["A (100%)"],"exam_id":34,"discussion":[{"content":"Selected Answer: A\nA - although B looks plausible as well, using the pattern in B won’t rotate the secret every 90 days.","timestamp":"1678294980.0","comment_id":"833187","poster":"csG13","upvote_count":"6"},{"timestamp":"1730596740.0","upvote_count":"1","content":"Selected Answer: A\nQuestion specifically asks for this: \"a hardcoded database password that must be rotated every 90 days.\" , means the method for rotating the hardcoded password every 90 days is mentioned only in option A. Whereas, all other options don't have that method defined in their methods so a big clue already in option A for those who get it.","comment_id":"1306386","poster":"Aamee"},{"timestamp":"1682624700.0","poster":"Gomer","upvote_count":"3","content":"Selected Answer: A\nThe AWS::SecretsManager::Secret directive is only used to \"Creates a new secret\". Doesn't sound right that it would be used to accept an existing password (originating from where?). The question and response \"B\" seems to suggest you are just wanting to re-use an existing password in SecretsManager for a new DB Cluster (which is plausible). If so, rotation would already be configured for existing secret, but you shouldn't also be parsing for length, etc. I also think response has to be \"A\", but not just because it specifies rotation schedule. There is more to this than that.","comment_id":"882995"},{"content":"Selected Answer: A\nOption A!","comment_id":"864791","timestamp":"1680966540.0","poster":"AndyMartinez","upvote_count":"2"}],"question_id":174,"topic":"1","answer_images":[],"answer_description":"","question_images":[],"choices":{"D":"Use the AWS::SSM::Parameter resource. Accept input as a CloudFormation parameter to store the parameter as a string. Configure the application to retrieve the parameter from AWS Systems Manager Parameter Store to access the database.","B":"Use the AWS::SecretsManager::Secret resource with the SecretString property Accept a password as a CloudFormation parameter Use the AllowedPattern property of the CloudFormation parameter to require a minimum length, uppercase and lowercase letters, and special characters. Configure the application to retrieve the secret from AWS Secrets Manager to access the database.","C":"Use the AWS::SSM::Parameter resource. Accept input as a CloudFormation parameter to store the parameter as a secure string. Configure the application to retrieve the parameter from AWS Systems Manager Parameter Store to access the database.","A":"Use the AWS::SecretsManager::Secret resource with the GenerateSecretString property to automatically generate a password. Use the AWS::SecretsManager::RotationSchedule resource to define a rotation schedule for the password. Configure the application to retrieve the secret from AWS Secrets Manager to access the database."},"answer":"A","isMC":true,"answer_ET":"A"},{"id":"FlHGlUw5oJFant1UjSr9","answers_community":["D (85%)","B (15%)"],"discussion":[{"comment_id":"1165787","poster":"March2023","timestamp":"1725459480.0","upvote_count":"2","content":"Selected Answer: D\nD is correct"},{"upvote_count":"2","timestamp":"1714367460.0","comment_id":"1056634","poster":"walala97","content":"Selected Answer: D\nflow log->vpc->nnetwork acl"},{"timestamp":"1711525380.0","comment_id":"1018491","poster":"ahrentom","upvote_count":"1","content":"Selected Answer: B\nI go with Vivec !!!"},{"timestamp":"1708594860.0","comment_id":"987191","upvote_count":"1","content":"Selected Answer: D\nCorrect answer here is D.\n\nBut you need to take to consideration that question can be changed on the exam, because from Aug 10, 2023 NLB started supporting Security groups.\nhttps://aws.amazon.com/about-aws/whats-new/2023/08/network-load-balancer-supports-security-groups/","poster":"xSohox"},{"content":"Selected Answer: D\nThe example from the link exactly matches \"D\" (other than the question example is using HTTP/8080 web proxy port instead of link ICMP example which doesn't show ports).\n\nSecurity group and network ACL rules\n[...]\n- \"An ACCEPT record for the originating ping that was allowed by both the network ACL and the security group, and therefore was allowed to reach your instance.\"\n- \"A REJECT record for the response ping that the network ACL denied.\"\n[...] 203.0.113.12 172.31.16.139 0 0 1 4 336 1432917027 1432917142 ACCEPT OK\n[...] 172.31.16.139 203.0.113.12 0 0 1 4 336 1432917094 1432917142 REJECT OK\nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-records-examples.html#flow-log-example-security-groups","comment_id":"883117","timestamp":"1698452640.0","poster":"Gomer","upvote_count":"1"},{"content":"Selected Answer: D\nThe network ACL that is associated with the subnet does not allow outbound traffic for the ephemeral port range.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-records-examples.html","comment_id":"875252","timestamp":"1697771760.0","poster":"noahsark","upvote_count":"1"},{"comment_id":"851016","upvote_count":"1","poster":"michele_scar","timestamp":"1695732600.0","content":"Selected Answer: D\nThe traffic isn't going out, so the SG is correct but not the outbound ACL rule."},{"content":"I go for D.\n\nIf your network ACL permits outbound ICMP traffic, the flow log displays two ACCEPT records (one for the originating ping and one for the response ping). If your security group denies inbound ICMP traffic, the flow log displays a single REJECT record, because the traffic was not permitted to reach your instance.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-records-examples.html#:~:text=1431280876%201431280934%20%2D%20SKIPDATA-,Security%20group%20and%20network%20ACL%20rules,-If%20you%27re%20using","comment_id":"840998","poster":"atseki","timestamp":"1694864640.0","upvote_count":"3"},{"comments":[{"content":"\"Other applications from an on-premises environment cannot communicate with Application A on port 8080\" means that from the provided sreenshot 192.168.0.13 is the NLB and 172.31.16.139 is tan OnPrem instance. To have this in mind the communicatin from OnPrem to NLB is reject on port 8080. The only right anwser could only be a security group which is blocking port 8080.\nI go with Vivec, the right anwser is B","comment_id":"1018490","poster":"ahrentom","upvote_count":"1","timestamp":"1711525320.0"},{"timestamp":"1694262420.0","content":"Option D is incorrect because if the network ACL that is associated with the subnet did not allow outbound traffic for the ephemeral port range, the flow log record would have shown that the traffic was rejected by the network ACL.","comment_id":"834033","poster":"Vivec","upvote_count":"2"}],"content":"Selected Answer: B\nAccording to the flow log record shown in the picture, the traffic is rejected by the security group of the NLB, which means that the traffic is not reaching the EC2 instances. The source IP address in the flow log is from an on-premises environment, which indicates that the issue is related to the communication between the on-premises environment and the NLB.\n\nSince the NLB is the entry point for the traffic to reach the EC2 instances, it is important to ensure that the security group of the NLB allows traffic from the on-premises environment. The security group rules should allow inbound traffic from the IP addresses or the CIDR blocks of the on-premises environment on the relevant port (8080 in this case).","timestamp":"1694262420.0","comment_id":"834032","poster":"Vivec","upvote_count":"1"},{"upvote_count":"3","comment_id":"833213","poster":"csG13","timestamp":"1694186760.0","content":"Selected Answer: D\nI’ll go for D. Looks like that NACL allows inbound traffic from ephemeral port range, but doesn’t allow outbound."}],"answer_images":[],"topic":"1","question_id":175,"question_text":"Application A runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The EC2 instances are in an Auto Scaling group and are in the same subnet that is associated with the NLB. Other applications from an on-premises environment cannot communicate with Application A on port 8080.\n\nTo troubleshoot the issue, a SysOps administrator analyzes the flow logs. The flow logs include the following records:\n\n//IMG//\n\n\nWhat is the reason for the rejected traffic?","choices":{"A":"The security group of the EC2 instances has no Allow rule for the traffic from the NLB.","D":"The network ACL that is associated with the subnet does not allow outbound traffic for the ephemeral port range.","B":"The security group of the NLB has no Allow rule for the traffic from the on-premises environment.","C":"The ACL of the on-premises environment does not allow traffic to the AWS environment."},"answer_description":"","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/101915-exam-aws-certified-sysops-administrator-associate-topic-1/","timestamp":"2023-03-08 18:26:00","question_images":["https://img.examtopics.com/aws-certified-sysops-administrator-associate/image1.png"],"answer_ET":"D","isMC":true,"unix_timestamp":1678296360,"exam_id":34}],"exam":{"lastUpdated":"11 Apr 2025","isBeta":false,"id":34,"name":"AWS Certified SysOps Administrator - Associate","provider":"Amazon","numberOfQuestions":477,"isImplemented":true,"isMCOnly":false},"currentPage":35},"__N_SSP":true}