{"pageProps":{"questions":[{"id":"kxXMLzmVKGIcmIFaNQI9","question_images":[],"answer_images":[],"answer_description":"","discussion":[{"content":"D any thoughts?\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\nNot B: https://docs.aws.amazon.com/quicksight/latest/user/working-with-aws-vpc.html","poster":"Priyanka_01","comment_id":"153991","timestamp":"1632108780.0","comments":[{"comment_id":"164423","timestamp":"1632529080.0","content":"Agreed","upvote_count":"1","poster":"awssp12345"},{"content":"D is the right answer!","timestamp":"1635103320.0","poster":"Monika14Sharma","upvote_count":"4","comment_id":"369676"},{"timestamp":"1640066700.0","upvote_count":"3","content":"Agree\nRemember cross region ingestion is also supported ( https://aws.amazon.com/about-aws/whats-new/2014/06/29/amazon-redshift-announces-cross-region-ingestion-and-improved-query-functionality/)\nAs of 23 Nov 21, there's a preview capability to have cross region data sharing as well https://aws.amazon.com/about-aws/whats-new/2014/06/29/amazon-redshift-announces-cross-region-ingestion-and-improved-query-functionality/","comments":[{"poster":"lakediver","timestamp":"1640067000.0","comments":[{"comment_id":"573927","poster":"CHRIS12722222","timestamp":"1648072200.0","content":"\"Set up an Amazon Redshift-managed VPC endpoint between the Amazon Redshift cluster VPC and QuickSight VPC. For instructions, see Connecting to Amazon Redshift using an interface VPC endpoint.\"\nhttps://aws.amazon.com/blogs/big-data/amazon-quicksight-deployment-models-for-cross-account-and-cross-region-access-to-amazon-redshift-and-amazon-rds/\n\n\nOption B says something very different","upvote_count":"1"},{"content":"Quicksight has no vpc. What is mentioned in the post is connection from quicksight to a vpc using vpcid subnet id and security grp id. Answer=D","comment_id":"573918","upvote_count":"2","poster":"CHRIS12722222","timestamp":"1648071360.0"},{"poster":"flanfranco","timestamp":"1678097580.0","upvote_count":"2","comment_id":"830713","content":"I think B is not correct: https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-redshift-private-connection/\n\"To create a private connection from QuickSight, you must provide a subnet and security group from a VPC in the same AWS Region.\" \n\"Note: The data source must be in the same account and Region that's used for QuickSight.\""}],"content":"Changing my answer to B\nhttps://aws.amazon.com/blogs/big-data/amazon-quicksight-deployment-models-for-cross-account-and-cross-region-access-to-amazon-redshift-and-amazon-rds/","upvote_count":"2","comment_id":"505847"}],"poster":"lakediver","comment_id":"505845"}],"upvote_count":"44"},{"poster":"singh100","timestamp":"1632503640.0","content":"D. As mentioned in the link shared by Priyanka, B is not the answer. \"QuickSight connects only to data located in the same AWS Region where you're currently using QuickSight. You can't connect QuickSight to data in another AWS Region, even if your VPC is configured to work across AWS Regions.\"","upvote_count":"11","comment_id":"159262"},{"comment_id":"1088211","poster":"teo2157","upvote_count":"1","content":"Selected Answer: B\nHi guys, there's a new amazing IA feature in the AWS documentation called Amazon Q, if you type this question there, the answer is or \"Create an Amazon Redshift cluster in the ap-northeast-1 region and copy the required data from us-east-1 Redshift\" or \"they can configure cross-region connectivity between QuickSight and Redshift. This involves setting up a VPC endpoint for Redshift in ap-northeast-1 region that connects to the us-east-1 Redshift cluster.\", so chosing the option B as it's cheaper than the A.","timestamp":"1701757020.0"},{"timestamp":"1695687900.0","poster":"debasishg","content":"Selected Answer: D\nD.\nNOT A - \"cross-Region snapshots ..\" - expensive\nNOT B - \"VPC endpoint..\" - cross region connectivity not possible from Quicksight\nNOT C - \"Redshift endpoint connection string ..\" - needs address to be added through Security Group to allow connetivity","upvote_count":"1","comment_id":"1017294"},{"timestamp":"1692946980.0","content":"Option D","upvote_count":"1","comment_id":"989747","poster":"nroopa"},{"timestamp":"1690905780.0","content":"Selected Answer: D\nGoing with D","comment_id":"969184","poster":"NikkyDicky","upvote_count":"1"},{"poster":"Espa","content":"Selected Answer: D\nD is the correct option","timestamp":"1684688100.0","comment_id":"903400","upvote_count":"1"},{"poster":"pk349","upvote_count":"1","content":"D: I passed the test","timestamp":"1682946780.0","comment_id":"886274"},{"content":"Definitely D. This scenario is explored in Stephane Maarek's and Abhishek Singh's Udemy practice exam set.","upvote_count":"1","timestamp":"1680251580.0","poster":"Aina","comment_id":"856797"},{"poster":"rich_knp","timestamp":"1679434260.0","comment_id":"846380","upvote_count":"2","content":"Selected Answer: D\nD is correct"},{"poster":"flanfranco","comment_id":"830706","content":"D: https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\n\"If you activated Amazon QuickSight in multiple AWS Regions, you can create inbound rules for each Amazon QuickSight endpoint CIDR. Doing this allows Amazon QuickSight to have access to the Amazon RDS DB instance from any AWS Region defined in the inbound rules.\n\nAn Amazon QuickSight user or administrator who uses Amazon QuickSight in multiple AWS Regions is treated as a single user. In other words, even if you are using Amazon QuickSight in every AWS Region, both your Amazon QuickSight account and your users are global\"","upvote_count":"3","timestamp":"1678096920.0"},{"timestamp":"1676544480.0","upvote_count":"1","comment_id":"810560","poster":"[Removed]","content":"Selected Answer: D\nAnswer is D!"},{"comment_id":"711135","upvote_count":"2","poster":"cloudlearnerhere","timestamp":"1667565360.0","content":"Selected Answer: D\nCorrect answer is D as the Redshift security group should be updated to allow inbound access from QuickSight IP addresses.\n\nOption B is wrong as QuickSight does not support cross-region access within the VPC.\n\nOption A is wrong as it leads to data duplication and is not the most efficient solution.\n\n\nOption C is wrong as changing the endpoint would not allow connectivity. The access needs to be enabled using security groups."},{"comment_id":"704020","poster":"Bansel","timestamp":"1666714920.0","upvote_count":"1","content":"D per AWS DA Course on Udemy!"},{"comment_id":"703730","timestamp":"1666689240.0","upvote_count":"1","content":"Ans is D\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html","poster":"thirukudil"},{"poster":"pgf909","timestamp":"1666498680.0","upvote_count":"1","content":"B is not correct as this: https://docs.aws.amazon.com/quicksight/latest/user/vpc-finding-setup-information.html\nMake sure also that you use Amazon QuickSight in the same AWS Region with the VPC.\nYou can't use QuickSight in one AWS Region and expect to connect to a VPC in a different AWS Region.","comment_id":"701901"},{"comment_id":"688431","timestamp":"1665128340.0","upvote_count":"1","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/quicksight/latest/user/vpc-creating-a-connection-in-quicksight.html \nAWS Region – The AWS Region where you plan to create a connection to your data source.\nVPC ID – The ID of the VPC that contains the data, the subnets, and the security groups that you plan to use.\nSubnet ID – The ID of the subnet that the QuickSight network interface is using.\nSecurity group ID – The ID of the security group.\nQuicksight can access using VPC ID and region so B is correct","poster":"LukeTran3206"},{"content":"Selected Answer: D\nQuicksight has different range of IP addresses, for different AWS region. These IP addresses need to be whitelisted in the security group for RedShift cluster.","poster":"Arka_01","timestamp":"1664080260.0","upvote_count":"2","comment_id":"678413"},{"upvote_count":"1","comment_id":"605238","timestamp":"1653204900.0","poster":"Bik000","content":"Selected Answer: D\nAnswer should be D"},{"timestamp":"1651357200.0","content":"Answer-D","poster":"jrheen","comment_id":"595326","upvote_count":"1"},{"upvote_count":"1","comment_id":"590855","poster":"sanjee000","content":"Selected Answer: D\nBy default Quicksight can only access data stored IN THE \nSAME REGION as the one Quicksight is running within\n• So if Quicksight is running in one region, and Redshift in \nanother, that’s a problem\n• A VPC configured to work across AWS regions won’t work!\n• Solution: create a new security group with an inbound rule \nauthorizing access from the IP range of QuickSight servers \nin that region\n• Those ranges are documented at \nhttps://docs.aws.amazon.com/quicksight/latest/user/regions.html","timestamp":"1650766860.0"},{"poster":"astalavista1","timestamp":"1648909260.0","upvote_count":"1","comment_id":"579885","content":"Selected Answer: D\nYou need to add a new IP range to the security as a new region now uses Quicksight."},{"poster":"aws2019","upvote_count":"2","timestamp":"1637422740.0","content":"Connectivity issues(not performance) = D","comment_id":"482666"},{"timestamp":"1636998300.0","upvote_count":"1","comment_id":"478883","content":"If it is a timeout situation option D works. But I think C is correct.","poster":"goutes"},{"upvote_count":"1","poster":"francisco94","content":"The answer should be B as per this quote: \"If you activated Amazon QuickSight in multiple AWS Regions, you can create inbound rules for each Amazon QuickSight endpoint CIDR. Doing this allows Amazon QuickSight to have access to the Amazon RDS DB instance from any AWS Region defined in the inbound rules.\"\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html","comment_id":"439591","timestamp":"1636259460.0"},{"poster":"iconara","timestamp":"1636118100.0","content":"the answer is D, but the question makes no sense. The correct action is to kove the QuickSight dashboard to the region where the database resides, not opening up ports.","upvote_count":"1","comment_id":"435306"},{"poster":"Donell","upvote_count":"5","timestamp":"1636003740.0","comments":[{"upvote_count":"1","content":"Answer is C for the fact that Manual Snapshots are used over automated snapshots to refresh repositories in another region","timestamp":"1647777180.0","poster":"ShilaP","comment_id":"571577"},{"content":"I believe it should be C according to this: https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-migrate-domain/","comments":[{"comment_id":"505853","content":"Agree \nYou can use manual snapshots NOT automated snapshots to migrate individual indices or entire OpenSearch Service domains. You can migrate data to a domain in the same account or to a domain in a different account - same or different region.","upvote_count":"3","timestamp":"1640067600.0","poster":"lakediver"}],"timestamp":"1637173620.0","upvote_count":"2","poster":"Olga2022","comment_id":"480201"}],"comment_id":"392644","content":"Options for Q1: (Not allowing to post everything at once may be because too lengthy,hence posting options separately).\nA. Copy the most recent snapshot in the us-east-1 S3 bucket to a new bucket in the ap-northeast-1 Region. Create the new cluster and register the ap-northeast-1 S3 bucket as the Elasticsearch repository. Then restore from thesnapshot.\nB. Create a new cluster in the ap-northeast-1 Region and use the _reindex API operation to copy the data betweenclusters.\nC. Create a new cluster in the ap-northeast-1 Region and register the S3 bucket used for manual snapshots to the us-east-1 Region as the repository for the cluster. Then restore from the latestsnapshot.\nD. Create a new cluster in the ap-northeast-1 Region and register the automated snapshot S3 bucket as the repository for the cluster. Then restore from the latestsnapshot."},{"poster":"Donell","timestamp":"1635953340.0","upvote_count":"3","comment_id":"392643","content":"Hello People! This is a recent and real exam question for AWS DAS C01.\nPlease comment your answers!\nAn online retail company has product data stored in Amazon Elasticsearch Service (Amazon ES) in the us- east-1 Region. The company is expanding globally and needs to deploy the same catalog data to a new Elasticsearch cluster in the ap-northeast-1 Region. The data in the new cluster must be no more than 1 hour older than the data in the original cluster. The company currently creates manual hourly snapshots saved to an Elasticsearch repository in an Amazon S3 bucket in the us-east-1 Region. The company also has hourly automated snapshots enabled.\nWhich solution meets these requirements?"},{"content":"Answer is D. C seems to be correct but actually the team in the question already did that I think. So the question focus to the security group setup.","comment_id":"387692","upvote_count":"1","poster":"Huy","timestamp":"1635930960.0"},{"content":"Answer D. Create a new security group for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in ap-northeast-1.","comment_id":"386239","timestamp":"1635393540.0","upvote_count":"2","poster":"Donell"},{"poster":"Shraddha","content":"Note: This is a textbook question. Although it looks like a weird answer, but actually you cannot have cross-region access if you put QuickSight in a VPC.\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html","comment_id":"383537","timestamp":"1635230100.0","upvote_count":"1"},{"poster":"MrCarter","upvote_count":"1","comment_id":"379109","content":"DDDDDDDDDD","timestamp":"1635120180.0"},{"upvote_count":"1","comment_id":"359155","poster":"leliodesouza","timestamp":"1634968020.0","content":"The answer is D."},{"timestamp":"1634861580.0","poster":"DerekKey","upvote_count":"1","comment_id":"338400","content":"C: \nUse the following procedure to create a connection to any database other than an autodiscovered Amazon Redshift cluster or Amazon RDS instance. Such databases include Amazon Redshift clusters and Amazon RDS instances that are in a different AWS Region or are associated with a different AWS account.\nFor an Amazon Redshift cluster or Amazon RDS instance, enter the endpoint of the cluster or instance without the port number. For example, if the endpoint value is clustername.1234abcd.us-west-2.redshift.amazonaws.com:1234, then enter clustername.1234abcd.us-west-2.redshift.amazonaws.com. You can get the endpoint value from the Endpoint field on the cluster or instance detail page in the AWS console.\nA: makes no sense because there will be old data in the snapshot\nB: will not work as stated explicitly by AWS in their documentation\nD: will not work, you need to set up VPC peering etc."},{"comment_id":"304347","upvote_count":"7","timestamp":"1634715540.0","content":"Rules:\nQuicksight can connect to RDS/Redshift in public subnet vpc(Same region requires security group changes)\nQuicksight can connect to RDS/Redshift in private subnet vpc(Same region using quicksight VPC & security group changes)\nQuicksight can connect to RDS/Redshift in public subnet vpc(different region requires security group changes)\nQuicksight can't connect to RDS/Redshift in private subnet vpc(different region)\n\nit was mentioned regions are different.\nAs it's not mentioned anything about VPC/subnet we can assume Redhsift/RDS are in public subnet not because as per rule 4 we can't connect if there are in private subnet across different region i.e we can eliminate option B and it's required to open security groups to establish connectivity in case of first 3 rules. So Answer is D.","poster":"Naresh_Dulam"},{"content":"QuickSight connects only to data located in the same AWS Region where you're currently using QuickSight. You can't connect QuickSight to data in another AWS Region, even if your VPC is configured to work across AWS Regions.","comment_id":"300505","comments":[{"content":"it will be able to connect to another regios, provided the RDS/Redshift are not in a private subnet VPC","upvote_count":"1","comment_id":"571580","timestamp":"1647777420.0","poster":"ShilaP"}],"timestamp":"1634501400.0","poster":"Jig","upvote_count":"2"},{"comments":[{"poster":"Pruthvi","content":"Answer is D, A is correct as well but its very expensive. \nIf you activated Amazon QuickSight in multiple AWS Regions, you can create inbound rules for each Amazon QuickSight endpoint CIDR. Doing this allows Amazon QuickSight to have access to the Amazon RDS DB instance from any AWS Region defined in the inbound rules.","upvote_count":"1","comment_id":"277524","timestamp":"1634344200.0"}],"timestamp":"1634081760.0","content":"D is the right answer","comment_id":"274252","poster":"lostsoul07","upvote_count":"4"},{"timestamp":"1633963380.0","upvote_count":"4","comment_id":"230542","content":"ANS C : NEW CONNECTION \nhttps://docs.aws.amazon.com/quicksight/latest/user/create-a-database-data-set.html\n6. In the FROM NEW DATA SOURCES section of the Create a Data Set page, choose the Redshift Manual connect icon if you want to connect to an Amazon Redshift cluster in another AWS Region or associated with a different AWS account. Or choose the appropriate database management system icon to connect to an instance of Amazon Aurora, MariaDB, Microsoft SQL Server, MySQL, Oracle, or PostgreSQL.","poster":"angadaws"},{"content":"If the Redshift cluster is public then you can add IP CIDR of Tokyo QuickSight to the it's SG and Redshift will allow connection from Tokyo then. (Answer D)\n\nIf Redshift is private (private subnet in a VPC) you have to then enable VPC endpoint in QuickSight and that doesn't work with VPC in other regions according to the docs:\nhttps://docs.aws.amazon.com/quicksight/latest/user/working-with-aws-vpc.html\nIn this scenario a possible option is then to restore Redshift to the QuickSight region and connect to the local cluster (Answer A) \n\nI personally go for A.","poster":"mmbox2050","comment_id":"225669","timestamp":"1633862640.0","upvote_count":"1"},{"poster":"BillyC","content":"D is correct","comment_id":"216824","timestamp":"1633746060.0","upvote_count":"1"},{"comments":[{"poster":"jove","timestamp":"1633654980.0","upvote_count":"1","content":"You can find the IP address range of QuickSight regions : https://docs.aws.amazon.com/quicksight/latest/user/regions.html","comment_id":"210759"}],"content":"Absolutely not D. You wouldn't know the IPs from that region and the link says clearly:QuickSight connects only to data located in the same AWS Region where you're currently using QuickSight. You can't connect QuickSight to data in another AWS Region, even if your VPC is configured to work across AWS Regions.\n It also mentions that this is meant for connection between on-prem and AWS. only A would work.","upvote_count":"1","timestamp":"1633587120.0","comment_id":"207271","poster":"[Removed]"},{"comments":[{"content":"Option A . D wont work as QuickSight connects only to data located in the same AWS Region where you're currently using QuickSight. You can't connect QuickSight to data in another AWS Region, even if your VPC is configured to work across AWS Regions.","comment_id":"206009","upvote_count":"2","timestamp":"1633477920.0","comments":[{"content":"Sorry! This is incorrect ; Redshift cluster can be in different region than Quicksight Region.\nIn fact multiple Quicksight region can be added in inbound rules of Redshift security group. Each Quicksight region has a specific CIDR range. Option D is correct answer.\nhttps://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html","comment_id":"207282","timestamp":"1633647240.0","upvote_count":"2","poster":"Prodip"}],"poster":"sparun"}],"content":"has someone tried using option D and it worked ?\nI am still confused between A and D :(","poster":"Warrior001","timestamp":"1633470300.0","upvote_count":"1","comment_id":"204711"},{"comment_id":"204674","poster":"sanjaym","content":"Answer should be D.\nhttps://docs.aws.amazon.com/quicksight/latest/user/configure-access.html","timestamp":"1633407120.0","upvote_count":"2"},{"upvote_count":"3","poster":"askblr","timestamp":"1633366320.0","comment_id":"192006","content":"Both of these explanations contradicts in choosing answer between \"A\" and \"D\". which one to ignore for the given question?\nAns:A - According to link https://docs.aws.amazon.com/quicksight/latest/user/working-with-aws-vpc.html\nNote: QuickSight connects only to data located in the same AWS Region where you're currently using QuickSight. You can't connect QuickSight to data in another AWS Region, even if your VPC is configured to work across AWS Regions.\nAns: D - According to link https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\nHeading: Manually Enabling Access to an Amazon Redshift Cluster That Is Not in a VPC.\nAs per the link for \"D\", it talks about connecting to different VPC (not different Region). So \"A\" holds strong..Any thoughts?"},{"comments":[{"comment_id":"231643","content":"According to the description in https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\n\"If you activated Amazon QuickSight in multiple AWS Regions, you can create inbound rules for each Amazon QuickSight endpoint CIDR. Doing this allows Amazon QuickSight to have access to the Amazon RDS DB instance from any AWS Region defined in the inbound rules.\"\nIt seems that QuickSight can access data of different region, right ?","poster":"liyungho","upvote_count":"3","timestamp":"1634016360.0"}],"comment_id":"182759","content":"Answer is A. \nFrom the page bellow, \nhttps://docs.aws.amazon.com/quicksight/latest/user/working-with-aws-vpc.html\nIt says, 'QuickSight connects only to data located in the same AWS Region where you're currently using QuickSight. You can't connect QuickSight to data in another AWS Region, even if your VPC is configured to work across AWS Regions.'\nSo we should move the Redshift instance to the same region with Quicksight.","poster":"hongy","upvote_count":"3","timestamp":"1633168560.0"},{"timestamp":"1632981300.0","content":"Option D, \"For Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in that AWS Region\"","poster":"Karan_Sharma","upvote_count":"1","comment_id":"177478"},{"poster":"Paitan","content":"Option A will work but will be expensive. Seems like using security groups will be a much easy implementation. In fact during a recent hands on exercise, I was facing a similar issue (although I was was using the same region) and playing around with the Redshift cluster's security group solved the issue.","timestamp":"1632875040.0","comment_id":"175266","upvote_count":"3"},{"poster":"pmjcr","timestamp":"1632542820.0","content":"Answer seems to be D. \nQuicksight while attached to vpc cannot connect cross region as stated here https://docs.aws.amazon.com/quicksight/latest/user/working-with-aws-vpc.html\nThen the only option is to enable access via the public endpoint of Quicksight on the SG of Redshift. List of public cidr of Quicksight in all regions is here https://docs.aws.amazon.com/quicksight/latest/user/regions.html","comment_id":"171585","upvote_count":"3"},{"timestamp":"1632519000.0","upvote_count":"1","poster":"abhineet","comment_id":"160176","content":"B does not make any sense and C you cannot include region info, no need to set up another cluster as per A, D is correct."},{"content":"B.\nUse snapshot is too slow and too complicated so A dropped. The VPC endpoint has nothing to do with the connection string, so C dropped. If VPC endpoint not be used, then it won't take the advantage of AWS high speed network backbone, and will route thru public internet, which is slow and not secure. So D dropped.","poster":"zanhsieh","comment_id":"159213","upvote_count":"1","timestamp":"1632446880.0"},{"upvote_count":"2","comment_id":"156528","content":"Im surprised about the security group thing. Thanks @Priyanka_01 for the links.","poster":"testtaker3434","timestamp":"1632396360.0"},{"upvote_count":"4","timestamp":"1632175140.0","comment_id":"155609","content":"Answer is D","poster":"Prodip"}],"answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/27815-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_text":"A US-based sneaker retail company launched its global website. All the transaction data is stored in Amazon RDS and curated historic transaction data is stored in Amazon Redshift in the us-east-1 Region. The business intelligence (BI) team wants to enhance the user experience by providing a dashboard for sneaker trends.\nThe BI team decides to use Amazon QuickSight to render the website dashboards. During development, a team in Japan provisioned Amazon QuickSight in ap- northeast-1. The team is having difficulty connecting Amazon QuickSight from ap-northeast-1 to Amazon Redshift in us-east-1.\nWhich solution will solve this issue and meet the requirements?","question_id":46,"answers_community":["D (87%)","13%"],"answer":"D","exam_id":20,"isMC":true,"topic":"1","timestamp":"2020-08-10 00:59:00","unix_timestamp":1597013940,"choices":{"D":"Create a new security group for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in ap-northeast-1.","B":"Create a VPC endpoint from the Amazon QuickSight VPC to the Amazon Redshift VPC so Amazon QuickSight can access data from Amazon Redshift.","A":"In the Amazon Redshift console, choose to configure cross-Region snapshots and set the destination Region as ap-northeast-1. Restore the Amazon Redshift Cluster from the snapshot and connect to Amazon QuickSight launched in ap-northeast-1.","C":"Create an Amazon Redshift endpoint connection string with Region information in the string and use this connection string in Amazon QuickSight to connect to Amazon Redshift."}},{"id":"NwdXb3Yr0Sfj0doXLXFp","answer_description":"","answers_community":["C (100%)"],"isMC":true,"answer_ET":"C","topic":"1","choices":{"D":"Add a dedicated compute node for small queries","C":"Configure short query acceleration in workload management (WLM)","B":"Increase the concurrency limit in workload management (WLM)","A":"Use Amazon Redshift Spectrum for small queries"},"unix_timestamp":1650708420,"question_id":47,"discussion":[{"upvote_count":"8","poster":"CHRIS12722222","comment_id":"590530","content":"C is ok","timestamp":"1650708420.0"},{"poster":"astalavista1","upvote_count":"6","comment_id":"590786","timestamp":"1650744180.0","content":"Selected Answer: C\nC - https://docs.aws.amazon.com/redshift/latest/dg/advisor-recommendations.html#enable-sqa-recommendation"},{"poster":"juanife","upvote_count":"1","timestamp":"1688001000.0","comment_id":"937303","content":"C is ok. No doubts at all."},{"poster":"pk349","upvote_count":"2","comment_id":"886637","content":"C: I passed the test","timestamp":"1682967420.0"},{"timestamp":"1679825940.0","poster":"CleverMonkey092","comment_id":"850887","upvote_count":"1","content":"C for me"},{"poster":"rocky48","timestamp":"1658445540.0","upvote_count":"1","content":"Selected Answer: C\nSelected Answer: C","comment_id":"634895"},{"upvote_count":"1","poster":"Ramshizzle","content":"Selected Answer: C\nIt is C","timestamp":"1655369340.0","comment_id":"617180"},{"poster":"jrheen","timestamp":"1651351680.0","upvote_count":"3","comment_id":"595283","content":"Answer - C"}],"timestamp":"2022-04-23 12:07:00","question_text":"An online retail company is using Amazon Redshift to run queries and perform analytics on customer shopping behavior. When multiple queries are running on the cluster, runtime for small queries increases significantly. The company's data analytics team to decrease the runtime of these small queries by prioritizing them ahead of large queries.\nWhich solution will meet these requirements?","answer_images":[],"question_images":[],"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/74215-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20},{"id":"MQ6rEVaDlwEWWpRldfys","answer_description":"","answers_community":["B (85%)","D (15%)"],"question_text":"A company uses Amazon Redshift as its data warehouse. A new table includes some columns that contain sensitive data and some columns that contain non- sensitive data. The data in the table eventually will be referenced by several existing queries that run many times each day.\nA data analytics specialist must ensure that only members of the company's auditing team can read the columns that contain sensitive data. All other users must have read-only access to the columns that contain non-sensitive data.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"B","unix_timestamp":1650897000,"exam_id":20,"answer":"B","question_images":[],"choices":{"C":"Grant all users read-only permissions to the columns that contain non-sensitive data. Attach an IAM policy to the auditing team with an explicit. Allow action that grants access to the columns that contain sensitive data.","D":"Grant the auditing team permission to read from the table. Create a view of the table that includes the columns that contain non-sensitive data. Grant the appropriate users read-only permissions to that view.","A":"Grant the auditing team permission to read from the table. Load the columns that contain non-sensitive data into a second table. Grant the appropriate users read-only permissions to the second table.","B":"Grant all users read-only permissions to the columns that contain non-sensitive data. Use the GRANT SELECT command to allow the auditing team to access the columns that contain sensitive data."},"topic":"1","isMC":true,"question_id":48,"discussion":[{"upvote_count":"7","comment_id":"593780","timestamp":"1651146000.0","poster":"Teraxs","content":"Selected Answer: B\nB - GRANT defines access privileges for a user or user group. https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html"},{"comment_id":"886638","poster":"pk349","content":"B: I passed the test","upvote_count":"3","timestamp":"1682967480.0"},{"poster":"MultiCloudIronMan","comment_id":"700022","timestamp":"1666275960.0","content":"Selected Answer: B\nThe question also says the least operational over head, that makes B the right answer.","comments":[{"comment_id":"744163","upvote_count":"5","content":"Views or AWS Lake Formation on Amazon Redshift Spectrum was used previously to manage such scenarios, however this adds extra overhead in creating and maintaining views or Amazon Redshift Spectrum. View based approach is also difficult to scale and can lead to lack of security controls. Amazon Redshift column-level access control is a new feature that supports access control at a column-level for data in Amazon Redshift. You can use column-level GRANT and REVOKE statements to help meet your security and compliance needs similar to managing any database object.\n\nhttps://aws.amazon.com/blogs/big-data/achieve-finer-grained-data-security-with-column-level-access-control-in-amazon-redshift/","timestamp":"1670943480.0","poster":"nadavw"}],"upvote_count":"4"},{"comment_id":"634375","upvote_count":"2","timestamp":"1658381160.0","content":"Selected Answer: B\nAnswer is B","poster":"rocky48"},{"content":"Selected Answer: B\nAnswer is B","timestamp":"1654706940.0","comment_id":"613389","upvote_count":"2","poster":"CloudTimes"},{"timestamp":"1653205680.0","content":"Selected Answer: B\nAnswer should be B","poster":"Bik000","upvote_count":"2","comment_id":"605257"},{"comment_id":"604867","timestamp":"1653133020.0","content":"It's B. \nhttps://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/","upvote_count":"1","poster":"certificationJunkie"},{"comment_id":"601805","poster":"facoxa","comments":[{"upvote_count":"2","timestamp":"1653057480.0","content":"Use this command to give specific privileges for a table, database, schema, function, procedure, language, or column, https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html. so it is B","comment_id":"604488","poster":"redpirate"},{"comment_id":"645611","content":"Grant command give access to table or database or column\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html","poster":"WonderTan","upvote_count":"1","timestamp":"1660251720.0"},{"upvote_count":"1","timestamp":"1660796700.0","comment_id":"648277","content":"columm level is enabled according to https://aws.amazon.com/cn/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/","poster":"Gavin_Y"}],"content":"Selected Answer: D\nGrant command give access to table or database level, not column level.\nView are selected projection of a table hence can be and shud be used here","timestamp":"1652564640.0","upvote_count":"3"},{"timestamp":"1651350000.0","poster":"jrheen","upvote_count":"1","comment_id":"595264","content":"Answer: B"},{"comment_id":"591771","content":"I think it's B","upvote_count":"4","timestamp":"1650897000.0","poster":"chp2022"}],"url":"https://www.examtopics.com/discussions/amazon/view/74484-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_images":[],"timestamp":"2022-04-25 16:30:00"},{"id":"4U8CoHtNh9neW6WpuAdX","answer_ET":"C","answer":"C","exam_id":20,"question_id":49,"discussion":[{"comment_id":"649599","content":"Selected Answer: C\nA - WRONG, EC2 adds operational overhead.\nB - WRONG, EC2 adds operational overhead.\nC - CORRECT, You can use an interface VPC endpoint to keep traffic between your Amazon VPC and Kinesis Data Streams from leaving the Amazon network. https://docs.aws.amazon.com/streams/latest/dev/vpc.html\nD - WRONG, even though MSK is used in this blog post ( https://aws.amazon.com/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics/ ), having MSK increases operational overhead compared to using KDS as in answer C.","timestamp":"1661058300.0","poster":"alfredofmt","upvote_count":"7"},{"upvote_count":"6","timestamp":"1650897480.0","poster":"chp2022","content":"Selected Answer: D\nD makes sense based on the link provided","comment_id":"591774"},{"timestamp":"1682969700.0","content":"C: I passed the test","comment_id":"886662","upvote_count":"3","poster":"pk349"},{"content":"I have a doubt about C and D:\n\"Configure the Kinesis Data Analytics application to have sources from Kinesis Data Streams and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect.\"\n\nKinesis Data Analytics can have, as a source, only Kinesis Data Stream, Kinesis Data Firehose and S3, so, as they are worded, it seems they are incorrect.\n\nAny thoughts?","comments":[{"comment_id":"803957","content":"C - KDA can have an on-premise Kafka cluster as a data source\nhttps://aws.amazon.com/ko/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics/","poster":"Merrick","upvote_count":"4","timestamp":"1676002740.0"}],"poster":"Zast","comment_id":"757738","timestamp":"1672078920.0","upvote_count":"2"},{"content":"Selected Answer: C\nSelected Answer: C","comments":[{"timestamp":"1664963340.0","content":"(https://docs.aws.amazon.com/streams/latest/dev/vpc.html)\nYou can use an interface VPC endpoint to keep traffic between your Amazon VPC and Kinesis Data Streams from leaving the Amazon network.","comment_id":"686755","upvote_count":"1","poster":"rocky48"}],"comment_id":"643517","upvote_count":"2","poster":"rocky48","timestamp":"1659817620.0"},{"content":"Selected Answer: C\nC because you can have interface endpoints for KDS (https://docs.aws.amazon.com/streams/latest/dev/vpc.html) and its less overhead","upvote_count":"5","timestamp":"1653219660.0","comment_id":"605406","poster":"f4bi4n"},{"upvote_count":"3","comments":[{"comments":[{"upvote_count":"2","comment_id":"591421","poster":"siju13","content":"Why not C?","timestamp":"1650870300.0"}],"upvote_count":"1","poster":"CHRIS12722222","timestamp":"1650717780.0","comment_id":"590625","content":"KDS provides public endpoint so not use for vpc connectivity"}],"poster":"CHRIS12722222","content":"Looks like D is right, based on the ref link provided in the question","comment_id":"590624","timestamp":"1650717780.0"}],"answer_description":"","answers_community":["C (70%)","D (30%)"],"choices":{"B":"Implement Flink on Amazon EC2 within the company's VPC. Use Amazon Kinesis Data Streams to collect data that comes from applications and databases within the VPC and the public internet. Configure Flink to have sources from Kinesis Data Streams and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect.","C":"Create an Amazon Kinesis Data Analytics application by uploading the compiled Flink .jar file. Use Amazon Kinesis Data Streams to collect data that comes from applications and databases within the VPC and the public internet. Configure the Kinesis Data Analytics application to have sources from Kinesis Data Streams and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect.","D":"Create an Amazon Kinesis Data Analytics application by uploading the compiled Flink .jar file. Create Amazon Managed Streaming for Apache Kafka (Amazon MSK) clusters in the company's VPC to collect data that comes from applications and databases within the VPC. Use Amazon Kinesis Data Streams to collect data that comes from the public internet. Configure the Kinesis Data Analytics application to have sources from Kinesis Data Streams, Amazon MSK, and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect.","A":"Implement Flink on Amazon EC2 within the company's VPC. Create Amazon Managed Streaming for Apache Kafka (Amazon MSK) clusters in the VPC to collect data that comes from applications and databases within the VPC. Use Amazon Kinesis Data Streams to collect data that comes from the public internet. Configure Flink to have sources from Kinesis Data Streams Amazon MSK, and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect."},"unix_timestamp":1650717780,"url":"https://www.examtopics.com/discussions/amazon/view/74238-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2022-04-23 14:43:00","question_text":"A company hosts an Apache Flink application on premises. The application processes data from several Apache Kafka clusters. The data originates from a variety of sources, such as web applications, mobile apps, and operational databases. The company has migrated some of these sources to AWS and now wants to migrate the Flink application. The company must ensure that data that resides in databases within the VPC does not traverse the internet. The application must be able to process all the data that comes from the company's AWS solution, on-premises resources, and the public internet.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"topic":"1","question_images":[],"isMC":true},{"id":"5ZnleyFmPKpngkYm3VIg","answer":"B","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/74242-exam-aws-certified-data-analytics-specialty-topic-1-question/","unix_timestamp":1650718380,"answers_community":["B (100%)"],"exam_id":20,"question_id":50,"topic":"1","question_text":"A technology company has an application with millions of active users every day. The company queries daily usage data with Amazon Athena to understand how users interact with the application. The data includes the date and time, the location ID, and the services used. The company wants to use Athena to run queries to analyze the data with the lowest latency possible.\nWhich solution meets these requirements?","timestamp":"2022-04-23 14:53:00","question_images":[],"answer_images":[],"answer_ET":"B","discussion":[{"timestamp":"1666276500.0","comment_id":"700029","upvote_count":"7","content":"B is the right answer, partitioning by data and data type","poster":"MultiCloudIronMan"},{"content":"Apache Parquet is a data format which allows fast retrieval. No doubts, it's C.","comment_id":"938429","upvote_count":"1","timestamp":"1688061780.0","poster":"juanife"},{"upvote_count":"2","content":"B: I passed the test","poster":"pk349","comment_id":"886663","timestamp":"1682969700.0"},{"timestamp":"1660375740.0","comment_id":"646189","upvote_count":"1","content":"B\nI think Parquet and ORC are the right choices but the partition will eliminate ORC.","poster":"kondi2309"},{"timestamp":"1658381760.0","upvote_count":"3","comment_id":"634384","poster":"rocky48","content":"Selected Answer: B\nAnswer - B"},{"comment_id":"628690","content":"Selected Answer: B\nThere is hardly the choice between Avro and ORC. The only thing we can use to eliminate the ORC option is that because of the wrong partitioning criteria given with that option.","poster":"dushmantha","timestamp":"1657269840.0","upvote_count":"1"},{"comment_id":"595274","timestamp":"1651351080.0","poster":"jrheen","content":"Answer - B","upvote_count":"1"},{"poster":"AWSRanger","comment_id":"591988","upvote_count":"1","timestamp":"1650926940.0","content":"Selected Answer: B\nB is right"},{"upvote_count":"4","comments":[{"comment_id":"744191","content":"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","poster":"nadavw","timestamp":"1670944560.0","upvote_count":"1"}],"comment_id":"590790","poster":"astalavista1","content":"Selected Answer: B\nB - Parquet increases speed, partition by date & time also brings better performance.","timestamp":"1650744720.0"},{"timestamp":"1650718380.0","comment_id":"590635","poster":"CHRIS12722222","content":"B is ok","upvote_count":"3"}],"choices":{"A":"Store the data in Apache Avro format with the date and time as the partition, with the data sorted by the location ID.","B":"Store the data in Apache Parquet format with the date and time as the partition, with the data sorted by the location ID.","C":"Store the data in Apache ORC format with the location ID as the partition, with the data sorted by the date and time.","D":"Store the data in .csv format with the location ID as the partition, with the data sorted by the date and time."},"answer_description":""}],"exam":{"provider":"Amazon","name":"AWS Certified Data Analytics - Specialty","numberOfQuestions":164,"lastUpdated":"11 Apr 2025","isImplemented":true,"isBeta":false,"isMCOnly":true,"id":20},"currentPage":10},"__N_SSP":true}