{"pageProps":{"questions":[{"id":"ujQUftMIZbQPJKGgO3eR","question_id":301,"answer_ET":"A","question_text":"A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is configured with a single DB instance. The application performs read and write operations on the database by using the cluster's instance endpoint.\nThe company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain available with the least possible interruption during the maintenance window.\nWhat should a DevOps engineer do to meet these requirements?","unix_timestamp":1680809040,"question_images":[],"timestamp":"2023-04-06 21:24:00","answer_description":"","topic":"1","answers_community":["A (78%)","12%","5%"],"url":"https://www.examtopics.com/discussions/amazon/view/105457-exam-aws-certified-devops-engineer-professional-dop-c02/","discussion":[{"poster":"junrun3","content":"Selected Answer: A\nB and D are incorrect because Aurora cluster provides cluster and read endpoints, but does not support creating custom ANY endpoints.\n\n\n\nC and D are incorrect because Amazon Aurora's multi-AZ option must be set when the DB instance is created.\n\n\n\nTherefore, A is correct.","upvote_count":"15","timestamp":"1688485260.0","comment_id":"942938"},{"upvote_count":"12","content":"Option A is the right choise for an existing Cluster without Multi-AZ!\nRefer to: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n \n#Read the Tip Box#\n\"You can set up a Multi-AZ cluster by making a simple choice when you create the cluster. The choice is simple whether you use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone.\"","poster":"Just_Ninja","timestamp":"1688555820.0","comment_id":"943640"},{"poster":"haazybanj","timestamp":"1727165580.0","upvote_count":"2","comment_id":"896069","content":"Selected Answer: C\nC. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.\n\nEnabling Multi-AZ ensures that the data in the Aurora cluster is replicated across multiple Availability Zones (AZs), providing high availability and durability. During maintenance, the update will be applied to one AZ at a time, allowing the cluster to remain available. Updating the application to use the cluster endpoint for write operations ensures that writes will continue to be directed to the primary instance in the cluster, while updating the reader endpoint for reads allows read traffic to be routed to the appropriate instance. Adding a reader instance or creating a custom ANY endpoint are not necessary for meeting the requirement of minimizing interruption during maintenance."},{"timestamp":"1727165580.0","comment_id":"1075192","upvote_count":"1","content":"Selected Answer: C\nTo meet the requirement of keeping the Aurora cluster available with the least possible interruption during the maintenance window, the DevOps engineer should choose option C: Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.\n\nOption A is incorrect because adding a reader instance alone does not provide high availability during maintenance, and updating the application to use the reader endpoint for reads is unnecessary when the Multi-AZ option is enabled","poster":"koenigParas2324"},{"poster":"alexleely","timestamp":"1727165520.0","comment_id":"1176201","upvote_count":"2","content":"Selected Answer: A\nOption A is the correct choice, adding a reader instance after provisioning is the same as setting a Multi-AZ during creation. In the event that the primary fails, the reader instance will be promoted to do both reading and writing automatically. \n\nAdditionally, reader instance can also be used for read activity if you use the cluster reader endpoint which you can serve to user/application closer to the region for better performance."},{"poster":"vietnguyen2","comment_id":"1195752","content":"Selected Answer: A\n\"You can set up a Multi-AZ DB cluster by making a simple choice when you create the cluster. You can use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also convert an existing Aurora DB cluster into a Multi-AZ DB cluster by adding a new reader DB instance and specifying a different Availability Zone.\"","upvote_count":"1","timestamp":"1727165520.0"},{"content":"Selected Answer: A\nTo meet the requirements of the given scenario, the DevOps engineer should do the following:\n\nAdd a reader instance to the Aurora cluster:\n\nThis will allow the application to offload read operations to the reader instance, reducing the load on the primary instance.\nThe application should be updated to use the Aurora cluster endpoint for write operations and the reader endpoint for read operations.\nThe engineer should not turn on the Multi-AZ option on the Aurora cluster.\n\nMulti-AZ is used to provide high availability and failover capabilities, but it does not necessarily minimize interruption during a maintenance window.\nAdding a reader instance is a more appropriate solution to maintain availability and distribute the read workload.\nTherefore, the correct option is A. Add a reader instance to the Aurora cluster and update the application to use the appropriate endpoints for read and write operations.","poster":"Rizwan_Shaukat","comment_id":"1204766","upvote_count":"2","timestamp":"1727165520.0"},{"comment_id":"1233520","upvote_count":"1","content":"Selected Answer: A\nYou cannot change the az option after creation but can deploy a reader instance in another az and use it for reading and writing in your main instance.","poster":"Rahul369","timestamp":"1718870880.0"},{"upvote_count":"1","content":"There isn't a specific Multi-AZ mode for Aurora it's multi-AZ by default (it uses all three AZs). So I think A or B. A is the most commonly used method but I think B offers less disruption because requests are always routed to the instance with greater availability.","timestamp":"1718700720.0","comment_id":"1232342","poster":"Malcnorth59"},{"poster":"c3518fc","timestamp":"1715277240.0","upvote_count":"1","content":"Selected Answer: D\nBy enabling Multi-AZ deployment, creating a custom ANY endpoint, and updating the application to use this endpoint for all read and write operations, the DevOps engineer can ensure that the Aurora cluster remains available during the maintenance window with minimal interruption. The application will be able to transparently connect to the available instances (primary or read-only replica), and Aurora will automatically fail over to the read-only replica if the primary instance becomes unavailable during the maintenance process.","comment_id":"1209023"},{"poster":"01037","content":"Selected Answer: B\nCan't tell the difference between A and C except Multi-AZ, both should be working if only read is needed during maintenance window.\nAnd also not understand why only read is needed when people choose them.\nSome say there is no custom ANY endpoint, I think it only means you can choose any instance or instances to that endpoint.\nSo I go with B","comment_id":"1206778","timestamp":"1714885620.0","upvote_count":"1"},{"content":"answer A).\nthe question doesn't mention what DB is behind the Aurora.\nMulti-AZ config avoids downtime EXCEPT MySQL/MariaDB.\nSo the question mentions \"the least possible interruption\", then A) is the appropriate one","poster":"Mackn","timestamp":"1711160160.0","upvote_count":"1","comment_id":"1180469"},{"upvote_count":"1","timestamp":"1708445580.0","content":"is corret is ='c' - > 'a' is not \nThis option leverages Aurora's built-in high availability and failover mechanisms to ensure minimal interruption. By using the cluster endpoint for writes, the application automatically writes to the primary instance. In case of maintenance or failure, Aurora handles failover to another instance with minimal downtime. The reader endpoint distributes read traffic across available replicas, enhancing read scalability and availability without affecting write operations. This setup ensures that the application remains as available as possible during maintenance","comment_id":"1154839","poster":"kyuhuck"},{"timestamp":"1706363340.0","upvote_count":"1","comment_id":"1133390","poster":"thanhnv142","content":"A is good"},{"content":"A is correct.","poster":"Jonalb","timestamp":"1705816440.0","upvote_count":"1","comment_id":"1127649"},{"timestamp":"1704637920.0","comment_id":"1115912","content":"Agree answer A. There is no ANY custom endpoint and multy-AZ can be set up during cluster creation","upvote_count":"1","poster":"yuliaqwerty"},{"content":"'A' - since Multi AZ has to be setup when the cluster is created. It cannot be updated later.","upvote_count":"2","comments":[{"content":"I agree with you. C D is the wrong selection. because we can't enable Multi-AZ after cluster is created","comment_id":"1177941","upvote_count":"1","poster":"phu0298","timestamp":"1710912780.0"}],"timestamp":"1704233640.0","comment_id":"1112319","poster":"n00b2023"},{"timestamp":"1701010860.0","upvote_count":"1","content":"A. Add a Reader Instance; Use Cluster Endpoint for Writes, Reader Endpoint for Reads:\n-during the maintenance of the primary instance, write operations might be affected.\n*B. Add a Reader Instance; Use Custom ANY Endpoint for Reads and Writes:\n- A custom ANY endpoint in Aurora can route read/write traffic based on custom criteria. It can be configured to send write traffic to the primary instance and read traffic to the reader instance.This setup allows for continuous read availability and write operations during maintenance, assuming the primary instance is available.\nC. Enable Multi-AZ; Use Cluster Endpoint for Writes, Reader Endpoint for Reads:\n-Amazon Aurora is inherently designed for high availability and does not have a \"Multi-AZ\" option like RDS.\nD. Enable Multi-AZ; Use Custom ANY Endpoint for Reads and Writes:\n-Aurora does not have a \"Multi-AZ\" option to turn on","poster":"wem","comment_id":"1080723"},{"poster":"wem","content":"Option C, turning on the Multi-AZ option and using the cluster endpoint for writes and the reader endpoint for reads, seems to be the best fit. It directly addresses the need for high availability during maintenance without overcomplicating the setup with a custom ANY endpoint, which may not be necessary in this context. The Multi-AZ feature in Aurora is specifically designed for scenarios like maintenance windows, where minimal downtime and high availability are crucial.","upvote_count":"1","timestamp":"1700398500.0","comment_id":"1074619"},{"poster":"HeyBlinkin","upvote_count":"2","comment_id":"1069641","timestamp":"1699900920.0","content":"Selected Answer: A\nIt can only be A."},{"timestamp":"1691838660.0","poster":"ixdb","comment_id":"979329","upvote_count":"3","content":"why not B? https://awscli.amazonaws.com/v2/documentation/api/latest/reference/rds/create-db-cluster-endpoint.html\nCreates a new custom endpoint and associates it with an Amazon Aurora DB cluster."},{"upvote_count":"2","comment_id":"933418","poster":"FunkyFresco","content":"Selected Answer: A\nOption A!","timestamp":"1687685340.0"},{"comment_id":"932375","upvote_count":"5","timestamp":"1687596600.0","poster":"tartarus23","content":"Selected Answer: A\n(A) Amazon Aurora is designed to distribute the read workload across multiple read replica instances when you connect to the reader endpoint. The cluster endpoint always points to the current primary instance for the DB cluster. Therefore, by adding a reader instance to the Aurora cluster and updating the application to use the cluster endpoint for writes and reader endpoint for reads, the application can continue to operate with minimal interruption during the maintenance window."},{"upvote_count":"4","comment_id":"925560","timestamp":"1686950880.0","poster":"allen_devops","comments":[{"poster":"Sisanda_giiven","upvote_count":"1","content":"You can turn an existing cluster to multi-az : https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html#Concepts.AuroraHighAvailability.Instances","timestamp":"1706167380.0","comment_id":"1131445"}],"content":"The correct answer is A. For option c and d, you cannot turn an existing cluster to multi-az by turning on the multi-az option. You can turn it to Multi-AZ by adding a reader instance. Refer to https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n\nFor option b, the custom endpoint is pointed to a group of db instances without considering read/write query. You should add writer and reader instance to the same endpoint. it works to replace reader endpoints normally."},{"timestamp":"1686567780.0","comment_id":"921366","content":"The answers are ambiguous. Conform AWS, \"For Amazon Aurora adding a reader instance is how we provide multi-az recovery.\"\n\nhttps://repost.aws/questions/QUwJB61VWcRFiS1WtzEJNzqg/unable-to-configure-multi-az-deployment-for-existing-aurora-mysql\n\nIn this case, the \"A\" and \"C\" answers are the same","upvote_count":"3","poster":"kacsabacsi78"},{"poster":"SanChan","content":"Selected Answer: A\noption C is the most straightforward and efficient solution to keep the Aurora cluster available with the least possible interruption during the maintenance window.\nBut According to the AWS documentation, Multi-AZ cannot be enabled after an Aurora cluster has been created. \nSo A is the answer","timestamp":"1686534840.0","upvote_count":"4","comment_id":"921060"},{"content":"Either A or C.\nAbout A, if the new read replica is created in the same AZ as the primary, this option will be wrong.\nAbout C, if “turn on Multi-AZ option in Aurora Cluster” means “make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone”, then this option is right. Otherwise, it is wrong because cannot turn on the Multi-AZ option of an Aurora Cluster after creation.","upvote_count":"2","poster":"rhinozD","timestamp":"1686266100.0","comment_id":"918737"},{"content":"Selected Answer: A\nA is the best option. C and D are not working for Aurora as you can't change it to Multi-AZ after creation (like other RDS). \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Modifying.html#Aurora.Modifying.SettingsNotApplicable","upvote_count":"2","comment_id":"918062","timestamp":"1686215940.0","poster":"madperro"},{"timestamp":"1686154140.0","comments":[{"upvote_count":"2","poster":"Manny20","content":"I apologize. In Amazon Aurora, the Multi-AZ configuration is set at the time of cluster creation and cannot be modified for an existing cluster. Therefore, Option C is not a valid solution for meeting the given requirements. So answer A should be correct. By adding a reader instance to the Aurora cluster, you create a read replica that can handle read traffic and offload some of the read workload from the primary instance.","comment_id":"917391","timestamp":"1686154560.0"}],"poster":"Manny20","upvote_count":"1","content":"Option C is the correct answer. \nSelect the Aurora cluster: Locate and select the Aurora cluster for which you want to enable Multi-AZ.\nChoose \"Instance actions\": From the \"Actions\" dropdown menu, select \"Modify\".\n Enable Multi-AZ: In the \"Modify DB Cluster\" page, scroll down to the \"Availability & durability\" section. Check the box next to \"Enable Multi-AZ deployment\" to enable Multi-AZ.","comment_id":"917384"},{"content":"Selected Answer: A\nA- Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads. MultiAZ is not available for existing DB","upvote_count":"2","timestamp":"1685965500.0","comment_id":"915400","poster":"BasselBuzz"},{"upvote_count":"3","comment_id":"910277","content":"Selected Answer: A\nFor the operation of the question to succeed, you need an extra instance (because one is supposed to go down for maintenance), you can add it in the same of different AZ.","poster":"bcx","timestamp":"1685453280.0"},{"comment_id":"906862","upvote_count":"1","poster":"Kiroo","content":"Selected Answer: D\nBased on this document, the best option to handle há for the maintenance in the given case is to have a custom endpoint to “balance” bd connection to any avaliable instance so it seems that it’s D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html","timestamp":"1685045580.0"},{"upvote_count":"2","poster":"qan1257","comment_id":"905502","comments":[{"comment_id":"906861","content":"Is multi az not multi region.\nThe definition of the product is that it allows multi az \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html","upvote_count":"1","timestamp":"1685045220.0","poster":"Kiroo"}],"content":"Selected Answer: A\nThe answer is A.\n\nThis is an Aurora cluster, there is no option to enable Multi-AZ. Users can only enable Muti-AZ by adding a read replica (up to 15 replicas).","timestamp":"1684905900.0"},{"content":"Selected Answer: C\nI think C is better, necause enabling Multi-AZ (or Global database ) allows users accessing with high availability. https://aws.amazon.com/es/rds/aurora/features/","comment_id":"883733","timestamp":"1682699580.0","poster":"mgonblan","upvote_count":"2"},{"timestamp":"1682312640.0","upvote_count":"2","content":"Selected Answer: C\nThe answer is C. \n\nEnabling Multi-AZ would provide high availability by automatically replicating data to a standby instance in a different availability zone. During the maintenance window, Amazon RDS would automatically failover to the standby instance with minimal downtime. Updating the application to use the Aurora cluster endpoint for write operations and the Aurora cluster's reader endpoint for reads would ensure that the application continues to function properly during the maintenance window.","poster":"5aga","comments":[{"upvote_count":"2","poster":"qan1257","timestamp":"1684906560.0","content":"The answer should be A.\n\nBecause there is no Multi-AZ option on the Aurora cluster.\n\n\"You can set up a Multi-AZ cluster by making a simple choice when you create the cluster. The choice is simple whether you use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone.\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html","comment_id":"905508"}],"comment_id":"879035"},{"timestamp":"1682224440.0","poster":"bakamon","upvote_count":"2","content":"Selected Answer: A\nOption A is correct \nOption B : Any endpoint is not required\nOption C : turning on Multi AZ does not create a read instance\nOption D : turning on multi az doesnt create a read instance","comment_id":"877865"},{"comment_id":"870442","timestamp":"1681501980.0","poster":"alce2020","upvote_count":"3","content":"Option C is the correct solution to meet the company's requirements with the least possible interruption. By turning on the Multi-AZ option on the Aurora cluster, a standby replica of the primary instance is created in a different availability zone (AZ). During the maintenance window, the update is first applied to the standby replica. After the update is applied and the replica is promoted to become the new primary instance, the old primary instance is updated. This ensures that the Aurora cluster remains available throughout the maintenance window, with minimal interruption.\n\n\nAdding a reader instance to the Aurora cluster (Options A and B) is not necessary and will not provide any additional benefits in terms of availability during the maintenance window.\n\n\nCreating a custom ANY endpoint for the Aurora cluster (Options B and D) is also not necessary and does not provide any additional benefits in terms of availability during the maintenance window"},{"upvote_count":"3","content":"Option C is the correct answer since enabling the Multi-AZ option on the Aurora cluster provides a highly available environment that can automatically failover in the event of an availability zone failure, making it a good choice to maintain high availability during maintenance windows. Additionally, the application can continue to use the same endpoint for both read and write operations, simplifying the configuration process.","poster":"jqso234","comment_id":"870354","timestamp":"1681492320.0"},{"poster":"henryyvr","timestamp":"1681395960.0","upvote_count":"1","content":"https://repost.aws/knowledge-center/rds-required-maintenance","comment_id":"869484"},{"comment_id":"869483","timestamp":"1681395900.0","poster":"henryyvr","content":"Selected Answer: D\nYou should turn ON Multi-az","upvote_count":"1"},{"timestamp":"1680868860.0","comment_id":"863804","comments":[{"timestamp":"1682148360.0","poster":"ele","upvote_count":"1","content":"Changing to C,\nB is wrong: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\nYou can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone.","comment_id":"877112"}],"upvote_count":"1","poster":"ele","content":"Selected Answer: B\nB, as A will only work for read. C,D are not available for existing cluster."},{"comment_id":"863259","content":"Selected Answer: B\nC and D are not working on existing cluster so it will be A or B, for me they are the same looking to downtime","timestamp":"1680809040.0","poster":"Dimidrol","upvote_count":"1"}],"exam_id":23,"choices":{"D":"Turn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations","A":"Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads.","B":"Add a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations.","C":"Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads."},"answer_images":[],"answer":"A","isMC":true},{"id":"brlQNCceXKNyTqmfzBlN","url":"https://www.examtopics.com/discussions/amazon/view/106268-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":"","question_text":"A company is implementing a well-architected design for its globally accessible API stack. The design needs to ensure both high reliability and fast response times for users located in North America and Europe.\nThe API stack contains the following three tiers:\n\nAmazon API Gateway -\n\nAWS Lambda -\n\nAmazon DynamoDB -\nWhich solution will meet the requirements?","answers_community":["B (100%)"],"timestamp":"2023-04-15 18:04:00","topic":"1","unix_timestamp":1681574640,"question_images":[],"answer_images":[],"answer_ET":"B","isMC":true,"choices":{"D":"Configure Amazon Route 53 to point to API Gateway API in North America using latency-based routing. Configure the API to forward requests to the Lambda function in the Region nearest to the user. Configure the Lambda function to retrieve and update the data in a DynamoDB table.","C":"Configure Amazon Route 53 to point to API Gateway in North America, create a disaster recovery API in Europe, and configure both APIs to forward requests to the Lambda functions in that Region. Retrieve the data from a DynamoDB global table. Deploy a Lambda function to check the North America API health every 5 minutes. In the event of a failure, update Route 53 to point to the disaster recovery API.","A":"Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function.","B":"Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using latency-based routing and health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB global table."},"discussion":[{"content":"Selected Answer: B\nB is the correct solution.\n\nThe requirement is to ensure both high reliability and fast response times for users located in North America and Europe. To meet this requirement, we can use Amazon Route 53 with latency-based routing to direct users to the closest API Gateway endpoint. Additionally, we can use health checks to monitor the health of each endpoint and direct traffic away from unhealthy endpoints.\n\nTo maintain high reliability, we can use AWS Lambda to handle the API requests. Since Lambda scales automatically, we don't need to worry about provisioning or maintaining infrastructure. We can also use DynamoDB as the database since it provides low latency access and automatic scaling.","poster":"haazybanj","timestamp":"1698985320.0","comment_id":"888089","upvote_count":"12"},{"upvote_count":"3","timestamp":"1722348060.0","poster":"thanhnv142","comment_id":"1135939","content":"B is correct: using both latency-based routing and health checks ensures high reliability and fast response\nA: only health check doesnt ensure fast response\nC: All traffic would be routed to one location only (either NA or Europe if NA failed)\nD: All traffic would be routed to one NA only. There would be no entry point which is near Europe users."},{"timestamp":"1719266700.0","comment_id":"1104931","content":"B is correct","upvote_count":"1","poster":"amrit1227"},{"comment_id":"1103849","upvote_count":"2","poster":"z_inderjot","timestamp":"1719106440.0","content":"Selected Answer: B\nB , \nUsing latency based routing for better response time . Having api gateway in each region reduce requrest fligh time. Lambda and Dynamo being a managed serivce scale automatically and having them in same region just reduce latency."},{"timestamp":"1705034640.0","upvote_count":"3","comment_id":"949439","poster":"Snape","content":"Selected Answer: B\nReliability is different that resiliency, hence A and C are out as they are focussing on health checks which is required for the resiliency. DR again for the resiliency"},{"content":"Selected Answer: B\nB is the best solution.","timestamp":"1703071140.0","comment_id":"928310","poster":"madperro","upvote_count":"4"},{"comment_id":"871059","timestamp":"1697385840.0","upvote_count":"2","poster":"alce2020","content":"Selected Answer: B\nB is correct"}],"question_id":302,"exam_id":23,"answer":"B"},{"id":"CzUCbfz00vtMPxQSUsTk","answers_community":["C (78%)","B (22%)"],"answer_ET":"C","answer_images":[],"timestamp":"2023-04-16 22:20:00","question_images":[],"question_text":"A rapidly growing company wants to scale for developer demand for AWS development environments. Development environments are created manually in the AWS Management Console. The networking team uses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for the Amazon VPC and all subnets. The development environments have common standards, such as Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables.\nTo keep up with demand, the DevOps engineer wants to automate the creation of development environments. Because the infrastructure required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will be used to create a template for the development environments.\nWhich approach will meet these requirements and quickly provide consistent AWS environments for developers?","url":"https://www.examtopics.com/discussions/amazon/view/106436-exam-aws-certified-devops-engineer-professional-dop-c02/","question_id":303,"answer_description":"","isMC":true,"discussion":[{"timestamp":"1700070960.0","poster":"ipsingh","upvote_count":"14","content":"C is Correct.\nB is WRONG because intrinsic functions can't be used in Parameter as per AWS documentation. \nhttps://repost.aws/knowledge-center/cloudformation-template-validation","comment_id":"898469","comments":[{"timestamp":"1710475860.0","upvote_count":"2","content":"You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes\nRefer \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html","poster":"aksliveswithaws","comment_id":"1008042"}]},{"timestamp":"1730486280.0","comment_id":"1205171","content":"Selected Answer: C\nC seems right","poster":"seetpt","upvote_count":"1"},{"timestamp":"1722399420.0","poster":"thanhnv142","upvote_count":"4","comment_id":"1136443","content":"C is correct: use nested stacks and Fn::ImportValue intrinsic functions with the resources of the nested stack\nA: no mention of nested stack\nB and D: Fn::ImportValue intrinsic function is used on child template to import values from parent template. So it should not be used on root template, which is the universal parent tempalte of all other templates"},{"comment_id":"928312","content":"Selected Answer: C\nC is the best answer. B is wrong as you need to use Fn::ImportValue in Resource section to import CFN template outputs.","poster":"madperro","timestamp":"1703071500.0","upvote_count":"2"},{"timestamp":"1703063700.0","content":"Selected Answer: C\nI will go with C","comment_id":"928229","upvote_count":"3","poster":"ducluanxutrieu"},{"comment_id":"927956","poster":"tartarus23","comments":[{"upvote_count":"4","comment_id":"986386","poster":"fanq10","timestamp":"1708517340.0","content":"B is WRONG, you cannot use `TemplateURL` to retrieve Network Stack export values."},{"poster":"sb333","comment_id":"966076","upvote_count":"2","content":"B is incorrect. One of the reasons is that intrinsic functions are not allowed in the Parameters section. https://repost.aws/knowledge-center/cloudformation-template-validation","timestamp":"1706509920.0"}],"upvote_count":"2","timestamp":"1703027940.0","content":"Selected Answer: B\nB. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.\n\nNested stacks allow you to modularize and reuse CloudFormation code. For this case, this is helpful because you have common infrastructure components that are shared across environments.\n\nThe Fn::ImportValue function is used to import values that have been exported in another stack. Since the networking team exports the VPC and subnet information, this can be used in the CloudFormation stack to reference those values."},{"poster":"bakamon","timestamp":"1702741380.0","comment_id":"925222","upvote_count":"2","content":"Selected Answer: C\nC is the correct answer"},{"timestamp":"1700765220.0","poster":"lunt","content":"Selected Answer: C\nC ipsingh is absolutely correct","upvote_count":"2","comment_id":"905088"},{"timestamp":"1700084040.0","comments":[{"timestamp":"1701330840.0","upvote_count":"1","comment_id":"910876","poster":"bcx","content":"The template URL in B makes it wrong IMHO. You import the values from an exiting template importing the parameter exported by it."}],"upvote_count":"2","poster":"2pk","comment_id":"898657","content":"Selected Answer: C\nIm 50/50 C or B, but B doesn't provide a clear approach for retrieving the exported values and placing position of Parameters section of the root template, which is not required to place it there, it must declare inside resource. So i think Answer C make sense."},{"content":"Selected Answer: C\nC makes more sense","upvote_count":"2","timestamp":"1699951140.0","poster":"ParagSanyashiv","comment_id":"897318"},{"content":"Selected Answer: C\nc is correct","comment_id":"893260","upvote_count":"2","timestamp":"1699551780.0","poster":"meisme"},{"poster":"haazybanj","timestamp":"1698819120.0","comments":[{"poster":"sb333","timestamp":"1706509980.0","content":"B is incorrect. One of the reasons is that intrinsic functions are not allowed in the Parameters section. https://repost.aws/knowledge-center/cloudformation-template-validation","comment_id":"966080","upvote_count":"2"}],"content":"Selected Answer: B\nB. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.\n\nThis approach is a good fit because it allows the developer to define reusable infrastructure components as nested stacks. To retrieve VPC and subnet values, the intrinsic function Fn::ImportValue is used in the Parameters section of the root template, which retrieves the values from the output of the networking team’s CloudFormation stack. To update existing environments, the CreateChangeSet and ExecuteChangeSet commands are used, which provides a way to easily update the deployed infrastructure. Additionally, the use of nested stacks helps to ensure consistency across environments.","upvote_count":"1","comment_id":"885851"},{"timestamp":"1698012180.0","upvote_count":"2","comment_id":"877664","content":"Selected Answer: B\nOption B is correct. Using nested stacks, the common infrastructure components can be defined in separate templates that can be referenced by the root template. This allows for easy updates and maintenance of the common components. The networking team’s CloudFormation template can be used to export the VPC and subnet values, which can be referenced in the root template using Fn::ImportValue intrinsic functions in the Parameters section. The CreateChangeSet and ExecuteChangeSet commands can be used to update the existing development environments.\n\nOption C is not the best choice because using Fn::ImportValue intrinsic functions with the resources of the nested stack can lead to circular dependencies and make it difficult to manage the infrastructure.","poster":"herohiro"},{"comment_id":"872143","timestamp":"1697487600.0","poster":"alce2020","upvote_count":"2","content":"Selected Answer: C\nC is correct"}],"choices":{"A":"Use Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of environments needed. Use the UpdateStackSet command to update existing development environments.","B":"Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.","D":"Use Fn::ImportValue intrinsic functions in the Parameters section of the root template to retrieve Virtual Private Cloud (VPC) and subnet values. Define the development resources in the order they need to be created in the CloudFormation nested stacks. Use the CreateChangeSet. and ExecuteChangeSet commands to update existing development environments.","C":"Use nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments."},"answer":"C","unix_timestamp":1681676400,"exam_id":23,"topic":"1"},{"id":"Sd9VIr5tLmRCI0eERL1F","choices":{"B":"Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.","D":"Deploy an IAM role to all accounts from a single trusted account. Build a pipeline with AWS CodePipeline with a stage in AWS Lambda to assume the IAM role, and list all EBS volumes in the account. Publish a report to Amazon S3.","A":"Create an AWS CloudFormation template that defines an AWS Inspector rule to check whether EBS encryption is enabled. Save the template to an Amazon S3 bucket that has been shared with all accounts within the company. Update the account creation script pointing to the CloudFormation template in Amazon S3.","C":"Create an SCP in Organizations. Set the policy to prevent the launch of Amazon EC2 instances without encryption on the EBS volumes using a conditional expression. Apply the SCP to all AWS accounts. Use Amazon Athena to analyze the AWS CloudTrail output, looking for events that deny an ec2:RunInstances action."},"unix_timestamp":1681573980,"answer_description":"","answers_community":["B (100%)"],"answer_ET":"B","answer_images":[],"discussion":[{"poster":"YucelFuat","upvote_count":"1","content":"Selected Answer: B\nExam Tip -> Compliance = AWS Config","comment_id":"1278444","timestamp":"1725476760.0"},{"comment_id":"1156201","poster":"dzn","upvote_count":"4","content":"Selected Answer: B\nDeploy CloudFormation template with encrypted-volumes in the ConfigRuleName property, AWS Config will automatically scan the environment and check for unencrypted EBS volumes.","timestamp":"1708585860.0"},{"upvote_count":"1","poster":"thanhnv142","timestamp":"1706688240.0","comment_id":"1136517","content":"B is correct"},{"upvote_count":"3","comment_id":"928314","timestamp":"1687253280.0","content":"Selected Answer: B\nB is the only solution meeting the criteria.","poster":"madperro"},{"content":"Selected Answer: B\nB. Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization, will accomplish the compliance check on all accounts.\n\nOption A is incorrect because an AWS Inspector rule is used to analyze the behavior of the application on the EC2 instance, not to check the encryption of the EBS volume.","upvote_count":"3","timestamp":"1683081240.0","comment_id":"888093","poster":"haazybanj"},{"poster":"haazybanj","upvote_count":"2","content":"Selected Answer: B\nB is right","comment_id":"885852","timestamp":"1682914380.0"},{"poster":"alce2020","comment_id":"871047","upvote_count":"2","content":"Selected Answer: B\nB is the answer","timestamp":"1681573980.0"}],"timestamp":"2023-04-15 17:53:00","question_id":304,"isMC":true,"exam_id":23,"topic":"1","question_text":"A company uses AWS Organizations to manage multiple accounts. Information security policies require that all unencrypted Amazon EBS volumes be marked as non-compliant. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is always present.\nWhich solution will accomplish this?","url":"https://www.examtopics.com/discussions/amazon/view/106267-exam-aws-certified-devops-engineer-professional-dop-c02/","question_images":[],"answer":"B"},{"id":"iIYzNHRbaGvXjXvqrfag","choices":{"C":"Grant inspector:StartAssessmentRun permissions to the IAM role that the DevOps engineer is using.","B":"Associate the target EC2 instances with security groups that allow outbound communication on port 443 to the AWS Systems Manager service endpoint.","E":"Associate the target EC2 instances with instance profiles that grant permissions to communicate with AWS Systems Manager.","A":"Verify that AWS Systems Manager Agent is installed and is running on the EC2 instances that Amazon Inspector is not scanning.","F":"Create a managed-instance activation. Use the Activation Code and the Activation ID to register the EC2 instances.","D":"Configure EC2 Instance Connect for the EC2 instances that Amazon Inspector is not scanning."},"unix_timestamp":1680804180,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/105446-exam-aws-certified-devops-engineer-professional-dop-c02/","timestamp":"2023-04-06 20:03:00","answer":"ABE","question_id":305,"discussion":[{"upvote_count":"6","comment_id":"863212","timestamp":"1696615380.0","poster":"Dimidrol","content":"Selected Answer: ABE\nA b e https://docs.aws.amazon.com/inspector/latest/user/scanning-ec2.html"},{"poster":"dzn","content":"Selected Answer: ABE\nC is not a fundamental solution. Because Inspector is actually able to run, and it is not the same IAM role that DevOps uses.","comment_id":"1156221","upvote_count":"3","timestamp":"1724306040.0"},{"upvote_count":"4","timestamp":"1722409320.0","content":"ABE are correct: Check if SSM agent is installed, check connection and permission of Ec2 that allows access to SSM\nC: no need to grant inspector:StartAssessmentRun permissions because the dev has already finish the scanning task\nD: There is not EC2 instance Connect, only need SSM agent\nF: there is no managed-instance activation","poster":"thanhnv142","comment_id":"1136558"},{"timestamp":"1717929420.0","upvote_count":"3","comment_id":"1091801","poster":"yorkicurke","content":"Selected Answer: ABE\nthe following link explains it all;\nhttps://repost.aws/knowledge-center/systems-manager-ec2-instance-not-appear"},{"upvote_count":"2","timestamp":"1703073420.0","poster":"madperro","content":"Selected Answer: ABE\nABE seem to be prerequisites to work with SSM and Inspector.","comment_id":"928325"},{"poster":"bcx","timestamp":"1701331320.0","upvote_count":"2","comment_id":"910888","content":"Selected Answer: ABE\nA B E is the correct one IMHO"},{"upvote_count":"2","comment_id":"892221","timestamp":"1699458780.0","content":"Selected Answer: ABE\nABE makes more sense.","poster":"ParagSanyashiv"},{"comment_id":"872165","content":"A,B,E are correct https://docs.aws.amazon.com/inspector/latest/user/scanning-ec2.html","poster":"alce2020","timestamp":"1697489040.0","upvote_count":"3"},{"upvote_count":"4","comment_id":"870453","timestamp":"1697314560.0","content":"Selected Answer: ABE\nOption C suggests granting inspector:StartAssessmentRun permissions to the IAM role being used by the DevOps engineer. However, this may not be relevant to the issue of instances not being scanned by Amazon Inspector, as the IAM role may already have the necessary permissions by default.\n\nTherefore, A, B, E is a better choice in this case as it includes the necessary steps to ensure that the instances can communicate with AWS Systems Manager, which is required for Amazon Inspector to scan the instances.","poster":"jqso234"}],"answer_images":[],"exam_id":23,"answers_community":["ABE (100%)"],"answer_ET":"ABE","topic":"1","question_text":"A company is performing vulnerability scanning for all Amazon EC2 instances across many accounts. The accounts are in an organization in AWS Organizations. Each account's VPCs are attached to a shared transit gateway. The VPCs send traffic to the internet through a central egress VPC. The company has enabled Amazon Inspector in a delegated administrator account and has enabled scanning for all member accounts.\nA DevOps engineer discovers that some EC2 instances are listed in the \"not scanning\" tab in Amazon Inspector.\nWhich combination of actions should the DevOps engineer take to resolve this issue? (Choose three.)","isMC":true}],"exam":{"name":"AWS Certified DevOps Engineer - Professional DOP-C02","lastUpdated":"11 Apr 2025","isMCOnly":true,"isBeta":false,"provider":"Amazon","isImplemented":true,"numberOfQuestions":355,"id":23},"currentPage":61},"__N_SSP":true}