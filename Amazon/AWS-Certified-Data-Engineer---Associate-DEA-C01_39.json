{"pageProps":{"questions":[{"id":"SzadKFLfb5b1S658kDok","question_id":191,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/142535-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"B":"Amazon Managed Workflows for Apache Airflow (Amazon MWAA)","A":"Amazon EventBridge","C":"AWS Step Functions","D":"AWS Glue Workflows"},"question_text":"A company uses Amazon EMR as an extract, transform, and load (ETL) pipeline to transform data that comes from multiple sources. A data engineer must orchestrate the pipeline to maximize performance.\n\nWhich AWS service will meet this requirement MOST cost effectively?","discussion":[{"poster":"artworkad","upvote_count":"5","content":"Selected Answer: C\nGlue Workflows is for Glue job orchestration. C is for orchestration with different AWS services.","comment_id":"1230571","timestamp":"1718383440.0"},{"comment_id":"1269398","upvote_count":"1","poster":"hcong","timestamp":"1724152980.0","content":"Selected Answer: B\nAmazon Managed Workflows for Apache Airflow (Amazon MWAA) is the best service for orchestrating complex data pipelines, especially for workloads already using Amazon EMR. Airflow is a powerful workflow orchestration tool that can be integrated with various AWS services, including EMR, to provide flexible scheduling, task dependency management, and monitoring capabilities. Using a hosted Airflow service (MWAA) can reduce administrative overhead while maintaining a familiar workflow orchestration environment."},{"content":"Selected Answer: C\nB is not cost effective, D is only to orchestrate Glue Jobs and Crawlers within AWS Glue itself. Hence C is correct, Step functions is cost effective and can link together your different AWS services.","timestamp":"1721741460.0","upvote_count":"3","poster":"chrispchrisp","comment_id":"1253680"},{"poster":"andrologin","upvote_count":"2","timestamp":"1721157780.0","comment_id":"1249177","content":"Selected Answer: C\nThis is EMR not Glue workflows hence step functions\nEventBridge is best for event driven architecture"},{"upvote_count":"1","timestamp":"1721152740.0","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/big-data/build-a-concurrent-data-orchestration-pipeline-using-amazon-emr-and-apache-livy/","poster":"LR2023","comment_id":"1249139"},{"content":"Selected Answer: D\nThe most cost-effective AWS service for orchestrating an ETL pipeline that maximizes performance is D. AWS Glue Workflows.\n\nAWS Glue is a fully managed ETL service that makes it easy to move data between your data stores. AWS Glue simplifies and automates the difficult and time-consuming tasks of data discovery, conversion mapping, and job scheduling. AWS Glue Workflows allows you to orchestrate complex ETL jobs involving multiple crawlers, jobs, and triggers.\n\nWhile the other services mentioned (Amazon EventBridge, Amazon MWAA, and AWS Step Functions) can be used for workflow orchestration, they are not specifically designed for ETL workloads and may not be as cost-effective for this use case. AWS Glue is designed for ETL workloads, and its workflows feature is specifically designed for orchestrating ETL jobs, making it the most suitable and cost-effective choice.","upvote_count":"2","timestamp":"1719916620.0","comment_id":"1240685","poster":"bakarys"},{"poster":"HunkyBunky","upvote_count":"1","content":"Selected Answer: C\nC - becuase AWS Glue can be used only for glue based ETL jobs","comment_id":"1233782","timestamp":"1718900160.0"},{"comment_id":"1230798","upvote_count":"2","timestamp":"1718435880.0","poster":"tgv","content":"Selected Answer: C\nWhile AWS Glue Workflows are excellent for orchestrating Glue-specific ETL tasks, AWS Step Functions is more suitable for orchestrating an Amazon EMR-based ETL pipeline due to its greater flexibility, broader integration capabilities, and effective cost management. Therefore, the correct choice remains [C]"}],"topic":"1","isMC":true,"exam_id":21,"answer_images":[],"answers_community":["C (76%)","12%","12%"],"timestamp":"2024-06-14 18:44:00","answer_description":"","answer":"C","unix_timestamp":1718383440,"answer_ET":"C"},{"id":"9KEyo1OeYMdXLmlbCDMM","choices":{"A":"Create an AWS Glue job that determines the schema of all ALB access logs and writes the partition metadata to AWS Glue Data Catalog.","C":"Create an AWS Lambda function to transform all ALB access logs. Save the results to Amazon S3 in Apache Parquet format. Partition the metadata. Use Athena to query the transformed data.","B":"Create an AWS Glue crawler that includes a classifier that determines the schema of all ALB access logs and writes the partition metadata to AWS Glue Data Catalog.","D":"Use Apache Hive to create bucketed tables. Use an AWS Lambda function to transform all ALB access logs."},"unix_timestamp":1718436000,"answers_community":["B (90%)","10%"],"isMC":true,"answer_ET":"B","answer":"B","answer_description":"","topic":"1","question_text":"An online retail company stores Application Load Balancer (ALB) access logs in an Amazon S3 bucket. The company wants to use Amazon Athena to query the logs to analyze traffic patterns.\n\nA data engineer creates an unpartitioned table in Athena. As the amount of the data gradually increases, the response time for queries also increases. The data engineer wants to improve the query performance in Athena.\n\nWhich solution will meet these requirements with the LEAST operational effort?","exam_id":21,"timestamp":"2024-06-15 09:20:00","discussion":[{"poster":"PGGuy","timestamp":"1718992980.0","content":"Selected Answer: B\nCreating an AWS Glue crawler (Option B) is the most straightforward and least operationally intensive approach to automatically determine the schema, partition the data, and keep the AWS Glue Data Catalog updated. This ensures Athena queries are optimized without requiring extensive manual management or additional processing steps.","upvote_count":"5","comment_id":"1234751"},{"timestamp":"1721157960.0","comment_id":"1249179","upvote_count":"1","content":"Selected Answer: C\nAWS Crawler with classifiers allow you to determine the schema pattern on files/data that can then be used to partition the data for Athena query optimization","poster":"andrologin"},{"timestamp":"1718436000.0","poster":"tgv","comment_id":"1230799","content":"Selected Answer: B\nAn AWS Glue crawler can automatically determine the schema of the logs, infer partitions, and update the Glue Data Catalog. Crawlers can be scheduled to run at intervals, minimizing manual intervention.","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/amazon/view/142559-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"question_id":192,"question_images":[]},{"id":"D3XBg1g4cPunXac7Mmm5","isMC":true,"topic":"1","exam_id":21,"choices":{"C":"Set up an on-demand AWS Glue workflow so that the data engineer can start the AWS Glue workflow when each file transfer is complete.","A":"Determine when the file transfers usually finish based on previous successful file transfers. Set up an Amazon EventBridge scheduled event to initiate the AWS Glue jobs at that time of day.","D":"Set up an AWS Lambda function that will invoke the AWS Glue Workflow. Set up an event for the creation of an S3 object as a trigger for the Lambda function.","B":"Set up an Amazon EventBridge event that initiates the AWS Glue workflow after every successful S3 File Gateway file transfer event."},"question_images":[],"question_id":193,"timestamp":"2024-06-15 09:25:00","unix_timestamp":1718436300,"answer_ET":"B","answer_description":"","answer":"B","answer_images":[],"question_text":"A company has a business intelligence platform on AWS. The company uses an AWS Storage Gateway Amazon S3 File Gateway to transfer files from the company's on-premises environment to an Amazon S3 bucket.\n\nA data engineer needs to setup a process that will automatically launch an AWS Glue workflow to run a series of AWS Glue jobs when each file transfer finishes successfully.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/142560-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","discussion":[{"timestamp":"1718436300.0","poster":"tgv","content":"Selected Answer: B\nUsing EventBridge directly to trigger the AWS Glue workflow upon S3 events is straightforward and leverages AWS's event-driven architecture, requiring minimal maintenance.","upvote_count":"5","comment_id":"1230802"},{"comment_id":"1249181","content":"Selected Answer: C\nEvent driven architecture with S3 file creation can only be EventBridge","timestamp":"1721158200.0","upvote_count":"1","poster":"andrologin"},{"timestamp":"1720129560.0","upvote_count":"2","comment_id":"1242304","poster":"bakarys","content":"Selected Answer: B\nSetting up an Amazon EventBridge event that initiates the AWS Glue workflow after every successful S3 File Gateway file transfer event would meet these requirements with the least operational overhead.\n\nThis solution is event-driven and does not require manual intervention or reliance on a schedule that might not align with the actual completion time of the file transfers. The AWS Glue workflow is triggered automatically when a new file is added to the S3 bucket, ensuring that the AWS Glue workflow starts processing the new data as soon as itâ€™s available."},{"poster":"bakarys","timestamp":"1719918060.0","upvote_count":"1","content":"Selected Answer: D\nThe solution that will meet these requirements with the least operational overhead is Option D.\n\nSetting up an AWS Lambda function that will invoke the AWS Glue Workflow, and setting up an event for the creation of an S3 object as a trigger for the Lambda function, will ensure that the workflow is automatically initiated each time a file transfer is successfully completed. This approach requires minimal operational overhead as it automates the process and does not require manual intervention or scheduling based on estimated completion times.\n\nOptions A and C involve manual intervention or assumptions about transfer times, which could lead to inefficiencies or inaccuracies. Option B is not feasible because Amazon EventBridge does not directly support triggering events based on S3 File Gateway file transfer events. Therefore, Option D is the most suitable solution.","comment_id":"1240704"},{"timestamp":"1718993040.0","comment_id":"1234753","content":"Selected Answer: B\nSetting up an Amazon EventBridge event (Option B) to initiate the AWS Glue workflow after every successful S3 File Gateway file transfer event is the most efficient solution. It provides real-time automation with minimal operational overhead, ensuring that the Glue workflow starts immediately after the file transfer is complete.","upvote_count":"1","poster":"PGGuy"}],"answers_community":["B (80%)","10%","10%"]},{"id":"qs0SfxzxHTOyetBSxoCb","choices":{"D":"Schedule a monthly job to copy data that is older than 15 months to Amazon S3 Glacier Flexible Retrieval by using the UNLOAD command. Delete the old data from the Redshift cluster. Configure Redshift Spectrum to access historical data from S3 Glacier Flexible Retrieval.","E":"Create a materialized view in Amazon Redshift that combines live, current, and historical data from different sources.","B":"Configure Amazon Redshift Spectrum to query live transactional data that is in the PostgreSQL database.","C":"Schedule a monthly job to copy data that is older than 15 months to Amazon S3 by using the UNLOAD command. Delete the old data from the Redshift cluster. Configure Amazon Redshift Spectrum to access historical data in Amazon S3.","A":"Configure the Amazon Redshift Federated Query feature to query live transactional data that is in the PostgreSQL database."},"exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/142537-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-06-14 20:12:00","question_images":[],"answer":"A","isMC":true,"question_text":"A retail company uses Amazon Aurora PostgreSQL to process and store live transactional data. The company uses an Amazon Redshift cluster for a data warehouse.\n\nAn extract, transform, and load (ETL) job runs every morning to update the Redshift cluster with new data from the PostgreSQL database. The company has grown rapidly and needs to cost optimize the Redshift cluster.\n\nA data engineer needs to create a solution to archive historical data. The data engineer must be able to run analytics queries that effectively combine data from live transactional data in PostgreSQL, current data in Redshift, and archived historical data. The solution must keep only the most recent 15 months of data in Amazon Redshift to reduce costs.\n\nWhich combination of steps will meet these requirements? (Choose two.)","question_id":194,"answer_ET":"A","discussion":[{"comment_id":"1230684","upvote_count":"7","poster":"lalitjhawar","timestamp":"1718402760.0","content":"Option A (A): Configuring Amazon Redshift Federated Query allows Redshift to directly query the live transactional data in the PostgreSQL database without needing to import it. This ensures that you can access the most recent live data efficiently.\n\nOption C (C): Scheduling a monthly job to copy data older than 15 months to Amazon S3 and then using Amazon Redshift Spectrum to access this historical data provides a cost-effective way to manage storage. This ensures that only the most recent 15 months of data are kept in Amazon Redshift, reducing storage costs. The historical data is still accessible via Redshift Spectrum for analytics queries."},{"poster":"Palee","comment_id":"1398916","upvote_count":"1","content":"Selected Answer: D\nOption A and D. \nOption C doesn't talk about archiving Historical data","timestamp":"1742053080.0"},{"comment_id":"1323939","upvote_count":"2","timestamp":"1733735280.0","content":"Selected Answer: A\nThe correct combination of steps is:\n\nA. Configure the Amazon Redshift Federated Query feature to query live transactional data that is in the PostgreSQL database.\n\nThis feature allows Amazon Redshift to directly query live transactional data in the PostgreSQL database without moving the data, enabling seamless integration with the data warehouse.\nC. Schedule a monthly job to copy data that is older than 15 months to Amazon S3 by using the UNLOAD command. Delete the old data from the Redshift cluster. Configure Amazon Redshift Spectrum to access historical data in Amazon S3.\n\nThis step archives older data to Amazon S3, which is more cost-effective than storing it in Redshift. Redshift Spectrum allows querying this archived data directly from S3, ensuring analytics queries can still access historical data.","poster":"Vidhi212"},{"comment_id":"1322618","content":"Selected Answer: A\nA & C. Redshift spectrum cant read from glacier","upvote_count":"1","timestamp":"1733460600.0","poster":"SambitParida"},{"timestamp":"1729599420.0","upvote_count":"1","poster":"rsmf","content":"Selected Answer: A\nA & C is the best choice","comment_id":"1301570"},{"poster":"mohamedTR","upvote_count":"1","timestamp":"1728290880.0","content":"Selected Answer: A\nA & C: allows exporting Redshift data to Amazon S3 and ability to frequent access","comment_id":"1294147"},{"content":"Selected Answer: A\nA / C is a best choice","timestamp":"1718900580.0","comment_id":"1233788","upvote_count":"1","poster":"HunkyBunky"},{"timestamp":"1718445780.0","upvote_count":"4","poster":"artworkad","content":"Selected Answer: A\nAC is correct. D is not correct, because Redshift Spectrum cannot read from S3 Glacier Flexible Retrieval.","comment_id":"1230883"},{"upvote_count":"4","timestamp":"1718437260.0","comment_id":"1230807","poster":"tgv","content":"Selected Answer: A\nChoice A ensures that live transactional data from PostgreSQL can be accessed directly within Redshift queries.\n\nChoice C archives historical data in Amazon S3, reducing storage costs in Redshift while still making the data accessible via Redshift Spectrum.\n\n(to Admin: I can't select multiple answers on the voting comment)"},{"poster":"GHill1982","content":"Correct answer is A and C.","timestamp":"1718388720.0","comment_id":"1230611","upvote_count":"2"}],"topic":"1","answers_community":["A (93%)","7%"],"answer_images":[],"unix_timestamp":1718388720,"answer_description":""},{"id":"9Mnl748IBDSbnORjNFvG","isMC":true,"topic":"1","answer":"A","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/142562-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"question_text":"A manufacturing company has many IoT devices in facilities around the world. The company uses Amazon Kinesis Data Streams to collect data from the devices. The data includes device ID, capture date, measurement type, measurement value, and facility ID. The company uses facility ID as the partition key.\n\nThe company's operations team recently observed many WriteThroughputExceeded exceptions. The operations team found that some shards were heavily used but other shards were generally idle.\n\nHow should the company resolve the issues that the operations team observed?","timestamp":"2024-06-15 09:43:00","answers_community":["A (100%)"],"question_id":195,"answer_description":"","question_images":[],"answer_images":[],"discussion":[{"timestamp":"1718437380.0","comment_id":"1230808","poster":"tgv","upvote_count":"6","content":"Selected Answer: A\nThe best solution to resolve the issue of uneven shard usage and WriteThroughputExceeded exceptions is to balance the load more evenly across the shards. This can be effectively achieved by changing the partition key to something that ensures a more uniform distribution of data across the shards."},{"timestamp":"1720129980.0","comment_id":"1242308","upvote_count":"2","poster":"bakarys","content":"Selected Answer: A\nThe correct answer is **A. Change the partition key from facility ID to a randomly generated key.**\n\nAmazon Kinesis Data Streams uses the partition key that you specify to segregate the data records in the stream into shards. If the company uses the facility ID as the partition key, and if some facilities produce more data than others, then the data will be unevenly distributed across the shards. This can lead to some shards being heavily used while others are idle, and can cause `WriteThroughputExceeded` exceptions.\n\nBy changing the partition key to a randomly generated key, the data records are more likely to be evenly distributed across all the shards, which can help to avoid the issue of some shards being heavily used and others being idle. This solution requires the least operational overhead and does not involve increasing costs (as in option B), archiving data (which might not be desirable or feasible, as in option C), or changing to a partition key that might also lead to uneven distribution (as in option D)."},{"poster":"didorins","comment_id":"1241374","timestamp":"1720006140.0","upvote_count":"2","content":"Selected Answer: A\nD is not good, because you're effectively making things worse by partitioning by date. My answer is A"}],"unix_timestamp":1718437380,"choices":{"A":"Change the partition key from facility ID to a randomly generated key.","B":"Increase the number of shards.","C":"Archive the data on the producer's side.","D":"Change the partition key from facility ID to capture date."}}],"exam":{"id":21,"isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":207,"isImplemented":true,"provider":"Amazon","name":"AWS Certified Data Engineer - Associate DEA-C01","isBeta":false},"currentPage":39},"__N_SSP":true}