{"pageProps":{"questions":[{"id":"hOJTRxNCOziohwYGLr24","question_id":221,"answer_ET":"C","topic":"1","question_images":[],"discussion":[{"timestamp":"1730556780.0","poster":"Peter_Hsieh","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/shadow-tests.html","upvote_count":"2","comment_id":"1205500"},{"content":"Selected Answer: C\nShadow deployment is new technique provided by sagemaker","upvote_count":"1","comment_id":"1165239","poster":"vkbajoria","timestamp":"1725408780.0"},{"upvote_count":"3","poster":"kyuhuck","timestamp":"1723025640.0","comment_id":"1143307","content":"Selected Answer: C\nAnswer: C\nExplanation:\nThe best solution for this scenario is to use shadow deployment, which is a technique that allows the\ncompany to run the new experimental model in parallel with the existing model, without exposing it\nto the end users. In shadow deployment, the company can route the same user requests to both\nmodels, but only return the responses from the existing model to the users. The responses from the\nnew experimental model are logged and analyzed for quality and performance metrics, such as\naccuracy, latency, and resource consumption12. This way, the company can validate the new\nexperimental model in a production environment, without affecting the current live traffic or user\nexperience"},{"upvote_count":"1","content":"Selected Answer: C\nShadow deployment consists of releasing version B alongside version A, fork version Aâ€™s incoming requests, and send them to version B without impacting production traffic. This is particularly useful to test production load on a new feature and measure model performance on a new version without impacting current live traffic.\n\nsource: https://aws.amazon.com/blogs/machine-learning/deploy-shadow-ml-models-in-amazon-sagemaker/","comment_id":"1142591","timestamp":"1722970680.0","poster":"delfoxete"}],"isMC":true,"unix_timestamp":1707253080,"answer_images":[],"choices":{"B":"Canary release","D":"Blue/green deployment","A":"A/B testing","C":"Shadow deployment"},"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/133096-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"C","exam_id":26,"timestamp":"2024-02-06 21:58:00","answers_community":["C (100%)"],"question_text":"An insurance company developed a new experimental machine learning (ML) model to replace an existing model that is in production. The company must validate the quality of predictions from the new experimental model in a production environment before the company uses the new experimental model to serve general user requests.\n\nNew one model can serve user requests at a time. The company must measure the performance of the new experimental model without affecting the current live traffic.\n\nWhich solution will meet these requirements?"},{"id":"bQj9orGMcqgOxJqQxn4C","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/133097-exam-aws-certified-machine-learning-specialty-topic-1/","answer_images":[],"choices":{"A":"Perform incremental training to update the model. Activate Amazon SageMaker Model Monitor to detect model performance issues and to send notifications.","D":"Use only data from the previous several months to perform incremental training to update the model. Use Amazon SageMaker Model Monitor to detect model performance issues and to send notifications.","C":"Use Amazon SageMaker Debugger with appropriate thresholds. Configure Debugger to send Amazon CloudWatch alarms to alert the team. Retrain the model by using only data from the previous several months.","B":"Use Amazon SageMaker Model Governance. Configure Model Governance to automatically adjust model hyperparameters. Create a performance threshold alarm in Amazon CloudWatch to send notifications."},"answer_ET":"A","exam_id":26,"question_id":222,"timestamp":"2024-02-06 22:00:00","question_images":[],"isMC":true,"topic":"1","answer_description":"","discussion":[{"content":"Selected Answer: D\nAvoids the model being influenced by outdated patterns that no longer apply. For example, if we enter a recession in a few months, we should not be using pricing data from last year.","upvote_count":"1","comment_id":"1554038","timestamp":"1743898860.0","poster":"nick3332"},{"poster":"f3a4b7c","content":"Selected Answer: D\nWhy not D?","comment_id":"1221455","comments":[{"upvote_count":"1","comment_id":"1260784","timestamp":"1722800100.0","poster":"rdiaz","content":"ChatGPT:\n\nWhile option D includes the use of Amazon SageMaker Model Monitor, it suggests using only the most recent data for incremental training. This could result in the loss of valuable information from older data, which might still be relevant. Incremental training should ideally update the model with new data while retaining useful insights from the entire dataset, not just the recent months."}],"timestamp":"1717055940.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1709848020.0","comment_id":"1168380","content":"Selected Answer: A\nIncremental training involves updating the model with new data over time. Amazon SageMaker Model Monitor is a suitable choice for monitoring model performance. It can detect drift and anomalies in real-time predictions and send notifications.","poster":"AIWave"},{"content":"Selected Answer: A\nOption A makes more sense in this case","poster":"Adzz","comment_id":"1160599","timestamp":"1709038560.0","upvote_count":"2"},{"timestamp":"1707253200.0","comment_id":"1142592","poster":"delfoxete","upvote_count":"2","content":"Selected Answer: A\nA cover all the requirements"}],"unix_timestamp":1707253200,"answers_community":["A (75%)","D (25%)"],"question_text":"A company deployed a machine learning (ML) model on the company website to predict real estate prices. Several months after deployment, an ML engineer notices that the accuracy of the model has gradually decreased.\n\nThe ML engineer needs to improve the accuracy of the model. The engineer also needs to receive notifications for any future performance issues.\n\nWhich solution will meet these requirements?"},{"id":"jWOwFuyLULImliahO3D9","question_images":[],"topic":"1","discussion":[{"timestamp":"1632328140.0","comment_id":"21884","content":"Answer is B","poster":"DonaldCMLIN","upvote_count":"31","comments":[{"timestamp":"1633727040.0","poster":"Antriksh","comments":[{"comments":[{"poster":"GeeBeeEl","comment_id":"149942","upvote_count":"5","content":"Actually question says \"The source systems send data in CSV format in real time The Data Engineering team wants to transform the data to the Apache Parquet format before storing it on Amazon S3\" same as saying data must be converted real time","timestamp":"1634482140.0"}],"comment_id":"147766","timestamp":"1634302500.0","content":"Even if the exam's answer is based on solution before AWS implemented the capability of AWS glue to process streaming data, this answer is still correct as Kinesis would output the data to S3 and Glue will pick it up from there and covert to parquet. Question does not say data must be converted to parquet in real time, it only says the csv data is received as a stream in real time.","upvote_count":"2","poster":"scuzzy2010"},{"poster":"zzeng","upvote_count":"6","comments":[{"upvote_count":"3","comment_id":"754359","timestamp":"1671813720.0","poster":"hamimelon","content":"This link is in Japanese"}],"timestamp":"1634117820.0","comment_id":"137853","content":"AWS Glue can do it now (2020 May)\nhttps://aws.amazon.com/jp/blogs/news/new-serverless-streaming-etl-with-aws-glue/"}],"content":"you cannot use AWS glue for streaming data. Clearly B is incorrect.","upvote_count":"3","comment_id":"112220"},{"upvote_count":"7","content":"the Approve Of B\nhttps://aws.amazon.com/blogs/aws/new-serverless-streaming-etl-with-aws-glue/","poster":"OmarSaadEldien","timestamp":"1635588720.0","comment_id":"248770"}]},{"comments":[{"comments":[{"poster":"samy666","upvote_count":"2","comments":[{"poster":"AdolinKholin","content":"But there's a D in Lambda","comment_id":"681600","upvote_count":"3","timestamp":"1664362500.0"}],"content":"But there is no Lambda in D","timestamp":"1651743720.0","comment_id":"597223"}],"poster":"zzeng","comment_id":"137849","content":"https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\nYou are right.\n\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\nIf you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first","upvote_count":"8","timestamp":"1634092800.0"}],"comment_id":"27151","timestamp":"1632536460.0","upvote_count":"24","content":"D is wrong as kinesis firehose can convert from JSON to parquet but here we have CSV.\nB is correct and here is another proof link: https://medium.com/searce/convert-csv-json-files-to-apache-parquet-using-aws-glue-a760d177b45f","poster":"vetal"},{"upvote_count":"3","poster":"LalBSingh","content":"Selected Answer: D\nKinesis Data Firehose supports real-time streaming ingestion and can automatically convert CSV to Parquet before storing it in S3.","comment_id":"1358191","timestamp":"1739868720.0"},{"timestamp":"1739652900.0","comment_id":"1357040","poster":"JonSno","comments":[{"content":"I take this back .. ans shd be B.. on researching further it is JSON or ORC to Parque that KDS supports.. So answer is B - not optimal but close to suitable\n. Amazon Kinesis Data Streams + AWS Glue AWS Glue can batch-process CSV and convert it to Parquet for S3. However, Glue is batch-oriented, not real-time.","comment_id":"1357044","timestamp":"1739653140.0","upvote_count":"1","poster":"JonSno"}],"upvote_count":"1","content":"Selected Answer: D\nAmazon Kinesis Data Streams + Amazon Kinesis Data Firehose\nEffort: Lowest effort \nWhy?\nAmazon Kinesis Data Firehose natively supports real-time CSV ingestion and automatic conversion to Parquet.\nFully managed, serverless, and directly integrates with Amazon S3.\nRequires zero infrastructure management compared to other solutions."},{"content":"Selected Answer: B\nAlthough I'd go with Glue and option B I'm pretty sure that this is one of those \"15 unscored questions that do not affect your score. AWS collects information about performance on these unscored questions to evaluate these questions for future use as scored questions\"\n\nJust for fun I asked perplexity, chatgpt, gemini, deepseek and claude: all gave D as first response\n\nWhen I pointed out that \"according to this https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html Kinesis can't convert directly cvs to parquet. It needs a Lambda\" each model responded in a different way (some of them contradictory). \n\nMy reasoning is that D (Kinesis + Firehose) is incorrect because Firehose does not support direct CSV-to-Parquet conversion and needs a Lambda not mentioned in the option. But discussing about questions like this one is nothing but I big waste of time ;-P","upvote_count":"1","timestamp":"1739564580.0","comment_id":"1356590","poster":"liquen14"},{"comment_id":"1329091","upvote_count":"1","content":"Selected Answer: D\nD\nKinesis Data Firehose is designed specifically for streaming data delivery to destinations like S3. It has built-in support for data format conversion, including CSV to Parquet. This eliminates the need for managing separate transformation services like Glue or Spark. The setup is significantly simpler: you configure a Firehose delivery stream, specify the data format conversion, and point it to your S3 bucket.\n\nTherefore, option D requires the least implementation effort because it leverages a fully managed service (Kinesis Data Firehose) with built-in functionality for data format conversion.","poster":"AbimbolaOlaniran","timestamp":"1734630900.0"},{"comment_id":"1327613","poster":"venksters","timestamp":"1734380160.0","upvote_count":"1","content":"Selected Answer: B\nAmazon Kinesis Data Firehose can only convert from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3."},{"poster":"TinTinAWS","comment_id":"1290379","upvote_count":"1","timestamp":"1727473920.0","content":"Answer B, \nYes, Amazon Kinesis Data Firehose can convert CSV to Apache Parquet, but you need to use a Lambda function to transform the CSV to JSON first: here the question is least effort to build, so B is the right answer with least effort to build the solution"},{"content":"Selected Answer: B\nUse Amazon Kinesis Data Streams to ingest customer data and configure a Kinesis Data Firehose delivery stream as a consumer to convert the data into Apache Parquet is incorrect. Although this could be a valid solution, it entails more development effort as Kinesis Data Firehose does not support converting CSV files directly into Apache Parquet, unlike JSON.","poster":"Keya","timestamp":"1727163780.0","comment_id":"1056368","upvote_count":"2"},{"content":"Selected Answer: B\nAmazon Kinesis Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first.\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html","upvote_count":"1","comment_id":"1076055","timestamp":"1727163780.0","poster":"geoan13"},{"comment_id":"1209602","content":"Selected Answer: D\nBetween B and D chose D.\nBecause Firehose can't handle csv directly.","timestamp":"1715404740.0","upvote_count":"1","poster":"rav009","comments":[{"comment_id":"1209603","timestamp":"1715404800.0","content":"Between B and D chose B.\nBecause Firehose can't handle csv directly.","poster":"rav009","upvote_count":"1"}]},{"poster":"s_k_aws","content":"Answer is B. \nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html \n\"If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first.\"","comment_id":"1179405","timestamp":"1711038000.0","upvote_count":"1"},{"poster":"chewasa","upvote_count":"1","timestamp":"1710235800.0","content":"Selected Answer: B\nu need glue to convert to parquet","comment_id":"1171571"},{"poster":"0c47783","comment_id":"1163697","timestamp":"1709321160.0","content":"D for sure, Firehose can convert csv to parquet","upvote_count":"3"},{"poster":"vkbajoria","upvote_count":"1","content":"Answer is unfortunately B. firehose cannot convert coma separated CSV to parquet directly.","timestamp":"1708391460.0","comment_id":"1154395"},{"comment_id":"1147258","timestamp":"1707658680.0","poster":"kyuhuck","upvote_count":"3","comments":[{"timestamp":"1722848160.0","content":"Kinesis Data Firehose doesn't convert anything, it rather calls a lambda function to do so which is the overhead we want to avoid. B is the correct answer.","upvote_count":"1","comment_id":"1260991","poster":"shammous"}],"content":"Selected Answer: D\nb is not goog but - >given the context of \"finding the solution that requires the least effort to implement,\" option D is the most suitable choice. Ingesting data from Amazon Kinesis Data Streams and using Amazon Kinesis Data Firehose to convert the data to Parquet format is a serverless approach. It allows for automatic data transformation and storage in Amazon S3 without the need for additional development or management of data conversion logic. Therefore, under the given conditions, option D is considered the solution that requires the \"least effort\" to implement"},{"comment_id":"1143465","upvote_count":"2","timestamp":"1707319740.0","comments":[{"comment_id":"1152633","upvote_count":"3","timestamp":"1708182780.0","poster":"Jonfernz","content":"Firehose cannot natively do the conversion. It requires a Lambda function for that purpose."}],"content":"Selected Answer: D\nAmazon Kinesis Data Streams is a service that can capture, store, and process streaming data in real\ntime. Amazon Kinesis Data Firehose is a service that can deliver streaming data to various\ndestinations, such as Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service. Amazon Kinesis\nData Firehose can also transform the data before delivering it, such as converting the data format,\ncompressing the data, or encrypting the data. One of the supported data formats that Amazon\nKinesis Data Firehose can convert to is Apache Parquet, which is a columnar storage format that can\nimprove the performance and cost-efficiency of analytics queries. By using Amazon Kinesis Data\nStreams and Amazon Kinesis Data Firehose, the Mobile Network Operator can ingest the .CSV data\nfrom the source systems and use Amazon Kinesis Data Firehose to convert the data into Parquet\nbefore storing it on Amazon S3","poster":"kyuhuck"},{"poster":"Alice1234","upvote_count":"1","comment_id":"1142482","content":"D. Ingest .CSV data from Amazon Kinesis Data Streams and use Amazon Kinesis Data Firehose to convert data into Parquet.\n\nThis is because Amazon Kinesis Data Firehose has built-in support for converting incoming streaming data into different formats like Parquet before storing it in S3. This requires less setup and management compared to the other options, making it a low-effort solution.","timestamp":"1707244800.0"},{"timestamp":"1707224280.0","content":"Selected Answer: D\nchat gpt4 is 'D'","comment_id":"1142155","upvote_count":"1","poster":"kyuhuck"},{"timestamp":"1704203280.0","poster":"bsb765","content":"Answer is B. Since Glue can work with real time data and work with csv files directly and store them in S3\nFor option D it needs a lambda function in between to convert csv to json then store to S3","upvote_count":"2","comment_id":"1111939"},{"timestamp":"1703885400.0","upvote_count":"1","comment_id":"1109130","content":"Selected Answer: D\nD is the most straightforward solution. Kinesis Data Firehose directly supports converting streaming data into Parquet format and requires the least amount of setup and operational overhead compared to the other options. It eliminates the need for server management and manual job scheduling, providing a more seamless and low-effort solution for real-time data ingestion and transformation.","poster":"Neet1983"},{"poster":"Arnie0014","content":"Selected Answer: B\nB is the right option. Using firehose , we will need to write a lambda to do the conversion as firehose only converts from JSON to parquet. Where as Glue has inbuilt integration for doing this conversion. This answer is verified using the recently launched Amazon Q","timestamp":"1703215380.0","comment_id":"1103110","upvote_count":"1"},{"comment_id":"1099144","timestamp":"1702838280.0","upvote_count":"2","poster":"edobip","content":"Selected Answer: D\nkinesis data firehose supports data Conversions from CSV / JSON to Parquet / ORC (only for\nS3)"},{"timestamp":"1702566780.0","poster":"khchan123","comment_id":"1096585","content":"The least effort solution to implement this use case would be option D - Ingest .CSV data from Amazon Kinesis Data Streams and use Amazon Kinesis Data Firehose to convert data into Parquet.\n\nAmazon Kinesis Data Firehose is a fully managed service that automatically delivers real-time streaming data to destinations such as Amazon S3, Redshift and Elasticsearch. It can ingest streaming data from Kinesis Data Streams and convert the data format to Parquet before delivering to S3. This avoids the need to setup and manage infrastructure for data ingestion and transformation.\n\nCompared to the other options:\n\nA) Using Kafka Streams and Kafka Connect would require more effort to setup and manage the Kafka cluster on EC2.\n\nB) Using Glue to convert data would require developing a Glue ETL job which is more complex than just using Firehose transformation feature.\n\nC) Setting up an EMR cluster and developing Spark Structured Streaming job is more complex than the serverless Firehose service.","upvote_count":"2"},{"timestamp":"1700022120.0","comment_id":"1071070","upvote_count":"1","poster":"cgsoft","content":"Selected Answer: B\nKinesis Data Firehose accepts only JSON input"},{"timestamp":"1698473220.0","content":"Selected Answer: D\nChecked in chatGPT","poster":"ZZYS","comment_id":"1055992","upvote_count":"2"},{"comment_id":"1039321","upvote_count":"1","content":"Selected Answer: B\nAnswer is B","timestamp":"1696927980.0","poster":"Hisayuki"},{"timestamp":"1696190820.0","comment_id":"1022636","upvote_count":"1","content":"Selected Answer: D\nthe key word over here is: The source systems send data in .CSV format in real time and Amazon Kinesis Data Firehose is more suitable for real-time streaming data processing so it is D","poster":"teka112233"},{"content":"Selected Answer: B\nVote for B. Because Glue can do the job and Glue is serverless","poster":"terrku","comment_id":"1007553","timestamp":"1694692440.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1006265","timestamp":"1694586480.0","content":"Selected Answer: D\nA. NO - too much dev effort\nB. NO - Glue is Spark, so some dev effort is required (AWS Glue Brew could avoid it)\nC. NO - too much dev effort\nD. YES - no code needed required with Firehose","poster":"loict"},{"content":"Selected Answer: D\nAmazon Kinesis Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Due to LEAST EFFORT mentioned in question I go with Option D","timestamp":"1694239260.0","comment_id":"1002964","poster":"Sharath1783","upvote_count":"1","comments":[{"content":"But question states that the input data is csv","timestamp":"1698237240.0","comment_id":"1053714","upvote_count":"1","poster":"DimLam"}]},{"poster":"Sharath1783","content":"The key term is LEAST effort so Kinesis Firehose has the ability to convert JSON data to Parquet or ORC format on the fly. Option D is the right answer","comment_id":"993011","upvote_count":"1","comments":[{"comment_id":"1053721","timestamp":"1698237360.0","poster":"DimLam","content":"What about the input data in a csv format?","upvote_count":"2"}],"timestamp":"1693307400.0"},{"upvote_count":"1","comment_id":"976384","timestamp":"1691570940.0","poster":"rav009","content":"D.\nFirehose is easier than Glue."},{"poster":"Mickey321","content":"Selected Answer: D\nThe solution that takes the least effort to implement is D. Ingest .CSV data from Amazon Kinesis Data Streams and use Amazon Kinesis Data Firehose to convert data into Parquet.","comment_id":"973016","timestamp":"1691239020.0","comments":[{"content":"Firehose can't directly convert csv to parquet","poster":"DimLam","timestamp":"1698237420.0","upvote_count":"1","comment_id":"1053725"}],"upvote_count":"1"},{"poster":"Mickey321","content":"Selected Answer: D\nIn this scenario, the source systems send data in .CSV format in real time. The Data Engineering team can ingest this data using Amazon Kinesis Data Streams and then use Amazon Kinesis Data Firehose to convert the data into the Apache Parquet format before storing it on Amazon S3. This solution requires minimal effort to implement as it makes use of fully managed services and does not require any additional infrastructure or development work.","timestamp":"1691237640.0","upvote_count":"1","comment_id":"972995"},{"upvote_count":"1","content":"Selected Answer: D\nAmazon Kinesis Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON1.","poster":"Mickey321","comment_id":"970239","timestamp":"1690983720.0"},{"comment_id":"961666","timestamp":"1690207140.0","poster":"Venkatesh_Babu","upvote_count":"1","content":"Selected Answer: B\nAnswer is b"},{"content":"Selected Answer: D\nWhile Amazon Glue can be used to transform data from CSV to Parquet format, it requires setting up and configuring additional infrastructure. Specifically, you would need to set up an Amazon Glue Crawler to discover the schema of the incoming CSV data, create a Glue Job to transform the data into Parquet format, and configure a Glue Connection to write the transformed data to S3. These additional steps make this option more complex and potentially more costly to implement than Option D, which utilizes the fully managed Kinesis Data Firehose service to automatically convert data to Parquet format without requiring additional infrastructure setup.","comments":[{"content":"Glue can convert CSV->Parquet; but Kinesis Firehouse need CSV->JSON (using lambda function), then JSON->Parquet, much complicated.","comment_id":"874088","upvote_count":"1","timestamp":"1681850520.0","poster":"ZSun"}],"timestamp":"1680549360.0","upvote_count":"1","poster":"oso0348","comment_id":"860281"},{"upvote_count":"1","poster":"brunokiyoshi","content":"Amazon Kinesis Data Streams is commonly used to ingest large quantities .csv data in real time.","comment_id":"846471","timestamp":"1679443020.0"},{"comment_id":"772722","poster":"expertguru","timestamp":"1673455260.0","upvote_count":"3","content":"Amazon Kinesis Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information, see Amazon Kinesis Data Firehose Data Transformation. So there is additional work involved with firehose see below url\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nso answer should be B"},{"content":"B, for realtime and covert to catalog","timestamp":"1672534440.0","comment_id":"762998","poster":"dreswardev","upvote_count":"2"},{"comment_id":"755719","upvote_count":"3","content":"Selected Answer: B\nfirehose requires lambda for csv to parquet. Glue studio drag and drop, select source and dest s3 file formats; and glue studio auto-creates scripts... so least amount of effort with glue...","timestamp":"1671979200.0","poster":"rrshah83"},{"upvote_count":"1","poster":"chizeni","content":"answer is D\ncan not use firehose to convert csv to parquet format.\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\ningest real time with kinesis and convert with glue","timestamp":"1660245600.0","comment_id":"645588"},{"timestamp":"1658835000.0","poster":"matteocal","content":"Selected Answer: B\nThe correct answer is B since Firehose cannot convert csv to parquet (only json):\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html","upvote_count":"3","comment_id":"637368"},{"timestamp":"1657385400.0","poster":"rjekstein","content":"B is correct.\nKinesis firehose converts only Json to Parquet or ORC directly.","upvote_count":"2","comment_id":"629271"},{"comments":[{"comment_id":"741559","timestamp":"1670750100.0","content":"Kinesis firehose would call a lambda function for the conversion which requires more effort than using the predefined functions of Glue.","upvote_count":"1","poster":"shammous"}],"timestamp":"1656011580.0","poster":"ovokpus","upvote_count":"1","content":"Selected Answer: D\nWith MINIMUM amount of work, Kinesis firehose can convert CSV on the fly before storing it in S3. Glue cannot do this. Only Kinesis Firehose can.","comment_id":"621238"},{"upvote_count":"2","comment_id":"476581","comments":[{"timestamp":"1651743780.0","poster":"samy666","upvote_count":"2","comment_id":"597225","content":"But the Lambda is not mentioned in the answer D"}],"poster":"[Removed]","content":"In AWS, one of the generally accepted theories is that writing a Lambda function is considered to be least effort. In this problem, although B is definitely possible, it requires to configure a Glue job and do the data transformation mapping. I tend to think that AWS considers writing a one-liner Lambda is possibly lesser effort that configuring the Glue job, Hence I would go with D.","timestamp":"1636678260.0"},{"poster":"StelSen","timestamp":"1636191720.0","content":"The Answer is B. LINK: https://aws.amazon.com/glue/faqs/\n(Search 'real' in this FAQ and you will get an answer. Answer D required one more component called Lambda. So straightaway rejected)","comment_id":"352445","upvote_count":"2"},{"content":"D. Glue cannot work with live streams, while only Firehose could ingest the data to S3 and make transformation from csv to parquet.","comment_id":"324134","upvote_count":"1","poster":"Vita_Rasta84444","timestamp":"1635731460.0"},{"comments":[{"content":"the format is .CSV firehose can only change JSON to parquet. or ORC.","upvote_count":"1","poster":"AmakamaxZanny","timestamp":"1645706100.0","comment_id":"555261"}],"timestamp":"1635716640.0","upvote_count":"1","content":"Answer is D go to: https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html","poster":"ahquiceno","comment_id":"279969"},{"comment_id":"259774","timestamp":"1635626760.0","content":"A-- cannot be anwser as Apache kafka S3 cannot write in parquet\nB-- seems like a good anwser but if the question is old , then at that time Glue did not had compatibility with Kinesis data streams \nC--- cannot be the anwser as Spark structured streaming need to read data from somehwere like Kafka topics , we cannot publish data directly to it and like other three options mention Apache kafka/Kinesis in it so this means it should have been in this one also if this was the correct anwser.\nD-- Seems like a good anwser , but it needs lambda to convert CSV -- JSON and then Firehose's inbuilt ability to covert JSON to Parquet before storing to S3\n\nNow the final analysis -- :)\n-- The comments on this question started some time in Decmeber 2019/January2020 and the Glue capability to consume real time Kiensis data streams was annoounce much later on April 27 , 2020 which means this anwser must be incorrect before this date\n--Hence the likely choice is D , even though firehose Lambda thing is not mentioned in it specifically\nAny counter comment ???","comments":[{"comments":[{"comment_id":"402334","poster":"Huy","timestamp":"1636194300.0","upvote_count":"1","content":"Even Kinesis can process streaming data, B still take more effort than D. Converting CVS to JSON is a piece of cake in python. I think D is correct. Let's say if you have to design this system, what will you do? I will of-course use Lambda and Firehose first."}],"content":"Very good points made, I would agree that the timing of the question needs to be taken into account. When the question was first posted Glue could not take real-time data and transform it, so that would make B a no starter and D the Answer however if you consider that there is now integration of Kinesis and Glue you can now do real-time processing so B is now possible and less effort than D. On balance, the question may not even be asked nowadays but it does demonstrate how answeres can change over time. Do your research and make sure you understand what capabilities a service has.","upvote_count":"4","comment_id":"282499","poster":"cnethers","timestamp":"1635726900.0"}],"upvote_count":"6","poster":"harmanbirstudy"},{"comment_id":"170853","poster":"mrsimoes","timestamp":"1635583320.0","content":"I think that the answer is B.","upvote_count":"2"},{"timestamp":"1635515220.0","upvote_count":"1","poster":"syu31svc","comment_id":"164318","content":"Answer is B as per link:\nhttps://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/"},{"content":"where do I find the link to other AWS Machine Learning questions ?","poster":"vivky247","timestamp":"1635073800.0","upvote_count":"1","comment_id":"156793"},{"content":"Answer is D\nhttps://docs.aws.amazon.com/zh_cn/firehose/latest/dev/record-format-conversion.html\n\nAmazon Kinesis Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON.","comment_id":"156549","timestamp":"1634808720.0","poster":"dikers","upvote_count":"1"},{"poster":"GeeBeeEl","timestamp":"1634639880.0","content":"Kinesis Firehose can only convert JSON to Parquet or ORC. It cannot convert CSV. For CSV you need a lambda transformation https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html hence D is out\nAccording to http://blogs.quovantis.com/how-to-convert-csv-to-parquet-files/ Apache Spark can convert csv to parquet but results will be in binary format, you will not be able to read it --- this is true for all parquet filesâ€¦â€¦. C is possible. Remember its streaming data\nGlue can convert CSV/JSON to Parquet see https://medium.com/searce/convert-csv-json-files-to-apache-parquet-using-aws-glue-a760d177b45f glue also supports streaming data coming from Kinesis data stream see https://aws.amazon.com/glue/faqs/\nSince the competition is between Spark and Glue, see https://stackshare.io/stackups/aws-glue-vs-spark , Glue is EASIER to implement (a requirement in the question)","upvote_count":"1","comment_id":"149962"},{"timestamp":"1634043540.0","poster":"Urban_Life","comment_id":"123798","content":"I think the answer will be B. Two reasons : 1. The Q's itself talks about Athena and S3. \n 2. I've done that in my project (It's proven concept)","upvote_count":"1"},{"comment_id":"108317","content":"The answer would be D. \nHere is my explanation:\nOption A clearly requires maximum amount of efforts\nB & C rules out because you cannot read directly through apache kafka, you need a streaming queue like Kinesis. Additionally Glue is not a suitable option for streaming jobs (Though they have introduced capability of streaming within Glue 7 days back from the day of this comment)\nSo finally option D is the right option, where, the data would be streamed through firehose, it converts JSON to Parquet format with LEAST effort. I believe there would be a lamda blueprint to convert CSV to JSON.","upvote_count":"1","poster":"Antriksh","timestamp":"1633526580.0"},{"poster":"ardisch","content":"Sorry, Answer is D\nhttps://aws.amazon.com/about-aws/whats-new/2018/05/stream_real_time_data_in_apache_parquet_or_orc_format_using_firehose/","upvote_count":"1","comment_id":"107870","timestamp":"1633461720.0"},{"content":"Answer is C, you need to save data so S3 and Firehose can convert data to Parque format automatically.\nhttps://aws.amazon.com/about-aws/whats-new/2018/05/stream_real_time_data_in_apache_parquet_or_orc_format_using_firehose/","poster":"ardisch","comment_id":"107869","upvote_count":"1","timestamp":"1633459680.0"},{"upvote_count":"5","timestamp":"1633407840.0","poster":"mawsman","comment_id":"88328","content":"This question is made to make you fail.\n- Least effort would indicate a managed service so that technically eliminates A and C.\n- CSV to Parquet would eliminate Firehose which would be my choice because it would deliver to S3 directly. You could do it with a Lambda function to pre-process, but because it doesn't say that then we can eliminate D.\n- Glue could do CSV to Parquet it but in that case you need to trigger the glue job somehow and it falls out of \"real-time\" scope and it doesn't say that the glue job would deliver to S3. So that eliminates B.\nThe answer is actually C because this exact use case is demonstrated in this AWS blog where an EMR cluster with a pyspark script transforms data from CSV to Parquet:\nhttps://aws.amazon.com/blogs/big-data/analyzing-data-in-s3-using-amazon-athena/\nIt's the only way to do it in real time and satisfy all the conditions. Whoever wrote this question just wants you to fail the exam. This isn't testing our knowledge, this is testing if you read the aws blogs."},{"upvote_count":"1","comment_id":"83367","poster":"roytruong","content":"B is possible, this link shows that glue can connect with streaming source easily like kinesis data stream or kafka.\nhttps://docs.aws.amazon.com/glue/latest/dg/add-job-streaming.html","timestamp":"1633320060.0"},{"content":"D. Least effort means remove anything to do with Infra, EC2, etc...moreover data needs to be sinked into S3 and for that Firehose is the best option...","upvote_count":"4","poster":"PRC","comment_id":"65507","timestamp":"1633178220.0"},{"comment_id":"58340","upvote_count":"1","poster":"AKT","timestamp":"1633023420.0","content":"Answer is D"},{"content":"A and C requires lot of setups, B is not right because AWS Glue ETL is batch oriented and won't work for streams, so it should be D. (as long as we make a Lambda function to do data transformation from CSV to JSON and then make Firehose convert this JSON to parquet format.)","poster":"VB","comment_id":"56716","upvote_count":"1","timestamp":"1632760620.0"},{"upvote_count":"5","comments":[{"upvote_count":"1","poster":"rajs","content":"On a second thought - after looking through the documentation - I feel D is the right answer as it requires lesser effort\n\nD","comments":[{"upvote_count":"1","content":"I feel that it is D as well, but not to the least effort requirement.\nEliminate A, C due to the LEAST effort requirement (implies fully managed services)\nIt seems like both B & D work with Parquet. The only differentiating factor seems like real-time vs non real time. \nThere's tons of evidence that Firehose handles real time data, but not so much for Glue.","timestamp":"1633083060.0","comment_id":"63827","poster":"sdsfsdsf"}],"comment_id":"54139","timestamp":"1632686220.0"}],"timestamp":"1632680880.0","comment_id":"54105","content":"A can be eliminated as it requires lot of work :)\n\nB is correct with the assumption that CSV file is created from the stream e.g., using Firehose and then Glue transformation converts the CSV file to Parquet\n\nC although feasible requires a lot of effort\n\nD will be correct with the assumption that Stream is first transformed from CSV to JSON in a Lambda and then converted to Parquet using Firehose built in transformer\n\nBetween B & D => B Requires least amount of work \n\nSo the answer is B :)","poster":"rajs"},{"timestamp":"1632654060.0","content":"D. GLue cannot be connected with streaming data\nhttps://aws.amazon.com/glue/faqs/","comments":[{"timestamp":"1633300920.0","poster":"HollyCrap","upvote_count":"4","comment_id":"82328","content":"In the above link, it says: \"AWS Glue also supports (in beta) data streams from Amazon MSK, Amazon Kinesis Data Streams, and Apache Kafka.\". So B is possible."}],"upvote_count":"1","comment_id":"52846","poster":"bhavesh0124"},{"content":"Ans is B\n1. Use Glue Crawler to build scheme from the structured csv file.\n2. Configure and run a job to transform the data from CSV to Parquet.\nhttps://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/","timestamp":"1632649320.0","upvote_count":"7","poster":"BigEv","comment_id":"41282"},{"poster":"ComPah","upvote_count":"2","content":"Why not C \nMobile data being high volume data might need distributed processing Spark looks like a good option .But LEAST EFFORT key word makes me swing to B","timestamp":"1632600900.0","comment_id":"39755"},{"upvote_count":"6","content":"any comment about D?","timestamp":"1632503100.0","comment_id":"25123","comments":[{"upvote_count":"2","content":"D is correct option.","timestamp":"1633943520.0","comment_id":"112221","poster":"Antriksh"}],"poster":"heihei"}],"exam_id":26,"answer":"D","isMC":true,"answer_images":[],"question_id":223,"answers_community":["D (60%)","B (40%)"],"choices":{"A":"Ingest .CSV data using Apache Kafka Streams on Amazon EC2 instances and use Kafka Connect S3 to serialize data as Parquet","B":"Ingest .CSV data from Amazon Kinesis Data Streams and use Amazon Glue to convert data into Parquet.","D":"Ingest .CSV data from Amazon Kinesis Data Streams and use Amazon Kinesis Data Firehose to convert data into Parquet.","C":"Ingest .CSV data using Apache Spark Structured Streaming in an Amazon EMR cluster and use Apache Spark to convert data into Parquet."},"question_text":"A Mobile Network Operator is building an analytics platform to analyze and optimize a company's operations using Amazon Athena and Amazon S3.\nThe source systems send data in .CSV format in real time. The Data Engineering team wants to transform the data to the Apache Parquet format before storing it on Amazon S3.\nWhich solution takes the LEAST effort to implement?","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/8303-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2019-11-16 03:04:00","unix_timestamp":1573869840,"answer_ET":"D"},{"id":"kzTIdO4GO2NoMctFLmFu","answer_ET":"ACF","topic":"1","isMC":true,"timestamp":"2019-11-16 07:05:00","question_images":[],"discussion":[{"timestamp":"1632358140.0","content":"THE ANSWER SHOUD BE CEF\nIAM ROLE, INSTANCE TYPE, OUTPUT PATH","comments":[{"poster":"hamimelon","comment_id":"754590","content":"Why not A? You don't need to tell Sagemaker where the training data is located?","upvote_count":"3","comments":[{"timestamp":"1681668900.0","comment_id":"872024","content":"You need to specify the InputDataConfig, but it does not need to be \"S3\"\nI think the reason why A and B are wrong, not because data location is not required, but because it doesn't need to be S3, it can be Amazon S3, EFS, or FSx location","poster":"ZSun","upvote_count":"1"}],"timestamp":"1671836280.0"},{"comment_id":"202880","poster":"HaiHN","content":"Should be C, E, F\n\nFrom the SageMaker notebook example:\nhttps://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc/semantic_segmentation_pascalvoc.ipynb\n# Create the sagemaker estimator object.\n\nss_model = sagemaker.estimator.Estimator(training_image,\n role, \n train_instance_count = 1, \n train_instance_type = 'ml.p3.2xlarge',\n train_volume_size = 50,\n train_max_run = 360000,\n output_path = s3_output_location,\n base_job_name = 'ss-notebook-demo',\n sagemaker_session = sess)","timestamp":"1634951160.0","comments":[{"timestamp":"1675368240.0","comment_id":"796394","poster":"uninit","upvote_count":"6","content":"It says InstanceClass - CPU/GPU in the question, not InstanceType"},{"poster":"mirik","timestamp":"1687527000.0","comment_id":"931649","content":"instance type has default value.","upvote_count":"3"}],"upvote_count":"12"}],"upvote_count":"29","poster":"DonaldCMLIN","comment_id":"21906"},{"timestamp":"1634092800.0","content":"From here https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/API_CreateTrainingJob.html .. the only \"Required: Yes\" attributes are: \n1. AlgorithmSpecification (in this TrainingInputMode is Required - i.e. File or Pipe)\n2. OutputDataConfig (in this S3OutputPath is Required - where the model artifacts are stored)\n3. ResourceConfig (in this EC2 InstanceType and VolumeSizeInGB are required)\n4. RoleArn (..The Amazon Resource Name (ARN) of an IAM role that Amazon SageMaker can assume to perform tasks on your behalf...the caller of this API must have the iam:PassRole permission.)\n5. StoppingCondition\n6. TrainingJobName (The name of the training job. The name must be unique within an AWS Region in an AWS account.)\n\nFrom the given options in the questions.. we have 2, 3, and 4 above. so, the answer is CEF.","poster":"VB","comment_id":"66115","comments":[{"upvote_count":"7","timestamp":"1635044520.0","content":"This is the best explanation that CEF is the right answer, IMO. The document at that url is very informative. It also specifically states that InputDataConfig is NOT required. Having said that, I have no idea how the model will train if it doesn't know where to find the training data, but that is what the document says. If someone can explain that, I'd like to hear the explanation.","poster":"cloud_trail","comment_id":"278220","comments":[{"upvote_count":"3","timestamp":"1635177960.0","comments":[{"comment_id":"348896","upvote_count":"2","content":"but you also need to specify the service role sagemaker should use otherwise it will not be able to perform actions on your behalf like provisioning the training instances.","poster":"CloudGuru_ZA","timestamp":"1635876060.0"}],"content":"If I see this question on the actual exam, I'm going with AEF. The model absolutely must know where the training data is. I have seen other documentation that does confirm that you need the location of the input data, the compute instance and location to output the model artifacts.","comment_id":"278227","poster":"cloud_trail"}]},{"comment_id":"504677","content":"Perfect explanation. It is CEF","timestamp":"1639897380.0","poster":"rafaelo","upvote_count":"1"},{"upvote_count":"1","content":"The question is asking about built in algorithms. It should be ADE. See https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/API_CreateTrainingJob.html","poster":"JK1977","timestamp":"1685263080.0","comment_id":"908441"},{"comment_id":"1019198","upvote_count":"1","content":"for \"3. ResourceConfig\", only VolumeSizeInGB is required. So, it's not about the instance type.\nCheck: https://docs.aws.amazon.com/zh_tw/sagemaker/latest/APIReference/API_ResourceConfig.html","timestamp":"1695843840.0","poster":"OAmine"}],"upvote_count":"27"},{"poster":"JonSno","comment_id":"1358035","content":"Selected Answer: ACF\nReason:\nWhen submitting Amazon SageMaker training jobs using built-in algorithms, the following parameters must be specified:\n\nTraining Data Location (A)\n\nSageMaker requires the training dataset's location in Amazon S3.\nProvided as a channel input in the training job.\nIAM Role (C) \n\nSageMaker needs IAM permissions to access data from S3 and execute tasks on behalf of the user.\nModel Output Path (F) \n\nSpecifies the S3 bucket location where the trained model artifacts will be stored.","timestamp":"1739835600.0","upvote_count":"2"},{"comment_id":"1343036","poster":"AbhayD","content":"Selected Answer: ACF\nInstance type is required but not specific class CPU/GPU. Sagamkaer can handle that.","timestamp":"1737297480.0","upvote_count":"1"},{"comment_id":"1305387","poster":"MultiCloudIronMan","upvote_count":"1","timestamp":"1730373960.0","content":"Selected Answer: ACF\nThese parameters ensure that the training job has access to the necessary data, permissions, and storage locations to function correctly."},{"poster":"MultiCloudIronMan","timestamp":"1730146440.0","comment_id":"1304165","upvote_count":"1","content":"Selected Answer: ACF\nOptions B, D, and E are important but not always mandatory for every training job. For example, validation data (Option B) is not always required, and hyperparameters (Option D) and instance types (Option E) can have default values or be optional depending on the specific algorithm and setup."},{"content":"import boto3\nimport sagemaker\n\nsess = sagemaker.Session()\n\n# Example for the linear learner \nlinear = sagemaker.estimator.Estimator(\n container,\n role, # role (c)\n instance_count=1,\n instance_type=\"ml.c4.xlarge\", # instance type (e)\n output_path=output_location, # output path (f)\n sagemaker_session=sess,\n)","comment_id":"1293341","upvote_count":"1","poster":"amlgeek","timestamp":"1728106800.0"},{"content":"Selected Answer: CEF\nGoing with cef","poster":"kiran15789","upvote_count":"1","timestamp":"1724520840.0","comment_id":"1271773"},{"poster":"ML_2","timestamp":"1723570620.0","upvote_count":"1","content":"Selected Answer: CEF\nANSWER IS CEF\nHere from Amazon docs \nInputDataConfig\nAn array of Channel objects. Each channel is a named input source. InputDataConfig describes the input data and its location.\nRequired: No\n\nOutputDataConfig\nSpecifies the path to the S3 location where you want to store model artifacts. SageMaker creates subfolders for the artifacts.\nRequired: Yes\n\nResourceConfig - Identifies the resources, ML compute instances, and ML storage volumes to deploy for model training. In distributed training, you specify more than one instance.\nRequired: Yes","comment_id":"1265269"},{"comment_id":"1247233","poster":"RathanKalluri","content":"CEF \nhttps://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html#API_CreateTrainingJob_RequestParameters","timestamp":"1720864500.0","upvote_count":"1"},{"poster":"ninomfr64","content":"Based on https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html\n\nRequired parameters are:\n- AlgorithmSpecification (registry path of the Docker image with the training algorithm)\n- OutputDataConfig (path to the S3 location where you want to store model artifacts)\n- ResourceConfig (resources, including the ML compute instances and ML storage volumes, to use for model training)\n- RoleArn\n- StoppingCondition (time limit for training job)\n- TrainingJobName\n\nThus, the answer is: C E F\nwording for option E is inaccurate \"EC2 instance class specifying whether training will be run using CPU or GPU\" but they do it on purpose","upvote_count":"1","comment_id":"1231987","timestamp":"1718637420.0"},{"comment_id":"1204473","poster":"rookiee1111","timestamp":"1714472340.0","content":"Selected Answer: ACF\nThe input channel and output channel are mandatory, as the training job needs to know where to get the input data from and where to publish the model artifact. IAM role is also needed, for AWS services. others are not mandatory, validation channel is not mandatory for instance in case of unsupervised learning, likewise hyper params can be be auto tuned for as well as the ec2 instance types can be default ones that will be picked","upvote_count":"2"},{"timestamp":"1714291620.0","comment_id":"1203457","upvote_count":"1","content":"As they narrowed it to S3, A is incorrect BUT when submitting Amazon SageMaker training jobs using one of the built-in algorithms, it is a MUST to identify the location of training data. While Amazon S3 is commonly used for storing training data, other sources like Docker containers, DynamoDB, or local disks of training instances can also be used. Therefore, specifying the location of training data is essential for SageMaker to know where to access the data during training.\n\nSo the right answer is CEF for me for this case... However if A was saying identify the location of training data, I think option A would be included in the MUST parameter.","poster":"Denise123"},{"comment_id":"1203382","content":"InputDataConffig is optional in create_training_job.Please check thte parameters that are required.\nSo answer is CEF: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html","timestamp":"1714282800.0","upvote_count":"1","poster":"sachin80"},{"poster":"vkbajoria","content":"Selected Answer: CEF\nInput is required only when calling Fit method. When initializing the Estimator, we do not need input","comment_id":"1193995","upvote_count":"1","timestamp":"1712866500.0"},{"content":"Selected Answer: ACF\nI open the sagemaker and tested. A C F\nB is not needed for non-supervised algorithm.","comment_id":"1184598","timestamp":"1711610160.0","upvote_count":"2","poster":"rav009"},{"upvote_count":"1","comment_id":"1158224","timestamp":"1708818960.0","poster":"vkbajoria","content":"C, E, F\nThe trick was the training channel, but all the data channel are passed during when actually training the model using fit method"},{"comments":[{"upvote_count":"1","poster":"VR10","comment_id":"1154149","content":"Correction, having gone thru the doc more closely, there is no default for instance type.\nSo the choices should be A, C, E.","timestamp":"1708363560.0"}],"timestamp":"1708362420.0","upvote_count":"2","comment_id":"1154135","content":"E is not important, some models could simply work on the default of CPU.\nA is a must and E is a must too.\nC is important for permission handling on S3 etc.\nIt has to be A, C, F","poster":"VR10"},{"poster":"Alice1234","upvote_count":"1","comment_id":"1142516","content":"A. The training channel identifying the location of training data on an Amazon S3 bucket: This is where SageMaker will get the input data for training the model.\nC. The IAM role that Amazon SageMaker can assume to perform tasks on behalf of the users: This role provides SageMaker the necessary permissions to access AWS resources.\nF. The output path specifying where on an Amazon S3 bucket the trained model will persist: After training, the model artifacts need to be saved in a specified S3 bucket location.","timestamp":"1707247380.0"},{"upvote_count":"1","comment_id":"1116382","content":"Selected Answer: CEF\nPlease go through the lab https://catalog.us-east-1.prod.workshops.aws/workshops/63069e26-921c-4ce1-9cc7-dd882ff62575/en-US/lab2","timestamp":"1704686940.0","poster":"Swagata23"},{"poster":"phdykd","content":"ACF is answer","comment_id":"1114874","upvote_count":"1","timestamp":"1704504840.0"},{"comment_id":"1109125","upvote_count":"3","content":"Selected Answer: ACF\nA. The training channel identifying the location of training data on an Amazon S3 bucket: This is essential because SageMaker needs to know where to find the data for training.\n\nC. The IAM role that Amazon SageMaker can assume to perform tasks on behalf of the users: SageMaker requires permissions to access resources on behalf of the user, and this is provided by specifying an IAM role with the necessary policies attached.\n\nF. The output path specifying where on an Amazon S3 bucket the trained model will persist: After the model is trained, SageMaker needs to save the output, which includes the model artifacts, to a specified S3 location.","timestamp":"1703885100.0","poster":"Neet1983"},{"timestamp":"1703517900.0","comment_id":"1105354","content":"Selected Answer: CEF\nhttps://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/API_CreateTrainingJob.html\n\nThe answer should be CEF\n\nThe only attributes strictly required(\"Required: Yes\" ) are:\nTrainingJobName\nAlgorithmSpecification\nOutputDataConfig\nResourceConfig\nRoleArn\nStoppingCondition\n\nSo, why is 'InputDataConfig' strictly required?\nWhen 'InputDataConfig' is not needed:\nAlgorithm with Pre-loaded Data where the data is already embedded or hardcoded within the training script or Docker container or pre-defined datasets available within SageMaker\nAlgorithm Generating (training) Data such as synthetic data generation or reinforcement learning scenarios","upvote_count":"2","poster":"mcwithimp"},{"content":"Can confirm, the answer is CEF...\n\nI just went into SageMaker console, and tried to create a training job...\n\nYou must have an IAM Role for permissions.\n\nLeaving the instance class/instance type blank is not possible it is always autofilled with a default instance, therefore REQUIRED.\n\nOutput path for the model after training is also required, you cannot create a training job without these fields... therefore the answer is CEF\n\n- IAM Role\n- Instance Class/Type\n- Output path for completed model","poster":"rdoty","timestamp":"1702905060.0","upvote_count":"2","comment_id":"1099719"},{"comment_id":"1086319","upvote_count":"1","content":"Selected Answer: CEF\nRequired Parameters: \nSagemaker Required Parameters\nAlgorothm Specs - path to docker image\nOutput Data Config -  (path to S3)\nResource Confg - Instance type and storage volume\nRole ARN\nStopping Conditions\nTraining job Name","poster":"vikaspd","timestamp":"1701535020.0"},{"upvote_count":"1","content":"Selected Answer: CEF\nCEF Is correct","comment_id":"1067000","timestamp":"1699600380.0","poster":"elvin_ml_qayiran25091992razor"},{"poster":"Leo2023aws","comment_id":"1054718","upvote_count":"1","timestamp":"1698334980.0","content":"ADE\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/common-info-all-im-models.html"},{"timestamp":"1696149000.0","upvote_count":"2","comment_id":"1022117","poster":"windy9","content":"iTS acf.\n\noFFICIAL dOCS sAY:\nThe URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n\nThe compute resources that you want SageMaker to use for model training. Compute resources are machine learning (ML) compute instances that are managed by SageMaker.\n\nThe URL of the S3 bucket where you want to store the output of the job.\n\nThe Amazon Elastic Container Registry path where the training code is stored. For more information, see Docker Registry Paths and Example Code.\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html#:~:text=To%20train%20a%20model%20in%20SageMaker%2C%20you%20create%20a%20training%20job.%20The%20training%20job%20includes%20the%20following%20information%3A"},{"timestamp":"1694605620.0","poster":"loict","upvote_count":"1","content":"Selected Answer: CEF\nRequired options can be tested via the doc (https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-mkt-algo-train.html) or better the UI\n\nA. NO - A training channel is required, but it can be S3 or FileSystem\nB. NO - validation channel is not required\nC. YES - The IAM role that Amazon SageMaker can assume to perform tasks on behalf of the users.\nD. NO - Hyperparameters optionnal\nE. YES - an EC2 instance class is asked for, in turn determining if GPU is used\nF. YES - The output path specifying where on an Amazon S3 bucket the trained model will persist.","comment_id":"1006557"},{"timestamp":"1693635900.0","poster":"Sharath1783","comment_id":"996640","content":"Selected Answer: AEF\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n\nAccording to above link below parameters are required for SageMaker training job,\n1. S3 bucket path where input data is stored.\n2. ML computing resource to be used by SageMaker to build the model.\n3. S3 path where output model should be placed.\n4. ECS path where training code needs to be picked from.\n\nSo I go with option AEF","upvote_count":"1"},{"poster":"teka112233","comment_id":"986121","timestamp":"1692573840.0","upvote_count":"1","content":"Selected Answer: ACF\nAccording to AWS documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html\n(Optional) For Instance type, choose the ML compute instance type that you want to use. In most cases, ml.m4.xlarge is sufficient.\nbut still, you have to identify (in order to training jobs):\n1-IAM role\n2- input (the training channel/s)\n3-output"},{"timestamp":"1690770000.0","upvote_count":"1","poster":"FloKo","content":"Selected Answer: AEF\nAccording to https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\no train a model in SageMaker, you create a training job. The training job includes the following information:\n\n The URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n\n The compute resources that you want SageMaker to use for model training. Compute resources are machine learning (ML) compute instances that are managed by SageMaker.\n\n The URL of the S3 bucket where you want to store the output of the job.\n\n The Amazon Elastic Container Registry path where the training code is stored.","comment_id":"967678"},{"poster":"jyrajan69","comment_id":"965843","upvote_count":"1","timestamp":"1690579080.0","content":"Based on the link below, it can be ACDF but only 3 as per question, unless C is considered wrong because it does not say RoleARN, but in any case, I hope I dont see this on the exam\n https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"},{"content":"Selected Answer: ADF\nActually, it can be ADF. When creating Sagemaker training jobs from console:\nC - when this is not the first training job, the AIM Role will have a default value.\nD - I checked all models and they all require setting some of the hyperparameters\nE - EC2 Instance type has default value - \"ml.m4.xlarge\"","poster":"mirik","upvote_count":"1","comment_id":"931661","timestamp":"1687527600.0"},{"comment_id":"931645","upvote_count":"1","poster":"mirik","content":"Selected Answer: ACF\nThe answer is ACF: I just tried to create a new SageMaker training job saw no \"The Amazon EC2 instance class specifying whether training will be run using CPU or GPU\".\nThe is only EC2 \"Instance type\" that has a default value - \"ml.m4.xlarge\". \nSome algorithms also require to set \"hyperparameters\".","timestamp":"1687526880.0"},{"poster":"SRB1337","timestamp":"1687258680.0","upvote_count":"1","comment_id":"928371","content":"AEF see here: https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n\nTo train a model in SageMaker, you create a training job. The training job includes the following information:\n\nThe URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n\nThe compute resources that you want SageMaker to use for model training. Compute resources are machine learning (ML) compute instances that are managed by SageMaker.\n\nThe URL of the S3 bucket where you want to store the output of the job.\n\nThe Amazon Elastic Container Registry path where the training code is stored. For more information, see Docker Registry Paths and Example Code."},{"poster":"JK1977","upvote_count":"1","comment_id":"908438","content":"Selected Answer: ADE\nSee: https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-choose.html#built-in-algorithms-benefits\n\"The built-in algorithms require no coding to start running experiments. The only inputs you need to provide are the data, hyperparameters, and compute resources. This allows you to run experiments more quickly, with less overhead for tracking results and code changes.\"\nHence ADE","timestamp":"1685263020.0"},{"content":"Selected Answer: CEF\nWithout providing the IAM role training will not start ... it is the basic requirement","timestamp":"1684956840.0","poster":"earthMover","comment_id":"906125","upvote_count":"1"},{"comment_id":"860301","upvote_count":"2","timestamp":"1680550860.0","content":"Selected Answer: ACD\nThe three common parameters that MUST be specified when submitting Amazon SageMaker training jobs using one of the built-in algorithms are:\nA. The training channel identifying the location of training data on an Amazon S3 bucket.\nC. The IAM role that Amazon SageMaker can assume to perform tasks on behalf of the users.\nD. Hyperparameters in a JSON array as documented for the algorithm used.\n\nNote that parameters B, E, and F are optional parameters that depend on the specific needs of the training job.","poster":"oso0348"},{"content":"Selected Answer: ACF\nAccording to the document:https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html, the required are AlgorithmSpecification, OutputDataConfig, ResourceConfig(The resources, including the ML compute instances and ML storage volumes, to use for model training), RoleArn, StoppingCondition, TrainingJobName. so The answer should be ACF.","poster":"hug_c0sm0s","upvote_count":"3","comment_id":"823812","timestamp":"1677508080.0"},{"comment_id":"814474","poster":"bakarys","timestamp":"1676836620.0","content":"A. The training channel identifying the location of training data on an Amazon S3 bucket.\nC. The IAM role that Amazon SageMaker can assume to perform tasks on behalf of the users.\nD. Hyperparameters in a JSON array as documented for the algorithm used.\n\nNote: The validation channel, instance class, and output path are not required but may be specified depending on the training job.","upvote_count":"1"},{"content":"A, C, F\nFor all those advocating E - please read the option carefully. It says - \"The Amazon EC2 instance class specifying whether training will be run using CPU or GPU.\". Nowhere do you specify the instance class - CPU/GPU. You specify the instance type - \nhttps://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html\nSo this option is automatically out.\nAnd, of course, you do need to specify the path of the training dataset, so A must be included.","upvote_count":"4","timestamp":"1675368180.0","poster":"uninit","comment_id":"796393"},{"upvote_count":"3","comment_id":"771051","poster":"vbal","timestamp":"1673323980.0","content":"As per this URL : https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ResourceConfig.html\nInstanceType is Not required so I think ACF make sense.\nInputDataConfig\nType: Array of Channel objects\nArray Members: Minimum number of 1 item. Maximum number of 20 items."},{"upvote_count":"3","timestamp":"1661953260.0","poster":"Ob1KN0B","comment_id":"655130","content":"Selected Answer: CEF\nNot A because: https://docs.aws.amazon.com/zh_tw/sagemaker/latest/APIReference/API_CreateTrainingJob.html\nInputDataConfig is marked as \"Required:No\". So its C, E, F."},{"timestamp":"1651026000.0","comment_id":"592769","content":"Selected Answer: CEF\nCEF https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/API_CreateTrainingJob.html","poster":"Abdelrahman_Omran","upvote_count":"1"},{"comment_id":"573993","upvote_count":"1","timestamp":"1648086060.0","poster":"TerrancePythonJava","content":"Selected Answer: AEF\nAEF not CEF"},{"timestamp":"1640125020.0","poster":"Freegirl","content":"Can anyone explain why D is wrong?","comment_id":"506482","upvote_count":"1"},{"poster":"AShahine21","comment_id":"371649","content":"After re-double checking the documentation (https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html):\nI will go with C E F\nA is not required, we can read more under InputDataConfig","upvote_count":"2","timestamp":"1636204680.0"},{"comment_id":"351735","upvote_count":"2","timestamp":"1636079340.0","content":"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html\nCmd + F \nsearch for 'Required: Yes' \nCEF","poster":"orangechickencombo"},{"poster":"gcpwhiz","comment_id":"340985","timestamp":"1635838680.0","upvote_count":"2","content":"This is a very simple question, one of the first items provided in the Developer Guide. Stop looking for alternative answers and minute details when the answer is simple. There are 4 items needed for training jobs: \n1. The URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n2. The compute resources that you want SageMaker to use for model training. Compute resources are ML compute instances that are managed by SageMaker.\n3. The URL of the S3 bucket where you want to store the output of the job.\n4. The Amazon Elastic Container Registry path where the training code is stored. \n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"},{"comment_id":"335417","timestamp":"1635646920.0","content":"CEF\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-mkt-algo-train.html","upvote_count":"1","poster":"tmld"},{"content":"CEF (CANT BE A or B BECAUSE INPUTS CAN BE ON LOCAL MODE inputpath = FILE:// https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase","upvote_count":"1","poster":"sonalev419","timestamp":"1635504480.0","comment_id":"324517"},{"comment_id":"311851","timestamp":"1635492420.0","upvote_count":"3","content":"I think it is about parameters required to \"running a training job\" vs \"creating a training job\". Question is focusing on \"Submitting a training job\" so answer should be A, E, F based on the AWS documentation.","poster":"exploringaws"},{"content":"When creating a training job you need:\n- image_uri \n- role \n- train_instance_count \n- train_instance_type\n- train_volume_size\n- output_path \n- sagemaker_session \n- rules\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html#ex1-train-model-sdk\n\nBased on that and the options avaliable the ANSWER = C (Role) E (Instance Type) F (output path)\n\nSome people feel training data and or validation data is required, this is not defined in the training job it is defined in the fitting of the algo","poster":"cnethers","upvote_count":"3","comment_id":"288423","timestamp":"1635424080.0"},{"timestamp":"1635405360.0","comment_id":"282292","content":"A,C,E & F can all be correct, BUT for E (EC2), it says \"specifying whether training will be run using CPU or GPU\" - when we select the machine size, we don't specify whether to use CPU or GPU, so I think that option is out. Answer should be ACF","upvote_count":"3","poster":"scuzzy2010"},{"poster":"Joe_Zhang","upvote_count":"4","timestamp":"1635261120.0","content":"FOR SAGEMAKER, THE TRAINING JOB DATA MUST FROM S3. SO AEF","comment_id":"280529"},{"content":"The answer is ACF according to the following documentation: The URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n\nThe compute resources that you want SageMaker to use for model training. Compute resources are ML compute instances that are managed by SageMaker.\n\nThe URL of the S3 bucket where you want to store the output of the job.\n\nThe Amazon Elastic Container Registry path where the training code is stored.","poster":"DzR","upvote_count":"5","comment_id":"263441","timestamp":"1634972820.0"},{"upvote_count":"1","content":"I think it's \"A,E,F\" as the documentation discuss ARN of IAM not IAM itself. Maybe wordings matter in the choices we make. \n\nThis is the link: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html#API_CreateTrainingJob_RequestParameters","comment_id":"169504","poster":"MennaMN","timestamp":"1634765580.0"},{"content":"This link states that the answer is AEF.\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html","timestamp":"1634486520.0","comment_id":"167663","poster":"bidds","comments":[{"timestamp":"1634656740.0","upvote_count":"1","poster":"bidds","content":"Then again, this seems to contradict that:\nhttps://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html#API_CreateTrainingJob_RequestParameters","comment_id":"167773"}],"upvote_count":"2"},{"content":"C E and F","poster":"Antriksh","comment_id":"108391","timestamp":"1634310000.0","upvote_count":"2"},{"upvote_count":"3","poster":"roytruong","comment_id":"98724","content":"CEF, see this https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html","timestamp":"1634198400.0"},{"poster":"PRC","timestamp":"1633976640.0","upvote_count":"1","comment_id":"65586","content":"CEF is correct"},{"comments":[{"poster":"L2007","comment_id":"57652","upvote_count":"1","content":"Note\nTo be able to pass this role to Amazon SageMaker, the caller of this API must have the iam:PassRole permission.\n\nhttps://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-training-job.html\n CEF is correct","timestamp":"1633768560.0"},{"content":"I agree with you in \"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf\". It mentioned that\n\"To train a model in Amazon SageMaker, you create a training job. The training job includes the following\ninformation:\nâ€¢ The URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training\ndata.\nâ€¢ The compute resources that you want Amazon SageMaker to use for model training. Compute\nresources are ML compute instances that are managed by Amazon SageMaker.\nâ€¢ The URL of the S3 bucket where you want to store the output of the job.\nâ€¢ The Amazon Elastic Container Registry path where the training code is stored. For more information.\"","comment_id":"64424","upvote_count":"1","poster":"georgeZ","timestamp":"1633785300.0"}],"timestamp":"1633750320.0","content":"Take a look at this Link\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/API_TrainingJobDefinition.html\n\nSearch for \"Required: Yes\"\n\nC is not even there ..... Out of the available options you will find AEF to be the right ones","upvote_count":"3","poster":"rajs","comment_id":"57290"},{"upvote_count":"5","content":"CEF is the right answer, though the documentation specifies RoleARN instead of IAM role.\n\nCreateTrainingJob requires: AlgorithmSpecification, OutputDataConfig, ResourceConfig, RoleARN, Stopping Condition, TrainingJobName\n\nAll other parameters are optional, including InputDataConfig.","poster":"devsean","comment_id":"50190","timestamp":"1633639140.0"},{"poster":"BigEv","comment_id":"42173","timestamp":"1633531320.0","upvote_count":"4","content":"Agree with CEF\nNot A: training data can be loaded from EFS."},{"comment_id":"36428","content":"A is wrong as training data is optional parameter.","upvote_count":"1","poster":"vetal","timestamp":"1633402140.0"},{"content":"answer should be ACE?","upvote_count":"1","comment_id":"35148","poster":"WWODIN","timestamp":"1633360800.0"},{"poster":"JayK","timestamp":"1633168380.0","comment_id":"34668","upvote_count":"3","content":"The answer is AEF, it is not C because the documentation talks about ARN not IAM"},{"upvote_count":"4","content":"That is right, the correct answer is CEF. The given dos state many required attributes but only C, E, and F are here on the list.","timestamp":"1633006080.0","poster":"vetal","comment_id":"28486"},{"content":"https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/API_CreateTrainingJob.html","comment_id":"21907","timestamp":"1632702060.0","poster":"DonaldCMLIN","upvote_count":"2"}],"answer_images":[],"answer_description":"","question_id":224,"unix_timestamp":1573884300,"answers_community":["ACF (45%)","CEF (37%)","Other"],"url":"https://www.examtopics.com/discussions/amazon/view/8316-exam-aws-certified-machine-learning-specialty-topic-1/","exam_id":26,"answer":"ACF","choices":{"E":"The Amazon EC2 instance class specifying whether training will be run using CPU or GPU.","B":"The validation channel identifying the location of validation data on an Amazon S3 bucket.","C":"The IAM role that Amazon SageMaker can assume to perform tasks on behalf of the users.","D":"Hyperparameters in a JSON array as documented for the algorithm used.","F":"The output path specifying where on an Amazon S3 bucket the trained model will persist.","A":"The training channel identifying the location of training data on an Amazon S3 bucket."},"question_text":"When submitting Amazon SageMaker training jobs using one of the built-in algorithms, which common parameters MUST be specified? (Choose three.)"},{"id":"ezuvaQpp6RAumn9WnO43","unix_timestamp":1707253320,"question_text":"A university wants to develop a targeted recruitment strategy to increase new student enrollment. A data scientist gathers information about the academic performance history of students. The data scientist wants to use the data to build student profiles. The university will use the profiles to direct resources to recruit students who are likely to enroll in the university.\n\nWhich combination of steps should the data scientist take to predict whether a particular student applicant is likely to enroll in the university? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/133098-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2024-02-06 22:02:00","choices":{"C":"Use a regression algorithm to run predictions.","E":"Use the built-in Amazon SageMaker k-means algorithm to cluster the data into two groups named \"enrolled\" or \"not enrolled.\"","A":"Use Amazon SageMaker Ground Truth to sort the data into two groups named \"enrolled\" or \"not enrolled.\"","B":"Use a forecasting algorithm to run predictions.","D":"Use a classification algorithm to run predictions."},"isMC":true,"answer_description":"","discussion":[{"content":"Correct AD","poster":"ggrodskiy","timestamp":"1730050680.0","comment_id":"1203188","upvote_count":"1"},{"poster":"vkbajoria","timestamp":"1725409080.0","content":"Selected Answer: AD\nfirst, classify the student profiles. then use classification algorithm to run predictions","comment_id":"1165243","upvote_count":"2"},{"comment_id":"1162719","poster":"Stokvisss","upvote_count":"3","content":"Selected Answer: AD\nK-means is unsupervised, so not useful for clustering. For grouping, use GroundTruth. Itâ€™s a classification problem. So, A and D are right.","timestamp":"1724936160.0"},{"timestamp":"1724756280.0","poster":"Adzz","content":"Selected Answer: AD\nThis question is focusing on either yes/no type of response (binary). So I think Classification algorithm would work the best as compared to K-means which is solely responsible for clustering the data.","comment_id":"1160602","upvote_count":"2"},{"content":"D. Use a classification algorithm to run predictions: This approach is suitable for binary outcomes, such as predicting whether a student will enroll (\"enrolled\") or not (\"not enrolled\").\n\nA. Use Amazon SageMaker Ground Truth to sort the data into two groups named \"enrolled\" or \"not enrolled.\": This service can help in labeling the dataset accurately, providing a strong foundation for training the classification model.","comment_id":"1145878","timestamp":"1723244040.0","poster":"Alice1234","upvote_count":"1"},{"comment_id":"1143154","content":"Selected Answer: AD\nThe data scientist should use Amazon SageMaker Ground Truth to sort the data into two groups\nnamed \"enrolled\" or \"not enrolled.\" This will create a labeled dataset that can be used for supervised\nlearning. The data scientist should then use a classification algorithm to run predictions on the test\ndata. A classification algorithm is a suitable choice for predicting a binary outcome, such as\nenrollment status, based on the input features, such as academic performance. A classification\nIT Certification Guaranteed, The Easy Way!\n163\nalgorithm will output a probability for each class label and assign the most likely label to each\nobservation.\nReferences:\nUse Amazon SageMaker Ground Truth to Label Data\nClassification Algorithm in Machine Learning","upvote_count":"2","poster":"kyuhuck","timestamp":"1723016760.0"},{"content":"Selected Answer: AE\nIt mentions combination of options. It is a classification problem and labels will be needed.","poster":"delfoxete","comment_id":"1142594","timestamp":"1722970920.0","upvote_count":"2"}],"question_id":225,"answer_ET":"AD","exam_id":26,"answer":"AD","answers_community":["AD (82%)","AE (18%)"],"answer_images":[],"topic":"1","question_images":[]}],"exam":{"numberOfQuestions":369,"id":26,"isMCOnly":false,"name":"AWS Certified Machine Learning - Specialty","lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Amazon","isBeta":false},"currentPage":45},"__N_SSP":true}