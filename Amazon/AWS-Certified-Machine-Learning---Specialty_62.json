{"pageProps":{"questions":[{"id":"OEg9KzbVXA4YNxK0TwcL","answer_ET":"C","discussion":[{"timestamp":"1663639080.0","upvote_count":"19","poster":"cybe001","comment_id":"38251","content":"C is correct"},{"timestamp":"1667129580.0","poster":"syu31svc","upvote_count":"5","content":"You want to reduce features/dimension so PCA is the answer","comment_id":"169006"},{"timestamp":"1722425040.0","upvote_count":"3","poster":"kaike_reis","comment_id":"968013","content":"Selected Answer: C\nC is the way"},{"content":"Selected Answer: C\nC is correct.\nD could be correct if the correlation is used to omit features.","timestamp":"1722398580.0","upvote_count":"1","comment_id":"967731","poster":"FloKo"},{"upvote_count":"1","comment_id":"832740","content":"Selected Answer: C\nPCA and T-SNE are for solving the curse of dimensionality mencioned here!","timestamp":"1709888640.0","poster":"Valcilio"},{"timestamp":"1705151040.0","upvote_count":"1","comment_id":"774502","poster":"DS2021","content":"I assume PCA is for unsupervised learning!...and the scenario in the question looks like supervised learning","comments":[{"upvote_count":"1","content":"data (x, y) --> (PCA) --> preprocessed data(x', y) --> learning \nwhy not for supervised learning?","comment_id":"810297","poster":"GiyeonShin","timestamp":"1708062600.0"}]},{"upvote_count":"1","content":"Selected Answer: C\nTricky. The sentence 'many features are highly correlated with each other' is no use.","timestamp":"1702121280.0","poster":"BethChen","comment_id":"740035","comments":[]},{"timestamp":"1694418060.0","upvote_count":"1","comment_id":"665918","content":"Answer C: Read through this carefully \"What should be done to reduce the impact of having such a large number of features?\" only answer comes in mind PCA","poster":"Shailendraa"},{"content":"Of course, it's PCA.","poster":"Urban_Life","comment_id":"125902","timestamp":"1665938940.0","upvote_count":"1"},{"comment_id":"102656","timestamp":"1663934400.0","upvote_count":"2","poster":"C10ud9","content":"PCA is the solution. So, answer is C"}],"timestamp":"2020-01-12 22:27:00","topic":"1","unix_timestamp":1578864420,"isMC":true,"question_id":306,"question_text":"A Machine Learning Specialist is building a prediction model for a large number of features using linear models, such as linear regression and logistic regression.\nDuring exploratory data analysis, the Specialist observes that many features are highly correlated with each other. This may make the model unstable.\nWhat should be done to reduce the impact of having such a large number of features?","url":"https://www.examtopics.com/discussions/amazon/view/11851-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"C","answers_community":["C (100%)"],"answer_images":[],"choices":{"B":"Use matrix multiplication on highly correlated features.","C":"Create a new feature space using principal component analysis (PCA)","D":"Apply the Pearson correlation coefficient.","A":"Perform one-hot encoding on highly correlated features."},"exam_id":26,"answer_description":"","question_images":[]},{"id":"YeePJg0vfri1HJgHSk74","answer":"A","answer_ET":"A","choices":{"D":"Binomial distribution","A":"Poisson distribution","C":"Normal distribution","B":"Uniform distribution"},"answers_community":["A (86%)","14%"],"topic":"1","question_text":"A Machine Learning Specialist is implementing a full Bayesian network on a dataset that describes public transit in New York City. One of the random variables is discrete, and represents the number of minutes New Yorkers wait for a bus given that the buses cycle every 10 minutes, with a mean of 3 minutes.\nWhich prior probability distribution should the ML Specialist use for this variable?","unix_timestamp":1573912980,"exam_id":26,"question_id":307,"question_images":[],"answer_images":[],"answer_description":"","isMC":true,"timestamp":"2019-11-16 15:03:00","url":"https://www.examtopics.com/discussions/amazon/view/8339-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"content":"A \nIf you have information about the average (mean) number of things that happen in some given time period / interval,Poisson distribution can give you a way to predict the odds of getting some other value on a given future day","timestamp":"1632406140.0","comment_id":"39791","upvote_count":"56","poster":"ComPah"},{"upvote_count":"8","timestamp":"1632594960.0","content":"Ans: A\nhttps://brilliant.org/wiki/poisson-distribution/","comment_id":"77530","poster":"swagy"},{"comments":[{"upvote_count":"1","content":"Why Not the Other Options?\n\n(A) Poisson Distribution\n Poisson is used for counting discrete events over a fixed period (e.g., number of buses arriving per hour). Since waiting time is continuous, Poisson is not appropriate.\n(C) Normal Distribution\n Normal (Gaussian) distribution assumes values cluster around the mean and extend infinitely. Here, waiting time is evenly spread between 0–10 minutes, not forming a bell curve.\n(D) Binomial Distribution\n Binomial is used for counting successes in a fixed number of trials (e.g., flipping a coin multiple times). Waiting time is continuous, not a count of discrete occurrences.","timestamp":"1738510680.0","comment_id":"1350523","poster":"Mobasher"}],"timestamp":"1738510680.0","content":"Selected Answer: B\nChatGPT's answer: B \n\nExplanation\n\nThe problem describes a random variable representing the waiting time for a bus, where buses arrive every 10 minutes, and the mean waiting time is 3 minutes.\nIn such a periodic arrival process, the waiting time follows a Uniform Distribution because:\n - Any given person’s waiting time is equally likely to be any value between 0 and 10 minutes.\n - There is no clustering around a particular value—every moment within the cycle is equally probable.\n\nThus, the waiting time follows a Uniform(0, 10) distribution.","comment_id":"1350522","upvote_count":"1","poster":"Mobasher"},{"content":"Selected Answer: B\nBuses cycle every 10 minutes, and waiting time can be modeled as a uniform random variable between [0, 10] minutes.\nThe average waiting time of 3 minutes suggests that waiting is uniformly distributed, not event-based like Poisson.\nIf buses arrive every 10 minutes and riders arrive randomly, the waiting time follows a Uniform Distribution (B) because:\n\nThe arrival process is regular (every 10 minutes).\nThere’s no stochastic randomness in the bus arrival schedule, ruling out Poisson.\nPoisson would apply if buses arrived randomly at an average rate rather than at fixed intervals.","poster":"growe","upvote_count":"2","comment_id":"1334403","timestamp":"1735591140.0"},{"comment_id":"1315565","content":"B\nPoisson is suitable for modeling the number of events (like buses arriving) in a fixed time frame, not the time between events when the events occur at regular intervals. The waiting time variable is not about the count of buses but rather the time to the next bus, which is evenly distributed.","upvote_count":"1","poster":"87ebc7d","timestamp":"1732148160.0"},{"content":"Selected Answer: A\nA is correct","comment_id":"1067015","timestamp":"1699601100.0","upvote_count":"1","poster":"elvin_ml_qayiran25091992razor"},{"poster":"Fred93","content":"Selected Answer: A\nPoisson distribution is discrete, and gives the number of events that occur in a given time interval","upvote_count":"2","comment_id":"1015660","timestamp":"1695550200.0"},{"comment_id":"1006631","upvote_count":"2","timestamp":"1694609580.0","poster":"loict","content":"Selected Answer: A\nA. YES - Poisson distribution is discrete, and gives the number of events that occur in a given time interval\nB. NO - Uniform distribution is continuous, we want discrete\nC. NO - Normal distribution is continuous we want discrete\nD. NO - Binomial distribution give the probability that a random variable is A or B (possibly in with different weight)"},{"poster":"Mickey321","timestamp":"1693246440.0","content":"Selected Answer: A\nOption A indeed","comment_id":"992543","upvote_count":"1"},{"content":"Selected Answer: A\nANSWER IS A\nhttps://www.investopedia.com/terms/d/discrete-distribution.asp","poster":"Nadia0012","timestamp":"1678189020.0","comment_id":"831817","upvote_count":"2"},{"timestamp":"1676886660.0","poster":"bakarys","upvote_count":"4","comment_id":"815065","content":"Selected Answer: A\nThe Poisson distribution is commonly used for count data, which is the case here as we are interested in the number of minutes New Yorkers wait for a bus. The Poisson distribution is characterized by a single parameter, lambda, which represents the mean and variance of the distribution. In this case, the mean is 3 minutes, so we would set lambda to 3. The Poisson distribution assumes that events occur independently of each other, which is a reasonable assumption in this case since the waiting time for each individual is likely to be independent of the waiting time for others."},{"comment_id":"804247","content":"Selected Answer: A\nThe Poisson distribution is a discrete probability distribution that is commonly used to model the number of events that occur in a fixed interval of time, given an average rate of occurrence. \n\nSince the buses cycle every 10 minutes and the mean wait time is 3 minutes, it is reasonable to assume that the number of minutes New Yorkers wait for a bus can be modeled by a Poisson distribution.","timestamp":"1676028720.0","upvote_count":"3","poster":"AjoseO"},{"comment_id":"777031","timestamp":"1673814120.0","poster":"Tomatoteacher","content":"Selected Answer: A\n100% A, as discrete, while binomial has to be binary data (success or failure)","upvote_count":"1"},{"upvote_count":"1","poster":"Sonoko","timestamp":"1670876640.0","content":"Selected Answer: A\nA is a discrete distribution","comment_id":"743303"},{"poster":"Peeking","content":"I do choose Poisson. A.","comment_id":"739619","timestamp":"1670545620.0","upvote_count":"1"},{"comment_id":"667387","content":"12-sep exam","poster":"Shailendraa","timestamp":"1663009440.0","upvote_count":"3"},{"comment_id":"665919","poster":"Shailendraa","content":"Answer is A .. these types on footfalls ,etc ..answer always Poisson-distribution","upvote_count":"1","timestamp":"1662882180.0"},{"poster":"KM226","comment_id":"510357","upvote_count":"1","content":"Selected Answer: A\nI agree that the answer is A: because the Poisson distribution is a probability distribution that is used to show how many times that buses are likely to occur over a specified period.","timestamp":"1640613720.0"},{"comment_id":"413341","timestamp":"1636287840.0","poster":"lcovarrubias","upvote_count":"1","content":"Definitely A, Poisson."},{"content":"A!!!!!!!!","poster":"tmld","comment_id":"335499","timestamp":"1636274100.0","upvote_count":"1"},{"timestamp":"1636021020.0","content":"A!!!!!!!!","upvote_count":"2","comment_id":"312308","poster":"littlewat"},{"poster":"takahirokoyama","content":"Completely A.\nCheck this site.(https://jakevdp.github.io/blog/2018/09/13/waiting-time-paradox/)","timestamp":"1635589680.0","comment_id":"282787","upvote_count":"3"},{"comment_id":"263964","content":"When ever you are given the mean occurrence of things in an interval, the best distribution to model such an event is Poisson. A is the correct answer.","upvote_count":"2","poster":"DzR","timestamp":"1635075720.0"},{"timestamp":"1634701200.0","comment_id":"263265","content":"Anwser is A, I am no expert here but if you just google \" buses cycle every 10 minutes, with a mean of 3 minutes\" , resulting page talk about Poisson distribution","upvote_count":"2","poster":"harmanbirstudy"},{"upvote_count":"2","comment_id":"241152","poster":"shoshi","timestamp":"1634172900.0","comments":[{"content":"what was the right response for this question?","comment_id":"243260","timestamp":"1634296980.0","upvote_count":"4","poster":"bm25"},{"upvote_count":"1","content":"I have mine tommorow .. what was your reply ?","comment_id":"263257","poster":"harmanbirstudy","timestamp":"1634557680.0"}],"content":"Hi, I passed the exam today, this was my question. good luck all"},{"content":"uniform distribution. Note the word \"every\". No randomness in bus arrivals is assumed here.\nhttps://stats.stackexchange.com/questions/122722/please-explain-the-waiting-paradox\nhttps://www.reddit.com/r/askscience/comments/59qkk7/how_does_the_wait_time_paradox_work/\nDon't rely on the phrase\"prior probability\".","timestamp":"1634053680.0","upvote_count":"2","comment_id":"217334","poster":"Thai_Xuan"},{"timestamp":"1633810500.0","upvote_count":"1","poster":"sebtac","content":"Answer: A. it is a classical Poisson Example","comment_id":"174050"},{"comment_id":"169008","timestamp":"1633480920.0","upvote_count":"2","content":"The Poisson distribution is used to model the number of events occurring within a given time interval.\nSo answer is A","poster":"syu31svc"},{"comment_id":"149368","content":"C is the answer -- Normal Check https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/statistical-distributions","comments":[{"comment_id":"226762","poster":"ybad","content":"Normal distribution is a continuous probability distribution, thats why poisson works better, since it is discrete!","timestamp":"1634088360.0","upvote_count":"4"}],"poster":"GeeBeeEl","timestamp":"1633352040.0","upvote_count":"1"},{"timestamp":"1633148280.0","poster":"Antriksh","content":"Poisson distribution it is","upvote_count":"3","comment_id":"108589"},{"timestamp":"1633002360.0","upvote_count":"6","content":"definitely Poisson distribution. No two ways about it. this a poisson process.","poster":"DScode","comment_id":"107058"},{"timestamp":"1632991920.0","content":"leaning towards A\nhttps://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/","comment_id":"89396","upvote_count":"3","poster":"deep_n"},{"content":"Answer is D.\nBinomial distribution: A binomial random variable is the number of successes in n trials of a random experiment. A random variable x is said to follow binomial distribution when, the random variable can have only two outcomes(success and failure).\nhttps://towardsdatascience.com/probability-and-statistics-explained-in-the-context-of-deep-learning-ed1509b2eb3f","comments":[{"comments":[{"comments":[{"upvote_count":"2","poster":"felbuch","timestamp":"1635097680.0","content":"It is not a Poisson distribution, because if it were, there would be no maximum time a person could stand waiting for the bus. But since buses go about every 10 minutes, then no person will wait for more than 10 minutes. So it's a Binomial Distr.","comment_id":"274165"}],"upvote_count":"8","timestamp":"1632551700.0","content":"I don't think the answer is just yes or no.. it is supposed to be a wait time from 0 to 10 minutes, with an average of 3 min. I think the answer should be A. Poisson Distr.","comment_id":"63059","poster":"VB"}],"poster":"cybe001","content":"Since the variable is discrete, all the possible values can be grouped into two","comment_id":"40724","upvote_count":"1","timestamp":"1632473160.0"}],"poster":"cybe001","comment_id":"38284","upvote_count":"1","timestamp":"1632399420.0"},{"timestamp":"1632352740.0","content":"SINCE \"prior probability distribution\", THE ANSWER B.","comments":[{"poster":"gaow","content":"But why the Suggested Answer by you for this question is D?","timestamp":"1633968240.0","comment_id":"197059","upvote_count":"1"}],"comment_id":"21962","poster":"DonaldCMLIN","upvote_count":"4"},{"comment_id":"21961","poster":"DonaldCMLIN","upvote_count":"3","timestamp":"1632326580.0","comments":[{"comment_id":"274170","content":"A person will wait no more than 10 minutes, and no less than 0 minutes. If the distribution were normal, this would mean that the average waiting time should be 5 minutes. But it's not. The average waiting time is 3 minutes. So it cannot be a Uniform distribution.","timestamp":"1635136980.0","upvote_count":"2","poster":"felbuch"},{"poster":"Aanish","upvote_count":"2","content":"I don't think prior probability distribution is the keyword here. A prior probability distribution can be any distribution. Key idea is the interval between the buses and the waiting period, which is discrete.","comment_id":"252586","timestamp":"1634453400.0"}],"content":"KEY WORD IS \"prior probability distribution\"\n\nDON'T CARE UNKNOW OR UNPRECIDT EVENT WILL CHANGE INTERVALS \nUNIFORM THE GAPFOR PREDICT,\nhttp://jakevdp.github.io/blog/2018/09/13/waiting-time-paradox/"}]},{"id":"LXssBUWhXFN8cdjesgzw","isMC":true,"answer_ET":"C","answer_description":"","exam_id":26,"timestamp":"2019-11-16 15:20:00","question_text":"A Data Science team within a large company uses Amazon SageMaker notebooks to access data stored in Amazon S3 buckets. The IT Security team is concerned that internet-enabled notebook instances create a security vulnerability where malicious code running on the instances could compromise data privacy.\nThe company mandates that all instances stay within a secured VPC with no internet access, and data communication traffic must stay within the AWS network.\nHow should the Data Science team configure the notebook instance placement to meet these requirements?","choices":{"C":"Associate the Amazon SageMaker notebook with a private subnet in a VPC. Ensure the VPC has S3 VPC endpoints and Amazon SageMaker VPC endpoints attached to it.","D":"Associate the Amazon SageMaker notebook with a private subnet in a VPC. Ensure the VPC has a NAT gateway and an associated security group allowing only outbound connections to Amazon S3 and Amazon SageMaker.","B":"Associate the Amazon SageMaker notebook with a private subnet in a VPC. Use IAM policies to grant access to Amazon S3 and Amazon SageMaker.","A":"Associate the Amazon SageMaker notebook with a private subnet in a VPC. Place the Amazon SageMaker endpoint and S3 buckets within the same VPC."},"url":"https://www.examtopics.com/discussions/amazon/view/8343-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"poster":"DonaldCMLIN","comment_id":"21968","upvote_count":"35","content":"NAT gateway COULD GO OUT TO THE INTERNET AND DOWNLOAD BACK MALICIOUS\n D. IS NOT A GOOD ANSWER.\n \n THE SAFE ONE IS ANSWER C. ASSOCIATE WITH VPC_ENDPOINT AND S3_ENDPOINT","timestamp":"1663641900.0"},{"upvote_count":"23","content":"C is correct\nWe must use the VPC endpoint (either Gateway Endpoint or Interface Endpoint)to comply with this requirement \"Data communication traffic must stay within the AWS network\". \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/notebook-interface-endpoint.html","poster":"BigEv","timestamp":"1664112360.0","comment_id":"43532"},{"content":"Selected Answer: C\nA. NO - We don't place a S3 bucket in a VPC, it is always in AWS Service Account\nB. NO - without an S3 VPC endpoint, traffic will go through the Internet\nC. YES - we need endpoints for both SageMaker and S3 to avoid Internet traffic\nD. NO - we need endpoints for both SageMaker and S3 to avoid Internet traffic","upvote_count":"2","poster":"loict","comment_id":"1006644","timestamp":"1726232400.0"},{"content":"Selected Answer: C\nOption C","upvote_count":"1","timestamp":"1724866380.0","poster":"Mickey321","comment_id":"992462"},{"poster":"kaike_reis","content":"Selected Answer: C\nC is the correct. A is not so correct, because it's possible to communicate two different VPCs inside AWS network (which is not optimized).","comment_id":"968369","timestamp":"1722447960.0","upvote_count":"1"},{"content":"Selected Answer: C\nThis configuration would meet the company's requirements for security, as the notebook instance would be placed within a private subnet in a VPC, and data communication traffic would stay within the AWS network through the use of VPC endpoints for S3 and Amazon SageMaker. \n\nAdditionally, the VPC would not have internet access, further reducing the security risk.","comment_id":"804253","timestamp":"1707565020.0","poster":"AjoseO","upvote_count":"2"},{"content":"C - \"and data communication traffic must stay within the AWS network.\" that discards D","upvote_count":"2","timestamp":"1694897040.0","comment_id":"671110","poster":"rb39"},{"upvote_count":"4","poster":"StelSen","comment_id":"353532","timestamp":"1667382180.0","content":"Answer should be C. Because, Security team don't want Internet Access, Option-D has NAT and will get to Internet somehow. Also connecting S3 and SageMaker EC2 instance via VPC endpoints is best way to secure the resources."},{"comment_id":"278265","poster":"cloud_trail","upvote_count":"2","content":"Using a NAT gateway is the old way to do it. Option C is the way to do it now. https://cloudacademy.com/blog/vpc-endpoint-for-amazon-s3/#:~:text=Accessing%20S3%20the%20old%20way%20%28without%20VPC%20Endpoint%29,has%20no%20access%20to%20any%20outside%20public%20resources","timestamp":"1667346240.0"},{"comment_id":"263366","poster":"harmanbirstudy","upvote_count":"3","timestamp":"1667299020.0","content":"\"and data communication traffic must stay within the AWS network\" , NAT gateway will always go over the Internet to access S3.with NAT you can put your instances in private subnet and NAT itself in public subnet , but still in order to access S3 it will go over the internet.\nSO answer cannot be D.\n-- C is the only correct option here , as S3 VPC endpoints is a real thing \"google it\" and it sole purpose is to create route from VPC endpoint to S3 , without going over the Internet."},{"upvote_count":"3","timestamp":"1667229420.0","poster":"scuzzy2010","comment_id":"228667","content":"C is correct answer. D is only applicable -\"If your model needs access to an AWS service that doesn't support interface VPC endpoints or to a resource outside of AWS, create a NAT gateway and configure your security groups to allow outbound connections. \"\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html"},{"comment_id":"221667","upvote_count":"1","content":"D is correct","poster":"v24143","timestamp":"1667206320.0"},{"poster":"krakow1234","content":"Answer is D, read third paragraph https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html","upvote_count":"1","timestamp":"1667105580.0","comment_id":"217977"},{"timestamp":"1666655040.0","upvote_count":"1","content":"NAT is the way that a VPC connect to internet and other ASW service when there is NO INTERNET ACCESS FOR VPC. Thus the answer is D.","poster":"Potato_Noodle","comment_id":"194950"},{"comments":[{"poster":"yeetusdeleetus","comment_id":"212445","content":"This is the correct answer.\n\nIf this answer is confusing, study some of the associate exams before going for this one. VPC endpoint and NAT gateway are similar, but NAT gateway is for giving resources in the VPC the chance to initiate connections with the internet, whereas a VPC endpoint only allows it to go to other AWS services, which is the best solution for this question.","upvote_count":"2","timestamp":"1666870200.0"}],"content":"\"concerned that internet-enabled notebook instances create a security vulnerability where malicious code running on the instances could compromise data privacy.\" NAT Gateway does not mitigate this risk!","comment_id":"180160","timestamp":"1666268520.0","poster":"Th3Dud3","upvote_count":"2"},{"content":"C: \nIf you configure your VPC so that it doesn't have internet access, models that use that VPC do not have access to resources outside your VPC. If your model needs access to resources outside your VPC, provide access with one of the following options:\n\nIf your model needs access to an AWS service that supports interface VPC endpoints, create an endpoint to connect to that service. For a list of services that support interface endpoints, see VPC Endpoints in the Amazon VPC User Guide. For information about creating an interface VPC endpoint, see Interface VPC Endpoints (AWS PrivateLink) in the Amazon VPC User Guide.\n\nIf your model needs access to an AWS service that doesn't support interface VPC endpoints or to a resource outside of AWS, create a NAT gateway and configure your security groups to allow outbound connections. For information about setting up a NAT gateway for your VPC, see Scenario 2: VPC with Public and Private Subnets (NAT) in the Amazon Virtual Private Cloud User Guide.","poster":"Th3Dud3","comment_id":"176923","timestamp":"1666153080.0","upvote_count":"5"},{"timestamp":"1666018860.0","comments":[{"comment_id":"707863","content":"It is not enough for sagemaker to communicate to S3 if both of them are inside the same VPC. Sagamaker inside a VPC needs to create a endpoint to connect to other AWS services which has endpoint too.","upvote_count":"1","poster":"jrff","timestamp":"1698677100.0"}],"comment_id":"174051","upvote_count":"1","poster":"sebtac","content":"what is the difference between A & C? are both answers OK?"},{"upvote_count":"1","poster":"syu31svc","comment_id":"169011","comments":[{"comments":[{"upvote_count":"1","comment_id":"415371","content":"You can connect to an S3 bucket through an S3 vpc endpoint","poster":"msamory","timestamp":"1667661480.0"}],"comment_id":"371657","timestamp":"1667384760.0","content":"S3 buckets don’t live in a VPC","upvote_count":"2","poster":"AShahine21"}],"content":"I would say the answer is C\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/interface-vpc-endpoint.html","timestamp":"1665936540.0"},{"timestamp":"1665894420.0","poster":"GeeBeeEl","content":"See https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html\nA is wrong, S3 buckets don’t live in a VPC\nB is wrong, you don’t use IAM policies to grant access to SageMaker\nC may be right depending on what “it” is --- attached to it If by it, this is talking about the notebook VPC, then yes\nD talks of security group and NAT gateway which was mentioned in the link. D is definitely correct based on link","upvote_count":"2","comment_id":"149373"},{"comment_id":"144576","poster":"haison8x","upvote_count":"1","comments":[{"timestamp":"1666884960.0","content":"take a look at https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html#train-vpc-s3","comment_id":"215124","poster":"Thai_Xuan","upvote_count":"2"}],"timestamp":"1665849720.0","content":"I think the answer is D, there are no such thing as S3 VPC endpoints and Amazon SageMaker"},{"upvote_count":"3","content":"Answer C\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html","poster":"tff","comment_id":"125500","timestamp":"1665586860.0"},{"comment_id":"121851","poster":"Wira","timestamp":"1665538440.0","content":"C \nvery obvious one\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/interface-vpc-endpoint.html","upvote_count":"2"},{"timestamp":"1665132540.0","comment_id":"73484","content":"The correct answer HAS TO be C. No internet access is the key word here.","poster":"dhs227","upvote_count":"3"},{"upvote_count":"1","comment_id":"58823","poster":"AKT","timestamp":"1665041940.0","content":"D is correct."},{"timestamp":"1664828880.0","upvote_count":"1","content":"This should be D","comment_id":"57870","comments":[{"upvote_count":"5","comment_id":"57871","poster":"kanto","timestamp":"1664966340.0","content":"correction: C actually.."}],"poster":"kanto"},{"poster":"Phong","comment_id":"51104","timestamp":"1664747520.0","upvote_count":"6","content":"go for C"},{"content":"\"The company mandates that all instances stay within a secured VPC with no internet access\" only VPC endpoint fullfills this requirement! C is correct.","comment_id":"46876","timestamp":"1664554440.0","upvote_count":"3","poster":"tap123"},{"upvote_count":"1","timestamp":"1664169840.0","content":"NAT Gateway has to be in public subnet. C is the best option looks like","poster":"ComPah","comment_id":"44475"},{"upvote_count":"3","timestamp":"1664100900.0","comments":[{"timestamp":"1665908700.0","poster":"scuzzy2010","upvote_count":"1","content":"Not if VPC endpoints are attached for accessing S3.","comment_id":"152734"},{"poster":"ChKl","content":"I think thats wrong. You only need access to the internet in case you want to download packages or libraries that are not part of the instance image that the notebook is running on.","upvote_count":"3","timestamp":"1665505560.0","comment_id":"111492"}],"comment_id":"40418","content":"D is correct. Sagemaker EC2 instances need internet access to run. If you want to use private VPN then NAT needs to be attached to the private VPN for Sagemaker to work.\nhttps://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/","poster":"cybe001"},{"upvote_count":"1","comment_id":"34401","poster":"lanny","timestamp":"1663660800.0","comments":[{"content":"WRONG. NO INTERNET ACCESS. ANSWER IS C","upvote_count":"14","comment_id":"37945","poster":"learnaws","timestamp":"1664054220.0"}],"content":"D is correct"}],"topic":"1","answers_community":["C (100%)"],"unix_timestamp":1573914000,"answer":"C","question_images":[],"question_id":308,"answer_images":[]},{"id":"jL3W0QKlQDH3bOTQ3o8H","question_text":"A Machine Learning Specialist has created a deep learning neural network model that performs well on the training data but performs poorly on the test data.\nWhich of the following methods should the Specialist consider using to correct this? (Choose three.)","answer":"BCF","answer_description":"","answers_community":["BCF (82%)","BCE (18%)"],"exam_id":26,"answer_ET":"BCF","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/8348-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","answer_images":[],"timestamp":"2019-11-16 16:10:00","choices":{"A":"Decrease regularization.","C":"Increase dropout.","D":"Decrease dropout.","E":"Increase feature combinations.","F":"Decrease feature combinations.","B":"Increase regularization."},"question_id":309,"isMC":true,"unix_timestamp":1573917000,"discussion":[{"upvote_count":"24","comment_id":"38285","content":"Yes, answer is BCF","timestamp":"1632650280.0","poster":"cybe001"},{"upvote_count":"14","content":"Go for BCF","poster":"Phong","timestamp":"1633574220.0","comment_id":"51106"},{"content":"Selected Answer: BCF\nI think here the point is around the definition of \"feature combinations\". \n\nIf you refer to it as \"combine the features to generate a smaller but more effective feature set\" this would end up to a smaller feature set thus a good thing for overfitting.\n\nHowever, if you refer to it as \"combine the features to generate additional features\" this would end up to a larger feature set thus a bad thing for overfitting.\n\nAlso, in some cases you implement feature combinations in your model (see hidden layers in feed-forward network) thus increasing model complexity which is bad for overfitting.\n\nTo me this question is poorly worded. I would pick F as my best guess is that you need to implement feature combination in your model, thus decreasing feature combination decrease complexity hence improving with overfitting issue","comments":[{"timestamp":"1720494240.0","content":"Great callout - what exactly the Feature combination is performing has not been elaborated\n\nIt can be: Using PCA or t-SNE, it is essentially optimizing features - good to address overfitting, and should be done\n\nOr, it can be: Using Cartesian Product, features are being combined to create additional features - this will aid overfitting and should NOT be done.\n\nWish questions and answer options are written clearly so that there is no room for ambiguity. Especially, taking into account that in real life, these kind of communication/write-up will trigger follow-up questions until addressed satisfactorily.","upvote_count":"1","poster":"cloudera3","comment_id":"1244657"}],"timestamp":"1718947080.0","comment_id":"1234196","upvote_count":"5","poster":"ninomfr64"},{"timestamp":"1712169960.0","comment_id":"1188843","content":"Selected Answer: BCE\nAbout option E: \nWhen increasing feature combinations, the goal is not to simply add more features indiscriminately, which could indeed lead to overfitting. Instead, it involves selecting and combining features in a way that captures important patterns and relationships in the data.\nWhen done effectively, increasing feature combinations can help the model generalize better to unseen data by providing more informative and discriminative features, thus reducing the risk of overfitting.","poster":"Denise123","upvote_count":"1"},{"upvote_count":"1","poster":"Piyush_N","timestamp":"1709446920.0","comment_id":"1164534","content":"Selected Answer: BCF\nIf your model is overfitting the training data, it makes sense to take actions that reduce model flexibility. To reduce model flexibility, try the following:\n\nFeature selection: consider using fewer feature combinations, decrease n-grams size, and decrease the number of numeric attribute bins.\n\nIncrease the amount of regularization used.\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html"},{"timestamp":"1703891280.0","comment_id":"1109239","content":"Selected Answer: BCF\nBest choices are B (Increase regularization), C (Increase dropout), and F (Decrease feature combinations), as these techniques are effective in reducing overfitting and improving the model's ability to generalize to new data.","poster":"Neet1983","upvote_count":"1"},{"comment_id":"1064933","content":"Selected Answer: BCE\nBCE The model has learnt training data. One approach is to increase complexity by increasing the features or remove some features to increase bias. In deep learning, i thinking increasing feature set is more workable.","poster":"akgarg00","timestamp":"1699370640.0","upvote_count":"1"},{"timestamp":"1690825320.0","comment_id":"968368","poster":"kaike_reis","content":"Selected Answer: BCF\nB-C-F. All of those options can be used to reduce model complexity and thus: overfit","upvote_count":"1"},{"timestamp":"1687344060.0","upvote_count":"1","poster":"SRB1337","comment_id":"929350","content":"its BCF"},{"comment_id":"839451","content":"BCF is correct.","upvote_count":"2","poster":"jackzhao","timestamp":"1678844340.0"},{"timestamp":"1676029200.0","upvote_count":"6","poster":"AjoseO","comment_id":"804255","content":"Selected Answer: BCF\nIncreasing regularization helps to prevent overfitting by adding a penalty term to the loss function to discourage the model from learning the noise in the data.\n\nIncreasing dropout helps to prevent overfitting by randomly dropping out some neurons during training, which forces the model to learn more robust representations that do not depend on the presence of any single neuron.\n\nDecreasing the number of feature combinations helps to simplify the model, making it less likely to overfit."},{"upvote_count":"1","timestamp":"1673815320.0","comment_id":"777052","poster":"Tomatoteacher","content":"Selected Answer: BCE\nI see all the comments for BCF, although when you look at F it just says decrease 'feature combinations', not features themselves. In one way to decrease feature combinations results in having more features (less feature engineering), which in turn will cause more overfitting. Unless the question in badly worded, saying less feature combinations just mean those combinations, which components will not be used, then it has to be BCE.","comments":[{"upvote_count":"1","comment_id":"860170","timestamp":"1680541800.0","poster":"cpal012","content":"Decrease feature combinations - too many irrelevant features can influence the model by drowning out the signal with noise"},{"timestamp":"1676029320.0","poster":"AjoseO","upvote_count":"1","content":"Increasing the number of feature combinations can sometimes improve the performance of a model if the model is underfitting the data. \n\nHowever, in this context, it is not likely to be a solution to overfitting.","comment_id":"804256"}]},{"timestamp":"1662649920.0","upvote_count":"3","poster":"Shailendraa","content":"BCF - Always remember in case of overfitting - reduce features, Add regularisation and increase dropouts.","comment_id":"663731"},{"poster":"ahquiceno","content":"BCE: The main objective of PCA (technic to feature combination) is to simplify your model features into fewer components to help visualize patterns in your data and to help your model run faster. Using PCA also reduces the chance of overfitting your model by eliminating features with high correlation.\nhttps://towardsdatascience.com/dealing-with-highly-dimensional-data-using-principal-component-analysis-pca-fea1ca817fe6","comment_id":"289008","timestamp":"1635836520.0","upvote_count":"2","comments":[{"upvote_count":"3","poster":"uninit","comment_id":"790903","content":"AWS Documentation explicitly mentions reducing feature combinations to prevent overfitting - https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\nIt's B C F","timestamp":"1674928860.0"}]},{"upvote_count":"1","content":"B/C/F Easy peasy.","comment_id":"278267","poster":"cloud_trail","timestamp":"1635748080.0"},{"content":"BCF 100%","comment_id":"266747","timestamp":"1635551760.0","poster":"apnu","upvote_count":"1"},{"content":"BCF\nF\nexplained in AWS document:\nFeature selection: consider using fewer feature combinations, decrease n-grams size, and decrease the number of numeric attribute bins.\nIncrease the amount of regularization used\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html","poster":"obaidur","timestamp":"1635529560.0","upvote_count":"5","comment_id":"187117"},{"content":"It is BCE:\nyou increase regularisation to reduce overfitting\nyou increase dropout to reduce overfitting\nand feature combination is the combination of the strength of multiple complementary features to yield a more powerful feature (which means, you are reducing number of features). Reducing number of features and using features with stronger information helps to reduce overfitting.","poster":"fhuadeen","upvote_count":"4","comment_id":"179970","timestamp":"1635468540.0"},{"content":"BCF\n\nit is C not D as higher value = higher ratio of 0-valued weights = higher regularization = less overfitting","poster":"sebtac","upvote_count":"1","timestamp":"1635441600.0","comment_id":"174057"},{"poster":"syu31svc","upvote_count":"2","content":"BCF for sure","timestamp":"1635140700.0","comment_id":"171878"},{"poster":"algorithmish","content":"overfitting.\nB - increase regularisation helps to generalize\nC - increase dropout (probability of a node to be turned off) also helps to generalize\nF - decreasing feature combinations (removing features like mean or variance, artificial features) helps to generalize","timestamp":"1634965860.0","upvote_count":"6","comment_id":"134759"},{"poster":"eji","upvote_count":"3","content":"i think BCE , E because if we increase features combination, we also decrease the number of feature","timestamp":"1634914080.0","comment_id":"130409"},{"upvote_count":"1","poster":"tff","comment_id":"125505","content":"B, C, F for sure","timestamp":"1634834220.0"},{"timestamp":"1634591160.0","content":"BCF is the combination to go.\nfor drop-out value of 1 means no output (or 100% filtering out). Features should definitely decrease as more features tends to overfit.","comment_id":"107065","upvote_count":"2","poster":"DScode"},{"upvote_count":"1","poster":"mrpwny2","content":"BDF\nReg & Dropout go in the opposite direction. Increase in regularization & decrease in dropout do the same thing. F less features is better. E is wrong as combining the features doesnt imply you are dropping the single ones","comment_id":"93277","timestamp":"1634493300.0"},{"timestamp":"1633902420.0","poster":"VB","content":"I thin B,D,E are correct. \nB. Increase regularization. -> regularization can be increased to avoid overfitting\nD. Decrease dropout. -> dropout values go from 0 (all removed, no output) to 1 (all allowed), so, decreasing the dropout value to 0.9 or 0.8 or 0.7 meaning more and more nodes are removed..so, this option is correct.\nE. Increase feature combinations. -> combining MORE features to reduce the total number of features or bring newer combined features.\n\nI think these are the right answers.","upvote_count":"4","comments":[{"comment_id":"79251","poster":"VB","upvote_count":"3","timestamp":"1634437800.0","content":"BCF .."}],"comment_id":"60570"},{"upvote_count":"5","poster":"rajs","content":"Agreed with B & F\n\nLet me explain why D not C\nDropout =1 For no dropout whereas Dropout = 0 is for ignoring the entire layer .... so to make dropout effective you need to decrease its value hence the correct answer is D","timestamp":"1633650420.0","comment_id":"57356"},{"upvote_count":"6","content":"BCE is correct. I believe that they mean combining the features rather than just adding new combinations of the features. I.e. X, Y goes to XY, not X, Y, XY","poster":"devsean","comment_id":"50219","timestamp":"1633130820.0"},{"content":"Agree that the correct answers are to increase regularization and dropout and decrease feature combinations but they are BCF.","upvote_count":"4","timestamp":"1632640080.0","poster":"vetal","comment_id":"28607"},{"comment_id":"21987","upvote_count":"7","poster":"DonaldCMLIN","timestamp":"1632220320.0","content":"Early Stopping\nL1(Lasso)\nL2(Ridge)\nDrop Out\n\nBY FOLLOWING BELOW TWO ARTCLES OF \nhttps://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\nhttps://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html\n\nA.REGULATION INCREASE \nC.DROP-OUT INCREASE\nF.FEATURE DECREASE","comments":[{"comments":[{"content":"Yep! It is going to be B,C and F. Increasing feature combination will make the model more complex. Using simpler straight forward features would make more sense.","upvote_count":"6","timestamp":"1634714460.0","comment_id":"108591","poster":"Antriksh"}],"comment_id":"28278","upvote_count":"20","timestamp":"1632282120.0","content":"do you mean: B, C, F?","poster":"rsimham"}]}]},{"id":"HlKe39qX9GbsakgrZaqF","answer_images":[],"choices":{"D":"Use Amazon Kinesis Data Analytics to ingest the streaming data and perform real-time SQL queries to convert the records to Apache Parquet before delivering to Amazon S3. Have the Analysts query the data directly from Amazon S3 using Amazon Athena and connect to BI tools using the Athena Java Database Connectivity (JDBC) connector.","C":"Write each JSON record to a staging location in Amazon S3. Use the S3 Put event to trigger an AWS Lambda function that transforms the data into Apache Parquet or ORC format and inserts it into an Amazon RDS PostgreSQL database. Have the Analysts query and run dashboards from the RDS database.","A":"Create a schema in the AWS Glue Data Catalog of the incoming data format. Use an Amazon Kinesis Data Firehose delivery stream to stream the data and transform the data to Apache Parquet or ORC format using the AWS Glue Data Catalog before delivering to Amazon S3. Have the Analysts query the data directly from Amazon S3 using Amazon Athena, and connect to BI tools using the Athena Java Database Connectivity (JDBC) connector.","B":"Write each JSON record to a staging location in Amazon S3. Use the S3 Put event to trigger an AWS Lambda function that transforms the data into Apache Parquet or ORC format and writes the data to a processed data location in Amazon S3. Have the Analysts query the data directly from Amazon S3 using Amazon Athena, and connect to BI tools using the Athena Java Database Connectivity (JDBC) connector."},"discussion":[{"poster":"DonaldCMLIN","upvote_count":"32","content":"Kinesis Data Analytics NO PARQET FORMAT, \nBESIDES THAT JSON NO NEED TO STORE IN S3.\nRDS ISN'T serverless ingestion and analytics solution\n\nANSWER IS A.","timestamp":"1647731220.0","comment_id":"21991"},{"comment_id":"64492","poster":"georgeZ","upvote_count":"14","content":"I thinks it should be A please check https://aws.amazon.com/blogs/big-data/analyzing-apache-parquet-optimized-data-using-amazon-kinesis-data-firehose-amazon-athena-and-amazon-redshift/","timestamp":"1648409400.0"},{"timestamp":"1739839560.0","comment_id":"1358054","upvote_count":"1","content":"Selected Answer: A\nAmazon Kinesis Data Firehose\n\nIngests real-time data with automatic buffering.\nSupports built-in transformation to Apache Parquet/ORC before writing to Amazon S3.\nRequires minimal code and infrastructure.\nAWS Glue Data Catalog\n\nCatalogs the schema for structured querying.\nEnables Athena to directly query data in S3.\nAmazon Athena\n\nServerless SQL querying on S3-based datasets.\nCan connect to BI tools (Tableau, QuickSight) via JDBC.","poster":"JonSno"},{"timestamp":"1722967440.0","poster":"Alice1234","content":"A. Create a schema in the AWS Glue Data Catalog of the incoming data format. Use Amazon Kinesis Data Firehose to buffer and transform the streaming JSON data to a columnar format like Apache Parquet or ORC using the AWS Glue Data Catalog before delivering to Amazon S3. Analysts can then query the data using Amazon Athena and connect to BI dashboards using the Athena JDBC connector. This solution is serverless, manages high-velocity data streams, supports SQL queries, and connects to BI tools—all while being highly available.","upvote_count":"3","comment_id":"1142553"},{"content":"Selected Answer: C\nA. YES - we need a catalog to create parquet (https://docs.aws.amazon.com/firehose/latest/APIReference/API_SchemaConfiguration.html)\nB. NO - no need for extra staging\nC. NO - no need for extra staging\nD. NO - we need a catalog","upvote_count":"1","poster":"loict","comment_id":"1006649","timestamp":"1710342360.0"},{"poster":"Mickey321","timestamp":"1709148060.0","comment_id":"992446","upvote_count":"1","content":"Selected Answer: A\nOption A"},{"poster":"kaike_reis","comment_id":"968374","timestamp":"1706730900.0","content":"Selected Answer: A\nA is correct. For those selecting B, answer me: how exactly the json will be stored in the S3? It's not mentioned in the answer. For me it's an incomplete solution.","upvote_count":"2"},{"upvote_count":"3","timestamp":"1691661120.0","content":"Selected Answer: A\nThis solution leverages AWS Glue to create a schema of the incoming data format, which helps to buffer and convert the records to a query-optimized, columnar format without data loss. \n\nThe Amazon Kinesis Data Firehose delivery stream is used to stream the data and transform it to Apache Parquet or ORC format using the AWS Glue Data Catalog, and the data is stored in Amazon S3, which is highly available. The Analysts can then query the data directly from Amazon S3 using Amazon Athena, and connect to BI tools using the Athena JDBC connector. \n\nThis solution provides a serverless, scalable, and cost-effective solution for real-time streaming data ingestion and analytics.","poster":"AjoseO","comment_id":"804261"},{"timestamp":"1691571120.0","content":"Selected Answer: A\nSince you want to buffer and convert data so A is correct answer. No other option is fulfilling this requirement","comment_id":"803105","upvote_count":"2","poster":"sqavi"},{"timestamp":"1686263880.0","content":"Selected Answer: A\nI go for A. However, I am not sure why AWS Glue is very important here given that Firehose can convert JSON to parquet.","upvote_count":"2","poster":"Peeking","comments":[{"timestamp":"1697956500.0","upvote_count":"1","comments":[{"timestamp":"1698892500.0","upvote_count":"1","poster":"ZSun","content":"once you ingest the data using Kinesis Firehose, you can set \"generate table\" and automatically create Glue schema. I think both Glue and Firehose can do data conversion from JSON to parquet.","comment_id":"886893"}],"content":"If I haven't remembered correctly. Athena requires a schema of the S3 object to perform SQL query. That's probably why we need Glue for the schema","comment_id":"877080","poster":"Tony_1406"}],"comment_id":"739627"},{"timestamp":"1686062340.0","content":"Why AWS Glue is needed? Firehose could convert to parquet directly...","comments":[{"poster":"587df71","content":"https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nAmazon Data Firehose requires a schema to determine how to interpret that data. Use AWS Glue to create a schema in the AWS Glue Data Catalog. Amazon Data Firehose then references that schema and uses it to interpret your input data","upvote_count":"1","comment_id":"1336198","timestamp":"1735947780.0"}],"upvote_count":"2","comment_id":"737001","poster":"itallomd"},{"upvote_count":"1","comment_id":"727474","content":"Selected Answer: B\nKinesis Data Analytics is near real-time, not real time","timestamp":"1685094060.0","poster":"Ccindy"},{"poster":"ryuhei","content":"Selected Answer: A\nAnswer is ”A”","comment_id":"679583","upvote_count":"1","timestamp":"1679823660.0"},{"comments":[{"upvote_count":"3","timestamp":"1672248420.0","comment_id":"624052","poster":"ovokpus","content":"Mind you, \"the ingestion process must buffer and transform incoming records from JSON to a query-optimized, columnar format\"\n\nThat is exactly what kinesis firehose does.\n\"Kinesis Data Firehose buffers incoming data before delivering it to Amazon S3. You can configure the values for S3 buffer size (1 MB to 128 MB) or buffer interval (60 to 900 seconds), and the condition satisfied first triggers data delivery to Amazon S3.\"\n\nSee link: https://aws.amazon.com/kinesis/data-firehose/faqs/#:~:text=Kinesis%20Data%20Firehose%20buffers%20incoming,data%20delivery%20to%20Amazon%20S3."}],"content":"Selected Answer: A\nThe difference between \"real-time\" and \"near-real-time\" is pretty semantic(60s). The fact that the data comes through kinesis data streams (real time) is implied as the only valid input to firehose.","poster":"ovokpus","comment_id":"624046","upvote_count":"1","timestamp":"1672248060.0"},{"poster":"TerrancePythonJava","content":"Selected Answer: B\nData Firehose is always Near Real Time not Real Time. The prompt clearly states that process must be done in Real Time.","comment_id":"560462","timestamp":"1662248520.0","upvote_count":"1"},{"poster":"anttan","timestamp":"1654231920.0","comments":[{"comment_id":"831229","poster":"cpal012","content":"There is no requirement for real time processing. It says the data is in real time but the processing of that data should buffer","upvote_count":"2","timestamp":"1694024220.0"}],"content":"Why A? Firehose is near real-time, and not real-time which is a requirement","upvote_count":"1","comment_id":"492991"},{"upvote_count":"5","poster":"harmanbirstudy","timestamp":"1651036200.0","comment_id":"263387","content":"ANSWER is A -- and every statement in it is accurate.\nFirehose does integrate with GLue data catalog and it also \"Buffers\" the data .\n\"When Kinesis Data Firehose processes incoming events and converts the data to Parquet, it needs to know which schema to apply.\" This is achived by glue data catalog and athena and it works on real-time data ingest.See link below.\n\nhttps://aws.amazon.com/blogs/big-data/analyzing-apache-parquet-optimized-data-using-amazon-kinesis-data-firehose-amazon-athena-and-amazon-redshift/"},{"upvote_count":"1","poster":"lightblue","content":"https://aws.amazon.com/blogs/aws/new-serverless-streaming-etl-with-aws-glue/ \n A is the answer imo","comment_id":"251590","timestamp":"1650634200.0"},{"poster":"oMARKOo","content":"Should be A. \nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\nit states that \" Use AWS Glue to create a schema in the AWS Glue Data Catalog. Kinesis Data Firehose then references that schema and uses it to interpret your input data. \"","timestamp":"1650500520.0","comment_id":"190836","upvote_count":"1"},{"content":"I was initially about to choose A, but I think the answer A would be better if using Kinesis Data stream before firehose, since data passed to firehose has to be loss-tolerant during transfer and conflicts with \"must buffer\" during ingestion process.","poster":"williamsuning","upvote_count":"1","comment_id":"187574","timestamp":"1650466620.0"},{"content":"Answer is A\nFollowing links support it:\nhttps://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html\nhttps://aws.amazon.com/about-aws/whats-new/2018/05/stream_real_time_data_in_apache_parquet_or_orc_format_using_firehose/","timestamp":"1650173880.0","comment_id":"169060","poster":"syu31svc","upvote_count":"2"},{"comments":[{"comment_id":"137550","upvote_count":"1","content":"because it's streaming data, so not choose B?","timestamp":"1650044100.0","poster":"Achievement"}],"comment_id":"126444","timestamp":"1649927400.0","content":"Answer 'A' is confusing. Still don't understand rational behind answer 'B'. Answer C & D is not even close. Anybody else has 2nd opinion? Either A or B?","upvote_count":"2","poster":"Urban_Life"},{"poster":"Sadhna","timestamp":"1649282040.0","comment_id":"85564","content":"I think A is correct answer","upvote_count":"2"},{"upvote_count":"4","timestamp":"1649230320.0","comment_id":"73487","content":"The correct answer HAS TO be A. Other choices are just too wacky","poster":"dhs227"},{"timestamp":"1648421580.0","comment_id":"65639","upvote_count":"2","poster":"PRC","content":"A it is..."},{"upvote_count":"3","comment_id":"41029","comments":[{"poster":"sdsfsdsf","timestamp":"1648324920.0","upvote_count":"1","comment_id":"64159","content":"Yeah, according to: https://aws.amazon.com/glue/faqs/\nGlue Data Catalog does not work with Firehose (it works with Athena, Redshift, EMR). I'd say B is the solution","comments":[{"timestamp":"1650540120.0","poster":"lightblue","content":"It is not required for glue to be attached to firehose in anyway. after data is in s3 glue will come into picture.","comment_id":"251589","upvote_count":"1"}]}],"content":"But Firehose does not interact with Glue for delivery to S3.. Firehose can convert JSON to ORC itself and deliver in S3 for downstream consumption... Shouldn't be B","poster":"NNR","timestamp":"1648099800.0"}],"answer_ET":"A","answer":"A","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/8351-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1573918320,"question_text":"A Data Scientist needs to create a serverless ingestion and analytics solution for high-velocity, real-time streaming data.\nThe ingestion process must buffer and convert incoming records from JSON to a query-optimized, columnar format without data loss. The output datastore must be highly available, and Analysts must be able to run SQL queries against the data and connect to existing business intelligence dashboards.\nWhich solution should the Data Scientist build to satisfy the requirements?","isMC":true,"timestamp":"2019-11-16 16:32:00","topic":"1","answers_community":["A (81%)","Other"],"question_id":310,"answer_description":"","exam_id":26}],"exam":{"provider":"Amazon","numberOfQuestions":369,"isMCOnly":false,"id":26,"isImplemented":true,"isBeta":false,"name":"AWS Certified Machine Learning - Specialty","lastUpdated":"11 Apr 2025"},"currentPage":62},"__N_SSP":true}