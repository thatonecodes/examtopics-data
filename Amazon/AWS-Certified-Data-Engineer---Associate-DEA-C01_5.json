{"pageProps":{"questions":[{"id":"eedud0JQ5JiPlsYPEvyV","unix_timestamp":1719647460,"discussion":[{"poster":"andrologin","timestamp":"1721363940.0","content":"Selected Answer: C\nC AWS GLue bookmarks are used to implement incremental processing","upvote_count":"2","comment_id":"1250840"},{"upvote_count":"1","content":"Selected Answer: C\nC. Job bookmarks\n\nHere's why job bookmarks are the appropriate feature:\n\nIncremental Processing: Job bookmarks in AWS Glue help track the last processed state of data in Amazon S3. They enable the ETL job to resume from where it left off in case of interruptions or subsequent runs, ensuring that only new or modified data since the last successful run is processed (incremental processing).\nAutomated ETL: Job bookmarks work seamlessly within AWS Glue ETL jobs, allowing the job to efficiently manage the state of processed data without the need for manual intervention.\nSupport for Compressed Files: AWS Glue natively supports reading compressed files from Amazon S3, so the ingestion pipeline can handle compressed data formats efficiently.","timestamp":"1720023480.0","poster":"Ja13","comment_id":"1241522"},{"upvote_count":"1","poster":"HunkyBunky","content":"Selected Answer: C\nC - is right","comment_id":"1240852","timestamp":"1719930960.0"},{"poster":"Bmaster","timestamp":"1719647460.0","content":"C is correct answer..\n\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html","comment_id":"1239120","upvote_count":"4"}],"answer_description":"","answers_community":["C (100%)"],"answer_ET":"C","isMC":true,"choices":{"D":"Classifiers","B":"Triggers","A":"Workflows","C":"Job bookmarks"},"timestamp":"2024-06-29 09:51:00","question_images":[],"topic":"1","answer":"C","question_id":21,"exam_id":21,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/143061-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"A data engineer is building an automated extract, transform, and load (ETL) ingestion pipeline by using AWS Glue. The pipeline ingests compressed files that are in an Amazon S3 bucket. The ingestion pipeline must support incremental data processing.\n\nWhich AWS Glue feature should the data engineer use to meet this requirement?"},{"id":"j0H4UnaIK98Fih4Eieke","discussion":[{"upvote_count":"1","poster":"Ramdi1","content":"Selected Answer: A\nAmazon Kinesis Data Streams does not provide exactly-once delivery natively. It ensures at-least-once delivery, meaning that under certain conditions (e.g., network failures, retries), duplicate records can occur. To achieve exactly-once processing, deduplication must be handled at the application level.","comment_id":"1387429","timestamp":"1741699080.0"},{"content":"Selected Answer: A\nA. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.","comment_id":"1285245","poster":"PashoQ","upvote_count":"1","timestamp":"1726578660.0"},{"poster":"Ja13","comment_id":"1244467","timestamp":"1720456860.0","upvote_count":"2","content":"Selected Answer: A\nA. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.\n\nExplanation:\nExactly-Once Delivery: Ensuring exactly-once delivery is a challenge in distributed systems, especially in the presence of network outages and retries. By embedding a unique ID in each record at the source, you can track and identify duplicate records during processing. This approach allows you to implement idempotent processing, where duplicate records can be detected and discarded, ensuring that each record is processed exactly once.\nDe-duplication Logic: Implementing de-duplication logic based on unique IDs ensures that even if the same record is ingested multiple times due to retries or network issues, it will be processed only once by the downstream applications."},{"timestamp":"1719923820.0","content":"Selected Answer: A\nA. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.\n\nThis approach ensures that even if a record is sent more than once due to network outages or other issues, it will only be processed once because the unique ID can be used to identify and remove any duplicates. This is a common pattern for achieving exactly-once processing semantics in distributed systems. The other options do not guarantee exactly-once delivery across the entire pipeline. Option B is partially correct but it only avoids duplicate processing within the Amazon Managed Service for Apache Flink, not across the entire pipeline. Option C is not always feasible because network issues and other factors can lead to events being ingested into Kinesis Data Streams multiple times. Option D involves changing the entire technology stack, which is not necessary to achieve the desired outcome and could introduce additional complexity and cost.","comment_id":"1240798","poster":"bakarys","upvote_count":"3"}],"question_id":22,"unix_timestamp":1719648000,"exam_id":21,"answer_description":"","answer":"A","isMC":true,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/143062-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"timestamp":"2024-06-29 10:00:00","answers_community":["A (100%)"],"answer_ET":"A","topic":"1","choices":{"A":"Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.","D":"Stop using Kinesis Data Streams. Use Amazon EMR instead. Use Apache Flink and Apache Spark Streaming in Amazon EMR.","C":"Design the data source so events are not ingested into Kinesis Data Streams multiple times.","B":"Update the checkpoint configuration of the Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) data collection application to avoid duplicate processing of events."},"question_text":"A banking company uses an application to collect large volumes of transactional data. The company uses Amazon Kinesis Data Streams for real-time analytics. The companyâ€™s application uses the PutRecord action to send data to Kinesis Data Streams.\n\nA data engineer has observed network outages during certain times of day. The data engineer wants to configure exactly-once delivery for the entire processing pipeline.\n\nWhich solution will meet this requirement?"},{"id":"O49O72oWO4VRwFiLsOMs","url":"https://www.examtopics.com/discussions/amazon/view/143120-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","question_id":23,"answer":"B","answer_images":[],"discussion":[{"content":"B seems to be the correct answer. But the side effect is, versioning of log files will be expensive as every change to the file will versioned.","upvote_count":"1","timestamp":"1729881900.0","poster":"0c2d840","comment_id":"1302984"},{"poster":"sdas1","timestamp":"1719937800.0","upvote_count":"3","content":"S3 Versioning keeps multiple versions of an object in the same bucket. When you enable versioning, every time an object is overwritten or deleted, a new version of that object is created, and the previous version is retained. This ensures that no data is lost permanently due to accidental deletions or overwrites.","comment_id":"1240887"},{"comment_id":"1240854","content":"Selected Answer: B\nB - is right answer","timestamp":"1719931140.0","upvote_count":"2","poster":"HunkyBunky"},{"timestamp":"1719784320.0","poster":"Bmaster","upvote_count":"3","content":"B is good..","comment_id":"1239857"}],"isMC":true,"answers_community":["B (100%)"],"choices":{"A":"Manually back up the S3 bucket on a regular basis.","D":"Use an Amazon S3 Glacier storage class to archive the data that is in the S3 bucket.","B":"Enable S3 Versioning for the S3 bucket.","C":"Configure replication for the S3 bucket."},"timestamp":"2024-06-30 23:52:00","answer_description":"","answer_ET":"B","exam_id":21,"question_images":[],"question_text":"A company stores logs in an Amazon S3 bucket. When a data engineer attempts to access several log files, the data engineer discovers that some files have been unintentionally deleted.\n\nThe data engineer needs a solution that will prevent unintentional file deletion in the future.\n\nWhich solution will meet this requirement with the LEAST operational overhead?","unix_timestamp":1719784320},{"id":"t6je9nAx2nSvKcfNDsLQ","timestamp":"2024-01-18 09:51:00","question_id":24,"unix_timestamp":1705567860,"topic":"1","discussion":[{"comments":[{"timestamp":"1722834120.0","comment_id":"1260928","upvote_count":"1","poster":"V0811","content":"No! :-)\nBecause: The company needs to display a REAL-TIME view of operational efficiency on a large screen in the manufacturing facility.\n\nSo it's for sure C","comments":[{"upvote_count":"1","content":"V0811 - and new firehose, ofc decrease latency, khomon man","timestamp":"1736603520.0","poster":"Udyan","comment_id":"1339170"}]}],"content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/database/near-real-time-processing-with-amazon-kinesis-amazon-timestream-and-grafana/\n\nLook at the architecture diagram","poster":"fceb2c1","timestamp":"1711199760.0","comment_id":"1180881","upvote_count":"12"},{"content":"Selected Answer: A\nreal time -> no Quicksight. And bookmarks to read sensor data real time is just as stupid as the flat earth theory. A it is.","upvote_count":"7","comment_id":"1125644","poster":"milofficial","timestamp":"1705567860.0"},{"comment_id":"1558831","poster":"Salmanbutt786","content":"Selected Answer: A\nA is correct\n\nC is close to A, but creating an additional Data Firehose delivery stream adds unnecessary complexity. Writing directly to Amazon Timestream from Apache Flink, as in option A, is more straightforward and ensures lower latency.","upvote_count":"1","timestamp":"1744098540.0"},{"poster":"Scotty_Nguyen","timestamp":"1742906820.0","content":"Selected Answer: A\nA is correct","upvote_count":"1","comment_id":"1410032"},{"poster":"Adrifersilva","timestamp":"1727668920.0","content":"Selected Answer: A\nGrafana:\nReal-time Performance:\nGrafana is known for its excellent real-time data visualization capabilities.\nIt's often used for operational dashboards that require frequent updates.\n\nIntegration:\nWorks well with time-series databases and streaming data sources. [2]","comment_id":"1291381","upvote_count":"3"},{"comment_id":"1269047","content":"Selected Answer: A\nFirehose cannot use Timestream as destination. Answer is A","timestamp":"1724127120.0","poster":"deepcloud","upvote_count":"5"},{"timestamp":"1723620960.0","content":"Option A is for processing data in Flink and then sending it to Timestream. This is advantageous when complex data processing is required in Flink, but the processing step where complex analytics are processed can handle additional latency.\n\nOption C performs data processing in Flink, sends the data directly to Timestream without any additional steps, and provides dashboards via QuickSight. Since data can be started immediately after arriving in Timestream, latency is likely to be higher.\n\nTherefore, option C is preferable because it can handle latency by performing data processing, publishing data directly to Timestream, and provides fast dashboards using QuickSight.","upvote_count":"1","comment_id":"1265555","poster":"samadal"},{"content":"Selected Answer: A\nAmazon QuickSight is primarily designed for business intelligence and data visualization, and it can provide near real-time views depending on the data refresh rate. However, it is not typically used for real-time streaming data visualization with very low latency. For real-time dashboards with very low latency, services like Grafana are more suitable.\nYou can use Amazon Managed Grafana to setup the dashboard so you're using an AWS service which is always preferible on these exams.","upvote_count":"4","comment_id":"1265186","poster":"teo2157","timestamp":"1723556340.0"},{"content":"Based on this it should be C, why use an open source app when you can an AWS Service\n\nhttps://community.amazonquicksight.com/t/real-time-data-visualization-capabilities-of-amazon-quicksight/24007","poster":"jyrajan69","upvote_count":"2","timestamp":"1722549480.0","comment_id":"1259589"},{"content":"The Question is: Which solution will meet these requirements with the LOWEST latency?\nSo just A can be the right answer \"lowest latency!!!!\"","timestamp":"1715080260.0","poster":"Just_Ninja","comment_id":"1207849","upvote_count":"1"},{"upvote_count":"1","poster":"LanoraMoe","comment_id":"1191887","content":"I go with Option A. Kinesis Data Firehose can connect to 3 AWS destinations so far S3, Redshift and OpenSearch.","timestamp":"1712624040.0"},{"poster":"certplan","content":"Option A:\n- Involves additional steps: Option A requires writing data to Amazon Timestream after processing with Apache Flink, potentially introducing additional latency compared to a more direct approach like Option C.\n- Grafana integration: While Grafana is a powerful visualization tool, setting up and configuring Grafana dashboards might require additional effort compared to using Amazon QuickSight, which offers more straightforward integration with AWS services like Amazon Timestream.","upvote_count":"1","timestamp":"1710872160.0","comment_id":"1177603"},{"upvote_count":"2","timestamp":"1710871980.0","content":"C. - **Processing Sensor Data with Amazon Flink**: Similar to option A, this approach uses Amazon Managed Service for Apache Flink to process sensor data, providing real-time analytics or transformation capabilities.\n - **Data Firehose Delivery Stream to Timestream**: Sets up a new Amazon Data Firehose delivery stream to publish processed data directly to Amazon Timestream. Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as data lakes, databases, and analytics services.\n - **Timestream Database as a Source for QuickSight Dashboard**: Similar to option B, the data stored in Amazon Timestream serves as the data source for creating an Amazon QuickSight dashboard.","comment_id":"1177600","poster":"certplan"},{"content":"Considerations:\n\nOption A utilizes Amazon Managed Service for Apache Flink to process sensor data and then writes the processed data to Amazon Timestream. From there, the Timestream database serves as a source to create a Grafana dashboard.\nThus the data goes through Apache Flink for processing, then to Timestream, and finally to Grafana. \"Each additional step introduces potential latency\".\n\nOption C processes sensor data using Amazon Managed Service for Apache Flink and then publishes data directly to Amazon Timestream via a Data Firehose delivery stream. Finally, it uses Timestream as a source to create an Amazon QuickSight dashboard.\n\nSo, in terms of latency, both options involve processing data in real-time using Apache Flink. However, Option C has a more direct data flow by publishing data directly to Timestream, potentially reducing latency compared to Option A, where the data has to go through an additional step of writing to Timestream.","upvote_count":"1","timestamp":"1710871680.0","poster":"certplan","comment_id":"1177595"},{"poster":"TonyStark0122","content":"A. Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to process the sensor data. Use a connector for Apache Flink to write data to an Amazon Timestream database. Use the Timestream database as a source to create a Grafana dashboard.\n\nExplanation:\nAmazon Managed Service for Apache Flink provides real-time stream processing capabilities, which can process sensor data with low latency. By using Apache Flink connectors, the processed data can be efficiently written to Amazon Timestream, which is optimized for time-series data storage and querying.","timestamp":"1706819040.0","comment_id":"1137897","upvote_count":"2"}],"answer_description":"","answer_images":[],"answers_community":["A (97%)","3%"],"choices":{"A":"Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to process the sensor data. Use a connector for Apache Flink to write data to an Amazon Timestream database. Use the Timestream database as a source to create a Grafana dashboard.","C":"Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to process the sensor data. Create a new Data Firehose delivery stream to publish data directly to an Amazon Timestream database. Use the Timestream database as a source to create an Amazon QuickSight dashboard.","B":"Configure the S3 bucket to send a notification to an AWS Lambda function when any new object is created. Use the Lambda function to publish the data to Amazon Aurora. Use Aurora as a source to create an Amazon QuickSight dashboard.","D":"Use AWS Glue bookmarks to read sensor data from the S3 bucket in real time. Publish the data to an Amazon Timestream database. Use the Timestream database as a source to create a Grafana dashboard."},"question_text":"A manufacturing company collects sensor data from its factory floor to monitor and enhance operational efficiency. The company uses Amazon Kinesis Data Streams to publish the data that the sensors collect to a data stream. Then Amazon Kinesis Data Firehose writes the data to an Amazon S3 bucket.\nThe company needs to display a real-time view of operational efficiency on a large screen in the manufacturing facility.\nWhich solution will meet these requirements with the LOWEST latency?","isMC":true,"answer":"A","answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/131474-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"exam_id":21},{"id":"Cb8LQnxiimgbNW143umY","isMC":true,"question_id":25,"answer_images":[],"discussion":[{"comment_id":"1295754","upvote_count":"3","content":"Selected Answer: B\nRegarding D, Database Activity Streams in Aurora are primarily for auditing databases actities, not for analyzing app data.","timestamp":"1728596880.0","poster":"Adrifersilva"},{"comment_id":"1262585","poster":"antun3ra","content":"Selected Answer: B\nB is the correct answer","upvote_count":"2","timestamp":"1723133100.0"},{"comment_id":"1261862","upvote_count":"2","poster":"portland","timestamp":"1722987480.0","content":"Selected Answer: B\nreduces latency because it is analyze before the data even gets to the Aurora DB"},{"poster":"sdas1","timestamp":"1720970880.0","upvote_count":"2","content":"Option D is the optimal choice because it leverages Aurora's Database Activity Streams to enable real-time monitoring and immediate response to changes in network usage data. This approach ensures the least latency in detecting and responding to sudden drops in network usage, crucial for the telecommunications company to take immediate remedial actions during network outages.","comment_id":"1247828"},{"timestamp":"1720970820.0","upvote_count":"2","poster":"sdas1","comment_id":"1247827","content":"Option D\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html\n\nIn Amazon Aurora, you start a database activity stream at the cluster level. All DB instances within your cluster have database activity streams enabled.\n\nYour Aurora DB cluster pushes activities to an Amazon Kinesis data stream in near real time. The Kinesis stream is created automatically. From Kinesis, you can configure AWS services such as Amazon Data Firehose and AWS Lambda to consume the stream and store the data.","comments":[{"comment_id":"1289156","content":"The Database Activity Streams feature of Amazon Aurora can help monitor database activity, but it doesn't directly detect drops in network usage:","timestamp":"1727296440.0","upvote_count":"1","poster":"LR2023","comments":[{"timestamp":"1733723040.0","poster":"altonh","content":"\"Activity monitored includes the full SQL statement, the row count of affected rows from DML commands\"\nYou can still keep track of the row inserts to see if there is a drop in network usage.","upvote_count":"1","comment_id":"1323885"}]}]},{"timestamp":"1720461540.0","upvote_count":"3","comment_id":"1244501","content":"Selected Answer: B\nThe best solution to identify sudden drops in network usage with the least latency is:\n\nB. Modify the processing application to publish the data to an Amazon Kinesis data stream. Create an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to detect drops in network usage.\n\nThis approach ensures real-time processing with minimal latency and allows immediate detection and response to network usage drops.","poster":"Ja13"},{"upvote_count":"3","content":"Selected Answer: D\nI guess D. The question is which solution helps to identitfy sudden drops to take immediate actions","poster":"HunkyBunky","comment_id":"1240857","timestamp":"1719931440.0"},{"poster":"Bmaster","timestamp":"1719784440.0","comment_id":"1239861","content":"B is good","upvote_count":"2","comments":[{"comment_id":"1242094","upvote_count":"2","poster":"didorins","timestamp":"1720098540.0","content":"Thank you B master."}]}],"unix_timestamp":1719784440,"answer_description":"","answers_community":["B (77%)","D (23%)"],"topic":"1","answer_ET":"B","question_text":"A telecommunications company collects network usage data throughout each day at a rate of several thousand data points each second. The company runs an application to process the usage data in real time. The company aggregates and stores the data in an Amazon Aurora DB instance.\n\nSudden drops in network usage usually indicate a network outage. The company must be able to identify sudden drops in network usage so the company can take immediate remedial actions.\n\nWhich solution will meet this requirement with the LEAST latency?","url":"https://www.examtopics.com/discussions/amazon/view/143122-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"choices":{"B":"Modify the processing application to publish the data to an Amazon Kinesis data stream. Create an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to detect drops in network usage.","D":"Create an AWS Lambda function within the Database Activity Streams feature of Aurora to detect drops in network usage.","A":"Create an AWS Lambda function to query Aurora for drops in network usage. Use Amazon EventBridge to automatically invoke the Lambda function every minute.","C":"Replace the Aurora database with an Amazon DynamoDB table. Create an AWS Lambda function to query the DynamoDB table for drops in network usage every minute. Use DynamoDB Accelerator (DAX) between the processing application and DynamoDB table."},"timestamp":"2024-06-30 23:54:00","exam_id":21,"answer":"B"}],"exam":{"isMCOnly":true,"provider":"Amazon","isImplemented":true,"id":21,"isBeta":false,"name":"AWS Certified Data Engineer - Associate DEA-C01","lastUpdated":"11 Apr 2025","numberOfQuestions":207},"currentPage":5},"__N_SSP":true}