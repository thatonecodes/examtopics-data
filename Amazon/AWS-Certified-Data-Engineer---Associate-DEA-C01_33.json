{"pageProps":{"questions":[{"id":"uX3yFQqCjyO2vfCeJsqQ","answer_images":[],"timestamp":"2024-02-02 11:48:00","url":"https://www.examtopics.com/discussions/amazon/view/132684-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"answer":"D","answer_ET":"D","question_id":161,"answers_community":["D (100%)"],"choices":{"D":"Create a trail of data events in AWS CloudTraiL. Configure the trail to receive data from the transactions S3 bucket. Specify an empty prefix and write-only events. Specify the logs S3 bucket as the destination bucket.","B":"Create a trail of management events in AWS CloudTraiL. Configure the trail to receive data from the transactions S3 bucket. Specify an empty prefix and write-only events. Specify the logs S3 bucket as the destination bucket.","A":"Configure an S3 Event Notifications rule for all activities on the transactions S3 bucket to invoke an AWS Lambda function. Program the Lambda function to write the event to Amazon Kinesis Data Firehose. Configure Kinesis Data Firehose to write the event to the logs S3 bucket.","C":"Configure an S3 Event Notifications rule for all activities on the transactions S3 bucket to invoke an AWS Lambda function. Program the Lambda function to write the events to the logs S3 bucket."},"topic":"1","answer_description":"","question_images":[],"discussion":[{"poster":"rralucard_","upvote_count":"5","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\nOption D, creating a trail of data events in AWS CloudTrail, is the best solution to meet the requirement with the least operational effort. It directly logs the desired activities to another S3 bucket and does not involve the development and maintenance of additional resources like Lambda functions or Kinesis Data Firehose streams.","timestamp":"1722588480.0","comment_id":"1138441"},{"content":"Selected Answer: D\nA: Don't need all activities on the S3 bucket\nB: Management events include not only the data log but also the admin log\nC: Don't need all activities on the S3 bucket\nOption D with the LEAST operational effort","poster":"VerRi","upvote_count":"5","comment_id":"1213880","timestamp":"1732041120.0"},{"upvote_count":"3","timestamp":"1730102820.0","content":"Selected Answer: D\nCorrect answer is D.\n\nOption A or C require writing custom Lambda code to handle the events and write them to the Kinesis or S3 bucket so they are not the LEAST operational effort.","poster":"khchan123","comment_id":"1203388"},{"comment_id":"1201421","content":"S3 object level activities such as GetObject, DeleteObject, PutObject etc are considered as Data event in cloud trail. Read and Write event be monitored separately.","upvote_count":"1","poster":"LanoraMoe","timestamp":"1729778160.0"},{"timestamp":"1729594380.0","upvote_count":"1","poster":"okechi","comment_id":"1200117","content":"The correct answer is B - CloudTrail of management events includes logging set ups like this"},{"upvote_count":"3","timestamp":"1726731420.0","content":"Although it might be tempting going with C, please keep in mind that if we go with C we must define lambda code, lambda permission, triggers, etc. If we go with D we just enable a trail data events and that's pretty much it.","poster":"GiorgioGss","comment_id":"1177174"},{"upvote_count":"2","comment_id":"1164024","poster":"Felix_G","comments":[{"content":"Option C is right , by employing S3 Event Notifications with a Lambda function directly writing to the logs S3 bucket, you achieve the desired logging functionality with minimal setup, management, and cost compared to the other options. This approach leverages the built-in integration between these services and avoids unnecessary service dependencies.","upvote_count":"1","comments":[{"timestamp":"1727633640.0","comment_id":"1185654","content":"Check Amazon S3 object-level actions that are tracked by AWS CloudTrail logging\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html\n\nYou can get CloudTrail logs for object-level Amazon S3 actions. To do this, enable data events for your S3 bucket or all buckets in your account.","poster":"Luke97","upvote_count":"1","comments":[{"timestamp":"1727633820.0","poster":"Luke97","content":"Write to S3 means PutObject, CopyObject API","comment_id":"1185655","upvote_count":"1"}]}],"comment_id":"1164026","timestamp":"1725262740.0","poster":"Felix_G"}],"content":"Other Options were Less Efficient:\nA. Leverage S3 Event Notifications, Lambda function, and Kinesis Data Firehose: While this option works, it involves setting up and managing three services, increasing complexity and operational overhead. Kinesis Data Firehose introduces an unnecessary intermediary step, adding complexity for a straightforward logging task.\nB. Utilize CloudTrail with Management Events: CloudTrail primarily tracks API calls and management activities related to S3 buckets, not data events like writes to objects. Consequently, it wouldn't capture the desired S3 bucket writes.\nD. Employ CloudTrail with Data Events: Similar to option B, CloudTrail with data events doesn't track individual object writes within a bucket. It focuses on object-level changes like creation, deletion, or metadata modification.","timestamp":"1725262680.0"}],"unix_timestamp":1706870880,"question_text":"A company stores details about transactions in an Amazon S3 bucket. The company wants to log all writes to the S3 bucket into another S3 bucket that is in the same AWS Region.\nWhich solution will meet this requirement with the LEAST operational effort?","isMC":true},{"id":"afAuKhJsDol2yynjEBpQ","question_text":"A data engineer needs to maintain a central metadata repository that users access through Amazon EMR and Amazon Athena queries. The repository needs to provide the schema and properties of many tables. Some of the metadata is stored in Apache Hive. The data engineer needs to import the metadata from Hive into the central metadata repository.\nWhich solution will meet these requirements with the LEAST development effort?","answers_community":["C (100%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/132685-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","answer_description":"","choices":{"C":"Use the AWS Glue Data Catalog.","D":"Use a metastore on an Amazon RDS for MySQL DB instance.","A":"Use Amazon EMR and Apache Ranger.","B":"Use a Hive metastore on an EMR cluster."},"exam_id":21,"unix_timestamp":1706871960,"answer_images":[],"answer":"C","timestamp":"2024-02-02 12:06:00","discussion":[{"upvote_count":"6","comment_id":"1138463","content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/big-data/metadata-classification-lineage-and-discovery-using-apache-atlas-on-amazon-emr/\nOption C, using the AWS Glue Data Catalog, is the best solution to meet the requirements with the least development effort. The AWS Glue Data Catalog is designed to be a central metadata repository that can integrate with various AWS services including EMR and Athena, providing a managed and scalable solution for metadata management with built-in Hive compatibility.","poster":"rralucard_","timestamp":"1706871960.0"},{"poster":"vic614","upvote_count":"1","comment_id":"1233897","content":"Selected Answer: C\nData Catalog.","timestamp":"1718908920.0"},{"upvote_count":"2","comments":[{"comment_id":"1164041","poster":"Felix_G","content":"Here's an analysis of each option:\n\nA) Amazon EMR and Apache Ranger would require significant coding to build a custom metadata repository solution\n\nB) A Hive metastore provides metadata to EMR, but would require substantial development work to share that metadata with Athena\n\nC) The AWS Glue Data Catalog integrates natively with EMR and Athena, providing a shared schema registry, making it the easiest solution\n\nD) An RDS database metastore would also require building custom integration points with Athena, EMR, and other services to enable metadata sharing\n\nSince AWS Glue provides a fully managed data catalog service purpose built for this metadata management use case across different analytics engines, Option C clearly stands out as the solution requiring the least development effort.","timestamp":"1709373960.0","upvote_count":"2"}],"timestamp":"1709373960.0","content":"Option C, using the AWS Glue Data Catalog, requires the least development effort to meet the requirements for a central metadata repository accessed from EMR and Athena.","poster":"Felix_G","comment_id":"1164039"}],"question_id":162,"answer_ET":"C","isMC":true},{"id":"yETEE7lxUq9avCDFgY7f","topic":"1","question_id":163,"discussion":[{"timestamp":"1724808660.0","content":"Selected Answer: D\nUsing Amazon S3 for storage and AWS Lake Formation for fine-grained access control like row-level or column-level access.","upvote_count":"1","comment_id":"1273735","poster":"Shanmahi"},{"comment_id":"1266970","content":"Selected Answer: D\nthis id D","timestamp":"1723804140.0","poster":"cas_tori","upvote_count":"3"},{"content":"Option D is the best solution to meet the requirements with the least operational overhead.\n\nUsing Amazon S3 for storage and AWS Lake Formation for access control and data access delivers the following advantages:\n\nS3 provides a highly durable, available, and scalable data lake storage layer\nLake Formation enables fine-grained access control down to column and row-level\nIntegrates natively with Athena, Redshift Spectrum, and EMR for simplified data access\nFully managed service minimizes admin overhead vs self-managing Ranger or piecemeal solutions","poster":"Felix_G","comment_id":"1164051","timestamp":"1709375160.0","upvote_count":"4","comments":[{"poster":"Felix_G","content":"Option A would require custom access control code development and greater ops effort\nOption B still requires managing Ranger integrated with EMR\nOption C does not natively support column-level security policies","comment_id":"1164052","upvote_count":"1","timestamp":"1709375160.0"}]},{"upvote_count":"4","poster":"rralucard_","comment_id":"1138475","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/cbac-tutorial.html\nOption D, using Amazon S3 for data lake storage and AWS Lake Formation for access control, is the most suitable solution. It meets the requirements for row-level and column-level access control and integrates well with Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on EMR, all with lower operational overhead compared to the other options.","timestamp":"1706872680.0"}],"unix_timestamp":1706872680,"timestamp":"2024-02-02 12:18:00","isMC":true,"answer_images":[],"answers_community":["D (100%)"],"answer":"D","exam_id":21,"question_images":[],"choices":{"D":"Use Amazon S3 for data lake storage. Use AWS Lake Formation to restrict data access by rows and columns. Provide data access through AWS Lake Formation.","B":"Use Amazon S3 for data lake storage. Use Apache Ranger through Amazon EMR to restrict data access by rows and columns. Provide data access by using Apache Pig.","A":"Use Amazon S3 for data lake storage. Use S3 access policies to restrict data access by rows and columns. Provide data access through Amazon S3.","C":"Use Amazon Redshift for data lake storage. Use Redshift security policies to restrict data access by rows and columns. Provide data access by using Apache Spark and Amazon Athena federated queries."},"question_text":"A company needs to build a data lake in AWS. The company must provide row-level data access and column-level data access to specific teams. The teams will access the data by using Amazon Athena, Amazon Redshift Spectrum, and Apache Hive from Amazon EMR.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_ET":"D","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/132686-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"Hiu68ITqAgklcR2HrngD","url":"https://www.examtopics.com/discussions/amazon/view/131469-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","question_id":164,"choices":{"C":"AWS Glue Studio","B":"AWS Glue workflows","A":"AWS Step Functions","D":"Amazon Managed Workflows for Apache Airflow (Amazon MWAA)"},"answer_ET":"B","question_text":"A company created an extract, transform, and load (ETL) data pipeline in AWS Glue. A data engineer must crawl a table that is in Microsoft SQL Server. The data engineer needs to extract, transform, and load the output of the crawl to an Amazon S3 bucket. The data engineer also must orchestrate the data pipeline.\nWhich AWS service or feature will meet these requirements MOST cost-effectively?","discussion":[{"poster":"milofficial","upvote_count":"9","comment_id":"1125629","content":"Selected Answer: B\nGlue workflows are the easiest solution here:\n\nhttps://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/\n\nhttps://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/","timestamp":"1705566720.0"},{"content":"Selected Answer: B\nI asked an AI.\nAnalysis of the answers:\nA. AWS Step Functions:\nIt is a good option for orchestrating workflows with steps from different AWS services, but requires additional development to connect to Microsoft SQL Server.\nB. AWS Glue Workflows:\nThis is the best and most profitable option. AWS Glue is designed specifically for ETL on AWS and integrates directly with data sources such as Microsoft SQL Server through connectors. This allows for easier configuration and avoids the need for additional development.\nC. AWS Glue Studio:\nIt is a visual interface for AWS Glue that makes it easy to create and manage ETL jobs. However, the underlying functionality comes from AWS Glue (B) workflows.\nD. Amazon Managed Workflows for Apache Airflow (Amazon MWAA):\nIt's a viable option, but it's generally more expensive than native AWS services like AWS Glue Workflows. Additionally, it requires some Airflow experience for setup and maintenance.","poster":"dev_vicente","comment_id":"1182435","timestamp":"1727080080.0","upvote_count":"7"},{"comment_id":"1291351","poster":"Adrifersilva","timestamp":"1727660280.0","upvote_count":"1","content":"Selected Answer: B\nhttps://community.aws/content/2iBQiAGS4RvEolgSQKu4iF8InTV/choose-the-right-data-orchestration-service-for-your-data-pipeline?lang=en"},{"comment_id":"1288857","timestamp":"1727233920.0","poster":"Shubham1989","content":"Selected Answer: B\nGlue is easiest here to choose from.","upvote_count":"1"},{"poster":"DevoteamAnalytix","content":"Selected Answer: B\nAgree with B. CRAWLING and ETL are the main functions of a Glue workflow and MS SQL is supported: https://docs.aws.amazon.com/glue/latest/dg/crawler-data-stores.html","upvote_count":"3","comment_id":"1205968","timestamp":"1714720680.0"},{"upvote_count":"1","poster":"Alcee","content":"Is B !","timestamp":"1708792320.0","comment_id":"1158037"}],"answer_images":[],"answers_community":["B (100%)"],"answer":"B","exam_id":21,"isMC":true,"question_images":[],"unix_timestamp":1705566720,"answer_description":"","timestamp":"2024-01-18 09:32:00"},{"id":"yynVfzCTEKiPlBbmcgWr","question_text":"An airline company is collecting metrics about flight activities for analytics. The company is conducting a proof of concept (POC) test to show how analytics can provide insights that the company can use to increase on-time departures.\nThe POC test uses objects in Amazon S3 that contain the metrics in .csv format. The POC test uses Amazon Athena to query the data. The data is partitioned in the S3 bucket by date.\nAs the amount of data increases, the company wants to optimize the storage solution to improve query performance.\nWhich combination of solutions will meet these requirements? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/132687-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":165,"unix_timestamp":1706873760,"discussion":[{"poster":"rralucard_","comment_id":"1138486","timestamp":"1722591360.0","content":"Selected Answer: CE\nhttps://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html","upvote_count":"6"},{"poster":"Ramdi1","upvote_count":"1","timestamp":"1741519080.0","comment_id":"1369677","content":"Selected Answer: CE\nC - Reduces latency and network costs → When Athena queries S3 data in the same AWS Region, data does not cross AWS Regions, improving performance.\nLower query execution time → No inter-region data transfer delays.\nCost-Effective → AWS charges for cross-region data transfers, but querying within the same region avoids these costs.\n\n\nE - Parquet is a columnar storage format → Queries can fetch only needed columns, reducing scanning costs."},{"poster":"tgv","timestamp":"1732787280.0","comment_id":"1220065","upvote_count":"1","content":"Selected Answer: CE\nI will go with C and E."},{"timestamp":"1728625740.0","poster":"matasejem","upvote_count":"1","comment_id":"1193465","content":"C is not mentioned anywhere in the https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html"},{"upvote_count":"1","timestamp":"1725649560.0","poster":"damaldon","content":"Answer C and E","comment_id":"1167502"}],"answer_ET":"CE","question_images":[],"timestamp":"2024-02-02 12:36:00","exam_id":21,"isMC":true,"choices":{"E":"Preprocess the .csv data to Apache Parquet format by fetching only the data blocks that are needed for predicates.","D":"Preprocess the .csv data to JSON format by fetching only the document keys that the query requires.","B":"Use an S3 bucket that is in the same account that uses Athena to query the data.","A":"Add a randomized string to the beginning of the keys in Amazon S3 to get more throughput across partitions.","C":"Use an S3 bucket that is in the same AWS Region where the company runs Athena queries."},"answers_community":["CE (100%)"],"answer_images":[],"answer":"CE","answer_description":"","topic":"1"}],"exam":{"numberOfQuestions":207,"isMCOnly":true,"provider":"Amazon","id":21,"isBeta":false,"name":"AWS Certified Data Engineer - Associate DEA-C01","lastUpdated":"11 Apr 2025","isImplemented":true},"currentPage":33},"__N_SSP":true}