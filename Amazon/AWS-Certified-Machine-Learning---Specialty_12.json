{"pageProps":{"questions":[{"id":"18CczHLsuUyHOQC221L7","choices":{"D":"Trigger an AWS Lambda function when social media posts are added to the S3 bucket. Call Amazon Comprehend for each post to capture the sentiment in the message and record the sentiment in a custom Amazon CloudWatch metric and in S3. Use CloudWatch alarms to notify analysts of trends.","A":"Train a model in Amazon SageMaker by using the BlazingText algorithm to detect sentiment in the corpus of social media posts. Expose an endpoint that can be called by AWS Lambda. Trigger a Lambda function when posts are added to the S3 bucket to invoke the endpoint and record the sentiment in an Amazon DynamoDB table and in a custom Amazon CloudWatch metric. Use CloudWatch alarms to notify analysts of trends.","C":"Trigger an AWS Lambda function when social media posts are added to the S3 bucket. Call Amazon Comprehend for each post to capture the sentiment in the message and record the sentiment in an Amazon DynamoDB table. Schedule a second Lambda function to query recently added records and send an Amazon Simple Notification Service (Amazon SNS) notification to notify analysts of trends.","B":"Train a model in Amazon SageMaker by using the semantic segmentation algorithm to model the semantic content in the corpus of social media posts. Expose an endpoint that can be called by AWS Lambda. Trigger a Lambda function when objects are added to the S3 bucket to invoke the endpoint and record the sentiment in an Amazon DynamoDB table. Schedule a second Lambda function to query recently added records and send an Amazon Simple Notification Service (Amazon SNS) notification to notify analysts of trends."},"answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/74079-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"D","timestamp":"2022-04-22 05:08:00","answer_images":[],"exam_id":26,"question_id":56,"topic":"1","question_images":[],"unix_timestamp":1650596880,"discussion":[{"comment_id":"591302","content":"Selected Answer: D\nD is the correct answer.\nFollowing from the previous comment. The company wants to minimize the infrastructure and data science resources needed to evaluate the messages. Therefore any custom services would be eliminated (A and B). Similarly DynamoDB would add complexity to the infrastructure there C is eliminated. leaving D","upvote_count":"13","poster":"cron0001","timestamp":"1650843480.0"},{"content":"Selected Answer: C\nRecording Sentiment in cloudwatch metric seems odd. DynamoDB seems more accurate.","timestamp":"1733371860.0","comment_id":"1322214","poster":"hk0308","upvote_count":"2"},{"poster":"Sharath1783","comment_id":"994973","upvote_count":"1","timestamp":"1693474560.0","content":"Selected Answer: D\nOption D is the right answer.\nFollowing are the key terms in question to notice,\nsentiment expressed in social media posts --> Comprehend\nconfigure alarms based on various thresholds --> CloudWatch (can send alerts without SNS)\nwants to minimize the infrastructure and data science resources --> AWS S3"},{"timestamp":"1693137120.0","comment_id":"991418","poster":"Mickey321","upvote_count":"1","content":"Selected Answer: D\nThe best services for the data science team to use to deliver this solution are option D, trigger an AWS Lambda function when social media posts are added to the S3 bucket, call Amazon Comprehend for each post to capture the sentiment in the message and record the sentiment in a custom Amazon CloudWatch metric and in S3, and use CloudWatch alarms to notify analysts of trends. By doing so, the data science team can use Amazon Comprehend, a natural language processing (NLP) service that uses machine learning to find insights and relationships in text, to evaluate the sentiment expressed in social media posts. Amazon Comprehend can detect positive, negative, neutral, or mixed sentiment from text input. The data science team can also use AWS Lambda, a service that lets you run code without provisioning or managing servers, to trigger a function when posts are added to the S3 bucket and call Amazon Comprehend for each post."},{"timestamp":"1674994140.0","content":"Selected Answer: D\nAmazingly D is possible - https://catalog.us-east-1.prod.workshops.aws/workshops/4faab440-8c3a-4527-bd11-0c88a6e6213c/en-US/30-build-the-application/400-send-sentiment-to-cloudwatch\nI was so sure of option C, because sending a sentiment to a custom CloudWatch metric just didn't make any sense. But you learn something new everyday.","upvote_count":"4","poster":"uninit","comment_id":"791611"},{"timestamp":"1653202920.0","content":"This is a puzzling question, as both answers C and D miss essential steps:\nC is missing DynamoDB Streams to capture new records\nD is missing a notification mechanism like SNS, as CloudWatch Alarms alone can only be used as a trigger, but are not sufficient for notification\nI agree that A and B should be eliminated for requiring data science development","upvote_count":"1","comment_id":"605220","poster":"dolorez"},{"timestamp":"1651346880.0","content":"I also do agree that D is correct answer. In A, why we are adding extra dependency of Dynamo DB.","poster":"NILKK","upvote_count":"4","comment_id":"595232"},{"upvote_count":"4","comments":[{"upvote_count":"1","comment_id":"879755","content":"BlazingText can do sentiment analysis: \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html","poster":"Maaayaaa","timestamp":"1682377680.0"}],"poster":"knightknt","timestamp":"1650596880.0","content":"D, blazing text is not for sentiment analysis.\nThe Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification.","comment_id":"589703"}],"answer_description":"","question_text":"A company is launching a new product and needs to build a mechanism to monitor comments about the company and its new product on social media. The company needs to be able to evaluate the sentiment expressed in social media posts, and visualize trends and configure alarms based on various thresholds.\nThe company needs to implement this solution quickly, and wants to minimize the infrastructure and data science resources needed to evaluate the messages.\nThe company already has a solution in place to collect posts and store them within an Amazon S3 bucket.\nWhat services should the data science team use to deliver this solution?","answers_community":["D (90%)","10%"],"isMC":true},{"id":"F0P60Y5zs0Ch7oFtnbyy","question_images":[],"answer":"A","answer_description":"","question_text":"A Machine Learning Specialist at a company sensitive to security is preparing a dataset for model training. The dataset is stored in Amazon S3 and contains\nPersonally Identifiable Information (PII).\nThe dataset:\n✑ Must be accessible from a VPC only.\n✑ Must not traverse the public internet.\nHow can these requirements be satisfied?","answer_images":[],"topic":"1","question_id":57,"exam_id":26,"discussion":[{"comment_id":"54342","poster":"rajs","timestamp":"1633072920.0","upvote_count":"41","content":"Important things to note here is that \n\n1. \"The Data in S3 Needs to be Accessible from VPC\"\n2. \"Traffic should not Traverse internet\"\n\nTo fulfill Requirement #2 we need a VPC endpoint\nTo RESTRICT the access to S3/Bucket\n - Access allowed only from VPC via VPC Endpoint\n\nEven though Sagemaker uses EC2 - we are NOT asked to secure the EC2 :) \n\nSo the answer is A"},{"poster":"sdsfsdsf","comment_id":"64104","content":"Between A & B, the answer should be A. From here:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html#vpc-endpoints-s3-bucket-policies\nWe can see that we restrict access using DENY if sourceVpce (vpc endpoint), or sourceVpc (vpc) is not equal to our VPCe/VPC. So we are using a DENY (choice A) and not an ALLOW policy (choice B).\n\nChoices C, D we eliminate because they don't address S3 access at all.","upvote_count":"12","timestamp":"1633440900.0"},{"timestamp":"1739655420.0","poster":"JonSno","upvote_count":"2","content":"Selected Answer: A\nCreate a VPC endpoint and apply a bucket access policy that restricts access to the given VPC endpoint and the VPC. \nWhy is this correct?\nVPC endpoint for S3 allows private connectivity between Amazon S3 and the VPC without using the public internet.\nBucket access policy can be written to allow access only from this VPC endpoint.\nThis ensures maximum security by:\nPreventing access from outside the VPC.\nBlocking public access.","comment_id":"1357064"},{"upvote_count":"1","timestamp":"1727164380.0","poster":"AjoseO","comment_id":"803142","content":"Selected Answer: A\nIn Option A, the Machine Learning Specialist would create a VPC endpoint for Amazon S3, which would allow traffic to flow directly between the VPC and Amazon S3 without traversing the public internet. Access to the S3 bucket containing PII can then be restricted to the VPC endpoint and the VPC using a bucket access policy. This would ensure that only instances within the VPC can access the data, and that the data does not traverse the public internet.\n\nOption B and D, allowing access from an Amazon EC2 instance, would not meet the requirement of not traversing the public internet, as the EC2 instance would be accessible from the internet. Option C, using Network Access Control Lists (NACLs) to allow traffic between only the VPC endpoint and an EC2 instance, would also not meet the requirement of not traversing the public internet, as the EC2 instance would still be accessible from the internet."},{"content":"Selected Answer: A\nA. YES - We first create a S3 endpoint in the VPC subnet so traffic does not flow through the Internet, then on the S3 bucket create an access policy that restricts access to the given VPC based on its ID \nB. NO - we don't want to be specific to an instance\nC. NO - the S3 bucket is on AWS network, you cannot change the NACL for it\nD. NO - not all instances in a VPC will necessarily have the same principal that can be specified in the policy","timestamp":"1727164380.0","poster":"loict","comment_id":"1006364","upvote_count":"2"},{"timestamp":"1691247240.0","poster":"Mickey321","upvote_count":"1","content":"Selected Answer: A\nDefinetly A","comment_id":"973107"},{"comment_id":"963962","content":"Selected Answer: A\nWell, but removing methodology, only A remains: The question never cited EC2","poster":"kaike_reis","timestamp":"1690388640.0","upvote_count":"3"},{"content":"Per https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html it's A","comment_id":"935832","upvote_count":"1","poster":"ADVIT","timestamp":"1687899120.0"},{"upvote_count":"4","timestamp":"1653729960.0","comment_id":"608338","poster":"exam887","content":"Selected Answer: A\nThe question do not mention EC2 at all, so should be A"},{"upvote_count":"1","content":"I think it should be B. Traning instance is a EC2 instance and need to be set an endpoint to load the data from S3.","timestamp":"1640887980.0","poster":"dunhill","comment_id":"513578"},{"timestamp":"1637588760.0","poster":"[Removed]","comment_id":"484252","content":"Selected Answer: B\nAWS security is a conservative security model, which implies that access are denied by default rather than granted by default. We have to explicitly allow access to a AWS resource. Additionally, B talks about allowing access FROM the VPC to S3 while A talks about allowing access from S3 to VPC (which is not what we need). \nSo, B.","comments":[{"upvote_count":"1","content":"Um, no. A VPC endpoint is outbound from the VPC to a supported AWS service.","comment_id":"844183","timestamp":"1679256360.0","poster":"cpal012"}],"upvote_count":"2"},{"poster":"technoguy","content":"Will go with B","upvote_count":"1","comment_id":"439525","timestamp":"1636273440.0"},{"poster":"spamicho","timestamp":"1636156620.0","content":"Betting on B here, we should control access from VPC, not to VPC.","comment_id":"402575","upvote_count":"1"},{"upvote_count":"2","comment_id":"316750","timestamp":"1635646020.0","poster":"achiko","content":"A! \nRestricting access to a specific VPC endpoint\nThe following is an example of an Amazon S3 bucket policy that restricts access to a specific bucket, awsexamplebucket1, only from the VPC endpoint with the ID vpce-1a2b3c4d. The policy denies all access to the bucket if the specified endpoint is not being used. The aws:SourceVpce condition is used to specify the endpoint. \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html"},{"content":"Can't be B. You simple cannot enable access to an endpoint to some selected instance. So A.","comments":[{"comment_id":"398898","timestamp":"1636073100.0","upvote_count":"1","content":"We shouldn't use private IP in bucket policy.","poster":"Huy"},{"timestamp":"1635594900.0","comment_id":"278034","poster":"cloud_trail","upvote_count":"1","content":"B does not say enable access TO the VPC endpoint. It says to allow access FROM the endpoint. So B is the correct answer. A talks about restricting access TO the VPC endpoint, so that option is irrelevant. We're worried about access TO the S3 bucket, not access to the VPC. The question is not poorly-worded, but it is tricky and you need to read it carefully."}],"timestamp":"1635509640.0","upvote_count":"1","poster":"senseikimoji","comment_id":"229515"},{"poster":"yeetusdeleetus","content":"I also vote A.","timestamp":"1635162660.0","upvote_count":"1","comment_id":"212154"},{"upvote_count":"3","comment_id":"190337","timestamp":"1635067500.0","poster":"Thai_Xuan","content":"A\nfound here\n\"You can control which VPCs or VPC endpoints have access to your buckets by using Amazon S3 bucket policies. For examples of this type of bucket policy access control, see the following topics on restricting access.\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies-vpc-endpoint.html"},{"poster":"tff","content":"agree with A","upvote_count":"1","timestamp":"1633926060.0","comment_id":"118234"},{"poster":"morara","timestamp":"1633871220.0","comment_id":"117700","content":"A is the answer. No need to bring in EC2 coz the requirements defined are between S3 and the VPC.","upvote_count":"1"},{"comment_id":"98682","upvote_count":"1","poster":"roytruong","timestamp":"1633770180.0","content":"It is A"},{"upvote_count":"1","poster":"ac427","content":"Instances in the VPC access the VPC EndPoint, not the VPC itself. So A is poorly worded like lots of AWS questions.","timestamp":"1633083900.0","comment_id":"60814"},{"timestamp":"1633072860.0","upvote_count":"4","content":"A THE VPC access the s3 through the VPC endpoint without internet traffic.","comment_id":"47971","poster":"grandgale"},{"upvote_count":"4","timestamp":"1632543540.0","poster":"JayK","comment_id":"35232","content":"I stand corrected. The answer is B as A is talking about \"restrict access to the VPC endpoint\" which is not what we want"},{"comments":[{"comment_id":"41825","poster":"BigEv","content":"I guess it talks about the ml instance spin up by SageMaker.","timestamp":"1632647040.0","upvote_count":"1"}],"poster":"JayK","comment_id":"34629","content":"Should be A. As the question is talking about Amazon S3 and A refers to the bucket policies that belong to S3. There is no mention of EC2 in the question","upvote_count":"6","timestamp":"1632198120.0"}],"timestamp":"2020-01-02 18:49:00","choices":{"D":"Create a VPC endpoint and use security groups to restrict access to the given VPC endpoint and an Amazon EC2 instance","B":"Create a VPC endpoint and apply a bucket access policy that allows access from the given VPC endpoint and an Amazon EC2 instance.","A":"Create a VPC endpoint and apply a bucket access policy that restricts access to the given VPC endpoint and the VPC.","C":"Create a VPC endpoint and use Network Access Control Lists (NACLs) to allow traffic between only the given VPC endpoint and an Amazon EC2 instance."},"isMC":true,"answers_community":["A (87%)","13%"],"url":"https://www.examtopics.com/discussions/amazon/view/11279-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1577987340,"answer_ET":"A"},{"id":"r1z7w0zK0AdXz8JTd27U","answer_description":"","answer_images":[],"exam_id":26,"isMC":true,"question_id":58,"topic":"1","answer_ET":"B","choices":{"B":"Use Amazon SageMaker Studio to rebuild the model. Create a notebook that uses the XGBoost training container to perform model training. Activate Amazon SageMaker Debugger, and configure it to calculate and collect Shapley values. Create a chart that shows features and SHapley Additive exPlanations (SHAP) values to explain to the credit team how the features affect the model outcomes.","C":"Create an Amazon SageMaker notebook instance. Use the notebook instance and the XGBoost library to locally retrain the model. Use the plot_importance() method in the Python XGBoost interface to create a feature importance chart. Use that chart to explain to the credit team how the features affect the model outcomes.","D":"Use Amazon SageMaker Studio to rebuild the model. Create a notebook that uses the XGBoost training container to perform model training. Deploy the model at an endpoint. Use Amazon SageMaker Processing to post-analyze the model and create a feature importance explainability chart automatically for the credit team.","A":"Use Amazon SageMaker Studio to rebuild the model. Create a notebook that uses the XGBoost training container to perform model training. Deploy the model at an endpoint. Enable Amazon SageMaker Model Monitor to store inferences. Use the inferences to create Shapley values that help explain model behavior. Create a chart that shows features and SHapley Additive exPlanations (SHAP) values to explain to the credit team how the features affect the model outcomes."},"question_text":"A bank wants to launch a low-rate credit promotion. The bank is located in a town that recently experienced economic hardship. Only some of the bank's customers were affected by the crisis, so the bank's credit team must identify which customers to target with the promotion. However, the credit team wants to make sure that loyal customers' full credit history is considered when the decision is made.\nThe bank's data science team developed a model that classifies account transactions and understands credit eligibility. The data science team used the XGBoost algorithm to train the model. The team used 7 years of bank transaction historical data for training and hyperparameter tuning over the course of several days.\nThe accuracy of the model is sufficient, but the credit team is struggling to explain accurately why the model denies credit to some customers. The credit team has almost no skill in data science.\nWhat should the data science team do to address this issue in the MOST operationally efficient manner?","unix_timestamp":1651349520,"url":"https://www.examtopics.com/discussions/amazon/view/74996-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"poster":"spaceexplorer","comments":[{"comment_id":"599399","timestamp":"1652158320.0","upvote_count":"7","content":"https://aws.amazon.com/blogs/machine-learning/ml-explainability-with-amazon-sagemaker-debugger/","poster":"siju13"}],"timestamp":"1651349520.0","upvote_count":"15","comment_id":"595254","content":"Selected Answer: B\nB, SageMaker Model Debugger is used to generate SHAP values"},{"poster":"V_B_","comment_id":"644667","timestamp":"1660076400.0","comments":[{"poster":"DimLam","upvote_count":"2","content":"It will show only importance of features not their contribution to the final score","comment_id":"1046854","timestamp":"1697628360.0"}],"upvote_count":"6","content":"Selected Answer: C\nI believe C is the right answer, it is simpler and more accurate than B."},{"poster":"2eb8df0","timestamp":"1741091640.0","comment_id":"1364895","upvote_count":"1","content":"Selected Answer: A\nThe problem is at inference time, not training time. So its A"},{"timestamp":"1728347400.0","content":"I hesitate between A and B...\nIn the question, the credit team wants to understand the reason why the model denies credit at inference time, not at training time...\nSagemaker Model Monitor compute SHAP values at inference time while Sagemaker Debugger compute SHAP values at training time...\nI'm leading more for A as an answer.","poster":"amlgeek","upvote_count":"2","comment_id":"1294507"},{"comment_id":"1147544","content":"Selected Answer: A\nThe best option is to use Amazon SageMaker Studio to rebuild the model and deploy it at an\nendpoint. Then, use Amazon SageMaker Model Monitor to store inferences and use the inferences to\ncreate Shapley values that help explain model behavior. Shapley values are a way of attributing the\ncontribution of each feature to the model output. They can help the credit team understand why the\nmodel makes certain decisions and how the features affect the model outcomes. A chart that shows\nfeatures and SHapley Additive exPlanations (SHAP) values can be created using the SHAP library in\nPython. This option is the most operationally efficient because it leverages the existing XGBoost\ntraining container and the built-in capabilities of Amazon SageMaker Model Monitor and SHAP\nlibrary.","timestamp":"1707672720.0","poster":"kyuhuck","upvote_count":"4"},{"timestamp":"1695121860.0","poster":"loict","upvote_count":"2","comment_id":"1011217","content":"Selected Answer: B\nA. NO - too complicated to compute SHAP\nB. YES - Debugger supports built-in SHAP \nC. NO - too complicated to compute SHAP\nD. NO - too complicated to compute SHAP"},{"upvote_count":"1","comment_id":"968931","timestamp":"1690883700.0","poster":"ccpmad","content":"Selected Answer: B\nOption B utilizes Amazon SageMaker Studio to build and train the model, and it also activates Amazon SageMaker Debugger, which allows calculating and collecting Shapley values. These Shapley values will help explain accurately why the model denies credit to certain customers. Generating a chart that displays the features and their SHAP values will provide a visual and clear explanation of the impact of each feature on the model's decisions, making it easier for the credit team with limited data science skills to understand."},{"comments":[{"content":"More towards B","timestamp":"1690561320.0","comment_id":"965673","poster":"Mickey321","upvote_count":"1"}],"comment_id":"965669","upvote_count":"2","poster":"Mickey321","content":"Either A or B\nSage Maker Monitor require no experience so A is preferred while B can provide more details but depend if require knowledge to use it.","timestamp":"1690560600.0"},{"content":"Selected Answer: A\nSageMaker Model Monitor is a tool that helps monitor the quality of model predictions over time by analyzing data inputs and outputs during inference. It can detect and alert when data drift or concept drift occurs, and can identify features that are most responsible for the changes in model behavior. Model Monitor can be used to continuously monitor and improve model performance, and can be integrated with SageMaker endpoints or SageMaker Pipelines.\n\nSageMaker Debugger is a tool that helps debug machine learning models during training by analyzing the internal states of the model, such as weights and gradients, as well as the data inputs and outputs during training. It can detect and alert when common training issues occur, such as overfitting or underfitting, and can identify the root causes of these issues. Debugger can be used to improve model accuracy and convergence, and can be integrated with SageMaker training jobs.","poster":"Ahmedhadi_","upvote_count":"2","timestamp":"1681772880.0","comments":[{"upvote_count":"2","poster":"Ahmedhadi_","comment_id":"879237","content":"After reconsideration, it is actually B.\nhttps://aws.amazon.com/blogs/machine-learning/ml-explainability-with-amazon-sagemaker-debugger/","timestamp":"1682332440.0"}],"comment_id":"873163"},{"comments":[{"comments":[{"comment_id":"892361","upvote_count":"2","poster":"ZSun","content":"not comparing between A and C, should be A and B","timestamp":"1683565140.0"}],"upvote_count":"2","comment_id":"892360","content":"There are so many explanations, but most of them are just superfacial, focusing on what service is related to SHAP. This is the only one really answer the difference between A and C.\n1. Both SagaMaker Model Monitor and Debugger can explain model, can generate SHAP. so it should be either A or C\n2. Monitor is about inference. After deploy the model, we may find some attributes start to contribute more to the model, contradict to the training dataset. This case we use SageMaker Model Monitor.\nBut our problem is not about deploying, is still in training stage. We only want to figure out why some customer with specfic characteristics are more likely to get loan, in other words, certain feature contribute more to the prediction. \nIt is C !!!!\nIf you don't fully understant the question, stop explaining !!!","timestamp":"1683565080.0","poster":"ZSun"}],"upvote_count":"1","content":"Selected Answer: B\nDebugger because we are in the context of \"training data\"","timestamp":"1680532800.0","comment_id":"860026","poster":"Mllb"},{"upvote_count":"1","comment_id":"839635","content":"Selected Answer: C\nC is the straight forward and simpler.","timestamp":"1678865940.0","poster":"Amit11011996"},{"poster":"Amit11011996","content":"Why not C?\n'C' is the most easiest way to find out.!","comment_id":"831701","upvote_count":"1","timestamp":"1678181220.0"},{"content":"Selected Answer: B\nThis is debugger’s work","upvote_count":"2","timestamp":"1677961320.0","comment_id":"829376","poster":"Chelseajcole"},{"content":"Selected Answer: A\nOption A suggests using Amazon SageMaker Model Monitor to store inferences and create Shapley values that can help explain the model's behavior. This option can be more operationally efficient because it doesn't require the credit team to understand the complexities of Shapley values, and it doesn't necessarily slow down the model's inference time.","comment_id":"811775","poster":"AjoseO","upvote_count":"1","comments":[{"comment_id":"811777","poster":"AjoseO","content":"After a review, I go with option B","upvote_count":"1","timestamp":"1676631720.0"}],"timestamp":"1676631540.0"}],"answer":"B","timestamp":"2022-04-30 22:12:00","answers_community":["B (58%)","A (22%)","C (19%)"],"question_images":[]},{"id":"klz2noO1tG9ay5RfRaj5","exam_id":26,"choices":{"D":"Use Amazon Comprehend for the part-of-speech tagging and key phase extraction tasks. Use AWS Deep Learning Containers with Amazon SageMaker to build the custom classifier.","A":"Use Amazon Comprehend for the part-of-speech tagging, key phase extraction, and classification tasks.","C":"Use Amazon Comprehend for the part-of-speech tagging and key phase extraction tasks. Use Amazon SageMaker built-in Latent Dirichlet Allocation (LDA) algorithm to build the custom classifier.","B":"Use an NLP library in Amazon SageMaker for the part-of-speech tagging. Use Amazon Comprehend for the key phase extraction. Use AWS Deep Learning Containers with Amazon SageMaker to build the custom classifier."},"topic":"1","answers_community":["D (65%)","A (35%)"],"isMC":true,"question_images":[],"timestamp":"2022-05-10 17:51:00","url":"https://www.examtopics.com/discussions/amazon/view/75436-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"upvote_count":"18","content":"I will go with A. Refer to link : https://aws.amazon.com/comprehend/features/","poster":"exam_prep","comment_id":"606335","timestamp":"1653344160.0","comments":[{"upvote_count":"1","content":"whoever select A misunderstant \"Custom classification\", it is model for custom classificaiton, not submitting your own script!!!!\nand for the above reply with document, read document first.","poster":"ZSun","comment_id":"888711","timestamp":"1683128820.0"},{"upvote_count":"10","poster":"tgaos","timestamp":"1653778680.0","comment_id":"608543","content":"Agree. A is my answer. \n1. part of speech tagging : https://docs.aws.amazon.com/comprehend/latest/dg/API_PartOfSpeechTag.html\n2. Key phas extraction\nhttps://docs.aws.amazon.com/comprehend/latest/dg/how-key-phrases.html\n3. custum classification algorithm\nhttps://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html"}]},{"poster":"ovokpus","content":"Selected Answer: D\nD is the answer. Using Apache MXNet rules out Comprehend from making the classification task","upvote_count":"14","comment_id":"621896","comments":[{"content":"any reference?","comment_id":"716429","upvote_count":"1","timestamp":"1668216840.0","poster":"VinceCar","comments":[{"comment_id":"716430","timestamp":"1668217200.0","poster":"VinceCar","upvote_count":"2","content":"\"Automatically improve performance with optimized model training for popular frameworks like TensorFlow, PyTorch, and Apache MXNet.\" \n\nhttps://aws.amazon.com/cn/machine-learning/containers/"}]}],"timestamp":"1656110460.0"},{"content":"Selected Answer: D\nPreprocessing using Comprehend. Then use preprocessed text as input to custom classifier.","comment_id":"1340067","upvote_count":"1","timestamp":"1736805300.0","poster":"2bc8f6c"},{"timestamp":"1727929560.0","content":"Selected Answer: D\nAmazon Comprehend cant bring your own model, the feature of \"custom classification\" is meaning that your can train a classifier on the service with your own data, not bring your own model on. \n\nSo the answer is definitely D.","comment_id":"1292630","upvote_count":"1","poster":"MJSY"},{"upvote_count":"1","comment_id":"1148007","timestamp":"1707736320.0","poster":"kyuhuck","content":"Selected Answer: D\nAmazon Comprehend is a natural language processing (NLP) service that can perform part-of-speech\ntagging and key phrase extraction tasks. AWS Deep Learning Containers are Docker images that are\npre-installed with popular deep learning frameworks such as Apache MXNet. Amazon SageMaker is a\nfully managed service that can help build, train, and deploy machine learning models. Using Amazon\nComprehend for the text preprocessing tasks and AWS Deep Learning Containers with Amazon\nSageMaker to build the custom classifier is the solution that can be built most quickly to meet the\nrequirements.\nReferences:\nAmazon Comprehend\nAWS Deep Learning Container"},{"comment_id":"1117154","upvote_count":"1","content":"Selected Answer: D\nThe Custom classification in AWS Comprehend cannot choose algorithm, you cannot use your own algorithm in it. You only feed dataset to it.\nSo A is wrong.\nThe data science team want to use their own MXNET model, so D.","poster":"rav009","timestamp":"1704765600.0"},{"timestamp":"1697909460.0","comment_id":"1049704","content":"Selected Answer: A\nWill go with A","upvote_count":"1","poster":"backbencher2022"},{"timestamp":"1695633300.0","upvote_count":"1","poster":"ixdb","content":"Selected Answer: A\nA is the most quickly solution.","comment_id":"1016658"},{"comment_id":"976666","content":"Selected Answer: D\nWe have to solve two NLP problems: part-of-speech tagging and key phase extraction. Note that the custom classifier already exists and has been trained! The question asks that it be done as quickly as possible, so the idea is to use a ready-made service. Letter A is wrong, as it uses another service compared to the already created model to classify. Letter B requires development and therefore would not be the fastest solution. Letter C is wrong for the same reason as Letter A, in addition it proposes an unsupervised service (LDA) for a supervised problem. Letter D is correct.","timestamp":"1691586540.0","upvote_count":"3","poster":"kaike_reis"},{"comment_id":"965683","upvote_count":"1","content":"Selected Answer: D\nTherefore, option D is the most efficient solution for building a NLP application that meets the requirements of the data science team.","timestamp":"1690562340.0","poster":"Mickey321"},{"timestamp":"1689081960.0","poster":"ADVIT","content":"Selected Answer: A\nQuickest A","upvote_count":"4","comment_id":"948978","comments":[{"poster":"kukreti18","content":"Latest is A.","comment_id":"955600","upvote_count":"1","timestamp":"1689696000.0"}]},{"timestamp":"1680466500.0","comment_id":"859330","poster":"Mllb","upvote_count":"3","content":"Selected Answer: D\nThe other mxnet model is the key"},{"upvote_count":"3","content":"Selected Answer: D\noption D is the most appropriate answer, given that the team has already written and trained a custom classification algorithm using Apache MXNet. \n\nOption D allows the team to use Amazon Comprehend for part-of-speech tagging and key phrase extraction, while also using AWS Deep Learning Containers with Amazon SageMaker to build and deploy the custom classifier.","comment_id":"811711","poster":"AjoseO","timestamp":"1676627700.0"},{"comments":[{"upvote_count":"2","comment_id":"805216","timestamp":"1676120940.0","poster":"drcok87","content":"I had selected \"A\" in my first go, thanks for understanding the question. Although, comprehend does all three, since they have already built custom classification, we only need to provide solution for first two.\n\nD for me too."}],"upvote_count":"5","content":"D for me. Question says \"The preprocessed text WILL be input to a custom classification algorithm that the data science team has already written and trained using Apache MXNet\". So for some reason they want to use MXNet to do the classification, not Amazon Comprehend. So using MXNet for classification is a part of their requirement. How do we meet these requirements quickly? Well, use Amazon Comprehend for part-of-speech and key phrase tasks; and use container for the MXNet stuff.","timestamp":"1675143780.0","poster":"wolfsong","comment_id":"793697"},{"content":"The question did not make it clear whether the new solution has to use the custom model that the team built or not.","timestamp":"1672625640.0","poster":"hamimelon","comment_id":"763456","upvote_count":"2"},{"timestamp":"1669600800.0","poster":"tsangckl","content":"Selected Answer: A\nA for me","upvote_count":"3","comment_id":"728728"},{"content":"Selected Answer: A\nAgreed with A, Comprehend 3 functions","timestamp":"1669559640.0","comment_id":"728334","poster":"ystotest","upvote_count":"3"},{"comment_id":"676500","poster":"HerbertK","timestamp":"1663873680.0","upvote_count":"5","content":"Selected Answer: A\nA for me.\nhttps://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html"},{"content":"Selected Answer: D\nD for me","timestamp":"1652197860.0","comment_id":"599678","upvote_count":"4","poster":"ckkobe24"}],"answer_ET":"D","answer_description":"","unix_timestamp":1652197860,"answer":"D","question_text":"A data science team is planning to build a natural language processing (NLP) application. The application's text preprocessing stage will include part-of-speech tagging and key phase extraction. The preprocessed text will be input to a custom classification algorithm that the data science team has already written and trained using Apache MXNet.\nWhich solution can the team build MOST quickly to meet these requirements?","question_id":59,"answer_images":[]},{"id":"vdyunaGwyfRN1PhQ2qQZ","answer_ET":"A","question_text":"A machine learning (ML) specialist must develop a classification model for a financial services company. A domain expert provides the dataset, which is tabular with 10,000 rows and 1,020 features. During exploratory data analysis, the specialist finds no missing values and a small percentage of duplicate rows. There are correlation scores of > 0.9 for 200 feature pairs. The mean value of each feature is similar to its 50th percentile.\nWhich feature engineering strategy should the ML specialist use with Amazon SageMaker?","isMC":true,"answer":"A","answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/75321-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","timestamp":"2022-05-09 06:36:00","unix_timestamp":1652070960,"question_id":60,"answer_images":[],"choices":{"D":"Concatenate the features with high correlation scores by using a Jupyter notebook.","C":"Apply anomaly detection by using the Random Cut Forest (RCF) algorithm.","B":"Drop the features with low correlation scores by using a Jupyter notebook.","A":"Apply dimensionality reduction by using the principal component analysis (PCA) algorithm."},"discussion":[{"upvote_count":"10","comment_id":"622302","content":"Selected Answer: A\nDimensions are too high. Use PCA","timestamp":"1687741680.0","poster":"ovokpus"},{"content":"A should be the answer to avoid the curse of dimensionality","timestamp":"1683626880.0","poster":"LydiaGom","upvote_count":"7","comment_id":"598973"},{"poster":"chet100","content":"Easy choice. Always choose PCA for dim reduction","upvote_count":"2","comment_id":"991051","timestamp":"1724707020.0"},{"upvote_count":"3","content":"Selected Answer: A\nthe best feature engineering strategy for the ML specialist to use with Amazon SageMaker is to apply dimensionality reduction by using the PCA algorithm.","poster":"Mickey321","comment_id":"965692","timestamp":"1722185220.0"},{"content":"Selected Answer: A\nGiven that the dataset has 1,020 features and 200 of them are highly correlated, it is likely that the dataset suffers from multicollinearity. In such cases, dimensionality reduction techniques like principal component analysis (PCA) can be used to transform the data into a lower dimensional space without losing much information. Therefore, option A, \"Apply dimensionality reduction by using the principal component analysis (PCA) algorithm\" is the most appropriate feature engineering strategy for the ML specialist to use with Amazon SageMaker. This would help reduce the computational complexity of the model, improve model performance, and help to avoid overfitting.","timestamp":"1714192440.0","comment_id":"882225","poster":"Gaby999","upvote_count":"1"},{"timestamp":"1708162680.0","poster":"AjoseO","upvote_count":"1","content":"Selected Answer: A\nA. Apply dimensionality reduction by using the principal component analysis (PCA) algorithm.\n\nSince the dataset has many features, and a significant number of them have high correlation scores, the model may suffer from the curse of dimensionality. To reduce the dimensionality of the dataset, the specialist can use a technique like PCA, which reduces the number of features while still retaining the maximum amount of information. PCA can help remove redundant features and improve the model's performance by reducing the chances of overfitting. Additionally, since there are no missing values and a small percentage of duplicate rows, no data cleaning techniques like anomaly detection or dropping the features are required. Concatenating features with high correlation scores is not an appropriate strategy since it may lead to collinearity issues.","comment_id":"811699"},{"comment_id":"805218","upvote_count":"1","timestamp":"1707657060.0","content":"A PCA: PCA is a linear dimensionality reduction technique (algorithm) that transforms a set of correlated variables (p) into a smaller k (k<p) number of uncorrelated variables called principal components while retaining as much of the variation in the original dataset as possible","poster":"drcok87"},{"content":"Selected Answer: A\nChoosing C is answer by ExamTopics is completely laughable.","comment_id":"740925","upvote_count":"1","poster":"Peeking","timestamp":"1702207800.0"},{"upvote_count":"4","poster":"DJiang","content":"Selected Answer: A\nI think it's A.","comment_id":"598836","timestamp":"1683606960.0"}],"question_images":[],"exam_id":26,"answer_description":""}],"exam":{"isMCOnly":false,"isBeta":false,"name":"AWS Certified Machine Learning - Specialty","id":26,"isImplemented":true,"lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":369},"currentPage":12},"__N_SSP":true}