{"pageProps":{"questions":[{"id":"ieiNW4gZCJHilOCQ33SX","choices":{"B":"Configure an S3 bucket policy to explicitly grant the AWS Glue job permissions to access the S3 bucket.","A":"Update the AWS Glue security group to allow inbound traffic from the Amazon S3 VPC gateway endpoint.","D":"Verify that the VPC's route table includes inbound and outbound routes for the Amazon S3 VPC gateway endpoint.","C":"Review the AWS Glue job code to ensure that the AWS Glue connection details include a fully qualified domain name."},"answer_ET":"D","isMC":true,"timestamp":"2024-02-06 12:05:00","answer_description":"","answers_community":["D (95%)","5%"],"answer_images":[],"question_images":[],"exam_id":21,"unix_timestamp":1707217500,"question_id":1,"discussion":[{"comment_id":"1154639","upvote_count":"6","content":"Selected Answer: D\nA - wrong - AWS glue - are serverless service, so it don't have any security groups\nB - wrong - Because we have error with VPC, not with S3 itself\nC - wrong - Becuase with S3 - we always have only FQDN for buckets","poster":"HunkyBunky","comments":[{"timestamp":"1720904100.0","upvote_count":"1","comment_id":"1247495","content":"they most certainly can have SGs.","poster":"alexbg88"}],"timestamp":"1727079900.0"},{"timestamp":"1744036320.0","comment_id":"1558598","poster":"ninomfr64","upvote_count":"1","content":"Selected Answer: D\nA- NO: on SG we just need to allow outbound traffic, as SG i statefull reurn traffic is allowed\nB - NO: since we configured IAM permission for Glue Job, there is no need to configure a resource-policy (cross account is not mentioned)\nC- NO: in bucket connection configuration you just need to provide s3://bucket-name/prefix\nD - YES: although there is no inbound and outbound routes in route table, we need to ensure a route is in place to reach a the VPC Gateway Policy"},{"timestamp":"1740314880.0","poster":"MephiboshethGumani","content":"Selected Answer: D\nD. Verify that the VPC's route table includes inbound and outbound routes for the Amazon S3 VPC gateway endpoint.\n\nExplanation:\n\nAWS Glue jobs need to connect to the S3 bucket through the Amazon S3 VPC gateway endpoint when they are in a VPC. If the route table does not have proper inbound and outbound routes to the S3 VPC gateway endpoint, the AWS Glue job will not be able to access S3, which results in an error.","comment_id":"1360500","upvote_count":"1"},{"content":"D is valid","poster":"wilsonfromnyc9","timestamp":"1727238000.0","upvote_count":"1","comment_id":"1288880"},{"timestamp":"1727079900.0","comment_id":"1167764","poster":"GiorgioGss","content":"Selected Answer: D\nAlthough there is no such thing as \"inbound and outbound routes\" when we talk about VPC route table, when we define a S3 gateway endpoint we must have proper routes in place. I will go with D.","upvote_count":"4"},{"timestamp":"1726862580.0","poster":"ampersandor","upvote_count":"2","content":"Selected Answer: D\nBe sure that the subnet configured for your AWS Glue connection has an Amazon S3 VPC gateway endpoint or a route to a NAT gateway in the subnet's route table.","comment_id":"1202956"},{"content":"Selected Answer: D\nD is correct","comment_id":"1280025","upvote_count":"1","timestamp":"1725714780.0","poster":"GZMartinelli"},{"content":"Selected Answer: D\nI think D. We check \"VPC's route table\"","poster":"lunachi4","comment_id":"1250058","timestamp":"1721267820.0","upvote_count":"1"},{"poster":"teo2157","content":"Selected Answer: C\nA - wrong - AWS glue doesn't have any security groups\nB - wrong - You can´t give permissions in the S3 to the AWS glue job but to the role\nD. wrong because there has to be a definend route for the S3 gateway endpoint in the subnet assigned to the glue job but not in the VPC's route table and also route tables doesn´t have inbound and outbound routes.","comments":[{"comment_id":"1279846","poster":"shammous","upvote_count":"2","content":"\"route tables don´t have inbound and outbound routes.\"? It does. You need to check how the VPC works in AWS.","timestamp":"1725668220.0"}],"comment_id":"1241275","timestamp":"1719994680.0","upvote_count":"1"},{"content":"Selected Answer: D\nD is correct answer.","comment_id":"1223301","poster":"nanaw770","timestamp":"1717367880.0","upvote_count":"2"},{"comment_id":"1219358","upvote_count":"1","content":"I will go with D, the other options don't seem to be related.","timestamp":"1716793560.0","poster":"tgv"},{"comment_id":"1213563","poster":"VerRi","upvote_count":"2","timestamp":"1716082500.0","content":"Selected Answer: D\n\"problems with the Amazon S3 VPC gateway endpoint\""},{"timestamp":"1707904860.0","upvote_count":"1","content":"Go with A:\nIf you receive an error, check the following:\n\nThe correct privileges are provided to the role selected.\nThe correct Amazon S3 bucket is provided.\nThe security groups and Network ACL allow the required incoming and outgoing traffic.\nThe VPC you specified is connected to an Amazon S3 VPC endpoint.","comment_id":"1150011","poster":"damaldon"},{"content":"some relevant info:\nmain: https://docs.aws.amazon.com/glue/latest/dg/connection-VPC-disable-proxy.html\nadditional (glue crawler instead of glue job here, but I think this is relevant for both): https://docs.aws.amazon.com/glue/latest/dg/connection-S3-VPC.html","upvote_count":"2","poster":"Aesthet","comment_id":"1142067","timestamp":"1707217860.0"},{"content":"Both ChatGPT and I agree with D","timestamp":"1707217500.0","comment_id":"1142057","upvote_count":"4","poster":"Aesthet","comments":[{"comment_id":"1200735","content":":-)) nice","poster":"DevoteamAnalytix","upvote_count":"1","timestamp":"1713876360.0"}]}],"url":"https://www.examtopics.com/discussions/amazon/view/133045-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"D","question_text":"A data engineer is configuring an AWS Glue job to read data from an Amazon S3 bucket. The data engineer has set up the necessary AWS Glue connection details and an associated IAM role. However, when the data engineer attempts to run the AWS Glue job, the data engineer receives an error message that indicates that there are problems with the Amazon S3 VPC gateway endpoint.\nThe data engineer must resolve the error and connect the AWS Glue job to the S3 bucket.\nWhich solution will meet this requirement?","topic":"1"},{"id":"x2srhrlPFFMG1EyGFMrw","choices":{"C":"Create an S3 event notification that has an event type of s3:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification.","B":"Create an S3 event notification that has an event type of s3:ObjectTagging:* for objects that have a tag set to .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification.","D":"Create an S3 event notification that has an event type of s3:ObjectCreated:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set an Amazon Simple Notification Service (Amazon SNS) topic as the destination for the event notification. Subscribe the Lambda function to the SNS topic.","A":"Create an S3 event notification that has an event type of s3:ObjectCreated:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification."},"unix_timestamp":1705567380,"discussion":[{"timestamp":"1705567380.0","content":"Selected Answer: A\n\"only if a user uploads data to an Amazon S3 bucket\" that excludes B & C because we need s3:ObjectCreated:*\n\nYou don't need SNS for S3 event notifications so A is easier.","upvote_count":"13","comment_id":"1125639","poster":"milofficial"},{"poster":"TonyStark0122","comment_id":"1137889","upvote_count":"8","content":"A. Create an S3 event notification that has an event type of s3:ObjectCreated:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification.\n\nExplanation:\nThis solution directly triggers the Lambda function only when a .csv file is uploaded to the S3 bucket, minimizing unnecessary invocations of the Lambda function. It uses a specific event type (s3:ObjectCreated:*) and a filter rule to ensure that the Lambda function is invoked only for relevant events. Additionally, it directly invokes the Lambda function without the need for additional services like Amazon SNS, reducing operational overhead.","timestamp":"1706818500.0"},{"upvote_count":"1","timestamp":"1727667240.0","comment_id":"1291364","poster":"Adrifersilva","content":"Selected Answer: A\ns3:ObjectCreated:* instead of s3:*: triggers the Lambda function only when objects are created in the bucket."},{"timestamp":"1727162640.0","content":"Selected Answer: A\nA is the answer for least operational. C also correct!","poster":"theloseralreadytaken","comment_id":"1288447","upvote_count":"1"},{"upvote_count":"1","poster":"pypelyncar","content":"Selected Answer: A\nsince is the least operational, the D its a candidate, however add a SNS operation, which in this case is not needed. so A includes S3 and triggering towards the lambda function. 2 services.","timestamp":"1717858860.0","comment_id":"1226792"},{"comment_id":"1209088","content":"Selected Answer: A\nS3 event notification to lamba for file prefix with.csv is the least overhead way","poster":"k350Secops","upvote_count":"1","timestamp":"1715293680.0"},{"poster":"DevoteamAnalytix","timestamp":"1714722780.0","comment_id":"1205983","content":"Selected Answer: A\n\"You can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted\"\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html","upvote_count":"2"}],"question_text":"A data engineer needs to create an AWS Lambda function that converts the format of data from .csv to Apache Parquet. The Lambda function must run only if a user uploads a .csv file to an Amazon S3 bucket.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_description":"","question_id":2,"question_images":[],"answers_community":["A (100%)"],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/131472-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"A","timestamp":"2024-01-18 09:43:00","exam_id":21,"answer_images":[],"answer_ET":"A","topic":"1"},{"id":"GCnQ3QXxxgqP7T4hz1B3","answer_images":[],"answers_community":["B (53%)","A (42%)","5%"],"unix_timestamp":1718441520,"discussion":[{"poster":"tgv","upvote_count":"9","content":"Selected Answer: A\nActually, I think A makes more sense.","comment_id":"1231313","timestamp":"1718535240.0"},{"poster":"AM027","content":"Selected Answer: A\ncold data stored in Glacier can \n be easily queried within minutes.","comment_id":"1559179","timestamp":"1744187400.0","upvote_count":"1"},{"comment_id":"1347373","content":"Selected Answer: B\nFor workloads with low access frequency where you only need to query data occasionally (for example, during audits), option (A)—S3 Glacier Flexible Retrieval combined with S3 Glacier Select—provides the most cost-effective solution.","timestamp":"1737978300.0","comments":[{"content":"mistake the answer.\nmy answer is A.","poster":"YUICH","upvote_count":"1","timestamp":"1737978360.0","comment_id":"1347374"}],"poster":"YUICH","upvote_count":"1"},{"comment_id":"1344640","content":"Selected Answer: C\nTransaction Data Refers To Data Which Are Updating Frequently and To Query That Data occasionally Means It Can Be Query At Any Time (In Question Time Is Not Define). So We Can't Take Risk For Customer To Wait For Hours To Get The Result And The Best Way To Query The Data On Top of The S3 Bucket We Can Use Athena.","poster":"div_div","timestamp":"1737528120.0","upvote_count":"1"},{"comment_id":"1336834","content":"Selected Answer: B\nGlacier Select incurs higher costs compared to S3 Select.","timestamp":"1736101740.0","poster":"BigMrT","upvote_count":"1"},{"upvote_count":"1","timestamp":"1731887160.0","comment_id":"1313770","poster":"ctndba","comments":[{"upvote_count":"1","timestamp":"1732553940.0","poster":"RockyLeon","comment_id":"1317651","content":"https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-glacier-select-makes-big-data-analytics-of-archive-data-possible/#:~:text=Amazon%20Glacier%20Select%20is%20a,archives%20to%20use%20for%20analytics.\n\n Glacier Select allows queries to run directly on data stored in Amazon Glacier"}],"content":"You cannot use S3 Select on S3 Glacier Flexible Retrieval storage class. So answer is B, based on given options."},{"poster":"mohamedTR","upvote_count":"2","comment_id":"1294234","timestamp":"1728301980.0","content":"Selected Answer: B\nB is the more cost-effective solution for occasional audits. It allows for easier access to the data without incurring high retrieval costs"},{"content":"gzip compressed data querying -> s3 select -Answer B","timestamp":"1727481480.0","comment_id":"1290442","upvote_count":"1","poster":"manig"},{"content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/aws/s3-glacier-select/\n\noption B is not cost effective as it is stored in standard S3","timestamp":"1727289960.0","upvote_count":"2","comment_id":"1289138","poster":"LR2023"},{"content":"Selected Answer: A\nOccasional audits, so go for S3 glacier select","comment_id":"1285202","poster":"PashoQ","timestamp":"1726575780.0","upvote_count":"2"},{"upvote_count":"4","timestamp":"1723629780.0","comment_id":"1265602","poster":"cas_tori","content":"Selected Answer: B\nthis is B"},{"timestamp":"1723439760.0","upvote_count":"1","comment_id":"1264489","poster":"IanJang","content":"IT is A"},{"timestamp":"1723210680.0","content":"Glacier is an expensive option in cases when you need to access data occasionally","upvote_count":"2","comment_id":"1263040","poster":"mns0173"},{"comment_id":"1260524","poster":"lenneth39","content":"Selected Answer: C\nI am not sure whether to go for B or C. Can anyone comment on this?\nB: No problem, but not available if Parquet is Gzip compressed. But the problem statement doesn't say Parquet is Gzip compressed.\nC: Correct if Parquet is Gzip compressed, but B is more cost-effective if csv or json is Gzip compressed","upvote_count":"1","timestamp":"1722748860.0"},{"timestamp":"1721274780.0","upvote_count":"3","comment_id":"1250086","content":"Selected Answer: B\nI think the solution is either B or D but I would go with B because they mentioned storing the data in gzip and not parquet which is optimised for Athena queries","poster":"andrologin"},{"content":"there is no such thing as Glacier Flexible Retrieval, so its no A . its either B or D and most likely its D for the cost","comment_id":"1245770","poster":"4bc91ae","comments":[{"upvote_count":"1","content":"There is glacier flexible retrieval","poster":"catoteja","timestamp":"1723217880.0","comment_id":"1263119"}],"upvote_count":"2","timestamp":"1720652460.0"},{"upvote_count":"2","poster":"bakarys","comment_id":"1242620","timestamp":"1720168320.0","content":"Selected Answer: B\nB. Store the data in Amazon S3. Use Amazon S3 Select to query the data.\n\nAmazon S3 is a cost-effective object storage service, and S3 Select allows you to retrieve only a subset of data from an object by using simple SQL expressions. S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also supports GZIP and BZIP2 compression formats, which makes it suitable for the given scenario where the data is compressed with gzip.\n\nWhile Amazon Athena is a powerful query service, it can be more expensive than S3 Select for occasional queries. Amazon Glacier and Glacier Select are designed for long-term archival storage and not for frequent access or queries, which might not be suitable for occasional audits. Therefore, option B is the most cost-effective choice for this scenario."},{"poster":"FunkyFresco","comment_id":"1241726","content":"Selected Answer: B\nill go with B, because. of cost to query","upvote_count":"2","timestamp":"1720054380.0"},{"content":"Selected Answer: B\nB. Store the data in Amazon S3. Use Amazon S3 Select to query the data.\n\nAmazon S3 is a cost-effective storage service, and S3 Select allows you to retrieve only a subset of data from an object by using simple SQL expressions. S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also supports GZIP compression, which is the format used by the company. This makes it a cost-effective solution for occasional queries needed for audits.","comment_id":"1240769","upvote_count":"2","poster":"bakarys","timestamp":"1719921300.0"},{"upvote_count":"3","poster":"Alagong","comment_id":"1239896","timestamp":"1719787200.0","content":"Selected Answer: B\nOption B (Amazon S3 with S3 Select) is generally more cost-effective and operationally efficient for occasional audits of gzip-compressed data. It provides faster access to data and lower querying costs, which are critical factors for ad-hoc and timely data retrievals. While Option A (Amazon Glacier Flexible Retrieval with S3 Glacier Select) offers cheaper storage, its longer retrieval times and potential higher querying costs make it less suitable for use cases requiring timely access to data."},{"timestamp":"1718933160.0","upvote_count":"3","content":"Looks like that A - fit better in question requirements","comment_id":"1234149","poster":"HunkyBunky"},{"upvote_count":"3","comment_id":"1231202","poster":"GHill1982","timestamp":"1718512500.0","content":"Selected Answer: B\nOn the assumptions that querying the audit data is time sensitive and the transaction data is compressed into a single object I would go with using S3 and S3 select to query the data."},{"content":"Selected Answer: A\nB and C are not cost effective. A is more cost effective than D. I go with A.","comment_id":"1230895","timestamp":"1718447880.0","upvote_count":"4","poster":"artworkad"}],"exam_id":21,"question_text":"An insurance company stores transaction data that the company compressed with gzip.\n\nThe company needs to query the transaction data for occasional audits.\n\nWhich solution will meet this requirement in the MOST cost-effective way?","timestamp":"2024-06-15 10:52:00","topic":"1","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/142575-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"B","answer_ET":"B","answer_description":"","question_images":[],"choices":{"A":"Store the data in Amazon Glacier Flexible Retrieval. Use Amazon S3 Glacier Select to query the data.","C":"Store the data in Amazon S3. Use Amazon Athena to query the data.","D":"Store the data in Amazon Glacier Instant Retrieval. Use Amazon Athena to query the data.","B":"Store the data in Amazon S3. Use Amazon S3 Select to query the data."},"question_id":3},{"id":"Qra1B2c2PlT9gWAfdXvt","choices":{"B":"Schedule and run the stored procedure by using the Amazon Redshift Data API in an Amazon EC2 Spot Instance.","C":"Use query editor v2 to run the stored procedure on a schedule.","A":"Create an AWS Lambda function to schedule a cron job to run the stored procedure.","D":"Schedule an AWS Glue Python shell job to run the stored procedure."},"answer":"C","timestamp":"2024-06-14 23:23:00","question_id":4,"unix_timestamp":1718400180,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/142543-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","exam_id":21,"question_text":"A data engineer finished testing an Amazon Redshift stored procedure that processes and inserts data into a table that is not mission critical. The engineer wants to automatically run the stored procedure on a daily basis.\n\nWhich solution will meet this requirement in the MOST cost-effective way?","topic":"1","answer_images":[],"discussion":[{"content":"Selected Answer: C\nThis can be achieved with query editor v2 (https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html)","timestamp":"1718448000.0","poster":"artworkad","comment_id":"1230897","upvote_count":"7"},{"comment_id":"1250089","timestamp":"1721274960.0","upvote_count":"4","content":"Selected Answer: C\nI go with C because it runs the query within the Redshift instance, B may not be appropriate because it involves other services on top of the Redshift instance and there is movement of data across the services.","poster":"andrologin"},{"comment_id":"1247424","timestamp":"1720895100.0","upvote_count":"1","poster":"4d716d6","content":"Selected Answer: B\ngiven that the table is not mission-critical and requires the \"MOST cost-effective way.\""},{"content":"Selected Answer: B\nI am going with option B, given that the table is not mission-critical and requires the \"MOST cost-effective way.\" \nAWS Spot Instances are Amazon EC2 instances that allow you to utilize spare EC2 capacity at a significantly lower cost than On-Demand instances. These instances are ideal for flexible workloads that can tolerate interruptions, such as batch processing, data analysis, and background processing jobs.","timestamp":"1719514020.0","upvote_count":"1","poster":"salayea28","comment_id":"1238353"},{"poster":"GHill1982","comment_id":"1231203","content":"Selected Answer: C\nI think all options other than using the query editor will incur additional costs.","timestamp":"1718512680.0","upvote_count":"2"},{"timestamp":"1718441640.0","content":"Selected Answer: A\nAWS Lambda, combined with Amazon CloudWatch Events for scheduling, provides a low-cost, serverless, and reliable way to automatically run the stored procedure daily.","comment_id":"1230856","upvote_count":"3","poster":"tgv"},{"comment_id":"1230662","poster":"lalitjhawar","content":"A. Create an AWS Lambda function to schedule a cron job to run the stored procedure.","upvote_count":"1","timestamp":"1718400180.0"}],"isMC":true,"answer_ET":"C","answers_community":["C (72%)","A (17%)","11%"]},{"id":"dCIhCgzu42WYnfI1aSK5","question_images":[],"topic":"1","answer":"BE","unix_timestamp":1718442000,"exam_id":21,"answer_description":"","question_text":"A marketing company collects clickstream data. The company sends the clickstream data to Amazon Kinesis Data Firehose and stores the clickstream data in Amazon S3. The company wants to build a series of dashboards that hundreds of users from multiple departments will use.\n\nThe company will use Amazon QuickSight to develop the dashboards. The company wants a solution that can scale and provide daily updates about clickstream activity.\n\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)","discussion":[{"comment_id":"1241462","upvote_count":"5","timestamp":"1720015500.0","poster":"Ja13","content":"Selected Answer: BE\nB. Use Amazon Athena to query the clickstream data.\nE. Access the query data through QuickSight SPICE (Super-fast, Parallel, In-memory Calculation Engine). Configure a daily refresh for the dataset.\n\nHere's why:\n\nB. Use Amazon Athena to query the clickstream data: Amazon Athena allows you to run SQL queries directly on data stored in Amazon S3 without the need for complex ETL processes. It is a cost-effective solution for querying large datasets on S3.\n\nE. Access the query data through QuickSight SPICE: QuickSight SPICE is designed for fast, in-memory data analysis and can scale to support many users and large datasets. By configuring a daily refresh, you ensure that the dashboards are updated with the latest data while keeping query performance high and costs low."},{"poster":"HagarTheHorrible","content":"Selected Answer: BE\nboth are more or less the only possible","timestamp":"1735113900.0","upvote_count":"1","comment_id":"1331437"},{"upvote_count":"2","timestamp":"1718513340.0","content":"Selected Answer: BE\nAgree with B & E. Athena would be cheaper than Redshift. S3 analytics is irrelevant. The functionality in SPICE should be more cost effective than direct SQL by reducing the frequency and volume of queries.","comment_id":"1231205","poster":"GHill1982"},{"timestamp":"1718442000.0","poster":"tgv","upvote_count":"1","content":"Selected Answer: BE\nAthena charges based on the amount of data scanned per query, which can be cost-effective for ad-hoc querying and periodic updates.\n\nSPICE can be more cost-effective for frequent access and analysis by multiple users as it reduces the load on the underlying data source.","comment_id":"1230861"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/142576-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-06-15 11:00:00","answers_community":["BE (100%)"],"choices":{"E":"Access the query data through QuickSight SPICE (Super-fast, Parallel, In-memory Calculation Engine). Configure a daily refresh for the dataset.","D":"Access the query data through a QuickSight direct SQL query.","B":"Use Amazon Athena to query the clickstream data","C":"Use Amazon S3 analytics to query the clickstream data.","A":"Use Amazon Redshift to store and query the clickstream data."},"answer_ET":"BE","question_id":5,"answer_images":[]}],"exam":{"numberOfQuestions":207,"provider":"Amazon","isImplemented":true,"name":"AWS Certified Data Engineer - Associate DEA-C01","isBeta":false,"lastUpdated":"11 Apr 2025","id":21,"isMCOnly":true},"currentPage":1},"__N_SSP":true}