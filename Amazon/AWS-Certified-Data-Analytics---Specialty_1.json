{"pageProps":{"questions":[{"id":"mG5gpKMFJaejd3DMX4Nn","answer_images":[],"timestamp":"2022-04-20 14:28:00","unix_timestamp":1650457680,"question_text":"A financial services company needs to aggregate daily stock trade data from the exchanges into a data store. The company requires that data be streamed directly into the data store, but also occasionally allows data to be modified using SQL. The solution should integrate complex, analytic queries running with minimal latency. The solution must provide a business intelligence dashboard that enables viewing of the top contributors to anomalies in stock prices.\nWhich solution meets the company's requirements?","discussion":[{"timestamp":"1650464160.0","comment_id":"588796","upvote_count":"17","poster":"CHRIS12722222","content":"complex, analytic queries running with minimal latency = REDSHIFT\nKDF load data into redshift\nAnswer = C"},{"comment_id":"1279440","timestamp":"1725606360.0","content":"the keywords \"complex, analytic queries running with minimal latency\"","poster":"[Removed]","upvote_count":"9"},{"comment_id":"1288873","content":"Selected Answer: C\nC. Use Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Use Amazon Redshift as a data source for Amazon to create a business intelligence dashboard.\nC is valid","poster":"[Removed]","upvote_count":"1","timestamp":"1727237160.0"},{"comment_id":"1174814","content":"Selected Answer: C\nthe keywords \"complex, analytic queries running with minimal latency\"","poster":"Camille1992","timestamp":"1710573300.0","upvote_count":"2"},{"upvote_count":"2","poster":"chinmayj213","comment_id":"1160190","timestamp":"1708997100.0","content":"\"but also occasionally allows data to be modified using SQL\". This is only possible in redshift and redshift does not support kinesis stream directly , so firehouse and then redshift will work as source for quick sight"},{"upvote_count":"1","poster":"metkillas","timestamp":"1705512060.0","content":"Answer is now B as you can alter data in the materialized view when ingesting from stream. \"You can now connect to and access the data from the stream using SQL and simplify your data pipelines by creating materialized views directly on top of the stream. The materialized views can also include SQL transforms as part of your ELT (extract, load and transform) pipeline.\"","comment_id":"1125168"},{"poster":"Krunal39","timestamp":"1704808740.0","comment_id":"1117539","content":"Can Kinesis Data Streams directly write to S3?","upvote_count":"1"},{"content":"D is a best. KFH provides near-real time data, so it's latency is little higher than that of KDS. Athena is the best data visualizer for complex and raw data. Since the data are in s3, it already appears in athena. Redshift is a data warehouse and can't reciev raw data.","timestamp":"1700052000.0","comment_id":"1071438","upvote_count":"1","poster":"GCPereira"},{"upvote_count":"3","timestamp":"1699748100.0","comment_id":"1068232","content":"Selected Answer: C\ncan't use kds to deliver to s3 so must use kdf. use athena or redshift","poster":"solvewithdata"},{"comment_id":"1052765","poster":"[Removed]","timestamp":"1698146880.0","upvote_count":"1","content":"KDS cant connect datahouse directly.\nThis is reason why We have to use KDF."},{"content":"Selected Answer: C\nThe key is \"ntegrate complex, analytic queries running with minimal latency.\"","comment_id":"1043492","upvote_count":"2","poster":"gofavad926","timestamp":"1697290800.0"},{"timestamp":"1695016500.0","content":"Definitely C. While you can stream data from KDS into Redshift, you cannot modify the materialised view using SQL, you can only run selects on a materialised view. S3 on the other hand doesn't support data modification using Athena.","comment_id":"1010311","poster":"OliverF","upvote_count":"1"},{"upvote_count":"5","content":"Selected Answer: B\nC is a good answer, but it is slower compared to data stream (B). Kinesis data firehose has to buffer stream data (thus near-realtime only), and then copy to s3 bucket as a staging area, then issue COPY command to Insert into Redshift table. Whereas Kinesis data stream just transfer data directly into the data warehouse via materialized view(s). \nSecond, the question requires the stream be transform-able via SQL, Data firehose do support transformation of data yes, but only via Lambda blueprints; Data stream transfer into Materialized View, which is written in SQL to convert Json body of the stream data, you can add more SQL here to transform it as you need - so Data Stream satisfies the requirement - albeit with lots of limitations.\n\nLink to back my claim up: https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html","poster":"cd93","comment_id":"994825","timestamp":"1693462140.0"},{"comment_id":"993894","content":"corect answer is D is wrong ?","upvote_count":"2","poster":"nishapagare97","timestamp":"1693387380.0"},{"timestamp":"1690836240.0","content":"Selected Answer: C\nit's a C","upvote_count":"1","poster":"NikkyDicky","comment_id":"968494"},{"comments":[{"timestamp":"1689087120.0","comment_id":"949054","upvote_count":"1","content":"\"You don't have to send data to an Amazon Kinesis Data Firehose delivery stream, because with streaming ingestion, data can be sent directly from Kinesis Data Streams to a materialized view in an Amazon Redshift database.\" https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\n\nyes, I just think what you say. The problem in this question is de modifying by SQL, and that is not possible with KDS.","poster":"ccpmad"}],"poster":"papercome","comment_id":"918873","timestamp":"1686281580.0","upvote_count":"2","content":"Selected Answer: C\nThere is a new feature of Redshift, supporting read KDS directly into a materialized view, which support lowest latency and make B quite promising. But it might not meet the modify by SQL requirements. So, this make C be the best choice."},{"content":"C: I passed the test","poster":"pk349","upvote_count":"1","comment_id":"886248","timestamp":"1682945760.0"},{"upvote_count":"1","content":"Complex query =Redshift\nKDF has direct integration with Redshift \nSo C is the Answer","comment_id":"876259","timestamp":"1682062440.0","poster":"anjuvinayan"},{"upvote_count":"1","comments":[{"content":"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\n\nNow, yes, it cant talk KDS to Redshift. B could be possible, but sql modify is required so C is correct here.","comment_id":"949055","poster":"ccpmad","upvote_count":"1","timestamp":"1689087180.0"}],"content":"Definitely C. D is wrong because KDS can't talk directly to S3. KDS can only talk either to Kinesis Analytics or Firehose, Lambda or things like SDK or Kinesis Client Library. KDF can talk directly to Redshift.","comment_id":"856688","poster":"Aina","timestamp":"1680246420.0"},{"upvote_count":"4","content":"B. You can now stream directly to Redshift from KDS.https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html","comments":[{"content":"Yes B. KDS can stream directly to Redshift now.","upvote_count":"1","comment_id":"864702","poster":"Pete987","timestamp":"1680959820.0"},{"timestamp":"1689087240.0","content":"yes, but can not modify the data with SQL before.","poster":"ccpmad","comments":[{"upvote_count":"1","comment_id":"1125167","content":"You can perform transformations in the materialized view\n\" You can now connect to and access the data from the stream using SQL and simplify your data pipelines by creating materialized views directly on top of the stream. The materialized views can also include SQL transforms as part of your ELT (extract, load and transform) pipeline.\"\n\nsource: https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/","timestamp":"1705512000.0","poster":"metkillas"}],"comment_id":"949058","upvote_count":"1"}],"comment_id":"842976","poster":"thirstylion","timestamp":"1679158980.0"},{"timestamp":"1678743120.0","upvote_count":"1","comment_id":"838335","content":"Selected Answer: C\n1 - data has to be modified and stream direct to store, S3 doesn't allow modification and kinesis stream doesnt allow Redshift target","poster":"Debi_mishra"},{"content":"C. Use Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Use Amazon Redshift as a data source for Amazon QuickSight to create a business intelligence dashboard.\n\nAmazon Redshift is a data warehouse that provides fast query performance and can handle complex analytical queries. Amazon Kinesis Data Firehose can be used to stream data from different sources, including stock exchanges, into Amazon Redshift with low latency. Redshift also supports SQL, which allows for modifying data using SQL. Finally, Amazon QuickSight can be used to create a business intelligence dashboard that displays the top contributors to anomalies in stock prices.","timestamp":"1678534140.0","poster":"AwsNewPeople","upvote_count":"1","comment_id":"835892"},{"content":"Team - I have cleared the exam .. Yes word to word - few questions came from this question set. Rest everything came with similar scenario - so the more you understand with each question scenario and finding out the best answer for yourself will help you to scope. Thanks and wish you all the best.","upvote_count":"2","comment_id":"821738","timestamp":"1677350460.0","poster":"Arjun777"},{"timestamp":"1675960620.0","comments":[{"poster":"ccpmad","content":"https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\nYes it can.\nbut not modify the data with sql","timestamp":"1689087300.0","upvote_count":"2","comment_id":"949060"}],"poster":"kk1211","comment_id":"803468","content":"Selected Answer: C\nKDS can't stream to redshift","upvote_count":"1"},{"poster":"tpompeu","content":"Selected Answer: C\nSQL modification just in Redshift","upvote_count":"1","comment_id":"796457","timestamp":"1675373280.0"},{"comment_id":"785678","poster":"NatarajNats","content":"Selected Answer: C\nRedshift complex analytic queries","upvote_count":"1","timestamp":"1674496380.0"},{"upvote_count":"1","comment_id":"772638","poster":"gopi_data_guy","content":"Selected Answer: C\nC without doubt","timestamp":"1673448660.0"},{"comment_id":"760054","content":"Do we need to pay money to get acess for these questions? Can anyone respond please.","poster":"saikr","upvote_count":"1","timestamp":"1672242840.0","comments":[{"upvote_count":"1","poster":"professionalseeker","timestamp":"1673095020.0","comment_id":"768549","content":"To get the access for later half questions, yes"}]},{"poster":"MaxwellBlackmore","upvote_count":"2","timestamp":"1668263940.0","content":"Selected Answer: C\nIt mentions the company \"requires\" that data be streamed \"directly\" into the data store so firehose should be the option. Complex queries means Redshift. So C should be the answer.","comment_id":"716750"},{"poster":"cloudlearnerhere","content":"Correct answer is C as Kinesis Data Firehose can help stream the data directly into Redshift. Redshift would allow data updates using SQL and also help execute complex, analytic queries running with minimal latency. Redshift integrated with QuickSight to provide visualizations.\n\nOption A is wrong as Athena would not provide ability to modify or run complex queries.\n\nOption B & D are wrong as Kinesis Data Streams do not stream data directly to S3 or Redshift.","comment_id":"710508","comments":[],"timestamp":"1667480700.0","upvote_count":"3"},{"poster":"Rejju","comment_id":"705199","timestamp":"1666842840.0","upvote_count":"1","content":"I am with C, still wondering why is the correct answer being mentioned as D?"},{"timestamp":"1666836780.0","poster":"nickwong64","upvote_count":"1","content":"Answer: C \nAs KDF loads into redshift, KDS cannot.\nRedShift is for running fast, complex query.","comment_id":"705150"},{"content":"Selected Answer: C\nC is correct!","timestamp":"1665418680.0","poster":"PHTR","comment_id":"691345","upvote_count":"1"},{"poster":"JHJHJHJHJ","timestamp":"1664200020.0","comment_id":"679806","content":"------------------------------*************************************-------------------\nComplex Analytical queries - Athena out (A, D) \nKinesis data stream cannot write to Redshift directly (B out) \nAnswer: C \n-----------------------------------------------------------------------------------------------","upvote_count":"3"},{"poster":"pass3in3mon","upvote_count":"1","comment_id":"674217","timestamp":"1663681560.0","content":"Selected Answer: C\ncomplex query ==redshift, KDF into s3 then copy into redshift"},{"content":"Selected Answer: C\nSelected Answer: C","timestamp":"1658804760.0","comment_id":"637083","poster":"rocky48","upvote_count":"2"},{"content":"Selected Answer: C\nAnswer should be C","timestamp":"1653204900.0","poster":"Bik000","upvote_count":"2","comment_id":"605239"},{"poster":"jrheen","upvote_count":"1","comment_id":"595324","content":"Answer-C","timestamp":"1651357080.0"},{"timestamp":"1651228200.0","poster":"finnliang","comment_id":"594393","comments":[{"comments":[{"poster":"ccpmad","comment_id":"949064","content":"now yes,\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html","timestamp":"1689087360.0","upvote_count":"1"}],"timestamp":"1651845240.0","content":"Kinesis Data Streams cannot deliver data into redshift, it needs kinesis firehose as a delivery stream.","upvote_count":"5","comment_id":"597749","poster":"MWL"}],"upvote_count":"1","content":"why B is not right?"},{"upvote_count":"1","comment_id":"594304","timestamp":"1651218480.0","poster":"Teraxs","content":"Selected Answer: C\nRedshift for performance, Firehose can stream to redshift (via S3 strictly speaking) but Datastream cannot (at least not without Firehose)"},{"comment_id":"590608","upvote_count":"1","comments":[{"timestamp":"1650716760.0","poster":"sanjee000","comment_id":"590610","comments":[{"timestamp":"1650870360.0","poster":"sanjee000","comment_id":"591423","upvote_count":"1","content":"C is looks correct"}],"upvote_count":"1","content":"Using Firehose can send data to Redshift but through S3 using copy command."}],"poster":"sanjee000","content":"B - requirement is real time stream data to be collected and stored, KDS can store data into Redshift and redshift can perform Complex queries, Hence B is correct","timestamp":"1650716700.0"},{"content":"C - Based on Db activities in the question","poster":"[Removed]","comment_id":"588802","upvote_count":"7","timestamp":"1650465240.0"},{"timestamp":"1650462360.0","upvote_count":"2","poster":"astalavista1","content":"C - As it needs to do some complex SQL queries and KDF can also stream to Redshift.","comment_id":"588773"},{"poster":"AWSRanger","content":"Answer is \"C\". Redshift is better for complex analytical queries.","comment_id":"588653","timestamp":"1650457680.0","upvote_count":"2"}],"question_images":[],"topic":"1","answer_ET":"C","answers_community":["C (83%)","B (17%)"],"exam_id":20,"question_id":1,"choices":{"A":"Use Amazon Kinesis Data Firehose to stream data to Amazon S3. Use Amazon Athena as a data source for Amazon QuickSight to create a business intelligence dashboard.","C":"Use Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Use Amazon Redshift as a data source for Amazon QuickSight to create a business intelligence dashboard.","B":"Use Amazon Kinesis Data Streams to stream data to Amazon Redshift. Use Amazon Redshift as a data source for Amazon QuickSight to create a business intelligence dashboard.","D":"Use Amazon Kinesis Data Streams to stream data to Amazon S3. Use Amazon Athena as a data source for Amazon QuickSight to create a business intelligence dashboard."},"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/73890-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"C"},{"id":"lb0djzwCkmnBelOlVP5j","discussion":[{"upvote_count":"37","content":"Answer: D\nData analysts analyze the data using Apache Spark SQL on Amazon EMR for the data stored on S3 in JSON format. \nInput JSON file landing in S3 triggers a Lambda which invokes Glue Crawler.","timestamp":"1632093720.0","poster":"singh100","comment_id":"159176","comments":[{"poster":"chinmayj213","upvote_count":"2","content":"D is correct out of all these answer , but at the same time running a crawler on bucket for just landing of one object. Is it a good idea ?","timestamp":"1695117420.0","comment_id":"1011171"}]},{"content":"A is on demand (triggered by hand). B minimum time required is 1 hr. C is 1 minute based on the cron schedule syntax. For D, it could reach to sub-minute level since it watches S3 new data events.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html#RateExpressions","poster":"zanhsieh","timestamp":"1632227700.0","upvote_count":"9","comment_id":"159206"},{"content":"Selected Answer: D\nOption A is not directly related to the issue of schema updates and would not address the staleness of data in the AWS Glue Data Catalog. Option B increases the frequency of crawls but still may not provide real-time updates. Option C is not practical or cost-effective due to the excessive number of crawler runs it would trigger, and the AWS Glue crawler cannot be scheduled to run every minute. Option D provides a dynamic, event-driven solution that ensures data analysts have access to the most current data available.","comment_id":"1155009","timestamp":"1708462320.0","poster":"NarenKA","upvote_count":"1"},{"content":"Selected Answer: D\nits a D","timestamp":"1690840800.0","comment_id":"968521","upvote_count":"1","poster":"NikkyDicky"},{"upvote_count":"1","timestamp":"1683891060.0","comments":[{"content":"yes metastore , but every filename in s3 bucket need to registered in metastore to pick the latest data.","poster":"chinmayj213","upvote_count":"2","timestamp":"1695117300.0","comment_id":"1011168"}],"poster":"Bdtri","comment_id":"895868","content":"Why triggering glue crawler can give us the latest data? Isn’t it only updating metastore?"},{"comment_id":"886267","poster":"pk349","comments":[{"poster":"kondi2309","comment_id":"1148902","content":"why D?","timestamp":"1707802860.0","upvote_count":"1"}],"content":"D: I passed the test","upvote_count":"1","timestamp":"1682946420.0"},{"upvote_count":"3","poster":"lk23","comment_id":"820688","content":"very curious to know every answer so far what it says it incorrect as discussion revleas something else, why?","timestamp":"1677254280.0"},{"poster":"cloudlearnerhere","timestamp":"1667564280.0","upvote_count":"3","comment_id":"711116","content":"Selected Answer: D\nCorrect answer is D as it is event-driven and would load the data as soon as the object-created event is triggered.\n\nOption A is wrong as this is still manual and on-demand.\n\nOption B is wrong as the refresh interval is still 1 hr.\n\nOption C is wrong as the minimum precision for the schedule is 5 mins."},{"comment_id":"660635","poster":"Abep","timestamp":"1662422820.0","upvote_count":"1","content":"Selected Answer: D\nAnswer: D"},{"upvote_count":"1","content":"Selected Answer: D\nAnswer: D","poster":"rocky48","timestamp":"1658804460.0","comment_id":"637080"},{"upvote_count":"1","timestamp":"1653129900.0","comment_id":"604814","poster":"Bik000","content":"Selected Answer: D\nAnswer is D"},{"comment_id":"595321","timestamp":"1651356720.0","poster":"jrheen","upvote_count":"1","content":"Answer-D"},{"comment_id":"571571","content":"D is correct","poster":"ShilaP","upvote_count":"1","timestamp":"1647776400.0"},{"comment_id":"482645","upvote_count":"1","content":"The answer is D.","poster":"aws2019","timestamp":"1637421060.0"},{"timestamp":"1636218120.0","poster":"iconara","comment_id":"430700","upvote_count":"1","content":"D seems correct, but could potentially be an expensive solution."},{"poster":"Huy","timestamp":"1636174560.0","comment_id":"387639","content":"Although D is correct answer, the answer should mention SQS. The crawler will not run fast enough to catch up with objects created.","upvote_count":"2"},{"comment_id":"383507","upvote_count":"2","timestamp":"1636105800.0","content":"This is a textbook question. A = wrong, won’t work because schema will update every 8 hours. B = wrong, not most up-to-date. C = wrong, minimum schedule is 5 minutes.\n\nhttps://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/","poster":"Shraddha"},{"content":"D is correct.","upvote_count":"1","timestamp":"1635616560.0","comment_id":"382961","poster":"gunjan4392"},{"content":"The answer is D.","comment_id":"359125","upvote_count":"1","poster":"leliodesouza","timestamp":"1635401100.0"},{"upvote_count":"3","timestamp":"1635378300.0","comment_id":"296655","poster":"nirmalmarathon","content":"Is there a typo in Option A? It is very misleading as it claims \"existing Redshift\" but in the question there is no Redshift?? \"Create an external schema based on the AWS Glue Data Catalog on the existing Amazon Redshift cluster to query new data in Amazon S3 ...\""},{"comment_id":"285119","content":"D. Using S3 event trigger Glue ETL job is a up-to-date data renew solution.\n\nA. Apache Spark SQL query data in S3 through Glue Data Catlog, not Redshift. Additionally, it doesn't provide up-to-date data.\nB. 1 hour is not up-to-date.\nC. 1 minute does not equal to up-to-date. Besides, it will significantly increase the cost.","upvote_count":"2","timestamp":"1635220260.0","poster":"Exia"},{"comment_id":"274245","poster":"lostsoul07","timestamp":"1635198840.0","content":"D is the right answer","upvote_count":"1"},{"upvote_count":"1","timestamp":"1635088740.0","content":"Correct answer = D","comment_id":"255716","poster":"mendelthegreat"},{"upvote_count":"4","poster":"Sai12","comment_id":"245776","timestamp":"1634860140.0","content":"What is the question actually telling us?\n\n1) JSON format that is sent without a predefined schedule - Data can be received at any time\n2) through an Amazon Kinesis Data Firehose delivery stream - Firehose has a minumum latency of 60 seconds or 1 minute. It could take longer than 1 minute to receive file in S3\n3) An AWS Glue crawler is scheduled to run every 8 hour - To be successful the file must be completely uploaded to S3 within the 8 hours and at the time the crawler process begins\n4) occasionally, the data they receive is stale - For the most part the process is successful. The problem is not in the 8 hour schedule. The problem is that the crawler maybe running at the same time as the latest file is still uploading\n\nBased on the above D is the answer. It will run the crawler after the file is successfully put."},{"content":"D is correct","comment_id":"216830","upvote_count":"1","timestamp":"1634531700.0","poster":"BillyC"},{"content":"As per document min precision allowed is 5 minutes; so I wonder how can D is correct if notification caused trigger every minute(https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html). Seems B is fine as it fired every 1 hour rather 8 hour","timestamp":"1633978260.0","comment_id":"213887","poster":"PareshRane","upvote_count":"2"},{"poster":"syu31svc","comment_id":"191273","content":"Answer is D as per link:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\n\"you can use a wildcard (for example, s3:ObjectCreated:*) to request notification when an object is created regardless of the API used\"\n\"AWS Lambda can run custom code in response to Amazon S3 bucket events. You upload your custom code to AWS Lambda and create what is called a Lambda function. When Amazon S3 detects an event of a specific type (for example, an object created event), it can publish the event to AWS Lambda and invoke your function in Lambda. In response, AWS Lambda runs your function.\"","timestamp":"1633652280.0","upvote_count":"3"},{"timestamp":"1633609500.0","comment_id":"178555","content":"I agree with D for this reason https://stackoverflow.com/questions/48828194/event-based-trigger-of-aws-glue-crawler-after-a-file-is-uploaded-into-a-s3-bucke","upvote_count":"1","poster":"GeeBeeEl"},{"timestamp":"1633584000.0","comment_id":"177457","content":"Option D. Lambda based trigger will enable the data to be refreshed as soon as the S3 event occurs, so analysts will never see stale data","poster":"Karan_Sharma","upvote_count":"1"},{"content":"I will go with option D and refresh the Glue catalog based on S3 trigger.","upvote_count":"1","comment_id":"175255","poster":"Paitan","timestamp":"1633523880.0"},{"timestamp":"1632906960.0","comment_id":"162878","upvote_count":"1","poster":"Nicki1013","content":"My answer is D"},{"comment_id":"159924","upvote_count":"1","content":"For me is also D!","timestamp":"1632507960.0","poster":"carol1522"},{"comment_id":"159368","content":"D is correct","upvote_count":"1","timestamp":"1632253800.0","poster":"abhineet"}],"timestamp":"2020-08-16 13:17:00","choices":{"C":"Using the AWS CLI, modify the execution schedule of the AWS Glue crawler from 8 hours to 1 minute.","A":"Create an external schema based on the AWS Glue Data Catalog on the existing Amazon Redshift cluster to query new data in Amazon S3 with Amazon Redshift Spectrum.","B":"Use Amazon CloudWatch Events with the rate (1 hour) expression to execute the AWS Glue crawler every hour.","D":"Run the AWS Glue crawler from an AWS Lambda function triggered by an S3:ObjectCreated:* event notification on the S3 bucket."},"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/28715-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_text":"An insurance company has raw data in JSON format that is sent without a predefined schedule through an Amazon Kinesis Data Firehose delivery stream to an\nAmazon S3 bucket. An AWS Glue crawler is scheduled to run every 8 hours to update the schema in the data catalog of the tables stored in the S3 bucket. Data analysts analyze the data using Apache Spark SQL on Amazon EMR set up with AWS Glue Data Catalog as the metastore. Data analysts say that, occasionally, the data they receive is stale. A data engineer needs to provide access to the most up-to-date data.\nWhich solution meets these requirements?","answer":"D","answers_community":["D (100%)"],"question_images":[],"isMC":true,"question_id":2,"topic":"1","answer_images":[],"answer_ET":"D","exam_id":20,"unix_timestamp":1597576620},{"id":"VDGyaqvDDoVTmr1WtrMq","isMC":true,"answer":"C","choices":{"A":"Create a different Amazon EC2 security group for each application. Configure each security group to have access to a specific topic in the Amazon MSK cluster. Attach the security group to each application based on the topic that the applications should read and write to.","C":"Use Kafka ACLs and configure read and write permissions for each topic. Use the distinguished name of the clients' TLS certificates as the principal of the ACL.","B":"Install Kafka Connect on each application instance and configure each Kafka Connect instance to write to a specific topic only.","D":"Create a different Amazon EC2 security group for each application. Create an Amazon MSK cluster and Kafka topic for each application. Configure each security group to have access to the specific cluster."},"question_id":3,"answer_images":[],"unix_timestamp":1620061500,"question_images":[],"exam_id":20,"question_text":"A central government organization is collecting events from various internal applications using Amazon Managed Streaming for Apache Kafka (Amazon MSK).\nThe organization has configured a separate Kafka topic for each application to separate the data. For security reasons, the Kafka cluster has been configured to only allow TLS encrypted data and it encrypts the data at rest.\nA recent application update showed that one of the applications was configured incorrectly, resulting in writing data to a Kafka topic that belongs to another application. This resulted in multiple errors in the analytics pipeline as data from different applications appeared on the same topic. After this incident, the organization wants to prevent applications from writing to a topic different than the one they should write to.\nWhich solution meets these requirements with the least amount of effort?","discussion":[{"upvote_count":"31","poster":"AjithkumarSL","comment_id":"350408","content":"Looks like C is the right option.\nhttps://docs.aws.amazon.com/msk/latest/developerguide/msk-acls.html","timestamp":"1635728700.0"},{"poster":"VikG12","timestamp":"1633425360.0","upvote_count":"8","comment_id":"348844","content":"C should be the answer."},{"content":"Option B:\nWhile Kafka Connect itself doesn't directly \"bind\" to a topic, you can configure connectors to interact with specific MSK topics in various ways:\n\n1. Connector Configuration:\n\nEach connector type has specific configuration options that define the source or sink topics it interacts with.\nFor source connectors, you'll typically specify the topic name where the connector will read data from. This could be done through properties like topics or source.topics depending on the connector type.\nFor sink connectors, you'll typically specify the topic name where the connector will write data to. This could be done through properties like topic or sink.topics based on the connector type.","poster":"chinmayj213","upvote_count":"1","timestamp":"1709132460.0","comment_id":"1161721"},{"content":"C: I passed the test","comment_id":"886518","upvote_count":"1","timestamp":"1682959620.0","poster":"pk349"},{"poster":"cloudlearnerhere","content":"C is the correct as per doc\nApache Kafka has a pluggable authorizer and ships with an out-of-box authorizer implementation that uses Apache ZooKeeper to store all ACLs. Amazon MSK enables this authorizer in the server.properties file on the brokers. For Apache Kafka version 2.4.1, the authorizer is AclAuthorizer. For earlier versions of Apache Kafka, it is SimpleAclAuthorizer.\n\nOption A is wrong as the Security group cannot be used to control which instance can access which topic.\n\nOption B is wrong as it does not restrict access and the applications can still push the data to other topics.\n\nOption D is wrong as it does not meet the least amount of effort requirement.","comment_id":"705534","upvote_count":"5","timestamp":"1666873320.0"},{"timestamp":"1658294760.0","upvote_count":"1","content":"Selected Answer: C\nC is correct one","comment_id":"633856","poster":"rocky48"},{"timestamp":"1650468720.0","comment_id":"588821","upvote_count":"1","content":"Selected Answer: C\nI vote C","poster":"chp2022"},{"upvote_count":"3","timestamp":"1646120940.0","content":"c is the answer","poster":"youonebe","comment_id":"558658"},{"upvote_count":"2","comment_id":"533148","poster":"jamesbond1983","timestamp":"1643222400.0","content":"B is correct"},{"timestamp":"1639613880.0","poster":"Rahulscrazy7","comment_id":"502539","content":"C is correct one","upvote_count":"2"},{"upvote_count":"1","timestamp":"1637180760.0","comment_id":"480253","poster":"aws2019","content":"C should be the Ans.."}],"url":"https://www.examtopics.com/discussions/amazon/view/51725-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"B","timestamp":"2021-05-03 19:05:00","topic":"1","answer_description":"","answers_community":["C (100%)"]},{"id":"gcVC7ZZ3fqJmeFVojCBa","isMC":true,"answer":"C","choices":{"C":"Use Amazon Managed Streaming for Apache Kafka. Configure a topic for the raw data. Use a Kafka producer to write data to the topic. Create an application on Amazon EC2 that reads data from the topic by using the Apache Kafka consumer API, cleanses the data, and writes to Amazon S3.","D":"Use Amazon Simple Queue Service (Amazon SQS). Configure an AWS Lambda function to read events from the SQS queue and upload the events to Amazon S3.","A":"Use Amazon Kinesis Data Streams. Configure a stream for the raw data. Use a Kinesis Agent to write data to the stream. Create an Amazon Kinesis Data Analytics application that reads data from the raw stream, cleanses it, and stores the output to Amazon S3.","B":"Use Amazon Kinesis Data Firehose. Configure a Firehose delivery stream with a preprocessing AWS Lambda function for data cleansing. Use a Kinesis Agent to write data to the delivery stream. Configure Kinesis Data Firehose to deliver the data to Amazon S3."},"question_id":4,"answer_images":[],"unix_timestamp":1620062220,"question_images":[],"question_text":"A company wants to collect and process events data from different departments in near-real time. Before storing the data in Amazon S3, the company needs to clean the data by standardizing the format of the address and timestamp columns. The data varies in size based on the overall load at each particular point in time. A single data record can be 100 KB-10 MB.\nHow should a data analytics specialist design the solution for data ingestion?","exam_id":20,"discussion":[{"poster":"ariane_tateishi","comment_id":"366424","content":"C. Should be the right answer, because of the main requirement \"A single data record can be 100 KB-10 MB.\"\n- Kinesis firehose - The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB. \n- Kinesis stream - The maximum size of the data payload of a record before base64-encoding is up to 1 MB.\n - SQS - https://aws.amazon.com/pt/about-aws/whats-new/2015/10/now-send-payloads-up-to-2gb-with-amazon-sqs/","timestamp":"1634683740.0","upvote_count":"26","comments":[{"poster":"lakediver","timestamp":"1640057760.0","comment_id":"505804","upvote_count":"5","comments":[{"comment_id":"547104","upvote_count":"3","timestamp":"1644841140.0","content":"The Kafka record size depends on message size. It is up to 100MB. I have experience setting the configuration for sending the encoded image through kafka.","poster":"lucaseo90"},{"comment_id":"778697","poster":"nadavw","timestamp":"1673943300.0","upvote_count":"5","content":"The limit for MSK mentioned here (8MB) is for MSK serverless only"}],"content":"I think answer should be D\nYou rightly mentioned above about Firehose and Stream limits. However MSK has maximum record size of 8MB - https://docs.aws.amazon.com/msk/latest/developerguide/limits.html\nSQS now allow payloads upto 2GB -\nUsing the Extended Client Library, message payloads larger than 256KB are stored in an Amazon Simple Storage Service (S3) bucket, using SQS to send and receive a reference to the payload location."},{"poster":"khchan123","upvote_count":"2","comment_id":"612174","content":"It should be D.\nMaximum message size for Amazon MSK is 8MB.\nhttps://docs.aws.amazon.com/msk/latest/developerguide/limits.html","timestamp":"1654492740.0"}]},{"content":"It should be C. 1 MB is the soft limit for the Kafka which can be increased up to 10 MB. That's the main difference b/w KDA and Kafka","upvote_count":"8","timestamp":"1634708460.0","comment_id":"367901","poster":"Monika14Sharma"},{"comments":[{"comment_id":"972828","upvote_count":"1","timestamp":"1691226720.0","poster":"MLCL","content":"But answer C really needs a lot of setup, if the question was about 1 MB records it would be Firehose immediately."}],"comment_id":"972827","content":"Selected Answer: C\nMSK is the only one that can handle single put of 10MB.\nFirehose can handle up to 128 MB Buffered data, but it just catches single records of up to 3MB in that window and sends them in batch, you can't put a record that is higher than 3MB","poster":"MLCL","timestamp":"1691226600.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1682959740.0","comment_id":"886520","poster":"pk349","content":"C: I passed the test"},{"content":"Correct answer is C as only Amazon Managed Streaming for Apache Kafka seems to provide a maximum size of a record that can be configured up to 10MB.\n\nAmazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data. Amazon MSK provides the control-plane operations, such as those for creating, updating, and deleting clusters. It lets you use Apache Kafka data-plane operations, such as those for producing and consuming data. It runs open-source versions of Apache Kafka. This means existing applications, tooling, and plugins from partners and the Apache Kafka community are supported without requiring changes to application code.\n\nOptions A & B are wrong as the maximum size of a record sent to both Kinesis Data Stream and Kinesis Data Firehose base64-encoding, is 1,000 KiB.\n\nOption D is wrong SQS data size limit is 256KB.","upvote_count":"5","timestamp":"1666891680.0","comment_id":"705768","poster":"cloudlearnerhere"},{"timestamp":"1666891560.0","upvote_count":"2","content":"Correct answer is C as only Amazon Managed Streaming for Apache Kafka seems to provide a maximum size of a record that can be configured up to 10MB. \nCorrect answer is C as only Amazon Managed Streaming for Apache Kafka seems to provide a maximum size of a record that can be configured up to 10MB. \n\nCorrect answer is C as only Amazon Managed Streaming for Apache Kafka seems to provide a maximum size of a record that can be configured up to 10MB. \n\nOption D is wrong SQS data size limit is 256KB.","comment_id":"705767","poster":"cloudlearnerhere"},{"comment_id":"702826","upvote_count":"1","timestamp":"1666599840.0","poster":"MultiCloudIronMan","content":"Selected Answer: D\nKalka can handle the limit, Firehose is 8mb max","comments":[{"timestamp":"1666599840.0","upvote_count":"1","content":"Sorry i meant C not D","poster":"MultiCloudIronMan","comment_id":"702828"}]},{"poster":"rocky48","comment_id":"634896","content":"Selected Answer: C\nSelected Answer: C","upvote_count":"3","timestamp":"1658445540.0"},{"timestamp":"1656421800.0","content":"Selected Answer: D\nKDS, KDF and MSK all have record/message limit lower than 10MB. https://aws.amazon.com/kinesis/data-streams/faqs/, https://aws.amazon.com/kinesis/data-streams/faqs/, https://docs.aws.amazon.com/msk/latest/developerguide/limits.html. SQS is recommended for \"Using the ability of Amazon SQS to scale transparently. For example, you buffer requests and the load changes as a result of occasional load spikes or the natural growth of your business. Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.\": https://aws.amazon.com/kinesis/data-streams/faqs/","comment_id":"623981","poster":"GarfieldBin","upvote_count":"1"},{"comment_id":"608045","timestamp":"1653649680.0","upvote_count":"1","content":"Although looks unnecessary to use EC2 to clean up data, C is still the right answer. Both Kinesis data streams and firehose have upper limit of 1 mb record size. Ideally, instead of using ec2, I would prefer to use something like Kafka Streams to perform required transformation.","poster":"certificationJunkie"},{"poster":"[Removed]","content":"C\nThe maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB.","comment_id":"600985","upvote_count":"1","timestamp":"1652423400.0"},{"comment_id":"595286","upvote_count":"1","content":"Answer - C","poster":"jrheen","timestamp":"1651352040.0"},{"comment_id":"543624","timestamp":"1644395280.0","content":"Selected Answer: C\nC. For AWS Lambda processing, you can set a buffering hint between 1 MiB and 3 MiB using the BufferSizeInMBs processor parameter.","upvote_count":"2","poster":"gaindas"},{"upvote_count":"1","comment_id":"506570","poster":"npt","timestamp":"1640136240.0","content":"C - cleanses the data, and writes to Amazon S3.\nD does not mention cleaning the data"},{"comment_id":"489519","upvote_count":"1","timestamp":"1638147180.0","poster":"arun004","content":"why not B ? Buffer size can be set up to 128 MB"},{"upvote_count":"1","timestamp":"1635995820.0","content":"Between B and C, I chose C because\nThe maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB.","comment_id":"423848","poster":"Dr_Kiko"},{"timestamp":"1635331140.0","content":"Why not B? \nQuestion is saying about near real time. and Firehose quota can be increased using Amazon Kinesis data firehose limit form.","poster":"tukai","upvote_count":"2","comments":[{"comment_id":"721464","timestamp":"1668793620.0","content":"Kinesis firehose - The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB.","upvote_count":"1","poster":"allanm"}],"comment_id":"370920"},{"comment_id":"362786","poster":"soni12390","upvote_count":"3","timestamp":"1634402760.0","content":"D is not talking about using the SQS extended client library so will choose C"},{"content":"https://aws.amazon.com/about-aws/whats-new/2015/10/now-send-payloads-up-to-2gb-with-amazon-sqs/#:~:text=Amazon%20Simple%20Queue%20Service%20(SQS,payloads%20were%20limited%20to%20256KB.","timestamp":"1634217300.0","comment_id":"362782","poster":"soni12390","upvote_count":"1"},{"upvote_count":"1","timestamp":"1634105340.0","comment_id":"362772","content":"Kafka also has record size limit to 1 MB for optimum performance. the max records size can be 10MB so looks like the answer should be D.","poster":"soni12390"},{"content":"Firehose has a limit of 1000KiB and in the question the record range is way beyond.But what for 'Near real time ' keyword? Kafka is real time streaming right ?","poster":"Heer","timestamp":"1633710780.0","upvote_count":"1","comment_id":"359644"},{"content":"C, Due to the size limits in Firehose\nhttps://docs.aws.amazon.com/firehose/latest/dev/limits.html","poster":"curioustester","timestamp":"1633671000.0","comment_id":"355904","upvote_count":"2"},{"upvote_count":"2","content":"B? firehorse integrates with lambda for short ETL.","timestamp":"1632257700.0","poster":"bryankeb","comment_id":"349077"},{"content":"Should be 'B'.","poster":"VikG12","upvote_count":"2","timestamp":"1632114180.0","comment_id":"348852"}],"url":"https://www.examtopics.com/discussions/amazon/view/51728-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2021-05-03 19:17:00","answer_ET":"C","topic":"1","answers_community":["C (75%)","D (25%)"],"answer_description":""},{"id":"2KOeGG93pTUaV4dxd2pc","choices":{"A":"Change the worker type from Standard to G.2X.","B":"Modify the AWS Glue ETL code to use the 'groupFiles': 'inPartition' feature.","D":"Modify maximum capacity to increase the total maximum data processing units (DPUs) used.","C":"Increase the fetch size setting by using AWS Glue dynamics frame."},"answer_ET":"B","unix_timestamp":1620062100,"question_images":["https://www.examtopics.com/assets/media/exam-media/04144/0006000001.png"],"isMC":true,"topic":"1","question_id":5,"answer_images":[],"answer_description":"","answer":"B","question_text":"An operations team notices that a few AWS Glue jobs for a given ETL application are failing. The AWS Glue jobs read a large number of small JSON files from an\nAmazon S3 bucket and write the data to a different S3 bucket in Apache Parquet format with no major transformations. Upon initial investigation, a data engineer notices the following error message in the History tab on the AWS Glue console: `Command Failed with Exit Code 1.`\nUpon further investigation, the data engineer notices that the driver memory profile of the failed jobs crosses the safe threshold of 50% usage quickly and reaches\n90`\"95% soon after. The average memory usage across all executors continues to be less than 4%.\nThe data engineer also notices the following error while examining the related Amazon CloudWatch Logs.\n//IMG//\n\nWhat should the data engineer do to solve the failure in the MOST cost-effective way?","exam_id":20,"timestamp":"2021-05-03 19:15:00","answers_community":["B (82%)","D (18%)"],"url":"https://www.examtopics.com/discussions/amazon/view/51727-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"poster":"jyrajan69","timestamp":"1632854580.0","comments":[{"content":"B It is\nhttps://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html","comment_id":"493550","poster":"lakeswimmer","upvote_count":"7","timestamp":"1638601380.0"}],"comment_id":"349101","upvote_count":"33","content":"Bssed on the link, I will go for B\n\nhttps://awsfeed.com/whats-new/big-data/optimize-memory-management-in-aws-glue"},{"timestamp":"1666891740.0","poster":"cloudlearnerhere","upvote_count":"16","content":"Correct answer is B as the key issue is the driver memory problem caused because of the glue job processing multiple small files. Grouping of the files helps group the files and hence Spark driver stores significantly less state in memory.\n\nIn this scenario, a Spark job is reading a large number of small files from Amazon Simple Storage Service (Amazon S3). It converts the files to Apache Parquet format and then writes them out to Amazon S3. The Spark driver is running out of memory. The input Amazon S3 data has more than 1 million files in different Amazon S3 partitions.\n\nYou can fix the processing of the multiple files by using the grouping feature in AWS Glue. Grouping is automatically enabled when you use dynamic frames and when the input dataset has a large number of files (more than 50,000). Grouping allows you to coalesce multiple files together into a group, and it allows a task to process the entire group instead of a single file. As a result, the Spark driver stores significantly less state in memory to track fewer tasks.","comment_id":"705769","comments":[{"comments":[{"comment_id":"705771","timestamp":"1666891800.0","content":"The driver runs below the threshold of 50 percent memory usage over the entire duration of the AWS Glue job. The executors stream the data from Amazon S3, process it, and write it out to Amazon S3. As a result, they consume less than 5 percent memory at any point in time.","poster":"cloudlearnerhere","upvote_count":"2"}],"timestamp":"1666891740.0","upvote_count":"3","poster":"cloudlearnerhere","content":"df = glueContext.create_dynamic_frame_from_options\n (\"s3\", {'paths': [\"s3://input_path\"], \"recurse\":True, 'groupFiles': 'inPartition'}, format=\"json\")\ndatasink = glueContext.write_dynamic_frame.from_options\n (frame = df, connection_type = \"s3\", connection_options = {\"path\": output_path}, format = \"parquet\", transformation_ctx = \"datasink\")","comment_id":"705770"}]},{"timestamp":"1691241660.0","poster":"MLCL","comment_id":"973058","upvote_count":"1","content":"Selected Answer: B\nB is the correct answer https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html"},{"upvote_count":"1","comment_id":"886521","content":"B: I passed the test","timestamp":"1682959740.0","poster":"pk349"},{"content":"B, more details you can find here\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-profile-debug-oom-abnormalities.html","poster":"Mang2000","upvote_count":"2","comment_id":"788427","timestamp":"1674712620.0"},{"upvote_count":"4","poster":"[Removed]","content":"B\nhttps://aws.amazon.com/blogs/big-data/optimize-memory-management-in-aws-glue/","timestamp":"1671618300.0","comment_id":"752131"},{"comment_id":"668308","upvote_count":"3","poster":"he11ow0rId","timestamp":"1663093020.0","content":"Selected Answer: B\nD would work, but it's not the most COST EFFECTIVE way. So B"},{"comment_id":"637087","poster":"rocky48","timestamp":"1658805240.0","content":"Selected Answer: B\nSelected Answer: B","upvote_count":"1"},{"content":"Selected Answer: D\nA data processing unit (DPU) is a relative measure of processing power that consists of vCPUs and memory. Change the maximum capacity parameter value and set it to a higher number","comment_id":"617408","timestamp":"1655414880.0","poster":"samsanta2012","upvote_count":"1"},{"comment_id":"613503","upvote_count":"1","content":"Selected Answer: B\nB -- Groupfiles will help here ..","timestamp":"1654720500.0","poster":"CloudTimes"},{"upvote_count":"1","comment_id":"605237","poster":"Bik000","content":"Selected Answer: D\nAnswer should be D","timestamp":"1653204780.0"},{"upvote_count":"1","timestamp":"1653130080.0","comment_id":"604820","content":"Selected Answer: D\nAnswer is D","poster":"Bik000"},{"content":"Selected Answer: B\nNo doubt B.","poster":"MWL","upvote_count":"2","comment_id":"597919","timestamp":"1651881960.0"},{"timestamp":"1651357440.0","content":"Answer - B","upvote_count":"2","comment_id":"595329","poster":"jrheen"},{"timestamp":"1646914200.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-profile-debug-oom-abnormalities.html#monitor-debug-oom-fix","comment_id":"564778","poster":"yusnardo","upvote_count":"3"},{"poster":"RSSRAO","comment_id":"551407","upvote_count":"3","timestamp":"1645313940.0","content":"Selected Answer: B\nB is the correct answer"},{"timestamp":"1637919960.0","comment_id":"487230","upvote_count":"1","poster":"Mobeen_Mehdi","content":"The answers is B according to this https://aws.amazon.com/premiumsupport/knowledge-center/glue-oom-java-heap-space-error/"},{"timestamp":"1637865960.0","content":"believe it is 'B' \nhttps://aws.amazon.com/premiumsupport/knowledge-center/glue-oom-java-heap-space-error/","upvote_count":"1","comment_id":"486869","poster":"awsmani"},{"comment_id":"483323","timestamp":"1637504100.0","poster":"aws2019","content":"'B' it is.","upvote_count":"1"},{"timestamp":"1635158400.0","comment_id":"446324","upvote_count":"1","poster":"ThomasKalva","content":"I feel it is D; \nmight be the answer since the question talk about error in individual file processing which could be larger, groupFIles may not address this yet, you need larger DPU to process the file i think."},{"upvote_count":"1","timestamp":"1635145740.0","comment_id":"423846","poster":"Dr_Kiko","content":"I'd say B"},{"upvote_count":"1","comment_id":"414225","poster":"nirmalmarathon","timestamp":"1634129820.0","content":"The answer is D but I see two people who say the answer is B. How do I know for sure which answer is correct??!!"},{"upvote_count":"11","content":"'B' it is. \nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-profile-debug-oom-abnormalities.html#monitor-debug-oom-fix","timestamp":"1632438960.0","poster":"VikG12","comment_id":"348851"}]}],"exam":{"isImplemented":true,"name":"AWS Certified Data Analytics - Specialty","isBeta":false,"isMCOnly":true,"provider":"Amazon","id":20,"numberOfQuestions":164,"lastUpdated":"11 Apr 2025"},"currentPage":1},"__N_SSP":true}