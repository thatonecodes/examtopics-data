{"pageProps":{"questions":[{"id":"YgWMb2xay0mBN8aFqeBn","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/109291-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"exam_id":31,"question_id":391,"answers_community":["B (100%)"],"timestamp":"2023-05-15 14:40:00","answer":"B","answer_description":"","discussion":[{"timestamp":"1737295500.0","content":"Selected Answer: B\nSage Maker - ML","poster":"vincent2023","comment_id":"1343025","upvote_count":"1"},{"comment_id":"1259258","timestamp":"1722500160.0","poster":"67db0ed","content":"https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.html","upvote_count":"2"},{"comment_id":"1115838","content":"Selected Answer: B\nMachine Learning = Sage Maker so B for least operational overhead\nA and D are not right technologies.\nC is possible but with more overhead of using AMI even if you can get OpenSearch to visualize the data somehow which I don't think is possible without massive overhead","timestamp":"1704632280.0","poster":"awsgeek75","upvote_count":"4"},{"timestamp":"1692979200.0","poster":"Guru4Cloud","content":"Selected Answer: B\nUse Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.","comment_id":"990194","upvote_count":"3"},{"content":"Selected Answer: B\nQuestion keyword \"machine learning\", answer keyword \"Amazon SageMaker\". Choose B. Use Amazon QuickSight for visualization. See \"Gaining insights with machine learning (ML) in Amazon QuickSight\" at https://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html","timestamp":"1689942840.0","poster":"james2033","comment_id":"958470","upvote_count":"3"},{"poster":"VellaDevil","timestamp":"1688665560.0","content":"Selected Answer: B\nSagemaker.","upvote_count":"2","comment_id":"944893"},{"upvote_count":"3","poster":"TariqKipkemei","comment_id":"917788","timestamp":"1686198600.0","content":"Selected Answer: B\nBusiness intelligence, visualiations = AmazonQuicksight\nML = Amazon SageMaker"},{"upvote_count":"2","poster":"antropaws","comment_id":"910266","timestamp":"1685452320.0","content":"Selected Answer: B\nMost likely B."},{"content":"Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy ML models quickly.","upvote_count":"2","comment_id":"901342","poster":"omoakin","timestamp":"1684424160.0"},{"timestamp":"1684408020.0","comment_id":"901102","poster":"cloudenthusiast","upvote_count":"4","content":"Amazon SageMaker is a fully managed service that provides a complete set of tools and capabilities for building, training, and deploying ML models. It simplifies the end-to-end ML workflow and reduces operational overhead by handling infrastructure provisioning, model training, and deployment.\nTo visualize the data and integrate it into business intelligence dashboards, Amazon QuickSight can be used. QuickSight is a cloud-native business intelligence service that allows users to easily create interactive visualizations, reports, and dashboards from various data sources, including the augmented data generated by the ML models."},{"poster":"Efren","timestamp":"1684290900.0","content":"Selected Answer: B\nML== SageMaker","upvote_count":"2","comment_id":"899658"},{"upvote_count":"2","content":"Selected Answer: B\nB sagemaker provide deploy ml models","comment_id":"898278","poster":"nosense","timestamp":"1684154400.0"}],"topic":"1","unix_timestamp":1684154400,"answer_images":[],"isMC":true,"question_text":"An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","choices":{"D":"Use Amazon QuickSight to build and train models by using calculated fields. Use Amazon QuickSight to visualize the data.","A":"Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch Service to visualize the data.","C":"Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models. Use Amazon OpenSearch Service to visualize the data.","B":"Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data."}},{"id":"ZqnilOSBX9U7nRokEG65","exam_id":31,"answers_community":["C (100%)"],"discussion":[{"upvote_count":"7","comment_id":"990176","poster":"Guru4Cloud","timestamp":"1708882200.0","content":"Selected Answer: C\nTip: AWS Organziaton + service control policy (SCP) - This for any questions, you see both together. then you tell me\nC. Create a service control policy (SCP) to prevent tag modification except by authorized principals."},{"upvote_count":"3","timestamp":"1720350060.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\n\nAWS example for this question/use case:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html#example-require-restrict-tag-mods-to-admin","comment_id":"1115840","poster":"awsgeek75"},{"comment_id":"958493","content":"Selected Answer: C\nD \"Amazon CloudWatch\" just for logging, not for prevent tag modification https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies-cwe.html\n\nAmazon Organziaton has \"Service Control Policy (SCP)\" with \"tag policy\" https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html . Choose C. \n\nAWS Config for technical stuff, not for tag policies. Not A.","timestamp":"1705848960.0","poster":"james2033","upvote_count":"4"},{"timestamp":"1702449360.0","comment_id":"921930","content":"Selected Answer: C\nService control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization.","poster":"TariqKipkemei","upvote_count":"2"},{"poster":"alexandercamachop","upvote_count":"3","timestamp":"1701706260.0","comment_id":"914721","content":"Selected Answer: C\nAnytime we need to restrict anything in an AWS Organization, it is SCP Policies."},{"poster":"Abrar2022","upvote_count":"2","comment_id":"914224","content":"AWS Config is for tracking configuration changes","comments":[{"poster":"Abrar2022","content":"so it's wrong. Right asnwer is C","upvote_count":"3","comment_id":"914225","timestamp":"1701680340.0"}],"timestamp":"1701680340.0"},{"poster":"antropaws","upvote_count":"3","comment_id":"910270","content":"Selected Answer: C\nI'd say C.","timestamp":"1701357240.0"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/ja_jp/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html","comment_id":"902296","timestamp":"1700448240.0","upvote_count":"4","poster":"hiroohiroo"},{"upvote_count":"3","poster":"nosense","timestamp":"1700137380.0","comment_id":"899058","content":"Selected Answer: C\nDenies tag: modify"}],"question_id":392,"answer_ET":"C","answer_images":[],"choices":{"A":"Create a custom AWS Config rule to prevent tag modification except by authorized principals.","B":"Create a custom trail in AWS CloudTrail to prevent tag modification.","C":"Create a service control policy (SCP) to prevent tag modification except by authorized principals.","D":"Create custom Amazon CloudWatch logs to prevent tag modification."},"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/109384-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modification of cost usage tags.\n\nWhich solution will meet these requirements?","topic":"1","answer":"C","unix_timestamp":1684232580,"isMC":true,"timestamp":"2023-05-16 12:23:00","answer_description":""},{"id":"wAsLX5kl607IRSiw9Yu0","answer_description":"","answer_ET":"A","unix_timestamp":1684155300,"answer_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/109294-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2023-05-15 14:55:00","isMC":true,"question_id":393,"exam_id":31,"choices":{"C":"Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.","B":"Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.","A":"Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.","D":"Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer."},"discussion":[{"comments":[{"content":"Yes it does but you configure it. Its not automated anymore. D is the best answer!","upvote_count":"2","poster":"Wablo","comment_id":"926780","comments":[{"comment_id":"1316325","content":"to be launched when needed => requires manual human intervention\n\nCetainly will not help with requirement for the least amount of downtime.","poster":"JA2018","upvote_count":"1","timestamp":"1732282020.0"},{"poster":"Kp88","content":"What are you talking about configuring ? Yes you have to configure everything at some point \nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html","comment_id":"966690","upvote_count":"2","timestamp":"1690671540.0"}],"timestamp":"1687102980.0"},{"content":"Did not see Route 53 in this question right? So my opinion is D","poster":"smartegnine","upvote_count":"1","timestamp":"1687675860.0","comment_id":"933325"}],"poster":"lucdt4","content":"Selected Answer: A\nA and D is correct.\nBut Route 53 haves a feature DNS failover when instances down so we dont need use Cloudwatch and lambda to trigger\n-> A correct","timestamp":"1685066100.0","comment_id":"906985","upvote_count":"16"},{"timestamp":"1704024900.0","content":"Selected Answer: C\nThey are not asking for automatic failover, they want to \"ensure the application can (!) be made available in another AWS Region with minimal downtime\". This works with C; they would just execute the template and it would be available in short time.\n\nA would create a DR environment that IS already available, which is not what the question asks for. \nD is like A, just abusing Lambda to update the DNS record (which doesn't make sense).\nB would create a separate, empty database","poster":"pentium75","upvote_count":"9","comment_id":"1110519"},{"timestamp":"1722339780.0","poster":"[Removed]","comment_id":"1258181","content":"ChatGPT: \n\n\nOption C involves creating an AWS CloudFormation template to create EC2 instances and a load balancer only when needed, and configuring the DynamoDB table as a global table. This approach might introduce more downtime because the infrastructure in the disaster recovery region is not pre-deployed and ready to take over immediately. The process of launching instances and configuring the load balancer can take some time, leading to delays during the failover.\n\nOption A, on the other hand, ensures that the necessary infrastructure (Auto Scaling group, load balancer, and DynamoDB global table) is already set up and running in the disaster recovery region. This pre-deployment reduces downtime since the failover can be handled quickly by updating DNS to point to the disaster recovery region's load balancer.","upvote_count":"2"},{"comment_id":"1122393","poster":"anikolov","content":"Selected Answer: A\nWith the LEAST amount of downtime = A\nCost effective = C , but risky some of EC2 types/capacity not available in Region at the time, when need to switch to DR","upvote_count":"7","timestamp":"1705221720.0"},{"poster":"awsgeek75","upvote_count":"4","comment_id":"1115850","comments":[{"content":"Option C relies on launching EC2 instances and load balancers only when needed, which may increase downtime. On contrary, I think A is better because it ensures resources are pre-configured and ready-to-go.\n\nIt's true nobody mentioned anything about infra availability in 2 regions... but it couldn't hurt since we simply need the least amount of downtime.","timestamp":"1733218260.0","poster":"LeonSauveterre","comment_id":"1321313","upvote_count":"1"}],"timestamp":"1704633000.0","content":"Selected Answer: C\nThere are 2 parts. DB and application. Dynamo DB recovery in another region is not possible without global table so option B is out.\nA will make the infra available in 2 regions which is not required. The question is about DR, not scaling.\nD Use Lambda to modify R53 to point to new region. This is going to cause delays but is possible and it will also be running a scaled EC2 instances in passive region.\nC Make a CF template which can launch the infra when needed. DB is global table so it will be available."},{"upvote_count":"1","comment_id":"1084358","poster":"meowruki","timestamp":"1701348540.0","content":"Selected Answer: C\nAWS CloudFormation Template: Use CloudFormation to define the infrastructure components (EC2 instances, load balancer, etc.) in a template. This allows for consistent and repeatable infrastructure deployment.\n\n EC2 Instances and Load Balancer: Launch the EC2 instances and load balancer in the disaster recovery (DR) Region using the CloudFormation template. This enables the deployment of the application in the DR Region when needed.\n\n DynamoDB Global Table: Configure the DynamoDB table as a global table. DynamoDB Global Tables provide automatic multi-region, multi-master replication, ensuring that the data is available in both the primary and DR Regions.\n\n DNS Failover: Configure DNS failover to point to the new DR Region's load balancer. This allows for seamless failover of traffic to the DR Region when needed.\n\nOption A is close, but it introduces an Auto Scaling group in the disaster recovery Region, which might introduce unnecessary complexity and potential scaling delays. Option D introduces a Lambda function triggered by CloudWatch alarms, which might add latency and complexity compared to the more direct approach in Option C."},{"poster":"bogobob","content":"Selected Answer: A\nAssuming theyre using Route53 as a DNS then A https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html","timestamp":"1700185080.0","upvote_count":"3","comment_id":"1072953"},{"timestamp":"1699799100.0","comment_id":"1068579","upvote_count":"1","content":"Selected Answer: C\nOnly B and C take care of EC2 instants. But since B does not take care of Data in the Dynamo DB, C is the only correct Answer.","poster":"EEK2k"},{"upvote_count":"1","timestamp":"1699207560.0","content":"Selected Answer: A\nRoute 53 haves a feature DNS failover when instances down","comment_id":"1063164","poster":"potomac"},{"timestamp":"1697811720.0","content":"C is the best choice here","upvote_count":"1","poster":"thanhnv142","comment_id":"1048867"},{"timestamp":"1697742000.0","content":"Selected Answer: C\nI think CloudFormation is easier than manual provision of Auto Scaling group and load balancer in DR region.","poster":"Wayne23Fang","upvote_count":"2","comment_id":"1048202"},{"timestamp":"1692977040.0","content":"Selected Answer: A\nCreating Auto Scaling group and load balancer in DR region allows fast launch of capacity when needed.\nConfiguring DynamoDB as a global table provides continuous data replication.\nUsing DNS failover via Route 53 to point to the DR region's load balancer enables rapid traffic shifting.","poster":"Guru4Cloud","upvote_count":"2","comment_id":"990173"},{"comments":[{"poster":"Kp88","upvote_count":"1","content":"Failover policy takes care of DNS record update so no need for cloud watch/lambda","timestamp":"1690671360.0","comment_id":"966687"}],"content":"Both Option A and Option D include the necessary steps of setting up an Auto Scaling group and load balancer in the disaster recovery Region, configuring the DynamoDB table as a global table, and updating DNS records. However, Option D provides a more detailed approach by explicitly mentioning the use of an Amazon CloudWatch alarm and AWS Lambda function to automate the DNS update process.\n\nBy leveraging an Amazon CloudWatch alarm, Option D allows for an automated failover mechanism. When triggered, the CloudWatch alarm can execute an AWS Lambda function, which in turn can update the DNS records in Amazon Route 53 to redirect traffic to the disaster recovery load balancer in the new Region. This automation helps reduce the potential for human error and further minimizes downtime.\nAnswer is D","timestamp":"1687102740.0","poster":"Wablo","upvote_count":"2","comment_id":"926776"},{"content":"Selected Answer: C\nThe company wants to ensure the application 'CAN' be made available in another AWS Region with minimal downtime. Meaning they want to be able to launch infra on need basis.\nBest answer is C.","comments":[{"poster":"Wablo","content":"minimal downtme not minimal effort!\n\n\nD","comment_id":"926777","timestamp":"1687102860.0","upvote_count":"1"},{"comment_id":"931359","poster":"dajform","content":"B, C are not OK because \"launching resources when needed\", which will increase the time to recover \"DR\"","upvote_count":"2","timestamp":"1687508280.0"}],"poster":"TariqKipkemei","timestamp":"1686631380.0","upvote_count":"2","comment_id":"921934"},{"upvote_count":"3","comment_id":"913222","timestamp":"1685762820.0","content":"I feel it is A\nConfigure DNS failover: Use DNS failover to point the application's DNS record to the load balancer in the disaster recovery Region. DNS failover allows you to route traffic to the disaster recovery Region in case of a failure in the primary Region.","comments":[{"content":"Once you configure manually the DNS , its no more automated like Lambda does.","timestamp":"1687102920.0","poster":"Wablo","upvote_count":"2","comment_id":"926778"}],"poster":"AshishRocks"},{"content":"Selected Answer: C\nC suits best","comment_id":"902614","upvote_count":"3","timestamp":"1684591440.0","poster":"Yadav_Sanjay"},{"content":"Selected Answer: A\nAがDNS フェイルオーバー","upvote_count":"1","poster":"hiroohiroo","timestamp":"1684543260.0","comment_id":"902292"},{"content":"A\nBy configuring the DynamoDB table as a global table, you can replicate the table data across multiple AWS Regions, including the primary Region and the disaster recovery Region. This ensures that data is available in both Regions and can be seamlessly accessed during a failover event.","timestamp":"1684408380.0","poster":"cloudenthusiast","comment_id":"901107","upvote_count":"2"},{"comment_id":"899660","upvote_count":"3","poster":"Efren","timestamp":"1684291080.0","content":"Selected Answer: A\nA for ME, DNs should failover"},{"timestamp":"1684155300.0","poster":"nosense","content":"Selected Answer: D\nD for me","comments":[{"upvote_count":"2","timestamp":"1684226400.0","poster":"Efren","content":"I would go for A. If we have DNS failover, why to burden with lambda updating the DNS records?","comment_id":"899002"},{"comment_id":"899062","timestamp":"1684232700.0","poster":"nosense","content":"Misunderstanding. Only A valid","upvote_count":"3"},{"upvote_count":"2","content":"I would pick A","timestamp":"1684326300.0","poster":"Macosxfan","comment_id":"900120"}],"upvote_count":"3","comment_id":"898286"}],"answers_community":["A (57%)","C (38%)","5%"],"question_text":"A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime.\n\nWhat should a solutions architect do to meet these requirements with the LEAST amount of downtime?","topic":"1","question_images":[]},{"id":"m0Jdoj1Z2JYfxFZwqk3b","answer_ET":"A","isMC":true,"answer_images":[],"choices":{"B":"Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowmobile vehicle back to AWS to finish the migration and continue the ongoing replication.","C":"Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to finish the migration and continue the ongoing replication","D":"Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.","A":"Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication."},"answers_community":["A (88%)","12%"],"question_id":394,"question_images":[],"timestamp":"2023-05-16 10:42:00","question_text":"A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime.\n\nWhich solution will migrate the database MOST cost-effectively?","answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/109377-exam-aws-certified-solutions-architect-associate-saa-c03/","topic":"1","exam_id":31,"discussion":[{"comments":[{"content":"Thanks, i was checking the speed more than price. Thanks for the clarification","comment_id":"899663","upvote_count":"3","timestamp":"1700195940.0","poster":"Efren"}],"content":"Selected Answer: A\nA) 300 first 10 days. 150 shipping\nD) 750 for 2 weeks","upvote_count":"12","comment_id":"899118","timestamp":"1700141700.0","poster":"nosense"},{"upvote_count":"7","content":"Selected Answer: A\nDirect Connect takes at least 1 month to setup - D is invalid\nAWS Snowmobile is used for transferring large amounts of data (petabytes) from remote locations where establishing a connection to the cloud is impossible - B is invalid\nAWS Snowball Edge Compute Optimized provides higher vCPU performance and lower storage as compared to Snowball storage optimized. As our need is solely data transfer, high vCPU performance is not required but high storage is - C is invalid","comment_id":"1073751","poster":"Goutham4981","timestamp":"1715988360.0"},{"upvote_count":"6","timestamp":"1729600740.0","content":"Selected Answer: A\nBut I didn't understand why we are using a schema conversion tool, because AWS have already a managed service engine for MySQL Db, (RDS for MySQL or Aurora for my SQL is on the table )","comment_id":"1200186","poster":"zinabu"},{"comments":[{"content":"It takes more way more than 2 weeks to setup Direct Connect. Therefore, D is not valid since we have to do the transfer within 2 weeks.","upvote_count":"3","poster":"Murtadhaceit","timestamp":"1717847340.0","comment_id":"1091118"}],"poster":"EEK2k","timestamp":"1715517480.0","comment_id":"1068586","upvote_count":"3","content":"Selected Answer: D\nTo calculate the time it would take to transfer 20TB of data over a 1 GB dedicated AWS Direct Connect, we can use the formula:\n\ntime = data size / data transfer rate\n\nHere, the data size is 20TB, which is equivalent to 20,000 GB or 20,000,000 MB. The data transfer rate is 1 GB/s.\n\nConverting the data size to MB, we get:\n\n20,000,000 MB / 1 GB/s = 20,000 seconds\n\nTherefore, it would take approximately 20,000 seconds or 5.56 hours to transfer 20TB of data over a 1 GB dedicated AWS Direct Connect."},{"upvote_count":"4","timestamp":"1714925340.0","content":"Selected Answer: A\nC is wrong, GPU is not needed","poster":"potomac","comment_id":"1063165"},{"poster":"Ramdi1","upvote_count":"3","comment_id":"1024872","timestamp":"1712239620.0","content":"Selected Answer: A\nHas to be A. the option for D would only work if they said they have like 6 Months plus. It would take too long to set up."},{"timestamp":"1708879980.0","comment_id":"990166","upvote_count":"4","content":"Selected Answer: A\nI agreed with A.\nWhy not D.?\nWhen you initiate the process by requesting an AWS Direct Connect connection, it typically starts with the AWS Direct Connect provider. This provider may need to coordinate with AWS to allocate the necessary resources. This initial setup phase can take anywhere from a few days to a couple of weeks.\nCouple of weeks? No Good","poster":"Guru4Cloud","comments":[{"upvote_count":"3","poster":"Guru4Cloud","timestamp":"1708880340.0","comment_id":"990167","content":"When you create a Snowball job in the AWS console, it will estimate the delivery date based on your location. Being near a facility shows 1-2 day estimated delivery.\nFor extremely urgent requests, you can contact AWS Support and inquire about expedited Snowball delivery. If inventory is available, they may be able to ship same day or next day."}]},{"content":"Selected Answer: A\nKeyword \"20 TB\", choose \"AWS Snowball\", there are A or C. C has word \"GPU\" what is not related, therefore choose A.","timestamp":"1705849080.0","poster":"james2033","comment_id":"958500","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: A\nAnswer A","comment_id":"947565","timestamp":"1704843300.0","poster":"Zox42"},{"comments":[{"poster":"pentium75","content":"No, takes months, not weeks","upvote_count":"2","comment_id":"1110521","timestamp":"1719742620.0"}],"upvote_count":"1","poster":"MrAWSAssociate","comment_id":"931313","content":"Selected Answer: D\nD is correct","timestamp":"1703323800.0"},{"poster":"DrWatson","timestamp":"1701755280.0","upvote_count":"3","comment_id":"915047","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html"},{"content":"Selected Answer: A\nD Direct Connection will need a long time to setup plus need to deal with Network and Security changes with existing environment. Ad then plus the Data trans time... No way can be done in 2 weeks.","poster":"RoroJ","upvote_count":"5","timestamp":"1701092040.0","comment_id":"907938"},{"timestamp":"1700773500.0","comment_id":"905163","upvote_count":"2","content":"Selected Answer: D\nOverall, option D combines the reliability and cost-effectiveness of AWS Direct Connect, AWS DMS, and AWS SCT to migrate the database efficiently and minimize downtime.","poster":"Joselucho38"},{"content":"Selected Answer: A\nD - Direct Connect takes atleast a month to setup! Requirement is for within 2 weeks.","timestamp":"1700707080.0","poster":"Abhineet9148232","upvote_count":"5","comment_id":"904445"},{"content":"Selected Answer: D\nAWS Snowball Edge Storage Optimized device is used for large-scale data transfers, but the lead time for delivery, data transfer, and return shipping would likely exceed the 2-week time frame. Also, ongoing database changes wouldn't be replicated while the device is in transit.","comment_id":"902445","upvote_count":"1","comments":[{"content":"Change to A because \"Most cost effective\"","upvote_count":"3","comment_id":"903720","poster":"Rob1L","timestamp":"1700636880.0"}],"poster":"Rob1L","timestamp":"1700475780.0"},{"timestamp":"1700447760.0","comment_id":"902289","upvote_count":"4","poster":"hiroohiroo","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/ja_jp/snowball/latest/developer-guide/device-differences.html#device-options\nAです。"},{"comment_id":"900068","timestamp":"1700228580.0","comments":[{"timestamp":"1700676000.0","comment_id":"904191","upvote_count":"1","content":"At least one month and expensive.","poster":"examtopictempacc"}],"poster":"norris81","content":"Selected Answer: A\nHow long does direct connect take to provision ?","upvote_count":"2"},{"comments":[{"poster":"Efren","upvote_count":"2","timestamp":"1700317140.0","content":"Wrong myself, i was checking time, but not price","comment_id":"901191","comments":[]}],"poster":"Efren","content":"Selected Answer: D\n20 TB 1G/S would take around 44 hours. I guess it takes less than snow devices to receive and send it back","upvote_count":"1","timestamp":"1700131320.0","comment_id":"899004"}],"answer_description":"","unix_timestamp":1684226520},{"id":"CKBLUOjiNKRCFD5UqqTZ","exam_id":31,"topic":"1","answer_description":"","question_text":"A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. The company successfully launched a new product. The workload on the database has increased. The company wants to accommodate the larger workload without adding infrastructure.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer":"A","unix_timestamp":1684149240,"answers_community":["A (84%)","Other"],"timestamp":"2023-05-15 13:14:00","discussion":[{"upvote_count":"16","content":"Selected Answer: A\nA.\n\"without adding infrastructure\" means scaling vertically and choosing larger instance.\n\"MOST cost-effectively\" reserved instances","comment_id":"909527","poster":"elmogy","timestamp":"1685376120.0","comments":[{"upvote_count":"6","timestamp":"1698047700.0","comment_id":"1051565","content":"\"MOST cost-effectively\" doesn't mean reserved instances. Only in this case it is but not in general.","poster":"wsdasdasdqwdaw"}]},{"upvote_count":"1","timestamp":"1737244560.0","comment_id":"1342804","content":"Selected Answer: B\nwithout adding infrastructure means without adding more capacity per instance, multi-az adding read replicas solve the problem.","poster":"zdi561"},{"timestamp":"1704633900.0","content":"Selected Answer: A\naccommodate the larger workload without adding infrastructure. = Reserved DB instance","comments":[{"poster":"awsgeek75","comment_id":"1115861","upvote_count":"3","content":"+ make the instance larger so most cost effective is to reserve a large instance suitable for workload which is A","timestamp":"1704634020.0"}],"upvote_count":"3","poster":"awsgeek75","comment_id":"1115859"},{"comment_id":"1110572","poster":"pentium75","content":"Selected Answer: A\nB - Multi-AZ is for HA, does not help 'accommodating the larger workload'\nC - Adding \"another instance\" will not help, we can't split the workload between two instances\nD - On-demand instance is a good choice for unknown workload, but here we know the workload, it's just higher than before","timestamp":"1704028500.0","upvote_count":"3"},{"timestamp":"1700271720.0","comment_id":"1073758","content":"Selected Answer: A\nCannot add more infrastructure - C is invalid\nMulti AZ DB instance is for high availability and failure mitigation, does not increase performance, higher workload support - B is invalid\nOn demand instances are costlier than Reserved instances - D is invalid","poster":"Goutham4981","upvote_count":"2"},{"timestamp":"1700185620.0","comments":[{"timestamp":"1732282680.0","upvote_count":"1","poster":"JA2018","comment_id":"1316327","content":"Please refer to following Keys in Stem:\n\n1. The company successfully launched a new product. \n\n2. The workload on the database has increased. \n\n3. The company wants to accommodate the larger workload without adding infrastructure."},{"upvote_count":"2","poster":"pentium75","comment_id":"1110568","content":"Question says nothing about unknown load. New product -> more total products -> load has increased.","timestamp":"1704028320.0"}],"content":"Selected Answer: D\nNot A : \"launched a new product\", reserved instances are for known workloads, a new product doesn't have known workload.\nNot B : \"accommodate the larger workload\", while Multi-AZ can help with larger workloads, they are more for higher availability.\nNot C : \"without adding infrastructure\", adding a PostGresQL instance is new infrastructure.","upvote_count":"3","poster":"bogobob","comment_id":"1072961"},{"poster":"Guru4Cloud","content":"Selected Answer: B\nB is the best approach in this scenario overall:\n\nMaking the RDS PostgreSQL instance Multi-AZ adds a standby replica to handle larger workloads and provides high availability.\nEven though it adds infrastructure, the cost is less than doubling the infrastructure with a separate DB instance.\nIt provides better performance, availability, and disaster recovery than a single larger instance.","upvote_count":"2","comments":[{"comment_id":"1022378","content":"Agreed the answer is B\nMulti-AZ deployments are cost-effective because they leverage the standby instance without incurring additional charges. You only pay for the primary instance's regular usage costs.","poster":"BillyBlunts","timestamp":"1696171980.0","upvote_count":"1"}],"comment_id":"990081","timestamp":"1692968400.0"},{"timestamp":"1689945840.0","content":"Selected Answer: A\nBuy larger instance.","upvote_count":"2","poster":"james2033","comment_id":"958539"},{"poster":"james2033","upvote_count":"2","timestamp":"1689656040.0","content":"Selected Answer: A\nKeyword \"Amazon RDS for PostgreSQL instance large\" . See list of size of instance at https://aws.amazon.com/rds/instance-types/","comment_id":"954955"},{"timestamp":"1684772580.0","upvote_count":"3","poster":"examtopictempacc","content":"Selected Answer: A\nA. \nNot C: without adding infrastructure","comment_id":"904206"},{"content":"Answer - C\nOption B, making the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance, would provide high availability and fault tolerance but may not directly address the need for increased capacity to handle the larger workload.\n\nTherefore, the recommended solution is Option C: Buy reserved DB instances for the workload and add another Amazon RDS for PostgreSQL DB instance to accommodate the increased workload in a cost-effective manner.","poster":"EA100","timestamp":"1684591080.0","upvote_count":"1","comment_id":"902606"},{"content":"C\n Option C: buying reserved DB instances for the total workload and adding another Amazon RDS for PostgreSQL DB instance seems to be the most appropriate choice. It allows for workload distribution across multiple instances, providing scalability and potential performance improvements. Additionally, reserved instances can provide cost savings in the long term.","poster":"cloudenthusiast","upvote_count":"1","comment_id":"901125","timestamp":"1684409040.0"},{"upvote_count":"4","comment_id":"899119","content":"A for me, because without adding additional infrastructure","timestamp":"1684236960.0","poster":"nosense"},{"upvote_count":"1","content":"Should be C","poster":"th3k33n","comment_id":"898206","comments":[{"upvote_count":"2","comments":[{"comment_id":"901126","upvote_count":"1","poster":"cloudenthusiast","content":"Option A involves making the existing Amazon RDS for PostgreSQL DB instance larger. While this can improve performance, it may not be sufficient to handle a significantly increased workload. It also doesn't distribute the workload or provide scalability.","comments":[{"poster":"nosense","comment_id":"901866","comments":[{"content":"A is the best","poster":"omoakin","timestamp":"1685392860.0","upvote_count":"2","comment_id":"909709"}],"upvote_count":"2","timestamp":"1684495080.0","content":"The main not HA, cost-effectively and without adding infrastructure"}],"timestamp":"1684409100.0"}],"poster":"Efren","timestamp":"1684226640.0","content":"That would add more infraestructure. A would increase the size, keeping the number of instances, i think","comment_id":"899006"}],"timestamp":"1684149240.0"}],"choices":{"C":"Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB instance.","D":"Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance.","A":"Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB instance larger.","B":"Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance."},"answer_ET":"A","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/109277-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"answer_images":[],"question_id":395}],"exam":{"isBeta":false,"provider":"Amazon","isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","isImplemented":true,"id":31},"currentPage":79},"__N_SSP":true}