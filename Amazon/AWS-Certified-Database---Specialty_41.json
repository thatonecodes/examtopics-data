{"pageProps":{"questions":[{"id":"fB2z1LsiFOwuigGQkPft","answer_description":"","question_text":"A company with branch offices in Portland, New York, and Singapore has a three-tier web application that leverages a shared database. The database runs on\nAmazon RDS for MySQL and is hosted in the us-west-2 Region. The application has a distributed front end deployed in the us-west-2, ap-southheast-1, and us- east-2 Regions.\nThis front end is used as a dashboard for Sales Managers in each branch office to see current sales statistics. There are complaints that the dashboard performs more slowly in the Singapore location than it does in Portland or New York. A solution is needed to provide consistent performance for all users in each location.\nWhich set of actions will meet these requirements?","exam_id":22,"choices":{"A":"Take a snapshot of the instance in the us-west-2 Region. Create a new instance from the snapshot in the ap-southeast-1 Region. Reconfigure the ap- southeast-1 front-end dashboard to access this instance.","D":"Create an RDS read replica in the us-west-2 Region where the primary instance resides. Create a read replica in the ap-southeast-1 Region from the read replica located on the us-west-2 Region. Reconfigure the ap-southeast-1 front-end dashboard to access this instance.","B":"Create an RDS read replica in the ap-southeast-1 Region from the primary RDS DB instance in the us-west-2 Region. Reconfigure the ap-southeast-1 front- end dashboard to access this instance.","C":"Create a new RDS instance in the ap-southeast-1 Region. Use AWS DMS and change data capture (CDC) to update the new instance in the ap-southeast-1 Region. Reconfigure the ap-southeast-1 front-end dashboard to access this instance."},"answer_ET":"B","isMC":true,"answer":"B","answer_images":[],"discussion":[{"comments":[{"content":"Agree it's B.","comment_id":"603730","upvote_count":"1","timestamp":"1652949420.0","poster":"khchan123"}],"content":"Leaning to B. \nhttps://aws.amazon.com/rds/features/read-replicas/\n\"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. \"","comment_id":"139882","upvote_count":"20","timestamp":"1633234440.0","poster":"BillyMadison"},{"timestamp":"1632996120.0","upvote_count":"6","content":"If they go with A, how do they keep the databases in sync?\nB for me","poster":"[Removed]","comment_id":"133097"},{"comment_id":"1096014","upvote_count":"1","timestamp":"1702524960.0","poster":"NishithShah","content":"Selected Answer: B\nB is correct."},{"upvote_count":"1","comments":[{"upvote_count":"1","poster":"jitesh_k","content":"They assume as much.","timestamp":"1700891580.0","comment_id":"1079763"}],"comment_id":"1025600","content":"apparently I need to now geography as well that Singapore is AP region","timestamp":"1696506660.0","poster":"Sathish_dbs"},{"comment_id":"1004251","content":"Selected Answer: B\nFrom the plausible options,\nA - this would make it a non-shared database, destroying the architecture.\nB - iff the dashboard is only for reading then the read replica in SGP might do.","poster":"Germaneli","timestamp":"1694372100.0","upvote_count":"1"},{"content":"Selected Answer: B\nBecause the read replica is designed for this purpose","poster":"ken_test1234","comment_id":"853090","upvote_count":"1","timestamp":"1679998260.0"},{"comment_id":"799979","upvote_count":"3","content":"Selected Answer: B\nA is incorrect in no data synchronization between 2 databases\nB is correct since the read replica is same region with the frontend\nC is incorrect because extra cost of DMS instance. Although it is feasible option.\nD is incorrect because of extra cost of new replica on primary region and replication lag.","poster":"im_not_robot","timestamp":"1675702560.0"},{"upvote_count":"1","content":"Selected Answer: B\nB is the answer","timestamp":"1669654140.0","poster":"examineme","comment_id":"729438"},{"comment_id":"627654","upvote_count":"1","poster":"sachin","content":"You can create read replication from a already existing replica, but the replication lag will be high. So D is operationally possible but replication lag will be high. \nSo correct answer is B.","timestamp":"1657068600.0"},{"comment_id":"620944","content":"Selected Answer: B\nAnswer:B","upvote_count":"1","timestamp":"1655984940.0","poster":"ryuhei"},{"poster":"novice_expert","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.XRgn.html\nYou can create replicas through Region same way as in same region\n\nCreate an RDS read replica in the ap-southeast-1 Region from the primary RDS DB instance in the us-west-2 Region. Reconfigure the ap-southeast-1 front- end dashboard to access this instance.","comment_id":"595194","timestamp":"1651342320.0","upvote_count":"1"},{"content":"Selected Answer: B\nB is the right answer!!","comment_id":"539238","upvote_count":"2","timestamp":"1643842980.0","poster":"AriraAWS"},{"timestamp":"1635971160.0","content":"B is correct. You can create replicas through Region\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.XRgn.html","poster":"ChauPhan","comment_id":"424319","upvote_count":"1"},{"upvote_count":"1","comment_id":"364652","content":"I will go with B","timestamp":"1635795360.0","poster":"Dip11"},{"upvote_count":"2","timestamp":"1635693840.0","content":"Would go with B","poster":"LMax","comment_id":"314798"},{"comment_id":"297917","upvote_count":"1","content":"Ans: B","poster":"myutran","timestamp":"1635661260.0"},{"content":"Not sure why you all excluded C. It is definitely a good solution to keep two databases in sync using DMS with CDC.","timestamp":"1635589080.0","upvote_count":"3","poster":"Bassel","comment_id":"285529"},{"poster":"Glendon","comments":[{"poster":"Radhaghosh","content":"That is not the reason why D is wrong. Read Replica is Source (us-west-2) is not required for this use case.\nIf a read replica is running any version of MySQL, you can specify it as the source DB instance for another read replica. For example, you can create ReadReplica1 from MyDBInstance, and then create ReadReplica2 from ReadReplica1. Updates made to MyDBInstance are replicated to ReadReplica1 and then replicated from ReadReplica1 to ReadReplica2. You can't have more than four instances involved in a replication chain. For example, you can create ReadReplica1 from MySourceDBInstance, and then create ReadReplica2 from ReadReplica1, and then create ReadReplica3 from ReadReplica2, but you can't create a ReadReplica4 from ReadReplica3.","upvote_count":"2","comment_id":"605589","timestamp":"1653232140.0"}],"content":"D is incorrect - we cannot replica-chain. E.g. we cannot create a read-replica from another read-replica. The answer is B.","comment_id":"274120","upvote_count":"1","timestamp":"1634833920.0"},{"poster":"Ashoks","comment_id":"212029","content":"Ans - B","upvote_count":"1","timestamp":"1634548440.0"},{"timestamp":"1634536560.0","upvote_count":"1","comment_id":"203003","poster":"Jack86","content":"A : FALSE . No SYNC Mechanism configured \n B : TRUE\n C : FALSE . aws dms Essentially for Migration Purposes\n D : FALSE : ( as stated by szmulder) You can only create a cross-Region Amazon RDS read replica from a source Amazon RDS DB instance that is not a read replica of another Amazon RDS DB instance.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn\n\nCorrect answer is B ."},{"upvote_count":"4","poster":"Ebi","comment_id":"153513","content":"Correct answer is B","timestamp":"1634470020.0"},{"timestamp":"1633593720.0","upvote_count":"1","comment_id":"145744","poster":"BillyC","content":"I think is correct"},{"timestamp":"1633301880.0","content":"Ans:D\nAmazon RDS doesn’t support circular replication. You can’t configure a DB instance to serve as a replication source for an existing DB instance. You can only create a new read replica from an existing DB instance. For example, if MyDBInstance replicates to ReadReplica1, you can’t configure ReadReplica1 to replicate back to MyDBInstance. For MariaDB, MySQL, and PostgreSQL, you can create a read replica from an existing read replica. For example, from ReadReplica1, you can create a new read replica, such as ReadReplica2. For Oracle and SQL Server, you can’t create a read replica from an existing read replica.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html","upvote_count":"2","comment_id":"141666","comments":[{"upvote_count":"1","comment_id":"150735","content":"It's not D. we don't need create a read replicates in the same region in order to create a new one in other regions. B is correct.\nhttps://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/#:~:text=The%20cross%2Dregion%20replicas%20are,encrypted%20using%20public%20key%20encryption.","poster":"szmulder","timestamp":"1633921080.0"},{"poster":"szmulder","upvote_count":"3","content":"Here is one more thing why D is incorrect.\nYou can only create a cross-Region Amazon RDS read replica from a source Amazon RDS DB instance that is not a read replica of another Amazon RDS DB instance.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn","timestamp":"1634510640.0","comment_id":"174186"}],"poster":"pan24"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/25516-exam-aws-certified-database-specialty-topic-1-question-28/","topic":"1","answers_community":["B (100%)"],"unix_timestamp":1594574160,"question_id":201,"timestamp":"2020-07-12 19:16:00"},{"id":"YtNOCRmz0gAoboxCeC9k","answer_images":[],"answer_description":"","exam_id":22,"question_images":[],"unix_timestamp":1669927680,"choices":{"C":"Use ElastiCache for Memcached with lazy loading and short time to live (TTL)","A":"Use ElastiCache for Memcached with write-through and long time to live (TTL)","B":"Use ElastiCache for Redis with lazy loading and short time to live (TTL)","D":"Use ElastiCache for Redis with write-through and long time to live (TTL)"},"question_text":"A media company hosts a highly available news website on AWS but needs to improve its page load time, especially during very popular news releases. Once a news page is published, it is very unlikely to change unless an error is identified. The company has decided to use Amazon ElastiCache.\n\nWhat is the recommended strategy for this use case?","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/89704-exam-aws-certified-database-specialty-topic-1-question-280/","timestamp":"2022-12-01 21:48:00","topic":"1","answers_community":["D (47%)","B (24%)","C (18%)","12%"],"discussion":[{"poster":"koki2847","timestamp":"1712526780.0","content":"Selected Answer: B\nB is answer. Write through means low availability while providing fresh data, so A and D are out. Memcached is incorrect in terms of HA, because it doesn't have replication feature and would lose data when node failure occur. There's an exception, though. If Memcached is severless, C would be answer. It supports replication https://aws.amazon.com/blogs/aws/amazon-elasticache-serverless-for-redis-and-memcached-now-generally-available/","upvote_count":"1","comment_id":"1191227"},{"content":"Selected Answer: C\nC. Noted that:\n1. The news website needs HA, not the cache system.\n2. Write-though can't rescue node failure or scaling out, whereas lazy-loading stale data can be overcome by adding TTL.\n3. Simpler, easy to scale out goes to Memcached; complex and fancy features goes to Redis.\nSo A and D dropped because no write-though. Pick C and drop B due to simpler, scale-out architecture.\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html","poster":"zanhsieh","timestamp":"1692119520.0","comment_id":"981861","upvote_count":"1"},{"upvote_count":"1","content":"It does not matter if you lose a Memcached node - any data lost will be reloaded from the source. You do not need replication of data offered by Redis. \"Short TTL\" is a relative term. A new story is newsworthy 24 hrs? Is that short enough to qualify as short? How long is a long TTL?","poster":"aviathor","timestamp":"1684936080.0","comment_id":"905942"},{"timestamp":"1684935420.0","comment_id":"905931","content":"Selected Answer: C\nThis is not an easy one... One has to choose between\n* Redis or Memcached\n* write-through and long TTL, or short TTL with lazy loading\n\nI'll choose lazy loading because there is no way of saying whether an article will be popular\nI'll choose Memcached because there is no need for complex data structures and Memcached scales better vertically.","poster":"aviathor","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: D\nA long time to live (TTL) is recommended in this scenario because news pages are unlikely to change frequently. Setting a long TTL will allow the cached data to be reused for a longer period, reducing the number of requests to the database and improving page load times.\n\nTherefore, the correct answer is D. Use ElastiCache for Redis with write-through and long time to live (TTL).","poster":"dougporto1988","comment_id":"849712","timestamp":"1679704080.0"},{"upvote_count":"4","content":"Selected Answer: D\nD. Use ElastiCache for Redis with write-through and long time to live (TTL).\n\nThe use case requires improving the page load time of a highly available news website, especially during popular news releases, where there is a high level of traffic. ElastiCache can help in achieving this by caching frequently accessed data in-memory, reducing the number of requests to the database and improving the overall performance of the website.\n\nRedis is recommended for this use case because it provides better performance for read-intensive workloads and has more advanced features for caching, such as write-through caching. Write-through caching ensures that any changes made to the data in the cache are also written back to the database, ensuring data consistency.","comment_id":"837117","timestamp":"1678631760.0","poster":"jpj"},{"poster":"milofficial","content":"Selected Answer: D\nlong time to live as the articles are unlikely to change. Redis over Memcached, as it supports a wider range of data structures.","comment_id":"807633","timestamp":"1676308380.0","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: A\nWhy not Memcache though? Can someone please explain?","timestamp":"1672786080.0","poster":"lollyj","comments":[{"upvote_count":"1","timestamp":"1680532260.0","poster":"Mintwater","comment_id":"860015","content":"I agree with A.\nWe need to look at the difference between Memcache and Redis: Memcache for simple value return, but redis for complex data model."},{"content":"Because the requirement is for highly available site - Memcached can not be made highly available out of the box, Redis can.","timestamp":"1681909260.0","upvote_count":"1","poster":"mawsman","comment_id":"874678"}],"comment_id":"765105"},{"timestamp":"1672233960.0","content":"D is the answer - \nSince the news is not likely to change it will be there once it is written and with long TTL it will be there for longer time.","comment_id":"759881","poster":"parle101","upvote_count":"1"},{"comment_id":"736902","poster":"catboy","timestamp":"1670337540.0","upvote_count":"1","content":"D right answer","comments":[{"content":"With Write through, if a node goes down it won't cache again until there is a write operation. So if the pages are not modified, caching won't happen. Also, with Write-Through, all the writes will be cached and it can reason for memory being used be less frequently accessed pages. High TTL can further increase that problem.","upvote_count":"1","comment_id":"738582","timestamp":"1670469600.0","poster":"Sab"}]},{"comment_id":"733091","content":"Selected Answer: B\nElasticache for redis since application needs to be highly available. Memcache dont have HA. Lazy Writing since there is no much modifications expected on page.","timestamp":"1669927680.0","poster":"Sab","comments":[{"poster":"khun","content":"How about the short TTL?","timestamp":"1671519300.0","comment_id":"750578","upvote_count":"2"}],"upvote_count":"3"}],"question_id":202,"answer_ET":"D","answer":"D"},{"id":"7xEiVwd7ZZHuvwRMTjd6","answer_images":[],"exam_id":22,"answer_description":"","question_images":[],"unix_timestamp":1670599440,"choices":{"B":"Install Oracle Statspack. Enable the performance statistics feature to collect, store, and display performance data to monitor database performance.","D":"Create a new DB parameter group that includes the AllocatedStorage, DBInstanceClassMemory, and DBInstanceVCPU variables. Enable RDS Performance Insights","C":"Enable RDS Performance Insights to visualize the database load. Enable Enhanced Monitoring to view how different threads use the CPU","A":"Publish RDS Performance insights metrics to Amazon CloudWatch. Add AWS CloudTrail filters to monitor database performance"},"question_text":"A company migrated an on-premises Oracle database to Amazon RDS for Oracle. A database specialist needs to monitor the latency of the database.\n\nWhich solution will meet this requirement with the LEAST operational overhead?","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/90820-exam-aws-certified-database-specialty-topic-1-question-281/","timestamp":"2022-12-09 16:24:00","topic":"1","discussion":[{"upvote_count":"1","poster":"MultiAZ","timestamp":"1705217820.0","content":"Selected Answer: C\nC\nAlthough I'd go with regular CloudWatch for monitoring latency. However, out of the provided answers, C is the only one that makes sense (barely)","comment_id":"1122358"},{"timestamp":"1693507080.0","upvote_count":"3","content":"Selected Answer: C\nC. Enable RDS Performance Insights to visualize the database load. Enable Enhanced Monitoring to view how different threads use the CPU\n\nhttps://aws.amazon.com/rds/performance-insights/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html","comment_id":"995407","poster":"Pranava_GCP"},{"upvote_count":"1","timestamp":"1689775740.0","content":"I vote for C.","poster":"Windy","comment_id":"956715"},{"comment_id":"753372","upvote_count":"2","timestamp":"1671719220.0","content":"Can somebody please answer and explain the answers to the question no 271,272,277,282,283,284,285.\nThere are no discussions on these questions it seems.","poster":"Kuntal97322"},{"content":"Selected Answer: C\nEnhance monitoring will give granular details and performance insights will give you database performance metrics","comment_id":"740225","poster":"amulbaba","timestamp":"1670599440.0","upvote_count":"4"}],"answers_community":["C (100%)"],"question_id":203,"answer_ET":"C","answer":"C"},{"id":"irs5lp0am9AoZzMoqRYe","timestamp":"2022-12-01 03:03:00","isMC":true,"exam_id":22,"question_id":204,"unix_timestamp":1669860180,"answer_images":[],"choices":{"D":"Turn on logging for the AWS DMS task by setting the TARGET_APPLY action with the level of severity' set to LOGGER_SEVERITY_DETAILED_DEBUG","A":"Export AWS DMS logs to Amazon CloudWatch and identify the DDL statement from the AWS Management Console","B":"Turn on logging for the AWS DMS task by setting the TARGET_LOAD action with the level of severity set to LOGGER_SEVERITY_DETAILED_DEBUG","C":"Turn on DDL activity tracing in the RDS for Oracle DB instance parameter group"},"answers_community":["D (100%)"],"answer_description":"","answer_ET":"D","discussion":[{"timestamp":"1669860180.0","poster":"Sab","content":"Answer D\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.Logging.html","upvote_count":"5","comment_id":"732170"},{"poster":"MultiAZ","content":"Selected Answer: D\nD\nTARGET_APPLY is for CDC. The question mentions the DMS task is CDC only.\nTARGE_LOAD is for the initial load","comment_id":"1122359","timestamp":"1705218000.0","upvote_count":"1"},{"comment_id":"908903","upvote_count":"4","content":"Selected Answer: D\nTARGET_APPLY – Data and data definition language (DDL) statements are applied to the target database. (correct)\nTARGET_LOAD – Data is loaded into the target database.","poster":"Kodoma","timestamp":"1685314800.0"},{"content":"D. \nTARGET_APPLY – Data and data definition language (DDL) statements are applied to the target database.\nLOGGER_SEVERITY_DETAILED_DEBUG – All information is written to the log.","poster":"khun","upvote_count":"4","timestamp":"1671519480.0","comment_id":"750582"}],"url":"https://www.examtopics.com/discussions/amazon/view/89489-exam-aws-certified-database-specialty-topic-1-question-282/","topic":"1","question_text":"A database administrator is working on transferring data from an on-premises Oracle instance to an Amazon RDS for Oracle DB instance through an AWS Database Migration Service (AWS DMS) task with ongoing replication only. The database administrator noticed that the migration task failed after running successfully for some time. The logs indicate that there was generic error. The database administrator wants to know which data definition language (DDL) statement caused this issue.\n\nWhat should the database administrator do to identify' this issue in the MOST operationally efficient manner?","answer":"D","question_images":[]},{"id":"J7azVPHsvJJY8AYgnQHz","answer_description":"","answer_images":[],"isMC":true,"topic":"1","discussion":[{"comment_id":"884052","upvote_count":"5","poster":"clarksu","timestamp":"1682745840.0","content":"Selected Answer: A\nabout the concerns about heterogeneous for SCT:\nPer: \nhttps://aws.amazon.com/dms/schema-conversion-tool/\nSCT would be used during the migration between:\nSrc : PostgreSQL Des : Aurora MySQL, Aurora PostgreSQL, MySQL, PostgreSQL\n\nA is the best practice\nC downtime,transfer, reload,verify ... too much workloads"},{"poster":"Sathish_dbs","comment_id":"1028109","timestamp":"1696779120.0","upvote_count":"1","content":"AWS encourages to use AWS services and it might expect you also to answer inline with that so will go with A as there is no proper difference between A and C"},{"upvote_count":"2","poster":"roymunson","comment_id":"1027832","timestamp":"1696755300.0","content":"Selected Answer: C\nWe can have downtime in this scenario. In the first link is mentioned that if you have more than 100 GB aws the pg_dump and pg_restore function MAY not be suitable for your usecase, but if time is not the problem here it could be suitable. The second link points out the current bp and there is nothing mentioned about DMS. I'm realy not sure about this one but I would go with c here.\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-full-load-pd_dump.html\n\nBest Practice:\nhttps://aws.amazon.com/blogs/database/best-practices-for-migrating-postgresql-databases-to-amazon-rds-and-amazon-aurora/"},{"poster":"DanShone","upvote_count":"1","comment_id":"1007709","timestamp":"1694699760.0","content":"Selected Answer: A\nOnly option here is A\nC. cannot be used - pg_dump and pg_restore is only for DBs 100 GB or less\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-full-load.html"},{"poster":"Pranava_GCP","timestamp":"1693914660.0","upvote_count":"1","comment_id":"999446","content":"Selected Answer: A\nA. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS)"},{"upvote_count":"1","comment_id":"967505","content":"I will pick A. Per linke, https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-full-load-publisher.html, 100G is the threshhold. If it is less than 100G, C works fine. But in this question, the database size is 200G. So I will go with A.","poster":"Windy","timestamp":"1690750020.0"},{"comment_id":"908856","content":"NOT A- This is a homogeneous migration . SCT is required only for Heterogeneous migration. \nNOT D- Can't crate RR using On-prem PG as source\nNOT B - Irrelevant \n'C' is the answer - Easy, Error free and fast migration method","timestamp":"1685305560.0","poster":"manig","upvote_count":"1"},{"comments":[{"timestamp":"1685109900.0","comment_id":"907422","content":"\"There is no need for schema conversion since it is a homogeneous migration\" >>> this alone means that you have never used DMS.","poster":"cnmc","upvote_count":"1"}],"timestamp":"1684994520.0","comment_id":"906426","upvote_count":"1","content":"Selected Answer: C\nA. It is quite a bit of work to set up. There is no need for schema conversion since it is a homogeneous migration.\nB. What?\nC. If you need to create secondary database objects, then pg_dump and pg_restore is the most appropriate option. However, this option incurs a performance tradeoff compared to other options.\nD. Creating Aurora read-replicas from self-managed databases is not supported.\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-summary.html","poster":"aviathor"},{"comments":[],"comment_id":"889552","content":"Ans: C\n- Nothing said about downtime\n- 200Gb is not so big size","upvote_count":"1","timestamp":"1683222600.0","poster":"yyy"},{"comment_id":"873197","comments":[{"poster":"cnmc","timestamp":"1685109960.0","content":"SCT isn't just for conversion... It's also for schema copy, which DMS needs. SCT + DMS is almost always a guaranteed combo.","comment_id":"907426","upvote_count":"1"}],"poster":"SeemaDataReader","upvote_count":"1","timestamp":"1681777200.0","content":"Selected Answer: C\nDont need SCT as the migration is not heterogenous.(PostgreSQL to PostgreSQL)"},{"poster":"Mintwater","content":"A.\nCustomers looking to migrate self-managed PostgreSQL databases to Amazon RDS for PostgreSQL or Aurora PostgreSQL, can use one of the three main approaches.\n\nUse a native or third-party database migration method such as pg_dump and pg_restore for full load only migrations.\n\nUse a managed service such as AWS Database Migration Service (AWS DMS) for full load and ongoing replication.\n\nUse a native tool for full load and a managed AWS DMS service for ongoing replication. We call this strategy the hybrid approach.\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql.html","comments":[{"comment_id":"874203","comments":[{"content":"I also selected A initially but I think C is the correct answer.\nBecause here the issue is secondary database objects, those would not be migrated properlyu using A.","comment_id":"926481","comments":[{"content":"https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql-summary.html#:~:text=pg_dump%20and%20pg_restore%20is%20the%20most%20appropriate%20option","timestamp":"1687070820.0","upvote_count":"1","comment_id":"926483","poster":"MrAliMohsan"}],"timestamp":"1687070760.0","poster":"MrAliMohsan","upvote_count":"1"}],"upvote_count":"2","poster":"Mintwater","timestamp":"1681866060.0","content":"A — no downtime once use the replication for ongoing data change\nC — downtime"}],"upvote_count":"3","comment_id":"860497","timestamp":"1680567600.0"},{"upvote_count":"2","poster":"backbencher2022","timestamp":"1680210660.0","content":"Selected Answer: C\nOption C is the best option here. Option A could have been correct if Schema conversion was not mentioned. Option B is not the most efficient way and option D is not applicable as Aurora doesn't support on-prem database source for creating read replica.","comment_id":"856277"}],"question_id":205,"answer_ET":"A","timestamp":"2023-03-30 23:11:00","question_text":"A company is migrating its 200 GB on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The original database columns include NOT NULL and foreign key constraints. A database administrator needs to complete the migration while following best practices for database migrations.\n\nWhich option meets these requirements to migrate the database to AWS?","exam_id":22,"answer":"A","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/104510-exam-aws-certified-database-specialty-topic-1-question-283/","answers_community":["A (54%)","C (46%)"],"unix_timestamp":1680210660,"choices":{"C":"Use the PostgreSQL tools pg_dump and pg_restore to migrate to the Aurora PostgreSQL DB cluster.","D":"Create an Aurora PostgreSQL read replica and promote the read replica to become primary once it is synchronized.","B":"Create an AWS Lambda function to connect to the source database and load the data into the target Aurora PostgreSQL DB cluster.","A":"Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to migrate the database to an Aurora PostgreSQL DB cluster."}}],"exam":{"isMCOnly":false,"lastUpdated":"11 Apr 2025","id":22,"isBeta":false,"provider":"Amazon","isImplemented":true,"numberOfQuestions":359,"name":"AWS Certified Database - Specialty"},"currentPage":41},"__N_SSP":true}