{"pageProps":{"questions":[{"id":"KtFwGWbbHgZofHcJQ8dR","topic":"1","choices":{"D":"Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region","A":"Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.","C":"Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.","B":"Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region."},"answer_ET":"C","isMC":true,"timestamp":"2023-02-14 13:19:00","answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/99150-exam-aws-certified-solutions-architect-professional-sap-c02/","answers_community":["C (97%)","3%"],"discussion":[{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html\nHard to believe a product from AWS can be designed in such an amateur way.","comments":[{"comment_id":"1341905","content":"It is unbelievable for such a solution. In particular, it happens in the company like AWS","timestamp":"1737070560.0","poster":"GabrielShiao","upvote_count":"1"}],"timestamp":"1702505400.0","comment_id":"1095864","poster":"bjexamprep","upvote_count":"12"},{"comment_id":"1315206","poster":"nimbus_00","content":"Selected Answer: C\nYeah...Deprecating CodeCommit was the right decision!","timestamp":"1732100640.0","upvote_count":"1"},{"comments":[{"poster":"AWSum1","upvote_count":"1","content":"C is correct","timestamp":"1728406860.0","comment_id":"1294798"}],"comment_id":"1294797","content":"AWS Backup does not support AWS CodeCommit directly.","timestamp":"1728406860.0","poster":"AWSum1","upvote_count":"2"},{"poster":"career360guru","timestamp":"1700892420.0","upvote_count":"2","comment_id":"1079771","content":"Selected Answer: C\nOption C"},{"timestamp":"1700205960.0","upvote_count":"3","comment_id":"1073078","poster":"severlight","content":"Selected Answer: C\nyes, AWS Backup cannot do this for you, so you should use Code Build to clone repo and upload zip to s3"},{"timestamp":"1688488560.0","poster":"NikkyDicky","comment_id":"943001","content":"Selected Answer: C\nits a C","upvote_count":"1"},{"comments":[{"timestamp":"1687281180.0","poster":"easytoo","content":"b in incorrect as AWS Backup does not backup code commit as a source.","upvote_count":"3","comment_id":"928632"}],"upvote_count":"1","comment_id":"928628","timestamp":"1687281120.0","poster":"easytoo","content":"b-b-b-b-b-b-b-b"},{"content":"Answer : C\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html","upvote_count":"4","timestamp":"1685293740.0","poster":"Roontha","comment_id":"908762"},{"poster":"mfsec","timestamp":"1679916300.0","upvote_count":"2","comment_id":"852048","content":"Selected Answer: C\nC for sure"},{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html","comment_id":"840547","upvote_count":"1","timestamp":"1678940760.0","poster":"God_Is_Love"},{"poster":"kiran15789","timestamp":"1677443880.0","content":"Selected Answer: C\nhttps://www.automat-it.com/post/backup-aws-codecommit","upvote_count":"3","comment_id":"822940"},{"comment_id":"812431","timestamp":"1676671020.0","content":"Selected Answer: C\nC is correct, AWS Backup does not backup code commit as a source.","poster":"c73bf38","upvote_count":"2"},{"content":"Selected Answer: C\nB is wrong > AWS Backup does not support CodeCommit as source.\nA is out.\nC is right.","upvote_count":"2","comment_id":"812184","timestamp":"1676656380.0","poster":"lunt"},{"timestamp":"1676500200.0","upvote_count":"2","poster":"Musk","comment_id":"810046","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html"},{"upvote_count":"1","content":"Selected Answer: B\nIt says backup so I think B is the answer:\n\nB. Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.","comment_id":"809767","timestamp":"1676480100.0","comments":[{"comment_id":"812411","timestamp":"1676670000.0","upvote_count":"2","content":"Changing to C, thanks.","poster":"c73bf38"}],"poster":"c73bf38"},{"timestamp":"1676377140.0","poster":"spd","upvote_count":"4","comment_id":"808358","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-code-in-multiple-aws-regions-using-aws-codepipeline-aws-codecommit-and-aws-codebuild.html\n\nhttps://medium.com/geekculture/replicate-aws-codecommit-repositories-between-regions-using-codebuild-and-codepipeline-39f6b8fcefd2"}],"answer_description":"","exam_id":33,"question_images":[],"answer_images":[],"question_text":"A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.\n\nWhich solution will meet these requirements?","unix_timestamp":1676377140,"question_id":81},{"id":"04A4PGYLCkdAWmDbcG2Z","unix_timestamp":1676377200,"answer":"C","answer_images":[],"timestamp":"2023-02-14 13:20:00","question_text":"A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several VPCs that have CIDR ranges that overlap. The company’s marketing team has created a new internal application and wants to make the application accessible to all the other business units. The solution must use private IP addresses only.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","topic":"1","answer_description":"","choices":{"D":"Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB. Activate IAM authorization for the API. Grant access to the accounts of the other business units.","B":"Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC. Create an AWS Site-to-Site VPN connection between the marketing team and each business unit's VPC. Perform NAT where necessary.","A":"Instruct each business unit to add a unique secondary CIDR range to the business unit's VPC. Peer the VPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team.","C":"Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses."},"exam_id":33,"answer_ET":"C","isMC":true,"question_id":82,"answers_community":["C (95%)","5%"],"discussion":[{"timestamp":"1676377200.0","comment_id":"808359","content":"Selected Answer: C\nPrivate link is the solution for IP Overlapping and Securely access the app between accounts","upvote_count":"15","poster":"spd"},{"timestamp":"1676480040.0","content":"Selected Answer: C\nWith AWS PrivateLink, the marketing team can create an endpoint service to share their internal application with other accounts securely using private IP addresses. They can grant permission to specific AWS accounts to connect to the service and create interface VPC endpoints in the other accounts to access the application by using private IP addresses. This option does not require any changes to the network of the other business units, and it does not require peering or NATing. This solution is both scalable and secure.","comment_id":"809765","upvote_count":"11","poster":"c73bf38"},{"content":"Selected Answer: B\n\"LEAST OPERATIONAL OVERHEAD\" - is key word in a question. Its not so easy to migrate any on-premise infra to any AWS. Looking at the answers here I see no one eve done that before and just answering as from AWS docs.\nThe easiest way to migrate any on-premise infra - ec2","comments":[{"timestamp":"1724650500.0","upvote_count":"1","content":"just C","poster":"helloworldabc","comment_id":"1272445"},{"upvote_count":"1","timestamp":"1710744240.0","content":"who mentioned migration?!","poster":"StevePace","comment_id":"1176308"}],"comment_id":"1101796","timestamp":"1703093760.0","poster":"alexsanteeno","upvote_count":"1"},{"poster":"honoga4853","timestamp":"1703093700.0","upvote_count":"1","content":"Selected Answer: B\n\"LEAST OPERATIONAL OVERHEAD\" - is key word in a question. Its not so easy to migrate any on-premise infra to any AWS. Looking at the answers here I see no one eve done that before and just answering as from AWS docs.\nThe easiest way to migrate any on-premise infra - ec2","comment_id":"1101794","comments":[]},{"content":"Selected Answer: C\nOption C","timestamp":"1700892600.0","upvote_count":"1","comment_id":"1079773","poster":"career360guru"},{"content":"Selected Answer: C\nC for sure","poster":"NikkyDicky","upvote_count":"1","timestamp":"1688488620.0","comment_id":"943002"},{"poster":"Alabi","timestamp":"1687471020.0","comment_id":"931079","upvote_count":"2","content":"Selected Answer: C\nThe solution that will meet the requirements with the least operational overhead is:\n\nC. Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application using private IP addresses.\n\nAWS PrivateLink provides secure and scalable private connectivity between VPCs, AWS services, and on-premises applications, without using public IP addresses. In this case, you can create an AWS PrivateLink endpoint service for the marketing application, which allows other business units to access the application using private IP addresses.\n\nBy granting permission to specific AWS accounts to connect to the PrivateLink endpoint service, you can control access to the marketing application. Then, in each business unit's VPC, you can create interface VPC endpoints to connect to the PrivateLink service, allowing them to access the marketing application privately."},{"upvote_count":"1","comment_id":"852049","timestamp":"1679916360.0","content":"Selected Answer: C\nPrivate link","poster":"mfsec"},{"content":"Selected Answer: C\nNetworking & Content Delivery blog -\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/","upvote_count":"5","poster":"God_Is_Love","timestamp":"1678944240.0","comment_id":"840568"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/99151-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"N340Z2nQZpc7UfHQcXKN","choices":{"B":"Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type “Access Analyzer Finding” with a filter for “isPublic: true.” Select the SNS topic as the EventBridge rule target.","A":"Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications.","C":"Create an Amazon EventBridge rule for the event type “Bucket-Level API Call via CloudTrail” with a filter for “PutBucketPolicy.” Select the SNS topic as the EventBridge rule target.","D":"Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type “Config Rules Re-evaluation Status” with a filter for “NON_COMPLIANT.” Select the SNS topic as the EventBridge rule target."},"question_images":[],"answer":"B","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/99152-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-02-14 13:26:00","unix_timestamp":1676377560,"isMC":true,"answer_images":[],"discussion":[{"poster":"dkx","comment_id":"942196","content":"A. No, because Amazon S3 can NOT currently publish notifications for isPublic events.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\n\nB. Yes, because IAM Access Analyzer for S3 alerts you to S3 buckets that are configured to allow access to anyone on the internet or other AWS accounts\nhttps://aws.amazon.com/blogs/security/how-to-prioritize-iam-access-analyzer-findings/\n\nC. No, because PutBucketPolicy notifies us of an Amazon S3 bucket policy event to an Amazon S3 bucket, and we are looking for a SPECIFIC event to the bucket permissions, not ALL events.\n\nD. No, because cloudtrail-s3-dataevents-enabled checks if at least one AWS CloudTrail trail is logging Amazon Simple Storage Service (Amazon S3) data events for all S3 buckets.\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-s3-dataevents-enabled.html","timestamp":"1704320040.0","upvote_count":"13"},{"upvote_count":"12","comment_id":"841530","comments":[{"timestamp":"1694911140.0","upvote_count":"8","poster":"God_Is_Love","content":"Click on the \"Create rule\" button.\n\nEnter a name for the rule and a brief description, if desired.\n\nUnder \"Define pattern\", select \"Event pattern\".\n\nSelect \"Custom pattern\".\n\nIn the \"Event pattern\" field, enter the following code:\n\n{\n\"source\": [\"aws.securityhub\"],\n\"detail-type\": [\"Access Analyzer Finding\"],\n\"detail\": {\n\"findings\": [\n{\n\"isPublic\": [\ntrue\n]\n}\n]\n}\n}\n\nThis code will match all Access Analyzer Finding events where the \"isPublic\" field is set to \"true\".","comment_id":"841533"}],"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-analyzer.html","poster":"God_Is_Love","timestamp":"1694910960.0"},{"timestamp":"1722536700.0","upvote_count":"1","poster":"AimarLeo","comment_id":"1137898","content":"This question.. is seriously ! a googling one"},{"timestamp":"1710253680.0","comment_id":"1005743","poster":"dkcloudguru","upvote_count":"1","content":"Option B"},{"upvote_count":"2","poster":"NikkyDicky","content":"Selected Answer: B\nit's B","comment_id":"943134","timestamp":"1704405600.0"},{"upvote_count":"1","content":"Selected Answer: B\nIdeally, I would use config rule, but here, of course, they suggest the wrong rule. The other option remains the access analyzer","comment_id":"932158","poster":"Maria2023","timestamp":"1703399220.0"},{"upvote_count":"2","poster":"SkyZeroZx","comment_id":"926924","content":"Selected Answer: B\nkeyword = AWS Identity and Access Management Access Analyzer\nthen B","timestamp":"1702942860.0"},{"timestamp":"1700896440.0","poster":"leehjworking","upvote_count":"1","comment_id":"906390","content":"Selected Answer: B\nThe code by God_is_love did not worked for me. I guess something has been changed.\nThe following code worked in my environment.\n{\n\"source\":[\"aws.access-analyzer\"],\n\"detail-type\":[\"Access Analyzer Finding\"],\n\"detail\":\n{\n\"isPublic\":[true]\n}\n}"},{"upvote_count":"1","content":"Selected Answer: B\nAws is letter B \n\nPrevious writing is a error","comment_id":"906149","poster":"SkyZeroZx","timestamp":"1700865840.0"},{"content":"Letter C","comments":[{"upvote_count":"2","timestamp":"1700865780.0","poster":"SkyZeroZx","comment_id":"906147","content":"Solution D will not meet the requirements because it will notify the data security team whenever an S3 bucket is not compliant with the cloudtrail-s3-dataevents-enabled rule, even if the bucket is not publicly exposed. The cloudtrail-s3-dataevents-enabled rule checks if at least one AWS CloudTrail trail is logging Amazon Simple Storage Service (Amazon S3) data events for all S3 buckets. If a bucket is not compliant with this rule, it does not mean that the bucket is publicly exposed. The bucket may simply not be logging S3 data events.","comments":[{"poster":"SkyZeroZx","upvote_count":"1","content":"Here are some reasons why an S3 bucket may not be logging S3 data events:\n\nThe bucket may not have a CloudTrail trail associated with it.\nThe CloudTrail trail for the bucket may not be enabled.\nThe CloudTrail trail for the bucket may not be configured to log S3 data events.\nIf the data security team is only interested in being notified when an S3 bucket becomes publicly exposed, then solution D is not the best solution. Solution B is a better solution because it will only notify the data security team when an S3 bucket becomes publicly exposed.","comment_id":"906148","timestamp":"1700865780.0"}]}],"upvote_count":"1","poster":"SkyZeroZx","timestamp":"1700865780.0","comment_id":"906146"},{"timestamp":"1700619120.0","comment_id":"903589","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer-eventbridge.html","poster":"y0eri","upvote_count":"1"},{"poster":"mfsec","content":"Selected Answer: B\nB eventbirdge and access analyser","comment_id":"852051","timestamp":"1695814080.0","upvote_count":"2"},{"upvote_count":"7","content":"Selected Answer: B\nB is the correct solution because it uses AWS Identity and Access Management Access Analyzer to continuously monitor access control configurations and detect whether any S3 buckets have been configured to be publicly accessible. When a publicly accessible bucket is detected, an Amazon EventBridge rule is triggered, and the SNS topic is notified with the finding.","comment_id":"814731","poster":"c73bf38","timestamp":"1692490020.0"},{"timestamp":"1692165780.0","poster":"masssa","upvote_count":"2","content":"Selected Answer: B\nAccess Analyzer is to assess the access policy.\nhttps://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/access-control-block-public-access.html","comment_id":"810413"},{"poster":"[Removed]","upvote_count":"2","timestamp":"1692115380.0","comment_id":"809824","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-iam-access-analyzer-api-to-automate-detection-of-public-access-to-aws-kms-keys/"},{"upvote_count":"2","poster":"mdijoux25","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-analyzer.html","timestamp":"1692105900.0","comment_id":"809683"},{"content":"Selected Answer: D\nD by elimination rule","comments":[{"poster":"Jay_2pt0_1","timestamp":"1699271820.0","upvote_count":"1","comment_id":"890660","content":"I thought D, as well, but it seems everyone else things Access Analyzer."}],"comment_id":"808366","upvote_count":"2","poster":"spd","timestamp":"1692008760.0"}],"answers_community":["B (95%)","5%"],"question_text":"A company needs to audit the security posture of a newly acquired AWS account. The company’s data security team requires a notification only when an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data security team's email address subscribed.\n\nWhich solution will meet these requirements?","topic":"1","answer_description":"","question_id":83,"exam_id":33},{"id":"4h3xwBHqhe6dOzPaqnS7","timestamp":"2023-02-14 13:47:00","isMC":true,"exam_id":33,"question_images":[],"answers_community":["C (97%)","3%"],"discussion":[{"content":"Selected Answer: C\nFirst need to evaluate","timestamp":"1676378820.0","upvote_count":"16","poster":"spd","comment_id":"808382"},{"poster":"c73bf38","comment_id":"809762","upvote_count":"7","content":"Selected Answer: C\nC. Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies.","timestamp":"1676479740.0"},{"content":"Selected Answer: C\nOption C","upvote_count":"1","timestamp":"1700893020.0","poster":"career360guru","comment_id":"1079776"},{"upvote_count":"1","content":"Selected Answer: C\nC for sure","timestamp":"1688500920.0","poster":"NikkyDicky","comment_id":"943136"},{"comment_id":"908720","timestamp":"1685289180.0","content":"Answer : C\nhttps://aws.amazon.com/migration-evaluator/","poster":"Roontha","upvote_count":"2"},{"content":"Selected Answer: B\nThe emphasis is on applications. \"Some applications are batch processes that run at the end of each month\" \nI do not understand why C is better than B","timestamp":"1685231520.0","comment_id":"908222","upvote_count":"1","poster":"F_Eldin","comments":[{"comment_id":"1272449","upvote_count":"1","content":"just C","timestamp":"1724650680.0","poster":"helloworldabc"}]},{"content":"Selected Answer: C\nUse migration evaluator","timestamp":"1679916720.0","comment_id":"852052","poster":"mfsec","upvote_count":"3"}],"answer_description":"","answer_ET":"C","answer_images":[],"question_id":84,"url":"https://www.examtopics.com/discussions/amazon/view/99153-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"D":"Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources.","A":"Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies.","B":"Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies.","C":"Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies."},"topic":"1","answer":"C","unix_timestamp":1676378820,"question_text":"A solutions architect needs to assess a newly acquired company’s portfolio of applications and databases. The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is not well documented. The solutions architect cannot immediately determine how many applications and databases exist. Traffic for the applications is variable. Some applications are batch processes that run at the end of each month.\n\nThe solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.\n\nWhich solution will meet these requirements?"},{"id":"DgZEkk0XwVRAwRyUDdaJ","question_id":85,"discussion":[{"upvote_count":"11","comment_id":"809760","content":"Selected Answer: A\nExplanation: Amazon EFS provides shared file storage that is highly available and durable. It is an ideal solution to share files between containers running on multiple instances in a cluster. Mounting an Amazon EFS file system on each subnet provides a shared file system for multiple instances running in different Availability Zones. Additionally, AWS Backup provides automated backup and recovery of Amazon EFS file systems.","poster":"c73bf38","timestamp":"1676479680.0"},{"content":"Selected Answer: A\nEFS = Fastest storage performance compare to S3/EBS","upvote_count":"7","poster":"spd","comments":[{"upvote_count":"1","timestamp":"1676535540.0","content":"I vote B.\nI think EBS is faster than S3/EBS.\nhttps://www.msp360.com/resources/blog/amazon-s3-vs-ebs-vs-efs/","comment_id":"810426","comments":[{"upvote_count":"1","poster":"helloworldabc","comment_id":"1272450","content":"just A","timestamp":"1724650800.0"},{"upvote_count":"2","comments":[{"timestamp":"1676566740.0","poster":"Musk","content":"I just read the question refers to multiple AZs, so B is not an option.","upvote_count":"9","comments":[{"upvote_count":"1","content":"I missed this too 👏 good spot","poster":"AWSum1","timestamp":"1728407340.0","comment_id":"1294804"}],"comment_id":"810937"}],"content":"typo.\nEBS faster than S3/EFS.","comment_id":"810427","timestamp":"1676535600.0","poster":"masssa"}],"poster":"masssa"}],"comment_id":"808389","timestamp":"1676379240.0"},{"content":"Selected Answer: A\nOption A","upvote_count":"1","timestamp":"1700893740.0","poster":"career360guru","comment_id":"1079779"},{"comment_id":"1059914","timestamp":"1698856980.0","upvote_count":"4","content":"Selected Answer: A\nA: sounds valid\nB: EBS multi attach can only do same AZ -> out \nC: S3 is for durability, not for perfomance\nD: can drop when seeing third party tool.","poster":"joleneinthebackyard"},{"comment_id":"943137","poster":"NikkyDicky","upvote_count":"2","timestamp":"1688501040.0","content":"Selected Answer: A\nA - EFS for multi-AZ"},{"timestamp":"1688412300.0","poster":"dkx","comment_id":"942170","upvote_count":"4","content":"A. Yes, because Amazon EFS offers you the choice of creating file systems using Standard or One Zone storage classes. Standard storage classes store data with and across multiple AZs.\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/run-stateful-workloads-with-persistent-data-storage-by-using-amazon-efs-on-amazon-eks-with-aws-fargate.html\n\nB. No, because Amazon EBS Multi-Attach enabled volumes can be attached to up to 16 Linux instances built on the Nitro System that are in the same Availability Zone. We need to solve for \"nodes in multiple Availability Zones\"\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\n\nC. No, because if you’re looking to run file-based applications that need to collaborate or coordinate on shared data across instances or users, AWS recommends fully managed file services, such as Amazon FSx or Amazon Elastic File System (EFS).\n\nD. No, because the company needs to back up the files, not backup the EKS Cluster."},{"content":"Selected Answer: A\nA for sure","upvote_count":"2","comment_id":"852106","poster":"mfsec","timestamp":"1679921160.0"},{"upvote_count":"3","poster":"ramyaram","content":"Selected Answer: A\nKeyword here is multiple small files and shared between multiple clusters","timestamp":"1679849520.0","comment_id":"851281"},{"content":"Selected Answer: A\nIn the past, EBS can be attached only to one ec2 instance but not anymore but there are limitations like - it works only on io1/io2 instance types and many others as described here. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\nEFS has shareable storage \n\nIn terms of performance, Amazon EFS is optimized for workloads that require high levels of aggregate throughput and IOPS, whereas EBS is optimized for low-latency, random access I/O operations. Amazon EFS is designed to scale throughput and capacity automatically as your storage needs grow, while EBS volumes can be resized on demand.","timestamp":"1679024820.0","upvote_count":"3","poster":"God_Is_Love","comment_id":"841555"},{"upvote_count":"1","comment_id":"833838","content":"I support A since their is a multi-AZ requirement.\n\nhttps://repost.aws/questions/QUK2RANw1QTKCwpDUwCCI72A/efs-vs-ebs-mult-attach\n\nEFS is also designed for high availability and high durability. To achieve these levels of availability and durability, EFS automatically replicates data within and across 3 Availability Zones, with no single points of failure. EBS multi-attach volumes can be used for clients within a single Availability Zone.","poster":"Zek","timestamp":"1678357380.0"},{"timestamp":"1678106700.0","upvote_count":"3","content":"Selected Answer: A\nWhen you have an EKS cluster and use the EBS that is local to the node, only Pods running on that node have access to the storage. If the node starts on any other Pod, it will potentially break. There are ways to fix this, but they are beyond this question. I believe we need shared fast storage here, so it should be S3 vs EFS the decision.","comment_id":"830798","poster":"Sarutobi"},{"content":"I've been reding here and there, and B does not seem that feasible, although if supported it would be faster than A.","upvote_count":"2","poster":"Musk","timestamp":"1676566680.0","comment_id":"810936"}],"choices":{"B":"Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year.","D":"Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year.","C":"Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year.","A":"Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year."},"isMC":true,"question_images":[],"answer_ET":"A","topic":"1","exam_id":33,"answers_community":["A (100%)"],"answer":"A","question_text":"A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The company needs to back up the files and retain the backups for 1 year.\n\nWhich solution will meet these requirements while providing the FASTEST storage performance?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/99154-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2023-02-14 13:54:00","answer_description":"","unix_timestamp":1676379240}],"exam":{"id":33,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true,"isMCOnly":true,"numberOfQuestions":529,"provider":"Amazon","lastUpdated":"11 Apr 2025"},"currentPage":17},"__N_SSP":true}