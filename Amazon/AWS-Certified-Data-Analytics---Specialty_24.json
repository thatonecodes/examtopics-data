{"pageProps":{"questions":[{"id":"eDEinyKLwZtpnBeFxhwx","question_text":"A media company wants to perform machine learning and analytics on the data residing in its Amazon S3 data lake. There are two data transformation requirements that will enable the consumers within the company to create reports:\n✑ Daily transformations of 300 GB of data with different file formats landing in Amazon S3 at a scheduled time.\n✑ One-time transformations of terabytes of archived data residing in the S3 data lake.\nWhich combination of solutions cost-effectively meets the company's requirements for transforming the data? (Choose three.)","isMC":true,"unix_timestamp":1596975840,"answer_ET":"ADE","topic":"1","discussion":[{"comment_id":"153590","timestamp":"1632119340.0","content":"To me, ADE.\nNot B. Athena will use Glue (option A)\nNot C. Its an antipattern to use Redshift to do transformations.\nNot F. Would pick EMR instead of Sagemaker to do one time transformations","upvote_count":"46","poster":"testtaker3434","comments":[{"upvote_count":"3","poster":"awssp12345","content":"Agreed","comment_id":"168751","timestamp":"1632506280.0"}]},{"upvote_count":"9","timestamp":"1632304920.0","poster":"zeronine","comment_id":"160817","content":"My answer is ADE."},{"poster":"pk349","upvote_count":"1","comment_id":"886362","content":"ADE: I passed the test","timestamp":"1682950080.0"},{"timestamp":"1667749500.0","content":"Selected Answer: ADE\nCorrect answers are A, D & E\n\nOptions A & D using Glue Crawler and Glue Workflows would provide ETL for daily transactions.\n\nOption E as EMR can help perform data transformation for archived data.\n\nOption B is wrong as Athena does not identify the schema but uses Glue Catalog.\n\nOption C is wrong as Redshift would need to be persistent and does not provide a cost-effective solution as compared to Glue.\n\nOption F is wrong as Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. It does not provide ETL capability on large data.","poster":"cloudlearnerhere","comment_id":"712422","comments":[{"timestamp":"1669546380.0","upvote_count":"1","poster":"sevensquare","content":"What about SageMaker Data Wrangler?","comment_id":"728140"}],"upvote_count":"6"},{"poster":"Arka_01","timestamp":"1664091660.0","content":"Selected Answer: ADE\ncost-effectively solution is required. So, A, D and E.","upvote_count":"1","comment_id":"678538"},{"comment_id":"643528","poster":"rocky48","content":"Selected Answer: ADE\nSelected Answer: ADE","timestamp":"1659818640.0","upvote_count":"1"},{"poster":"girish123456","comment_id":"635587","timestamp":"1658584860.0","upvote_count":"2","content":"Selected Answer: ADE\nA: For schema and new partition of data for Incremental load\nD: Incremental transformation\nE: Historical data migration using EMR"},{"content":"sorry, for the 41 upvotes. Ans A can't be it. Athena doesn't scan and identify schema. Athena use the Glue Data Catalog, which is generated by Glue Crawler.\n\nMy answer: A,D,E.","comment_id":"622057","timestamp":"1656146700.0","upvote_count":"2","poster":"GiveMeEz"},{"upvote_count":"1","poster":"aws2019","timestamp":"1637786160.0","content":"ADE is ans","comment_id":"486243"},{"upvote_count":"5","content":"A, D, E is the right answer","poster":"lostsoul07","timestamp":"1635778140.0","comment_id":"280758"},{"content":"Yep ADE also.\nI guess SageMaker will use the data more than 1 time for learning processes","comment_id":"238271","timestamp":"1635674700.0","poster":"Draco31","upvote_count":"3"},{"content":"100% ADE","upvote_count":"5","comment_id":"205232","poster":"sanjaym","timestamp":"1634647500.0"},{"poster":"syu31svc","content":"Notice that the answers given are paired so if you were to break it down:\nIdentify schema --> Glue\nTransformations --> Glue Jobs\nArchived TBs worth of data --> EMR\nSo is ADE","upvote_count":"5","timestamp":"1634413980.0","comment_id":"191894"},{"comment_id":"175589","timestamp":"1633563900.0","content":"A, D and E.","poster":"Paitan","upvote_count":"5"},{"comment_id":"175534","upvote_count":"1","comments":[{"comment_id":"622058","poster":"GiveMeEz","content":"glue can. you can properly size the glue cluster for the glue job with one simple dial.","upvote_count":"1","timestamp":"1656146820.0"},{"comment_id":"176448","timestamp":"1633978020.0","comments":[{"poster":"omar_bahrain","content":"Thanks for describing the origion of EMR","timestamp":"1635491040.0","comment_id":"233733","upvote_count":"1"}],"poster":"Phoenyx89","content":"Absolutely! Glue can handle same amount of data as EMR because in the end Glue is a simplified EMR cluster with Spark, HDFS, YARN and the Glue dependencies but have the advantage of being serverless. Configuring the appropriate amount and type of DPUs you can handle 300GB of data","upvote_count":"8"}],"content":"can glue handle 300GB data every day? It seems too much for glue.","timestamp":"1632582840.0","poster":"manish9363"}],"question_images":[],"answer_description":"","exam_id":20,"question_id":116,"timestamp":"2020-08-09 14:24:00","answer":"ADE","choices":{"F":"For archived data, use Amazon SageMaker to perform data transformations.","A":"For daily incoming data, use AWS Glue crawlers to scan and identify the schema.","E":"For archived data, use Amazon EMR to perform data transformations.","C":"For daily incoming data, use Amazon Redshift to perform transformations.","B":"For daily incoming data, use Amazon Athena to scan and identify the schema.","D":"For daily incoming data, use AWS Glue workflows with AWS Glue jobs to perform transformations."},"url":"https://www.examtopics.com/discussions/amazon/view/27709-exam-aws-certified-data-analytics-specialty-topic-1-question/","answers_community":["ADE (100%)"],"answer_images":[]},{"id":"BA690bArElO5XzuQyVUR","choices":{"D":"Ingest the data using Amazon Kinesis Data Firehose to write the data to Amazon S3. Implement a transformation AWS Lambda function that parses the sensor data to remove all PHI.","A":"Ingest the data using Amazon Kinesis Data Streams, which invokes an AWS Lambda function using Kinesis Client Library (KCL) to remove all PHI. Write the data in Amazon S3.","B":"Ingest the data using Amazon Kinesis Data Firehose to write the data to Amazon S3. Have Amazon S3 trigger an AWS Lambda function that parses the sensor data to remove all PHI in Amazon S3.","C":"Ingest the data using Amazon Kinesis Data Streams to write the data to Amazon S3. Have the data stream launch an AWS Lambda function that parses the sensor data and removes all PHI in Amazon S3."},"question_id":117,"answers_community":["D (100%)"],"answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/28830-exam-aws-certified-data-analytics-specialty-topic-1-question/","discussion":[{"poster":"Prodip","timestamp":"1632297300.0","content":"D; transformation AWS Lambda function applied with stream data; before loading to s3","comment_id":"161084","upvote_count":"37","comments":[{"timestamp":"1632601920.0","upvote_count":"2","content":"Changed to D","poster":"carol1522","comment_id":"162464"},{"upvote_count":"2","poster":"awssp12345","timestamp":"1632690000.0","comment_id":"168755","content":"Agreed"},{"poster":"certificationJunkie","content":"where does it say 'before' in option D ?","comment_id":"606704","upvote_count":"2","timestamp":"1653395280.0"},{"comments":[{"timestamp":"1646620560.0","poster":"cnmc","upvote_count":"5","content":"Because B removes the PHI *after* it is stored into S3. The question asks that PHI is removed from \"streaming data\", and it is also better practice to remove sensitive info before reaching storage","comment_id":"562366"}],"content":"Why is it not B?","timestamp":"1643353560.0","comment_id":"534493","upvote_count":"1","poster":"Ipc01"}]},{"upvote_count":"1","timestamp":"1682950140.0","comment_id":"886365","poster":"pk349","content":"D: I passed the test"},{"content":"D as IOT rules can send info to Firehose using an action https://docs.aws.amazon.com/firehose/latest/dev/writing-with-iot.html","poster":"nadavw","timestamp":"1669544520.0","upvote_count":"1","comment_id":"728128"},{"content":"Selected Answer: D\nCorrect answer is D as Kinesis Data Firehose can be used for data ingestion and storage to S3 with Lambda function for data filtering and transformation. This solution involves the least operational overhead.\n\nOption A is wrong as Kinesis Data Streams and KCLs involve operational overhead as Data Streams need to be provisioned and maintained.\n\nOption B is wrong as the solution removes the PHI after the data is stored.\n\nOption C is wrong as Kinesis Data Streams does not integrate with S3 directly and involves the operational overhead as Data Streams need to be provisioned and maintained.","comment_id":"713040","poster":"cloudlearnerhere","upvote_count":"3","timestamp":"1667825640.0"},{"content":"Selected Answer: D\nAns is D. \nSolution need least operational overhead, so kinesis data stream is out. option B is also out bcoz it is removing PHI data after putting the data in S3. \nSo Option D is correct. Firehose will do the transformation via lambda to filter out the PHI data from stream and store the non-PHI in S3.","comment_id":"707200","timestamp":"1667048280.0","poster":"thirukudil","upvote_count":"2"},{"content":"Selected Answer: D\nAWS Kinesis Data Firehose is required as destination is S3. Also, Lambda function should be called as a transformation from Firehose before sending data to S3.","upvote_count":"1","comment_id":"678540","timestamp":"1664091780.0","poster":"Arka_01"},{"timestamp":"1658885340.0","poster":"rocky48","comment_id":"637725","content":"Selected Answer: D\nSelected Answer: D","upvote_count":"1"},{"comment_id":"604832","timestamp":"1653130920.0","poster":"Bik000","upvote_count":"2","content":"Selected Answer: D\nMy Answer is D"},{"timestamp":"1636118220.0","content":"Answer D.\nIts true that the answer wordings are bad and confusing.\nKinesis Data Firehose can invoke your Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream.","comment_id":"387161","poster":"Donell","upvote_count":"2"},{"comment_id":"354764","content":"ANSWER D:\nEXPLAINATION:With the keyword 'near-real-time',option A and C are filtered out as KDS is real time streaming .NOw between option B & D ,We already have transformation lambda attached to 'Firehose' by default to do the necessary transformation .","timestamp":"1635993300.0","poster":"Heer","upvote_count":"2"},{"poster":"DerekKey","timestamp":"1635903780.0","upvote_count":"1","content":"Correct D\nKDF uses transformation (Lambda) before writing to S3\nIncorrect A\nImplementing Lambda to process and write data to S3 with streams is crazy. You should use KDF as KDS consumer. Shuld be least operational overhead.\nIncorrect B, C - security of data","comment_id":"350889"},{"comment_id":"349693","upvote_count":"1","timestamp":"1635845880.0","poster":"AjithkumarSL","content":"Going with D, Even the reference in the answer also pointing the same (Reference:\nhttps://aws.amazon.com/blogs/big-data/persist-streaming-data-to-amazon-s3-using-amazon-kinesis-firehose-and-aws-lambda/)"},{"poster":"lostsoul07","comment_id":"274349","timestamp":"1635630120.0","upvote_count":"2","content":"D is the right answer"},{"timestamp":"1635322740.0","poster":"kempstonjoystick","upvote_count":"1","content":"Badly worded answer for D, but that's the correct answer here.","comment_id":"267035"},{"timestamp":"1635089940.0","poster":"Roontha","upvote_count":"1","content":"Answer : D\n\nRefer : https://aws.amazon.com/blogs/compute/amazon-kinesis-firehose-data-transformation-with-aws-lambda/#:~:text=Introducing%20Firehose%20Data%20Transformations&text=When%20you%20enable%20Firehose%20data,then%20delivered%20to%20the%20destination","comment_id":"263603"},{"comment_id":"244518","timestamp":"1634132160.0","content":"I think D means that the data is cleaned already before writing to s3, but the formulation is bad.","poster":"hans1234","upvote_count":"2","comments":[{"comment_id":"254837","timestamp":"1634832960.0","content":"Agree, formulation is confusing. I guess it means lambda transformation is applied before wrting to S3, so D is right.","upvote_count":"1","poster":"Manue"}]},{"upvote_count":"3","timestamp":"1633661460.0","content":"Answer is D.\nIt's near real-time so no need to use KDS\nA - task can be handled much easier (less complicated) way by D then A.\nB - PHI data is written to S3 before removal which is not acceptable.\nC - KDS cannot write data to S3.","comment_id":"205240","poster":"sanjaym","comments":[{"upvote_count":"1","content":"Agree, D","comment_id":"214552","timestamp":"1633691040.0","poster":"LMax"},{"poster":"Roontha","comment_id":"245201","timestamp":"1634322060.0","upvote_count":"1","content":"Hi Sanjay\n have you completed data analytic professional exam recently"},{"poster":"ricksun","content":"disagree, I'll go A since firehose natually integrated with lambda.","timestamp":"1634450340.0","comment_id":"247149","upvote_count":"2"}]},{"upvote_count":"2","comment_id":"191897","comments":[{"content":"With firehose, you don't need S3 to trigger Lambda function.\nPlease refer to https://aws.amazon.com/tw/blogs/compute/amazon-kinesis-firehose-data-transformation-with-aws-lambda/\n\"When you enable Firehose data transformation, Firehose buffers incoming data and invokes the specified Lambda function with each buffered batch asynchronously. The transformed data is sent from Lambda to Firehose for buffering and then delivered to the destination.\"","comment_id":"238922","timestamp":"1633908480.0","upvote_count":"1","poster":"liyungho"}],"content":"Near real time so Firehose needed; A and C are eliminated\nYou can trigger Lambda from S3\n\"You can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted\"\n(https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html)\nSo answer is B","timestamp":"1633403640.0","poster":"syu31svc"},{"upvote_count":"2","comments":[{"upvote_count":"2","content":"D; transformation AWS Lambda function applied to the Firhore not S3","timestamp":"1633378680.0","poster":"helloyeops","comment_id":"187436","comments":[{"comment_id":"196485","poster":"vicks316","content":"Agreed, changing to D.","upvote_count":"2","timestamp":"1633445820.0"}]}],"poster":"vicks316","comment_id":"180776","content":"B. You need an event notification to trigger lambda\nCheck out link https://aws.amazon.com/blogs/compute/amazon-kinesis-firehose-data-transformation-with-aws-lambda\nIt has a line that says \"customers deliver data to an intermediate destination, such as a S3 bucket, and use S3 event notification to trigger a Lambda function to perform the transformation before delivering it to the final destination.\"","timestamp":"1633212300.0"},{"timestamp":"1632722520.0","comment_id":"175533","poster":"manish9363","content":" An S3 Bucket that is used to store the De-Identified records.\n A Firehose Delivery Stream and associated IAM Role used to buffer and collect the DeIdentified records compressed in a zip file and stored in the S3 Bucket\n An AWS Lambda Function that performs the De-Identification of the incoming\nmessages by removing the PHI/PII Data. The function also stores the PHI / PII Data into\nDynamoDB along with PatientID for cross-reference. The PHI / PII data are encrypted\nusing AWS KMS Keys.\n An AWS Lambda Function that does hospital Device Simulation for the use case. The\nLambda function uses generates sensor simulation data and publishes to IoT MQTT\nTopic\n A DynamoDB table that stores encrypted cross-reference data Patient ID, Timestamp,\nPatient Name and Patient Date of Birth","upvote_count":"1"},{"timestamp":"1632497460.0","comment_id":"162424","poster":"Prodip","content":"https://aws.amazon.com/blogs/compute/amazon-kinesis-firehose-data-transformation-with-aws-lambda/#:~:text=Introducing%20Firehose%20Data%20Transformations&text=When%20you%20enable%20Firehose%20data,then%20delivered%20to%20the%20destination.","upvote_count":"1"},{"poster":"zeronine","upvote_count":"4","timestamp":"1632296760.0","content":"I think B. Any thoughts?","comment_id":"160821","comments":[{"timestamp":"1632327060.0","upvote_count":"3","content":"Changing my answer to D","comment_id":"161467","poster":"zeronine"}]},{"timestamp":"1632187260.0","content":"A\nBCD all data landed to S3 without wipe out sensitive data so dropped.","poster":"zanhsieh","comments":[{"timestamp":"1632448140.0","poster":"carol1522","upvote_count":"1","content":"Agree with A","comment_id":"161776"},{"comment_id":"175594","poster":"Paitan","timestamp":"1632767580.0","upvote_count":"4","content":"Kinesis Data Stream cannot write to S3. So option D is the right choice as using transformation we can remove sensitive data before writing to S3.","comments":[{"upvote_count":"3","comment_id":"222815","timestamp":"1633703340.0","content":"It is the lambda function which writes to S3, so to me A is correct","poster":"tleflond"}]},{"comment_id":"175795","poster":"Jh2501","content":"Shouldn't go for Kinesis Data Stream - it says \"near-real-time\"...","upvote_count":"1","timestamp":"1632943080.0"}],"upvote_count":"2","comment_id":"159891"}],"answer_description":"","answer_ET":"D","question_text":"A hospital uses wearable medical sensor devices to collect data from patients. The hospital is architecting a near-real-time solution that can ingest the data securely at scale. The solution should also be able to remove the patient's protected health information (PHI) from the streaming data and store the data in durable storage.\nWhich solution meets these requirements with the least operational overhead?","answer":"D","topic":"1","question_images":[],"exam_id":20,"timestamp":"2020-08-17 12:37:00","unix_timestamp":1597660620},{"id":"FHR2VqxxLGAocfpQqtpW","question_images":[],"answers_community":["C (100%)"],"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/28897-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1","discussion":[{"content":"I think the answer is C\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-elasticmapreduce-cluster.html","upvote_count":"30","comment_id":"161469","comments":[{"upvote_count":"2","comments":[{"timestamp":"1635420480.0","poster":"LMax","content":"me too","comment_id":"214555","upvote_count":"2"}],"poster":"carol1522","content":"Agree with c","comment_id":"161779","timestamp":"1633054020.0"},{"comment_id":"168769","upvote_count":"3","content":"Agreed","timestamp":"1633057260.0","poster":"awssp12345"},{"timestamp":"1640059380.0","comment_id":"505809","content":"Agree C.\nIf you are using an Amazon EMR version earlier than 5.24.0, an encrypted EBS root device volume is supported only when using a custom AMI.\nFor Amazon EMR version 5.24.0 and later, you can use a security configuration option to encrypt EBS root device and storage volumes when you specify AWS KMS as your key provider. \nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-localdisk","upvote_count":"4","poster":"lakediver"}],"timestamp":"1632865800.0","poster":"zeronine"},{"poster":"Huy","upvote_count":"7","comment_id":"389228","timestamp":"1636273020.0","content":"Agree with C. D is a trap, it is Security Configuration section not bootstrap action in Configuration section."},{"upvote_count":"2","poster":"pk349","content":"C: I passed the test","comment_id":"886367","timestamp":"1682950260.0"},{"upvote_count":"1","timestamp":"1669702860.0","poster":"Ryo0w0o","content":"I will go for D.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html#emr-encryption-localdisk\n\nAccording to the link, we can use EBS encryption from a security configuration and it says \"We recommend using EBS encryption\".","comment_id":"730008"},{"content":"Selected Answer: C\nCorrect answer is C as CloudFormation can be used to launch an EMR cluster with custom AMI with encrypted root device volumes.\n\nOption A is wrong as open source Hadoop would not be provisioned using CloudFormation.\n\nOption B is wrong as TLS does not provide data at rest encryption.\n\nOption D is wrong as bootstrap actions cannot be used to encrypt root device volume.","comments":[{"poster":"Naku","content":"bro, can you tell if we just do first 80 questions , can we pass?","comment_id":"758769","timestamp":"1672157760.0","upvote_count":"5"}],"upvote_count":"2","poster":"cloudlearnerhere","timestamp":"1667825760.0","comment_id":"713041"},{"poster":"Arka_01","upvote_count":"2","comment_id":"678543","content":"Selected Answer: C\n\"without changing the underlying code\" and \"CloudFomation Template\" are the keys here. So CustomAMIID for including a Custom AMI with encrypted root volume will work.","timestamp":"1664091900.0"},{"content":"Selected Answer: C\nC is the right answer.","comment_id":"634921","timestamp":"1658451840.0","upvote_count":"2","poster":"rocky48"},{"poster":"jrheen","timestamp":"1651353360.0","comment_id":"595300","upvote_count":"1","content":"Answer - C"},{"poster":"lakediver","content":"If you are using an Amazon EMR version earlier than 5.24.0, an encrypted EBS root device volume is supported only when using a custom AMI. For more information, see Creating a custom AMI with an encrypted Amazon EBS root device volume in the Amazon EMR Management Guide\nBeginning with Amazon EMR version 5.24.0, you can use a security configuration option to encrypt EBS root device and storage volumes when you specify AWS KMS as your key provider. For more information, see Local disk encryption.","comment_id":"505807","timestamp":"1640059260.0","upvote_count":"2"},{"timestamp":"1637380740.0","comment_id":"482251","content":"Agree with c","upvote_count":"1","poster":"aws2019"},{"timestamp":"1636120620.0","content":"C is the right answer","upvote_count":"3","poster":"lostsoul07","comment_id":"274352"},{"timestamp":"1635978480.0","comment_id":"220267","content":"C sounds right but where in CF can you define a CustomAmild? Its imageID and that's it. An AMI is an AMI. For D to work, you would have to use a 3rd party software, but it would work","comments":[{"content":"Scratch that, you can do CustomAmiID in an EMR cluster..... C is indeed the answer.","poster":"[Removed]","timestamp":"1636075380.0","comment_id":"220318","upvote_count":"1"}],"upvote_count":"2","poster":"[Removed]"},{"timestamp":"1635348960.0","upvote_count":"2","poster":"jove","content":"C is correct","comment_id":"209008"},{"poster":"sanjaym","timestamp":"1634956680.0","comment_id":"205241","content":"Sensing answer should be C.","upvote_count":"1"},{"content":"Its C, you cant use bootstrap action to encrypt the root volume, you need to pass it using security configurations.","poster":"jack42","timestamp":"1634167320.0","comment_id":"194999","upvote_count":"2"},{"timestamp":"1633784640.0","comment_id":"191908","poster":"syu31svc","content":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-root-volume-property/\nAnswer is C","upvote_count":"3"},{"timestamp":"1633620480.0","poster":"Paitan","comments":[{"content":"I think the bootstrap config is only for installing additional softwares https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-elasticmapreduce-cluster-bootstrapactionconfig.html","comment_id":"179259","poster":"KoMo","timestamp":"1633736580.0","upvote_count":"2"}],"upvote_count":"1","comment_id":"175597","content":"Confused between C and D."},{"comment_id":"160353","timestamp":"1632490620.0","upvote_count":"1","content":"is it D?","poster":"paul0099"}],"isMC":true,"exam_id":20,"question_id":118,"answer_description":"","answer":"C","answer_images":[],"timestamp":"2020-08-17 22:52:00","choices":{"C":"Create a custom AMI with encrypted root device volumes. Configure Amazon EMR to use the custom AMI using the CustomAmild property in the CloudFormation template.","D":"Use a CloudFormation template to launch an EMR cluster. In the configuration section of the cluster, define a bootstrap action to encrypt the root device volume of every node.","B":"Use a CloudFormation template to launch an EMR cluster. In the configuration section of the cluster, define a bootstrap action to enable TLS.","A":"Install open-source Hadoop on Amazon EC2 instances with encrypted root device volumes. Configure the cluster in the CloudFormation template."},"question_text":"A company is migrating its existing on-premises ETL jobs to Amazon EMR. The code consists of a series of jobs written in Java. The company needs to reduce overhead for the system administrators without changing the underlying code. Due to the sensitivity of the data, compliance requires that the company use root device volume encryption on all nodes in the cluster. Corporate standards require that environments be provisioned though AWS CloudFormation when possible.\nWhich solution satisfies these requirements?","unix_timestamp":1597697520},{"id":"MKSuoTOrAWUniYjgSdlu","exam_id":20,"choices":{"B":"Use Amazon EMR to convert each .csv file to Apache Avro. COPY the files into Amazon Redshift and query the file with Athena from Amazon S3.","D":"Use AWS Glue to convert the files from .csv to Apache Parquet to create 20 Parquet files. COPY the files into Amazon Redshift and query the files with Athena from Amazon S3.","C":"Use AWS Glue to convert the files from .csv to a single large Apache ORC file. COPY the file into Amazon Redshift and query the file with Athena from Amazon S3.","A":"Use AWS Glue to convert all the files from .csv to a single large Apache Parquet file. COPY the file into Amazon Redshift and query the file with Athena from Amazon S3."},"discussion":[{"timestamp":"1633989720.0","content":"D, is the good answer. In fact each nodes have 2 slices so ideally we can parrelize the copy process by sending a multiple of 20.","upvote_count":"27","comment_id":"166007","comments":[{"timestamp":"1635462540.0","comment_id":"214557","poster":"LMax","upvote_count":"3","content":"D for sure"}],"poster":"ali_baba_acs"},{"poster":"Paitan","timestamp":"1634077740.0","upvote_count":"8","comments":[{"upvote_count":"1","poster":"roymunson","timestamp":"1700219460.0","content":"And why it is a trick question? IMO it's an obv hint.","comment_id":"1073237"}],"content":"Trick question. Since we have 10 nodes with 2 slices each, ideally a multiple of 20 files should help in the parallelize the Copy process. So D is the right answer.","comment_id":"175600"},{"content":"D: I passed the test","timestamp":"1682950320.0","comment_id":"886368","poster":"pk349","upvote_count":"1"},{"comment_id":"713045","timestamp":"1667825880.0","content":"Selected Answer: D\nCorrect answer is D as AWS Glue can be used to combine the .csv files to 20. parquet files. This would allow even processing across 2 slices. Also, multiple files help rapid loading of data to Redshift.\n\nOptions A & C are wrong as single large file is not efficient as it would use only single slice.\n\n\nOption B is wrong as using Glue would be more cost effective as compared to EMR.","comments":[{"content":"I agree with your response - however EMR is more cost effective than Glue.. Glue is serverless while EMR is just a managed service.","timestamp":"1683417360.0","upvote_count":"1","comment_id":"891075","poster":"bill1214"}],"upvote_count":"4","poster":"cloudlearnerhere"},{"content":"Selected Answer: D\nOption D is perfect solution to achieve both the requirements - to reduce the cost of querying (when we query on parquet files, lower the amount of data would be scanned which in turn reduce the cost ) and also improve the speed of data loading into the Amazon\nRedshift cluster(Split large files wherever possible to a number equal to a multiple of total number of slices. So here 20*n would be the correct splitting of the large data file.).","poster":"thirukudil","upvote_count":"1","comment_id":"707220","timestamp":"1667050860.0"},{"content":"Selected Answer: D\nBest practices to Copy data from S3 to Redshift - \n1) Use Columnar data format. Which is Parquet/ORC.\n2) Split large files wherever possible to a number equal to a multiple of total number of slices. So here 20*n would be the correct splitting of the large data file.","timestamp":"1664092080.0","comment_id":"678546","upvote_count":"3","poster":"Arka_01"},{"content":"Selected Answer: D\nAns is D","upvote_count":"1","poster":"Hruday","timestamp":"1661030880.0","comment_id":"649540"},{"poster":"rocky48","timestamp":"1658985000.0","upvote_count":"1","content":"Selected Answer: D\nSelected Answer: D","comment_id":"638468"},{"upvote_count":"1","poster":"Ramshizzle","content":"Selected Answer: D\nD is the right answer. 20 files = one per slice. If you use COPY on a dataset all the files will be divided over the available nodes/slices.","comment_id":"620301","timestamp":"1655891160.0"},{"content":"Per the link https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-use-multiple-files.html , the answer shall be D, for the compressed files multiple copy commands for its slices adhere parallel load whereas for the delimited file a single copy command works better. Here in all the options the file compression is taking place , so option D seems the best choice here.","comment_id":"602164","timestamp":"1652628960.0","upvote_count":"1","poster":"somenath"},{"content":"Not sure D is correct or not but only reason i choose D since A and C are almost same and B won't work","comment_id":"503813","upvote_count":"1","timestamp":"1639762080.0","poster":"arun004"},{"upvote_count":"1","comment_id":"486248","content":"D is the right answer","timestamp":"1637787120.0","poster":"aws2019"},{"timestamp":"1636234380.0","content":"D is the answer, but I doubt the total time will be shorter.The load will be quicker, shure, but it’s not as if Spark reads CSV files quicker in any way, so all that you get is overhead. The Athena queries will run faster on fewer files, though, and if that wasthe focus this question would have made sense.","upvote_count":"1","comment_id":"440933","poster":"iconara"},{"upvote_count":"1","poster":"afantict","timestamp":"1635792900.0","comment_id":"353348","content":"Is Athena query cheaper than the existing redshift query?"},{"timestamp":"1635738780.0","comment_id":"274353","poster":"lostsoul07","content":"D is the right answer","upvote_count":"3"},{"upvote_count":"2","timestamp":"1635174600.0","poster":"sanjaym","comment_id":"205243","content":"D is correct answer."},{"content":"is D ok? In understand you should avoid multiple concurrency copy commands\n\"We strongly recommend using the COPY command to load large amounts of data. Using individual INSERT statements to populate a table might be prohibitively slow. Alternatively, if your data already exists in other Amazon Redshift database tables, use INSERT INTO ... SELECT or CREATE TABLE AS to improve performance. For information, see INSERT or CREATE TABLE AS\".\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_Loading_tables_with_the_COPY_command.html","upvote_count":"1","poster":"apuredol","comments":[{"timestamp":"1648295040.0","poster":"CHRIS12722222","upvote_count":"2","content":"Single COPY command loads multiple files into Redshift in parallel","comment_id":"575508"}],"timestamp":"1634778660.0","comment_id":"200234"}],"url":"https://www.examtopics.com/discussions/amazon/view/29562-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_images":[],"topic":"1","question_text":"A transportation company uses IoT sensors attached to trucks to collect vehicle data for its global delivery fleet. The company currently sends the sensor data in small .csv files to Amazon S3. The files are then loaded into a 10-node Amazon Redshift cluster with two slices per node and queried using both Amazon Athena and Amazon Redshift. The company wants to optimize the files to reduce the cost of querying and also improve the speed of data loading into the Amazon\nRedshift cluster.\nWhich solution meets these requirements?","answer_description":"","answer":"D","timestamp":"2020-08-25 15:29:00","question_images":[],"question_id":119,"isMC":true,"answers_community":["D (100%)"],"answer_ET":"D","unix_timestamp":1598362140},{"id":"zuQRTrDa0cr6VxKVpJRm","question_text":"An online retail company with millions of users around the globe wants to improve its ecommerce analytics capabilities. Currently, clickstream data is uploaded directly to Amazon S3 as compressed files. Several times each day, an application running on Amazon EC2 processes the data and makes search options and reports available for visualization by editors and marketers. The company wants to make website clicks and aggregated data available to editors and marketers in minutes to enable them to connect with users more effectively.\nWhich options will help meet these requirements in the MOST efficient way? (Choose two.)","answer":"AD","unix_timestamp":1650450300,"question_id":120,"exam_id":20,"answer_images":[],"choices":{"E":"Upload clickstream records from Amazon S3 to Amazon Kinesis Data Streams and use a Kinesis Data Streams consumer to send records to Amazon OpenSearch Service (Amazon Elasticsearch Service).","D":"Use OpenSearch Dashboards (Kibana) to aggregate, filter, and visualize the data stored in Amazon OpenSearch Service (Amazon Elasticsearch Service). Refresh content performance dashboards in near-real time.","C":"Use Amazon OpenSearch Service (Amazon Elasticsearch Service) deployed on Amazon EC2 to aggregate, filter, and process the data. Refresh content performance dashboards in near-real time.","A":"Use Amazon Kinesis Data Firehose to upload compressed and batched clickstream records to Amazon OpenSearch Service (Amazon Elasticsearch Service).","B":"Upload clickstream records to Amazon S3 as compressed files. Then use AWS Lambda to send data to Amazon OpenSearch Service (Amazon Elasticsearch Service) from Amazon S3."},"answer_description":"","timestamp":"2022-04-20 12:25:00","isMC":true,"question_images":[],"discussion":[{"poster":"astalavista1","content":"Selected Answer: AD\nOpenSearch can ingest from KDF and results not in real-time but in minutes, as such, KDF can still be used.","upvote_count":"10","timestamp":"1650547680.0","comment_id":"589423"},{"content":"AD for sure.","comment_id":"961393","upvote_count":"1","poster":"penguins2","timestamp":"1690190700.0"},{"poster":"pk349","timestamp":"1682950380.0","comment_id":"886370","upvote_count":"3","content":"AD: I passed the test"},{"upvote_count":"4","content":"Selected Answer: AD\nCorrect answers are A & D as Kinesis Data Firehose can be used for data ingestion with its micro batching to push the data directly to Elastic Search. Kibana can be used for visualization.\n\nOptions B, C & E are wrong as they are not the MOST efficient way.","timestamp":"1667826000.0","comment_id":"713047","poster":"cloudlearnerhere"},{"timestamp":"1667051880.0","upvote_count":"3","content":"Selected Answer: AD\nKDF can ingest that data directly to Opensearch ( Still KDF has 60 sec buffer time, they want results in minutes. so this is fine). Choose opensearch dashboards for visualization when it comes to time-sensitive.\n\nhere, some people choosing B over A. My explanation why we can ignore B is - they want result in minutes not real-time. So KDF is sufficient and will replace the whole process of streaming the data to s3 and then loading the data to OpenSearch via lambda.","poster":"thirukudil","comment_id":"707234"},{"content":"Selected Answer: AD\n\"in minutes\" - this is the key here. Always choose OpenSearch over Athena+Quicksight, in case of time sensitivity.","comment_id":"678551","poster":"Arka_01","upvote_count":"2","timestamp":"1664092260.0"},{"timestamp":"1660566360.0","comment_id":"647175","poster":"rrshah83","content":"Selected Answer: BD\nB and D. \n(Reason for B as opposed to A: Firehose has minimum 1 min delay. Lambda will be \"instantly\". Question )","upvote_count":"2"},{"upvote_count":"1","comment_id":"637091","content":"Selected Answer: AD\nSelected Answer: AD","timestamp":"1658805360.0","poster":"rocky48"},{"upvote_count":"1","content":"Selected Answer: AD\nAD IS THE ANSWER","timestamp":"1654463340.0","poster":"Balki","comment_id":"612027"},{"upvote_count":"3","timestamp":"1653204780.0","content":"Selected Answer: AD\nAnswer should be A & D","poster":"Bik000","comment_id":"605235"},{"comments":[],"comment_id":"605092","upvote_count":"2","content":"B and D. AWS Lambda can be triggered for s3 events generated while copying to s3. And then it loads to OpenSearch in near real time. Then use Kibana for visualization and search requirement.","timestamp":"1653186960.0","poster":"certificationJunkie"},{"timestamp":"1653130140.0","content":"Selected Answer: DE\nMy Answer is D & E","upvote_count":"1","poster":"Bik000","comments":[],"comment_id":"604821"},{"upvote_count":"1","comment_id":"595335","content":"Answer - A,D","timestamp":"1651358100.0","poster":"jrheen"},{"timestamp":"1650470640.0","comment_id":"588834","upvote_count":"1","poster":"G_C_P","content":"A, D most efficient and direct to destination. B instead of A feasible but uses S3. But qn also says data stored in S3 - assume for the best approach to A, D"},{"poster":"G_C_P","upvote_count":"2","content":"A, D - firehose direct to ES; Kibana built in","timestamp":"1650450300.0","comment_id":"588559"}],"url":"https://www.examtopics.com/discussions/amazon/view/73879-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_ET":"AD","topic":"1","answers_community":["AD (89%)","7%"]}],"exam":{"isImplemented":true,"numberOfQuestions":164,"provider":"Amazon","id":20,"isMCOnly":true,"isBeta":false,"name":"AWS Certified Data Analytics - Specialty","lastUpdated":"11 Apr 2025"},"currentPage":24},"__N_SSP":true}