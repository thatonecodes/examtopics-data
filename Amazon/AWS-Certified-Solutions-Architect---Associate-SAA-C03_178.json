{"pageProps":{"questions":[{"id":"CeDRpi9uhQXFtsB5Ot5K","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/139619-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"choices":{"B":"Configure the application to use an Amazon ElastiCache cluster in front of the Aurora PostgreSQL DB cluster.","E":"Configure an Amazon AP! Gateway REST API with a usage plan.","C":"Update the application to send the purchase requests to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an Auto Scaling group of new EC2 instances that read from the SQS queue.","A":"Configure an Auto Scaling group of new EC2 instances to retry the purchases until the processing is complete. Update the applications to connect to the DB cluster by using Amazon RDS Proxy.","D":"Configure an AWS Lambda function to retry the ticket purchases until the processing is complete."},"answers_community":["AC (50%)","BC (38%)","12%"],"question_id":886,"discussion":[{"comment_id":"1204502","upvote_count":"8","poster":"sandordini","content":"Selected Answer: AC\nA) uses RDS Proxy which is mainly for connection pooling and availability issues. Proxy is for too many connections(, not for performance: read replicas, caching) \nB is caching which is designed for solving read-issues. (Here we have timeouts, and connection issues.)\nC: SQS is good method for decoupling.","timestamp":"1714474140.0"},{"comment_id":"1202332","timestamp":"1714095480.0","content":"Selected Answer: BC\ni think it's BC","poster":"Abdullah_Cloud","upvote_count":"5"},{"timestamp":"1742896800.0","content":"Selected Answer: BC\napplication timeouts... not database connection timeout","comment_id":"1409986","poster":"tch","upvote_count":"1"},{"upvote_count":"1","timestamp":"1741582080.0","poster":"nadeerm","content":"Selected Answer: BC\nAmazon ElastiCache (Option B): ElastiCache can cache frequently accessed data, reducing the load on the Aurora PostgreSQL database. This improves read performance and reduces latency, which helps handle peak traffic more efficiently.\n\nAmazon SQS with Auto Scaling (Option C): By sending purchase requests to an SQS queue, the application can decouple the front-end from the back-end processing. An Auto Scaling group of EC2 instances can process messages from the queue, ensuring that the system scales dynamically based on demand. This approach ensures that purchase requests are processed asynchronously, reducing the likelihood of timeouts during peak usage.","comment_id":"1377576"},{"poster":"ieffiong","upvote_count":"1","timestamp":"1740915720.0","content":"Selected Answer: BC\nCombining A&C just leads to redundant auto scaling groups especially when you have SQS configured already. The question talks about combination of actions.","comment_id":"1363920"},{"comments":[{"comment_id":"1356408","content":"Even though your RDS proxy approach is correct, by choosing \"AC\" you are configuring two ASGs: one to process the purchase (option C) and one for... Retry the purchase (?) (option A) This is not cost effective\n\nIn my opinion, designing architectures specifically to \"retry\" transactions (options AD), specially with timeout issues, is something that everyone should avoid, as it can easily degeneres into the \"snowball\" effect","upvote_count":"1","poster":"GOTJ","timestamp":"1739523240.0"}],"content":"Selected Answer: AC\nA - RDS Proxy improves scalability and database connection pooling.\nB - Only helpful for read-heavy workloads. This question indicates \"purchase transactions\" (writes), so the main bottleneck is still there.\nC - SQS allows for better handling of bursts in traffic by queueing requests (which is also true for so many questions before #879).\nD - Cost-prohibitive at scale due to high invocation rates. And lambda is not suitable for long-running operations like database transactions.\nE - Irrelevant.","poster":"LeonSauveterre","upvote_count":"2","timestamp":"1735778880.0","comment_id":"1335324"},{"poster":"EllenLiu","content":"Selected Answer: AB\nwhy not choose B? C is wired design.","upvote_count":"1","timestamp":"1735035960.0","comment_id":"1331081"},{"upvote_count":"2","poster":"Cpso","content":"Selected Answer: AB\nAs a developer choose both A&C is wired? while we use SQS solution to buffer request before handle by EC2 fleet. why we need EC2 in front of SQS from A.? what of thier propose.","comment_id":"1320952","timestamp":"1733144460.0"},{"timestamp":"1728981240.0","content":"BC.\nNot idea why retry helps in this scenario besides it adds more complexity into the current design and also doesn't resolve the avalibility issue...","upvote_count":"3","poster":"XXXXXlNN","comment_id":"1298128"},{"timestamp":"1728810180.0","poster":"b3b5fdd","upvote_count":"1","content":"Selected Answer: BC\nB and C!","comment_id":"1296816"},{"comment_id":"1287185","content":"Selected Answer: BC\nA - simply pointless. \n\nB- You're already using SQS (C), so why using ec2 to \"retry the purchase\"? They will stay in the queue until the purchase is processed. Otherwise, they will simply return to the queue.\n\nC - This decouples the application from direct database calls, allowing the processing of purchase requests to scale independently and manage load more effectively.","upvote_count":"1","poster":"MatAlves","timestamp":"1726901940.0"},{"content":"when we have SQS in option C why do you have to retry it again \n\nI think the answer is B and C","comment_id":"1266719","upvote_count":"4","poster":"pujithacg8","timestamp":"1723759620.0"},{"upvote_count":"3","comment_id":"1231119","poster":"EdricHoang","timestamp":"1718483040.0","content":"Selected Answer: AC\nCombine SQS and auto-scaling EC2:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html"}],"question_images":[],"answer_ET":"AC","unix_timestamp":1714095480,"answer":"AC","answer_description":"","answer_images":[],"timestamp":"2024-04-26 03:38:00","topic":"1","question_text":"A company runs an ecommerce application on AWS. Amazon EC2 instances process purchases and store the purchase details in an Amazon Aurora PostgreSQL DB cluster.\n\nCustomers are experiencing application timeouts during times of peak usage. A solutions architect needs to rearchitect the application so that the application can scale to meet peak usage demands.\n\nWhich combination of actions will meet these requirements MOST cost-effectively? (Choose two.)"},{"id":"nrLifurJEykD7Ydask2S","url":"https://www.examtopics.com/discussions/amazon/view/85738-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2022-10-17 17:37:00","answer":"A","discussion":[{"timestamp":"1666645440.0","poster":"Six_Fingered_Jose","content":"Selected Answer: B\nthis question is too vague imho\nif the question is looking for a way to incur charges to the European company instead of the US company, then requester pay makes sense.\n\nif they are looking to reduce overall data transfer cost, then B makes sense because the data does not leave the AWS network, thus data transfer cost should be lower technically?\n\nA. makes sense because the US company saves money, but the European company is paying for the charges so there is no overall saving in cost when you look at the big picture\n\nI will go for B because they are not explicitly stating that they want the other company to pay for the charges","comment_id":"703358","comments":[{"content":"When you use S3 Cross-Region Replication (CRR), the source bucket owner (in this case, the company sharing the data) is responsible for paying the data replication costs, including:\n\nData transfer fees: The cost of transferring data from the source S3 bucket to the destination bucket in a different AWS Region.\nReplication requests: Charges for each PUT request made during replication to the destination bucket.\nB minimizes the data transfer cost of the requester, the Europe company. A minimize the data transfer cost of survey company.","poster":"FlyingHawk","comment_id":"1346361","upvote_count":"1","timestamp":"1737786420.0"},{"content":"The question mentions that the consumer survey company has granted the marketing firm access to the S3 bucket. This suggests that the consumer survey company is the one managing the bucket and likely wants to minimize its own costs.","poster":"FlyingHawk","comment_id":"1346363","timestamp":"1737786720.0","upvote_count":"1"},{"comment_id":"1325083","content":"Don't you still have to pay for cross region transfer? Yes it's lower than out-of-aws but I think there's still a cost betw regions. Maybe it's just certain services but I thought S3 was one of them.","timestamp":"1733931180.0","poster":"tvtvtv","upvote_count":"1"},{"poster":"tonybuivannghia","upvote_count":"4","content":"I disagree with your opinion, because S3 Cross-Region is only transfer new data to new region, the old data can't. So A is correct.","timestamp":"1726931580.0","comment_id":"1287378"}],"upvote_count":"72"},{"comment_id":"698244","upvote_count":"50","poster":"123jhl0","content":"Selected Answer: A\n\"Typically, you configure buckets to be Requester Pays buckets when you want to share data but not incur charges associated with others accessing the data. For example, you might use Requester Pays buckets when making available large datasets, such as zip code directories, reference data, geospatial information, or web crawling data.\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html","timestamp":"1666096920.0"},{"poster":"dattateja8","upvote_count":"1","content":"Selected Answer: A\nThe Requester Pays feature in Amazon S3 allows the company to charge the marketing firm for the data transfer costs when they access the company's S3 bucket. By enabling this feature, the marketing firm will be responsible for the data transfer costs when they access the data, rather than the company. This ensures that the company does not incur high data transfer costs when sharing large datasets with an external party, especially across regions.","timestamp":"1743008160.0","comment_id":"1410486"},{"content":"Selected Answer: A\n• In general, bucket owners pay for all\nAmazon S3 storage and data transfer\ncosts associated with their bucket\n• With Requester Pays buckets, the\nrequester instead of the bucket owner\npays the cost of the request and the\ndata download from the bucket","upvote_count":"1","comment_id":"1387318","timestamp":"1741679640.0","poster":"mc0226"},{"content":"Selected Answer: A\nThe question is clear and answer too. undoubtedly A - one can have \"Requester pays\" feature on a S3 bucket! My retention isn't great but I remember this from the S3 lesson!\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html","timestamp":"1738176840.0","upvote_count":"1","comment_id":"1348713","poster":"Dharmarajan"},{"timestamp":"1737952740.0","content":"Selected Answer: A\nB is not right because we do not know the usage pattern by the marketing company. copying whole data may cost much more","upvote_count":"1","poster":"zdi561","comment_id":"1347230"},{"comment_id":"1346356","poster":"FlyingHawk","timestamp":"1737785640.0","upvote_count":"1","content":"Selected Answer: A\nThe survey company wants to share its data transfer costs as low as possible, to replicate the data to the Europe company's account, there will still be some data transfer costs for survey company to pay, so it will be better to request the Europe's market company to pay.","comments":[{"comment_id":"1346358","timestamp":"1737785760.0","poster":"FlyingHawk","content":"Another reason to select A is the A can scale in the case of more than one company want their survey dataset. B cannot scale.","upvote_count":"1"},{"timestamp":"1737786300.0","content":"For B, When you use S3 Cross-Region Replication (CRR), the source bucket owner (in this case, the company sharing the data) is responsible for paying the data replication costs, including:\n\nData transfer fees: The cost of transferring data from the source S3 bucket to the destination bucket in a different AWS Region.\nReplication requests: Charges for each PUT request made during replication to the destination bucket.","comment_id":"1346360","upvote_count":"1","poster":"FlyingHawk"}]},{"upvote_count":"1","timestamp":"1733995200.0","content":"Selected Answer: B\nAnswer is B, cross region replication.","poster":"chirag_a_parikh","comment_id":"1325504"},{"content":"Selected Answer: A\nOption A. When you use S3 Cross-Region Replication (CRR), you will incur data transfer costs. These costs include a fee for transferring data between regions, which is approximately $0.02 per GB.","comment_id":"1318163","poster":"Balliache520505","upvote_count":"1","timestamp":"1732637580.0"},{"content":"Selected Answer: A\nWhy Not the Other Options?\nB. S3 Cross-Region Replication:\nCross-Region Replication (CRR) copies data between S3 buckets in different AWS Regions but incurs data transfer costs for replication, which does not minimize costs for the survey company.\nIt is also unnecessary if the marketing firm can directly access the bucket.\nC. Cross-Account Access:\nGranting cross-account access allows the marketing firm to read data directly, but the data transfer costs would still be borne by the survey company, which contradicts the goal of minimizing the company’s costs.\nD. S3 Intelligent-Tiering:\nS3 Intelligent-Tiering optimizes storage costs for infrequently accessed data but does not address the need to minimize data transfer costs during sharing. Syncing the bucket to the marketing firm’s bucket would also incur transfer costs.","poster":"[Removed]","upvote_count":"1","timestamp":"1732593120.0","comment_id":"1317878"},{"upvote_count":"1","poster":"0de7d1b","content":"Selected Answer: A\nThe Requester Pays feature is the most cost-effective solution for this scenario, as it shifts the data transfer costs to the marketing firm while keeping the data accessible.","timestamp":"1732123800.0","comment_id":"1315421"},{"content":"Selected Answer: A\nB has actually more cost. A is the answer.","poster":"Carlini2020","timestamp":"1731697560.0","upvote_count":"1","comment_id":"1312784"},{"content":"Selected Answer: A\nIf I understand the question correctly, the owner is already paying to maintain the data (size is still growing) in its S3 bucket. The owner wants to ensure that its data transfer costs remain as low as possible (implies that it is best if the transfer cost is $0).\n\nFor option A, the requestor will have to bear the cost of the data transfer request from the bucket, with the data owner incurring a $0 transfer cost, which is optimal for the data owner.\n\nThis question does not state that the original data owner must also consider the cost incurred by the requestor and then find an optimal ways to share the data transfer cost for both parties.\n\nMy 2 cents' worth.","upvote_count":"1","timestamp":"1731648180.0","comment_id":"1312411","poster":"JA2018"},{"timestamp":"1731604740.0","poster":"Neilossi","content":"Selected Answer: A\nThe company wants to ensure that \"its\" data transfer costs remain as low as possible, so I choose A","upvote_count":"1","comment_id":"1312188"},{"timestamp":"1729316040.0","poster":"DevanshGupta","upvote_count":"2","content":"Per my understanding, the company is already paying to maintain the data(data keeps growing) in the S3 bucket. The company wants to ensure that its data transfer costs remain as low as possible (implies that it is best if the transfer cost is $0).\nAs per option A, the requestor would bear the cost of the request and the data downloaded from the bucket, causing the data owner to incur a $0 transfer cost, which is optimal for the data owner.\nThe question does not say that the data owner must also consider the cost incurred by the requestor and then find an optimal cost solution.","comment_id":"1299900"},{"comment_id":"1293693","upvote_count":"1","poster":"aturret","timestamp":"1728179460.0","content":"Selected Answer: B\nNo idea but I guess B. request payer sounds like stpd"},{"content":"Selected Answer: A\nit's A because the company will not pay extra cost","comment_id":"1290644","timestamp":"1727525160.0","poster":"Omariox","upvote_count":"2"}],"exam_id":31,"answer_description":"","isMC":true,"topic":"1","question_text":"A survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a European marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as low as possible.\nWhich solution will meet these requirements?","answer_ET":"A","choices":{"D":"Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the marketing firm's S3 buckets.","C":"Configure cross-account access for the marketing firm so that the marketing firm has access to the company's S3 bucket.","B":"Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets.","A":"Configure the Requester Pays feature on the company's S3 bucket."},"unix_timestamp":1666021020,"question_images":[],"answer_images":[],"question_id":887,"answers_community":["A (46%)","B (45%)","9%"]},{"id":"ItnZGVpV15d6Gg3qhRHL","answer_ET":"B","topic":"1","answer_images":[],"answers_community":["B (100%)"],"discussion":[{"content":"dashboard => quicksight, S3 query => athena","upvote_count":"2","poster":"mk168898","comment_id":"1301860","timestamp":"1729656240.0"},{"comment_id":"1231353","timestamp":"1718539260.0","upvote_count":"3","content":"Selected Answer: B\nQuickSight for dashboard and Athena for query each month so it is B","poster":"KennethNg923"},{"poster":"sandordini","comment_id":"1204504","upvote_count":"3","content":"Selected Answer: B\nSenior Leadership, custom dashboard, visualization: Quicksight Dashboard\nS3 query: Athena","timestamp":"1714474440.0"},{"comment_id":"1204158","upvote_count":"2","content":"Selected Answer: B\nB. Share an Amazon QuickSight dashboard that includes the requested table visual. Configure QuickSight to use Amazon Athena to query the new report. \n\nQuickSight works well with Athena and it can interact S3","poster":"d401c0d","timestamp":"1714413720.0"},{"comment_id":"1189540","upvote_count":"4","timestamp":"1712264220.0","content":"Selected Answer: B\nYou definitely use Athena to request S3.\nBoth cloudwatch and quicksight can interact with S3.\nSince we are taking about \"The company’s senior leadership\" I'd tend to use quicksight for a better format.","poster":"Mikado211"}],"timestamp":"2024-04-04 22:57:00","choices":{"D":"Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure CloudWatch to use Amazon Athena to query the new report.","A":"Share an Amazon QuickSight dashboard that includes the requested table visual. Configure QuickSight to use AWS DataSync to query the new report.","C":"Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure CloudWatch to use AWS DataSync to query the new report.","B":"Share an Amazon QuickSight dashboard that includes the requested table visual. Configure QuickSight to use Amazon Athena to query the new report."},"unix_timestamp":1712264220,"answer":"B","exam_id":31,"question_id":888,"url":"https://www.examtopics.com/discussions/amazon/view/137926-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"question_images":[],"answer_description":"","question_text":"A company that uses AWS Organizations runs 150 applications across 30 different AWS accounts. The company used AWS Cost and Usage Report to create a new report in the management account. The report is delivered to an Amazon S3 bucket that is replicated to a bucket in the data collection account.\n\nThe company’s senior leadership wants to view a custom dashboard that provides NAT gateway costs each day starting at the beginning of the current month.\n\nWhich solution will meet these requirements?"},{"id":"TkWvFYYxfTGuetYwArc3","timestamp":"2024-04-04 00:13:00","discussion":[{"upvote_count":"8","comment_id":"1205565","timestamp":"1714655760.0","content":"Selected Answer: AE\nAE.\nBy default, each file automatically expires after 24 hours, but you can change the default behavior in two ways:\n\n1. To change the cache duration for all files that match the same path pattern, you can change the CloudFront settings for Minimum TTL, Maximum TTL, and Default TTL for a cache behavior. \n\n2. To change the cache duration for an individual file, you can configure your origin to add a Cache-Control header with the max-age or s-maxage directive, or an Expires header to the file.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html#expiration-individual-objects","poster":"BBR01"},{"comment_id":"1287191","poster":"MatAlves","comments":[{"upvote_count":"2","timestamp":"1731522120.0","poster":"Sergantus","content":"Setting Cache-Control: private would prevent CloudFront from caching the content entirely, which is not the goal outlined, as it wants to use caching. After some time with updates, the caching performance will degrade for the entire solution as more and more objects get that directive.","comment_id":"1311564"}],"content":"Selected Answer: AC\nYou simply can't have A and E in the same approach:\n\n\"Default TTL applies only when your origin does not add HTTP headers such as Cache-Control max-age, Cache-Control s-maxage, or Expires to objects.\" \n\nC - Cache-Control private directive specifies that the response is intended for a single user and should not be cached by shared caches - it can still be cached, but only on a client device.\n\nThis combination of steps would provide the best solution for the case. \n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesDefaultTTL","timestamp":"1726902900.0","upvote_count":"7"},{"poster":"tch","content":"Selected Answer: AE\nControlling how long Amazon S3 content is cached by Amazon CloudFront\nhttps://docs.aws.amazon.com/whitepapers/latest/build-static-websites-aws/controlling-how-long-amazon-s3-content-is-cached-by-amazon-cloudfront.html","timestamp":"1742818020.0","upvote_count":"1","comment_id":"1409656","comments":[{"poster":"tch","content":"Set maximum TTL value\nSpecify cache-control headers","timestamp":"1742818140.0","upvote_count":"1","comment_id":"1409657"}]},{"content":"Selected Answer: A\nThis question has gaps:\nB) Can not be done in relation to S3 catching, this is a CloudFront specific configuration\nC) Question does not make any comment in the sense of \"private\" objects... any asumption in that sense is your personal oppinion, not the question stating it\nD) Collisions with A, why would you do something like that programatically?\nE) Can not be because why would you put 24h when it is not requested, also why would you invalidate a deployment that is already TTL=0, everything is up to date in the caches with that config","timestamp":"1737014280.0","comment_id":"1341529","poster":"robotgeek","upvote_count":"1"},{"comment_id":"1336868","timestamp":"1736109000.0","poster":"Salilgen","upvote_count":"1","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html#ExpirationDownloadDist"},{"upvote_count":"2","content":"Selected Answer: AE\nA - Setting the default TTL to 2 minutes allows CloudFront to cache content for a short period.\nB - TTL is not a concept applied directly to S3 buckets. TTL and caching behaviors are managed through Cache-Control headers and CloudFront settings.\nC - The *private directive* lets caching occur locally only, not in shared caches like CloudFront.\nD - Ensures content is served fresh by tightly controlling cache expiration, not even \"a few minutes\".\nE - Using a long Cache-Control max-age directive improves performance, while targeted CloudFront invalidations ensure that updated content is served immediately after deployment. This is common practice.","timestamp":"1735779780.0","poster":"LeonSauveterre","comment_id":"1335328"},{"timestamp":"1733177100.0","poster":"Cpso","content":"Selected Answer: AE\nbrowser will always get cache-control value set by S3 not cloudfront.\ncloudfront will use parameter ttl override the cache-control header for decision time to cache on cloudfront.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html?utm_source=chatgpt.com#ExpirationDownloadDist","comment_id":"1321120","upvote_count":"2"},{"content":"Selected Answer: AE\nA - make browser to cache 2 minutes. after 2 minute browser will ask cloudfront\n\nE: cloudfront would not ask S3 for 24hour . invalidation will clearing this cacheing.\n\nso A+E is most effective.","upvote_count":"2","comment_id":"1321111","poster":"Cpso","timestamp":"1733175660.0"},{"content":"Selected Answer: AD\n#A - Set the CloudFront default TTL to 2 minutes: This directly controls how long content is cached at the CloudFront edge locations, allowing for a short caching window to meet the requirement of not serving stale content for more than a few minutes.\n\n#D - Create a Lambda@Edge function to add an Expires header: By using a Lambda@Edge function triggered on the \"viewer response\", you can dynamically set an Expires header with a precise expiration time on each request, ensuring fine-grained control over caching behavior.","timestamp":"1733033400.0","comments":[{"poster":"JA2018","upvote_count":"1","content":"#B - Set a default TTL of 2 minutes on the S3 bucket: This would not be as effective as setting the TTL at the CloudFront level as the caching would happen at the origin server (S3) instead of the edge locations, potentially impacting performance.\n\n# C - Add a Cache-Control private directive: This directive would prevent browser caching, which is not the desired outcome in this scenario.\n\n# E - Add a Cache-Control max-age directive of 24 hours with CloudFront invalidation: Setting a large max-age on objects in S3 and invalidating on deployment would not provide the necessary granular control for keeping content fresh within a few minutes.","timestamp":"1733033400.0","comment_id":"1320468"}],"upvote_count":"1","poster":"JA2018","comment_id":"1320467"},{"comment_id":"1258061","poster":"1ba9aa0","upvote_count":"3","content":"Selected Answer: AC\nA-C,\n\nBecause A-E is not possible following this link: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesDefaultTTL\n\"Default TTL applies only when your origin does not add HTTP headers such as Cache-Control max-age, Cache-Control s-maxage, or Expires to objects.\"","timestamp":"1722331380.0"},{"upvote_count":"3","comment_id":"1244005","timestamp":"1720385280.0","poster":"EdricHoang","content":"Selected Answer: AC\nIf the content still keep client's cache in 24h, its wrong (answer E)"},{"poster":"ug56c","content":"Selected Answer: AE\nIf your minimum TTL is greater than 0, CloudFront uses the cache policy’s minimum TTL, even if the Cache-Control: no-cache, no-store, and/or private directives are present in the origin headers.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html","upvote_count":"5","comment_id":"1229236","timestamp":"1718202060.0"},{"upvote_count":"4","content":"Selected Answer: AE\n\"However, the company also wants to ensure that stale content is not served for more than a few minutes after a deployment.\"\nAfter a deployment","comment_id":"1223706","comments":[{"content":"Exactly, it was not outlined that the user shouldn't see the stale content, only that it's not served.","upvote_count":"1","poster":"Sergantus","timestamp":"1731521700.0","comment_id":"1311562"}],"poster":"Nm55569","timestamp":"1717433580.0"},{"content":"Selected Answer: AC\nAnswer (AC)\n\nPer table on URL\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html#expiration-individual-objects\nanswer E is incorrect because if we will change cache-control max-age to 24h it will means that customer browser will cache web for 24h and customer want to be sure that it will be not longer then few min.\nExpires header (answer D) from my understanding can be used only on full folder of web not as lambda function which will reply to customer reqeusts.\nWe are setting Default TTL for CloudFront (answer A) not on S3 (answer B) and it will say CloudFront to cache web for 2min.","poster":"Scheldon","comments":[{"content":"Adding Cache-control private (answerC) will work per customer wish but only if we will add them to the objects which are changed very often or if we will set minimum TTL. \nIn the 1 situation User Browser will not store files which we designate to be often changed and mentioned files will be downloaded every time from origin.\nIn the 2 situation, Cloud front will cache web files for min TTL time but customer browser will not store them.\nTaking all that in to account I would go with AC","upvote_count":"3","timestamp":"1716889260.0","poster":"Scheldon","comment_id":"1220113"}],"upvote_count":"4","timestamp":"1716889200.0","comment_id":"1220112"},{"poster":"Linuslin","upvote_count":"2","timestamp":"1716276660.0","content":"Selected Answer: AE\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html#expiration-individual-objects\n\nhttps://stackoverflow.com/questions/43343759/confused-with-minimum-maximum-and-default-ttl-in-cloudfront","comment_id":"1214782"},{"content":"Selected Answer: DE\nSince it don’t want to cache more than a minute, A cannot be an answer","timestamp":"1714977180.0","comments":[{"poster":"mk168898","content":"no where in the question did it say 1 minute. You mean more than a few minutes?","upvote_count":"1","comment_id":"1301861","timestamp":"1729656480.0"}],"comment_id":"1207212","upvote_count":"1","poster":"02ffe1c"},{"content":"Answer is AE , C would only be on the user browser and would not cache to the cloud front and would be useless","timestamp":"1714692720.0","comment_id":"1205867","poster":"kelmryan1","upvote_count":"1"},{"comment_id":"1188930","upvote_count":"4","content":"Selected Answer: AC\nAdd a Cache-Control Private Directive to Objects in Amazon S3 (Option C):\nBy setting the Cache-Control header to private for objects in the S3 bucket, you control caching behavior.\nThe private directive indicates that the content is intended for a single user and should not be cached by intermediate proxies or CDNs.\nThis helps prevent stale content from being served to multiple users.\nAdditionally, consider using other Cache-Control directives (e.g., max-age, no-cache, no-store) as needed.","timestamp":"1712182380.0","poster":"xBUGx"}],"question_text":"A company is hosting a high-traffic static website on Amazon S3 with an Amazon CloudFront distribution that has a default TTL of 0 seconds. The company wants to implement caching to improve performance for the website. However, the company also wants to ensure that stale content is not served for more than a few minutes after a deployment.\n\nWhich combination of caching methods should a solutions architect implement to meet these requirements? (Choose two.)","unix_timestamp":1712182380,"isMC":true,"answer_ET":"AE","question_images":[],"answer_description":"","answer_images":[],"choices":{"E":"Add a Cache-Control max-age directive of 24 hours to the objects in Amazon S3. On deployment, create a CloudFront invalidation to clear any changed files from edge caches.","C":"Add a Cache-Control private directive to the objects in Amazon S3.","D":"Create an AWS Lambda@Edge function to add an Expires header to HTTP responses. Configure the function to run on viewer response.","A":"Set the CloudFront default TTL to 2 minutes.","B":"Set a default TTL of 2 minutes on the S3 bucket."},"url":"https://www.examtopics.com/discussions/amazon/view/137850-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":889,"answer":"AE","answers_community":["AE (51%)","AC (43%)","2%"],"topic":"1","exam_id":31},{"id":"uqdiyMWyI4b2CZDYiy5j","topic":"1","question_id":890,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/138489-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C","choices":{"A":"Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to the private subnets that contain the EC2 instances.","D":"Purchase a Compute Savings Plan. Keep the Lambda functions in the Lambda service VPC.","B":"Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to new public subnets in the same VPC where the EC2 instances run.","C":"Purchase a Compute Savings Plan. Connect the Lambda functions to the private subnets that contain the EC2 instances."},"discussion":[{"poster":"Guru4Cloud","content":"Selected Answer: C\nCompute Savings Plan: This plan offers significant discounts on Lambda functions compared to on-demand pricing. Since the application will run for a year, a sustained use discount like Compute Savings Plan is ideal.\nPrivate Subnets: Lambda functions in private subnets can directly access EC2 instances within the VPC without needing internet access, reducing security risks and potential egress costs.","timestamp":"1712840100.0","upvote_count":"7","comment_id":"1193792"},{"content":"Selected Answer: C\nWhy Compute Savings Plan: Covers both EC2 and Lambda, providing flexibility as Lambda usage increases.\nWhy Private Subnets: Ensures network access between Lambda and EC2 instances.\n\n!!\nAn EC2 Instance Savings Plan doesn't cover Lambda. Also, Lambda functions in public subnets cannot directly communicate with EC2 instances in private subnets.","timestamp":"1735794540.0","poster":"LeonSauveterre","comment_id":"1335362","upvote_count":"1"},{"poster":"mk168898","content":"A and B is not because it is talkign about EC2, but the question is asking for Lambda","upvote_count":"2","timestamp":"1729656600.0","comment_id":"1301862"},{"content":"Selected Answer: C\nCompute savings include lamdba and EC2, Ec2 savings only EC2 instances.\nhttps://aws.amazon.com/savingsplans/compute-pricing/","poster":"jacinml","comment_id":"1297560","timestamp":"1728910020.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"1241402","comments":[{"upvote_count":"2","comment_id":"1247532","comments":[{"poster":"navneet_sh","upvote_count":"2","timestamp":"1720910820.0","content":"Sorry I mean Compute Savings Plan.","comment_id":"1247533"}],"content":"But composite saving plan discount Automatically applies to Lambda.","poster":"navneet_sh","timestamp":"1720910760.0"}],"timestamp":"1720009200.0","content":"I confuse this Question. Instance saving plan is cheaper than compute saving plan.\nhttps://aws.amazon.com/savingsplans/compute-pricing/","poster":"Lin878"},{"upvote_count":"2","poster":"sheilawu","timestamp":"1717737540.0","content":"Selected Answer: A\nIn this question has point out \"access EC2 instances\" within VPC,=> Lambda VPC to an ENI (Elastic network interface) in your account VPC.=>No charge.\nTherefore I stick with A, Not D.","comment_id":"1225915","comments":[{"timestamp":"1729243980.0","comment_id":"1299608","upvote_count":"3","poster":"JohnYu","content":"An EC2 Instance Savings Plan is limited to savings on EC2 instances only. It does not provide cost savings for Lambda functions. Therefore, it is not the best choice given that the number of Lambda functions is expected to increase."},{"upvote_count":"3","content":"I am sorry I mean C","timestamp":"1717737600.0","comment_id":"1225916","poster":"sheilawu"}]},{"timestamp":"1716895080.0","content":"Selected Answer: D\nAnswerD\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/foundation-networking.html","poster":"Scheldon","upvote_count":"1","comment_id":"1220174"}],"answers_community":["C (77%)","A (15%)","8%"],"exam_id":31,"answer_ET":"C","isMC":true,"answer_images":[],"unix_timestamp":1712840100,"question_text":"A company runs its application by using Amazon EC2 instances and AWS Lambda functions. The EC2 instances run in private subnets of a VPC. The Lambda functions need direct network access to the EC2 instances for the application to work.\n\nThe application will run for 1 year. The number of Lambda functions that the application uses will increase during the 1-year period. The company must minimize costs on all application resources.\n\nWhich solution will meet these requirements?","answer_description":"","timestamp":"2024-04-11 14:55:00"}],"exam":{"numberOfQuestions":1019,"isMCOnly":true,"isBeta":false,"isImplemented":true,"provider":"Amazon","id":31,"name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025"},"currentPage":178},"__N_SSP":true}