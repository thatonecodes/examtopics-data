{"pageProps":{"questions":[{"id":"zKDkN1RVdf2wRVanGWqg","discussion":[{"timestamp":"1732506840.0","comment_id":"1317312","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/big-data/enforce-customized-data-quality-rules-in-aws-glue-databrew/","upvote_count":"2","poster":"emupsx1"}],"choices":{"B":"Implement custom data quality rules in DataBrew. Apply the custom rules across datasets.","A":"Manually review the data for custom PII categories.","C":"Develop custom Python scripts to detect the custom PII categories. Call the scripts from DataBrew.","D":"Implement regex patterns to extract PII information from fields during extract transform, and load (ETL) operations into the data lake."},"url":"https://www.examtopics.com/discussions/amazon/view/151939-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","question_id":81,"answer":"B","isMC":true,"timestamp":"2024-11-25 04:54:00","unix_timestamp":1732506840,"exam_id":21,"answer_images":[],"question_text":"A company analyzes data in a data lake every quarter to perform inventory assessments. A data engineer uses AWS Glue DataBrew to detect any personally identifiable formation (PII) about customers within the data. The company's privacy policy considers some custom categories of information to be PII. However, the categories are not included in standard DataBrew data quality rules.\n\nThe data engineer needs to modify the current process to scan for the custom PII categories across multiple datasets within the data lake.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answers_community":["B (100%)"],"answer_ET":"B","question_images":[],"answer_description":""},{"id":"MvsE5XAc2Ap8QA9iGUnT","url":"https://www.examtopics.com/discussions/amazon/view/151940-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"answers_community":["B (100%)"],"unix_timestamp":1732507080,"answer_description":"","answer":"B","timestamp":"2024-11-25 04:58:00","question_text":"A company receives a data file from a partner each day in an Amazon S3 bucket. The company uses a daily AWS Glue extract, transform, and load (ETL) pipeline to clean and transform each data file. The output of the ETL pipeline is written to a CSV file named Daily.csv in a second S3 bucket.\n\nOccasionally, the daily data file is empty or is missing values for required fields. When the file is missing data, the company can use the previous dayâ€™s CSV file.\n\nA data engineer needs to ensure that the previous day's data file is overwritten only if the new daily file is complete and valid.\n\nWhich solution will meet these requirements with the LEAST effort?","question_id":82,"answer_ET":"B","discussion":[{"comment_id":"1317314","poster":"emupsx1","upvote_count":"1","timestamp":"1732507080.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-data-quality.html"}],"question_images":[],"topic":"1","isMC":true,"exam_id":21,"choices":{"B":"Configure the AWS Glue ETL pipeline to use AWS Glue Data Quality rules. Develop rules in Data Quality Definition Language (DQDL) to check for missing values in required fields and empty files.","A":"Invoke an AWS Lambda function to check the file for missing data and to fill in missing values in required fields.","C":"Use AWS Glue Studio to change the code in the ETL pipeline to fill in any missing values in the required fields with the most common values for each field.","D":"Run a SQL query in Amazon Athena to read the CSV file and drop missing rows. Copy the corrected CSV file to the second S3 bucket."}},{"id":"8YLOe4nIy3wGh5UWa8dh","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/150343-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answers_community":["C (100%)"],"unix_timestamp":1730003820,"answer_description":"","answer":"C","timestamp":"2024-10-27 05:37:00","question_text":"A marketing company uses Amazon S3 to store marketing data. The company uses versioning in some buckets. The company runs several jobs to read and load data into the buckets.\n\nTo help cost-optimize its storage, the company wants to gather information about incomplete multipart uploads and outdated versions that are present in the S3 buckets.\n\nWhich solution will meet these requirements with the LEAST operational effort?","question_id":83,"answer_ET":"C","discussion":[{"upvote_count":"1","content":"Selected Answer: C\nAmazon S3 Storage Lens provides a comprehensive view of your S3 storage usage and activity. It includes metrics and insights related to incomplete multipart uploads, outdated versions of objects, and other storage characteristics.","timestamp":"1734952440.0","poster":"HagarTheHorrible","comment_id":"1330791"},{"timestamp":"1732507920.0","comment_id":"1317315","upvote_count":"1","poster":"emupsx1","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html"},{"upvote_count":"3","timestamp":"1730365500.0","content":"Selected Answer: C\nShould be C AWS Storage Lens: I asked ChatGPT it answered AWS S3 Storage Lens too","poster":"truongnguyen86","comment_id":"1305344"},{"comment_id":"1303485","upvote_count":"3","poster":"Parandhaman_Margan","content":"Answer:B","timestamp":"1730003820.0"}],"question_images":[],"topic":"1","isMC":true,"exam_id":21,"choices":{"A":"Use AWS CLI to gather the information.","B":"Use Amazon S3 Inventory configurations reports to gather the information.","C":"Use the Amazon S3 Storage Lens dashboard to gather the information.","D":"Use AWS usage reports for Amazon S3 to gather the information."}},{"id":"r76KGQoYf0j6pKTuQdiK","isMC":true,"answer_images":[],"question_text":"A gaming company uses Amazon Kinesis Data Streams to collect clickstream data. The company uses Amazon Data Firehose delivery streams to store the data in JSON format in Amazon S3. Data scientists at the company use Amazon Athena to query the most recent data to obtain business insights.\n\nThe company wants to reduce Athena costs but does not want to recreate the data pipeline.\n\nWhich solution will meet these requirements with the LEAST management effort?","answer":"A","answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/150779-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"D":"Integrate an AWS Lambda function with Firehose to convert source records to Apache Parquet and write them to Amazon S3. In parallel, run an AWS Glue extract, transform, and load (ETL) job to combine the JSON files and convert the JSON files to large Parquet files. Create a custom S3 object YYYYMMDD prefix. Use the ALTER TABLE ADD PARTITION statement to reflect the partition on the existing Athena table.","C":"Create a Kinesis data stream as a delivery destination for Firehose. Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to run Apache Flink on the Kinesis data stream. Use Flink to aggregate the data and save the data to Amazon S3 in Apache Parquet format with a custom S3 object YYYYMMDD prefix. Use the ALTER TABLE ADD PARTITION statement to reflect the partition on the existing Athena table.","B":"Create an Apache Spark job that combines JSON files and converts the JSON files to Apache Parquet files. Launch an Amazon EMR ephemeral cluster every day to run the Spark job to create new Parquet files in a different S3 location. Use the ALTER TABLE SET LOCATION statement to reflect the new S3 location on the existing Athena table.","A":"Change the Firehose output format to Apache Parquet. Provide a custom S3 object YYYYMMDD prefix expression and specify a large buffer size. For the existing data, create an AWS Glue extract, transform, and load (ETL) job. Configure the ETL job to combine small JSON files, convert the JSON files to large Parquet files, and add the YYYYMMDD prefix. Use the ALTER TABLE ADD PARTITION statement to reflect the partition on the existing Athena table."},"question_images":[],"answer_ET":"A","answers_community":["A (100%)"],"question_id":84,"exam_id":21,"discussion":[{"upvote_count":"4","timestamp":"1731927720.0","content":"Selected Answer: A\nIf you have JSON, Firehose should convert it without the needs of a Lambda","poster":"michele_scar","comment_id":"1313953"},{"poster":"kupo777","comment_id":"1307360","timestamp":"1730809440.0","content":"Correct Answer: A\n\nChange the Firehose output format to Apache Parquet. Provide a custom S3 object YYYYMMDD prefix expression and specify a large buffer size. For the existing data, create an AWS Glue extract, transform, and load (ETL) job. Configure the ETL job to combine small JSON files, convert the JSON files to large Parquet files, and add the YYYYMMDD prefix. Use the ALTER TABLE ADD PARTITION statement to reflect the partition on the existing Athena table.","upvote_count":"1"}],"timestamp":"2024-11-05 13:24:00","unix_timestamp":1730809440},{"id":"WDBfz2AhQKq8eYIqWCCF","isMC":true,"question_images":[],"answers_community":["A (100%)"],"answer_ET":"A","answer_images":[],"topic":"1","question_id":85,"answer":"A","question_text":"A company needs a solution to manage costs for an existing Amazon DynamoDB table. The company also needs to control the size of the table. The solution must not disrupt any ongoing read or write operations. The company wants to use a solution that automatically deletes data from the table after 1 month.\n\nWhich solution will meet these requirements with the LEAST ongoing maintenance?","discussion":[{"timestamp":"1736965200.0","content":"Selected Answer: A\nDynamoDB TTL will automatically delete items based on how you configure.","poster":"MerryLew","upvote_count":"1","comment_id":"1341189"},{"poster":"emupsx1","comment_id":"1317317","upvote_count":"1","timestamp":"1732508460.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"}],"unix_timestamp":1732508460,"timestamp":"2024-11-25 05:21:00","exam_id":21,"url":"https://www.examtopics.com/discussions/amazon/view/151941-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_description":"","choices":{"D":"Use an AWS Lambda function to periodically scan the DynamoDB table for data that is older than 1 month. Configure the Lambda function to delete old data.","A":"Use the DynamoDB TTL feature to automatically expire data based on timestamps.","C":"Configure a stream on the DynamoDB table to invoke an AWS Lambda function. Configure the Lambda function to delete data in the table that is older than 1 month.","B":"Configure a scheduled Amazon EventBridge rule to invoke an AWS Lambda function to check for data that is older than 1 month. Configure the Lambda function to delete old data."}}],"exam":{"isImplemented":true,"isBeta":false,"name":"AWS Certified Data Engineer - Associate DEA-C01","lastUpdated":"11 Apr 2025","id":21,"numberOfQuestions":207,"provider":"Amazon","isMCOnly":true},"currentPage":17},"__N_SSP":true}