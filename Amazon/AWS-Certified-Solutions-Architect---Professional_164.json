{"pageProps":{"questions":[{"id":"b3GjFaddHDp8djpfcjBS","unix_timestamp":1641658200,"answer":"B","choices":{"B":"Add tenant ID information to the partition key of the DynamoDB table. Create a service that uses the JWT token to retrieve the appropriate Lambda execution role that is tenant-specific. Attach IAM policies to the execution role to allow access to items in the table only when the key matches the tenant ID.","A":"Create a DynamoDB table for each tenant by using the tenant ID in the table name. Create a service that uses the JWT token to retrieve the appropriate Lambda execution role that is tenant-specific. Attach IAM policies to the execution role to allow access only to the DynamoDB table for the tenant.","D":"Add tenant ID as a sort key in every DynamoDB table. Add logic to each Lambda function to use the tenant ID that comes from the JWT token as the sort key in every operation on the DynamoDB table.","C":"Create a separate AWS account for each tenant of the application. Use dedicated infrastructure for each tenant. Ensure that no cross-account network connectivity exists."},"url":"https://www.examtopics.com/discussions/amazon/view/69682-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"isMC":true,"discussion":[{"timestamp":"1641777480.0","comments":[{"comment_id":"595888","poster":"sashsz","timestamp":"1651469400.0","content":"Tips: https://aws.amazon.com/blogs/apn/isolating-saas-tenants-with-dynamically-generated-iam-policies/","upvote_count":"3"}],"upvote_count":"13","poster":"vinodkp","comment_id":"520568","content":"I support B"},{"timestamp":"1641658200.0","content":"Answer seems to be B. Rather than creating table for each tenant, its better to use partition key in the already available table. This can be achieved with the LEAST operational.","upvote_count":"6","comment_id":"519642","poster":"Smartphone"},{"upvote_count":"1","timestamp":"1704193080.0","comment_id":"1111789","poster":"aewis","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/security/how-to-secure-your-saas-tenant-data-in-dynamodb-with-abac-and-client-side-encryption/"},{"content":"answer seems B\nFollowing the context above, the tenant id would be created when the user is created \"The company will have hundreds of customers within the first year.\" <- If the answer is A, it would make Table hundreds a year. (operational)","upvote_count":"1","comment_id":"875668","poster":"yyees","timestamp":"1681998660.0"},{"upvote_count":"1","content":"Looks like its Option A:\n\nOne solution that meets these requirements with the least operational overhead would be to use DynamoDB table partitioning (also known as sharding) to create separate partitions for each tenant. Each partition would have a unique primary key that includes the tenant ID, and all requests for a particular tenant would be directed to the corresponding partition. This would provide stronger isolation between tenants, as each tenant's data would be physically separated in the DynamoDB table. Additionally, by using a custom authorizer in the API Gateway, the Lambda function could easily extract the tenant ID from the JWT token and use it to determine which partition to access in the DynamoDB table. This solution would not require any significant changes to the existing architecture, and would scale easily as the number of tenants increases.","poster":"Heer","timestamp":"1674983940.0","comment_id":"791495"},{"content":"Selected Answer: B\nB is correct:\nhttps://aws.amazon.com/blogs/apn/multi-tenant-storage-with-amazon-dynamodb/","timestamp":"1663512300.0","upvote_count":"2","comment_id":"672467","poster":"Yashar1691"},{"comment_id":"656948","upvote_count":"1","content":"This Question is very confuse. Maybe is error from admin.\n\"LEAST operational overhead\" => if A, seperate by tablename, in the future when need to add more tenant you need create more table, with B you dont need do anymore\n\n\"a stronger isolation between tenants\" i think, for this stituation mean of question: find a soluiton that stronger isolation than current solution, not mean find the best soluiton for isolation.","timestamp":"1662093540.0","poster":"kadev"},{"poster":"gnic","comment_id":"654980","timestamp":"1661941380.0","upvote_count":"1","content":"Selected Answer: A\nPartition key is not for isolation.\nI support A"},{"content":"Selected Answer: A\nA. reason \"To comply with security standards, the company needs a stronger isolation between tenants.\"\n\nhttps://aws.amazon.com/blogs/apn/multi-tenant-storage-with-amazon-dynamodb/\n\n1. Separate database – each tenant has a fully isolated database with its own representation of the data.... best ISOLATION but the question is about multi-tent.\n\n\n2. Shared database, separate schema (Table Name Partitioning) – tenants all reside in the same database, but each tenant can have its own representation of the data. STRONGEST ISOLATION FOR MULTI-TENANT. but requires more operational than partition.\n\n3. Shared everything (Index Partitioning)– tenants all reside in the same database and all leverage a universal representation of the data....EASIEST TO MANAGE BUT NOT LESS ISOLATION, prone to nosy neighbors.","timestamp":"1658770920.0","poster":"cen007","upvote_count":"2","comment_id":"636869"},{"upvote_count":"4","comments":[{"timestamp":"1655075100.0","upvote_count":"3","content":"The link also explains to take the approach further by introducing a randomised suffix to the partition key, and introduce fine grain controlled access in coordination with STS","poster":"makpk","comment_id":"615528"}],"poster":"ArreRaja","timestamp":"1654003560.0","comment_id":"609742","content":"B. \nThe choice is augmenting partition key with tenant id vs Silo keys in multiple tables (increased overhead)\nhttps://aws.amazon.com/blogs/apn/partitioning-pooled-multi-tenant-saas-data-with-amazon-dynamodb/"},{"content":"for those selecting B/D, you cant update the partition key and sort key in an existing table","timestamp":"1652520960.0","upvote_count":"4","poster":"ssSsEclipse","comment_id":"601509"},{"content":"Selected Answer: B\nQns deals with the implementation of custom permission policies. The details of each policy should be stored in a central table with the client id as the partition key for quick retrival andthe subsequent creation of the custom policy. Much less operational overhead and also side benefit, better for backups","upvote_count":"3","timestamp":"1651233420.0","comment_id":"594428","poster":"jnxtx"},{"upvote_count":"4","timestamp":"1641721980.0","poster":"tkanmani76","comment_id":"520078","content":"Creating seperate accounts, different tables for tenants has more operational overhead. Partionkey based approach is the right way - Option B."}],"answer_ET":"B","topic":"1","question_text":"A company has a serverless multi-tenant content management system on AWS. The architecture contains a web-based front end that interacts with an Amazon\nAPI Gateway API that uses a custom AWS Lambda authorizer. The authorizer authenticates a user to its tenant ID and encodes the information in a JSON Web\nToken (JWT) token. After authentication, each API call through API Gateway targets a Lambda function that interacts with a single Amazon DynamoDB table to fulfill requests.\nTo comply with security standards, the company needs a stronger isolation between tenants. The company will have hundreds of customers within the first year.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_description":"","question_id":816,"timestamp":"2022-01-08 17:10:00","question_images":[],"answers_community":["B (67%)","A (33%)"],"exam_id":32},{"id":"LgnEfmA2bXPRVGfETYsy","isMC":true,"answer_images":[],"choices":{"C":"Convert the data volume to the Provisioned IOPS SSD (io2) Block Express type. Leave the volume as 512 GB. Set the volume IOPS to 1,500.","A":"Convert the data volume to the Cloud HDD (sc1) type. Leave the volume as 512 GB. Set the volume IOPS to 1,500.","B":"Convert the data volume to the Provisioned IOPS SSD (io2) type. Resize the volume to 256 GB. Set the volume IOPS to 1,500.","D":"Convert the data volume to the General Purpose SSD (gp3) type. Resize the volume to 256 GB. Set the volume IOPS to 1,500."},"unix_timestamp":1641114360,"answers_community":["D (71%)","C (24%)","5%"],"discussion":[{"content":"Selected Answer: C\nResize the volume to 256 GB -> not possible, you can't shrink an ebs volume. You would have to create a new volume and migrate existing data to it. Answer is C IMO","poster":"ghfalcon7","timestamp":"1651917000.0","upvote_count":"5","comment_id":"598060"},{"content":"Selected Answer: D\nAll answers are wrong - its not possible to shrink EBS - need to create it from scratch so its either A or C\nA - will not provide the needed IOPS\nC - super expensive\nSo I guess the only correct option should be D - only problem with it is wording need to \"recreate\" the volume and also we can set it to 3000","poster":"asfsdfsdf","timestamp":"1657972860.0","comments":[{"poster":"AYANtheGLADIATOR","upvote_count":"2","content":"This is a valid point.","comment_id":"661211","timestamp":"1662467460.0"}],"upvote_count":"5","comment_id":"632139"},{"comment_id":"791507","content":"The right ans should be A here :\n\nOne solution that the solutions architect could recommend is to switch to a lower-performance EBS volume type, such as a Throughput Optimized HDD (st1) or Cold HDD (sc1) volume. These volume types are significantly less expensive than gp2 volumes, but they are also slower and have lower IOPS. Since the current database volume never goes above 256 GB utilization and consistently uses around 1,500 IOPS, it's likely that these volume types would provide enough performance for the database.","upvote_count":"1","poster":"Heer","timestamp":"1674984420.0"},{"upvote_count":"1","timestamp":"1668218520.0","content":"We can resize the existing volume to 256, so we can lean towards C, the only concern is provisioned I/O block express is not supported on all instance type, the question states instance is running on latest generation","comment_id":"716435","poster":"breathingcloud"},{"upvote_count":"4","timestamp":"1660434360.0","comment_id":"646470","content":"Selected Answer: D\nhttps://aws.amazon.com/cn/ebs/general-purpose/","poster":"kenchou73"},{"poster":"hilft","comment_id":"637708","content":"I would go for B in normal cirumstances but the question is asking for the cost so, D","timestamp":"1658883300.0","upvote_count":"1"},{"timestamp":"1653468660.0","comment_id":"607120","upvote_count":"3","poster":"bobsmith2000","content":"Selected Answer: D\nCheck here\nhttps://aws.amazon.com/ebs/pricing/\n\nA) jeopardises performance.\nBC) - any io type is more expensive per GB then gp\nD) the lowers price per GB without affecting performance."},{"timestamp":"1649034780.0","comment_id":"580509","content":"Selected Answer: D\nalthough gp3 baseline is 3000 iops it is still the most cost efficient.","poster":"Jonfernz","upvote_count":"3"},{"poster":"czarno","content":"Max IOPS for HDD sc1 is indeed 250.\nGp3 is becoming the new default in place of gp2.\nAlthough you can't set the IOPS of gp3 to 1500 as it's baseline is 3000, I think the question/answer is just slightly wrong and looks a bit different in the actual exam. \nWhich makes D the only correct answer","upvote_count":"1","timestamp":"1648369980.0","comment_id":"576076"},{"comment_id":"566356","content":"option - D","poster":"Ducer_deuceworld","upvote_count":"2","timestamp":"1647111240.0"},{"comment_id":"560437","timestamp":"1646354580.0","upvote_count":"1","poster":"Alvindo","comments":[{"timestamp":"1647288660.0","content":"D, despite the 3000 IOPS baseline\nThe cold HDD sc1 has a max IOPS volume of 250, therfore can't reached the needed 1500 IOPS.\nhttps://aws.amazon.com/ebs/volume-types/","poster":"tobstar86","comment_id":"567912","upvote_count":"3"}],"content":"Selected Answer: A\ndon't think it;s gp3 since it's baseline performance is 3000 iops: https://aws.amazon.com/ebs/volume-types/\nDon't think it's io2 since it's min volume size is 4gb and the iops increase is 500/gb so won't be able to get 1500 iops\nso i think it's cold hdd"},{"upvote_count":"2","poster":"Z_dane_23","timestamp":"1645629480.0","comment_id":"554596","content":"I think it's B as io2 block express has the same price as io2, however as they question asks for the cheapest, by reducing size to 256GB it will be the cheaper than block express with 512GB"},{"upvote_count":"2","comment_id":"552420","poster":"jyrajan69","content":"All answers are wrong in terms of IOPS of 1500, but process of elimination, A is wrong, it is for data that is long term, cold, B and C do not satisfy the cost aspect, so only possible is D","timestamp":"1645404720.0"},{"comments":[{"comment_id":"528913","content":"D seems to be good, but it's wrong cause it's clearly stating \"set the IOPS to 1,500\" which is impossible :\nIOPS must be between 3000 and 16000.","timestamp":"1642733820.0","upvote_count":"1","poster":"Yecine11y"}],"comment_id":"526085","content":"AWS designed gp3 to provide predictable 3,000 IOPS baseline performance and 125 MiB/s, regardless of volume size. With gp3 volumes, you can provision IOPS and throughput independently, without increasing storage size, at costs up to 20% lower per GB compared to gp2 volumes. This means you can provision smaller volumes while maintaining high performance, at a cheaper cost.","timestamp":"1642451160.0","upvote_count":"2","poster":"vissi"},{"upvote_count":"3","timestamp":"1642451100.0","comment_id":"526083","poster":"vissi","content":"Option D is correct \nhttps://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/"},{"content":"Option-D. General Purpose SSD (gp3) type","poster":"ByomkeshDas","timestamp":"1642311060.0","comment_id":"524642","upvote_count":"1"},{"comment_id":"519091","poster":"padel","upvote_count":"1","timestamp":"1641573600.0","content":"D even if the minimal IOPS is 3'000"},{"timestamp":"1641247620.0","upvote_count":"1","comments":[{"comments":[{"upvote_count":"3","comment_id":"524644","timestamp":"1642311120.0","content":"However still General Purpose SSD (gp3) type will result in the MOST cost reduction while maintaining the database's performance.","poster":"ByomkeshDas"}],"upvote_count":"2","content":"gp3 can't set volume IOPS to 1500.\nbaseline performance of 3,000 IOPS \n\nTherefore, D is wrong.\n\nhttps://aws.amazon.com/jp/about-aws/whats-new/2020/12/introducing-new-amazon-ebs-general-purpose-volumes-gp3/?nc1=h_ls","timestamp":"1641316680.0","comment_id":"516859","poster":"geburyam"}],"content":"It’s D","comment_id":"516092","poster":"Firelord"},{"poster":"GeniusMikeLiu","upvote_count":"1","timestamp":"1641114360.0","content":"why C? I am so confused.","comment_id":"514894"}],"topic":"1","exam_id":32,"answer_description":"","question_id":817,"question_text":"A company is running a custom database in the AWS Cloud. The database uses Amazon EC2 for compute and uses Amazon Elastic Block Store (Amazon EBS) for storage. The database runs on the latest generation of EC2 instances and uses a General Purpose SSD (gp2) EBS volume for data.\nThe current data volume has the following characteristics:\n✑ The volume is 512 GB in size.\n✑ The volume never goes above 256 GB utilization.\n✑ The volume consistently uses around 1,500 IOPS.\nA solutions architect needs to conduct an analysis of the current database storage layer and make a recommendation about ways to reduce cost.\nWhich solution will provide the MOST cost savings without impacting the performance of the database?","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/69242-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2022-01-02 10:06:00","answer":"D","answer_ET":"D"},{"id":"UycGuLQzixqGzySLEj6Z","question_text":"A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all times for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.\nWhich solution will provide the HIGHEST availability for the database?","timestamp":"2022-01-04 12:06:00","choices":{"B":"Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.","D":"Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.","A":"Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.","C":"Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region."},"answer_description":"","question_images":[],"question_id":818,"unix_timestamp":1641294360,"url":"https://www.examtopics.com/discussions/amazon/view/69438-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"answer":"D","discussion":[{"upvote_count":"5","poster":"kangtamo","content":"Selected Answer: D\nAgree with D.","comment_id":"623537","timestamp":"1656372540.0"},{"poster":"Greanny","content":"why not B?","timestamp":"1712074500.0","upvote_count":"1","comment_id":"1188148"},{"poster":"ggrodskiy","upvote_count":"1","content":"Correct D.","timestamp":"1689789900.0","comment_id":"956892"},{"content":"Those who have selected Option D .\n Read replicas do not provide high availability for writes to the database. Only the primary instance can accept write requests, and if the primary instance becomes unavailable, the read replicas will not be able to accept write requests.\n\nOption A seems to be a slight better option here","poster":"Heer","timestamp":"1674985140.0","upvote_count":"1","comment_id":"791519"},{"poster":"KennethTam","content":"D is correct","comment_id":"572717","timestamp":"1647931140.0","upvote_count":"3"},{"poster":"Ishu_awsguy","comments":[{"comment_id":"581064","poster":"adsdadasdad","timestamp":"1649140080.0","content":"They are more tough than associate and require more knowledge, if they seem easy it shows how little you actually know.","upvote_count":"5"},{"timestamp":"1665923040.0","upvote_count":"1","poster":"wassb","comment_id":"696228","content":"Don't think so. \nAssociate lvl answer in this case would simply be : \"Configure Multi AZ on your RDS\" ... and you are done"}],"comment_id":"543516","content":"These look like associate level questions","upvote_count":"2","timestamp":"1644379140.0"},{"content":"Agree on D","timestamp":"1641814500.0","poster":"tkanmani76","comment_id":"520814","upvote_count":"1"},{"timestamp":"1641294360.0","upvote_count":"4","comment_id":"516506","content":"A is working option, but with data loss. Solution I really won't recommend as automatic solution. D seems to be right answer.","poster":"Riho"}],"answers_community":["D (100%)"],"topic":"1","answer_ET":"D","isMC":true,"exam_id":32},{"id":"J9WU0lp4GrcYdzetHULk","answer":"E","unix_timestamp":1641232500,"timestamp":"2022-01-03 18:55:00","discussion":[{"timestamp":"1723808880.0","content":"Selected Answer: E\nWe want Glacier deep archive as that is the cheapest and they can wait for the files in DR solution.\nFile gateway is a type of storage gateway, which leave 2 correct answers. Go with file gateway as that is the correct type. E. \nBut its a poor question as Storage Gateway isn't wrong, just not specific.","comment_id":"1267024","upvote_count":"1","poster":"devilman222"},{"timestamp":"1665800520.0","comment_id":"695130","poster":"AwsBRFan","upvote_count":"1","content":"Selected Answer: E\nKey=NFS, so file gateway.\nhttps://aws.amazon.com/storagegateway/?nc1=h_ls"},{"content":"Between A and E...\n\nI would go for A since Data in S3 can move to Deep Glacier only after 90days.","upvote_count":"2","poster":"skywalker","comment_id":"691607","timestamp":"1665447240.0"},{"upvote_count":"3","timestamp":"1657973280.0","comment_id":"632147","poster":"asfsdfsdf","content":"Selected Answer: E\nE most cost effective and file gateway"},{"comment_id":"624365","timestamp":"1656467460.0","poster":"TechX","upvote_count":"4","content":"Selected Answer: E\n100% E\nFile gateway support NFS protocol, while volume gateway support iCSI protocol. And we need glacier deep archive to save cost, cause the company willing to wait for few days retrival time."},{"poster":"Bigbearcn","timestamp":"1644365940.0","upvote_count":"3","content":"Selected Answer: E\nIt's E.","comment_id":"543431"},{"content":"Selected Answer: E\nI go for E. \"a few days\" is enough. The new Glacier Deep Archive storage class is designed to provide durable and secure long-term storage for large amounts of data at a price that is competitive with off-premises tape archival services. Data is stored across 3 or more AWS Availability Zones and can be retrieved in 12 hours or less.","poster":"Akaza","upvote_count":"3","timestamp":"1641707340.0","comment_id":"519939"},{"content":"Confirmed E","upvote_count":"1","comment_id":"517828","timestamp":"1641419340.0","poster":"Firelord"},{"content":"E\nOnly File Gateway will support NFS and since the wait time for retrieval is few days Glacier deep archive is also fit.","timestamp":"1641316500.0","upvote_count":"2","poster":"krisvija12","comment_id":"516857"},{"poster":"kemalgoklen","timestamp":"1641232500.0","comments":[{"timestamp":"1648440180.0","content":"It said \"file storage system\". It think it should be E.","upvote_count":"2","poster":"Nano803","comment_id":"576601"},{"timestamp":"1665923340.0","comment_id":"696231","content":"NFS/SMB protocol goes with the Storage Gateway","upvote_count":"1","poster":"wassb"}],"comment_id":"515944","content":"Selected Answer: B\ncost-effectiveness is the key while chosing the tier of s3.\nOnce they're ok to wait couple of days it should be Glacier Deep Archive.\nDeep Archive option rules out A,C,D\nOnce they have custom solution for storage it should be volume gateway","upvote_count":"1"}],"topic":"1","answers_community":["E (94%)","6%"],"question_id":819,"question_images":[],"answer_ET":"E","isMC":true,"exam_id":32,"answer_description":"","choices":{"C":"Deploy an AWS Storage Gateway tape gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.","D":"Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.","E":"Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.","A":"Deploy an AWS Storage Gateway files gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the file to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.","B":"Deploy an AWS Storage Gateway volume gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the volume gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days."},"answer_images":[],"question_text":"A company wants to use Amazon S3 to back up its on-premises file storage solution. The company's on-premises file storage solution supports NFS, and the company wants its new solution to support NFS. The company wants to archive the backup files after 5 days. If the company needs archived files for disaster recovery, the company is willing to wait a few days for the retrieval of those files.\nWhich solution meets these requirements MOST cost-effectively?","url":"https://www.examtopics.com/discussions/amazon/view/69380-exam-aws-certified-solutions-architect-professional-topic-1/"},{"id":"Qaq2eqgMRn2Puiy1Aqvz","unix_timestamp":1641071940,"answer_images":[],"answer_description":"","question_id":820,"discussion":[{"poster":"RVivek","comment_id":"546398","timestamp":"1644748680.0","upvote_count":"10","content":"Selected Answer: A\nThe key is Data tarsfer should be completed in 3 week.\n\nB: wrong /Getting a Direct connect will take at least 1 month\nC & D will take arprximately two years to complete Data taransfer"},{"content":"A, it executes just once","timestamp":"1641071940.0","upvote_count":"5","comment_id":"514684","poster":"Firelord"},{"comment_id":"1295029","content":"Selected Answer: A\nKey Words : within 3 week, one-time transfer + MOST cost-effectively = Snow Family","poster":"nimbus_00","upvote_count":"1","timestamp":"1728461280.0"},{"comment_id":"956946","content":"Correct A.","upvote_count":"1","timestamp":"1689804540.0","poster":"ggrodskiy"},{"poster":"asfsdfsdf","comment_id":"632148","timestamp":"1657973520.0","upvote_count":"1","content":"Selected Answer: A\nA is the most cost effective and quickest + need to do one time transfer"},{"comment_id":"593344","upvote_count":"2","timestamp":"1651083840.0","content":"Selected Answer: A\nA \nB is not an option...once you added the vpn tunnel the max bandwidth is 1.25 Gbps regardless of the DX bandwidth","poster":"snakecharmer2"},{"upvote_count":"3","timestamp":"1643349300.0","content":"Snowball edge has 80 TB of usable HDD storage. We need at least 8 of these to transfer 600 TB data. Based on the pricing here https://aws.amazon.com/snowball/pricing/ for each snowball device, it will not come cheap.\n\nTransferring 60 TB or 600,000GB of data via a 10Gbps connection will only take around 60,000 secs, 1,000 mins or approx 16.7 hours. I think a 1 month 10Gbps connection will be much much cheaper than Snowball.","comments":[{"content":"So yes - i think the original answer B makes sense.","poster":"timlow84","timestamp":"1643349780.0","comment_id":"534444","upvote_count":"1","comments":[{"comment_id":"824595","upvote_count":"1","poster":"enzomv","content":"The company needs to complete the data transfer to AWS within 3 weeks and the measured upload speed of the company's internet connection is 100 Mbps:\n100 Mbps = 12.5 MB/s --> 12.5x60x60x24 = 1080000 MB In one day, it transfers 1,08 TB.\n\nIn 3 weeks, it transfers only 7x3x1.08=22.68 TB!\n\nSo the answer is A.","timestamp":"1677577980.0"}]},{"upvote_count":"1","content":"yeah but you only have 3 weeks, which is not even enough for the DX setup","comment_id":"689829","poster":"[Removed]","timestamp":"1665279240.0"}],"comment_id":"534440","poster":"timlow84"},{"comment_id":"520865","poster":"frankzeng","timestamp":"1641820560.0","upvote_count":"1","content":"A. Direct connect takes one month to create","comments":[{"upvote_count":"1","comment_id":"843966","timestamp":"1679243820.0","content":"The company needs to complete the data transfer to AWS within 3 weeks!","poster":"enzomv"}]}],"answers_community":["A (100%)"],"choices":{"B":"Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.","C":"Create a VPN connection between the on-premises network storage and the nearest AWS Region. Transfer the data over the VPN connection.","A":"Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.","D":"Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway."},"answer":"A","answer_ET":"A","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/69220-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.\nThe company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps, and multiple departments share the connection.\nWhich solution will meet these requirements MOST cost-effectively?","question_images":[],"timestamp":"2022-01-01 22:19:00","isMC":true,"exam_id":32}],"exam":{"isMCOnly":false,"name":"AWS Certified Solutions Architect - Professional","isImplemented":true,"id":32,"isBeta":false,"provider":"Amazon","lastUpdated":"11 Apr 2025","numberOfQuestions":1019},"currentPage":164},"__N_SSP":true}