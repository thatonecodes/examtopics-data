{"pageProps":{"questions":[{"id":"MV2tErJU0FxIOxyDHwrk","answer_description":"","question_text":"A company is building custom deep learning models in Amazon SageMaker by using training and inference containers that run on Amazon EC2 instances. The company wants to reduce training costs but does not want to change the current architecture. The SageMaker training job can finish after interruptions. The company can wait days for the results.\n\nWhich combination of resources should the company use to meet these requirements MOST cost-effectively? (Choose two.)","question_images":[],"isMC":true,"choices":{"C":"Reserved Instances","D":"Incremental training","B":"Checkpoints","E":"Spot instances","A":"On-Demand Instances"},"exam_id":26,"answer_ET":"BE","topic":"1","answer_images":[],"unix_timestamp":1702814220,"answer":"BE","url":"https://www.examtopics.com/discussions/amazon/view/128798-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2023-12-17 12:57:00","answers_community":["BE (100%)"],"discussion":[{"timestamp":"1721387760.0","upvote_count":"5","comment_id":"1126691","poster":"prash_vz","content":"Selected Answer: BE\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html"},{"upvote_count":"1","timestamp":"1725405540.0","content":"Selected Answer: BE\nto pick up where you left off, checkpointing is important and 'sport instances' to save cost","poster":"vkbajoria","comment_id":"1165213"},{"poster":"Adzz","comment_id":"1160509","content":"Selected Answer: BE\nCheckpoints & Spot instances","timestamp":"1724751060.0","upvote_count":"1"},{"comment_id":"1104274","timestamp":"1719167340.0","poster":"taustin2","content":"Selected Answer: BE\nSpot instances and checkpoints.","upvote_count":"2"},{"timestamp":"1718807160.0","upvote_count":"2","poster":"taustin2","content":"Selected Answer: BE\nSpot Instance are cheapest and can be used with Checkpoints.","comment_id":"1100809"},{"poster":"aquanaveen","content":"Selected Answer: BE\nTo meet the requirements of reducing training costs and being cost-effective in an Amazon SageMaker environment, the company should consider the following combination of resources:\n\nE. Spot Instances: Spot Instances are spare EC2 instances that are available at a lower cost compared to On-Demand Instances.\nBy using Spot Instances for training, the company can significantly reduce the cost of running SageMaker training jobs.\nB. Checkpoints: Checkpoints allow the model training process to save the model's current state during training. If the training job is interrupted (e.g., due to a Spot Instance termination), the model can resume from the last saved checkpoint rather than starting from scratch.","upvote_count":"2","comment_id":"1098860","timestamp":"1718618220.0"}],"question_id":206},{"id":"J9csGd12aptLN1q8YVqF","url":"https://www.examtopics.com/discussions/amazon/view/128799-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"B","discussion":[{"comment_id":"1104280","upvote_count":"2","timestamp":"1719167520.0","content":"Selected Answer: B\nB. Blazing Text for text classification.","poster":"taustin2"},{"poster":"taustin2","timestamp":"1718807520.0","upvote_count":"2","content":"Selected Answer: B\nBlazingText's implements a supervised multi-class, multi-label text classification algorithm.","comment_id":"1100819"},{"comment_id":"1098862","upvote_count":"1","content":"Selected Answer: B\nB. Use the SageMaker BlazingText algorithm.\n\nExplanation:\n\nBlazingText for Text Classification:\n\nSageMaker BlazingText is designed for efficient and scalable text classification tasks.\nIt supports multi-class classification, making it suitable for the scenario where user feedback needs to be classified into fixed categories.\nBlazingText uses a fast implementation of the Word2Vec algorithm, making it highly performant.","timestamp":"1718618460.0","poster":"aquanaveen"}],"timestamp":"2023-12-17 13:01:00","isMC":true,"answer_images":[],"question_id":207,"answer_description":"","choices":{"A":"Use the SageMaker Latent Dirichlet Allocation (LDA) algorithm.","C":"Use the SageMaker Neural Topic Model (NTM) algorithm.","D":"Use the SageMaker CatBoost algorithm.","B":"Use the SageMaker BlazingText algorithm."},"topic":"1","exam_id":26,"question_text":"A company hosts a public web application on AWS. The application provides a user feedback feature that consists of free-text fields where users can submit text to provide feedback. The company receives a large amount of free-text user feedback from the online web application. The product managers at the company classify the feedback into a set of fixed categories including user interface issues, performance issues, new feature request, and chat issues for further actions by the company's engineering teams.\n\nA machine learning (ML) engineer at the company must automate the classification of new user feedback into these fixed categories by using Amazon SageMaker. A large set of accurate data is available from the historical user feedback that the product managers previously classified.\n\nWhich solution should the ML engineer apply to perform multi-class text classification of the user feedback?","answers_community":["B (100%)"],"question_images":[],"answer":"B","unix_timestamp":1702814460},{"id":"FHJcT0lG6xxgo66bzUOv","answer_images":[],"discussion":[{"comment_id":"1097777","content":"B\n1.Data Cleaning: SageMaker Data Wrangler is designed for data preparation tasks, including handling missing values, duplicates, and rare values. It provides a visual interface to clean and transform tabular data efficiently. This addresses the data cleaning requirements mentioned in the question.\n\n2.Model Training: Using the built-in SageMaker XGBoost algorithm is a common and effective choice for classification tasks like customer churn prediction. XGBoost is a powerful and widely used algorithm for binary classification problems.","upvote_count":"7","timestamp":"1702686720.0","poster":"xiaoeason"},{"comment_id":"1098863","content":"Selected Answer: B\nB. Use SageMaker Data Wrangler to clean the data. Use the built-in SageMaker XGBoost algorithm to train a classification model.\n\nExplanation:\n\nSageMaker Data Wrangler:\n\nSageMaker Data Wrangler is designed for efficient data cleaning and preparation.\nIt provides a visual interface that simplifies the process of cleaning tabular data, handling missing values, and addressing duplicate or rare values.\nData Wrangler can generate the necessary preprocessing code automatically, reducing the development effort.\nSageMaker XGBoost (for Classification):\n\nXGBoost is a popular and powerful algorithm for classification tasks, including customer churn prediction.\nSageMaker provides a built-in XGBoost algorithm, making it easy to train a classification model without the need for extensive coding.","upvote_count":"7","timestamp":"1702814640.0","poster":"aquanaveen"},{"poster":"ef12052","timestamp":"1742974680.0","comment_id":"1410275","content":"Selected Answer: A\nhttps://aws.amazon.com/it/blogs/machine-learning/predicting-customer-churn-with-no-code-machine-learning-using-amazon-sagemaker-canvas/","upvote_count":"1"},{"poster":"MultiCloudIronMan","upvote_count":"1","comment_id":"1305968","timestamp":"1730488920.0","content":"Selected Answer: A\nOption B involves using SageMaker Data Wrangler to clean the data and the built-in SageMaker XGBoost algorithm to train a classification model. While this is a valid approach, it requires more manual intervention and development effort compared to using SageMaker Canvas."},{"upvote_count":"1","content":"Selected Answer: A\nA is correct","comment_id":"1297348","poster":"SamHan","timestamp":"1728895080.0"},{"poster":"pandkast","content":"Selected Answer: B\nSageMaker Canvas is an excellent tool for those without ML expertise to build models, but it may not provide the detailed control needed for data cleaning and may not be as robust as Data Wrangler for complex cleaning tasks.","comment_id":"1235574","upvote_count":"2","timestamp":"1719085080.0"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://aws.amazon.com/tw/about-aws/whats-new/2022/05/amazon-sagemaker-canvas-adds-new-data-capabilities-usability-updates/","timestamp":"1715167740.0","poster":"Peter_Hsieh","comment_id":"1208346"},{"upvote_count":"1","content":"Answer A- Sagemaker Canvas + categorical model \nReason :\nSageMaker Canvas:\nSageMaker Canvas is a no-code machine learning tool that allows users to perform data preparation, feature engineering, and model training with minimal technical expertise.\nIt automatically handles tasks like data cleaning, including the removal of duplicates, filling missing values, and managing rare categories.\nCategorical Model:\nA categorical (classification) model is the correct type for churn prediction, as it aims to classify whether a customer will stop using the service (churn) or not.\nSageMaker Canvas provides user-friendly tools to build and evaluate this type of model.","poster":"JonSno","timestamp":"1714889460.0","comment_id":"1206787"},{"upvote_count":"1","comment_id":"1189314","poster":"F1Fan","content":"While Amazon SageMaker Canvas can perform automatic data cleaning and preparation, it has certain limitations when it comes to handling complex data cleaning tasks.\n\nSageMaker Canvas is designed for building machine learning models with minimal code and effort, primarily targeting business analysts and non-technical users. It provides a guided user interface and automates many steps in the machine learning pipeline, including data cleaning and preparation.\n\nHowever, SageMaker Canvas has a set of built-in data cleaning and preparation operations, which may not be sufficient for handling all types of data quality issues or complex data transformations. If the data requires more advanced cleaning techniques or custom transformations, SageMaker Data Wrangler (option B) would be a better choice.","timestamp":"1712234100.0"},{"upvote_count":"4","content":"Selected Answer: A\nA is correct Canvas can do without writing single line of code","comment_id":"1165219","timestamp":"1709515680.0","poster":"vkbajoria"},{"comment_id":"1162493","upvote_count":"2","timestamp":"1709206680.0","poster":"Stokvisss","content":"Selected Answer: A\nThis can be done without code using SageMaker Canvas: https://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-no-code-machine-learning-using-amazon-sagemaker-canvas/\n\nHence, A is right."},{"comment_id":"1143063","poster":"kyuhuck","content":"Selected Answer: A\nThe best solution, meeting the requirements with the least development effort and correctly addressing the problem nature, is:\n\nA. Use SageMaker Canvas to automatically clean the data and to prepare a categorical model.\nThis option leverages the simplicity and automatic features of SageMaker Canvas, ensuring minimal development effort while accurately targeting the need for a classification model in customer churn prediction.","upvote_count":"1","timestamp":"1707294600.0"},{"comment_id":"1104287","upvote_count":"1","content":"Selected Answer: A\nSee: https://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-no-code-machine-learning-using-amazon-sagemaker-canvas/\nCanvas also does no-code data cleaning and preparation. So, least development effort is Canvas.","poster":"taustin2","timestamp":"1703364060.0"}],"timestamp":"2023-12-16 01:32:00","answer_description":"","question_text":"A digital media company wants to build a customer churn prediction model by using tabular data. The model should clearly indicate whether a customer will stop using the company's services. The company wants to clean the data because the data contains some empty fields, duplicate values, and rare values.\n\nWhich solution will meet these requirements with the LEAST development effort?","isMC":true,"exam_id":26,"choices":{"C":"Use SageMaker Canvas automatic data cleaning and preparation tools. Use the built-in SageMaker XGBoost algorithm to train a regression model.","D":"Use SageMaker Data Wrangler to clean the data. Use the SageMaker Autopilot to train a regression model","B":"Use SageMaker Data Wrangler to clean the data. Use the built-in SageMaker XGBoost algorithm to train a classification model.","A":"Use SageMaker Canvas to automatically clean the data and to prepare a categorical model."},"url":"https://www.examtopics.com/discussions/amazon/view/128698-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"A","topic":"1","answers_community":["A (57%)","B (43%)"],"question_images":[],"question_id":208,"answer":"A","unix_timestamp":1702686720},{"id":"SWHbqvh2o9r2DZllNlY5","question_text":"A data engineer is evaluating customer data in Amazon SageMaker Data Wrangler. The data engineer will use the customer data to create a new model to predict customer behavior.\n\nThe engineer needs to increase the model performance by checking for multicollinearity in the dataset.\n\nWhich steps can the data engineer take to accomplish this with the LEAST operational effort? (Choose two.)","discussion":[{"upvote_count":"2","comment_id":"1306332","content":"B,E\nhttps://aws.amazon.com/about-aws/whats-new/2021/08/detect-multicollinearity-amazon-sagemaker-data-wrangler/","timestamp":"1730583300.0","poster":"spinatram"},{"poster":"MultiCloudIronMan","content":"Selected Answer: BE\nUse SageMaker Data Wrangler diagnostic visualization. Use principal components analysis (PCA) and singular value decomposition (SVD) to calculate singular values (Option B). PCA and SVD are effective techniques for identifying multicollinearity by reducing the dimensionality of the data and highlighting the relationships between variables1.\nUse SageMaker Data Wrangler diagnostic visualization. Use least absolute shrinkage and selection operator (LASSO) to plot coefficient values from a LASSO model that is trained on the dataset (Option E). LASSO helps in identifying and mitigating multicollinearity by shrinking some coefficients to zero, effectively selecting a subset of predictors","timestamp":"1727202840.0","comment_id":"1288738","upvote_count":"1"},{"upvote_count":"1","poster":"JonSno","timestamp":"1714889700.0","content":"B. Use SageMaker Data Wrangler diagnostic visualization. Use principal components analysis (PCA) and singular value decomposition (SVD) to calculate singular values.\n\nPCA and SVD: These methods help identify multicollinearity by reducing the dataset's dimensionality, revealing relationships among variables. Multicollinear features often become evident through high correlations in principal components or singular values.\nC. Use the SageMaker Data Wrangler Quick Model visualization to quickly evaluate the dataset and to produce importance scores for each feature.\n\nQuick Model Visualization: This feature enables rapid evaluation of feature importance scores, which can help detect multicollinearity by identifying features that may be overly correlated and thus less impactful independently.","comment_id":"1206788"},{"upvote_count":"1","poster":"vkbajoria","comment_id":"1165220","timestamp":"1709515920.0","content":"Selected Answer: BE\nB and E make sense"},{"timestamp":"1705668360.0","comment_id":"1126674","content":"Selected Answer: BE\nhttps://aws.amazon.com/about-aws/whats-new/2021/08/detect-multicollinearity-amazon-sagemaker-data-wrangler/","upvote_count":"3","poster":"prash_vz"},{"upvote_count":"2","content":"Selected Answer: BE\nPCA and SVD calculate singular values, which indicate the contribution of each feature to the overall variance. Features with high singular values have less multicollinearity. \n\nLASSO regularization shrinks coefficient values of highly correlated features towards zero, highlighting potential multicollinearity through their relative sizes.","timestamp":"1703004120.0","poster":"taustin2","comment_id":"1100827"},{"content":"Selected Answer: BD\nB. Use SageMaker Data Wrangler diagnostic visualization. Use principal components analysis (PCA) and singular value decomposition (SVD) to calculate singular values.\n\nPCA and SVD can help in identifying multicollinearity by analyzing the correlation structure of the variables. High condition numbers or small singular values may indicate multicollinearity issues.\nD. Use the SageMaker Data Wrangler Min Max Scaler transform to normalize the data.\n\nNormalizing the data using techniques like Min-Max scaling can mitigate the impact of multicollinearity. Normalization helps in bringing the features to a similar scale, reducing the sensitivity to differences in magnitudes.","upvote_count":"1","poster":"aquanaveen","comment_id":"1098871","timestamp":"1702815000.0"},{"comment_id":"1097779","content":"B and E\nExplanation:\n\nOption B: Principal components analysis (PCA) and singular value decomposition (SVD) are techniques used to identify multicollinearity in a dataset. By visualizing the singular values, the data engineer can assess the level of multicollinearity present in the features. This approach is effective for detecting relationships among variables.\n\nOption E: LASSO (Least Absolute Shrinkage and Selection Operator) is a regularization technique that can be used to penalize certain coefficients and, in turn, highlight the most important features. By plotting the coefficient values from a LASSO model, the data engineer can identify variables that contribute the most to the model. This can be useful for identifying and mitigating multicollinearity.","timestamp":"1702686960.0","poster":"xiaoeason","upvote_count":"1"}],"question_id":209,"url":"https://www.examtopics.com/discussions/amazon/view/128699-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"BE","answers_community":["BE (88%)","13%"],"answer_description":"","question_images":[],"answer_images":[],"unix_timestamp":1702686960,"exam_id":26,"choices":{"E":"Use SageMaker Data Wrangler diagnostic visualization. Use least absolute shrinkage and selection operator (LASSO) to plot coefficient values from a LASSO model that is trained on the dataset.","C":"Use the SageMaker Data Wrangler Quick Model visualization to quickly evaluate the dataset and to produce importance scores for each feature.","A":"Use SageMaker Data Wrangler to refit and transform the dataset by applying one-hot encoding to category-based variables.","B":"Use SageMaker Data Wrangler diagnostic visualization. Use principal components analysis (PCA) and singular value decomposition (SVD) to calculate singular values.","D":"Use the SageMaker Data Wrangler Min Max Scaler transform to normalize the data."},"topic":"1","answer":"BE","timestamp":"2023-12-16 01:36:00","isMC":true},{"id":"yryq1PTAv5JYLEstiSkO","url":"https://www.examtopics.com/discussions/amazon/view/133244-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"D","discussion":[{"comment_id":"1341818","content":"Selected Answer: C\nDirect Access","poster":"CloudGyan","upvote_count":"1","timestamp":"1737055200.0"},{"upvote_count":"1","comment_id":"1328258","poster":"LeoD","timestamp":"1734495420.0","content":"Selected Answer: C\nKinesis Data Stream do not support to save data to S3 directly. -- B is out\nKinesis Data Firehose do not access data from S3 directly. -- D is out"},{"poster":"MultiCloudIronMan","upvote_count":"2","content":"Selected Answer: C\nDirect Access: Using an API call allows QuickSight to access the data in DynamoDB directly, ensuring near real-time insights without the need for intermediate steps like exporting data to S3.\nMinimal Latency: This approach minimizes latency since it eliminates the delay associated with data transfer and storage in S3.","timestamp":"1729362660.0","comment_id":"1300122"},{"upvote_count":"1","content":"Selected Answer: D\nKinesis Data Stream didnt support any feature to save data to s3 directly, so the answer is D.\n\nif it mention that a lambda after Data Stream to consume data to s3, the B is the LEAST delay option.","timestamp":"1728186180.0","comment_id":"1293717","poster":"MJSY"},{"content":"ChatGPT","poster":"INeedtopassthisexam","timestamp":"1723443900.0","comment_id":"1264511","upvote_count":"1"},{"timestamp":"1714250460.0","poster":"ggrodskiy","content":"B\nAmazon QuickSight dashboard to display near real-time! order insights\nB provides the most efficient solution for near real-time access to new order information in QuickSight","comment_id":"1203281","upvote_count":"1"},{"content":"Selected Answer: D\nOption C involves using an API call from QuickSight to access the data directly in Amazon DynamoDB. While this option can provide real-time access to the data, it requires direct integration between QuickSight and DynamoDB, which may involve additional development effort. Additionally, QuickSight's native integration with DynamoDB for real-time data access might be limited compared to its integration with data stored in Amazon S3. Therefore, while option C might offer real-time access, option D with Kinesis Data Firehose to S3 could be a more robust and scalable solution, especially considering the potential limitations of direct DynamoDB integration with QuickSight.","upvote_count":"1","poster":"Denise123","timestamp":"1714224240.0","comment_id":"1203131"},{"poster":"vkbajoria","comment_id":"1186507","timestamp":"1711847160.0","content":"Selected Answer: D\nD is the best solution given options.\n\nif not directly, QuickSight can connect to DynamoDB via Athena using a connector\nhttps://aws.amazon.com/blogs/big-data/visualize-amazon-dynamodb-insights-in-amazon-quicksight-using-the-amazon-athena-dynamodb-connector-and-aws-glue/","upvote_count":"3"},{"timestamp":"1709788560.0","comment_id":"1167670","poster":"AIWave","upvote_count":"4","content":"Selected Answer: D\nQuicksight doesn't integrate with DynamoDB directly. It could use S3, Redshift, Aurora/RDS, Athena, IOT analytics and EC2 hosted databases as data sources. Glouw would work as well but Firehose (D) is the least delay option."},{"upvote_count":"2","comment_id":"1162487","comments":[{"comment_id":"1167668","poster":"AIWave","timestamp":"1709788200.0","upvote_count":"2","content":"Quicksight doesn't integrate with DynamoDB directly"}],"content":"Selected Answer: C\nQuickSight provides the ability to connect to various data sources, including DynamoDB, to create visualizations and dashboards. QuickSight supports a direct connection to DynamoDB tables, allowing you to query and visualize data stored in DynamoDB in real-time.\n\nNo need to consume a DynamoDB stream with firehose. C is right.","poster":"Stokvisss","timestamp":"1709206200.0"},{"timestamp":"1707295020.0","upvote_count":"2","content":"Selected Answer: D\nhe best solution, considering the requirement for the least delay and the ability to handle continuous data flow efficiently, would be:\n\nD. Use Amazon Kinesis Data Firehose to export the data from Amazon DynamoDB to Amazon S3, and configure QuickSight to access the data in Amazon S3.\nThis solution leverages the automatic, scalable streaming capture of Kinesis Data Firehose to move data into S3, where it can be readily accessed by QuickSight for analytics and visualization purposes. This approach balances the need for near real-time insights with the capabilities of AWS services to handle streaming data effectively.","poster":"kyuhuck","comment_id":"1143067"}],"timestamp":"2024-02-07 09:37:00","isMC":true,"answer_images":[],"question_id":210,"choices":{"A":"Use AWS Glue to export the data from Amazon DynamoDB to Amazon S3. Configure QuickSight to access the data in Amazon S3.","B":"Use Amazon Kinesis Data Streams to export the data from Amazon DynamoDB to Amazon S3. Configure QuickSight to access the data in Amazon S3.","C":"Use an API call from QuickSight to access the data that is in Amazon DynamoDB directly.","D":"Use Amazon Kinesis Data Firehose to export the data from Amazon DynamoDB to Amazon S3. Configure QuickSight to access the data in Amazon S3."},"answer_description":"","topic":"1","exam_id":26,"question_text":"A company processes millions of orders every day. The company uses Amazon DynamoDB tables to store order information. When customers submit new orders, the new orders are immediately added to the DynamoDB tables. New orders arrive in the DynamoDB tables continuously.\n\nA data scientist must build a peak-time prediction solution. The data scientist must also create an Amazon QuickSight dashboard to display near real-time order insights. The data scientist needs to build a solution that will give QuickSight access to the data as soon as new order information arrives.\n\nWhich solution will meet these requirements with the LEAST delay between when a new order is processed and when QuickSight can access the new order information?","answers_community":["D (65%)","C (35%)"],"answer":"D","question_images":[],"unix_timestamp":1707295020}],"exam":{"provider":"Amazon","numberOfQuestions":369,"id":26,"isBeta":false,"isImplemented":true,"isMCOnly":false,"name":"AWS Certified Machine Learning - Specialty","lastUpdated":"11 Apr 2025"},"currentPage":42},"__N_SSP":true}