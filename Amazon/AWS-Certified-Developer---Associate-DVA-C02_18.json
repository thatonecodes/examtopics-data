{"pageProps":{"questions":[{"id":"Gus2M0oXpFG8EJOgjv0i","url":"https://www.examtopics.com/discussions/amazon/view/122596-exam-aws-certified-developer-associate-dva-c02-topic-1/","discussion":[{"upvote_count":"1","poster":"65703c1","timestamp":"1732373160.0","content":"Selected Answer: D\nD is the correct answer.","comment_id":"1216575"},{"upvote_count":"4","timestamp":"1721494020.0","poster":"SerialiDr","comment_id":"1127450","content":"Selected Answer: D\nAWS Systems Manager Parameter Store is specifically designed for managing configuration data and secrets. It can store large numbers of parameters, including environment variables, and makes them easily accessible and manageable. It also provides features like versioning, fine-grained access control, and integration with AWS Identity and Access Management (IAM)."},{"timestamp":"1712823240.0","content":"Selected Answer: D\nBest solution is D","upvote_count":"4","poster":"dilleman","comment_id":"1040357"},{"poster":"Digo30sp","upvote_count":"4","content":"Selected Answer: D\nD) https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html","comment_id":"1026574","timestamp":"1712405220.0"}],"exam_id":24,"answer_images":[],"choices":{"D":"Use AWS Systems Manager Parameter Store to store large numbers of environment variables.","C":"Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables.","A":"Add the export LC_ALL=\"en_US.utf8\" command to the pre_build section to ensure POSIX localization.","B":"Use Amazon Cognito to store key-value pairs for large numbers of environment variables."},"question_images":[],"topic":"1","answer_ET":"D","unix_timestamp":1696552140,"isMC":true,"answers_community":["D (100%)"],"answer":"D","question_text":"When a developer tries to run an AWS CodeBuild project, it raises an error because the length of all environment variables exceeds the limit for the combined maximum of characters.\n\nWhat is the recommended solution?","question_id":86,"answer_description":"","timestamp":"2023-10-06 02:29:00"},{"id":"I5EByQaKBFnNB4hRnCr4","exam_id":24,"unix_timestamp":1696552320,"url":"https://www.examtopics.com/discussions/amazon/view/122597-exam-aws-certified-developer-associate-dva-c02-topic-1/","isMC":true,"answer_description":"","question_id":87,"answer_ET":"D","discussion":[{"content":"This appear at 17 Jun exam","poster":"tsangckl","timestamp":"1718597940.0","upvote_count":"1","comment_id":"1231710"},{"poster":"65703c1","comment_id":"1216577","timestamp":"1716468480.0","content":"Selected Answer: D\nD is the correct answer.","upvote_count":"2"},{"upvote_count":"1","content":"C\nI dont see any aws docs about Cloutfront cache=> so maybe it is cost-effective","comment_id":"1190317","timestamp":"1712394360.0","poster":"Melisa202401"},{"timestamp":"1706931900.0","poster":"joshnort","content":"Selected Answer: D\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/resizing-images-with-amazon-cloudfront-lambdaedge-aws-cdn-blog/","comment_id":"1138987","upvote_count":"3"},{"timestamp":"1705776720.0","comment_id":"1127453","poster":"SerialiDr","upvote_count":"4","content":"Selected Answer: D\nThis solution is the most cost-effective. Lambda@Edge processes the photos dynamically based on the device's requirements, which means no pre-generation of multiple variants is required. Processed photos are stored on S3, ensuring that subsequent requests for the same photo variant are served directly from S3, reducing Lambda@Edge invocations and further optimizing costs."},{"poster":"Mimi666","upvote_count":"4","comment_id":"1090449","content":"Selected Answer: D\nhttps://aws.amazon.com/es/blogs/networking-and-content-delivery/image-optimization-using-amazon-cloudfront-and-aws-lambda/","timestamp":"1701967740.0"},{"comments":[{"upvote_count":"1","content":"CloudFront has a Maximum TTL of 365 days. Would it not be cheaper to store the images in the CloudFront cache, instead of storing it in S3 which would incur costs?\n\nWe may need to assume it would be unlikely the users would access the same photo more than a year after the initial access.","comment_id":"1254643","poster":"BrainFried","timestamp":"1721869560.0"}],"comment_id":"1063838","timestamp":"1699274820.0","upvote_count":"1","content":"According to https://aws.amazon.com/blogs/networking-and-content-delivery/resizing-images-with-amazon-cloudfront-lambdaedge-aws-cdn-blog/, \"static resources like images should have a long Time to Live (TTL) as possible to improve cache-hit ratios.\". The photo cache here is likely to be static and should be preserved forever.","poster":"jingle4944"},{"poster":"ut18","comment_id":"1055989","timestamp":"1698472620.0","content":"Why not B?\nThe developer can use S3 Batch Operations to create new variants of the photos with the required dimensions and resolutions.","upvote_count":"1"},{"content":"Selected Answer: D\nYou only want to convert the pictures that get requests. If you convert them all through batch processing, you have wasted time and expense on any possible photo that never gets viewed. The Minimum TTL is set to 60 seconds, the Default TTL is set to 300 seconds, and the Maximum TTL is set to 3600 seconds. S3 is the way to go.","upvote_count":"2","comments":[{"comment_id":"1254644","timestamp":"1721869620.0","content":"CloudFront cache has a Maximum TTL of 365 days. Would it not be cheaper to store the images in the CloudFront cache, instead of storing it in S3 which would incur costs?\n\nWe may need to assume it would be unlikely the users would access the same photo more than a year after the initial access.","upvote_count":"1","poster":"BrainFried"}],"comment_id":"1046281","poster":"TallManDan","timestamp":"1697568480.0"},{"upvote_count":"1","content":"Selected Answer: D\nD is correct","comment_id":"1040360","poster":"dilleman","timestamp":"1697012220.0"},{"poster":"Digo30sp","upvote_count":"1","timestamp":"1696594080.0","content":"Selected Answer: D\nD) https://www.examtopics.com/discussions/amazon/view/89564-exam-aws-certified-developer-associate-topic-1-question-320/","comment_id":"1026576"}],"topic":"1","choices":{"A":"Use S3 Batch Operations to invoke an AWS Lambda function to create new variants of the photos with the required dimensions and resolutions. Create a dynamic CloudFront origin that automatically maps the request of each device to the corresponding photo variant.","D":"Create a Lambda@Edge function that optimizes the photos upon request and returns the photos as a response. In the same function, store a copy of the processed photos on Amazon S3 for subsequent requests.","C":"Create a Lambda@Edge function that optimizes the photos upon request and returns the photos as a response. Change the CloudFront TTL cache policy to the maximum value possible.","B":"Use S3 Batch Operations to invoke an AWS Lambda function to create new variants of the photos with the required dimensions and resolutions. Create a Lambda@Edge function to route requests to the corresponding photo variant by using request headers."},"timestamp":"2023-10-06 02:32:00","answers_community":["D (100%)"],"answer_images":[],"answer":"D","question_images":[],"question_text":"A company is expanding the compatibility of its photo-sharing mobile app to hundreds of additional devices with unique screen dimensions and resolutions. Photos are stored in Amazon S3 in their original format and resolution. The company uses an Amazon CloudFront distribution to serve the photos. The app includes the dimension and resolution of the display as GET parameters with every request.\n\nA developer needs to implement a solution that optimizes the photos that are served to each device to reduce load time and increase photo quality.\n\nWhich solution will meet these requirements MOST cost-effectively?"},{"id":"ZZxFoOFwQax9jBU4U7ta","unix_timestamp":1696552380,"isMC":true,"topic":"1","question_images":[],"choices":{"A":"Add local secondary indexes (LSIs) for the trading data.","B":"Store the trading data in Amazon S3, and use S3 Transfer Acceleration.","D":"Use DynamoDB Accelerator (DAX) to cache the trading data.","C":"Add retries with exponential backoff for DynamoDB queries."},"answer_ET":"D","exam_id":24,"answer_images":[],"timestamp":"2023-10-06 02:33:00","answers_community":["D (100%)"],"question_text":"A company is building an application for stock trading. The application needs sub-millisecond latency for processing trade requests. The company uses Amazon DynamoDB to store all the trading data that is used to process each trading request.\n\nA development team performs load testing on the application and finds that the data retrieval time is higher than expected. The development team needs a solution that reduces the data retrieval time with the least possible effort.\n\nWhich solution meets these requirements?","answer":"D","discussion":[{"timestamp":"1732373400.0","poster":"65703c1","content":"Selected Answer: D\nD is the correct answer.","upvote_count":"2","comment_id":"1216578"},{"content":"Selected Answer: D\nhttps://aws.amazon.com/dynamodb/dax/","poster":"joshnort","upvote_count":"1","timestamp":"1722649620.0","comment_id":"1138990"},{"timestamp":"1721494560.0","upvote_count":"4","content":"Selected Answer: D\nDAX is an in-memory cache for DynamoDB that delivers fast read performance for your tables at scale by enabling you to get sub-millisecond response times for accessing your data. DAX is particularly beneficial for read-heavy and bursty workloads. Since it reduces the time to retrieve data, it's the most appropriate solution for achieving sub-millisecond latency in data retrieval.","poster":"SerialiDr","comment_id":"1127457"},{"content":"Selected Answer: D\nUse DynamoDB Accelerator (DAX)","poster":"JohnPl","timestamp":"1721344500.0","upvote_count":"2","comment_id":"1126300"},{"upvote_count":"2","timestamp":"1718766060.0","comment_id":"1100311","content":"Selected Answer: D\nhttps://aws.amazon.com/dynamodb/dax/\nDAX delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second. only pay for the capacity you provision.","poster":"TanTran04"},{"comment_id":"1040364","upvote_count":"3","timestamp":"1712823540.0","content":"Selected Answer: D\nThis is a perfect scenario for DAX so correct answer is D","poster":"dilleman"},{"poster":"Digo30sp","upvote_count":"3","timestamp":"1712405340.0","comment_id":"1026578","content":"Selected Answer: D\nD) https://www.examtopics.com/discussions/amazon/view/4971-exam-aws-certified-developer-associate-topic-1-question-14/"}],"answer_description":"","question_id":88,"url":"https://www.examtopics.com/discussions/amazon/view/122598-exam-aws-certified-developer-associate-dva-c02-topic-1/"},{"id":"pXv44Lo8cbfY508kGSB6","discussion":[{"timestamp":"1712405460.0","poster":"Digo30sp","comment_id":"1026580","content":"Selected Answer: BE\nThe correct answers are (E) and (B).\n\n(E) is the most important action to enable application request tracking using AWS X-Ray. The AWS X-Ray SDK for Python provides a set of APIs that a developer can use to instrument their application code for tracing.\n\n(B) is the second most important action. The AWS X-Ray daemon runs on each EC2 instance and collects application trace data","upvote_count":"9"},{"upvote_count":"1","comment_id":"1216581","poster":"65703c1","timestamp":"1732373520.0","content":"Selected Answer: BE\nBE is the correct answer."},{"upvote_count":"3","content":"Selected Answer: BE\nB. Install the AWS X-Ray daemon on the EC2 instances: This is a required step for enabling AWS X-Ray tracing. The X-Ray daemon listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. This is necessary for collecting and sending trace data from the application to X-Ray.\nE. Install and configure the AWS X-Ray SDK for Python in the application: This is a critical step for enabling X-Ray tracing in your Python application. The X-Ray SDK for Python provides classes and methods to collect data about the requests that your application serves, and sends this data to the X-Ray daemon.","comment_id":"1127460","poster":"SerialiDr","timestamp":"1721494800.0"},{"content":"Answer: E,B","comment_id":"1058278","poster":"NinjaCloud","timestamp":"1714504140.0","upvote_count":"3"},{"comment_id":"1040409","content":"Selected Answer: BE\nB and E","upvote_count":"4","timestamp":"1712826420.0","poster":"dilleman"}],"answer_description":"","answers_community":["BE (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/122600-exam-aws-certified-developer-associate-dva-c02-topic-1/","unix_timestamp":1696552500,"exam_id":24,"answer":"BE","question_text":"A developer is working on a Python application that runs on Amazon EC2 instances. The developer wants to enable tracing of application requests to debug performance issues in the code.\n\nWhich combination of actions should the developer take to achieve this goal? (Choose two.)","question_id":89,"answer_ET":"BE","choices":{"D":"Configure the application to write trace data to /var/log/xray.","E":"Install and configure the AWS X-Ray SDK for Python in the application.","B":"Install the AWS X-Ray daemon on the EC2 instances.","C":"Configure the application to write JSON-formatted logs to /var/log/cloudwatch.","A":"Install the Amazon CloudWatch agent on the EC2 instances."},"answer_images":[],"question_images":[],"timestamp":"2023-10-06 02:35:00","topic":"1","isMC":true},{"id":"gKJpO0MMq4zjxCecBikU","answer_description":"","answer_images":[],"unix_timestamp":1679398380,"answer_ET":"B","choices":{"B":"Publish custom metrics to CloudWatch that record the failures of the external payment processing API calls. Configure a CloudWatch alarm to notify the existing SNS topic when error rate exceeds the specified rate.","C":"Publish the results of the external payment processing API calls to a new Amazon SNS topic. Subscribe the support team members to the new SNS topic.","A":"Write the results of payment processing API calls to Amazon CloudWatch. Use Amazon CloudWatch Logs Insights to query the CloudWatch logs. Schedule the Lambda function to check the CloudWatch logs and notify the existing SNS topic.","D":"Write the results of the external payment processing API calls to Amazon S3. Schedule an Amazon Athena query to run at regular intervals. Configure Athena to send notifications to the existing SNS topic when the error rate exceeds the specified rate."},"isMC":true,"question_text":"A company is building a serverless application on AWS. The application uses an AWS Lambda function to process customer orders 24 hours a day, 7 days a week. The Lambda function calls an external vendor's HTTP API to process payments.\nDuring load tests, a developer discovers that the external vendor payment processing API occasionally times out and returns errors. The company expects that some payment processing API calls will return errors.\nThe company wants the support team to receive notifications in near real time only when the payment processing external API error rate exceed 5% of the total number of transactions in an hour. Developers need to use an existing Amazon Simple Notification Service (Amazon SNS) topic that is configured to notify the support team.\nWhich solution will meet these requirements?","question_images":[],"exam_id":24,"url":"https://www.examtopics.com/discussions/amazon/view/103466-exam-aws-certified-developer-associate-dva-c02-topic-1/","question_id":90,"timestamp":"2023-03-21 12:33:00","answers_community":["B (100%)"],"discussion":[{"poster":"Bibay","upvote_count":"14","content":"Selected Answer: B\nB. Publish custom metrics to CloudWatch that record the failures of the external payment processing API calls. Configure a CloudWatch alarm to notify the existing SNS topic when the error rate exceeds the specified rate is the best solution to meet the requirements.\n\nWith CloudWatch custom metrics, developers can publish and monitor custom data points, including the number of failed requests to the external payment processing API. A CloudWatch alarm can be configured to notify an SNS topic when the error rate exceeds the specified rate, allowing the support team to be notified in near real-time.\n\nOption A is not optimal since it involves scheduling a Lambda function to check the CloudWatch logs. Option C may not provide the desired functionality since it does not specify a rate at which to notify the support team. Option D is more complex than necessary, as it involves writing the results to S3 and configuring an Athena query to send notifications to an SNS topic.","comment_id":"890733","timestamp":"1683376140.0"},{"content":"Selected Answer: B\nThe correct answer is B.\nYou can use the Embedded Metrics format to embed custom metrics alongside detailed log event data. CloudWatch automatically extracts the custom metrics so you can visualize and alarm on them, for real-time incident detection.\nhttps://docs.aws.amazon.com/lambda/latest/operatorguide/custom-metrics.html","comment_id":"845860","upvote_count":"7","timestamp":"1679398380.0","poster":"Untamables"},{"poster":"sumanshu","upvote_count":"2","timestamp":"1734772920.0","content":"Selected Answer: B\nA) Eliminated - Requires manual log queries and a custom Lambda function to process logs, introducing operational overhead.\n\nB) Correct - CloudWatch Alarms can calculate error rates using metric math and automatically notify the SNS topic.\n\nC) Eliminated - This approach notifies the support team for every API failure, not just when the error rate exceeds 5%.\n\nD) Eliminated - Adds significant operational complexity (managing S3 storage, Athena queries, and scheduling).","comment_id":"1329898"},{"poster":"trieudo","content":"Selected Answer: B\nkeyword: error rate exceed 5%, receive notifications in near real time \n\n==> discard A,: use lamdba or athena with schedule, violate ' receive notifications in near real time '\n==> discard C: die in spam of message if any, and don't know when error up 5%\n\nB is most good solution: CloudWatch Alarm can calculate 5% with setup, and notify righ away when reach thresold set-up","comment_id":"1326094","timestamp":"1734088860.0","upvote_count":"1"},{"poster":"rue_","upvote_count":"1","timestamp":"1730002380.0","content":"Selected Answer: B\nIt's B, allows you to customize the metrics as required in the question, and sends notification in near real time instead of polling","comment_id":"1303479"},{"timestamp":"1716300780.0","content":"Selected Answer: B\nB is the correct answer.","comment_id":"1215013","poster":"65703c1","upvote_count":"1"},{"poster":"Tony88","content":"Selected Answer: B\nRequire \"near real-time\" notification, so you should not use scheduled solution.\nCreating a new SNS topic is no sense.","comments":[{"timestamp":"1699084440.0","content":"In the question, it is also mentioned that \"Developer needs to use the existing SNS topic....\"","poster":"Ponyi","comment_id":"1061954","upvote_count":"1"}],"timestamp":"1693742400.0","upvote_count":"2","comment_id":"997578"},{"upvote_count":"1","timestamp":"1691612280.0","comment_id":"977003","content":"Option B. Using custom metrics, Developers will be able to publish and monitor custom data points such as the no. of failed requests to the external payment processing API. Create a CloudWatch alarm and configure it to be triggered when the rate of error exceeds the specified number in the question.","poster":"jayvarma"},{"poster":"svrnvtr","timestamp":"1679431560.0","content":"Selected Answer: B\nIt is B","upvote_count":"3","comment_id":"846331"}],"answer":"B","topic":"1"}],"exam":{"provider":"Amazon","isMCOnly":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":551,"isImplemented":true,"isBeta":false,"id":24,"name":"AWS Certified Developer - Associate DVA-C02"},"currentPage":18},"__N_SSP":true}