{"pageProps":{"questions":[{"id":"159fCnGnZbIs0rqL79J4","topic":"1","answer":"C","isMC":true,"answer_description":"","unix_timestamp":1594257240,"answer_ET":"C","timestamp":"2020-07-09 03:14:00","exam_id":22,"question_images":[],"discussion":[{"comment_id":"434397","upvote_count":"7","poster":"aws4myself","timestamp":"1636243860.0","content":"Correct Answer: C"},{"comment_id":"1187440","poster":"storm0710","timestamp":"1711976640.0","upvote_count":"1","content":"B. Since requirement is of handling random activity peaks"},{"timestamp":"1692487320.0","upvote_count":"1","comment_id":"985470","content":"Selected Answer: C\nC. The security group assigned to the DB instance does not have the necessary rules to allow inbound connections from the application servers.","poster":"Pranava_GCP"},{"poster":"haison8x","upvote_count":"2","comment_id":"893680","content":"I got this question in the exam:\nan aurora cluster has some replica in current region and other regions\nthe admin accidentally delete primary cluster.\na. all replica are promoted to standalone\nb. all replica in other regions are promoted\nc. only replica in current region are promoted\nd. dont remember","comments":[{"upvote_count":"1","comment_id":"903678","poster":"Kodoma","content":"Do you know what the correct answer was?","timestamp":"1684723920.0"},{"upvote_count":"1","timestamp":"1695043620.0","comment_id":"1010636","poster":"chikorita","content":"please try to remember option D"}],"timestamp":"1683701100.0"},{"timestamp":"1651195380.0","poster":"novice_expert","comment_id":"594154","upvote_count":"2","content":"Selected Answer: C\nDB needs to allow inbound"},{"poster":"megadba","content":"Selected Answer: C\nCorrect Answer: C","upvote_count":"2","timestamp":"1650199560.0","comment_id":"587221"},{"upvote_count":"1","comments":[{"timestamp":"1636285560.0","comment_id":"444715","poster":"guru_ji","comments":[{"poster":"lihze","content":"Guru - 40 questions from out of 112 or 149? I can able to access only 112. Can you possible to share the dump if you have 149 questions.","timestamp":"1637980500.0","comment_id":"487791","upvote_count":"2"}],"upvote_count":"2","content":"I gave exam today. Only 40 questions came from here out of 65."}],"comment_id":"432509","poster":"guru_ji","timestamp":"1636190580.0","content":"Anyone passed \"AWS Certified Database - Specialty\" exam recently, could you please share your experience of actual exam??\nHow many Questions were came from examtopics Q set of 145 in real exam?\n\nYour comments will be highly appreciated.. Thanks.."},{"comment_id":"432434","comments":[{"upvote_count":"1","comment_id":"434019","poster":"guru_ji","timestamp":"1636214460.0","content":"Are you planning for exam?\nWe can share study material, it would be beneficial for both. You can email me on \"awsdbguru at gmail\""}],"poster":"aws4myself","timestamp":"1636173840.0","content":"Basically, it is a connection timeout error, so A, D can be deleted.\nB is wrong, because if it has a problem in SG rules, it should affect at outbound not inbound,\nHence C is correct, which is the inbound rule for DB instance SG.","upvote_count":"1"},{"poster":"guru_ji","content":"Answer is C\nAnyone preparing for DB Speciality and want to do group study with us, comment below with email","upvote_count":"1","timestamp":"1636080240.0","comment_id":"428738"},{"poster":"anandbabu","timestamp":"1636073280.0","upvote_count":"2","comment_id":"427549","content":"its C for me"},{"poster":"guru_ji","upvote_count":"1","timestamp":"1635943740.0","content":"C ==>> Correct Answer.","comment_id":"425796"},{"poster":"Dr_Kiko","comment_id":"414627","upvote_count":"1","timestamp":"1635705060.0","content":"It's C. I actually had to configure SG on my MYSQL instance to allow me connect from home PC with a SQL client :)"},{"upvote_count":"1","comment_id":"344754","poster":"db_interest","comments":[{"upvote_count":"1","comments":[{"content":"Yep, B looks right. Since autoscaling would increase the WCU/RCU which is not needed.","upvote_count":"1","comment_id":"358061","timestamp":"1635633900.0","poster":"db_interest"}],"content":"Looks like B","timestamp":"1635445200.0","comment_id":"355111","poster":"Zhongkai"}],"content":"A company has an application that uses an Amazon DynamoDB table to store user dat\na. Every morning, a single-threaded process calls the DynamoDB API Scan operation to scan the entire table and generate a critical start-of-day report for management. A successful marketing campaign recently doubled the number of items in the table, and now the process takes too long to run and the report is not generated in time. A database specialist needs to improve the performance of the process. The database specialist notes that, when the process is running, 15% of the table’s provisioned read capacity units (RCUs) are being used.\n\nWhat should the database specialist do?\nA. Enable auto scaling for the DynamoDB table.\nB. Use four threads and parallel DynamoDB API Scan operations.\nC. Double the table’s provisioned RCUs.\nD. Set the Limit and Offset parameters before every call to the API.","timestamp":"1635359520.0"},{"comment_id":"344719","poster":"db_interest","comments":[{"content":"random activity peaks -> Aurora Serverless -> B","upvote_count":"2","timestamp":"1635503640.0","comment_id":"355112","poster":"Zhongkai"},{"upvote_count":"1","timestamp":"1640537460.0","comment_id":"509712","poster":"ch321","content":"Should be B"}],"content":"A company is running an on-premises application comprised of a web tier, an application tier, and a\nMySQL database tier. The database is used primarily during business hours with random activity peaks\nthroughout the day. A database specialist needs to improve the availability and reduce the cost of the\nMySQL database tier as part of the company’s migration to AWS.\nWhich MySQL database option would meet these requirements?\nA. Amazon RDS for MySQL with Multi-AZ\nB. Amazon Aurora Serverless MySQL cluster\nC. Amazon Aurora MySQL cluster\nD. Amazon RDS for MySQL with read replica","timestamp":"1635338280.0","upvote_count":"1"},{"timestamp":"1634849040.0","content":"A company is running an on-premises application comprised of a web tier, an application tier, and a\nMySQL database tier. The database is used primarily during business hours with random activity peaks\nthroughout the day. A database specialist needs to improve the availability and reduce the cost of the\nMySQL database tier as part of the company’s migration to AWS.\nWhich MySQL database option would meet these requirements?\nA. Amazon RDS for MySQL with Multi-AZ\nB. Amazon Aurora Serverless MySQL cluster\nC. Amazon Aurora MySQL cluster\nD. Amazon RDS for MySQL with read replica","poster":"agrawalachin","upvote_count":"1","comment_id":"344712"},{"timestamp":"1634845980.0","comment_id":"342919","comments":[{"poster":"db_interest","upvote_count":"1","timestamp":"1635014580.0","content":"B. Since requirement is of handling random activity peaks","comments":[{"upvote_count":"1","poster":"db_interest","content":"Ignore the above answer. It's for another question :)","timestamp":"1635217920.0","comment_id":"344718"}],"comment_id":"344717"},{"poster":"Zhongkai","upvote_count":"4","content":"DDB cannot change Partition key of a table -> C/D wrong.\nDMS cannot choose DDB as source -> B wrong.\nSo A should be the correct one although it appears to need a lot of efforts.","comment_id":"355118","timestamp":"1635517140.0"}],"content":"A database specialist needs to review and optimize an Amazon DynamoDB table that is experiencing performance issues. A thorough investigation by the database specialist reveals that the partition key is causing hot partitions, so a new partition key is created. The database specialist must effectively apply this new partition key to all existing and new data.\nHow can this solution be implemented?\nA. Use Amazon EMR to export the data from the current DynamoDB table to Amazon S3. Then use Amazon EMR again to import the data from Amazon S3 into a new DynamoDB table with the new partition key.\nB. Use AWS DMS to copy the data from the current DynamoDB table to Amazon S3. Then import the DynamoDB table to create a new DynamoDB table with the new partition key.\nC. Use the AWS CLI to update the DynamoDB table and modify the partition key.\nD. Use the AWS CLI to back up the DynamoDB table. Then use the restore-table-from-backup command and modify the partition key.","poster":"agrawalachin","upvote_count":"1"},{"timestamp":"1634753760.0","upvote_count":"1","comment_id":"314103","poster":"LMax","content":"It's C"},{"content":"A company has a heterogeneous six-node production Amazon Aurora DB cluster that handles online transaction processing (OLTP) for the core business and OLAP reports for the human resources department. To match compute resources to the use case, the company has decided to have the reporting workload for the human resources department be directed to two small nodes in the Aurora DB cluster, while every other workload goes to four large nodes in the same DB cluster.\nWhich option would ensure that the correct nodes are always available for the appropriate workload while meeting these requirements?\n\nA. Use the writer endpoint for OLTP and the reader endpoint for the OLAP reporting workload.\nB. Use automatic scaling for the Aurora Replica to have the appropriate number of replicas for the desired workload.\nC. Create additional readers to cater to the different scenarios.\nD. Use custom endpoints to satisfy the different workloads.","comment_id":"297071","upvote_count":"1","comments":[{"content":"Hard to decide between A & D.. Leaning more towards A","poster":"kalyan_krishna742020","timestamp":"1634652060.0","upvote_count":"1","comment_id":"300596"},{"comment_id":"317822","content":"it's D.","poster":"ozan11","upvote_count":"2","timestamp":"1634819760.0"},{"comment_id":"355119","upvote_count":"1","poster":"Zhongkai","timestamp":"1635548400.0","content":"only custom endpoints could let you choose a specific node. so I choose D"}],"timestamp":"1634465940.0","poster":"RimaAws"},{"content":"A database specialist manages a critical Amazon RDS for MySQL DB instance for a company. The data stored daily could vary from .01% to 10% of the current database size. The database specialist needs to ensure that the DB instance storage grows as needed.\nWhat is the MOST operationally efficient and cost-effective solution?\n\nA. Configure RDS Storage Auto Scaling.\nB. Configure RDS instance Auto Scaling.\nC. Modify the DB instance allocated storage to meet the forecasted requirements.\nD. Monitor the Amazon CloudWatch FreeStorageSpace metric daily and add storage as required.","comments":[{"comments":[{"comment_id":"300958","timestamp":"1634711880.0","comments":[{"timestamp":"1634820720.0","upvote_count":"1","comment_id":"342891","poster":"agrawalachin","content":"You mean A?"}],"content":"changing it to Answer B after reviewing the question - \"storage grows as needed\"","poster":"kalyan_krishna742020","upvote_count":"2"}],"comment_id":"300546","timestamp":"1634642100.0","content":"Ans: C. Since we already know the expected size.","upvote_count":"1","poster":"kalyan_krishna742020"},{"comment_id":"355120","timestamp":"1635580740.0","upvote_count":"3","comments":[{"content":"A is correct since its efficient and cost-effective. We dont know if the variance would be 0.01 or 10%","comment_id":"359009","upvote_count":"1","poster":"db_interest","timestamp":"1635694020.0"}],"content":"A - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling","poster":"Zhongkai"},{"upvote_count":"2","comment_id":"508105","content":"It is A","poster":"jove","timestamp":"1640287680.0"}],"comment_id":"297069","upvote_count":"1","poster":"RimaAws","timestamp":"1634398320.0"},{"timestamp":"1634368860.0","comment_id":"297067","comments":[{"poster":"Windy","comments":[{"content":"Answer is A. I just figured it out.","timestamp":"1634598480.0","comment_id":"299839","upvote_count":"1","poster":"Windy"}],"content":"What is the answer?","upvote_count":"1","comment_id":"299835","timestamp":"1634575740.0"},{"comment_id":"355145","poster":"Zhongkai","timestamp":"1635589440.0","upvote_count":"1","content":"Looks like TLS is not enabled by default - https://docs.aws.amazon.com/documentdb/latest/developerguide/security.encryption.ssl.html\nso we need to enable it manually. -A"}],"poster":"RimaAws","content":"A company's database specialist disabled TLS on an Amazon DocumentDB cluster to perform benchmarking tests. A few days after this change was implemented, a database specialist trainee accidentally deleted multiple tables. The database specialist restored the database from available snapshots. An hour after restoring the cluster. the database specialist is still unable to connect to the new cluster endpoint. What should the database specialist do to connect to the new. restored Amazon DocumentDB cluster?\nA. Change the restored cluster's parameter group to the original cluster's custom parameter group.\nB. Change the restored cluster's parameter group to the Amazon DocumentDB default parameter group.\nC. Configure the interface VPC endpoint and associate the new Amazon DocumentDB cluster.\nD. Run the synclnstances command in AWS DataSync.","upvote_count":"1"},{"comment_id":"296263","content":"Ans: C","poster":"myutran","upvote_count":"2","timestamp":"1634227500.0"},{"comment_id":"283991","timestamp":"1633727220.0","poster":"RimaAws","comments":[{"timestamp":"1633908000.0","upvote_count":"2","poster":"Roontha","comment_id":"287785","content":"Answer: C"}],"upvote_count":"1","content":"A company has a 20 TB production Amazon Aurora DB cluster. The company runs a large batch job overnight to load data into the Aurora DB cluster. To ensure the company’s development team has the most up-to-date data for testing, a copy of the DB cluster must be available in the shortest possible time after the batch job completes.\nHow should this be accomplished?\n\nA. Use the AWS CLI to schedule a manual snapshot of the DB cluster. Restore the snapshot to a new DB cluster using the AWS CLI.\nB. Create a dump file from the DB cluster. Load the dump file into a new DB cluster.\nC. Schedule a job to create a clone of the DB cluster at the end of the overnight batch process.\nD. Set up a new daily AWS DMS task that will use cloning and change data capture (CDC) on the DB cluster to copy the data to a new DB cluster. Set up a time for the AWS DMS stream to stop when the new cluster is current."},{"content":"A company has two separate AWS accounts: one for the business unit and another for corporate analytics. The company wants to replicate the business unit data stored in Amazon RDS for MySQL in us-east-1 to its corporate analytics Amazon Redshift environment in us-west-1. The company wants to use AWS DMS with Amazon RDS as the source endpoint and Amazon Redshift as the target endpoint.\nWhich action will allow AVS DMS to perform the replication?\n\nA. Configure the AWS DMS replication instance in the same account and Region as Amazon Redshift.\nB. Configure the AWS DMS replication instance in the same account as Amazon Redshift and in the same Region as Amazon RDS.\nC. Configure the AWS DMS replication instance in its own account and in the same Region as Amazon Redshift.\nD. Configure the AWS DMS replication instance in the same account and Region as Amazon RDS.","comment_id":"283990","upvote_count":"1","comments":[{"upvote_count":"1","content":"Answer: B","comment_id":"287784","timestamp":"1633798260.0","poster":"Roontha"},{"comment_id":"299804","content":"The answer is A per below document:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html","timestamp":"1634566680.0","poster":"Windy","upvote_count":"2"},{"comment_id":"508107","upvote_count":"1","timestamp":"1640288040.0","poster":"jove","content":"Answer is A : The Amazon Redshift cluster must be in the same AWS account and same AWS Region as the replication instance."}],"poster":"RimaAws","timestamp":"1633586940.0"},{"content":"A database specialist is managing an application in the us-west-1 Region and wants to set up disaster recovery in the useast-1 Region. The Amazon Aurora MySQL DB cluster needs an RPO of 1 minute and an RTO of 2 minutes.\nWhich approach meets these requirements with no negative performance impact?\n\nA. Enable synchronous replication.\nB. Enable asynchronous binlog replication.\nC. Create an Aurora Global Database.\nD. Copy Aurora incremental snapshots to the us-east-1 Region.","comments":[{"poster":"Roontha","upvote_count":"2","timestamp":"1633796280.0","comment_id":"287782","content":"Answer: C"},{"content":"It is C","poster":"jove","upvote_count":"1","comment_id":"508109","timestamp":"1640288100.0"}],"upvote_count":"1","poster":"RimaAws","comment_id":"283986","timestamp":"1633475700.0"},{"content":"A gaming company is developing a new mobile game and decides to store the data for each user in Amazon DynamoDB. To make the registration process as easy as possible, users can log in with their existing Facebook or Amazon accounts. The company expects more than 10,000 users.\nHow should a database specialist implement access control with the LEAST operational effort?\n\nA. Use web identity federation on the mobile app and AWS STS with an attached IAM role to get temporary credentials to access DynamoDB.\nB. Use web identity federation on the mobile app and create individual IAM users with credentials to access DynamoDB.\nC. Use a self-developed user management system on the mobile app that lets users access the data from DynamoDB through an API.\nD. Use a single IAM user on the mobile app to access DynamoDB.","comments":[{"content":"What is the answer? Is it A?","upvote_count":"3","poster":"Windy","comment_id":"299782","timestamp":"1634527200.0"},{"content":"It is A","poster":"jove","timestamp":"1640288160.0","comment_id":"508110","upvote_count":"1"}],"poster":"RimaAws","upvote_count":"1","timestamp":"1633420320.0","comment_id":"283984"},{"comments":[{"content":"What is final answer D orC","upvote_count":"1","poster":"RimaAws","comment_id":"297065","timestamp":"1634293500.0"}],"content":"Answer: C","timestamp":"1633391820.0","comment_id":"279759","upvote_count":"1","poster":"Roontha"},{"timestamp":"1632510120.0","content":"A company is releasing a new mobile game featuring a team play mode. As a group of mobile device users\nplay together, an item containing their statuses is updated in an Amazon DynamoDB table. Periodically, the\nother users’ devices read the latest statuses of their teammates from the table using the BatchGetltemn\noperation.\nPrior to launch, some testers submitted bug reports claiming that the status data they were seeing in the game\nwas not up-to-date. The developers are unable to replicate this issue and have asked a database specialist for\na recommendation.\nWhich recommendation would resolve this issue?\nA. Ensure the DynamoDB table is configured to be always consistent.\nB. Ensure the BatchGetltem operation is called with the ConsistentRead parameter set to false.\nC. Enable a stream on the DynamoDB table and subscribe each device to the stream to ensure all devices\nreceive up-to-date status information.\nD. Ensure the BatchGetltem operation is called with the ConsistentRead parameter set to true.","comments":[{"upvote_count":"1","comment_id":"258479","timestamp":"1632525960.0","content":"answer is D","poster":"beebatov"},{"content":"Answer: D\nRefer: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\n https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html","poster":"Roontha","comment_id":"279757","upvote_count":"2","timestamp":"1633021020.0"}],"upvote_count":"1","poster":"awser103","comment_id":"253826"},{"upvote_count":"1","timestamp":"1632509760.0","comment_id":"253817","poster":"awser103","content":"A database specialist is building a system that uses a static vendor dataset of postal codes and related territory\ninformation that is less than 1 GB in size. The dataset is loaded into the application’s cache at start up. The\ncompany needs to store this data in a way that provides the lowest cost with a low application startup time.\nWhich approach will meet these requirements?\nA. Use an Amazon RDS DB instance. Shut down the instance once the data has been read.\nB. Use Amazon Aurora Serverless. Allow the service to spin resources up and down, as needed.\nC. Use Amazon DynamoDB in on-demand capacity mode.\nD. Use Amazon S3 and load the data from flat files.","comments":[{"comment_id":"257730","upvote_count":"1","content":"Answer: a","poster":"awser103","comments":[{"content":"seems B","timestamp":"1632802980.0","upvote_count":"1","comments":[{"comment_id":"287789","timestamp":"1633957620.0","poster":"Roontha","upvote_count":"1","content":"Aurora serverless use case is inpredicatbale load and it cost more $$ than S3."}],"poster":"beebatov","comment_id":"258480"},{"timestamp":"1634173920.0","content":"@awser103, can you justify why answer is A","poster":"Roontha","upvote_count":"1","comment_id":"287790"}],"timestamp":"1632523800.0"},{"poster":"Robbb","comment_id":"277397","upvote_count":"1","timestamp":"1632879480.0","content":"Surely this is D? There's no reason to use a database when you can just load the flat file with the static information into a map."},{"poster":"Roontha","content":"My take is D. \nI agree with Robbb","upvote_count":"3","comment_id":"279758","timestamp":"1633197120.0"},{"upvote_count":"1","poster":"Zhongkai","timestamp":"1635601020.0","content":"Static dataset + lowest cost -> S3. I agree with D","comment_id":"355174"},{"comment_id":"358731","content":"Lowest cost with low startup time. I think the best option is C.","upvote_count":"1","poster":"Huy","timestamp":"1635693840.0"}]},{"comment_id":"243055","upvote_count":"1","poster":"kilkar","content":"Agree w/ 'C'. Database inbound rule must include Application server IP's","timestamp":"1632409740.0"},{"timestamp":"1632384180.0","comment_id":"147150","content":"agree with C","poster":"sonobab","upvote_count":"2"},{"content":"C is correct","upvote_count":"1","comment_id":"136277","timestamp":"1632240900.0","poster":"bigaws"},{"comment_id":"134132","poster":"BillyC","timestamp":"1632172380.0","upvote_count":"1","content":"C its Correct!"},{"comment_id":"130262","poster":"activeprep1","timestamp":"1632120780.0","content":"it is 'C'","upvote_count":"1"}],"choices":{"D":"The user name and password are correct, but the user is not authorized to use the DB instance.","A":"The user name and password the application is using are incorrect.","B":"The security group assigned to the application servers does not have the necessary rules to allow inbound connections from the DB instance.","C":"The security group assigned to the DB instance does not have the necessary rules to allow inbound connections from the application servers."},"url":"https://www.examtopics.com/discussions/amazon/view/25182-exam-aws-certified-database-specialty-topic-1-question-1/","answer_images":[],"answers_community":["C (100%)"],"question_id":1,"question_text":"A company has deployed an e-commerce web application in a new AWS account. An Amazon RDS for MySQL Multi-AZ DB instance is part of this deployment with a database-1.xxxxxxxxxxxx.us-east-1.rds.amazonaws.com endpoint listening on port 3306. The company's Database Specialist is able to log in to MySQL and run queries from the bastion host using these details.\nWhen users try to utilize the application hosted in the AWS account, they are presented with a generic error message. The application servers are logging a `could not connect to server: Connection times out` error message to Amazon CloudWatch Logs.\nWhat is the cause of this error?"},{"id":"JXynfbCDhIcuJpyhdhhr","answer_ET":"C","question_text":"A Database Specialist is migrating a 2 TB Amazon RDS for Oracle DB instance to an RDS for PostgreSQL DB instance using AWS DMS. The source RDS Oracle\nDB instance is in a VPC in the us-east-1 Region. The target RDS for PostgreSQL DB instance is in a VPC in the use-west-2 Region.\nWhere should the AWS DMS replication instance be placed for the MOST optimal performance?","answer_description":"","timestamp":"2020-07-07 21:56:00","unix_timestamp":1594151760,"url":"https://www.examtopics.com/discussions/amazon/view/25049-exam-aws-certified-database-specialty-topic-1-question-10/","answers_community":["C (80%)","B (20%)"],"exam_id":22,"choices":{"A":"In the same Region and VPC of the source DB instance","B":"In the same Region and VPC as the target DB instance","D":"In the same VPC and Availability Zone as the source DB instance","C":"In the same VPC and Availability Zone as the target DB instance"},"question_id":2,"answer":"C","answer_images":[],"topic":"1","isMC":true,"discussion":[{"comment_id":"153303","comments":[{"timestamp":"1638692760.0","content":"There is also another question related to migration between a RDS resource and a Redshift cluster where they are in different VPCs and the Redshift cluster is the target. It asks readers where should the DMS replication instance be placed and the answer is the replication instance should be placed with the same VPC as the target.","poster":"scottkerker","comment_id":"494157","upvote_count":"2"}],"upvote_count":"17","timestamp":"1632914400.0","content":"C. See the diagram here:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html#CHAP_ReplicationInstance.VPC.Configurations.ScenarioVPCPeer\nIn fact, all the configurations list on above url prefer the replication instance putting into target vpc region / subnet / az.","poster":"zanhsieh"},{"comments":[{"timestamp":"1632702660.0","upvote_count":"1","comment_id":"139003","poster":"BillyMadison","content":"Going with D as well since it mentions in your link \"reducing load on your source database\"","comments":[{"poster":"BillyMadison","timestamp":"1633239660.0","content":"Changing my mind and going with C now. Talked with an AWS solutions architect. He said \"target DB instance\" has worked better for him in the past.","upvote_count":"5","comment_id":"161168"}]},{"upvote_count":"1","poster":"guru_ji","content":"C is correct.\n\nglat mtt kar dena exam mein. All the Best !!","comment_id":"429446","timestamp":"1635129240.0"}],"content":"D is correct based on :\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html\nit shows Choose the Availability Zone where your source database is located.","poster":"halol","timestamp":"1632590880.0","upvote_count":"6","comment_id":"136917"},{"poster":"michalf84","timestamp":"1729429980.0","content":"Selected Answer: B\nB there is similar question for redshift migration. You want to reduce latency","upvote_count":"1","comment_id":"1300470"},{"content":"Selected Answer: C\nIt is C - same VPC and AZ as the target.","poster":"MultiAZ","timestamp":"1705073520.0","comment_id":"1120883","upvote_count":"1"},{"timestamp":"1694935200.0","upvote_count":"1","comment_id":"1009646","content":"Selected Answer: B\nAnswer is B. In the network configuration for database migrations section on this page https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html States --> When practical, we recommend that you create a DMS replication instance in the same Region as your target endpoint, and in the same VPC or subnet as your target endpoint.","poster":"blah88"},{"content":"Selected Answer: C\nC. In the same VPC and Availability Zone as the target DB instance","comment_id":"1007662","upvote_count":"1","poster":"Pranava_GCP","timestamp":"1694696520.0"},{"poster":"aws2023a","content":"https://repost.aws/questions/QUUUL_7NsZS8OBI_GkCRSRRg/why-place-dms-replication-instance-in-target-vpc-instead-of-source-vpc","upvote_count":"2","comment_id":"973997","timestamp":"1691337660.0"},{"poster":"leotoras","content":"D is the right answer. It is a heterogeneous migration, some data transformations need to be performed before loading in the target. That's why it should be in the source database VPC.","upvote_count":"1","timestamp":"1689103740.0","comment_id":"949263"},{"comment_id":"829664","upvote_count":"1","poster":"f___16","timestamp":"1677995580.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html#CHAP_ReplicationInstance.VPC.Configurations.ScenarioVPCPeer"},{"upvote_count":"1","comment_id":"797686","timestamp":"1675494480.0","content":"Ans C : Generally, you’d get better performance by placing primary DMS replication instance is in the same AZ as the target DB","poster":"Vgarg2091"},{"timestamp":"1671050400.0","comment_id":"745428","poster":"lollyj","upvote_count":"2","comments":[{"upvote_count":"2","poster":"Zdujgfr567783ff","timestamp":"1682367300.0","content":"just a dummy random answer","comment_id":"879688"}],"content":"Selected Answer: C\nAnswer: C. I'm new to this site. Is the suggested answer typically the correct answer. I'm really confused. Who provides the suggested answers on this site?"},{"comment_id":"595603","content":"Selected Answer: C\nC. In the same VPC and Availability Zone as the target DB instance\n(BTW, target region is implicit)\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html#CHAP_ReplicationInstance.VPC.Configurations.ScenarioVPCPeer","poster":"novice_expert","timestamp":"1651410300.0","upvote_count":"3"},{"poster":"jnassp1","upvote_count":"2","timestamp":"1635573720.0","content":"C - \"For best results, we recommend that you locate your replication instance in the same VPC and Availability Zone as your target database, in this case Aurora MySQL.\" -","comment_id":"444398"},{"comment_id":"439217","poster":"Anuragdba","timestamp":"1635549780.0","content":"C: we will get performance improvement if place dms replication instance in target (VPC & availability zone same )","upvote_count":"1"},{"comment_id":"436759","poster":"LB","content":"C is the right answer here ie, the DMS replication instance should be placed in the target AZ.\n\nD would have been correct if the source data needed a lot of filtering and manipulation before transferring data to the target.","upvote_count":"2","comments":[{"content":"But data will needed a lot of filtering and manipulation before transferring data to the target going from Oracle to PostgreSQL, D is the correct answer","upvote_count":"1","poster":"Paulv82003","comment_id":"912711","timestamp":"1685700480.0"}],"timestamp":"1635446340.0"},{"content":"C => AZ of target instance","poster":"aws4myself","upvote_count":"1","comment_id":"434495","timestamp":"1635252360.0"},{"comment_id":"425847","timestamp":"1635038460.0","content":"C ==>> Correct Answer","upvote_count":"1","poster":"guru_ji"},{"content":"It seems a tough question and need to check in real practice, not on AWS books. I read the best practice but don't see any mention where these DMSs are located.\nDMS is not an replica DB, it is like a machine that read the information from source DB and re-procedure it to target DB.\n\"AWS DMS is a managed service that runs on an Amazon EC2 instance. This service connects to the source database, reads the source data, formats the data for consumption by the target database, and loads the data into the target database.\"\nAs above, I prefer to put it close to the source DB, where it can access source DB and read data faster, then send DDL information??? to target DB to re-procedure table etc","upvote_count":"1","timestamp":"1634937300.0","comment_id":"422152","comments":[{"comment_id":"422158","poster":"ChauPhan","content":"Imagine that we need to do full scan of source DB to find out the change. If we put DMS at target VPC/Region, it needs to connect back to source DB and process.\nThat's I think, but need real practice so if somebody did it, he/she can prove.","upvote_count":"1","timestamp":"1634983560.0"}],"poster":"ChauPhan"},{"comment_id":"378427","upvote_count":"1","content":"CCCCCCCCCCCC\n\nRef:\nhttps://docs.aws.amazon.com/dms/latest/sbs/CHAP_SQLServer2Aurora.Steps.CreateReplicationInstance.html","timestamp":"1634933520.0","poster":"Suresh108"},{"upvote_count":"1","timestamp":"1634766960.0","poster":"AWSGuru99","comment_id":"327674","content":"Answer is - C. \nFollow this link - https://docs.aws.amazon.com/dms/latest/sbs/CHAP_SQLServer2Aurora.Steps.CreateReplicationInstance.html"},{"timestamp":"1634738460.0","upvote_count":"1","poster":"LMax","content":"Answer C for me","comment_id":"314732"},{"upvote_count":"1","poster":"myutran","content":"Ans: C","timestamp":"1634603040.0","comment_id":"296371"},{"poster":"Exia","comments":[{"poster":"Exia","upvote_count":"1","content":"And I found the answer: Availability zone\nGenerally, performance is better if you locate your primary replication server in the same Availability Zone as your target database.\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/CHAP_SQLServer2Aurora.Steps.CreateReplicationInstance.html","comment_id":"292187","timestamp":"1634462520.0"}],"timestamp":"1634223960.0","upvote_count":"1","content":"I will go with C because the migration workload is a huge burden. For reducing the influence on the source DB instance, we should reduce the cross-region or cross-AZ traffic from the source DB to DMS replication instance.","comment_id":"292186"},{"upvote_count":"1","poster":"Glendon","content":"According to this AWS documentation link: https://docs.aws.amazon.com/dms/latest/sbs/CHAP_SQLServer2Aurora.Steps.CreateReplicationInstance.html \n\n\"Generally, performance is better if you locate your primary replication server in the same Availability Zone as your target database.\"\n\nThe answer should be C.","comment_id":"273838","timestamp":"1634147760.0"},{"comment_id":"272568","content":"The right answer is C, and I tell you this from lots of experience. For performance (2 TB) you need DMS instance as close to your TARGET as possible.","poster":"MultiAZ","upvote_count":"1","timestamp":"1633994160.0"},{"upvote_count":"1","comment_id":"244872","poster":"kilkar","content":"Agree to Answer: D\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html\n\nAWS DMS uses some resources on your source database. During a full load task, AWS DMS performs a full table scan of the source table for each table processed in parallel. Additionally, each task you create as part of a migration queries the source for changes as part of the CDC process.","timestamp":"1633932660.0"},{"timestamp":"1633892220.0","poster":"AwsSuperTrooper","comment_id":"216843","upvote_count":"1","content":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_GettingStarted.html#CHAP_GettingStarted.ReplicationInstance\n\nIt states:\n= Create a replication instance in the target AWS Region.\n= Create a replication instance in the VPC of the target AWS Region. \n\nThat indicates option B."},{"comment_id":"211587","upvote_count":"1","poster":"Ashoks","timestamp":"1633623540.0","content":"I would select C, replication instance needs to be close to target instance"},{"comment_id":"211285","content":"This question is a terrible headache. I would reply C.\nDue to data capture , numerous sequential reads will occurs, so i think that we have to idealy unload our source DB.\nAlso i have read carrefully https://d1.awsstatic.com/whitepapers/RDS/AWS_Database_Migration_Service_Best_Practices.pdf whic clearly put in bold : \nReducing Load on Your Source System ( page 12 of 17 ). \nI think it's better to be closed to the target.","poster":"Jack86","upvote_count":"1","timestamp":"1633551900.0"},{"content":"Answer D:\nAWS recommends keeping DMS close to the source","timestamp":"1633412100.0","upvote_count":"1","comment_id":"208875","poster":"Manmohan"},{"upvote_count":"1","content":"B is the correct answer\n- https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html","timestamp":"1633311120.0","poster":"Amogh77","comment_id":"182686"},{"timestamp":"1633221840.0","upvote_count":"3","comments":[{"upvote_count":"2","comment_id":"164761","poster":"cloud4gr8","timestamp":"1633274820.0","content":"You get the best performance by placing DMS replication instance in the same AZ as the target DB"}],"comment_id":"159444","poster":"gaurav31","content":"C is the correct answer"},{"comment_id":"150012","poster":"szmulder","upvote_count":"2","timestamp":"1632906120.0","content":"I found why we need choose the source db,\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.Creating.html"},{"poster":"BillyC","content":"Sorry, i mean C","comment_id":"134789","upvote_count":"4","timestamp":"1632440340.0"},{"upvote_count":"1","poster":"BillyC","comment_id":"134164","timestamp":"1632317700.0","content":"D for me... i mean close to target"},{"timestamp":"1632304440.0","content":"Revising my Answer. Best practice is to be close to the destination, not the source. So C would be my choice","upvote_count":"4","comment_id":"129702","poster":"[Removed]"},{"timestamp":"1632279480.0","comments":[{"timestamp":"1632287820.0","upvote_count":"1","poster":"[Removed]","comment_id":"129624","content":"Scratch that, I meant reads will happen"}],"upvote_count":"1","comment_id":"129623","poster":"[Removed]","content":"I can't find a clear answer on this either. D makes sense since that's where all the writes will happen"},{"comment_id":"129248","timestamp":"1632253680.0","upvote_count":"1","poster":"chicagomassageseeker","content":"I cannot find a definitve answer for this. AWS recommends to create replication isntance in the same VPC of either source or target but does not talk about any preference for performance."}],"question_images":[]},{"id":"WFcKKt8qti2T0cJZHmeE","topic":"1","choices":{"B":"Scale up the DB instance class.","C":"Modify applications to commit transactions in batches.","A":"Add an Aurora Replica to scale the read traffic.","D":"Modify applications to avoid conflicts by taking locks."},"answer":"B","timestamp":"2021-04-01 21:04:00","discussion":[{"content":"C is answer. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html. Check frank's link.","upvote_count":"12","poster":"Huy","timestamp":"1634627700.0","comment_id":"360390"},{"timestamp":"1633544400.0","upvote_count":"11","content":"C\nhttps://blog.dbi-services.com/aws-aurora-xactsync-batch-commit/","comment_id":"345768","poster":"frankzeng"},{"content":"Selected Answer: B\nIO:XactSync with high CPU wait - means DB load exceed allocated vCPUs, so you should reduce those workloads or scale up to higher CPUs\nBut if the IO:XactSync is due to high commit, then you should modify your application to commit transactions in batches","upvote_count":"1","timestamp":"1704178440.0","comment_id":"1111660","poster":"Hisayuki"},{"poster":"jitesh_k","comment_id":"1086523","upvote_count":"1","content":"Answer seems C since IO:Xactsync is about commit/rollback.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html\nBatch commit seems to be the answer.","timestamp":"1701568800.0"},{"comment_id":"1086522","timestamp":"1701568620.0","poster":"jitesh_k","upvote_count":"3","content":"Why not A? Add Aurora replica."},{"comment_id":"987691","upvote_count":"1","poster":"Pranava_GCP","timestamp":"1692729720.0","content":"Selected Answer: B\nB. Scale up the DB instance class.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html#apg-waits.xactsync.actions.scalecpu"},{"upvote_count":"2","poster":"IhorK","timestamp":"1691332680.0","comment_id":"973928","content":"According to \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html\nwe should \n- Scale up the CPU;\n- Reduce the number of commits.\nThere are 2 correct answers: B and C. I don't know who to give preference to :("},{"comment_id":"931438","content":"Selected Answer: B\nHard choice between B&C, but i select B. \n AWS recommends in documentation ( https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html ) to Scale up the CPU or Reduce the number of commits.\nModify application will require some time -> therefore Scaling up will help to resolve current issues with CPU","upvote_count":"1","poster":"[Removed]","timestamp":"1687513860.0"},{"timestamp":"1686519300.0","poster":"Zdujgfr567783ff","content":"Selected Answer: C\nBy default in PostgreSQL, single-row inserts auto-commit for each row. This means that the database must wait for durability (write to storage) for every insert. To improve performance, PostgreSQL supports multi-row inserts and disabling auto-commit.","upvote_count":"1","comment_id":"921000"},{"content":"C\nBy default in PostgreSQL, single-row inserts auto-commit for each row. This means that the database must wait for durability (write to storage) for every insert. To improve performance, PostgreSQL supports multi-row inserts and disabling auto-commit.","timestamp":"1686518100.0","comment_id":"920994","poster":"Zdujgfr567783ff","upvote_count":"1"},{"timestamp":"1683826020.0","comments":[{"comments":[{"upvote_count":"1","content":"By default in PostgreSQL, single-row inserts auto-commit for each row. This means that the database must wait for durability (write to storage) for every insert. To improve performance, PostgreSQL supports multi-row inserts and disabling auto-commit.","poster":"Zdujgfr567783ff","timestamp":"1686518040.0","comment_id":"920993"}],"content":"why CPU?\n\nIO:XactSync. PDF RSS. The IO:XactSync event occurs when the database is waiting for the Aurora storage subsystem to acknowledge the commit of a regular transaction, or the commit or rollback of a prepared transaction.","comment_id":"920992","poster":"Zdujgfr567783ff","timestamp":"1686517680.0","upvote_count":"1"}],"poster":"Pankaj24hrs","upvote_count":"1","content":"B\nIf CPU would not be an issue then definitely C, since a high CPU then it effects writing to the Storage layer as well.\n\nCPU pressure\nA heavy workload might be preventing the Aurora storage daemon from getting sufficient CPU time.\n\nTo address CPU starvation issues, consider changing to an instance type with more CPU capacity","comment_id":"895255"},{"poster":"guau","timestamp":"1676222940.0","upvote_count":"2","comment_id":"806608","content":"Selected Answer: C\nAfter reading the comments C is the right answer"},{"comments":[{"timestamp":"1679243700.0","content":"vote for you!\nC","poster":"Mintwater","upvote_count":"2","comment_id":"843957"}],"timestamp":"1675037520.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html#apg-waits.xactsync.actions.commits","upvote_count":"3","comment_id":"792197","poster":"anantarb"},{"poster":"tucobbad","comment_id":"754941","content":"Selected Answer: B\n(with typos fixed)\nMy answer is B. Despite we know that CPU is related to it as well as COMMIT frequency is high, tell me honestly what would be the fastest/easiest approach here to resolve the issue? Scale up the instance (B) or Modify an application to commit in batches (C).\nSecond, this post by Franck Pachot has plenty info which basically leds to CPU issue: https://www.dbi-services.com/blog/aws-aurora-xactsync-batch-commit/","timestamp":"1671892680.0","upvote_count":"3","comments":[{"poster":"Sathish_dbs","timestamp":"1673088780.0","upvote_count":"2","comment_id":"768453","content":"There is no ask about fastest/easiest approach here. so as a company the best approach to be taken here which is fixing the performance issue in the code. you can't keep increasing the hardware forever without fixing the code first!"}]},{"timestamp":"1671892560.0","content":"Selected Answer: B\nMy answer is B. Despite we know that CPU is related to it as well as COMMIT frequency is high, tell me honestly what would be the fastest/easiest approach here to resolve the issue? Scale up the instance (B) or Modify an application to commit in batches (C). \nSecond, this post by Frank Pachot has planty info which basically leds to CPU issue: https://www.dbi-services.com/blog/aws-aurora-xactsync-batch-commit/","poster":"tucobbad","upvote_count":"1","comment_id":"754938"},{"poster":"bikash7758","timestamp":"1667769480.0","upvote_count":"2","comment_id":"712654","content":"C IS CORRECT"},{"content":"Answer : C","upvote_count":"2","comment_id":"705048","poster":"rags1482","timestamp":"1666823220.0"},{"comment_id":"700094","content":"Selected Answer: C\nthat will resolve the issue","upvote_count":"2","poster":"awsjjj","timestamp":"1666281120.0"},{"comment_id":"696678","upvote_count":"1","timestamp":"1665966600.0","poster":"awsjjj","content":"Selected Answer: B\nCPU is the core issue. B"},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html\nAnswer is clearly B","poster":"venimus_vidimus_vicimus","comment_id":"676519","upvote_count":"1","timestamp":"1663875120.0"},{"poster":"bigticket","upvote_count":"2","content":"B\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html","timestamp":"1658433420.0","comment_id":"634784"},{"timestamp":"1656876060.0","upvote_count":"2","poster":"elf78","comment_id":"626711","content":"Selected Answer: B\nWhile both B&C can be the solutions according to the links that shared in comments, but batch commit only solves the XactSync issue, while upscaling the instance will address both, including the poor performance on load time and high cpu usage. I go with B.","comments":[{"timestamp":"1665162900.0","upvote_count":"1","poster":"Jiang_aws1","content":"High CPU due to IO from often commit base events logs. so \"C\" is correct.","comment_id":"688838"}]},{"comment_id":"609805","poster":"Dantas","content":"Selected Answer: B\n\"The load on the chart for average active sessions is often more than the line denoting maximum CPU use\", so B.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html","timestamp":"1654013520.0","upvote_count":"3"},{"upvote_count":"5","timestamp":"1651331460.0","content":"Selected Answer: C\nXactSync is caused by too many commits\nsolution is commit in batches","comment_id":"595108","poster":"novice_expert"},{"upvote_count":"1","comment_id":"557641","content":"i vote for B , since it is an ongoing issue CPU \nC is good if it is during peak hours only","timestamp":"1645997760.0","poster":"user0001"},{"timestamp":"1645761600.0","content":"Selected Answer: C\nbatch mode will fix it","comment_id":"555742","poster":"tugboat","upvote_count":"4"},{"content":"Answer is B. CPU issue needs resolving first as that is the core issue","comment_id":"540465","timestamp":"1643983980.0","poster":"jeyp12","upvote_count":"1"},{"timestamp":"1642815300.0","poster":"Shinytopology","content":"Selected Answer: B\nB. Active sessions are using up CPU. C is not correct, INSERT is not mentioned unlike the question in earlier page. Don't just match keyword. \n\"The load on the chart for average active sessions is often more than the line denoting maximum CPU use\" It","comment_id":"529518","upvote_count":"4","comments":[{"content":"High CPU from active sessions but wait events show IO:XactSync so active sessions wait for IO:XacSync : commit in batch is first thing to try to fix issue.","upvote_count":"1","timestamp":"1665163260.0","comment_id":"688841","poster":"Jiang_aws1"}]},{"content":"Both B and C are correct response to IO:XactSync. However, as a best practice, we should tune the application first. If it doesn't help, then we will consider scaling the instance. Afterall scaling the instance results in additional costs.\n\nI vote for C.","upvote_count":"2","comment_id":"529155","timestamp":"1642771800.0","poster":"awsmonster"},{"upvote_count":"2","content":"Selected Answer: B\nI will go to B.\n\"The load on the chart for average active sessions is often more than the line denoting maximum CPU use\". You need more CPU for active sessions.","poster":"Shunpin","comment_id":"510766","timestamp":"1640660760.0"},{"comment_id":"507328","upvote_count":"3","poster":"jove","timestamp":"1640201340.0","content":"If the instance's CPU utilization is above 90 percent, the Aurora storage daemon might not be getting sufficient time on the CPU. In this case, I/O performance degrades. To address CPU starvation issues, consider changing to an instance type with more CPU capacity.\n\nThe correct option seems B"},{"poster":"Justu","upvote_count":"1","comment_id":"491500","content":"B or C: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html\n\nI would choose B, because it says CPU is the bottleneck, not commits.","timestamp":"1638352320.0"},{"poster":"Scunningham99","content":"CCCC https://blog.dbi-services.com/aws-aurora-xactsync-batch-commit/\nIO:XactSync – In this wait event, a session is issuing a COMMIT or ROLLBACK, requiring the current transaction’s changes to be persisted. Aurora is waiting for Aurora storage to acknowledge persistence.","upvote_count":"2","comment_id":"449002","timestamp":"1635318960.0"},{"timestamp":"1635063420.0","upvote_count":"2","poster":"guru_ji","comment_id":"446122","content":"This Q came in my Exam."},{"comment_id":"430489","timestamp":"1635032280.0","poster":"ChauPhan","content":"B. Only scale up to solve this issue.","upvote_count":"1"},{"content":"C for sure.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html\nYou can sometimes alleviate this wait by modifying applications to commit transactions in batches","upvote_count":"1","comment_id":"415855","comments":[{"poster":"TonyGe","content":"Change to B. It mentioned \"the load in the chart for average active sessions is often above the line that denotes maximum CPU usage\"\n\nAs per document said:\"You might see this wait at the same time as CPU waits in a case where the DB load exceeds the number of virtual CPUs (vCPUs) for the DB instance. In this case, the storage persistence might be competing for CPU with CPU-intensive database workloads. To alleviate this scenario, you can try reducing those workloads, or scaling up to a DB instance with more vCPUs.\"","upvote_count":"7","comment_id":"415857","timestamp":"1634774760.0"}],"poster":"TonyGe","timestamp":"1634634360.0"},{"upvote_count":"2","comment_id":"358988","poster":"Aesthet","content":"C final answer","timestamp":"1634320380.0"},{"poster":"Zhongkai","comment_id":"356430","timestamp":"1634242020.0","content":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html\nthe doc says, \"IO:XactSync - ...In this case, the storage persistence might be competing for CPU with CPU-intensive database workloads. To alleviate this scenario, you can try reducing those workloads, or scaling up to a DB instance with more vCPUs.\"\nthis qustions mentioned \"the load in the chart ... is often above the line that denotes maximum CPU usage\". Hence I prefer B","upvote_count":"2"},{"content":"Sorry I meant A","poster":"AM","comment_id":"351519","timestamp":"1633805220.0","upvote_count":"1"},{"poster":"AM","timestamp":"1633712640.0","comment_id":"351517","content":"My answer is D. Queries take longer during peak times. That is read related. So is I/O. Read replica will load balance reads thus reducing high CPU as well as IO.","upvote_count":"2"},{"poster":"manan728","comments":[{"upvote_count":"3","comment_id":"359332","content":"My Answer is B. \nI agree with you. \"This wait most often arises when there is a very high rate of commit activity on the system. You can sometimes alleviate this wait by modifying applications to commit transactions in batches. You might see this wait at the same time as CPU waits in a case where the DB load exceeds the number of virtual CPUs (vCPUs) for the DB instance. In this case, the storage persistence might be competing for CPU with CPU-intensive database workloads. To alleviate this scenario, you can try reducing those workloads, or scaling up to a DB instance with more vCPUs.\"","poster":"gelsm","timestamp":"1634421120.0"}],"timestamp":"1633504440.0","content":"B is the right choice. IO:xactsync can occur even for other reasons during high cpu load as in this case. You will only go with C when the question specifically mentioned the reason as high amount of inserts, otherwise you need to scale up the instance.","comment_id":"342305","upvote_count":"4"},{"upvote_count":"2","timestamp":"1632630360.0","poster":"shantest1","content":"C. answer","comment_id":"326656"},{"poster":"std2021","content":"Option C","comment_id":"326093","timestamp":"1632627780.0","upvote_count":"4"}],"question_id":3,"question_images":[],"answer_images":[],"unix_timestamp":1617303840,"url":"https://www.examtopics.com/discussions/amazon/view/48690-exam-aws-certified-database-specialty-topic-1-question-100/","answers_community":["B (54%)","C (46%)"],"exam_id":22,"isMC":true,"answer_description":"","answer_ET":"B","question_text":"A company is using Amazon Aurora PostgreSQL for the backend of its application. The system users are complaining that the responses are slow. A database specialist has determined that the queries to Aurora take longer during peak times. With the Amazon RDS Performance Insights dashboard, the load in the chart for average active sessions is often above the line that denotes maximum CPU usage and the wait state shows that most wait events are IO:XactSync.\nWhat should the company do to resolve these performance issues?"},{"id":"hsz4VrZsGiM3FbTayTbE","answer_description":"","question_text":"A database specialist deployed an Amazon RDS DB instance in Dev-VPC1 used by their development team. Dev-VPC1 has a peering connection with Dev-VPC2 that belongs to a different development team in the same department. The networking team confirmed that the routing between VPCs is correct; however, the database engineers in Dev-VPC2 are getting a timeout connections error when trying to connect to the database in Dev-VPC1.\nWhat is likely causing the timeouts?","topic":"1","answer_images":[],"timestamp":"2021-04-25 04:54:00","exam_id":22,"unix_timestamp":1619319240,"discussion":[{"content":"C it is.","poster":"manan728","comment_id":"342306","timestamp":"1633202580.0","upvote_count":"6"},{"comments":[{"timestamp":"1634981220.0","content":"\"A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html","upvote_count":"5","poster":"Aesthet","comment_id":"360631"}],"timestamp":"1633698600.0","upvote_count":"6","content":"C final answer","comment_id":"358998","poster":"Aesthet"},{"poster":"Pranava_GCP","timestamp":"1692728640.0","upvote_count":"1","content":"Selected Answer: C\nC. The database is deployed with misconfigured security groups.\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules-reference.html#sg-rules-db-server","comment_id":"987684"},{"timestamp":"1651323900.0","content":"Selected Answer: C\nsecurity group needs inbound rule","upvote_count":"3","poster":"novice_expert","comment_id":"595019"},{"poster":"tugboat","comment_id":"555722","upvote_count":"2","content":"Selected Answer: C\nSecurity groups need to be fixed","timestamp":"1645758480.0"},{"timestamp":"1635582600.0","content":"CCCCCC","comment_id":"379432","upvote_count":"2","poster":"Suresh108"}],"answer":"C","choices":{"C":"The database is deployed with misconfigured security groups.","A":"The database is deployed in a VPC that is in a different Region.","D":"The database is deployed with the wrong client connect timeout configuration.","B":"The database is deployed in a VPC that is in a different Availability Zone."},"answer_ET":"C","question_id":4,"question_images":[],"isMC":true,"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/50908-exam-aws-certified-database-specialty-topic-1-question-101/"},{"id":"smNtZDfTGptVvpofbdJP","exam_id":22,"question_text":"A company has a production environment running on Amazon RDS for SQL Server with an in-house web application as the front end. During the last application maintenance window, new functionality was added to the web application to enhance the reporting capabilities for management. Since the update, the application is slow to respond to some reporting queries.\nHow should the company identify the source of the problem?","topic":"1","timestamp":"2021-04-05 14:46:00","answer":"B","question_id":5,"isMC":true,"answer_description":"","question_images":[],"choices":{"C":"Use AWS X-Ray deployed with Amazon RDS to track query system traces.","D":"Create a support request and work with AWS Support to identify the source of the issue.","B":"Enable RDS Performance Insights and determine which query is creating the problem. Request changes to the query to address the problem.","A":"Install and configure Amazon CloudWatch Application Insights for Microsoft .NET and Microsoft SQL Server. Use a CloudWatch dashboard to identify the root cause."},"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/49204-exam-aws-certified-database-specialty-topic-1-question-102/","answer_ET":"B","unix_timestamp":1617626760,"answer_images":[],"discussion":[{"timestamp":"1639055400.0","upvote_count":"7","poster":"2025flakyt","comment_id":"497743","content":"B is correct\nAmazon RDS Performance Insights is a database performance tuning and monitoring feature that helps you quickly assess the load on your database, and determine when and where to take action. Performance Insights allows non-experts to detect performance problems with an easy-to-understand dashboard that visualizes database load.\nhttps://aws.amazon.com/rds/performance-insights/"},{"comment_id":"1002194","upvote_count":"2","poster":"chen0305_099","timestamp":"1694154360.0","content":"ＭＡＹＢＥ Ａ"},{"upvote_count":"1","content":"Selected Answer: B\nB. Enable RDS Performance Insights and determine which query is creating the problem. Request changes to the query to address the problem.\n\n\"Performance Insights uses lightweight data collection methods that don’t impact the performance of your applications, and makes it easy to see which SQL statements are causing the load, and why. It requires no configuration or maintenance, and is currently available for Amazon Aurora (PostgreSQL- and MySQL-compatible editions), Amazon RDS for PostgreSQL, MySQL, MariaDB, SQL Server and Oracle.\"\n\"Whether your database performance problem is due to database configuration or application design issues, you can quickly identify the bottleneck and see which SQL statements are contributing to it.\"\n\nhttps://aws.amazon.com/rds/performance-insights/","comment_id":"987681","poster":"Pranava_GCP","timestamp":"1692728340.0"},{"content":"Selected Answer: B\nPerformance Insights to analyze database performance by waits, SQL statements, hosts, or users.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.Overview.ActiveSessions.html\nAmazon CloudWatch Application Insights for Microsoft .NET and Microsoft SQL Server allows you to monitor the health of the SQL server, general issues not related to a specific query. Metrics that are monitored for the SQL Server instance:\n - CPUUtilization\n - SQLServer:Buffer Manager cache hit ratio\n - SQLServer:Locks Number of Deadlocks/sec\n - VolumeQueueLength\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/appinsights-what-is.html","poster":"IhorK","timestamp":"1691353560.0","comment_id":"974215","upvote_count":"2"},{"content":"Selected Answer: B\nI think it can´t be A because it's an in-house web application, saying that going for B","upvote_count":"1","poster":"teo2157","comment_id":"787913","timestamp":"1674665400.0"},{"timestamp":"1664463480.0","content":"i think it should be A: as an Application, insight can provide more details to SQL server","upvote_count":"2","poster":"niteshdba","comment_id":"682793"},{"poster":"sachin","content":"I think this should be A.\nThe question no where states the slowness is due to query. it states response is slow to certain request for repoting. \n\nCloudWatch Application Insights is For common problems in .NET and SQL application stacks, such as application latency, SQL Server failed backups, memory leaks, large HTTP requests, and canceled I/O operations, it provides additional insights that point to a possible root cause and steps for resolution. Built-in integration with AWS SSM OpsCenter allows you to resolve issues by running the relevant Systems Manager Automation document. \nFirst we need to understand the reason of slow reponse. It could be application issue rather SQL issue where Performance Insight can be helpful","comment_id":"624353","upvote_count":"3","timestamp":"1656464820.0"},{"comment_id":"595005","poster":"novice_expert","upvote_count":"2","timestamp":"1651322760.0","content":"Selected Answer: B\nadditional features to improve management reporting capabilities => new queries needing Performance Insights to check"},{"content":"Selected Answer: B\nNew reports = new queries needing Performance Insights to check","poster":"tugboat","upvote_count":"3","comment_id":"555718","timestamp":"1645758180.0"},{"comments":[{"comment_id":"506407","poster":"jove","timestamp":"1640116740.0","content":"Correct answer is B.. The issue is not about the infrastructure, it is about the additional features added during with the application maintenance.","upvote_count":"2"}],"comment_id":"478238","timestamp":"1636909860.0","poster":"Sp230","content":"A \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html \n\" For common problems in .NET and SQL application stacks, such as application latency, SQL Server failed backups, memory leaks, large HTTP requests, and canceled I/O operations, it provides additional insights that point to a possible root cause and steps for resolution\"","upvote_count":"3"},{"poster":"Scunningham99","comment_id":"449019","upvote_count":"2","content":"New functionality added to update reporting capabilities. Reports from application are generated via queries. These queries need to be investigated via answer which is > B","comments":[{"timestamp":"1656464880.0","upvote_count":"1","poster":"sachin","comment_id":"624355","content":"Reports are generated by SQL but no where it is mentioned that reporting sql is slow. It says response of reporting request it slow which could due to application issue as well"}],"timestamp":"1636246200.0"},{"timestamp":"1635807420.0","upvote_count":"2","content":"B is the answer","poster":"ChauPhan","comment_id":"430722"},{"poster":"AM","upvote_count":"2","content":"sql server application issue. Does not say ueries. A is the answer.","comment_id":"386527","comments":[{"comment_id":"403879","upvote_count":"1","poster":"ExtHo","timestamp":"1635200340.0","content":"Problem started due new functionality was added to the web application to enhance the reporting capabilities for management. Since the update, the application is slow to respond to some reporting queries so issue is with query performance not with application that points B as correct answer"}],"timestamp":"1634548080.0"},{"comment_id":"379436","poster":"Suresh108","upvote_count":"2","timestamp":"1633866540.0","content":"BBBBBBBBB"},{"poster":"Aesthet","timestamp":"1632490440.0","upvote_count":"3","content":"B final answer","comment_id":"360605"},{"content":"Option B","poster":"swarndeep","timestamp":"1632283740.0","comment_id":"342401","upvote_count":"2"},{"upvote_count":"4","comment_id":"328673","timestamp":"1632085020.0","poster":"shantest1","content":"B. Answer"}]}],"exam":{"id":22,"isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"name":"AWS Certified Database - Specialty","numberOfQuestions":359,"provider":"Amazon","isMCOnly":false},"currentPage":1},"__N_SSP":true}