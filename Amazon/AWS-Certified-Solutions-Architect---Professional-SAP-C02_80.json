{"pageProps":{"questions":[{"id":"OtefjIvxZIRAZusReoO9","question_images":[],"unix_timestamp":1710831360,"answer_description":"","answers_community":["B (81%)","A (19%)"],"question_text":"A company needs to migrate 60 on-premises legacy applications to AWS. The applications are based on the NET Framework and run on Windows.\n\nThe company needs a solution that minimizes migration time and requires no application code changes. The company also does not want to manage the infrastructure.\n\nWhich solution will meet these requirements?","isMC":true,"answer_ET":"B","exam_id":33,"question_id":396,"discussion":[{"poster":"0b43291","comment_id":"1312912","timestamp":"1731725160.0","upvote_count":"2","content":"Selected Answer: B\nBy using the Windows Web Application Migration Assistant and AWS Elastic Beanstalk, the company can migrate their .NET applications to AWS with minimal migration time, no code changes, and without the need to manage the underlying infrastructure, meeting all the stated requirements.\n\nThe other options have limitations or do not fully meet the requirements:\nOptions A and D (containerization with ECS or EKS) would require refactoring the applications, which goes against the requirement of avoiding code changes. Additionally, these options would require more infrastructure management compared to Elastic Beanstalk.\n\n Option C (migration to EC2 instances) would require the company to manage the EC2 instances, configure networking, load balancing, and auto-scaling, which contradicts the requirement of not managing the infrastructure."},{"timestamp":"1731273000.0","comment_id":"1309646","upvote_count":"1","poster":"AzureDP900","content":"Option B uses the Windows Web Application Migration Assistant, a tool specifically designed for .NET Framework-based applications on Windows. It helps migrate these applications to AWS Elastic Beanstalk without requiring code changes or manual infrastructure management. Elastic Beanstalk then takes care of deploying and managing the applications, meeting the company's requirements.\nThis approach ensures a smooth migration with minimal disruption, while also avoiding any significant changes to the application code or infrastructure management responsibilities."},{"upvote_count":"1","timestamp":"1728753600.0","comment_id":"1296598","poster":"JoeTromundo","content":"Selected Answer: B\nElastic Beanstalk abstracts the infrastructure, so the company won’t need to manage EC2 instances, scaling, load balancing, or patching. Elastic Beanstalk takes care of these tasks automatically, which fits the requirement of not managing infrastructure. This is not the same thing as serverless (which is NOT a requirement), as Zas1 commented.\nThe answer can't be A because refactoring=code change, as asquared16 have already commented."},{"timestamp":"1723812600.0","poster":"asquared16","upvote_count":"2","comment_id":"1267080","content":"Selected Answer: B\nRefactoring = Code Change"},{"upvote_count":"2","poster":"titi_r","content":"Selected Answer: B\nGetting started with Windows .NET on Elastic Beanstalk\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/dotnet-getstarted.html","comment_id":"1201218","timestamp":"1713948900.0"},{"comment_id":"1197175","upvote_count":"1","timestamp":"1713350760.0","poster":"AwsZora","content":"Selected Answer: A\nNo, AWS Elastic Beanstalk is not a serverless platform.","comments":[{"timestamp":"1716217740.0","comment_id":"1214449","upvote_count":"2","content":"B, Not word Serverless appears: \"The company also does not want to manage the infrastructure.\" \nhttps://aws.amazon.com/elasticbeanstalk\nQuickly launch web applications: Deploy scalable web applications in minutes without the complexity of provisioning and managing underlying infrastructure.","poster":"Zas1"}]},{"timestamp":"1711344180.0","upvote_count":"2","comment_id":"1182223","poster":"VerRi","content":"Selected Answer: B\nThis is a typical Beanstalk feature. \nRefactoring and containerizing applications often involve some level of code change."},{"comment_id":"1181988","content":"Selected Answer: B\nI vote for B\nwhen googling Windows Web Application Migration Assistant, all top 3 are using EB.\nhttps://github.com/awslabs/windows-web-app-migration-assistant\nCompare to EC2 in C, the question mentioned do not manage infrastructure\nSee below wording\nWith Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html","poster":"pangchn","upvote_count":"2","comments":[{"upvote_count":"1","content":"AC\nAWS Toolkit will change code in some way\nhttps://aws.amazon.com/visual-studio-net/","poster":"pangchn","timestamp":"1711315740.0","comment_id":"1181990"}],"timestamp":"1711315620.0"},{"timestamp":"1711187700.0","comment_id":"1180762","content":"Selected Answer: A\nA\nNot B as company does not want to manage the infra.","poster":"yog927","upvote_count":"1"},{"content":"Selected Answer: B\nCorrect answer is B, use Beanstalk. It's a classic use for Beanstalk: remember - no application changes is a requirement.\n\nA involves quite a bit of work and application changes. AWS Toolkit for .NET is a help, but there's operational overhead. Also, moving to ECS Fargate, serverless as it is, requires containerising the application, which also adds overhead.","upvote_count":"2","timestamp":"1710966060.0","comment_id":"1178672","poster":"Dgix"},{"content":"Selected Answer: A\nRefactoring the applications and containerizing them using AWS Toolkit for .NET Refactoring allows for easy migration without needing to modify application code. Using Amazon ECS with the Fargate launch type is optimized for running containers (when comparing to #D) and allows the provisioning and scaling of containers. #A provides a streamlined migration process with minimal management overhead.","upvote_count":"1","timestamp":"1710847620.0","comment_id":"1177250","poster":"CMMC"},{"poster":"ovladan","comment_id":"1177044","timestamp":"1710831360.0","upvote_count":"2","content":"Solution: B\nIf you look at the request \"Company needs a solution that minimizes migration time and requires no changes to application code,\" you can eliminate the answer under A & D (refactoring suggested).\nThe answers under B & C are fine, but the \"minimize migration time\" part, the better solution is under B."}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/136565-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2024-03-19 07:56:00","answer_images":[],"choices":{"B":"Use the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk. Use Elastic Beanstalk to deploy and manage the applications.","D":"Refactor the applications and containerize them by using AWS Toolkit for NET Refactoring. Use Amazon Elastic Kubernetes Service (Amazon EKS) with the Fargate launch type to host the containerized applications.","C":"Use the Windows Web Application Migration Assistant to migrate the applications to Amazon EC2 instances. Use the EC2 instances to deploy and manage the applications.","A":"Refactor the applications and containerize them by using AWS Toolkit for NET Refactoring. Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to host the containerized applications."},"answer":"B"},{"id":"deztQZnw88ubENCTyRZs","answer_description":"","question_id":397,"choices":{"B":"Create an AWS Batch compute environment that includes Amazon EC2 Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy.","D":"Use Amazon Elastic Kubernetes Service (Amazon EKS) to run the processing jobs. Use managed node groups that contain a combination of Amazon EC2 On-Demand Instances and Spot Instances.","C":"Create an AWS Batch compute environment that includes Amazon EC2 On-Demand Instances and Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy for the Spot Instances.","A":"Create a serverless data pipeline. Use AWS Step Functions for orchestration. Use AWS Lambda functions with provisioned capacity to process the data."},"isMC":true,"answer_ET":"B","answers_community":["B (100%)"],"question_text":"A company needs to run large batch-processing jobs on data that is stored in an Amazon S3 bucket. The jobs perform simulations. The results of the jobs are not time sensitive, and the process can withstand interruptions.\n\nEach job must process 15-20 GB of data when the data is stored in the S3 bucket. The company will store the output from the jobs in a different Amazon S3 bucket for further analysis.\n\nWhich solution will meet these requirements MOST cost-effectively?","question_images":[],"timestamp":"2024-03-19 12:31:00","exam_id":33,"answer":"B","unix_timestamp":1710847860,"discussion":[{"upvote_count":"7","comment_id":"1182227","timestamp":"1711344480.0","poster":"VerRi","content":"Selected Answer: B\n\"large batch-processing jobs\" -> Batch\n\"not time sensitive, and the process can withstand interruptions\" -> Spot"},{"content":"Option B utilizes AWS Batch with EC2 Spot Instances and the SPOT_CAPACITY_OPTIMIZED allocation strategy. This approach takes advantage of the Spot Instance pricing model, which provides a significant discount compared to On-Demand pricing. By using this configuration, the company can process large batch jobs at a lower cost while still meeting the specified requirements.","timestamp":"1731272820.0","comment_id":"1309642","upvote_count":"1","poster":"AzureDP900"},{"comment_id":"1306674","timestamp":"1730674380.0","poster":"AzureDP900","content":"B is right because The results of the jobs are not time sensitive, and the process can withstand interruptions","upvote_count":"1"},{"comment_id":"1204108","poster":"TonytheTiger","content":"Selected Answer: B\nOption B - AWS Blog \nhttps://aws.amazon.com/blogs/compute/cost-effective-batch-processing-with-amazon-ec2-spot/","upvote_count":"1","timestamp":"1714406520.0"},{"content":"Selected Answer: B\nB\nC is wrong due to the following\nAWS Batch selects one or more instance types that are large enough to meet the requirements of the jobs in the queue. Instance types that are less likely to be interrupted are preferred. This allocation strategy is only available for Spot Instance compute resources.\nhttps://docs.aws.amazon.com/batch/latest/userguide/allocation-strategies.html","comment_id":"1181996","upvote_count":"1","timestamp":"1711317480.0","poster":"pangchn"},{"timestamp":"1710966240.0","upvote_count":"1","content":"Selected Answer: B\nThe correct answer is B.","comment_id":"1178674","poster":"Dgix"},{"poster":"CMMC","timestamp":"1710847860.0","upvote_count":"1","comment_id":"1177254","content":"Selected Answer: B\nAWS Batch with Spot instances given not time sensitive"}],"answer_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/136597-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"ruHVw6hTgAxi5kWiSNRm","answer_ET":"B","choices":{"D":"Deploy an AWS Storage Gateway Tape Gateway on premises in the Hyper-V environment. Connect the Tape Gateway to AWS. Use automatic tape creation. Specify an Amazon S3 Glacier Deep Archive pool. Eject the tape after the batch of images is copied.","C":"Deploy an AWS DataSync agent on a new general purpose Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Standard. After the successful copy, delete the data from the on-premises storage. Create an S3 Lifecycle rule to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 day.","B":"Deploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage.","A":"Deploy an AWS DataSync agent on a new GPU-based Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Instant Retrieval. After the successful copy, delete the data from the on-premises storage."},"timestamp":"2024-03-19 12:46:00","question_text":"A company has an application that analyzes and stores image data on premises. The application receives millions of new image files every day. Files are an average of 1 MB in size. The files are analyzed in batches of 1 GB. When the application analyzes a batch, the application zips the images together. The application then archives the images as a single file in an on-premises NFS server for long-term storage.\n\nThe company has a Microsoft Hyper-V environment on premises and has compute capacity available. The company does not have storage capacity and wants to archive the images on AWS. The company needs the ability to retrieve archived data within 1 week of a request.\n\nThe company has a 10 Gbps AWS Direct Connect connection between its on-premises data center and AWS. The company needs to set bandwidth limits and schedule archived images to be copied to AWS during non-business hours.\n\nWhich solution will meet these requirements MOST cost-effectively?","isMC":true,"answer_images":[],"unix_timestamp":1710848760,"answer_description":"","question_id":398,"discussion":[{"poster":"TonytheTiger","upvote_count":"2","comment_id":"1204114","content":"Selected Answer: B\nOption B: AWS Blog - \nhttps://aws.amazon.com/blogs/storage/protect-your-file-and-backup-archives-using-aws-datasync-and-amazon-s3-glacier/\n\nHow do I use AWS DataSync to archive cold data? - https://aws.amazon.com/datasync/faqs/","timestamp":"1730225760.0"},{"upvote_count":"1","timestamp":"1727668920.0","content":"Selected Answer: B\nDeploy the DataSync agent to the source.","poster":"VerRi","comment_id":"1186569"},{"poster":"Dgix","timestamp":"1726857000.0","upvote_count":"4","content":"Selected Answer: B\nA is out because of Glacier Instant Retrieval (milliseconds)\nB is the correct answer: goes directly to Glacier Deep Archive\nC needlessly stores data in S3 Standard for a day\nD is an awkward use case.","comment_id":"1178679"},{"comment_id":"1177267","upvote_count":"1","timestamp":"1726739160.0","content":"Selected Answer: B\ndeploy the AWS DataSync in Hyper-V env, use more cost effice S3 Glacier Deep Archive","poster":"CMMC"}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/136604-exam-aws-certified-solutions-architect-professional-sap-c02/","answer":"B","exam_id":33,"answers_community":["B (100%)"],"question_images":[]},{"id":"kkd2GcCmeVT885UqMDCz","choices":{"A":"Configure an Amazon CloudWatch Logs metric filter that saves each successful login as a metric. Configure the user name and client name as dimensions for the metric.","D":"Configure an AWS Lambda function to consume an Amazon CloudWatch Logs stream of the application logs. Additionally, configure the Lambda function to increment a custom metric in CloudWatch that uses the user name and client name as dimensions for the metric.","C":"Configure the CloudWatch agent to extract successful login metrics from the logs. Additionally, configure the CloudWatch agent to save the successful login metrics as a custom metric that uses the user name and client name as dimensions for the metric.","B":"Change the application logic to make each successful login generate a call to the AWS SDK to increment a custom metric that records user name and client name dimensions in CloudWatch."},"answer":"D","question_images":[],"question_id":399,"answer_description":"","question_text":"A company wants to record key performance indicators (KPIs) from its application as part of a strategy to convert to a user-based licensing schema. The application is a multi-tier application with a web-based UI. The company saves all log files to Amazon CloudWatch by using the CloudWatch agent. All logins to the application are saved in a log file.\n\nAs part of the new license schema, the company needs to find out how many unique users each client has on a daily basis, weekly basis, and monthly basis.\n\nWhich solution will provide this information with the LEAST change to the application?","url":"https://www.examtopics.com/discussions/amazon/view/136611-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","timestamp":"2024-03-19 13:02:00","answer_images":[],"unix_timestamp":1710849720,"isMC":true,"discussion":[{"poster":"itsjunukim","content":"Selected Answer: D\nMetric Filters only provide simple pattern counting functionality and cannot handle duplicate users.","timestamp":"1742392380.0","upvote_count":"1","comment_id":"1400552"},{"content":"Selected Answer: A\nBoth A and B are workable. A is the simplest and has no code development effort","upvote_count":"1","poster":"GabrielShiao","timestamp":"1738314900.0","comment_id":"1349444"},{"comments":[{"timestamp":"1735252080.0","content":"You can use up to 3 custom dimensions in cw logs metric filter and thereby capture the username and client name from the logs. \n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntaxForMetricFilters.html#logs-metric-filters-dimensions","comment_id":"1332132","upvote_count":"2","poster":"pk0619"}],"upvote_count":"3","timestamp":"1731727500.0","comment_id":"1312921","content":"Selected Answer: D\nWith https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\nthe documentation states that CloudWatch Logs metric filters can extract and publish metrics based on log data, but the dimensions for these metrics are limited to the following:\n LogGroupName\n LogStreamName\n Namespace (optional)\n\nThere is no mention of the ability to use custom dimensions like user name or client name with CloudWatch Logs metric filters.\n\nGiven this limitation, the solution that would provide the required information with the least change to the application is:\n\nD. Configure an AWS Lambda function to consume an Amazon CloudWatch Logs stream of the application logs. Additionally, configure the Lambda function to increment a custom metric in CloudWatch that uses the user name and client name as dimensions for the metric.","poster":"0b43291"},{"comment_id":"1309633","timestamp":"1731272160.0","upvote_count":"3","content":"Option A involves configuring a CloudWatch Logs metric filter to extract login metrics from log files. This approach can provide the required KPIs with minimal changes to the application, as it does not require modifying the application code or adding additional services.\nThe solution also uses dimensions to capture user name and client name information, which will help identify unique users for each client on a daily, weekly, and monthly basis.","poster":"AzureDP900"},{"poster":"Danm86","upvote_count":"1","comment_id":"1303656","timestamp":"1730045880.0","content":"Answer seems to be option D. Metric Filters can only count the occurrence of a pattern in the log, they cannot extract specific data fields like user name or client name. Metric Filters do not automatically create custom metrics in CloudWatch. They only send the counted values to an existing metric."},{"content":"Selected Answer: D\nwas at first for A but then for D.. ChatGPT is also for D: \n\nD: This option provides the most flexibility and capability for processing data. AWS Lambda can process the incoming log stream to apply more complex logic, such as checking for and ignoring duplicate entries within a set time frame (daily, weekly, monthly) before incrementing the metrics. This allows for the implementation of logic to ensure that users are only counted once per period, effectively tracking unique logins.\n\nConclusion:\nAmong the given options, Option D using an AWS Lambda function is best equipped to handle the requirement of counting unique user logins accurately over specified periods. Lambda functions offer the flexibility to implement any necessary logic to filter duplicates and manage counts over time, aligning with the need to track unique users on a daily, weekly, and monthly basis.","timestamp":"1728918840.0","upvote_count":"2","poster":"chris_spencer","comment_id":"1297703"},{"poster":"Syre","timestamp":"1726594500.0","upvote_count":"4","comment_id":"1285386","content":"Selected Answer: D\nA is not because Metric filters can't directly solve the problem of counting unique users across different time periods. They can count how many logins happened, but not how many distinct users logged in during those time periods."},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html","poster":"thotwielder","upvote_count":"4","timestamp":"1712623500.0","comment_id":"1191883"},{"poster":"VerRi","comment_id":"1186572","upvote_count":"1","content":"Selected Answer: A\nWith existing logs, we don't have to make changes to the application.","timestamp":"1711858380.0"},{"content":"Selected Answer: A\nI would go for A\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html","comment_id":"1182021","upvote_count":"1","poster":"pangchn","timestamp":"1711321500.0"},{"poster":"AWSPro1234","upvote_count":"2","content":"Answer is C.","comment_id":"1180310","timestamp":"1711140300.0"},{"content":"Selected Answer: A\nA is the correct answer: it has the least changes to the application. C and D are rubbish.","comment_id":"1178681","timestamp":"1710966840.0","poster":"Dgix","upvote_count":"2"},{"content":"Selected Answer: C\nNo app code change by configuring the agent to extract & save successful login metrics as custom metrics with user name and client name dimensions.\n\n#A and #B requires app changes.\n#D needs additional lamba infra and increase complexity","poster":"CMMC","timestamp":"1710849720.0","comment_id":"1177284","upvote_count":"1"}],"exam_id":33,"answers_community":["D (50%)","A (45%)","5%"],"answer_ET":"D"},{"id":"dwfk11QRD38fYM8h9YGA","question_id":400,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/136613-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"comments":[{"content":"https://aws.amazon.com/blogs/devops/integrating-with-github-actions-ci-cd-pipeline-to-deploy-a-web-app-to-amazon-ec2/","comments":[{"timestamp":"1711321800.0","upvote_count":"4","content":"B\nas in your KB link:\nThe GitHub Actions workflows must access resources in your AWS account. Here we are using IAM OpenID Connect identity provider and IAM role with IAM policies to access CodeDeploy and Amazon S3 bucket. OIDC lets your GitHub Actions workflows access resources in AWS without needing to store the AWS credentials as long-lived GitHub secrets","poster":"pangchn","comment_id":"1182024"}],"timestamp":"1711257420.0","comment_id":"1181324","poster":"lasithasilva709","upvote_count":"2"}],"poster":"Dgix","comment_id":"1178685","timestamp":"1710967080.0","upvote_count":"8","content":"Selected Answer: B\nA is incorrect because GitHub doesn't support the aging SAML protocol.\nB is correct because GitHub does support OIDC.\nC is hysterically overengineered for this use case.\nD even more so."},{"poster":"AzureDP900","comment_id":"1309624","content":"Option B involves creating an IAM OpenID Connect (OIDC) identity provider in AWS Identity and Access Management (IAM). This will allow the company to use GitHub OIDC authentication, which provides a short-lived token for authentication.\nThe solution also creates a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. This ensures that the pipeline can assume the necessary permissions without using long-lived secret keys.","timestamp":"1731271560.0","upvote_count":"2"},{"timestamp":"1711858680.0","comments":[{"poster":"0b43291","comment_id":"1312927","timestamp":"1731728160.0","content":"In summary, the feedback suggests that options A and D are not suitable because they involve the sts:AssumeRole API call, which typically requires long-lived credentials. Instead, option B, which uses the sts:AssumeRoleWithWebIdentity API call with GitHub's OIDC identity provider, is recommended as the solution with the least operational overhead for replacing the long-lived secret key with a short-lived solution.","upvote_count":"1"}],"upvote_count":"1","poster":"VerRi","content":"Selected Answer: B\nA and D are out because of sts:AssumeRole. \nB with the least operational overhead.","comment_id":"1186575"}],"answers_community":["B (100%)"],"isMC":true,"exam_id":33,"question_images":[],"unix_timestamp":1710850260,"timestamp":"2024-03-19 13:11:00","answer_ET":"B","topic":"1","answer_images":[],"choices":{"C":"Create an Amazon Cognito identity pool. Configure the authentication provider to use GitHub. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub authentication provider. Configure the pipeline to use Cognito as its authentication provider.","D":"Create a trust anchor to AWS Private Certificate Authority. Generate a client certificate to use with AWS IAM Roles Anywhere. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Configure the pipeline to use the credential helper tool and to reference the client certificate public key to assume the new IAM role.","B":"Create an IAM OpenID Connect (OIDC) identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. Update GitHub to assume the role for the pipeline.","A":"Create an IAM SAML 2.0 identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Update GitHub to use SAML authentication for the pipeline."},"answer":"B","question_text":"A company is using GitHub Actions to run a CI/CD pipeline that accesses resources on AWS. The company has an IAM user that uses a secret key in the pipeline to authenticate to AWS. An existing IAM role with an attached policy grants the required permissions to deploy resources.\n\nThe company’s security team implements a new requirement that pipelines can no longer use long-lived secret keys. A solutions architect must replace the secret key with a short-lived solution.\n\nWhich solution will meet these requirements with the LEAST operational overhead?"}],"exam":{"provider":"Amazon","numberOfQuestions":529,"isMCOnly":true,"name":"AWS Certified Solutions Architect - Professional SAP-C02","isImplemented":true,"id":33,"isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":80},"__N_SSP":true}