{"pageProps":{"questions":[{"id":"0Gfgxe4z4cFKrLI2be9l","question_text":"A company in the United States (US) has acquired a company in Europe. Both companies use the AWS Cloud. The US company has built a new application with a microservices architecture. The US company is hosting the application across five VPCs in the us-east-2 Region. The application must be able to access resources in one VPC in the eu-west-1 Region.\nHowever, the application must not be able to access any other VPCs.\n\nThe VPCs in both Regions have no overlapping CIDR ranges. All accounts are already consolidated in one organization in AWS Organizations.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_images":[],"choices":{"C":"Create a full mesh VPC peering connection configuration between all the VPCs. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection.","D":"Create one VPC peering connection for each VPC in us-east-2 to the VPC in eu-west-1. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection.","A":"Create one transit gateway in eu-west-1. Attach the VPCs in us-east-2 and the VPC in eu-west-1 to the transit gateway. Create the necessary route entries in each VPC so that the traffic is routed through the transit gateway.","B":"Create one transit gateway in each Region. Attach the involved subnets to the regional transit gateway. Create the necessary route entries in the associated route tables for each subnet so that the traffic is routed through the regional transit gateway. Peer the two transit gateways."},"unix_timestamp":1720425960,"timestamp":"2024-07-08 10:06:00","question_id":471,"answers_community":["D (88%)","13%"],"url":"https://www.examtopics.com/discussions/amazon/view/143548-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_ET":"D","answer":"D","topic":"1","discussion":[{"upvote_count":"5","timestamp":"1720425960.0","poster":"ahrentom","comment_id":"1244189","content":"Selected Answer: D\nis most cost-effectively"},{"timestamp":"1731810120.0","content":"D meets the requirements most cost-effectively because:\nMinimum infrastructure: Creating a single VPC peering connection between each of the five VPCs in us-east-2 and the VPC in eu-west-1 requires minimal infrastructure changes.\nSimple management: This solution requires only one VPC peering connection, making it easier to manage and monitor network connectivity.\nNo need for transit gateway: Since you already have a dedicated VPC in eu-west-1 that needs to be accessed, creating a VPC peering connection is the most straightforward approach.","poster":"AzureDP900","upvote_count":"1","comment_id":"1313350"},{"timestamp":"1731176520.0","upvote_count":"1","comment_id":"1309163","content":"D is best in the scnerio.","poster":"AzureDP900"},{"content":"Selected Answer: D\nVPC peer-to-peer connection is a free service in AWS used for communication between VPCs.\nAWS's Transit Gateway is mainly used for connecting across multiple VPCs or accounts and does not directly support cross regional VPC connections.","poster":"liuliangzhou","comment_id":"1284552","upvote_count":"2","timestamp":"1726475100.0"},{"poster":"GDuque","timestamp":"1722683280.0","content":"Selected Answer: A\nTaking into account what solutions are possible, only A or B can do it, because we need a transit gateway to connect VPCs that are in different regions. You cannot peer both vpcs directly. And as for costing, A is more economic.","comments":[{"timestamp":"1722971940.0","poster":"GDuque","content":"After reconsidering, the answer is D. \nWith Inter-region VPC peering you can peer 2 VPCs in different regions. So, the most economic solution is D.","upvote_count":"3","comment_id":"1261808"}],"comment_id":"1260247","upvote_count":"1"}],"answer_description":"","exam_id":33,"question_images":[],"isMC":true},{"id":"qc4scXAFr0CXyxvp6Fpq","topic":"1","answer_description":"","isMC":true,"choices":{"B":"Enable AWS CloudTrail logging. Specify an Amazon S3 bucket as the destination for the logs.","A":"Create an Amazon SES configuration set with Amazon Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket.","D":"Create an Amazon CloudWatch log group. Configure Amazon SES to send logs to the log group.","C":"Use Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent.","E":"Use Amazon Athena to query the logs in Amazon CloudWatch for recipient, subject, and time sent."},"answers_community":["AC (80%)","13%","7%"],"timestamp":"2024-07-02 14:12:00","answer":"AC","question_id":472,"url":"https://www.examtopics.com/discussions/amazon/view/143190-exam-aws-certified-solutions-architect-professional-sap-c02/","question_images":[],"answer_images":[],"question_text":"A travel company built a web application that uses Amazon Simple Email Service (Amazon SES) to send email notifications to users. The company needs to enable logging to help troubleshoot email delivery issues. The company also needs the ability to do searches that are based on recipient, subject, and time sent.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)","exam_id":33,"unix_timestamp":1719922320,"answer_ET":"AC","discussion":[{"upvote_count":"1","timestamp":"1732491180.0","comment_id":"1317244","content":"A,C \nBy creating a configuration set with Data Firehose as the destination (A), the SES logs will be stored in an S3 bucket. Then, using Athena (C) allows you to query those logs for specific information, such as recipient, subject, and time sent","poster":"AzureDP900"},{"upvote_count":"2","content":"Selected Answer: AC\nA: An Amazon SES configuration set allows you to capture event data related to email sending, such as delivery status, bounces, and complaints. By setting Amazon Kinesis Data Firehose as the destination, you can stream these logs to an Amazon S3 bucket.\nC: Once the logs are in the S3 bucket, Amazon Athena allows you to run SQL queries directly on the data stored in S3. This enables easy searching and filtering by recipient, subject, and time sent, without having to move or load the data into a database.\nB: CloudTrail logs API calls to Amazon SES but does NOT provide detailed information about email delivery, bounces, or complaints.\nD: While you can monitor metrics in CloudWatch, it’s not a good fit for storing or searching detailed SES email logs.\nE: CloudWatch logs are not natively queryable using Athena.","comment_id":"1297004","timestamp":"1728836220.0","poster":"JoeTromundo"},{"comment_id":"1284556","poster":"liuliangzhou","comments":[{"timestamp":"1728699300.0","comment_id":"1296302","upvote_count":"1","content":"Yes it does with the connector -> https://docs.aws.amazon.com/athena/latest/ug/connectors-cloudwatch.html","poster":"starcub"}],"upvote_count":"4","content":"Selected Answer: AC\nA. Amazon Data Firehose is configured as the target of SES configuration set, which can capture and transfer SES log data in real-time to Amazon S3 storage buckets.\nB. CloudTrail is primarily used to track AWS management operations, rather than service level operation logs.\nC. Amazon Athena allows you to directly analyze data stored in Amazon S3 using SQL queries.\nD. Amazon SES does not directly support sending logs to CloudWatch.\nE. Athena does not support directly querying logs in CloudWatch.","timestamp":"1726476420.0"},{"upvote_count":"1","poster":"jopaca1216","comment_id":"1280366","content":"Selected Answer: AC\nJust It.\nhttps://docs.aws.amazon.com/ses/latest/dg/event-publishing-add-event-destination-firehose.html","timestamp":"1725799140.0"},{"timestamp":"1723742160.0","content":"Selected Answer: AC\nThe answers seem pretty split on this. Based on this https://docs.aws.amazon.com/athena/latest/ug/querying-ses-logs.html I'd go A/C","comments":[{"content":"I'd go A/C too.","timestamp":"1725571140.0","comment_id":"1279184","poster":"jopaca1216","upvote_count":"1"}],"upvote_count":"2","poster":"neta1o","comment_id":"1266577"},{"poster":"[Removed]","upvote_count":"3","comment_id":"1253640","content":"Selected Answer: AC\nvote A&C","timestamp":"1721736780.0"},{"upvote_count":"4","content":"A & C is the correct answer.","comment_id":"1243845","timestamp":"1720354320.0","poster":"Russs99"},{"upvote_count":"1","poster":"vip2","content":"Selected Answer: CE\nAthena can not direct query Cloudwatch log, so C is correct instead of E.","timestamp":"1720192020.0","comment_id":"1242845"},{"comment_id":"1242210","timestamp":"1720114500.0","content":"A & C. Cloudtrail does not track the emails.","poster":"G4Exams","upvote_count":"3"},{"comments":[{"comment_id":"1244567","poster":"Hizumi","content":"It cannot be D&E because SES Event data with Cloudwatch is not able to retrieve recipient, mail headers, and timestamp, this is what it can as per this article: https://docs.aws.amazon.com/ses/latest/dg/event-publishing-retrieving-cloudwatch.html\n\nA&C is a better choice as it is able to retrieve the information the questions is asking as per this article: https://docs.aws.amazon.com/ses/latest/dg/event-publishing-retrieving-firehose-contents.html","timestamp":"1720473420.0","upvote_count":"1"}],"upvote_count":"2","poster":"[Removed]","timestamp":"1720091160.0","comment_id":"1241995","content":"Selected Answer: DE\nhttps://aws.amazon.com/blogs/messaging-and-targeting/how-to-log-amazon-ses-details-using-amazon-cloudwatch/"}]},{"id":"qg41ezM5qJVVt5K3kV5r","question_id":473,"question_images":[],"answer":"C","isMC":true,"exam_id":33,"choices":{"D":"Create an AWS Lambda function to run daily to retrieve utilization data for all EC2 instances. Save the data to an Amazon DynamoDB table. Create an Amazon QuickSight dashboard that uses the DynamoDB table as a data source to identify and stop underutilized development EC2 instances.","A":"Configure Amazon CloudWatch dashboards to monitor EC2 instance utilization based on tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.","C":"Create an Amazon EventBridge rule to detect low utilization of EC2 instances reported by AWS Trusted Advisor. Configure the rule to invoke an AWS Lambda function that filters the data by tags for department, business unit, and environment and stops underutilized development EC2 instances.","B":"Configure AWS Systems Manager to track EC2 instance utilization and report underutilized instances to Amazon CloudWatch. Filter the CloudWatch data by tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances."},"topic":"1","question_text":"A company migrated to AWS and uses AWS Business Support. The company wants to monitor the cost-effectiveness of Amazon EC2 instances across AWS accounts. The EC2 instances have tags for department, business unit, and environment. Development EC2 instances have high cost but low utilization.\n\nThe company needs to detect and stop any underutilized development EC2 instances. Instances are underutilized if they had 10% or less average daily CPU utilization and 5 MB or less network I/O for at least 4 of the past 14 days.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/143186-exam-aws-certified-solutions-architect-professional-sap-c02/","answer_images":[],"answer_description":"","answer_ET":"C","discussion":[{"poster":"alexbraila","timestamp":"1733741220.0","content":"Selected Answer: C\nDue to the link posted by Kinnam and sam2ng, together with this\n\nhttps://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html","comment_id":"1323998","upvote_count":"1"},{"content":"Option C, This solution meets the requirements with the least operational overhead because it uses Amazon EventBridge (formerly CloudWatch Events) to trigger a response based on low utilization reports from AWS Trusted Advisor.\nAWS Trusted Advisor provides pre-configured dashboards that can be used to monitor various aspects of your AWS resources, including EC2 instance utilization. By leveraging these pre-configured dashboards, you don't need to set up additional monitoring infrastructure or write custom code.\n\nThe EventBridge rule will automatically invoke the Lambda function when a low utilization report is received from Trusted Advisor, which eliminates the need for daily polling or manual intervention. The other options are more resource-intensive and require additional setup and maintenance:","timestamp":"1732491000.0","poster":"AzureDP900","comment_id":"1317241","upvote_count":"1"},{"upvote_count":"2","comment_id":"1297006","poster":"JoeTromundo","timestamp":"1728836520.0","content":"Selected Answer: C\nAWS Trusted Advisor provides insights into underutilized EC2 instances automatically, including recommendations for cost-saving based on utilization metrics like CPU and network usage. Since the company is using AWS Business Support, they already have access to Trusted Advisor, making this a low-overhead solution.\nAmazon EventBridge can be used to create a rule that detects when Trusted Advisor reports low-utilization instances. This avoids the need for custom-built CloudWatch dashboards or manual tracking.\nAWS Lambda can be triggered to handle the logic of stopping instances that meet the specific criteria of low CPU utilization and network I/O, filtering by tags for department, business unit, and environment. Lambda is serverless and scales automatically, so it minimizes operational overhead."},{"timestamp":"1724078220.0","poster":"Kinnam","upvote_count":"2","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html#low-utilization-amazon-ec2-instances","comment_id":"1268778"},{"poster":"sam2ng","upvote_count":"3","timestamp":"1723038180.0","content":"Selected Answer: C\nThis is exactly the same criteria provided by the Trusted Advisor:\nhttps://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html#low-utilization-amazon-ec2-instances","comment_id":"1262117"},{"timestamp":"1720967460.0","comment_id":"1247807","upvote_count":"2","content":"Selected Answer: C\nC, for sure.\nTA for 10% or less average daily CPU utilization and 5 MB or less network I/O for at least 4 of the past 14 days.\nAnd least operational overhead","poster":"gfhbox0083"},{"timestamp":"1720639860.0","upvote_count":"1","content":"Selected Answer: C\nA - involves continuous monitoring and potential updates to dashboards and metrics.\nC - minimizes ongoing maintenance by relying on Trusted Advisor's automated reports.","comment_id":"1245712","poster":"Moumita"},{"upvote_count":"1","comment_id":"1244877","content":"Selected Answer: C\nA is not correct as it's missing setting up alarms for the \"detect\" part. I go with C.","timestamp":"1720522680.0","poster":"asquared16"},{"upvote_count":"3","poster":"vip2","content":"Selected Answer: A\nIt would be A as correct answer\nTagging with EC2 instances for department, business unit, and environment . \n\nCloudWatch to collect and monitor CPU utilization and network I/O metrics.\n\nCreate CloudWatch Alarms to detect underutilized instances with composite alarmwith boh CPU utilization and network I/O are low.\n\nAWS Lambda Function to be triggered by the CloudWatch Alarms and check\nthe conditions (10% or less average daily CPU utilization and 5 MB or less network I/O) hold true for at least 4 of the past 14 days. Stop the instances that meet these criteria.","comments":[{"timestamp":"1725048060.0","upvote_count":"1","comment_id":"1275250","poster":"zolthar_z","content":"Unless you create an alarm or something cloudwatch dashboard will not take any action to delete the instances, it will only show the metric data"}],"timestamp":"1720191540.0","comment_id":"1242843"},{"poster":"paderni","comments":[{"poster":"c22ddd8","upvote_count":"2","timestamp":"1720550220.0","comment_id":"1245053","content":"Will B monitor multi account ? NO Ans is C , question says to stop of the instance is low for 4 days in last 14 days."}],"upvote_count":"1","comment_id":"1241481","timestamp":"1720018620.0","content":"Not c because AWS Trusted Advisor does not provide real-time utilization metrics suitable for detecting underutilized instances over a specific timeframe. It focuses more on best practices and recommendations rather than real-time operational metrics. Should be B"}],"timestamp":"2024-07-02 14:03:00","answers_community":["C (80%)","A (20%)"],"unix_timestamp":1719921780},{"id":"jLpzHe3S5lHpQjcDg4yX","answer":"D","question_id":474,"answer_ET":"D","answers_community":["D (100%)"],"question_images":[],"topic":"1","isMC":true,"unix_timestamp":1719838920,"answer_images":[],"timestamp":"2024-07-01 15:02:00","discussion":[{"upvote_count":"1","timestamp":"1731175680.0","content":"D is more cost effective solution, most of the time CPU 10% and peak time only we need more instance to serve customer traffic.","comments":[{"comments":[{"content":"How do you know that 28 is enough during peak?","poster":"altonh","upvote_count":"1","timestamp":"1740378600.0","comment_id":"1360883"}],"timestamp":"1732490640.0","comment_id":"1317239","upvote_count":"1","content":"The minimum capacity of 4 instances will provide sufficient resources to handle normal traffic, while the maximum capacity of 28 instances can be scaled up to handle busy periods.\nBy purchasing Reserved Instances for four instances, the company can also take advantage of a discounted rate for usage of EC2 instances, which will help reduce costs further.","poster":"AzureDP900"}],"comment_id":"1309161","poster":"AzureDP900"},{"upvote_count":"1","content":"Selected Answer: D\nIt should be D","timestamp":"1720045920.0","comment_id":"1241693","poster":"Alagong"},{"upvote_count":"3","timestamp":"1719920580.0","content":"D\nD\nSince the CPU utilization of the instance is 10% during normal application use, a minimum capacity of 4 is required, which is the minimum configuration and most cost-effective.","poster":"kupo777","comment_id":"1240751"}],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/143134-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"A":"Create an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 20 and the desired capacity to 28. Purchase Reserved Instances for 20 instances.","B":"Create a Spot Fleet that has a request type of request. Set the TotalTargetCapacity parameter to 20. Set the DefaultTargetCapacityType parameter to On-Demand. Specify the NLB when creating the Spot Fleet.","C":"Create a Spot Fleet that has a request type of maintain. Set the TotalTargetCapacity parameter to 20. Set the DefaultTargetCapacityType parameter to Spot. Replace the NLB with an Application Load Balancer.","D":"Create an Auto Scaling group. Attach the Auto Scaling group to the target group of the NLB. Set the minimum capacity to 4 and the maximum capacity to 28. Purchase Reserved Instances for four instances."},"exam_id":33,"question_text":"A company is hosting an application on AWS for a project that will run for the next 3 years. The application consists of 20 Amazon EC2 On-Demand Instances that are registered in a target group for a Network Load Balancer (NLB). The instances are spread across two Availability Zones. The application is stateless and runs 24 hours a day, 7 days a week.\n\nThe company receives reports from users who are experiencing slow responses from the application. Performance metrics show that the instances are at 10% CPU utilization during normal application use. However, the CPU utilization increases to 100% at busy times, which typically last for a few hours.\n\nThe company needs a new architecture to resolve the problem of slow responses from the application.\n\nWhich solution will meet these requirements MOST cost-effectively?"},{"id":"fjqe9Jvmps1rInK0O4o5","answer":"B","topic":"1","question_id":475,"choices":{"C":"Create a topic in AWS IoT Core to ingest the sensor data. Configure an AWS IoT rule action to send the data to an Amazon Timestream table. Create an AWS Lambda, function to read the data from Timestream. Configure the Lambda function to enrich the data and to write the data to Amazon S3.","A":"Create a topic in AWS IoT Core to ingest the sensor data. Create an AWS Lambda function to enrich the data and to write the data to Amazon S3. Configure an AWS IoT rule action to invoke the Lambda function.","D":"Use AWS loT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Streams. Create a consumer AWS Lambda function to process the data from Kinesis Data Streams and to enrich the data. Call the S3 PutObject API operation from the Lambda function to write the data to Amazon S3.","B":"Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3."},"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/143133-exam-aws-certified-solutions-architect-professional-sap-c02/","timestamp":"2024-07-01 14:55:00","unix_timestamp":1719838500,"question_text":"Accompany is building an application to collect and transmit sensor data from a factory. The application will use AWS IoT Core to send data from hundreds of devices to an Amazon S3 data lake. The company must enrich the data before loading the data into Amazon S3.\n\nThe application will transmit the sensor data every 5 seconds. New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data. No other applications are processing the sensor data from AWS IoT Core.\n\nWhich solution will meet these requirements MOST cost-effectively?","answer_images":[],"discussion":[{"content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/iot/ingesting-enriched-iot-data-into-amazon-s3-using-amazon-kinesis-data-firehose/","comment_id":"1247872","timestamp":"1720977300.0","poster":"mark_232323","upvote_count":"8"},{"comments":[{"comment_id":"1275251","poster":"zolthar_z","upvote_count":"3","content":"Answer is C, hundreds of devices sending data every 5 seconds, if you have 100 devices you will trigger the lambda 1200 times in one minute,","timestamp":"1725048360.0"}],"upvote_count":"6","poster":"053081f","timestamp":"1720956360.0","content":"Selected Answer: A\nIn this application, sensor data is transmitted at the following intervals:\n\n1. Device to IoT Core (every 5 seconds)\n2. IoT Core to S3 (every 30 minutes)\n\nThe data load from IoT Core to S3 doesn't necessarily need to be real-time, and the most cost-effective solution is option A. Option A uses the simplest method to load data without using resources like Kinesis.","comment_id":"1247745"},{"comment_id":"1356077","upvote_count":"1","timestamp":"1739445060.0","poster":"85b5b55","content":"Selected Answer: A\nthe requirements is MOST cost-effective solutions. hence, I choosed A. (i.e. Less resources)"},{"poster":"AzureDP900","comment_id":"1317236","content":"B is right, this meets the requirement of making new sensor data available in Amazon S3 less than 30 minutes after the application collects the data. The buffering interval of 900 seconds (15 minutes) is sufficient to meet this requirement, and the use of Kinesis Data Firehose ensures that the data is processed and delivered to Amazon S3 in a timely manner.\nThis solution is also cost-effective because it:\nUses AWS IoT Core Basic Ingest, which is free for up to 10 GB of incoming data per month.\nUses Kinesis Data Firehose, which has a low cost compared to other services like Lambda or Timestream.\nDoes not require the creation of multiple resources (e.g., Lambda functions, topics) as in some other solutions.","upvote_count":"1","timestamp":"1732490340.0"},{"timestamp":"1731104820.0","upvote_count":"1","comment_id":"1308940","poster":"sashenka","comments":[{"upvote_count":"1","timestamp":"1731104880.0","comment_id":"1308941","poster":"sashenka","content":"Why not Option B? : IoT Core → Firehose → Lambda → S3 \n* 900-second (15-minute) buffer adds unnecessary delay\n* Additional cost for Firehose service\n* More complex than necessary for the use case"}],"content":"Selected Answer: A\nOption A is the most cost-effective solution because:\n* Uses minimal services while meeting all requirements\n* Leverages serverless architecture for automatic scaling\n* Provides immediate processing without buffering delays\n* Minimizes costs by eliminating unnecessary services\n* Direct integration between IoT Core and Lambda ensures low latency\n\nThe Lambda function can process messages immediately as they arrive from IoT Core, enrich the data, and write to S3 well within the 30-minute requirement. This architecture is both simple and cost-effective, avoiding unnecessary services and their associated costs."},{"upvote_count":"1","content":"B. buffering helps with cost of lambda","comment_id":"1307341","timestamp":"1730806200.0","poster":"doobc"},{"timestamp":"1729822380.0","upvote_count":"2","comment_id":"1302709","content":"AWS IoT Core Basic is cheaper than AWS IoT core, also Kinesis Data Firehose Batching will reduce the number of write operations to S3 and Lambda invocations by buffering data. Hence even though there is an additional component of Kinesis Data Firehose, it is more cost effective than option A. According to me, the answer is Option B","poster":"Danm86"},{"upvote_count":"1","content":"Selected Answer: B\nNo other applications are processing the sensor data from AWS IoT Core: \nUse AWS IoT Core Basic Ingest to ingest the sensor data to reduce messaging cost: B or D\nhttps://docs.aws.amazon.com/iot/latest/developerguide/iot-basic-ingest.html\n\nConfigure an AWS IoT rule action to write the data to Amazon KDF or KDS? \"New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data.\" =>near real time, stream data to s3, no need storage or replay, we shd use autoscaling and fully managed KDF.","comment_id":"1285535","poster":"Daniel76","timestamp":"1726623480.0"},{"comment_id":"1284634","timestamp":"1726486440.0","content":"Selected Answer: A\nA. The advantage of this method is its simplicity and high real-time performance, as Lambda functions can immediately respond to IoT events. The cost of Lambda functions is based on execution time and resource usage, which is very economical for small data processing tasks.\nB. The 900 second buffer interval of Kinesis Data Firehose does not meet real-time requirements (data needs to be processed within 30 minutes, while the set buffer here is 15 minutes). In addition, introducing Kinesis Data Firehose adds additional cost and complexity, especially when Lambda functions can directly process data.","upvote_count":"1","poster":"liuliangzhou"},{"poster":"jopaca1216","content":"Selected Answer: B\nMany devices sending data every 5 seconds, it's not necessary, due that you just need the data available in S3 within 30 minutes!","timestamp":"1725575460.0","comment_id":"1279199","upvote_count":"1"},{"comment_id":"1275059","timestamp":"1725019260.0","upvote_count":"1","poster":"_Jassybanga_","content":"the data emitting time is 5 sec and lambda may take upto 15 mins to enrich the data , this detail is only captured in buffer section of KFS , hence going with B, If there is SQS queue in option A before lambda then i would have choosen that"},{"content":"Selected Answer: B\nAs per this link it is B, firehose is used:\nhttps://aws.amazon.com/blogs/iot/ingesting-enriched-iot-data-into-amazon-s3-using-amazon-kinesis-data-firehose/","poster":"dzidis","upvote_count":"2","timestamp":"1723719780.0","comment_id":"1266388"},{"comment_id":"1265091","upvote_count":"2","timestamp":"1723543380.0","poster":"Incognito013","content":"Selected Answer: A\nWe need simple and cost effective so choosing A"},{"comment_id":"1258623","upvote_count":"3","content":"Selected Answer: B\nI prefer B","timestamp":"1722407640.0","poster":"tsangckl"},{"upvote_count":"4","timestamp":"1721688360.0","poster":"Chakanetsa","content":"Selected Answer: B\nBest Answer: B. Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3.\nReasoning:\nCost-effective: IoT Core Basic Ingest is the most cost-effective option for high-volume, low-value data.\nLow latency: Kinesis Data Firehose with a 900-second buffering interval provides a balance between cost and latency, meeting the requirement of data availability in S3 within 30 minutes.\nScalability: Kinesis Data Firehose can handle high throughput, making it suitable for large volumes of sensor data.\nSimplicity: The solution involves a straightforward pipeline with minimal components.","comment_id":"1253314"},{"comment_id":"1251847","content":"Selected Answer: B\nB is correct","timestamp":"1721493300.0","upvote_count":"2","poster":"vip2"},{"content":"Selected Answer: A\nVote A because only required minimum services are involved. IoT core topic to hold income data, Lambda to enrich data and save to s3. IoT rule call Lambda and consume the incoming data.","poster":"Helpnosense","timestamp":"1720097400.0","upvote_count":"4","comment_id":"1242078"},{"upvote_count":"1","content":"Selected Answer: D\nD is a typical scenario","poster":"[Removed]","timestamp":"1720095720.0","comment_id":"1242063"},{"upvote_count":"2","timestamp":"1719951120.0","content":"A is incorrect.\nC is correct.\nAWS loT Core Basic Ingest → Cost optimization\nAmazon Kinesis Data Streams → Send every 5 seconds","comment_id":"1241025","poster":"kupo777"},{"content":"A\nSimple and most cost-effective.","poster":"kupo777","timestamp":"1719838500.0","upvote_count":"3","comment_id":"1240150"}],"isMC":true,"exam_id":33,"question_images":[],"answer_description":"","answers_community":["B (57%)","A (41%)","3%"]}],"exam":{"isImplemented":true,"isMCOnly":true,"numberOfQuestions":529,"lastUpdated":"11 Apr 2025","id":33,"isBeta":false,"provider":"Amazon","name":"AWS Certified Solutions Architect - Professional SAP-C02"},"currentPage":95},"__N_SSP":true}