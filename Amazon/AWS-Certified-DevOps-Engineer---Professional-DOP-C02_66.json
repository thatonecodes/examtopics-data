{"pageProps":{"questions":[{"id":"73i0VkovSzRjp5elQ4jZ","question_images":[],"answer_description":"","unix_timestamp":1683088380,"question_text":"A company wants to set up a continuous delivery pipeline. The company stores application code in a private GitHub repository. The company needs to deploy the application components to Amazon Elastic Container Service (Amazon ECS). Amazon EC2, and AWS Lambda. The pipeline must support manual approval actions.\nWhich solution will meet these requirements?","discussion":[{"timestamp":"1683088380.0","content":"Selected Answer: B\nB is correct","comment_id":"888142","upvote_count":"8","poster":"haazybanj"},{"comment_id":"949583","content":"Selected Answer: B\nBecause the Term \"The pipeline must support manual approval actions.\" \nThat is not possible without a pipeline :)","timestamp":"1689149760.0","upvote_count":"7","poster":"Just_Ninja"},{"content":"Selected Answer: B\nLambda is not defined as a deployment provider. Only as an \"invoke\" option. Amazon ECS is possible as deploy provider , check https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html where it gives: CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. . So I go for B.","upvote_count":"1","timestamp":"1735649820.0","poster":"1rob","comment_id":"1334809"},{"poster":"Zdujgfr567783ff","content":"Selected Answer: A\nOption B is partially correct but lacks native support for ECS deployments. AWS CodeDeploy is excellent for deploying to EC2 and Lambda, but it doesn't natively handle ECS deployments without additional configuration.","timestamp":"1735087740.0","upvote_count":"1","comment_id":"1331292"},{"upvote_count":"1","poster":"Zdujgfr567783ff","content":"Selected Answer: A\nasked chat GPT says a","timestamp":"1735087620.0","comment_id":"1331291"},{"content":"A is correct, CodeDeploy can't deploy the ECS","timestamp":"1725192000.0","comment_id":"1276039","upvote_count":"1","poster":"hzaki"},{"comment_id":"1138388","upvote_count":"1","content":"A is correct","poster":"thanhnv142","timestamp":"1706866800.0","comments":[{"poster":"thanhnv142","content":"Correction: B is correct","comment_id":"1147995","timestamp":"1707735000.0","upvote_count":"1"}]},{"comment_id":"1070137","timestamp":"1699948680.0","content":"Selected Answer: B\nThe solution for deploy ECS by codePipeline and codeDeploy\n\nCreate your CodeDeploy application and deployment group (ECS compute platform\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-ecs-ecr-codedeploy.html#tutorials-ecs-ecr-codedeploy-deployment","upvote_count":"4","poster":"due"},{"upvote_count":"3","timestamp":"1695110400.0","poster":"RVivek","comments":[{"upvote_count":"3","comment_id":"1104443","content":"using codedeploy we can deploy to ecs and even can perform blue / green deployment. Codedeploy support all there of the deployment strategies","timestamp":"1703391540.0","poster":"z_inderjot"}],"content":"Selected Answer: A\nWhy not A ?\nB (Code depoly providr does not support ECS)\nD does not have \"codepipeleine\" and the question says \"\"The pipeline must support manual approval actions.\" \n\nSo A is the only feasible option","comment_id":"1011087"},{"upvote_count":"1","comments":[{"poster":"Radeeka","upvote_count":"1","timestamp":"1693015140.0","content":"My bad, Above only support EC2 and OnPrem.","comment_id":"990471"}],"comment_id":"985739","content":"Selected Answer: D\nAnswer D.\nSource: https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-partners-github.html\nThe question is asking for a application code stored in a GitHub repository.","timestamp":"1692530820.0","poster":"Radeeka"},{"poster":"Aja1","comments":[{"comment_id":"964678","timestamp":"1690459200.0","content":"i think B is correct","poster":"Aja1","upvote_count":"1"}],"timestamp":"1690452420.0","comment_id":"964576","content":"option A with AWS CodePipeline and individual deployment actions for Amazon ECS, Amazon EC2, and AWS Lambda, along with support for manual approval actions, is the most suitable solution to meet the requirements of the continuous delivery pipeline.\nB mentions using AWS CodeDeploy as the deploy provider, but it does not explicitly mention support for deploying to Amazon ECS, Amazon EC2, and AWS Lambda. AWS CodeDeploy primarily focuses on deploying applications to Amazon EC2 instances, and while it does have support for AWS Lambda, it might not be as straightforward to use for deploying to Amazon ECS.","upvote_count":"2"},{"poster":"habros","upvote_count":"6","timestamp":"1688867220.0","comment_id":"946823","content":"Selected Answer: B\nYou will need a deployment tool (CodeDeploy) for this. You cannot directly deploy via CodePipeline. Hence, B."}],"question_id":326,"answers_community":["B (81%)","Other"],"answer_images":[],"choices":{"D":"Use AWS CodeDeploy with GitHub integration to deploy the application.","A":"Use AWS CodePipeline with Amazon ECS. Amazon EC2, and Lambda as deploy providers.","C":"Use AWS CodePipeline with AWS Elastic Beanstalk as the deploy provider.","B":"Use AWS CodePipeline with AWS CodeDeploy as the deploy provider."},"exam_id":23,"url":"https://www.examtopics.com/discussions/amazon/view/108356-exam-aws-certified-devops-engineer-professional-dop-c02/","isMC":true,"answer":"B","timestamp":"2023-05-03 06:33:00","answer_ET":"B","topic":"1"},{"id":"rPGz3iRnhJCDfJ53vHLo","unix_timestamp":1680957180,"timestamp":"2023-04-08 14:33:00","answer_images":[],"discussion":[{"upvote_count":"18","poster":"haazybanj","timestamp":"1682919420.0","content":"Selected Answer: A\nOption A is the most cost-effective solution. By configuring a warm pool of EC2 instances in the Stopped state, the company can reduce the time it takes for new instances to be ready to serve requests. When the Auto Scaling group launches a new instance, it can attach the stopped EC2 instance from the warm pool. The instance can then be started up immediately, rather than having to wait for the data to be downloaded and processed. This reduces the overall startup time for the application.\n\nOption C is also a solution that involves a warm pool of EC2 instances, but the instances are in the Running state. This means that they are already running and incurring costs, even though they are not currently serving requests. This is not a cost-effective solution.","comment_id":"885896"},{"upvote_count":"2","comment_id":"1255565","timestamp":"1721979900.0","content":"Selected Answer: A\nkeywords: MOST cost-effective way to reduce the application startup time","poster":"jamesf"},{"comments":[{"timestamp":"1714553400.0","content":"I thought this, as well, but A appears to be correct. See https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/","poster":"Jay_2pt0_1","comment_id":"1204961","upvote_count":"1"}],"comment_id":"1182493","content":"Selected Answer: C\nC \" The company must reduce the time that elapses before new EC2 instances are ready to serve requests.\"!!!!!!!!!! this cannot happen with a stopped instance as it will still need to read the data from S3 upon startup,","upvote_count":"1","timestamp":"1711373040.0","poster":"stoy123"},{"content":"A \nfor warm pool in the hibernated or stop status we will pay only for the attached EBS volume, therefore its much cost effective rather than running instance","upvote_count":"1","comment_id":"1162638","timestamp":"1709214840.0","poster":"Shasha1"},{"comment_id":"1158319","poster":"dzn","timestamp":"1708831140.0","upvote_count":"1","content":"Selected Answer: A\nWarm Pool allows instances to be set to a stopped state after performing any process (e.g., running initialization scripts, warm-up tasks, etc.)."},{"upvote_count":"1","content":"A is correct: the question says <the application needs to process data from an Amazon S3 bucket before the application can start to serve requests> but <The size of the data that is stored in the S3 bucket is growing>. This means we should maintain a warm pool for EC2 so that they are always ready to process data (reduce the time that elapses before new EC2 instances are ready)\nB and D: no mention of warmpool\nC: If the instance is up and running, no need to configure warm pool","comment_id":"1138394","poster":"thanhnv142","timestamp":"1706867100.0"},{"poster":"zolthar_z","comment_id":"1078531","content":"Selected Answer: A\nAnswer is A, the question is cost-effective, and even with A you will have less wait time to download the S3 data, it will download the delta from the warm up process to ready to join to ASG","comments":[{"timestamp":"1704513180.0","poster":"Jaguaroooo","upvote_count":"1","content":"A&C are both good in terms of solutions, however, the caveat here is the \"cost-effective\" solution and that's why I agree with A. \nhttps://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/","comment_id":"1114960"}],"upvote_count":"2","timestamp":"1700753460.0"},{"poster":"RVivek","timestamp":"1694507340.0","comment_id":"1005554","upvote_count":"1","content":"Selected Answer: A\nexcerpt from the url: https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/\nEC2 Auto Scaling Warm Pools works by launching a configured number of EC2 instances in the background, allowing any lengthy application initialization processes to run as necessary, and then stopping those instances until they are needed"},{"upvote_count":"1","poster":"beanxyz","timestamp":"1692698760.0","content":"Selected Answer: A\nA is the most cost-effective solution. Besides, when the warm EC2 was created, it already downloaded the contents from S3 so the next time when it started, it would just download any new files from S3. ( e.g s3 sync )","comment_id":"987326"},{"upvote_count":"2","poster":"ixdb","content":"C is right.\nplease carefully check the question:\nThe company must reduce the time that elapses before new EC2 instances are ready to serve requests.\nWhen the application starts up. the application needs to process data from an Amazon S3 bucket before the application can start to serve requests.","timestamp":"1692066960.0","comment_id":"981230"},{"content":"Selected Answer: C\nI understand the question is asking for the most cost-effective. keeping it stopped state is most cost efficient but it would not work because in the question it also states that, \"When the application starts up. the application needs to process data\" and to process that data takes time. If the Ec2 instance is stopped then started at the time of need, then again it will take time to process the data, right? so in this scenario, the EC2 instance need to be running.","comment_id":"973959","poster":"Chetantest07","upvote_count":"3","timestamp":"1691334900.0"},{"content":"I think it should be C, as the A option would not be effective. Coming from the instance stop state the application will start up again and need to process the data from S3 bucket.","timestamp":"1690306680.0","poster":"Suyx","comment_id":"962963","upvote_count":"2"},{"poster":"Snape","content":"Selected Answer: A\nWarm pool with stopped state is most cost efficient option","timestamp":"1689222600.0","upvote_count":"3","comment_id":"950325"},{"poster":"pepecastr0","upvote_count":"1","comment_id":"940931","content":"Selected Answer: A\nA - Keep it stopped until you need it to save money","timestamp":"1688305260.0"},{"comment_id":"870470","poster":"jqso234","upvote_count":"1","content":"Selected Answer: C\nWhile A can also be a cost-effective solution, C is the MOST cost-effective solution because it utilizes Amazon S3 Transfer Acceleration, which is a feature that enables fast, easy, and secure transfers of files over the internet between Amazon S3 buckets and EC2 instances located in different regions or across the internet. By using S3 Transfer Acceleration, the data transfer speed can be increased significantly, which can reduce the time that elapses before new EC2 instances are ready to serve requests.\n\nIn contrast, A suggests using a larger instance size with more CPU and network capacity, which can be more expensive than the current instance size. Moreover, this approach may not be scalable in the long run since as the data in the S3 bucket continues to grow, the instance size may need to be further increased, which can incur more costs. Therefore, while A can also be a viable solution, C is the most cost-effective and scalable solution.","timestamp":"1681505340.0"},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html\nKeeping instances in a Stopped state is an effective way to minimize costs.","poster":"ma_rio","upvote_count":"4","comment_id":"869166","timestamp":"1681368840.0"},{"timestamp":"1680957180.0","poster":"Dimidrol","content":"Selected Answer: A\nA for me to decrease costs","upvote_count":"2","comment_id":"864680"}],"question_id":327,"answer_description":"","isMC":true,"topic":"1","question_text":"A company has an application that runs on Amazon EC2 instances that are in an Auto Scaling group. When the application starts up. the application needs to process data from an Amazon S3 bucket before the application can start to serve requests.\nThe size of the data that is stored in the S3 bucket is growing. When the Auto Scaling group adds new instances, the application now takes several minutes to download and process the data before the application can serve requests. The company must reduce the time that elapses before new EC2 instances are ready to serve requests.\nWhich solution is the MOST cost-effective way to reduce the application startup time?","choices":{"C":"Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Running state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.","A":"Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Stopped state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.","B":"Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.","D":"Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook and to place the new instance in the Standby state when the application is ready to serve requests."},"question_images":[],"answer":"A","answer_ET":"A","exam_id":23,"answers_community":["A (88%)","13%"],"url":"https://www.examtopics.com/discussions/amazon/view/105586-exam-aws-certified-devops-engineer-professional-dop-c02/"},{"id":"TPyYwWHhS46hgulzpGHX","timestamp":"2023-05-01 07:39:00","topic":"1","question_text":"A company is using an AWS CodeBuild project to build and package an application. The packages are copied to a shared Amazon S3 bucket before being deployed across multiple AWS accounts.\nThe buildspec.yml file contains the following:\n//IMG//\n\nThe DevOps engineer has noticed that anybody with an AWS account is able to download the artifacts.\nWhat steps should the DevOps engineer take to stop this?","exam_id":23,"discussion":[{"content":"Selected Answer: D\nD is correct","comment_id":"885897","timestamp":"1682919540.0","poster":"haazybanj","upvote_count":"14"},{"comments":[{"timestamp":"1692700260.0","upvote_count":"6","content":"I mean D...","comment_id":"987344","poster":"beanxyz"}],"content":"Selected Answer: A\n--acl authenticated-read means any authenticated users can read the S3 bucket. We should remove it and configure the bucket policy to explicitly grant access","timestamp":"1692700260.0","upvote_count":"5","poster":"beanxyz","comment_id":"987341"},{"poster":"jamesf","upvote_count":"3","comment_id":"1255567","content":"Selected Answer: D\n\"--acl authenticated-read\" means any authenticated users can read the S3 bucket. We should remove it and configure the bucket policy to explicitly grant access","timestamp":"1721980080.0"},{"timestamp":"1713294000.0","poster":"zijo","upvote_count":"2","comment_id":"1196756","content":"D is the answer\n\nACL-authenticated users: This refers to any user who has successfully authenticated with AWS credentials, including IAM users and federated users. It does not include anonymous users (public access).\nIt's generally recommended to use bucket policies for access control in S3 rather than ACLs. Bucket policies offer more granular control and better security practices. You can achieve \"acl-authenticated reads\" access using a bucket policy as well."},{"content":"Selected Answer: D\n`remove --acl authenticated-read` is required to fulfill the requirement.","upvote_count":"4","comment_id":"1158329","poster":"dzn","timestamp":"1708833480.0"},{"poster":"thanhnv142","content":"B is correct: In the \"buildspec.yml file\", we see that there is \"--acl authenticated-read\". This allow all aws users who successfully authen to AWS can download the file. To restrict access, we need to modify ACL that only grant access to some specific users. \nNote that we should not use bucket policy because it will affect all ojbects in the bucket (that is why it is called BUCKET policy). We only need to restrict acess to an object, then ACL is the right choice.\nA is incorrect: Use use --acl public-read means we allow all user to access the object \nC and D: Use bucket policy, which is incorrect","comment_id":"1138423","upvote_count":"1","timestamp":"1706869560.0"},{"timestamp":"1700753580.0","upvote_count":"2","comment_id":"1078533","poster":"zolthar_z","content":"Selected Answer: D\nD is correct"}],"choices":{"D":"Modify the post_build command to remove --acl authenticated-read and configure a bucket policy that allows read access to the relevant AWS accounts only.","B":"Configure a default ACL for the S3 bucket that defines the set of authenticated users as the relevant AWS accounts only and grants read-only access.","A":"Modify the post_build command to use --acl public-read and configure a bucket policy that grants read access to the relevant AWS accounts only.","C":"Create an S3 bucket policy that grants read access to the relevant AWS accounts and denies read access to the principal “*”."},"question_images":["https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image5.png"],"answer":"D","question_id":328,"isMC":true,"answers_community":["D (82%)","A (18%)"],"answer_description":"","answer_images":[],"answer_ET":"D","unix_timestamp":1682919540,"url":"https://www.examtopics.com/discussions/amazon/view/108079-exam-aws-certified-devops-engineer-professional-dop-c02/"},{"id":"cVaQWCbKsY9MZNtnDziU","choices":{"C":"Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.","D":"Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Write the secret to AWS Systems Manager Parameter Store as a string. Update the SAM templates and the Python code to pull the secret from Parameter Store.","A":"Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Write the secret to AWS Systems Manager Parameter Store as a secure string. Update the SAM templates and the Python code to pull the secret from Parameter Store.","B":"Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager."},"answer_description":"","timestamp":"2023-05-01 07:42:00","answer_images":[],"answer_ET":"B","discussion":[{"content":"Selected Answer: B\nB\nThe MOST secure solution that meets the requirement of automatically detecting and preventing hardcoded secrets is to use AWS CodeGuru Reviewer to check the code for any hardcoded secrets, and then update the SAM templates and Python code to retrieve the secrets from AWS Secrets Manager.\n\nOption B is the correct answer. By associating the CodeCommit repository with Amazon CodeGuru Reviewer, the code can be checked for any hardcoded secrets during code reviews. When a hardcoded secret is detected, CodeGuru Reviewer will recommend updating the code to retrieve the secret from a secure storage service like AWS Secrets Manager. The DevOps engineer can choose the option to protect the secret and then update the SAM templates and Python code to retrieve the secret from AWS Secrets Manager instead of hardcoding it in the code.","poster":"haazybanj","comment_id":"885900","timestamp":"1698824520.0","upvote_count":"12"},{"poster":"rhinozD","upvote_count":"8","timestamp":"1702652220.0","content":"Selected Answer: B\nB is correct.\nCodeGuru Reviewer for security problems.\nAmazon CodeGuru Profiler is for performance.","comment_id":"924199"},{"upvote_count":"1","poster":"thanhnv142","timestamp":"1722589380.0","comment_id":"1138461","content":"B is correct: <implement a solution to automatically detect and prevent hardcoded secrets> means we need CodeGuru reviewer to analyze the code and uncover hardcoded credentials. \nA and C: no mention of CodeGuru reviewer\nD: using System Manager Parameter store is a good method to avoid hardcoded credentials. However, the question requires <the MOST secure solution>, so we should use AWS secret manager (option B). It costs more than Para store, but more secure."},{"upvote_count":"1","comment_id":"1105066","timestamp":"1719290880.0","content":"Selected Answer: C\nI'd say it's C, because the system to examine includes Python code and CodeGuru profiles for Python needs the decorator: https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda.html","poster":"a16a848"},{"content":"option C\nhttps://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda-command-line.html","comments":[{"content":"Sorry B\n\nAmazon CodeGuru Reviewer and Amazon CodeGuru Profiler are both tools that can be used to improve the quality and security of your code. However, they have different strengths and weaknesses.\n\nCodeGuru Reviewer is a static code analysis tool that can be used to find potential defects in your code. It can scan your code for hardcoded secrets, security vulnerabilities, and other potential problems. CodeGuru Reviewer can also provide recommendations on how to fix the problems that it finds.\n\nCodeGuru Profiler is a dynamic code analysis tool that can be used to understand how your code performs. It can track the performance of your code, identify bottlenecks, and suggest ways to improve performance. CodeGuru Profiler can also be used to find potential memory leaks and other performance problems.","poster":"Aja1","timestamp":"1707412320.0","upvote_count":"6","comment_id":"975793"}],"comment_id":"964652","upvote_count":"1","poster":"Aja1","timestamp":"1706362320.0"},{"poster":"MarDog","upvote_count":"2","content":"Selected Answer: B\nDefinitely B.","timestamp":"1703027220.0","comment_id":"927948"}],"exam_id":23,"question_images":[],"isMC":true,"answer":"B","question_id":329,"answers_community":["B (96%)","4%"],"topic":"1","question_text":"A company has developed a serverless web application that is hosted on AWS. The application consists of Amazon S3. Amazon API Gateway, several AWS Lambda functions, and an Amazon RDS for MySQL database. The company is using AWS CodeCommit to store the source code. The source code is a combination of AWS Serverless Application Model (AWS SAM) templates and Python code.\nA security audit and penetration test reveal that user names and passwords for authentication to the database are hardcoded within CodeCommit repositories. A DevOps engineer must implement a solution to automatically detect and prevent hardcoded secrets.\nWhat is the MOST secure solution that meets these requirements?","url":"https://www.examtopics.com/discussions/amazon/view/108080-exam-aws-certified-devops-engineer-professional-dop-c02/","unix_timestamp":1682919720},{"id":"0zc0DZ0BLwmIaFCdUR3G","discussion":[{"content":"B caters to both existing and new buckets. \nC is triggered on when new bucket is created, existing buckets are not handled by the event.","comment_id":"908018","timestamp":"1685194320.0","poster":"paali","upvote_count":"13"},{"timestamp":"1683349200.0","poster":"Zoe_zoe","comment_id":"890488","upvote_count":"10","content":"Selected Answer: B\nB to me"},{"poster":"Gomer","upvote_count":"2","timestamp":"1718158920.0","comment_id":"1228782","content":"Selected Answer: C\nI think neither \"B\" or \"C\" is complete solution. They both need to be done to deal with both existing and new buckets.\nA carefull reading of the question doesn't preclude the need to do both.\nHowever, the specific and emphasized criteria of enabling encryption \"as soon as the S3 buckets are created\" can only be done by \"C\" (event driven action)\nI think this may be a trick question. I'm very confident they are defining an event driven action as part of the solution, and only \"C\" provides that.\n\nB: (NO) \"Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.\"\nComment: Doesn't achieve \"encryption must be enabled on new S3 buckets as soon as the S3 buckets are created.\""},{"comment_id":"1158334","upvote_count":"1","content":"Selected Answer: B\n`s3-bucket-server-side-encryption-enabled` checks if your Amazon S3 bucket either has the Amazon S3 default encryption enabled or that the Amazon S3 bucket policy explicitly denies put-object requests without server side encryption that uses AES-256 or AWS Key Management Service.","timestamp":"1708834560.0","poster":"dzn"},{"comment_id":"1138480","poster":"thanhnv142","comments":[{"upvote_count":"1","comment_id":"1149042","poster":"thanhnv142","content":"Correct: D","timestamp":"1707816780.0"}],"upvote_count":"2","timestamp":"1706873100.0","content":"A is correct: <implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets>: We can use lambda to configure all S3. Use Eventbridge to schedule-run lambda. \nB: This option uses AWS config rule to activate AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook, which is incorrect. Remember that AWS config have no action and cannot trigger anything. It only collect data and report. Additionally, this option does not mention actions to new S3 bucket\nC: <define the rule with an event pattern that matches the creation of new S3 buckets> means that this only affect newly-created bucket, not existing ones. \nD: No mention of enforcing encryption on S3\n\nNote: Should not use chatgpt for this exam, its answers are mostly wrong"},{"content":"Selected Answer: B\nAnswer is B\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-server-side-encryption-enabled.html","comment_id":"1116589","upvote_count":"5","poster":"davdan99","timestamp":"1704715440.0"},{"upvote_count":"1","comment_id":"1115175","poster":"Jaguaroooo","timestamp":"1704545460.0","content":"I would have chose B over D because aws config can do this with lambda."},{"content":"A has automation. I didn't like B: because of this statement: Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.","timestamp":"1704545400.0","upvote_count":"1","poster":"Jaguaroooo","comment_id":"1115174"},{"content":"Amazon S3 Encrypts New Objects By Default\nhttps://aws.amazon.com/blogs/aws/amazon-s3-encrypts-new-objects-by-default/#:~:text=At%20AWS%2C%20security%20is%20the,specify%20a%20different%20encryption%20option.","timestamp":"1703521080.0","comment_id":"1105390","poster":"Jamshif01","upvote_count":"1"},{"content":"Selected Answer: B\nB to me.\nAWS Config can monitor resource compliance against desired configurations. The managed rule s3-bucket-server-side-encryption-enabled checks whether Amazon S3 buckets have server-side encryption enabled. The AWS Systems Manager Automation runbook, AWS-EnableS3BucketEncryption, can be used as a remediation action to enable default encryption. This solution would also work for new buckets as soon as they're created, making it an effective solution.","timestamp":"1698317160.0","comment_id":"1054472","poster":"zenith_cloud","upvote_count":"1"},{"timestamp":"1686834060.0","poster":"rhinozD","upvote_count":"4","comment_id":"924203","content":"Selected Answer: B\nB is right.\nDoable solution for new buckets as well as existing buckets."},{"comments":[],"poster":"marcoforexam","timestamp":"1683449760.0","comment_id":"891257","content":"Selected Answer: C\nOption C meets the requirement of modifying the policy immediately after creating the bucket.","upvote_count":"1"}],"choices":{"C":"Create an AWS Lambda function that is invoked by an Amazon EventBridge event rule. Define the rule with an event pattern that matches the creation of new S3 buckets. Program the Lambda function to parse the EventBridge event, check the configuration of the S3 buckets from the event, and set AES-256 as the default encryption.","B":"Set up and activate the s3-bucket-server-side-encryption-enabled AWS Config managed rule. Configure the rule to use the AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook as the remediation action. Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.","A":"Create an AWS Lambda function that is invoked periodically by an Amazon EventBridge scheduled rule. Program the Lambda function to scan all current S3 buckets for encryption status and to set AES-256 as the default encryption for any S3 bucket that does not have an encryption configuration.","D":"Configure an IAM policy that denies the s3:CreateBucket action if the s3:x-amz-server-side-encryption condition key has a value that is not AES-256. Create an IAM group for all the company’s IAM users. Associate the IAM policy with the IAM group."},"timestamp":"2023-05-06 07:00:00","answer_description":"","answers_community":["B (88%)","13%"],"question_id":330,"isMC":true,"question_text":"A company is using Amazon S3 buckets to store important documents. The company discovers that some S3 buckets are not encrypted. Currently, the company’s IAM users can create new S3 buckets without encryption. The company is implementing a new requirement that all S3 buckets must be encrypted.\n\nA DevOps engineer must implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets. The encryption must be enabled on new S3 buckets as soon as the S3 buckets are created. The default encryption type must be 256-bit Advanced Encryption Standard (AES-256).\n\nWhich solution will meet these requirements?","answer_ET":"B","exam_id":23,"answer_images":[],"unix_timestamp":1683349200,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/108608-exam-aws-certified-devops-engineer-professional-dop-c02/","topic":"1","answer":"B"}],"exam":{"isMCOnly":true,"numberOfQuestions":355,"isImplemented":true,"provider":"Amazon","id":23,"name":"AWS Certified DevOps Engineer - Professional DOP-C02","isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":66},"__N_SSP":true}