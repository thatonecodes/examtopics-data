{"pageProps":{"questions":[{"id":"ULHo9HLLi3Q5IqiL0hzi","topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/131675-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-01-20 11:48:00","answer_images":[],"answer":"B","question_images":[],"choices":{"C":"Use the Amazon Redshift Data API to publish a message to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the SQS queue to invoke the Lambda function.","D":"Use a second Lambda function to invoke the first Lambda function based on AWS CloudTrail events.","A":"Use a second Lambda function to invoke the first Lambda function based on Amazon CloudWatch events.","B":"Use the Amazon Redshift Data API to publish an event to Amazon EventBridge. Configure an EventBridge rule to invoke the Lambda function."},"answers_community":["B (88%)","13%"],"exam_id":21,"discussion":[{"upvote_count":"13","comment_id":"1127208","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/data-api-monitoring-events.html","poster":"milofficial","timestamp":"1705747680.0"},{"comment_id":"1137923","timestamp":"1706820840.0","poster":"TonyStark0122","upvote_count":"9","content":"The most appropriate way for the data engineer to invoke the Lambda function to write load statuses to the DynamoDB table is:\n\nB. Use the Amazon Redshift Data API to publish an event to Amazon EventBridge. Configure an EventBridge rule to invoke the Lambda function.\n\nExplanation:\nOption B leverages the Amazon Redshift Data API to publish events to Amazon EventBridge, which provides a serverless event bus service for handling events across AWS services. By configuring an EventBridge rule to invoke the Lambda function in response to events published by the Redshift Data API, the data engineer can ensure that the Lambda function is triggered whenever there is a new transaction data load in Amazon Redshift. This approach offers a straightforward and scalable solution for tracking table load statuses without relying on additional Lambda functions or services."},{"upvote_count":"1","poster":"MephiboshethGumani","comment_id":"1395673","timestamp":"1741975440.0","content":"Selected Answer: B\nthe data engineer should use Amazon EventBridge (formerly CloudWatch Events) to trigger the Lambda function based on a schedule or events that correspond to the completion of the data load process in Amazon Redshift."},{"upvote_count":"2","timestamp":"1733274060.0","comment_id":"1321607","content":"Selected Answer: D\nThe statement in B is inaccurate.\nYou don't 'use Amazon Redshift Data API to publish' event to EventBridge. Redshift Data API has no function to write to EventBridge. Instead, the statement should be \"Use EventBridge to monitor Data API events...\" Perhaps this is a typo.\n\nBut if I assume there are no typos in all the statements, then I would go for D. Although not a perfect solution, the cloud trail events have more info than the Redshift Data API events.","poster":"altonh"},{"poster":"taxo","timestamp":"1724049060.0","upvote_count":"1","content":"This job doesn’t need a real time check","comment_id":"1268439"},{"poster":"John2025","timestamp":"1719063600.0","comment_id":"1235417","upvote_count":"1","content":"Why not used SQS to keep API change in the Queue ?"},{"upvote_count":"1","comment_id":"1226983","poster":"pypelyncar","comments":[{"upvote_count":"1","comment_id":"1269237","content":"It seems that Redshift Data API could directly publishing events in EventBridge (see first comment). For monitoring the Redshift Data API, you could use both EventBridge (near-real-time) or CloudTrail (stored in S3): https://docs.aws.amazon.com/redshift/latest/mgmt/data-api-monitoring.html\nBut both services are related to \"Data API\" not Redshift database itself. So it is really tricky.","comments":[{"poster":"San_Juan","comment_id":"1269239","content":"So, you could use the Redshift table STV_LOAD_STATE,\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_STV_LOAD_STATE.html\nand running a \"select\" query on that table for getting status of tables (filtering by timestamp) and add the result to EventBridge, applying a rule on those events to invoke the lambda function. I guess that B is the most appropiate answer.","upvote_count":"1","timestamp":"1724137440.0"}],"poster":"San_Juan","timestamp":"1724137260.0"}],"content":"Im not 100% sure of B or C, this is a tricky question. The reason is due to either SQS or EventBridge has not direct connection natively speaking to Redshift Data API. There is no way to publish events by itself. So, this means either SQS / EventBridge eventually need a \"proxy\" (e.g lambda function) in order to publish events or process events to this 2 sources. In both services we need something to publish those events from Redshift. so Yes, we need a lamda function between Redshift Data API and (SQS|EB). so either B,C doesnt seem to be 100% right. I think this question its a good candidate to be \"Choose two options\" but none has 100% right. Both are valid considering that there is an adapter function between 2 solutions.","timestamp":"1717893000.0"}],"unix_timestamp":1705747680,"question_text":"A company loads transaction data for each day into Amazon Redshift tables at the end of each day. The company wants to have the ability to track which tables have been loaded and which tables still need to be loaded.\nA data engineer wants to store the load statuses of Redshift tables in an Amazon DynamoDB table. The data engineer creates an AWS Lambda function to publish the details of the load statuses to DynamoDB.\nHow should the data engineer invoke the Lambda function to write load statuses to the DynamoDB table?","question_id":46,"answer_ET":"B","isMC":true},{"id":"Iu0xZ0VfliO1qEqOGtO5","answer_description":"","unix_timestamp":1723637340,"exam_id":21,"choices":{"D":"Use Amazon EMR instead of AWS Glue to group the raw input files. Process the files in Amazon EMR. Load the files into the Amazon Redshift tables.","A":"Use AWS Lambda to group the raw input files into larger files. Write the larger files back to Amazon S3. Use AWS Glue to process the files. Load the files into the Amazon Redshift tables.","C":"Use the Amazon Redshift COPY command to move the raw input files from Amazon S3 directly into the Amazon Redshift tables. Process the files in Amazon Redshift.","B":"Use the AWS Glue dynamic frame file-grouping option to ingest the raw input files. Process the files. Load the files into the Amazon Redshift tables."},"answer_ET":"B","discussion":[{"content":"Selected Answer: B\nThe key requirement is to reduce processing time for millions of small JSON files stored in Amazon S3. The solution needs to address the inefficiencies caused by the large number of small files while leveraging the existing AWS Glue and Amazon Redshift setup.","comment_id":"1336642","upvote_count":"1","timestamp":"1736054700.0","poster":"minhhnh"},{"timestamp":"1723639800.0","upvote_count":"1","comment_id":"1265764","poster":"aragon_saa","content":"Selected Answer: B\nAnswer is B"},{"content":"Selected Answer: B\nOption B: Use the AWS Glue dynamic frame file-grouping option to ingest the raw input files. Process the files. Load the files into the Amazon Redshift tables.","comment_id":"1265701","timestamp":"1723637340.0","upvote_count":"1","poster":"matt200"}],"answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/145729-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"question_text":"A company receives test results from testing facilities that are located around the world. The company stores the test results in millions of 1 KB JSON files in an Amazon S3 bucket. A data engineer needs to process the files, convert them into Apache Parquet format, and load them into Amazon Redshift tables. The data engineer uses AWS Glue to process the files, AWS Step Functions to orchestrate the processes, and Amazon EventBridge to schedule jobs.\n\nThe company recently added more testing facilities. The time required to process files is increasing. The data engineer must reduce the data processing time.\n\nWhich solution will MOST reduce the data processing time?","topic":"1","isMC":true,"answers_community":["B (100%)"],"question_id":47,"question_images":[],"timestamp":"2024-08-14 14:09:00"},{"id":"0cMk3YyDdi2I9p7JQY7R","answer_description":"","isMC":true,"unix_timestamp":1722921120,"answer_ET":"D","choices":{"C":"YourEnvironmentName-DAGProcessing","B":"YourEnvironmentName-Scheduler","D":"YourEnvironmentName-Task","A":"YourEnvironmentName-WebServer"},"question_text":"A data engineer uses Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to run data pipelines in an AWS account.\n\nA workflow recently failed to run. The data engineer needs to use Apache Airflow logs to diagnose the failure of the workflow.\n\nWhich log type should the data engineer use to diagnose the cause of the failure?","timestamp":"2024-08-06 07:12:00","exam_id":21,"answer_images":[],"discussion":[{"upvote_count":"6","comment_id":"1264521","comments":[{"content":"Tasks within the workflow but the workflow failed to run from the question. It could be that workflow didn't start at all. I think C is most suitable","upvote_count":"1","poster":"demmybite","comment_id":"1269948","timestamp":"1724226660.0"}],"poster":"150b64e","timestamp":"1723446660.0","content":"Selected Answer: D\nhttps://pupuweb.com/amazon-dea-c01-which-apache-airflow-log-type-should-you-use-to-diagnose-workflow-failures-in-amazon-mwaa/\n\nWhen a workflow fails to run in Amazon MWAA, the task logs (YourEnvironmentName-Task) are the most relevant for diagnosing the issue. Task logs contain detailed information about the execution of individual tasks within the workflow, including any error messages or stack traces that can help pinpoint the cause of the failure."},{"timestamp":"1723526280.0","poster":"teo2157","comment_id":"1265000","upvote_count":"3","content":"Selected Answer: D\nAgree with D based on 150b64e comments"},{"comment_id":"1261445","content":"Selected Answer: C\nReference --> https://aws.amazon.com/managed-workflows-for-apache-airflow/","timestamp":"1722921120.0","upvote_count":"2","poster":"Shanmahi"}],"topic":"1","answer":"D","question_id":48,"url":"https://www.examtopics.com/discussions/amazon/view/145096-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_images":[],"answers_community":["D (82%)","C (18%)"]},{"id":"UvwwSX81mqcpmbINLquT","answer_images":[],"answers_community":["AC (45%)","AE (36%)","CE (18%)"],"question_id":49,"question_text":"A finance company uses Amazon Redshift as a data warehouse. The company stores the data in a shared Amazon S3 bucket. The company uses Amazon Redshift Spectrum to access the data that is stored in the S3 bucket. The data comes from certified third-party data providers. Each third-party data provider has unique connection details.\n\nTo comply with regulations, the company must ensure that none of the data is accessible from outside the company's AWS environment.\n\nWhich combination of steps should the company take to meet these requirements? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/147826-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-09-19 03:11:00","discussion":[{"comment_id":"1336874","upvote_count":"2","timestamp":"1736109900.0","content":"Selected Answer: CE\nA doesn't make sense considering the NAT gateway since that's usually used to facilitate traffic to the internet? Maybe if it was a S3 Gateway Endpoint it would make more sense but E makes sense if the configurations are correct?","poster":"BigMrT"},{"poster":"kailu","content":"Selected Answer: AE\nShouldn't it be E and not C? Federated Queries: This method allows Redshift to query data directly from external sources without needing to store the data in Amazon S3. By using federated queries, the company can query third-party data sources without moving data into S3, reducing the attack surface.\nGateway VPC Endpoint: A gateway VPC endpoint allows secure access to S3 from within the VPC without routing traffic over the public internet. This is crucial for maintaining compliance with regulations by ensuring that no data leaves the AWS environment.","timestamp":"1734809940.0","upvote_count":"4","comment_id":"1330177"},{"upvote_count":"2","content":"Selected Answer: AC\nWhy do we need NAT GW when we can have VPC GW or Interface Endpoints for S3 as well.","comment_id":"1327307","poster":"paali","timestamp":"1734349860.0"},{"content":"Selected Answer: AC\nNone of the answers satisfy the constraints. A C both dont address how s3 bucket will be accessed through a VPC.","timestamp":"1734073680.0","comment_id":"1326041","poster":"hk0308","upvote_count":"1"},{"timestamp":"1726708260.0","upvote_count":"2","comment_id":"1286078","content":"Selected Answer: AC\nA. Replace the existing Redshift cluster with a new Redshift cluster that is in a private subnet. Use an interface VPC endpoint to connect to the Redshift cluster. Use a NAT gateway to give Redshift access to the S3 bucket.\nC. Turn on enhanced VPC routing for the Amazon Redshift cluster. Set up an AWS Direct Connect connection and configure a connection between each data provider and the finance company’s VPC.","poster":"EJGisME"}],"isMC":true,"answer":"AC","answer_description":"","unix_timestamp":1726708260,"topic":"1","exam_id":21,"question_images":[],"answer_ET":"AC","choices":{"C":"Turn on enhanced VPC routing for the Amazon Redshift cluster. Set up an AWS Direct Connect connection and configure a connection between each data provider and the finance company’s VPC.","B":"Create an AWS CloudHSM hardware security module (HSM) for each data provider. Encrypt each data provider's data by using the corresponding HSM for each data provider.","A":"Replace the existing Redshift cluster with a new Redshift cluster that is in a private subnet. Use an interface VPC endpoint to connect to the Redshift cluster. Use a NAT gateway to give Redshift access to the S3 bucket.","D":"Define table constraints for the primary keys and the foreign keys.","E":"Use federated queries to access the data from each data provider. Do not upload the data to the S3 bucket. Perform the federated queries through a gateway VPC endpoint."}},{"id":"gZLQNKxvlJKrveq37BGa","answer_description":"","unix_timestamp":1725515220,"exam_id":21,"choices":{"B":"Use the zero-ETL integration between Amazon Aurora and Amazon Redshift to load new files into Amazon Redshift.","D":"Use S3 Event Notifications to invoke an AWS Lambda function that loads new files into Amazon Redshift.","C":"Use AWS Glue job bookmarks to extract, transform, and load (ETL) load new files into Amazon Redshift.","A":"Use the query editor v2 to schedule a COPY command to load new files into Amazon Redshift."},"answer_ET":"D","discussion":[{"poster":"italiancloud2025","upvote_count":"1","content":"Selected Answer: D\nA: No, Query Editor v2 isn't designed for event-driven near-real-time loads.\nB: No, the zero-ETL integration applies to Aurora, not to files arriving in S3.\nC: No, Glue job bookmarks are for batch ETL jobs, not near real-time processing.\nD: Sí, because using S3 Event Notifications to trigger a Lambda function enables near-real-time ingestion via COPY commands into Redshift.","timestamp":"1739916840.0","comment_id":"1358482"},{"timestamp":"1725515220.0","content":"Selected Answer: D\nSeems like the trigger on upload would be the fastest option","comment_id":"1278699","upvote_count":"3","poster":"dashapetr"}],"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/146986-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"question_text":"Files from multiple data sources arrive in an Amazon S3 bucket on a regular basis. A data engineer wants to ingest new files into Amazon Redshift in near real time when the new files arrive in the S3 bucket.\n\nWhich solution will meet these requirements?","isMC":true,"topic":"1","answers_community":["D (100%)"],"question_id":50,"question_images":[],"timestamp":"2024-09-05 07:47:00"}],"exam":{"isBeta":false,"name":"AWS Certified Data Engineer - Associate DEA-C01","provider":"Amazon","isImplemented":true,"isMCOnly":true,"numberOfQuestions":207,"lastUpdated":"11 Apr 2025","id":21},"currentPage":10},"__N_SSP":true}