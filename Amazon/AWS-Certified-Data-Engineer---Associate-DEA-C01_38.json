{"pageProps":{"questions":[{"id":"GkmZ6p9vxzHjbFdz4D2Z","question_id":186,"url":"https://www.examtopics.com/discussions/amazon/view/131471-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_description":"","answers_community":["B (100%)"],"answer":"B","discussion":[{"timestamp":"1705567080.0","comment_id":"1125636","upvote_count":"17","poster":"milofficial","content":"Selected Answer: B\nHaha they copied this from the old DA Specialty. It's B\n\nhttps://docs.aws.amazon.com/athena/latest/ug/user-created-workgroups.html"},{"content":"B. Create an Athena workgroup for each use case. Apply tags to the workgroup. Create an IAM policy that uses the tags to apply appropriate permissions to the workgroup.\n\nExplanation:\nAthena workgroups allow you to isolate and manage different workloads, users, and permissions. By creating a separate workgroup for each use case, you can control access to query history, manage permissions, and enforce resource usage limits independently for each workload. Applying tags to workgroups allows you to categorize and organize them based on the use case, which simplifies policy management.","upvote_count":"14","poster":"TonyStark0122","comment_id":"1137885","timestamp":"1706817900.0"},{"poster":"Scotty_Nguyen","upvote_count":"1","comment_id":"1409673","content":"Selected Answer: B\nB is correct","timestamp":"1742824320.0"},{"content":"Selected Answer: B\nB is correct.","poster":"Manohar24","upvote_count":"2","timestamp":"1720676820.0","comment_id":"1245917"},{"timestamp":"1715293140.0","content":"Selected Answer: B\nThe only other answer that's confusing is C But its not the one. Creating separate IAM roles for each use case and associating them with Athena would not provide the necessary isolation and access control for query processes and query history.","upvote_count":"2","comment_id":"1209086","poster":"k350Secops"},{"poster":"dev_vicente","content":"Selected Answer: B\nB is more granular","timestamp":"1711368060.0","comment_id":"1182440","upvote_count":"1"}],"exam_id":21,"isMC":true,"question_text":"A company uses Amazon Athena for one-time queries against data that is in Amazon S3. The company has several use cases. The company must implement permission controls to separate query processes and access to query history among users, teams, and applications that are in the same AWS account.\nWhich solution will meet these requirements?","answer_ET":"B","answer_images":[],"question_images":[],"choices":{"B":"Create an Athena workgroup for each use case. Apply tags to the workgroup. Create an IAM policy that uses the tags to apply appropriate permissions to the workgroup.","D":"Create an AWS Glue Data Catalog resource policy that grants permissions to appropriate individual IAM users for each use case. Apply the resource policy to the specific tables that Athena uses.","A":"Create an S3 bucket for each use case. Create an S3 bucket policy that grants permissions to appropriate individual IAM users. Apply the S3 bucket policy to the S3 bucket.","C":"Create an IAM role for each use case. Assign appropriate permissions to the role for each use case. Associate the role with Athena."},"timestamp":"2024-01-18 09:38:00","topic":"1","unix_timestamp":1705567080},{"id":"9RGcb52pOGtimcTYEV9O","question_text":"A data engineer needs to build an extract, transform, and load (ETL) job. The ETL job will process daily incoming .csv files that users upload to an Amazon S3 bucket. The size of each S3 object is less than 100 MB.\nWhich solution will meet these requirements MOST cost-effectively?","timestamp":"2024-02-02 14:34:00","question_id":187,"discussion":[{"comments":[{"timestamp":"1720680060.0","content":"thats true for the 1 DPU, but thats not good because the minimum DPU for PySpark Job is 1 DPU. But for Python Job the minimum DPU is 0.0625. So the Python job is way more cheaper for small dataset and quick ETL transformation","comment_id":"1245949","poster":"GustonMari","upvote_count":"4"}],"poster":"halogi","content":"Selected Answer: C\nAWS Glue Python Shell Job is billed $0.44 per DPU-Hour for each job\nAWS Glue PySpark is billed $0.29 per DPU-Hour for each job with flexible execution and $0.44 per DPU-Hour for each job with standard execution\nSource: https://aws.amazon.com/glue/pricing/","upvote_count":"10","comment_id":"1184529","timestamp":"1711596360.0"},{"upvote_count":"9","timestamp":"1708149780.0","content":"Selected Answer: D\nOption D: Write an AWS Glue Python shell job and use pandas to transform the data, is the most cost-effective solution for the described scenario.\n\nAWS Glue’s Python shell jobs are a good fit for smaller-scale ETL tasks, especially when dealing with .csv files that are less than 100 MB each. The use of pandas, a powerful and efficient data manipulation library in Python, makes it an ideal tool for processing and transforming these types of files. This approach avoids the overhead and additional costs associated with more complex solutions like Amazon EKS or EMR, which are generally more suited for larger-scale, more complex data processing tasks.\n\nGiven the requirements – processing daily incoming small-sized .csv files – this solution provides the necessary functionality with minimal resources, aligning well with the goal of cost-effectiveness.","poster":"atu1789","comment_id":"1152392"},{"content":"Selected Answer: D\nIt is important not to compare just the “price per DPU hour,” but to consider the total cost by factoring in overhead for job startup, minimum DPU count, execution time, and data volume. For a relatively lightweight workload—such as processing approximately 100 MB of CSV files on a daily basis—option (D), using an AWS Glue Python shell job, is the most cost-effective choice.","poster":"YUICH","comment_id":"1347367","timestamp":"1737977280.0","upvote_count":"2"},{"content":"Selected Answer: D\ngoing with D https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html","timestamp":"1721092380.0","comment_id":"1248620","upvote_count":"3","poster":"LR2023"},{"content":"Selected Answer: D\ngood candidate to be (2 options) for real, either spark and py have similar approaches. I would go with Pandas, although... 50/50.. it could be Spark. I hope not to find this question in the exam","comment_id":"1228736","timestamp":"1718151540.0","poster":"pypelyncar","upvote_count":"7"},{"content":"Selected Answer: C\nPySpark with Spark(Flexible Execution): $0.29/hr for 1 DPU\nPySpark with Spark(Standard Execution): $0.44/hr for 1 DPU\nPython Shell with Pandas: $0.44/hr for 1 DPU","poster":"VerRi","upvote_count":"3","comment_id":"1214850","timestamp":"1716286680.0"},{"comment_id":"1206732","poster":"cloudata","content":"Selected Answer: D\nPython Shell is cheaper and can handle small to medium tasks.\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html","upvote_count":"6","timestamp":"1714870200.0"},{"poster":"chakka90","content":"D. \nBecause the pyspark is still being the cheap you have to use minimum of 2 DPU. Which would increase the cost anyway so, i feel that d should be correct","comment_id":"1204707","upvote_count":"3","timestamp":"1714503780.0"},{"comment_id":"1203460","timestamp":"1714292340.0","content":"Selected Answer: D\nD.\n\nWhile AWS Glue PySpark jobs are scalable and suitable for large workloads, C may be overkill for processing small .csv files (less than 100 MB each). The overhead of using Apache Spark may not be cost-effective for this specific use case.","poster":"khchan123","upvote_count":"4"},{"upvote_count":"4","poster":"Leo87656789","comment_id":"1202991","content":"Selected Answer: D\nOption D:\n\nEven though the Python Shell Job is more expensive on a DPU-Hour basis, you can select the option \"1/16 DPU\" in the Job details for a Python Shell Job, which is definetly cheaper than a Pyspark job.","timestamp":"1714203120.0"},{"upvote_count":"6","timestamp":"1712086920.0","poster":"lucas_rfsb","content":"Selected Answer: C\nAWS Glue Python Shell Job is billed $0.44 per DPU-Hour for each job\nAWS Glue PySpark is billed $0.29 per DPU-Hour for each job with flexible execution and $0.44 per DPU-Hour for each job with standard execution\nSource: https://aws.amazon.com/glue/pricing/","comment_id":"1188279"},{"content":"Selected Answer: D\nhttps://medium.com/@navneetsamarth/reduce-aws-cost-using-glue-python-shell-jobs-70a955d4359f#:~:text=The%20cheapest%20Glue%20Spark%20ETL,1%2F16th%20of%20a%20DPU.&text=This%20can%20result%20in%20massive,just%20a%20better%20design%20overall!","poster":"[Removed]","comment_id":"1187209","upvote_count":"5","timestamp":"1711949700.0"},{"comment_id":"1177379","poster":"GiorgioGss","content":"Selected Answer: D\nD is more cheaper than C. Not so scalable but is cheaper...","timestamp":"1710856500.0","upvote_count":"4"},{"comment_id":"1138573","content":"Selected Answer: C\nAWS Glue is a fully managed ETL service, which means you don't need to manage infrastructure, and it automatically scales to handle your data processing needs. This reduces operational overhead and cost.\n\nPySpark, as a part of AWS Glue, is a powerful and widely-used framework for distributed data processing, and it's well-suited for handling data transformations on a large scale.","upvote_count":"5","timestamp":"1706880840.0","poster":"rralucard_"}],"exam_id":21,"answers_community":["D (65%)","C (35%)"],"unix_timestamp":1706880840,"url":"https://www.examtopics.com/discussions/amazon/view/132708-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_description":"","answer_images":[],"answer":"D","question_images":[],"isMC":true,"answer_ET":"D","topic":"1","choices":{"A":"Write a custom Python application. Host the application on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.","B":"Write a PySpark ETL script. Host the script on an Amazon EMR cluster.","C":"Write an AWS Glue PySpark job. Use Apache Spark to transform the data.","D":"Write an AWS Glue Python shell job. Use pandas to transform the data."}},{"id":"u5mWVT3watK8Ki7sFWun","answer_description":"","isMC":true,"choices":{"D":"ALTER TABLE Orders MODIFY PARTITION(order_date=’2023-01-01’) LOCATION ‘s3://transactions/orders/2023-01-01’;\nALTER TABLE Orders MODIFY PARTITION(order_date=’2023-01-02’) LOCATION ‘s3://transactions/orders/2023-01-02’;","C":"REPAIR TABLE Orders;","B":"MSCK REPAIR TABLE Orders;","A":"ALTER TABLE Orders ADD PARTITION(order_date=’2023-01-01’) LOCATION ‘s3://transactions/orders/order_date=2023-01-01’;\nALTER TABLE Orders ADD PARTITION(order_date=’2023-01-02’) LOCATION ‘s3://transactions/orders/order_date=2023-01-02’;"},"url":"https://www.examtopics.com/discussions/amazon/view/142527-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_images":[],"exam_id":21,"discussion":[{"upvote_count":"5","comment_id":"1243973","poster":"Ja13","timestamp":"1720377660.0","content":"Selected Answer: A\nWhy the Other Options Are Incorrect:\nOption B: MSCK REPAIR TABLE Orders: This command is used to repair the partitions of a table by scanning all the files in the specified location. This is not efficient if you know the specific partitions you want to add, as it will scan the entire table location.\nOption C: REPAIR TABLE Orders: This is not a valid Athena DDL command.\nOption D: ALTER TABLE Orders MODIFY PARTITION: This command is used to modify the location of existing partitions, not to add new partitions. It would not work for adding new partitions."},{"comment_id":"1230570","poster":"artworkad","content":"Selected Answer: A\nA is correct as per https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html","timestamp":"1718383200.0","upvote_count":"4"},{"comment_id":"1230567","poster":"artworkad","content":"A is correct as per https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html","timestamp":"1718383080.0","upvote_count":"1"},{"content":"Selected Answer: A\nA is correct because it uses the appropriate DDL statements to add the new partitions directly without scanning all folders and files, meeting the requirements stated in the question.\nB is incorrect because while it would update the partitions, it would involve scanning all files and folders.\nC is incorrect because REPAIR TABLE is not a valid command.\nD is incorrect because it modifies partitions instead of adding new ones.","timestamp":"1718367060.0","comment_id":"1230471","upvote_count":"4","poster":"tgv"}],"answer":"A","question_id":188,"question_images":[],"unix_timestamp":1718367060,"answer_ET":"A","question_text":"A data engineer creates an AWS Glue Data Catalog table by using an AWS Glue crawler that is named Orders. The data engineer wants to add the following new partitions:\n\ns3://transactions/orders/order_date=2023-01-01\ns3://transactions/orders/order_date=2023-01-02\n\nThe data engineer must edit the metadata to include the new partitions in the table without scanning all the folders and files in the location of the table.\n\nWhich data definition language (DDL) statement should the data engineer use in Amazon Athena?","topic":"1","timestamp":"2024-06-14 14:11:00","answers_community":["A (100%)"]},{"id":"JN0MELepQLjJs6tvi69T","choices":{"C":"Apache Parquet format compressed with Snappy","A":".csv format compressed with zip","B":"JSON format compressed with bzip2","D":"Apache Avro format compressed with LZO"},"answer_ET":"C","answer_images":[],"unix_timestamp":1718375100,"answers_community":["C (100%)"],"exam_id":21,"question_id":189,"isMC":true,"question_images":[],"timestamp":"2024-06-14 16:25:00","answer_description":"","question_text":"A company stores 10 to 15 TB of uncompressed .csv files in Amazon S3. The company is evaluating Amazon Athena as a one-time query engine.\n\nThe company wants to transform the data to optimize query runtime and storage costs.\n\nWhich file format and compression solution will meet these requirements for Athena queries?","answer":"C","discussion":[{"poster":"tgv","timestamp":"1718375100.0","comment_id":"1230532","upvote_count":"6","content":"Selected Answer: C\nParquet provides efficient columnar storage, enabling Athena to read only the necessary data for queries, which reduces scan times and speeds up query performance.\nSnappy compression offers a good balance between compression speed and efficiency, reducing storage costs without significantly impacting query times."},{"upvote_count":"3","timestamp":"1718383200.0","poster":"artworkad","comment_id":"1230569","content":"Selected Answer: C\nParquet + Snappy"}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/142529-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"},{"id":"gDxEtvdwCnNwQ96GwkvW","answers_community":["C (100%)"],"question_images":[],"discussion":[{"content":"Selected Answer: C\nThe solution that will meet these requirements with the least amount of refactoring is Option C: Migrate the existing Airflow orchestration configuration into Amazon Managed Workflows for Apache Airflow (Amazon MWAA). Create the data quality checks during the ingestion to validate the data quality by using SQL tasks in Airflow.\n\nAmazon Managed Workflows for Apache Airflow (MWAA) is a fully managed service that makes it easy to run open-source versions of Apache Airflow on AWS. It allows you to build workflows to design and visualize pipelines, automate complex tasks, and monitor executions. Since the company is already using Apache Airflow for orchestration, migrating to Amazon MWAA would require minimal refactoring.","comment_id":"1240682","timestamp":"1719916320.0","poster":"bakarys","upvote_count":"3"},{"timestamp":"1718900040.0","upvote_count":"3","poster":"HunkyBunky","comment_id":"1233780","content":"Selected Answer: C\nAmazon MWAA - becuase we already uses Apache Airflow"},{"timestamp":"1718435520.0","comment_id":"1230796","poster":"tgv","content":"Selected Answer: C\nAmazon MWAA is a managed service for running Apache Airflow. It allows migrating existing Airflow configurations with minimal changes. Data quality checks can continue to be implemented as SQL tasks in Airflow, similar to the current setup.","upvote_count":"3"}],"choices":{"A":"Setup AWS Outposts in the AWS Region that is nearest to the location where the company uses Airflow. Migrate the servers into Outposts hosted Amazon EC2 instances. Update the pipelines to interact with the Outposts hosted EC2 instances instead of the on-premises pipelines.","D":"Convert the pipelines to AWS Step Functions workflows. Recreate the data quality checks in SQL as Python based AWS Lambda functions.","C":"Migrate the existing Airflow orchestration configuration into Amazon Managed Workflows for Apache Airflow (Amazon MWAA). Create the data quality checks during the ingestion to validate the data quality by using SQL tasks in Airflow.","B":"Create a custom Amazon Machine Image (AMI) that contains the Airflow application and the code that the company needs to migrate. Use the custom AMI to deploy Amazon EC2 instances. Update the network connections to interact with the newly deployed EC2 instances."},"topic":"1","timestamp":"2024-06-15 09:12:00","answer":"C","exam_id":21,"isMC":true,"unix_timestamp":1718435520,"answer_images":[],"answer_description":"","question_id":190,"answer_ET":"C","question_text":"A company uses Apache Airflow to orchestrate the company's current on-premises data pipelines. The company runs SQL data quality check tasks as part of the pipelines. The company wants to migrate the pipelines to AWS and to use AWS managed services.\n\nWhich solution will meet these requirements with the LEAST amount of refactoring?","url":"https://www.examtopics.com/discussions/amazon/view/142558-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/"}],"exam":{"provider":"Amazon","isMCOnly":true,"numberOfQuestions":207,"name":"AWS Certified Data Engineer - Associate DEA-C01","id":21,"isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025"},"currentPage":38},"__N_SSP":true}