{"pageProps":{"questions":[{"id":"mQ6S9mlmOUDIGArpKbGx","exam_id":31,"answer_images":[],"answers_community":["C (100%)"],"answer":"C","topic":"1","choices":{"A":"Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.","B":"Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.","D":"Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.","C":"Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment."},"timestamp":"2023-02-17 15:33:00","question_id":241,"answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/99653-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"timestamp":"1692275580.0","poster":"bdp123","upvote_count":"22","content":"Selected Answer: C\nA subnet must reside within a single Availability Zone.\nhttps://aws.amazon.com/vpc/faqs/#:~:text=Can%20a%20subnet%20span%20Availability,within%20a%20single%20Availability%20Zone.","comment_id":"811982"},{"upvote_count":"7","poster":"zjcorpuz","timestamp":"1706625420.0","comment_id":"967103","content":"a subnet only resides on a one AZ, it does not span to another AZ."},{"comment_id":"1141222","content":"Selected Answer: C\nA subnet can't \"Extend\" across multiple AZs: B,D out\nHA = RDS Multi-AZ: A out\n\nC","timestamp":"1722864420.0","poster":"LoXoL","upvote_count":"4"},{"comment_id":"1137750","poster":"thewalker","upvote_count":"3","timestamp":"1722520680.0","content":"Selected Answer: C\nAn Auto Scaling group can span across two Availability Zones, where one subnet is created in each AZ.\n\nWhen creating an Auto Scaling group, you need to specify at least one subnet. You can add additional subnets later on, including subnets across multiple AZs.\n\nAuto Scaling will distribute instances evenly across the specified subnets to maintain availability and optimize performance. If one AZ becomes unavailable, instances can be launched in the other AZ.\n\nThe associated load balancer should also span the same subnets/AZs as the Auto Scaling group. This allows traffic to be routed to instances in different subnets and AZs, increasing fault tolerance of the application."},{"timestamp":"1712210580.0","upvote_count":"2","comment_id":"1024505","content":"Selected Answer: C\nProvision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment","poster":"TariqKipkemei"},{"upvote_count":"3","poster":"Guru4Cloud","timestamp":"1709664060.0","content":"Selected Answer: C\nThis solution will ensure that the EC2 instances and the DB instance are not located in the same Availability Zone, which will improve the availability of the application.","comment_id":"999712"},{"comment_id":"929171","upvote_count":"3","timestamp":"1703152200.0","content":"Selected Answer: C\nD is completely wrong, because each subnet must reside entirely within one Availability Zone and cannot span zones. By launching AWS resources in separate Availability Zones, you can protect your applications from the failure of a single Availability Zone.","poster":"MrAWSAssociate"},{"comment_id":"922467","upvote_count":"2","timestamp":"1702501380.0","content":"The key word here was extend.","poster":"Anmol_1010"},{"comment_id":"843212","upvote_count":"3","content":"This discards B and D: Subnet basics. Each subnet must reside entirely within one Availability Zone and cannot span zones. By launching AWS resources in separate Availability Zones, you can protect your applications from the failure of a single Availability Zone","poster":"GalileoEC2","timestamp":"1695069060.0"},{"timestamp":"1693681980.0","upvote_count":"2","comment_id":"827447","poster":"Steve_4542636","content":"Selected Answer: C\na subnet is per AZ. a scaling group can span multiple AZs. https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html"},{"timestamp":"1692688920.0","poster":"KZM","upvote_count":"3","comments":[{"upvote_count":"1","comment_id":"817658","timestamp":"1692689040.0","poster":"KZM","comments":[{"comment_id":"817665","upvote_count":"2","poster":"KZM","content":"Sorry I think C is correct.","timestamp":"1692689220.0"},{"content":"Nope. The answer is indeed C. \nYou cannot span like that. Check the link below:\n\"Each subnet must reside entirely within one Availability Zone and cannot span zones.\"\nhttps://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html","comments":[{"upvote_count":"3","content":"Thanks, Leoattf for the link you shared.","poster":"KZM","timestamp":"1693114860.0","comment_id":"823348"}],"comment_id":"818398","timestamp":"1692730440.0","poster":"leoattf","upvote_count":"4"}],"content":"Can span like that?"}],"comment_id":"817655","content":"I think D.\nSpan the single subnet in both Availability Zones can access the DB instances in either zone without going over the public internet."},{"poster":"Babba","content":"Selected Answer: C\nit's C","timestamp":"1692553800.0","upvote_count":"2","comment_id":"815757"}],"isMC":true,"unix_timestamp":1676644380,"answer_description":"","question_text":"A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance.\n\nThe design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone.\n\nWhich solution will make the application highly available?","question_images":[]},{"id":"oTLml76tZGEcQaGlq5ZX","unix_timestamp":1676651220,"question_id":242,"discussion":[{"upvote_count":"20","comment_id":"812121","timestamp":"1692282420.0","content":"Selected Answer: B\nKeyword here is a minimum throughput of 6 GBps. Only the FSx for Lustre with SSD option gives the sub-milli response and throughput of 6 GBps or more.\nB. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.\n\nRefrences:\nhttps://aws.amazon.com/fsx/when-to-choose-fsx/","poster":"Bhawesh"},{"timestamp":"1692292440.0","poster":"bdp123","content":"Selected Answer: B\nCreate an Amazon S3 bucket to store the raw data Create an Amazon FSx for Lustre file system that\nuses persistent SSD storage Select the option to import data from and export data to Amazon S3\nMount the file system on the EC2 instances. Amazon FSx for Lustre uses SSD storage for submillisecond latencies and up to 6 GBps throughput, and can import data from and export data to\nAmazon S3. Additionally, the option to select persistent SSD storage will ensure that the data is stored on the disk and not lost if the file system is stopped.","comment_id":"812261","upvote_count":"8"},{"comment_id":"1160529","content":"Answer is B : FSx for Lustre with SSD option gives the sub-milli response and throughput of 6 GBps or more","timestamp":"1724752440.0","upvote_count":"3","poster":"MrPCarrot"},{"comment_id":"1150395","content":"I dont even think that NetApp comes for Linux","poster":"Pangian","upvote_count":"1","timestamp":"1723651860.0"},{"content":"Selected Answer: B\nAmazon FSx for Lustre for compute-intensive workloads.\n - allows file-based applications to access data with hundreds of gigabytes per second of data, millions of IOPS, and sub millisecond latencies.\n - supports file access to thousands of EC2 instances\n\nand well SSD always wins ;)","timestamp":"1720311840.0","comment_id":"1115560","poster":"djgodzilla","upvote_count":"3"},{"content":"Selected Answer: B\nsub-millisecondes == Lustre\nHDD vs SSD == for performance use SSD","upvote_count":"4","poster":"Mikado211","comment_id":"1093783","timestamp":"1718127540.0"},{"comment_id":"999709","timestamp":"1709663640.0","upvote_count":"3","content":"Selected Answer: B\nAmazon FSx for Lustre with SSD: Amazon FSx for Lustre is designed for high-performance, parallel file processing workloads. Choosing SSD storage ensures fast I/O and meets the sub-millisecond latency requirement.","poster":"Guru4Cloud"},{"timestamp":"1709174640.0","content":"Voto por la B","comment_id":"993610","upvote_count":"2","poster":"rolervengador"},{"timestamp":"1703786460.0","poster":"Gooniegoogoo","content":"So many of these are wrong, its good we have people that vote so we can get to the right answer!!","upvote_count":"2","comment_id":"936865"},{"comment_id":"884552","content":"Selected Answer: B\n• Amazon FSx for Lustre with SSD storage can provide up to 260 GB/s of aggregate throughput and sub-millisecond latencies needed for this workload.\n• Persistent SSD storage ensures data durability in the file system. Data is also exported to S3 for backup storage. \n• The file system will import the initial 8 TB of raw data from S3, providing a fast storage tier for processing while retaining the data in S3.\n• The file system is mounted to the EC2 compute instances to distribute processing.\n• FSx for Lustre is optimized for high-performance computing workloads running on Linux, matching the EC2 environment.","poster":"kruasan","upvote_count":"3","timestamp":"1698606540.0","comments":[{"upvote_count":"2","timestamp":"1698606540.0","poster":"kruasan","comment_id":"884553","content":"Option A - FSx for NetApp ONTAP with ALL tiering policy would not provide fast enough storage tier for sub-millisecond latency. HDD tiers have higher latency.\nOption C - FSx for Lustre with HDD storage would not provide the throughput, IOPS or low latency needed.\nOption D - FSx for NetApp ONTAP with NONE tiering policy would require much more expensive SSD storage to meet requirements, increasing cost."}]},{"upvote_count":"2","comment_id":"827448","content":"Selected Answer: B\nI vote B","poster":"Steve_4542636","timestamp":"1693682100.0"},{"poster":"AlmeroSenior","content":"Selected Answer: B\nFSX Lusture is 1000mbs per TB provisioned and we have 8TBs so gives us 8GBs . The netapp FSX appears a hard limit of 4gbs . \n\nhttps://aws.amazon.com/fsx/lustre/faqs/?nc=sn&loc=5\nhttps://aws.amazon.com/fsx/netapp-ontap/faqs/","timestamp":"1692673680.0","comment_id":"817480","upvote_count":"6"},{"content":"Selected Answer: B\nB is the best choice as it utilizes Amazon S3 for data storage, which is cost-effective and durable, and Amazon FSx for Lustre for high-performance file storage, which provides the required sub-millisecond latencies and minimum throughput of 6 GBps. Additionally, the option to import and export data to and from Amazon S3 makes it easier to manage and move data between the two services.\nB is the best option as it meets the performance requirements for sub-millisecond latencies and a minimum throughput of 6 GBps.","timestamp":"1692636840.0","upvote_count":"2","comment_id":"817015","poster":"LuckyAro"},{"poster":"everfly","comment_id":"815055","content":"Selected Answer: B\nAmazon FSx for Lustre provides fully managed shared storage with the scalability and performance of the popular Lustre file system. It can deliver sub-millisecond latencies and hundreds of gigabytes per second of throughput.","timestamp":"1692517020.0","upvote_count":"4"}],"choices":{"B":"Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.","A":"Create an Amazon FSx for NetApp ONTAP file system. Sat each volume’ tiering policy to ALL. Import the raw data into the file system. Mount the fila system on the EC2 instances.","C":"Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.","D":"Create an Amazon FSx for NetApp ONTAP file system. Set each volume’s tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances."},"answer_description":"","exam_id":31,"answers_community":["B (100%)"],"question_text":"A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data.\n\nWhich solution will meet the performance requirements?","answer_ET":"B","isMC":true,"question_images":[],"topic":"1","timestamp":"2023-02-17 17:27:00","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/99676-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"B"},{"id":"hMEiLyEiel7moow0beOp","answers_community":["A (97%)","3%"],"url":"https://www.examtopics.com/discussions/amazon/view/84838-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","discussion":[{"timestamp":"1665389100.0","comments":[{"comment_id":"692567","poster":"BoboChow","timestamp":"1665541500.0","upvote_count":"21","content":"the condition key aws:PrincipalOrgID can prevent the members who don't belong to your organization to access the resource"}],"poster":"ude","content":"Selected Answer: A\naws:PrincipalOrgID Validates if the principal accessing the resource belongs to an account in your organization.","upvote_count":"78","comment_id":"690855"},{"poster":"Naneyerocky","comment_id":"711246","timestamp":"1726903080.0","content":"Selected Answer: A\nCondition keys: AWS provides condition keys that you can query to provide more granular control over certain actions. \nThe following condition keys are especially useful with AWS Organizations:\n\naws:PrincipalOrgID – Simplifies specifying the Principal element in a resource-based policy. This global key provides an alternative to listing all the account IDs for all AWS accounts in an organization. Instead of listing all of the accounts that are members of an organization, you can specify the organization ID in the Condition element.\n\naws:PrincipalOrgPaths – Use this condition key to match members of a specific organization root, an OU, or its children. The aws:PrincipalOrgPaths condition key returns true when the principal (root user, IAM user, or role) making the request is in the specified organization path. A path is a text representation of the structure of an AWS Organizations entity.","upvote_count":"26","comments":[{"comment_id":"978292","timestamp":"1691731680.0","content":"are we not choosing ou because the least overhead term was use? option B also seems correct","comments":[{"poster":"EMPERBACH","upvote_count":"3","comment_id":"1195298","content":"As there are many OU, you need more effort to list down OU path. And question mention about least management overhead to allow users in Organization, not single OU.","timestamp":"1713073800.0"},{"content":"Exactly","upvote_count":"1","poster":"BlackMamba_4","comment_id":"989417","timestamp":"1692903240.0"}],"poster":"Sleepy_Lazy_Coder","upvote_count":"4"}]},{"upvote_count":"1","timestamp":"1743371820.0","comment_id":"1413834","poster":"Wylla","content":"Selected Answer: A\naws:PrincipalOrgID - global key provides an alternative to listing all the account IDs for all AWS accounts in an organization."},{"comment_id":"1366126","timestamp":"1741316220.0","content":"Selected Answer: A\nNot B because: aws:PrincipalOrgPaths require you change the OU path when the user switch to another OU\nNot C because: you need to put effort to update policy which introduces operational overhead.\nNot D because: require a good tag management across multiple accounts. Like team \"HR\", team \"QA\", team \"Security\" and so forth. Also need to update if the user is moved to another team with different tag.","poster":"francisizme","upvote_count":"2"},{"poster":"Mrigraj12","comment_id":"1345874","upvote_count":"1","content":"Selected Answer: A\nPrincipalOrgID is used to validate that the iam user accesing data is from the organisation only","timestamp":"1737696360.0"},{"poster":"MGKYAING","upvote_count":"1","comment_id":"1331606","timestamp":"1735138860.0","content":"Selected Answer: A\naws:PrincipalOrgID is the most efficient and straightforward way to restrict access to resources to entities within an AWS Organization.\nIt reduces the need for constant monitoring, tagging, or OU management, making it the optimal solution for scenarios requiring minimal operational overhead."},{"timestamp":"1726903140.0","content":"Selected Answer: A\nuse a new condition key, aws:PrincipalOrgID, in these policies to require all principals accessing the resource to be from an account (including the master account) in the organization. For example, let’s say you have an Amazon S3 bucket policy and you want to restrict access to only principals from AWS accounts inside of your organization. To accomplish this, you can define the aws:PrincipalOrgID condition and set the value to your organization ID in the bucket policy. Your organization ID is what sets the access control on the S3 bucket. Additionally, when you use this condition, policy permissions apply when you add new accounts to this organization without requiring an update to the policy.","comment_id":"749519","poster":"psr83","upvote_count":"2"},{"comment_id":"750250","content":"Selected Answer: A\nAnswered by ChatGPT with an explanation.\n\nThe correct solution that meets these requirements with the least amount of operational overhead is Option A: Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.\n\nOption A involves adding the aws:PrincipalOrgID global condition key to the S3 bucket policy, which allows you to specify the organization ID of the accounts that you want to grant access to the bucket. By adding this condition to the policy, you can limit access to the bucket to only users of accounts within the organization.","timestamp":"1726903140.0","comments":[{"comment_id":"750251","timestamp":"1671485040.0","content":"Option B involves creating organizational units (OUs) for each department and adding the aws:PrincipalOrgPaths global condition key to the S3 bucket policy. This option would require more operational overhead, as it involves creating and managing OUs for each department.\n\nOption C involves using AWS CloudTrail to monitor certain events and updating the S3 bucket policy accordingly. While this option could potentially work, it would require ongoing monitoring and updates to the policy, which could increase operational overhead.","upvote_count":"3","comments":[{"content":"Option D involves tagging each user that needs access to the S3 bucket and adding the aws:PrincipalTag global condition key to the S3 bucket policy. This option would require you to tag each user, which could be time-consuming and could increase operational overhead.\n\nOverall, Option A is the most straightforward and least operationally complex solution for limiting access to the S3 bucket to only users of accounts within the organization.","timestamp":"1671485100.0","upvote_count":"1","poster":"Buruguduystunstugudunstuy","comment_id":"750252"}],"poster":"Buruguduystunstugudunstuy"}],"poster":"Buruguduystunstugudunstuy","upvote_count":"5"},{"upvote_count":"1","timestamp":"1726903140.0","poster":"SilentMilli","comment_id":"767811","content":"Selected Answer: A\nThis is the least operationally overhead solution because it requires only a single configuration change to the S3 bucket policy, which will allow access to the bucket for all users within the organization. The other options require ongoing management and maintenance. Option B requires the creation and maintenance of organizational units for each department. Option C requires monitoring of specific CloudTrail events and updates to the S3 bucket policy based on those events. Option D requires the creation and maintenance of tags for each user that needs access to the bucket."},{"content":"Selected Answer: A\nOption A proposes adding the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy. This would limit access to the S3 bucket to only users of accounts within the organization in AWS Organizations, as the aws PrincipalOrgID condition key can check if the request is coming from within the organization.","timestamp":"1726903140.0","poster":"linux_admin","comment_id":"855972","upvote_count":"2"},{"upvote_count":"1","comment_id":"849473","content":"B. Create an organizational unit (OU) for each department. Add the AWS: Principal Org Paths global condition key to the S3 bucket policy. This solution allows for the S3 bucket to only be accessed by users within the organization in AWS Organizations while minimizing operational overhead by organizing users into OUs and using a single global condition key in the bucket policy. Option A, adding the Principal ID global condition key, would require frequent updates to the policy as new users are added or removed from the organization. Option C, using CloudTrail to monitor events, would require manual updating of the policy based on the events. Option D, tagging each user, would also require manual tagging updates and may not be scalable for larger organizations with many users.","timestamp":"1726903140.0","poster":"martin451"},{"comment_id":"860456","timestamp":"1726903140.0","poster":"PhucVuu","content":"Selected Answer: A\nKeywords: \n- Company uses AWS Organizations\n- Limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations\n- LEAST amount of operational overhead\nA: Correct - We just add PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy\nB: Incorrect - We can limit access by this way but this will take more amount of operational overhead\nC: Incorrect - AWS CloudTrail only log API events, we can not prevent user access to S3 bucket. For update S3 bucket policy to make it work you should manually add each account -> this way will not be cover in case of new user is added to Organization.\nD: Incorrect - We can limit access by this way but this will take most amount of operational overhead","upvote_count":"12"},{"content":"Selected Answer: A\nOption A, which suggests adding the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy, is a valid solution to limit access to the S3 bucket to users within the organization in AWS Organizations. It can effectively achieve the desired access control.\n\nIt restricts access to the S3 bucket based on the organization ID, ensuring that only users within the organization can access the bucket. This method is suitable if you want to restrict access at the organization level rather than individual departments or organizational units.\n\nThe operational overhead for Option A is also relatively low since it involves adding a global condition key to the S3 bucket policy. However, it is important to note that the organization ID must be accurately configured in the bucket policy to ensure the desired access control is enforced.\n\nIn summary, Option A is a valid solution with minimal operational overhead that can limit access to the S3 bucket to users within the organization using the aws PrincipalOrgID global condition key.","upvote_count":"1","comment_id":"926110","timestamp":"1726903140.0","poster":"cookieMr"},{"content":"AWS Identity and Access Management (IAM) now makes it easier for you to control access to your AWS resources by using the AWS organization of IAM principals (users and roles). For some services, you grant permissions using resource-based policies to specify the accounts and principals that can access the resource and what actions they can perform on it. Now, you can use a new condition key, aws:PrincipalOrgID, in these policies to require all principals accessing the resource to be from an account (including the master account) in the organization.","upvote_count":"4","comment_id":"1073144","timestamp":"1726903080.0","poster":"Ruffyit"},{"timestamp":"1726903080.0","content":"Selected Answer: A\nA. Correct answer. Bucket policy controls who can access to S3 and their objects. If we refer in the bucket policy to the organization, we can limit who can access inside that organization.\nB. Despite this option is correct, it is unnecessarily complex. We don’t need to separate the AWS Organization users for the requirements imposed in the question. So, it only aggregates more operational overhead.\nC. Using CloudTrail for controlling the S3 access permissions is not suitable and require so many events to be monitored. Additionally, it only registers the logs, so CloudTrail cannot impose restrictions over the accounts that access to S3. \nD. Tagging each user is not an scalable or efficient solution since you need to tag every user in the infrastructure, which is probably not static. Additionally, it makes unnecessary verbose the S3 bucket policy associated to that bucket.","comment_id":"1116812","upvote_count":"7","poster":"Andreshere"},{"poster":"PaulGa","timestamp":"1723277580.0","comment_id":"1263378","upvote_count":"1","content":"Ans A: LEAST amount of organisational overhead: instead of listing all accounts which are members of organisation, instead specify the orgn. ID in the Condition element"},{"upvote_count":"1","poster":"ChymKuBoy","content":"Selected Answer: A\nA for sure","comment_id":"1223973","timestamp":"1717481520.0"},{"poster":"Ishu_","comment_id":"1220901","content":"Selected Answer: A\nThe aws:PrincipalOrgID condition key allows you to restrict access based on the organization ID, ensuring that only principals (users, roles, etc.) from accounts within your AWS Organization can access the S3 bucket.","timestamp":"1716984480.0","upvote_count":"1"},{"content":"answer A is correct . Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.","timestamp":"1713346380.0","poster":"Muavia","comment_id":"1197125","upvote_count":"1"},{"upvote_count":"1","content":"EX: \"arn:aws:iam::094697565646:user/Steve\"\n Even if steve is added accidentally, he will not have access to financial data if he does not belong to the account in organization.As aws:PrincipalOrgID Validates if the principal accessing the resource belongs to an account in your organization.","timestamp":"1712405220.0","poster":"smalipeddi","comment_id":"1190378"},{"content":"Selected Answer: A\nPrincipalOrgID global condition is simples way to limit access\nBCD even if possible is too much work","timestamp":"1705170240.0","comment_id":"1121949","poster":"awsgeek75","upvote_count":"1"},{"content":"Selected Answer: A\nA is correct","comment_id":"1121601","poster":"A_jaa","timestamp":"1705148340.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1053679","content":"Answer: A","timestamp":"1698234120.0","poster":"Ruffyit"},{"content":"Selected Answer: A\nThis is the least operationally overhead solution because it does not require any additional infrastructure or configuration. AWS Organizations already tracks the organization ID of each account, so you can simply add the aws:PrincipalOrgID condition key to the S3 bucket policy and reference the organization ID. This will ensure that only users of accounts within the organization can access the S3 bucket","timestamp":"1689601080.0","comment_id":"954259","upvote_count":"5","poster":"Guru4Cloud"},{"upvote_count":"3","comment_id":"951953","poster":"james2033","content":"Selected Answer: A\nA","timestamp":"1689388080.0"},{"upvote_count":"1","timestamp":"1689176100.0","content":"Option A MET THE REQUIREMENT","poster":"miki111","comment_id":"949947"},{"poster":"karloscetina007","upvote_count":"1","comment_id":"925240","timestamp":"1686923880.0","content":"A is the correct answer."},{"timestamp":"1681192200.0","poster":"Musti35","content":"You can now use the aws:PrincipalOrgID condition key in your resource-based policies to more easily restrict access to IAM principals from accounts in your AWS organization. For more information about this global condition key and policy examples using aws:PrincipalOrgID, read the IAM documentation.","upvote_count":"1","comment_id":"866979"},{"poster":"iamRohanKaushik","timestamp":"1679373960.0","content":"Selected Answer: A\nAnswer is A.","comment_id":"845530","upvote_count":"1"},{"comment_id":"807018","timestamp":"1676258760.0","upvote_count":"1","content":"Selected Answer: A\nA is correct","poster":"buiducvu"},{"timestamp":"1671209280.0","upvote_count":"1","comment_id":"747427","poster":"NikaCZ","content":"Selected Answer: A\naws:PrincipalOrgID – Simplifies specifying the Principal element in a resource-based policy. This global key provides an alternative to listing all the account IDs for all AWS accounts in an organization."},{"comment_id":"747119","upvote_count":"1","timestamp":"1671191340.0","content":"Selected Answer: A\nI think that LEAST is the key. So A!","poster":"Myxa"},{"content":"Selected Answer: A\nA is the correct answer","poster":"9014","upvote_count":"1","comment_id":"732676","timestamp":"1669903260.0"},{"poster":"Wpcorgan","comment_id":"723459","upvote_count":"1","content":"A is correct","timestamp":"1669033200.0"},{"content":"Selected Answer: A\nA is correct","poster":"VTI_Training","comment_id":"714289","timestamp":"1667968080.0","upvote_count":"1"},{"comment_id":"713904","timestamp":"1667918460.0","content":"Selected Answer: A\n.... and it's A","poster":"Saiofy","upvote_count":"1"},{"poster":"pm2229","comment_id":"711924","timestamp":"1667670840.0","content":"It's A, IAM now makes it easier for you to control access to your AWS resources by using the AWS organization of IAM principals (users and roles). You can use the aws:PrincipalOrgID condition key in your resource-based policies to more easily restrict access to IAM principals from accounts in your AWS organization.","upvote_count":"1"},{"timestamp":"1666993800.0","content":"Selected Answer: A\nans is A. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.","comment_id":"706769","poster":"17Master","upvote_count":"1"},{"comment_id":"696440","upvote_count":"1","poster":"queen101","timestamp":"1665946140.0","content":"AAAAAAAAAA"},{"content":"Selected Answer: A\nA requires the LEAST effort","comment_id":"695607","poster":"123jhl0","upvote_count":"2","timestamp":"1665858480.0"},{"content":"Selected Answer: A\nAnswer is A.","timestamp":"1665792120.0","comment_id":"695057","poster":"bilel500","upvote_count":"1"},{"poster":"huiy","comment_id":"693573","content":"Selected Answer: A\nAgree A","timestamp":"1665633240.0","upvote_count":"1"},{"timestamp":"1665538200.0","poster":"josh_fan","upvote_count":"3","content":"Selected Answer: A\nNow, you can use a new condition key, aws:PrincipalOrgID, in these policies to require all principals accessing the resource to be from an account (including the master account) in the organization.","comment_id":"692549"},{"poster":"Rock08","content":"Selected Answer: B\nNow you can arrange your AWS accounts into groups called organizational units (OUs) and apply policies to OUs or directly to accounts. For example, you can organize your accounts by application, environment, team, or any other grouping that makes sense for your business.","timestamp":"1665314880.0","upvote_count":"5","comments":[{"timestamp":"1667206620.0","content":"But the goal here is to limit access just for the users of the account, not groups or departments. That's why A is correct.","comment_id":"708301","poster":"ArielSchivo","upvote_count":"3"},{"upvote_count":"3","timestamp":"1667842440.0","poster":"thangvu1890","content":"It's ok. But It's not the LEAST amount of operational overhead.","comments":[{"timestamp":"1667885160.0","poster":"debillion","upvote_count":"1","comment_id":"713501","content":"Yeah! I thought as much."}],"comment_id":"713207"}],"comment_id":"690108"}],"answer_images":[],"isMC":true,"timestamp":"2022-10-09 13:28:00","choices":{"D":"Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.","B":"Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.","C":"Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.","A":"Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy."},"unix_timestamp":1665314880,"exam_id":31,"answer":"A","topic":"1","question_images":[],"answer_ET":"A","question_text":"A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.\nWhich solution meets these requirements with the LEAST amount of operational overhead?","question_id":243},{"id":"ANIe5YxTVmjnoUG6ZMQ0","answer_images":[],"question_text":"A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance.\nWhich solution meets these requirements MOST cost-effectively?","discussion":[{"timestamp":"1665411900.0","upvote_count":"41","comment_id":"691245","content":"Selected Answer: C\nAnswer C, you still pay for storage when an RDS database is stopped","poster":"hanhdroid"},{"content":"Selected Answer: C\nC - Create a manual Snapshot of DB and shift to S3- Standard and Restore form Manual Snapshot when required.\n\nNot A - By stopping the DB although you are not paying for DB hours you are still paying for Provisioned IOPs , the storage for Stopped DB is more than Snapshot of underlying EBS vol. and Automated Back ups .\nNot D - Is possible but not MOST cost effective, no need to run the RDS when not needed.","poster":"KVK16","timestamp":"1665811680.0","comment_id":"695208","upvote_count":"14"},{"content":"Selected Answer: B\nB is right i think","upvote_count":"1","comment_id":"1410993","timestamp":"1743102720.0","poster":"Kazmin"},{"comment_id":"1332013","content":"Selected Answer: C\nAn RDS instance can be stopped for up to seven days so it cannot be A. \n\"The instance stops running, up to a maximum of 7 consecutive days.\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html","poster":"John54321","timestamp":"1735232700.0","upvote_count":"3"},{"upvote_count":"1","content":"Selected Answer: A\nLa opción A es la más rentable porque detiene la instancia de base de datos cuando no se está utilizando, ahorrando los costos de cómputo, y luego se reinicia cuando sea necesario, sin comprometer el rendimiento durante las pruebas.","comment_id":"1328319","poster":"f51a8bd","timestamp":"1734506820.0"},{"content":"Selected Answer: C\nThe correct answer is:\n\nC. Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.\nExplanation:\nThis solution is the most cost-effective because it allows the development team to:\n\nSave Costs by Terminating the Instance:\n\nWhen the DB instance is not in use (28 out of 30 days), terminating it eliminates the costs associated with running a live instance.\nYou only pay for storage costs associated with the snapshot, which is significantly lower than the cost of keeping a running instance.\nRestoring a Snapshot:\n\nRestoring from a snapshot is straightforward and brings the database back with the same data and configuration as before.\nIt’s a good practice when dealing with periodic, resource-intensive tasks like the monthly tests.","comment_id":"1302299","timestamp":"1729744560.0","upvote_count":"2","poster":"mzeynalli"},{"upvote_count":"2","timestamp":"1723663020.0","comment_id":"1265944","poster":"PaulGa","content":"Selected Answer: C\nAns C – take Snapshots and restore them, because otherwise you're still paying for RDS storage."},{"timestamp":"1716508020.0","comment_id":"1217109","upvote_count":"6","content":"Selected Answer: C\nC. This option allows you to save on costs by only paying for storage of the snapshot when the DB instance is terminated. When needed again, you can restore the DB instance from the snapshot, which is a cost-effective way to handle infrequent but resource-intensive tasks.","poster":"OBIOHAnze"},{"comment_id":"1201214","upvote_count":"2","timestamp":"1713948720.0","poster":"ManikRoy","content":"Selected Answer: C\nMost cost effective is to create a snapshot and get rid of the DB instance after testing. \nNote that A is not correct option as While your database instance is stopped, you are charged for provisioned storage, manual snapshots and automated backup storage within your specified retention window, but not for database instance hours."},{"upvote_count":"4","content":"Selected Answer: A\nA Since the tests only run once a month for 48 hours, this approach minimizes costs while still retaining the same compute and memory attributes when the instance is restarted.\nwhen Snapshot resorte i new config","comment_id":"1198969","timestamp":"1713592800.0","poster":"AWSCLOUDLMD"},{"comment_id":"1168325","timestamp":"1709841720.0","poster":"vi24","upvote_count":"2","content":"My question is: isn't this DB collecting new data during the testing period ( 48 hrs.) ? after the snapshot is taken ? stop and restore db from the snapshot is the most cost effective but I think some data might be lost in between, so wouldn't be feasible !"},{"upvote_count":"5","comments":[{"timestamp":"1736134560.0","upvote_count":"1","content":"Based on the AWS RDS documentation, an RDS instance can only be stopped for a maximum of 7 days. After 7 days, the instance is automatically started by AWS to ensure data integrity and maintenance.","comment_id":"1336950","poster":"FlyingHawk"}],"poster":"VanDacker","comment_id":"1153037","timestamp":"1708231680.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html"},{"poster":"lsomas","upvote_count":"4","comment_id":"1150777","timestamp":"1707978300.0","content":"Answer C because,\nYou can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started so that it doesn't fall behind any required maintenance updates.\n\nSo, the \"auto starting\" behaviour is expected.\n\nIf you rarely use the database, BEST option is to Snapshot and Delete the database. Then, when you need it again, you could launch a new database from the Snapshot.\n\nAmazon RDS is not intended to be stopped for long periods."},{"timestamp":"1705185180.0","content":"Selected Answer: C\nDB is used one a month for 48 hours only so there is no point in keeping it up for rest of the month.\nB: more cost\nD: not allowed to reduce computing power\nA: It will work but C is much cheaper as instance is not only stopped but terminated.","comment_id":"1122086","poster":"awsgeek75","upvote_count":"2"},{"content":"Selected Answer: C\nAnswer-C","comment_id":"1121644","upvote_count":"1","poster":"A_jaa","timestamp":"1705149540.0"},{"poster":"boooliyooo","content":"Selected Answer: C\nOption A (Stop and Restart) is less operationally complex and provides a quicker way to resume the database. It's suitable if the primary concern is operational simplicity and quick availability.\n Option C (Snapshot, Terminate, and Restore) may offer higher cost savings, especially if the instance is large and expensive to run, as you're avoiding charges for the time the instance is down. However, it comes with higher operational complexity and longer lead times to bring the database back online.\n\nIn Amazon RDS, you do incur charges for a DB instance even when it's stopped. This is a key distinction from Amazon EC2, where you are not charged for instance hours while an EC2 instance is stopped. For RDS, the charges related to the instance's storage and backups continue to accrue even when the instance itself is not running.","upvote_count":"4","comment_id":"1108308","timestamp":"1703821380.0"},{"content":"It is C\nAmazon RDS allows you to easily stop and start your database instances ONLY for up to seven days at a time. So Snapshot and Restore is proper solution.","comment_id":"1106928","timestamp":"1703689260.0","upvote_count":"2","poster":"ignajtpolandstrong"},{"timestamp":"1703645820.0","poster":"upliftinghut","upvote_count":"1","content":"Selected Answer: A\nDatabase not used by any other applications/processes, only use 48 hours per month while still need the same compute and RAM for intense testing => stop and start the instance will be the most cost effective. Option C needs to pay for snapshot of DB and more complex for what purpose?","comments":[{"poster":"upliftinghut","timestamp":"1703680980.0","content":"Sorry, should be C. Key word: Most cost-effective so C - which is taking snapshot is most cost-effective","upvote_count":"3","comment_id":"1106796"}],"comment_id":"1106524"},{"poster":"axayprabhu","comment_id":"1084521","content":"Can you please clear my confusion, If the answer is C,\nWhat is the use of DB here, when every time DB is terminated and restored from snapshots? Data will remain the same right? If DB is terminated how new data is stored when db is not present","timestamp":"1701359760.0","upvote_count":"1"},{"content":"Selected Answer: A\nThe Answer is A.\nOption A does reduce costs when RDS is not running, because RDS does not charge execution fees when it is not running. When an RDS instance is stopped, you only pay the associated storage charges. In Amazon RDS, storage and backup charges are based on the amount of storage you use. Therefore, when you stop an RDS execution instance, you will still pay the costs associated with storage, but not the execution fees.\nIn contrast, if you use option C, which is to take a snapshot and terminate the instance, there may be costs associated with storing the snapshot and Amazon Machine Image (AMI). Overall, option A minimizes costs because when you stop an RDS execution instance, you only have to pay a relatively low storage cost rather than an execution fee.","comment_id":"1073130","timestamp":"1700212140.0","poster":"MiniYang","upvote_count":"2"},{"upvote_count":"2","poster":"roberto_rrt","content":"Selected Answer: A\nA. Stop the DB instance when tests are completed. Restart the DB instance when required.\n\nHere's why option A is the most suitable choice:\n\nCost Reduction: Stopping the DB instance when not in use effectively reduces the cost to zero during the idle period. You only pay for storage when the instance is stopped. This is a cost-effective way to handle infrequent, resource-intensive tasks without incurring ongoing costs.\n\nPerformance Insights Enabled: This option allows you to keep Performance Insights enabled when the DB instance is stopped, which provides visibility into database performance. You can resume the instance and monitor performance during the testing period.","comment_id":"1043659","timestamp":"1697308380.0"},{"upvote_count":"3","poster":"hrushikeshrelekar","timestamp":"1696042380.0","content":"Selected Answer: D\nA. Stop the DB instance when tests are completed. Restart the DB instance when required.\n\nExplanation:\n\nStopping and starting a DB instance is the most cost-effective solution for scenarios where the database is not in use all the time. Amazon RDS allows you to stop and start the database instances, and you are not charged for the instance hours while the database is stopped.","comment_id":"1021221"},{"timestamp":"1693669560.0","content":"chatgpt is saying one option is to start/stop db instance, so choice A even though the popular choice is C, otherwise use Aurora but that is not an option, nor would it probably be the most cost effective option","poster":"Chiquitabandita","upvote_count":"1","comment_id":"996981"},{"comment_id":"980045","poster":"Fresbie99","content":"Selected Answer: C\nAs DB snapshots is cost efficient.","upvote_count":"1","timestamp":"1691932800.0"},{"upvote_count":"1","content":"Option C is the right answer for this.","poster":"miki111","comment_id":"953603","timestamp":"1689538140.0"},{"comment_id":"926606","timestamp":"1687082460.0","poster":"cookieMr","content":"Selected Answer: C\nOption C can be a cost-effective solution for reducing the cost of running tests on the RDS instance.\n\nBy creating a snapshot and terminating the DB instance, you effectively stop incurring costs for the running instance. When you need to run the tests again, you can restore the snapshot to create a new instance and resume testing. This approach allows you to save costs during the periods when the tests are not running.\n\nHowever, it's important to note that option C involves additional steps and may result in some downtime during the restoration process. You need to consider the time required for snapshot creation, termination, and restoration when planning the testing schedule.","upvote_count":"3"},{"comment_id":"926573","poster":"Abrar2022","upvote_count":"1","timestamp":"1687078980.0","content":"Selected Answer: C\nCan't be A because you're still charged for provisioned storage even when it's stopped."},{"timestamp":"1685642040.0","content":"Selected Answer: C\nBy only stopping an Amazon RDS DB instance, you stop billing for additional instance hours, but you will still incur storage costs. See: https://aws.amazon.com/rds/pricing/","upvote_count":"1","poster":"Peng001","comment_id":"912323"},{"poster":"studynoplay","timestamp":"1682881140.0","content":"Selected Answer: C\nTrick: in a stopped RDS database, you will still pay for storage. If you plan on\nstopping it for a long time, you should snapshot & restore instead","comment_id":"885535","upvote_count":"2"},{"content":"Selected Answer: C\nCompare A and C, for a 48 hours usage among a month, C's cost lower.","comment_id":"858625","upvote_count":"2","poster":"channn","timestamp":"1680422520.0"},{"poster":"linux_admin","upvote_count":"3","timestamp":"1680264960.0","comment_id":"857040","content":"Selected Answer: A\nOption A, stopping the DB instance when tests are completed and restarting it when required, would be the most cost-effective solution to reduce the cost of running the tests while maintaining the same compute and memory attributes of the DB instance.\n\nBy stopping the DB instance when the tests are completed, the company will only be charged for storage and not for compute resources while the instance is stopped. This can result in significant cost savings as compared to running the instance continuously.\n\nWhen the tests need to be run again, the company can simply start the DB instance, and it will be available for use. This solution is straightforward and does not require any additional configuration or infrastructure.","comments":[{"timestamp":"1681646760.0","poster":"ImKingRaje","content":"if you stopped RDS it gets auto start after 7 days. Here the requirement is once in month ..hence C","comment_id":"871727","upvote_count":"3"}]},{"poster":"cheese929","upvote_count":"1","content":"Selected Answer: C\nC is the most cost effective.","timestamp":"1679815140.0","comment_id":"850797"},{"poster":"Tiba","comment_id":"770766","upvote_count":"1","content":"You can't stop an Amazon RDS for SQL Server DB instance in a Multi-AZ configuration.","timestamp":"1673289360.0"},{"upvote_count":"1","poster":"SilentMilli","timestamp":"1673041980.0","comment_id":"768100","content":"Selected Answer: C\nAmazon RDS for MySQL allows you to create a snapshot of your DB instance and store it in Amazon S3. You can then terminate the DB instance and restore it from the snapshot when required. This will allow you to reduce the cost of running the resource-intensive tests without reducing the compute and memory attributes of the DB instance."},{"content":"Selected Answer: C\nC is right choice here","poster":"techhb","upvote_count":"1","comment_id":"763514","timestamp":"1672638180.0"},{"upvote_count":"5","comments":[{"comment_id":"758956","poster":"HayLLlHuK","upvote_count":"3","timestamp":"1672168740.0","content":"INCORRECT: \"Stop the DB instance once all tests are completed. Start the DB instance again when required” is incorrect. You will be charged when your instance is stopped. When an instance is stopped you are charged for provisioned storage, manual snapshots, and automated backup storage within your specified retention window, but not for database instance hours. This is more costly compared to using snapshots.\nINCORRECT: \"Create an Auto Scaling group for the DB instance and reduce the desired capacity to 0 once the tests are completed” is incorrect. You cannot use Auto Scaling groups with Amazon RDS instances.\nINCORRECT: \"Modify the DB instance size to a smaller capacity instance when all the tests have been completed. Scale up again when required” is incorrect. This will reduce compute and memory capacity and will be more costly than taking a snapshot and terminating the DB."}],"timestamp":"1672168680.0","poster":"HayLLlHuK","content":"Selected Answer: C\nExplanation from the same question on UDEMY!\nTaking a snapshot of the instance and storing the snapshot is the most cost-effective solution. When needed, a new database can be created from the snapshot. Performance Insights can be enabled on the new instance if needed. Note that the previous data from Performance Insights will not be associated with the new instance, however this was not a requirement.\nCORRECT: \"Create a snapshot of the database when the tests are completed. Terminate the DB instance. Create a new DB instance from the snapshot when required” is the correct answer (as explained above.)","comment_id":"758953"},{"poster":"benjl","timestamp":"1671769740.0","content":"Answer is C,\nBecause the question say monthly test, and you can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html\nSo, in this case, if it run a test once a month, creating a snapshot is more appropriate and cost-effective way.","comment_id":"753845","upvote_count":"3"},{"timestamp":"1671503700.0","upvote_count":"2","comment_id":"750411","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: A\nOption A, stopping the DB instance when tests are completed and restarting it when required, would be the most cost-effective solution for reducing the cost of running resource-intensive tests on an Amazon RDS for MySQL DB instance.\n\nBy stopping the DB instance, you will no longer be charged for any compute or memory resources used by the instance. When the tests are completed, you can restart the DB instance to resume using it. This will allow you to avoid paying for resources that are not being used, while still maintaining the same compute and memory attributes of the DB instance for the tests.","comments":[{"comments":[{"upvote_count":"1","comment_id":"750413","content":"Option D, modifying the DB instance to a low-capacity instance when tests are completed and then modifying it back again when required, would not meet the requirement to maintain the same compute and memory attributes of the DB instance for the tests. Modifying the DB instance to a low-capacity instance would result in a reduction in the resources available to the DB instance, which would not be sufficient for the resource-intensive tests.","timestamp":"1671503760.0","poster":"Buruguduystunstugudunstuy"},{"content":"https://docs.aws.amazon.com/pt_br/AmazonRDS/latest/UserGuide/USER_StopInstance.html\nImportant\n\"You can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started. This way, it doesn't fall behind any required maintenance updates.\"","timestamp":"1673263200.0","upvote_count":"3","comment_id":"770281","poster":"bombtux"}],"timestamp":"1671503760.0","poster":"Buruguduystunstugudunstuy","upvote_count":"1","comment_id":"750412","content":"Option B, using an Auto Scaling policy with the DB instance to automatically scale when tests are completed, would not be a cost-effective solution as it would not reduce the cost of running the tests. Auto Scaling allows you to automatically increase or decrease the capacity of your DB instance based on predefined rules, but it does not provide a way to reduce the cost of running the tests.\n\nOption C, creating a snapshot when tests are completed and then terminating the DB instance and restoring the snapshot when required, would also not be a cost-effective solution. While creating a snapshot can be a useful way to save a copy of your database, it does not reduce the cost of running the tests. Additionally, restoring a snapshot to a new DB instance would require you to pay for the resources used by the new instance."}]},{"content":"Selected Answer: C\nC is the best and most cost effective option","timestamp":"1671430860.0","poster":"career360guru","upvote_count":"1","comment_id":"749526"},{"comment_id":"743790","upvote_count":"1","poster":"Shasha1","timestamp":"1670922360.0","content":"A\nStopping the DB instance when tests are completed and restarting it when required will be the most cost-effective solution for reducing the cost of running the resource-intensive tests. When an Amazon RDS for MySQL DB instance is stopped, the instance will no longer be charged for compute and memory usage, which will significantly reduce the cost of running the tests. Option C is not correct for me, it is because, Snapshots are used to create backups of data, but do not reduce the cost of running a DB instance."},{"timestamp":"1669035540.0","upvote_count":"1","comment_id":"723503","content":"C is correct","poster":"Wpcorgan"},{"content":"Selected Answer: C\nis correct","comment_id":"708098","poster":"17Master","upvote_count":"1","timestamp":"1667176620.0"},{"comment_id":"692724","content":"Selected Answer: A\nIf instance state is stopped, it's not billed.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html","comments":[{"poster":"mj61","upvote_count":"1","timestamp":"1674005700.0","comment_id":"779490","content":"The underlying EBS volumes or provisioned IOPS are. Those charges are higher than storing a snapshot in S3 and restoring once a month from that."},{"content":"While your DB instance is stopped, you are charged for provisioned storage (including Provisioned IOPS)","poster":"Rachness","comment_id":"695131","upvote_count":"2","timestamp":"1665800820.0"},{"content":"It's a DB instance, not an EC2 instance. If the DB instance is stopped, you are still paying for the storage.","upvote_count":"10","timestamp":"1666013700.0","comment_id":"697432","poster":"ArielSchivo","comments":[{"timestamp":"1666935300.0","content":"Thank you for your explanation","comment_id":"706164","poster":"BoboChow","upvote_count":"3"},{"comment_id":"716551","poster":"Jerry84","upvote_count":"1","timestamp":"1668241260.0","content":"Thanks for your reply."}]}],"poster":"BoboChow","timestamp":"1665554460.0","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/85030-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["C (78%)","A (19%)","2%"],"answer":"C","question_id":244,"topic":"1","timestamp":"2022-10-10 16:25:00","answer_description":"","exam_id":31,"answer_ET":"C","isMC":true,"unix_timestamp":1665411900,"question_images":[],"choices":{"D":"Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required.","C":"Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.","B":"Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.","A":"Stop the DB instance when tests are completed. Restart the DB instance when required."}},{"id":"Qb2ZOf46pazKzaj1PSSu","question_text":"A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time.\n\nWhat should a solutions architect do to meet these requirements MOST cost-effectively?","choices":{"B":"Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.","A":"Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.","C":"Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.","D":"Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances."},"answer":"C","answers_community":["C (86%)","14%"],"topic":"1","unix_timestamp":1676817480,"exam_id":31,"answer_description":"","discussion":[{"poster":"LuckyAro","comment_id":"817002","content":"Selected Answer: C\nAmazon EC2 Reserved Instances allow for significant cost savings compared to On-Demand instances for long-running, steady-state workloads like this one. Reserved Instances provide a capacity reservation, so the instances are guaranteed to be available for the duration of the reservation period.\n\nAmazon Aurora is a highly scalable, cloud-native relational database service that is designed to be compatible with MySQL and PostgreSQL. It can automatically scale up to meet growing storage requirements, so it can accommodate the application's database storage needs over time. By using Reserved Instances for Aurora, the cost savings will be significant over the long term.","timestamp":"1692636180.0","upvote_count":"22"},{"timestamp":"1692448680.0","comments":[{"upvote_count":"5","timestamp":"1692538800.0","content":"Since the application's database storage is continuously growing over time, it may be difficult to estimate the appropriate size of the Aurora cluster in advance, which is required when reserving Aurora.\n\nIn this case, it may be more cost-effective to use Amazon RDS On-Demand Instances for the data storage layer. With RDS On-Demand Instances, you pay only for the capacity you use and you can easily scale up or down the storage as needed.","comment_id":"815466","poster":"NolaHOla","comments":[{"comment_id":"910803","poster":"hristni0","timestamp":"1701327480.0","upvote_count":"2","content":"Answer is C. From Aurora Reserved Instances documentation:\nIf you have a DB instance, and you need to scale it to larger capacity, your reserved DB instance is automatically applied to your scaled DB instance. That is, your reserved DB instances are automatically applied across all DB instance class sizes. Size-flexible reserved DB instances are available for DB instances with the same AWS Region and database engine."},{"upvote_count":"2","timestamp":"1692800040.0","content":"The Answer is C.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html","poster":"Joxtat","comment_id":"819388"}]},{"poster":"pentium75","upvote_count":"3","content":"Database STORAGE will grow, not performance need (and required instance size).","timestamp":"1719658440.0","comment_id":"1108681"}],"comment_id":"814175","content":"Option B based on the fact that the DB storage will continue to grow, so on-demand will be a more suitable solution","poster":"NolaHOla","upvote_count":"15"},{"timestamp":"1738551960.0","upvote_count":"1","content":"Selected Answer: B\nRIs are best for steady-state database workloads.","comment_id":"1350726","poster":"zdi561"},{"timestamp":"1727460480.0","upvote_count":"3","comment_id":"1184338","content":"cost-effectively - the answer is C.\nThe application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time.","poster":"hro"},{"comment_id":"1160537","timestamp":"1724752680.0","upvote_count":"3","poster":"MrPCarrot","content":"Answer is C: Amazon EC2 Reserved Instances and Amazon Aurora Reserved Instances = less expensive than RDS."},{"content":"Amazon Aurora reserved instances is used for the work load on predictable, so answer should be B","poster":"andyngkh86","timestamp":"1722205200.0","upvote_count":"1","comment_id":"1134554"},{"poster":"Priyapani","upvote_count":"2","content":"Selected Answer: B\nI think it's B as database storage will grow","timestamp":"1720960080.0","comment_id":"1122598","comments":[{"comment_id":"1125445","upvote_count":"4","poster":"awsgeek75","content":"Application runs 24x7 which means database is also used 24x7. The storage will grow and RDS On-Demand does not have auto-grow storage. You have to configure a storage size for RDS which means it will eventually run out of space. RDS On-Demand just scales CPU, not storage.\n\nAurora has no storage limitation and can scale storage according to need which is what is required here","timestamp":"1721253120.0"}]},{"content":"Selected Answer: C\n24/7 forbids spot instances , so A is excluded\nCost efficience require reserved instances , so D is excluded\nBetween RDS and Aurora, Aurora is less expensive thanks to the reserved instance, so B is finally excluded\n\nAnswer is C","comment_id":"1086777","upvote_count":"4","timestamp":"1717407000.0","poster":"Mikado211"},{"comment_id":"1062319","upvote_count":"1","content":"Selected Answer: B\nI hope it should be B considering Database growth","timestamp":"1714843860.0","comments":[{"upvote_count":"3","comment_id":"1108684","poster":"pentium75","content":"Reserved instance applies to the DB instance size (CPU, RAM etc.), not storage.","timestamp":"1719658500.0"}],"poster":"cciesam"},{"timestamp":"1711049160.0","content":"My research concludes that From pure price point of view Aurora Reserved might/ usually be slightly more expensive than On-demand RDS. But RDS has less Operation overhead. For the 24x7 nature, I would vote C. But for pure cost-effective, B is less costly.","comment_id":"1013304","poster":"Wayne23Fang","upvote_count":"2"},{"upvote_count":"2","comment_id":"999697","content":"Selected Answer: C\nThis option involves migrating the application layer to Amazon EC2 Reserved Instances and migrating the data storage layer to Amazon Aurora Reserved Instances. Amazon EC2 Reserved Instances provide a significant discount (up to 75%) compared to On-Demand Instance pricing, making them a cost-effective choice for applications that have steady state or predictable usage. Similarly, Amazon Aurora Reserved Instances provide a significant discount (up to 69%) compared to On-Demand Instance pricing.","poster":"Guru4Cloud","timestamp":"1709663220.0"},{"poster":"ajchi1980","comment_id":"937955","upvote_count":"2","content":"Selected Answer: C\nTo meet the requirements of migrating a legacy application from an on-premises data center to the AWS Cloud in a cost-effective manner, the most suitable option would be:\n\nC. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.\n\nExplanation:\n\nMigrating the application layer to Amazon EC2 Reserved Instances allows you to reserve EC2 capacity in advance, providing cost savings compared to On-Demand Instances. This is especially beneficial if the application runs 24/7.\n\nMigrating the data storage layer to Amazon Aurora Reserved Instances provides cost optimization for the growing database storage needs. Amazon Aurora is a fully managed relational database service that offers high performance, scalability, and cost efficiency.","timestamp":"1703855880.0"},{"comment_id":"909813","poster":"cpen","upvote_count":"1","content":"nnascncnscnknkckl","timestamp":"1701315960.0"},{"comment_id":"874295","timestamp":"1697692740.0","poster":"TariqKipkemei","upvote_count":"2","content":"Answer is C"},{"poster":"QuangPham810","upvote_count":"2","timestamp":"1697617200.0","content":"Answer is C. Refer https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithReservedDBInstances.html => Size-flexible reserved DB instances","comment_id":"873406"},{"timestamp":"1694170800.0","upvote_count":"2","comment_id":"832964","content":"Selected Answer: C\nC: With Aurora Serverless v2, each writer and reader has its own current capacity value, measured in ACUs. Aurora Serverless v2 scales a writer or reader up to a higher capacity when its current capacity is too low to handle the load. It scales the writer or reader down to a lower capacity when its current capacity is higher than needed.\n\nThis is sufficient to accommodate the growing data changes. \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.how-it-works.html#aurora-serverless-v2.how-it-works.scaling","poster":"Abhineet9148232"},{"comment_id":"827453","timestamp":"1693682580.0","upvote_count":"2","content":"Selected Answer: C\nTypically Amazon RDS cost less than Aurora. But here, it's Aurora reserved.","comments":[{"content":"although agree and AWS wants you to choose Answer C. You can't convince a cloud accounting analyst that Aurora is cheaper than RDS. no matter what","poster":"djgodzilla","comment_id":"1115563","upvote_count":"2","timestamp":"1720312740.0"}],"poster":"Steve_4542636"},{"poster":"ACasper","comment_id":"827184","content":"Answer C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithReservedDBInstances.html\nDiscounts for reserved DB instances are tied to instance type and AWS Region.","upvote_count":"2","timestamp":"1693670340.0"},{"poster":"AlmeroSenior","timestamp":"1692931740.0","content":"Selected Answer: C\nBoth RDS and RDS aurora support Storage Auto scale . \nAurora is more expensive than base RDS , But between B and C , the Aurora is reserved instance and base RDS is on demand . Also it states the DB strorage will grow , so no concern about a bigger DB instance ( server ) , only the actual storage","comment_id":"821193","upvote_count":"3"},{"poster":"Joxtat","upvote_count":"2","comment_id":"819385","timestamp":"1692799980.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html"},{"comment_id":"818755","timestamp":"1692754680.0","upvote_count":"2","content":"Selected Answer: B\nI also think it is B. Otherewise there is no point in mentionig about growing storage requirements.","poster":"Samuel03"},{"comments":[{"timestamp":"1692703860.0","upvote_count":"1","poster":"Americo32","content":"Mudando para opção C, Observações importantes sobre compras\nOs preços de instâncias reservadas cobrem apenas os custos da instância. O armazenamento e a E/S ainda são faturados separadamente.","comment_id":"817864"}],"poster":"Americo32","content":"Selected Answer: B\nA opção B com base no fato de que o armazenamento de banco de dados continuará a crescer, portanto, sob demanda será uma solução mais adequada","upvote_count":"1","timestamp":"1692703380.0","comment_id":"817857"},{"comment_id":"817321","upvote_count":"3","content":"Why not B?","timestamp":"1692658380.0","poster":"ManOnTheMoon"},{"comment_id":"814335","upvote_count":"3","timestamp":"1692460320.0","poster":"skiwili","content":"Selected Answer: C\nCcccccc"}],"timestamp":"2023-02-19 15:38:00","url":"https://www.examtopics.com/discussions/amazon/view/99948-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"question_images":[],"question_id":245,"answer_images":[],"answer_ET":"C"}],"exam":{"name":"AWS Certified Solutions Architect - Associate SAA-C03","isMCOnly":true,"isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":31,"provider":"Amazon","numberOfQuestions":1019},"currentPage":49},"__N_SSP":true}