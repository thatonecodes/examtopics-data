{"pageProps":{"questions":[{"id":"GdHnsTo6wvN2SQaY6rg8","question_text":"A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable.\nWhich solution will meet these requirements?","exam_id":31,"unix_timestamp":1667251620,"answers_community":["B (83%)","Other"],"question_id":51,"timestamp":"2022-10-31 22:27:00","choices":{"B":"Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.","A":"Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.","D":"Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately and to S3 Glacier Deep Archive after 2 years.","C":"Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive."},"answer":"B","discussion":[{"upvote_count":"17","comment_id":"720345","comments":[{"timestamp":"1689352140.0","comments":[{"comment_id":"951671","upvote_count":"2","timestamp":"1689352260.0","content":"Just to clarify, my previous comment is about how answer B) might be correct and the MOST cheapest option under the correct configuration.","comments":[{"comment_id":"951673","timestamp":"1689352320.0","poster":"MutiverseAgent","upvote_count":"1","content":"Sorry, I meant answer C) might be correct"}],"poster":"MutiverseAgent"}],"content":"Mmm.. You can enable Intelligent-Tiering and take advantage of of the infrequent Access tier and thus reducing costs. To avoid moving objects to the deep archive tier before the two years it would be enough to enable ONLY the check \"Deep Archive Access tier\" and set days to 720 (two years, which is curiously the maximum value), and keep disabled the check \"Archive Access tier\" to avoid the Intelligent-Tiering move objects to the non-instant retrieval tier. That will work, offcourse this specific configuration is not mention in the question which leaves some doubts about which option is the correct.","comment_id":"951669","poster":"MutiverseAgent","upvote_count":"2"},{"comment_id":"984307","timestamp":"1692348900.0","comments":[{"timestamp":"1714816080.0","poster":"sandordini","comment_id":"1206420","content":"S3 Standard IA is NOT single AZ. (One-Zone IA is single Az.)","comments":[{"timestamp":"1714816320.0","poster":"sandordini","comment_id":"1206422","content":"The issue is with the missing \"flag\" for 2 years, and not S3 Intelligent Tiering. It needs to be B.","upvote_count":"2"}],"upvote_count":"2"}],"upvote_count":"2","content":"but your S3 intelligent-tiering will move the object to S3 infrequent access tier which a is a single AZ tier , and then the HA requirement will not be respected","poster":"Abdou1604"}],"timestamp":"1668675600.0","content":"Selected Answer: B\nWhy Not C? Because Intelligent Tier the objects are automatically moved to different tiers.\nThe question says \"the data from most recent 2 yrs should be highly available and immediately retrievable\", which means in intelligent tier , if you activate archiving option(as Option C specifies) , the objects will be moved to Archive tiers(instant to access to deep archive access tiers) in 90 to 730 days. Remember these archive tiers performance will be similar to S3 glacier flexible and s3 deep archive which means files cannot be retrieved immediately within 2 yrs .\n\nWe have a hard requirement in question which says it should be retreivable immediately for the 2 yrs. which cannot be acheived in Intelligent tier. So B is the correct option imho.\n\nBecause of the above reason Its possible only in S3 standard and then configure lifecycle configuration to move to S3 Glacier Deep Archive after 2 yrs.","poster":"rjam"},{"upvote_count":"9","timestamp":"1668720660.0","content":"Selected Answer: B\nB is the only right answer. C does not indicate archiving after 2 years. If it did specify 2 years, then C would also be an option.","comment_id":"720882","poster":"TelaO"},{"content":"Selected Answer: B\nS3 Lifecycle policies works best here as we no longer need frequently access to data that has been over 2 years and hence moving it to S3 Glacier Deep Archive help reduce costs.","timestamp":"1736336220.0","upvote_count":"1","poster":"satyaammm","comment_id":"1337914"},{"content":"Selected Answer: B\nAns B - I did initially think Ans C, but rjam (1 yr, 10 mth ago) quickly quashed that notion: \n\"Why Not C? Because Intelligent Tier the objects are automatically moved to different tiers. The question says \"the data from most recent 2 yrs should be highly available and immediately retrievable\", which means in intelligent tier , if you activate archiving option(as Option C specifies) , the objects will be moved to Archive tiers(instant to access to deep archive access tiers) in 90 to 730 days. Remember these archive tiers performance will be similar to S3 glacier flexible and s3 deep archive which means files cannot be retrieved immediately within 2 yrs.\"","poster":"PaulGa","timestamp":"1726661580.0","upvote_count":"1","comment_id":"1285711"},{"timestamp":"1725003540.0","poster":"bignatov","upvote_count":"1","comment_id":"1274965","content":"Selected Answer: B\nB is correct, because it fits to the requirements and it still cheaper than the option C."},{"poster":"JaegEr_2k1","timestamp":"1722565800.0","comment_id":"1259634","upvote_count":"2","content":"Stupid question: \nA: No immediately retrievable and cost \nB: No immediately retrievable\nC: Not unpredicted access\nD: Immediately retrievable but not HA\nF*ck the guy who made this question"},{"comment_id":"1253507","upvote_count":"2","timestamp":"1721722320.0","content":"Selected Answer: B\nB is the correct choice, the requirements are very clear, intelligent tiering is used only when we don't have a clear pattern for the access of the data, when it's unpredictable.","poster":"jaradat02"},{"poster":"LoXoL","content":"Selected Answer: B\nA. We can't move to Glacier immediately as data from last 2 yrs need to be immediately retrievable\nB. It's the perfect fit: getting HA and instant access (with current solution = S3 std), then moving to Deep Archive after 2 yrs (very cheap)\nC: Highly expensive because of Intelligent Tiering\nD: it lacks HA with One Zone","timestamp":"1704878640.0","upvote_count":"2","comment_id":"1118403"},{"comment_id":"1101217","poster":"SaurabhTiwari1","timestamp":"1703047140.0","upvote_count":"3","content":"Selected Answer: B\nData remain in S3 standard storage for 2 years then it will be move to s3 glacier deep archive after 2 year."},{"upvote_count":"1","poster":"Ruffyit","timestamp":"1698638400.0","comment_id":"1057334","content":"but your S3 intelligent-tiering will move the object to S3 infrequent access tier which a is a single AZ tier , and then the HA requirement will not be respected"},{"timestamp":"1697446800.0","comment_id":"1044795","content":"Selected Answer: B\ni understand why \"B\" is more correct than \"C\" and is because \"C\" is bad formulated, if in the answer would say \"life cycle after 2 years of using intelligent tiring\" then it would be the correct answer. so \"B\" is correct","upvote_count":"2","poster":"David_Ang"},{"comment_id":"995645","timestamp":"1693540980.0","content":"Selected Answer: B\nI would not opt for C simply because S3IT was specifically designed for scenarios where the access patterns are unknown.\nThis scenario has clearly known access patterns making option B the best.","poster":"TariqKipkemei","upvote_count":"3"},{"upvote_count":"3","timestamp":"1687504140.0","comments":[{"timestamp":"1694351940.0","upvote_count":"2","content":"this makes sense the question is a bit tricky. I now uderstand that all the data is already kept in S3 Standard meaning immediate retrieval of the most recent data is remains highly available.","poster":"kambarami","comment_id":"1004009"}],"comment_id":"931299","content":"Selected Answer: B\nOption A is incorrect because immediately transitioning objects to S3 Glacier Deep Archive would not fulfill the requirement of keeping the most recent 2 years of data highly available and immediately retrievable.\n\nOption C is also incorrect because using S3 Intelligent-Tiering with archiving option would not meet the requirement of immediately retrievable data for the most recent 2 years.\n\nOption D is not the best choice because transitioning objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) and then to S3 Glacier Deep Archive would not satisfy the requirement of immediately retrievable data for the most recent 2 years.\n\nOption B is the correct solution. By setting up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years, the company can keep all data for at least 25 years while ensuring that data from the most recent 2 years remains highly available and immediately retrievable in the Amazon S3 Standard storage class. This solution optimizes storage costs by leveraging the Glacier Deep Archive for long-term storage.","poster":"cookieMr"},{"comment_id":"925758","timestamp":"1686975540.0","upvote_count":"2","comments":[{"content":"\"Data from the most recent 2 years must be highly available and immediately retrievable.\"","upvote_count":"1","timestamp":"1697631360.0","comment_id":"1046893","poster":"RNess"},{"poster":"RNess","timestamp":"1697631540.0","comment_id":"1046896","content":"Additionally,\nS3 Standard Availability = 99.99%\nS3 One Zone-IA Availability = 99.5%","upvote_count":"2"}],"content":"Why not D","poster":"Yadav_Sanjay"},{"upvote_count":"1","timestamp":"1681755120.0","content":"Selected Answer: B\nB is the only one possible.","poster":"Robrobtutu","comment_id":"872954"},{"comment_id":"870283","upvote_count":"2","poster":"rushlav","content":"C would not work as the names of these S3 archives are called Archive Access Tier and Deep Archive access tiers, so since they mention glacier in option C , I think its B which is the correct.","timestamp":"1681485420.0"},{"comment_id":"798940","timestamp":"1675611240.0","upvote_count":"2","content":"It's pretty straight forward.\n\nS3 Standard answers for High Availaibility/Immediate retrieval for 2 years. S3 Intelligent Tiering would just incur additional cost of analysis while the company insures that it requires immediate retrieval in any moment and without risk to Availability. So a capital B","poster":"CaoMengde09"},{"poster":"G3","comments":[{"comment_id":"829432","poster":"Sdraju","timestamp":"1677965940.0","upvote_count":"1","content":"Intelligent tiering appears to be best suited for unknown usage pattern.. but with a known usage pattern Life cycle policy may be optimal."},{"content":"The option just says Intelligent Tiering, it doesn't specify when it would transition the date to Deep Archive, so how do we know it would do it at the correct time? It has to be A.","comment_id":"872950","timestamp":"1681754940.0","upvote_count":"1","poster":"Robrobtutu"}],"content":"C appears to be appropriate - good case for intelligent tiering","comment_id":"792037","timestamp":"1675023900.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: C\nC. Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.\n\nS3 Intelligent Tiering supports changing the default archival time to 730 days (2 years) from the default 90 or 180 days. Other levels of tiering are instant access tiers.","poster":"DaveNL","timestamp":"1673548620.0","comment_id":"773787"},{"timestamp":"1672499820.0","poster":"Zerotn3","comments":[{"comment_id":"770357","timestamp":"1673267520.0","content":"If the option for D was Infrequent Access it would be good, but here it is One Zone-IA which is not highly available. Then it must be B","poster":"lfrad","upvote_count":"5"},{"content":"Option A is not a good solution because it would transition all objects to S3 Glacier Deep Archive immediately, making the data from the most recent 2 years not immediately retrievable. Option B is not a good solution because it would not make the data from the most recent 2 years immediately retrievable.\n\nOption C is not a good solution because S3 Intelligent-Tiering is designed to automatically move objects between two storage classes (Standard and Infrequent Access) based on object access patterns. It does not provide a way to transition objects to S3 Glacier Deep Archive, which is required for long-term storage.\n\nOption D is the correct solution because it would transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately, making the data from the most recent 2 years immediately retrievable. After 2 years, the objects would be transitioned to S3 Glacier Deep Archive for long-term storage. This solution meets the requirements of the company to keep all data for at least 25 years and make the data from the most recent 2 years immediately retrievable.","poster":"Zerotn3","upvote_count":"2","comment_id":"762692","comments":[{"poster":"Ello2023","upvote_count":"1","content":"B is immediately retrievable, has high availability and using the lifecycle you can transition to deep archive after the 2 years time period.","timestamp":"1673873340.0","comment_id":"777652"},{"poster":"hahahumble","content":"S3 One Zone-IA is not highly available compared with S3 standard\nhttps://aws.amazon.com/about-aws/whats-new/2018/04/announcing-s3-one-zone-infrequent-access-a-new-amazon-s3-storage-class/?nc1=h_ls","timestamp":"1673784540.0","comment_id":"776509","upvote_count":"2"}],"timestamp":"1672499880.0"}],"upvote_count":"4","content":"Selected Answer: D\nOption D is the correct solution for this scenario.\n\nS3 Lifecycle policies allow you to automatically transition objects to different storage classes based on the age of the object or other specific criteria. In this case, the company needs to keep all data for at least 25 years, and the data from the most recent 2 years must be highly available and immediately retrievable.","comment_id":"762691"},{"timestamp":"1671950700.0","comment_id":"755447","poster":"k1kavi1","content":"Selected Answer: B\nB looks correct","upvote_count":"2"},{"upvote_count":"1","timestamp":"1671394140.0","poster":"career360guru","content":"Selected Answer: B\nOption B","comment_id":"749142"},{"upvote_count":"2","comment_id":"740730","timestamp":"1670654400.0","poster":"lapaki","content":"Selected Answer: B\nB. Most correct"},{"upvote_count":"1","comments":[{"content":"From your link \"We added S3 Intelligent-Tiering to Amazon Amazon S3 to solve the problem of using the right storage class and optimizing costs when access patterns are irregular.\". But access patterns are not irregular, they are clearly stated on the question, so this is not required.","timestamp":"1671624180.0","comment_id":"752252","poster":"JayBee65","upvote_count":"3"}],"content":"Selected Answer: C\nhttps://aws.amazon.com/blogs/aws/s3-intelligent-tiering-adds-archive-access-tiers/","comment_id":"729126","timestamp":"1669641360.0","poster":"Cizzla7049"},{"comment_id":"723948","timestamp":"1669066260.0","upvote_count":"1","poster":"Wpcorgan","content":"B is correct"},{"timestamp":"1668367920.0","comments":[{"content":"\"moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier...\" This is not required, they should remain where they are for 2 years.","comments":[{"upvote_count":"1","poster":"JayBee65","content":"Once you have activated one or both of the archive access tiers, S3 Intelligent-Tiering will automatically move objects that haven’t been accessed for 90 days to the Archive Access tier, ...Objects in the archive access tiers are retrieved in 3-5 hours!\nYet the requirements are \"Data from the most recent 2 years must be highly available and immediately retrievable\". Not C!","comment_id":"752257","timestamp":"1671624420.0"}],"comment_id":"752253","timestamp":"1671624240.0","upvote_count":"1","poster":"JayBee65"}],"upvote_count":"1","poster":"Jtic","content":"Selected Answer: C\nC - S3 Intelligent-Tiering\nCustomers saving on storage with S3 Intelligent-Tiering\n\nS3 Intelligent-Tiering automatically stores objects in three access tiers: one tier optimized for frequent access, a lower-cost tier optimized for infrequent access, and a very-low-cost tier optimized for rarely accessed data. For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier for savings of 40%; and after 90 days of no access, they’re\n\nThere are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they’ll always be charged at the Frequent Access tier rates and don’t incur the monitoring and automation charge","comment_id":"717478"},{"comments":[{"timestamp":"1668535020.0","content":"https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html#:~:text=S3%20Intelligent%2DTiering%20provides%20you,minimum%20of%2090%20consecutive%20days. Option B / S3 Glacier Deep Archive seems correct to reduce a company's storage costs.","poster":"Wilson_S","upvote_count":"1","comment_id":"718998"}],"content":"Selected Answer: B\nOption C doesn't look correct for me because it is not clear when it will be moved to the Deep Archive. It could be earlier then 2 years, which is not correct","upvote_count":"4","comment_id":"717346","timestamp":"1668348660.0","poster":"Deplake"},{"timestamp":"1668278580.0","content":"Selected Answer: C\nThe answer C seems correct","upvote_count":"3","comment_id":"716865","poster":"MyNameIsJulien"},{"poster":"ArielSchivo","comment_id":"713907","content":"Glacier Deep Archive restores objects within 12 hours, so option A is out.\nOption B could work but you will be paying S3 Standard for 2 years.\nI would go with Option C then.\nOption D is out since S3 One Zone IA is not highly available.","upvote_count":"1","timestamp":"1667919060.0"},{"poster":"rjam","comment_id":"708730","upvote_count":"1","comments":[{"timestamp":"1667413860.0","content":"Data from the most recent 2 years must be highly available and immediately retrievable.","poster":"masetromain","comment_id":"709990","upvote_count":"5"}],"timestamp":"1667251620.0","content":"Option D as one-zone IA is cheaper than standard s3 . they never mentioned about multi zone. so we will go for one zone IA. The question mainly talks about reducing storage costs"}],"question_images":[],"answer_ET":"B","isMC":true,"topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/86731-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":""},{"id":"7indDOZAEpUG9vVA50Zk","question_text":"A media company is evaluating the possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possible I/O performance for video processing, 300 TB of very durable storage for storing media content, and 900 TB of storage to meet requirements for archival media that is not in use anymore.\nWhich set of services should a solutions architect recommend to meet these requirements?","topic":"1","choices":{"C":"Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon S3 for archival storage","D":"Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage","A":"Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage","B":"Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage"},"question_images":[],"unix_timestamp":1665710820,"isMC":true,"answer_images":[],"answer_description":"","discussion":[{"comments":[{"content":"Answer is A\nWhile instance store provides fast local storage, it is ephemeral (data is lost when the instance stops or terminates). It’s not suitable for workloads that require persistence, like video processing that may span across multiple EC2 instances over time. EBS provides persistent, high-performance storage.","poster":"Tsige","timestamp":"1729292760.0","comments":[{"comment_id":"1313095","timestamp":"1731769080.0","content":"err, stem did not mention requirement for durable storage for the 10TB used for video processing....","upvote_count":"1","poster":"JA2018"}],"upvote_count":"7","comment_id":"1299828"},{"poster":"ishitamodi4","content":"instance store volume for the root volume, the size of this volume varies by AMI, but the maximum size is 10 GB","upvote_count":"1","comment_id":"749826","comments":[{"content":"This link shows a max capacity of 30TB, so what is the problem? https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-volumes","poster":"JayBee65","timestamp":"1671624840.0","upvote_count":"2","comments":[{"content":"Only the following instance types support an instance store volume as the root device: C3, D2, G2, I2, M3, and R3, and we're using an I3, so an instance store volume is irrelevant.","comments":[{"upvote_count":"3","content":"THE CORRECT ANSWER IS A. \n\nThe biggest Instance Store Storage Optimized option (is4gen.8xlarge) has a capacity of only 3TB. \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-store-volumes.html#instance-store-vol-so","comment_id":"911811","timestamp":"1685601480.0","poster":"antropaws"}],"timestamp":"1671625080.0","poster":"JayBee65","comment_id":"752265","upvote_count":"5"}],"comment_id":"752262"}],"timestamp":"1671454320.0"},{"upvote_count":"3","content":"Update: i3en.metal and i3en.24xlarge = 8 x 7500 GB (60TB)","poster":"michellemeloc","comment_id":"887622","timestamp":"1683040920.0"}],"poster":"Sauran","timestamp":"1666188060.0","comment_id":"699085","upvote_count":"38","content":"Selected Answer: D\nMax instance store possible at this time is 30TB for NVMe which has the higher I/O compared to EBS.\n\nis4gen.8xlarge 4 x 7,500 GB (30 TB) NVMe SSD\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-volumes"},{"upvote_count":"25","timestamp":"1702360140.0","content":"Selected Answer: A\nThe correct Answer is A : \nAmazon EC2 instance store (Instance Store) is usually not the best choice because the storage it provides is temporary and tied to the life cycle of the instance. When an instance is stopped or terminated, data on the instance store is lost.\n\nIn this scenario, the company's requirements were to have the maximum possible I/O performance and required durable data storage. Therefore, using Amazon EC2 Instance Store does not meet these requirements because it lacks durability.\n\nIn contrast, Amazon EBS (Elastic Block Store) provides persistent regional block storage and can meet the needs of high-performance I/O. Therefore, the answer should include Amazon EBS, not Amazon EC2 instance storage.","comment_id":"1094155","poster":"MiniYang","comments":[{"comments":[{"poster":"LoXoL","upvote_count":"6","timestamp":"1704878940.0","content":"pentium75 is right.","comment_id":"1118404"}],"upvote_count":"14","timestamp":"1703578440.0","poster":"pentium75","comment_id":"1105810","content":"\"The company's requirements were to have the maximum possible I/O performance and required durable data storage.\" Yeah, but not for the same data.\n\n10 TB \"maximum possible I/O performance\" for processing (= temporary)\n300 TB \"very durable\" (= S3)\n900 TB \"for archival\" (= Glacier)"}]},{"content":"Selected Answer: A\nAmazon EBS Provisioned IOPS SSD (io2) for the 10 TB of storage needed for high I/O performance video processing.","poster":"CloudExpert01","comment_id":"1533726","upvote_count":"1","timestamp":"1743854940.0"},{"poster":"Mimine87","comment_id":"1503939","upvote_count":"1","content":"Selected Answer: A\nAt least 10 TB of storage with max I/O performance (for video processing):\nAmazon EBS (Elastic Block Store) provides high-performance block storage for EC2.\n\nProvisioned IOPS SSD (io2) volumes offer the maximum performance and consistency required for intensive workloads like video processing.\n\n❌ EC2 instance store is fast but ephemeral — data is lost if the instance stops or fails.","timestamp":"1743817740.0"},{"poster":"SirDNS","comment_id":"1401485","upvote_count":"1","timestamp":"1742548200.0","content":"Selected Answer: D\nCan be a bit confusing due to the presence of option A: EBS Provisioned IOPS SSD, Durable = S3, Archive = Glacier, but maximum performance is always equal to instance store. Here they do not need a permanent storage with maximum performance. It will only be used for processing and then sent to a highly durable storage.\nD logically cannot be the answer"},{"comment_id":"1357287","upvote_count":"1","poster":"sk1974","timestamp":"1739721720.0","content":"Selected Answer: A\nInstance storage is ephimeral data that gets destroyed when EC2 is stopped/restarted. Unless the question says that data is not important , I will assume that every data that app processed is imporant . So , I will go with A"},{"content":"Selected Answer: D\nAns D.\n\nThe requirements for every kind of data is specific. So, you need:\n10 TB high I/O / performance = Instance Store ( the reqs. doesn't mention anything about durability, just performance )\n300 TB very durable = S3\n900 TB archival = Glacier","comment_id":"1351033","poster":"kyd0nix","upvote_count":"1","timestamp":"1738601220.0"},{"timestamp":"1736336880.0","poster":"satyaammm","content":"Selected Answer: D\nEC2 Instance Store Volumes provide the highest storage performance in AWS and S3 provides durable storage while S3 Glacier is best for archival storage.","upvote_count":"2","comment_id":"1337919"},{"poster":"DavidPhyo","timestamp":"1736000220.0","upvote_count":"1","comment_id":"1336399","content":"Selected Answer: A\nanswer is A"},{"poster":"Mischi","content":"Selected Answer: A\nThe combination of Amazon EBS , Amazon S3 and Amazon S3 Glacier (option A) is the most suitable solution for this media company. It offers:\n\nMaximum I/O performance for video processing with Amazon EBS.\nDurability and reliability for media content with Amazon S3.\nCost-effective storage for archived data with Amazon S3 Glacier.","timestamp":"1734927000.0","upvote_count":"2","comment_id":"1330670"},{"upvote_count":"2","timestamp":"1734673500.0","comment_id":"1329310","poster":"salman7540","content":"Selected Answer: A\nI prefer A over D.\n\ninstance storage is not recommended for video processing because it's designed for temporary storage and doesn't provide the durability and persistence needed for video processing. Instead, Amazon EBS is a better choice for video processing because it provides the required performance, durability, and persistence. \nHere's some more information about instance storage and Amazon EBS:\nInstance storage\nA temporary storage volume that acts as a physical hard drive. It's located on disks that are physically attached to the host computer. Instance storage is ideal for temporary storage of information that changes frequently, such as buffers, caches, and scratch data. Data in an instance store persists during the lifetime of its instance, but it's not persistent through instance stops, terminations, or hardware failures."},{"timestamp":"1734051480.0","poster":"kimm_10","comment_id":"1325934","content":"Selected Answer: B\nWhy not EC2 instance store?\nInstance store is ephemeral (data is lost when the instance stops or terminates).\nIt lacks durability, making it unsuitable for critical video processing.","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"C & D. Amazon EC2 instance store for maximum performance: EC2 instance store provides temporary block storage tied to the lifecycle of an instance, so it is not durable or persistent, making it unsuitable for video processing workflows that require consistent data availability","comment_id":"1316076","timestamp":"1732235220.0","poster":"0de7d1b"}],"timestamp":"1732235220.0","comment_id":"1316075","upvote_count":"3","content":"Selected Answer: A\nExplanation:\nAmazon EBS (Elastic Block Store) for maximum performance:\n\nAmazon EBS provides high IOPS and low latency storage, which is ideal for video processing workloads that require fast performance.\nAmazon S3 for durable data storage:\n\nAmazon S3 is highly durable, scalable, and designed for storing large amounts of data, such as media content (300 TB). It also provides 99.999999999% (11 nines) durability, making it suitable for this requirement.\nAmazon S3 Glacier for archival storage:\n\nAmazon S3 Glacier is a cost-effective storage solution for archiving large amounts of data (900 TB) that is infrequently accessed but still needs to be stored securely and durably.","poster":"0de7d1b"},{"content":"answer is A","timestamp":"1731094680.0","poster":"Gizmo2022","upvote_count":"1","comment_id":"1308897"},{"content":"Selected Answer: A\ninstance store cannot be durable","comment_id":"1294380","upvote_count":"2","timestamp":"1728317700.0","poster":"aturret"},{"timestamp":"1726738140.0","comment_id":"1286252","content":"Selected Answer: D\nThe requirement ist \"maximum performance\". Instance storage is more performant than EBS, therefor D must be correct.","poster":"trainee46","upvote_count":"4"},{"content":"Selected Answer: A\nAns A - ideally suits situation since its media, fast access and long-term storage... \": EBS for maximum performance, S3 for durable data storage, and S3 Glacier for archival storage.\"","comment_id":"1285713","timestamp":"1726661820.0","upvote_count":"2","poster":"PaulGa"},{"poster":"Mayur_B","comment_id":"1279277","content":"Selected Answer: A\nA. \nEC2 Instance store is used mainly for temp data like caching, memory for computation etc","timestamp":"1725584880.0","upvote_count":"2"},{"timestamp":"1725076140.0","content":"A is correct answer. ask for copilot as well","poster":"AbhiBK","upvote_count":"1","comments":[{"timestamp":"1725076200.0","upvote_count":"1","poster":"AbhiBK","comment_id":"1275354","content":"Amazon EBS io2 Block Express Volumes: For 10 TB of high I/O performance storage.\nAmazon S3 Standard Storage Class: For 300 TB of durable storage for media content.\nAmazon S3 Glacier Deep Archive: For 900 TB of archival storage."}],"comment_id":"1275353"},{"timestamp":"1725005940.0","upvote_count":"2","poster":"bignatov","content":"Selected Answer: A\nI think it is A, because even D to comply the requirements it is a temporary storage and it is not the best choice.","comment_id":"1274982"},{"poster":"example_","comment_id":"1263998","timestamp":"1723369920.0","upvote_count":"3","content":"Selected Answer: D\nhttps://repost.aws/questions/QUHCySI6otStqGftOFHhHeOQ/saa-c03-question"},{"comment_id":"1253516","content":"Selected Answer: D\nAlthough the EC2 instance store is ephemeral, it provides the highest performance, and there is no demand of a presistant data storage for video processing, the best option for storing media is s3, which is an object storage system.","upvote_count":"3","timestamp":"1721722920.0","poster":"jaradat02"},{"upvote_count":"1","poster":"Seb888","timestamp":"1720872360.0","comment_id":"1247289","content":"Selected Answer: A\nCorrect Answer:\nA. Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\n\nExplanation:\nAmazon EBS for maximum performance: Amazon EBS (Elastic Block Store) provides high-performance block storage for use with Amazon EC2. It is well-suited for workloads requiring high I/O performance, such as video processing.\nAmazon S3 for durable data storage: Amazon S3 (Simple Storage Service) offers highly durable and scalable object storage. It is ideal for storing large amounts of media content with high durability.\nAmazon S3 Glacier for archival storage: Amazon S3 Glacier is a low-cost storage service for data archiving and long-term backup. It is perfect for storing archival media that is not frequently accessed but needs to be preserved."},{"poster":"jatric","timestamp":"1720149300.0","upvote_count":"1","comment_id":"1242445","content":"Selected Answer: A\nthe correct answer is A, max 10TB storage need per instance (not instances)"},{"poster":"ChymKuBoy","timestamp":"1718758680.0","upvote_count":"1","comment_id":"1232668","content":"Selected Answer: A\nA for sure"},{"poster":"richiexamaws","upvote_count":"1","timestamp":"1716876720.0","content":"Selected Answer: A\nAmazon EBS (Elastic Block Store): Provides high I/O performance suitable for video processing tasks.\nAmazon S3 (Simple Storage Service): Offers very durable and scalable storage, ideal for storing large amounts of media content.\nAmazon EFS (Elastic File System): While it provides scalable and durable storage, it is optimized for file-based workloads rather than object storage, which is less suitable for large-scale media storage","comment_id":"1220017"},{"poster":"chief05","comment_id":"1184646","upvote_count":"2","timestamp":"1711615980.0","content":"The best option for the given requirements would be:\n\nA. Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.\n\nExplanation:\n\nAmazon EBS for maximum performance: Amazon Elastic Block Store (EBS) provides high-performance block storage volumes for use with Amazon EC2 instances. This is suitable for the 10 TB of storage with maximum possible I/O performance required for video processing.\n\nAmazon S3 for durable data storage: Amazon Simple Storage Service (S3) is highly durable, scalable, and secure object storage. It's suitable for the 300 TB of very durable storage needed for storing media content.\n\nAmazon S3 Glacier for archival storage: Amazon S3 Glacier provides secure, durable, and low-cost storage for data archiving and long-term backup. It's ideal for the 900 TB of storage required for archival media that is not in use anymore."},{"timestamp":"1708727400.0","comment_id":"1157541","upvote_count":"2","content":"the answer is D\nFrom a maximum I/O perspective, Amazon EBS is significantly better than Amazon EC2 instance store for two main reasons:\n\n1. Underlying Storage Technology:\n\nAmazon EBS: Utilizes Solid State Drives (SSDs) and NVMe storage options, offering high-performance IOPS (Input/Output Operations Per Second) and throughput.\nAmazon EC2 Instance Store: Relies on the local hard disk drives (HDDs) attached to the EC2 instance, which have significantly lower IOPS and throughput compared to SSDs.","poster":"henna2024"},{"comment_id":"1151839","content":"Selected Answer: D\nInstance store is more than 10T","poster":"vip2","timestamp":"1708073940.0","upvote_count":"2"},{"upvote_count":"2","poster":"awsgeek75","comment_id":"1124387","content":"Selected Answer: D\nA sounds good but D is better as the store is physically attached to the instance!","comments":[{"poster":"awsgeek75","comment_id":"1124388","upvote_count":"2","timestamp":"1705426140.0","content":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"}],"timestamp":"1705426080.0"},{"poster":"foha2012","timestamp":"1705376280.0","upvote_count":"4","content":"D - I chose A. But its a trick question. Upon reading Pentium75's Comment, I agree it should be D. However, I would feel more confident if it mentioned \"temporary 10Gb\" in the question...","comment_id":"1123837"},{"poster":"awsgeek75","upvote_count":"2","timestamp":"1704031200.0","content":"Selected Answer: A\n\"D\" is dependant on instance type of hpc6id.32xlarge is 16TB for accelerated computing with NVMe SSD. I will go for \"A\" because it does not depend on EC2 instance as a requirement.","comment_id":"1110619"},{"timestamp":"1703820000.0","content":"Selected Answer: A\nA looks right.","upvote_count":"1","poster":"[Removed]","comment_id":"1108280"},{"timestamp":"1703048160.0","upvote_count":"2","comment_id":"1101233","poster":"SaurabhTiwari1","content":"Selected Answer: A\nA is right"},{"comment_id":"1077889","content":"Selected Answer: D\nvote for D since the demand is asking for maximum I/O while did not specify how durable the performance should be. So D. otherwise more realistic and durable option is A with high I/O performance as well","poster":"Marco_St","upvote_count":"1","timestamp":"1700691540.0"},{"comment_id":"1066684","poster":"Chiznitz","content":"Selected Answer: D\nThe keyword here is \"maximum possible I/O performance\".\nEBS and Ec2 instance store are good options, but EC2 is higher than EBS in terms of I/O performance. Maximum possible is clearly Ec2 instance storage.\nThere are some concerns about the 10TB needed, however, storage optimized Ec2 instance stores can take up to 24 x 13980 GB (ie 312 TB)\nSo option D is the winner here.","timestamp":"1699559940.0","upvote_count":"3"},{"comments":[{"upvote_count":"3","timestamp":"1703578500.0","comment_id":"1105813","poster":"pentium75","content":"Huh? \"Instance store\" is the name of the service, and it's what provides \"maximum I/O performance possible\", WAY above what EBS can."}],"comment_id":"1059219","upvote_count":"1","poster":"Azure55","timestamp":"1698787920.0","content":"Selected Answer: A\nwell! read option D again, it says EC2 Instance, not EC2 Instances! \nso the answer is obviously A."},{"upvote_count":"2","poster":"tom_cruise","comment_id":"1058178","timestamp":"1698690900.0","content":"Selected Answer: D\n\"An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content. It can also be used to store temporary data that you replicate across a fleet of instances, such as a load-balanced pool of web servers.\"\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"},{"comments":[{"poster":"foha2012","comment_id":"1123838","upvote_count":"1","timestamp":"1705376460.0","content":"I agree with you. However, I would have felt more comfortabe if the question had \"Temporary 10GB\" in the question. I chose A first but Now I agree with you and I think it should be D."}],"poster":"Ruffyit","content":"We are talkimng about storage here and EC2 instance store in not a viable solution for for a storage.","upvote_count":"1","timestamp":"1698638700.0","comment_id":"1057337"},{"poster":"aptx4869","timestamp":"1698605940.0","content":"Selected Answer: A\nWe are talkimng about storage here and EC2 instance store in not a viable solution for for a storage.","upvote_count":"1","comments":[{"poster":"pentium75","content":"Why would a \"store\" not be \"a viable solution for storage\"? Nothing says the 10 TB must be durable. We need maximum I/O, which only instance store provides.","timestamp":"1703578560.0","upvote_count":"1","comment_id":"1105815"}],"comment_id":"1057086"},{"comments":[],"content":"Selected Answer: A\ndude literally EC2 instances storage systems are based on EBS volumes, who it would be more efficient to use an instance, than use a service that is meant for that job. \"C\" and \"D\" are simply not cost-efficient.","timestamp":"1697447040.0","upvote_count":"1","comment_id":"1044800","poster":"David_Ang"},{"poster":"BrijMohan08","timestamp":"1694902500.0","upvote_count":"2","content":"Selected Answer: D\n10tb, good enough for EC2\n10 TB required only for processing -> Temp memory\n\nFor durable storage s3 is a perfect fit in this scenario.","comment_id":"1009354"},{"timestamp":"1692206760.0","content":"Selected Answer: D\nThe best set of services to meet the storage requirements are:\n\nD) Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\n\nThe rationale is:\n\nEC2 instance store provides the highest performance storage for I/O intensive video processing.\nS3 provides durable, scalable object storage for the media content library.\nGlacier provides the lowest cost archival storage for media no longer in active use.\nEBS volumes don't offer the IOPS needed for video processing.\nEFS file storage isn't as durable or cost effective for large media libraries as S3.\nBy matching each storage need with the optimal storage service - EC2, S3, Glacier - this combination meets the performance, durability, and cost requirements for each storage use case.","upvote_count":"8","comment_id":"982813","poster":"Guru4Cloud"},{"comment_id":"978169","poster":"JummyFash","timestamp":"1691715720.0","upvote_count":"1","content":"Option B suggests using Amazon EFS for durable data storage. While Amazon EFS is a managed file storage service, it may not provide the same level of performance and cost-effectiveness as Amazon EBS for maximum I/O performance.\n\nOptions C and D suggest using Amazon EC2 instance store, which is ephemeral storage that is directly attached to an EC2 instance. While it can provide high I/O performance, it is not as durable as Amazon EBS or Amazon S3 and does not meet the durability requirements for long-term data storage.\n\nTherefore, option A is the most suitable recommendation to meet the specified storage requirements for the media company."},{"timestamp":"1689564300.0","comments":[{"upvote_count":"1","timestamp":"1703578620.0","comment_id":"1105817","poster":"pentium75","content":"Who says that the 10 TB must be persistent?"}],"comment_id":"953800","upvote_count":"2","content":"A because we need at least 10TB of storage (Persistent) with max I/O, as instance storage is not persistent so that is why it is out of picture, otherwise answer should be D","poster":"vikashverma93"},{"poster":"MNotABot","content":"D\nI will go for D as here we need max I/O:\nAmazon EC2 Instance Store is suited for temporary storage needs where high performance and low latency are critical. Amazon EBS, on the other hand, is ideal for long-term data storage with better durability and accessibility features.","timestamp":"1689257520.0","comment_id":"950733","upvote_count":"2"},{"upvote_count":"3","poster":"cookieMr","timestamp":"1687504860.0","content":"Selected Answer: D\nOption D is the recommended solution. Amazon EC2 instance store provides maximum performance for video processing, offering local, high-speed storage that is directly attached to the EC2 instances. Amazon S3 is suitable for durable data storage, providing the required capacity of 300 TB for storing media content. Amazon S3 Glacier serves as a cost-effective solution for archival storage, meeting the requirement of 900 TB of archival media storage.\n\nOption A suggests using Amazon EBS for maximum performance, but it may not deliver the same level of performance as instance store for I/O-intensive workloads.\n\nOption B recommends Amazon EFS for durable data storage, but it may not provide the required performance for video processing.\n\nOption C suggests using Amazon EC2 instance store for maximum performance and Amazon EFS for durable data storage, but instance store may not offer the durability and scalability required for the storage needs of the media company.","comment_id":"931304"},{"content":"The instance i4g has capacity 15TB\n\nhttps://aws.amazon.com/es/ec2/instance-types/","timestamp":"1690227720.0","comment_id":"961994","poster":"manuelemg2007","upvote_count":"1"},{"comment_id":"885309","content":"Selected Answer: D\nIn terms of speed, instance store can generally offer higher I/O performance and lower latency than EBS, due to the fact that it is physically attached to the host. However, the performance of EBS can be optimized based on the specific use case, by selecting the appropriate volume type, size, and configuration.","poster":"mell1222","timestamp":"1682863860.0","upvote_count":"4"},{"poster":"Ankit_EC_ran","content":"Selected Answer: D\nINstance store gives the best I/O performance","upvote_count":"1","comment_id":"880578","timestamp":"1682436060.0"},{"content":"The keyword here is \"maximum possible I/O performance\". \nEBS and Ec2 instance store are good options, but EC2 is higher than EBS in terms of I/O performance. Maximum possible is clearly Ec2 instance storage. \nThere are some concerns about the 10TB needed, however, storage optimized Ec2 instance stores can take up to 24 x 13980 GB (ie 312 TB) \nSo option D is the winner here.","comment_id":"872218","upvote_count":"4","timestamp":"1681687440.0","poster":"C_M_M"},{"content":"Selected Answer: D\nD of course","upvote_count":"1","poster":"channn","comment_id":"865457","timestamp":"1681039860.0"},{"timestamp":"1680841140.0","poster":"jdr75","upvote_count":"1","content":"Selected Answer: D\nThe instance-storage is a block storage directly attached to the EC2 instance (also has options to be accelerated with fast NVMe (Non-Volatile Memory Express) interface) is ins FASTER than EBS.\nAlso there're types that reach top value of 30 TB.","comment_id":"863468"},{"poster":"TheAbsoluteTruth","content":"Selected Answer: A\nLa opción A es la más adecuada para cumplir con los requisitos establecidos por la empresa de medios. Amazon EBS ofrece el máximo rendimiento de E/S posible y es una opción adecuada para el procesamiento de video, mientras que Amazon S3 es la solución de almacenamiento de datos duradero que puede manejar 300 TB de contenido multimedia. Amazon S3 Glacier es una opción adecuada para el almacenamiento de archivos de medios de archivo que ya no están en uso, y su costo es más bajo que el de Amazon S3. Por lo tanto, la opción A proporcionará la solución de almacenamiento más adecuada para la empresa de medios con una combinación de alto rendimiento, durabilidad y costo eficacia","upvote_count":"2","comment_id":"858994","timestamp":"1680450540.0"},{"comment_id":"855771","upvote_count":"1","timestamp":"1680180960.0","poster":"jaswantn","content":"Instance store backed Instances can't be upgraded; means volumes can be added only at the time of launching. If Instance is accidentally terminated or stopped, all the data is lost. In order to prevent that unto some extent, we need to back up data from Instance store volumes to persistent storage on a regular basis. So, if we are spending more money on Instance store volume and still we have additional responsibility of backing them up on regular basis; no worth. We can use EBS volume type that can provide higher I/O performance."},{"comment_id":"845951","upvote_count":"3","content":"When you want to compare S3 storage and EBS as durable storage types according to the maximum IOPS, you will see that s3 is better than EBS based on storage-optimized values.\nExp: Whereas EBS has 40000 max IOPS for storage-optimized value, EC2 provides you a better option with a max of 2146664 random read and 1073336 write.\nTo get further information, you can visit the below links:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html#compute-ssd-perf\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html\n\nSo my answer is D","poster":"Erbug","timestamp":"1679402880.0"},{"upvote_count":"2","comment_id":"829447","content":"Selected Answer: D\nInstance store for max I/O, S3 for durable storage and Glacier for archival","poster":"Sdraju","timestamp":"1677967020.0"},{"content":"Selected Answer: A\nThe issue with using an instance store that size seems to be you have to have a specific ami, but paying for an 8xlarge for those extra IO will normally not be a good solution, the question is open as to compute requirments and cost isn't mentioned","timestamp":"1677175740.0","poster":"anthony2021","comment_id":"819526","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nfor valuable, long-term data. Instead, use more durable data storage, such as Amazon S3, Amazon EBS, or Amazon EFS.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html","poster":"bdp123","timestamp":"1677116760.0","comment_id":"818644"},{"timestamp":"1676930760.0","upvote_count":"1","comment_id":"815932","content":"---Chat GTP-----\nThere are several Amazon EC2 instance types that support 30 TB of instance store volume storage. The specific instance types available may vary depending on the AWS region. Here are a few examples of EC2 instance types that support 30 TB of instance store:\n\n i3en.24xlarge: This instance type is part of the I3en family of instances and provides 24 vCPUs, 96 GiB of memory, and 30.5 TB of NVMe SSD instance store. It is optimized for high-performance workloads and applications that require large amounts of storage, such as data warehousing, Hadoop, and NoSQL databases.","poster":"SmartDude"},{"content":"Selected Answer: A\nA & D looks most close. But in question it never gives a clue for temporary storage as AWS EC2 instance store is \" An instance store provides temporary block-level storage for your instance\" Hence I will choose A as per my understanding. Pls correct if I am wrong.\nRef#https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html","comment_id":"791159","upvote_count":"4","timestamp":"1674944940.0","poster":"Vicky_2023"},{"poster":"LuckyAro","comment_id":"786073","upvote_count":"3","timestamp":"1674527340.0","content":"Selected Answer: A\nEBS is more durable than Instance store, I don't think anyone would risk that much data on a non-durable storage system."},{"comment_id":"778345","upvote_count":"1","timestamp":"1673907840.0","content":"A, Amazon EBS for high I/O compute performance","poster":"mackeda"},{"upvote_count":"2","comment_id":"777658","poster":"Ello2023","content":"Selected Answer: A\nA. It says \"The company needs at least 10 TB of STORAGE with the MAXIMUM possible I/O performance for video processing\" for high performance it is instance store but the risk is that instance storage is ephemeral, if anything happens than than 10TB of storage is lost. There is no High Availability. Where as EBS has HA and use IO2 to maximise performance. \nCorrect me if i am wrong.","timestamp":"1673873820.0"},{"comment_id":"763094","upvote_count":"2","timestamp":"1672558440.0","comments":[{"comment_id":"763095","timestamp":"1672558440.0","poster":"Zerotn3","upvote_count":"2","content":"Amazon Elastic Compute Cloud (EC2) instance store is a temporary storage option that is located on the same physical hardware as the EC2 instance. It is designed to provide high performance for workloads that require the lowest possible latency, such as video processing. However, instance store data is not persisted when the EC2 instance is stopped or terminated, so it is not a good fit for storing data that needs to be persisted long-term."}],"content":"Selected Answer: A\nAmazon Elastic Block Store (EBS) is a service that provides raw block-level storage for Amazon Elastic Compute Cloud (EC2) instances. It is designed to provide high performance for workloads that require the lowest possible latency, such as video processing.","poster":"Zerotn3"},{"upvote_count":"2","poster":"mp165","timestamp":"1672324620.0","content":"I was going A....but after reading this. EC2 has newer feature to support video\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/instance-store-vs-ebs/","comment_id":"761128"},{"upvote_count":"2","timestamp":"1672207800.0","content":"Selected Answer: D\nThe correct answer is D. To meet the requirements, the solutions architect should recommend using Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.\n\nAmazon EC2 is a good fit for the requirement of 10 TB of storage with the maximum possible I/O performance for video processing.\n\nAmazon S3 is a good fit for the requirement of 300 TB of very durable storage for storing media content.\n\nAmazon S3 Glacier is a good fit for the requirement of 900 TB of storage to meet the requirements for archival media that is not in use anymore.","poster":"Buruguduystunstugudunstuy","comment_id":"759427"},{"content":"Selected Answer: D\nMax Instance Store is 30 TB ,so our requirment is getting fulfilled here.Instance store will give high iops,COMPARE to EBS.","upvote_count":"2","poster":"techhb","timestamp":"1672075680.0","comment_id":"757690"},{"timestamp":"1672034520.0","content":"Selected Answer: A\nA solutions architect should recommend Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.\n\nAmazon EBS is a block storage service that provides high I/O performance for applications such as video processing. It is suitable for the company's requirement of at least 10 TB of storage with the maximum possible I/O performance.\n\nAmazon S3 is a durable object storage service that can store unlimited amounts of data with 99.999999999% durability. It is suitable for the company's requirement of 300 TB of very durable storage for storing media content.\n\nAmazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 storage class for data archiving and long-term backup. It is suitable for the company's requirement of 900 TB of storage for archival media that is not in use anymore.","upvote_count":"3","comment_id":"757100","poster":"muhtoy"},{"timestamp":"1671394620.0","poster":"career360guru","comment_id":"749148","upvote_count":"2","content":"Selected Answer: D\nOption D - You will have to select right instance type that can support 10TB of instance store."},{"timestamp":"1669066380.0","poster":"Wpcorgan","comment_id":"723949","upvote_count":"2","content":"D is corect"},{"timestamp":"1668257940.0","comment_id":"716688","content":"Selected Answer: D\nAnswer: D","upvote_count":"1","poster":"mabotega"},{"timestamp":"1667100240.0","upvote_count":"1","content":"Selected Answer: D","comment_id":"707588","poster":"dokaedu"},{"timestamp":"1666801260.0","upvote_count":"8","comment_id":"704858","poster":"Six_Fingered_Jose","content":"Selected Answer: D\nagree with D, since it is only used for video processing instance store should be the fastest here (being ephemeral shouldnt be an issue because they are moving the data to S3 after processing)"},{"poster":"tubtab","comment_id":"704447","timestamp":"1666766280.0","upvote_count":"2","content":"Selected Answer: D\ndddddd"},{"content":"The answer is D","comments":[{"content":"wrong ! it is A, as there is 10TB needed:\ntaken from: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html\n\nA block device mapping always specifies the root volume for the instance. The root volume is either an Amazon EBS volume or an instance store volume. For more information, see Storage for the root device. The root volume is mounted automatically. For instances with an instance store volume for the root volume, the size of this volume varies by AMI, but the maximum size is 10 GB.","comment_id":"695556","comments":[{"content":"It is possible to add an instance store volume up to 30TB at this time. The question doesn't say the instance store has to be the root volume. For instance stores, the root volume max size is 10GB but you can add instance stores up to 30TB.\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-volumes","timestamp":"1666187880.0","upvote_count":"4","comments":[{"content":"D is correct","poster":"17Master","comment_id":"742289","timestamp":"1670808780.0","upvote_count":"1"}],"poster":"Sauran","comment_id":"699081"}],"upvote_count":"2","timestamp":"1665854700.0","poster":"brushek"}],"timestamp":"1665710820.0","poster":"Lilibell","comment_id":"694426","upvote_count":"2"}],"answers_community":["D (58%)","A (42%)","1%"],"answer_ET":"D","exam_id":31,"timestamp":"2022-10-14 03:27:00","url":"https://www.examtopics.com/discussions/amazon/view/85432-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":52,"answer":"D"},{"id":"CpyJJSAOTcduqJ7Q2SiE","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/85404-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2022-10-13 17:24:00","unix_timestamp":1665674640,"answers_community":["B (74%)","A (25%)","1%"],"exam_id":31,"isMC":true,"question_text":"A company wants to run applications in containers in the AWS Cloud. These applications are stateless and can tolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost and operational overhead.\nWhat should a solutions architect do to meet these requirements?","answer_ET":"B","answer":"B","choices":{"A":"Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.","C":"Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.","D":"Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.","B":"Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group."},"topic":"1","question_images":[],"question_id":53,"answer_images":[],"discussion":[{"comments":[{"comment_id":"1299831","comments":[{"timestamp":"1729606440.0","upvote_count":"1","content":"When you use EKS is add operational over head, if ECS was in option then it could have justified here","comment_id":"1301618","poster":"19d92c7"}],"upvote_count":"4","timestamp":"1729293240.0","poster":"Tsige","content":"Using Amazon EKS with managed node groups simplifies container orchestration by reducing the operational overhead of managing the underlying infrastructure and Kubernetes control plane. EKS automatically handles tasks like patching, scaling, and deploying containers across multiple nodes, further reducing operational effort.\nWhile Spot Instances in an EC2 Auto Scaling group can reduce costs, it requires more manual management of container orchestration (e.g., deploying, scaling, and managing containers across instances), increasing operational overhead compared to EKS.\nIn the exam its highly recommended to select Managed services Like EKS in this case.\nMy choice is B."},{"comment_id":"1197968","content":"Really ??? \nYou want more configuration efforts for container workload on EC2, instead of using EKS ::))","poster":"EMPERBACH","timestamp":"1713448920.0","upvote_count":"7"},{"comment_id":"914007","upvote_count":"10","poster":"Lalo","timestamp":"1685839860.0","content":"USING SPOT INSTANCES WITH EKS\nhttps://ec2spotworkshops.com/using_ec2_spot_instances_with_eks.html"},{"upvote_count":"6","comment_id":"952262","timestamp":"1689416760.0","poster":"MutiverseAgent","content":"In my opinion option A) seems to be a reasonable at first because setting up AWS EKS might be seem as an operation overhead comparing to the option of running the containers inside the EC2 using docker just as you we do on your own machines. However, consider installing docker on multiple EC2 instances and manually manage docker instances and images will end up in chaos, so, as a conclusion, the operational cost of setting up AWS EKS will worth the effort."},{"content":"option A is the worst option in terms of operational overhead ... you have to install your own kubernetes cluster!!! B is a more suitable option","timestamp":"1684673580.0","comments":[{"upvote_count":"1","comment_id":"952259","poster":"MutiverseAgent","timestamp":"1689416340.0","content":"you do not necessary need to install K8S, in terms of plain containers you can run them using docker just as you do on your own machine."}],"poster":"ruqui","upvote_count":"6","comment_id":"903229"}],"content":"Selected Answer: A\nRequirement is \"minimizes cost and operational overhead\"\nA is better option than B as EKS add additional cost and operational overhead.","poster":"bgsanata","timestamp":"1684045440.0","upvote_count":"18","comment_id":"897310"},{"comment_id":"831915","poster":"GalileoEC2","timestamp":"1678194240.0","upvote_count":"10","content":"Answer is A:\nAmazon ECS: ECS itself is free, you pay only for Amazon EC2 resources you use.\nAmazon EKS: The EKS management layer incurs an additional cost of $144 per month per cluster.\nAdvantages of Amazon ECS include: Spot instances: Because containers are immutable, you can run many workloads using Amazon EC2 Spot Instances (which can be shut down with no advance notice) and save 90% on on-demand instance costs."},{"timestamp":"1738205940.0","comment_id":"1348887","upvote_count":"1","poster":"zdi561","content":"Selected Answer: A\nai response \"Generally, an ECS EC2 Auto Scaling Group is considered cheaper than an EKS Managed Node Group because with ECS, you only pay for the EC2 instances used to run your containers, while EKS adds an additional cost per cluster hour on top of the compute costs for the managed nodes\""},{"timestamp":"1738196700.0","comment_id":"1348836","content":"Selected Answer: B\nDefinitely B. running containers on spot EC2 instances means there has to be additional configuration requirements every time an instance comes up. That means making a custom AMI with prepping the spot instance to launch the container = additional operational overhead in maintaining the AMI - with EKS, that overhead is eliminated.","upvote_count":"2","poster":"Dharmarajan"},{"upvote_count":"2","content":"Selected Answer: B\nSpot Instances are the cheapest and most suitable here along with EKS for automation and thereby reducing operational overhead.","poster":"satyaammm","comment_id":"1337922","timestamp":"1736337180.0"},{"timestamp":"1726662060.0","poster":"PaulGa","upvote_count":"1","comment_id":"1285715","content":"Selected Answer: A\nAns A - hint: \"stateless\" and 'don't care' infrastructure - so Spot and containerisation"},{"poster":"huaze_lei","content":"Spot instance is definitely the answer for interruptible process. It's between A and B now.\nI would reckon Option B requires lesser operational overheads than to maintain own fleet of EC2 servers with containers.","timestamp":"1725540120.0","comment_id":"1278928","upvote_count":"1"},{"timestamp":"1725255960.0","upvote_count":"2","poster":"SaurabhTiwari1","content":"Selected Answer: B\nB is correct","comment_id":"1276428"},{"timestamp":"1718881260.0","poster":"ChymKuBoy","content":"Selected Answer: B\nB for sure","comment_id":"1233585","upvote_count":"2"},{"upvote_count":"4","poster":"Anthony_Rodrigues","timestamp":"1716976800.0","comment_id":"1220800","content":"IMHO, the option **B** is the right one.\nBreaking down the reasons for it:\n1 - Spot is much cheaper than on-demand, which already eliminates C&D for cost related.\n2 - Even though we can create a bootstrap script to install docker, managing this can be complicated, especially if any of the applications require more than one instance running."},{"comment_id":"1219826","upvote_count":"3","timestamp":"1716840360.0","content":"Container in ec2 or container on a container platform? \nB","poster":"lofzee"},{"upvote_count":"3","timestamp":"1713448800.0","poster":"EMPERBACH","comment_id":"1197966","content":"Selected Answer: B\nrun applications in containers -> Container service not EC2 (no operational overhead to config container workload on EC2)\nSpot instance < On-demand cost"},{"comment_id":"1151842","poster":"vip2","upvote_count":"2","content":"Selected Answer: A\nDoes A implicitly means run spot Instance on ECS?","timestamp":"1708074480.0"},{"comment_id":"1110628","poster":"awsgeek75","upvote_count":"2","timestamp":"1704031800.0","content":"Selected Answer: B\nSpot instances for disruption friendly containers which are also cheaper.\nEKS allows using spot instances from a managed node group that takes away the EC2 operational overhead.\nLink: https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-provisioning-and-managing-ec2-spot-instances-in-managed-node-groups/\n\"Previously, customers had to run Spot Instances as self-managed worker nodes in their EKS clusters. This meant doing some heavy lifting such as building and maintaining configuration for Spot Instances in EC2 Auto Scaling groups, deploying a tool for handling Spot interruptions gracefully, deploying AMI updates, and updating the kubelet version running on their worker nodes. Now, all you need to do is supply a single parameter to indicate that a managed node group should launch Spot Instances, and provide multiple instance types that would be used by the underlying EC2 Auto Scaling group.\""},{"poster":"pipici","comment_id":"1069743","content":"Selected Answer: A\nA has less operational overhead","timestamp":"1699908480.0","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: B\nrunning containers without container service like EKS introduce huge operational effort","poster":"xdkonorek2","comment_id":"1062783","timestamp":"1699181940.0"},{"comment_id":"1044804","poster":"David_Ang","timestamp":"1697447220.0","content":"Selected Answer: B\ndude always if you have a service that is meant to be used for a job there is the correct answer, is logic.","upvote_count":"7"},{"timestamp":"1697042520.0","content":"Selected Answer: B\nIt is a lot of work to manage docker environment on ec2 instance by yourself.","upvote_count":"3","comment_id":"1040912","poster":"tom_cruise"},{"timestamp":"1696595880.0","poster":"poponpo","upvote_count":"1","comment_id":"1026621","content":"Selected Answer: A\nk8s is not easy solution. there are too many to study about it. You have to know about ingress, storageclass, cni, namesapce, etc... they make burdened to operate."},{"upvote_count":"1","content":"Selected Answer: A\nreponse A","comment_id":"1020188","timestamp":"1695928740.0","poster":"Modulopi"},{"timestamp":"1693541880.0","comment_id":"995658","poster":"TariqKipkemei","upvote_count":"5","content":"Selected Answer: B\nMinimize costs = Spot instances\nMinimize operational overhead = Amazon EKS is a managed Kubernetes service that makes it easy for you to run Kubernetes on AWS and on-premises.\n\nhttps://aws.amazon.com/pm/eks/?trk=c69c708c-c423-4c07-9fc8-513781540cc7&sc_channel=ps&ef_id=Cj0KCQjw9MCnBhCYARIsAB1WQVWD7pSyGgjzsk6QHMNAIZrHvuAzZd4cy9b4QAaCcB5QTn6MR_czhWkaAm6UEALw_wcB:G:s&s_kwcid=AL!4422!3!669047416746!e!!g!!eks!20433874212!155230227787#:~:text=is%20Amazon%20EKS%3F-,Amazon%20EKS,-is%20a%20managed\n\nI would not try to overthink this."},{"comment_id":"982817","timestamp":"1692207000.0","poster":"Guru4Cloud","upvote_count":"4","content":"Selected Answer: B\nThe key reasons are:\n\nUsing Spot Instances reduces EC2 costs significantly compared to On-Demand.\nEKS managed node groups simplify running and scaling containerized applications vs self-managed Kubernetes.\nSince the applications are stateless and fault-tolerant, intermittent Spot interruptions are acceptable.\nThe combination of Spot + EKS provides the most cost-efficient infrastructure with minimal operational overhead.\nOptions A, C and D either use On-Demand instances or self-managed infrastructure, which increases costs and overhead."},{"content":"to run application with minimum cost, use spot instances and to reduce operational overhead, run it on EKS. \nHence B should be right answer.","upvote_count":"2","timestamp":"1688267640.0","poster":"aadityaravi8","comment_id":"940494"},{"poster":"cookieMr","content":"Selected Answer: B\nOption B is the recommended solution. Using Spot Instances within an Amazon EKS managed node group allows you to run containers in a managed Kubernetes environment while taking advantage of the cost savings offered by Spot Instances. Spot Instances provide access to spare EC2 capacity at significantly lower prices than On-Demand Instances. By utilizing Spot Instances in an EKS managed node group, you can reduce costs while maintaining high availability for your stateless applications.\n\nOption A suggests using Spot Instances in an EC2 Auto Scaling group, which is a valid approach. However, utilizing Amazon EKS provides a more streamlined and managed environment for running containers.\n\nOptions C and D suggest using On-Demand Instances, which would provide stable capacity but may not be the most cost-effective solution for minimizing costs, as On-Demand Instances typically have higher prices compared to Spot Instances.","comment_id":"931306","timestamp":"1687505100.0","upvote_count":"6"},{"comment_id":"906453","poster":"Abrar2022","content":"There are no additional costs to use Amazon EKS managed node groups. You only pay for the AWS resources that you provision.","upvote_count":"4","timestamp":"1684997100.0"},{"upvote_count":"4","comment_id":"859002","timestamp":"1680451080.0","poster":"TheAbsoluteTruth","content":"Selected Answer: B\nLa opción B es la mejor para cumplir con los requisitos de minimización de costos y gastos generales operativos mientras se ejecutan contenedores en la nube de AWS. Amazon EKS es un servicio de orquestación de contenedores altamente escalable y de alta disponibilidad que se encarga de administrar y escalar automáticamente los nodos de contenedor subyacentes. El uso de instancias de spot en un grupo de nodos administrados de Amazon EKS ayudará a reducir los costos en comparación con las instancias bajo demanda, ya que las instancias de spot son instancias de EC2 disponibles a precios significativamente más bajos, pero pueden ser interrumpidas con poco aviso. Al aprovechar la capacidad no utilizada de EC2 a un precio reducido, la empresa puede ahorrar dinero en costos de infraestructura sin comprometer la tolerancia a fallos o la escalabilidad de sus aplicaciones en contenedores."},{"poster":"alexiscloud","upvote_count":"1","content":"B: Sport instance save cost","comment_id":"855275","timestamp":"1680153000.0"},{"timestamp":"1678685700.0","upvote_count":"1","comment_id":"837659","comments":[{"content":"Spot instances are the correct option for this case.","poster":"Robrobtutu","comment_id":"872963","upvote_count":"1","timestamp":"1681755420.0"}],"content":"Selected Answer: D\nThe answer should be D. Spot instance is not good option at all. The question say \"...can tolerate disruptions\" this doesn't mean it can run at random time intervals.","poster":"bgsanata"},{"poster":"Sdraju","comment_id":"829450","content":"Selected Answer: B\nSpot instances for cost optimisation and Kubernetes for container management","upvote_count":"2","timestamp":"1677967320.0"},{"upvote_count":"5","poster":"Zerotn3","content":"Selected Answer: B\nA and B are working. but requirements have \"operational overhead\". EKS would allow the company to use Amazon EKS to manage the containerized applications.","timestamp":"1672559880.0","comment_id":"763101"},{"content":"Selected Answer: B\nThe correct answer is B. To minimize cost and operational overhead, the solutions architect should use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group to run the application containers.\n\nAmazon EKS is a fully managed service that makes it easy to run Kubernetes on AWS. By using a managed node group, the company can take advantage of the operational benefits of Amazon EKS while minimizing the operational overhead of managing the Kubernetes infrastructure. Spot Instances provide a cost-effective way to run stateless, fault-tolerant applications in containers, making them a good fit for the company's requirements.","upvote_count":"6","comment_id":"759428","timestamp":"1672207860.0","poster":"Buruguduystunstugudunstuy"},{"poster":"JayBee65","upvote_count":"6","timestamp":"1671626160.0","content":"Running your Kubernetes and containerized workloads on Amazon EC2 Spot Instances is a great way to save costs. ... AWS makes it easy to run Kubernetes with Amazon Elastic Kubernetes Service (EKS) a managed Kubernetes service to run production-grade workloads on AWS. To cost optimize these workloads, run them on Spot Instances. https://aws.amazon.com/blogs/compute/cost-optimization-and-resilience-eks-with-spot-instances/","comment_id":"752285"},{"content":"Selected Answer: B\nOption B","upvote_count":"1","poster":"career360guru","comment_id":"749150","timestamp":"1671394740.0"},{"content":"B. Use Spot Instances - Supports Disruption ( stop and start at anytime)\nElastic Kubernetes Service (Amazon EKS) managed node group - Supports containerized application.","upvote_count":"2","timestamp":"1670868300.0","poster":"Qjb8m9h","comment_id":"743143"},{"timestamp":"1670377920.0","upvote_count":"3","content":"why not A, EC2 can run container with lower cost than EKS...","comment_id":"737358","poster":"jiemin","comments":[{"comment_id":"752281","timestamp":"1671625980.0","poster":"JayBee65","upvote_count":"3","content":"There are no additional costs to use Amazon EKS managed node groups, you only pay for the AWS resources you provision, so I disagree"}]},{"comment_id":"723950","timestamp":"1669066440.0","poster":"Wpcorgan","content":"B is correct","upvote_count":"1"},{"content":"Selected Answer: B\nThis should explain\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html","upvote_count":"5","timestamp":"1668652260.0","poster":"study_aws1","comment_id":"720145"},{"comment_id":"716692","content":"Selected Answer: B\nAnswer B","poster":"mabotega","timestamp":"1668258120.0","upvote_count":"1"},{"timestamp":"1666801380.0","upvote_count":"2","content":"Selected Answer: B\nagreed with B cause container","comment_id":"704859","poster":"Six_Fingered_Jose"},{"upvote_count":"1","comment_id":"704448","poster":"tubtab","timestamp":"1666766580.0","content":"Selected Answer: B\nbbbbbb"},{"poster":"Lilibell","comment_id":"694433","upvote_count":"1","content":"The answer is B","timestamp":"1665711000.0"},{"content":"Selected Answer: B\nit should be B:\n\nhttps://aws.amazon.com/about-aws/whats-new/2020/12/amazon-eks-support-ec2-spot-instances-managed-node-groups/","poster":"brushek","timestamp":"1665674640.0","comment_id":"694034","upvote_count":"7"}]},{"id":"d5pCLXADsyP9LYQzYQEI","answer_images":[],"answer_ET":"AE","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/86658-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"AE","unix_timestamp":1667107680,"answer_description":"","isMC":true,"question_id":54,"choices":{"A":"Migrate the PostgreSQL database to Amazon Aurora.","C":"Set up an Amazon CloudFront distribution for the web application content.","D":"Set up Amazon ElastiCache between the web application and the PostgreSQL database.","E":"Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).","B":"Migrate the web application to be hosted on Amazon EC2 instances."},"question_text":"A company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure.\nWhich combination of actions should the solutions architect take to accomplish this? (Choose two.)","answers_community":["AE (98%)","2%"],"exam_id":31,"timestamp":"2022-10-30 06:28:00","topic":"1","discussion":[{"upvote_count":"14","content":"Selected Answer: AE\nI would say A and E since Aurora and Fargate are serverless (less operational overhead).","poster":"ArielSchivo","comment_id":"713913","comments":[{"upvote_count":"2","comment_id":"1004589","content":"There’s a difference between Amazon Aurora and Amazon Aurora Serverless","poster":"baba365","timestamp":"1694425200.0","comments":[{"upvote_count":"4","content":"\"Aurora serverless\" is still a flavor of Aurora, it's not a different product.","timestamp":"1703578920.0","poster":"pentium75","comment_id":"1105821"}]}],"timestamp":"1667919480.0"},{"poster":"Manimgh","upvote_count":"1","content":"Selected Answer: AE\nAurora and Fargate are the answers","timestamp":"1744357200.0","comment_id":"1559808"},{"timestamp":"1726662480.0","content":"Selected Answer: AE\nAns A,E - \n-PostgreSQL is compatible with Aurora\n-Fargate for container service \nBoth services are serverless","poster":"PaulGa","upvote_count":"2","comment_id":"1285718"},{"comment_id":"1253524","timestamp":"1721723940.0","upvote_count":"2","content":"Selected Answer: AE\nA and E since both Aurora and fargate are serverless.","poster":"jaradat02"},{"upvote_count":"2","poster":"JTruong","comment_id":"1109260","content":"Selected Answer: AE\nPostgreSQL is compatible w/ Aurora\nFargate & ECS are also paired with containers \nA&E","timestamp":"1703893500.0"},{"upvote_count":"3","content":"I would say A and E since Aurora and Fargate are serverless (less operational overhead)","poster":"Ruffyit","comment_id":"1057341","timestamp":"1698639780.0"},{"comment_id":"995675","content":"Selected Answer: AE\nRequirement is to reduce operational overhead, \nAmazon Aurora provides built-in security, continuous backups, serverless compute, up to 15 read replicas, automated multi-Region replication.\nAWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers.","poster":"TariqKipkemei","timestamp":"1693542840.0","upvote_count":"4"},{"upvote_count":"2","comment_id":"982822","content":"Selected Answer: AE\nThe reasons are:\n\nMigrating the database to Amazon Aurora provides a high performance, scalable PostgreSQL-compatible database with minimal overhead.\nMigrating the containerized web app to Fargate removes the need to provision and manage EC2 instances. Fargate auto-scales.\nTogether, Aurora and Fargate reduce operational overhead and complexity for the data and application tiers.","poster":"Guru4Cloud","timestamp":"1692207600.0"},{"timestamp":"1687505340.0","upvote_count":"3","poster":"cookieMr","comment_id":"931310","content":"Selected Answer: AE\nA is the correct answer because migrating the database to Amazon Aurora reduces operational overhead and offers scalability and automated backups.\n\nE is the correct answer because migrating the web application to AWS Fargate with Amazon ECS eliminates the need for infrastructure management, simplifies deployment, and improves resource utilization.\n\nB. Migrating the web application to Amazon EC2 instances would not directly address the operational overhead and capacity planning concerns mentioned in the scenario.\n\nC. Setting up an Amazon CloudFront distribution improves content delivery but does not directly address the operational overhead or capacity planning limitations.\n\nD. Configuring Amazon ElastiCache improves performance but does not directly address the operational overhead or capacity planning challenges mentioned.\n\nTherefore, the correct answers are A and E as they address the requirements, while the incorrect answers (B, C, D) do not provide the desired solutions."},{"upvote_count":"4","comment_id":"893407","timestamp":"1683662820.0","poster":"studynoplay","content":"Selected Answer: AE\nImprove the application's infrastructure = Modernize Infrastructure = Least Operational Overhead = Serverless"},{"comment_id":"872970","upvote_count":"1","poster":"Robrobtutu","content":"Selected Answer: AE\nA and E are the best options.","timestamp":"1681755720.0"},{"upvote_count":"1","timestamp":"1678685820.0","comment_id":"837660","poster":"bgsanata","content":"Selected Answer: AE\nA and E"},{"comment_id":"783907","timestamp":"1674357480.0","content":"Selected Answer: AE\na e..............","poster":"rapatajones","upvote_count":"1"},{"content":"One should that Aurora is not serverless. Aurora serverless and Aurora are 2 Amazon services. I prefer C, however the question does not mention any frontend requirements.","upvote_count":"1","poster":"goodmail","timestamp":"1673708280.0","comment_id":"775548"},{"content":"Selected Answer: AE\nYes, go for A and E since thes two ressources are serverless.","comment_id":"764641","timestamp":"1672750320.0","poster":"aba2s","upvote_count":"3"},{"upvote_count":"2","poster":"Buruguduystunstugudunstuy","comment_id":"759430","timestamp":"1672207920.0","content":"Selected Answer: AE\nThe correct answers are A and E. To improve the application's infrastructure, the solutions architect should migrate the PostgreSQL database to Amazon Aurora and migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).\n\nAmazon Aurora is a fully managed, scalable, and highly available relational database service that is compatible with PostgreSQL. Migrating the database to Amazon Aurora would reduce the operational overhead of maintaining the database infrastructure and allow the company to focus on building and scaling the application.\n\nAWS Fargate is a fully managed container orchestration service that enables users to run containers without the need to manage the underlying EC2 instances. By using AWS Fargate with Amazon Elastic Container Service (Amazon ECS), the solutions architect can improve the scalability and efficiency of the web application and reduce the operational overhead of maintaining the underlying infrastructure."},{"comment_id":"757702","timestamp":"1672076280.0","content":"A and E are obvious choices.","upvote_count":"1","poster":"techhb"},{"timestamp":"1671394920.0","upvote_count":"1","content":"Selected Answer: AE\nOption A and E","poster":"career360guru","comment_id":"749154"},{"upvote_count":"1","comment_id":"746256","content":"Selected Answer: AE\nA and E","poster":"SilentMilli","timestamp":"1671118560.0"},{"poster":"333666999","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"1105822","poster":"pentium75","content":"So you'd move the database from on-premises to AWS Aurora, but leave the application containers on premises, and place Cloud Front in front of them?","timestamp":"1703578980.0"}],"content":"Selected Answer: CE\nC not A. and E","timestamp":"1670975460.0","comment_id":"744560"},{"poster":"Wpcorgan","comment_id":"723953","content":"A and E","timestamp":"1669066500.0","upvote_count":"1"},{"poster":"Nigma","content":"https://www.examtopics.com/discussions/amazon/view/46457-exam-aws-certified-solutions-architect-associate-saa-c02/","upvote_count":"1","comments":[{"comment_id":"713563","poster":"Nigma","timestamp":"1667892600.0","upvote_count":"1","content":"A and E\n\nAurora and serverless"}],"timestamp":"1667892540.0","comment_id":"713561"},{"timestamp":"1667107680.0","upvote_count":"2","comment_id":"707622","content":"Selected Answer: AE\nB(X) E(O) not sure about A,C,D but A looks making sense","poster":"SimonPark"}]},{"id":"KNXtjcPq9X3V9COScVUs","question_images":[],"timestamp":"2022-10-08 15:28:00","topic":"1","exam_id":31,"answer":"A","question_text":"A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"question_id":55,"discussion":[{"upvote_count":"24","comment_id":"689350","content":"Selected Answer: A\nA is correct.","poster":"rein_chau","timestamp":"1665235680.0"},{"timestamp":"1726904760.0","comment_id":"862869","content":"Selected Answer: A\nKeywords: \n- rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions\n- LEAST operational overhead\n\nA: Correct - AWS Secrets Manager supports\n- Encrypt credential for RDS, DocumentDb, Redshift, other DBs and key/value secret.\n- multi-region replication.\n- Remote base on schedule\nB: Incorrect - Secure string parameter only apply for Parameter Store. All the data in AWS Secrets Manager is encrypted\nC: Incorrect - don't mention about replicate S3 across region.\nD: Incorrect - So many steps compare to answer A =))","upvote_count":"14","poster":"PhucVuu"},{"content":"Selected Answer: A\nCapability to rotate secrets and tight coupling with RDS","timestamp":"1740698340.0","comment_id":"1362772","poster":"MundiChor","upvote_count":"1"},{"timestamp":"1739811900.0","poster":"Vandaman","comment_id":"1357879","upvote_count":"1","content":"Selected Answer: A\nNo other explanation necessary"},{"content":"Selected Answer: A\nA is the right answer as it takes minutes to setup. so least operational head","poster":"Mrigraj12","comment_id":"1345996","upvote_count":"1","timestamp":"1737708360.0"},{"poster":"trinh_le","upvote_count":"1","comment_id":"1308196","timestamp":"1730940300.0","content":"Selected Answer: A\nA: correct - the secret manager supports rotating credentials\nB: incorrect - Parameter Store does not perform any cryptographic operations. Instead, it relies on AWS KMS to encrypt and decrypt secure string parameter values\nC and D: incorrect - handles through Lambda, require more operational overhead"},{"timestamp":"1726904820.0","upvote_count":"5","poster":"SilentMilli","comment_id":"767924","content":"Selected Answer: A\nAWS Secrets Manager is a secrets management service that enables you to store, manage, and rotate secrets such as database credentials, API keys, and SSH keys. Secrets Manager can help you minimize the operational overhead of rotating credentials for your Amazon RDS for MySQL databases across multiple Regions. With Secrets Manager, you can store the credentials as secrets and use multi-Region secret replication to replicate the secrets to the required Regions. You can then configure Secrets Manager to rotate the secrets on a schedule so that the credentials are rotated automatically without the need for manual intervention. This can help reduce the risk of secrets being compromised and minimize the operational overhead of credential management."},{"upvote_count":"3","comments":[{"comment_id":"758994","upvote_count":"2","poster":"Buruguduystunstugudunstuy","content":"Option B, storing the credentials as secrets in AWS Systems Manager and using multi-Region secret replication, would not provide automatic rotation of secrets on a schedule. \n\nOption C, storing the credentials in an S3 bucket with SSE enabled and using EventBridge to invoke an AWS Lambda function to rotate the credentials, would not provide automatic rotation of secrets on a schedule. \n\nOption D, encrypting the credentials as secrets using KMS multi-Region customer managed keys and storing the secrets in a DynamoDB global table, would not provide automatic rotation of secrets on a schedule and would require additional operational overhead to retrieve the secrets from DynamoDB and use the RDS API to rotate the secrets.","timestamp":"1672171440.0"}],"timestamp":"1726904820.0","content":"Selected Answer: A\nOption A, storing the credentials as secrets in AWS Secrets Manager and using multi-Region secret replication for the required Regions, and configuring Secrets Manager to rotate the secrets on a schedule, would meet the requirements with the least operational overhead.\n\nAWS Secrets Manager allows you to store, manage, and rotate secrets, such as database credentials, across multiple AWS Regions. By enabling multi-Region secret replication, you can replicate the secrets across the required Regions to allow for seamless rotation of the credentials during maintenance activities. Additionally, Secrets Manager provides automatic rotation of secrets on a schedule, which would minimize the operational overhead of rotating the credentials on a monthly basis.","comment_id":"758992","poster":"Buruguduystunstugudunstuy"},{"timestamp":"1726904760.0","poster":"Musti35","comment_id":"867035","upvote_count":"3","content":"Selected Answer: A\nWith Secrets Manager, you can store, retrieve, manage, and rotate your secrets, including database credentials, API keys, and other secrets. When you create a secret using Secrets Manager, it’s created and managed in a Region of your choosing. Although scoping secrets to a Region is a security best practice, there are scenarios such as disaster recovery and cross-Regional redundancy that require replication of secrets across Regions. Secrets Manager now makes it possible for you to easily replicate your secrets to one or more Regions to support these scenarios."},{"content":"Selected Answer: A\nA. Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate the secrets on a schedule.\n\nThis solution is the best option for meeting the requirements with the least operational overhead. AWS Secrets Manager is designed specifically for managing and rotating secrets like database credentials. Using multi-Region secret replication, you can easily replicate the secrets across the required AWS Regions. Additionally, Secrets Manager allows you to configure automatic secret rotation on a schedule, further reducing the operational overhead.","upvote_count":"1","timestamp":"1726904760.0","poster":"linux_admin","comment_id":"856322"},{"timestamp":"1723377300.0","poster":"PaulGa","comment_id":"1264094","content":"Selected Answer: A\nAns A. What's to debate...?","upvote_count":"1"},{"timestamp":"1720852980.0","comment_id":"1247151","poster":"creamymangosauce","content":"Selected Answer: A\nA - Secrets Manager automates the rotation of secrets for RDS without own implementation required, the options require effort to implement the secret rotation logic","upvote_count":"1"},{"comment_id":"1147930","upvote_count":"1","poster":"ics_911","content":"Selected Answer: A\nA is correct.","timestamp":"1707730620.0"},{"poster":"A_jaa","content":"Selected Answer: A\nAnswer-A","comment_id":"1121621","timestamp":"1705149180.0","upvote_count":"1"},{"comment_id":"1042626","timestamp":"1697197620.0","upvote_count":"1","poster":"gldiazcardenas","content":"Selected Answer: A\nClearly A is the correct one."},{"poster":"TariqKipkemei","comment_id":"965278","content":"Selected Answer: A\n'The company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions' = AWS Secrets Manager","upvote_count":"1","timestamp":"1690518480.0"},{"comment_id":"949997","poster":"miki111","timestamp":"1689178920.0","upvote_count":"1","content":"Option A MET THE REQUIREMENT"},{"content":"Selected Answer: A\nOption A: Storing the credentials as secrets in AWS Secrets Manager provides a dedicated service for secure and centralized management of secrets. By using multi-Region secret replication, the company ensures that the secrets are available in the required Regions for rotation. Secrets Manager also provides built-in functionality to rotate secrets automatically on a defined schedule, reducing operational overhead. This automation simplifies the process of rotating credentials for the Amazon RDS for MySQL databases during monthly maintenance activities.","poster":"cookieMr","comment_id":"926526","upvote_count":"6","timestamp":"1687075080.0"},{"upvote_count":"1","comment_id":"912261","poster":"Bmarodi","timestamp":"1685636040.0","content":"Selected Answer: A\nA is correct answer."},{"comment_id":"814080","upvote_count":"1","content":"Selected Answer: A\nA is correct.","timestamp":"1676811360.0","poster":"cheese929"},{"upvote_count":"3","poster":"BlueVolcano1","timestamp":"1673957640.0","content":"Selected Answer: A\nA","comment_id":"778888"},{"comment_id":"773240","poster":"Abdel42","content":"Selected Answer: A\nit's A, here the question specify that we want the LEAST overhead","upvote_count":"2","timestamp":"1673510280.0"},{"content":"vote A !","comment_id":"757117","upvote_count":"1","poster":"Zerotn3","timestamp":"1672035900.0"},{"timestamp":"1671212100.0","poster":"NikaCZ","content":"Selected Answer: A\nAWS Secret Manager","comment_id":"747456","upvote_count":"1"},{"poster":"ngochieu276","comment_id":"744296","timestamp":"1670951280.0","upvote_count":"1","content":"A is correct"},{"timestamp":"1669034400.0","upvote_count":"1","poster":"Wpcorgan","comment_id":"723477","content":"A is correct"},{"comment_id":"723155","poster":"Megako","content":"Selected Answer: A\nMe Pick A","timestamp":"1669002600.0","upvote_count":"1"},{"timestamp":"1668917820.0","upvote_count":"1","poster":"ABCMail","comment_id":"722378","content":"Selected Answer: A\nAWS secrets manager"},{"comment_id":"707144","content":"Selected Answer: A\nAns is A","poster":"17Master","timestamp":"1667042160.0","upvote_count":"2"},{"comment_id":"697648","upvote_count":"1","timestamp":"1666033920.0","content":"Selected Answer: A\nA is correct","poster":"GameDad09"},{"comment_id":"692604","upvote_count":"1","poster":"BoboChow","timestamp":"1665545520.0","content":"Selected Answer: A\nAWS Secrets Manageris Newer service, meant for storing secrets\nCapability to force rotation of scerets every X days\nIntergraion with Amazon RDS\nSecrets are encypted using KMS"},{"timestamp":"1665407880.0","comment_id":"691171","poster":"D2w","upvote_count":"2","content":"Selected Answer: A\nAWS Secrets Manager meant for storing secrets, Capability to force rotation of secrets every X days, Automate generation of secrets on rotation (uses Lambda), Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)."}],"url":"https://www.examtopics.com/discussions/amazon/view/84728-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"A","unix_timestamp":1665235680,"answer_description":"","answers_community":["A (100%)"],"isMC":true,"choices":{"A":"Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate the secrets on a schedule.","C":"Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the credentials.","D":"Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an Amazon DynamoDB global table. Use an AWS Lambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets.","B":"Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-Region secret replication for the required Regions. Configure Systems Manager to rotate the secrets on a schedule."}}],"exam":{"numberOfQuestions":1019,"isImplemented":true,"provider":"Amazon","isBeta":false,"name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31,"isMCOnly":true,"lastUpdated":"11 Apr 2025"},"currentPage":11},"__N_SSP":true}