{"pageProps":{"questions":[{"id":"17yAKeHZ8hrdV7tVD31d","question_text":"A data engineer is processing and analyzing multiple terabytes of raw data that is in Amazon S3. The data engineer needs to clean and prepare the data. Then the data engineer needs to load the data into Amazon Redshift for analytics.\n\nThe data engineer needs a solution that will give data analysts the ability to perform complex queries. The solution must eliminate the need to perform complex extract, transform, and load (ETL) processes or to manage infrastructure.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","timestamp":"2024-08-07 01:46:00","isMC":true,"answer_ET":"B","unix_timestamp":1722987960,"answer":"B","discussion":[{"comments":[{"content":"It does, the argument is not the best to discard D :\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.Sources.html","timestamp":"1731421680.0","upvote_count":"1","poster":"Asmunk","comment_id":"1310643"}],"poster":"teo2157","upvote_count":"1","comment_id":"1272036","timestamp":"1724567040.0","content":"Selected Answer: B\nIt can´t be D as DMS doesn´t support S3 as a source, it's B as it achieve all the goals described in the subject."},{"comment_id":"1270475","upvote_count":"1","content":"Selected Answer: D\nthe LEAST operational overhead ...","timestamp":"1724298180.0","poster":"seouk"},{"timestamp":"1723384680.0","comment_id":"1264184","upvote_count":"1","content":"Selected Answer: B\nB. They can do the \"complex\" queries in redshift.","poster":"catoteja"},{"timestamp":"1722987960.0","comment_id":"1261866","content":"Option B","upvote_count":"2","poster":"phkhadse"}],"exam_id":21,"choices":{"B":"Use AWS Glue DataBrew to prepare the data. Use AWS Glue to load the data into Amazon Redshift. Use Amazon Redshift to run queries.","A":"Use Amazon EMR to prepare the data. Use AWS Step Functions to load the data into Amazon Redshift. Use Amazon QuickSight to run queries.","C":"Use AWS Lambda to prepare the data. Use Amazon Kinesis Data Firehose to load the data into Amazon Redshift. Use Amazon Athena to run queries.","D":"Use AWS Glue to prepare the data. Use AWS Database Migration Service (AVVS DMS) to load the data into Amazon Redshift. Use Amazon Redshift Spectrum to run queries."},"answer_images":[],"question_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/145188-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","answer_description":"","question_images":[],"answers_community":["B (67%)","D (33%)"]},{"id":"2Rd77GRTCqBJVTIpJGvi","exam_id":21,"unix_timestamp":1722987900,"question_images":[],"answer_images":[],"answers_community":["B (86%)","14%"],"url":"https://www.examtopics.com/discussions/amazon/view/145187-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","discussion":[{"content":"Selected Answer: B\nOption B - VPC Gateway Endpoint for Amazon S3","upvote_count":"6","poster":"ArunRav","comment_id":"1262108","timestamp":"1723036800.0"},{"upvote_count":"5","content":"Option B - VPC Gateway Endpoint for Amazon S3\nWhile interface endpoints is a viable solution, it can be more complex and expensive compared to a gateway endpoint. VPC interface endpoints charge per hour and per gigabyte of data transferred.","poster":"phkhadse","timestamp":"1722987900.0","comment_id":"1261865"},{"poster":"Ashishk1","content":"Selected Answer: C\nThe solution that will meet the requirements of resolving the timeout issues when uploading files from the Lambda function to Amazon S3 buckets in a secure and cost-effective way is C. Create a VPC interface endpoint for Amazon S3. Route network traffic to the VPC interface endpoint .","comment_id":"1271020","timestamp":"1724380020.0","upvote_count":"1"}],"answer_ET":"B","topic":"1","question_id":27,"answer":"B","answer_description":"","isMC":true,"choices":{"A":"Create a NAT gateway in the public subnet of the VPC. Route network traffic to the NAT gateway.","B":"Create a VPC gateway endpoint for Amazon S3. Route network traffic to the VPC gateway endpoint.","D":"Use a VPC internet gateway to connect to the internet. Route network traffic to the VPC internet gateway.","C":"Create a VPC interface endpoint for Amazon S3. Route network traffic to the VPC interface endpoint."},"timestamp":"2024-08-07 01:45:00","question_text":"A company uses an AWS Lambda function to transfer files from a legacy SFTP environment to Amazon S3 buckets. The Lambda function is VPC enabled to ensure that all communications between the Lambda function and other AVS services that are in the same VPC environment will occur over a secure network.\n\nThe Lambda function is able to connect to the SFTP environment successfully. However, when the Lambda function attempts to upload files to the S3 buckets, the Lambda function returns timeout errors. A data engineer must resolve the timeout issues in a secure way.\n\nWhich solution will meet these requirements in the MOST cost-effective way?"},{"id":"IzBIvnb6XUfBu4cUw1gs","exam_id":21,"topic":"1","timestamp":"2024-08-09 08:59:00","question_text":"A company reads data from customer databases that run on Amazon RDS. The databases contain many inconsistent fields. For example, a customer record field that iPnamed place_id in one database is named location_id in another database. The company needs to link customer records across different databases, even when customer record fields do not match.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/145289-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_id":28,"isMC":true,"answer":"B","unix_timestamp":1723186740,"answers_community":["B (100%)"],"answer_ET":"B","answer_description":"","answer_images":[],"discussion":[{"comment_id":"1328353","content":"Selected Answer: B\nAWS Glue Crawler:\nAutomatically discovers the schema and structure of data in the RDS databases, saving significant manual effort.\nCreates a unified data catalog that can be queried or transformed.","timestamp":"1734511680.0","upvote_count":"1","poster":"HagarTheHorrible"},{"poster":"komorebi","timestamp":"1723243680.0","comment_id":"1263249","content":"Selected Answer: B\nAnswer is B","upvote_count":"4"}],"choices":{"D":"Create a provisioned Amazon EMR cluster to process and analyze data in the databases. Connect to the Apache Zeppelin notebook. Use an Apache Spark ML model to find duplicate records in the data. Evaluate and tune the model by evaluating the performance and results.","B":"Create an AWS Glue crawler to craw the databases. Use the FindMatches transform to find duplicate records in the data. Evaluate and tune the transform by evaluating the performance and results.","A":"Create a provisioned Amazon EMR cluster to process and analyze data in the databases. Connect to the Apache Zeppelin notebook. Use the FindMatches transform to find duplicate records in the data.","C":"Create an AWS Glue crawler to craw the databases. Use Amazon SageMaker to construct Apache Spark ML pipelines to find duplicate records in the data."},"question_images":[]},{"id":"ATffmBNIqO3Vhjb7e0c0","exam_id":21,"discussion":[{"upvote_count":"1","content":"Selected Answer: AD\nI have seen this official answer in the practical exam in the AWS Skills builder website","timestamp":"1737832260.0","poster":"Salam9","comment_id":"1346612"},{"timestamp":"1736383560.0","comment_id":"1338145","upvote_count":"1","poster":"kailu","content":"Selected Answer: AB\nD focuses on the S3 prefix structure, which affects partitioning but not the creation of a single table. Consistency in file format and schema is much more important in determining how AWS Glue handles the data."},{"timestamp":"1723243980.0","upvote_count":"1","comment_id":"1263252","content":"Selected Answer: AD\nAnswer is AD","poster":"komorebi"},{"poster":"teo2157","content":"Selected Answer: AD\nTo ensure that the AWS Glue crawler creates only one table and handles the object format, compression type, schema, and prefix structure consistently:\nEnsure Consistent Object Format, Compression Type, Schema, and Prefix Structure\n1. **Consistent Object Format**:\n - Ensure that all objects in the S3 bucket are in the same format (e.g., CSV, JSON, Parquet).\n\n2. **Consistent Compression Type**:\n - Ensure that all objects use the same compression type (e.g., GZIP, Snappy).\n\n3. **Consistent Schema**:\n - Ensure that all objects have the same schema (i.e., the same fields with the same data types).\n\n4. **Consistent Prefix Structure**:\n - Ensure that all objects follow a consistent naming convention and prefix structure in the S3 bucket (e.g., `s3://your-bucket/path/to/data/`).","upvote_count":"3","comment_id":"1262806","timestamp":"1723188300.0"}],"unix_timestamp":1723188300,"timestamp":"2024-08-09 09:25:00","answers_community":["AD (83%)","AB (17%)"],"question_text":"A finance company receives data from third-party data providers and stores the data as objects in an Amazon S3 bucket.\n\nThe company ran an AWS Glue crawler on the objects to create a data catalog. The AWS Glue crawler created multiple tables. However, the company expected that the crawler would create only one table.\n\nThe company needs a solution that will ensure the AVS Glue crawler creates only one table.\n\nWhich combination of solutions will meet this requirement? (Choose two.)","question_id":29,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/145291-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","topic":"1","answer_description":"","isMC":true,"answer":"AD","question_images":[],"choices":{"D":"Ensure that the structure of the prefix for each S3 object name is consistent.","E":"Ensure that all S3 object names follow a similar pattern.","A":"Ensure that the object format, compression type, and schema are the same for each object.","C":"Ensure that the schema is the same for each object. Do not enforce consistency for the file format and compression type of each object.","B":"Ensure that the object format and schema are the same for each object. Do not enforce consistency for the compression type of each object."},"answer_ET":"AD"},{"id":"IzQseynAwdztRgTfMupX","url":"https://www.examtopics.com/discussions/amazon/view/145713-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","question_text":"An application consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue. The application experiences occasional downtime. As a result of the downtime, messages within the queue expire and are deleted after 1 day. The message deletions cause data loss for the application.\n\nWhich solutions will minimize data loss for the application? (Choose two.)","exam_id":21,"timestamp":"2024-08-14 13:45:00","unix_timestamp":1723635900,"answer_images":[],"topic":"1","choices":{"E":"Reduce message processing time.","B":"Increase the visibility timeout.","A":"Increase the message retention period","C":"Attach a dead-letter queue (DLQ) to the SQS queue.","D":"Use a delay queue to delay message delivery"},"isMC":true,"answer_description":"","question_id":30,"question_images":[],"answers_community":["AC (63%)","AE (38%)"],"discussion":[{"poster":"axantroff","content":"Selected Answer: AE\nIn my opinion, A is obvious and one of the two correct answers. Additionally, I checked B, C, and D in more detail, and they basically do not make sense as they do not contribute in any way to handling messages that were just delayed. See the documentation for reference:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\nhttps://aws.amazon.com/what-is/dead-letter-queue/\nSo, only E remains as another valid option. It makes sense because the faster we are able to process events, the less likely we are to violate the expiration policy","comment_id":"1331279","upvote_count":"1","timestamp":"1735084800.0"},{"content":"Selected Answer: AC\nIncreasing the message retention period (A) ensures messages are available longer, while attaching a dead-letter queue (C) allows recovery and reprocessing of unprocessed messages, effectively minimizing data loss.","comment_id":"1328359","timestamp":"1734512040.0","upvote_count":"1","poster":"HagarTheHorrible"},{"comment_id":"1323908","content":"Selected Answer: AE\nIt cannot be C. Messages go to DLQ only if processed. But if the message is not processed at all and it expires, then it will be deleted from the queue.","upvote_count":"2","timestamp":"1733729580.0","poster":"altonh"},{"poster":"aragon_saa","comment_id":"1265754","content":"Selected Answer: AC\nAnswer is AC","upvote_count":"1","timestamp":"1723639680.0"},{"upvote_count":"3","timestamp":"1723635900.0","poster":"matt200","content":"Selected Answer: AC\nTo minimize data loss for the application consuming messages from an Amazon SQS queue, the following two solutions are most effective:\n\nA. Increase the message retention period**: By increasing the message retention period, you ensure that messages remain in the queue for a longer duration before being automatically deleted. This provides more time for the application to recover from downtime and process the messages, thereby reducing the chance of data loss due to message expiration.\n\nC. Attach a dead-letter queue (DLQ) to the SQS queue**: A DLQ can be used to capture messages that cannot be processed successfully. When messages fail to be processed after a certain number of attempts (as defined by the redrive policy), they are moved to the DLQ. This allows you to investigate and handle these messages separately, preventing data loss.","comment_id":"1265671"}],"answer_ET":"AC","answer":"AC"}],"exam":{"name":"AWS Certified Data Engineer - Associate DEA-C01","numberOfQuestions":207,"provider":"Amazon","isBeta":false,"id":21,"isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":true},"currentPage":6},"__N_SSP":true}