{"pageProps":{"questions":[{"id":"1G4p5PetMrt3dCtCTISN","unix_timestamp":1731970260,"exam_id":23,"answers_community":["A (100%)"],"question_id":251,"question_text":"A DevOps administrator is configuring a repository to store a company's container images. The administrator needs to configure a lifecycle rule that automatically deletes container images that have a specific tag and that are older than 15 days.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","choices":{"A":"Create a repository in Amazon Elastic Container Registry (Amazon ECR). Add a lifecycle policy to the repository to expire images that have the matching tag after 15 days.","D":"Create an EC2 Image Builder container recipe. Add a build component to expire the container that has the matching tag after 15 days.","B":"Create a repository in AWS CodeArtifact. Add a repository policy to the CodeArtifact repository to expire old assets that have the matching tag after 15 days.","C":"Create a bucket in Amazon S3. Add a bucket lifecycle policy to expire old objects that have the matching tag after 15 days"},"discussion":[{"poster":"Srikantha","comment_id":"1538283","timestamp":"1743861540.0","content":"Selected Answer: A\nAmazon ECR supports lifecycle policies that automatically manage the retention of container images. You can:\n\nMatch images by tags, including specific values or untagged.\nSpecify age conditions like deleting images older than 15 days.\nSet up the policy once for automatic cleanup, requiring minimal ongoing maintenance.\nThis is the most operationally efficient solution, purpose-built for container image lifecycle management.","upvote_count":"1"},{"upvote_count":"3","comment_id":"1331374","content":"Selected Answer: A\nA. Create a repository in Amazon Elastic Container Registry (Amazon ECR).","poster":"matt200","timestamp":"1735104960.0"},{"upvote_count":"3","comment_id":"1314269","poster":"uncledana","content":"The requirement is to automatically manage container images based on a specific tag and age. Amazon Elastic Container Registry (Amazon ECR) supports lifecycle policies, which are specifically designed to automate the management of image lifecycle events like expiration. Lifecycle policies in ECR allow you to define rules for expiring or retaining images based on criteria like image tags and age, providing an efficient solution with minimal operational overhead.","timestamp":"1731970260.0"}],"answer_description":"","answer_images":[],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151567-exam-aws-certified-devops-engineer-professional-dop-c02/","timestamp":"2024-11-18 23:51:00","isMC":true,"answer":"A","answer_ET":"A","topic":"1"},{"id":"SoGig0qsCyglnx7nwQr9","answers_community":["CD (77%)","BE (15%)","8%"],"answer_ET":"CD","answer_description":"","question_images":[],"choices":{"A":"Create an Amazon CloudWatch log group. Create an AWS CloudTrail trail that writes to the CloudWatch log group.","D":"Create an Amazon CloudWatch dashboard that has a log widget. Configure the widget to display user details from the Redshift logs.","C":"Configure the Redshift cluster database audit logging to include user activity logs. Configure Amazon CloudWatch as the target.","E":"Create an AWS Lambda function that uses Amazon Athena to query the Redshift logs. Create an Amazon CloudWatch dashboard that has a custom widget type that uses the Lambda function.","B":"Create a new Amazon S3 bucket. Configure default audit logging on the Redshift cluster. Configure the S3 bucket as the target."},"answer_images":[],"question_id":252,"exam_id":23,"topic":"1","isMC":true,"answer":"CD","unix_timestamp":1730869800,"url":"https://www.examtopics.com/discussions/amazon/view/150837-exam-aws-certified-devops-engineer-professional-dop-c02/","discussion":[{"comment_id":"1538399","upvote_count":"1","poster":"Srikantha","content":"Selected Answer: BE\nTo track Redshift user activity and query execution, you need to enable Redshift audit logging. This allows you to analyze:\n\nUser connections and disconnections\nSQL queries run\nChanges to database users and privileges\nB. Enable Redshift Audit Logging to S3\n\nRedshift supports audit logging which includes user activity, connection logs, and user changes.\nLogs are delivered to an Amazon S3 bucket, which you can then query using tools like Athena.\nThis is the most efficient and supported way to persist and analyze historical Redshift activity.\nE. Query logs via Athena + Display on CloudWatch Dashboard\n\nOnce Redshift logs are in S3, you can set up an Athena table to query them.\nYou can use an AWS Lambda function to run these queries.\nA custom widget in an Amazon CloudWatch Dashboard can call the Lambda and display the results, such as recent user actions or frequent queries.\nThis enables a near-real-time, visual dashboard with low operational overhead.","timestamp":"1743861840.0"},{"poster":"tdlAws","content":"Selected Answer: BE\nSetting up the default audit log on your Redshift cluster will allow you to record database activity, including user changes and queries performed.\nSpecifying an Amazon S3 bucket as the destination will store the logs for later analysis.\nUsing Amazon Athena to query the logs stored in S3 allows you to create custom queries and extract the relevant data.\nThe Amazon CloudWatch dashboard with a custom AWS Lambda-based widget can display the information directly on the dashboard.","upvote_count":"1","comment_id":"1344858","timestamp":"1737562260.0"},{"content":"Selected Answer: CD\nTo log users queries you need to include user activity logs which require additional setup on the Redshift cluster. So C, because it is not mentioned on B.\nThen to display this logs the only option is D, because E requires the logs to be on S3.","timestamp":"1733308860.0","poster":"pma17","upvote_count":"4","comment_id":"1321814"},{"comment_id":"1317015","poster":"Impromptu","content":"Selected Answer: CD\naudit logging can be sent to cloudwatch logs. So easier to just have them in cloudwatch and make a dashboard","upvote_count":"3","timestamp":"1732448280.0"},{"content":"Selected Answer: BC\nvote BC","upvote_count":"1","poster":"siheom","timestamp":"1732080060.0","comment_id":"1315054"},{"timestamp":"1731970500.0","comment_id":"1314272","poster":"uncledana","content":"B. Create a new Amazon S3 bucket. Configure default audit logging on the Redshift cluster. Configure the S3 bucket as the target.\nE. Create an AWS Lambda function that uses Amazon Athena to query the Redshift logs. Create an Amazon CloudWatch dashboard that has a custom widget type that uses the Lambda function.\n\nExplanation:\n\nTo create a dashboard for viewing changes to Amazon Redshift users and the queries they perform, you need to capture the necessary audit logs and process them into a dashboard-friendly format.","upvote_count":"1"},{"comment_id":"1312474","content":"Selected Answer: CD\nYou need to use CloudWatch for dashboard, so the target must be CloudWatch. B is wrong.\nCloudTrail does not record user queries. A is wrong.\nUse Lambda to create your own solution is not recommended, E is wrong.","timestamp":"1731654120.0","poster":"tinyshare","upvote_count":"3"}],"question_text":"A company uses Amazon Redshift as its data warehouse solution. The company wants to create a dashboard to view changes to the Redshift users and the queries the users perform.\n\nWhich combination of steps will meet this requirement? (Choose two.)","timestamp":"2024-11-06 06:10:00"},{"id":"f0c7ODkOEy9jrrjGWEuh","question_text":"A company uses an organization in AWS Organizations to manage its 500 AWS accounts. The organization has all features enabled. The AWS accounts are in a single OU. The developers need to use the CostCenter tag key for all resources in the organization's member accounts. Some teams do not use the CostCenter tag key to tag their Amazon EC2 instances.\n\nThe cloud team wrote a script that scans all EC2 instances in the organization's member accounts. If the EC2 instances do not have a CostCenter tag key, the script will notify AWS account administrators. To avoid this notification, some developers use the CostCenter tag key with an arbitrary string in the tag value.\n\nThe cloud team needs to ensure that all EC2 instances in the organization use a CostCenter tag key with the appropriate cost center value.\n\nWhich solution will meet these requirements?","topic":"1","exam_id":23,"question_id":253,"choices":{"D":"Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the policy to the OU. Configure an AWS Lambda function that adds an empty CostCenter tag key to an EC2 instance. Create an Amazon EventBridge rule that matches events to the RunInstances API action with the Lambda function as the target.","A":"Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the policy to the OU. Update the script to scan the tag keys and tag values. Modify the script to update noncompliant resources with a default approved tag value for the CostCenter tag key.","C":"Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Create an IAM permission boundary in the organization's member accounts that restricts the CostCenter tag values to a list of valid cost centers.","B":"Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Update the script to scan the tag keys and tag values and notify the administrators when the tag values are not valid."},"question_images":[],"answer_description":"","timestamp":"2024-11-22 21:18:00","discussion":[{"timestamp":"1732306680.0","upvote_count":"5","content":"Selected Answer: A\nService Control Policy (SCP): Creating an SCP ensures that any attempt to create EC2 instances without the CostCenter tag key is denied right from the start. This enforces the requirement at the organizational level.\n\nTag Policy: By creating a tag policy that enforces the CostCenter tag values to be from a known list, you can ensure that only valid cost center values are used across all EC2 instances.\n\nScript Update: Updating the script to not only scan for tag keys and values but also to update noncompliant resources with a default approved tag value ensures compliance and mitigates the issue of arbitrary string values.\n\nComprehensive Solution: This approach addresses both the presence of the CostCenter tag and the correctness of its value, providing a comprehensive solution to the problem.","comment_id":"1316482","poster":"f4b18ba"},{"content":"Selected Answer: A\nSCP to require the CostCenter tag key:\nPrevents users from launching EC2 instances without the required tag.\nTag policy to validate values:\nAWS tag policies can enforce allowed values for tag keys like CostCenter, across all accounts in an OU.\nScript enhancement for compliance:\nScript detects noncompliant resources and applies a default value, maintaining tag integrity and reducing manual intervention.","poster":"Srikantha","timestamp":"1743862200.0","comment_id":"1538542","upvote_count":"1"},{"content":"Selected Answer: A\nThe answer is A","poster":"Changwha","timestamp":"1732358880.0","comment_id":"1316637","upvote_count":"3"}],"answer":"A","answer_ET":"A","answers_community":["A (100%)"],"unix_timestamp":1732306680,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/151850-exam-aws-certified-devops-engineer-professional-dop-c02/","isMC":true},{"id":"0IMWvUB5gfe5tkVwHXWI","question_text":"A DevOps engineer uses a pipeline in AWS CodePipeline. The pipeline has a build action and a deploy action for a single-page web application that is delivered to an Amazon S3 bucket. Amazon CloudFront serves the web application. The build action creates an artifact for the web application.\n\nThe DevOps engineer has created an AWS CloudFormation template that defines the S3 bucket and configures the S3 bucket to host the application. The DevOps engineer has configured a CloudFormation deploy action before the S3 action. The CloudFormation deploy action creates the S3 bucket. The DevOps engineer needs to configure the S3 deploy action to use the S3 bucket from the CloudFormation template.\n\nWhich combination of steps will meet these requirements? (Choose two.)","answer":"BD","answer_ET":"BD","unix_timestamp":1731971700,"timestamp":"2024-11-19 00:15:00","isMC":true,"choices":{"B":"Add an output named BucketName to the CloudFormation template. Set the output's value to refer to the S3 bucket from the CloudFormation template. Set the CloudFormation action's namespace to StackVariables in the pipeline.","E":"Configure the build artifact from the build action and the AWS Systems Manager parameter as the inputs to the deploy action. Configure the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable.","A":"Add an output named BucketName to the CloudFormation template. Set the output's value to refer to the S3 bucket from the CloudFormation template. Configure the output value to export to an AWS::SSM::Parameter resource named Stackvariables.","C":"Configure the output artifacts of the CloudFormation action in the pipeline to be an AWS Systems Manager Parameter Store parameter named StackVariables. Name the artifact BucketName.","D":"Configure the build artifact from the build action as the input to the CodePipeline S3 deploy action. Configure the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable."},"answers_community":["BD (100%)"],"question_id":254,"topic":"1","question_images":[],"answer_description":"","answer_images":[],"exam_id":23,"url":"https://www.examtopics.com/discussions/amazon/view/151568-exam-aws-certified-devops-engineer-professional-dop-c02/","discussion":[{"poster":"Changwha","upvote_count":"3","timestamp":"1732359240.0","content":"Selected Answer: BD\n1. Define a BucketName output in the CloudFormation template and set the CloudFormation action’s namespace in the pipeline to StackVariables.\n2. Use the StackVariables.BucketName variable in the S3 deploy action to specify the S3 bucket dynamically, while using the build artifact as input.","comment_id":"1316640"},{"content":"Selected Answer: BD\nOption B:\nAdding an output to the CloudFormation template allows the pipeline to extract the S3 bucket name created during the stack deployment.\nSetting the namespace (StackVariables) in the CloudFormation action enables other actions in the pipeline to reference the output values using that namespace.\nOption D:\n\nThe build artifact needs to be input to the S3 deploy action to provide the files for deployment.\nBy using StackVariables.BucketName, the deploy action dynamically references the bucket created by the CloudFormation stack, ensuring that the files are deployed to the correct location.","poster":"f4b18ba","timestamp":"1732307520.0","comment_id":"1316486","upvote_count":"3"},{"timestamp":"1731971700.0","content":"Selected Answer: BD\nTo dynamically deploy a single-page web application to an S3 bucket created by a CloudFormation deploy action in AWS CodePipeline, you should:\n\n 1. Define a BucketName output in the CloudFormation template and set the CloudFormation action’s namespace in the pipeline to StackVariables.\n 2. Use the StackVariables.BucketName variable in the S3 deploy action to specify the S3 bucket dynamically, while using the build artifact as input.\n\nThis approach ensures the pipeline is correctly configured to handle dynamic resource creation with minimal complexity.","poster":"uncledana","comment_id":"1314280","upvote_count":"3"}]},{"id":"L8bxVgTFVBIFJNF9asC1","answers_community":["D (100%)"],"answer_ET":"D","answer_description":"","question_images":[],"choices":{"A":"Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora Serverless database. Create an Application Load Balancer and an Auto Scaling group for the Redis cache.","C":"Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora Serverless database. Create an Amazon ElastiCache (Redis OSS) cluster for the cache. Create a target group that has a DNS target type that contains the ElastiCache (Redis OSS) cluster hostname.","D":"Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora database that has a Multi-AZ deployment. Create an Amazon ElastiCache (Redis OSS) cluster for the cache.","B":"Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora database that has a Multi-AZ deployment. Create a Network Load Balancer and an Auto Scaling group in a single Availability Zone for the Redis cache."},"answer_images":[],"question_id":255,"exam_id":23,"topic":"1","answer":"D","isMC":true,"unix_timestamp":1735105020,"url":"https://www.examtopics.com/discussions/amazon/view/153431-exam-aws-certified-devops-engineer-professional-dop-c02/","discussion":[{"poster":"Srikantha","content":"Selected Answer: D\nThis solution separates the components of the application to improve availability and performance while addressing the specific issues the company is facing, such as variations in response times and heavy load on a single EC2 instance.\n\nKey steps in this solution:\n\nApplication Load Balancer (ALB):\nDistributes incoming web traffic across multiple EC2 instances in an Auto Scaling group.\nEnsures high availability and horizontal scaling of the web application.\nAmazon Aurora (Multi-AZ):\nAurora is a fully managed relational database service, and using Multi-AZ deployment ensures high availability and automatic failover.\nHelps offload the database work from the EC2 instances.\nAmazon ElastiCache (Redis):\nOffloads caching from the EC2 instances to ElastiCache (Redis OSS), which is purpose-built for high-performance caching and can handle load more effectively than having Redis run on each EC2 instance.\nImproves response times by reducing the database load.","upvote_count":"1","comment_id":"1539970","timestamp":"1743864300.0"},{"poster":"matt200","comment_id":"1331375","timestamp":"1735105020.0","content":"Selected Answer: D\nshould be D","upvote_count":"4"}],"question_text":"A company used a lift and shift strategy to migrate a workload to AWS. The company has an Auto Scaling group of Amazon EC2 instances. Each EC2 instance runs a web application, a database, and a Redis cache.\n\nUsers are experiencing large variations in the web application's response times. Requests to the web application go to a single EC2 instance that is under significant load. The company wants to separate the application components to improve availability and performance.\n\nWhich solution will meet these requirements?","timestamp":"2024-12-25 06:37:00"}],"exam":{"numberOfQuestions":355,"isImplemented":true,"isMCOnly":true,"name":"AWS Certified DevOps Engineer - Professional DOP-C02","id":23,"provider":"Amazon","lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":51},"__N_SSP":true}