{"pageProps":{"questions":[{"id":"0tJfam5WG6brV3AfXWGd","answers_community":["AD (80%)","AE (20%)"],"discussion":[{"comment_id":"830060","content":"Selected Answer: AD\nAD is right one as ASG is involved here. See https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html","poster":"bgc1","upvote_count":"2","timestamp":"1678032300.0"},{"comments":[{"upvote_count":"1","content":"The company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI.\nwhy E ??????????","poster":"vn_thanhtung","timestamp":"1715086260.0","comment_id":"1207877"}],"comment_id":"806750","content":"Selected Answer: AE\nMaybe E is better than D?","timestamp":"1676231280.0","poster":"Piccaso","upvote_count":"2"},{"upvote_count":"2","comment_id":"790782","timestamp":"1674922020.0","content":"Selected Answer: AD\nA and D are correct. B is incorrect because we cannot create an IAM role within the script. Its too late, the code is already running on the EC2 instance by now and it requires the Instance role to grant necessary permission to EC2 to access the artifacts in S3 for deploying them on EC2 instances.","poster":"Bulti"},{"upvote_count":"1","comment_id":"788729","timestamp":"1674737580.0","content":"Selected Answer: AD\nA and D for me","poster":"wzh5831"},{"comment_id":"781906","timestamp":"1674192900.0","content":"Selected Answer: AD\nA and D for me","upvote_count":"3","poster":"saeidp"}],"url":"https://www.examtopics.com/discussions/amazon/view/96103-exam-aws-devops-engineer-professional-topic-1-question-158/","choices":{"B":"Create a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec file that contains application deployment scripts and grants access to CodeDeploy.","A":"Create a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow access to CodeDeploy.","D":"Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.","E":"Create an application in CodeDeploy. Configure an in-place deployment type. Specify the EC2 instances that are launched from the common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.","C":"Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Configure CodeDeploy to deploy the newly created AMI."},"exam_id":35,"isMC":true,"answer":"AD","question_id":66,"topic":"1","question_text":"A company uses AWS CodePipeline pipelines to automate releases of its application. A typical pipeline consists of three stages: build, test, and deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants to use AWS CodeDeploy to handle the deployment stage of the pipelines.\n\nThe company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI.\n\nWhich combination of steps should a DevOps engineer perform to meet these requirements? (Choose two.)","answer_images":[],"unix_timestamp":1674192900,"answer_description":"","timestamp":"2023-01-20 06:35:00","answer_ET":"AD","question_images":[]},{"id":"GLNp9GNKHjN6eHuHCNBe","unix_timestamp":1673795100,"choices":{"C":"Create a new EFS file system in Account B. Use AWS Database Migration Service (AWS DMS) to keep data from Account A and Account B synchronized.","D":"Update the Lambda execution roles with permission to access the VPC and the EFS file system. E. Create a VPC peering connection to connect Account A to Account B.","E":"Configure the Lambda functions in Account B to assume an existing IAM role in Account A.","A":"Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A.","B":"Create SCPs to set permission guardrails with fine-grained control for Amazon EFS."},"url":"https://www.examtopics.com/discussions/amazon/view/95441-exam-aws-devops-engineer-professional-topic-1-question-159/","answer_ET":"ADF","exam_id":35,"answer_description":"","discussion":[{"poster":"Bulti","comments":[{"poster":"vn_thanhtung","content":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\nwhy you need assume role ???","timestamp":"1715046720.0","comment_id":"1207672","upvote_count":"1"}],"content":"AEF are the right answers. \n1. Need to update the file system policy on EFS to allow mounting the file system into Account B. \n## File System Policy\n$ cat file-system-policy.json\n{\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"elasticfilesystem:ClientMount\",\n \"elasticfilesystem:ClientWrite\"\n ],\n \"Principal\": {\n \"AWS\": \"arn:aws:iam::<aws-account-id-A>:root\" # Replace with AWS account ID of EKS cluster\n }\n } \n ]\n}\n2. Need VPC peering between Account A and Account B as the pre-requisite\n3. Need to assume cross-account IAM role to describe the mounts so that a specific mount can be chosen.","upvote_count":"8","timestamp":"1674925020.0","comment_id":"790831"},{"upvote_count":"2","comment_id":"1245672","timestamp":"1720635300.0","content":"Selected Answer: AD\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem-cross-account.html#:~:text=For%20your%20Lambda%20function%20to,Elastic%20File%20System%20User%20Guide.\n\nA \nD \nE","poster":"auxwww"},{"comment_id":"1185207","content":"Why is there need for peering? We have one VPC, no mention of any addition, so is out, it's ADF","poster":"jyrajan69","upvote_count":"1","timestamp":"1711683660.0"},{"upvote_count":"2","poster":"Dgix","timestamp":"1698054840.0","content":"ADE. Peering is needed. F is unnecessary.","comment_id":"1051689"},{"timestamp":"1690551420.0","comment_id":"965592","upvote_count":"2","content":"At the time of writing this comment, there is no option E. Only A,B,C,D,F - E is missing. Sweet.","poster":"AndrewD1234"},{"timestamp":"1682458200.0","upvote_count":"1","content":"AEF is best","comment_id":"880880","poster":"easytoo"},{"content":"AEF for me based on explanation here - https://aws.amazon.com/ru/blogs/storage/mount-amazon-efs-file-systems-cross-account-from-amazon-eks/","poster":"bgc1","timestamp":"1678034400.0","comment_id":"830098","upvote_count":"2"},{"upvote_count":"2","comment_id":"807153","timestamp":"1676274540.0","content":"Selected Answer: BF\nE is lacking in the \"Chosen Answer\"\nE and F are obviously correct. \nI prefer B to A because of the least privilege principle.","poster":"Piccaso"},{"poster":"DerekKey","content":"A E F\nShould be E instead of D: \nA Lambda function in one account can mount a file system in a different account. For this scenario, you configure VPC peering between the function VPC and the file system VPC.\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-efs.html","upvote_count":"1","comment_id":"797270","timestamp":"1675450140.0"},{"timestamp":"1675121280.0","upvote_count":"1","comment_id":"793447","poster":"ozlaoliu","content":"Vote for AEF\nhttps://aws.amazon.com/premiumsupport/knowledge-center/access-efs-across-accounts/"},{"upvote_count":"2","timestamp":"1674193620.0","comments":[{"poster":"saeidp","comment_id":"781915","content":"ADE\nVPC peering is needed","timestamp":"1674194460.0","upvote_count":"3"}],"content":"Selected Answer: ADF\nA D F for me","poster":"saeidp","comment_id":"781909"},{"content":"Selected Answer: ADF\nA D F for me","poster":"Dimidrol","upvote_count":"2","comment_id":"779280","timestamp":"1673984340.0","comments":[{"poster":"Dimidrol","timestamp":"1673984460.0","content":"https://aws.amazon.com/ru/blogs/storage/mount-amazon-efs-file-systems-cross-account-from-amazon-eks/","comment_id":"779282","comments":[{"timestamp":"1678034280.0","upvote_count":"1","comment_id":"830096","content":"This link mentioned VPC peering requirement as well as need to assume role. AEF?","poster":"bgc1"}],"upvote_count":"2"}]},{"timestamp":"1673795100.0","content":"Selected Answer: ADF\ni think ADF","poster":"Oleg_gol","comment_id":"776696","upvote_count":"2"}],"isMC":true,"timestamp":"2023-01-15 16:05:00","question_id":67,"answers_community":["ADF (60%)","BF (20%)","AD (20%)"],"question_images":[],"answer":"ADF","topic":"1","answer_images":[],"question_text":"A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company's development team wants to use AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a VPC. The development team is using Amazon Elastic File System (Amazon EFS) as shared storage in Account A in the organization.\n\nThe company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.\n\nA DevOps engineer needs to reconfigure an existing EFS file system to allow Lambda functions to access the data through an existing EFS access point.\n\nWhich combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)"},{"id":"pVcYXqx295eNNlfJuLju","answer":"A","answer_description":"","question_images":[],"answers_community":["A (100%)"],"question_id":68,"question_text":"A DevOps Engineer administers an application that manages video files for a video production company. The application runs on Amazon EC2 instances behind an ELB Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. Data is stored in an Amazon RDS PostgreSQL\nMulti-AZ DB instance, and the video files are stored in an Amazon S3 bucket. On a typical day, 50 GB of new video are added to the S3 bucket. The Engineer must implement a multi-region disaster recovery plan with the least data loss and the lowest recovery times. The current application infrastructure is already described using AWS CloudFormation.\nWhich deployment option should the Engineer choose to meet the uptime and recovery objectives for the system?","choices":{"A":"Launch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1. Create an Amazon RDS read replica in the second region. In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket. To fail over, promote the read replica as master. Update the CloudFormation stack and increase the capacity of the Auto Scaling group.","C":"Launch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1. Use Amazon CloudWatch Events to schedule a nightly task to take a snapshot of the database, copy the snapshot to the second region, and replace the DB instance in the second region from the snapshot. In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket. To fail over, increase the capacity of the Auto Scaling group.","D":"Use Amazon CloudWatch Events to schedule a nightly task to take a snapshot of the database and copy the snapshot to the second region. Create an AWS Lambda function that copies each object to a new S3 bucket in the second region in response to S3 event notifications. In the second region, launch the application from the CloudFormation template and restore the database from the most recent snapshot.","B":"Launch the application from the CloudFormation template in the second region, which sets the capacity of the Auto Scaling group to 1. Create a scheduled task to take daily Amazon RDS cross-region snapshots to the second region. In the second region, enable cross-region replication between the original S3 bucket and Amazon Glacier. In a disaster, launch a new application stack in the second region and restore the database from the most recent snapshot."},"answer_images":[],"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/2335-exam-aws-devops-engineer-professional-topic-1-question-16/","timestamp":"2019-07-10 00:40:00","exam_id":35,"answer_ET":"A","unix_timestamp":1562712000,"discussion":[{"timestamp":"1632235200.0","comment_id":"3822","upvote_count":"48","content":"answer is A","poster":"toma"},{"content":"A is the right choice","comments":[{"comment_id":"113672","upvote_count":"1","content":"I think D is correct based on comments form @inf","poster":"un","timestamp":"1634991180.0","comments":[{"poster":"GreatFunana","content":"Its not. Restoring any DB is a time consuming procedure, and this answer implies a restore would have after disaster.\n\nA is the only answer that implies replication, and active replication to bring a failover DB live is the FASTEST way to restore a downed DB.","comment_id":"502717","upvote_count":"5","timestamp":"1639636320.0"},{"poster":"z_inderjot","timestamp":"1703568300.0","comment_id":"1105723","upvote_count":"1","content":"not to mention we will lose data of the current day , since we rely on the snapshot taken at night previous day"}]}],"poster":"un","comment_id":"35003","timestamp":"1633932300.0","upvote_count":"6"},{"content":"Answer is A: to meet the uptime and recovery objectives for the system, and this has the lowest RPO","timestamp":"1688971920.0","poster":"DaddyDee","upvote_count":"1","comment_id":"947830"},{"content":"Selected Answer: A\nB is not correct -> Glacier tier does not match the requirement of lowest recovery times.\nC is not correct -> the snapshot copy -> replace operation can fail sometimes\nD is not correct -> the Lambda function on S3 objects is not efficient and stable enough.","upvote_count":"1","poster":"Piccaso","comment_id":"797258","timestamp":"1675448700.0"},{"comment_id":"763074","upvote_count":"1","poster":"Bulti","timestamp":"1672551540.0","content":"A is the right answer."},{"content":"Selected Answer: A\nA: cross-Region read replica\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.XRgn","upvote_count":"1","timestamp":"1672148400.0","poster":"ceros399","comment_id":"758565"},{"content":"answer is A , \nI think, solution do not need new s3 bucket.","timestamp":"1668972780.0","comment_id":"722934","upvote_count":"1","poster":"fvztpc"},{"comment_id":"660750","timestamp":"1662434220.0","content":"Selected Answer: A\nAns: A","upvote_count":"1","poster":"SamHan"},{"timestamp":"1636081320.0","content":"Go A -1","comment_id":"447657","poster":"oopsy","upvote_count":"3"},{"upvote_count":"2","content":"I'll go with A)\nFor those choosing D) recall enable cross-region replication for S3 is always the best practice and is strongly recommended.\nCopying such volume of data using lambda will result in high costs for Lambda execution time, due the time it takes to copy and process .","timestamp":"1636068600.0","poster":"WhyIronMan","comment_id":"320370"},{"timestamp":"1635955560.0","comment_id":"231981","upvote_count":"1","content":"D is totally: 1 \"copies each object\" , 2 kind of situations RDS-PG is the best option for the cross region","poster":"svjl"},{"upvote_count":"1","content":"I would keep it simple and efficient: A","poster":"Coffeinerd","timestamp":"1635739920.0","comment_id":"229698"},{"comment_id":"223195","content":"I'll go with A","upvote_count":"3","poster":"jackdryan","timestamp":"1635695880.0"},{"comment_id":"205244","poster":"ChauPhan","content":"Only A is quick for DB recovery in RPO. Other options which go with daily or night schedule will take ~ 24 hours for backup interval, so it does meet RPO condition.","upvote_count":"2","timestamp":"1635688440.0"},{"comment_id":"188386","content":"A says: \"In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket.\" - the part \"In second region\" is tricky as S3 does not require region selection, but the second part says \"enable cross-region replication between the original S3 bucket and a new S3 bucket\" which does not explicitly says on which bucket to configure CRR. \nSo the correct answer is A.","poster":"[Removed]","upvote_count":"2","timestamp":"1635562980.0"},{"content":"I think it's A","timestamp":"1635525180.0","poster":"nqobza","upvote_count":"1","comment_id":"166303"},{"upvote_count":"1","timestamp":"1635440340.0","content":"I think it should be A, but the wording of the question is incorrect.\nNormally the response options are created in a pretty formulaic way: you get a bunch of statements and each response contains some combination of those statements, plus a few unique ones. In this case the statement about S3 CRR appears a few times but is incorrectly worded. I suspect they wanted this to be a valid statement, in which case A is clearly the best answer, since the focus is on RPO and A has an RPO measured in minutes, whereas B, C and D appear to have a 24h RPO.","poster":"cowshield","comment_id":"146488"},{"poster":"CamNhungDinh","content":"I think using Amazon CloudWatch Events to schedule a nightly task to take a snapshot is great idea.\nhttps://couponforless.com/store/amazon.com","upvote_count":"1","comment_id":"135472","timestamp":"1635214320.0"},{"comment_id":"115401","poster":"tolik505","content":"I'd go with an option A.\nIt looks like warm standby approach that allows smaller RPO and RTO.\nRegarding `In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket`, I believe that the main part is \"enable cross-region replication between the original S3 bucket and a new S3 bucket\". We shouldn't overthink.","timestamp":"1635089760.0","upvote_count":"1"},{"comment_id":"110066","upvote_count":"2","comments":[{"comments":[{"timestamp":"1635207840.0","upvote_count":"1","poster":"inf","content":"Valid points, but this is how I read it\n\"Create an AWS Lambda function that copies each object to a new S3 bucket in the second region in response to S3 event notifications\" - it copies ALL objects, not just the new one - an S3 notification invokes a copy against all objects - doesn't necessarily have to copy them all - ie can do a source/destination comparison. Other statements look good except they are technically incorrect. Tough one","comment_id":"135375"}],"poster":"Stec1980","upvote_count":"2","content":"I don't actually see how D caters for existing objects because it states that it will copy objects \"in response to S3 event notifications\", it doesn't actually say anything about the existing objects specifically unless you take \"Create an AWS Lambda function that copies each object to a new S3 bucket\" as an independent statement, which i don't think it is.\n\nI want A to be the correct answer due to the RPO's offered between that and D, but as you' (and others have said) it's wrong to state you would enable CRR in the target region. Tha statement in A, B and C in theory rules them all out immediately, but i don't feel comfortable with the 24 hour RPO...hmmm...honestly not sure what i'd commit to here if this comes up...I'd be conflicted for sure.","timestamp":"1635207720.0","comment_id":"128848"}],"poster":"inf","timestamp":"1634858580.0","content":"Answer: D\n(Reluctantly, because of RPO, but technically the only correct answer)\nAs a few others have pointed out, S3 CRR is \"Enabled\" in the \"source region\". You create the CRR Rules on the \"source\" bucket. Not on the S3 bucket in the destination/second region. Because the first three answers A,B,C all indicate \"enable\" CRR in the \"second\" region, they have to be incorrect. Technically, A,B,C cannot be the answer.\nAbove all else - Cross-Region Replication does NOT replicate EXISTING objects (no mention of support or other methods to copy existing objects). The only answer that caters for existing object is D - so it absolutely must be the correct answer. \n\nObviously read replicas make sense, you can also enable automatic backups on the read replicas to protect against corruption (which was another concern, DR could mean corruption as well - and [automatic] backups are not mentioned in A for the replica)\nA - wrong (CRR enabled in source, not destination)\nB - wrong (Glacier?)\nC - wrong (CRR enabled in source, not destination, plus 24hr db replica)"},{"comment_id":"93809","upvote_count":"5","content":"A - correct. least Recovery time and data loss.\nB - involves data loss based on frequency of snapshot\nC - involves 24 hrs of data loss\nD - involves 24 hrs of data loss and additional lambda to manage","poster":"Raj9","timestamp":"1634817420.0"},{"timestamp":"1634801280.0","poster":"secreatUser","comment_id":"80543","content":"go with A","upvote_count":"1"},{"comment_id":"62133","upvote_count":"1","timestamp":"1634621220.0","poster":"yassu","content":"A is correct"},{"comment_id":"54800","content":"It is A","poster":"xaocho","upvote_count":"1","timestamp":"1634291160.0"},{"poster":"consultsk","comment_id":"40595","upvote_count":"2","content":"Answer: A\nhttps://aws.amazon.com/about-aws/whats-new/2016/06/amazon-rds-for-postgresql-now-supports-cross-region-read-replicas/\nRDS-PG is the best option for DB: https://aws.amazon.com/about-aws/whats-new/2016/06/amazon-rds-for-postgresql-now-supports-cross-region-read-replicas/. With all the options, I chose 'A' as the best option.","timestamp":"1634106480.0"},{"upvote_count":"2","content":"the answer should be A","poster":"dinhvu","timestamp":"1633934700.0","comment_id":"35654"},{"upvote_count":"4","poster":"jiedee","content":"“ lowest recovery times” indicates that A is the right answer","comment_id":"28342","timestamp":"1633764960.0"},{"content":"@yxyc you're right, it's tricky: \"You add the replication configuration to your source bucket. The minimum configuration must provide the following:\nThe destination bucket where you want Amazon S3 to replicate objects...\" - it must be enabled in the 1st region. The only option which looks correct for me is D . Although I dont really like \"copies each object\" - it would be better if it says \"copies each new object\". Anyway - for me D is correct.","upvote_count":"2","comment_id":"16765","timestamp":"1633183860.0","comments":[{"upvote_count":"2","timestamp":"1633330680.0","comment_id":"22435","poster":"Arragon","content":"You're right about the configuration of the S3 bucket CRR option. I guess they made a small mistake forming the answers since really ask for the least data loss and lowest recovery time. I would go for answer A but still not 100% sure."},{"upvote_count":"1","comment_id":"24257","comments":[{"content":"Yes, the questions says \"least data loss and lowest recovery time.\" - but it's not exact time, like e.g. RPO 15 min. it's the least from technically correct answers. For me, the option A is just too obvious, would be for associate level. Professional exam is full of tricky questions.... A would be good if we assume it;s just a typo about S3 bucket replication enabling.","timestamp":"1634737980.0","upvote_count":"1","comment_id":"65258","poster":"ele"}],"timestamp":"1633612440.0","content":"For the DB replication, Cross region Read replica is the way to go for least data loss and lowest recovery time. DB snapshot cannot meet this requirement since DB changes should happen all the time.","poster":"JHtest"}],"poster":"ele"},{"content":"what about `In the second region, enable cross-region replication between the original S3 bucket and a new S3 bucket` for option A? this shall be set in region1, not in region2","comment_id":"15347","poster":"yxyc","timestamp":"1633155120.0","upvote_count":"2"},{"comment_id":"12418","poster":"YashBindlish","content":"I will go with A as Read replica can be promoted as Master","upvote_count":"2","timestamp":"1632775200.0"},{"comment_id":"4658","upvote_count":"2","content":"Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL and Oracle as well as Amazon Aurora.","comments":[{"timestamp":"1632593580.0","upvote_count":"1","poster":"jvoorhees","comment_id":"12352","content":"I agree. Option C provides a RPO of 24 hours because of the daily snapshots at midnight"}],"poster":"agomes","timestamp":"1632564480.0"}]},{"id":"F5uj3nk8PTuUOll1lHkC","question_images":[],"discussion":[{"content":"B D E\nA - wrong -> Modify the trust relationship to allow the sts:AssumeRole action from -->the workload accounts<-- \nB - correct -> You need to have roles with proper permissions in each workload account that can be used from the operations account\nC - wrong -> why do you want to use Cognito - it is not mentioned as used or required\nD - correct -> each member must have an account\nE - correct -> each member should be part of the group that can assume the workload role in each workload account\nF - wrong -> why do you want to use Cognito - it is not mentioned as used or required","timestamp":"1675452420.0","upvote_count":"9","poster":"DerekKey","comment_id":"797299"},{"poster":"Christina666","comment_id":"783780","content":"Selected Answer: BCE\nI chose BCE. \n1. Create an assume role for operation team to access workload team, \n2. then add operation team members to a group, attach this assume role","timestamp":"1674339480.0","upvote_count":"5"},{"timestamp":"1682458440.0","comment_id":"880882","upvote_count":"1","poster":"easytoo","content":"BDE for me."},{"content":"Selected Answer: BDE\nBDE looks good. Good summary by DerekKey","upvote_count":"1","poster":"bgc1","timestamp":"1678033740.0","comment_id":"830086"},{"timestamp":"1677233700.0","comment_id":"820338","content":"Selected Answer: BDE\nB. Create a SysAdmin role in each workload account, attach the AdministratorAccess policy to the role, and modify the trust relationship to allow the sts:AssumeRole action from the operations account. This will enable the SysAdmin role in the workload accounts to be assumed by the operations team members in the operations account.\n\nD. In the operations account, create an IAM user for each operations team member.\n\nE. In the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group. This will grant the operations team members the necessary permissions to assume the SysAdmin role in each workload account.\n\nOption A, C, and F are not necessary because creating an Amazon Cognito identity pool or Amazon Cognito user pool, or modifying the trust relationship of the SysAdmin role to allow the sts:AssumeRole action from the workload accounts, is not required to provide the necessary access to the operations team members.","poster":"joseribas89","upvote_count":"1"},{"poster":"Piccaso","timestamp":"1676276160.0","upvote_count":"1","content":"Selected Answer: BCE\nThe suggested answer is \"BCF\". I think it's a type error there. C and F are in a pair, only one of them should be picked.\nAbviously, B is the correct one from the AB pair. F is the one from CF pair. \nFrom DE pair, E is the correct one, because we need to use sts:AssumeRole.","comment_id":"807178"},{"comments":[{"content":"I meant between C and F*","comment_id":"790845","poster":"Bulti","timestamp":"1674925740.0","upvote_count":"1"}],"content":"Selected Answer: BEF\nI think the answer is B, E, F. \nB and E are correct for sure. Now between C and E, I am not sure what C really means. Does it mean create IAM user for each operations team member and add them to the IAM Group? I dont' think so because the question reads \"all users are centrally managed in the operations account\". It indicates that AWS SSO is configured and Cognito User Pool is setup as the Identity Provider. Once the user logs in using AWS SSO and authenticates against the Cognito user pool, they can be mapped to the IAM group that has required permissions to assume an SysAsdmin role containing the AdmnistratorAccess permissions in the workload accounts.","poster":"Bulti","timestamp":"1674925680.0","comment_id":"790844","upvote_count":"1"},{"poster":"saeidp","comment_id":"781919","upvote_count":"4","timestamp":"1674195360.0","content":"Selected Answer: BDE\nBDE for me"},{"poster":"Oleg_gol","comments":[{"upvote_count":"1","timestamp":"1674025020.0","content":"I am unsure of the answer yet, but I am 90% sure that it is not BDE because there are 3 pairs.\nAB, DE, CF. The answer should be one from each pair. So, DE together is wrong.","poster":"devops7","comments":[{"content":"Why bde is wrong. We used such approach before migrating to sso","upvote_count":"1","comments":[{"comment_id":"781673","poster":"devops7","timestamp":"1674168240.0","upvote_count":"1","content":"As I said before, \"I am unsure.\" However, based on all previous AWS question patterns; for example, questions 9, 12, 13 & etc. The answers are 3 pairs and pick one from each pair. You could be right and BDE is the answer but I have not seen this kind of AWS answer pattern before."}],"comment_id":"781232","timestamp":"1674139140.0","poster":"Dimidrol"}],"comment_id":"779656"}],"content":"Selected Answer: BDE\nBDE https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html","timestamp":"1673793240.0","comment_id":"776670","upvote_count":"4"}],"answer_ET":"BDE","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/95439-exam-aws-devops-engineer-professional-topic-1-question-160/","timestamp":"2023-01-15 15:34:00","answers_community":["BDE (59%)","BCE (35%)","6%"],"exam_id":35,"unix_timestamp":1673793240,"isMC":true,"choices":{"A":"Create a SysAdmin role in the operations account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the workload accounts.","B":"Create a SysAdmin role in each workload account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the operations account.","F":"Create an Amazon Cognito user pool in the operations account. Create an Amazon Cognito user for each operations team member.","D":"In the operations account, create an IAM user for each operations team member.","E":"In the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group.","C":"Create an Amazon Cognito identity pool in the operations account. Attach the SysAdmin role as an authenticated role."},"question_text":"A company has an organization in AWS Organizations. The organization includes workload accounts that contain enterprise applications. The company centrally manages users from an operations account. No users can be created in the workload accounts. The company recently added an operations team and must provide the operations team members with administrator access to each workload account.\n\nWhich combination of actions will provide this access? (Choose three.)","answer_description":"","topic":"1","question_id":69,"answer":"BDE"},{"id":"TKKSOIgEEfhcv7HcjPcP","unix_timestamp":1673791920,"choices":{"B":"Create a CloudFormation template that deploys an AWS Config rule to detect stacks that do not have termination protection enabled. Add a remediation action to the rule to enable termination protection. Deploy the template to the OU of the production accounts by using CloudFormation StackSets.","A":"Create an AWS Config rule to detect stacks that do not have termination protection enabled. Add a remediation action to the rule to enable termination protection. Deploy the rule across the organization by using the PutOrganizationConfigRule API operation.","C":"Create an SCP that denies cloudformation:DeleteStack actions. Apply the SCP to the OU of the production accounts by using CloudFormation StackSets.","D":"Create a CloudFormation stack policy that denies Update:Delete actions. Apply the policy to the OU of the production accounts by using CloudFormation StackSets."},"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/95434-exam-aws-devops-engineer-professional-topic-1-question-161/","exam_id":35,"answer_description":"","discussion":[{"comment_id":"849384","upvote_count":"5","timestamp":"1679667840.0","poster":"asfsdfsdf","content":"Selected Answer: B\nA - is wrong if you put this across the organization it will also impact non-prod accounts\nC - might work but it denies on organization level and not enabling termination protection - also how can you apply the SCP by using cloudformation stackssets? you apply it via organizations\nD- how do you apply a cloudformation a new stack policy to other stacks using stacksset? you cant, you need to update current ones.\nB- the only valid answer as a custom config rule can be created and deployed using stacksset to an OU."},{"upvote_count":"1","content":"Selected Answer: B\nCreate a custom config rule to detect the stacks that don't enable the termination protection. In remediation actions to make the changes (enable it).","timestamp":"1678846680.0","comment_id":"839485","poster":"einn"},{"poster":"LoveToronto","upvote_count":"1","timestamp":"1677044100.0","content":"C is the answer","comment_id":"817493"},{"poster":"LoveToronto","timestamp":"1677043920.0","content":"C is the right answer. B is incorrect because there for no Config rule StackSets that can detect Termination Protection.","upvote_count":"1","comment_id":"817491"},{"poster":"BelloMio","comments":[{"timestamp":"1676841900.0","comment_id":"814573","poster":"BelloMio","content":"To the people saying B is incorrect as there is no rule for termination protection, I guess it just means it must be a lambda rule and not aws managed.","upvote_count":"1"}],"timestamp":"1676841720.0","upvote_count":"2","comment_id":"814571","content":"A is wrong as this will detect stacks from the whole organization and apply remediation actions to all accounts, not just production accounts\nB is correct\nC is wrong as we want to enable terminate protection, we don't want to prevent the deletion of the stacks for good.\nD does not make sense"},{"comment_id":"814140","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html","upvote_count":"2","poster":"Piccaso","timestamp":"1676814960.0"},{"upvote_count":"3","timestamp":"1675457760.0","comments":[{"upvote_count":"1","timestamp":"1675458000.0","content":"There is a difference between SCP policy and termination protection. Termination protection will disregard the delete request even if you have permission to do it. You have to disable protection to be able to delete CF.","poster":"DerekKey","comment_id":"797378"}],"poster":"DerekKey","content":"Selected Answer: A\nC & D - wrong - company wants to -->enable<-- termination protection\nB - wrong - there are only two AWS Config rules for cloud formation: cloudformation-stack-drift-detection-check and cloudformation-stack-notification-check\nA - my choice - you have to create a rule (with Guard) and then deploy it - PutOrganizationConfigRule can be done with aws cli - it allows you to exclude accounts that you don't want to target (--excluded-accounts)","comment_id":"797372"},{"comments":[{"upvote_count":"3","content":"A could have been a potential choice considering we can create custom Config rules using Lambda even if AWS config rule is not available for CF termiation protection. However it states calling the PutOrganizationConfigRule API across the entire organization but we don't want Termination protection in the Non-Prod accounts. So I will still go with C.","comment_id":"792733","poster":"Bulti","timestamp":"1675082100.0"}],"upvote_count":"3","timestamp":"1674926760.0","comment_id":"790856","poster":"Bulti","content":"Selected Answer: C\nC is the right answer. B is incorrect because there for no Config rule StackSets that can detect Termination Protection."},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html","poster":"Goksori","comment_id":"788304","upvote_count":"2","timestamp":"1674700200.0","comments":[{"upvote_count":"1","content":"aws config rule?","comment_id":"789136","poster":"saeidp","timestamp":"1674770940.0"}]},{"comment_id":"781961","timestamp":"1674199560.0","poster":"saeidp","content":"Selected Answer: C\nIt seems there are no config rules for cloudformation terminate protection\nThen C is the best","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: B\nI'll go with B","poster":"saeidp","comment_id":"781936","timestamp":"1674197040.0","comments":[]},{"upvote_count":"2","poster":"Oleg_gol","timestamp":"1673791920.0","comment_id":"776651","content":"Selected Answer: C\ni vote C"}],"isMC":true,"timestamp":"2023-01-15 15:12:00","question_id":70,"answers_community":["B (52%)","C (33%)","14%"],"question_images":[],"answer":"B","answer_images":[],"topic":"1","question_text":"A company's DevOps engineer manages an organization in AWS Organizations. The organization includes many accounts. The company needs all AWS CloudFormation stacks in production accounts to have termination protection enabled. Non-production accounts do not need termination protection.\n\nThe company has designated a centralized account for AWS Config aggregation and has configured all accounts to support the use of CloudFormation and AWS Config. The company also has grouped all production accounts into an OU.\n\nWhich solution will meet these requirements?"}],"exam":{"isBeta":false,"isImplemented":true,"isMCOnly":false,"id":35,"lastUpdated":"11 Apr 2025","provider":"Amazon","numberOfQuestions":208,"name":"AWS DevOps Engineer Professional"},"currentPage":14},"__N_SSP":true}