{"pageProps":{"questions":[{"id":"JHlckfAkZxhwtnOt4JHL","answer_images":[],"question_id":146,"topic":"1","choices":{"B":"Configure an Amazon Kinesis data stream with one shard per application. Write an AWS Lambda function to read usage data objects from the shards. Have the function perform .csv conversion, reformatting, and compression of the data. Have the function store the output in Amazon S3.","D":"Store usage data objects in an Amazon DynamoDB table. Configure a DynamoDB stream to copy the objects to an S3 bucket. Configure an AWS Lambda function to be triggered when objects are written to the S3 bucket. Have the function convert the objects into .csv format.","C":"Configure an Amazon Kinesis data stream for each application. Write an AWS Lambda function to read usage data objects from the stream for each application. Have the function perform .csv conversion, reformatting, and compression of the data. Have the function store the output in Amazon S3.","A":"Configure an Amazon Kinesis Data Firehose delivery stream for each application. Write AWS Lambda functions to read log data objects from the stream for each application. Have the function perform reformatting and .csv conversion. Enable compression on all the delivery streams."},"answer_description":"","unix_timestamp":1620056040,"exam_id":20,"answer_ET":"A","discussion":[{"content":"ANSWER:A\nEXPLANATION:\nFirehose can invoke an AWS Lambda function to transform incoming data before delivering it to destinations\nAmazon Kinesis Data Firehose allows you to compress your data before delivering it to Amazon S3 .","poster":"Heer","timestamp":"1633607340.0","comment_id":"357559","comments":[{"upvote_count":"2","comments":[{"timestamp":"1688576460.0","poster":"juanife","content":"I don't understand why not opcion B yet.","comments":[{"comment_id":"943972","upvote_count":"1","content":"aaaaaaaah, I got it. AWS KDS + AWS Lambda means Lambda polling from AWS KDS steam, but in the case of AWS KDF + AWS Lambda, Lambda is not polling from AWS KDF, so it's faster than the other solution. It also requires custom code, because you have to create 8 shards...","timestamp":"1688576580.0","poster":"juanife"}],"upvote_count":"1","comment_id":"943970"}],"comment_id":"778144","content":"You can use the Amazon Kinesis Data Firehose API to send data to a Kinesis Data Firehose delivery stream using the AWS SDK\nhttps://docs.aws.amazon.com/firehose/latest/dev/writing-with-sdk.html \n\nAs explicitly mentioned in the case: \nTo request an increase in quota, use the Amazon Kinesis Data Firehose Limits form\nhttps://docs.aws.amazon.com/firehose/latest/dev/limits.html","timestamp":"1673896740.0","poster":"nadavw"}],"upvote_count":"17"},{"poster":"teo2157","upvote_count":"1","comment_id":"1100539","content":"Selected Answer: A\nThe requirements can be met by using a combination of AWS services: Amazon Kinesis Data Firehose, AWS Lambda, and Amazon S3.\n\nHere's a high-level overview of how these services can be used to meet the requirements:\nThis solution requires minimal custom code - only the Lambda function to convert JSON to .csv needs to be written. The rest of the pipeline can be configured using AWS Management Console or AWS CLI.","timestamp":"1702983540.0"},{"upvote_count":"1","timestamp":"1699508460.0","poster":"LocalHero","comment_id":"1066136","comments":[{"poster":"LocalHero","timestamp":"1699508580.0","content":"Probably, This question asks that we know Firehose's compress feature or not.","upvote_count":"1","comment_id":"1066137"}],"content":"I choose A.\nBecause of A has minimum custom code.\nD is wrong . not compress.\nBC are wrong. They have compress code in Lambda."},{"timestamp":"1682958420.0","poster":"pk349","content":"A: I passed the test","comment_id":"886488","upvote_count":"1"},{"comment_id":"858867","upvote_count":"2","content":"Selected Answer: A\nLeast cutom code","timestamp":"1680441540.0","poster":"rsn"},{"comment_id":"776800","upvote_count":"1","comments":[{"upvote_count":"3","timestamp":"1674783420.0","poster":"Merrick","comment_id":"789259","content":"A source can be a logging server running on Amazon EC2 instances, an application running on mobile devices, or a sensor on an IoT device. You can connect your sources to Kinesis Data Firehose using 1) Amazon Kinesis Data Firehose API, which uses the AWS SDK for Java, .NET, Node.js, Python, or Ruby. 2) Kinesis Data Stream, where Kinesis Data Firehose reads data easily from an existing Kinesis data stream and load it into Kinesis Data Firehose destinations...\nin Kinesis Data Firehose FAQ"}],"timestamp":"1673800020.0","poster":"Arjun777","content":"Firehose can it connect directly connect to source ? by that rule - A does not seem to be right .. C and B - KDS are required for large volume 10GB and above in general .. D seems to be the closest though compression to parquet is not mentioned .."},{"upvote_count":"4","content":"Selected Answer: A\nCorrect answer is A as Kinesis Data Firehose can be used to collect the data, it integrates with Lambda to provide custom transformation and then stores the compressed data to S3. There is a limit of throughput that KDF can support and it can be increased using quota increase requests.\n\nOptions B & C are wrong as Kinesis Data Stream would require more handling and custom code.\n\nOption D is wrong as it does not meet the requirement that the usage data objects need to be reformatted, converted to .csv format, and then compressed before they are stored in Amazon S3.","comment_id":"713189","poster":"cloudlearnerhere","timestamp":"1667841300.0"},{"content":"Go with C:\nA– Doesn’t mention saving to S3, and the Lambda read log data instead of usage data. (unless this is a typo, then A is about right) \nB- Cannot be done as the Shards have a hard limit of 1MB/Sec. (Can AWS increase that quota?)\nC- Correct. With Custom code on Lambda (which I can live with). \nD – no compression was mentioned.","poster":"jazzok","timestamp":"1665296100.0","comment_id":"689932","upvote_count":"4"},{"comment_id":"679950","content":"my answer is 'A' considering the least custom code possible condition.","upvote_count":"1","timestamp":"1664209200.0","poster":"karanbhasin"},{"poster":"yemauricio","upvote_count":"3","comment_id":"643262","content":"Selected Answer: A\nKDF + lambda +compression","timestamp":"1659776400.0"},{"content":"Selected Answer: A\nANSWER : A","comment_id":"637119","poster":"rocky48","timestamp":"1658806980.0","upvote_count":"3"},{"poster":"samsanta2012","timestamp":"1655417820.0","comment_id":"617417","upvote_count":"1","content":"Selected Answer: D\nAWS Lambda supports Parallelization Factor, a feature that allows you to process DynamoDB data stream with more than one Lambda invocation simultaneously."},{"timestamp":"1647412200.0","upvote_count":"2","comment_id":"568808","content":"Ans: A\nB is wrong because:\n1. It will create 2 hot shards\n2. Max object size per shard is 1MB in Kinesis Data Stream.","poster":"youonebe","comments":[{"poster":"alfredofmt","content":"The question says 2 MiB per second, not 2 MiB of payload","comment_id":"640352","timestamp":"1659330900.0","upvote_count":"2"}]},{"upvote_count":"1","content":"A is really make sense... but wondering why cant we use option B with enhance fan out ?","timestamp":"1639890540.0","poster":"sanpak","comment_id":"504643"},{"poster":"ThomasKalva","content":"A is my answer: Forehose provides way to introduce lambda for transformations before loading files to s3 and it clearly states before loading files to s3 which removes option D for sure. Option C is close but question asks clearly finding a solution to store and does not ask for any real-time processing, keeping cost in mind, A stands.","upvote_count":"3","comment_id":"446265","timestamp":"1635911700.0"},{"comment_id":"439157","upvote_count":"3","timestamp":"1635792900.0","content":"The question clearly states - \"The usage data objects need to be reformatted..... \" Emphasis on the phrase USAGE DATA OBJECTS. Why are you guys suggesting option A in that case. Option A talks of LOG DATA OBJECTS not USAGE DATA OBJECTS.\nThis leaves B & C - with my vote on C.","poster":"virendrapsingh"},{"timestamp":"1635700800.0","content":"A because:\nAmazon Kinesis Data Firehose allows you to compress your data before delivering it to Amazon S3. The service currently supports GZIP, ZIP, and SNAPPY compression formats. \nFirehose can invoke an AWS Lambda function to transform incoming data before delivering it to destinations. You can configure a new Lambda function using one of the Lambda blueprints we provide or choose an existing Lambda function.","comment_id":"423927","poster":"Dr_Kiko","upvote_count":"3"},{"timestamp":"1635216480.0","content":"Its A in my opinion.\nKinesis Data Streams is suitable for customization. KDF is apt for this scenario as it can handle compression without writing code in lambda.","comment_id":"406383","upvote_count":"3","poster":"Donell"},{"upvote_count":"1","comments":[{"content":"A ends with \"Enable compression on all the delivery streams.\". Not sure what you mean by no compression happening in A\".","poster":"asg76","comment_id":"398820","timestamp":"1635075240.0","upvote_count":"2"}],"timestamp":"1634890860.0","comment_id":"396623","content":"C is the answer for me. There is no compression happening in A, So A is out.","poster":"gunjan4392"},{"poster":"Huy","content":"A. The maximum total data per second is 500KiB*6 + 2*2MiB = 7MiBs. In the worse case, need to increase quota of KDF.","upvote_count":"3","comment_id":"389707","timestamp":"1634707740.0"},{"upvote_count":"2","comment_id":"369890","timestamp":"1634258580.0","poster":"tukai","content":"I Think its A. Service quota can be increased in KDF.\nhttps://docs.aws.amazon.com/firehose/latest/dev/limits.html"},{"content":"A and C are options as its Scalable going with A\nB is incorrect \nD does not seem to be discussed","poster":"ksaws","timestamp":"1633689900.0","upvote_count":"1","comment_id":"357887"},{"comment_id":"355803","content":"I think, since the \"The company requires the solution to include the least custom code possible and has authorized the data engineer to request a service quota increase if needed.\", the easiest option to implement is C, B cannot be done as the Shards have hard limit of 1MB/Sec. All other options needs more settings to work.. your thoughts...","upvote_count":"1","poster":"AjithkumarSL","timestamp":"1633027740.0"},{"comment_id":"354511","upvote_count":"4","content":"Option 'D' doesn't have the provision for compression, where as KFH allow us to compress the data before delivering it to S3 which reduces our development effort .So answer in my opinion should be option A.","timestamp":"1632753300.0","poster":"Heer"},{"upvote_count":"1","timestamp":"1632280740.0","poster":"VikG12","comment_id":"348750","comments":[{"timestamp":"1635390720.0","poster":"mickies9","comment_id":"416482","upvote_count":"4","content":"Dynamo DB can hold upto 400KB of data only right?"}],"content":"Will go with D."}],"url":"https://www.examtopics.com/discussions/amazon/view/51698-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2021-05-03 17:34:00","question_text":"A company needs to store objects containing log data in JSON format. The objects are generated by eight applications running in AWS. Six of the applications generate a total of 500 KiB of data per second, and two of the applications can generate up to 2 MiB of data per second.\nA data engineer wants to implement a scalable solution to capture and store usage data in an Amazon S3 bucket. The usage data objects need to be reformatted, converted to .csv format, and then compressed before they are stored in Amazon S3. The company requires the solution to include the least custom code possible and has authorized the data engineer to request a service quota increase if needed.\nWhich solution meets these requirements?","question_images":[],"isMC":true,"answer":"A","answers_community":["A (93%)","7%"]},{"id":"V3V3YxTWdeCMdbpk9CDt","answers_community":["C (100%)"],"topic":"1","exam_id":20,"question_id":147,"question_text":"A data analytics specialist is building an automated ETL ingestion pipeline using AWS Glue to ingest compressed files that have been uploaded to an Amazon S3 bucket. The ingestion pipeline should support incremental data processing.\nWhich AWS Glue feature should the data analytics specialist use to meet this requirement?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/51410-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"C","unix_timestamp":1619871420,"question_images":[],"isMC":true,"answer_description":"","discussion":[{"content":"C it is.","upvote_count":"20","comment_id":"348747","poster":"VikG12","timestamp":"1632763320.0"},{"timestamp":"1632154620.0","comment_id":"346932","content":"I am ok with c as job bookmarks handle incremental data processing.","upvote_count":"9","poster":"bermo","comments":[{"upvote_count":"1","comment_id":"832815","poster":"imjustgoodataws","timestamp":"1678271220.0","content":"Definitely.."}]},{"upvote_count":"1","timestamp":"1682958480.0","poster":"pk349","content":"C: I passed the test","comment_id":"886490"},{"comment_id":"798003","timestamp":"1675518120.0","content":"Selected Answer: C\nTextbook question","poster":"milofficial","upvote_count":"1"},{"comment_id":"777181","poster":"Mirandaali","upvote_count":"1","timestamp":"1673826300.0","content":"Selected Answer: C\nTracking processed data using job bookmarks - AWS Glue\nJob bookmarks are implemented for JDBC data sources, the Relationalize transform, and some Amazon Simple Storage Service (Amazon S3) sources (JSON, CSV, Avro, XML, Parquite, ORC)."},{"poster":"cloudlearnerhere","comment_id":"713191","upvote_count":"3","timestamp":"1667841420.0","content":"Selected Answer: C\nCorrect answer is C as Job Bookmarks supports incremental data processing by persisting the state information.\n\nOption A is wrong as Glue Workflow helps create and visualize complex extract, transform, and load (ETL) activities involving multiple crawlers, jobs, and triggers.\n\nOption B is wrong a Glue trigger can start specified jobs and crawlers. A trigger fires on-demand, based on a schedule, or based on a combination of events.\n\n\nOption D is wrong as the Glue classifier reads the data in a data store. If it recognizes the format of the data, it generates a schema. The classifier also returns a certain number to indicate how certain the format recognition was."},{"timestamp":"1658984580.0","poster":"rocky48","upvote_count":"1","content":"Selected Answer: C\nSelected Answer: C","comment_id":"638461"},{"comment_id":"570262","upvote_count":"1","poster":"jmensah60","content":"Selected Answer: C\nDefinitely Bookmarks","timestamp":"1647581760.0"},{"timestamp":"1646239200.0","poster":"KnightVictor","content":"C for sure i.e. bookmark","comment_id":"559571","upvote_count":"1"},{"comment_id":"446267","timestamp":"1635857100.0","content":"C as question is clearly asking for support on incremental data processing","upvote_count":"2","poster":"ThomasKalva"},{"content":"C for sure.","poster":"ariane_tateishi","timestamp":"1633602900.0","upvote_count":"4","comment_id":"365782"}],"choices":{"A":"Workflows","B":"Triggers","C":"Job bookmarks","D":"Classifiers"},"timestamp":"2021-05-01 14:17:00","answer_ET":"C"},{"id":"2XJDUPjDgqyaBdghq4tf","answers_community":["A (100%)"],"exam_id":20,"answer_description":"","answer_images":[],"answer_ET":"A","unix_timestamp":1620056460,"answer":"A","question_text":"A telecommunications company is looking for an anomaly-detection solution to identify fraudulent calls. The company currently uses Amazon Kinesis to stream voice call records in a JSON format from its on-premises database to Amazon S3. The existing dataset contains voice call records with 200 columns. To detect fraudulent calls, the solution would need to look at 5 of these columns only.\nThe company is interested in a cost-effective solution using AWS that requires minimal effort and experience in anomaly-detection algorithms.\nWhich solution meets these requirements?","question_id":148,"discussion":[{"upvote_count":"29","comment_id":"354521","timestamp":"1633228680.0","content":"OPTION A: Amazon QuickSight uses proven Amazon technology to continuously run ML-powered anomaly detection across millions of metrics to discover hidden trends and outliers in your data. This anomaly detection enables you to get deep insights that are often buried in the aggregates and not scalable with manual analysis.","poster":"Heer"},{"content":"Correct answer is A as only limited columns are required it would be best to convert the data in columnar format and expose it through Athena to QuickSight for anomaly detection. Using parquet with Athena helps provide a cost-effective solution using QuickSight requires minimal effort and experience in anomaly-detection algorithms.\n\nLink : https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html\n\nOption B is wrong as Kinesis Data Firehose does not provide analytic functions and cannot be used for anamoly detection\n\nOption C is wrong as using SageMaker would require more effort.\n\nOption D is wrong as Kinesis Data Analytics does not integrate with QuickSight directly.","comment_id":"705437","timestamp":"1666866060.0","upvote_count":"7","poster":"cloudlearnerhere"},{"content":"A is the best among worst but still wrong. Why the need of creating Athena table again on top of Catalog? Its already Parquet.","upvote_count":"1","comment_id":"908377","timestamp":"1685254800.0","poster":"Debi_mishra"},{"timestamp":"1682958540.0","poster":"pk349","content":"A: I passed the test","comment_id":"886491","upvote_count":"1"},{"timestamp":"1663614540.0","upvote_count":"1","poster":"James_Mao","comment_id":"673575","content":"C :\nhttps://aws.amazon.com/blogs/big-data/detecting-anomalous-values-by-invoking-the-amazon-athena-machine-learning-inference-function/"},{"content":"Selected Answer: A\nAnswer - A","upvote_count":"1","comment_id":"637085","timestamp":"1658805120.0","poster":"rocky48"},{"content":"Answer - A","poster":"jrheen","comment_id":"595327","timestamp":"1651357380.0","upvote_count":"1"},{"timestamp":"1648909500.0","poster":"astalavista1","comment_id":"579889","content":"Selected Answer: A\nML-powered anomaly detection.","upvote_count":"3"},{"content":"The question is based on below blog and answer is A:\nhttps://aws.amazon.com/blogs/big-data/detect-fraudulent-calls-using-amazon-quicksight-ml-insights/","comment_id":"510304","timestamp":"1640609760.0","poster":"yogen","upvote_count":"3"},{"content":"Answer is A\nOption D also wrong as QuickSight cannot connect to any of Kinesis services.","upvote_count":"4","timestamp":"1638197820.0","comment_id":"489987","poster":"Thiya"},{"content":"You can use D also but less effort = Option A (Quicksight)\nhttps://aws.amazon.com/blogs/big-data/detect-fraudulent-calls-using-amazon-quicksight-ml-insights/","timestamp":"1637424180.0","upvote_count":"1","poster":"aws2019","comment_id":"482681"},{"comment_id":"480463","timestamp":"1637213700.0","upvote_count":"1","poster":"goutes","content":"A because : Quicksight Machine Learning Insights can use random_cut_forest ."},{"timestamp":"1635646680.0","upvote_count":"1","content":"Option A based on https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html","comment_id":"446272","poster":"ThomasKalva"},{"timestamp":"1635290400.0","content":"Answer is A.","poster":"Donell","comment_id":"406393","upvote_count":"1"},{"poster":"Huy","content":"A is correct.\nB wrong. Kinesis Data Firehose has no analytic function\nD wrong. SQL is not for anomaly detection and can't output to Quicksight\nC works but more effort.","comment_id":"389712","timestamp":"1635014040.0","upvote_count":"1"},{"comment_id":"370992","poster":"KingD","content":"Best answer is A. kindly refer to documentation below for confirmation thanks.\nhttps://aws.amazon.com/quicksight/features-ml/","timestamp":"1634912340.0","upvote_count":"2"},{"comment_id":"365795","comments":[{"poster":"asg76","timestamp":"1633638420.0","content":"but what about \"Connect Amazon QuickSight to Kinesis Data Analytics to visualize the anomaly scores\" ? Not sure if you can connect Quicksight to Data Analytics like this. So A seems to be a more logical answer.","comment_id":"367542","upvote_count":"2"},{"comment_id":"380876","timestamp":"1634983500.0","comments":[{"content":"Answer is A. from d above quicksight docs: External destination – You can persist data to a Kinesis data stream, a Kinesis Data Firehose delivery stream, or a Lambda function.","comment_id":"500250","timestamp":"1639346100.0","upvote_count":"1","poster":"tobsam"}],"poster":"shato944","content":"Kinesis Data Analytics can write only to three services:\n - Firehose\n - Kinesis data streams\n - lambda\nSee https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-output.html\n\nThere is no way to write to QuickSight","upvote_count":"3"}],"poster":"ariane_tateishi","upvote_count":"3","content":"D should be the right answer, considering that the requirement is develop a cost-effective solution using AWS that requires minimal effort and experience in anomaly-detection algorithms. With the option A we need the glue, athena and quicksight. With option D we only need stream analytics (using SQL sintax, very easy) and the quicksight.\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html","timestamp":"1633451160.0"},{"upvote_count":"3","content":"Should be A.","poster":"VikG12","timestamp":"1633034040.0","comment_id":"348762"}],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/51704-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2021-05-03 17:41:00","isMC":true,"question_images":[],"choices":{"C":"Use an AWS Glue job to transform the data from JSON to Apache Parquet. Use AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Use Amazon SageMaker to build an anomaly detection model that can detect fraudulent calls by ingesting data from Amazon S3.","B":"Use Kinesis Data Firehose to detect anomalies on a data stream from Kinesis by running SQL queries, which compute an anomaly score for all calls and store the output in Amazon RDS. Use Amazon Athena to build a dataset and Amazon QuickSight to visualize the results.","D":"Use Kinesis Data Analytics to detect anomalies on a data stream from Kinesis by running SQL queries, which compute an anomaly score for all calls. Connect Amazon QuickSight to Kinesis Data Analytics to visualize the anomaly scores.","A":"Use an AWS Glue job to transform the data from JSON to Apache Parquet. Use AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Use Amazon Athena to create a table with a subset of columns. Use Amazon QuickSight to visualize the data and then use Amazon QuickSight machine learning-powered anomaly detection."}},{"id":"n51lgzFxL8nzdDeCjIfG","choices":{"B":"The stream's value for the IteratorAgeMilliseconds metric is too high.","A":"The producer has a network-related timeout.","E":"The max_records configuration property was set to a number that is too high.","D":"The AggregationEnabled configuration property was set to true.","C":"There was a change in the number of shards, record processors, or both."},"answer_images":[],"question_images":[],"question_text":"An online retailer is rebuilding its inventory management system and inventory reordering system to automatically reorder products by using Amazon Kinesis Data\nStreams. The inventory management system uses the Kinesis Producer Library (KPL) to publish data to a stream. The inventory reordering system uses the\nKinesis Client Library (KCL) to consume data from the stream. The stream has been configured to scale as needed. Just before production deployment, the retailer discovers that the inventory reordering system is receiving duplicated data.\nWhich factors could be causing the duplicated data? (Choose two.)","isMC":true,"timestamp":"2021-05-01 14:16:00","answers_community":["AC (100%)"],"exam_id":20,"url":"https://www.examtopics.com/discussions/amazon/view/51409-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_description":"","unix_timestamp":1619871360,"answer_ET":"AC","topic":"1","question_id":149,"answer":"AC","discussion":[{"poster":"VikG12","upvote_count":"32","content":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\nA,C","timestamp":"1635801060.0","comment_id":"348768"},{"timestamp":"1655612760.0","poster":"dushmantha","comment_id":"618516","upvote_count":"15","content":"Duplication can happen in two ways, either in producer side or consumer side, obviously. \nIn proudcer side it happens due to network delays/timeouts, specifically producer is waiting for an successful acknowledgement, yet it is lost due to network failure, and producer sends data again until it receives acknowledgement.\nIn consumer side it happens due to record processor restart. This can happen due to 4 reasons. A worker terminates unexpectedly, Worker instances are added or removed, Shards are merged or split, application is deployed.\nIn any of the situations the best way to prevent duplicates is to have a unique identifier in data.\nAns should be A, C"},{"poster":"Saintu","timestamp":"1694379420.0","content":"Correct ans is BC, A network-related timeout for the producer could potentially lead to failed record ingestion, but it is less likely to cause duplicated data.","comment_id":"1004297","upvote_count":"1"},{"timestamp":"1682958600.0","upvote_count":"1","poster":"pk349","comment_id":"886493","content":"AC: I passed the test"},{"content":"Selected Answer: AC\nA&C are the correct answers.\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html","poster":"Ody__","upvote_count":"2","timestamp":"1673039940.0","comment_id":"768088"},{"timestamp":"1666866240.0","comments":[{"comment_id":"705444","timestamp":"1666866240.0","upvote_count":"2","content":"Consider a producer that experiences a network-related timeout after it makes a call to PutRecord, but before it can receive an acknowledgement from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have been written to retry the call with the same data. If both PutRecord calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records. Although the two records have identical data, they also have unique sequence numbers. Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries.\n\nConsumer (data processing application) retries happen when record processors restart.","poster":"cloudlearnerhere","comments":[{"upvote_count":"2","content":"Record processors for the same shard restart in the following cases:\n\n A worker terminates unexpectedly\n Worker instances are added or removed\n Shards are merged or split\n The application is deployed","timestamp":"1666866300.0","poster":"cloudlearnerhere","comment_id":"705445"}]}],"content":"Correct answers are A & C as there is a chance of duplicates in case a producer retries cause of network connectivity issues or consumer retries when a shard is merged or split. \n\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\n\nThere are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer retries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times.","comment_id":"705443","poster":"cloudlearnerhere","upvote_count":"2"},{"timestamp":"1658428140.0","content":"answer is A - AS Archive objects that are queried by S3 Glacier Select must be in uncompressed comma-separated values (CSV).","poster":"Raje14k","upvote_count":"1","comment_id":"634760"},{"content":"Selected Answer: AC\nA&C is the correct answer","poster":"tweeeeeeety","upvote_count":"1","comment_id":"620326","timestamp":"1655892540.0"},{"upvote_count":"1","poster":"zinic","comment_id":"616310","timestamp":"1655225160.0","content":"Selected Answer: AC\nAC should be the correct one"},{"upvote_count":"1","timestamp":"1653428580.0","poster":"YahiaAglan74","content":"Selected Answer: AC\nAC is the correct answer","comment_id":"606918"},{"comment_id":"557183","content":"Selected Answer: AC\nit's made clear in the statement that the problem showed up when the solution was deployed to production, and connection timeouts are another option that can cause that issue","timestamp":"1645946220.0","poster":"simo40010","upvote_count":"2"},{"timestamp":"1645374240.0","content":"Selected Answer: AC\nA and C are correct.","poster":"t_singh","comment_id":"552007","upvote_count":"1"},{"comment_id":"546387","upvote_count":"1","poster":"RSSRAO","content":"A and C Are correct","timestamp":"1644747720.0"},{"timestamp":"1636926840.0","content":"A and C","upvote_count":"1","poster":"aws2019","comment_id":"478357"},{"upvote_count":"3","comment_id":"367847","timestamp":"1635826020.0","poster":"Monika14Sharma","content":"For sure A and C"},{"comment_id":"346929","content":"I am ok for A and C","timestamp":"1635139980.0","poster":"bermo","upvote_count":"3"}]},{"id":"7Ies6PHP1C228mBLPENS","discussion":[{"upvote_count":"23","poster":"VikG12","content":"Looks like it's D.\nhttps://github.com/awsdocs/amazon-redshift-developer-guide/issues/21","comments":[{"poster":"lakediver","content":"Agree \nfor further reading please see \nhttps://aws.amazon.com/about-aws/whats-new/2018/12/amazon-redshift-automatic-vacuum/","comment_id":"505841","upvote_count":"4","timestamp":"1640066100.0"}],"comment_id":"348773","timestamp":"1634278740.0"},{"content":"D: I passed the test","upvote_count":"1","comment_id":"886494","timestamp":"1682958660.0","poster":"pk349"},{"content":"Correct answer is D as the reason for the drop in query performance might because of the long nightly data refreshes operation which blocked the vacuum operation and left it in the need of the vacuum operation. \n\nAmazon Redshift can automatically sort and perform a VACUUM DELETE operation on tables in the background. To clean up tables after a load or a series of incremental updates, you can also run the VACUUM command, either against the entire database or against individual tables.\n\nA vacuum operation might not be able to start if a load or insert operation is already in progress. Vacuum operations temporarily require exclusive access to tables in order to start. This exclusive access is required briefly, so vacuum operations don't block concurrent loads and inserts for any significant period of time.","upvote_count":"4","comment_id":"705478","timestamp":"1666869900.0","poster":"cloudlearnerhere"},{"poster":"lygf","comment_id":"680388","upvote_count":"1","content":"Selected Answer: D\nC. if there is a lingering transaction, it will block other read queries or transactions. The Question said the next day query ran slow, not being blcoked, so C is out.","timestamp":"1664253180.0"},{"comment_id":"640571","content":"Selected Answer: D\nA is always can be but as fas as it worked before night transaction A and B is out.\nC is rear case because typically transactions block dara read and not making redshift slower( due to wlm and queues it hardly a case) after any DML or DDL operation there is a lot of junk an need of vacume, During vacuum there can be seom perfomance issue for queries that tryes to use that table that is why redshift autowacuum doesn't start when there is high user activity. So we need to vacuum explisitly to make things faster.","upvote_count":"1","poster":"arboles","timestamp":"1659350760.0"},{"timestamp":"1658804580.0","content":"Selected Answer: D\nCorrect Answer is D","poster":"rocky48","upvote_count":"1","comment_id":"637081"},{"content":"C sounds correct answer. It could be between C and D. But considering Redshift automatically vacuumes the tables periodically, and that user work load is the reason for the slowness, I would pick C over D.","timestamp":"1653184320.0","comment_id":"605078","poster":"certificationJunkie","upvote_count":"1"},{"timestamp":"1653129900.0","poster":"Bik000","upvote_count":"1","content":"Selected Answer: D\nAnswer is D","comment_id":"604815","comments":[{"upvote_count":"1","poster":"Bik000","content":"Sorry, Answer could be B","timestamp":"1653204960.0","comment_id":"605240"}]},{"comment_id":"595323","timestamp":"1651356840.0","upvote_count":"2","poster":"jrheen","content":"Answer-D"},{"timestamp":"1647776820.0","poster":"ShilaP","content":"C is equally possible.. Cannot rule it out....","comment_id":"571574","upvote_count":"1"},{"comment_id":"482654","content":"ans cloud be D \nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html","upvote_count":"2","timestamp":"1637421540.0","poster":"aws2019"},{"content":"D should be the answer but the Correct Answer shows B. Ridiculous.","comments":[{"timestamp":"1636063080.0","content":"welcome to examtopics....","poster":"Hariru","upvote_count":"14","comment_id":"414771"}],"timestamp":"1635743400.0","comment_id":"414272","upvote_count":"1","poster":"nirmalmarathon"},{"timestamp":"1635240420.0","comment_id":"367848","poster":"Monika14Sharma","content":"Correct Answer is D","upvote_count":"3"},{"timestamp":"1632794760.0","comment_id":"346926","content":"I choose D","upvote_count":"3","poster":"bermo"}],"choices":{"A":"The dashboards are suffering from inefficient SQL queries.","D":"The nightly data refreshes left the dashboard tables in need of a vacuum operation that could not be automatically performed by Amazon Redshift due to ongoing user workloads.","B":"The cluster is undersized for the queries being run by the dashboards.","C":"The nightly data refreshes are causing a lingering transaction that cannot be automatically closed by Amazon Redshift due to ongoing user workloads."},"unix_timestamp":1619871300,"answer_description":"","answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/51408-exam-aws-certified-data-analytics-specialty-topic-1-question/","question_id":150,"question_images":[],"answer_images":[],"exam_id":20,"timestamp":"2021-05-01 14:15:00","answer_ET":"D","topic":"1","question_text":"A large retailer has successfully migrated to an Amazon S3 data lake architecture. The company's marketing team is using Amazon Redshift and Amazon\nQuickSight to analyze data, and derive and visualize insights. To ensure the marketing team has the most up-to-date actionable information, a data analyst implements nightly refreshes of Amazon Redshift using terabytes of updates from the previous day.\nAfter the first nightly refresh, users report that half of the most popular dashboards that had been running correctly before the refresh are now running much slower. Amazon CloudWatch does not show any alerts.\nWhat is the MOST likely cause for the performance degradation?","answers_community":["D (100%)"],"isMC":true}],"exam":{"name":"AWS Certified Data Analytics - Specialty","isImplemented":true,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true,"id":20,"numberOfQuestions":164,"provider":"Amazon"},"currentPage":30},"__N_SSP":true}