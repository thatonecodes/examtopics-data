{"pageProps":{"questions":[{"id":"oEGF6RqNJgW1634SiBdZ","choices":{"B":"Amazon SageMaker notebook instances are based on the Amazon ECS service within customer accounts.","A":"Amazon SageMaker notebook instances are based on the EC2 instances within the customer account, but they run outside of VPCs.","D":"Amazon SageMaker notebook instances are based on AWS ECS instances running within AWS service accounts.","C":"Amazon SageMaker notebook instances are based on EC2 instances running within AWS service accounts."},"answer":"C","question_images":[],"discussion":[{"upvote_count":"24","timestamp":"1632671520.0","poster":"mlyu","content":"I think the answer should be C","comment_id":"36222"},{"comments":[{"poster":"scuzzy2010","upvote_count":"10","timestamp":"1634320800.0","comment_id":"147770","content":"Can't be A because A says \"but they run outside of VPCs\", which is not correct. They are attached to VPC, but it can either be AWS Service VPC or Customer VPC, or Both, as per the explanation url you provided.","comments":[{"upvote_count":"1","timestamp":"1635930540.0","comment_id":"278012","poster":"cloud_trail","content":"This is exactly right. According to that document, if the notebook instance is not in a customer VPC, then it has to be in the Sagemaker managed VPC. See Option 1 in that document."}]},{"poster":"mawsman","timestamp":"1633618440.0","content":"Actually your link says: The notebook instance is running in an Amazon SageMaker managed VPC as shown in the above diagram. That means the correct answer is C. An Amazon SageMaker managed VPC can only be created in an Amazon managed Account.","comment_id":"88375","upvote_count":"18"}],"comment_id":"73107","poster":"dhs227","timestamp":"1633280400.0","upvote_count":"18","content":"The correct answer HAS TO be A\n\nThe instances are running in customer accounts but it's in an AWS managed VPC while exposing ENI to customer VPC if it was chosen.\nSee explanation at https://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/"},{"upvote_count":"5","timestamp":"1739653740.0","comment_id":"1357048","content":"Selected Answer: C\nC. Amazon SageMaker notebook instances are based on EC2 instances running within AWS service accounts. \nWhy?\nAmazon SageMaker does use EC2 instances, but they are not directly managed within the customer's AWS account.\nInstead, these instances are provisioned within AWS-managed service accounts, which is why they do not appear within the customer’s VPC or EC2 console.\nThe only way to access the underlying EBS volume is via SageMaker APIs, rather than the EC2 console.","poster":"JonSno"},{"content":"Selected Answer: B\nAlthough I'd go with Glue and option B I'm pretty sure that this is one of those \"15 unscored questions that do not affect your score. AWS collects information about performance on these unscored questions to evaluate these questions for future use as scored questions\"\n\nJust for fun I asked perplexity, chatgpt, gemini, deepseek and claude: all gave D as first response\n\nWhen I pointed out that \"according to this https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html Kinesis can't convert directly cvs to parquet. It needs a Lambda\" each model responded in a different way (some of them contradictory). \n\nMy reasoning is that D (Kinesis + Firehose) is incorrect because Firehose does not support direct CSV-to-Parquet conversion and needs a Lambda not mentioned in the option. But discussing about questions like this one is nothing but I big waste of time ;-P","comment_id":"1356213","timestamp":"1739466180.0","upvote_count":"1","comments":[{"upvote_count":"1","poster":"liquen14","content":"Forget about this please I posted this here incorrectly. This corresponds to Question 3. Apologies","comment_id":"1356592","timestamp":"1739564700.0"}],"poster":"liquen14"},{"content":"Selected Answer: A\nAmazon SageMaker notebook instances are indeed based on EC2 instances, but they are managed by the SageMaker service and do not appear as standard EC2 instances in the customer's VPC. Instead, they run in a managed environment that abstracts away the underlying EC2 instances, which is why the ML Specialist cannot see the instance in the VPC.","comment_id":"1327514","poster":"reginav","upvote_count":"1","timestamp":"1734367320.0"},{"poster":"Mickey321","timestamp":"1727163900.0","content":"Selected Answer: C\nThe explanation for this choice is that Amazon SageMaker notebook instances are fully managed by AWS and run on EC2 instances that are not visible to customers. These EC2 instances are launched in AWS-owned accounts and are isolated from customer accounts by using AWS PrivateLink1. This means that customers cannot access or manage these EC2 instances directly, nor can they see the EBS volumes attached to them.","upvote_count":"1","comment_id":"973033"},{"upvote_count":"6","timestamp":"1727163900.0","poster":"loict","comment_id":"1006276","content":"Selected Answer: C\nA. NO - AEC2 instances within the customer account are necessarily in a VPCb\nB. NO - Amazon ECS service is not within customer accounts\nC. YES - EC2 instances running within AWS service accounts are not visible to customer account\nD. NO - SageMaker manages EC2 instance, not ECS"},{"timestamp":"1727163900.0","poster":"ninomfr64","upvote_count":"1","content":"Selected Answer: C\nA. NO. If the EC2 instance of the notebook was in the customer account, customer would be able to see it. Also, \"they run outside VPCs\" isn't true as they run in service managed VPC or can be also attached to customer provided VPC -> https://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/\nB. NO, Notebooks are based on EC2 + EBS\nC. YES -> https://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/\nD. NO, Notebooks are based on EC2 + EBS\n\nI also actually tested it in my account: I created a Notebook and attached it to my VPC, I was not able to see the EC2 instance behind the Notebook but I was able to see the its ENI with the following description \"[Do not delete] Network Interface created to access resources in your VPC for SageMaker Notebook Instance ...\"","comment_id":"1230836"},{"upvote_count":"1","comment_id":"1002093","timestamp":"1694146260.0","poster":"Rejju","content":"Selected Answer: A\nalready given below"},{"comment_id":"1001324","upvote_count":"1","content":"I am pretty sure the answer is A : Amazon SageMaker notebook instances are indeed based on EC2 instances, and these instances are within your AWS customer account. However, by default, SageMaker notebook instances run outside of your VPC (Virtual Private Cloud), which is why they may not be visible within your VPC. SageMaker instances are designed to be easily accessible for data science and machine learning tasks, which is why they typically do not reside within a VPC. If you need them to operate within a VPC, you can configure them accordingly, but this is not the default behavior.","timestamp":"1694074380.0","poster":"Rejju"},{"upvote_count":"1","comment_id":"961671","content":"Selected Answer: C\nI think it should be c","timestamp":"1690207320.0","poster":"Venkatesh_Babu"},{"content":"Per https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-and-internet-access.html it's C","poster":"ADVIT","timestamp":"1687898100.0","upvote_count":"1","comment_id":"935821"},{"content":"Selected Answer: C\nNotebooks can run inside AWS managed VPC or customer managed VPC","poster":"BeCalm","upvote_count":"1","timestamp":"1683661920.0","comment_id":"893404"},{"upvote_count":"5","timestamp":"1680868200.0","content":"Selected Answer: C\nC, check the digram in https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-and-internet-access.html","comment_id":"863784","poster":"Maaayaaa"},{"timestamp":"1680549840.0","comments":[{"timestamp":"1683557880.0","content":"what you described is C \n\"This is because the EC2 instance is managed by AWS, and it is outside of the VPC.\"","comment_id":"892275","upvote_count":"1","poster":"ZSun"}],"poster":"oso0348","comment_id":"860290","upvote_count":"2","content":"Selected Answer: A\nWhen a SageMaker notebook instance is launched in a VPC, it creates an Elastic Network Interface (ENI) in the subnet specified, but the underlying EC2 instance is not visible in the VPC. This is because the EC2 instance is managed by AWS, and it is outside of the VPC. The ENI acts as a bridge between the VPC and the notebook instance, allowing network connectivity between the notebook instance and other resources in the VPC. Therefore, the EBS volume of the notebook instance is also not visible in the VPC, and you cannot take a snapshot of the volume using VPC-based tools. Instead, you can create a snapshot of the EBS volume directly from the SageMaker console, AWS CLI, or SDKs."},{"timestamp":"1678262820.0","content":"Selected Answer: C\nNotebooks run inside a VPC not outside!","comment_id":"832661","upvote_count":"1","poster":"Valcilio"},{"timestamp":"1676799720.0","comment_id":"813917","content":"Selected Answer: C\nDefinitely C","upvote_count":"2","poster":"krzyhoo"},{"timestamp":"1672061820.0","upvote_count":"1","comment_id":"757477","poster":"rrshah83","content":"Selected Answer: C\nSagemaker notebook instances run in AWS sagemaker service accounts: \n https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-and-internet-access.html\n\nAdditionally, A is wrong because sagemaker instances run in a VPC (not outside)... It is just in aws service account"},{"timestamp":"1656428220.0","comment_id":"624035","poster":"ovokpus","upvote_count":"3","content":"Selected Answer: A\nAWS Managed services do not happen outside the customer account. So the Sagemaker Managed Instance is primarily attached to a Sagemaker managed VPC inside a customer account. There has never been a hint of cross-account operations taking place in basic sagemaker operations. And this will not be the only AWS managed service situated inside a customer AWS account.\n\nA is the logical answer here. He cannot see the instance, because it will not show up in the EC2 console, especially if it is not attached to any customer VPC"},{"timestamp":"1636130160.0","poster":"technoguy","content":"C is correct","upvote_count":"4","comment_id":"439514"},{"content":"I will go with C, as \"but they run outside of VPCs\" is wrong","upvote_count":"2","timestamp":"1635995220.0","poster":"AShahine21","comment_id":"371604"},{"upvote_count":"5","content":"Answer C. I've gone back and forth on this but reading the info at the link below, the customer notebook instance has to run either in a customer managed VPC or the Sagemaker service managed VPC. Either way, it's in a VPC. https://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/","timestamp":"1635828900.0","comment_id":"278010","poster":"cloud_trail"},{"timestamp":"1635582900.0","upvote_count":"1","comment_id":"275539","content":"the answer should be C: see this link: https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-and-internet-access.html","poster":"Joe_Zhang"},{"poster":"crispogioele","comments":[{"poster":"cnethers","content":"I would agree that A is the answer. That is the same ref article I have looked at","timestamp":"1635193560.0","upvote_count":"1","comment_id":"271978"}],"content":"I think the answer is A. They are EC2 instance and they are attached to the customer VPC through an ENI (as shown in this link https://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/ )","comment_id":"260598","upvote_count":"2","timestamp":"1634906400.0"},{"content":"C is correct as it is Fully managed service so you cannot have OS level access to its resources.\nA-- cannot be correct because it says Sagemakers EC2 instances run with VPC which is not correct , as with VPC means governed by its network policies incase of Sagemaker","poster":"harmanbirstudy","upvote_count":"2","comment_id":"259892","timestamp":"1634399940.0"},{"comment_id":"137510","content":"C should be correct since the EC2 instance is not under customer manage","poster":"Achievement","timestamp":"1634280780.0","upvote_count":"2"},{"poster":"AdityaB","upvote_count":"5","content":"A is the right answer .","timestamp":"1634052720.0","comment_id":"100321"},{"poster":"roytruong","timestamp":"1633621140.0","upvote_count":"4","comment_id":"98664","content":"go for C"},{"poster":"cybe001","content":"C looks correct, all sagemaker runs on EC2\nhttps://aws.amazon.com/sagemaker/pricing/instance-types/","upvote_count":"2","comment_id":"37747","timestamp":"1633214520.0"}],"answer_ET":"C","isMC":true,"answers_community":["C (75%)","A (22%)","3%"],"exam_id":26,"unix_timestamp":1578374700,"question_text":"A Machine Learning Specialist is using an Amazon SageMaker notebook instance in a private subnet of a corporate VPC. The ML Specialist has important data stored on the Amazon SageMaker notebook instance's Amazon EBS volume, and needs to take a snapshot of that EBS volume. However, the ML Specialist cannot find the Amazon SageMaker notebook instance's EBS volume or Amazon EC2 instance within the VPC.\nWhy is the ML Specialist not seeing the instance visible in the VPC?","answer_images":[],"topic":"1","timestamp":"2020-01-07 06:25:00","question_id":326,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/11559-exam-aws-certified-machine-learning-specialty-topic-1/"},{"id":"sSzwn9e7fPG1SBV9BOfu","timestamp":"2019-11-17 07:32:00","exam_id":26,"question_text":"A Machine Learning Specialist is assigned a TensorFlow project using Amazon SageMaker for training, and needs to continue working for an extended period with no Wi-Fi access.\nWhich approach should the Specialist use to continue working?","discussion":[{"timestamp":"1649155740.0","poster":"DonaldCMLIN","content":"ANSWER B. \n\nYOU COULD INSTALL DOCKER-COMPOSE (AND NVIDIA-DOCKER IF TRAINING WITH A GPU) FOR LOCAL TRAINING\n\nHTTPS://SAGEMAKER.READTHEDOCS.IO/EN/STABLE/OVERVIEW.HTML#LOCAL-MODE\nHTTPS://GITHUB.COM/AWSLABS/AMAZON-SAGEMAKER-EXAMPLES/BLOB/MASTER/SAGEMAKER-PYTHON-SDK/TENSORFLOW_DISTRIBUTED_MNIST/TENSORFLOW_LOCAL_MODE_MNIST.IPYNB","comment_id":"22115","upvote_count":"42","comments":[{"poster":"sqavi","content":"None of these links are working","comment_id":"803212","upvote_count":"3","timestamp":"1691577480.0"}]},{"content":"https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/\n\nB","poster":"VB","comment_id":"60917","upvote_count":"10","timestamp":"1650877260.0"},{"upvote_count":"1","poster":"ef12052","timestamp":"1743484440.0","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/\n\nstop using gpt....","comment_id":"1416266"},{"upvote_count":"1","content":"Selected Answer: D\nGPT said SageMaker Python SDK is less suitable for offline","poster":"sfwewv","timestamp":"1739468640.0","comment_id":"1356226"},{"comment_id":"1204525","upvote_count":"1","content":"Selected Answer: B\nCorrection it will be B, while D is possible, it cannot exactly mimic the sagemaker env, with docker all the configuration and libs will be available to the user which would be an ideal working setup for the DS to work with.","timestamp":"1730293920.0","poster":"rookiee1111"},{"timestamp":"1730293440.0","poster":"rookiee1111","comment_id":"1204517","upvote_count":"2","content":"Selected Answer: D\nYou can easily download the notebook instance, and work locally using jupyter notebook configured on your laptop\nwhich is one the advantages of using sagemaker, and that is what Amazon also promotes imo."},{"upvote_count":"2","poster":"ArchMelody","content":"Selected Answer: D\nBoth Amazon Q (AWS Expert) and ChatGPT insist on D. Plus all the links that I see here about Docker/Git and stuff, they either not working or deprecated so far. Not to mention their complexity to my eyes.\nThus, I will go for D.","comment_id":"1156593","timestamp":"1724341680.0"},{"content":"Selected Answer: B\nthe local mode of sagemaker SDK:\nhttps://sagemaker.readthedocs.io/en/stable/overview.html#local-mode\nB","timestamp":"1712991600.0","comment_id":"1042381","upvote_count":"2","poster":"rav009"},{"poster":"Mickey321","timestamp":"1709141280.0","upvote_count":"1","comment_id":"992340","content":"Selected Answer: B\nOption B"},{"comment_id":"938671","timestamp":"1703905080.0","poster":"ADVIT","upvote_count":"1","content":"B, \nhttps://github.com/aws/sagemaker-tensorflow-serving-container"},{"timestamp":"1694159280.0","poster":"Valcilio","upvote_count":"1","comment_id":"832787","content":"Selected Answer: B\nIt's B"},{"comment_id":"561443","timestamp":"1662378300.0","upvote_count":"2","poster":"SriAkula","content":"Answer : D"},{"poster":"noblate","comments":[{"timestamp":"1661289780.0","comments":[{"comments":[{"comment_id":"1204514","upvote_count":"1","poster":"rookiee1111","timestamp":"1730293320.0","content":"That is incorrect, once jupyter notebook is configured you can use it offline."}],"poster":"Tomatoteacher","timestamp":"1689512220.0","upvote_count":"1","comment_id":"777810","content":"Cannot be D. If you used Jupyter notebook, you are unable to use it without internet access."}],"content":"My assumption is that D there is no way to test the code. You need the Sagemaker SDK in order to utilize dockerized container of Tensorflow from Sagemaker is my best guess.","comment_id":"554940","upvote_count":"2","poster":"AddiWei"}],"timestamp":"1656508560.0","content":"why not D?","comment_id":"512312","upvote_count":"1"},{"content":"Agreed for B","upvote_count":"4","poster":"CMMC","comment_id":"109316","timestamp":"1651256700.0"}],"choices":{"B":"Download the TensorFlow Docker container used in Amazon SageMaker from GitHub to their local environment, and use the Amazon SageMaker Python SDK to test the code.","A":"Install Python 3 and boto3 on their laptop and continue the code development using that environment.","D":"Download the SageMaker notebook to their local environment, then install Jupyter Notebooks on their laptop and continue the development in a local notebook.","C":"Download TensorFlow from tensorflow.org to emulate the TensorFlow kernel in the SageMaker environment."},"url":"https://www.examtopics.com/discussions/amazon/view/8392-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"answers_community":["B (55%)","D (45%)"],"answer_description":"","topic":"1","answer_images":[],"question_id":327,"answer_ET":"B","unix_timestamp":1573972320,"answer":"B","question_images":[]},{"id":"FRue8Y2KLoY5YoCW5GM2","unix_timestamp":1573973340,"question_text":"A Machine Learning Specialist is working with a large cybersecurity company that manages security events in real time for companies around the world. The cybersecurity company wants to design a solution that will allow it to use machine learning to score malicious events as anomalies on the data as it is being ingested. The company also wants be able to save the results in its data lake for later processing and analysis.\nWhat is the MOST efficient way to accomplish these tasks?","discussion":[{"comments":[{"content":"Donald, do you know your CAPS LOCK has been on the whole time?","comments":[{"comments":[{"timestamp":"1706627820.0","poster":"ccpmad","upvote_count":"1","comment_id":"967142","content":"yes, but it works with minus also..."}],"poster":"Nadia0012","timestamp":"1694162460.0","upvote_count":"4","comment_id":"832828","content":"I know why his caps lock has been on :D to enter the \"I am not robot\" code easier :D"}],"timestamp":"1687126140.0","poster":"hamimelon","upvote_count":"15","comment_id":"749312"}],"poster":"DonaldCMLIN","timestamp":"1648042620.0","comment_id":"22120","content":"I WOULD LIKE TO CHOOSE ANSWER A.\n\nhttps://aws.amazon.com/tw/blogs/machine-learning/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection/","upvote_count":"60"},{"upvote_count":"15","comments":[{"comment_id":"1326612","timestamp":"1734210960.0","poster":"Shakespeare","upvote_count":"2","content":"I think it would have been more accurate if the options were kinetic data stream -> kinesis data analytics -> kinesis firehose -> S3"}],"comment_id":"35304","poster":"JayK","timestamp":"1648842240.0","content":"Answer is A. As the word anamoly talks about Random Cut Forest in the exam and that can be done in a cost effective manner using Kinesis Data Analytics"},{"timestamp":"1729059540.0","comment_id":"1196382","content":"The question says REAL TIME events doesn't that eliminate Data Firehose as it is technically NEAR real time but not real time like Data Stream? Though Random Cut Forest seems like the best option for anomaly detection. I'm torn between A and B","poster":"saclim","upvote_count":"1"},{"upvote_count":"1","poster":"vkbajoria","content":"Selected Answer: A\nKinesis Firehose and Data Analytics with random cut forest should do it.","comment_id":"1187710","timestamp":"1727824980.0"},{"timestamp":"1720480200.0","content":"A.\nBased on these considerations, Option A is the most efficient way to accomplish the tasks. It provides a seamless, real-time data ingestion and processing pipeline, leverages machine learning for anomaly detection, and efficiently stores data in a data lake, meeting all the key requirements of the cybersecurity company.","poster":"phdykd","comment_id":"1117136","upvote_count":"1"},{"comment_id":"1067033","content":"Selected Answer: A\nONLY A","poster":"elvin_ml_qayiran25091992razor","upvote_count":"1","timestamp":"1715320320.0"},{"comment_id":"1057104","upvote_count":"2","timestamp":"1714411740.0","content":"Selected Answer: A\nB not as efficient for real-time processing and storing results as using Kinesis services.","poster":"sonoluminescence"},{"upvote_count":"1","poster":"DimLam","comments":[{"poster":"Dun6","content":"KDF does support KDA as destination","upvote_count":"1","timestamp":"1716279240.0","comment_id":"1076186"}],"comment_id":"1054324","content":"Selected Answer: B\nAt least B is a possible solution, but A will not work as KDF doesn't support KDA as a destination service https://docs.aws.amazon.com/firehose/latest/dev/create-name.html . In my opinion, KDF should always be the latest Kinesis Service in a streaming pipeline","timestamp":"1714111980.0"},{"timestamp":"1712993040.0","content":"Selected Answer: A\nA has all the required steps","comment_id":"1042404","poster":"AmeeraM","upvote_count":"1"},{"timestamp":"1710407640.0","upvote_count":"1","content":"Selected Answer: A\nA. YES - Firehose can pipe into KDA, and KDA supports RCF\nB. NO - RCF best for anomality detection\nC. NO - no need for intermediary S3 storage\nD. NO - no need for intermediary S3 storage","poster":"loict","comment_id":"1007316"},{"content":"Selected Answer: A\noption A","comment_id":"992353","poster":"Mickey321","timestamp":"1709142780.0","upvote_count":"1"},{"comment_id":"969353","content":"Selected Answer: A\nA is the correct. One tip for the exam: When you see Data Streaming, possibly the solution should contains a Kinesis Service. B is too much complex!","timestamp":"1706824440.0","poster":"kaike_reis","upvote_count":"3"},{"comment_id":"930545","content":"Selected Answer: A\nMakes sense to select A here.","poster":"nilmans","timestamp":"1703257740.0","upvote_count":"1"},{"upvote_count":"1","poster":"earthMover","comment_id":"906168","timestamp":"1700869980.0","content":"Selected Answer: A\nI strongly believe A is the right answer. At a minimum there should be some justification provided for your answer."},{"comments":[{"timestamp":"1714111740.0","content":"The problem with A, is that there is that KDF doesn't support KDA as a destination service https://docs.aws.amazon.com/firehose/latest/dev/create-name.html . In my opinion, KDF should always be the latest Kinesis Service in a streaming pipeline","upvote_count":"1","poster":"DimLam","comment_id":"1054321"}],"comment_id":"804608","content":"Selected Answer: A\nAmazon Kinesis Data Firehose is a fully managed service for streaming real-time data to Amazon S3 and can handle the ingestion of large amounts of data in real time. Kinesis Data Analytics Random Cut Forest (RCF) is a fully managed service that can be used to perform anomaly detection on streaming data, making it well suited for this use case. The results of the anomaly detection can then be streamed to Amazon S3 using Kinesis Data Firehose, providing a scalable and cost-effective data lake for later processing and analysis.","timestamp":"1691681220.0","upvote_count":"2","poster":"AjoseO"},{"content":"I would select A","upvote_count":"1","poster":"OssamaAbdelatif","comment_id":"728483","timestamp":"1685202000.0"},{"content":"Selected Answer: A\nB is too resource intensive for that use case. I choose A, but I think the data should be better ingested using Kinesis streams","comment_id":"621695","poster":"ovokpus","upvote_count":"3","timestamp":"1671897600.0"},{"upvote_count":"2","comment_id":"551174","poster":"EuTuXia","timestamp":"1660922700.0","content":"Selected Answer: A\nAns is A, but I have a doubt: firehose is not real-time (it's near real time) so as the data are ingested using firehose, we are not meeting the requirements, can someone pls explain this? thx"},{"content":"Selected Answer: A\nA is 100% correct.","upvote_count":"2","poster":"apprehensive_scar","comment_id":"540613","timestamp":"1659635100.0"},{"comment_id":"400339","timestamp":"1651862760.0","upvote_count":"2","poster":"Huy","content":"A. Only A address all requirements. https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html"},{"timestamp":"1651602180.0","content":"B is possible But A is correct i think","upvote_count":"1","comment_id":"374060","poster":"btsql"},{"content":"Most efficient, streaming = A","upvote_count":"2","timestamp":"1651425060.0","poster":"yeetusdeleetus","comment_id":"212478"},{"comments":[{"comment_id":"623072","content":"Answer should be B https://stackoverflow.com/questions/65617845/obtaining-k-means-centroids-and-outliers-in-python-pyspark","timestamp":"1672126500.0","upvote_count":"3","poster":"ogm1"},{"timestamp":"1651456740.0","upvote_count":"1","content":"\"as it is being ingested\" is not necessarily realtime.. it could be near realtime :)","poster":"NotAnMLProfessional","comment_id":"312934"}],"upvote_count":"2","poster":"KK3","content":"The answer is B since the requirement was real time and KFH is near real time i.e it has 60 sec latency","timestamp":"1651262460.0","comment_id":"209469"},{"poster":"fhuadeen","upvote_count":"2","comment_id":"173945","timestamp":"1650356400.0","content":"A doesn't seem correct actually. There is no service in AWS that is called \"Amazon Kinesis Data Analytics Random Cut Forest (RCF)\" for anomaly detection. Pay close attention to that phrase, there is nothing separating them, it is like a full name for one service.","comments":[{"poster":"RLai","timestamp":"1686896220.0","comment_id":"746951","content":"There is RCF in KDA.\n\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html","upvote_count":"2"},{"upvote_count":"2","content":"There is actually. Please check\n\nhttps://www.google.com/url?sa=t&source=web&rct=j&url=https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html&ved=2ahUKEwj_x-_pzovsAhXFURUIHYlvCssQFjAAegQIDRAC&usg=AOvVaw2a7_hUr7fxkO40fd_2eF5P","poster":"Predicare","timestamp":"1650759480.0","comment_id":"188899"}]},{"comment_id":"169156","content":"Answer is A 100%; stream the results to S3 would answer the part on \"save the results in its data lake\" from the question","timestamp":"1650194520.0","upvote_count":"3","poster":"syu31svc"},{"poster":"PRC","timestamp":"1649914560.0","upvote_count":"3","comment_id":"65491","content":"A is the answer..Anomaly RCF, Real time Kinesis Analytics..Data Lake - S3 via Firehose"},{"comment_id":"51117","poster":"Phong","timestamp":"1649844720.0","content":"A is the best suitable answer","upvote_count":"4"},{"content":"It's A. B doesn't put the data into the datalake in the end, which was one of the requirements.","poster":"devsean","upvote_count":"4","comment_id":"50262","timestamp":"1649313780.0"},{"upvote_count":"5","timestamp":"1648272420.0","comment_id":"28776","comments":[{"poster":"ComPah","content":"Doesn't Key Word Large means it needs distributed architecture SPARK","timestamp":"1648343220.0","comment_id":"34986","upvote_count":"1"}],"poster":"vetal","content":"It depends on what \"most efficient\" means. The simplest solution is A - and it still supports all the requirements."}],"exam_id":26,"question_id":328,"timestamp":"2019-11-17 07:49:00","url":"https://www.examtopics.com/discussions/amazon/view/8394-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","topic":"1","question_images":[],"choices":{"C":"Ingest the data and store it in Amazon S3. Use AWS Batch along with the AWS Deep Learning AMIs to train a k-means model using TensorFlow on the data in Amazon S3.","D":"Ingest the data and store it in Amazon S3. Have an AWS Glue job that is triggered on demand transform the new data. Then use the built-in Random Cut Forest (RCF) model within Amazon SageMaker to detect anomalies in the data.","A":"Ingest the data using Amazon Kinesis Data Firehose, and use Amazon Kinesis Data Analytics Random Cut Forest (RCF) for anomaly detection. Then use Kinesis Data Firehose to stream the results to Amazon S3.","B":"Ingest the data into Apache Spark Streaming using Amazon EMR, and use Spark MLlib with k-means to perform anomaly detection. Then store the results in an Apache Hadoop Distributed File System (HDFS) using Amazon EMR with a replication factor of three as the data lake."},"answer_ET":"A","answers_community":["A (95%)","5%"],"answer_images":[],"isMC":true,"answer":"A"},{"id":"RDsp281R1Z1zAinx5PZg","answer_ET":"A","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/11384-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":329,"isMC":true,"question_images":[],"choices":{"D":"Amazon Kinesis Data Firehose to transform the data and put it into an Amazon S3 bucket.","B":"AWS Glue with a custom ETL script to transform the data.","C":"An Amazon Kinesis Client Library to transform the data and save it to an Amazon ES cluster.","A":"Amazon Kinesis Data Analytics with an AWS Lambda function to transform the data."},"topic":"1","discussion":[{"upvote_count":"44","content":"A is correct. Kinesis Data Analytics can use lamda to convert GZIP and can run SQL on the converted data.\nhttps://aws.amazon.com/about-aws/whats-new/2017/10/amazon-kinesis-analytics-can-now-pre-process-data-prior-to-running-sql-queries/","comment_id":"38674","poster":"cybe001","timestamp":"1649590440.0"},{"content":"A is correct:\n\nhttps://aws.amazon.com/about-aws/whats-new/2017/10/amazon-kinesis-analytics-can-now-pre-process-data-prior-to-running-sql-queries/\n\n\"To get started, simply select an AWS Lambda function from the Kinesis Analytics application source page in the AWS Management console. Your Kinesis Analytics application will automatically process your raw data records using the Lambda function, and send transformed data to your SQL code for further processing.\nKinesis Analytics provides Lambda blueprints for common use cases like converting GZIP\n...\"","poster":"VB","comment_id":"63233","timestamp":"1650697500.0","upvote_count":"17"},{"upvote_count":"1","poster":"ef12052","comment_id":"1416285","timestamp":"1743484560.0","content":"Selected Answer: A\nUse Amazon Kinesis Data Analytics if you need SQL-based processing and advanced analytics capabilities for streaming data.\nUse Amazon Kinesis Data Firehose if your primary requirement is to deliver, transform, and load streaming data into various AWS destinations with simplified configurations, but not for SQL-based processing."},{"comments":[{"comment_id":"1154745","timestamp":"1724153880.0","poster":"Denise123","upvote_count":"1","content":"The answer can be A , please comment if you have more clarity. After searching more, I also found out the following: \n(I have missed the SQL requirement in the question)\nUse Amazon Kinesis Data Analytics if you need SQL-based processing and advanced analytics capabilities for streaming data.\nUse Amazon Kinesis Data Firehose if your primary requirement is to deliver, transform, and load streaming data into various AWS destinations with simplified configurations, but not for SQL-based processing."}],"poster":"Denise123","comment_id":"1154740","upvote_count":"1","timestamp":"1724153520.0","content":"Selected Answer: D\nIf gaining real-time insights involves complex analytics or custom processing, Amazon Kinesis Data Analytics with AWS Lambda is likely a more suitable choice. If the requirements can be met with simpler data transformations, Amazon Kinesis Data Firehose might provide a more straightforward and potentially lower-latency solution. \n\nIn other words, if this data is in GZIP files and the processing requirements are relatively simple, Amazon Kinesis Data Firehose might be a more straightforward and efficient choice. GZIP files typically contain compressed data, and if our primary objective is to ingest, transform, and load this data into other AWS services for real-time insights, Kinesis Data Firehose provides a managed and streamlined solution that can handle GZIP compression."},{"poster":"elvin_ml_qayiran25091992razor","content":"Selected Answer: A\nA is correct, why D xiyarsan sen?","upvote_count":"1","timestamp":"1715320380.0","comment_id":"1067034"},{"upvote_count":"1","timestamp":"1709142960.0","comment_id":"992357","poster":"Mickey321","content":"Selected Answer: A\nA is correct"},{"content":"Selected Answer: A\n\"allow the use ohttps://www.examtopics.com/exams/amazon/aws-certified-machine-learning-specialty/view/13/#f SQL to query the stream with the LEAST latency?\"\nWell, the only solution that presents SQL query is (A). It's a description of KDA.","comment_id":"969356","upvote_count":"2","poster":"kaike_reis","timestamp":"1706824680.0"},{"comment_id":"832832","timestamp":"1694162640.0","content":"Selected Answer: A\nthe term \"lease latency\" is the the hidden point. with Glue we can have near real-time but Kinesis data analytics will give you real-time transformation with internal lambda","upvote_count":"3","poster":"Nadia0012"},{"upvote_count":"2","content":"Selected Answer: A\nA is correct, with KDA you can run sql queries in the data during the streaming (real-time SQL queries).","poster":"Valcilio","timestamp":"1694159400.0","comment_id":"832790"},{"timestamp":"1692541920.0","comment_id":"815535","poster":"bakarys","upvote_count":"3","comments":[{"content":"Query has to be run on stream so firehose not possible.","poster":"akgarg00","timestamp":"1717431480.0","comment_id":"1087021","upvote_count":"1"}],"content":"Selected Answer: D\nD. Amazon Kinesis Data Firehose to transform the data and put it into an Amazon S3 bucket would be the best solution for allowing the use of SQL to query the stream with the least latency. Amazon Kinesis Data Firehose can be configured to transform the data before writing it to Amazon S3 in real-time. Once the data is in S3, it can be queried using SQL with Amazon Athena, which is a serverless query service that allows running standard SQL queries against data stored in Amazon S3. This approach provides the lowest latency compared to other options and requires minimal setup and maintenance."},{"content":"Selected Answer: A\nA is correct.","upvote_count":"1","poster":"OssamaAbdelatif","comment_id":"728485","timestamp":"1685202120.0"},{"comment_id":"543416","content":"And somehow \"transformation\" is added to the answer as a requirement when it clearly was not part of the requirement from the question.","poster":"AddiWei","upvote_count":"2","timestamp":"1659995400.0"},{"poster":"apprehensive_scar","content":"AAAAAAA","timestamp":"1659499020.0","upvote_count":"1","comment_id":"539423"},{"comment_id":"135399","upvote_count":"4","timestamp":"1651814940.0","content":"what about \"LEAST latency\"?","poster":"HalloSpencer"},{"timestamp":"1651434000.0","content":"A is correct. you can pre-process data prior to running SQL queries with Kinesis Data Analytics and Lambda (more or less) is always a best practice :)","upvote_count":"3","poster":"Erso","comment_id":"81613"},{"comment_id":"35305","timestamp":"1647763200.0","poster":"JayK","upvote_count":"2","content":"Answer is B. Kinesis Data Analytics does not do any transformation, it is only for querying. Glue ETL can have scripts that can transform the data","comments":[{"comment_id":"296975","content":"so you need lambda","timestamp":"1651831620.0","upvote_count":"1","poster":"SophieSu"},{"timestamp":"1648131000.0","comment_id":"37145","upvote_count":"1","poster":"am7","content":"But we need to run SQL on real time stream data."}]}],"answer":"A","unix_timestamp":1578150180,"exam_id":26,"question_text":"A Data Scientist wants to gain real-time insights into a data stream of GZIP files.\nWhich solution would allow the use of SQL to query the stream with the LEAST latency?","timestamp":"2020-01-04 16:03:00","answers_community":["A (73%)","D (27%)"],"answer_description":""},{"id":"Zm0LLUedzth8Pe7DykGn","discussion":[{"comment_id":"28464","timestamp":"1663991700.0","content":"Ans: A XGBoost multi class classification. https://medium.com/@gabrielziegler3/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d\n\nCNN is used for image classificaiton problems","poster":"rsimham","upvote_count":"34"},{"content":"Answer is A. This a classification problem thus XGBoost and the fact that there are six categories SOFTMAX is the right activation function","timestamp":"1664592720.0","comment_id":"35306","poster":"JayK","upvote_count":"14"},{"content":"Selected Answer: A\nDeep convolutional neural networks (CNNs) are primarily used for image processing tasks. Given that the dataset provided is structured/tabular in nature (with features like dimensions, weight, and price) and does not mention image data, a CNN is not the most appropriate choice.","poster":"sonoluminescence","upvote_count":"2","comment_id":"1057107","timestamp":"1730230740.0"},{"poster":"loict","content":"Selected Answer: A\nA. YES - perfect fit, multi:softmax the highest probability class is assigned\nB. NO - CNN is for imaging\nC. NO - regression forest is for continuuous variables, we can discrete classification\nD. NO - it is classification, not forecasting","upvote_count":"3","comment_id":"1007324","timestamp":"1726298820.0"},{"comment_id":"992358","upvote_count":"1","content":"Selected Answer: A\nOption A XGBoost multi class classification","timestamp":"1724860620.0","poster":"Mickey321"},{"content":"Selected Answer: A\nA is the answer.","upvote_count":"1","poster":"kaike_reis","timestamp":"1722542460.0","comment_id":"969359"},{"poster":"oso0348","timestamp":"1710001560.0","upvote_count":"1","content":"Selected Answer: A\nThe XGBoost algorithm is a popular and effective technique for multi-class classification. The objective parameter can be set to multi:softmax, which uses a softmax objective function for multi-class classification. This will train the model to predict the probability of each product belonging to each category, and the most probable category will be chosen as the final prediction.\n\nA deep convolutional neural network (CNN) (B) is a powerful technique commonly used for image recognition tasks. However, it is less appropriate for tabular data like the dataset provided.","comment_id":"834172"},{"timestamp":"1706407020.0","upvote_count":"1","comment_id":"790168","content":"Selected Answer: A\nA, CNN is used for image classification. It would be suitable if we were classifying products using pictures of them.","poster":"Konga98"},{"timestamp":"1703336640.0","comment_id":"754224","upvote_count":"2","content":"Selected Answer: A\nhttps://xgboost.readthedocs.io/en/stable/parameter.html","poster":"yemauricio"},{"timestamp":"1703208420.0","content":"Selected Answer: A\nB - CNN is used for dataset that have \"local intermediate features\" ex) images, or textCNN, etc\nC - We need classfication model, not regression model\nD - RNN is used for dataset that have sequential features\nA is correct","comment_id":"752912","upvote_count":"3","poster":"GiyeonShin"},{"upvote_count":"1","poster":"Peeking","content":"Selected Answer: A\nA is the best option here. Only 1200 items and 6 classes are not enough data to involve a deep neural architecture for classification.","timestamp":"1702096920.0","comment_id":"739799"},{"comment_id":"666350","timestamp":"1694455020.0","poster":"Shailendraa","content":"Ans- A … For multiclassification - multi: SoftMax","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: A\nThat is a classification problem so A is the answer","timestamp":"1689526080.0","comment_id":"632277","poster":"Morsa"},{"comment_id":"540060","poster":"apprehensive_scar","content":"Selected Answer: A\nEasy one. A is correct","timestamp":"1675463580.0","upvote_count":"2"},{"comment_id":"486469","timestamp":"1669357800.0","content":"Selected Answer: A\nA is correct","poster":"Kevinkoo","upvote_count":"3"},{"timestamp":"1666884240.0","upvote_count":"1","comment_id":"415448","content":"Definitely A.","poster":"stardustWu"},{"content":"100% is A; the the others are clearly wrong\n\nConvolutional Neural Network (ConvNet or CNN) is a special type of Neural Network used effectively for image recognition and classification\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language","timestamp":"1666615380.0","upvote_count":"5","poster":"syu31svc","comment_id":"169181"},{"content":"A is correct. XGBoost with softmax","upvote_count":"1","comment_id":"108610","timestamp":"1666388820.0","poster":"Antriksh"},{"poster":"PRC","upvote_count":"1","content":"A should be the answer. Multi-classification problem and hence XGBoost...CNN is for image classification","timestamp":"1666064460.0","comment_id":"65493"},{"content":"CNN is mostly used for images. A is the answer","timestamp":"1665499920.0","comment_id":"51120","upvote_count":"3","poster":"Phong"}],"exam_id":26,"choices":{"D":"A DeepAR forecasting model based on a recurrent neural network (RNN)","A":"AnXGBoost model where the objective parameter is set to multi:softmax","C":"A regression forest where the number of trees is set equal to the number of product categories","B":"A deep convolutional neural network (CNN) with a softmax activation function for the last layer"},"timestamp":"2019-12-10 05:34:00","answer_images":[],"topic":"1","question_text":"A retail company intends to use machine learning to categorize new products. A labeled dataset of current products was provided to the Data Science team. The dataset includes 1,200 products. The labeled dataset has 15 features for each product such as title dimensions, weight, and price. Each product is labeled as belonging to one of six categories such as books, games, electronics, and movies.\nWhich model should be used for categorizing new products using the provided dataset for training?","isMC":true,"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/10089-exam-aws-certified-machine-learning-specialty-topic-1/","answer":"A","answer_description":"","question_images":[],"answer_ET":"A","question_id":330,"unix_timestamp":1575952440}],"exam":{"isImplemented":true,"name":"AWS Certified Machine Learning - Specialty","isMCOnly":false,"id":26,"numberOfQuestions":369,"isBeta":false,"provider":"Amazon","lastUpdated":"11 Apr 2025"},"currentPage":66},"__N_SSP":true}