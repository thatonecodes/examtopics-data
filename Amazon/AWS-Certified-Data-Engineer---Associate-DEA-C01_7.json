{"pageProps":{"questions":[{"id":"gqMspiDbwO2HCt1xsAhC","answers_community":["A (80%)","B (20%)"],"answer_description":"","topic":"1","timestamp":"2024-08-14 13:48:00","url":"https://www.examtopics.com/discussions/amazon/view/145714-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","choices":{"D":"Use AWS Glue to catalog the data. Use S3 Select to query the Avro objects in Amazon S3. Connect Amazon QuickSight to the S3 bucket to create the dashboards.","A":"Create OpenSearch Dashboards by using the data from OpenSearch Service.","B":"Use Amazon Athena with an Apache Hive metastore to query the Avro objects in Amazon S3. Use Amazon Managed Grafana to connect to Athena and to create the dashboards.","C":"Use Amazon Athena to query the data from the Avro objects in Amazon S3. Configure Amazon Keyspaces as the data catalog. Connect Amazon QuickSight to Athena to create the dashboards."},"answer":"A","question_text":"A company is creating near real-time dashboards to visualize time series data. The company ingests data into Amazon Managed Streaming for Apache Kafka (Amazon MSK). A customized data pipeline consumes the data. The pipeline then writes data to Amazon Keyspaces (for Apache Cassandra), Amazon OpenSearch Service, and Apache Avro objects in Amazon S3.\n\nWhich solution will make the data available for the data visualizations with the LEAST latency?","answer_ET":"A","exam_id":21,"question_id":31,"question_images":[],"unix_timestamp":1723636080,"isMC":true,"discussion":[{"comment_id":"1316994","poster":"jacob_nz","upvote_count":"1","content":"Selected Answer: B\nTimeseries/ Graph visualizations we use Graphana","timestamp":"1732442700.0"},{"upvote_count":"1","comment_id":"1265757","timestamp":"1723639680.0","content":"Selected Answer: A\nAnswer is A","poster":"aragon_saa"},{"timestamp":"1723636080.0","poster":"matt200","content":"Selected Answer: A\nOption A: Create OpenSearch Dashboards by using the data from OpenSearch Service is the best choice for achieving the least latency. OpenSearch is designed for low-latency data retrieval and visualization, making it ideal for near real-time dashboards","upvote_count":"3","comment_id":"1265672"}],"answer_images":[]},{"id":"SdOzTczlO5hNUefCufy6","discussion":[{"comment_id":"1457086","upvote_count":"1","poster":"bad1ccc","timestamp":"1743756720.0","content":"Selected Answer: B\nWhen you TRUNCATE a materialized view in Amazon Redshift, it removes all rows from the view and reclaims the most storage space because the operation does not log individual row deletions. This is far more efficient in terms of both time and space than a DELETE operation."},{"upvote_count":"2","content":"Selected Answer: B\nthe key is \"reclaim database storage space\"\nDelete does not reclaim disk space","timestamp":"1741028640.0","poster":"JimOGrady","comment_id":"1364548"},{"content":"Selected Answer: B\nin AWS Redshift, you can use the \"TRUNCATE\" command to delete all rows from a materialized view, effectively \"truncating\" it, especially when the materialized view is configured for streaming ingestion; this is a faster way to clear the data compared to a \"DELETE\" statement.","poster":"sravanscr","upvote_count":"1","comment_id":"1349579","timestamp":"1738338060.0"},{"poster":"YUICH","content":"Selected Answer: A\n(B) TRUNCATE is invalid for materialized views, so it is excluded.\nIn actual operations, to most effectively reuse storage, you need to delete all rows with a DELETE statement and then run VACUUM, as shown in (A) or (D).\nIf you want to delete everything, option (A) is the most straightforward approach.","timestamp":"1737947040.0","comment_id":"1347207","upvote_count":"2"},{"comment_id":"1343745","timestamp":"1737391740.0","poster":"A_E_M","content":"Selected Answer: A\nWhy this is the best option:\nEfficiency:\nBy using \"WHERE 1=1\", the database doesn't need to iterate through each row individually to check a specific condition, resulting in faster deletion of all data.\nStorage reclamation:\nDeleting all rows using this method will free up the most storage space within the materialized view. \nImportant Considerations:\nTRUNCATE vs DELETE:\nWhile \"TRUNCATE\" can also be used to remove all data from a table, it is not recommended for materialized views in Redshift as it might not always reclaim all the storage space effectively.\nVACUUM command:\n\"VACUUM\" is used to reclaim space within a table after deletions, but it's not necessary when deleting all rows using \"DELETE FROM ... WHERE 1=1;\" as the entire table will be emptied.","upvote_count":"3"},{"poster":"AgboolaKun","comment_id":"1308849","timestamp":"1731080220.0","upvote_count":"1","content":"Selected Answer: B\nB is the correct answer.\n\nHere is why:\nTRUNCATE is the most efficient way to remove all rows from a table or materialized view in Amazon Redshift. It's faster than DELETE and immediately reclaims disk space.\n\nTRUNCATE removes all rows in a table without scanning them individually. This makes it much faster than DELETE operations, especially for large tables.\n\nTRUNCATE automatically performs a VACUUM operation, which sorts the table and reclaims space.\n\nTRUNCATE resets any auto-increment columns."},{"content":"Answer:B\nTRUNCATE Command: The TRUNCATE command is the most efficient way to delete all rows from a table or materialized view. It does not scan the table, does not generate individual row delete actions, and effectively frees up space immediately by removing all data at once. It also resets any identity columns, if applicable.","poster":"Parandhaman_Margan","timestamp":"1730000100.0","comment_id":"1303465","upvote_count":"1"}],"timestamp":"2024-10-27 04:35:00","question_id":32,"choices":{"D":"DELETE FROM materialized_view_name where load_date<=current_date","A":"DELETE FROM materialized_view_name where 1=1","B":"TRUNCATE materialized_view_name","C":"VACUUM table_name where load_date<=current_date\nmaterializedview"},"answer":"B","answer_images":[],"answer_ET":"B","exam_id":21,"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/150336-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_description":"","unix_timestamp":1730000100,"answers_community":["B (50%)","A (50%)"],"question_images":[],"question_text":"A data engineer maintains a materialized view that is based on an Amazon Redshift database. The view has a column named load_date that stores the date when each row was loaded.\n\nThe data engineer needs to reclaim database storage space by deleting all the rows from the materialized view.\n\nWhich command will reclaim the MOST database storage space?","topic":"1"},{"id":"2M4MwFPvcrDWHD8EBW7p","discussion":[{"comment_id":"1359056","poster":"Evan_Lin","upvote_count":"2","timestamp":"1740018420.0","content":"Selected Answer: B\nwhy not B?\nLogstash is an open-source data ingestion tool that allows you to collect data from various sources, transform it, and send it to your desired destination. With prebuilt filters and support for over 200 plugins, Logstash allows users to easily ingest data regardless of the data source or type."},{"comment_id":"1338999","timestamp":"1736550480.0","upvote_count":"2","poster":"maddyr","content":"Selected Answer: B\nLogstash is a lightweight, open-source, server-side data processing pipeline that allows you to collect data from various sources, transform it on the fly, and send it to your desired destination. It is most often used as a data pipeline for Elasticsearch, an open-source analytics and search engine\nhttps://aws.amazon.com/what-is/elk-stack/#seo-faq-pairs#what-is-the-elk-stack"},{"comment_id":"1268222","content":"Amazon Kinesis Data Firehose is a fully managed service that reliably loads streaming data into data lakes, data stores and analytics services like OpenSearch Service. It can automatically scale to match the throughput of your data and requires no ongoing administration.\n\nAnswer A","poster":"mzansikiller","timestamp":"1724018040.0","upvote_count":"1"},{"poster":"aragon_saa","comment_id":"1265758","timestamp":"1723639740.0","content":"Selected Answer: A\nAnswer is A","upvote_count":"1"},{"upvote_count":"3","timestamp":"1723636200.0","poster":"matt200","comment_id":"1265673","content":"Selected Answer: A\nOption A: Use Amazon Kinesis Data Firehose and an AWS Lambda function to transform the data and deliver the transformed data to OpenSearch Service is the best choice for achieving the least operational overhead. Kinesis Data Firehose is a managed service that automates the data ingestion process, scales seamlessly, and integrates directly with OpenSearch Service, minimizing the need for manual intervention and infrastructure management."}],"answer_description":"","exam_id":21,"question_id":33,"unix_timestamp":1723636200,"answer_images":[],"timestamp":"2024-08-14 13:50:00","isMC":true,"topic":"1","question_text":"A media company wants to use Amazon OpenSearch Service to analyze rea-time data about popular musical artists and songs. The company expects to ingest millions of new data events every day. The new data events will arrive through an Amazon Kinesis data stream. The company must transform the data and then ingest the data into the OpenSearch Service domain.\n\nWhich method should the company use to ingest the data with the LEAST operational overhead?","choices":{"D":"Use the Kinesis Client Library (KCL) to transform the data and deliver the transformed data to OpenSearch Service.","C":"Use an AWS Lambda function to call the Amazon Kinesis Agent to transform the data and deliver the transformed data OpenSearch Service.","A":"Use Amazon Kinesis Data Firehose and an AWS Lambda function to transform the data and deliver the transformed data to OpenSearch Service.","B":"Use a Logstash pipeline that has prebuilt filters to transform the data and deliver the transformed data to OpenSearch Service."},"answers_community":["A (50%)","B (50%)"],"answer":"A","url":"https://www.examtopics.com/discussions/amazon/view/145715-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer_ET":"A","question_images":[]},{"id":"bo6XtQqyJxsnPbCPOhZQ","answer_description":"","answer":"A","exam_id":21,"answer_ET":"A","unix_timestamp":1723197180,"answers_community":["A (100%)"],"question_id":34,"isMC":true,"discussion":[{"timestamp":"1731080820.0","poster":"AgboolaKun","content":"Selected Answer: A\nThe solution that will meet the requirement with the least operational effort is A.\n\nHere's why:\n\nRow-level security: AWS Lake Formation provides built-in row-level security, which allows you to control access to specific rows in a table based on conditions. This is precisely what's needed in this scenario.\n\nLeast operational effort: Once set up, this filter will automatically apply to all queries without needing to modify the data or create complex IAM policies.\n\nScalability: As new data is added to the table, the filter will automatically apply, requiring no additional effort.\n\nPrecision: It directly addresses the requirement by preventing access to rows where the country is Canada, without affecting other data.","comment_id":"1308854","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: A\nAnswer is A","comment_id":"1263250","poster":"komorebi","timestamp":"1723243740.0"}],"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/145293-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","timestamp":"2024-08-09 11:53:00","question_text":"A company stores customer data tables that include customer addresses in an AWS Lake Formation data lake. To comply with new regulations, the company must ensure that users cannot access data for customers who are in Canada.\n\nThe company needs a solution that will prevent user access to rows for customers who are in Canada.\n\nWhich solution will meet this requirement with the LEAST operational effort?","answer_images":[],"choices":{"C":"Set a column-level filter to prevent user access to a row where the country is Canada.","A":"Set a row-level filter to prevent user access to a row where the country is Canada.","B":"Create an IAM role that restricts user access to an address where the country is Canada.","D":"Apply a tag to all rows where Canada is the country. Prevent user access where the tag is equal to “Canada”."},"topic":"1"},{"id":"FFP8wbiFRXNlVNkx7q1U","question_text":"A company stores daily records of the financial performance of investment portfolios in .csv format in an Amazon S3 bucket. A data engineer uses AWS Glue crawlers to crawl the S3 data.\nThe data engineer must make the S3 data accessible daily in the AWS Glue Data Catalog.\nWhich solution will meet these requirements?","exam_id":21,"answers_community":["B (100%)"],"answer_ET":"B","answer_description":"","question_id":35,"choices":{"A":"Create an IAM role that includes the AmazonS3FullAccess policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Create a daily schedule to run the crawler. Configure the output destination to a new path in the existing S3 bucket.","B":"Create an IAM role that includes the AWSGlueServiceRole policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Create a daily schedule to run the crawler. Specify a database name for the output.","D":"Create an IAM role that includes the AWSGlueServiceRole policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Allocate data processing units (DPUs) to run the crawler every day. Configure the output destination to a new path in the existing S3 bucket.","C":"Create an IAM role that includes the AmazonS3FullAccess policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Allocate data processing units (DPUs) to run the crawler every day. Specify a database name for the output."},"unix_timestamp":1705800900,"topic":"1","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/131709-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/","answer":"B","discussion":[{"comment_id":"1137915","poster":"TonyStark0122","timestamp":"1726863120.0","content":"B. Create an IAM role that includes the AWSGlueServiceRole policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Create a daily schedule to run the crawler. Specify a database name for the output.\n\nExplanation:\nOption B correctly sets up the IAM role with the necessary permissions using the AWSGlueServiceRole policy, which is designed for use with AWS Glue. It specifies the S3 bucket path of the source data as the crawler's data store and creates a daily schedule to run the crawler. Additionally, it specifies a database name for the output, ensuring that the crawled data is properly cataloged in the AWS Glue Data Catalog.","upvote_count":"8"},{"content":"Selected Answer: B\nanswer B is incomplete. Even we include AWSGlueServiceRole policy on IAM role, S3 access is not garantee","upvote_count":"2","poster":"plutonash","comment_id":"1339450","timestamp":"1736681580.0"},{"poster":"LrdKanien","upvote_count":"1","comment_id":"1303374","content":"How does Glue get access to S3 if you don't do B?","comments":[{"poster":"LrdKanien","comments":[{"comments":[{"poster":"sam_pre","timestamp":"1742831280.0","upvote_count":"1","content":"It adds only for the glue related buckets, but it doesnt grant permissions for S3 that we need to read in order to fetch data, isnt it ?","comment_id":"1409702"}],"upvote_count":"1","content":"S3 access is part of the AWSGlueServiceRole Policy\nhttps://docs.aws.amazon.com/aws-managed-\npolicy/latest/reference/AWSGlueServiceRole.html","comment_id":"1305963","timestamp":"1730488140.0","poster":"Asmunk"}],"comment_id":"1303375","content":"I meant A","upvote_count":"1","timestamp":"1729966620.0"}],"timestamp":"1729966560.0"},{"timestamp":"1715297220.0","upvote_count":"4","comment_id":"1209098","poster":"k350Secops","content":"Selected Answer: B\nGlue Crawlers are serverless. Assigning DPUs is the point where i decided it option B"},{"content":"Selected Answer: B\nA,C are wrong because you use don't need full S3 access. D is wrong because you don't need to provision DPU and the destination should be a database, not an s3 bucket. so it's B","comment_id":"1167945","timestamp":"1709812320.0","poster":"GiorgioGss","upvote_count":"4"}],"timestamp":"2024-01-21 02:35:00","answer_images":[],"isMC":true}],"exam":{"isImplemented":true,"provider":"Amazon","id":21,"lastUpdated":"11 Apr 2025","isBeta":false,"name":"AWS Certified Data Engineer - Associate DEA-C01","isMCOnly":true,"numberOfQuestions":207},"currentPage":7},"__N_SSP":true}