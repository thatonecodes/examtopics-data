{"pageProps":{"questions":[{"id":"mFVnjYnxrYoMaNketd0B","answer_images":[],"discussion":[{"content":"D is correct. Amazon Comprehend syntax analysis =/= Amazon Comprehend sentiment analysis. You need to read choices very carefully.","comments":[{"upvote_count":"4","timestamp":"1648195620.0","content":"We're looking only to improve the validation accuracy and Comprehend syntax analysis would help that because the word set is rich and the sentiment carying words infrequent. We're not looking to replace the sentiment analysis tool with Comprehend.","comment_id":"93625","poster":"mawsman"}],"comment_id":"47726","poster":"tap123","upvote_count":"35","timestamp":"1648094700.0"},{"comment_id":"22127","content":"AWS COMPREHEND IS A NATURAL LANGUAGE PROCESSING (NLP) SERVICE THAT USES MACHINE LEARNING TO DISCOVER INSIGHTS FROM TEXT.\nAMAZON COMPREHEND PROVIDES KEYPHRASE EXTRACTION, SENTIMENT ANALYSIS, ENTITY RECOGNITION, TOPIC MODELING, AND LANGUAGE DETECTION APIS SO YOU CAN EASILY INTEGRATE NATURAL LANGUAGE PROCESSING INTO YOUR APPLICATIONS.\n\nHTTPS://AWS.AMAZON.COM/COMPREHEND/FEATURES/?NC1=H_LS\n\nJUST THROUGH AMAZON COMPREHEND IS MUCH EASY THAN OTHER\nTHE MUCH MORE CONVENIENT ANSWER IS A.","poster":"DonaldCMLIN","upvote_count":"23","timestamp":"1647859680.0","comments":[{"comment_id":"34991","timestamp":"1648056000.0","poster":"ComPah","content":"Agree Also Keyword is TOOL rest are frameworks","upvote_count":"2"}]},{"comment_id":"1154705","content":"Selected Answer: A\nBoth Amazon Comprehend and the TF-IDF with a classifier solution are valid. If ease of use and pre-trained capabilities are high priorities, Comprehend is a solid option. If customization and dataset-specific nuances are crucial, building a custom model with TF-IDF may be needed.\nSince Comprehend is a tool, I am going with A.","poster":"VR10","timestamp":"1724149560.0","upvote_count":"1"},{"comment_id":"1117148","timestamp":"1720482360.0","content":"D. Scikit-learn term frequency-inverse document frequency (TF-IDF) vectorizer\n\nHere's why:\n\nTF-IDF Vectorizer: This tool from Scikit-learn is effective in handling issues of rich vocabularies and low frequency words. TF-IDF down-weights words that appear frequently across documents (thus might be less informative) and gives more weight to words that appear less frequently but might be more indicative of the sentiment. This approach can enhance the model's ability to focus on more relevant features, potentially improving validation accuracy.","poster":"phdykd","upvote_count":"4"},{"content":"C I think c is correct. stemming involves reducing words to their root or base form, and stop word removal involves removing common words (e.g., \"the,\" \"and,\" \"is\") that may not contribute much to sentiment analysis. By using NLTK for stemming and stop word removal, you can simplify the vocabulary and potentially improve the model's ability to capture sentiment from the remaining meaningful words.\nA - syntax and entity recognition wont solve the scenario\nB - blaze text for words. \nD - capturing the importance of words in a document collection. frequency of a word in a document.","poster":"geoan13","comment_id":"1069041","upvote_count":"4","timestamp":"1715569560.0"},{"content":"Selected Answer: D\nD is the correct guys","comment_id":"1067038","upvote_count":"1","timestamp":"1715320500.0","poster":"elvin_ml_qayiran25091992razor"},{"content":"Amazon Comprehend's syntax analysis and entity detection are more about understanding the structure of sentences and identifying entities within the text rather than tackling the problem of a rich vocabulary with low average frequency of words.\n\nTF-IDF vectorization is a technique that can help reduce the impact of common, low-information words in the dataset while emphasizing the importance of more informative, less frequent words. This could potentially improve the validation accuracy by addressing the identified problem.","timestamp":"1713241920.0","poster":"wendaz","upvote_count":"1","comment_id":"1044655"},{"poster":"loict","content":"Selected Answer: A\nA. YES - he works on an application and not a model, Amazon Comprehend is the ready-to-use tool he wants; TF-IDF is built-in\nB. NO - word2vec will be challenged with low frequency terms; GloVe and FastText are better for that\nC. NO - the vocabulary is righ, so stemming and stop word removal will not address the core issue\nD. NO - right approach, but that is not \"a tool\"","comment_id":"1007331","upvote_count":"1","timestamp":"1710409320.0"},{"timestamp":"1709143200.0","content":"Selected Answer: D\nOption D. This approach can help in reducing the impact of words that occur frequently in the dataset and increasing the impact of words that occur less frequently. This can help in improving the accuracy of the model.","comment_id":"992359","upvote_count":"2","poster":"Mickey321"},{"comment_id":"986075","poster":"ashii007","content":"The anwer is B.\nBlazing text can hadle OOV words as explained below. https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html","upvote_count":"2","timestamp":"1708469940.0"},{"poster":"jyrajan69","timestamp":"1707086760.0","upvote_count":"2","content":"This is an AWS exam, so why would you choose anything other than A or B, and based on the link, it looks like B most likely","comment_id":"972498"},{"upvote_count":"2","content":"Selected Answer: D\nThe passage “low average frequency of words” points directly to the use of TF-IDF. Letter A deviates from what the question proposes and is discarded. Letter B proposes a radical change in my POV. Letter C does not solve the passage mentioned at the beginning. Letter D is correct.","poster":"kaike_reis","timestamp":"1706825160.0","comment_id":"969363"},{"timestamp":"1699188120.0","upvote_count":"1","comment_id":"889965","content":"The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as *****sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification.","poster":"GOSD"},{"timestamp":"1697722140.0","upvote_count":"2","comment_id":"874714","content":"Selected Answer: D\nI would say since the buzzword \"low average frequency\" comes up, the safe choise would be the tfid vectorizer.\n\nI go for D.","poster":"vassof95"},{"timestamp":"1696660320.0","upvote_count":"5","comment_id":"863547","content":"Selected Answer: D\nThe Scikit-learn term frequency-inverse document frequency (TF-IDF) vectorizer is a widely used tool to mitigate the high dimensionality of text data. \nOption A, Amazon Comprehend syntax analysis, and entity detection, can help in extracting useful features from the text, but it does not address the issue of high dimensionality.\n\nOption B, Amazon SageMaker BlazingText cbow mode, is a tool for training word embeddings, which can help to represent words in a lower dimensional space. However, it does not directly address the issue of high dimensionality and low frequency of words.\n\nOption C, Natural Language Toolkit (NLTK) stemming and stop word removal, can reduce the dimensionality of the feature space, but it does not address the issue of low-frequency words that are important for sentiment analysis.","poster":"ParkXD"},{"content":"Selected Answer: C\nEmphasis is on the rich words - so stemming can help reduce these to more common words. Blazing Text in cbow mode doesnt seem relevant is about providing words given a context. And TF-IDF I'm not sure would do anything except highlight the problem you are already having?","upvote_count":"1","timestamp":"1694029560.0","poster":"cpal012","comment_id":"831315"},{"poster":"bakarys","upvote_count":"5","comment_id":"815546","timestamp":"1692542400.0","content":"Selected Answer: D\nD. Scikit-learn term frequency-inverse document frequency (TF-IDF) vectorizer would be the best tool to use in this scenario. The TF-IDF vectorizer will give less weight to the less frequent words in the dataset, and allow the more informative and frequent words to have a greater impact on the sentiment analysis. This can help to improve the validation accuracy of the model."},{"content":"Refer to https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/blazingtext_word2vec_subwords_text8/blazingtext_word2vec_subwords_text8.html\n\nPhrase \"This is a limitation, especially for languages with large vocabularies and many rare words”\n\nSageMaker BlazingText can learn vector representations associated with character n-grams; representing words as the sum of these character n-grams representations [1]. This method enables BlazingText to generate vectors for out-of-vocabulary (OOV) words, as demonstrated in this notebook.\n\nAlso, I suspect the solution would be one of \"aws provided\" services or algorithms\nb","comment_id":"813655","poster":"drcok87","upvote_count":"1","timestamp":"1692403380.0"},{"content":"Selected Answer: C\nC. Natural Language Toolkit (NLTK) stemming and stop word removal.\n\nStemming and stop word removal are two common text pre-processing techniques that can be used to improve the accuracy of sentiment analysis models. Stemming involves reducing words to their root form, which can help reduce the size of the vocabulary and increase the average frequency of words in the dataset. Stop word removal involves removing commonly used words such as \"the,\" \"and,\" and \"a\" that do not carry much meaning, again reducing the size of the vocabulary and increasing the average frequency of words.\n\nOption D, Scikit-learn term frequency-inverse document frequency (TF-IDF) vectorizer, is a common technique for converting text into numerical representations, but it does not address the issue of a rich vocabulary and low average frequency of words in the dataset.","upvote_count":"1","timestamp":"1691681520.0","poster":"AjoseO","comment_id":"804612"},{"upvote_count":"5","comment_id":"804130","poster":"sqavi","timestamp":"1691649600.0","content":"Selected Answer: D\nTF-IDF gives larger values for less frequent words in the document corpus. TF-IDF value is high when both IDF and TF values are high i.e the word is rare in the whole document but frequent in a document."},{"poster":"yemauricio","content":"Selected Answer: A\nA is the right answer\nB is not possible because it's an implementation of word2vec https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html","upvote_count":"1","timestamp":"1687518420.0","comment_id":"754226"},{"timestamp":"1685707020.0","upvote_count":"2","content":"C is the correct answer https://www.bogotobogo.com/python/NLTK/Stemming_NLTK.php","comment_id":"733791","poster":"Pg690"},{"timestamp":"1684560180.0","comment_id":"722444","upvote_count":"8","content":"Selected Answer: C\nThe answer should be C.\nNote that the question said \"...cause may be a rich vocabulary and a low average frequency of words\". It is a data processing problem, not modeling problem!\nA is irrelevant.\nB is wrong. The question did not mention that he is using a deep learning model. You can't assume a word embedding layer can easily fit into his model.\nC is correct because stemming and stop word removal can reduce noises. It solves the root problem in processing step without touching the model.\nD is wrong. TF-IDF vectorizer does not solve the problem and you can't assume it fits with the model.","poster":"jason_wong6"},{"timestamp":"1674852600.0","content":"Selected Answer: B\nB\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html","poster":"milan_ml","upvote_count":"2","comment_id":"638284"},{"timestamp":"1671462060.0","poster":"f4bi4n","comment_id":"618699","content":"C is the only solution which takes on the root cause... removing noise and increasing the value of the dataset","upvote_count":"2"},{"timestamp":"1651308900.0","comment_id":"409579","upvote_count":"2","poster":"spamicho","content":"Betting on B here, it's Amazon service doing text classification through text embeddings. We have low frequency problem and with this one we also recognizing word similarities into account."},{"comment_id":"320457","upvote_count":"9","timestamp":"1651288980.0","poster":"srinu3054","content":"Its B!! Blazing text has out-of-vocabulary (OOV) feature which can embed the non vocabulary words. \nhttps://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html"},{"upvote_count":"3","timestamp":"1650886440.0","poster":"mlpcr","comment_id":"316075","content":"I think Answer is C. Problem statement - \"validation accuracy is poor, and ... cause may be a rich vocabulary and a low average frequency of words in the dataset.\"\nSo to improve frequency of useful/meaningful words in given text we need to remove noisy words and for other purpose so stemming."},{"content":"I go with B. As another commenter said, go with the Amazon service over non-Amazon. Syntax and entity detection does not address the problem. Word embedding will, by providing context to words. Stemming and stop word removal won't help. TF-IDF might help a little but not nearly as much as Word2Vec, which is option B.","timestamp":"1650337200.0","upvote_count":"12","comment_id":"278755","poster":"cloud_trail"},{"upvote_count":"8","comment_id":"212485","content":"I'm a vote for B.\n\nThe other solutions do not directly address the stated problem. Word2vec, which is what BlazingText is, does.","timestamp":"1650304260.0","comments":[{"timestamp":"1650782400.0","content":"I vote B too -> \"The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis\"","poster":"scuzzy2010","upvote_count":"7","comment_id":"282341"}],"poster":"yeetusdeleetus"},{"upvote_count":"10","comments":[{"content":"I think people/answers here focus more on keyword \"frequency\" - but less focus on the problem caused from the rich vocabulary and low avg freq","timestamp":"1649319120.0","poster":"williamsuning","comment_id":"190851","upvote_count":"1"}],"timestamp":"1649290860.0","content":"validation document has rich vocabulary, it sounds to me creating difficulty and sparsity in classification. Especially, one-hot encoding/ tf-idf doesn't/less take into accounts the semantic of words. So word embedding helps to reduce the sparsity caused by rich vocabulary, and more importantly the semantic relationships between words are reflected in representation. \nSo I choose B, one more bonus for B is it also applies AWS Sagemaker features:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\nany answer better than this?","poster":"williamsuning","comment_id":"190848"},{"upvote_count":"7","comment_id":"164250","comments":[{"comment_id":"278756","content":"All that TF-IDF does is tell you what you already know: that have many low-frequency words. So what? That doesn't address the problem.","upvote_count":"2","timestamp":"1650583860.0","poster":"cloud_trail"},{"poster":"trphng","timestamp":"1649502840.0","upvote_count":"1","content":"Agree. Question has pointed out: the cause may be a rich vocabulary and a low average frequency of words in the dataset","comment_id":"211644"}],"poster":"yddmj","timestamp":"1649257620.0","content":"It mentioned such NPL techniques: syntax analysis, entity detection, cbow, stemming, stop word removal and TF-IDF. TF-IDF is the only one related to frequency of words. \n\nIt seems TF-IDF weighted method import the performance.\n\nhttps://appliedmachinelearning.blog/2017/02/12/sentiment-analysis-using-tf-idf-weighting-pythonscikit-learn/\n\nI vote D"},{"comments":[{"upvote_count":"7","content":"Based on the above and https://medium.com/analytics-vidhya/fundamentals-of-bag-of-words-and-tf-idf-9846d301ff22\nLook out for \n• tf–idf or TFIDF, short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.\nI go with D","timestamp":"1648809480.0","poster":"GeeBeeEl","comment_id":"149992"}],"content":"I dont think the use of the term tool implies Comprehend, after all BlazingText may be considered a tool. The problem is a low average frequency of words in the dataset. Even if you use Comprehend, it will not increase the frequency of words. If however, you can make it possible to get a value that increases proportionally to the number of times a word appears in the document, then you are working on the frequency. A vectorizer is a tool, not a framework. Also I do agree with the concept that AWS would prefer to promote its service like Comprehend, but the question is more generic than that. The option on Comprehend is syntax anaysis ( Syntax analysis —enabling customers to analyze text using tokenization and Parts of Speech (PoS).) The request is for frequency","timestamp":"1648631940.0","comment_id":"149988","upvote_count":"4","poster":"GeeBeeEl"},{"poster":"Urban_Life","content":"From my last couple of cert prep- I've seen that Amazon service always comes 1st rather go with right solution. Therefore, I will go with A.","timestamp":"1648538100.0","upvote_count":"7","comment_id":"127195"}],"unix_timestamp":1573976820,"url":"https://www.examtopics.com/discussions/amazon/view/8395-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2019-11-17 08:47:00","answers_community":["D (59%)","C (27%)","8%"],"question_text":"A Data Scientist is working on an application that performs sentiment analysis. The validation accuracy is poor, and the Data Scientist thinks that the cause may be a rich vocabulary and a low average frequency of words in the dataset.\nWhich tool should be used to improve the validation accuracy?","isMC":true,"choices":{"D":"Scikit-leam term frequency-inverse document frequency (TF-IDF) vectorizer","A":"Amazon Comprehend syntax analysis and entity detection","B":"Amazon SageMaker BlazingText cbow mode","C":"Natural Language Toolkit (NLTK) stemming and stop word removal"},"topic":"1","exam_id":26,"answer_ET":"D","question_images":[],"answer_description":"","question_id":331,"answer":"D"},{"id":"B6EUN0HiIDQeWECaA48s","answer_ET":"C","question_text":"Machine Learning Specialist is building a model to predict future employment rates based on a wide range of economic factors. While exploring the data, the\nSpecialist notices that the magnitude of the input features vary greatly. The Specialist does not want variables with a larger magnitude to dominate the model.\nWhat should the Specialist do to prepare the data for model training?","topic":"1","answer_images":[],"discussion":[{"comment_id":"28465","poster":"rsimham","timestamp":"1663610520.0","comments":[{"comment_id":"341081","timestamp":"1667665800.0","poster":"gcpwhiz","upvote_count":"4","content":"Ans is not C. What is listed there is the definition of STANDARDIZATION. Normalization just scales and is not useful for reducing the effect of outliers","comments":[{"timestamp":"1667758920.0","content":"nevermind ignore this","poster":"gcpwhiz","upvote_count":"4","comment_id":"341082"}]}],"upvote_count":"34","content":"Ans: C; Normalization is correct"},{"content":"Guys, I passed the exam today. It is a tough one but there are many questions here. Good luck everyone! Thank examtopics","timestamp":"1666193700.0","comment_id":"51583","upvote_count":"14","comments":[{"comment_id":"146230","poster":"haison8x","timestamp":"1666935120.0","upvote_count":"2","content":"Hi Phong!\nPlease add my skype: haison8x"}],"poster":"Phong"},{"poster":"Mickey321","timestamp":"1724862060.0","upvote_count":"2","comment_id":"992385","content":"Selected Answer: C\nAns: C; Normalization is correct"},{"comment_id":"969371","content":"C (Yep, STANDARDIZATION is the correct name)\nThat's an odd question for me","poster":"kaike_reis","upvote_count":"1","timestamp":"1722543060.0"},{"comment_id":"728498","poster":"OssamaAbdelatif","content":"Selected Answer: C\nans C is correct.","timestamp":"1701107760.0","upvote_count":"1"},{"poster":"Deepsachin","comment_id":"476190","content":"ANS should be C as Normalization work best in case of amplitude diff","timestamp":"1668168900.0","upvote_count":"1"},{"content":"Hi, guys, \nFirst thanks this website for the information it provided. \nHowever, the ML exam has updated most of the questions. only 20+ questions here are included in today's test. Anyway, it is still helpful. \nGOOD LUCK EVERYONE!","comment_id":"49452","timestamp":"1665906240.0","upvote_count":"10","poster":"grandgale","comments":[{"upvote_count":"2","content":"So there are 40+ other questions on the exam that aren't included in Examtopics?","comment_id":"60813","timestamp":"1666257720.0","poster":"joker34"}]},{"content":"QUESTION 69\nA large consumer goods manufacturer has the following products on sale:\n• 34 different toothpaste variants\n• 48 different toothbrush variants\n• 43 different mouthwash variants\nThe entire sales history of all these products is available in Amazon S3. Currently, the company is using\ncustom-built autoregressive integrated moving average (ARIMA) models to forecast demand for these\nproducts. The company wants to predict the demand for a new product that will soon be launched.\nWhich solution should a Machine Learning Specialist apply?\nA. Train a custom ARIMA model to forecast demand for the new product.\nB. Train an Amazon SageMaker DeepAR algorithm to forecast demand for the new product.\nC. Train an Amazon SageMaker k-means clustering algorithm to forecast demand for the new product.\nD. Train a custom XGBoost model to forecast demand for the new product.\nCorrect Answer: B","comments":[{"poster":"VB","comment_id":"61141","upvote_count":"1","content":"https://aws.amazon.com/blogs/machine-learning/forecasting-time-series-with-dynamic-deep-learning-on-aws/\n\nAnswer: B","timestamp":"1666822980.0"}],"upvote_count":"4","comment_id":"32869","poster":"nez15","timestamp":"1664314320.0"},{"poster":"nez15","upvote_count":"5","content":"QUESTION 68\nAn agency collects census information within a country to determine healthcare and social program needs by\nprovince and city. The census form collects responses for approximately 500 questions from each citizen.\nWhich combination of algorithms would provide the appropriate insights? (Select TWO.)\nA. The factorization machines (FM) algorithm\nB. The Latent Dirichlet Allocation (LDA) algorithm\nC. The principal component analysis (PCA) algorithm\nD. The k-means algorithm\nE. The Random Cut Forest (RCF) algorithm\nCorrect Answer: CD","timestamp":"1664203980.0","comments":[{"content":"https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/\n\nAnswer: C and D","comment_id":"61134","timestamp":"1666612560.0","poster":"VB","upvote_count":"4"},{"upvote_count":"2","poster":"cybe001","content":"I think the answer is A and B.\nThe census question and answer will be in text. Use LDA (unsupervised algorithm) which takes the census question/answer and groups them into categories. Use the categorization to group the people and identify similar people. \n\nUse the Factorization Machine to group the people. For each person identify if they answer a question or not. Find the total questions they answered and that will be the Target variable. Now the problem is similar to movie recommendation (consider each question a movie and the total number of questions answered will be the Rating). Based on the questions a Person answered, Factorization Machine groups the people.\n\nFindings from both the algorithms can be used to compare and identify the people for the social programs.","comments":[{"timestamp":"1722543000.0","comment_id":"969369","upvote_count":"1","poster":"kaike_reis","content":"it's CD"},{"timestamp":"1664733540.0","comment_id":"44092","upvote_count":"1","poster":"jasonsunbao","content":"FM is mainly used in recommendation system to find hidden variables between two known variables to find correlation between two variables."}],"comment_id":"39126","timestamp":"1664658720.0"}],"comment_id":"32868"},{"comment_id":"32867","poster":"nez15","content":"QUESTION 67\nA. Use AWS Lambda to trigger an AWS Step Functions workflow to wait for dataset uploads to complete in\nAmazon S3. Use AWS Glue to join the datasets. Use an Amazon CloudWatch alarm to send an SNS\nnotification to the Administrator in the case of a failure.\nB. Develop the ETL workflow using AWS Lambda to start an Amazon SageMaker notebook instance. Use a\nlifecycle configuration script to join the datasets and persist the results in Amazon S3. Use an Amazon\nCloudWatch alarm to send an SNS notification to the Administrator in the case of a failure.\nC. Develop the ETL workflow using AWS Batch to trigger the start of ETL jobs when data is uploaded to\nAmazon S3. Use AWS Glue to join the datasets in Amazon S3. Use an Amazon CloudWatch alarm to send\nan SNS notification to the Administrator in the case of a failure.\nD. Use AWS Lambda to chain other Lambda functions to read and join the datasets in Amazon S3 as soon as\nthe data is uploaded to Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the\nAdministrator in the case of a failure.\nCorrect Answer: A","upvote_count":"6","timestamp":"1664187720.0"},{"upvote_count":"3","content":"QUESTION 67\nA Machine Learning Specialist is developing a daily ETL workflow containing multiple ETL jobs. The workflow\nconsists of the following processes:\n• Start the workflow as soon as data is uploaded to Amazon S3.\n• When all the datasets are available in Amazon S3, start an ETL job to join the uploaded datasets with multiple\nterabyte-sized datasets already stored in Amazon S3.\n• Store the results of joining datasets in Amazon S3.\n• If one of the jobs fails, send a notification to the Administrator.\nWhich configuration will meet these requirements?","timestamp":"1664013360.0","comment_id":"32866","poster":"nez15"},{"upvote_count":"11","comment_id":"32865","timestamp":"1663797900.0","poster":"nez15","content":"QUESTION 66\nA Machine Learning Specialist must build out a process to query a dataset on Amazon S3 using Amazon\nAthena. The dataset contains more than 800,000 records stored as plaintext CSV files. Each record contains\n200 columns and is approximately 1.5 MB in size. Most queries will span 5 to 10 columns only.\nHow should the Machine Learning Specialist transform the dataset to minimize query runtime?\nA. Convert the records to Apache Parquet format.\nB. Convert the records to JSON format.\nC. Convert the records to GZIP CSV format.\nD. Convert the records to XML format.\nCorrect Answer: A"}],"url":"https://www.examtopics.com/discussions/amazon/view/10090-exam-aws-certified-machine-learning-specialty-topic-1/","answer_description":"","question_images":[],"timestamp":"2019-12-10 05:39:00","choices":{"C":"Apply normalization to ensure each field will have a mean of 0 and a variance of 1 to remove any significant magnitude.","D":"Apply the orthogonal sparse bigram (OSB) transformation to apply a fixed-size sliding window to generate new features of a similar magnitude.","A":"Apply quantile binning to group the data into categorical bins to keep any relationships in the data by replacing the magnitude with distribution.","B":"Apply the Cartesian product transformation to create new combinations of fields that are independent of the magnitude."},"question_id":332,"answers_community":["C (100%)"],"answer":"C","exam_id":26,"isMC":true,"unix_timestamp":1575952740},{"id":"mmHoVLVf91GatwbIEoLr","question_id":333,"answer_description":"","question_images":[],"choices":{"C":"Convert the records to GZIP CSV format.","D":"Convert the records to XML format.","B":"Convert the records to JSON format.","A":"Convert the records to Apache Parquet format."},"url":"https://www.examtopics.com/discussions/amazon/view/19369-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2020-04-30 14:03:00","answer_images":[],"answer":"A","isMC":true,"answer_ET":"A","discussion":[{"comments":[{"timestamp":"1667058360.0","upvote_count":"1","poster":"Erso","content":"sorry, the link https://aws.amazon.com/blogs/big-data/prepare-data-for-model-training-and-invoke-machine-learning-models-with-amazon-athena/","comment_id":"81663"}],"poster":"Erso","timestamp":"1665527880.0","comment_id":"81662","upvote_count":"12","content":"Answer A seems correct..."},{"comment_id":"324837","timestamp":"1667273100.0","content":"A ( Most queries will span 5 to 10 columns only)","poster":"sonalev419","upvote_count":"5"},{"upvote_count":"1","timestamp":"1724861880.0","content":"Selected Answer: A\nOption A","poster":"Mickey321","comment_id":"992381"},{"comment_id":"607433","content":"clue is: most queries will span 5 to 10 column while there are 200 columns. Indicating Data Warehouse means columner storage. Option A is correct.","poster":"exam_prep","upvote_count":"2","timestamp":"1685060820.0"},{"content":"Selected Answer: A\nA. See https://aws.amazon.com/blogs/big-data/analyzing-data-in-s3-using-amazon-athena/","poster":"edvardo","upvote_count":"4","comment_id":"480948","timestamp":"1668796620.0"}],"question_text":"A Machine Learning Specialist must build out a process to query a dataset on Amazon S3 using Amazon Athena. The dataset contains more than 800,000 records stored as plaintext CSV files. Each record contains 200 columns and is approximately 1.5 MB in size. Most queries will span 5 to 10 columns only.\nHow should the Machine Learning Specialist transform the dataset to minimize query runtime?","answers_community":["A (100%)"],"unix_timestamp":1588248180,"topic":"1","exam_id":26},{"id":"4DW3W2obJuOk1Vu2dISh","topic":"1","timestamp":"2020-07-18 11:12:00","isMC":true,"question_text":"A Machine Learning Specialist is developing a daily ETL workflow containing multiple ETL jobs. The workflow consists of the following processes:\n* Start the workflow as soon as data is uploaded to Amazon S3.\n* When all the datasets are available in Amazon S3, start an ETL job to join the uploaded datasets with multiple terabyte-sized datasets already stored in Amazon\nS3.\n* Store the results of joining datasets in Amazon S3.\n* If one of the jobs fails, send a notification to the Administrator.\nWhich configuration will meet these requirements?","answer_description":"Reference:\nhttps://aws.amazon.com/step-functions/use-cases/","url":"https://www.examtopics.com/discussions/amazon/view/26038-exam-aws-certified-machine-learning-specialty-topic-1/","question_images":[],"answer_images":[],"choices":{"C":"Develop the ETL workflow using AWS Batch to trigger the start of ETL jobs when data is uploaded to Amazon S3. Use AWS Glue to join the datasets in Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure.","B":"Develop the ETL workflow using AWS Lambda to start an Amazon SageMaker notebook instance. Use a lifecycle configuration script to join the datasets and persist the results in Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure.","A":"Use AWS Lambda to trigger an AWS Step Functions workflow to wait for dataset uploads to complete in Amazon S3. Use AWS Glue to join the datasets. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure.","D":"Use AWS Lambda to chain other Lambda functions to read and join the datasets in Amazon S3 as soon as the data is uploaded to Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure."},"unix_timestamp":1595063520,"discussion":[{"timestamp":"1664300700.0","poster":"HaiHN","comment_id":"203827","content":"A: Correct. S3 events can trigger AWS Lambda function.\nB: Wrong. There's nothing to do with SageMaker in the provided context.\nC: Wrong. AWS Batch cannot receive events from S3 directly.\nD: Wrong. Will not meed the requirement: \"When all the datasets are available in Amazon S3...\"\n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html","comments":[{"timestamp":"1664790120.0","poster":"scuzzy2010","content":"I agree. Step Functions can be used to implement a workflow. In this case, wait for all the datasets to be loaded before triggering the glue job.","upvote_count":"3","comment_id":"228690"},{"upvote_count":"4","content":"Actually, I think that D does meet the requirement of waiting until all datasets are in S3, BUT you do need Glue to join the datasets. Answer is still A.","timestamp":"1665451440.0","comment_id":"278765","poster":"cloud_trail"}],"upvote_count":"35"},{"poster":"Mickey321","timestamp":"1724861820.0","upvote_count":"1","comment_id":"992380","content":"Selected Answer: A\nOption A"},{"content":"Selected Answer: A\nBatch isn't event driven, answer is A.","timestamp":"1709901060.0","upvote_count":"1","poster":"Valcilio","comment_id":"832947"},{"comment_id":"638113","poster":"matteocal","content":"If EMR were present I would have chose that because of the size of dataset, else is Glue","upvote_count":"1","timestamp":"1690465260.0","comments":[{"upvote_count":"1","comment_id":"874880","timestamp":"1713543540.0","poster":"ZSun","content":"exactly, this is also where I got confused. Since Glue is not good at handling such large dataset, multiple terabyte-sized datasets + multiple ETL jobs + daily"}]},{"timestamp":"1667697660.0","poster":"Huy","upvote_count":"2","content":"A. The answer omits stuffs like Lambda functions and Event Bridge. https://aws.amazon.com/blogs/big-data/orchestrate-multiple-etl-jobs-using-aws-step-functions-and-aws-lambda/","comment_id":"400408"},{"timestamp":"1667004780.0","upvote_count":"2","comment_id":"328210","poster":"johnvik","content":"https://d1.awsstatic.com/r2018/a/product-page-diagram-aws-step-functions-use-case-aws-glue.bc69d97a332c2dd29abb724dd747fd82ae110352.png"}],"answer_ET":"A","question_id":334,"answers_community":["A (100%)"],"exam_id":26,"answer":"A"},{"id":"pW1E8sJQ6Eqtfh52QLSU","answer_images":[],"question_images":[],"topic":"1","question_id":335,"discussion":[{"timestamp":"1635573120.0","content":"C: (OK) Use PCA for reducing number of variables. Each citizen's response should have answer for 500 questions, so it should have 500 variables\nD: (OK) Use K-means clustering\n\nA: (Not OK) Factorization Machines Algorithm is usually used for tasks dealing with high dimensional sparse datasets\nB: (Not OK) The Latent Dirichlet Allocation (LDA) algorithm should be used for task dealing topic modeling in NLP\nE: (Not OK) Random Cut Forest should be used for detecting anormal in data","comment_id":"199573","poster":"HaiHN","upvote_count":"33"},{"comment_id":"147329","upvote_count":"12","poster":"hans1234","timestamp":"1635160860.0","content":"https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/\n\nAnswer: C and D"},{"timestamp":"1731429120.0","poster":"rodrick10","comment_id":"1310727","upvote_count":"1","content":"Selected Answer: BD\nIf the form contains free-text answers, it would be interesting to apply LDA to identify the most frequent/relevant topics in the answers"},{"poster":"Mickey321","upvote_count":"1","comment_id":"992371","comments":[{"upvote_count":"1","content":"The answer depends on the type of question is it is open ended then would need LDA hence B and D but if the question is a feature then PCA should work","comment_id":"992375","timestamp":"1693239120.0","poster":"Mickey321"}],"timestamp":"1693238880.0","content":"Selected Answer: CD\nOption C and D"},{"timestamp":"1690921380.0","content":"Selected Answer: CD\nC and D are the way","comment_id":"969385","poster":"kaike_reis","upvote_count":"1"},{"content":"CD,\nC - for reduce number of columns.\nD - for data clustering","timestamp":"1688088360.0","comment_id":"938691","upvote_count":"1","poster":"ADVIT"},{"timestamp":"1676052000.0","comment_id":"804627","content":"Selected Answer: CD\nC. The principal component analysis (PCA) algorithm\nD. The k-means algorithm\n\nPCA is a dimensionality reduction technique that can be used to identify the underlying structure of the census data. This algorithm can help to identify the most important questions and provide an overview of the relationship between the questions and the responses.\n\nK-means is an unsupervised learning algorithm that can be used to segment the population into different groups based on their responses to the census questions. This algorithm can help to determine the healthcare and social program needs by province and city based on the responses collected from each citizen.\n\nThese algorithms can help to provide insights into the patterns and relationships within the census data, which can inform decision making for healthcare and social program planning.","upvote_count":"5","poster":"AjoseO"},{"upvote_count":"2","poster":"Peeking","timestamp":"1670562300.0","comment_id":"739809","content":"Selected Answer: CD\nReduce dimensionality and cluster subjects."},{"content":"This is the same question as Topic 2 Q3","comments":[{"upvote_count":"1","poster":"muralee_xo","content":"how to reach Topic 2 every questions here seem to belong to topic 1","timestamp":"1674552900.0","comment_id":"786360"}],"poster":"ac427","comment_id":"67116","upvote_count":"1","timestamp":"1632556320.0"}],"isMC":true,"answer":"CD","answers_community":["CD (90%)","10%"],"question_text":"An agency collects census information within a country to determine healthcare and social program needs by province and city. The census form collects responses for approximately 500 questions from each citizen.\nWhich combination of algorithms would provide the appropriate insights? (Choose two.)","answer_ET":"CD","choices":{"E":"The Random Cut Forest (RCF) algorithm","D":"The k-means algorithm","B":"The Latent Dirichlet Allocation (LDA) algorithm","C":"The principal component analysis (PCA) algorithm","A":"The factorization machines (FM) algorithm"},"unix_timestamp":1584915600,"exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/17281-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2020-03-22 23:20:00","answer_description":""}],"exam":{"provider":"Amazon","name":"AWS Certified Machine Learning - Specialty","isMCOnly":false,"numberOfQuestions":369,"isImplemented":true,"id":26,"lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":67},"__N_SSP":true}