{"pageProps":{"questions":[{"id":"EtkREmjhqqeoj5elmJfo","answer_description":"","question_images":[],"answer_images":[],"isMC":true,"timestamp":"2023-06-21 11:07:00","choices":{"D":"Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.","B":"Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.","A":"Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.","C":"Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication from us-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-wes&1. Configure the application to use the RDS for MySQL endpoint in eu-west-1."},"question_id":211,"answers_community":["A (65%)","D (35%)"],"url":"https://www.examtopics.com/discussions/amazon/view/112787-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"poster":"ggrodskiy","timestamp":"1689895620.0","upvote_count":"23","comment_id":"957943","content":"Correct D.\nYou cannot convert RDS MySQL to Aurora MySQL natively, but you can create an Aurora read replica of the RDS MySQL DB instance and then promote it to a standalone Aurora MySQL DB clusterhttps://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.html. This is the first step of option A in the question. However, this option also requires pausing application writes and reconfiguring the application, which can cause downtime and data inconsistency. Therefore, option A is not the best solution for the given requirements. Option D is still the correct answer because it does not require pausing writes or reconfiguring the application, and it enables cross-Region replication and write forwarding for the database.","comments":[{"timestamp":"1723337580.0","upvote_count":"2","content":"D is wrong, you should choose A.\n\nlook at this blog: https://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/\n\nIn step 4 you do need to pause write","comments":[{"upvote_count":"1","comment_id":"1263685","content":"Sorry, typo, step 4","poster":"kgpoj","timestamp":"1723337640.0"}],"poster":"kgpoj","comment_id":"1263683"},{"comment_id":"1098785","poster":"ayadmawla","upvote_count":"1","content":"You need the pause of writing to the old db because of the lag in the replication.","timestamp":"1702807800.0"}]},{"upvote_count":"10","content":"Selected Answer: A\nYou cannot natively convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Instead, you can create an Amazon Aurora MySQL replica of the RDS MySQL RDS DB instance:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html\nhttps://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/","timestamp":"1697514420.0","poster":"totten","comment_id":"1045498"},{"poster":"albert_kuo","content":"Selected Answer: D\n[us-east-1] [eu-west-1]\n| [Aurora Primary] | [Aurora Secondary]\n| | | |\n[US App] [EU App]\n| | | |\n[Writes] [Write Forwarding]","comment_id":"1366518","upvote_count":"1","timestamp":"1741418280.0"},{"comment_id":"1335106","timestamp":"1735703820.0","content":"Selected Answer: D\nOption D is correct because Option A involves additional steps and downtime for promoting the replica and reconfiguring the application. It also does not directly address the need for real-time updates between Regions.","poster":"Peaches35","upvote_count":"1"},{"upvote_count":"1","comments":[{"content":"Changed to A","upvote_count":"1","comment_id":"1301690","timestamp":"1729625520.0","poster":"sashenka"}],"timestamp":"1729625280.0","content":"Selected Answer: D\nKey Issues with Option A\nUnnecessary Complexity and Risk\nOption A requires multiple steps including creating a replica, pausing writes, promoting the replica, and reconfiguring the application.\nEach step introduces potential points of failure and complexity.\nThe process requires application downtime during the conversion process.\nBusiness Impact\nPausing application writes means service interruption for customers.\nThe multi-step process extends the duration of service disruption.\nReconfiguring applications multiple times increases the risk of errors.","poster":"sashenka","comment_id":"1301688"},{"comment_id":"1263438","timestamp":"1723287480.0","upvote_count":"1","poster":"Daniel76","content":"Selected Answer: D\nAurora supports cross-region replication and write forwarding. Only need to promote DB Custer in the failover scenario, not for migration."},{"poster":"Helpnosense","comment_id":"1244640","upvote_count":"2","content":"Selected Answer: D\nVote D.\nOn top of ggrodskiy's point, standalone DB cluster only support 1 regoin. If multiple regions are requireed then DB cluster won't be standalone.","timestamp":"1720489140.0"},{"upvote_count":"1","comment_id":"1205996","timestamp":"1714725540.0","content":"Selected Answer: A\nA for me","poster":"seetpt"},{"content":"Selected Answer: D\nThis approach leverages Amazon Aurora's Global Database capability, which allows for a single database to span multiple AWS regions, thus enabling low-latency reads and writes in multiple regions and providing data replication across regions with minimal latency. By converting the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster and enabling write forwarding, the solution supports writes in multiple regions and ensures that the data is synchronized across the regions in real time. This setup allows customers in both the US and Europe to see updates from each other as they happen, meeting the requirement for real-time data consistency and low application latency.","upvote_count":"3","comment_id":"1168231","timestamp":"1709833020.0","poster":"Russs99"},{"upvote_count":"1","timestamp":"1706549100.0","poster":"LazyAutonomy","content":"Galera + ProxySQL ftw","comment_id":"1135145"},{"upvote_count":"5","poster":"tmlong18","content":"Selected Answer: A\nD said 'Convert' but not 'Mirgrate'. You cannot convert RDS MySQL to Aurora MySQL natively.","comment_id":"1123035","timestamp":"1705290420.0"},{"upvote_count":"3","comments":[{"timestamp":"1702807980.0","poster":"ayadmawla","content":"The first statement in D (\"Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster.\") is wrong, therefore D is wrong. The multiple choice is based on these tricks. Real life is a different matter when we say \"Convert\" to mean go through the process of replacing by replicating, etc.","comment_id":"1098789","upvote_count":"1"}],"poster":"duriselvan","timestamp":"1702309860.0","comment_id":"1093635","content":"D CORRECT\nD. Aurora Global Database with Write Forwarding:\n\nThis solution addresses all requirements:\nReal-time data access and updates: Aurora provides global secondary databases in the chosen region (eu-west-1) for low latency and consistent data.\nMinimal downtime: Aurora automatically handles failovers and data synchronization between regions.\nWrite forwarding: Both regions can perform write operations, ensuring real-time updates for all users.\nHigh availability: Aurora offers automatic backups and failover capabilities.\nTherefore, D. Converting the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster with a secondary Region in eu-west-1 and enabling write forwarding is the most suitable solution. It meets all requirements for data availability, minimal latency, real-time updates, and high availability for both US and European customers."},{"content":"Selected Answer: A\nOption A","timestamp":"1700438460.0","upvote_count":"2","poster":"career360guru","comment_id":"1075020"},{"comment_id":"1074590","timestamp":"1700391840.0","content":"Selected Answer: A\nyes, migration is done through the replica promotion","upvote_count":"3","poster":"severlight"},{"content":"Selected Answer: A\nRDS MySQL to aurora replica, then promote the replica as aurora cluster.","comment_id":"1060850","poster":"Ustad","upvote_count":"3","timestamp":"1698955140.0"},{"timestamp":"1696305480.0","content":"A is ans:","comment_id":"1023584","upvote_count":"1","poster":"duriselvan"},{"upvote_count":"5","comment_id":"1016240","poster":"nharaz","timestamp":"1695599340.0","content":"Selected Answer: D\nWrite forwarding is a feature of Aurora that allows writes to be directed to the primary cluster while maintaining read access to the replica cluster, ensuring data consistency and low latency."},{"content":"Selected Answer: A\nIt's clearly A , not any other option","timestamp":"1692877860.0","poster":"xav1er","upvote_count":"2","comment_id":"989148"},{"content":"Selected Answer: A\nA, ‘cause of the conversion which is not possible","poster":"Asds","comment_id":"968322","upvote_count":"4","timestamp":"1690821960.0"},{"timestamp":"1689691740.0","upvote_count":"3","comment_id":"955545","content":"Selected Answer: A\nThe reason you create an Amazon Aurora MySQL Replica is because \"replication lag between source DB instance and Aurora Read Replica approaches zero\" , and here are the steps recommended and isntructed as part of an AWS Workshop https://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/#:~:text=2.1%20-%20Open%20the%20Amazon%20RDS,choose%20Create%20Aurora%20read%20replica.","poster":"Mom305"},{"comment_id":"950999","content":"Selected Answer: A\nAnswer A","timestamp":"1689285300.0","poster":"Zox42","upvote_count":"2"},{"poster":"NikkyDicky","timestamp":"1688785740.0","upvote_count":"3","comment_id":"946110","content":"Selected Answer: A\nA - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html"},{"comments":[{"comment_id":"946109","content":"ehh, that article describes A, not D","timestamp":"1688785620.0","upvote_count":"1","poster":"NikkyDicky"}],"upvote_count":"1","timestamp":"1688714940.0","poster":"hexie","comment_id":"945434","content":"Selected Answer: D\nI'm going for D just because the doc I'll send below says its possible to do what A says, but its a read replica. A read replica doesnt perform operations itself, its just for read purposes.\nhttps://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/"},{"poster":"YodaMaster","timestamp":"1688640120.0","upvote_count":"2","content":"Selected Answer: A\nhttps://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/","comment_id":"944544"},{"comment_id":"943369","timestamp":"1688538480.0","content":"Selected Answer: A\nYou cannot convert RDS MySQL to Aurora MySQL natively. Need to create a AUrora read replica first.","upvote_count":"3","poster":"qwertyuio"},{"content":"Selected Answer: A\nI would go for A. D would be great but it would require some effort to I am not usre it posible to convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Option A gives more details and is perfectly possible\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html","upvote_count":"3","poster":"javitech83","timestamp":"1688122740.0","comment_id":"939028","comments":[{"timestamp":"1693296180.0","poster":"aviathor","upvote_count":"1","content":"The Aurora read replica can be promoted to a stand-alone cluster.","comment_id":"992903"}]},{"poster":"james55","timestamp":"1687888860.0","upvote_count":"3","content":"Selected Answer: A\nI could be wrong but I can find documentation for A but not D.\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Replica.html\n\n\"Before you promote your Aurora read replica, stop any transactions from being written to the source MySQL DB instance, and then wait for the replica lag on the Aurora read replica to reach 0\"","comment_id":"935676"},{"content":"Selected Answer: A\nA - See step-by-step here -> https://aws.amazon.com/getting-started/hands-on/migrate-rdsmysql-to-auroramysql/","poster":"SmileyCloud","timestamp":"1687887540.0","comment_id":"935652","upvote_count":"3"},{"comment_id":"934548","timestamp":"1687789260.0","content":"Selected Answer: D\nA seems to work as well, but D is more clear and direct, and does not involve pausing the DB.\n\nFor write forwarding : In an Amazon RDS DB cluster, write forwarding is a feature that allows write operations to be directed to the primary instance even when connecting to a reader instance. This helps ensure that write operations are always sent to the appropriate instance in the cluster.\n\nPS: no where mentionbed Aurora Global Database in the question, and Aurora Global Database is not the default behaviour of DB. so it is somethign you have to turn on and have it configured.","upvote_count":"2","poster":"nexus2020"},{"comment_id":"933904","timestamp":"1687730400.0","poster":"Alabi","upvote_count":"4","content":"Selected Answer: D\nOption D is the recommended solution. By converting the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster, the company can add eu-west-1 as a secondary Region to the cluster, enabling low-latency access to the data in Europe. Write forwarding can be enabled on the DB cluster, allowing writes to be directed to the primary Region (us-east-1) and automatically forwarded to the secondary Region (eu-west-1). This ensures real-time updates and data consistency between the two Regions. By deploying the application in eu-west-1 and configuring it to use the Aurora MySQL endpoint in eu-west-1, customers in Europe will have access to the data with low latency and real-time updates."},{"poster":"gd1","timestamp":"1687624620.0","comment_id":"932767","content":"Selected Answer: D\nWith Aurora Global Database, you can set up one primary region where the read-write operations take place, and up to five read-only secondary regions. In Aurora, you can use write forwarding, which allows Aurora MySQL DB clusters that are secondary clusters in an Aurora global database to automatically forward write operations to the primary DB cluster and return the results of the operations back to your application.","upvote_count":"3"},{"comment_id":"929251","upvote_count":"4","poster":"psyx21","timestamp":"1687338420.0","content":"Selected Answer: D\nCorrect Answer is D"}],"topic":"1","question_text":"A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data available to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not tolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of customers need to see updates from the other group in real time.\n\nWhich solution will meet these requirements?","answer":"A","answer_ET":"A","unix_timestamp":1687338420,"exam_id":33},{"id":"BhyrKfdd2HFtraNbJ4hc","question_id":212,"isMC":true,"answers_community":["A (95%)","5%"],"url":"https://www.examtopics.com/discussions/amazon/view/91457-exam-aws-certified-solutions-architect-professional-sap-c02/","choices":{"B":"In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources.","A":"In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.","D":"Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance team’s AWS account.","C":"In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources."},"answer_ET":"A","question_text":"A company that has multiple AWS accounts is using AWS Organizations. The company’s AWS accounts host VPCs, Amazon EC2 instances, and containers.\nThe company’s compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2 instances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-related resources with a key of “costCenter” and a value or “compliance”.\nThe company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the compliance team’s AWS account. The cost calculation must be as accurate as possible.\nWhat should a solutions architect do to meet these requirements?","discussion":[{"comment_id":"774816","timestamp":"1673640540.0","poster":"masetromain","content":"Selected Answer: A\nAnswer A : because we do not depend on the users, I prefer management account\n\nOption C or A would be the correct answer. In option C, the solution architect would activate the costCenter user-defined tag in the member accounts of the organization, and then schedule a monthly AWS Cost and Usage Report from the management account to retrieve the reports and calculate the total cost for the costCenter tagged resources. In option A, the management account of the organization would activate the costCenter user-defined tag and configure monthly AWS Cost and Usage Reports to be saved to an Amazon S3 bucket in the management account. Then, use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources. Both options would allow the company to accurately identify the cost of the security tools running on the EC2 instances and charge the compliance team’s AWS account.","upvote_count":"20","comments":[{"content":"Only a management account in an organization and single accounts that aren't members of an organization have access to the cost allocation tags manager in the Billing and Cost Management console.\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html","timestamp":"1687775940.0","poster":"dkx","upvote_count":"13","comment_id":"934309"},{"timestamp":"1685417280.0","poster":"chathur","comments":[{"comment_id":"1240631","content":"Did you mean from member account? in this sentence \"User-defined tags can not be allowed from management accounts in AWS Organization.\"","upvote_count":"1","timestamp":"1719907680.0","poster":"Reval"}],"content":"User-defined tags can not be allowed from management accounts in AWS Organization. It must done from the management Account.","upvote_count":"2","comment_id":"909841"}]},{"content":"Selected Answer: A\nI vote A.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/configurecostallocreport.html","comment_id":"759868","timestamp":"1672233180.0","poster":"Untamables","upvote_count":"6"},{"comment_id":"1319806","content":"Selected Answer: A\nA. In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.","upvote_count":"1","poster":"Tiger4Code","timestamp":"1732898460.0"},{"poster":"amministrazione","content":"A. In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.","timestamp":"1725093060.0","comment_id":"1275471","upvote_count":"1"},{"content":"Selected Answer: A\nThe most ideal way to get this job done is to use: AWS Cost Explorer\n\n\nBut among all the given options, we should go with option A, as the user defined tag can only be managed in management account","timestamp":"1722685920.0","upvote_count":"1","poster":"Jason666888","comment_id":"1260262"},{"comment_id":"1175357","upvote_count":"1","content":"Selected Answer: A\nA is correct","timestamp":"1710626760.0","poster":"gofavad926"},{"poster":"subbupro","comment_id":"1088461","content":"A is ccorect, we need to login to management account to create","upvote_count":"1","timestamp":"1701777840.0"},{"comment_id":"1068356","poster":"severlight","content":"Selected Answer: A\nyes, you need to activate cost allocation tags before using, you can do this the same place where you would like to see your reports - management account","upvote_count":"2","timestamp":"1699775460.0"},{"comment_id":"1046478","poster":"whenthan","upvote_count":"1","content":"Selected Answer: C\nlines up correctly \nactivate tag in member accounts and generating AWS CUR from management account ( has ability to see costs across all member accounts) and Tag breakfdown in report","timestamp":"1697592960.0"},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html\n\"For tags to appear on your billing reports, you must activate them.\"\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html\n\"Only a management account in an organization and single accounts that aren't members of an organization have access to the cost allocation tags manager in the Billing and Cost Management console.\"\n-> eliminate B,C. D is not relevant","comment_id":"1022058","upvote_count":"2","timestamp":"1696142460.0","poster":"imvb88"},{"content":"Selected Answer: A\nhttps://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/building-a-cost-allocation-strategy.html","comment_id":"998936","timestamp":"1693877220.0","poster":"whenthan","upvote_count":"1"},{"timestamp":"1693456980.0","content":"Selected Answer: A\nOnly a management account in an organization and single accounts that aren't members of an organization have access to the cost allocation tags manager in the Billing and Cost Management console.","upvote_count":"3","comment_id":"994749","poster":"bur4an"},{"upvote_count":"1","poster":"NikkyDicky","content":"it's an A","timestamp":"1687987080.0","comment_id":"937145"},{"content":"I go with D","poster":"rtguru","timestamp":"1684943580.0","upvote_count":"1","comment_id":"906016"},{"poster":"mfsec","upvote_count":"1","timestamp":"1679981340.0","comment_id":"852820","content":"Selected Answer: A\nCost center tag int he management account."},{"upvote_count":"1","comment_id":"832277","poster":"kiran15789","timestamp":"1678221120.0","content":"Selected Answer: A\nManagement account for reports"},{"comment_id":"792940","upvote_count":"2","content":"Selected Answer: A\nAnswer A","timestamp":"1675090380.0","poster":"zozza2023"},{"comment_id":"759369","timestamp":"1672203840.0","comments":[{"comment_id":"759376","timestamp":"1672204020.0","content":"Change to A, the activation of user tag for billing can only be done by management account","poster":"yimicc","upvote_count":"5"}],"upvote_count":"1","poster":"yimicc","content":"Selected Answer: C\nShould be a C"},{"timestamp":"1671561240.0","upvote_count":"4","poster":"tman22","content":"A. You want the cost information across all accounts - So you use the management account.","comment_id":"751340"},{"timestamp":"1670946000.0","poster":"masetromain","content":"I want to answer C","upvote_count":"1","comment_id":"744217"}],"timestamp":"2022-12-13 16:40:00","unix_timestamp":1670946000,"answer":"A","question_images":[],"answer_description":"","topic":"1","exam_id":33,"answer_images":[]},{"id":"xok2Qr4Evm41G5PKxtwb","isMC":true,"question_text":"A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single Amazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for authentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses.\n\nA solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the disruption to customers who access files. The solution must not change the way customers connect.\n\nWhich solution will meet these requirements?","answer_ET":"B","answer":"B","unix_timestamp":1687338480,"url":"https://www.examtopics.com/discussions/amazon/view/112788-exam-aws-certified-solutions-architect-professional-sap-c02/","topic":"1","timestamp":"2023-06-21 11:08:00","discussion":[{"timestamp":"1688351760.0","comment_id":"941382","content":"Selected Answer: B\nB\nQuestion say \" The EC2 instance also has an attached security group that allows access from all customer IP addresses.\"\n\nB say \"Attach the security group with customer IP addresses to the new endpoint\"\n\nShould be Security Group for working with security for customer","poster":"SkyZeroZx","upvote_count":"7"},{"timestamp":"1687887840.0","poster":"SmileyCloud","upvote_count":"5","content":"Selected Answer: B\nIt's B. You can't attach elastic IP with A). -> https://repost.aws/knowledge-center/aws-sftp-endpoint-type - look at the table","comment_id":"935659"},{"comment_id":"1294842","upvote_count":"2","timestamp":"1728412200.0","poster":"JoeTromundo","content":"Selected Answer: B\nhttps://repost.aws/knowledge-center/aws-sftp-endpoint-type"},{"poster":"Syre","comment_id":"1281116","timestamp":"1725901080.0","upvote_count":"1","content":"Selected Answer: A\nB is wrong, it's similar to A but uses a VPC-hosted endpoint, which is unnecessary for this public-facing scenario and adds complexity without any clear benefit."},{"timestamp":"1702898880.0","content":"https://docs.aws.amazon.com/transfer/latest/userguide/create-server-in-vpc.html -b ans","poster":"duriselvan","comment_id":"1099673","upvote_count":"2"},{"content":"S Fargate and a Network Load Balancer provides the most efficient and secure solution, meeting all the requirements without compromising availability, introducing unnecessary complexity, or disrupting existing customer access.","upvote_count":"1","poster":"duriselvan","comment_id":"1091192","timestamp":"1702053660.0"},{"poster":"career360guru","timestamp":"1700438760.0","content":"Selected Answer: B\nOption B","upvote_count":"1","comment_id":"1075023"},{"comment_id":"1046800","timestamp":"1697623080.0","upvote_count":"1","content":"Answer is B.\nhttps://aws.amazon.com/blogs/storage/use-ip-whitelisting-to-secure-your-aws-transfer-for-sftp-servers/","poster":"rlf"},{"poster":"NikkyDicky","upvote_count":"1","comment_id":"946113","timestamp":"1688785920.0","content":"Selected Answer: B\nB of course. need SG to whitelist IPs"},{"content":"Selected Answer: B\nhttps://repost.aws/knowledge-center/aws-sftp-endpoint-type","timestamp":"1688640420.0","upvote_count":"2","poster":"YodaMaster","comment_id":"944548"},{"timestamp":"1687666320.0","comment_id":"933171","upvote_count":"4","content":"Selected Answer: B\nIt's B: https://repost.aws/knowledge-center/aws-sftp-endpoint-type","poster":"ozelllll"},{"content":"Selected Answer: B\nA is public access; the requirement says need Security Group with Ip addresses - B is correct","poster":"gd1","upvote_count":"1","comment_id":"932776","timestamp":"1687625040.0"},{"timestamp":"1687472460.0","comment_id":"931089","upvote_count":"2","poster":"Jackhemo","content":"Selected Answer: B\nOlabiba.ai Says B:\n\nOption B suggests disassociating the Elastic IP address from the EC2 instance and creating an Amazon S3 bucket for SFTP file hosting. An AWS Transfer Family server is then created and configured with a VPC-hosted, internet-facing endpoint. The SFTP Elastic IP address is associated with the new endpoint, and the security group with customer IP addresses is attached to the endpoint. The Transfer Family server is pointed to the S3 bucket, and all files from the SFTP server are synced to the S3 bucket."},{"content":"Selected Answer: A\nCorrect Answer is A","poster":"psyx21","timestamp":"1687338480.0","comments":[{"upvote_count":"2","poster":"rxhan","content":"again wrong, dont be quick and wrong.","comment_id":"968492","timestamp":"1690835880.0"}],"upvote_count":"3","comment_id":"929252"}],"question_images":[],"answer_images":[],"answer_description":"","exam_id":33,"answers_community":["B (86%)","14%"],"choices":{"C":"Disassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used for SFTP file hosting. Create an AWS Fargate task definition to run an SFTP server. Specify the EFS file system as a mount in the task definition. Create a Fargate service by using the task definition, and place a Network Load Balancer (NLB) in front of the service. When configuring the service, attach the security group with customer IP addresses to the tasks that run the SFTP server. Associate the Elastic IP address with the NLB. Sync all files from the SFTP server to the S3 bucket.","A":"Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a publicly accessible endpoint. Associate the SFTP Elastic IP address with the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.","B":"Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a VPC-hosted, internet-facing endpoint. Associate the SFTP Elastic IP address with the new endpoint. Attach the security group with customer IP addresses to the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.","D":"Disassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be used for SFTP file hosting. Create a Network Load Balancer (NLB) with the Elastic IP address attached. Create an Auto Scaling group with EC2 instances that run an SFTP server. Define in the Auto Scaling group that instances that are launched should attach the new multi-attach EBS volume. Configure the Auto Scaling group to automatically add instances behind the NLB. Configure the Auto Scaling group to use the security group that allows customer IP addresses for the EC2 instances that the Auto Scaling group launches. Sync all files from the SFTP server to the new multi-attach EBS volume."},"question_id":213},{"id":"TF2LVc1sit0dJiEyuqFh","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/112789-exam-aws-certified-solutions-architect-professional-sap-c02/","question_text":"A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics takes 4 hours to complete. The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run fails.\n\nThe current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations. These EC2 instances run full time to ingest and store the streaming data in attached Amazon Elastic Block Store (Amazon EBS) volumes. A scheduled script launches EC2 On-Demand Instances each night to perform the nightly processing. The instances access the stored data from NFS shares on the ingestion servers. The script terminates the instances when the processing is complete.\n\nThe Reserved Instance reservations are expiring. The company needs to determine whether to purchase new reservations or implement a new design.\n\nWhich solution will meet these requirements MOST cost-effectively?","choices":{"B":"Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price.","A":"Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a scheduled script to launch a fleet of EC2 On-Demand Instances each night to perform the batch processing of the S3 data. Configure the script to terminate the instances when the processing is complete.","D":"Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use Amazon EventBridge to schedule an AWS Lambda function to run nightly to query Amazon Redshift to generate the daily statistics.","C":"Update the ingestion process to use a fleet of EC2 Reserved Instances with 3-year reservations behind a Network LoadBalancer. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price."},"question_id":214,"answer":"B","question_images":[],"topic":"1","unix_timestamp":1687338480,"answer_ET":"B","isMC":true,"exam_id":33,"answers_community":["B (96%)","4%"],"timestamp":"2023-06-21 11:08:00","answer_images":[],"discussion":[{"timestamp":"1707663720.0","comment_id":"978635","upvote_count":"6","poster":"softarts","content":"Selected Answer: B\nA=> Use a scheduled script to launch a fleet of EC2 On-Demand wrong\nC=> Update the ingestion process to use a fleet of EC2 Reserved Instances wrong\nD=> lambda wrong"},{"upvote_count":"2","content":"Selected Answer: B\nAnswer B:\nhttps://docs.aws.amazon.com/batch/latest/userguide/best-practices.html","timestamp":"1721596440.0","poster":"kejam","comment_id":"1128241"},{"poster":"Niko13","upvote_count":"1","content":"Selected Answer: B\nCorrect Answer is B","timestamp":"1719482400.0","comment_id":"1106757"},{"poster":"career360guru","timestamp":"1716170640.0","content":"Selected Answer: B\nB is right answer. In C in addition to 3 year reserved instances NLB is extra cost.","comments":[{"timestamp":"1716170820.0","poster":"career360guru","content":"Compared to on-demand, Reserved instances can be upto 73% reduction but Spot can go upto 90%.","comment_id":"1075133","upvote_count":"2"}],"comment_id":"1075132","upvote_count":"4"},{"timestamp":"1707321240.0","content":"Selected Answer: C\nFor a stable rate of ingestion I choose EC2 with 3yr reservation over Firehose & S3API costs. Using Spot instances for the low priority aggregation will lower the costs further","upvote_count":"1","poster":"hglopes","comment_id":"974753"},{"comment_id":"946560","content":"Selected Answer: B\nits a B","poster":"NikkyDicky","upvote_count":"1","timestamp":"1704731580.0"},{"timestamp":"1703709840.0","poster":"SmileyCloud","upvote_count":"4","content":"Selected Answer: B\nB - Correct. And only because of this -> \" The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run fails.\"\nSpot instances are not guaranteed and if the condition above was not there, than probably C.","comment_id":"935727"},{"content":"b-b-b-b-b-b-b","poster":"easytoo","upvote_count":"2","timestamp":"1703623140.0","comment_id":"934716"},{"comment_id":"932781","poster":"gd1","timestamp":"1703443800.0","content":"Selected Answer: B\nS3 + Batch with SOT servers","upvote_count":"2"},{"upvote_count":"1","comment_id":"930033","content":"Support B as answer. MOST cost effective","timestamp":"1703213760.0","poster":"Don2021"},{"comment_id":"929253","poster":"psyx21","upvote_count":"2","content":"Selected Answer: B\nCorrect Answer is B","timestamp":"1703156880.0"}]},{"id":"Ic39co2eazmpUwSCdj1x","question_id":215,"answer_ET":"A","topic":"1","answer_description":"","exam_id":33,"choices":{"B":"Create an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.","C":"Use AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the EC2 instance. Mount an Amazon Elastic File System (Amazon EFS) file system to the EC2 instance. Configure the SFTP server to place files in the EFS file system. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.","A":"Create an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP address for each subnet. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.","D":"Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon FSx for Lustre file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the FSx for Lustre endpoint instead."},"timestamp":"2023-06-21 11:09:00","discussion":[{"upvote_count":"7","content":"Selected Answer: A\nA is correct\nB is incorrect because for Publicly accessible endpoints for AWS Transfer Family you can’t attach a static IP address. AWS provides IP addresses that are subject to change. IPs are provided via AWS Global Accelerator, which uses static Anycast IP addresses\nhttps://repost.aws/knowledge-center/aws-sftp-endpoint-type","timestamp":"1687613940.0","comment_id":"932592","poster":"bhanus"},{"timestamp":"1725901920.0","upvote_count":"1","content":"Selected Answer: B\nA is wrong. The use of an internet-facing VPC endpoint for AWS Transfer Family is not necessary here. The publicly accessible endpoint provided by AWS Transfer Family itself meets the requirement for external access. Also, assigning Elastic IPs to subnets is unnecessary because AWS Transfer Family manages public IPs.","poster":"Syre","comment_id":"1281130"},{"upvote_count":"3","timestamp":"1702052640.0","content":"a IS ANS\nHere's why this solution is optimal:\n\nManaged SFTP: AWS Transfer Family eliminates the need to manage and maintain SFTP servers, reducing operational overhead compared to EC2-based solutions.\nHigh availability: It provides built-in high availability, ensuring continuous access to SFTP services even in case of component failures.\nStatic IP addresses: The internet-facing VPC endpoint with Elastic IP addresses provides fixed IPs for external vendors, meeting their security requirements.\nSecure file storage: EFS offers a managed, scalable, and highly available file system, ensuring secure file storage and access for downstream applications.\nNFS compatibility: EFS integrates seamlessly with NFS, allowing easy migration of downstream applications to the new file system.","comment_id":"1091185","poster":"duriselvan"},{"timestamp":"1700453520.0","poster":"career360guru","content":"Selected Answer: A\nOption A","upvote_count":"2","comment_id":"1075136"},{"content":"Correct A.","poster":"ggrodskiy","timestamp":"1689894780.0","comment_id":"957934","upvote_count":"2"},{"content":"Selected Answer: A\nAAAAAAAAA","comment_id":"952384","poster":"Jonalb","upvote_count":"2","timestamp":"1689425160.0"},{"timestamp":"1688826900.0","comment_id":"946562","upvote_count":"1","poster":"NikkyDicky","content":"Selected Answer: A\nits an A.. static IPs"},{"upvote_count":"2","comment_id":"935730","content":"Selected Answer: A\nIt's A. You can't have elastic IP with B.","poster":"SmileyCloud","timestamp":"1687891560.0"},{"upvote_count":"3","poster":"Jonalb","comment_id":"935729","content":"Selected Answer: A\nA https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-sftp-server-to-aws-using-aws-transfer-for-sftp.html","timestamp":"1687891560.0"},{"comment_id":"933905","content":"Selected Answer: A\nOption A suggests creating an AWS Transfer Family server and configuring an internet-facing VPC endpoint for it. By specifying an Elastic IP address for each subnet, the company can provide a set of static public IP addresses to external vendors. The Transfer Family server can be configured to place files into an Amazon Elastic File System (Amazon EFS) file system, which provides a scalable and highly available storage solution across multiple Availability Zones. This allows the company to maintain high availability for the SFTP site and its downstream applications without the need for manual intervention or additional operational overhead.","poster":"Alabi","timestamp":"1687731720.0","upvote_count":"3"},{"content":"Selected Answer: A\nA is correct for Pvt IP addresses.","comment_id":"932795","poster":"gd1","timestamp":"1687626480.0","upvote_count":"1"},{"poster":"bhanus","content":"Selected Answer: A\nA is correct\nIn B there is NO mention of elasticIPs. the question asks \"The solution must provide external vendors with a set of static public IP addresses that the vendors can allow\"","comment_id":"932585","upvote_count":"2","timestamp":"1687613640.0"},{"comment_id":"929254","poster":"psyx21","content":"Selected Answer: A\nCorrect Answer is A","timestamp":"1687338540.0","upvote_count":"2"}],"answer":"A","question_text":"A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to downstream applications through an NFS share.\n\nAs part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of static public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data center and its VPC.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","answer_images":[],"unix_timestamp":1687338540,"question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/112790-exam-aws-certified-solutions-architect-professional-sap-c02/","isMC":true,"answers_community":["A (96%)","4%"]}],"exam":{"isBeta":false,"isMCOnly":true,"provider":"Amazon","isImplemented":true,"lastUpdated":"11 Apr 2025","numberOfQuestions":529,"id":33,"name":"AWS Certified Solutions Architect - Professional SAP-C02"},"currentPage":43},"__N_SSP":true}