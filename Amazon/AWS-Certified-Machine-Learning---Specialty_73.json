{"pageProps":{"questions":[{"id":"sJQNBgPJ75xTICDpyvNw","isMC":true,"question_id":361,"exam_id":26,"discussion":[{"comment_id":"282376","upvote_count":"53","comments":[{"comment_id":"514337","content":"AGREE WITH YOU","poster":"youjun","upvote_count":"2","timestamp":"1672544400.0"},{"timestamp":"1664977320.0","content":"Agree, the answer is A","upvote_count":"7","comment_id":"291716","poster":"astonm13"}],"content":"This is a supervised problem and needs labels. Can't use clustering to find when faults can happen. CNN is for images not for timeseries data here. Hence, A seems appropriate.","timestamp":"1664557140.0","poster":"ac71"},{"content":"Selected Answer: A\nA. YES - RNN good for time series as we want to use previous input\nB. NO - we know the class (fault) ahead of time, it is supervised\nC. NO - CNN is for images\nD. NO - seq2seq is for word generation","timestamp":"1726388340.0","comment_id":"1008267","upvote_count":"1","poster":"loict"},{"content":"Selected Answer: A\nAnswer is A","poster":"Mickey321","timestamp":"1724848080.0","upvote_count":"1","comment_id":"992158"},{"poster":"AjoseO","content":"Selected Answer: A\nA recurrent neural network (RNN) is a more suitable choice than a convolutional neural network (CNN) because the data collected from the engines is a sequence of values over time, and the goal is to predict a future event (an engine fault). RNNs are designed to handle sequential data and can learn patterns and dependencies over time, making them well-suited for time-series data like this. \n\nOn the other hand, CNNs are designed for image processing and are not ideal for sequential data.","comment_id":"804852","timestamp":"1707606540.0","upvote_count":"3"},{"poster":"spidy20","content":"Selected Answer: A\nAnswer should be A","timestamp":"1692335640.0","comment_id":"648303","upvote_count":"1"},{"poster":"Morsa","timestamp":"1689338460.0","comment_id":"631363","content":"Selected Answer: A\nIt can only be A. Agree with the comments before","upvote_count":"1"},{"upvote_count":"2","timestamp":"1685178540.0","poster":"irimala","comment_id":"608005","content":"Selected Answer: A\nObviously A"},{"timestamp":"1675379220.0","content":"Selected Answer: A\nA - obviously.","comment_id":"539241","poster":"apprehensive_scar","upvote_count":"1"},{"content":"Seq2Seq also uses RNN under the hood, BUT option D. did not mention anything about \"adding labels\"--which is required here--hence --> A","timestamp":"1674230220.0","upvote_count":"3","poster":"bitsplease","comment_id":"528568"},{"upvote_count":"1","poster":"geekgirl007","timestamp":"1673102460.0","content":"Selected Answer: A\nA is correct. CNN is for images and RNN is for timeseries.","comment_id":"519023"},{"upvote_count":"2","content":"AAAAAAAAAAAA\nhttps://towardsdatascience.com/how-to-implement-machine-learning-for-predictive-maintenance-4633cdbe4860","timestamp":"1667619420.0","comment_id":"428320","poster":"loyor94478"},{"upvote_count":"1","timestamp":"1667385360.0","comment_id":"413193","content":"I think A is correct","poster":"omar8024"},{"comment_id":"324952","content":"It is A","poster":"Vita_Rasta84444","timestamp":"1666145460.0","upvote_count":"1"}],"topic":"1","question_text":"A manufacturer of car engines collects data from cars as they are being driven. The data collected includes timestamp, engine temperature, rotations per minute\n(RPM), and other sensor readings. The company wants to predict when an engine is going to have a problem, so it can notify drivers in advance to get engine maintenance. The engine data is loaded into a data lake for training.\nWhich is the MOST suitable predictive model that can be deployed into production?","answer_images":[],"unix_timestamp":1612321440,"answer":"A","answer_ET":"A","choices":{"C":"Add labels over time to indicate which engine faults occur at what time in the future to turn this into a supervised learning problem. Use a convolutional neural network (CNN) to train the model to recognize when an engine might need maintenance for a certain fault.","D":"This data is already formulated as a time series. Use Amazon SageMaker seq2seq to model the time series.","B":"This data requires an unsupervised learning algorithm. Use Amazon SageMaker k-means to cluster the data.","A":"Add labels over time to indicate which engine faults occur at what time in the future to turn this into a supervised learning problem. Use a recurrent neural network (RNN) to train the model to recognize when an engine might need maintenance for a certain fault."},"answer_description":"","question_images":[],"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/43867-exam-aws-certified-machine-learning-specialty-topic-1/","timestamp":"2021-02-03 04:04:00"},{"id":"jV6A7IGk43LNrBSCeHrp","question_text":"A company wants to predict the sale prices of houses based on available historical sales data. The target variable in the company's dataset is the sale price. The features include parameters such as the lot size, living area measurements, non-living area measurements, number of bedrooms, number of bathrooms, year built, and postal code. The company wants to use multi-variable linear regression to predict house sale prices.\nWhich step should a machine learning specialist take to remove features that are irrelevant for the analysis and reduce the model's complexity?","choices":{"C":"Build a heatmap showing the correlation of the dataset against itself. Remove features with low mutual correlation scores.","D":"Run a correlation check of all features against the target variable. Remove features with low target variable correlation scores.","A":"Plot a histogram of the features and compute their standard deviation. Remove features with high variance.","B":"Plot a histogram of the features and compute their standard deviation. Remove features with low variance."},"question_images":[],"discussion":[{"timestamp":"1666784400.0","content":"D should be the more comprehensive answer. If it's not correlated, you can't make use of it in a linear regression\nA lot of others say B, but low variance can also be due to the nature/typical magnitudes of the variable itself","comments":[{"poster":"hamimelon","comment_id":"749381","upvote_count":"1","content":"I think the problem with B is that what is considered \"low variance\"? The features are on different scales.","timestamp":"1702952760.0"},{"poster":"V_B_","content":"Correlation indicates only linear relation, but, there might be non linear as well. To exploit it in the Linear Regression, you can take the variables to some power or run some non linear preprocessing on it, and you don't have to change the algorithm for it. \nSo, answer B seem much more solid for me.","upvote_count":"2","comment_id":"644178","timestamp":"1691505780.0"}],"comment_id":"397907","poster":"puffpuff","upvote_count":"29"},{"poster":"ahquiceno","content":"Answer B. Is not the best solucion prior can use other analysis. https://community.dataquest.io/t/feature-selection-features-with-low-variance/2418\nIf the variance is low or close to zero, then a feature is approximately constant and will not improve the performance of the model. In that case, it should be removed. Or if only a handful of observations differ from a constant value, the variance will also be very low.","upvote_count":"16","comments":[{"content":"Low variance does not mean the feature is not important, right?\nIf variance of target true value is also small and the correlation between above feature and target, the feature can be important feature.","timestamp":"1668768360.0","poster":"fshkkento","upvote_count":"7","comment_id":"480619","comments":[{"upvote_count":"1","content":"it does. If feature and target are correlated and you expect the target to change, the feature must have some sort of variance. Otherwise it means feature is almost constant so does target.","poster":"rb39","timestamp":"1694352720.0","comment_id":"665440"}]}],"comment_id":"282803","timestamp":"1663723020.0"},{"content":"Selected Answer: D\nD is the best answer as it is mentioned multivariable linear regression applied where correlation is strong between dependent and independent variables.","comment_id":"1074342","timestamp":"1731970560.0","upvote_count":"1","poster":"akgarg00"},{"upvote_count":"2","timestamp":"1718544240.0","comment_id":"925206","content":"Selected Answer: D\nD: We should remove features that are strongly correlated with each other and weakly correlated with the target:\n\nhttps://androidkt.com/find-correlation-between-features-and-target-using-the-correlation-matrix/\nYou can evaluate the relationship between each feature and target using a correlation and selecting those features that have the strongest relationship with the target variable.","poster":"mirik"},{"timestamp":"1714175520.0","upvote_count":"1","comment_id":"882094","content":"Selected Answer: D\nI think D is the correct answer. If I remember correctly, Benjamini-Hochberg Method is essentially answer D if you consider the Hypothesis to be: the feature is powerfully influential to the target.\nMy problem with B is that the variance can be easily affected by the scale. In the question, the number of bedroom's variance is very low, while the sqrt of the house has a high variance, both of these could be very useful. Furthermore, zip codes are included, and it is safe to assume the variance of zip codes can be high, but the information is very limited, especially if you use them as numerical instead of categorical features.","poster":"HunterZ9527"},{"content":"Selected Answer: D\nB is correct but the answer in D is better.","upvote_count":"2","comment_id":"830209","poster":"Valcilio","timestamp":"1709665320.0"},{"upvote_count":"3","comment_id":"804850","poster":"AjoseO","timestamp":"1707606300.0","content":"Selected Answer: D\nD is preferred over C because the goal is to predict the sale price of houses, which is the target variable. By checking the correlation of each feature against the target variable, the machine learning specialist can identify which features are most relevant to the prediction of the sale price and which are less relevant. Removing features with low correlation to the target variable helps reduce the complexity of the model and potentially improve its accuracy.\n\nOn the other hand, a heatmap showing the correlation of the dataset against itself (C) doesn't directly address the relevance of the features to the target variable, and so it's not as effective in reducing the complexity of the model."},{"content":"Answer should be D, THIS is feature elimination /selection during feature Enginering. Choice c is so close just to confuse test takers to pick the wrong choice! See below C and D answers -- C should have been correct if the question asked about how to visualize correlation among independent variables! PROVIDED second sentence in C needs to be removed or to say which feature you will eliminate in such case then the one with low correlation against target out of those two.\n\nC. Build a heatmap showing the correlation of the dataset against itself. Remove features with low mutual correlation scores.\nD. Run a correlation check of all features against the target variable. Remove features with low target variable correlation scores.","poster":"expertguru","comment_id":"768890","upvote_count":"1","timestamp":"1704658140.0"},{"content":"Selected Answer: D\nThe multiple regression model is based on the following assumptions:\n\nThere is a linear relationship between the dependent variables and the independent variables\nThe independent variables are not too highly correlated with each other\nyi observations are selected independently and randomly from the population\nResiduals should be normally distributed with a mean of 0 and variance σ","upvote_count":"5","comment_id":"648163","poster":"Ob1KN0B","timestamp":"1692291840.0"},{"upvote_count":"3","content":"I think the answer is D.\nIf the model is a decision tree or something like that, I don't think it is possible to make a decision based only on the direct correlation with the target variable.\nBut in multiple linear regression, the only thing that matters is the relationship between the target variable and the feature variable.\nB, if the standard deviation is small but not zero, then we have information.","timestamp":"1676554140.0","poster":"wakuwaku","comment_id":"548620"},{"timestamp":"1675377060.0","content":"Selected Answer: B\nB is correct.","upvote_count":"2","comment_id":"539225","poster":"apprehensive_scar"},{"comment_id":"520687","upvote_count":"2","content":"To eliminate extraneous information. So, the answer is D.","poster":"Peasfull","timestamp":"1673332380.0"},{"upvote_count":"4","timestamp":"1671041580.0","comment_id":"501593","poster":"Asrivastava3","content":"Correct answer is D. The reason B is wrong because it is difficult to reason out why would you plot a histogram? Absolutely unnecessary step and distraction choice."},{"timestamp":"1669988280.0","poster":"[Removed]","content":"Selected Answer: B\nD is not the proper answer. Here is why:\nIt says that it is comparing with the target variable (dependent variable), which implies it is comparing the correlation between the dependent and independent variables. This type of comparison is usually done after a model is constructed in order to prevent assessing the predictive strength of the model. To compare the target label, the label you wish to predict, with the other variables before - is premature and will likely result in weakening your model.\n\nVariables with low variance has very less information and the inclusion of which will likely weaken the model performance.\n\nHence, B.","upvote_count":"4","comment_id":"492557"},{"content":"Answer is D. \nhttps://deep-r.medium.com/difference-between-variance-co-variance-and-correlation-ea0b7ddbaa1","upvote_count":"5","poster":"MikkyO","comment_id":"423419","timestamp":"1667435460.0"},{"upvote_count":"1","comments":[{"poster":"mahmoudai","comment_id":"433314","upvote_count":"5","content":"but is mentioned, \"Remove features with low mutual correlation scores.\" which is wrong you should drop features with high correlation scores. so Answer is D","timestamp":"1667630700.0"}],"poster":"Huy","timestamp":"1667091000.0","content":"Answer C. Heatmaps is used to visualize for correlation matrix https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec","comment_id":"403190"},{"timestamp":"1666337160.0","poster":"hero67","upvote_count":"1","comment_id":"397178","content":"The problem with correlation tasks is it capture linear relations only. So, I would go with B"},{"poster":"YJ4219","content":"I think the answer is D, because the correlation between each feature and the target, if this feature has low variance (as B suggests) this correlation calculation will be low and thus will be removed.\nIn short i think D is a more general and more accurate answer.","upvote_count":"4","comment_id":"391272","timestamp":"1665938100.0"},{"poster":"Vita_Rasta84444","comment_id":"324954","content":"It is D. We want to exclude the values with low predictive power. It is although more probable that variables with low variance have the lower predictive power, but it is not necessary so.","upvote_count":"3","timestamp":"1665494700.0"},{"content":"B is the correct answer. Variation is information. No variation = Constant.","upvote_count":"2","poster":"SophieSu","comments":[{"content":"The question is how small is considered small? 1%? 5%? 10%?","upvote_count":"5","poster":"Madwyn","timestamp":"1667622660.0","comment_id":"427041"}],"comment_id":"298750","timestamp":"1664558640.0"},{"upvote_count":"6","comment_id":"284213","poster":"cnethers","comments":[{"comment_id":"384613","poster":"jerto97","upvote_count":"1","content":"Consider feature A is uncorrelated, and B is uncorrelated with the output. BUT It might be that the correlation is with A + B. You cannot remove it like that, that's the point of multivariable regression","timestamp":"1665676140.0"},{"timestamp":"1665106800.0","comment_id":"318524","upvote_count":"4","content":"D says low correlation score against target variable. B is the answer","poster":"achiko"}],"timestamp":"1664200800.0","content":"If features are linearly dependent they should be removed as there is no benefit in having then, for that reason I choose option D"}],"question_id":362,"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/43930-exam-aws-certified-machine-learning-specialty-topic-1/","answer_ET":"D","answer_images":[],"unix_timestamp":1612364880,"answer_description":"","exam_id":26,"timestamp":"2021-02-03 16:08:00","topic":"1","isMC":true,"answers_community":["D (70%)","B (30%)"]},{"id":"aYp8gIRnHYM2bk9X0sx7","answers_community":["C (52%)","B (48%)"],"answer":"C","topic":"1","answer_images":[],"answer_ET":"C","question_id":363,"unix_timestamp":1612323360,"exam_id":26,"timestamp":"2021-02-03 04:36:00","discussion":[{"timestamp":"1632211020.0","content":"Due to straight angles, I would choose Decision tree. See https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py","comment_id":"282394","upvote_count":"24","poster":"[Removed]","comments":[{"comment_id":"372851","content":"From your link it is obvious that the best answer is still SVM with RBF kernel. In your link the SVM-RBF got 88% accuracy on the 'square-like' dataset whereas the Decision tree achieved only 80%. Answer is SVM with RBF kernel","comments":[{"upvote_count":"7","poster":"ttsun","timestamp":"1637714220.0","content":"note the data from sklearn link is shaped as a ball of mass not a square. the RBF kernel would be better but the question shows a square. Decision tree should be better fit for this problem.","comment_id":"485530"}],"poster":"MrCarter","upvote_count":"16","timestamp":"1634619840.0"}]},{"comments":[{"comment_id":"1006270","timestamp":"1694586900.0","poster":"robotgeek","content":"Your statement \"The black points will be identified as 8 different classes\" does not make a lot of sense because the leaf node in a tree with be 1 of 2 classes, not 8 different classes just because they are visually in one place or the other","upvote_count":"1"},{"poster":"Madwyn","content":"The tree works like this with this branch with 4 nodes:\nAge > 49? Y\nAge > 51? N\nTransaction > 28? Y\nTransaction > 31? N\nPositive\n\nCorrect answer is B.","upvote_count":"10","comment_id":"427042","timestamp":"1635720120.0"}],"content":"B - Decision tree - is not the best answer. If you use decision tree to do clustering, every time you need to partition the space into 2 parts. Hence you will split the space into 3*3. The red points in the center box and the black points will fall into the 8 boxes around it. The black points will be identified as 8 different classes.\n\nC is the correct answer. SVM with non-linear kernel is appropriate for non-linear clustering. Even if the shape is close to rectangular. SVM with non-linear kernel will be ale to approximate the rectangular boundary shape.","upvote_count":"18","poster":"SophieSu","comment_id":"298752","timestamp":"1632294360.0"},{"upvote_count":"1","content":"Selected Answer: B\nTip: When details are missing, assume ideal conditions, so assume no overfitting issues. Therefore is B is better than C. If there are overfitting issues or a posibility of overfitting then C is the right answer.","timestamp":"1743879240.0","poster":"nick3332","comment_id":"1549539"},{"content":"Selected Answer: B\nDecision tree makes more sense, this decision boundary isn't complex at all and there is no risk of overfitting, all the points are inside the square","comment_id":"1362262","poster":"2eb8df0","timestamp":"1740599580.0","upvote_count":"2"},{"timestamp":"1729498860.0","upvote_count":"2","content":"Selected Answer: C\nThis is because the RBF kernel can handle non-linear relationships between features, which is often necessary for complex classification tasks.","comment_id":"1300831","poster":"MultiCloudIronMan"},{"timestamp":"1727843940.0","upvote_count":"2","poster":"MJSY","content":"Selected Answer: C\nDecision Tree can treat the training data well but will have a risk of overfitting. the SVM with RBF kernel will be more robust.","comment_id":"1292206"},{"upvote_count":"1","comment_id":"1204795","content":"Selected Answer: B\nAs the positive cases can be interpreted and separated from non positive ones by decision tree easily. SVM would have made sense if the two classes were inseparable or had complex relationship in data.","poster":"rookiee1111","timestamp":"1714526760.0"},{"upvote_count":"2","poster":"vkbajoria","timestamp":"1712876640.0","comment_id":"1194042","content":"Selected Answer: C\nIt is C SVM with RBF Kernel can classify this image. For decision tree, it will be more difficult"},{"upvote_count":"2","content":"Selected Answer: C\nFrom the visual information provided, an SVM with an RBF kernel (Option C) would likely be the best choice because it can handle the circular class distribution. The RBF kernel is especially good at dealing with such scenarios where the boundary between classes is not linear.","timestamp":"1707443820.0","comment_id":"1145199","poster":"kyuhuck"},{"upvote_count":"2","comment_id":"1142454","poster":"Alice1234","content":"Answer C\n\nB. Decision Tree: Decision trees can capture non-linear patterns and are capable of splitting the feature space in complex ways. They can be very effective if the decision boundary is not linear, but they might also overfit if the decision boundary is too complex.\n\nC. SVM with RBF Kernel: An SVM with a radial basis function (RBF) kernel is designed to handle non-linear boundaries by mapping input features into higher-dimensional spaces where the classes are more likely to be separated by a hyperplane. Given the clustered nature of the classes in the image, an SVM with an RBF kernel would likely be able to separate the classes with a higher degree of accuracy.","timestamp":"1707243600.0"},{"content":"Selected Answer: C\nSVM-RBF is the correct solution","timestamp":"1704126780.0","poster":"praveenaws","upvote_count":"1","comment_id":"1111347"},{"content":"Support vector machine (SVM) with a radial basis function kernel would likely have the highest accuracy for this task because it can handle the non-linear separation required by the data.","timestamp":"1703977380.0","upvote_count":"1","poster":"Neet1983","comment_id":"1110115"},{"timestamp":"1701121200.0","comment_id":"1081967","upvote_count":"1","poster":"endeesa","content":"Selected Answer: C\nI will lean with C"},{"comment_id":"1074344","poster":"akgarg00","content":"Answer is B as Decision tree can attain 100% accuracy in this case.","timestamp":"1700348400.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1694777400.0","comment_id":"1008410","poster":"loict","content":"Selected Answer: C\nSVM with RBF and proper C and Gamma value can accomodate this square shape (https://vitalflux.com/svm-rbf-kernel-parameters-code-sample/)"},{"upvote_count":"1","content":"Selected Answer: C\nconfusing between SVN or devision tree. learning towards C","timestamp":"1693228260.0","poster":"Mickey321","comment_id":"992201"},{"poster":"rags1482","upvote_count":"2","timestamp":"1685160540.0","comment_id":"907738","content":"Answer C\nIn general, SVMs are a good choice for tasks where accuracy is critical, such as fraud detection and medical diagnosis. Decision trees are a good choice for tasks where interpretability is important, such as customer segmentation and product recommendation."},{"upvote_count":"3","timestamp":"1676069520.0","content":"Selected Answer: B\nA Decision tree produces a stepwise boundary that consists of rectilinear splits in the feature space that are perpendicular to the axes. The boundary is determined by the conditions specified in the tree.\n\nSupport Vector Machines (SVM) produce a non-linear boundary in the feature space that separates the classes. The boundary is defined as the maximum margin hyperplane, which is the line that maximally separates the classes while having the greatest margin between the classes. The boundary can be linear, polynomial, radial basis function (RBF) or other types of non-linear functions, depending on the choice of kernel and the configuration of the SVM.","poster":"AjoseO","comment_id":"804839"},{"comment_id":"790383","content":"Correct Answer is C","timestamp":"1674892080.0","poster":"CertArvind","upvote_count":"1"},{"poster":"ovokpus","upvote_count":"4","timestamp":"1656092580.0","comment_id":"621801","content":"Selected Answer: C\nSVM with RBF kernel is the best answer here. KNN is also a solution, but it is not one of the options"},{"upvote_count":"8","poster":"vetaal","content":"Selected Answer: B\nThis answer is decision tree due to the Square decision boundaries.","comment_id":"530830","timestamp":"1642973040.0"},{"comment_id":"372853","upvote_count":"4","timestamp":"1634825940.0","poster":"MrCarter","content":"Answer is definitely C."},{"poster":"gcpwhiz","comment_id":"341516","upvote_count":"5","content":"answer is decision tree. Decision tree produces square boundary, SVM with radial function produces a circular boundary.","timestamp":"1634498400.0"},{"poster":"Vita_Rasta84444","comment_id":"324958","comments":[{"comment_id":"494798","poster":"anttan","content":"but isnt SVM with radical function meant for non-linear?","timestamp":"1638755220.0","upvote_count":"1"}],"content":"It is B. We do not use decision trees for clustering but for prediction. Straight lines provides that decision tree is most precise, while SVM is better with linear cernels.","timestamp":"1634354880.0","upvote_count":"3"}],"choices":{"D":"Single perceptron with a Tanh activation function","A":"Linear support vector machine (SVM)","B":"Decision tree","C":"Support vector machine (SVM) with a radial basis function kernel"},"url":"https://www.examtopics.com/discussions/amazon/view/43870-exam-aws-certified-machine-learning-specialty-topic-1/","isMC":true,"question_text":"A company wants to classify user behavior as either fraudulent or normal. Based on internal research, a machine learning specialist will build a binary classifier based on two features: age of account, denoted by x, and transaction month, denoted by y. The class distributions are illustrated in the provided figure. The positive class is portrayed in red, while the negative class is portrayed in black.\n//IMG//\n\nWhich model would have the HIGHEST accuracy?","answer_description":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04145/0005400001.png"]},{"id":"fYefT25sJGbkfbvm3Que","question_images":[],"discussion":[{"content":"when looking at an overfitting issue :\nhttps://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html\n1. Simplifying The Model (reduce number of layers)\n2. Early Stopping\n3. Use Data Augmentation\n4. Use Regularization (L1 + L2)\n5. Use Dropouts\n\nSo looking at the options:\nB, D, F","timestamp":"1649873700.0","comment_id":"284225","upvote_count":"48","poster":"cnethers"},{"comment_id":"297062","upvote_count":"8","poster":"SophieSu","content":"BDF !!!","timestamp":"1650647160.0"},{"poster":"asthamishra","timestamp":"1721127660.0","content":"looking at last 100 questions many answers were wrong , thanks to the discussion forum to provide correct answer","comment_id":"1124220","upvote_count":"2"},{"upvote_count":"1","comment_id":"1044749","poster":"AmeeraM","content":"Selected Answer: BDF\nI would say BCD or BDF","timestamp":"1713253740.0"},{"poster":"Mickey321","upvote_count":"1","content":"Selected Answer: BDF\nAgree with BDF","comment_id":"992204","timestamp":"1709133180.0"},{"poster":"JK1977","comment_id":"904998","content":"Selected Answer: BDF\nOver fitting problem. All the options B, D, F reduce over fitting.","timestamp":"1700757360.0","upvote_count":"1"},{"timestamp":"1699776360.0","upvote_count":"1","content":"In what world is ACE the answer ?","poster":"Debayandt91","comment_id":"895683"},{"comment_id":"890728","timestamp":"1699279980.0","poster":"Dota_addict","content":"Selected Answer: BDF\nBDF is the answer","upvote_count":"1"},{"poster":"Aninina","content":"Selected Answer: BDF\nBDF is the correct","upvote_count":"1","timestamp":"1688309820.0","comment_id":"763862"},{"poster":"Peeking","content":"Selected Answer: BDF\nADE is absolutely wrong. 50 layers is already overfitting the model. We cannot increase the number of layers again.","comment_id":"740395","upvote_count":"1","timestamp":"1686332640.0"},{"content":"Selected Answer: BDF\nBDF is the correct answer","comment_id":"716830","poster":"GauravLahotiML","upvote_count":"1","timestamp":"1683904500.0"},{"content":"Selected Answer: BDF\nshould be BDF","timestamp":"1670209260.0","comment_id":"611642","upvote_count":"1","poster":"exam887"},{"upvote_count":"1","comment_id":"594102","poster":"NILKK","content":"One of the correct answer is showing as A. I wanted to understand how A(Choose Higher Number of Layers) is the correct Answer ?","timestamp":"1666998480.0"},{"poster":"KM226","comments":[{"content":"C might not be, because the model yielded 99% accuracy on the training set","poster":"windy9","timestamp":"1712069940.0","comment_id":"1023192","upvote_count":"2"}],"content":"Selected Answer: BCE\nI believe the answer is BCE because the model is overfitting.","timestamp":"1656092340.0","upvote_count":"1","comment_id":"508791"},{"upvote_count":"1","content":"choose smaller learning rate c, d, f,","comments":[{"content":"ignore answer is correct BDF","poster":"johnvik","upvote_count":"1","comment_id":"328236","timestamp":"1651235880.0"}],"poster":"johnvik","timestamp":"1650943020.0","comment_id":"328234"},{"timestamp":"1650815280.0","content":"BDF!!!","poster":"Vita_Rasta84444","comment_id":"324960","upvote_count":"3"},{"comment_id":"291718","timestamp":"1649922000.0","poster":"astonm13","upvote_count":"2","content":"It is supposed to be BDF"},{"comments":[{"poster":"[Removed]","comment_id":"291957","content":"I meant to say B, D and F. Ignore above.","upvote_count":"4","timestamp":"1650539220.0"}],"poster":"[Removed]","comment_id":"283177","upvote_count":"1","content":"I would choose B, D and E.","timestamp":"1649689020.0"},{"content":"is it BDF?","upvote_count":"1","comment_id":"283025","poster":"asdflkjh","timestamp":"1648488480.0"},{"comments":[{"poster":"Madwyn","comment_id":"427046","timestamp":"1651933020.0","content":"Yes, it's overfitting and smaller learning rate won't help.","upvote_count":"2"}],"upvote_count":"2","content":"Ans. is B,,C,D.\nBecause this problem is ovefitting.","poster":"takahirokoyama","comment_id":"283004","timestamp":"1648341180.0"},{"timestamp":"1647913500.0","upvote_count":"1","content":"Answers BDF","poster":"ahquiceno","comment_id":"282805"}],"url":"https://www.examtopics.com/discussions/amazon/view/43931-exam-aws-certified-machine-learning-specialty-topic-1/","answers_community":["BDF (89%)","11%"],"choices":{"F":"Enable early stopping","C":"Choose a smaller learning rate","E":"Include all the images from the test set in the training set","D":"Enable dropout","A":"Choose a higher number of layers","B":"Choose a lower number of layers"},"question_id":364,"topic":"1","isMC":true,"answer":"BDF","exam_id":26,"answer_ET":"BDF","question_text":"A health care company is planning to use neural networks to classify their X-ray images into normal and abnormal classes. The labeled data is divided into a training set of 1,000 images and a test set of 200 images. The initial training of a neural network model with 50 hidden layers yielded 99% accuracy on the training set, but only 55% accuracy on the test set.\nWhat changes should the Specialist consider to solve this issue? (Choose three.)","answer_images":[],"answer_description":"","unix_timestamp":1612365780,"timestamp":"2021-02-03 16:23:00"},{"id":"3kcoyATTsmuCaboCSKby","url":"https://www.examtopics.com/discussions/amazon/view/43932-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"timestamp":"1647817140.0","comment_id":"282806","upvote_count":"22","content":"Answer A.","poster":"ahquiceno"},{"poster":"eganilovic","comments":[{"content":"Appreciates your explanation. Cheers","poster":"StelSen","timestamp":"1649604240.0","upvote_count":"2","comment_id":"354716"}],"content":"The answer is Early Stopping. Stopp the training before accuracy start do decrease.","comment_id":"343870","timestamp":"1648917900.0","upvote_count":"8"},{"timestamp":"1723833120.0","poster":"AIWave","upvote_count":"1","content":"I will go with A\nEarly stopping is a powerful technique to prevent overfitting. It involves monitoring the model’s performance on a validation dataset during training.\nIf the validation loss starts increasing or plateaus, early stopping stops further training. This ensures that the model doesn’t overfit to the training data.\nBased on the graph, if the validation loss begins to stagnate or increase after a certain number of epochs, enabling early stopping could lead to better generalization.","comment_id":"1152218"},{"poster":"AmeeraM","comment_id":"1044756","timestamp":"1713254160.0","upvote_count":"1","content":"Selected Answer: A\nearly stopping before error increase"},{"timestamp":"1712906160.0","upvote_count":"1","poster":"seifskl","content":"Selected Answer: A\nEarly stopping","comment_id":"1041460"},{"upvote_count":"1","content":"Selected Answer: A\nEarly stopping","poster":"Mickey321","comment_id":"992210","timestamp":"1709133300.0"},{"timestamp":"1701497700.0","comment_id":"912518","upvote_count":"1","poster":"vbal","content":"A: stop the training process of a neural network before it reaches the maximum number of epochs or iterations; in this case stop close to 64 Epochs."},{"comment_id":"740396","poster":"Peeking","content":"Selected Answer: A\nEarly stopping and not increasing epochs.","upvote_count":"1","timestamp":"1686332820.0"},{"content":"Selected Answer: A\nAnswer is ”A”","upvote_count":"1","timestamp":"1679486400.0","poster":"ryuhei","comment_id":"675961"},{"upvote_count":"3","timestamp":"1651244820.0","poster":"chrisabc","comment_id":"383137","content":"Early Stopping can improve the model?"},{"upvote_count":"2","timestamp":"1648502760.0","poster":"Vita_Rasta84444","content":"A is the answer","comment_id":"324961"},{"content":"I would go for A","comment_id":"291719","upvote_count":"2","timestamp":"1648278360.0","poster":"astonm13"},{"poster":"[Removed]","content":"I would choose A.","comment_id":"283180","timestamp":"1647854340.0","upvote_count":"5"}],"answer_images":[],"question_id":365,"isMC":true,"answer":"A","answer_ET":"A","answers_community":["A (100%)"],"answer_description":"","question_images":["https://www.examtopics.com/assets/media/exam-media/04145/0005500004.png","https://www.examtopics.com/assets/media/exam-media/04145/0005600001.jpg"],"question_text":"This graph shows the training and validation loss against the epochs for a neural network.\nThe network being trained is as follows:\n✑ Two dense layers, one output neuron\n✑ 100 neurons in each layer\n✑ 100 epochs\nRandom initialization of weights\n//IMG//\n\n//IMG//\n\nWhich technique can be used to improve model performance in terms of accuracy in the validation set?","exam_id":26,"choices":{"B":"Random initialization of weights with appropriate seed","C":"Increasing the number of epochs","A":"Early stopping","D":"Adding another layer with the 100 neurons"},"topic":"1","unix_timestamp":1612365840,"timestamp":"2021-02-03 16:24:00"}],"exam":{"isMCOnly":false,"id":26,"isImplemented":true,"provider":"Amazon","numberOfQuestions":369,"lastUpdated":"11 Apr 2025","name":"AWS Certified Machine Learning - Specialty","isBeta":false},"currentPage":73},"__N_SSP":true}