{"pageProps":{"questions":[{"id":"WOA2fdv5B9T3cRd7trO1","question_id":391,"answers_community":["B (59%)","D (31%)","10%"],"unix_timestamp":1710824400,"question_images":[],"answer_ET":"B","discussion":[{"poster":"trap","comment_id":"1200302","timestamp":"1713805740.0","upvote_count":"9","content":"Correct: D\nThe option talks about LEAST operational complexity not LEAST operational overhead. Option B is quite complex"},{"content":"Selected Answer: B\nLEAST operational complexity, considering the report already is available in the bucket: B. After the initial setup, the process is fully automatic, which means the operational complexity involving separate actions by account managers isn't needed.","comments":[{"content":"\"Modify the S3 bucket policy to allow each member account to access its own prefix\". What happens when you have new accounts in the organization? :D","upvote_count":"1","comment_id":"1322658","poster":"mike5656","timestamp":"1733471940.0"}],"timestamp":"1710958380.0","upvote_count":"9","comment_id":"1178581","poster":"Dgix"},{"upvote_count":"1","comment_id":"1332126","content":"Selected Answer: B\nMultiple accounts since multiple business units has their own account. So, it is complex to do it in each member account rather than lambda solution in option B.","timestamp":"1735251000.0","poster":"SIJUTHOMASP"},{"content":"Selected Answer: D\nIt is easy to setup CUR. B works but unnecessarily complicated.","timestamp":"1733168580.0","upvote_count":"1","poster":"Spike2020","comment_id":"1321077"},{"content":"Selected Answer: B\nAfter the initial setup of the S3 event, Lambda function, and bucket policy modifications, the process becomes fully automatic, minimizing the ongoing operational complexity involving separate actions by account managers.","poster":"0b43291","comment_id":"1312889","timestamp":"1731722460.0","upvote_count":"1"},{"poster":"sashenka","timestamp":"1731290760.0","upvote_count":"1","comment_id":"1309781","content":"Selected Answer: D\nA Lambda-based solution for sharing Cost and Usage Reports, while powerful, introduces significant operational complexity due to the need to manage and maintain multiple AWS services and components. This includes Lambda functions, S3 events, S3 bucket policy, etc. The solution requires ongoing code maintenance, careful configuration management, and monitoring of multiple services, making it more complex than simpler alternatives like setting up individual CURs in member accounts. While it offers flexibility and automation capabilities, the added complexity might outweigh the benefits for basic cost-sharing requirements across AWS accounts.","comments":[{"timestamp":"1731291180.0","upvote_count":"1","poster":"sashenka","comment_id":"1309789","content":"Key Differences between Operational Complexity and Operational Overhead\nScope\nComplexity: Describes the system's inherent intricacy and difficulty level\nOverhead: Represents the actual cost and effort needed to keep the system running1\nMeasurement\nComplexity: Often measured in terms of system architecture and integration points\nOverhead: Measured in terms of time, money, and resource consumption13\nManagement\nComplexity: Managed through system design and architecture decisions\nOverhead: Managed through efficient processes, automation, and resource allocation"}]},{"timestamp":"1731288180.0","comment_id":"1309765","poster":"AzureDP900","content":"Configures an S3 event that triggers a Lambda function every time a new file arrives in the central Cost and Usage Report bucket.\nThe Lambda function extracts each member account's data from the central report.\nStores the extracted data under separate prefixes for each member account in Amazon S3.\nModifies the S3 bucket policy to grant access to each member account's prefix.\nBy automating this process, Option B minimizes operational complexity while ensuring that each member account has access to its own cost and usage data without requiring manual setup or maintenance.","upvote_count":"1"},{"content":"Already its mentioned the consolidated billing report is available in centralized bucket. Here if option D has to be chosen, then the Cost and Usage report have to be configured in individual accounts seperately again at individual accounts, which could add operational complexity, hence Option B seems to be right.","comment_id":"1303698","poster":"Danm86","upvote_count":"1","timestamp":"1730052120.0"},{"comment_id":"1296260","poster":"JoeTromundo","timestamp":"1728684960.0","content":"Selected Answer: B\nIn addition to what user Dgix commented, the fact that the S3 bucket must be in the account that creates the CUR does not make option B unfeasible. On the contrary, this option already assumes that the initial configuration of the bucket and the processing of the CUR report happen in the management account. Option B remains the recommended solution because it: Automates the data segmentation process. Ensures compliance with documentation by keeping the S3 bucket in the management account. Simplifies access control by using bucket policies to ensure that each account sees only its own data. Meets the requirement of lower operational complexity by centralizing the processing of the CUR. Therefore, even with the restriction that the S3 bucket must be in the management account, option B remains the best choice to meet the business requirements with the least operational effort.","upvote_count":"2"},{"poster":"asquared16","timestamp":"1724242500.0","comments":[{"poster":"asquared16","content":"\"Each business unit can have access to only its own cost and utilization data\"","comment_id":"1270089","upvote_count":"1","timestamp":"1724242740.0"}],"content":"Selected Answer: D\nB sounds like quite the adventure.","comment_id":"1270087","upvote_count":"2"},{"poster":"neta1o","upvote_count":"1","timestamp":"1723482600.0","comment_id":"1264737","content":"Selected Answer: D\nB would be very complex to parse the incoming files and separate by prefix. Then managing all the individual prefix shares. For that reason D seems like a better choice. Also the question mentions having the right permissions setup so they can configure their own CUR."},{"timestamp":"1720345140.0","content":"Answer: Option D\n\nFirst Reason: The Cost and Usage Report (CUR) cannot be set up for cross-account delivery. According to the AWS documentation, “The account that creates the Cost and Usage Report must also own the Amazon S3 bucket that AWS sends the reports to.” This means each account must set up its own S3 bucket to receive its respective CUR.\nhttps://docs.aws.amazon.com/cur/latest/userguide/cur-consolidated-billing.html\nSecond Reason: The question asks for the solution with the least operational complexity. Option D simplifies the process by allowing each account to independently manage its own CUR setup without requiring complex configurations or custom Lambda functions.","poster":"tqphuong","comment_id":"1243771","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: A\nAfter some investigation, I found A could be a suitable choice, however it lacks a few details\nBy using AWS RAM, you can share the S3 bucket (or specific prefixes within the bucket) containing the Cost and Usage Report with the member accounts.\nEach member account can set up Athena queries to access and analyze their own cost and utilization data from the shared S3 bucket. This approach ensures that each business unit can view its own data without accessing other units' data.\n\nB: too complicated\nC: Cost Explorer doesn't provide the raw cost and usage data that might be needed for detailed analysis with Athena.\nD: multiple Cost and Usage Reports, one for each account => out","poster":"trungtd","comments":[{"comment_id":"1358739","timestamp":"1739969220.0","upvote_count":"1","content":"Not true. You can only S3 on outpost","poster":"altonh"}],"comment_id":"1229542","timestamp":"1718233680.0"},{"comment_id":"1229541","upvote_count":"2","content":"Selected Answer: B\nThe question asks for LEAST operational complexity\nBut it seems that only the most complex option can solve the problem","timestamp":"1718233260.0","poster":"trungtd"},{"content":"Selected Answer: D\nWhy B? The question talk about LEAST operations. D for me","timestamp":"1715953080.0","comment_id":"1212913","poster":"red_panda","upvote_count":"4"},{"content":"Selected Answer: B\nThe most straightforward option","comment_id":"1182135","timestamp":"1711335840.0","upvote_count":"1","poster":"VerRi"},{"poster":"pangchn","upvote_count":"2","comment_id":"1181169","content":"B\nI don't like this type of question that shows the current AWS limit which need to use sneaky way, like lambda, to automate the process. This should be a potential new feature that AWS should improve in future since the billing and report is such a common scenrio as in the question.","timestamp":"1711230060.0"},{"comment_id":"1176995","content":"Selected Answer: B\nWith the Lambda to extract and separate each member account's cost and utilization data from the central Cost and Usage Report stored in the S3 bucket and S3 events to trigger the Lambda function, the process is automated and requires minimal ongoing management. Each member account can be given access only to its own prefix within the S3 bucket, ensuring that each business unit can only access its own cost data. Other options involve higher operational complexity and overhead.","upvote_count":"1","poster":"CMMC","timestamp":"1710824400.0"}],"answer_description":"","topic":"1","isMC":true,"timestamp":"2024-03-19 06:00:00","answer_images":[],"answer":"B","choices":{"C":"In each member account, access AWS Cost Explorer. Create a new report that contains relevant cost information for the account. Save the report in Cost Explorer. Provide instructions that the account administrators can use to access the saved report.","B":"In the organization's management account, configure an S3 event to invoke an AWS Lambda function each time a new file arrives in the S3 bucket that contains the central Cost and Usage Report. Configure the Lambda function to extract each member account’s data and to place the data in Amazon S3 under a separate prefix. Modify the S3 bucket policy to allow each member account to access its own prefix.","D":"In each member account, create a new S3 bucket to store Cost and Usage Report data. Set up a Cost and Usage Report to deliver the data to the new S3 bucket.","A":"In the organization's management account, use AWS Resource Access Manager (AWS RAM) to share the Cost and Usage Report data with each member account."},"exam_id":33,"question_text":"A company that has multiple business units is using AWS Organizations with all features enabled. The company has implemented an account structure in which each business unit has its own AWS account. Administrators in each AWS account need to view detailed cost and utilization data for their account by using Amazon Athena.\n\nEach business unit can have access to only its own cost and utilization data. The IAM policies that govern the ability to set up AWS Cost and Usage Reports are in place. A central Cost and Usage Report that contains all data for the organization is already available in an Amazon S3 bucket.\n\nWhich solution will meet these requirements with the LEAST operational complexity?","url":"https://www.examtopics.com/discussions/amazon/view/136559-exam-aws-certified-solutions-architect-professional-sap-c02/"},{"id":"n67A8NrJJF54XRN2doPP","answer_images":[],"unix_timestamp":1710824940,"question_id":392,"answer_description":"","discussion":[{"content":"Selected Answer: D\nMACsec is only supported on 10gbps and 100gbps Direct Connect \nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-mac-sec-getting-started.html","comment_id":"1180683","timestamp":"1711179000.0","poster":"oayoade","upvote_count":"12","comments":[{"upvote_count":"2","comments":[{"timestamp":"1725576240.0","poster":"Daniel76","content":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/MACsec.html","comment_id":"1279204","upvote_count":"1"}],"timestamp":"1725576240.0","poster":"Daniel76","content":"This URL was updated as it supports 400gbps. (it does not change the answer).","comment_id":"1279203"}]},{"comment_id":"1315658","content":"Why not C?\nAdding multiple VIFs to your Direct Connect connection is a cost-effective way to increase redundancy and improve performance.\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html#:~:text=Option%201%3A%20Create%20a%20private,allowing%20you%20to%20connect%20to","upvote_count":"1","timestamp":"1732166160.0","comments":[{"timestamp":"1733125740.0","comment_id":"1320828","content":"A single AWS Direct Connect connection with multiple private virtual interfaces (VIFs) does not provide redundancy, as all the VIFs share the same underlying physical connection.","poster":"nimbus_00","upvote_count":"1"}],"poster":"TomTom"},{"timestamp":"1717475580.0","poster":"trungtd","comment_id":"1223904","content":"Selected Answer: D\nmentioned by oayoade.","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nAnswer: D\nTo encrypt data over DX, you use MACsec for 10 Gbps and 100 Gbps links, and S2S VPN for slower links (e.g. 1 Gbps).\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html\nhttps://repost.aws/knowledge-center/create-vpn-direct-connect\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/adding-macsec-security-to-aws-direct-connect-connections/","timestamp":"1713939180.0","comment_id":"1201148","poster":"titi_r"},{"poster":"pangchn","content":"Selected Answer: D\nvote for D too","upvote_count":"1","timestamp":"1712450100.0","comment_id":"1190679"},{"upvote_count":"1","timestamp":"1712288100.0","poster":"ArunRav","comment_id":"1189640","content":"Selected Answer: D\nD as mentioned by oayoade."},{"content":"Selected Answer: D\nsame as oayosde mentioned,","upvote_count":"1","comment_id":"1184495","timestamp":"1711588260.0","poster":"zawminhtay.it.ucsm"},{"comment_id":"1183201","upvote_count":"2","poster":"joseribas89","timestamp":"1711448520.0","content":"Selected Answer: D\nas oayoade says we need at least 10gbps to use MACsec, so option D"},{"timestamp":"1711312920.0","poster":"pangchn","comment_id":"1181965","content":"Selected Answer: D\nD as mentioned by oayoade.","upvote_count":"1"},{"comment_id":"1179431","upvote_count":"2","poster":"k23319","content":"Selected Answer: A\nMACSec is the difference here for the additional security for Direct Connect.","timestamp":"1711039560.0"},{"poster":"ahmadraufsyahputra","content":"A because dynamic IP is more resilence than static IP","comment_id":"1178898","timestamp":"1710983460.0","upvote_count":"1"},{"comment_id":"1178647","content":"Selected Answer: A\nA is the correct answer.\nD uses static routing which is less suitable.","timestamp":"1710964080.0","poster":"Dgix","upvote_count":"1"},{"content":"Selected Answer: D\nWith A the VPN is dependent on the DX connection, so not adding any resilience. VPN is encrypted by default, D.","upvote_count":"2","timestamp":"1710927540.0","comment_id":"1178112","poster":"djangoUnchained"},{"timestamp":"1710830460.0","comment_id":"1177035","poster":"ovladan","upvote_count":"1","content":"Solution: A\nIf we look at the request \"MOST cost-effectively\" we can eliminate the answer under B.\nIf we look at this part of the requirement \"the solution is highly available, fault tolerant\" we can eliminate C.\nIf we look at this part \"The company has configured BGP for the connection\" and \"the solution is ... secure\" we can eliminate D, because the current Direct Connect connection is not encrypted and answer under D does not offer a solution to encrypt the traffic.\nBase on this answer under A is right choice."},{"timestamp":"1710824940.0","content":"Selected Answer: A\nProvide resilience for the Direct Connect connection. Configure MACsec to encrypt traffic inside the Direct Connect connection. More cost effective than the static Site-to-Site VPN in Option D (which does not have the MACsec encryption for additional security).","comment_id":"1176997","upvote_count":"3","poster":"CMMC"}],"timestamp":"2024-03-19 06:09:00","topic":"1","answer_ET":"D","choices":{"A":"Add a dynamic private IP AWS Site-to-Site VPN as a secondary path to secure data in transit and provide resilience for the Direct Connect connection. Configure MACsec to encrypt traffic inside the Direct Connect connection.","C":"Configure multiple private VIFs. Load balance data across the VIFs between the on-premises data center and AWS to provide resilience.","D":"Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection.","B":"Provision another Direct Connect connection between the company's on-premises data center and AWS to increase the transfer speed and provide resilience. Configure MACsec to encrypt traffic inside the Direct Connect connection."},"question_text":"A company is designing an AWS environment for a manufacturing application. The application has been successful with customers, and the application's user base has increased. The company has connected the AWS environment to the company's on-premises data center through a 1 Gbps AWS Direct Connect connection. The company has configured BGP for the connection.\n\nThe company must update the existing network connectivity solution to ensure that the solution is highly available, fault tolerant, and secure.\n\nWhich solution will meet these requirements MOST cost-effectively?","answers_community":["D (79%)","A (21%)"],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/136560-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"answer":"D","question_images":[]},{"id":"xQE6UQFlHKhSff4rr9fS","answer_images":[],"unix_timestamp":1710846120,"question_id":393,"answer_description":"","discussion":[{"content":"Selected Answer: B\nNo doubt","timestamp":"1727227500.0","comment_id":"1182155","upvote_count":"1","poster":"VerRi"},{"content":"Selected Answer: B\nB\n4GB in file size would be S3\nAmazon Keyspaces (for Apache Cassandra) is not relevant at all","upvote_count":"1","poster":"pangchn","comment_id":"1181968","timestamp":"1727203560.0"},{"comment_id":"1178648","timestamp":"1726854540.0","poster":"Dgix","content":"Selected Answer: B\nB is the correct answer.","upvote_count":"1"},{"upvote_count":"2","poster":"CMMC","timestamp":"1726736520.0","comment_id":"1177233","content":"Selected Answer: B\nStoring the videos as objects in S3 is scalable and cost-effective for storing large files. DynamoDB can store video metadata (including the S3 key), allowing for efficient retrieval and management of the videos."}],"topic":"1","timestamp":"2024-03-19 12:02:00","answer_ET":"B","choices":{"A":"Migrate the database to Amazon Aurora PostgreSQL by using AWS Database Migration Service (AWS DMS). Store the videos as base64-encoded strings in a TEXT column in the database.","C":"Migrate the database to Amazon Keyspaces (for Apache Cassandra) by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 object identifier in the corresponding Amazon Keyspaces entry.","B":"Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 key in the corresponding DynamoDB item.","D":"Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as base64-encoded strings in the corresponding DynamoDB item."},"question_text":"A company needs to modernize an application and migrate the application to AWS. The application stores user profile data as text in a single table in an on-premises MySQL database.\n\nAfter the modernization, users will use the application to upload video files that are up to 4 GB in size. Other users must be able to download the video files from the application. The company needs a video storage solution that provides rapid scaling. The solution must not affect application performance.\n\nWhich solution will meet these requirements?","isMC":true,"answers_community":["B (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/136587-exam-aws-certified-solutions-architect-professional-sap-c02/","exam_id":33,"answer":"B","question_images":[]},{"id":"Bb7I6CIYWfu1zs26Xf4x","question_text":"A company stores and manages documents in an Amazon Elastic File System (Amazon EFS) file system. The file system is encrypted with an AWS Key Management Service (AWS KMS) key. The file system is mounted to an Amazon EC2 instance that runs proprietary software.\n\nThe company has enabled automatic backups for the file system. The automatic backups use the AWS Backup default backup plan.\n\nA solutions architect must ensure that deleted documents can be recovered within an RPO of 100 minutes.\n\nWhich solution will meet these requirements?","unix_timestamp":1710846540,"isMC":true,"timestamp":"2024-03-19 12:09:00","topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/136589-exam-aws-certified-solutions-architect-professional-sap-c02/","discussion":[{"comment_id":"1182217","upvote_count":"7","timestamp":"1711342980.0","content":"Selected Answer: A\nThe default backup plan is once a day, which cannot meet the RPO, so C and D are out. \nWe need both EventBridge and Lambda functions to frequently backup the EFS, so B is out.","poster":"VerRi"},{"content":"Selected Answer: A\nhttps://community.aws/content/2iCkeS4XUmYdFf8Mlz6C7DFg5K3/protecting-amazon-s3-using-aws-backup","upvote_count":"1","timestamp":"1727866920.0","poster":"Syre","comment_id":"1292372"},{"timestamp":"1720759560.0","upvote_count":"4","poster":"053081f","comment_id":"1246499","content":"Selected Answer: A\nI checked the AWS Backup console and you cannot setup backup plan less than 1 hour, so 30 min backup(B) will be excluded."},{"comment_id":"1201202","content":"Selected Answer: A\nAnswer A.","upvote_count":"1","timestamp":"1713947400.0","poster":"titi_r"},{"comment_id":"1193644","poster":"Aesthet","content":"Selected Answer: A\nC is not supported, see here: https://docs.aws.amazon.com/aws-backup/latest/devguide/backup-feature-availability.html#features-by-resource\nB is not possible (minimum is 1 hour, according to https://aws.amazon.com/blogs/storage/automating-backups-and-optimizing-backup-costs-for-amazon-efs-using-aws-backup/#:~:text=cron%20expression%20that%20creates%20backups%20as%20frequently%20as%20hourly).\nSo I vote for A","upvote_count":"4","timestamp":"1712827800.0"},{"poster":"pangchn","timestamp":"1711314360.0","comments":[{"comment_id":"1297648","timestamp":"1728913920.0","upvote_count":"2","content":"A: I've tried it and it doesn't work, you get an error message \"\nError in some rules due to : The interval between backup jobs shouldn't be less than 60 minutes.\"","poster":"chris_spencer"}],"content":"Selected Answer: B\nB\nUsing the AWS Backup console, you can choose a frequency of every 12 hours, daily, weekly, or monthly. You can also create a cron expression that creates backups as frequently as hourly\nref:\nhttps://aws.amazon.com/blogs/storage/automating-backups-and-optimizing-backup-costs-for-amazon-efs-using-aws-backup/\n\nPITR is not supported for EFS mentioned by djangoUnchained, so C is out\nFrom AWS console, the most frequently backup is daily.","upvote_count":"2","comment_id":"1181977"},{"comment_id":"1180296","timestamp":"1711138860.0","content":"Answer C.","poster":"AWSPro1234","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: A\nFirst of all, using the existing default backup plan means backups only once a day, which disqualifies both C and D. We are thus left with A and B, which both fulfil the RPO. B is slightly more wasteful in that 30-minute backups are overkill. Also, B requires a custom cron task to be set up using EventBridge as it is a non-standard one for AWS Backup.\n\nA, however, can be accomplished without extra operational overhead. Therefore, A.","timestamp":"1710965460.0","poster":"Dgix","comment_id":"1178663"},{"comment_id":"1177237","comments":[{"upvote_count":"3","content":"It seems PITR is not supported for EFS https://docs.aws.amazon.com/aws-backup/latest/devguide/point-in-time-recovery.html","poster":"djangoUnchained","comment_id":"1178129","timestamp":"1710928200.0"}],"timestamp":"1710846540.0","poster":"CMMC","content":"Selected Answer: C\nCreating a new IAM role and updating the KMS key policy to allow the role to use the key ensures that the backup mechanism has the necessary permissions for encryption. Enabling continuous backups for point-in-time recovery to increases the likelihood of being able to recover deleted documents within the specified RPO of 100 minutes.","upvote_count":"1"}],"question_images":[],"answer_ET":"A","answer":"A","answers_community":["A (87%)","9%"],"exam_id":33,"question_id":394,"choices":{"C":"Create a new IAM role. Use the existing backup plan. Update the KMS key policy to allow the new IAM role to use the key. Enable continuous backups for point-in-time recovery.","A":"Create a new IAM role. Create a new backup plan. Use the new IAM role to create backups. Update the KMS key policy to allow the new IAM role to use the key. Implement an hourly backup schedule for the file system.","D":"Use the existing backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Enable Cross-Region Replication for the file system.","B":"Create a new backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Implement a custom cron expression to run a backup of the file system every 30 minutes."},"answer_images":[]},{"id":"2Jh2g52HYqYIHLVLNiUj","answers_community":["D (100%)"],"answer_images":[],"answer":"D","exam_id":33,"question_text":"A solutions architect must provide a secure way for a team of cloud engineers to use the AWS CLI to upload objects into an Amazon S3 bucket. Each cloud engineer has an IAM user, IAM access keys, and a virtual multi-factor authentication (MFA) device. The IAM users for the cloud engineers are in a group that is named S3-access. The cloud engineers must use MFA to perform any actions in Amazon S3.\n\nWhich solution will meet these requirements?","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/136593-exam-aws-certified-solutions-architect-professional-sap-c02/","unix_timestamp":1710847140,"timestamp":"2024-03-19 12:19:00","answer_description":"","isMC":true,"answer_ET":"D","choices":{"D":"Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3.","C":"Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Use IAM access keys with the AWS CLI to call Amazon S3.","A":"Attach a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM user performs actions on the S3 bucket. Use IAM access keys with the AWS CLI to call Amazon S3.","B":"Update the trust policy for the S3-access group to require principals to use MFA when principals assume the group. Use IAM access keys with the AWS CLI to call Amazon S3."},"question_id":395,"question_images":[],"discussion":[{"timestamp":"1711315200.0","poster":"pangchn","content":"Selected Answer: D\nD\nSTS seems to be the answer\nhttps://advancedweb.hu/aws-how-to-secure-access-keys-with-mfa/\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html","upvote_count":"5","comment_id":"1181984"},{"upvote_count":"1","content":"Selected Answer: D\nThe other options have limitations or do not fully meet the requirements:\nOption A (bucket policy with MFA prompt) does not enforce MFA for all S3 actions and may not work consistently with the AWS CLI.\n Option B (trust policy update for the group) does not enforce MFA for S3 actions specifically and may not work as intended with the AWS CLI.\n Option C (deny policy without temporary credentials) would require the cloud engineers to use their long-term IAM access keys, which is less secure and does not follow the principle of least privilege.\n\nBy using temporary credentials obtained from AWS STS with MFA enforcement and attaching them to a named profile in the AWS CLI, you can provide a secure way for the cloud engineers to perform S3 operations while ensuring that MFA is required for those actions.","comment_id":"1312910","timestamp":"1731724680.0","poster":"0b43291"},{"comment_id":"1309649","poster":"AzureDP900","content":"Option D uses IAM access keys with the AWS CLI and requests temporary credentials from AWS Security Token Service (AWS STS) that include MFA. This solution ensures that cloud engineers must use MFA when performing actions in Amazon S3 while also providing a secure way to use the AWS CLI.\nThis approach aligns with the requirements of using MFA for S3 actions, minimizing security risks, and ensuring compliance with organizational policies.","upvote_count":"1","timestamp":"1731273240.0"},{"upvote_count":"4","content":"Selected Answer: D\naccess keys with AWS CLI will just skip the MFA","timestamp":"1711343640.0","comment_id":"1182221","poster":"VerRi"},{"content":"Selected Answer: D\nD is the correct answer, as STS is required here.","timestamp":"1710965760.0","poster":"Dgix","upvote_count":"1","comment_id":"1178667"},{"poster":"CMMC","content":"Selected Answer: D\nA & C are incorrect - Using IAM access keys with the AWS CLI would bypass the requirement for MFA.\n\nNot B - MFA should be required for specific actions, not just when assuming a role or group.","upvote_count":"1","comment_id":"1177243","timestamp":"1710847140.0"}]}],"exam":{"isImplemented":true,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional SAP-C02","id":33,"isMCOnly":true,"numberOfQuestions":529,"provider":"Amazon","lastUpdated":"11 Apr 2025"},"currentPage":79},"__N_SSP":true}