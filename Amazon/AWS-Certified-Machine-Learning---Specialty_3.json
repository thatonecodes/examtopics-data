{"pageProps":{"questions":[{"id":"gxAOcbGyKJr1x5ZFpDc4","answer_images":[],"question_id":11,"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/44530-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A data scientist is developing a pipeline to ingest streaming web traffic data. The data scientist needs to implement a process to identify unusual web traffic patterns as part of the pipeline. The patterns will be used downstream for alerting and incident response. The data scientist has access to unlabeled historic data to use, if needed.\nThe solution needs to do the following:\n✑ Calculate an anomaly score for each web traffic entry.\nAdapt unusual event identification to changing web patterns over time.\n//IMG//\n\nWhich approach should the data scientist implement to meet these requirements?","topic":"1","answer_ET":"D","exam_id":26,"answer_description":"","answers_community":["D (100%)"],"timestamp":"2021-02-12 11:08:00","isMC":true,"unix_timestamp":1613124480,"choices":{"C":"Collect the streaming data using Amazon Kinesis Data Firehose. Map the delivery stream as an input source for Amazon Kinesis Data Analytics. Write a SQL query to run in real time against the streaming data with the k-Nearest Neighbors (kNN) SQL extension to calculate anomaly scores for each record using a tumbling window.","A":"Use historic web traffic data to train an anomaly detection model using the Amazon SageMaker Random Cut Forest (RCF) built-in model. Use an Amazon Kinesis Data Stream to process the incoming web traffic data. Attach a preprocessing AWS Lambda function to perform data enrichment by calling the RCF model to calculate the anomaly score for each record.","D":"Collect the streaming data using Amazon Kinesis Data Firehose. Map the delivery stream as an input source for Amazon Kinesis Data Analytics. Write a SQL query to run in real time against the streaming data with the Amazon Random Cut Forest (RCF) SQL extension to calculate anomaly scores for each record using a sliding window.","B":"Use historic web traffic data to train an anomaly detection model using the Amazon SageMaker built-in XGBoost model. Use an Amazon Kinesis Data Stream to process the incoming web traffic data. Attach a preprocessing AWS Lambda function to perform data enrichment by calling the XGBoost model to calculate the anomaly score for each record."},"question_images":["https://www.examtopics.com/assets/media/exam-media/04145/0006500003.png"],"discussion":[{"comment_id":"288793","poster":"jiadong","upvote_count":"24","timestamp":"1664067240.0","content":"I think the answer is D - RCF works together with Data Analytics, and sliding window helped on new information","comments":[{"comment_id":"299237","poster":"SophieSu","content":"better to say \"RCF is a built-in algorithm/function in Kinesis Data Analytics\"","upvote_count":"3","timestamp":"1664844480.0"}]},{"upvote_count":"4","poster":"Mickey321","timestamp":"1724672640.0","content":"Selected Answer: D\nD uses the built-in RCF algorithm, which is designed for anomaly detection on streaming data and can adapt to changing patterns over time.It does not require any training data or preprocessing steps, as the RCF algorithm can learn from the streaming data directly.\nIt uses a sliding window, which allows for continuous updating of the anomaly scores based on the most recent data points.\nIt leverages the Amazon Kinesis Data Analytics service, which provides a scalable and managed platform for running SQL queries on streaming data.\nOption A requires training an RCF model on historic data, which may not reflect the current web traffic patterns. It also adds complexity and latency by invoking a Lambda function for each record.","comment_id":"990693"},{"upvote_count":"1","comment_id":"989364","timestamp":"1724519940.0","poster":"Mickey321","content":"Selected Answer: D\nAnswer D"},{"content":"Selected Answer: D\nLetra B está descartada, pois trás um modelo supervisionado de classificação para um problema não supervisionado. Letra C trás outro modelo que não é recomendado também, em comparação ao RCF. A solução mais fácil de implementar e que atinge os critérios pedidos é a Letra D. Letra A está errada, pois usamos KDS para ingestão apenas.","poster":"kaike_reis","upvote_count":"1","timestamp":"1722683160.0","comment_id":"971032"},{"comment_id":"967856","upvote_count":"2","timestamp":"1722410520.0","poster":"ccpmad","content":"Selected Answer: D\nthe data scientist needs to identify unusual web traffic patterns in real-time and adapt to changing web patterns over time. Amazon Kinesis Data Analytics provides real-time analytics capabilities on streaming data. The Amazon Random Cut Forest (RCF) SQL extension is designed for anomaly detection in streaming data, which fits the requirement to calculate an anomaly score for each web traffic entry."},{"content":"Answer is D\n\"The algorithm starts developing the machine learning model using current records in the stream when you start the application. The algorithm does not use older records in the stream for machine learning, nor does it use statistics from previous executions of the application.\" https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html","timestamp":"1692023220.0","comment_id":"646776","upvote_count":"4","poster":"Sidekick"},{"poster":"apprehensive_scar","upvote_count":"2","content":"Selected Answer: D\nD it is. easy one","timestamp":"1675572600.0","comment_id":"540781"},{"timestamp":"1674800640.0","upvote_count":"3","poster":"vetaal","comment_id":"533551","content":"Selected Answer: D\nRCF is dynamic and adapts with time. D seems more appropriate."},{"comment_id":"519179","comments":[{"poster":"ZSun","content":"But, the question does not require using historical data. BTW, it only has unlabled historic data, and unlabled data is not really useful training a detection model.","comment_id":"875901","timestamp":"1713638220.0","upvote_count":"1"}],"content":"It is A. The only way to handle the historic data is using sagemaker and you can preprocess a data stream using a lambda.","timestamp":"1673119740.0","poster":"hess","upvote_count":"2"},{"comment_id":"431587","upvote_count":"1","poster":"AMEJack","timestamp":"1667630040.0","content":"Definitly D, Data Anaytics is using RCF, Using window for selecting data with SQL"},{"content":"One more reason to select D, not A, is there is no Lambda function to preprocess record in Kinesis Data Stream.","upvote_count":"1","timestamp":"1667487720.0","comments":[{"content":"That's not true: https://docs.aws.amazon.com/kinesisanalytics/latest/dev/lambda-preprocessing.html","poster":"DimLam","upvote_count":"1","timestamp":"1729074000.0","comment_id":"1044847"}],"comment_id":"401485","poster":"Huy"},{"content":"“ Adapt unusual event identification to changing web patterns over time.” -> option A does not satisfy this, only mentions build the model once","poster":"gbrnq","comment_id":"339346","timestamp":"1667412060.0","upvote_count":"2"},{"upvote_count":"4","poster":"randomnamer","content":"The data scientist has access to unlabeled historic data to use, if needed. D has no mention of this. Also, A says the lambda function provides data enrichment. For me it's A.","comment_id":"324643","timestamp":"1667350020.0"},{"comment_id":"302858","content":"A and D both seems to works. But A does not satisfy requirement 2, adapt to patterns over time. Since the model is only trained on old data. So D may be better.","timestamp":"1666260240.0","upvote_count":"3","poster":"seanLu"},{"timestamp":"1664739780.0","poster":"astonm13","upvote_count":"1","content":"It is definitely D","comment_id":"291748"}]},{"id":"9etrg9EvjIyG1wTV4SED","answers_community":["C (100%)"],"answer_ET":"C","choices":{"B":"Reinforcement learning using claim IDs and timestamps where the agent will identify how many claims in each category to expect from month to month.","C":"Forecasting using claim IDs and timestamps to identify how many claims in each category to expect from month to month.","A":"Classification month-to-month using supervised learning of the 200 categories based on claim contents.","D":"Classification with supervised learning of the categories for which partial information on claim contents is provided, and forecasting using claim IDs and timestamps for all other categories."},"question_text":"A Data Scientist received a set of insurance records, each consisting of a record ID, the final outcome among 200 categories, and the date of the final outcome.\nSome partial information on claim contents is also provided, but only for a few of the 200 categories. For each outcome category, there are hundreds of records distributed over the past 3 years. The Data Scientist wants to predict how many claims to expect in each category from month to month, a few months in advance.\nWhat type of machine learning model should be used?","timestamp":"2021-02-08 19:58:00","exam_id":26,"topic":"1","answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/44290-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":12,"answer_images":[],"isMC":true,"answer":"C","question_images":[],"unix_timestamp":1612810680,"discussion":[{"upvote_count":"23","poster":"JBX2010","comments":[{"poster":"abdohanfi","content":"he said for a few what about the unclassified many i think we need to make classification for the rest first as it will help us with forecasting later with month to month forecasting","timestamp":"1666221180.0","comment_id":"369256","upvote_count":"2"}],"timestamp":"1664447760.0","content":"I think it should be C as the final outcome among 200 categories is already know. No need to build a classification model. It's pure forecasting problem.","comment_id":"288551"},{"content":"C is my answer.\n\nNo need to do classification. Because you know whether the insurance has a claim or not\nin the dataset. The claim contents do not provide additional information.","comment_id":"299254","poster":"SophieSu","timestamp":"1664788860.0","upvote_count":"7"},{"content":"Selected Answer: C\nforcasting","timestamp":"1724676000.0","upvote_count":"2","comment_id":"990729","poster":"Mickey321"},{"upvote_count":"1","comment_id":"971036","poster":"kaike_reis","timestamp":"1722683400.0","content":"Selected Answer: C\nIt's pure forescasting problem."},{"content":"I would say no machine learning model needed at all. Just using count group by categories SQL is enough","poster":"Chelseajcole","upvote_count":"1","comment_id":"819749","timestamp":"1708722180.0"},{"timestamp":"1708003440.0","content":"FinalOutcome\n1\n2\n.\n200\n\n\nRecordID, FinalOutcome, Date, ClaimContents\n1\n2\n.\n100000\n\nNote: claim content has partial information, only for few of 200 categories \n\npredict how many claims to expect in each category from month to month, a few months in advance\nWe dont need the claim contents, we have all we need from first 3 columns to train a forecast model\n\nc","upvote_count":"1","poster":"drcok87","comment_id":"809579"},{"timestamp":"1707664440.0","upvote_count":"4","poster":"AjoseO","content":"Selected Answer: C\nForecasting using claim IDs and timestamps to identify how many claims in each category to expect from month to month.\n\nThe problem requires the prediction of the number of claims in each category for each month, which is a time series forecasting problem. The timestamps and record IDs can be used to model the underlying patterns in the data, and the model can be trained to predict the number of claims in each category for future months based on these patterns.\n\nWhile the claim contents might provide additional information, the fact that partial information is only available for a few categories suggests that this information might not be enough to build a robust model, and that it might not be possible to apply supervised learning to all 200 categories. Instead, the model should be trained on the time series data (claim IDs and timestamps) for all categories, and the claim contents can be used to improve the accuracy of the model only for the categories for which such information is available.","comment_id":"805335"},{"content":"how can a forecasting/classification model can be based on the claim ID? (that should be unique)","timestamp":"1704208800.0","comment_id":"763807","poster":"informatica","upvote_count":"1"},{"poster":"matteocal","timestamp":"1690478940.0","comment_id":"638253","content":"Selected Answer: C\nit's a forecasting problem, not a classification one","upvote_count":"2"},{"content":"predict how many claims to expect in each category from month to month, a few months in advance\nC is the only one mentioning forecasting","upvote_count":"2","comment_id":"435815","poster":"Dr_Kiko","timestamp":"1667105820.0"},{"upvote_count":"5","comment_id":"404960","content":"D is correct. Multi-label classification to impute the missing claim contents, then forecasting what we want. C is missing the imputation part.","comments":[{"timestamp":"1687172760.0","content":"The question is, can we get something useful out of the handful of 200s and will this impact the forecast as we could forecast the numbers without...","comment_id":"618647","upvote_count":"1","poster":"f4bi4n"}],"timestamp":"1666973880.0","poster":"kezzzzz"},{"upvote_count":"2","content":"It is true that the final outcome is known. But C does not use the partial information from the 200 categories. Reinforcement learning currently is state of the art in stock prediction and other time series. Why waste valuable information? For me it's B.","comment_id":"324645","timestamp":"1665353220.0","poster":"randomnamer"},{"comment_id":"286389","upvote_count":"2","poster":"cnethers","content":"This is a supervised learning approach:\nSupervised learning problems can be further grouped into regression and classification problems.\n\n Classification: A classification problem is when the output variable is a category, such as “red” and “blue” or “disease” and “no disease.”\n Regression: A regression problem is when the output variable is a real value, such as “dollars” or “weight.”","timestamp":"1664289780.0"}]},{"id":"tCcqXZgdSWjlzJxaqzNX","answers_community":["C (88%)","13%"],"question_images":[],"question_id":13,"url":"https://www.examtopics.com/discussions/amazon/view/8304-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A Machine Learning Specialist receives customer data for an online shopping website. The data includes demographics, past visits, and locality information. The\nSpecialist must develop a machine learning approach to identify the customer shopping patterns, preferences, and trends to enhance the website for better service and smart recommendations.\nWhich solution should the Specialist recommend?","answer":"C","discussion":[{"comment_id":"34982","upvote_count":"21","content":"answer should be C\nCollaborative filtering is for recommendation, LDA is for topic modeling","poster":"WWODIN","timestamp":"1633266480.0"},{"poster":"syu31svc","content":"In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n\nAmazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set\n\nNeural network is used for image detection\n\nAnswer is C","comment_id":"170707","timestamp":"1635185580.0","upvote_count":"11"},{"upvote_count":"1","timestamp":"1739654640.0","poster":"JonSno","comment_id":"1357056","content":"Selected Answer: C\nCollab filtering it is..\nCollaborative filtering is the most widely used approach for recommendation systems.\nIt uses customer interactions (purchases, clicks, ratings) to determine preferences based on similar users or items.\nImplicit collaborative filtering (based on user behavior) and explicit collaborative filtering (based on ratings) can effectively personalize recommendations."},{"upvote_count":"2","timestamp":"1727164200.0","comment_id":"1006322","content":"Selected Answer: C\nA. NO - LDA is for topic modeling\nB. NO - NN is a too generic term, you want Neural Collaborative\nC. YES - Collaborative filtering best fit\nD. NO - Random Cut Forest (RCF) for anomalities","poster":"loict"},{"timestamp":"1727164200.0","upvote_count":"1","poster":"Mickey321","content":"Selected Answer: C\nCollaborative filtering is a machine learning technique that recommends products or services to users based on the ratings or preferences of other users. This technique is well-suited for identifying customer shopping patterns and preferences because it takes into account the interactions between users and products.","comment_id":"973051"},{"timestamp":"1710861780.0","poster":"killermouse0","upvote_count":"1","comment_id":"1177488","content":"Selected Answer: A\nFrom the doc: \"You can use LDA for a variety of tasks, from clustering customers based on product purchases to automatic harmonic analysis in music.\" \n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/lda-how-it-works.html"},{"upvote_count":"1","timestamp":"1690207500.0","content":"Selected Answer: C\nI think it should be c","comment_id":"961677","poster":"Venkatesh_Babu"},{"comment_id":"832670","poster":"Valcilio","content":"Selected Answer: C\nC, always when talk about recommendation you can think about collaborative patterns!","timestamp":"1678263240.0","upvote_count":"2"},{"comment_id":"830533","poster":"stjokerli","content":"A\nLDA used before collaborative filtering is largely adopted.\n1) the input data that we have doesn't lend itself to collaborative filtering - it requires a set of items and a set of users who have reacted to some of the items, which is NOT what we have\n2) recommendation is just one thing that we want to do. What about trends?\n3) collaborative filtering isn't one of the pre-built algorithms (weak argument, admittedly)","upvote_count":"2","timestamp":"1678074900.0"},{"content":"collaborative","upvote_count":"1","poster":"Shailendraa","comment_id":"667396","timestamp":"1663010220.0"},{"content":"C. Easy question.","comment_id":"539227","poster":"apprehensive_scar","upvote_count":"1","timestamp":"1643841180.0"},{"content":"its a appropriate use case of Collaborative filtering","comment_id":"439516","poster":"technoguy","upvote_count":"1","timestamp":"1635267360.0"},{"timestamp":"1634126100.0","comment_id":"98676","content":"this is C","poster":"roytruong","upvote_count":"1"},{"upvote_count":"6","content":"I'm thinking that it is A because:\n1) the input data that we have doesn't lend itself to collaborative filtering - it requires a set of items and a set of users who have reacted to some of the items, which is NOT what we have\n2) recommendation is just one thing that we want to do. What about trends? \n3) collaborative filtering isn't one of the pre-built algorithms (weak argument, admittedly)","timestamp":"1633645200.0","poster":"sdsfsdsf","comment_id":"63907"},{"comments":[{"poster":"cybe001","timestamp":"1633566900.0","upvote_count":"4","content":"Collaborative filtering is appropriate","comment_id":"40712"}],"content":"Answer is C, demographics, past visits, and locality information data, LDA is appropriate","upvote_count":"3","poster":"cybe001","comment_id":"37759","timestamp":"1633429620.0"},{"content":"Answer A might be more suitable than other\n\nhttps://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/lda-how-it-works.html","upvote_count":"4","timestamp":"1632561180.0","comments":[{"content":"Not convinced with A. Answer C seems to be a better fit than A for recommendation model (LDA appears to be a topic-based model on unavailable data with similar patterns)\nhttps://aws.amazon.com/blogs/machine-learning/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations/","comment_id":"28027","poster":"rsimham","timestamp":"1632978060.0","upvote_count":"10"}],"comment_id":"21885","poster":"DonaldCMLIN"}],"topic":"1","choices":{"A":"Latent Dirichlet Allocation (LDA) for the given collection of discrete data to identify patterns in the customer database.","B":"A neural network with a minimum of three layers and random initial weights to identify patterns in the customer database.","C":"Collaborative filtering based on user interactions and correlations to identify patterns in the customer database.","D":"Random Cut Forest (RCF) over random subsamples to identify patterns in the customer database."},"isMC":true,"answer_images":[],"timestamp":"2019-11-16 03:13:00","answer_description":"","unix_timestamp":1573870380,"answer_ET":"C","exam_id":26},{"id":"xZo5JBuN7orvMEBxGORM","answer_images":[],"question_text":"A company that promotes healthy sleep patterns by providing cloud-connected devices currently hosts a sleep tracking application on AWS. The application collects device usage information from device users. The company's Data Science team is building a machine learning model to predict if and when a user will stop utilizing the company's devices. Predictions from this model are used by a downstream application that determines the best approach for contacting users.\nThe Data Science team is building multiple versions of the machine learning model to evaluate each version against the company's business goals. To measure long-term effectiveness, the team wants to run multiple versions of the model in parallel for long periods of time, with the ability to control the portion of inferences served by the models.\nWhich solution satisfies these requirements with MINIMAL effort?","url":"https://www.examtopics.com/discussions/amazon/view/44917-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"B":"Build and host multiple models in Amazon SageMaker. Create an Amazon SageMaker endpoint configuration with multiple production variants. Programmatically control the portion of the inferences served by the multiple models by updating the endpoint configuration.","D":"Build and host multiple models in Amazon SageMaker. Create a single endpoint that accesses multiple models. Use Amazon SageMaker batch transform to control invoking the different models through the single endpoint.","A":"Build and host multiple models in Amazon SageMaker. Create multiple Amazon SageMaker endpoints, one for each model. Programmatically control invoking different models for inference at the application layer.","C":"Build and host multiple models in Amazon SageMaker Neo to take into account different types of medical devices. Programmatically control which model is invoked for inference based on the medical device type."},"exam_id":26,"answers_community":["B (89%)","11%"],"discussion":[{"timestamp":"1650826860.0","poster":"SophieSu","upvote_count":"43","content":"B is the correct answer. \n\nA/B testing with Amazon SageMaker is required in the Exam.\n\nIn A/B testing, you test different variants of your models and compare how each variant performs. \n\nAmazon SageMaker enables you to test multiple models or model versions behind the `same endpoint` using `production variants`.\n\nEach production variant identifies a machine learning (ML) model and the resources deployed for hosting the model. \n\nTo test multiple models by `distributing traffic` between them, specify the `percentage of the traffic` that gets routed to each model by specifying the `weight` for each `production variant` in the endpoint configuration.","comment_id":"299270"},{"poster":"[Removed]","timestamp":"1650206520.0","upvote_count":"10","comment_id":"291978","content":"I would answer B, it seems similar to this AWS example: https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html#model-testing-target-variant"},{"poster":"Mickey321","comment_id":"989398","upvote_count":"1","timestamp":"1708806060.0","content":"Selected Answer: B\nOption B"},{"timestamp":"1691762040.0","poster":"AjoseO","content":"Selected Answer: B\nThis solution allows the Data Science team to build and host multiple models in Amazon SageMaker, which is a fully managed service for training, deploying, and managing machine learning models. \n\nThe team can then create an endpoint configuration with multiple production variants, which are different versions of the models. By programmatically updating the endpoint configuration, the team can control the portion of inferences served by the different models. \n\nThis allows them to evaluate the models against their business goals and measure their long-term effectiveness without having to make changes at the application layer.","upvote_count":"3","comment_id":"805364"},{"comment_id":"632804","comments":[{"content":"How can you create a single endpoint for batch transforms? this answer is nonsensical.","timestamp":"1696415880.0","comments":[{"content":"It is possible to create a single endpoint for AWS Batch transforms. Here are the key steps:\n\nCreate an interface endpoint for AWS Batch in your VPC using the AWS CLI or console. The endpoint service name will be in the format of \ncom.amazonaws.<region>.batch\n.\n\nWhen creating the endpoint, assign an IAM role with necessary permissions to make calls to the Batch API.\n\nYou can then submit batch transform jobs to AWS Batch referencing resources in both public and private subnets of the VPC. The endpoint ensures private connectivity to Batch.\n\nThe single endpoint allows chaining multiple transforms together in a pipeline efficiently without needing internet access. New transforms can be added without redeploying the endpoint.\n\nAWS Batch will automatically provision the required compute environments like EC2 instances or containers to run the transforms and scale as needed based on job requirements.","poster":"VR10","timestamp":"1724240340.0","upvote_count":"1","comment_id":"1155557"}],"poster":"cpal012","comment_id":"860901","upvote_count":"1"}],"content":"Selected Answer: D\nAnswer D as it is said “the team intends to run numerous versions in parallel for extended periods of time,” so batch transform","upvote_count":"1","poster":"Morsa","timestamp":"1674019740.0"},{"content":"it says,\"host a sleep monitoring application\", it is the host which means online, not batch, b is correct","upvote_count":"1","timestamp":"1671238500.0","comment_id":"617430","poster":"[Removed]"},{"upvote_count":"4","comment_id":"551654","timestamp":"1660978560.0","content":"Selected Answer: B\nThe possibility to alter the percentage of inferences supplied by the models.\nWhich method achieves these criteria with the LEAST amount of effort?","poster":"John_Pongthorn"},{"poster":"apprehensive_scar","content":"B. Easy","comment_id":"540169","timestamp":"1659576660.0","upvote_count":"2"},{"comment_id":"494951","comments":[{"comment_id":"497822","poster":"[Removed]","content":"The question talks about the LEAST amount of effort. In this case, there will be as many transform jobs required to be built as there are variants. That may not be the least amount of effort.","upvote_count":"2","timestamp":"1654779960.0"}],"poster":"anttan","content":"Think anser is D, below is from the Sagemaker doc.\n\n\"https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html\"\nUse Batch Transform to Test Production Variants\nTo test different models or various hyperparameter settings, create a separate transform job for each new model variant and use a validation dataset. For each transform job, specify a unique model name and location in Amazon S3 for the output file. To analyze the results, use Inference Pipeline Logs and Metrics.","timestamp":"1654490940.0","upvote_count":"4"}],"question_images":[],"isMC":true,"answer_ET":"B","topic":"1","question_id":14,"unix_timestamp":1613498580,"answer_description":"","timestamp":"2021-02-16 19:03:00","answer":"B"},{"id":"oBmceZtNkk2IUAtWPViq","answer_images":[],"question_text":"An agricultural company is interested in using machine learning to detect specific types of weeds in a 100-acre grassland field. Currently, the company uses tractor-mounted cameras to capture multiple images of the field as 10 ֳ— 10 grids. The company also has a large training dataset that consists of annotated images of popular weed classes like broadleaf and non-broadleaf docks.\nThe company wants to build a weed detection model that will detect specific types of weeds and the location of each type within the field. Once the model is ready, it will be hosted on Amazon SageMaker endpoints. The model will perform real-time inferencing using the images captured by the cameras.\nWhich approach should a Machine Learning Specialist take to obtain accurate predictions?","url":"https://www.examtopics.com/discussions/amazon/view/44063-exam-aws-certified-machine-learning-specialty-topic-1/","choices":{"C":"Prepare the images in RecordIO format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the model using an object- detection single-shot multibox detector (SSD) algorithm.","D":"Prepare the images in Apache Parquet format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the model using an image classification algorithm to categorize images into various weed classes.","B":"Prepare the images in Apache Parquet format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the model using an object- detection single-shot multibox detector (SSD) algorithm.","A":"Prepare the images in RecordIO format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the model using an image classification algorithm to categorize images into various weed classes."},"exam_id":26,"answers_community":["C (100%)"],"discussion":[{"content":"C is my answer.\nPay attention that the question is asking for 2 things:\n1. detect specific types of weeds\n2. detect the location of each type within the field.\n\nImage Classification can only classify images.\nObject detection algorithm:\n1.identifies all instances of objects within the image scene.\n2.its location and scale in the image are indicated by a rectangular bounding box.\n\nData format for Computer Vision algorithms in SageMaker:\nRecommend to use RecordIO.","poster":"SophieSu","comment_id":"299284","upvote_count":"33","timestamp":"1634424540.0"},{"content":"Selected Answer: C\nRecordIO Format: This format is efficient for storing and processing large datasets, which is beneficial for training deep learning models.\nObject Detection SSD Algorithm: This algorithm is designed to detect and locate multiple objects within an image, making it ideal for identifying and pinpointing various types of weeds in the field","upvote_count":"1","poster":"MultiCloudIronMan","timestamp":"1727176080.0","comment_id":"1288555"},{"timestamp":"1692901920.0","upvote_count":"1","poster":"Mickey321","content":"Selected Answer: C\nRecord IO preffered and also object detection due to several types of weeds","comment_id":"989403"},{"timestamp":"1676219820.0","upvote_count":"3","poster":"AjoseO","content":"Selected Answer: C\nThe goal is to detect specific types of weeds and their locations within a field, which is a task that requires object detection, rather than image classification. Object detection algorithms are designed to identify objects and their locations within an image, whereas image classification algorithms only categorize an entire image into various classes.\n\nSingle-shot multibox detectors (SSD) are a type of object detection algorithm that are well-suited for real-time inferencing and have been shown to be effective for a variety of object detection tasks.\n\nBy preparing the images in RecordIO format and using Amazon SageMaker, the company can easily train, test, and validate the model, making it easier to deploy the model in a scalable and secure environment.","comment_id":"806562"},{"content":"C is the right answer. you need to detect location","upvote_count":"3","comment_id":"540584","timestamp":"1644000300.0","poster":"apprehensive_scar"},{"comment_id":"371835","content":"C\nYou can detect the type of weeds and the location within the field.","poster":"AShahine21","timestamp":"1635638160.0","upvote_count":"1"},{"content":"If they had an answer with \"Faster R-CNN\" then it would be different.\nThis is a good article talking about SSD, Faster R-CNN, R-FCN and others which is a good read.\nhttps://jonathan-hui.medium.com/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359","poster":"cnethers","comment_id":"286594","upvote_count":"2","timestamp":"1633925460.0"},{"upvote_count":"2","comment_id":"286427","timestamp":"1632923100.0","poster":"cnethers","content":"I would go with answer C ..\nSSD are new architectures faster than the old CNN\nhttps://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab"},{"timestamp":"1632589980.0","upvote_count":"3","comments":[{"upvote_count":"1","comments":[{"comments":[{"timestamp":"1680605040.0","comment_id":"860903","poster":"cpal012","content":"So you expect precisely one weed per grid (total 100) of an entire field? If a field is a hectare, then each grid would be 100m2","upvote_count":"1"}],"upvote_count":"1","content":"since field is divided in to 10x10 grid I felt A is more suitable.","poster":"attaraya","comment_id":"486573","timestamp":"1637834520.0"}],"timestamp":"1634011260.0","comment_id":"293278","poster":"crispogioele","content":"I think it's better go with C, since the question also ask for the location of the weed on the field, while the example you posted is just a classifier."},{"poster":"Bala1212081","content":"The llink says clearly only classification and not talking about object detection. Answer should be C","timestamp":"1635420540.0","comment_id":"367069","upvote_count":"1"}],"poster":"[Removed]","content":"I would select answer A, situation is very similar to this one: https://aws.amazon.com/blogs/machine-learning/building-a-lawn-monitor-and-weed-detection-solution-with-aws-machine-learning-and-iot-services/","comment_id":"284280"}],"question_images":[],"isMC":true,"answer_ET":"C","topic":"1","question_id":15,"unix_timestamp":1612544160,"answer_description":"","timestamp":"2021-02-05 17:56:00","answer":"C"}],"exam":{"id":26,"isMCOnly":false,"isBeta":false,"numberOfQuestions":369,"provider":"Amazon","isImplemented":true,"name":"AWS Certified Machine Learning - Specialty","lastUpdated":"11 Apr 2025"},"currentPage":3},"__N_SSP":true}