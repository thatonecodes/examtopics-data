{"pageProps":{"questions":[{"id":"OdLy7ynMUAhdyXYqZkXM","answers_community":["BDE (67%)","CDE (22%)","11%"],"unix_timestamp":1650702360,"answer_images":[],"choices":{"E":"Grant permissions in Lake Formation to allow the Amazon Redshift Spectrum role to access the three promotion columns of the advertising table.","D":"Create an external schema in Amazon Redshift by using the Amazon Redshift Spectrum IAM role. Grant usage to the marketing Amazon Redshift user.","F":"Grant permissions in Lake Formation to allow the marketing IAM group to access the three promotion columns of the advertising table.","A":"Grant permissions in Amazon Redshift to allow the marketing Amazon Redshift user to access the three promotion columns of the advertising external table.","B":"Create an Amazon Redshift Spectrum IAM role with permissions for Lake Formation. Attach it to the Amazon Redshift cluster.","C":"Create an Amazon Redshift Spectrum IAM role with permissions for the marketing S3 bucket. Attach it to the Amazon Redshift cluster."},"exam_id":20,"question_images":[],"answer_description":"","answer":"BDE","url":"https://www.examtopics.com/discussions/amazon/view/74206-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"timestamp":"2022-04-23 10:26:00","question_id":41,"topic":"1","discussion":[{"comment_id":"602994","poster":"chp2022","content":"Selected Answer: BDE\nBDE makes the most sense to me","timestamp":"1652806560.0","upvote_count":"6","comments":[{"comment_id":"801643","comments":[{"poster":"liar_p","comment_id":"852699","comments":[{"timestamp":"1691335200.0","poster":"MLCL","upvote_count":"1","content":"Lake Formation handles the permissions to S3 once the buckets are registered in the datalake.","comment_id":"973964"}],"timestamp":"1679971320.0","upvote_count":"1","content":"But option B doesn't mention anything about the permission to S3."}],"content":"Lake Formation provides the security and governance of the Data Catalog. Within Lake Formation, you can grant and revoke permissions to the Data Catalog objects, such as databases, tables, columns, and underlying Amazon S3 storage. hence B over C.","timestamp":"1675828980.0","poster":"Merrick","upvote_count":"4"}]},{"content":"Selected Answer: CDF\nI'll go with C, D, F\nCreate an IAM role with the necessary permissions to access the S3 bucket and assume this role in Amazon Redshift.** This role should have permissions to access the specific S3 bucket where the Parquet files are stored. You can then associate this role with Amazon Redshift, allowing Redshift to access the data in the S3 bucket.\nCreate an external schema in Amazon Redshift pointing to the data lake.** This allows Redshift to access the data in the data lake. You can use the `CREATE EXTERNAL SCHEMA` SQL command in Redshift to do this.\nGrant the necessary permissions in AWS Lake Formation.** You need to grant the marketing data analysts the necessary permissions to the specific columns in the table. You can do this in the AWS Lake Formation console or using the AWS CLI.","upvote_count":"1","timestamp":"1703235540.0","poster":"teo2157","comment_id":"1103258"},{"timestamp":"1682967240.0","content":"BDE: I passed the test","poster":"pk349","upvote_count":"3","comment_id":"886631"},{"poster":"CleverMonkey092","timestamp":"1680420540.0","content":"I think it is BDE\nLake formation will be used to grant access for redshift spectrum to the s3 bucket","comment_id":"858598","upvote_count":"2"},{"content":"To meet the requirements, the combination of the following steps should be taken:\nC. Create an Amazon Redshift Spectrum IAM role with permissions for the marketing S3 bucket. Attach it to the Amazon Redshift cluster.\nD. Create an external schema in Amazon Redshift by using the Amazon Redshift Spectrum IAM role. Grant usage to the marketing Amazon Redshift user.\nE. Grant permissions in Lake Formation to allow the Amazon Redshift Spectrum role to access the three promotion columns of the advertising table.\n\nThese steps would grant the necessary permissions for the marketing Amazon Redshift user to access the data stored in the marketing S3 bucket through Amazon Redshift Spectrum, while limiting access to only the three promotion columns of the advertising table. The Amazon Redshift Spectrum IAM role with the necessary permissions for the marketing S3 bucket would be attached to the Amazon Redshift cluster, and then an external schema would be created in Amazon Redshift using that role. Finally, the necessary permissions would be granted in Lake Formation to allow the Amazon Redshift Spectrum role to access the three promotion columns.","upvote_count":"3","poster":"Arjun777","comment_id":"807845","timestamp":"1676325780.0"},{"comment_id":"799425","upvote_count":"2","poster":"Gabba","timestamp":"1675659180.0","content":"Selected Answer: CDE\nRole access to S3 buckets are required hence C over B. \nD & E anyways are correct."},{"content":"Agreed BDE","upvote_count":"2","timestamp":"1650702360.0","poster":"CHRIS12722222","comment_id":"590480"}],"question_text":"A company is providing analytics services to its sales and marketing departments. The departments can access the data only through their business intelligence\n(BI) tools, which run queries on Amazon Redshift using an Amazon Redshift internal user to connect. Each department is assigned a user in the Amazon Redshift database with the permissions needed for that department. The marketing data analysts must be granted direct access to the advertising table, which is stored in\nApache Parquet format in the marketing S3 bucket of the company data lake. The company data lake is managed by AWS Lake Formation. Finally, access must be limited to the three promotion columns in the table.\nWhich combination of steps will meet these requirements? (Choose three.)","answer_ET":"BDE"},{"id":"U0Stb1p9D32R6APPOl8v","exam_id":20,"question_text":"A web retail company wants to implement a near-real-time clickstream analytics solution. The company wants to analyze the data with an open-source package.\nThe analytics application will process the raw data only once, but other applications will need immediate access to the raw data for up to 1 year.\nWhich solution meets these requirements with the LEAST amount of operational effort?","topic":"1","answer":"B","answer_description":"","unix_timestamp":1650526620,"discussion":[{"comment_id":"589178","comments":[{"upvote_count":"8","timestamp":"1651830540.0","comment_id":"597657","poster":"MWL","content":"The question requires to store \"raw data\" for 1 year, but for the \"processed data\"."}],"upvote_count":"10","timestamp":"1650526620.0","content":"Selected Answer: D\nstore data in S3 not in Kinesis","poster":"rb39"},{"comments":[{"poster":"astalavista1","upvote_count":"1","timestamp":"1650741840.0","content":"Plus C only stores the log data, not the data itself.","comment_id":"590777"},{"timestamp":"1651356240.0","comment_id":"595314","content":"KDS can store 1 Year data:\nhttps://aws.amazon.com/blogs/big-data/retaining-data-streams-up-to-one-year-with-amazon-kinesis-data-streams/","poster":"jrheen","upvote_count":"12"}],"poster":"astalavista1","content":"Selected Answer: D\nA & B out as you cannot store data for more than 7 days in KDS.\nC - Possibly but won't be cost-effective.\nD - Cost-Effective with least amount of operational overhead.","comment_id":"590775","timestamp":"1650741720.0","upvote_count":"8"},{"poster":"MLCL","timestamp":"1691337120.0","upvote_count":"4","comment_id":"973991","content":"Selected Answer: B\nB is the right answer, Kinesis Data Stream can keep raw data up to 365 days.\nAnytime they ask about click-stream analytics it should go to Kinesis Data Analytics."},{"content":"B: I passed the test","poster":"pk349","comment_id":"886632","timestamp":"1682967240.0","upvote_count":"2"},{"poster":"akashm99101001com","upvote_count":"2","timestamp":"1679666940.0","comment_id":"849357","content":"Selected Answer: B\nanalytics application will process the raw data only once so why store in S3?"},{"timestamp":"1679387280.0","poster":"AwsNewPeople","content":"Selected Answer: B\nThe solution that meets the requirements with the least amount of operational effort is Option B: Use Amazon Kinesis Data Streams to collect the data. Use Amazon Kinesis Data Analytics with Apache Flink to process the data in real-time. Set the retention period of the Kinesis data stream to 8,760 hours.\n\nThis solution uses Amazon Kinesis Data Streams to collect the data and processes it in real-time using Amazon Kinesis Data Analytics with Apache Flink. This allows for near-real-time clickstream analytics without the need for additional data processing or storage. Additionally, the retention period of the Kinesis data stream can be set to 8,760 hours (1 year), which allows other applications to have immediate access to the raw data for up to 1 year without the need for additional storage or processing. This solution requires the least amount of operational effort as it does not require additional steps for data processing or storage.","upvote_count":"6","comment_id":"845657"},{"content":"For those debating B and D, I am going with B. Least operational overhead, and the giveway is the hours : \"The maximum value of a stream's retention period is 8760 hours (365 days).\" https://docs.aws.amazon.com/kinesis/latest/APIReference/API_IncreaseStreamRetentionPeriod.html. Be wary of the attempt in the question to use 8.760 hours which i think is sneaky.","poster":"np2021","upvote_count":"2","timestamp":"1679141160.0","comment_id":"842743"},{"content":"Selected Answer: B\nFocus on \"LEAST amount of operational effort\" so you should eliminate EMR from the picture. D recommends EMR and it is operational overhead. \n\nB - make sense. Since it can keep data for 365 days and there is no operational overhead.","upvote_count":"2","comment_id":"809030","poster":"murali12180","timestamp":"1676429520.0"},{"content":"Selected Answer: D\nIt says the raw data need to be accessed by other applications, if we put in the Kinesis Stream, how other application can access it? Besides, KDS is not good for store data. S3 is the right choice.","timestamp":"1673458020.0","poster":"Chelseajcole","upvote_count":"2","comment_id":"772760"},{"timestamp":"1670941620.0","poster":"nadavw","comments":[{"content":"KDS can keep between 24 hours to 7 days.","poster":"learnazureportal","upvote_count":"1","comment_id":"770851","timestamp":"1673297880.0","comments":[{"poster":"np2021","content":"Incorrect. https://docs.aws.amazon.com/kinesis/latest/APIReference/API_IncreaseStreamRetentionPeriod.html","timestamp":"1679141220.0","upvote_count":"1","comment_id":"842744"}]}],"upvote_count":"1","content":"Selected Answer: B\nA Kinesis data stream stores records for 24 hours by default, up to 365 days (8,760 hours). \nhttps://aws.amazon.com/blogs/big-data/retaining-data-streams-up-to-one-year-with-amazon-kinesis-data-streams/","comment_id":"744128"},{"comment_id":"743963","content":"Selected Answer: C\nC. Using Amazon Managed Streaming for Apache Kafka (Amazon MSK) to collect the data, Amazon EMR with Apache Flink to process the data, and setting the log retention hours to 8,760 would meet the requirements with the least amount of operational effort. Amazon MSK is a fully managed service that makes it easy to set up, maintain, and scale Apache Kafka clusters. Amazon EMR can be used to process data from an Amazon MSK stream in real time, and the log retention hours can be set to 8,760 to retain the data for up to 1 year. This solution would require minimal effort to set up and maintain, and would allow other applications to access the raw data for up to 1 year.","upvote_count":"1","poster":"IvanHuang","timestamp":"1670932980.0"},{"comments":[],"timestamp":"1670160180.0","upvote_count":"3","poster":"thuyeinaung","comment_id":"735105","content":"Selected Answer: B\nB for {{ LEAST amount of operational effort }}"},{"poster":"b33f","comment_id":"714160","timestamp":"1667951700.0","content":"Selected Answer: B\nI vote for B. I think C with EMR requires more operational effort.","upvote_count":"1"},{"comment_id":"703814","content":"Selected Answer: B\nKDS can keep the data for one year.","poster":"rav009","upvote_count":"1","timestamp":"1666698000.0"},{"content":"Selected Answer: B\nAnswer is B, KDS can natively store raw data for up to 1 year","timestamp":"1660741980.0","comment_id":"648097","upvote_count":"3","poster":"APIsche"},{"content":"Selected Answer: B\nAnswer-B","comment_id":"634940","poster":"rocky48","comments":[{"content":"Kinesis data stream stores records for 24 hours by default, up to 365 days (8,760 hours).","comment_id":"749450","timestamp":"1671424080.0","upvote_count":"1","poster":"rocky48"}],"timestamp":"1658454900.0","upvote_count":"1"},{"comment_id":"631719","content":"Selected Answer: B\nRetention and Minimal Operational \nKDS(8760 hours (365 days)) + KDA(Flink available out of the box) >> KDS + EMR with Flink + S3","timestamp":"1657885200.0","upvote_count":"2","poster":"ru4aws"},{"content":"Selected Answer: B\nI don't like keeping raw_data in Kinesis for one year and think it is generally better to store the raw data in S3. However, the question does state that it is necessary for the \"raw data\" to be accessible. In option D the raw_data is not stored, only the processed data (or it is just worded badly). So option B is the answer.","comment_id":"623815","comments":[{"poster":"Ramshizzle","timestamp":"1656401700.0","upvote_count":"3","content":"Also this solution provides the minimum operational overhead, because running an EMR cluster requires more operational overhead then Kinesis Data Analytics.","comment_id":"623816"}],"upvote_count":"4","poster":"Ramshizzle","timestamp":"1656401640.0"},{"content":"Selected Answer: B\nB: Because its for a solution with least amt of operational exercise. And also for keeping raw data without any transformation.","comment_id":"609368","upvote_count":"5","timestamp":"1653932280.0","poster":"Ob1KN0B"},{"poster":"certificationJunkie","timestamp":"1653180600.0","content":"There is no need to keep the output of analytics data in kinesis for 8760 hours (or 365 days). Better move it to s3. Hence D.","comment_id":"605062","upvote_count":"2"},{"upvote_count":"4","timestamp":"1651831680.0","content":"Selected Answer: B\nA is out, it says \"8.760\" hour, even it is a type, \"EMR with Apache Flink\" is not very cost effective.\nBoth Kinesis data stream and kafka can store stream data for 1 year.\nThe question requires to store \"raw data\" for 1 year, but for the \"processed data\". So D is not correct.","poster":"MWL","comment_id":"597664"},{"upvote_count":"1","content":"The answer has to be B, KDS can hold data up to 1 year","timestamp":"1651503180.0","poster":"chp2022","comment_id":"596089"},{"content":"Answer-B","timestamp":"1651356300.0","upvote_count":"1","comment_id":"595315","poster":"jrheen"},{"content":"I think option D is okay","comment_id":"590484","timestamp":"1650702660.0","poster":"CHRIS12722222","upvote_count":"3"}],"question_id":42,"timestamp":"2022-04-21 09:37:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/73987-exam-aws-certified-data-analytics-specialty-topic-1-question/","answers_community":["B (65%)","D (33%)","2%"],"isMC":true,"choices":{"B":"Use Amazon Kinesis Data Streams to collect the data. Use Amazon Kinesis Data Analytics with Apache Flink to process the data in real time. Set the retention period of the Kinesis data stream to 8,760 hours.","D":"Use Amazon Kinesis Data Streams to collect the data. Use Amazon EMR with Apache Flink to consume and process the data from the Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Set an S3 Lifecycle policy to delete the data after 365 days.","C":"Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to collect the data. Use Amazon EMR with Apache Flink to consume and process the data from the Amazon MSK stream. Set the log retention hours to 8,760.","A":"Use Amazon Kinesis Data Streams to collect the data. Use Amazon EMR with Apache Flink to consume and process the data from the Kinesis data stream. Set the retention period of the Kinesis data stream to 8.760 hours."},"answer_ET":"B","answer_images":[]},{"id":"b5xxyhV9YVNfUkqcv1MD","answer_description":"","isMC":true,"answer_ET":"C","answer_images":[],"discussion":[{"upvote_count":"10","timestamp":"1653119460.0","poster":"f4bi4n","comment_id":"604750","content":"Not sure if C fulfills the \"long term solution\" part of the question. In the worst case, he needs to adjust it every time the data grows and the query takes again longer. What about A?"},{"upvote_count":"5","timestamp":"1691338380.0","comment_id":"974001","content":"Selected Answer: C\nRight answer is A but exam answer is C, they want you to know about service quotas.","poster":"MLCL"},{"content":"Default Athena’s max query time out minute is 30min.\nso we needs to up Service Quoata.","comment_id":"1063528","poster":"LocalHero","upvote_count":"1","timestamp":"1699248780.0"},{"content":"Selected Answer: A\nI believe the best choice is A. From the given information, we aren't clear on how much we can extend the timeout—would 60 minutes suffice? It's uncertain. Increasing timeouts without a clear rationale isn't a recommended approach, as there are costs associated with prolonged query times, both in terms of resources and expenses.\n\nOperational Best Practices: Increasing timeouts without understanding the actual need can be considered a \"band-aid\" solution. It might temporarily solve the symptom (i.e., timeout) but not the underlying problem causing the delay.:)","upvote_count":"1","poster":"rlnd2000","timestamp":"1696857540.0","comment_id":"1029364"},{"comment_id":"900052","timestamp":"1684323360.0","upvote_count":"3","poster":"Mirandaali","content":"Selected Answer: A\nC is not a sustainable solution. So A is correct"},{"comment_id":"886633","poster":"pk349","timestamp":"1682967300.0","content":"A: I passed the test","upvote_count":"2"},{"comment_id":"858324","timestamp":"1680387180.0","poster":"rsn","upvote_count":"2","content":"Selected Answer: A\nA seems to be the long term fix here. I go with A"},{"content":"why not D?","poster":"CleverMonkey092","upvote_count":"1","comment_id":"850881","timestamp":"1679825400.0"},{"poster":"AwsNewPeople","timestamp":"1679388540.0","comment_id":"845674","upvote_count":"4","content":"Selected Answer: C\nAnswer is C:\nBy default, the maximum timeout limit for a single query execution in Athena is 30 minutes. If the query requires more than 30 minutes to complete, it can be terminated with a \"Query timeout\" error.\n\nSplitting the query into smaller queries, as mentioned in option A, can work but may be time-consuming and not a scalable solution for large datasets. Requesting an increase in the DML query timeout via the Service Quotas console, as mentioned in option C, can provide a more scalable and long-term solution to the problem.\n\nOption D, saving tables as compressed .csv files, is not a solution to the problem described in the scenario. It will not address the query timeout issue and does not provide a long-term solution for the problem."},{"poster":"rsn","upvote_count":"1","comment_id":"842839","timestamp":"1679150340.0","content":"Selected Answer: A\nI will go with A as that appears to be the long term fix. C talks about increasing the timeout but there is no mention of increase by what extent"},{"comment_id":"822805","timestamp":"1677435600.0","content":"The 30-minute DDL query limit is a soft limit, and an increase can be requested in the Service Quotas console.","poster":"aara98","upvote_count":"1"},{"poster":"Merrick","comment_id":"802930","content":"C\nhttps://docs.aws.amazon.com/athena/latest/ug/service-limits.html","timestamp":"1675927680.0","upvote_count":"1"},{"comment_id":"689773","timestamp":"1665274080.0","comments":[{"content":"It says \"a query failed\" so effort is not to split many queries but one. Since \"data analyst\" is running the query, splitting query is also possible. Not sure between A or C but cannot rule out A.","comment_id":"689968","poster":"dinodragon","timestamp":"1665300300.0","upvote_count":"2"}],"content":"For A, as there’re a large number of queries that need to be split, the effort is huge. Some queries may still need a longer time to complete, and it’s OK for the data analyst to wait for them. So increasing the timeout limit is the long-term solution. I go with C.","upvote_count":"3","poster":"jazzok"},{"poster":"rocky48","upvote_count":"2","content":"Selected Answer: C\nSelected Answer: C","comment_id":"634379","timestamp":"1658381580.0"},{"content":"A looks correct option. timeout could occur if the spool size of a query is too much an spice is not able to generate output in specified time. Increasing timeout is not a long term solution as other queries may continue to timeout with such large data sets. So better to limit the scope by splitting the queries for specific limited dataset even if that means running query multiple times.","comment_id":"604876","upvote_count":"4","timestamp":"1653133740.0","poster":"certificationJunkie"},{"timestamp":"1651350660.0","upvote_count":"1","poster":"jrheen","content":"Answer - C","comment_id":"595269"},{"poster":"astalavista1","content":"Selected Answer: C\nC - Per amazon doc - https://docs.aws.amazon.com/athena/latest/ug/service-limits.html","comment_id":"590779","timestamp":"1650742500.0","comments":[{"upvote_count":"2","content":"The quota page specifically shows 30 mins max for DML if an increase isn't requested.\nhttps://docs.aws.amazon.com/general/latest/gr/athena.html#amazon-athena-limits","comment_id":"590780","timestamp":"1650742560.0","poster":"astalavista1"}],"upvote_count":"3"},{"timestamp":"1650705300.0","upvote_count":"5","comment_id":"590504","content":"C\nhttps://docs.aws.amazon.com/general/latest/gr/athena.html#amazon-athena-limits","poster":"CHRIS12722222"}],"question_images":[],"answers_community":["C (67%)","A (33%)"],"exam_id":20,"topic":"1","unix_timestamp":1650705300,"url":"https://www.examtopics.com/discussions/amazon/view/74208-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"C","choices":{"C":"In the Service Quotas console, request an increase for the DML query timeout","D":"Save the tables as compressed .csv files","A":"Split the query into smaller queries to search smaller subsets of data","B":"In the settings for Athena, adjust the DML query timeout limit"},"question_id":43,"timestamp":"2022-04-23 11:15:00","question_text":"A data analyst runs a large number of data manipulation language (DML) queries by using Amazon Athena with the JDBC driver. Recently, a query failed after it ran for 30 minutes. The query returned the following message: java.sql.SQLException: Query timeout\nThe data analyst does not immediately need the query results. However, the data analyst needs a long-term solution for this problem.\nWhich solution will meet these requirements?"},{"id":"NKAOLg1jRXxDD2tLXPc3","answer_ET":"BCF","timestamp":"2022-04-23 11:22:00","question_id":44,"question_images":["https://www.examtopics.com/assets/media/exam-media/04144/0008100001.png"],"question_text":"A retail company is using an Amazon S3 bucket to host an ecommerce data lake. The company is using AWS Lake Formation to manage the data lake.\nA data analytics specialist must provide access to a new business analyst team. The team will use Amazon Athena from the AWS Management Console to query data from existing web_sales and customer tables in the ecommerce database. The team needs read-only access and the ability to uniquely identify customers by using first and last names. However, the team must not be able to see any other personally identifiable data. The table structure is as follows:\n//IMG//\n\nWhich combination of steps should the data analytics specialist take to provide the required permission by using the principle of least privilege? (Choose three.)","topic":"1","exam_id":20,"discussion":[{"comment_id":"620252","upvote_count":"8","timestamp":"1655888460.0","poster":"Ramshizzle","content":"Selected Answer: BCF\nIt should be BCF. Athena always publishes the results of queries in a S3 bucket different from the source-bucket. To access the results you need permissions to this bucket. \nhttps://docs.aws.amazon.com/athena/latest/ug/querying.html \nQuote: To access and view query output files, IAM principals (users and roles) need permission to the Amazon S3 GetObject action for the query result location, as well as permission for the Athena GetQueryResults action"},{"comment_id":"641402","upvote_count":"7","content":"Selected Answer: BCF\nA - WRONG, ALTER grants write access to raw data.\nB - CORRECT, SELECT grants read access to the transaction data, where no customer information is present.\nC - CORRECT, Lake Formation allows specifying which columns are accessible. customer_id is needed in order to join with web_sales table.\nD - WRONG, ALTER grants write access to raw data.\nE - WRONG, s3:GetObject is needed to see Athena results ( https://docs.aws.amazon.com/athena/latest/ug/querying.html ).\nF - CORRECT, even though it's Athena that puts the objects in the bucket and not the IAM user itself, granting s3:PutObject on the Athena bucket doesn't provide for the original S3 bucket where raw data resides.","poster":"alfredofmt","timestamp":"1659464040.0"},{"timestamp":"1682967360.0","content":"BCF: I passed the test","upvote_count":"2","comments":[{"timestamp":"1700838540.0","comment_id":"1079390","comments":[{"timestamp":"1707653280.0","comment_id":"1147203","upvote_count":"3","poster":"god_father","content":"pk349 has never given rationale behind the answers, but one has to admit that all the answers have consistently remained correct."}],"poster":"pkethireddy","content":"In all the questions you mentioned you passed but please confirm if you corrected any mistakes you had in the test?","upvote_count":"3"}],"poster":"pk349","comment_id":"886635"},{"upvote_count":"1","timestamp":"1667952900.0","poster":"b33f","comment_id":"714167","content":"Selected Answer: BCF\nF is definitely needed. Athena automatically saves query results in S3.\nhttps://docs.aws.amazon.com/athena/latest/ug/querying.html"},{"timestamp":"1666843380.0","content":"Selected Answer: BCF\nBCF for sure. You need putobject permission to save athena query results.","comment_id":"705211","upvote_count":"1","poster":"rav009"},{"timestamp":"1650705720.0","content":"I think BCE.\nRead-only access mean no need for ALTER permission. No need to give write access (s3:putObject)","upvote_count":"1","comments":[{"content":"so the answer is BCF?","upvote_count":"2","comments":[{"upvote_count":"1","content":"No its BCE\nF is granting Put....","poster":"f4bi4n","comment_id":"605247","timestamp":"1653205200.0"}],"timestamp":"1651233480.0","comment_id":"594430","poster":"finnliang"}],"poster":"CHRIS12722222","comment_id":"590507"}],"answer":"BCF","choices":{"A":"In AWS Lake Formation, grant the business_analyst group SELECT and ALTER permissions for the web_sales table.","E":"Create users under a business_analyst IAM group. Create a policy that allows the lakeformation:GetDataAccess action, the athena:* action, and the glue:Get* action.","F":"Create users under a business_analyst IAM group. Create a policy that allows the lakeformation:GetDataAccess action, the athena:* action, and the glue:Get* action. In addition, allow the s3:GetObject action, the s3:PutObject action, and the s3:GetBucketLocation action for the Athena query results S3 bucket.","D":"In AWS Lake Formation, grant the business_analyst group SELECT and ALTER permissions for the customer table. Under columns, choose filter type ג€Include columnsג€ with columns fisrt_name and last_name.","B":"In AWS Lake Formation, grant the business_analyst group the SELECT permission for the web_sales table.","C":"In AWS Lake Formation, grant the business_analyst group the SELECT permission for the customer table. Under columns, choose filter type ג€Include columnsג€ with columns fisrt_name, last_name, and customer_id."},"answer_description":"","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/74209-exam-aws-certified-data-analytics-specialty-topic-1-question/","unix_timestamp":1650705720,"answers_community":["BCF (100%)"],"isMC":true},{"id":"emLYj8jppBNt1WobJ9g3","discussion":[{"poster":"astalavista1","timestamp":"1650743100.0","upvote_count":"7","content":"Selected Answer: B\nB - Step Functions\nhttps://docs.aws.amazon.com/step-functions/latest/dg/use-cases-data-processing.html","comment_id":"590785"},{"comment_id":"886636","poster":"pk349","timestamp":"1682967360.0","content":"S: I passed the test","upvote_count":"1"},{"comment_id":"860672","timestamp":"1680586020.0","content":"B - AWS Step Funtions \nhttps://aws.amazon.com/blogs/big-data/prepare-transform-and-orchestrate-your-data-using-aws-glue-databrew-aws-glue-etl-and-aws-step-functions/","upvote_count":"1","poster":"yama234"},{"content":"C for me","timestamp":"1679826000.0","comment_id":"850888","poster":"CleverMonkey092","upvote_count":"1"},{"comment_id":"700806","upvote_count":"1","timestamp":"1666352160.0","content":"B, AWS Step Functions","poster":"Bansel"},{"poster":"JHJHJHJHJ","comment_id":"681102","content":"Answer : B \n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/use-cases-data-processing.html\nDepending upon your data processing needs, Step Functions directly integrates with other data processing services provided by AWS such as AWS Batch for batch processing, Amazon EMR for big data processing, AWS Glue for data preparation, Athena for data analysis, and AWS Lambda for compute.","timestamp":"1664305860.0","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: B\nSelected Answer: B","timestamp":"1658806380.0","poster":"rocky48","comment_id":"637108"},{"timestamp":"1650707820.0","upvote_count":"3","comments":[{"poster":"CHRIS12722222","upvote_count":"5","comment_id":"590642","timestamp":"1650719340.0","content":"Changed to B\nhttps://docs.aws.amazon.com/step-functions/latest/dg/connect-emr.html"}],"poster":"CHRIS12722222","comment_id":"590525","content":"I think A"}],"answers_community":["B (100%)"],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/74213-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_description":"","isMC":true,"exam_id":20,"topic":"1","question_id":45,"choices":{"C":"AWS Lambda","A":"AWS Glue workflows","B":"AWS Step Functions","D":"AWS Batch"},"answer":"B","question_images":[],"unix_timestamp":1650707820,"timestamp":"2022-04-23 11:57:00","question_text":"A company has multiple data workflows to ingest data from its operational databases into its data lake on Amazon S3. The workflows use AWS Glue and Amazon\nEMR for data processing and ETL. The company wants to enhance its architecture to provide automated orchestration and minimize manual intervention.\nWhich solution should the company use to manage the data workflows to meet these requirements?","answer_ET":"B"}],"exam":{"provider":"Amazon","isImplemented":true,"lastUpdated":"11 Apr 2025","isBeta":false,"id":20,"numberOfQuestions":164,"isMCOnly":true,"name":"AWS Certified Data Analytics - Specialty"},"currentPage":9},"__N_SSP":true}