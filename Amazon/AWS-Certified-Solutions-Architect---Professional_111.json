{"pageProps":{"questions":[{"id":"Mj0ggwfEZNVKBXTQ9n0R","answer":"A","timestamp":"2020-08-10 19:02:00","question_text":"A Solutions Architect wants to make sure that only AWS users or roles with suitable permissions can access a new Amazon API Gateway endpoint. The Solutions\nArchitect wants an end-to-end view of each request to analyze the latency of the request and create service maps.\nHow can the Solutions Architect design the API Gateway access control and perform request inspections?","topic":"1","answer_images":[],"question_images":[],"exam_id":32,"unix_timestamp":1597078920,"choices":{"B":"For the API Gateway resource, set CORS to enabled and only return the company's domain in Access-Control-Allow-Origin headers. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.","C":"Create an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system. Use AWS X-Ray to trace and analyze user requests to API Gateway.","A":"For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.","D":"Create a client certificate for API Gateway. Distribute the certificate to the AWS users and roles that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway."},"answers_community":["A (100%)"],"discussion":[{"comment_id":"154706","upvote_count":"25","timestamp":"1632119820.0","comments":[{"poster":"joe16","timestamp":"1636159680.0","comment_id":"456114","upvote_count":"2","content":"A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/"}],"poster":"Nemer","content":"A. Access control using Role, and request inspection with X-Ray."},{"content":"Answer is A","upvote_count":"5","timestamp":"1634781960.0","comment_id":"275821","poster":"Ebi"},{"upvote_count":"1","content":"Selected Answer: A\nA. Access control using Role, and request inspection with X-Ray.","timestamp":"1688312880.0","comment_id":"941026","poster":"SkyZeroZx"},{"content":"A. For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.","upvote_count":"1","comment_id":"497352","poster":"cldy","timestamp":"1639027740.0"},{"comment_id":"494847","poster":"AzureDP900","upvote_count":"1","timestamp":"1638762120.0","content":"A is right answer"},{"content":"Selected Answer: A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/","upvote_count":"2","timestamp":"1637627700.0","poster":"acloudguru","comment_id":"484661"},{"poster":"acloudguru","comment_id":"484660","upvote_count":"1","timestamp":"1637627640.0","content":"Selected Answer: A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/"},{"upvote_count":"2","content":"I'll go with A","timestamp":"1636116360.0","poster":"WhyIronMan","comment_id":"411869"},{"content":"it's A","comment_id":"350185","upvote_count":"5","timestamp":"1635764400.0","poster":"Waiweng"},{"content":"A is the answer, XRay is needed here.","upvote_count":"1","comment_id":"349054","poster":"blackgamer","timestamp":"1635646320.0"},{"upvote_count":"3","poster":"Pupu86","comment_id":"333869","content":"End-to-end request already hints towards the usage of AWS X-ray. Automatically filtering out option B and D. Further the authorisation via role rather than parsing secrets through AWS clients - so A","timestamp":"1635561540.0"},{"comment_id":"295275","content":"A is the correct answer. Role +X-ray for better analysis","timestamp":"1635550320.0","upvote_count":"2","poster":"kiev"},{"timestamp":"1635475380.0","comment_id":"292527","upvote_count":"1","content":"going with A","poster":"Kian1"},{"poster":"Bulti","content":"A is correct.","comment_id":"253453","upvote_count":"3","timestamp":"1634460480.0"},{"poster":"T14102020","comment_id":"244068","content":"Correct is A. AWS Signature + X-Ray","timestamp":"1634119680.0","upvote_count":"2"},{"upvote_count":"3","comment_id":"231635","content":"I'll go with A","timestamp":"1633941180.0","poster":"jackdryan"},{"comment_id":"215837","timestamp":"1633925160.0","poster":"taoteching1","content":"A is correct - https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-examples.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-using-xray-maps.html","upvote_count":"2"},{"timestamp":"1633692600.0","comment_id":"211100","poster":"liono","upvote_count":"1","content":"A\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-examples.html"},{"comment_id":"209289","timestamp":"1633544880.0","content":"A. Control access for API gateway through roles and IAM and AWS Signature. Use X-Ray to troubleshoot and provide analysis of the end to end traffic.","upvote_count":"1","poster":"CYL"},{"content":"Correct answer is A. \nCheck the details at https://docs.aws.amazon.com/apigateway/latest/developerguide/security-iam.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/security-monitoring.html","poster":"perio","timestamp":"1633499220.0","comment_id":"185155","upvote_count":"1"},{"timestamp":"1632688140.0","content":"Answer is D Client certificates are used to authenticate an API by the backend server. To authenticate an API client (or user), use IAM roles and policies, a custom Authorizer or an Amazon Cognito user pool. https://docs.aws.amazon.com/apigateway/api-reference/resource/client-certificate/","upvote_count":"5","comment_id":"184165","poster":"sam422"},{"comment_id":"184161","timestamp":"1632606960.0","poster":"sam422","upvote_count":"4","content":"Question is about restricting iam user and roles to access API gateway, I think answer is D"},{"upvote_count":"1","timestamp":"1632487860.0","content":"A.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/","poster":"directconnect","comment_id":"159504"},{"content":"A is right answer as X-Ray gives very detailed analysis when compared to Cloudwatch and enable AWS Signature for sign the requests.","comment_id":"157049","timestamp":"1632429060.0","upvote_count":"1","poster":"Anila_Dhharisi"},{"upvote_count":"3","poster":"TK2019","timestamp":"1632189780.0","comment_id":"156598","content":"A is correct . API can be signed with https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html and end to end can be traced with Xray"}],"question_id":551,"url":"https://www.examtopics.com/discussions/amazon/view/27957-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"A","isMC":true,"answer_description":""},{"id":"mfF0W37fqztqJoVY1803","question_id":552,"unix_timestamp":1597079460,"url":"https://www.examtopics.com/discussions/amazon/view/27958-exam-aws-certified-solutions-architect-professional-topic-1/","timestamp":"2020-08-10 19:11:00","answer_ET":"B","topic":"1","answer":"B","choices":{"A":"Deploy the application on Amazon EC2 instances. Use Amazon Route 53 to forward requests to the EC2 instances. Use Amazon DynamoDB to save the authenticated connection details.","D":"Deploy the application on Amazon EC2 instances in an Auto Scaling group. Use an internet-facing Application Load Balancer on the front end. Use EC2 instances hosting a MySQL database to save the authenticated connection details.","B":"Deploy the application on Amazon EC2 instances in an Auto Scaling group. Use an internet-facing Application Load Balancer to handle requests. Use Amazon DynamoDB to save the authenticated connection details.","C":"Deploy the application on Amazon EC2 instances in an Auto Scaling group. Use an internet-facing Application Load Balancer on the front end. Use EC2 instances to save the authenticated connection details."},"question_text":"A Solutions Architect needs to design a highly available application that will allow authenticated users to stay connected to the application even when there are underlying failures.\nWhich solution will meet these requirements?","answer_description":"","answer_images":[],"question_images":[],"discussion":[{"timestamp":"1632134160.0","poster":"Nemer","comment_id":"154713","upvote_count":"24","comments":[{"upvote_count":"1","timestamp":"1635966180.0","content":"The requirement said, stay connection after certification. Which means the info should be cached in backend. That's why DB is not necessary. Otherwise every time user submit request will trigger a DB query which is really slow.","comments":[{"upvote_count":"1","poster":"kirrim","timestamp":"1636230660.0","content":"Could front-end DDB with ElasticCache if you're worried about the DDB queries being too slow and you truly need that level of performance on auth/session data and are willing to pay for it. But still need DDB behind it to populate the cache misses in that scenario.","comment_id":"461571"}],"poster":"oscargee","comment_id":"362922"}],"content":"B. ALB + ASG + DynamoDB make sense."},{"upvote_count":"1","comment_id":"1127928","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-schema-session-management.html\n\nC would lose session information of users if the node they where on failed or they were routed to a different node. This is why session stores are typically separate from web servers.","timestamp":"1705853340.0","poster":"3a632a3"},{"comment_id":"941028","poster":"SkyZeroZx","content":"Selected Answer: B\nB. ALB + ASG + DynamoDB make sense.","timestamp":"1688313000.0","upvote_count":"1"},{"content":"Selected Answer: B\nB. DynamoDB is a better option to save the authenticated connection details rather than a standard EC2 instance.","upvote_count":"1","timestamp":"1651360500.0","comment_id":"595341","poster":"tartarus23"},{"content":"Selected Answer: B\nB is highly available, scalable, ALB allows connection stickiness and handling with help of DDB to save the connections and sessions.","poster":"tartarus23","comment_id":"586314","upvote_count":"2","timestamp":"1650021600.0"},{"content":"Selected Answer: B\nB is most right I would say. I am pretty certain it is the answer that AWS wants to hear","timestamp":"1643186340.0","poster":"shotty1","upvote_count":"1","comment_id":"532705"},{"poster":"CloudChef","content":"B or not 2 B, that is the question, and the answer is B","upvote_count":"2","comment_id":"529349","timestamp":"1642795380.0"},{"comment_id":"522092","upvote_count":"1","content":"Selected Answer: B\nVoting B. the answer given makes no sense. If you store session data on an ec2 and you lost it you lost the session.","poster":"pititcu667","timestamp":"1641987600.0"},{"upvote_count":"1","content":"I will go with DynamoDB , B is right","poster":"AzureDP900","comment_id":"494851","timestamp":"1638762540.0"},{"comment_id":"448543","timestamp":"1636204080.0","upvote_count":"1","poster":"moon2351","content":"Answer is B"},{"comment_id":"411870","poster":"WhyIronMan","upvote_count":"2","content":"I'll go with B","timestamp":"1636124880.0"},{"poster":"Waiweng","content":"it's B","comment_id":"350191","timestamp":"1635897420.0","upvote_count":"3"},{"content":"Reading highly available, A & D are ruled out. \nB, C talk about auto scaling group+application load balancer, so highly available. \nBetween B & C, since DynamoDB makes more sense in this case, so going for B\n\nMy take: B","comment_id":"331527","timestamp":"1635678840.0","upvote_count":"3","poster":"KnightVictor"},{"upvote_count":"1","timestamp":"1635415860.0","comment_id":"295279","poster":"kiev","content":"Absolutely B"},{"timestamp":"1635116580.0","upvote_count":"1","poster":"Kian1","comment_id":"292530","content":"going for B"},{"upvote_count":"1","timestamp":"1635015180.0","poster":"Firststack","content":"B is the answer","comment_id":"281217"},{"timestamp":"1634557020.0","comment_id":"276773","content":"No option other than B makes sense, answer is B","poster":"Ebi","upvote_count":"3"},{"content":"C. the question is asking: \"stay connected to the application even when there are underlying failures\", it means when the DynamoDB fails so the only way is to save the connection details locally on the EC2 instances.","poster":"Superomam","upvote_count":"1","comment_id":"267861","comments":[{"comment_id":"333958","timestamp":"1635723780.0","upvote_count":"1","poster":"sarah_t","content":"DynamoDB is HA by default, EC2 instances can fail."}],"timestamp":"1634236800.0"},{"upvote_count":"1","timestamp":"1634001360.0","poster":"Bulti","comment_id":"253466","content":"Answer is B."},{"poster":"Britts","content":"A bit tricky to read. Mentions failure in the system, actually implying autoscaling action","timestamp":"1633603080.0","comment_id":"250730","upvote_count":"3"},{"comment_id":"244074","timestamp":"1633524300.0","upvote_count":"2","content":"Correct is B. AutoScalingGroup + DynamoDB","poster":"T14102020"},{"comment_id":"231636","content":"I'll go with B","timestamp":"1633436760.0","poster":"jackdryan","upvote_count":"3"},{"upvote_count":"1","timestamp":"1633200600.0","poster":"CYL","comment_id":"209290","content":"B. Storing client specific details is usually faster with DynamoDB."},{"upvote_count":"1","content":"B for sure","poster":"Paitan","comment_id":"199499","timestamp":"1633198980.0"},{"comment_id":"157052","content":"B is right answer. EC2 with ASG + ALB + DynamoDB","timestamp":"1633157280.0","upvote_count":"3","poster":"Anila_Dhharisi"},{"timestamp":"1632541320.0","comment_id":"156131","poster":"TK2019","content":"B makes sense","upvote_count":"3"}],"exam_id":32,"isMC":true,"answers_community":["B (100%)"]},{"id":"42NbyqnPxX1wAK8JqUb8","choices":{"C":"Use S3 access logs with Amazon Elasticsearch Service and Kibana to identify remote IP addresses. Use an Amazon Inspector assessment template to automatically remediate S3 bucket policy changes. Use Amazon SNS for alerts.","B":"Use Amazon Athena with S3 access logs to identify remote IP addresses. Use AWS Config rules with AWS Systems Manager Automation to automatically remediate S3 bucket policy changes. Use Amazon SNS with AWS Config rules for alerts.","A":"Use Amazon CloudWatch Logs with CloudWatch filters to identify remote IP addresses. Use CloudWatch Events rules with AWS Lambda to automatically remediate S3 bucket policy changes. Use Amazon SES with CloudWatch Events rules for alerts.","D":"Use Amazon Macie with an S3 bucket to identify access patterns and remote IP addresses. Use AWS Lambda with Macie to automatically remediate S3 bucket policy changes. Use Macie automatic alerting capabilities for alerts."},"unix_timestamp":1597118520,"url":"https://www.examtopics.com/discussions/amazon/view/28001-exam-aws-certified-solutions-architect-professional-topic-1/","answer":"B","question_images":[],"question_id":553,"answer_ET":"B","discussion":[{"upvote_count":"46","content":"B. 1)To id remote IPs, need to look at S3 access logs. Athena helps in analyzing those logs.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html\n\n2) For auto-remediation, use AWS Config with Systems Manager.\nhttps://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\n\n4) For alerting, use SNS with AWS Config.\nhttps://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html","comments":[{"upvote_count":"3","content":"Agree. Answer is B.","comment_id":"362624","poster":"kpcert","timestamp":"1636050960.0"}],"timestamp":"1632203220.0","poster":"Nemer","comment_id":"155023"},{"poster":"SkyZeroZx","comment_id":"941032","timestamp":"1688313240.0","content":"Selected Answer: B\nB. 1)To id remote IPs, need to look at S3 access logs. Athena helps in analyzing those logs.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html\n\n2) For auto-remediation, use AWS Config with Systems Manager.\nhttps://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\n\n4) For alerting, use SNS with AWS Config.\nhttps://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html\n\nAddicionally remember usually usage case of AWS SES is for bulk email and marketing or legacy apps for SMTP Credentials","upvote_count":"1"},{"content":"D. \nAthena needs a lot of work and is not automatic. Macie is professional on Sensitive data discovery and protection.\nhttps://aws.amazon.com/blogs/security/how-to-create-custom-alerts-with-amazon-macie/","timestamp":"1673706660.0","comment_id":"775512","upvote_count":"3","poster":"jhonivy"},{"timestamp":"1668062520.0","poster":"janvandermerwer","comment_id":"714976","upvote_count":"1","content":"Selected Answer: B\nB - Need to retrive \"remote IP addreses\", alerts when the bucket changes and remediate the changes automatically\n--- Config rules --> detect change --> send sns alert + trigger config remediation.\n--> S3 acess logs search, Athena can probably do the job here.\n\nD - Macie is good but wont' meet the criteria to detect changes."},{"content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html\nKey word S3 server access\nIspector is for EC2 and ECS","poster":"AwsBRFan","upvote_count":"1","timestamp":"1663863060.0","comment_id":"676347"},{"content":"B. Use Amazon Athena with S3 access logs to identify remote IP addresses. Use AWS Config rules with AWS Systems Manager Automation to automatically remediate S3 bucket policy changes. Use Amazon SNS with AWS Config rules for alerts.","poster":"cldy","upvote_count":"1","comment_id":"495039","timestamp":"1638786180.0"},{"timestamp":"1637767020.0","poster":"pcops","comment_id":"486056","upvote_count":"1","content":"B: Athena + S3 access logs to identify IP address. SNS for notifications and SM to automate the requests."},{"upvote_count":"1","comment_id":"481264","timestamp":"1637286240.0","content":"One actually CAN get the IP ADDRESS using Amazon Macie:\npolicyDetails.actor.ipAddressDetails.ipAddressV4\n\nhttps://docs.aws.amazon.com/de_de/macie/latest/user/findings-filter-fields.html","poster":"sashenka"},{"timestamp":"1636082400.0","content":"I'll go with B\nFor those choosing D, read the question again. Twice.\n✑ Identify remote IP addresses that are accessing the bucket objects.\n✑ Receive alerts when the security policy on the bucket is changed.\n✑ Remediate the policy changes automatically.\n^ this is called \"Requirements\" ^\nMacie is about the DATA itself; question wants to prevent a series of events like public explicit buckets, notify and set they private again. \nTypical use case of AWS Config rules","upvote_count":"1","poster":"WhyIronMan","comment_id":"411876"},{"poster":"digimaniac","content":"D\nB can't monitor S3 policy change. versus Macie can \"Macie generates policy findings when the policies or settings for an S3 bucket are changed in a way that reduces the security of the bucket and its objects. Macie does this only if the change occurs after you enable your Macie account.\"","timestamp":"1636046520.0","upvote_count":"2","comment_id":"348140"},{"content":"Answer is D. AWS macie is built specifically for protecting of PII information","poster":"Pupu86","comments":[{"poster":"blackgamer","comment_id":"422136","timestamp":"1636238160.0","content":"Answer is B. please refer to below link for details explanation.\n\nhttps://aws.amazon.com/blogs/mt/using-aws-systems-manager-opscenter-and-aws-config-for-compliance-monitoring/","upvote_count":"1"}],"comment_id":"333867","upvote_count":"2","timestamp":"1636006380.0"},{"content":"i go with B","comment_id":"321756","upvote_count":"1","poster":"alisyech","timestamp":"1635030600.0"},{"content":"going with B","comment_id":"292534","poster":"Kian1","upvote_count":"1","timestamp":"1634899800.0"},{"content":"I will go with B","upvote_count":"4","poster":"Ebi","timestamp":"1634852400.0","comment_id":"276782"},{"comment_id":"253704","content":"Change my mind to B .. D can not do the last point in the question.","timestamp":"1634670900.0","poster":"petebear55","upvote_count":"1"},{"upvote_count":"1","timestamp":"1634583660.0","comment_id":"253698","content":"D: https://aws.amazon.com/blogs/security/how-to-create-custom-alerts-with-amazon-macie/","poster":"petebear55"},{"comments":[{"poster":"vbal","timestamp":"1639600740.0","content":"Amazon Macie is a security service that makes it easy for you to discover, classify, and protect sensitive data in Amazon Simple Storage Service (Amazon S3). Question is About Bucket Policy Changes...Can Macie look for changes in AWS resources Configuration???","comment_id":"502467","upvote_count":"1"}],"content":"D is correct as its designed for just this scenario with S3","timestamp":"1634479380.0","poster":"petebear55","upvote_count":"1","comment_id":"253697"},{"timestamp":"1634221800.0","comment_id":"253486","poster":"Bulti","comments":[{"poster":"Bulti","timestamp":"1634769540.0","upvote_count":"1","comment_id":"269582","content":"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/"}],"content":"B is the correct answer. Only AWS config can continuously monitor changes to bucket polices and enable automatic remediation.","upvote_count":"2"},{"comments":[{"content":"D seems latest and accurate: Supporting link: https://aws.amazon.com/blogs/security/deploy-an-automated-chatops-solution-for-remediating-amazon-macie-findings/?nc1=b_rp","upvote_count":"2","poster":"SD13","comment_id":"264188","comments":[{"poster":"SD13","comment_id":"329909","upvote_count":"2","content":"Correct answer is B, Macie cannot detect remote IP Athena can","timestamp":"1635118560.0"}],"timestamp":"1634709540.0"}],"upvote_count":"4","timestamp":"1633923720.0","content":"D is correct because:\n1. Macie can detect the source IP (https://docs.aws.amazon.com/macie/latest/user/monitoring.html)\n2. It can easily send alerts out\n3. Can integrate with event bridge to trigger lambda for remediation (https://docs.aws.amazon.com/macie/latest/user/findings-monitor.html)","poster":"darthvoodoo","comment_id":"251327"},{"comment_id":"246324","timestamp":"1633877640.0","poster":"kj07","content":"I will go with B. \nWe will not know if the bucket policy is changed unless you will use an AWS Config rule.","upvote_count":"1"},{"comment_id":"233022","poster":"GopiSivanathan","content":"Answer A","timestamp":"1633686420.0","comments":[{"comments":[{"comment_id":"248381","content":"A - CloudWatch Logs use filter expressions • For example, find a specific IP inside of a log","timestamp":"1633920960.0","poster":"arulrajjayaraj","upvote_count":"1"}],"comment_id":"239749","poster":"cloudgc","upvote_count":"2","content":"Answer should be B - CloudWatch will not have source IP details unless it is enabled with AWS CloudTrail.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/external-ip-address-s3-bucket/","timestamp":"1633728600.0"}],"upvote_count":"1"},{"upvote_count":"1","timestamp":"1633641180.0","comment_id":"233020","poster":"GopiSivanathan","content":"Question is about Object level access:\nNeed to Use CloudWatch event/S3 events for the object level monitoring.\nOption 1: \n1. Turn on object-level logging for Amazon S3.\n2. Configure a CloudWatch event to notify by using an SNS topic when a PutObject API call with public-read permission is detected in the CloudTrail logs.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html\nOption 2:\nConfigure an Amazon CloudWatch Events rule that invokes an AWS Lambda function to secure the S3 bucket.\nThis actively remediate the public access. \nhttps://aws.amazon.com/blogs/security/how-to-detect-and-automatically-remediate-unintended-permissions-in-amazon-s3-object-acls-with-cloudwatch-events/\n\nwith respect to S3 event logs: There is a possibility that the event may be missed using this method. Amazon S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer. On very rare occasions, events might be lost. https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html"},{"poster":"jackdryan","timestamp":"1633321980.0","content":"I'll go with B","upvote_count":"3","comment_id":"231640"},{"poster":"SamAWSExam99","upvote_count":"1","timestamp":"1633308960.0","content":"B: https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/","comment_id":"207812"},{"content":"B is the way...\n- s3 access logs should be enabled for audit purposes\n- athena can be used for forensics\n- AWS Config for auto fixing..( https://aws.amazon.com/es/blogs/mt/aws-config-auto-remediation-s3-compliance/)","comment_id":"183229","timestamp":"1632808320.0","upvote_count":"1","poster":"ipindado2020"},{"timestamp":"1632631800.0","content":"Answer is A Systems Manager is for EC2 instances not S3","comments":[{"comment_id":"183225","content":"https://aws.amazon.com/es/blogs/mt/aws-config-auto-remediation-s3-compliance/","timestamp":"1632669720.0","poster":"ipindado2020","upvote_count":"2"}],"poster":"smithyt","upvote_count":"1","comment_id":"182441"},{"poster":"mgat","comment_id":"179253","upvote_count":"2","timestamp":"1632443760.0","content":"I would go with A since Athena is not real time"},{"upvote_count":"2","poster":"Anila_Dhharisi","comments":[{"upvote_count":"5","poster":"Anila_Dhharisi","content":"Sorry, I take back my answer. It should be D. Macie is very effective in checking out the patterns of behavior when compared to Athena or whatever.","timestamp":"1633000800.0","comments":[{"content":"correct MACIE is designed for this scenario in s3","upvote_count":"1","timestamp":"1634309820.0","poster":"petebear55","comment_id":"253694"}],"comment_id":"206054"}],"comment_id":"157059","timestamp":"1632320100.0","content":"B is right answer as the Config rules are very helpful to remediate the changes. Elastic search with Kibana is useful for analyzing the data in the form of patterns, bar graph or line graphs with S3 access logs. When compared with Athena and ElasticSearch with Kibana, Athena is better in analyzing the S3 logs."}],"question_text":"A company experienced a breach of highly confidential personal information due to permission issues on an Amazon S3 bucket. The Information Security team has tightened the bucket policy to restrict access. Additionally, to be better prepared for future attacks, these requirements must be met:\n✑ Identify remote IP addresses that are accessing the bucket objects.\n✑ Receive alerts when the security policy on the bucket is changed.\n✑ Remediate the policy changes automatically.\nWhich strategies should the Solutions Architect use?","answer_description":"","timestamp":"2020-08-11 06:02:00","answer_images":[],"answers_community":["B (100%)"],"topic":"1","isMC":true,"exam_id":32},{"id":"49lL6J4PWBjg3uf4nKeL","timestamp":"2020-01-12 05:23:00","isMC":true,"choices":{"C":"Implement message passing between EC2 instances within a batch by exchanging messages through SQS.","D":"Coordinate number of EC2 instances with number of job requests automatically thus Improving cost effectiveness.","A":"Reduce the overall lime for executing jobs through parallel processing by allowing a busy EC2 instance that receives a message to pass it to the next instance in a daisy-chain setup.","E":"Handle high priority jobs before lower priority jobs by assigning a priority metadata field to SQS messages.","B":"Implement fault tolerance against EC2 instance failure since messages would remain in SQS and worn can continue with recovery of EC2 instances implement fault tolerance against SQS failure by backing up messages to S3."},"question_id":554,"url":"https://www.examtopics.com/discussions/amazon/view/11794-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":["https://www.examtopics.com/assets/media/exam-media/04241/0005900001.png"],"answer_description":"There are cases where a large number of batch jobs may need processing, and where the jobs may need to be re-prioritized.\nFor example, one such case is one where there are differences between different levels of services for unpaid users versus subscriber users (such as the time until publication) in services enabling, for example, presentation files to be uploaded for publication from a web browser. When the user uploads a presentation file, the conversion processes, for example, for publication are performed as batch processes on the system side, and the file is published after the conversion. Is it then necessary to be able to assign the level of priority to the batch processes for each type of subscriber?\nExplanation of the Cloud Solution/Pattern\nA queue is used in controlling batch jobs. The queue need only be provided with priority numbers. Job requests are controlled by the queue, and the job requests in the queue are processed by a batch server. In Cloud computing, a highly reliable queue is provided as a service, which you can use to structure a highly reliable batch system with ease. You may prepare multiple queues depending on priority levels, with job requests put into the queues depending on their priority levels, to apply prioritization to batch processes. The performance (number) of batch servers corresponding to a queue must be in accordance with the priority level thereof.\n\nImplementation -\nIn AWS, the queue service is the Simple Queue Service (SQS). Multiple SQS queues may be prepared to prepare queues for individual priority levels (with a priority queue and a secondary queue). Moreover, you may also use the message Delayed Send function to delay process execution.\nUse SQS to prepare multiple queues for the individual priority levels.\nPlace those processes to be executed immediately (job requests) in the high priority queue.\nPrepare numbers of batch servers, for processing the job requests of the queues, depending on the priority levels.\nQueues have a message \"Delayed Send\" function. You can use this to delay the time for starting a process.\n\nConfiguration -\n\n\nBenefits -\nYou can increase or decrease the number of servers for processing jobs to change automatically the processing speeds of the priority queues and secondary queues.\nYou can handle performance and service requirements through merely increasing or decreasing the number of EC2 instances used in job processing.\nEven if an EC2 were to fail, the messages (jobs) would remain in the queue service, enabling processing to be continued immediately upon recovery of the EC2 instance, producing a system that is robust to failure.\n\nCautions -\nDepending on the balance between the number of EC2 instances for performing the processes and the number of messages that are queued, there may be cases where processing in the secondary queue may be completed first, so you need to monitor the processing speeds in the primary queue and the secondary queue.","exam_id":32,"topic":"1","answers_community":["D (50%)","E (50%)"],"answer":"D","question_text":"//IMG//\nRefer to the architecture diagram above of a batch processing solution using Simple Queue Service (SQS) to set up a message queue between EC2 instances which are used as batch processors Cloud Watch monitors the number of Job requests (queued messages) and an Auto Scaling group adds or deletes batch servers automatically based on parameters set in Cloud Watch alarms.\nYou can use this architecture to implement which of the following features in a cost effective and efficient manner?","discussion":[{"timestamp":"1632344880.0","upvote_count":"8","poster":"amog","content":"Answer is D\nhttps://acloud.guru/forums/aws-certified-solutions-architect-associate/discussion/-KQROiiTiwuq4744rUiV/aws-associate-questions","comment_id":"37912"},{"poster":"madmike123","comment_id":"1327077","content":"Selected Answer: E\nE because D is already implemented","timestamp":"1734300180.0","upvote_count":"1"},{"timestamp":"1723751940.0","upvote_count":"1","content":"D. Coordinate number of EC2 instances with number of job requests automatically thus Improving cost effectiveness.","comment_id":"1266658","poster":"amministrazione"},{"timestamp":"1712576640.0","content":"D is correct","upvote_count":"1","comment_id":"1191561","poster":"Kubernetes"},{"content":"Selected Answer: D\nD. Coordinate number of EC2 instances with number of job requests automatically thus Improving cost effectiveness.","comment_id":"577832","poster":"jj22222","upvote_count":"1","timestamp":"1648579320.0"},{"upvote_count":"2","timestamp":"1643142180.0","content":"I think this is E, since D is already implemented","comment_id":"532386","poster":"shotty1"},{"comment_id":"521571","content":"I am going to say e just as the message says you can delay visibility of a low priority message hence you might not force an autoscale event. From the diagram d is already implemented hence going for e -> priority queue with delay visibility.","poster":"pititcu667","upvote_count":"2","timestamp":"1641910260.0"},{"content":"Yes, D","poster":"01037","timestamp":"1635471180.0","comment_id":"349144","upvote_count":"1"},{"comment_id":"340766","content":"D.\nThis is a common patern","upvote_count":"1","poster":"Malcnorth59","timestamp":"1633569060.0"},{"poster":"cldy","content":"D.\nASG efficient & saves cost by removing surplus instances.","upvote_count":"1","comment_id":"325520","timestamp":"1633562820.0"},{"content":"D is correcr","upvote_count":"2","comment_id":"143897","poster":"fullaws","timestamp":"1632982560.0"},{"poster":"manoj101","comment_id":"117952","content":"D: Depending up on the workload you can autoscale eC2 instances and save cost.","timestamp":"1632824580.0","upvote_count":"4"},{"content":"My answer is D","comment_id":"51221","upvote_count":"3","timestamp":"1632804300.0","poster":"miracle"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04241/0005700001.png"],"unix_timestamp":1578802980,"answer_ET":"D"},{"id":"YRpFZsntPM2PpHfwLlvB","answer_description":"","discussion":[{"comments":[{"content":"Second this, System manager have pre-defined run book that can allow you to patch os :)","comment_id":"276929","timestamp":"1635453660.0","upvote_count":"4","poster":"rcher"}],"content":"A. Systems Manager to update the ASG with patched AMI, CodeDeploy to push the code, and EFS for the 500 GB static data.","upvote_count":"27","comment_id":"155030","timestamp":"1632305340.0","poster":"Nemer"},{"timestamp":"1635261900.0","poster":"Ebi","content":"Answer is A, \nB although is correct as well but does not satisfy multiple deployments per day","comment_id":"276788","upvote_count":"9"},{"poster":"WayneYi","upvote_count":"1","timestamp":"1665199260.0","content":"The issue with option B is that it only pushes application code once per day, but we need multiple deployments per day.","comment_id":"689003"},{"content":"C Has all 4 requirements","timestamp":"1648111740.0","poster":"Bennycy","comment_id":"574176","upvote_count":"1"},{"poster":"pititcu667","comment_id":"523501","content":"Selected Answer: A\nA the keywords are automatic multiple releases -> CodeDeploy, Least amount of startup time shared efs for data is faster than downloading 500 gb from s3.","timestamp":"1642159200.0","upvote_count":"2"},{"timestamp":"1641013320.0","content":"A: CodeDeploy + EFS.","comment_id":"514370","poster":"cldy","upvote_count":"1"},{"timestamp":"1638763080.0","upvote_count":"1","content":"I will go with OPTION A","comment_id":"494857","poster":"AzureDP900"},{"comment_id":"491039","content":"Selected Answer: A\ncodedeploy is better than B","poster":"acloudguru","timestamp":"1638312120.0","upvote_count":"2"},{"comment_id":"411879","timestamp":"1635818580.0","poster":"WhyIronMan","content":"I'll go with A","upvote_count":"2"},{"upvote_count":"2","timestamp":"1635803280.0","poster":"Waiweng","content":"it's A","comment_id":"350231"},{"timestamp":"1635565320.0","upvote_count":"1","content":"A seems to be better option","poster":"blackgamer","comment_id":"349096"},{"poster":"Kian1","comment_id":"292535","timestamp":"1635518040.0","upvote_count":"1","content":"going with A"},{"upvote_count":"1","poster":"Bulti","comment_id":"253500","timestamp":"1634761440.0","content":"I will go with A instead of B. B is a bit confusing because it appears that the AWS System Manager would be able to create new AMI as new OS patches are released and replace the existing ones. Not sure why there is a need to do that when deploying the application code as well in B. So I will go with A."},{"poster":"T14102020","comment_id":"244539","upvote_count":"2","content":"Correct is A. CodeDeploy + EFS(faster then S3)","timestamp":"1634698080.0"},{"poster":"T14102020","content":"Correct is A. CodeDeploy + EFS","comment_id":"244522","upvote_count":"2","timestamp":"1634694300.0"},{"timestamp":"1634363760.0","poster":"rscloud","upvote_count":"2","comment_id":"242024","content":"A\nCode deploy for multiply deploy and EFS for static data"},{"poster":"gookseang","upvote_count":"1","timestamp":"1633514640.0","content":"A, CodeDeploy + EFS","comment_id":"233137"},{"timestamp":"1633473000.0","upvote_count":"3","comment_id":"231644","poster":"jackdryan","content":"I'll go with A"},{"poster":"cloudgc","comment_id":"229135","content":"A - https://www.youtube.com/watch?v=yV3BYzoScBI","timestamp":"1633444020.0","upvote_count":"1"},{"timestamp":"1632862200.0","upvote_count":"2","poster":"cpd","comment_id":"210018","content":"Cannot be B because of this sentence \"Update the OS patches and the application code as batch job every night\". Question explicitly asks to deploy multiple times a day; code deploy enables this option."},{"timestamp":"1632827340.0","comment_id":"209294","upvote_count":"1","content":"A. Both A and B are quite close. Codedeploy to allow for multiple deploys within the same day.","poster":"CYL"},{"upvote_count":"1","poster":"SamAWSExam99","content":"B: Update the OS patches and the application code as batch job every night.","comment_id":"207828","timestamp":"1632746580.0"},{"comment_id":"204227","timestamp":"1632667680.0","content":"I prefer B, since that also performs OS patching later on.\nA only seems to patch once (initially), nothing about subsequent patches.","upvote_count":"2","poster":"drexciya28"},{"upvote_count":"4","content":"A is the right answer . AWS Systems Manager to create a new AMI with the updated OS patches and ASG to use the updated AMI. CodeDeploy to push the code and EFS for static data of 500 GB","comment_id":"157063","poster":"Anila_Dhharisi","timestamp":"1632638640.0"}],"isMC":true,"choices":{"B":"Use AWS Systems Manager to create a new AMI with updated OS patches. Update the Auto Scaling group to use the patched AMI and replace existing unpatched instances. Update the OS patches and the application code as batch job every night. Store the static data in Amazon EFS.","A":"Use AWS Systems Manager to create a new AMI with the updated OS patches. Update the Auto Scaling group to use the patched AMI and replace existing unpatched instances. Use AWS CodeDeploy to push the application code to the instances. Store the static data in Amazon EFS.","C":"Use an Amazon-provided AMI for the OS. Configure an Auto Scaling group set to a static instance count. Configure an Amazon EC2 user data script to download the data from Amazon S3. Install OS patches with AWS Systems Manager when they are released. Use AWS CodeDeploy to push the application code to the instances.","D":"Use an Amazon-provided AMI for the OS. Configure an Auto Scaling group. Configure an Amazon EC2 user data script to download the data from Amazon S3. Replace existing instances after each updated Amazon-provided AMI release. Use AWS CodeDeploy to push the application code to the instances."},"answers_community":["A (100%)"],"unix_timestamp":1597119300,"question_images":[],"question_text":"A Solutions Architect is designing a deployment strategy for an application tier and has the following requirements:\n✑ The application code will need a 500 GB static dataset to be present before application startup.\n✑ The application tier must be able to scale up and down based on demand with as little startup time as possible.\n✑ The Development team should be able to update the code multiple times each day.\n✑ Critical operating system (OS) patches must be installed within 48 hours of being released.\nWhich deployment strategy meets these requirements?","topic":"1","timestamp":"2020-08-11 06:15:00","url":"https://www.examtopics.com/discussions/amazon/view/28002-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"exam_id":32,"answer_ET":"A","answer":"A","question_id":555}],"exam":{"isBeta":false,"id":32,"numberOfQuestions":1019,"isMCOnly":false,"lastUpdated":"11 Apr 2025","name":"AWS Certified Solutions Architect - Professional","provider":"Amazon","isImplemented":true},"currentPage":111},"__N_SSP":true}