{"pageProps":{"questions":[{"id":"8Ikc9kGpe8Hk5miYPeLX","discussion":[{"poster":"Andy_09","content":"B is the correct answer","comment_id":"1141275","upvote_count":"11","timestamp":"1707150000.0"},{"content":"Selected Answer: B\nA - Cloudtrail is for API Calls and changes on AWS account. \nB - Going for athena in S3. - Correct\nC - Manual work \nD - Distractor","comment_id":"1149248","upvote_count":"9","poster":"Marunio","timestamp":"1707835980.0"},{"upvote_count":"1","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","poster":"Salilgen","timestamp":"1735560060.0","comment_id":"1334087"},{"upvote_count":"2","timestamp":"1719826620.0","poster":"Scheldon","content":"Selected Answer: B\nAnswer B","comment_id":"1240080"},{"comment_id":"1205277","poster":"[Removed]","content":"Correct answer is B","upvote_count":"2","timestamp":"1714618380.0"},{"poster":"Naveena_Devanga","upvote_count":"5","comment_id":"1155646","timestamp":"1708529580.0","content":"B - \nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL."},{"comment_id":"1150783","poster":"c48b4e2","upvote_count":"2","content":"Why there is a \"Correct answer\" (the green bordered one) at all while most of the time the community thinks (correctly) otherwise?","timestamp":"1707979020.0"},{"content":"why not A?","timestamp":"1707762120.0","comments":[{"comment_id":"1177433","timestamp":"1710859080.0","upvote_count":"3","poster":"Kezuko","content":"Access logs is an optional feature of Elastic Load Balancing that is disabled by default"}],"poster":"betttty","upvote_count":"3","comment_id":"1148482"}],"answer_ET":"B","answers_community":["B (100%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/132874-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"question_id":696,"unix_timestamp":1707150000,"answer_images":[],"choices":{"D":"Use Amazon EMR on a dedicated Amazon EC2 instance to directly query the ALB to acquire traffic access log information.","C":"Enable ALB access logging to Amazon S3. Open each file in a text editor, and search each line for the relevant information.","B":"Enable ALB access logging to Amazon S3. Create a table in Amazon Athena, and query the logs.","A":"Create a table in Amazon Athena for AWS CloudTrail logs. Create a query for the relevant information."},"isMC":true,"question_images":[],"timestamp":"2024-02-05 17:20:00","question_text":"A company is using an Application Load Balancer (ALB) to present its application to the internet. The company finds abnormal traffic access patterns across the application. A solutions architect needs to improve visibility into the infrastructure to help the company understand these abnormalities better.\n\nWhat is the MOST operationally efficient solution that meets these requirements?","answer":"B","topic":"1"},{"id":"C9M8CWY5LDHqKZFcJg8k","discussion":[{"upvote_count":"16","timestamp":"1723021740.0","content":"Selected Answer: C\nShould be C: Public NAT GW in Public Subnet to have access to internet. Private NAT GW is used for VPC or on-prem","poster":"anikolov","comment_id":"1143263"},{"upvote_count":"9","comment_id":"1142822","timestamp":"1722978420.0","content":"Selected Answer: C\nI think the correct is C, because D would require more than just private NAT gateway.\n\nPrivate – Instances in private subnets can connect to other VPCs or your on-premises network through a private NAT gateway. You can route traffic from the NAT gateway through a transit gateway or a virtual private gateway. You cannot associate an elastic IP address with a private NAT gateway. You can attach an internet gateway to a VPC with a private NAT gateway, but if you route traffic from the private NAT gateway to the internet gateway, the internet gateway drops the traffic.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html","poster":"mestule"},{"comment_id":"1177437","poster":"Kezuko","upvote_count":"3","timestamp":"1726749600.0","content":"Selected Answer: C\nPublic NAT Gateway in public subnets for the internet access\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"},{"comment_id":"1158173","poster":"knben","upvote_count":"2","content":"Selected Answer: C\nPublic NAT GW in Public Subnet to have access to internet","timestamp":"1724528220.0"},{"comment_id":"1141277","timestamp":"1722867960.0","content":"Looks correct","poster":"Andy_09","upvote_count":"2"}],"answer_description":"","answer_ET":"C","answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/132875-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"question_id":697,"unix_timestamp":1707150360,"answer_images":[],"choices":{"D":"Create private NAT gateways in public subnets in the same VPCs as the EC2 instances.","B":"Create private NAT gateways in the same private subnets as the EC2 instances.","A":"Create public NAT gateways in the same private subnets as the EC2 instances.","C":"Create public NAT gateways in public subnets in the same VPCs as the EC2 instances."},"question_images":[],"isMC":true,"timestamp":"2024-02-05 17:26:00","question_text":"A company wants to use NAT gateways in its AWS environment. The company's Amazon EC2 instances in private subnets must be able to connect to the public internet through the NAT gateways.\n\nWhich solution will meet these requirements?","answer":"C","topic":"1"},{"id":"g6Jm9LC62JJepBGL63tG","exam_id":31,"question_text":"A company has an organization in AWS Organizations. The company runs Amazon EC2 instances across four AWS accounts in the root organizational unit (OU). There are three nonproduction accounts and one production account. The company wants to prohibit users from launching EC2 instances of a certain size in the nonproduction accounts. The company has created a service control policy (SCP) to deny access to launch instances that use the prohibited types.\n\nWhich solutions to deploy the SCP will meet these requirements? (Choose two.)","isMC":true,"answer_images":[],"answer_description":"","discussion":[{"upvote_count":"12","content":"Selected Answer: BE\nMy vote is for BE","poster":"anikolov","timestamp":"1707304380.0","comment_id":"1143268"},{"content":"Selected Answer: BE\nI think it's B (directly attach) and E (attach via OU).","upvote_count":"6","comment_id":"1142825","poster":"mestule","timestamp":"1707261000.0"},{"poster":"MatAlves","content":"Selected Answer: BE\nB - Attach the SPC to the three accounts\nE - Creates an OU > moves the member accounts to OU > attach the SCP to OU\n\n\"If you apply an authorization policy (for example, a service control policy (SCP)), to the \nroot, it applies to all organizational units (OUs) and member accounts in the organization.\"\n\n\"A\" would also affect the one production account, which we clearly don't want. \n\nYou can \"attach an SCP to a root, OU, or account\"","upvote_count":"2","comment_id":"1285491","comments":[{"comment_id":"1285492","upvote_count":"2","content":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html","poster":"MatAlves","timestamp":"1726614540.0"}],"timestamp":"1726614540.0"},{"comment_id":"1202070","content":"Selected Answer: BE\nOnly the non-prods need to be limited.","poster":"sandordini","upvote_count":"4","timestamp":"1714056420.0"},{"content":"According to GPT-4 it's AE:\nA. Attach the SCP to the root OU for the organization. This approach will apply the SCP to all accounts under the organization, including both nonproduction and production accounts. However, without additional context or actions, this does not meet the requirement to exclude the production account from the restrictions.\n\nE. Create an OU for the required accounts. Attach the SCP to the OU. Move the nonproduction member accounts into the new OU. This is the correct approach as it directly addresses the requirement. By creating a separate OU for nonproduction accounts and attaching the SCP to this OU, you can specifically target the policy to only those accounts, effectively exempting the production account from the restrictions.","comment_id":"1153994","timestamp":"1708352460.0","upvote_count":"1","poster":"67a3f49"},{"comments":[{"content":"The link you provided says:\n\n\"If you apply an authorization policy (for example, a service control policy (SCP)), to the \nroot, it applies to all organizational units (OUs) and member accounts in the organization.\"\n\n\"A\" would also affect the one production account, which we clearly don't want.\n\nYou can \"attach an SCP to a root, OU, or account\"\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html","poster":"MatAlves","timestamp":"1726614420.0","comment_id":"1285490","upvote_count":"2"}],"content":"Selected Answer: AC\nAC - same answer\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html","timestamp":"1708177200.0","poster":"1Alpha1","comment_id":"1152567","upvote_count":"1"},{"timestamp":"1707404100.0","content":"Selected Answer: AD\nFrom Chat\n\nA. Attach the SCP to the root OU for the organization: Attaching the SCP to the root OU ensures that it applies to all member accounts within the organization, including both nonproduction and production accounts.\n\nD. Create an OU for the production account. Attach the SCP to the OU. Move the production member account into the new OU: By creating a separate OU for the production account and attaching the SCP to that OU, you can ensure that the SCP only affects the nonproduction accounts while allowing the production account to operate without restrictions.","poster":"Cali182","comment_id":"1144582","upvote_count":"2"},{"content":"CE should be the correct answer","timestamp":"1707150600.0","comment_id":"1141279","poster":"Andy_09","upvote_count":"1"}],"answer_ET":"BE","question_images":[],"choices":{"E":"Create an OU for the required accounts. Attach the SCP to the OU. Move the nonproduction member accounts into the new OU.","B":"Attach the SCP to the three nonproduction Organizations member accounts.","C":"Attach the SCP to the Organizations management account.","D":"Create an OU for the production account. Attach the SCP to the OU. Move the production member account into the new OU.","A":"Attach the SCP to the root OU for the organization."},"answers_community":["BE (89%)","7%"],"question_id":698,"answer":"BE","topic":"1","unix_timestamp":1707150600,"url":"https://www.examtopics.com/discussions/amazon/view/132876-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-02-05 17:30:00"},{"id":"JCvjrCSFkasezD4FqhzJ","question_images":[],"answers_community":["B (100%)"],"isMC":true,"answer_description":"","exam_id":31,"topic":"1","choices":{"D":"Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot.","C":"Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.","A":"Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.","B":"Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time."},"discussion":[{"comment_id":"697522","upvote_count":"48","comments":[{"comment_id":"789421","timestamp":"1674807600.0","poster":"LionelSid","upvote_count":"4","comments":[{"content":"What is \"DynamoDB service on the instance\" ?","upvote_count":"2","comment_id":"862501","poster":"piavik","timestamp":"1680731700.0"},{"timestamp":"1703502300.0","comment_id":"1105202","poster":"pentium75","upvote_count":"1","content":"DynamoDB is a native cloud service, there is no \"instance\" that you could \"stop\", or detach an \"EBS volume\" from."}],"content":"Yes, it is possible to take EBS snapshots of a DynamoDB table. The process for doing this involves the following steps:\n\nCreate a new Amazon Elastic Block Store (EBS) volume from the DynamoDB table.\n\nStop the DynamoDB service on the instance.\n\nDetach the EBS volume from the instance.\n\nCreate a snapshot of the EBS volume.\n\nReattach the EBS volume to the instance.\n\nStart the DynamoDB service on the instance.\n\nYou can also use AWS Data pipeline to automate the above process and schedule regular snapshots of your DynamoDB table.\n\nNote that, if your table is large and you want to take a snapshot of it, it could take a long time and consume a lot of bandwidth, so it's recommended to use the Global Tables feature from DynamoDB in order to have a Multi-region and Multi-master DynamoDB table, and you can snapshot each region separately."}],"timestamp":"1666021020.0","poster":"123jhl0","content":"Selected Answer: B\nA - DynamoDB global tables provides multi-Region, and multi-active database, but it not valid \"in case of data corruption\". In this case, you need a backup. This solutions isn't valid.\n**B** - Point in Time Recovery is designed as a continuous backup juts to recover it fast. It covers perfectly the RPO, and probably the RTO. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html\nC - A daily export will not cover the RPO of 15min.\nD - DynamoDB is serverless... so what are these EBS snapshots taken from???"},{"timestamp":"1671577860.0","comment_id":"751623","poster":"Buruguduystunstugudunstuy","comments":[{"upvote_count":"7","poster":"Buruguduystunstugudunstuy","comment_id":"751624","content":"***WRONG***\nOption A (configuring DynamoDB global tables) would not meet the RPO requirement, as global tables are designed to replicate data to multiple regions for high availability, but they do not provide a way to restore data to a specific point in time.\n\nOption C (exporting data to S3 Glacier) would not meet the RPO or RTO requirements, as S3 Glacier is a cold storage service with a retrieval time of several hours.\n\nOption D (scheduling EBS snapshots) would not meet the RPO requirement, as EBS snapshots are taken on a schedule, rather than continuously. Additionally, restoring a DynamoDB table from an EBS snapshot can take longer than 1 hour, so it would not meet the RTO requirement.","timestamp":"1671577920.0"}],"upvote_count":"14","content":"Selected Answer: B\nThe best solution to meet the RPO and RTO requirements would be to use DynamoDB point-in-time recovery (PITR). This feature allows you to restore your DynamoDB table to any point in time within the last 35 days, with a granularity of seconds. To recover data within a 15-minute RPO, you would simply restore the table to the desired point in time within the last 35 days.\n\nTo meet the RTO requirement of 1 hour, you can use the DynamoDB console, AWS CLI, or the AWS SDKs to enable PITR on your table. Once enabled, PITR continuously captures point-in-time copies of your table data in an S3 bucket. You can then use these point-in-time copies to restore your table to any point in time within the retention period.\n\n***CORRECT***\nOption B. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time."},{"timestamp":"1735915020.0","upvote_count":"1","poster":"satyaammm","content":"Selected Answer: B\nDynamoDB Point-In-Time-Recovery is the most suitable option here for recovering an oject.","comment_id":"1336051"},{"content":"Selected Answer: B\nAns B - just use the built-in DynamoDB PITR... simples...","comment_id":"1284044","poster":"PaulGa","timestamp":"1726394880.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: B\nWith point-in-time recovery, you can restore that table to any point in time during the last 35 days. After you enable point-in-time recovery, you can restore to any point in time from five minutes before the current time until 35 days ago. DynamoDB maintains incremental backups of your table.","poster":"ManikRoy","comment_id":"1202601","timestamp":"1714136760.0"},{"timestamp":"1705253580.0","content":"Selected Answer: B\nA: Scalability across regions which is not required\nC: Glacier exports and backup restore won't meet 1 hour RPO time\nD EBS for DynamoDB table? Sounds impractical\nB: DynamoDB point-in-time recovery is for this scenario.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html","comment_id":"1122719","upvote_count":"2","poster":"awsgeek75"},{"content":"Selected Answer: B\nThe best option to meet the RPO of 15 minutes and RTO of 1 hour is B) Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.\n\nThe key points:\n\nDynamoDB point-in-time recovery can restore to any point in time within the last 35 days. This supports an RPO of 15 minutes.\nRestoring from a point-in-time backup meets the 1 hour RTO.\nPoint-in-time recovery is specifically designed to restore DynamoDB tables with second-level granularity.","upvote_count":"1","comment_id":"977970","timestamp":"1691689740.0","poster":"Guru4Cloud"},{"content":"A. Global tables provide multi-region replication for disaster recovery purposes, they may not meet the desired RPO of 15 minutes without additional configuration and potential data loss.\n\nC. Exporting and importing data on a daily basis does not align with the desired RPO of 15 minutes.\n\nD. EBS snapshots can be used for data backup, they are not directly applicable to DynamoDB and cannot provide the desired RPO and RTO without custom implementation.\n\nIn comparison, option B utilizing DynamoDB's built-in point-in-time recovery functionality provides the most straightforward and effective solution for meeting the specified RPO of 15 minutes and RTO of 1 hour. By enabling PITR and restoring the table to the desired point in time, the company can recover the customer information with minimal data loss and within the required time frame.","timestamp":"1687356960.0","poster":"cookieMr","comment_id":"929612","upvote_count":"3"},{"content":"The answer is in the question. Read the question again!!! Option B. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.","poster":"Abrar2022","comment_id":"902373","upvote_count":"1","timestamp":"1684561920.0"},{"upvote_count":"1","content":"If there is anyone who is willing to share his/her contributor access, then please write to vinaychethi99@gmail.com","timestamp":"1682524980.0","comment_id":"881799","poster":"[Removed]"},{"upvote_count":"1","comment_id":"749344","content":"Selected Answer: B\nOption B","timestamp":"1671414420.0","poster":"career360guru"},{"timestamp":"1670953380.0","upvote_count":"1","content":"B is correct\nDynamoDB point-in-time recovery allows the solutions architect to recover the DynamoDB table to a specific point in time, which would meet the RPO of 15 minutes. This feature also provides an RTO of 1 hour, which is the desired recovery time objective for the application. Additionally, configuring DynamoDB point-in-time recovery does not require any additional infrastructure or operational effort, making it the best solution for this scenario.\nOption D is not correct because scheduling Amazon EBS snapshots for the DynamoDB table every 15 minutes would not meet the RPO or RTO requirements. While EBS snapshots can be used to recover data from a DynamoDB table, they are not designed to provide real-time data protection or recovery capabilities","poster":"Shasha1","comment_id":"744326"},{"content":"B is correct","upvote_count":"1","timestamp":"1669042500.0","poster":"Wpcorgan","comment_id":"723648"},{"content":"Selected Answer: B\nB is the answer","comment_id":"707105","poster":"SimonPark","timestamp":"1667035680.0","upvote_count":"1"},{"comment_id":"700582","content":"Selected Answer: B\nI think DynamoDB global tables also work here, but Point in Time Recovery is a better choice","poster":"BoboChow","timestamp":"1666335360.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"698572","timestamp":"1666138440.0","content":"I THINK B. \n https://dynobase.dev/dynamodb-point-in-time-recovery/","poster":"Kikiokiki"},{"comments":[{"timestamp":"1666021200.0","content":"DynamoDB is serverless, so no storage snapshots available. https://aws.amazon.com/dynamodb/","comment_id":"697524","upvote_count":"2","poster":"123jhl0"}],"timestamp":"1665923820.0","upvote_count":"1","comment_id":"696237","poster":"priya2224","content":"answer is D"}],"answer_ET":"B","question_text":"A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.\nWhat should the solutions architect recommend to meet these requirements?","question_id":699,"url":"https://www.examtopics.com/discussions/amazon/view/85603-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1665923820,"answer_images":[],"answer":"B","timestamp":"2022-10-16 14:37:00"},{"id":"D3fIzFIVEKIArxI9fCKa","unix_timestamp":1707504360,"question_id":700,"question_text":"A company’s website hosted on Amazon EC2 instances processes classified data stored in Amazon S3. Due to security concerns, the company requires a private and secure connection between its EC2 resources and Amazon S3.\n\nWhich solution meets these requirements?","topic":"1","answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/133462-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-02-09 19:46:00","answers_community":["A (100%)"],"answer":"A","exam_id":31,"question_images":[],"answer_description":"","discussion":[{"upvote_count":"7","poster":"Ashy1313","content":"Selected Answer: A\nA VPC endpoint enables customers to privately connect to supported AWS services .","comment_id":"1145706","timestamp":"1723221960.0"},{"comment_id":"1202073","poster":"sandordini","content":"Selected Answer: A\nI think this question asks about the connection not about authorization, and for a secure S3 connection (e.g. without internet exposure, etc. ) should be a VPC endpoint.","timestamp":"1729867800.0","upvote_count":"7"},{"poster":"Naveena_Devanga","timestamp":"1724247300.0","content":"D is the correct answer.","upvote_count":"1","comment_id":"1155648"},{"upvote_count":"3","poster":"Darshan07","timestamp":"1723545960.0","comment_id":"1149163","content":"Selected Answer: A\nA is the correct answer"}],"answer_ET":"A","choices":{"B":"Set up an IAM policy to grant read-write access to the S3 bucket.","D":"Set up an access key ID and a secret access key to access the S3 bucket.","A":"Set up S3 bucket policies to allow access from a VPC endpoint.","C":"Set up a NAT gateway to access resources outside the private subnet."}}],"exam":{"isImplemented":true,"provider":"Amazon","isBeta":false,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025","isMCOnly":true,"id":31},"currentPage":140},"__N_SSP":true}