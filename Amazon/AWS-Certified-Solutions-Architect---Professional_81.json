{"pageProps":{"questions":[{"id":"GRz9fZoh2HOg3jHkreNb","answer":"B","answer_images":[],"choices":{"C":"Move the entire website to Amazon S3 using the S3 website hosting feature. Remove all the web servers and have Amazon S3 communicate directly with the application servers in Amazon VPC.","A":"Set up a new Amazon EFS share, move all image and video files to this share, and then attach this new drive as a mount point to all existing servers. Create an Elastic Load Balancer with Auto Scaling general purpose instances. Enable Amazon CloudFront to the Elastic Load Balancer. Enable Cost Explorer and use AWS Trusted Advisor checks to continue monitoring the environment for future savings.","B":"Implement Auto Scaling with general purpose instance types and an Elastic Load Balancer. Enable an Amazon CloudFront distribution to Amazon S3 and move images and video files to Amazon S3. Reserve general purpose instances to meet base performance requirements. Use Cost Explorer and AWS Trusted Advisor checks to continue monitoring the environment for future savings.","D":"Use AWS Elastic Beanstalk to deploy the .NET application. Move all images and video files to Amazon EFS. Create an Amazon CloudFront distribution that points to the EFS share. Reserve the m4.4xl instances needed to meet base performance requirements."},"answers_community":["B (100%)"],"unix_timestamp":1569545880,"exam_id":32,"topic":"1","question_id":401,"url":"https://www.examtopics.com/discussions/amazon/view/5756-exam-aws-certified-solutions-architect-professional-topic-1/","question_text":"A company is running a .NET three-tier web application on AWS. The team currently uses XL storage optimized instances to store and serve the website's image and video files on local instance storage. The company has encountered issues with data loss from replication and instance failures. The Solutions Architect has been asked to redesign this application to improve its reliability while keeping costs low.\nWhich solution will meet these requirements?","question_images":[],"timestamp":"2019-09-27 02:58:00","answer_ET":"B","discussion":[{"comments":[{"poster":"Moon","upvote_count":"4","comment_id":"14261","timestamp":"1632283320.0","content":"great analysis."},{"timestamp":"1634937600.0","poster":"Kopa","content":"Also remaining costly instances stay the same!!","upvote_count":"1","comment_id":"389376"},{"timestamp":"1635096060.0","content":"Only caveat with this analysis is that \"currently uses XL storage optimized instances to store and serve image and video files\". If S3 and CloudFront is used, where do the EC2 instances and the load balancer fit in?\nBut Elastic Beanstalk can be used with EFS. Does it matter if the languahe is .NET? So the answer should be D.","comment_id":"392410","upvote_count":"1","poster":"DashL"}],"comment_id":"12844","timestamp":"1632078780.0","poster":"donathon","content":"B\nA: EFS is more than 10 times more expensive than S3 although it has better performance. This is where CloudFront comes in to mitigate the performance impact caused by S3.\nC: S3 does not suppose server side scripting (.net).\nD: Cloudfront origin must be S3 or HTTP based. EFS is not HTTP.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html","upvote_count":"60"},{"timestamp":"1632370560.0","poster":"AWSPro24","content":"This is .NET and talking tiers not microservices so likely on Windows Server EC2 instances. EFS is not great for Windows. It can be connected by SMB but this is not recommended. FSX is preferable where Windows shares are needed. Supports SMB and NTFS natively.","upvote_count":"6","comments":[{"timestamp":"1635976620.0","content":"Good observation","comment_id":"430252","upvote_count":"2","poster":"AWS_Noob"}],"comment_id":"50228"},{"upvote_count":"1","timestamp":"1664813520.0","content":"Selected Answer: B\nBased on all comments","comment_id":"685652","poster":"dmscountera"},{"comment_id":"513210","content":"so many duplicated questions","timestamp":"1640858400.0","upvote_count":"1","poster":"GeniusMikeLiu"},{"poster":"cldy","comment_id":"498679","upvote_count":"1","timestamp":"1639145520.0","content":"B. Implement Auto Scaling with general purpose instance types and an Elastic Load Balancer. Enable an Amazon CloudFront distribution to Amazon S3 and move images and video files to Amazon S3. Reserve general purpose instances to meet base performance requirements. Use Cost Explorer and AWS Trusted Advisor checks to continue monitoring the environment for future savings."},{"content":"B is perfect","timestamp":"1638635760.0","comment_id":"493802","poster":"AzureDP900","upvote_count":"1"},{"upvote_count":"1","poster":"acloudguru","comment_id":"489099","content":"such question seems easier than those ones, hope I can get such one in my exam","timestamp":"1638100380.0"},{"upvote_count":"1","poster":"andylogan","comment_id":"450716","content":"It's B","timestamp":"1636101180.0"},{"comment_id":"409528","content":"I'll go with B","poster":"WhyIronMan","upvote_count":"1","timestamp":"1635668280.0"},{"timestamp":"1635131700.0","comment_id":"406650","poster":"Akhil254","content":"B COrrect","upvote_count":"1"},{"content":"B is correct","poster":"Waiweng","timestamp":"1634855460.0","comment_id":"344261","upvote_count":"2"},{"comment_id":"289557","content":"I am going with B","upvote_count":"2","poster":"Kian1","timestamp":"1634576760.0"},{"comment_id":"283123","timestamp":"1634488860.0","poster":"Ebi","upvote_count":"4","content":"B is my choice"},{"timestamp":"1634436000.0","comment_id":"275064","poster":"gookseang","content":"seems B","upvote_count":"2"},{"comment_id":"266563","timestamp":"1634049300.0","poster":"sanjaym","upvote_count":"1","content":"B for sure"},{"upvote_count":"1","timestamp":"1633735500.0","content":"Correct answer is B. EFS more expensive then S3.","poster":"T14102020","comment_id":"242398"},{"timestamp":"1633682460.0","comment_id":"229428","content":"I'll go with B","upvote_count":"2","poster":"jackdryan"},{"timestamp":"1633618320.0","comment_id":"228640","content":"B is correct. Not C because no word on replication or CloudFront to accelerate retrieval.","upvote_count":"1","poster":"Bulti"},{"comment_id":"149438","poster":"fullaws","content":"B is correct","upvote_count":"2","timestamp":"1633508940.0"},{"poster":"shputhan","content":"Ans B \nAgree with Donathon's comments","upvote_count":"2","timestamp":"1633155540.0","comment_id":"143233"},{"upvote_count":"2","comment_id":"140648","poster":"Anila_Dhharisi","content":"B is correct","timestamp":"1633132260.0"},{"poster":"noisonnoiton","content":"B Acceptable","timestamp":"1632753420.0","comment_id":"136905","upvote_count":"2"},{"upvote_count":"2","poster":"NikkyDicky","timestamp":"1632647760.0","comment_id":"134235","content":"B more likely"}],"isMC":true,"answer_description":""},{"id":"Y0DvrT3SfK4NqeWYsCt4","timestamp":"2019-09-12 13:35:00","choices":{"B":"Write a bash script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions.","C":"Write a CloudFormation template describing the application's infrastructure in the resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application.","A":"Write a bash script that uses the AWS CLI to query the current state in one region and output a JSON representation. Pass the JSON representation to the AWS CLI, specifying the --region parameter to deploy the application to other regions.","D":"Write a CloudFormation template describing the application's infrastructure in the Resources section. Use a CloudFormation stack set from an administrator account to launch stack instances that deploy the application to other regions."},"url":"https://www.examtopics.com/discussions/amazon/view/5113-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"discussion":[{"content":"D\nA stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set's AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that template requires.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html","upvote_count":"34","comment_id":"12847","timestamp":"1632206220.0","poster":"donathon"},{"comments":[{"poster":"jimmy_sticks","content":"based on your link it is C","comment_id":"123336","timestamp":"1633296300.0","upvote_count":"2","comments":[{"poster":"shammous","comment_id":"279907","upvote_count":"1","content":"There is no --regions parameter. The right parameter is --region. Also, you need to use CFo stackset to deploy stacks to other regions (MOST efficiently)\n\"AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.\"\nRef: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html","timestamp":"1635691860.0"}]}],"upvote_count":"5","comment_id":"10778","content":"d:\n\nhttps://sanderknape.com/2017/07/cloudformation-stacksets-automated-cross-account-region-deployments/","timestamp":"1632114840.0","poster":"awsec2"},{"poster":"SkyZeroZx","content":"Selected Answer: D\nAnswer is D. \"Stack set\" is the key.","comment_id":"926235","timestamp":"1687032360.0","upvote_count":"1"},{"timestamp":"1664813520.0","comment_id":"685654","poster":"dmscountera","content":"Selected Answer: D\nBased on all comments","upvote_count":"2"},{"timestamp":"1644252720.0","content":"DDDDDDDDDDD","comment_id":"542550","upvote_count":"1","poster":"jj22222"},{"content":"D. Write a CloudFormation template describing the applicationג€™s infrastructure in the Resources section. Use a CloudFormation stack set from an administrator account to launch stack instances that deploy the application to other regions.","upvote_count":"2","timestamp":"1639218840.0","comment_id":"499332","poster":"cldy"},{"comment_id":"493804","content":"D is right answer","poster":"AzureDP900","timestamp":"1638636000.0","upvote_count":"1"},{"content":"It's D","poster":"andylogan","timestamp":"1636300380.0","comment_id":"450721","upvote_count":"1"},{"content":"I'll go with D","upvote_count":"3","poster":"WhyIronMan","timestamp":"1636040340.0","comment_id":"409532"},{"content":"it's D create cloud formation Stack set first","upvote_count":"3","timestamp":"1635984900.0","comment_id":"344263","poster":"Waiweng"},{"comment_id":"289560","timestamp":"1635812700.0","upvote_count":"2","poster":"Kian1","content":"I will go with D"},{"upvote_count":"5","content":"I go with D","comment_id":"283131","timestamp":"1635798780.0","poster":"Ebi"},{"upvote_count":"2","poster":"sanjaym","timestamp":"1635244320.0","content":"D for sure.","comment_id":"266564"},{"timestamp":"1635212280.0","comment_id":"264506","poster":"halfdeaf","content":"Answer is D. \"Stack set\" is the key.\n\nA stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template.","comments":[{"upvote_count":"1","timestamp":"1658883000.0","comment_id":"637705","content":"excellent keyword","poster":"hilft"},{"poster":"lalitsrana","timestamp":"1635480780.0","content":"A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set's AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that template requires.","upvote_count":"1","comment_id":"278002"}],"upvote_count":"2"},{"content":"D - quoting to the first paragraph of https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html","upvote_count":"1","poster":"elf78","comment_id":"261307","timestamp":"1635082800.0"},{"content":"D.\ncreate-stack doesn't have --regions parameter.\ncreate-stack-instances has --regions parameter, but it's to add stack instances to the stack set. So it's first parameter is --stack-set-name.","upvote_count":"1","timestamp":"1635080760.0","poster":"newme","comment_id":"244139"},{"timestamp":"1635003060.0","content":"Correct answer is D. We need to create a stack set first to create stack instances using the CloudFormation template associated it. You cannot directly create a stack in different regions without first creating a stack set.","comment_id":"242401","upvote_count":"1","poster":"T14102020"},{"content":"c IS THE OLD WAY OF DOING IT .. Thus D ... https://surevine.com/creating-cloudformation-stacks-in-multiple-aws-regions-with-common-resources/","timestamp":"1634830140.0","poster":"petebear55","upvote_count":"2","comment_id":"240341"},{"comment_id":"240340","timestamp":"1634744160.0","content":"D https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html","poster":"petebear55","upvote_count":"1"},{"upvote_count":"2","poster":"petebear55","timestamp":"1634470500.0","content":"I know others have raised concerns about donathon in the past as well ... I'm becoming concerned that he is just deliberately giving wrong answers out to throw people ?","comment_id":"240101"},{"timestamp":"1634340420.0","comment_id":"229429","upvote_count":"3","content":"I'll go with D","poster":"jackdryan"},{"upvote_count":"1","content":"Answer is D because we need to create a stack set first to create stack instances using the CloudFormation template associated it. You cannot directly create a stack in different regions without first creating a stack set.","poster":"Bulti","comments":[{"comment_id":"279887","upvote_count":"1","poster":"shammous","content":"So C not D based on that.","timestamp":"1635683280.0"}],"timestamp":"1634137140.0","comment_id":"228669"},{"content":"D is correct\n\nhttps://ubertasconsulting.com/2020/04/06/blog-9-cross-region-stack-management/","poster":"AK2020","comment_id":"218317","upvote_count":"1","timestamp":"1633893960.0"},{"timestamp":"1633525080.0","poster":"a724412639","content":"C\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started-create.html\n-----------------\naws cloudformation create-stack-instances --stack-set-name my-awsconfig-stackset --accounts '[\"account_ID_1\",\"account_ID_2\"]' --regions '[\"region_1\",\"region_2\"]' --operation-preferences FailureToleranceCount=0,MaxConcurrentCount=1\n-----------------","upvote_count":"4","comment_id":"179839"},{"poster":"a724412639","comments":[{"upvote_count":"1","timestamp":"1633914240.0","comment_id":"224946","poster":"thiennx","content":"stackset here -> D true ?"}],"timestamp":"1633506480.0","comment_id":"179838","upvote_count":"2","content":"C\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started-create.html\naws cloudformation create-stack-instances --stack-set-name my-awsconfig-stackset --accounts '[\"account_ID_1\",\"account_ID_2\"]' --regions '[\"region_1\",\"region_2\"]' --operation-preferences FailureToleranceCount=0,MaxConcurrentCount=1"},{"timestamp":"1633502460.0","comment_id":"150015","upvote_count":"1","poster":"Kibana01","content":"Option D, is more aligned to the conditions giving in the question"},{"timestamp":"1633436100.0","upvote_count":"1","content":"Answer is D","poster":"[Removed]","comment_id":"144189"},{"upvote_count":"1","comment_id":"136910","timestamp":"1633411020.0","poster":"noisonnoiton","content":"D acceptable\nuse stack set"},{"timestamp":"1633399020.0","upvote_count":"1","poster":"noisonnoiton","comment_id":"136906","content":"D acceptable"},{"upvote_count":"1","comment_id":"134236","poster":"NikkyDicky","timestamp":"1633348500.0","content":"D most likely"},{"content":"Option D.\nwe need to create an IAM role called AWSCloudFormationStackSetAdministrationRole in what is called the “administrator account”. This is the account from which we create the StackSet and from where we'll deploy the stacks in other accounts and regions. Next, we have the AWSCloudFormationStackSetExecutionRole IAM role. This role must exist in all accounts where we're going to provision stacks. The first role will assume the latter role.","timestamp":"1633249680.0","poster":"JAWS1600","upvote_count":"1","comment_id":"95115"},{"timestamp":"1632911580.0","content":"New Question: A company wants to analyse a log data using date ranges with a custom application running on AWS. The application generates about 10 GB of data every day, which is expected to grow. A solution architect is tasked with storing the data in S3 and using Amazon Anthena to query data.\nWhich combination of steps will ensure optimal performance as the data grows? (Choose 2)\na> Store each object in S3 with a random string at the front of each key.\nb.store data in multiple s3 buckets\ncstore the data in s3 in a columnal format, such as apache parquet or apache orc)\nd. store data in amazon s3 in objects that are smaller than 10mb\ne. store the data using apache hive partitioning in s3 using a key that includes a date such as dt=\n2019-2\nAnswer : C & E (https://docs.aws.amazon.com/athena/latest/ug/partitions.html & https://docs.aws.amazon.com/athena/latest/ug/convert-to-columnar.html)","poster":"subdas","comment_id":"84465","upvote_count":"4"},{"timestamp":"1632890460.0","upvote_count":"2","content":"New Question: A company wants to analyse a log data using date ranges with a custom application running on AWS. The application generates about 10 GB of data every day, which is expected to grow. A solution architect is tasked with storing the data in S3 and using Amazon Anthena to query data.\nWhich combination of steps will ensure optimal performance as the data grows? (Choose 2)\na> Store each object in S3 with a random string at the front of each key.\nb.store data in multiple s3 buckets\ncstore the data in s3 in a columnal format, such as apache parquet or apache orc)\nd. store data in amazon s3 in objects that are smaller than 10mb\ne. store the data using apache hive partitioning in s3 using a key that includes a date such as dt=\n2019-2","comment_id":"69918","poster":"EVAAWS"},{"upvote_count":"3","comment_id":"49524","content":"Answer is D\n\"CloudFormation stack set\"\nhttps://docs.aws.amazon.com/cli/latest/reference/cloudformation/create-stack-instances.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started-create.html","poster":"amog","timestamp":"1632866340.0"},{"poster":"Zek","upvote_count":"5","timestamp":"1632714780.0","comment_id":"47921","content":"I will go with D. To add to siasiasia's point, --regions is a valid parameter for create-stack-instances and not for create-stack-set. Option C would be wrong because it talks about creating the cloudformation stack instead of creating cloudformation stack set and stack instances. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started-create.html"},{"timestamp":"1632646140.0","upvote_count":"1","comment_id":"41246","poster":"markpark","content":"D is the answer"},{"content":"D is the right one, \nC is wrong, the --regions is not a valid parameter.","comment_id":"36934","comments":[{"comment_id":"36935","upvote_count":"1","content":"sorry I maybe wrong, https://docs.aws.amazon.com/cli/latest/reference/cloudformation/create-stack-instances.html","poster":"siasiasia","timestamp":"1632620640.0"}],"timestamp":"1632593820.0","upvote_count":"1","poster":"siasiasia"},{"timestamp":"1632230460.0","comment_id":"23055","comments":[{"content":"D most efficient way to deploy?","timestamp":"1632483180.0","poster":"johannes756","comment_id":"26948","upvote_count":"1"},{"poster":"tan9","content":"I will go for C too.\n\nIsn't create using CLI with all target regions specified by `--regions` parameter MOST EFFICIENT?","timestamp":"1632548580.0","upvote_count":"4","comment_id":"29499"}],"upvote_count":"3","content":"Can i ask why not c?","poster":"Requium"}],"answer":"D","unix_timestamp":1568288100,"question_id":402,"answers_community":["D (100%)"],"answer_description":"","question_images":[],"topic":"1","isMC":true,"answer_images":[],"question_text":"A company has developed a web application that runs on Amazon EC2 instances in one AWS Region. The company has taken on new business in other countries and must deploy its application into other regions to meet low-latency requirements for its users. The regions can be segregated, and an application running in one region does not need to communicate with instances in other regions.\nHow should the company's Solutions Architect automate the deployment of the application so that it can be MOST efficiently deployed into multiple regions?","answer_ET":"D"},{"id":"dFlBBt4q22rimAqdHoZG","choices":{"B":"Set up an AWS Storage Gateway, tape gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the tape gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video in the tape gateway, retrieve the required metadata, and push the metadata into the MAM solution.","D":"Set up an Amazon EC2 instance that runs the OpenCV libraries. Copy the videos, images, and face catalog from the on-premises library into an Amazon EBS volume mounted on this EC2 instance. Process the videos to retrieve the required metadata, and push the metadata into the MAM solution, while also copying the video files to an Amazon S3 bucket.","A":"Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution.","C":"Configure a video ingestion stream by using Amazon Kinesis Video Streams. Use the catalog of faces to build a collection in Amazon Rekognition. Stream the videos from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed videos. Then, use a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Configure the stream to store the videos in Amazon S3."},"answer":"A","topic":"1","answer_description":"","answer_ET":"A","question_id":403,"discussion":[{"poster":"donathon","timestamp":"1632269520.0","upvote_count":"32","comment_id":"13204","comments":[{"timestamp":"1632582540.0","poster":"pra276","upvote_count":"9","comment_id":"23116","comments":[{"comment_id":"213968","timestamp":"1634248620.0","poster":"zijdldfddidxfqpdzf","upvote_count":"1","content":"(MINIMAL disruption to the existing system), in order to move forward with option C we have to edit MAM software to utilize Kenisis video stream library, so i prefer option A"}],"content":"Answer is C: Vidoes are stored in tape. I don't see they mentioned tape format etc."},{"comment_id":"604171","poster":"Ddssssss","content":"I think the issue with B is that no where does it say that Rek can ingest S3 Tape Archives. It can consume files in S3, but no mention of Tape gateway storage. Its a virtual tape library and when ejected from your backup application they are archived to glacier.","upvote_count":"1","timestamp":"1653002340.0"}],"content":"A\nA\\B: By replacing the physical tape library with file gateway, it has the least amount of management and disruption. It would require us to restore the 30TB to another format (NFS\\SMB style) in order to put it on the file gateway which cannot serve as a VTL. I don’t think it’s possible for Amazon Rekognition to process the video in the tape gateway which is on premise or access the data from S3 directly.\nA: File gateway supports Linux clients connecting to the gateway using Network File System (NFS) versions 3 and 4.1 for Linux clients, and supports Windows clients connecting to the gateway using Server Message Block (SMB) versions 2 and 3. \nB: No. You cannot access virtual tape data using Amazon S3 or Amazon S3 Glacier APIs. However, you can use the tape gateway APIs to manage your virtual tape library and your virtual tape shelf. \nhttps://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6\nC: This is not possible because the data are in tape format.\nD: This loosk more like the job for Rekognition."},{"comment_id":"22335","content":"A is wrong. Analysis stored video using AWS Rekognition is an async process. Therefore you need additional component to inject result to the MAM solution. Example here uses SQS and SNS to get the recognition result (https://docs.aws.amazon.com/rekognition/latest/dg/video.html)\n\nC is correct as it is almost the same as described here: https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video.html","timestamp":"1632369180.0","poster":"Frank1","upvote_count":"15","comments":[{"comment_id":"50207","poster":"AWSPro24","content":"Sorry, meant to include this link to a description of how the async API works. Says you can use SNS / SQS OR a Lambda function.","upvote_count":"1","timestamp":"1632723480.0"},{"content":"While C is possible, it is definitely not \"LEAST amount of ongoing management overhead\" with so many moving parts.","upvote_count":"1","timestamp":"1633768680.0","comment_id":"143913","poster":"MultiAZ"},{"poster":"AWSPro24","timestamp":"1632704520.0","comments":[{"poster":"Joeylee","content":"https://docs.aws.amazon.com/rekognition/latest/dg/video.html\nRekognition is very picky on video size & format. Better use streaming","comment_id":"75590","comments":[{"timestamp":"1635492480.0","upvote_count":"1","poster":"WillCloud","content":"In your link: \nWith Amazon Rekognition Video, you can detect labels, faces, people, celebrities, and adult (suggestive and explicit) content in videos that are stored in an Amazon Simple Storage Service (Amazon S3) bucket.","comment_id":"448177"}],"timestamp":"1632760740.0","upvote_count":"1"}],"comment_id":"50204","upvote_count":"1","content":"I'm not so sure you are correct. The following says you can use a Lambda function to get the metadata. You use SNS/SQS to get the state of the analysis process.\nhttps://aws.amazon.com/rekognition/faqs/#Video_Analytics\n\nIn addition, why would we convert stored video into streaming video when Rek has the ability to analyze stored video https://docs.aws.amazon.com/rekognition/latest/dg/video.html The question says \"move the MAM solution video content directly from its current file system\" file-to-file/object seems more direct than streaming the files in through Kinesis.\nhttps://docs.aws.amazon.com/rekognition/latest/dg/video.html"}]},{"content":"Selected Answer: A\nTape Gateway:\nAWS Storage Gateway tape gateway enables you to replace your on-premises tape infrastructure with virtual tape libraries (VTLs) in AWS. It integrates with your existing backup infrastructure and allows you to store and retrieve virtual tapes using Amazon S3 or Glacier storage.\nUse Case: Tape gateway is suitable when you want to replace or extend your on-premises tape infrastructure with a virtual tape library in AWS. It's commonly used for backup and archiving purposes.\nFile Gateway:\nAWS Storage Gateway file gateway provides access to your on-premises file-based data through NFS and SMB protocols. It allows you to store files as objects in Amazon S3, while still providing the same file-based access for applications and users.\nUse Case: File gateway is ideal when you want to integrate your on-premises file-based applications and workflows with AWS storage. It's commonly used for hybrid cloud scenarios, content distribution, and file-based workloads.\n\nThen A better than B","comment_id":"926238","timestamp":"1687032660.0","poster":"SkyZeroZx","upvote_count":"1"},{"timestamp":"1676298660.0","poster":"zWarez","comment_id":"807486","upvote_count":"1","content":"A. B was wrong. You cannot process the file on Tape like you do on disk. C is not necessary we don't need real time video processing here. D requires a lot of efforts."},{"timestamp":"1664813580.0","upvote_count":"2","comment_id":"685655","poster":"dmscountera","content":"Selected Answer: A\nBased on all comments"},{"content":"Selected Answer: A\nI don't understand why people are going with B, Answer of this question is definitely A. You can't directly fetch the media files from your tape gateway in real-time since this is backed up using Glacier. People are getting confused with the tape word in question. This problem can't be solved using tape gateway and will need file gateway.","poster":"bihani","timestamp":"1663121220.0","comment_id":"668547","upvote_count":"3"},{"upvote_count":"1","comment_id":"657070","content":"Source for Rekognition is S3 or Kinesis\n=> B wrong, Rekognition can not rertrive video on tape gateway \n=> A and C is right, but \"LEAST amount of ongoing management\" => pick A, to re-use MAM feature of existing system","poster":"kadev","comments":[{"content":"Tape drive <=== extract by MMA ==> put to file gateway ==> S3","comment_id":"657074","upvote_count":"1","poster":"kadev","timestamp":"1662101280.0"}],"timestamp":"1662101160.0"},{"upvote_count":"1","timestamp":"1661957220.0","comment_id":"655204","poster":"Sizuma","content":"C IS CORRECT"},{"comment_id":"616184","poster":"Ddssssss","upvote_count":"1","content":"Selected Answer: A\nMy take is that the MAM tool and the company don't \"NEED\" the archives to be on tape or virtual tape. They are just on tape now as it was a legacy long term storage plan. They simply need the files in S3 and the MAM tools to absorb the metadata from REK. No need for B. Answer A.","timestamp":"1655206680.0"},{"content":"Selected Answer: B\nWhy not B? The MAM software in the question is already designed to work with the existing tape catalog, introducing the File Gateway from the and A would force us to re-write the catalog concept for the MAM app.","upvote_count":"1","timestamp":"1650454440.0","poster":"mirnuj_atom","comment_id":"588602"},{"timestamp":"1641697740.0","comment_id":"519878","poster":"frankzeng","content":"B. These movies are archived on tape in an on-premises tape library\nand accessed using a Media Asset Management (MAM) system. It means after moving to AWS, still use MAM to access the tapes. Tape Gateway enables you to replace using physical tapes on premises with virtual tapes in AWS without changing existing backup workflows","upvote_count":"1"},{"comment_id":"504438","comments":[{"upvote_count":"1","timestamp":"1639858560.0","poster":"vbal","comment_id":"504445","comments":[{"poster":"Cal88","content":"You don’t need to know about MAM to answer the question since its not relevant\nThe question is on how would you extract the meta data from the videos.\nPutting MAM there is just to distract you and make you think its a hard question.\nThis question is mainly about storage gateways , Rekognition and Kinesis Video Stream\nSo think about it , if you need to extract faces , objects etc from videos what’s the best way? AWS Rekognition\nSo that will eliminate answer D for sure \nNow what’s the best way to copy your videos to AWS to be used with Rekognition?\nIf you know about storage gateways you will know that when using a tape virtual gateway your backup will be sent to glacier and stored in something called tape archive so Rekognition can’t read a tape , it reads media (ex mp4 , jpg etc) so that will eliminate B as well\nC can be done and will work but remember in the question its says minimal change\nSo the correct answer is A as most people in the comment are saying","comment_id":"707376","timestamp":"1667066280.0","upvote_count":"1"}],"content":"I am confused with A after reading more about Media Asset Management. This is ridiculously hard..How could someone be tested on something like Media Asset Management provides you the files...???"}],"content":"No One really talking about B here ...its shocking.. wht am I missing...? the following picture suggests B is absolutely correct. https://aws.amazon.com/storagegateway/vtl/","upvote_count":"2","timestamp":"1639857960.0","poster":"vbal"},{"comment_id":"496587","content":"A. Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution.","poster":"cldy","timestamp":"1638944820.0","upvote_count":"1"},{"timestamp":"1638636240.0","comment_id":"493807","poster":"AzureDP900","content":"archived on tape in an on-premises tape library is keyword, I will go with A","upvote_count":"1"},{"upvote_count":"1","comment_id":"483807","poster":"acloudguru","timestamp":"1637546460.0","content":"Selected Answer: A\nB, there is no need to use Tape at all"},{"upvote_count":"2","comment_id":"455787","poster":"StelSen","timestamp":"1636054980.0","content":"I chose A. Although Ans A & C seems good solution. The Kinesis Video Stream usecase is for mostly live streaming (such as Camera). Not sure streaming a file is a good use case. Those who support Option-C, can you please paste the architecture link where Kinesis Video Streams process from Video File?"},{"timestamp":"1635880020.0","content":"It's A","poster":"andylogan","comment_id":"450723","upvote_count":"1"},{"upvote_count":"2","poster":"WhyIronMan","timestamp":"1635490860.0","comment_id":"409550","content":"I'll go with A"},{"poster":"Akhil254","upvote_count":"1","content":"A Correct","comment_id":"406655","timestamp":"1635452280.0"},{"comments":[{"content":"great point btw","comment_id":"501675","timestamp":"1639516860.0","poster":"rb39","upvote_count":"1"},{"poster":"vbal","timestamp":"1639858320.0","comment_id":"504443","upvote_count":"1","content":"https://aws.amazon.com/storagegateway/vtl/"}],"poster":"blackgamer","timestamp":"1635184140.0","upvote_count":"2","comment_id":"344955","content":"A is the answer. B is wrong as there is no tape gateway applince."},{"timestamp":"1635182460.0","content":"I'll go for A","comment_id":"344265","poster":"Waiweng","upvote_count":"2"},{"timestamp":"1635113700.0","upvote_count":"2","poster":"Kian1","content":"going with A","comment_id":"289565"},{"upvote_count":"5","content":"my answer is A","poster":"Ebi","comment_id":"283138","timestamp":"1635016920.0"},{"content":"A is the answer you cannot process videos in a tape gateway","upvote_count":"2","poster":"kopper2019","timestamp":"1634751300.0","comment_id":"273413"},{"upvote_count":"1","timestamp":"1634481720.0","comment_id":"242405","content":"A is correct answer. Tape -> S3","poster":"T14102020"},{"content":"I'll go with A","upvote_count":"2","comment_id":"229433","poster":"jackdryan","timestamp":"1634481540.0"},{"content":"Answer is A- Creates least amount of disruption to the existing MAM solution. MAM software just need to extract the video from the tape and push them to the File Storage Gateway. C is an option but you would need to change the existing MAM solution to integrate with Kinesis Data Stream which is more effort than what it needs to do in A.","timestamp":"1634289480.0","upvote_count":"5","comment_id":"228697","poster":"Bulti"},{"comments":[{"content":"correction, it's the MAM that extracts the files from the tape backups, so it needs to push them as files. Answer is A","upvote_count":"2","poster":"lostri","comment_id":"226665","timestamp":"1634286420.0"}],"timestamp":"1634275200.0","poster":"lostri","comment_id":"226661","upvote_count":"2","content":"Answer is B, videos are already available in tape backups"},{"timestamp":"1634137260.0","upvote_count":"1","content":"C is incorrect, because you need to change the source code of the MAM solution to work with kinesis (cause disruption), if it say using another ec2 to get the video from MAM solution and then push to kinesis, will be different case.","poster":"fullaws","comment_id":"149468"},{"content":"A is correct, MAM solution include feature of compressing into tape, and extract back the data from tape.","timestamp":"1633791360.0","poster":"fullaws","upvote_count":"1","comment_id":"149462"},{"timestamp":"1633637820.0","content":"A mentions file gateway, whereas, on-prem is Tape library. Is this possible?","poster":"IAmNotLambda","upvote_count":"1","comment_id":"138630"},{"poster":"noisonnoiton","content":"A acceptable","comment_id":"136921","timestamp":"1633604100.0","comments":[{"poster":"noisonnoiton","upvote_count":"6","comment_id":"143786","timestamp":"1633712700.0","content":"revised C,,"}],"upvote_count":"2"},{"poster":"NikkyDicky","timestamp":"1633185240.0","comment_id":"131791","content":"looks like a trick question. A&B must be wrong since there is no Javaascript Rekognition library for videos stored on s3 (https://docs.aws.amazon.com/rekognition/latest/dg/video-analyzing-with-sqs.html)\nSo most likely C, even though the answer is missing info on streaming the video from MAM and data stream for consuming meta data","upvote_count":"2","comments":[{"timestamp":"1633239840.0","content":"The company has a high-speed AWS Direct Connect connection with AWS and would like to move the MAM solution video content directly from its current file system","comments":[{"poster":"xhova","timestamp":"1633413420.0","upvote_count":"2","comment_id":"136803","content":"C is correct"}],"upvote_count":"1","comment_id":"136801","poster":"xhova"},{"content":"There is reference about Javascript SDK for rekognition actually. check this out.\n\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Rekognition.html","timestamp":"1634146620.0","poster":"proxyolism","upvote_count":"3","comment_id":"167126"}]},{"comment_id":"121899","timestamp":"1633125660.0","upvote_count":"1","content":"A is correct","poster":"cloud4gr8"},{"comment_id":"118606","timestamp":"1632993540.0","comments":[{"upvote_count":"1","comment_id":"167134","timestamp":"1634227740.0","poster":"proxyolism","content":"I thought B is the answer as you said TAPE GATEWAY mentioned, but It was not. It is certain that MAM refers TAPE library; so the answer should be A as chicagomassageseeker said. using MAM solution to extract videos fro archive and after pushes Storage Gateway is set by Tape Gateway?? It is worthless."}],"content":"Answer is A. You may think B is the answer because it has 'Tape gateway\" but its not.\n\nThis is how I interpret A. \"Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive (ON PREMISE TAPE LIBRARY and push them into the file gateway. \n\nSo essentially you are decompressing the on premise tape library files into a brand new AWS storage fate, file Gateway. Decompression is needed because you cannot access a single file directly in a tape which is compressed system.","upvote_count":"4","poster":"chicagomassageseeker"},{"poster":"Shawn1","comment_id":"111534","timestamp":"1632970500.0","content":"Answer A. From Current file system -> Storage gateway (A or B); Rekognition Video process a video stored in an Amazon S3 bucket -> S3 (A not B) https://aws.amazon.com/rekognition/faqs/?nc=sn&loc=7","upvote_count":"1"},{"poster":"oatif","content":"the answer is b, it is in a tape format, in a tape library. hence you would need a tape appliance to move the data to aws.","upvote_count":"5","comment_id":"103534","timestamp":"1632916500.0"},{"timestamp":"1632869220.0","poster":"JAWS1600","content":"Key work is Minimimal distruption. For this A is the best option. If we user option C, It would require modification in MAM solution to send data to kinesis stream. Also, the last part of option C \"stream consumer\" is looking fishy. As per AWS docs, we woul d need to use Kinesis data streams as consumer, or custome consumer, such as lambda to write in to MAM solution.","comment_id":"91552","upvote_count":"2"},{"upvote_count":"3","poster":"amog","content":"Should be A\n\"tape library\" => Could be in tape format","comment_id":"49530","timestamp":"1632686580.0"},{"timestamp":"1632596400.0","comment_id":"41328","poster":"markpark","upvote_count":"2","content":"A is the answer"},{"content":"it shows in an on-premises tape library and referenced by a Media Asset\nManagement (MAM) system-means referenced by a Media So C is the answer","upvote_count":"6","comment_id":"22988","poster":"sara_an","timestamp":"1632531360.0"},{"poster":"awspro","comment_id":"13009","upvote_count":"1","content":"Why B ?","timestamp":"1632262080.0"},{"timestamp":"1632100980.0","upvote_count":"3","comment_id":"12923","poster":"awsdog","content":"A is Good"},{"timestamp":"1632096840.0","comment_id":"12326","upvote_count":"1","poster":"awsec2","content":"Why B ?"}],"exam_id":32,"answer_images":[],"isMC":true,"question_text":"A media company has a 30-TB repository of digital news videos. These videos are stored on tape in an on-premises tape library and referenced by a Media Asset\nManagement (MAM) system. The company wants to enrich the metadata for these videos in an automated fashion and put them into a searchable catalog by using a MAM feature. The company must be able to search based on information in the video, such as objects, scenery items, or people's faces. A catalog is available that contains faces of people who have appeared in the videos that include an image of each person. The company would like to migrate these videos to\nAWS.\nThe company has a high-speed AWS Direct Connect connection with AWS and would like to move the MAM solution video content directly from its current file system.\nHow can these requirements be met by using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system?","timestamp":"2019-09-23 20:29:00","answers_community":["A (89%)","11%"],"unix_timestamp":1569263340,"url":"https://www.examtopics.com/discussions/amazon/view/5618-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[]},{"id":"mShyPyemXO9bnoR7IKAZ","isMC":true,"answer_description":"Reference:\nhttps://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","timestamp":"2019-09-18 06:22:00","answer":"B","answer_ET":"B","discussion":[{"timestamp":"1632569040.0","comment_id":"13800","upvote_count":"19","poster":"donathon","content":"B\nA: You cannot optimize workloads if you lift and shift.\nC: You cannot modify the custom tooling.\nD: This is not even an option.\nHere you might make a few cloud (or other) optimizations in order to achieve some tangible benefit, but you aren’t otherwise changing the core architecture of the application.\nhttps://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/"},{"comment_id":"11529","content":"B.\nhttps://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","upvote_count":"13","poster":"dpvnme","timestamp":"1632549480.0"},{"content":"Selected Answer: B\nB. Re-platform.","upvote_count":"1","timestamp":"1668286800.0","poster":"DarthYoda","comment_id":"716926"},{"timestamp":"1664813580.0","poster":"dmscountera","comment_id":"685656","upvote_count":"1","content":"Selected Answer: B\nBased on all comments"},{"upvote_count":"1","timestamp":"1638936720.0","content":"B. Re-platform","comment_id":"496518","poster":"cldy"},{"upvote_count":"1","content":"It's B","timestamp":"1636245120.0","poster":"andylogan","comment_id":"450725"},{"timestamp":"1635780000.0","comment_id":"437818","poster":"tgv","upvote_count":"1","content":"BBB\n---"},{"upvote_count":"1","content":"I'll go with B","comment_id":"409555","timestamp":"1635606000.0","poster":"WhyIronMan"},{"comment_id":"344269","content":"it's B","poster":"Waiweng","upvote_count":"2","timestamp":"1635161820.0"},{"timestamp":"1634625300.0","content":"Agree with B\n\nRef: https://cloud.netapp.com/blog/aws-migration-strategy-the-6-rs-in-depth","upvote_count":"2","poster":"Mansur","comment_id":"291380"},{"content":"B is correct","upvote_count":"2","timestamp":"1633873200.0","comment_id":"289568","poster":"Kian1"},{"poster":"Ebi","content":"B is my choice","upvote_count":"3","comment_id":"283141","timestamp":"1633857060.0"},{"poster":"sanjaym","comment_id":"266569","upvote_count":"2","content":"B for sure.","timestamp":"1633667160.0"},{"comment_id":"242407","timestamp":"1633578900.0","poster":"T14102020","upvote_count":"2","content":"B is correct answer."},{"upvote_count":"2","timestamp":"1633352400.0","comment_id":"229437","poster":"jackdryan","content":"I'll go with B"},{"poster":"Bulti","comment_id":"228715","timestamp":"1633314180.0","content":"B is the answer.","upvote_count":"1"},{"content":"B acceptable","poster":"noisonnoiton","upvote_count":"2","timestamp":"1633177440.0","comment_id":"136923"},{"upvote_count":"2","content":"B for sure","comment_id":"134240","timestamp":"1632983160.0","poster":"NikkyDicky"},{"timestamp":"1632721980.0","upvote_count":"3","comment_id":"49537","content":"Should be B","poster":"amog"},{"upvote_count":"3","timestamp":"1632718500.0","comment_id":"41330","poster":"markpark","content":"B is the answer"},{"upvote_count":"6","content":"Support Answer \"B\".\nA: REhosting: should be for system that is cloud-compatible.\nB: Optimize the solution but not changing the core architecture of the application. Change DB to better DB, app to automated app like EB.\nC: Re-imagining how the application is architected, with a need to add features, scale, or performance.","poster":"Moon","timestamp":"1632569640.0","comment_id":"14163"},{"poster":"awsec2","upvote_count":"6","content":"B is right","timestamp":"1632560700.0","comment_id":"12327"}],"question_id":404,"question_text":"A company is planning the migration of several lab environments used for software testing. An assortment of custom tooling is used to manage the test runs for each lab. The labs use immutable infrastructure for the software test runs, and the results are stored in a highly available SQL database cluster. Although completely rewriting the custom tooling is out of scope for the migration project, the company would like to optimize workloads during the migration.\nWhich application migration strategy meets this requirement?","unix_timestamp":1568780520,"exam_id":32,"topic":"1","answers_community":["B (100%)"],"choices":{"A":"Re-host","D":"Retire","C":"Re-factor/re-architect","B":"Re-platform"},"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/5347-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[]},{"id":"5kGX6onyL7HVaVZYbM80","answer_ET":"A","question_text":"A company is implementing a multi-account strategy; however, the Management team has expressed concerns that services like DNS may become overly complex. The company needs a solution that allows private DNS to be shared among virtual private clouds (VPCs) in different accounts. The company will have approximately 50 accounts in total.\nWhat solution would create the LEAST complex DNS architecture and ensure that each VPC can resolve all AWS resources?","topic":"1","exam_id":32,"question_images":[],"choices":{"A":"Create a shared services VPC in a central account, and create a VPC peering connection from the shared services VPC to each of the VPCs in the other accounts. Within Amazon Route 53, create a privately hosted zone in the shared services VPC and resource record sets for the domain and subdomains. Programmatically associate other VPCs with the hosted zone.","B":"Create a VPC peering connection among the VPCs in all accounts. Set the VPC attributes enableDnsHostnames and enableDnsSupport to ג€trueג€ for each VPC. Create an Amazon Route 53 private zone for each VPC. Create resource record sets for the domain and subdomains. Programmatically associate the hosted zones in each VPC with the other VPCs.","D":"Set the VPC attributes enableDnsHostnames and enableDnsSupport to ג€falseג€ in every VPC. Create an AWS Direct Connect connection with a private virtual interface. Allow UDP and TCP port 53 over the virtual interface. Use the on-premises DNS servers to resolve the IP addresses in each VPC on AWS.","C":"Create a shared services VPC in a central account. Create a VPC peering connection from the VPCs in other accounts to the shared services VPC. Create an Amazon Route 53 privately hosted zone in the shared services VPC with resource record sets for the domain and subdomains. Allow UDP and TCP port 53 over the VPC peering connections."},"unix_timestamp":1570171200,"discussion":[{"comment_id":"13801","comments":[{"poster":"Moon","content":"good analysis.\nI do support \"A\" also.","upvote_count":"1","timestamp":"1632361860.0","comment_id":"14160"},{"poster":"Smart","upvote_count":"4","timestamp":"1632867660.0","content":"On side note: enableDnsHostnames & enableDnsSupport is required for Private Hosted Zone","comment_id":"70157"},{"poster":"donathon","timestamp":"1632312060.0","comments":[{"poster":"virtual","comment_id":"60735","upvote_count":"2","timestamp":"1632860460.0","content":"Thanks for your explanation."},{"comment_id":"392425","content":"First of all, the solution doesn't need hybrid architecture.\nA much better solution is available using DNS Resolver (which doesn't even need VPC peering): https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/\nHowever, with the given options, the most logical answer is A","poster":"DashL","timestamp":"1635312480.0","upvote_count":"2"}],"content":"In this setup you want to query Route 53 private hosted zone resolution across multiple accounts, and VPC’s from your resources on-premises. In this design setup you will use a shared services VPC to accomplish this. At the same time, you also want to conditionally forward queries for on-premises domains from the VPCs to the on-premises DNS resolver. These VPCs are inter-connected using a hub and spoke topology. Each of the spoke VPCs belongs to a different account, and they are managed by their respective accounts.\nWhen a Route 53 private hosted zone needs to be resolved in multiple VPCs and AWS accounts as described earlier, the most reliable pattern is to share the private hosted zone between accounts and associate it to each VPC that needs it. Although it’s possible to use Route 53 Resolver forwarding to solve this use case, this introduces additional costs, possible inter-Availability Zone dependencies, and complexity, which directly associating zones avoids.\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/","comment_id":"13802","upvote_count":"18"}],"content":"A \nB: enableDnsHostnames: Indicates whether instances with public IP addresses get corresponding public DNS hostnames. If this attribute is true, instances in the VPC get public DNS hostnames, but only if the enableDnsSupport attribute is also set to true. enableDnsSupport: Indicates whether the DNS resolution is supported. This is not needed.\nC: Do it from the central account is less complex and faster.\nD: This is not recommended and not the least complex solution. This will be difficult to maintain too. I don’t think it’s even possible.","timestamp":"1632308700.0","upvote_count":"27","poster":"donathon"},{"poster":"SkyZeroZx","timestamp":"1687033500.0","comment_id":"926258","content":"Selected Answer: A\nDo it from the central account is less complex and faster , then A","upvote_count":"1"},{"timestamp":"1675538460.0","poster":"unknownUser22952","content":"C is incorrect, because the other VPCs are not associated with the central hosted zone, and just because the VPC is peered to the other VPC which is associated with the hosted zone, it cannot send requests to the hosted zone which doesn't have VPC attachment. We need to explicitly mention the Route53 Resolver IP while making the request.\n\nHence I go with option A","comment_id":"798290","upvote_count":"2"},{"timestamp":"1672156560.0","content":"Selected Answer: A\nYou can use AWS CLI – See create-vpc-association-authorization in the AWS CLI Command Reference\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html","poster":"evargasbrz","upvote_count":"1","comment_id":"758743"},{"content":"Selected Answer: C\nA) You cannot programatically associate vpcs in a simple way. They are AWS resources. \nB) same\nC) sure this is how it works. DNS in the central account listens on port 53 udp. Create a peering and allow access. Go with the subnet IP of the dns server (Network ip plus 2) will resolve.\nD) direct connect: even more nonsense.","comment_id":"749045","upvote_count":"1","poster":"hobokabobo","timestamp":"1671381960.0"},{"upvote_count":"1","timestamp":"1664813640.0","comment_id":"685657","poster":"dmscountera","content":"Selected Answer: A\nBased on all comments"},{"timestamp":"1638637860.0","comment_id":"493835","upvote_count":"1","content":"A is perfect based on https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/ provided by donathon","poster":"AzureDP900"},{"comment_id":"450736","timestamp":"1635793020.0","upvote_count":"1","poster":"andylogan","content":"It's A"},{"comment_id":"409556","content":"I'll go with A","upvote_count":"3","timestamp":"1635400200.0","poster":"WhyIronMan"},{"poster":"Waiweng","upvote_count":"2","content":"A is correct","timestamp":"1635030780.0","comment_id":"344270"},{"poster":"gpark","timestamp":"1634935740.0","comment_id":"294773","comments":[{"content":"VPC peering/TGW is not even required to shared a PHZ with different AWS accounts","comment_id":"404884","timestamp":"1635317760.0","upvote_count":"1","poster":"student2020"}],"content":"A\n---\nB would be correct with there is the limited number of VPC like 2.\nMore than 3 VPC scenarios will need Transit Gateway.\nIt would be a burden to VPC peering all the VPCs.","upvote_count":"1"},{"poster":"Kian1","timestamp":"1634730420.0","content":"going with A","upvote_count":"2","comment_id":"289570"},{"poster":"Ebi","timestamp":"1634625240.0","upvote_count":"3","content":"My answer is A","comment_id":"283146"},{"content":"Correct answer is A. Use shared VPC services","comment_id":"242410","poster":"T14102020","timestamp":"1634434200.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1634379060.0","content":"I'll go with A","comment_id":"229440","poster":"jackdryan"},{"poster":"Bulti","upvote_count":"4","content":"Answer is A. You need to programmatically associate the VPC in another account to the private hosted zone in a central account. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html\nC is similar to A in terms of how VPCs are peered but the association of VPC in each account to the Route 3 private hosted zone is incorrectly described.","comment_id":"228752","timestamp":"1634214000.0"},{"poster":"fullaws","content":"A is correct","upvote_count":"2","timestamp":"1633861620.0","comment_id":"149470"},{"comment_id":"136934","upvote_count":"2","timestamp":"1633756440.0","poster":"noisonnoiton","content":"A Acceptable\nassociate other VPCs with the hosted zone"},{"content":"A most likely","poster":"NikkyDicky","upvote_count":"2","comment_id":"134244","timestamp":"1633570980.0"},{"comments":[{"upvote_count":"1","poster":"ipindado2020","comment_id":"182640","timestamp":"1633933920.0","content":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html\nIn order to use the DNS it should be done like A"}],"poster":"ricoyao","comment_id":"128657","timestamp":"1633561380.0","content":"Still not quite understand why C is incorrect?","upvote_count":"1"},{"timestamp":"1633252920.0","comment_id":"97182","comments":[{"upvote_count":"1","timestamp":"1633476060.0","content":"https://aws.amazon.com/blogs/security/how-to-centralize-dns-management-in-a-multi-account-environment/","poster":"JohnyGaddar","comment_id":"97185"}],"content":"a - correct\nc - important step : associating VPCs with the hosted zone is missing\nb - no need for peering among all vpcs\nd - on-premise dns will complicate architecture","upvote_count":"2","poster":"JohnyGaddar"},{"upvote_count":"2","content":"B (Invalid): \"Programmatically associate the hosted zones in each VPC with the other VPCs.\"\nC (Invalid): \"Allow UDP and TCP port 53 over the VPC peering connections.\" Cannot be restrictive here.\nD (Invalid): Not least complex.","timestamp":"1632912360.0","comment_id":"76196","poster":"Smart","comments":[{"timestamp":"1633189320.0","poster":"Smart","upvote_count":"1","content":"Restrictive or specific at route tables when setting VPC peering.","comment_id":"76197"}]},{"upvote_count":"1","content":"I am slightly confused here. The VPC peering limit is 40 per VPC. How can we accommodate 50 VPCs.","timestamp":"1632557220.0","poster":"dojo","comments":[{"timestamp":"1632611040.0","upvote_count":"1","content":"Default peering limit is 50 with the maximum quota of 125.","poster":"abrahamrohithroy","comment_id":"31115"},{"content":"My bad, VPC peering is allowed up to 125 peering connections.\nAnswer is A","timestamp":"1632831720.0","poster":"dojo","upvote_count":"1","comment_id":"31136"}],"comment_id":"30487"},{"upvote_count":"1","comments":[{"content":"If the domain in the hosted zone is targeting endpoint in another VPC, you must have VPC peering to reach that endpoint. It is not enough to only have the address resolved.","timestamp":"1632511920.0","poster":"tan9","comment_id":"29501","upvote_count":"2"}],"comment_id":"22339","content":"For \"Programmatically associate other VPCs with the hosted zone.\" https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/\n\nThis blog does not mention VPC peering is required though.","timestamp":"1632495840.0","poster":"Frank1"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/6088-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":405,"answer_description":"","timestamp":"2019-10-04 08:40:00","isMC":true,"answer":"A","answers_community":["A (75%)","C (25%)"]}],"exam":{"isBeta":false,"id":32,"numberOfQuestions":1019,"provider":"Amazon","isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","isMCOnly":false},"currentPage":81},"__N_SSP":true}