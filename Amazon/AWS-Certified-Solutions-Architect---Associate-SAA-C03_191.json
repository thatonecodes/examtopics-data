{"pageProps":{"questions":[{"id":"dLZR1atMgrOPsunbjpxu","answer":"C","answers_community":["C (100%)"],"question_text":"A company has an internal application that runs on Amazon EC2 instances in an Auto Scaling group. The EC2 instances are compute optimized and use Amazon Elastic Block Store (Amazon EBS) volumes.\n\nThe company wants to identify cost optimizations across the EC2 instances, the Auto Scaling group, and the EBS volumes.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","url":"https://www.examtopics.com/discussions/amazon/view/145011-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","answer_images":[],"question_id":951,"unix_timestamp":1722812940,"answer_ET":"C","exam_id":31,"timestamp":"2024-08-05 01:09:00","discussion":[{"poster":"Sarayounisaldossary","upvote_count":"1","timestamp":"1744065780.0","content":"Selected Answer: C\nthe question here focuses on (COST Optimization)\n\n- AWS Cost and Usage Report is the most detailed report that shows your AWS usage and charges.\n\n- cloudwatch is only for setting alerts based on setting policies-\n\n- would go right-away with AWS compute Optimizer because this service gives recommendations to optimize your compute resources (like EC2, Lambda, etc.) for better performance and lower cost.\n\nwhich is C and D\n\nBUT when reading D choice there's nothing wrong with AWS Cost and Usage report but why would i make it more complex to get the cost optimization? AWS compute Optimizer is more than enough which is C option","comment_id":"1558746"},{"content":"Selected Answer: C\nGo with Option C for the most efficient and straightforward solution for sure. Just one more thing: AWS Compute Optimizer doesn’t provide cost data directly (such as detailed dollar amounts), so supplementing it with AWS Cost Explorer or Trusted Advisor is a good practice for deeper cost analysis.","upvote_count":"1","comment_id":"1336627","poster":"LeonSauveterre","timestamp":"1736051880.0"},{"poster":"[Removed]","timestamp":"1724160540.0","comment_id":"1269514","upvote_count":"2","content":"Selected Answer: C\nC looks right"},{"comment_id":"1260985","content":"Selected Answer: C\nAnswer is C","poster":"komorebi","upvote_count":"2","timestamp":"1722847560.0"},{"comment_id":"1260840","upvote_count":"3","content":"Selected Answer: C\nhttps://aws.amazon.com/compute-optimizer/\nhttps://docs.aws.amazon.com/compute-optimizer/latest/ug/what-is-compute-optimizer.html","timestamp":"1722812940.0","poster":"example_"}],"isMC":true,"topic":"1","choices":{"B":"Create new Amazon CloudWatch billing alerts. Check the alert statuses for cost recommendations for the EC2 instances, the Auto Scaling group, and the EBS volumes.","D":"Configure AWS Compute Optimizer for cost recommendations for the EC2 instances. Create a new AWS Cost and Usage Report. Search the report for cost recommendations for the Auto Scaling group and the EBS volumes.","C":"Configure AWS Compute Optimizer for cost recommendations for the EC2 instances, the Auto Scaling group and the EBS volumes.","A":"Create a new AWS Cost and Usage Report. Search the report for cost recommendations for the EC2 instances the Auto Scaling group, and the EBS volumes."},"question_images":[]},{"id":"A6f7wCAFwGg2qkSt3yAU","answer":"D","discussion":[{"poster":"Sarayounisaldossary","upvote_count":"1","comment_id":"1558739","timestamp":"1744063320.0","content":"Selected Answer: D\nWhy is EFS the correct answer?\n\nAmazon EFS is a fully managed, shared file system.\n\nIt can be mounted on multiple EC2 instances at the same time.\n\nIt works across multiple Availability Zones.\n\nHigh performance, and keeps all data within the VPC.\n\nPerfect fit for the use case: “Share data between EC2 instances.”\n\nOther options:\n\nS3 is object storage, not a shared file system.\n\nEBS can be attached to only one instance at a time.\n\nMounting S3 isn’t officially supported or reliable."},{"poster":"aragon_saa","comment_id":"1265404","upvote_count":"4","timestamp":"1723587120.0","content":"Selected Answer: D\nAnswer is D"},{"upvote_count":"3","poster":"muhammadahmer36","comment_id":"1265305","content":"Selected Answer: D\nD is the right answer","timestamp":"1723575960.0"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/145679-exam-aws-certified-solutions-architect-associate-saa-c03/","answers_community":["D (100%)"],"question_images":[],"timestamp":"2024-08-13 21:06:00","unix_timestamp":1723575960,"isMC":true,"question_text":"A company is running a media store across multiple Amazon EC2 instances distributed across multiple Availability Zones in a single VPC. The company wants a high-performing solution to share data between all the EC2 instances, and prefers to keep the data within the VPC only.\n\nWhat should a solutions architect recommend?","exam_id":31,"topic":"1","choices":{"C":"Configure an Amazon Elastic Block Store (Amazon EBS) volume and mount it across all instances","D":"Configure an Amazon Elastic File System (Amazon EFS) file system and mount it across all instances","B":"Create an Amazon S3 bucket and configure all instances to access it as a mounted volume","A":"Create an Amazon S3 bucket and call the service APIs from each instance's application"},"answer_description":"","answer_ET":"D","question_id":952},{"id":"DWY8Jcvn7KHUQPGTFRor","question_text":"A company uses an Amazon RDS for MySQL instance. To prepare for end-of-year processing, the company added a read replica to accommodate extra read-only queries from the company's reporting tool. The read replica CPU usage was 60% and the primary instance CPU usage was 60%.\n\nAfter end-of-year activities are complete, the read replica has a constant 25% CPU usage. The primary instance still has a constant 60% CPU usage. The company wants to rightsize the database and still provide enough performance for future growth.\n\nWhich solution will meet these requirements?","timestamp":"2024-08-13 21:08:00","question_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/145680-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"comment_id":"1558729","content":"Selected Answer: D\nYou should remove an underused resource (read replica).\n\nUse that budget to increase primary capacity, solving the CPU load.\n\nEven though Option B is “safe” and cost-conscious, it ignores the performance needs of the primary, which is where the real issue lies.\nOption D is more aligned with AWS best practices and the question's goal of performance + future growth.","timestamp":"1744060920.0","poster":"Sarayounisaldossary","upvote_count":"1"},{"poster":"LeonSauveterre","timestamp":"1736052300.0","upvote_count":"2","comment_id":"1336629","content":"Selected Answer: B\nNot A or D because they risk future scalability issues during traffic spikes. Moreover, when the end of next year comes, you have to bring back the deleted instance and configure it all over again. As a solutions architect, I wouldn't recommend things like that (personally)."},{"comment_id":"1326769","content":"Selected Answer: D\nD. Delete the read replica Resize the primary instance to a larger instance\nbecause \nno need for read replica after year end\nstill provide enough performance for future growth.","timestamp":"1734255480.0","poster":"Ghoneam","upvote_count":"3"},{"content":"Answer is B\nSince the read replica is now underutilized with only 25% CPU usage, it can be resized to a smaller instance to save costs while still handling the reduced read queries. No changes to the primary instance are needed, as it is consistently running at 60% CPU usage, which is manageable, and the read replica will still offload read queries in the future.","timestamp":"1728549720.0","upvote_count":"3","comment_id":"1295493","poster":"Oghare"},{"content":"No clear explanation here and everyone agreed with default answer","poster":"Abdullah2004","upvote_count":"1","comment_id":"1275204","timestamp":"1725039900.0"},{"poster":"[Removed]","upvote_count":"1","content":"Selected Answer: B\nB looks good","comment_id":"1269516","timestamp":"1724160840.0"},{"upvote_count":"1","content":"Selected Answer: B\nAnswer is B","comment_id":"1265406","timestamp":"1723587120.0","poster":"aragon_saa"},{"content":"Selected Answer: B\nResize the read replica to a smaller instance size Do not make changes to the primary instance","upvote_count":"2","comment_id":"1265306","timestamp":"1723576080.0","poster":"muhammadahmer36"}],"question_id":953,"choices":{"D":"Delete the read replica Resize the primary instance to a larger instance","B":"Resize the read replica to a smaller instance size Do not make changes to the primary instance","C":"Resize the read replica to a larger instance size Resize the primary instance to a smaller instance size","A":"Delete the read replica Do not make changes to the primary instance"},"answer_description":"","answer":"B","isMC":true,"topic":"1","answer_images":[],"answer_ET":"B","exam_id":31,"unix_timestamp":1723576080,"answers_community":["B (60%)","D (40%)"]},{"id":"6vVAfIvFpcYWBV3NSJ1t","answer_ET":"C","question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/86676-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"unix_timestamp":1667136120,"timestamp":"2022-10-30 14:22:00","question_id":954,"choices":{"A":"Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DB cluster.","D":"Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster.","B":"Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.","C":"Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB."},"exam_id":31,"answer_description":"","discussion":[{"upvote_count":"17","comment_id":"707826","content":"Option C\nDynamo DB is a NoSQL-JSON supported","timestamp":"1667136120.0","poster":"rjam","comments":[{"comment_id":"707827","upvote_count":"12","content":"also Use an AWS Lambda - serverless - less operational overhead","timestamp":"1667136180.0","poster":"rjam"}]},{"poster":"cookieMr","content":"Selected Answer: C\nA. Configuring EMR and an Aurora DB cluster for this use case would introduce unnecessary complexity and operational overhead. EMR is typically used for processing large datasets and running big data frameworks like Apache Spark or Hadoop.\n\nB. While using S3 event notifications and SQS for decoupling is a good approach, using EC2 to process the data would introduce operational overhead in terms of managing and scaling the EC2.\n\nD. Using EventBridge and Kinesis Data Streams for this use case would introduce additional complexity and operational overhead compared to the other options. EventBridge and Kinesis are typically used for real-time streaming and processing of large volumes of data.\n\nIn summary, option C is the recommended solution as it provides a serverless and scalable approach for processing uploaded files using S3 event notifications, SQS, and Lambda. It offers low operational overhead, automatic scaling, and efficient handling of varying demand. Storing the resulting JSON file in DynamoDB aligns with the requirement of saving the data for later analysis.","comment_id":"930243","timestamp":"1687418340.0","upvote_count":"10"},{"timestamp":"1738177500.0","poster":"Dharmarajan","content":"Selected Answer: C\nC is the simplest and most widely used architecture, for example, processing photos to generate thumbnails.","comment_id":"1348717","upvote_count":"1"},{"poster":"satyaammm","content":"Selected Answer: C\nC is most suitable here as we need DynamoDB for low latency, AWS Lambda for scaling as per the demand.","comment_id":"1336757","timestamp":"1736081820.0","upvote_count":"1"},{"comment_id":"1284154","poster":"PaulGa","upvote_count":"2","content":"Selected Answer: C\nAns C - as per cookieMr (1 yr, 2 mth ago) \n\"...In summary, option C is the recommended solution as it provides a serverless and scalable approach for processing uploaded files using S3 event notifications, SQS, and Lambda. It offers low operational overhead, automatic scaling, and efficient handling of varying demand. Storing the resulting JSON file in DynamoDB aligns with the requirement of saving the data for later analysis.\"","timestamp":"1726407600.0"},{"timestamp":"1721576820.0","content":"Selected Answer: C\nOption C, fulfills the least operational overhead condition.","upvote_count":"2","poster":"jaradat02","comment_id":"1252545"},{"content":"Selected Answer: C\nB where we use EC2 instances for processing would be ideal in situations where runtime is > 15 minutes. However the question mentions 'simple processing', hence we go for Lambda.","poster":"TilTil","upvote_count":"3","timestamp":"1710818040.0","comment_id":"1176959"},{"content":"Selected Answer: C\nLEAST operational overhead\nA: EMR is massive programming effort for this\nB: EC2 is considerable overhead\nD: Nice solution but why would you use Kinesis as there is no streaming scenario here\nC: Simplest and all managed services so least operational overhead compared to other options","upvote_count":"2","timestamp":"1705269720.0","comment_id":"1122899","poster":"awsgeek75"},{"upvote_count":"2","poster":"Ruffyit","comment_id":"1056640","timestamp":"1698564480.0","content":"Option C is the best solution that meets the requirements with the least operational overhead:\n\nConfigure Amazon S3 to send event notification to SQS queue\nUse Lambda function triggered by SQS to process each file\nStore output JSON in DynamoDB\nThis leverages serverless components like S3, SQS, Lambda, and DynamoDB to provide automated file processing without needing to provision and manage servers.\n\nSQS queues the notifications and Lambda scales automatically to handle spikes and drops in file uploads. No EMR cluster or EC2 Fleet is needed to manage."},{"comment_id":"1020086","content":"Selected Answer: C\nC: Lambdas are made for that","upvote_count":"1","poster":"Modulopi","timestamp":"1695917100.0"},{"timestamp":"1692850140.0","poster":"TariqKipkemei","comment_id":"988817","content":"Selected Answer: C\nC is best","upvote_count":"1"},{"upvote_count":"2","poster":"Guru4Cloud","comment_id":"978732","timestamp":"1691765400.0","content":"Selected Answer: C\nOption C is the best solution that meets the requirements with the least operational overhead:\n\nConfigure Amazon S3 to send event notification to SQS queue\nUse Lambda function triggered by SQS to process each file\nStore output JSON in DynamoDB\nThis leverages serverless components like S3, SQS, Lambda, and DynamoDB to provide automated file processing without needing to provision and manage servers.\n\nSQS queues the notifications and Lambda scales automatically to handle spikes and drops in file uploads. No EMR cluster or EC2 Fleet is needed to manage."},{"poster":"beginnercloud","comment_id":"904019","upvote_count":"1","content":"Selected Answer: C\nOption C is correct - Dynamo DB is a NoSQL-JSON supported","timestamp":"1684757820.0"},{"poster":"Abrar2022","timestamp":"1684738980.0","upvote_count":"2","content":"Selected Answer: C\nSQS + LAMDA + JSON >>>>>> Dynamo DB","comment_id":"903785"},{"comment_id":"903399","upvote_count":"1","timestamp":"1684687920.0","poster":"Bmarodi","content":"Selected Answer: C\nThe option C is right answer."},{"timestamp":"1682613900.0","upvote_count":"1","poster":"jy190","comment_id":"882830","content":"can someone explain why SQS? it's a poll-based messaging, does it guarantee reacting the event asap?"},{"timestamp":"1672472400.0","poster":"Zerotn3","content":"Selected Answer: C\nDynamo DB is a NoSQL-JSON supported","comment_id":"762536","upvote_count":"1"},{"comment_id":"752680","comments":[{"content":"Using a serverless solution like AWS Lambda can help to reduce operational overhead because it automatically scales to meet demand and does not require you to provision and manage infrastructure. Additionally, using an SQS queue as a buffer between the S3 event notification and the Lambda function can help to decouple the processing of the data from the uploading of the data, allowing the processing to happen asynchronously and improving the overall efficiency of the system.","poster":"Buruguduystunstugudunstuy","comment_id":"752682","upvote_count":"2","timestamp":"1671650760.0"}],"upvote_count":"5","timestamp":"1671650760.0","poster":"Buruguduystunstugudunstuy","content":"Selected Answer: C\nOption C, Configuring Amazon S3 to send an event notification to an Amazon Simple Queue Service (SQS) queue and using an AWS Lambda function to read from the queue and process the data, would likely be the solution with the least operational overhead.\n\nAWS Lambda is a serverless computing service that allows you to run code without the need to provision or manage infrastructure. When a new file is uploaded to Amazon S3, it can trigger an event notification which sends a message to an SQS queue. The Lambda function can then be set up to be triggered by messages in the queue, and it can process the data and store the resulting JSON file in Amazon DynamoDB."},{"comment_id":"747511","poster":"career360guru","content":"Selected Answer: C\nOption C as JSON is supported by DynamoDB. RDS or AuroraDB are not suitable for JSON data.\nA - Because this is not a Bigdata analytics usecase.","timestamp":"1671216720.0","upvote_count":"2"},{"comment_id":"743525","timestamp":"1670899380.0","poster":"gloritown","upvote_count":"1","content":"Selected Answer: C\nCCCCCCCC"},{"comment_id":"739749","poster":"AlaN652","content":"Selected Answer: C\nAnswer C","timestamp":"1670559060.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: C\nanswer is C","comment_id":"730289","timestamp":"1669720200.0","poster":"HussamShokr"},{"upvote_count":"1","poster":"Kapello10","comment_id":"728155","timestamp":"1669548480.0","content":"Selected Answer: C\ncccccccccccc"},{"timestamp":"1669485600.0","comment_id":"727732","upvote_count":"1","content":"Selected Answer: C\nOption C","poster":"DivaLight"},{"comment_id":"723760","poster":"Wpcorgan","upvote_count":"1","content":"C is correct","timestamp":"1669049460.0"},{"upvote_count":"1","timestamp":"1668413160.0","content":"Selected Answer: C\nhttps://www.examtopics.com/discussions/amazon/view/67958-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"Pamban","comment_id":"717783"},{"comment_id":"717501","timestamp":"1668370560.0","content":"Selected Answer: C\nSQS + LAMDA + JSON to Dynamo DB","upvote_count":"1","poster":"Jtic"},{"comment_id":"709490","timestamp":"1667349360.0","upvote_count":"1","content":"With explanations \n\nhttps://www.examtopics.com/discussions/amazon/view/67958-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"Hunkie"}],"question_text":"A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis.\nEach file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files.\nWhich solution meets these requirements with the LEAST operational overhead?","isMC":true,"answers_community":["C (100%)"],"answer":"C"},{"id":"CjMnm59sSAjGzlKUcezP","exam_id":31,"answer_description":"","choices":{"A":"Use On-Demand Instances for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year Compute Savings Plan with the No Upfront option for the EC2 instances.","D":"Purchase Reserved Instances for a 3 year term with the All Upfront option for the Amazon RDS for PostgreSQL workloads. Purchase a 3 year EC2 Instance Savings Plan with the All Upfront option for the EC2 instances.","B":"Purchase Reserved Instances for a 1 year term with the No Upfront option for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year EC2 Instance Savings Plan with the No Upfront option for the EC2 instances.","C":"Purchase Reserved Instances for a 1 year term with the Partial Upfront option for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year EC2 Instance Savings Plan with the Partial Upfront option for the EC2 instances."},"answer_images":[],"question_text":"A company is migrating its databases to Amazon RDS for PostgreSQL. The company is migrating its applications to Amazon EC2 instances. The company wants to optimize costs for long-running workloads.\n\nWhich solution will meet this requirement MOST cost-effectively?","unix_timestamp":1722583860,"timestamp":"2024-08-02 09:31:00","discussion":[{"timestamp":"1736052840.0","upvote_count":"5","comments":[{"upvote_count":"1","timestamp":"1736052900.0","poster":"LeonSauveterre","content":"uitable => suitable sorry","comment_id":"1336631"}],"content":"Selected Answer: D\nWhat you should know:\n1. No Upfront - Pay nothing at the time of purchase. Payments are spread out monthly over the term (1 or 3 years). Lowest savings with high flexibility.\n2. Partial Upfront - Pay a portion of the cost upfront, with reduced monthly payments over the term. Moderate savings with moderate flexibility.\n3. All Upfront - Pay the full cost upfront at the time of purchase. No additional payments required during the term. Highest savings with the least flexibility.\n\nMerely by the question, option D would be the answer because it's best for companies with a stable workload, long-term commitment, and sufficient budget for upfront payments.\n\nJust to add something, generally option C would be the best in real world cases because it strikes a good balance between savings and upfront investment, and uitable for mid-term commitments with moderate budget flexibility.","poster":"LeonSauveterre","comment_id":"1336630"},{"content":"the longer the reservation the more the savings too for reserved","poster":"744fdad","comment_id":"1265896","upvote_count":"2","timestamp":"1723657380.0"},{"content":"Selected Answer: D\nD is right","comment_id":"1265308","timestamp":"1723576320.0","poster":"muhammadahmer36","upvote_count":"1"},{"poster":"nebajp","comment_id":"1263598","upvote_count":"1","timestamp":"1723313760.0","content":"Selected Answer: D\nCorrect Answer - D,\nAll upfront is cheaper than partial and no upfront."},{"content":"D is correct","comment_id":"1263215","poster":"swati1508","timestamp":"1723234020.0","upvote_count":"2"},{"timestamp":"1722583860.0","content":"Selected Answer: D\nletter D is the correct, It gives you big discount if you purchase with all upfront","comment_id":"1259771","poster":"JunsK1e","upvote_count":"3"}],"isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/144895-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"D","answer_ET":"D","question_id":955,"answers_community":["D (100%)"],"question_images":[],"topic":"1"}],"exam":{"provider":"Amazon","lastUpdated":"11 Apr 2025","isMCOnly":true,"numberOfQuestions":1019,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"id":31,"isBeta":false},"currentPage":191},"__N_SSP":true}