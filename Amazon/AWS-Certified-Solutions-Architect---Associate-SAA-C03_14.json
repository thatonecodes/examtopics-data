{"pageProps":{"questions":[{"id":"MlqvYN3sEF1LzkN78Ldd","answer_description":"","topic":"1","answer_images":[],"question_id":66,"exam_id":31,"answer":"C","question_text":"A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance.\nThe database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a solution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability.\nWhich solution will meet these requirements?","choices":{"C":"Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.","D":"Use Amazon ElastiCache for Memcached with EC2 Spot Instances.","B":"Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a different Availability Zone.","A":"Use Amazon Redshift with a single node for leader and compute functionality."},"timestamp":"2022-10-10 15:28:00","question_images":[],"discussion":[{"content":"Selected Answer: C\nC, AURORA is 5x performance improvement over MySQL on RDS and handles more read requests than write,; maintaining high availability = Multi-AZ deployment","timestamp":"1665408480.0","poster":"D2w","upvote_count":"51","comment_id":"691182"},{"content":"Selected Answer: C\nOption C, using Amazon Aurora with a Multi-AZ deployment and configuring Aurora Auto Scaling with Aurora Replicas, would be the best solution to meet the requirements.\n\nAurora is a fully managed, MySQL-compatible relational database that is designed for high performance and high availability. Aurora Multi-AZ deployments automatically maintain a synchronous standby replica in a different Availability Zone to provide high availability. Additionally, Aurora Auto Scaling allows you to automatically scale the number of Aurora Replicas in response to read workloads, allowing you to meet the demand of unpredictable read workloads while maintaining high availability. This would provide an automated solution for scaling the database to meet the demand of the application while maintaining high availability.","timestamp":"1672171500.0","upvote_count":"24","poster":"Buruguduystunstugudunstuy","comments":[{"upvote_count":"9","content":"Option A, using Amazon Redshift with a single node for leader and compute functionality, would not provide high availability. \n\nOption B, using Amazon RDS with a Single-AZ deployment and configuring RDS to add reader instances in a different Availability Zone, would not provide high availability and would not automatically scale the number of reader instances in response to read workloads. \n\nOption D, using Amazon ElastiCache for Memcached with EC2 Spot Instances, would not provide a database solution and would not meet the requirements.","poster":"Buruguduystunstugudunstuy","comment_id":"758997","timestamp":"1672171500.0"}],"comment_id":"758996"},{"upvote_count":"1","comment_id":"1400901","timestamp":"1742442540.0","content":"Selected Answer: C\nUsing aurora is the best answer for this issue since aurora is ideal for upredictable workloads and we can manage read heavy workloads by deploying the database with aurora replicas","poster":"melvis8"},{"poster":"Tjazz04","timestamp":"1733938080.0","upvote_count":"2","comment_id":"1325116","content":"Selected Answer: C\nKeyword: High Availability\nOnly letter C has multi-AZ deployment. Also, it automatically scales replicas for unpredictable read workload"},{"upvote_count":"1","content":"Selected Answer: C\nCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC","timestamp":"1729626840.0","comment_id":"1301694","poster":"VINVIN99"},{"comment_id":"1264098","content":"Selected Answer: C\nAns C. Its the only one that offers right level of scaling and availability","timestamp":"1723377480.0","poster":"PaulGa","upvote_count":"2"},{"comment_id":"1216189","timestamp":"1716436200.0","content":"Selected Answer: C\nHere's why C is the best solution:\n\nAmazon Aurora: A managed, high-performance MySQL-compatible relational database engine.\nMulti-AZ deployment: Ensures high availability in case of an AZ failure.\nAurora Auto Scaling with Aurora Replicas: Automatically scales read replicas based on traffic, improving read performance.","upvote_count":"3","poster":"OBIOHAnze"},{"poster":"A_jaa","content":"Selected Answer: C\nAnswer-c","upvote_count":"1","timestamp":"1705149240.0","comment_id":"1121622"},{"poster":"Ndlesty","upvote_count":"2","timestamp":"1700038440.0","comment_id":"1071240","content":"Selected Answer: C\nkey statement: \"...will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability."},{"content":"Selected Answer: C\nAurora","timestamp":"1696090200.0","comment_id":"1021678","poster":"AWSGuru123","upvote_count":"1"},{"timestamp":"1692013140.0","upvote_count":"1","content":"Selected Answer: C\nC fit perfectly","poster":"Syruis","comment_id":"980736"},{"timestamp":"1690518900.0","poster":"TariqKipkemei","content":"Selected Answer: C\nUnpredictable read workloads while maintaining high availability = Amazon Aurora with a Multi-AZ deployment, Auto Scaling with Aurora read replicas.","upvote_count":"1","comment_id":"965284"},{"upvote_count":"2","poster":"Guru4Cloud","comment_id":"957563","timestamp":"1689857580.0","content":"Selected Answer: C\nAs the application handles more read requests than write transactions, using read replicas with Aurora is an ideal choice as it allows read scaling without sacrificing write performance on the primary instance."},{"content":"Option C MET THE REQUIREMENT","comment_id":"950000","timestamp":"1689179100.0","poster":"miki111","upvote_count":"1"},{"content":"Selected Answer: C\nOption C","poster":"hiepdz98","timestamp":"1687851060.0","comment_id":"935105","upvote_count":"1"},{"poster":"cookieMr","upvote_count":"2","comment_id":"926531","content":"Selected Answer: C\nOption C: Using Amazon Aurora with a Multi-AZ deployment and configuring Aurora Auto Scaling with Aurora Replicas is the most appropriate solution. Aurora is a MySQL-compatible relational database engine that provides high performance and scalability. With Multi-AZ deployment, the database is automatically replicated across multiple Availability Zones for high availability. Aurora Auto Scaling allows the database to automatically add or remove Aurora Replicas based on the workload, ensuring that read requests can be distributed effectively and the database can scale to meet demand. This provides both high availability and automatic scaling to handle unpredictable read workloads.","timestamp":"1687075200.0"},{"upvote_count":"1","content":"Selected Answer: C\nC meets the requirements.","timestamp":"1685637000.0","poster":"Bmarodi","comment_id":"912268"},{"timestamp":"1685368020.0","upvote_count":"1","content":"C Aurora with read replicas","comment_id":"909459","poster":"Mehkay"},{"upvote_count":"4","comment_id":"898736","content":"Key words:\n- Must support MySQL\n- High Availability (must be mulit-az)\n- Auto Scaling","poster":"big0007","timestamp":"1684186680.0"},{"poster":"cheese929","comment_id":"897341","content":"Selected Answer: C\nC is correct since cost is not a concern.","upvote_count":"1","timestamp":"1684048620.0"},{"timestamp":"1683956040.0","comment_id":"896416","upvote_count":"2","poster":"Abrar2022","content":"It's Aurora with Multi-AZ deployment - Keywords > \"unpredictable read workloads while maintaining high availability\""},{"upvote_count":"2","comment_id":"896403","timestamp":"1683952980.0","content":"To automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability, you can use Amazon Aurora with a Multi-AZ deployment. Aurora is a fully managed, MySQL-compatible database service that can automatically scale up or down based on workload demands. With a Multi-AZ deployment, Aurora maintains a synchronous standby replica in a different Availability Zone (AZ) to provide high availability in the event of an outage.","poster":"Abrar2022"},{"timestamp":"1680779820.0","poster":"PhucVuu","comment_id":"862887","content":"Selected Answer: C\nKeywords:\n- The database's performance degrades quickly as application load increases.\n- The application handles more read requests than write transactions.\n- Automatically scale the database to meet the demand of unpredictable read workloads\n- Maintaining high availability.\n\nA: Incorrect - Amazon Redshift is used columnar block storage which useful Data Analytic and warehouse.\nIt also have the issue when migrate from MySql to Redshift: storage procedure, trigger,.. Single node for leader don't maintaining high availability.\nB: Incorrect - The requirement said that: \"Automatically scale the database to meet the demand of unpredictable read workloads\" -> missing auto scaling.\nC: Correct - it's resolved the issue high availability and auto scaling.\nD: Incorrect - Stop instance don't maintaining high availability.","upvote_count":"6"},{"upvote_count":"1","comment_id":"859123","timestamp":"1680454860.0","poster":"gx2222","content":"Selected Answer: C\nAmazon Aurora is a relational database engine that is compatible with MySQL and PostgreSQL. It is designed for high performance, scalability, and availability. With a Multi-AZ deployment, Aurora automatically replicates the database to a standby instance in a different Availability Zone. This provides high availability and fast failover in case of a primary instance failure.\n\nAurora Auto Scaling allows you to add or remove Aurora Replicas based on CPU utilization, connections, or custom metrics. This enables you to automatically scale the read capacity of the database in response to application load. Aurora Replicas are read-only instances that can offload read traffic from the primary instance. They are kept in sync with the primary instance using Aurora's distributed storage architecture, which enables low-latency updates across the replicas."},{"content":"Selected Answer: C\nOption C: Using Amazon Aurora with a Multi-AZ deployment and configuring Aurora Auto Scaling with Aurora Replicas will provide both read scalability and high availability. Aurora is a MySQL-compatible database that is designed to handle high read workloads. With Aurora's Multi-AZ deployment, a replica will be created in a different Availability Zone for disaster recovery purposes. Aurora Replicas can also be used to scale read workloads by adding read replicas.","timestamp":"1680212640.0","comment_id":"856338","poster":"linux_admin","upvote_count":"1"},{"comment_id":"823069","poster":"bilel500","upvote_count":"1","timestamp":"1677456120.0","content":"Selected Answer: C\nRight Answer C."},{"upvote_count":"1","poster":"buiducvu","comment_id":"807143","content":"Selected Answer: C\nAmazon Aurora","timestamp":"1676273280.0"},{"timestamp":"1673510460.0","comment_id":"773246","poster":"Abdel42","content":"Selected Answer: C\nC because other answers are not a good-fit for the question","upvote_count":"1"},{"upvote_count":"1","timestamp":"1673026380.0","comment_id":"767927","poster":"SilentMilli","content":"Selected Answer: C\nTo automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability, you can use Amazon Aurora with a Multi-AZ deployment. Aurora is a fully managed, MySQL-compatible database service that can automatically scale up or down based on workload demands. With a Multi-AZ deployment, Aurora maintains a synchronous standby replica in a different Availability Zone (AZ) to provide high availability in the event of an outage."},{"timestamp":"1671756780.0","upvote_count":"1","comments":[{"timestamp":"1673400960.0","poster":"hahahumble","comment_id":"771946","upvote_count":"1","content":"B can’t scale well"},{"poster":"MichaelCarrasco","comment_id":"806430","upvote_count":"1","content":"The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones so you need you database work with Multi AZ too.","timestamp":"1676212380.0"}],"poster":"SmartDude","comment_id":"753777","content":"Why is B incorrect??"},{"poster":"NikaCZ","timestamp":"1671131220.0","content":"Selected Answer: C\nAurora 5x faster and 3x improves performance","upvote_count":"2","comment_id":"746442"},{"upvote_count":"1","content":"no drought Ans is C","timestamp":"1670653260.0","comment_id":"740721","poster":"sanjay3x1"},{"comment_id":"723480","content":"C is correct","poster":"Wpcorgan","timestamp":"1669034460.0","upvote_count":"1"},{"upvote_count":"1","poster":"ABCMail","comment_id":"722423","timestamp":"1668924480.0","content":"Selected Answer: C\nAurora offers multi AZ for HA"},{"comment_id":"707147","poster":"17Master","upvote_count":"2","timestamp":"1667042340.0","content":"Selected Answer: C\nAns is Aurora"},{"content":"C is the answer. Aurora is fast, and for this case will support unpredictable workloads through its read replicas. Simple!","upvote_count":"2","comment_id":"707026","timestamp":"1667025000.0","poster":"keezbadger"},{"poster":"ukwafabian","upvote_count":"2","timestamp":"1666827240.0","comment_id":"705066","content":"Selected Answer: C\n\"Read workloads\" \"Maintaining high availability\" = Read replica's"},{"timestamp":"1666034760.0","upvote_count":"1","content":"Selected Answer: C\nC is correct.","poster":"GameDad09","comment_id":"697652"},{"comment_id":"693972","poster":"KVK16","upvote_count":"3","content":"Selected Answer: C\n1. Migration from My SQL, Postgres SQL to Aurora is 5x and 3x times improves performance . Also provision for Read replicas","timestamp":"1665670860.0"},{"content":"Selected Answer: C\nHigh availability + SQL -> C","comment_id":"693741","timestamp":"1665649320.0","poster":"Sinaneos","upvote_count":"1"},{"poster":"BoboChow","upvote_count":"1","timestamp":"1665545700.0","comment_id":"692605","content":"Selected Answer: C\nC is better than B about availibility"}],"answer_ET":"C","isMC":true,"unix_timestamp":1665408480,"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/85019-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"sScgp3WZ66zHTf6a6yuX","question_images":[],"timestamp":"2022-10-20 23:18:00","question_text":"A solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture.\nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year.\nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)","answer_images":[],"topic":"1","unix_timestamp":1666300680,"question_id":67,"answer_ET":"AC","exam_id":31,"answers_community":["AC (100%)"],"choices":{"C":"Purchase a 1-year Compute Savings Plan for the front end and API layer.","B":"Use On-Demand Instances for the data ingestion layer","A":"Use Spot Instances for the data ingestion layer","D":"Purchase 1-year All Upfront Reserved instances for the data ingestion layer.","E":"Purchase a 1-year EC2 instance Savings Plan for the front end and API layer."},"discussion":[{"poster":"SimonPark","upvote_count":"21","content":"Selected Answer: AC\nEC2 instance Savings Plan saves 72% while Compute Savings Plans saves 66%. But according to link, it says \"Compute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, region, OS or tenancy, and also apply to Fargate and Lambda usage.\" EC2 instance Savings Plans are not applied to Fargate or Lambda","comment_id":"707646","timestamp":"1667111460.0"},{"comment_id":"765040","upvote_count":"9","timestamp":"1672779060.0","content":"Selected Answer: AC\nCompute Savings Plans can be used for EC2 instances and Fargate. Whereas EC2 Savings Plans support EC2 only.","poster":"aba2s"},{"poster":"PaulGa","upvote_count":"2","comment_id":"1287274","timestamp":"1726911840.0","content":"Selected Answer: AC\nAns A, C - \nA: Spot obvious for unpredictable, 'don't care' usage \nC: Not so obvious... but its more than just EC2 - its about Compute power using Fargate, Lambda, API call processing so it has to be C (as opposed to E)"},{"content":"Selected Answer: AC\nBe mindful that the question is asking about API. So it should be Compute Savings Plans.\n\nIf it is for EC2, the Reserved Instance will be correct.","upvote_count":"3","comment_id":"1278949","poster":"huaze_lei","timestamp":"1725542220.0"},{"content":"Selected Answer: AC\nCompute Savings Plans can also apply to Fargate and Lambda Usage.","upvote_count":"5","timestamp":"1693976700.0","poster":"TariqKipkemei","comment_id":"1000184"},{"upvote_count":"1","poster":"AKBM7829","comments":[{"timestamp":"1697056860.0","comment_id":"1041101","upvote_count":"1","poster":"awashenko","content":"Spot instances can auto scale so Spot instance is correct."}],"timestamp":"1693187520.0","content":"BC is the answer \ndata ingestion = Spot Instance but\nKeyword \"Usage Unpredictable\" : On-Demand \n\nand for APi its Compute Savings Plan","comment_id":"991746"},{"timestamp":"1692212580.0","content":"Selected Answer: AC\nThe two most cost-effective purchasing options for this architecture are:\n\nA) Use Spot Instances for the data ingestion layer\n\nC) Purchase a 1-year Compute Savings Plan for the front end and API layer\n\nThe reasons are:\n\nSpot Instances provide the greatest savings for flexible, interruptible EC2 workloads like data ingestion.\nSavings Plans offer significant discounts for predictable usage like the front end and API layer.\nAll Upfront and partial/no Upfront RI's don't align well with the sporadic EC2 usage.\nOn-Demand is more expensive than Spot for flexible EC2 workloads.\nBy matching purchasing options to the workload patterns, Spot for unpredictable EC2 and Savings Plans for steady-state usage, the solutions architect optimizes cost efficiency.","upvote_count":"3","comment_id":"982912","poster":"Guru4Cloud"},{"upvote_count":"4","timestamp":"1687511100.0","content":"Selected Answer: AC\nUsing Spot Instances for the data ingestion layer will provide the most cost-effective option for sporadic and unpredictable workloads, as Spot Instances offer significant cost savings compared to On-Demand Instances (Option A).\n\nPurchasing a 1-year Compute Savings Plan for the front end and API layer will provide cost savings for predictable utilization over the course of a year (Option C).\n\nOption B is less cost-effective as it suggests using On-Demand Instances for the data ingestion layer, which does not take advantage of cost-saving opportunities.\n\nOption D suggests purchasing 1-year All Upfront Reserved instances for the data ingestion layer, which may not be optimal for sporadic and unpredictable workloads.\n\nOption E suggests purchasing a 1-year EC2 instance Savings Plan for the front end and API layer, but Compute Savings Plans are typically more suitable for predictable workloads.","comment_id":"931395","poster":"cookieMr"},{"comment_id":"910924","timestamp":"1685515260.0","poster":"Abrar2022","upvote_count":"2","content":"Spot instances for data injection because the task can be terminated at anytime and tolerate disruption. Compute Saving Plan is cheaper than EC2 instance Savings plan."},{"upvote_count":"1","comment_id":"908347","content":"EC2 instance Savings Plans are not applied to Fargate or Lambda","timestamp":"1685251080.0","poster":"Abrar2022"},{"poster":"Noviiiice","comment_id":"841615","content":"Why not B?","upvote_count":"1","comments":[{"comment_id":"859359","content":"because onDemand is more expensive than spot additionally that the workload has no problem with being interrupted at any time","upvote_count":"2","poster":"SkyZeroZx","timestamp":"1680469860.0"}],"timestamp":"1679030460.0"},{"comment_id":"760588","content":"Selected Answer: AC\nTo optimize the cost of running this application on AWS, you should consider the following options:\n\nA. Use Spot Instances for the data ingestion layer\nC. Purchase a 1-year Compute Savings Plan for the front-end and API layer\n\nTherefore, the most cost-effective solution for hosting this application would be to use Spot Instances for the data ingestion layer and to purchase either a 1-year Compute Savings Plan or a 1-year EC2 instance Savings Plan for the front-end and API layer.","comments":[{"upvote_count":"1","content":"Yes, but in the question it also states that it is 'Unpredictable' So, On-Demand is suitable over Spot Instance right which makes BC as the answer","comment_id":"991745","timestamp":"1693187400.0","comments":[{"timestamp":"1716882360.0","poster":"lofzee","upvote_count":"2","comment_id":"1220064","content":"the question clearly says \"can be interrupted at any time\" - anything that mentions these words with cost saving, you should automatically think Spot instances"},{"poster":"awashenko","content":"Spot instances can auto scale so Spot is still correct.","upvote_count":"2","comment_id":"1041102","timestamp":"1697056980.0"}],"poster":"AKBM7829"}],"timestamp":"1672289700.0","upvote_count":"3","poster":"Buruguduystunstugudunstuy"},{"content":"Selected Answer: AC\nToo obvious answer.","poster":"techhb","comment_id":"757812","upvote_count":"1","timestamp":"1672085940.0"},{"content":"Selected Answer: AC\nAC\n can be interrupted at any time => spot","upvote_count":"3","poster":"berks","comment_id":"755359","timestamp":"1671931320.0"},{"poster":"TECHNOWARRIOR","upvote_count":"1","timestamp":"1671879660.0","comment_id":"754820","content":"A,E::\nSavings Plan — EC2\nSavings Plan offers almost the same savings from a cost as RIs and adds additional Automation around how the savings are being applied. One way to understand is to say that EC2 Savings Plan are Standard Reserved Instances with automatic switching depending on Instance types being used within the same instance family and additionally applied to ECS Fargate and Lambda.\n\nSavings Plan — Compute\nSavings Plan offers almost the same savings from a cost as RIs and adds additional Automation around how the savings are being applied. For example, they provide flexibility around instance types and regions so that you don’t have to monitor new instance types that are being launched. It is also applied to Lambda and ECS Fargate workloads. One way to understand is to say that Compute Savings Plan are Convertible Reserved Instances with automatic switching depending on Instance types being used."},{"upvote_count":"1","poster":"career360guru","content":"Selected Answer: AC\nA and C","timestamp":"1671313200.0","comment_id":"748436"},{"content":"its A and C . https://www.densify.com/finops/aws-savings-plan","poster":"rjam","upvote_count":"1","timestamp":"1668508080.0","comment_id":"718669"},{"timestamp":"1666868760.0","content":"Selected Answer: AC\napi is not EC2.need to use compute savings plan","comment_id":"705466","upvote_count":"4","poster":"bunnychip"},{"timestamp":"1666300680.0","comment_id":"700308","upvote_count":"4","comments":[{"upvote_count":"6","content":"Isn't the EC2 Instance Savings Plan not applicable to Fargate and Lambda?\nhttps://aws.amazon.com/savingsplans/compute-pricing/","poster":"capepenguin","comment_id":"705262","timestamp":"1666849020.0"},{"content":"I Agree","upvote_count":"1","timestamp":"1683087660.0","poster":"Yadav_Sanjay","comment_id":"888137"}],"poster":"Chunsli","content":"E makes more sense than C. See https://aws.amazon.com/savingsplans/faq/, EC2 instance Savings Plan (up to 72% saving) costs less than Compute Savings Plan (up to 66% saving)"}],"isMC":true,"answer_description":"","answer":"AC","url":"https://www.examtopics.com/discussions/amazon/view/86083-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"sfQhn2E0jgSawDIsRTWy","choices":{"A":"Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.","D":"Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region.","B":"Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.","C":"Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB."},"exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/85439-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"answer_ET":"A","topic":"1","question_id":68,"answers_community":["A (72%)","B (24%)","3%"],"question_text":"A company runs a web-based portal that provides users with global breaking news, local alerts, and weather updates. The portal delivers each user a personalized view by using mixture of static and dynamic content. Content is served over HTTPS through an API server running on an Amazon EC2 instance behind an Application Load Balancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as possible.\nHow should a solutions architect design the application to ensure the LEAST amount of latency for all users?","isMC":true,"unix_timestamp":1665721260,"timestamp":"2022-10-14 06:21:00","discussion":[{"poster":"huiy","upvote_count":"34","comment_id":"694483","timestamp":"1665721260.0","comments":[{"timestamp":"1689615120.0","content":"Also, option B does not use CloudFront which means all the traffic will go through the internet; So, despite deploying resources in two regions and using the lowest latency point, that public internet connection might probably be slower than a connection through a private aws network as Cloudfront can use.","poster":"MutiverseAgent","comment_id":"954448","upvote_count":"4"}],"content":"Selected Answer: A\nAnswer is A.\nAmazon CloudFront is a web service that speeds up distribution of your static and dynamic web content\nhttps://www.examtopics.com/discussions/amazon/view/81081-exam-aws-certified-solutions-architect-associate-saa-c02/"},{"content":"Selected Answer: B\nAnswer should be B,\n\nCloudFront reduces latency if its only static content, which is not the case here.\nFor Dynamic content, CF cant cache the content so it sends the traffic through the AWS Network which does reduces latency, but it still has to travel through another region.\n\nFor the case with 2 region and Route 53 latency routing, Route 53 detects the nearest resouce (with lowest latency) and routes the traffic there. Because the traffic does not have to travel to resources far away, it should have the least latency in this case here.","timestamp":"1666803300.0","poster":"Six_Fingered_Jose","upvote_count":"16","comment_id":"704873","comments":[{"poster":"lofzee","content":"All it takes is for you to go to the Amazon Cloudfront webpage hosted by AWS where it tells you \"Amazon CloudFront is a content delivery network (CDN) service that helps you distribute your static and dynamic content quickly and reliably with high speed\".\n\nAnswer is A bro. Your answer is B literally makes no sense. Do some studying.","timestamp":"1716882780.0","comment_id":"1220066","upvote_count":"9"},{"comment_id":"732007","upvote_count":"1","content":"Can you pls. provide a ref. link from where this info. got extracted?","timestamp":"1669844220.0","poster":"Aamee","comments":[{"upvote_count":"4","poster":"manuelemg2007","comment_id":"963150","content":"this is link https://aws.amazon.com/es/blogs/aws-spanish/cloudfront-para-la-distribucion-de-contenido-estatico-y-dinamico/","timestamp":"1690322040.0"}]},{"upvote_count":"4","poster":"Abdou1604","timestamp":"1696492440.0","comment_id":"1025369","content":"What about accross the word :)"},{"timestamp":"1667647860.0","content":"Cf works for both static and dynamic content","comment_id":"711725","upvote_count":"12","poster":"Onimole"},{"poster":"awsgeek75","upvote_count":"1","timestamp":"1705428720.0","comment_id":"1124406","content":"two regions won't cover the whole world."},{"poster":"Mahadeva","timestamp":"1673114280.0","upvote_count":"6","comment_id":"768795","content":"CloudFront does not cache dynamic content. But Latency can be still low for dynamic content because the traffic is on the AWS global network which is faster than the internet.","comments":[{"timestamp":"1673965020.0","content":"Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. When users request your content, CloudFront delivers it through a worldwide network of edge locations that provide low latency and high performance.","poster":"Joxtat","upvote_count":"8","comment_id":"779010"}]},{"timestamp":"1703586180.0","upvote_count":"3","poster":"pentium75","content":"So if I set up in the application in two regions, say, us-east-1 and us-east-2, this would reduce latency for users in Italy?","comment_id":"1105886"}]},{"poster":"tch","content":"Selected Answer: C\nDynamic content can also be served via CloudFront by configuring the WordPress website as an origin. Since dynamic content includes personalized content, you need to configure CloudFront to forward certain HTTP cookies and HTTP headers as part of a request to your custom origin server. CloudFront uses the forwarded cookie values as part of the key that identifies a unique object in its cache. To ensure that you maximize the caching efficiency, you should configure CloudFront to only forward those HTTP cookies and HTTP headers that really vary the content (not cookies that are only used on the client side or by third-party applications, for example, for web analytics).","comment_id":"1408343","upvote_count":"1","timestamp":"1742773440.0"},{"upvote_count":"1","content":"Selected Answer: A\nA is correct, because the dynamic content is also cached - which means first time the data is brought in, then subsequently it does not have to go to the source to get the same content.\nThere are many ways to set these combinations up, even with Route 53, but of the given options, \"A\" seems to fit the bill best.","comment_id":"1352533","poster":"Dharmarajan","timestamp":"1738862160.0"},{"upvote_count":"1","timestamp":"1738344120.0","content":"Selected Answer: A\nCloudFront is suitable for both static and dynamic content.","poster":"satyaammm","comment_id":"1349603"},{"upvote_count":"1","timestamp":"1737417060.0","poster":"FlyingHawk","content":"Selected Answer: B\nThe best approach combines multi-region deployment for dynamic content and Amazon CloudFront for static content. However, none of the options explicitly describe this combination. Among the given options, Option B is the best because:\n\nIt uses two regions to reduce latency for dynamic content.\n\nRoute 53 latency routing ensures users are directed to the closest region.\n\nFor a more optimal solution (not listed in the options):\n\nDeploy the application stack in multiple AWS Regions.\n\nUse Amazon CloudFront to serve static content from edge locations.\n\nUse Route 53 latency routing to direct users to the closest region for dynamic content.","comment_id":"1343934","comments":[{"upvote_count":"1","content":"A : CloudFront is a global CDN that caches static content at edge locations, reducing latency for users worldwide.\n\nHowever, dynamic content served through CloudFront with a single ALB origin will still incur latency for users far from the single AWS Region.\n\nThis solution does not address latency for dynamic content effectively","comments":[{"comment_id":"1343936","timestamp":"1737417180.0","content":"B - Deploying in two regions reduces latency for users closer to those regions.\n\nRoute 53 latency routing ensures users are directed to the closest region.\n\nHowever, this solution does not leverage a CDN for static content, which could further reduce latency.\n\nVerdict: Better than Option A but not optimal for static content delivery.","poster":"FlyingHawk","upvote_count":"1"}],"timestamp":"1737417120.0","poster":"FlyingHawk","comment_id":"1343935"}]},{"content":"Selected Answer: A\nAmazon cloud front is a better choice in terms of delivering both static and dynamic content globally. Also option B says deploy the application stack to only two regions. but the use case is to access the portal globally. Don't think Amazon Route 53 latency routing policy will have a bigger impact in terms of low latency.","comment_id":"1328030","upvote_count":"2","timestamp":"1734452040.0","poster":"rmanuraj"},{"timestamp":"1726912560.0","comment_id":"1287279","upvote_count":"2","content":"Selected Answer: B\nAns B - \"If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency\" ...because it needs to be dynamic: \"Latency between hosts on the internet can change over time as a result of changes in network connectivity and routing. Latency-based routing is based on latency measurements taken over a period of time, and the measurements reflect these changes. A request that is routed to the Oregon Region this week might be routed to the Singapore Region next week.\"\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html","poster":"PaulGa"},{"timestamp":"1718777280.0","poster":"ChymKuBoy","upvote_count":"1","comment_id":"1232734","content":"Selected Answer: B\nB for sure"},{"timestamp":"1714824000.0","content":"With Amazon CloudFront, your end users connections are terminated at CloudFront locations closer to them, which helps in reducing the overall round trip time required to establish a connection. This is irrespective of static a dynamic content.","poster":"ManikRoy","comment_id":"1206489","upvote_count":"2"},{"content":"Selected Answer: A\nYou can still have improved performance by distributing the dynamic traffic through CDN instead of ALB. Refer below link. \nAlso for other 2 options, using just 2 other regions for world wide distribution doesn't make much of a sense.\nhttps://aws.amazon.com/cloudfront/dynamic-content/","poster":"ManikRoy","upvote_count":"3","timestamp":"1714823760.0","comment_id":"1206488"},{"poster":"Uzbekistan","comments":[{"poster":"eb7be10","content":"C was my choice for the reasons stated here. What am I missing?","upvote_count":"3","timestamp":"1713953580.0","comment_id":"1201253","comments":[{"comments":[{"content":"https://docs.aws.amazon.com/whitepapers/latest/best-practices-wordpress/dynamic-content.html","poster":"tch","upvote_count":"1","comment_id":"1408357","timestamp":"1742773500.0"}],"timestamp":"1716882900.0","comment_id":"1220068","upvote_count":"3","content":"How can you SERVE content from a load balancer?\nAmazon Cloudfront is designed for static and dynamic content. Why would you pick any other option that isn't A?","poster":"lofzee"}]}],"content":"Selected Answer: C\nCloudFront for Static Content: By leveraging Amazon CloudFront, static content such as images, stylesheets, and scripts can be cached and distributed globally across a network of edge locations. This ensures that users receive static content from the nearest edge location, reducing latency and improving performance.\n\nServe Dynamic Content from ALB: Since dynamic content requires real-time processing and cannot be effectively cached at edge locations, serving dynamic content directly from the Application Load Balancer (ALB) is appropriate. The ALB can handle dynamic requests efficiently within the AWS Region where the application is deployed.","timestamp":"1711207020.0","comment_id":"1180970","upvote_count":"3"},{"poster":"Parul25","comment_id":"1137116","upvote_count":"2","timestamp":"1706737440.0","content":"CloudFront improves the performance, availability, and security of your dynamic content but not the latency as compared to Route 53 Latency Routing policy. Hence option B\nhttps://aws.amazon.com/cloudfront/dynamic-content/"},{"poster":"Parul25","comment_id":"1137112","timestamp":"1706736960.0","upvote_count":"1","content":"I choose option B.\n\nWhile CloudFront can accelerate content delivery by caching static content at edge locations, it may not be the most effective solution in this scenario. Since the portal delivers a mixture of static and dynamic content, leveraging Route 53 latency routing for dynamic content delivery ensures that users are directed to the nearest AWS Region hosting the dynamic content."},{"upvote_count":"9","comment_id":"1105885","content":"Selected Answer: A\n\"Least amount of latency for all users\" \"across the world\" = CloudFront, thus B and D are out. Also, deploying the stack in \"two regions\" would benefit those two regions, but not users \"across the world\".\n\nCloudFront can also cache dynamic content, thus A.","poster":"pentium75","timestamp":"1703586060.0"},{"upvote_count":"2","poster":"Bennyseg","comment_id":"1088032","content":"Selected Answer: A\nAnswer is option A:\nEarth Networks uses a CDN so that they can provide dynamic and personalized web based content quickly to their users with very low latency and high performing response times. Specifically, they need to be able to provide local information to the end user, in near real time, and need a CDN that allows them to adjust things like time to live, query strings, and cookie information so that they can pass all that information back to the origin to pull just what the user needs.","timestamp":"1701728280.0"},{"upvote_count":"2","poster":"AZ_Master","content":"Selected Answer: B\nThose are personalized content - where CloudFront could not help much.","comment_id":"1074397","timestamp":"1700359200.0"},{"timestamp":"1697647920.0","content":"Selected Answer: A\n\"A\" because cloud front is more efficient","poster":"David_Ang","comment_id":"1047081","upvote_count":"2"},{"upvote_count":"2","poster":"Wayne23Fang","comment_id":"1038973","content":"Selected Answer: B\nA or B very close. But the (B) camp arguments earlier made me lean to B: Cloudfront doesn't help much for dynamic content, which is probably the bottleneck; On average, two dynamic server could cut response half.","timestamp":"1696882080.0"},{"poster":"BrijMohan08","timestamp":"1694913060.0","comment_id":"1009480","upvote_count":"2","content":"Selected Answer: D\nOption D is the most suitable choice for minimizing latency for all users. It leverages the use of multiple AWS regions, geolocation routing, and the ALB to ensure that users are directed to the closest region, reducing latency for both static and dynamic content. This approach provides a high level of availability and performance for global users."},{"comment_id":"1001162","upvote_count":"2","timestamp":"1694059620.0","content":"Selected Answer: A\nCloudFront to the rescue....whoosh","poster":"TariqKipkemei"},{"timestamp":"1692213120.0","upvote_count":"3","poster":"Guru4Cloud","comment_id":"982921","content":"Selected Answer: A\nThe solution that will ensure the LEAST amount of latency for all users is:\n\nA. Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.\n\nHere's why:\n\nOption A (Single AWS Region, Amazon CloudFront for both static and dynamic content):\n\nDeploying the application stack in a single AWS Region helps reduce complexity and potential data synchronization issues that might arise from using multiple regions"},{"content":"Selected Answer: B\nI think CloudFront does not improve latency in this case, because CF works as kind of cache of data. Cache works fine in case of static data, but here each user can have its own dynamically created data, this every user will need to go to origin. So in this case CF can make the latency worse. On the other hand route53 with latency routing to ALB in different regions may actually increase the average user latency.","comment_id":"974043","poster":"MM_Korvinus","upvote_count":"1","timestamp":"1691340660.0"},{"content":"Selected Answer: A\nIt's A, according this page (https://aws.amazon.com/cloudfront/dynamic-content/) CloudFront is commonly used for \"News, sports, local, weather\" as this is content mostly bounded to a region.","poster":"MutiverseAgent","upvote_count":"5","comments":[{"content":"Also, option B does not use CloudFront which means all the traffic will go through the internet; So, despite deploying resources in two regions and using the lowest latency point, that public internet connection might probably be slower than a connection through a private aws network as Cloudfront can use.","timestamp":"1689615420.0","poster":"MutiverseAgent","comment_id":"954453","upvote_count":"2"}],"comment_id":"954437","timestamp":"1689614640.0"},{"content":"Selected Answer: A\nCloudFront is a CDN thats is well adapted for dynamic content. \nNews, sports, local, weather\nWeb applications of this type often have a geographic focus with customized content for end users. Content can be cached at edge locations for varying lengths of time depending on type of content. For example, hourly updates can be cached for up to an hour, while urgent alerts may only be cached for a few seconds so end users always have the most up to date information available to them. A content delivery network is a great platform for serving common types of experiences for news and weather such as articles, dynamic map tiles, overlays, forecasts, breaking news or alert tickers, and video.\n\nhttps://aws.amazon.com/cloudfront/dynamic-content/","poster":"ayeah","timestamp":"1687541280.0","upvote_count":"2","comment_id":"931822"},{"timestamp":"1685618820.0","comment_id":"912009","upvote_count":"2","poster":"smartegnine","content":"I would definitely go to C \nf you are serving dynamic content such as web applications or APIs directly from an Amazon Elastic Load Balancer (ELB) or Amazon EC2 instances to end users on the internet, you can improve the performance, availability, and security of your content by using Amazon CloudFront as your content delivery network. \n\nhttps://aws.amazon.com/cloudfront/dynamic-content/"},{"upvote_count":"2","comment_id":"911864","timestamp":"1685605920.0","poster":"antropaws","content":"Selected Answer: A\nA is correct. CF distributes the content globally. Why not deploy the application in 4 o 5 regions instead of 2? It's an arbitrary choice, that's one of the reasons why B and D are not a solid solution."},{"upvote_count":"2","comment_id":"904863","timestamp":"1684843020.0","poster":"Bmarodi","content":"Selected Answer: A\nI gor for option A, CF uses edge locations to speed up S3 content, both static and dynamic, hence A is right ans."},{"upvote_count":"1","timestamp":"1684728000.0","comment_id":"903700","poster":"bgsanata","content":"Selected Answer: B\nI would say B.\n2 regions is always better if you aim for better distribution of the traffic. This will split the amount of request send to the Single EC2 instance by half => indirectly improve latency.\nIt's true that CloudFront improve latency but it's hard to say if this will be true for ALL users. Having second region will definitely improve the performance for the users will less latency atm."},{"comment_id":"885155","poster":"cheese929","upvote_count":"3","timestamp":"1682854740.0","content":"Selected Answer: A\nA is correct. Cloudfront can serve both static and dynamic content fast. \n\nhttps://aws.amazon.com/cloudfront/dynamic-content/"},{"comment_id":"884796","content":"Selected Answer: B\nthe lowest latency (option B) is not always equal to the closest resource (option D). And the requirement ask for lowest latency","poster":"kevinkn","upvote_count":"1","timestamp":"1682815860.0"},{"content":"A.\nIf you are serving dynamic content such as web applications or APIs directly from an Amazon Elastic Load Balancer (ELB) or Amazon EC2 instances to end users on the internet, you can improve the performance, availability, and security of your content by using Amazon CloudFront as your content delivery network.\n\nhttps://aws.amazon.com/cloudfront/dynamic-content/","poster":"Shrestwt","timestamp":"1682050260.0","upvote_count":"3","comment_id":"876181"},{"content":"CloudFront caches the static content. It also accepts requests for dynamic content and forward it to the ALB via AWS backbone (very fast).","comment_id":"872267","upvote_count":"2","timestamp":"1681693080.0","poster":"C_M_M"},{"poster":"TECHNOWARRIOR","comment_id":"865522","timestamp":"1681045800.0","content":"ANSWER -B :To achieve the least amount of latency for all users, the best approach would be to deploy the application stack in two AWS Regions and use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest region. This approach will ensure that users are directed to the lowest latency endpoint available based on their location, which can significantly reduce latency and improve the performance of the application.\nWhile using Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin can also improve the performance of the application, it may not be the best approach to achieve the least amount of latency for all users. This is because CloudFront may not always direct users to the closest endpoint based on their location, which can result in higher latency for some users.\nTherefore, using an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest region is the best approach to achieve the least amount of latency for all users","upvote_count":"2"},{"timestamp":"1680654000.0","content":"Selected Answer: A\nCloudfront is global and serves all regions equally. Route 53 latency option provides the lowest latency option of the two regions, but this could still be terrible latency for users outside of those regions.","upvote_count":"3","poster":"Bang3R","comment_id":"861630"},{"upvote_count":"1","poster":"jaswantn","timestamp":"1679598120.0","comment_id":"848571","content":"Having stack in two Regions is always better than one Region, when portal has to be used globally. This crosses out Option A and C. \nRequirement is to have LEAST amount of latency , so instead of choosing Route 53 Geolocation routing policy (Option D), we should go for Latency based routing; which is Option B."},{"timestamp":"1678903320.0","comment_id":"840137","content":"Something is wrong with the question, or the answers.\n\nThe best way to do it is deploy the website in one region, use CloudFront to reduce latency and use a geolocation Route 53 routing policy as the application provides local alerts and weather alerts.\n\nWithout geolocation the application will provide local alerts in London to people living in Australia.\n\nAnswer D is the closet, however - it's wrong.","upvote_count":"3","poster":"UnluckyDucky"},{"comment_id":"838055","timestamp":"1678723620.0","poster":"mell1222","upvote_count":"2","content":"Selected Answer: A\nUse Amazon CloudFront as a content delivery network (CDN) to distribute static and dynamic content to edge locations around the world"},{"comment_id":"826080","content":"Selected Answer: A\nA for me.","poster":"Gary_Phillips_2007","timestamp":"1677688800.0","upvote_count":"1"},{"content":"A is impossible I think. Because when using Amazon CloudFront to serve static content, the content should be stored in an Amazon S3 bucket, and CloudFront should be configured to use that S3 bucket as the origin instead of ALB.","comment_id":"825484","upvote_count":"1","poster":"KZM","timestamp":"1677638040.0"},{"comment_id":"820205","upvote_count":"5","poster":"moaaz86","timestamp":"1677223380.0","content":"For those who doubt the fact about CloudFront and dynamic content, see this video on how Slack utilized CloudFront for this purpose. Pretty interesting stuff.\n\nhttps://aws.amazon.com/cloudfront/dynamic-content/"},{"content":"Selected Answer: B\nAs its a new site even the static content will be frequently refeshed, requiring cloudfront to request the content, a two region solution looks best","poster":"anthony2021","timestamp":"1677179640.0","comment_id":"819613","upvote_count":"1"},{"upvote_count":"4","comment_id":"812781","timestamp":"1676709060.0","poster":"Rehan33","content":"Why not going for option C\nUse cloud front for static content \nUse application load balancer for dynamic content"},{"content":"Selected Answer: A\nCloudfront does static and dynamic. It is purpose is to provide common data in the shortest time possible.","comment_id":"810619","poster":"Help2023","upvote_count":"2","timestamp":"1676549580.0"},{"poster":"KZM","content":"I think, it is A.\nOption A, deploying the application stack in a single AWS Region and using Amazon CloudFront to serve all static and dynamic content, may not provide the least amount of latency for all users as users located far away from the single region may experience higher latency due to the distance between their location and the region hosting the application stack.\n\nOption B, deploying the application stack in two AWS Regions and using an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest region, is a better solution as it allows the application to be closer to the users, resulting in lower latency for users located in different regions of the world.","upvote_count":"2","comment_id":"801860","timestamp":"1675850280.0","comments":[{"poster":"KZM","content":"CloudFront is not designed to cache dynamic content, but it can cache static content, such as images, videos, or JavaScript and CSS files. Dynamic content is content that changes frequently, such as news articles or weather updates, and is generated by a server in real-time in response to each user's request.","upvote_count":"1","timestamp":"1675911900.0","comment_id":"802768"}]},{"timestamp":"1673965800.0","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html#CloudFrontRegionaledgecaches","upvote_count":"2","poster":"Joxtat","comment_id":"779025"},{"comment_id":"774948","timestamp":"1673653920.0","poster":"mj61","upvote_count":"2","content":"Option A is incorrect because it deploys the application stack in a single AWS region and uses Amazon CloudFront to serve all static and dynamic content. While this approach will cache the static content at edge locations, it does not take into account the geographical location of the users, and therefore will not minimize the latency for all users. The dynamic content will still be served from the origin which is the ALB, so users far from the region where the ALB is deployed will have high latency.\nIt also does not provide redundancy and fault tolerance as it only deployed in single region.\n\nIn summary, deploying the application stack in a single region and using CloudFront to serve all content may improve performance for users in close proximity to the region, but it will not minimize latency for all users globally, while option B takes into account the geographical location of the users and serves them the content from the closest region which results in low latency."},{"content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/","upvote_count":"4","timestamp":"1672780980.0","poster":"aba2s","comment_id":"765065"},{"poster":"Buruguduystunstugudunstuy","timestamp":"1672290840.0","comments":[{"timestamp":"1673965740.0","upvote_count":"1","comment_id":"779022","content":"Answer is A.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html#CloudFrontRegionaledgecaches","poster":"Joxtat"},{"upvote_count":"4","poster":"Buruguduystunstugudunstuy","timestamp":"1672291080.0","content":"Link to the documentation for Amazon Route 53 Latency-Based Routing:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency\n\nThis routing policy allows you to route traffic to the Amazon EC2 instance, Amazon S3 bucket, Amazon CloudFront distribution, or other resources with the lowest latency. It is useful when you want to serve users the content from the location that provides the lowest latency.","comment_id":"760600"},{"poster":"ruqui","timestamp":"1684769700.0","content":"B is wrong!! how can you determine which 2 regions to use? (nothing is specified about the user locations) ... on the other hand CloudFront serve both static and dynamic content and, even though dynamic content is not cached, the information goes through optimized internal AWS network ... answer should be A","comment_id":"904180","upvote_count":"1"}],"upvote_count":"4","comment_id":"760597","content":"Selected Answer: B\nI would go for Option B as the correct answer.\n\nBy deploying the application stack in two regions and using an Amazon Route 53 latency routing policy, you can ensure that users are served from the ALB in the region that is closest to them, reducing latency. Amazon Route 53 latency routing works by monitoring the latency between the users and the different regions and routing traffic to the region with the lowest latency.\n\nOption A is incorrect, while using Amazon CloudFront to serve static and dynamic content can improve the performance of the application, deploying the application stack in a single region may not be sufficient to reduce latency for users located in different parts of the world.\n\nTherefore, the correct solution to ensure the least amount of latency for all users is to deploy the application stack in two AWS Regions and use either an Amazon Route 53 latency routing policy or an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest region."},{"upvote_count":"3","timestamp":"1671313740.0","poster":"career360guru","comments":[{"content":"Read following two articles on why A is right option.\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/\nhttps://aws.amazon.com/cloudfront/dynamic-content/","comment_id":"754605","poster":"career360guru","upvote_count":"4","timestamp":"1671838620.0"}],"content":"Selected Answer: A\nA is the best option as Cloudfront can deliver the content from the Edge location that will be lowest latency for static content to all users across the globe. Deploying in two region is not sufficient for the users that might be still far away from two regions. So option B will will not provide lowest possible latency to \"ALL users\" which is the key here.","comment_id":"748442"},{"poster":"nexus2020","comment_id":"743520","content":"Selected Answer: A\nB is not right, reason is that the users are all over the world. so 2 regions will not make sense as it will not cover all over the world. So, A.","timestamp":"1670899140.0","upvote_count":"4"},{"content":"Answer is A- \nCloudFront is used for fast content delivery( Static and dynamic content) by leveraging the AWS global network and other benefits, such as security, edge capabilities, and availability.\nOne of the purposes of using CloudFront is to reduce the number of requests that your origin server must respond to directly. With CloudFront caching, more objects are served from CloudFront edge locations, which are closer to your users. This reduces the load on your origin server and reduces latency.","upvote_count":"1","poster":"Qjb8m9h","comment_id":"743443","timestamp":"1670890980.0"},{"comment_id":"742250","upvote_count":"2","poster":"tz1","content":"A is the answer. CF can serve dynamic content as well https://aws.amazon.com/cloudfront/dynamic-content/","timestamp":"1670805720.0"},{"upvote_count":"3","timestamp":"1670577000.0","comment_id":"739949","poster":"RBSK","content":"Selected Answer: A\nKey requirement is \"The company wants the portal to provide this content to its users across the world as quickly as possible\". A do cover the entire globe whereas B does not."},{"timestamp":"1668336780.0","content":"Selected Answer: A\nAnswer A. refer Question #: 612 in SAA-C002 \nhttps://www.examtopics.com/discussions/amazon/view/81081-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"Pamban","upvote_count":"2","comment_id":"717251"},{"upvote_count":"2","comment_id":"714168","comments":[{"timestamp":"1670448840.0","content":"But it also states that the stack is deployed in a 'Single' Region...so how come it can come from a diff. region?..","comment_id":"738407","upvote_count":"1","poster":"Aamee"}],"poster":"Bevemo","content":"Selected Answer: B\nB = as quickly as possible = lowest latency dynamic + static content.\nCloudFront works for static and dynamic content, but the dynamic content delivery will not be 'as fast as possible' if it's coming from another region.","timestamp":"1667953320.0"},{"timestamp":"1666841820.0","poster":"tubtab","content":"Selected Answer: A\naaaaaaaaaaaaa","comment_id":"705188","upvote_count":"3"},{"poster":"trancex","comment_id":"694637","upvote_count":"2","timestamp":"1665739860.0","content":"not sure but A https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/"}],"answer":"A","answer_description":"","answer_images":[]},{"id":"ynqOYn90qtwHgrUbnvX9","question_id":69,"isMC":true,"unix_timestamp":1667116740,"answer_description":"","exam_id":31,"question_images":[],"answer":"C","timestamp":"2022-10-30 08:59:00","topic":"1","answer_ET":"C","choices":{"A":"Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda for the application in AWS Application Auto Scaling.","B":"Configure Amazon CloudFront to forward requests to a Network Load Balancer. Use AWS Lambda for the application in an AWS Application Auto Scaling group.","C":"Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group.","D":"Configure Amazon API Gateway to forward requests to an Application Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group."},"question_text":"A gaming company is designing a highly available architecture. The application runs on a modified Linux kernel and supports only UDP-based traffic. The company needs the front-end tier to provide the best possible user experience. That tier must have low latency, route traffic to the nearest edge location, and provide static IP addresses for entry into the application endpoints.\nWhat should a solutions architect do to meet these requirements?","answers_community":["C (100%)"],"discussion":[{"comments":[{"poster":"praveenas400","content":"Explained very well. ty","timestamp":"1672768440.0","upvote_count":"2","comment_id":"764913"},{"content":"Thank you, your explanation helped me to better understand even the answer of question 29","comment_id":"724888","timestamp":"1669178760.0","poster":"iCcma","upvote_count":"4"},{"comment_id":"739858","upvote_count":"9","content":"On top of this, lambda would not be able to run application that is running on a modified Linux kernel. The answer is C .","poster":"stepman","timestamp":"1670568960.0"}],"timestamp":"1667116740.0","content":"Correct Answer: C \nAWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.","upvote_count":"87","poster":"dokaedu","comment_id":"707684"},{"upvote_count":"10","timestamp":"1672291500.0","comments":[{"upvote_count":"4","poster":"Buruguduystunstugudunstuy","comment_id":"760608","timestamp":"1672291920.0","content":"My mistake, correction on Option A, it is the Application Load Balancers do not support UDP traffic. They are designed to load balance HTTP and HTTPS traffic, and they do not support other protocols such as UDP."}],"content":"Selected Answer: C\nThe correct answer is Option C. To meet the requirements;\n\n* AWS Global Accelerator is a service that routes traffic to the nearest edge location, providing low latency and static IP addresses for the front-end tier. It supports UDP-based traffic, which is required by the application.\n\n* A Network Load Balancer is a layer 4 load balancer that can handle UDP traffic and provide static IP addresses for the application endpoints.\n\n* An EC2 Auto Scaling group ensures that the required number of Amazon EC2 instances is available to meet the demand of the application. This will help the front-end tier to provide the best possible user experience.\n\nOption A is not a valid solution because Amazon Route 53 does not support UDP traffic.\nOption B is not a valid solution because Amazon CloudFront does not support UDP traffic.\nOption D is not a valid solution because Amazon API Gateway does not support UDP traffic.","poster":"Buruguduystunstugudunstuy","comment_id":"760606"},{"content":"Selected Answer: C\nGobal Accelarator rules the roost in this use case! gives a static IP, and does everything asked here.","timestamp":"1738862280.0","poster":"Dharmarajan","comment_id":"1352536","upvote_count":"1"},{"poster":"PaulGa","upvote_count":"3","timestamp":"1726913160.0","comment_id":"1287281","content":"Selected Answer: C\nAns C - hint: \"That tier must have low latency, route traffic to the nearest edge location, and provide static IP addresses for entry into the application endpoints.\"\n\n\"AWS Global Accelerator... provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones... [to] always routes user traffic to the optimal endpoint based on performance, reacting instantly to changes in application health, your user’s location, and policies that you configure.\"\n\nhttps://aws.amazon.com/global-accelerator/faqs/"},{"content":"Selected Answer: C\nNon HTTP/S, the answer is always Global Accelerator without doubts. GA serves like Cloudfront on providing low latency through edge locations, with the exception of handling different protocols.","poster":"huaze_lei","upvote_count":"3","timestamp":"1725542460.0","comment_id":"1278952"},{"comment_id":"1220071","timestamp":"1716883200.0","content":"Whenever I see this line : \"and provide static IP addresses for entry into the application endpoints.\" - my brain automatically thinks Global Accelerator.","poster":"lofzee","upvote_count":"1"},{"comment_id":"1164641","content":"If the situation demands for UDP or some protocols that are not at application level then it would be better to use Global Accelerator and here they need top notch perfromance hence using it with NLB would be the best answer. Cloud Front does not support UDP nor does it support use of NLB","timestamp":"1709457720.0","poster":"sidharthwader","upvote_count":"2"},{"upvote_count":"3","poster":"Murtadhaceit","timestamp":"1701349020.0","comment_id":"1084366","content":"Selected Answer: C\nUDP: NLB.\nStatic IP: Global Accelerator."},{"poster":"TariqKipkemei","timestamp":"1694059980.0","upvote_count":"2","comment_id":"1001163","content":"Selected Answer: C\nUDP, static IP = Global Accelerator and Network Load Balancer"},{"upvote_count":"3","comment_id":"982967","timestamp":"1692216060.0","content":"Selected Answer: C\nAWS Global Accelerator provides static IP addresses that serve as a fixed entry point to application endpoints. This allows optimal routing to the nearest edge location.\nUsing a Network Load Balancer (NLB) allows support for UDP traffic, as NLBs can handle TCP and UDP protocols.\nThe application runs on a modified Linux kernel, so using Amazon EC2 instances directly will provide the needed customization and low latency.\nThe EC2 instances can be auto scaled based on demand to provide high availability.\nAPI Gateway and Application Load Balancer are more suited for HTTP/HTTPS and REST API type workloads. For a UDP gaming workload, Global Accelerator + NLB + EC2 is a better architectural fit.","poster":"Guru4Cloud"},{"content":"Selected Answer: C\nAWS Global Accelerator is designed to improve the availability and performance of applications by routing traffic through the AWS global network to the nearest edge locations, reducing latency. By configuring AWS Global Accelerator to forward requests to a Network Load Balancer, UDP-based traffic can be efficiently distributed across multiple EC2 instances in an Auto Scaling group. Using Amazon EC2 instances for the application allows for customization of the Linux kernel and support for UDP-based traffic. This solution provides static IP addresses for entry into the application endpoints, ensuring consistent access for users.\n\nOption A suggests using AWS Lambda for the application, but Lambda is not suitable for long-running UDP-based applications and may not provide the required low latency.\nOption B suggests using CloudFront, which is primarily designed for HTTP/HTTPS traffic and does not have native support for UDP-based traffic.\nOption D suggests using API Gateway, which is primarily used for RESTful APIs and does not support UDP-based traffic.","timestamp":"1687513680.0","poster":"cookieMr","upvote_count":"4","comment_id":"931435"},{"timestamp":"1685514120.0","content":"aws global accelarator provides static IP addresses.","poster":"Abrar2022","comment_id":"910904","upvote_count":"1"},{"poster":"Bmarodi","comment_id":"904927","timestamp":"1684846980.0","upvote_count":"2","content":"Selected Answer: C\nMy choice is option C, due to the followings: Amazon Global accelator route the traffic to nearest edge locations, it supports UDP-based traffic, and it provides static ip addresses as well, hence C is right answer."},{"comment_id":"858126","content":"Answer : C\nCloudFront : Doesn't support static IP addresses\nALB : Doesn't support UDP","poster":"bakamon","upvote_count":"1","timestamp":"1680367500.0"},{"upvote_count":"1","timestamp":"1678348620.0","content":"C - https://aws.amazon.com/global-accelerator/","poster":"Devsin2000","comment_id":"833691"},{"content":"Selected Answer: C\nTo meet the requirements of providing low latency, routing traffic to the nearest edge location, and providing static IP addresses for entry into the application endpoints, the best solution would be to use AWS Global Accelerator. This service routes traffic to the nearest edge location and provides static IP addresses for the application endpoints. The front-end tier should be configured with a Network Load Balancer, which can handle UDP-based traffic and provide high availability. Option C, \"Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group,\" is the correct answer.","poster":"SilentMilli","timestamp":"1673271720.0","upvote_count":"1","comment_id":"770457"},{"timestamp":"1672087920.0","content":"Selected Answer: C\nC is obvious choice here.","comment_id":"757840","upvote_count":"1","poster":"techhb"},{"comment_id":"748445","timestamp":"1671314040.0","upvote_count":"1","content":"Selected Answer: C\nC as Global Accelerator is the best choice for UDP based traffic needing static IP address.","poster":"career360guru"},{"content":"Selected Answer: C\nc correct","upvote_count":"1","poster":"Certified101","comment_id":"745385","timestamp":"1671044700.0"},{"comment_id":"743447","content":"CloudFront is designed to handle HTTP protocol meanwhile Global Accelerator is best used for both HTTP and non-HTTP protocols such as TCP and UDP. HENCE C is the ANSWER!","upvote_count":"1","poster":"Qjb8m9h","timestamp":"1670891640.0"},{"poster":"Wpcorgan","timestamp":"1669312380.0","comment_id":"726093","upvote_count":"1","content":"C is correct"},{"upvote_count":"1","comment_id":"712936","timestamp":"1667810520.0","content":"Selected Answer: C\nCloud Fronts supports both Static and Dynamic and Global Accelerator means low latency over UDP","poster":"PS_R"}],"url":"https://www.examtopics.com/discussions/amazon/view/86667-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[]},{"id":"2cvexcCbKojBZ3fpIRTN","question_images":[],"answer":"D","topic":"1","question_text":"A company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications. A different team will manage each application. The company needs a highly scalable solution that minimizes operational overhead.\nWhich solution will meet these requirements?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/86473-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":"","isMC":true,"choices":{"D":"Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.","C":"Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.","A":"Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.","B":"Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda."},"exam_id":31,"discussion":[{"upvote_count":"39","timestamp":"1667068860.0","content":"I think the answer here is \"D\" because usually when you see terms like \"monolithic\" the answer will likely refer to microservices.","comment_id":"707404","poster":"Ken701"},{"poster":"Bevemo","upvote_count":"21","comment_id":"714174","timestamp":"1667953860.0","content":"Selected Answer: D\nD is organic pattern, lift and shift, decompose to containers, first making most use of existing code, whilst new features can be added over time with lambda+api gw later.\nA is leapfrog pattern. requiring refactoring all code up front."},{"poster":"Mimine87","timestamp":"1743899520.0","content":"Selected Answer: D\nMonolith to Smaller Apps (Microservices-Ready)\nECS (or EKS) is ideal when breaking a monolithic app into smaller services (aka microservices).\n\nDifferent teams can manage individual containers running different parts of the app.","upvote_count":"1","comment_id":"1554313"},{"upvote_count":"1","timestamp":"1738862520.0","content":"Selected Answer: D\nD because the operational overhead is the smallest among the given options. The company may do all that breaking up of functionalities and let teams manage the parts, but operationally for the site, hosting in Containers is the lowest maintenance. No ASG tuning, no ALB limitations and so on.","poster":"Dharmarajan","comment_id":"1352540"},{"content":"Selected Answer: D\nThe company wants to keep as much of the front-end code and the backend code as possible, so containization is less code changes than B which uses lambda.","upvote_count":"1","comment_id":"1350660","timestamp":"1738530660.0","poster":"FlyingHawk"},{"content":"Selected Answer: C\nAuto scaling group meets highly scalable requirement. D is not right 1. it is unknown if the app can be containerized , 2. and it maintains EC2 as C, so D has no operational advantage. Microservice is not equivalent to container.","comment_id":"1349734","poster":"zdi561","upvote_count":"1","timestamp":"1738379640.0"},{"comment_id":"1298891","poster":"PaulGa","upvote_count":"3","timestamp":"1729104540.0","content":"Selected Answer: D\nAns D - hint: \"...break the application into smaller applications\""},{"poster":"PaulGa","upvote_count":"2","content":"Selected Answer: D\nAns D - hint: \"The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications.\" Containerisation will help the company achieve a scaleable, more manageable solution.","comment_id":"1287282","timestamp":"1726913400.0"},{"poster":"pedro_vieira","timestamp":"1721731380.0","upvote_count":"4","content":"For the folks suggesting Amplify: Have any of you actually shipped anything on Amplify? There are tons of adaptations needed to port a monolith to Amplify, specially around the backend that will need severe refactor.\nAnswer D allows for decomposing the application into different containers, enabling a distributed monolith.","comment_id":"1253609"},{"timestamp":"1714825920.0","upvote_count":"2","poster":"ManikRoy","comment_id":"1206499","content":"Selected Answer: B\nAmazon API Gateway and Amplify both server less. Also you can import your code from GitHub in the amplify.","comments":[{"upvote_count":"1","content":"Option D does not mention AWS Fargate which would cover the 'least operational overhead ' part.","timestamp":"1714825980.0","comment_id":"1206500","poster":"ManikRoy"}]},{"poster":"ManikRoy","timestamp":"1714825740.0","content":"The company wants to keep much of its existing code. So the preferable solution is ECS. However the option D does not mention AWS Fargate which would cover the 'least operational overhead ' part.","comment_id":"1206498","upvote_count":"2"},{"upvote_count":"4","content":"Different teams working means \" microservices based architecture\" so basically decoupling the application ..u can achieve this only by containerizing the app so answer is D","poster":"jbkrishna","comment_id":"1176296","timestamp":"1710742560.0"},{"timestamp":"1707928200.0","upvote_count":"4","comment_id":"1150340","poster":"NayeraB","content":"Selected Answer: B\nB allows for a serverless architecture using AWS Lambda functions, which are highly scalable and require minimal operational overhead. AWS Amplify can help in managing the front-end code, while Amazon API Gateway integrated with AWS Lambda can handle the backend services.\n\nD imo is not the best option in this scenario. While ECS can be a good choice for containerized workloads, it might introduce more operational overhead compared to a serverless solution like AWS Lambda and AWS Amplify."},{"poster":"awsgeek75","content":"Selected Answer: D\nI have a problem with this question. \n\"The company wants to keep as much of the front-end code and the backend code as possible\"\nSo containerization is the solution here (D)? ABC don't make much sense so I will go with D but using containers for FE/BE code and configuring ALB for ECS (hopefully for frontend containers) is a pain in practice. Maybe this is worded in a bad way.","upvote_count":"5","comment_id":"1124413","timestamp":"1705429080.0"},{"comment_id":"1120617","poster":"vip2","content":"Selected Answer: B\nOriginal state: monolithic with FE and BE code\nWanted state: seperate to mutilple components for diff. teams as Microservices\n\nB is correct to decouple monolithic to microservices.\n \nD still keep monolithic application in ECS.","timestamp":"1705050840.0","upvote_count":"3"},{"upvote_count":"2","comment_id":"1112428","content":"IT is B. AWS amplify.\nAWS Amplify will help seperate FE and BE. I agree with MM_Korvinus answer.","poster":"06042022","timestamp":"1704248400.0"},{"upvote_count":"3","timestamp":"1703896320.0","poster":"JTruong","comment_id":"1109292","content":"Selected Answer: D\nhttps://aws.amazon.com/tutorials/break-monolith-app-microservices-ecs-docker-ec2/module-three/\nThis page explained clearly why D is the correct answer"},{"upvote_count":"5","comment_id":"1001166","content":"Selected Answer: D\n'Non-monolithic', 'smaller applications', 'minimized operational overhead' all screaming 'microservices'.","timestamp":"1694060340.0","poster":"TariqKipkemei"},{"upvote_count":"6","comment_id":"982973","poster":"Guru4Cloud","content":"Selected Answer: D\nThe reasons are:\n\nECS allows running Docker containers, so the existing monolithic app can be containerized and run on ECS with minimal code changes.\nThe app can be broken into smaller microservices by containerizing different components and managing them separately.\nECS provides auto scaling capabilities to scale each microservice independently.\nUsing an Application Load Balancer with ECS enables distributing traffic across containers and auto scaling.\nECS has minimal operational overhead compared to managing EC2 instances directly.\nServerless options like Lambda and API Gateway would require significant code refactoring which is not ideal for migrating an existing app.","timestamp":"1692216420.0"},{"timestamp":"1691341620.0","poster":"MM_Korvinus","comments":[{"timestamp":"1691492940.0","content":"I actually agree with this, they have a monolithic application that contains the Front-end and Back-end. They clearly state they want different teams managing different applications. This is telling me they want a team to manage the front-end and a team to manage the back-end. A,C and D suggest simply running copies of the monolith application (containing front and back end). So how will different teams manage different applications?? B is the only one that actually splits front and back end","poster":"Fielies23","comment_id":"975534","upvote_count":"1"},{"timestamp":"1703586360.0","upvote_count":"2","poster":"pentium75","content":"How do you want to run a \"monolithic application\" in Lambda?","comment_id":"1105887"}],"upvote_count":"9","content":"Selected Answer: B\nHonestly, from my experience, the minimal operational overhead is with Amplify and API Gateway with lambdas. Both services have neat release features, you do not need to fiddle around ECS configurations as everything is server-less, which is also highly scalable. Eventhough it is much harder to refactor monolithic app to this set-up it is definitely easier to operate. Not talking about complexities around ALB.","comment_id":"974053"},{"content":"Selected Answer: D\nECS provides a highly scalable and managed environment for running containerized applications, reducing operational overhead. By setting up an ALB with ECS as the target, traffic can be distributed across multiple instances of the application for scalability and availability. This solution enables different teams to manage each application independently, promoting team autonomy and efficient development.\n\nA is more suitable for event-driven and serverless workloads. It may not be the ideal choice for migrating a monolithic application and maintaining the existing codebase.\n\nB integrates with Lambda and API Gateway, it may not provide the required flexibility for breaking the application into smaller applications and managing them independently.\n\nC would involve managing the infrastructure and scaling manually. It may result in higher operational overhead compared to using a container service like ECS.","upvote_count":"3","poster":"cookieMr","comment_id":"932178","timestamp":"1687582500.0"},{"content":"Selected Answer: D\nI was confused about this, but actually Amazon ECS service can be configured to use Elastic Load Balancing to distribute traffic evenly across the tasks in your service.\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/create-application-load-balancer.html","comment_id":"911873","poster":"antropaws","upvote_count":"2","timestamp":"1685606400.0"},{"upvote_count":"5","poster":"studynoplay","timestamp":"1683677160.0","comment_id":"893492","content":"Selected Answer: D\nmonolithic = microservices = ECS"},{"upvote_count":"2","poster":"C_M_M","content":"I thought ALB is about distributing load. How do we want to use it to connect decoupled applications that needs to call themselves. I am kind of confused why most people are going with D. \nI think I will go with A.","timestamp":"1681693860.0","comment_id":"872269"},{"content":"I think the answer is A \nB is wrong because the requirement is not for the backend. C and D are not suitable because the ALB Is not best suited for middle tier applications.","comment_id":"833700","poster":"Devsin2000","upvote_count":"2","timestamp":"1678349460.0","comments":[]},{"timestamp":"1674396360.0","comments":[{"content":"ECS with Fargate. I don't think a \"monolithic application\" can run on Lambda.","upvote_count":"1","poster":"pentium75","timestamp":"1703586420.0","comment_id":"1105889"}],"comment_id":"784372","content":"I will go with A because - less operational and High availability (Lambda has these)\n\nIf it is ECS, operational overhead and can only be scaled up to an EC2 assigned under it.","upvote_count":"2","poster":"aws4myself"},{"timestamp":"1673272140.0","poster":"SilentMilli","content":"Selected Answer: D\nTo meet the requirement of breaking the application into smaller applications that can be managed by different teams, while minimizing operational overhead and providing high scalability, the best solution would be to host the applications on Amazon Elastic Container Service (Amazon ECS). Amazon ECS is a fully managed container orchestration service that makes it easy to run, scale, and maintain containerized applications. Additionally, setting up an Application Load Balancer with Amazon ECS as the target will allow the company to easily scale the application as needed. Option D, \"Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target,\" is the correct answer.","comment_id":"770476","upvote_count":"2"},{"upvote_count":"2","timestamp":"1672567380.0","content":"Selected Answer: D\n. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.\n\nHosting the application on Amazon ECS would allow the company to break the monolithic application into smaller, more manageable applications that can be managed by different teams. Amazon ECS is a fully managed container orchestration service that makes it easy to deploy, run, and scale containerized applications. By setting up an Application Load Balancer with Amazon ECS as the target, the company can ensure that the solution is highly scalable and minimizes operational overhead.","poster":"Zerotn3","comment_id":"763136"},{"poster":"Buruguduystunstugudunstuy","content":"Selected Answer: D\nThe correct answer is Option D. To meet the requirements, the company should host the application on Amazon Elastic Container Service (Amazon ECS) and set up an Application Load Balancer with Amazon ECS as the target.\n\nOption A is not a valid solution because AWS Lambda is not suitable for hosting long-running applications.\n\nOption B is not a valid solution because AWS Amplify is a framework for building, deploying, and managing web applications, not a hosting solution.\n\nOption C is not a valid solution because Amazon EC2 instances are not fully managed container orchestration services. The company will need to manage the EC2 instances, which will increase operational overhead.","comment_id":"760633","upvote_count":"6","timestamp":"1672293900.0"},{"upvote_count":"3","content":"Selected Answer: D\nIt can be C or D depending on how easy it would be to containerize the application. If application needs persistent local data store then C would be a better choice. \nAlso from the usecase description it is not clear whether application is http based application or not though all options uses ALB only so we can safely assume that this is http based application only.","timestamp":"1671337200.0","comments":[{"upvote_count":"1","content":"After reading this question again A will be minimum operational overhead. \nD has higher operational overhead as D will have operational overhead of scaling EC2 servers up/down for running ECS containers.","poster":"career360guru","comments":[{"content":"Who says that D is about ECS with EC2? Can use Fargate. It is a \"monolithic application\" and they want to \"keep as much of the code as possible\", that might not work with Lambda.","upvote_count":"1","poster":"pentium75","timestamp":"1703586480.0","comment_id":"1105890"}],"comment_id":"754595","timestamp":"1671837240.0"}],"comment_id":"748602","poster":"career360guru"},{"content":"D is correct","timestamp":"1669312440.0","poster":"Wpcorgan","upvote_count":"1","comment_id":"726095"},{"timestamp":"1667697060.0","poster":"backbencher2022","comment_id":"712109","upvote_count":"2","content":"Selected Answer: D\nI think D is the right choice as they want application to be managed by different people which could be enabled by breaking it into different containers"},{"timestamp":"1667112180.0","poster":"SimonPark","upvote_count":"3","content":"Selected Answer: D\nimho, it's D because \"break the application into smaller applications\" doesn't mean it has to be 'serverless'. Rather it can be divided into smaller application running on containers.","comment_id":"707652"},{"poster":"Six_Fingered_Jose","upvote_count":"2","comments":[{"upvote_count":"4","comment_id":"725466","poster":"Newptone","timestamp":"1669249920.0","content":"The reason for not choosing A: \"The company wants to keep as much of the front-end code and the backend code as possible\""}],"comment_id":"704875","content":"Selected Answer: A\nI think A is the answer here, breaking into smaller pieces so lambda makes the most sense.\nI don't see any restrictions in the question that forbids the usage of lambda","timestamp":"1666803600.0"}],"answers_community":["D (78%)","B (19%)","2%"],"timestamp":"2022-10-26 19:00:00","question_id":70,"unix_timestamp":1666803600,"answer_ET":"D"}],"exam":{"isImplemented":true,"numberOfQuestions":1019,"id":31,"provider":"Amazon","name":"AWS Certified Solutions Architect - Associate SAA-C03","isMCOnly":true,"lastUpdated":"11 Apr 2025","isBeta":false},"currentPage":14},"__N_SSP":true}