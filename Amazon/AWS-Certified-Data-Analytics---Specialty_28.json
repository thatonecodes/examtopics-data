{"pageProps":{"questions":[{"id":"XA9is94j5USVD1Qx4GGu","discussion":[{"upvote_count":"20","timestamp":"1635709620.0","content":"B is the right answer","poster":"lostsoul07","comment_id":"280836"},{"comment_id":"191957","content":"From the link: https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html\n\"The per-query control limit specifies the total amount of data scanned per query. If any query that runs in the workgroup exceeds the limit, it is canceled\"\nAnswer is B","poster":"syu31svc","timestamp":"1635455400.0","upvote_count":"15"},{"upvote_count":"1","poster":"pk349","comment_id":"886474","content":"B: I passed the test","timestamp":"1682957880.0"},{"upvote_count":"8","content":"Selected Answer: B\nCorrect answer is B as Athena Workgroups help set control limits and per-query control limit helps specific a limit which if exceeded by a query it would be canceled.\n\n\nA is wrong as you can't configure Atehna for this purpose\n\nC is incorrect because you can't set a threshold in Athena using S3 bucket policies.\n\nD is incorrect because the workgroup-wide data usage control limit specifies the total amount of data scanned for all queries that run in the entire workgroup, and not on a specific query only. Remember that the requirement is to immediately cancel queries that exceed the recommended threshold.","poster":"cloudlearnerhere","comment_id":"713100","timestamp":"1667830800.0"},{"upvote_count":"1","poster":"rocky48","content":"Selected Answer: B\nSelected Answer: B","timestamp":"1658805000.0","comment_id":"637084"},{"comment_id":"604816","content":"Selected Answer: B\nAnswer is B","poster":"Bik000","upvote_count":"2","timestamp":"1653130080.0"},{"upvote_count":"1","poster":"Marvel_jarvis","content":"why not A or C? Can we set RecordMaxBufferTime to control scan?","timestamp":"1639362420.0","comment_id":"500334"},{"upvote_count":"1","comment_id":"482250","content":"B is the right answer","poster":"aws2019","timestamp":"1637380440.0"},{"upvote_count":"1","content":"Answer B","poster":"Donell","timestamp":"1635886020.0","comment_id":"388052"},{"poster":"Starboy","comment_id":"321816","upvote_count":"3","content":"B is the right answer","timestamp":"1635863460.0"},{"upvote_count":"2","content":"Will go with B for this scenario. I think the use case if to set a per query limit","comment_id":"185613","poster":"AjNapa","timestamp":"1634652420.0"},{"timestamp":"1634622600.0","content":"\"per-query control limit\" vs \"workgroup-wide data usage control limit\"\nhttps://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html","comment_id":"179336","upvote_count":"2","poster":"KoMo"},{"upvote_count":"3","timestamp":"1634564700.0","comment_id":"175673","poster":"Paitan","content":"B and D both serve similar purpose. However in this example B is the right option.\nhttps://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html"},{"upvote_count":"3","comment_id":"173922","poster":"jersyl","content":"It is B based on this link:\nhttps://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html","timestamp":"1634098920.0"},{"comment_id":"169578","content":"I apologize, the Answer is B.","upvote_count":"3","poster":"awssp12345","timestamp":"1633152600.0"},{"comment_id":"169577","timestamp":"1632304980.0","upvote_count":"1","content":"Agree with D.","poster":"awssp12345"}],"question_id":136,"answer_description":"","unix_timestamp":1598734560,"choices":{"C":"Enforce the prescribed threshold on all Amazon S3 bucket policies","A":"Configure Athena to invoke an AWS Lambda function that terminates queries when the prescribed threshold is crossed.","D":"For each workgroup, set the workgroup-wide data usage control limit to the prescribed threshold.","B":"For each workgroup, set the control limit for each query to the prescribed threshold."},"question_text":"A company's data analyst needs to ensure that queries run in Amazon Athena cannot scan more than a prescribed amount of data for cost control purposes.\nQueries that exceed the prescribed threshold must be canceled immediately.\nWhat should the data analyst do to achieve this?","exam_id":20,"question_images":[],"answer":"B","timestamp":"2020-08-29 22:56:00","answers_community":["B (100%)"],"answer_images":[],"answer_ET":"B","isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/30050-exam-aws-certified-data-analytics-specialty-topic-1-question/"},{"id":"lncZNXgbWZcOtyqfQsxA","question_images":[],"answers_community":["AE (67%)","AC (33%)"],"topic":"1","exam_id":20,"isMC":true,"answer_images":[],"answer_description":"","choices":{"D":"Use an Amazon DynamoDB table to store the list of required applications. Trigger an AWS Lambda function with DynamoDB Streams to install the software.","B":"Place the required installation scripts in Amazon S3 and execute them through Apache Spark in Amazon EMR.","A":"Place the required installation scripts in Amazon S3 and execute them using custom bootstrap actions.","E":"Launch an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance. Create an AMI and use that AMI to create the EMR cluster.","C":"Install the required third-party libraries in the existing EMR master node. Create an AMI out of that master node and use that custom AMI to re-create the EMR cluster."},"timestamp":"2020-08-28 13:14:00","answer_ET":"AE","question_text":"A marketing company is using Amazon EMR clusters for its workloads. The company manually installs third-party libraries on the clusters by logging in to the master nodes. A data analyst needs to create an automated solution to replace the manual process.\nWhich options can fulfill these requirements? (Choose two.)","question_id":137,"url":"https://www.examtopics.com/discussions/amazon/view/29836-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"AE","discussion":[{"comment_id":"168369","timestamp":"1632250020.0","poster":"ramozo","content":"I will choose A and E.\nhttps://aws.amazon.com/about-aws/whats-new/2017/07/amazon-emr-now-supports-launching-clusters-with-custom-amazon-linux-amis/\n\nhttps://docs.aws.amazon.com/de_de/emr/latest/ManagementGuide/emr-plan-bootstrap.html","comments":[{"comment_id":"168794","upvote_count":"3","comments":[{"poster":"testtaker3434","timestamp":"1633132860.0","upvote_count":"4","content":"Its A and E, if you do one or the other, but you shouldn't do both (as if it was a step 1 and after that step 2)","comment_id":"169207"}],"content":"Doubt in this one... Documentation says you use E as a option to avoid A.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html","poster":"testtaker3434","timestamp":"1632257340.0"},{"content":"Agreed!","comment_id":"169568","timestamp":"1633222620.0","poster":"awssp12345","upvote_count":"1"}],"upvote_count":"25"},{"comment_id":"175666","content":"A and E.","timestamp":"1633352040.0","poster":"Paitan","upvote_count":"8"},{"comment_id":"1075247","poster":"roymunson","timestamp":"1700467620.0","content":"AE: I'll pass the test.","upvote_count":"3"},{"poster":"LocalHero","upvote_count":"2","content":"I think existing cluster must not change.\nIt is danger.\nInstalling software for new EC2 instance is more safe.\nso A and E correct.I think.","timestamp":"1699527300.0","comment_id":"1066359"},{"upvote_count":"2","comment_id":"967295","poster":"confuzz","content":"AE\nCustom AMIs created from the base EMR AMI are not supported and will lead to application provisioning errors upon cluster startup.\nhttps://medium.com/@amberrunnels/creating-a-custom-ami-on-amazon-emr-a60ddeb7821b","timestamp":"1690735140.0"},{"poster":"Debi_mishra","content":"A is very obvious. Between C and E - I will prefer E , as creating AMI from a master node may create a bulky AMI with lot of redundant hadoop libraries that can be done during bootstrap process.","upvote_count":"3","timestamp":"1685215260.0","comment_id":"908175"},{"poster":"pk349","timestamp":"1682957940.0","comment_id":"886475","upvote_count":"1","content":"AE: I passed the test"},{"poster":"VijiTu","timestamp":"1671709140.0","upvote_count":"1","content":"AE\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-custom-ami.html#emr-custom-ami-preconfigure","comment_id":"753241"},{"comment_id":"708624","upvote_count":"1","poster":"nharaz","content":"A AND E According to Stephane Maarek Udemy course \nA= You can use a bootstrap action to install additional software or customize the configuration of the EMR cluster instances. Bootstrap actions are scripts that run on the cluster after Amazon EMR launches the instance using the Amazon Linux Amazon Machine Image (AMI). Bootstrap actions run before Amazon EMR installs the applications that you specify when you create the cluster and before cluster nodes begin processing data.\nE= You can create Amazon EMR clusters that have custom Amazon Machine Images (AMI) running Amazon Linux. You can create the AMI from an EC2 instance running Amazon Linux. Make sure that you have installed all the required third-party libraries on this EC2 instance. This allows you to preload additional software on your AMI and use these AMIs to launch your EMR clusters.","timestamp":"1667239260.0"},{"content":"AC Confirmed by paid dumps","upvote_count":"1","comments":[{"poster":"JoellaLi","content":"Could you give the reason for C?","comment_id":"700448","timestamp":"1666315620.0","comments":[{"timestamp":"1680198360.0","poster":"sly_tail","upvote_count":"1","comment_id":"856016","content":"C instead of E because it's absolutely redundant and inefficient to launch another instance when you already have the same master node."}],"upvote_count":"1"}],"timestamp":"1664216940.0","comment_id":"680056","poster":"JHJHJHJHJ"},{"poster":"Sanmeda","comment_id":"671167","content":"Answer A & E","timestamp":"1663374480.0","upvote_count":"1"},{"timestamp":"1658294460.0","content":"Selected Answer: AE\nA and E.","upvote_count":"1","comment_id":"633853","poster":"rocky48"},{"poster":"Bik000","content":"Selected Answer: AE\nAnswer is A & E","upvote_count":"1","timestamp":"1653206100.0","comment_id":"605269"},{"timestamp":"1653102900.0","content":"A and E are right answers. How would installing libraries on Master Nodes resolve anything? Computation happens on Data nodes (slaves) and all required packages should be installed there.","upvote_count":"3","comments":[{"timestamp":"1668306240.0","content":"But the quention says the company manually installs third-party libraries on the clusters \"by logging in to the master nodes\". Does it mean that they log in there but install the libraries in slave-nodes?","upvote_count":"2","comment_id":"717058","poster":"Ryo0w0o"}],"poster":"certificationJunkie","comment_id":"604700"},{"content":"Selected Answer: AC\nAlthough most of others choose A.E. But I think C is right instead of E.\nFor E: Launch EC2 instance and install these softwares are not easy for EMR. I have installed hadoop and skark one time. And it took my much time. And if I want to make a hadoop/hive/spark... environment to be used as AWS EMR, it will take much efford.\nBut C: I can login into master node with ssh, install the lib, and use the master node EC2 instance to cretae a custom AMI. Although it will waste the previous EMR cluster, but the if I want to establish an hadoop/spark/hive clusters using EC2, I still need several instances to prepare AMI.\nSo, I vote for AC.","comment_id":"593520","upvote_count":"2","timestamp":"1651120200.0","poster":"MWL"},{"content":"Although most of others choose A.E. But I think C is right instead of E.\nFor E: Launch EC2 instance and install these softwares are not easy for EMR. I have installed hadoop and skark one time. And it took my much time. And if I want to make a hadoop/hive/spark... environment to be used as AWS EMR, it will take much efford.\nBut C: I can login into master node with ssh, install the lib, and use the master node EC2 instance to cretae a custom AMI. Although it will waste the previous EMR cluster, but the if I want to establish an hadoop/spark/hive clusters using EC2, I still need several instances to prepare AMI.\nSo, I vote for AC.","upvote_count":"1","poster":"MWL","comment_id":"593519","timestamp":"1651120140.0"},{"content":"Selected Answer: AE\nA and E is correct","upvote_count":"2","timestamp":"1645312800.0","poster":"RSSRAO","comment_id":"551402"},{"timestamp":"1637211000.0","upvote_count":"1","poster":"Mobeen_Mehdi","content":"It's A and E. AS I have myself experienced this, installing third-party libraries through bootstrap action (like jar file for delta lake/table) and instance as well.","comment_id":"480450"},{"poster":"aws2019","content":"A and E","comment_id":"480252","timestamp":"1637180640.0","upvote_count":"1"},{"content":"It should be AE.\nQuestion asks which possible options can do this. So it can be done either via A or E.","comment_id":"394314","poster":"gunjan4392","upvote_count":"1","timestamp":"1636004400.0"},{"comment_id":"388053","content":"Answer A,E.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html","timestamp":"1635816000.0","upvote_count":"1","poster":"Donell"},{"upvote_count":"3","poster":"lostsoul07","content":"A, E is the right answer","timestamp":"1634939340.0","comment_id":"280829"},{"content":"C is not possible?","comment_id":"227703","upvote_count":"1","poster":"hans1234","timestamp":"1633839120.0","comments":[{"upvote_count":"1","timestamp":"1635142560.0","comment_id":"284432","poster":"kalyan_krishna742020","content":"C says install it in Master node"},{"upvote_count":"1","timestamp":"1635181080.0","comments":[{"content":"It is very complicated to install all necessary components in a Linux to work as a EMR master node. It is more feasible to create AMI based on current master instance.","comment_id":"597154","timestamp":"1651732260.0","upvote_count":"3","poster":"MWL"}],"poster":"Naresh_Dulam","comment_id":"284523","content":"it's waste of creating emr cluster and then creating AMI from cluster. D also do same and it's efficient"}]},{"upvote_count":"5","content":"From link: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html\n\"You can use a bootstrap action to install additional software or customize the configuration of cluster instances\"\nSo A is of the options\nDynamoDB stream records changes in DynamoDB so D is out\nE would be correct as per link: https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-emr-now-supports-launching-clusters-with-custom-amazon-linux-amis/","comment_id":"191946","timestamp":"1633554780.0","poster":"syu31svc"}],"unix_timestamp":1598613240},{"id":"ZmQUSqLeL1XHZhYIbyFo","question_images":[],"answer_description":"","exam_id":20,"topic":"1","isMC":true,"question_text":"A data engineering team within a shared workspace company wants to build a centralized logging system for all weblogs generated by the space reservation system. The company has a fleet of Amazon EC2 instances that process requests for shared space reservations on its website. The data engineering team wants to ingest all weblogs into a service that will provide a near-real-time search engine. The team does not want to manage the maintenance and operation of the logging system.\nWhich solution allows the data engineering team to efficiently set up the web logging system within AWS?","answer":"B","unix_timestamp":1598718660,"discussion":[{"comment_id":"169398","timestamp":"1632111120.0","upvote_count":"25","poster":"Nicki1013","comments":[{"comment_id":"170032","timestamp":"1632559020.0","poster":"awssp12345","upvote_count":"3","content":"Agreed with B."},{"comment_id":"185690","poster":"GeeBeeEl","content":"Do you have a link to back this up? check https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html","timestamp":"1633595220.0","upvote_count":"3"}],"content":"My answer is B"},{"poster":"pk349","comment_id":"886476","upvote_count":"1","content":"B: I passed the test","timestamp":"1682958000.0"},{"timestamp":"1667831040.0","comment_id":"713101","content":"Selected Answer: B\nCorrect answer is B as Kinesis Data Firehose provides a managed solution to integrate with CloudWatch logs and stream data into ElasticSearch.\n\nOption A is wrong as Kinesis Data Stream would increase the maintenance and operation of the logging system in terms of data collections and ingestion to ElasticSearch.\n\nOption C is wrong as Kinesis Data Stream would increase the maintenance and operation of the logging system and Splunk would need a separate purchase.\n\nOption D is wrong as Kinesis Firehose does not support DynamoDB as its destination and DynamoDB is not an ideal storage solution for logs.","upvote_count":"3","poster":"cloudlearnerhere"},{"comment_id":"707836","content":"Selected Answer: B\nnear real-time search engine for weblogs -> Opensearch. KDF can ingest the logs directly into OS.","upvote_count":"2","timestamp":"1667137080.0","poster":"thirukudil"},{"content":"---------------------------------------------------------------\nAnswer B - KDS cannot write to open search\n------------------------------------------------------------------------","poster":"JHJHJHJHJ","comment_id":"680718","upvote_count":"3","timestamp":"1664279760.0"},{"content":"Selected Answer: B\nAnswer is B","upvote_count":"2","poster":"rocky48","comment_id":"643519","timestamp":"1659817740.0"},{"comment_id":"604870","content":"Selected Answer: B\nI think Answer should be B","timestamp":"1653133320.0","poster":"Bik000","upvote_count":"2"},{"poster":"Dr_Kiko","timestamp":"1636103280.0","comment_id":"423957","upvote_count":"2","content":"it's A or B because of Elasticsearch as destination; KDS cannot feed from CWLogs but Firehose can, hence the answer is B"},{"comment_id":"280837","content":"B is the right answer","upvote_count":"3","poster":"lostsoul07","timestamp":"1635568860.0"},{"upvote_count":"3","comment_id":"272796","timestamp":"1635553380.0","content":"It is B. They should use Firehose, not Data Streams (A). This sentence gives it away: \"The team does not want to manage the maintenance and operation of the logging system.\"","poster":"Lucas88"},{"poster":"syu31svc","content":"I agree with A; Firehose can't stream to CloudWatch","comments":[{"poster":"syu31svc","upvote_count":"3","content":"Sorry I meant B","timestamp":"1634904480.0","comment_id":"196368"}],"upvote_count":"1","timestamp":"1633790100.0","comment_id":"191960"},{"upvote_count":"3","poster":"Paitan","content":"B is the right option.","timestamp":"1633516440.0","comment_id":"175675"}],"url":"https://www.examtopics.com/discussions/amazon/view/30035-exam-aws-certified-data-analytics-specialty-topic-1-question/","timestamp":"2020-08-29 18:31:00","choices":{"B":"Set up the Amazon CloudWatch agent to stream weblogs to CloudWatch logs and subscribe the Amazon Kinesis Data Firehose delivery stream to CloudWatch. Choose Amazon OpenSearch Service (Amazon Elasticsearch Service) as the end destination of the weblogs.","C":"Set up the Amazon CloudWatch agent to stream weblogs to CloudWatch logs and subscribe the Amazon Kinesis data stream to CloudWatch. Configure Splunk as the end destination of the weblogs.","A":"Set up the Amazon CloudWatch agent to stream weblogs to CloudWatch logs and subscribe the Amazon Kinesis data stream to CloudWatch. Choose Amazon OpenSearch Service (Amazon Elasticsearch Service) as the end destination of the weblogs.","D":"Set up the Amazon CloudWatch agent to stream weblogs to CloudWatch logs and subscribe the Amazon Kinesis Firehose delivery stream to CloudWatch. Configure Amazon DynamoDB as the end destination of the weblogs."},"answer_ET":"B","question_id":138,"answers_community":["B (100%)"],"answer_images":[]},{"id":"Dyzvp8ja36X2HHldATww","answer_images":[],"question_id":139,"answers_community":["B (100%)"],"question_images":[],"choices":{"D":"Resize the cluster node type to the dense storage node type (DS2) for an additional 16 TB storage capacity on each individual node in the Amazon Redshift cluster. Then use Amazon Redshift for the additional analysis.","C":"Keep the data from the last 90 days in Amazon Redshift. Move data older than 90 days to Amazon S3 and store it in Apache Parquet format partitioned by date. Then provision a persistent Amazon EMR cluster and use Apache Presto for the additional analysis.","B":"Keep the data from the last 90 days in Amazon Redshift. Move data older than 90 days to Amazon S3 and store it in Apache Parquet format partitioned by date. Then use Amazon Redshift Spectrum for the additional analysis.","A":"Increase the size of the Amazon Redshift cluster to 120 nodes so it has enough storage capacity to hold 1 year of data. Then use Amazon Redshift for the additional analysis."},"answer_description":"","isMC":true,"answer":"B","question_text":"A company wants to research user turnover by analyzing the past 3 months of user activities. With millions of users, 1.5 TB of uncompressed data is generated each day. A 30-node Amazon Redshift cluster with 2.56 TB of solid state drive (SSD) storage for each node is required to meet the query performance goals.\nThe company wants to run an additional analysis on a year's worth of historical data to examine trends indicating which features are most popular. This analysis will be done once a week.\nWhat is the MOST cost-effective solution?","timestamp":"2020-08-28 12:38:00","topic":"1","answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/29829-exam-aws-certified-data-analytics-specialty-topic-1-question/","exam_id":20,"unix_timestamp":1598611080,"discussion":[{"comments":[{"poster":"awssp12345","comment_id":"169572","content":"Agree!","upvote_count":"1","timestamp":"1633095120.0"},{"content":"why 30 node can save 90 days data?","comment_id":"171160","timestamp":"1633616880.0","comments":[{"upvote_count":"5","comment_id":"181615","content":"You are right, 30 nodes cannot save 90 days uncompressed data. But we can always compress the data while storing in Redshift. So that will definitely reduce the storage requirement and can be managed by the 30 nodes.","timestamp":"1635696660.0","poster":"Paitan"},{"upvote_count":"2","poster":"Huy","content":"https://aws.amazon.com/blogs/aws/data-compression-improvements-in-amazon-redshift/","timestamp":"1635801840.0","comment_id":"391693"}],"poster":"lui","upvote_count":"1"}],"upvote_count":"33","timestamp":"1632274620.0","content":"B. Redshift Spectrum. \"Amazon Redshift Spectrum executes queries across thousands of parallelized nodes to deliver fast results, regardless of the complexity of the query or the amount of data. \"\nhttps://aws.amazon.com/redshift/features/","comment_id":"168338","poster":"ramozo"},{"content":"B: I passed the test","poster":"pk349","upvote_count":"1","timestamp":"1682958060.0","comment_id":"886477"},{"timestamp":"1667832120.0","upvote_count":"4","content":"Selected Answer: B\nCorrect answer is B as the data can be stored in Redshift for 90 days for analyzing the past 3 months of user activities. Data older than 90 days can be moved to S3 and analyzed using Redshift Spectrum once a week. This provides the most cost-effective solution.\n\n\nUsing Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Multiple clusters can concurrently query the same dataset in Amazon S3 without the need to make copies of the data for each cluster.\nOptions A & D are wrong as increasing the size of the Redshift cluster would not be cost-effective.\n\n\nOption C is wrong as analyzing data using a persistent EMR cluster would not be cost-effective.","poster":"cloudlearnerhere","comment_id":"713113"},{"timestamp":"1658444940.0","poster":"rocky48","upvote_count":"2","comment_id":"634883","content":"Selected Answer: B\nB is the right answer"},{"content":"Answer - B","timestamp":"1651351320.0","poster":"jrheen","comment_id":"595279","upvote_count":"2"},{"timestamp":"1637378940.0","poster":"aws2019","upvote_count":"2","content":"B is right","comment_id":"482243"},{"comment_id":"280830","content":"B is the right answer","poster":"lostsoul07","upvote_count":"3","timestamp":"1635785700.0"},{"comment_id":"175668","timestamp":"1635011700.0","content":"B for sure.","poster":"Paitan","upvote_count":"3"},{"content":"Agree its B","comment_id":"168796","upvote_count":"3","timestamp":"1632339480.0","poster":"testtaker3434"}]},{"id":"G9W11QQTtNwlIkfTDsyH","question_text":"A bank operates in a regulated environment. The compliance requirements for the country in which the bank operates say that customer data for each state should only be accessible by the bank's employees located in the same state. Bank employees in one state should NOT be able to access data for customers who have provided a home address in a different state.\nThe bank's marketing team has hired a data analyst to gather insights from customer data for a new campaign being launched in certain states. Currently, data linking each customer account to its home state is stored in a tabular .csv file within a single Amazon S3 folder in a private S3 bucket. The total size of the S3 folder is 2 GB uncompressed. Due to the country's compliance requirements, the marketing team is not able to access this folder.\nThe data analyst is responsible for ensuring that the marketing team gets one-time access to customer data for their campaign analytics project, while being subject to all the compliance requirements and controls.\nWhich solution should the data analyst implement to meet the desired requirements with the LEAST amount of setup effort?","answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/30037-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"D","topic":"1","unix_timestamp":1598719080,"answer_ET":"D","question_images":[],"question_id":140,"discussion":[{"comments":[{"poster":"awssp12345","comment_id":"170024","content":"agreed with D.","upvote_count":"4","timestamp":"1632143880.0"}],"content":"My answer is D","poster":"Nicki1013","comment_id":"169403","upvote_count":"26","timestamp":"1632132060.0"},{"upvote_count":"8","comment_id":"713114","timestamp":"1667832240.0","poster":"cloudlearnerhere","content":"Selected Answer: D\nCorrect answer is D as using QuickSight with its built-in-row-level security features allows the data analyst to provide limited one-time access while maintaining data compliance requirements and controls and a minimal amount of setup.\n\nIn the Enterprise edition of Amazon QuickSight, you can restrict access to a dataset by configuring row-level security (RLS) on it. You can do this before or after you have shared the dataset. Only the people whom you shared with can see any of the data. By adding row-level security, you can further control their access.\n\n\nOption A is wrong as it would take some amount of setup to repartition the data in S3.\n\nOptions B & C are wrong as using EMR and Redshift would need set up and provisioning effort."},{"poster":"bansalhp","comment_id":"1114771","upvote_count":"1","timestamp":"1704484860.0","content":"I think option A is right answer.\nOption A proposes reorganizing the data by storing customer data for each state in a different S3 folder within the same bucket. This makes it easier to manage access control at the folder level.\n\nSetting up S3 bucket policies allows for controlling access to specific folders, meeting the compliance requirements without requiring additional services.\n\nAfter the project is complete, the bucket policies can be deleted, ensuring that access control is removed as needed.\nOption C and D would be costlier as we need to spin up redshift or Quicksight for 2 GB data"},{"comment_id":"886479","upvote_count":"2","content":"D: I passed the test","timestamp":"1682958120.0","poster":"pk349"},{"upvote_count":"1","content":"Selected Answer: D\nLeast operational overhead is D","poster":"milofficial","comment_id":"798909","timestamp":"1675608840.0"},{"content":"Redshift now has RLS. Can't the answer be C as well?\nhttps://aws.amazon.com/about-aws/whats-new/2022/07/amazon-redshift-row-level-security/\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_rls.html\nhttps://aws.amazon.com/blogs/big-data/achieve-fine-grained-data-security-with-row-level-access-control-in-amazon-redshift/","comment_id":"712184","poster":"b33f","timestamp":"1667720580.0","comments":[{"timestamp":"1669015380.0","comment_id":"723227","poster":"Gabba","upvote_count":"2","content":"The solution should be with least efforts. Setting up redshift is big task hence C is incorrect."}],"upvote_count":"1"},{"poster":"t47","comment_id":"707574","upvote_count":"1","timestamp":"1667096820.0","content":"answer is D"},{"timestamp":"1664760180.0","upvote_count":"1","content":"Will go with D.","poster":"awsdatacert","comment_id":"685146"},{"comment_id":"637722","upvote_count":"1","content":"Selected Answer: D\nSelected Answer: D","poster":"rocky48","timestamp":"1658885220.0"},{"upvote_count":"1","comment_id":"605226","content":"Selected Answer: D\nD, everything else has too much setup or is not usable (A)","poster":"f4bi4n","timestamp":"1653203520.0"},{"poster":"keitahigaki","content":"Answer: D Quicksight Enterprise provides row-level security.\nhttps://docs.aws.amazon.com/ja_jp/quicksight/latest/user/restrict-access-to-a-data-set-using-row-level-security.html","upvote_count":"4","timestamp":"1636301340.0","comment_id":"433614"},{"comment_id":"297942","timestamp":"1635847500.0","poster":"sayed","comments":[{"poster":"god_father","content":"There is! https://docs.aws.amazon.com/redshift/latest/dg/t_rls.html\n\"Using row-level security (RLS) in Amazon Redshift, you can have granular access control over your sensitive data. You can decide which users or roles can access specific records of data within schemas or tables, based on security policies that are defined at the database objects level. In addition to column-level security, where you can grant users permissions to a subset of columns, use RLS policies to further restrict access to particular rows of the visible columns.\"","comment_id":"1125881","upvote_count":"1","timestamp":"1705584180.0"}],"content":"for C there is no built-in row-level security feature in Amazon Redshift it is in quicksight\nso i think C is not the correct answer","upvote_count":"3"},{"comment_id":"280839","upvote_count":"1","content":"B is the right answer","timestamp":"1635736860.0","poster":"lostsoul07"},{"comment_id":"267267","upvote_count":"3","content":"Topic 2 - Question 57\nA healthcare company uses Amazon S3 to store all its data and is planning to use Amazon EMR, backed with EMR File System (EMRFS), to process and transform the data. The company data is stored in multiple buckets and encrypted using different encryption keys for each bucket. How can the EMR Cluster be configured to access the encrypted data?\n\nA) Modify the S3 bucket policies to grant public access to the S3 buckets.\nB) Create a security configuration that specifies the encryption keys for the buckets using per bucket encryption overrides.\nC) Configure the cluster to use 53 Select to access the data in the buckets and specify the encryption keys as options.\nD) Copy the encryption keys to the master node and create a security configuration that references the keys.","timestamp":"1635414960.0","poster":"LRyan2020","comments":[{"timestamp":"1635482220.0","content":"Again BrainCert practice question:\nCorrect Answer\nB. Create a security configuration that specifies the encryption keys for the buckets using per bucket encryption overrides.\nExplanation\nCorrect answer is B as EMR provides per bucket encryption overrides option. Refer AWS documentation - EMR Securing Data\nhttps://aws.amazon.com/blogs/big-data/secure-your-data-on-amazon-emr-using-native-ebs-and-per-bucket-s3-encryption-options/\nWith S3 encryption on Amazon EMR, all the encryption modes use a single CMK by default to encrypt objects in S3. If you have highly sensitive content in specific S3 buckets, you may want to manage the encryption of these buckets separately by using different CMKs or encryption modes for individual buckets. You can accomplish this using the per bucket encryption overrides option in Amazon EMR.","comments":[{"upvote_count":"1","comment_id":"272778","content":"Are BrainCert practice question not real exam questions?","timestamp":"1635575580.0","poster":"blubb"}],"comment_id":"268151","poster":"Umer24","upvote_count":"3"},{"content":"@LRyan2020 Why are these questions being posted here ? Are they sample questions or exam practice questions ?","poster":"rocky48","comment_id":"637721","upvote_count":"1","timestamp":"1658885100.0"}]},{"comment_id":"267264","timestamp":"1635395040.0","upvote_count":"1","content":"Topic 2 - Question 56\nA company currently processes real-time streaming data using Apache Kafka. The company is facing challenges with managing, setting up, and scaling during production. The company want to optimize the deployment with the Kafka brokers. The solution should be managed by AWS, secure, and require minimal changes to the current client code.\n\nWhich solution meets these requirements?\n\nA) Use Apache Zookeeper to scale Kafka installed on Amazon EC2 instances.\nB) Use Amazon Managed Streaming for Kafka to scale the brokers.\nC) Use Apache Zookeeper to scale the brokers of Amazon Managed Streaming for Kafka\nD) Scale the number of client machines, and use a single broker with Amazon Managed Streaming for Kafka.","comments":[{"upvote_count":"3","comment_id":"268148","content":"@LRyan2020. These are BrainCert practice questions buddy !!!! \nCorrect Answer\nB. Use Amazon Managed Streaming for Kafka to scale the brokers.\nExplanation\nCorrect answer is B as Amazon Managed Streaming is fully managed, secure Kafka streaming solution with no operation overhead. Refer AWS documentation - Managed Streaming Kafka https://aws.amazon.com/msk/ Amazon MSK lets you focus on creating your streaming applications without having to worry about the operational overhead of managing your Apache Kafka environment. Amazon MSK manages the provisioning, configuration, and maintenance of Apache Kafka clusters and Apache ZooKeeper nodes for you. Amazon MSK also shows key Apache Kafka performance metrics in the AWS console. Options A, C & D are wrong as they are not managed by AWS and would need user involvement.","timestamp":"1635471480.0","poster":"Umer24"}],"poster":"LRyan2020"},{"timestamp":"1635327000.0","upvote_count":"1","comment_id":"266476","comments":[{"comments":[{"content":"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/stagger-window-concepts.html","upvote_count":"2","timestamp":"1635356940.0","poster":"Umer24","comment_id":"266734"}],"timestamp":"1635330420.0","poster":"Umer24","comment_id":"266733","content":"Correct Answer\nD. Stagger window queries\nExplanation\nCorrect answer is D as using stagger windows is a windowing method that is suited for\nanalyzing groups of data that arrive at inconsistent times. It is well suited for any timeseries\nanalytics use case, such as a set of related sales or log records.\nRefer AWS documentation - Kinesis Stagger Window Concepts\nOption A is wrong as Continuous queries is a query over a stream that executes\ncontinuously over streaming data. This continuous execution enables scenarios, such as\nthe ability for applications to continuously query a stream and generate alerts.\nOption B is wrong as Tumbling window queries are suitable when a windowed query\nprocesses each window in a non-overlapping manner\nOption C is wrong as Sliding window queries help define a time-based or row-based\nwindow, instead of grouping records using GROUP BY","upvote_count":"4"}],"poster":"LRyan2020","content":"Question 55\nAn online retailer is planning to capture clickstream data from its ecommerce website and then use the data to drive a new custom-built recommendation engine that provides product recommendations to online users. The retailer will use Amazon Kinesis Data Streams to ingest the streaming data and Amazon Kinesis Data Analytics to perform SQL queries on the stream, using windowed queries to process the data that arrives at inconsistent intervals.\nWhat type of windowed query must be used to aggregate the data using time-based windows that open as data arrives?\n\nA) Continuous queries\n B) Tumbling window queries\nC) Sliding window queries\nD) Stagger window queries"},{"comments":[{"timestamp":"1635088260.0","upvote_count":"4","comment_id":"265617","poster":"Roontha","content":"Answer : A, C"},{"timestamp":"1635868680.0","poster":"dkp","content":"E also works.\n\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/","comment_id":"307564","upvote_count":"1"},{"content":"A and C","comment_id":"342870","timestamp":"1635989220.0","upvote_count":"2","poster":"sivajiboss"}],"timestamp":"1634986800.0","poster":"Umer24","upvote_count":"1","content":"Question-54\nA marketing company is storing its campaign response data in Amazon S3. A consistent set of sources has generated the data for each campaign. The data is saved into Amazon S3 as .csv files. A business analyst will use Amazon Athena to analyze each campaign's data. The company needs the cost of ongoing data analysis with Athena to be minimized. Which combination of actions should a data analytics specialist take to meet these requirements? (Select TWO.)\nA. Convert the .csv files to Apache Parquet. \nB. Convert the .csv files to Apache Avro. \nC. Partition the data by campaign. \nD. Partition the data by source. \nE. Compress the .csv files.","comment_id":"264712"}],"isMC":true,"answers_community":["D (100%)"],"answer_description":"","timestamp":"2020-08-29 18:38:00","choices":{"B":"Load tabular data from Amazon S3 to an Amazon EMR cluster using s3DistCp. Implement a custom Hadoop-based row-level security solution on the Hadoop Distributed File System (HDFS) to provide marketing employees with appropriate data access under compliance controls. Terminate the EMR cluster after the project.","D":"Load tabular data from Amazon S3 to Amazon QuickSight Enterprise edition by directly importing it as a data source. Use the built-in row-level security feature in Amazon QuickSight to provide marketing employees with appropriate data access under compliance controls. Delete Amazon QuickSight data sources after the project is complete.","C":"Load tabular data from Amazon S3 to Amazon Redshift with the COPY command. Use the built-in row-level security feature in Amazon Redshift to provide marketing employees with appropriate data access under compliance controls. Delete the Amazon Redshift tables after the project.","A":"Re-arrange data in Amazon S3 to store customer data about each state in a different S3 folder within the same bucket. Set up S3 bucket policies to provide marketing employees with appropriate data access under compliance controls. Delete the bucket policies after the project."},"exam_id":20}],"exam":{"name":"AWS Certified Data Analytics - Specialty","isMCOnly":true,"isImplemented":true,"provider":"Amazon","id":20,"isBeta":false,"lastUpdated":"11 Apr 2025","numberOfQuestions":164},"currentPage":28},"__N_SSP":true}