{"pageProps":{"questions":[{"id":"zsqOsqGQbrwyGNV8vFUe","answer":"B","unix_timestamp":1693564980,"exam_id":31,"answer_ET":"B","answer_description":"","topic":"1","question_id":561,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/119645-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A company has five organizational units (OUs) as part of its organization in AWS Organizations. Each OU correlates to the five businesses that the company owns. The company's research and development (R&D) business is separating from the company and will need its own organization. A solutions architect creates a separate new management account for this purpose.\n\nWhat should the solutions architect do next in the new management account?","question_images":[],"discussion":[{"upvote_count":"9","poster":"awsgeek75","comment_id":"1120267","content":"Selected Answer: B\nAn account can only join another org when it leaves the first org.\nA is wrong as it's not possible\nC that's a new account so not really a migration\nD The R&D department is separating from the company so you don't want the OU to join via nesting","timestamp":"1705013640.0"},{"timestamp":"1704185580.0","content":"Selected Answer: B\nB as exactly described here: https://repost.aws/knowledge-center/organizations-move-accounts","poster":"pentium75","comment_id":"1111731","upvote_count":"7"},{"upvote_count":"2","comment_id":"1239456","timestamp":"1719707340.0","content":"Selected Answer: B\nOption B: Invite the R&D AWS account to be part of the new organization after the R&D AWS account has left the prior organization is the appropriate approach. This option ensures that the R&D AWS account transitions smoothly from the old organization to the new one. The steps involved are:\n\n Remove the R&D AWS account from the existing organization: This is done from the existing organization’s management account.\n\n Invite the R&D AWS account to join the new organization: Once the R&D account is no longer part of the previous organization, it can be invited to and accepted into the new organization.","poster":"emakid"},{"poster":"Marco_St","comment_id":"1119877","upvote_count":"4","content":"Selected Answer: B\nhttps://aws.amazon.com/blogs/mt/migrating-accounts-between-aws-organizations-with-consolidated-billing-to-all-features/","timestamp":"1704985260.0"},{"upvote_count":"3","timestamp":"1703837280.0","content":"Selected Answer: B\nhttps://repost.aws/knowledge-center/organizations-move-accounts\nRemove the member account from the old organization.\nSend an invite to the member account from the new organization.\nAccept the invite to the new organization from the member account.","comment_id":"1108468","poster":"ale_brd_111"},{"poster":"Derek_G","comments":[{"poster":"pentium75","timestamp":"1704185400.0","comment_id":"1111727","content":"What kind of \"data lose\" would happen when you change the account to a new organization? And why should you migrate ALL RESOURCES of the account to a new account?","upvote_count":"2"}],"timestamp":"1703427060.0","comment_id":"1104644","upvote_count":"1","content":"Selected Answer: C\nC is better. first migrate , then delete. avoid the data lose."},{"comment_id":"1104641","timestamp":"1703426820.0","poster":"Derek_G","upvote_count":"1","content":"C is better. first migrate , then delete. avoid the data lose."},{"timestamp":"1700727780.0","upvote_count":"2","content":"Selected Answer: B\nAs per this document, B is clearly the answer.\nhttps://repost.aws/knowledge-center/organizations-move-accounts#:~:text=In%20either%20case%2C-,perform%20these%20actions,-for%20each%20member","poster":"TariqKipkemei","comment_id":"1078225"},{"timestamp":"1695827520.0","comment_id":"1018934","upvote_count":"6","poster":"Joben","content":"Selected Answer: B\nIn either case, perform these actions for each member account:\n- Remove the member account from the old organization.\n- Send an invite to the member account from the new organization.\n- Accept the invite to the new organization from the member account.\n\nhttps://repost.aws/knowledge-center/organizations-move-accounts"},{"timestamp":"1695293100.0","comments":[{"content":"\"A clean separation\" is already existing, they have their own account. \"Migration of only the necessary resources from the old account to the new\" is not asked for. They have an account in an existing organization, they need their own organization, thus move the existing account to a new organisation (B), done.","timestamp":"1704185460.0","upvote_count":"2","comment_id":"1111729","poster":"pentium75"}],"comment_id":"1012997","upvote_count":"2","poster":"Guru4Cloud","content":"Selected Answer: C\nCreating a brand new AWS account in the new organization (Option C) allows for a clean separation and migration of only the necessary resources from the old account to the new."},{"comments":[{"timestamp":"1704185520.0","upvote_count":"2","comment_id":"1111730","content":"Says who?","poster":"pentium75"}],"content":"Selected Answer: C\nWhen separating a business unit from an AWS Organizations structure, best practice is to:\n\nCreate a new AWS account dedicated for the business unit in the new organization\nMigrate resources from the old account to the new account\nRemove the old account from the original organization\nThis allows a clean break between the organizations and avoids any linking between them after separation.","comment_id":"1007537","upvote_count":"1","poster":"Guru4Cloud","timestamp":"1694691720.0"},{"timestamp":"1693568220.0","content":"B\nhttps://aws.amazon.com/blogs/mt/migrating-accounts-between-aws-organizations-with-consolidated-billing-to-all-features/","upvote_count":"3","poster":"ErnShm","comment_id":"996006"},{"content":"Selected Answer: B\naccount can leave current organization and then join new organization.","poster":"gispankaj","comment_id":"995954","timestamp":"1693564980.0","upvote_count":"4"}],"timestamp":"2023-09-01 12:43:00","isMC":true,"answers_community":["B (90%)","10%"],"choices":{"B":"Invite the R&D AWS account to be part of the new organization after the R&D AWS account has left the prior organization.","A":"Have the R&D AWS account be part of both organizations during the transition.","D":"Have the R&D AWS account join the new organization. Make the new management account a member of the prior organization.","C":"Create a new R&D AWS account in the new organization. Migrate resources from the prior R&D AWS account to the new R&D AWS account."}},{"id":"9dd1aijKdWtMF1eLmXMd","answer_ET":"C","exam_id":31,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/119576-exam-aws-certified-solutions-architect-associate-saa-c03/","choices":{"C":"Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization.","A":"Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives in an Amazon Elastic File System (Amazon EFS) file system. Authorization is resolved at the GWLB.","B":"Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that stores the information that the company receives in an Amazon S3 bucket. Use an AWS Lambda function to resolve authorization.","D":"Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives on an Amazon Elastic File System (Amazon EFS) file system. Use an AWS Lambda function to resolve authorization."},"question_id":562,"answer":"C","isMC":true,"question_images":[],"answer_description":"","answer_images":[],"discussion":[{"content":"Selected Answer: C\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html","poster":"ralfj","upvote_count":"9","timestamp":"1693494300.0","comment_id":"995222"},{"timestamp":"1737600480.0","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html","comment_id":"1345061","poster":"FlyingHawk","upvote_count":"1"},{"upvote_count":"4","poster":"emakid","timestamp":"1719707760.0","content":"Selected Answer: C\nOption C: Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization.\n\nThis solution meets the requirements in the following ways:\n\n Handles Unpredictable Traffic: Amazon Kinesis Data Firehose can handle variable amounts of streaming data and automatically scales to accommodate sudden increases in traffic.\n\n Integration with Web Applications: Amazon API Gateway provides a RESTful API endpoint for integrating with web applications.\n\n Authorization: An API Gateway Lambda authorizer provides the necessary authorization step to secure API access.\n\n Data Storage: Amazon Kinesis Data Firehose can deliver data directly to an Amazon S3 bucket for storage, making it suitable for long-term analytics and predictions.","comment_id":"1239458"},{"comments":[{"comment_id":"1284512","content":"You cannot use Kinesis Data stream to store data in S3. You need Firehose for that.","poster":"MatAlves","timestamp":"1726469280.0","upvote_count":"3"},{"comment_id":"1221366","content":"Ummm. This link (Use API Gateway Lambda authorizers) helps to validate \"C\" as the correct answer, not \"B\".","upvote_count":"2","poster":"Mr_Marcus","timestamp":"1717033020.0"}],"timestamp":"1716524580.0","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html","comment_id":"1217201","upvote_count":"2","poster":"Matte_"},{"timestamp":"1705424700.0","content":"Selected Answer: B\nB. Amazon Kinesis Data Firehose does not save anything","comments":[{"content":"Amazone Data Firehose can send data to S3: https://docs.aws.amazon.com/firehose/latest/dev/s3-object-name.html","poster":"FlyingHawk","upvote_count":"1","timestamp":"1737600540.0","comment_id":"1345062"},{"content":"option C...Amazon Kinesis Data Firehose that stores the information (that the company receives) in an Amazon S3 bucket.\nThis answer statement is worded in a complex way. It means to say that Firehose stores the data in S3 ...which company receives from API Gateway.","upvote_count":"3","poster":"jaswantn","timestamp":"1707494940.0","comment_id":"1145623"}],"comment_id":"1124371","poster":"4fad2f8","upvote_count":"2"},{"poster":"TariqKipkemei","comment_id":"1078228","timestamp":"1700727960.0","upvote_count":"4","content":"Selected Answer: C\nConfigure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization."},{"timestamp":"1698596100.0","upvote_count":"4","poster":"wsdasdasdqwdaw","content":"Using ECS just to stores the information is a overkill. So B or C then, lambda authoriser is the key word => C","comment_id":"1056956"},{"comment_id":"996169","poster":"Eminenza22","timestamp":"1693581720.0","upvote_count":"3","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-kinesisfirehose.html"},{"content":"C\n\n authorizer is configured for the method. If it is, API Gateway calls the Lambda function. The Lambda function authenticates the caller by means such as the following: Calling out to an OAuth provider to get an OAuth access token","comment_id":"996015","poster":"ErnShm","timestamp":"1693569120.0","upvote_count":"3"},{"comment_id":"995960","timestamp":"1693565340.0","upvote_count":"3","poster":"gispankaj","content":"Selected Answer: C\nlambda authoriser seems to be logical solution."}],"question_text":"A company is designing a solution to capture customer activity in different web applications to process analytics and make predictions. Customer activity in the web applications is unpredictable and can increase suddenly. The company requires a solution that integrates with other web applications. The solution must include an authorization step for security purposes.\n\nWhich solution will meet these requirements?","timestamp":"2023-08-31 17:05:00","unix_timestamp":1693494300,"answers_community":["C (86%)","14%"]},{"id":"qJdzIeDFekjKG0oYvuDP","discussion":[{"poster":"TiagueteVital","upvote_count":"5","timestamp":"1693692360.0","comment_id":"997158","content":"Selected Answer: D\nSnapshots are always a cost-efficience way to have a DR plan."},{"upvote_count":"5","timestamp":"1705013880.0","poster":"awsgeek75","content":"Selected Answer: D\nCross region data transfer is billable so think of smallest amount of data to transfer every 24 hours","comment_id":"1120268"},{"poster":"MatAlves","timestamp":"1726469520.0","content":"A, B, C => cross-region $$\nD => copy snapshots -> most cost-effectively.","comment_id":"1284516","upvote_count":"2"},{"timestamp":"1719707940.0","comment_id":"1239461","content":"Selected Answer: D\nOption D: Copy automatic snapshots to another Region every 24 hours.\n\n Explanation: This option involves copying RDS automatic snapshots to another Region. It is a straightforward way to ensure that snapshots are available in the event of a disaster. Since RDS snapshots are typically incremental and copied periodically, this solution matches the 24-hour RPO requirement effectively and is cost-effective compared to maintaining constant cross-Region replication.","poster":"emakid","upvote_count":"2"},{"content":"Selected Answer: D\nAmazon RDS creates and saves automated backups of your DB instance or Multi-AZ DB cluster during the backup window of your DB instance. RDS creates a storage volume snapshot of your DB instance, backing up the entire DB instance and not just individual databases. RDS saves the automated backups of your DB instance according to the backup retention period that you specify. If necessary, you can recover your DB instance to any point in time during the backup retention period.","timestamp":"1699296000.0","upvote_count":"3","comment_id":"1064158","poster":"potomac"},{"content":"most cost-effective way is just copying the snapshot (24h delta in the storage). => D","comment_id":"1053630","poster":"wsdasdasdqwdaw","timestamp":"1698229500.0","upvote_count":"3"},{"comment_id":"1007483","upvote_count":"4","poster":"Guru4Cloud","content":"Selected Answer: D\nDddddddddd","timestamp":"1694687760.0"},{"poster":"Eminenza22","content":"Selected Answer: D\nThis is the most cost-effective solution because it does not require any additional AWS services. Amazon RDS automatically creates snapshots of your DB instances every hour. You can copy these snapshots to another Region every 24 hours to meet your RPO and RTO requirements.\n\nThe other solutions are more expensive because they require additional AWS services. For example, AWS DMS is a more expensive service than AWS RDS.","timestamp":"1693840740.0","upvote_count":"4","comment_id":"998638"}],"choices":{"D":"Copy automatic snapshots to another Region every 24 hours.","B":"Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication.","C":"Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket.","A":"Create a cross-Region read replica and promote the read replica to the primary instance."},"unix_timestamp":1693692360,"question_images":[],"exam_id":31,"answer":"D","answer_description":"","isMC":true,"question_text":"An ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances that run Microsoft SQL Server Enterprise Edition. The company's current recovery point objective (RPO) and recovery time objective (RTO) are 24 hours.\n\nWhich solution will meet these requirements MOST cost-effectively?","timestamp":"2023-09-03 00:06:00","url":"https://www.examtopics.com/discussions/amazon/view/119718-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":563,"answer_ET":"D","answer_images":[],"topic":"1","answers_community":["D (100%)"]},{"id":"ZxdKjxjaF3EYIepiEgze","answers_community":["B (91%)","9%"],"exam_id":31,"discussion":[{"timestamp":"1694686680.0","poster":"Guru4Cloud","content":"Selected Answer: B\nThe key points are:\n\nElastiCache Redis provides in-memory caching that can deliver microsecond latency for session data.\nRedis supports replication and multi-AZ which can provide high availability for the cache.\nThe application can be updated to store session data in ElastiCache Redis rather than locally on the web servers.\nIf a web server fails, the user can be routed via the load balancer to another web server which can retrieve their session data from the highly available ElastiCache Redis cluster.","upvote_count":"9","comment_id":"1007468"},{"content":"Selected Answer: B\nhigh availability => use redis instead of Elastich memcache","timestamp":"1693435140.0","comment_id":"994505","upvote_count":"5","poster":"czyboi"},{"timestamp":"1719708300.0","upvote_count":"2","content":"Selected Answer: B\nOption B: Use Amazon ElastiCache for Redis to store the session state. Update the application to use ElastiCache for Redis to store the session state.\n\n Explanation: Amazon ElastiCache for Redis is suitable for session state storage because Redis provides both in-memory data storage and persistence options. Redis supports features like replication, persistence, and high availability (through Redis Sentinel or clusters). This ensures that session state is preserved and available even if individual web servers fail.","comment_id":"1239463","poster":"emakid"},{"poster":"pentium75","comment_id":"1111736","timestamp":"1704186300.0","content":"Selected Answer: B\nAs Memcached is not HA","upvote_count":"4"},{"timestamp":"1701740640.0","comment_id":"1088131","content":"A \nAs cache needs to be distributed as ALB is used.","poster":"SHAAHIBHUSHANAWS","upvote_count":"1"},{"timestamp":"1699296060.0","content":"Selected Answer: B\nB is correct","upvote_count":"3","comment_id":"1064160","poster":"potomac"},{"timestamp":"1695785040.0","comments":[],"poster":"franbarberan","content":"Selected Answer: D\nElastic cache is Only for RDS","comment_id":"1018384","upvote_count":"3"},{"upvote_count":"5","content":"Selected Answer: B\nredis is correct since it provides high availability and data persistance","timestamp":"1693565580.0","poster":"gispankaj","comment_id":"995967"},{"timestamp":"1693498980.0","poster":"Eminenza22","upvote_count":"4","content":"Selected Answer: B\nB is the correct answer. It suggests using Amazon ElastiCache for Redis to store the session state. Update the application to use ElastiCache for Redis to store the session state. This solution is cost-effective and requires minimal development effort.","comment_id":"995274"}],"question_id":564,"choices":{"B":"Use Amazon ElastiCache for Redis to store the session state. Update the application to use ElastiCache for Redis to store the session state.","C":"Use an AWS Storage Gateway cached volume to store session data. Update the application to use AWS Storage Gateway cached volume to store the session state.","D":"Use Amazon RDS to store the session state. Update the application to use Amazon RDS to store the session state.","A":"Use an Amazon ElastiCache for Memcached instance to store the session data. Update the application to use ElastiCache for Memcached to store the session state."},"answer":"B","timestamp":"2023-08-31 00:39:00","question_images":[],"topic":"1","question_text":"A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer that has sticky sessions enabled. The web server currently hosts the user session state. The company wants to ensure high availability and avoid user session state loss in the event of a web server outage.\n\nWhich solution will meet these requirements?","answer_description":"","unix_timestamp":1693435140,"answer_images":[],"answer_ET":"B","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/119487-exam-aws-certified-solutions-architect-associate-saa-c03/"},{"id":"7iKjWkS1xYOvbylDmfJs","url":"https://www.examtopics.com/discussions/amazon/view/85793-exam-aws-certified-solutions-architect-associate-saa-c03/","exam_id":31,"choices":{"A":"Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.","B":"Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.","C":"Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to process the data for analysis.","D":"Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis."},"answer_description":"","unix_timestamp":1666092540,"question_id":565,"answer_images":[],"question_images":[],"question_text":"A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day.\nWhat should a solutions architect do to transmit and process the clickstream data?","answer":"D","isMC":true,"topic":"1","timestamp":"2022-10-18 13:29:00","answer_ET":"D","answers_community":["D (91%)","9%"],"discussion":[{"comment_id":"751236","timestamp":"1671555120.0","poster":"Buruguduystunstugudunstuy","upvote_count":"39","content":"Selected Answer: D\nOption D is the most appropriate solution for transmitting and processing the clickstream data in this scenario.\n\nAmazon Kinesis Data Streams is a highly scalable and durable service that enables real-time processing of streaming data at a high volume and high rate. You can use Kinesis Data Streams to collect and process the clickstream data in real-time.\n\nAmazon Kinesis Data Firehose is a fully managed service that loads streaming data into data stores and analytics tools. You can use Kinesis Data Firehose to transmit the data from Kinesis Data Streams to an Amazon S3 data lake.\n\nOnce the data is in the data lake, you can use Amazon Redshift to load the data and perform analysis on it. Amazon Redshift is a fully managed, petabyte-scale data warehouse service that allows you to quickly and efficiently analyze data using SQL and your existing business intelligence tools.","comments":[{"timestamp":"1671555120.0","poster":"Buruguduystunstugudunstuy","comments":[{"content":"The question does not say that real-time is needed here","poster":"MutiverseAgent","comments":[{"comment_id":"1105112","poster":"pentium75","upvote_count":"1","content":"Question asks how to \"transmit and process the clickstream data\", NOT how to analyze it. Thus D.","timestamp":"1703493540.0"}],"upvote_count":"3","timestamp":"1688668320.0","comment_id":"944928"}],"upvote_count":"10","content":"Option A, which involves using AWS Data Pipeline to archive the data to an Amazon S3 bucket and running an Amazon EMR cluster with the data to generate analytics, is not the most appropriate solution because it does not involve real-time processing of the data.\n\nOption B, which involves creating an Auto Scaling group of Amazon EC2 instances to process the data and sending it to an Amazon S3 data lake for Amazon Redshift to use for analysis, is not the most appropriate solution because it does not involve a fully managed service for transmitting the data from the processing layer to the data lake.\n\nOption C, which involves caching the data to Amazon CloudFront, storing the data in an Amazon S3 bucket, and running an AWS Lambda function to process the data for analysis when an object is added to the S3 bucket, is not the most appropriate solution because it does not involve a scalable and durable service for collecting and processing the data in real-time.","comment_id":"751238"}]},{"content":"Selected Answer: D\nOption D. \n\nhttps://aws.amazon.com/es/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/","comment_id":"698169","poster":"ArielSchivo","comments":[{"timestamp":"1670318340.0","content":"Unsure if this is right URL for this scenario. Option D is referring to S3 and then Redshift. Whereas URL discuss about eliminating S3 :- We’re excited to launch Amazon Redshift streaming ingestion for Amazon Kinesis Data Streams, which enables you to ingest data directly from the Kinesis data stream without having to stage the data in Amazon Simple Storage Service (Amazon S3). Streaming ingestion allows you to achieve low latency in the order of seconds while ingesting hundreds of megabytes of data into your Amazon Redshift cluster.","comment_id":"736674","poster":"RBSK","upvote_count":"5"}],"upvote_count":"17","timestamp":"1666092540.0"},{"comment_id":"1335235","poster":"satyaammm","upvote_count":"1","timestamp":"1735750500.0","content":"Selected Answer: D\nKinesis data firehouse is the most suitable for streaming data and Redshift is the most suitable for large data sets."},{"content":"Selected Answer: D\nAns D - using Kinesis Streams / Firehouse (data in/out) is fast and reliable. Using Redshift allows all sorts of permutations of data analyses and interfacing to user apps","upvote_count":"2","timestamp":"1726057620.0","comment_id":"1282090","poster":"PaulGa"},{"timestamp":"1720418940.0","content":"D is the best option","upvote_count":"1","comment_id":"1244118","poster":"effiecancode"},{"timestamp":"1705246560.0","poster":"awsgeek75","content":"Selected Answer: D\nA: Not sure how recent this question is but Data Pipeline is not really a product AWS is recommending anymore https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\n\nB: 30TB of clickstream data could be done with EC2 but it would be challenging\nC: CloudFront is for CDN and caching and mostly outgoing data, not incoming.\nD: Kinesis, S3 data lake and Redshift will work perfectly for this case","upvote_count":"4","comment_id":"1122631"},{"upvote_count":"1","content":"Selected Answer: A\nThe answer should be A. Clickstream does not mean real time, it just means they capture user interactions on the web page. Kinesis data streaming is not required. Furthermore, redshift is a data warehousing solution, it cant run complex analysis as well as EMR. My vote goes for A","comments":[{"poster":"pentium75","upvote_count":"1","comment_id":"1105113","timestamp":"1703493600.0","content":"Question asks how to \"transmit and process the clickstream data\", NOT how to analyze it. Also question does NOT ask how to archive the data (as is mentioned in A). Thus D."}],"comment_id":"1104440","timestamp":"1703390520.0","poster":"clumsyninja4life"},{"comment_id":"983533","timestamp":"1692270120.0","poster":"Reckless_Jas","content":"when you see clickstream data, think about Kinesis Data Stream","upvote_count":"6"},{"poster":"Guru4Cloud","content":"Selected Answer: D\nThe key reasons are:\n\nKinesis Data Streams can continuously capture and ingest high volumes of clickstream data in real-time. This handles the large 30TB daily data intake.\nKinesis Firehose can automatically load the streaming data into S3. This creates a data lake for further analysis.\nFirehose can transform and analyze the data in flight before loading to S3 using Lambda. This enables real-time processing.\nThe data in S3 can be easily loaded into Amazon Redshift for interactive analysis at scale.\nKinesis auto scales to handle the high data volumes. Minimal effort is needed for infrastructure management.","comment_id":"976957","timestamp":"1691606880.0","upvote_count":"2"},{"timestamp":"1689780900.0","upvote_count":"2","poster":"miki111","content":"Option D is the correct answer","comment_id":"956771"},{"content":"Selected Answer: D\nA. This option utilizes S3 for data storage and EMR for analytics, Data Pipeline is not ideal service for real-time streaming data ingestion and processing. It is better suited for batch processing scenarios.\n\nB. This option involves managing and scaling EC2, which adds operational overhead. It is also not real-time streaming solution. Additionally, use of Redshift for analyzing clickstream data might not be most efficient or cost-effective approach.\n\nC. CloudFront is CDN service and is not designed for real-time data processing or analytics. While using Lambda to process data can be an option, it may not be most efficient solution for processing large volumes of clickstream data.\n\nTherefore, collecting the data from Kinesis Data Streams, using Kinesis Data Firehose to transmit it to S3 data lake, and loading it into Redshift for analysis is the recommended approach. This combination provides scalable, real-time streaming solution with storage and analytics capabilities that can handle high volume of clickstream data.","upvote_count":"2","comment_id":"929413","timestamp":"1687346940.0","poster":"cookieMr"},{"upvote_count":"1","poster":"Rahulbit34","comment_id":"886804","content":"Clickstream is the key - Answer is D","timestamp":"1682981340.0"},{"comments":[{"timestamp":"1688668560.0","comments":[{"upvote_count":"2","poster":"juanrasus2","timestamp":"1697472480.0","content":"Also the Kinesis family is related to real time or near real time services. This is not a requirement at all. We have to process data daily, but not need to do it in real time","comment_id":"1045125"}],"poster":"MutiverseAgent","upvote_count":"2","comment_id":"944932","content":"I think I agree with you, I does not make sense in option D) using Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake and then to Redshift, as you can send directly the data from Firehose to Redshift."},{"upvote_count":"1","comment_id":"1105114","content":"Question asks how to \"transmit and process the clickstream data\", NOT how to analyze it. This picture shows exactly scenario D:\n\nProducer - Kinesis - Intermediate S3 bucket - Redshift\n\nhttps://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2020/07/30/StreamTransformAnalyzeKinesisLambdaRedshift1.png","poster":"pentium75","timestamp":"1703493660.0"}],"poster":"PaoloRoma","timestamp":"1679753820.0","comment_id":"850189","upvote_count":"6","content":"Selected Answer: A\nI am going to be unpopular here and I'll go for A). Even if here are other services that offer a better experience, data Pipeline can do the job here. \"you can use AWS Data Pipeline to archive your web server's logs to Amazon Simple Storage Service (Amazon S3) each day and then run a weekly Amazon EMR (Amazon EMR) cluster over those logs to generate traffic reports\" https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html In the question there is no specific timing requirement for analytics. Also the EMR cluster job can be scheduled be executed daily.\n\nOption D) is a valid answer too, however with Amazon Redshift Streaming Ingestion \"you can connect to Amazon Kinesis Data Streams data streams and pull data directly to Amazon Redshift without staging data in S3\" https://aws.amazon.com/redshift/redshift-streaming-ingestion. So in this scenario Kinesis Data Firehose and S3 are redundant."},{"content":"Selected Answer: D\nOption D","poster":"career360guru","upvote_count":"1","comment_id":"749425","timestamp":"1671421680.0"},{"comment_id":"747383","poster":"studis","upvote_count":"1","comments":[{"content":"Makes sense, but this is D, not C","poster":"pentium75","timestamp":"1703493420.0","comment_id":"1105111","upvote_count":"1"}],"timestamp":"1671207840.0","content":"It is C.\nThe image in here https://aws.amazon.com/kinesis/data-firehose/ shows how kinesis can send data collected to firehose who can send it to Redshift. \nIt is also possible to use an intermediary S3 bucket between firehose and redshift. See image in here \nhttps://aws.amazon.com/blogs/big-data/stream-transform-and-analyze-xml-data-in-real-time-with-amazon-kinesis-aws-lambda-and-amazon-redshift/"},{"poster":"sebasta","upvote_count":"4","content":"Why not A? \nYou can collect data with AWS Data Pipeline and then analyze it with EMR. Whats wrong with this option?","comments":[{"content":"It's not A, the wording is tricky! It says \"to archive the data to S3\" - there is no mention of archiving in the question, so it has to be D :)","poster":"bearcandy","comment_id":"742193","upvote_count":"3","timestamp":"1670800500.0","comments":[{"poster":"pentium75","timestamp":"1703493720.0","comment_id":"1105115","content":"And, the the question is not asking about analyzing the data at all, just about \"transmitting and processing\".","upvote_count":"1"}]}],"timestamp":"1669968360.0","comment_id":"733563"},{"upvote_count":"1","timestamp":"1669039500.0","poster":"Wpcorgan","content":"D is correct","comment_id":"723578"},{"upvote_count":"3","content":"Click Stream & Analyse/ process- Think KDS,","poster":"PS_R","timestamp":"1667751360.0","comment_id":"712460"},{"comment_id":"700479","poster":"BoboChow","upvote_count":"4","timestamp":"1666322520.0","content":"Selected Answer: D\nD seems to make sense"},{"upvote_count":"1","timestamp":"1666297500.0","poster":"JesseeS","comment_id":"700279","content":"Option D is correct... See the resource. Thank you Ariel"}]}],"exam":{"provider":"Amazon","numberOfQuestions":1019,"lastUpdated":"11 Apr 2025","isImplemented":true,"isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31,"isBeta":false},"currentPage":113},"__N_SSP":true}