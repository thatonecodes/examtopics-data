{"pageProps":{"questions":[{"id":"sSvEPaRIq6eVYAmrMBr0","question_text":"A company has a data warehouse in Amazon Redshift that is approximately 500 TB in size. New data is imported every few hours and read-only queries are run throughout the day and evening. There is a particularly heavy load with no writes for several hours each morning on business days. During those hours, some queries are queued and take a long time to execute. The company needs to optimize query execution and avoid any downtime.\nWhat is the MOST cost-effective solution?","exam_id":20,"discussion":[{"timestamp":"1632131880.0","content":"Answer A is correct- https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html","comment_id":"164810","poster":"awssp12345","upvote_count":"28"},{"upvote_count":"10","poster":"Thiya","content":"Answer: A\nWLM Concurrency scaling feature automatically adds additional capacity for both read and write queries and charged only for the duration the queries are actively running. Hence, it is the cost-effective approach. https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html","timestamp":"1638025680.0","comment_id":"488229"},{"timestamp":"1704969960.0","poster":"GCPereira","content":"Easy question, A any througs?\n\nThrough workload management, you can prioritize queries and define execution patterns across your cluster...\n\nresizing, snapshot, or other solutions in these answers is a bit expansive and ineffective.","upvote_count":"2","comment_id":"1119621"},{"content":"A: I passed the test","upvote_count":"4","comment_id":"886311","poster":"pk349","timestamp":"1682948100.0"},{"content":"Selected Answer: A\nA. Enable concurrency scaling in the workload management (WLM) queue is the most cost-effective solution.\n\nEnabling concurrency scaling in the workload management (WLM) queue allows Amazon Redshift to add more cluster capacity to handle the increased query load during peak hours. This is done automatically and can be configured based on the number of users or the number of queries. Concurrency scaling can be turned off during off-peak hours to save costs. This is a more cost-effective solution compared to adding more nodes or using elastic resize, which can be more expensive and take longer to configure. Snapshot, restore, and resize operations can also be time-consuming and may result in downtime.","poster":"AwsNewPeople","comment_id":"837219","timestamp":"1678638540.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: A\nCorrect answer is A as Redshift Concurrency Scaling can help scale the cluster to support virtually unlimited concurrent users and queries.\nOptions B, C & D are wrong as they scale the cluster by adding resources that would not be cost-effective.","comment_id":"711814","poster":"cloudlearnerhere","timestamp":"1667658600.0"},{"poster":"Arka_01","content":"Selected Answer: A\nWherever query is stuck for other long-running queries, we can use WLM.","timestamp":"1664082540.0","comment_id":"678448","upvote_count":"1"},{"poster":"rocky48","upvote_count":"1","comment_id":"633863","timestamp":"1658295300.0","content":"Selected Answer: A\nAnswer A"},{"timestamp":"1650648360.0","comment_id":"590153","poster":"AWSRanger","upvote_count":"2","content":"Selected Answer: A\nA is right"},{"timestamp":"1637182200.0","poster":"aws2019","comment_id":"480265","content":"A is correct","upvote_count":"1"},{"content":"Answer A is correct. B, C, D = wrong, because they all talk about resizing or scaling which will not be cost-effective.","upvote_count":"3","timestamp":"1636286520.0","comment_id":"448392","poster":"rosnl"},{"timestamp":"1636047060.0","upvote_count":"2","content":"A is the right answer","poster":"lostsoul07","comment_id":"274288"},{"timestamp":"1634779500.0","upvote_count":"2","content":"A is correct for me","poster":"BillyC","comment_id":"216763"},{"poster":"syu31svc","timestamp":"1634392080.0","comment_id":"191566","content":"https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html\nAnswer is A 100%","upvote_count":"2"},{"comment_id":"175532","timestamp":"1632688080.0","upvote_count":"4","poster":"Paitan","content":"A for sure.\nhttps://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html"}],"unix_timestamp":1598234820,"question_images":[],"topic":"1","choices":{"D":"Use a snapshot, restore, and resize operation. Switch to the new target cluster.","B":"Add more nodes using the AWS Management Console during peak hours. Set the distribution style to ALL.","A":"Enable concurrency scaling in the workload management (WLM) queue.","C":"Use elastic resize to quickly add nodes during peak times. Remove the nodes when they are not needed."},"answer_description":"","timestamp":"2020-08-24 04:07:00","question_id":91,"answer":"A","isMC":true,"answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/29408-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer_images":[],"answer_ET":"A"},{"id":"UeGeIJ76VUjPQDJyNKb7","question_images":[],"answers_community":["B (47%)","A (32%)","C (21%)"],"question_id":92,"answer_ET":"B","timestamp":"2020-08-20 10:32:00","url":"https://www.examtopics.com/discussions/amazon/view/29115-exam-aws-certified-data-analytics-specialty-topic-1-question/","answer":"B","question_text":"A company analyzes its data in an Amazon Redshift data warehouse, which currently has a cluster of three dense storage nodes. Due to a recent business acquisition, the company needs to load an additional 4 TB of user data into Amazon Redshift. The engineering team will combine all the user data and apply complex calculations that require I/O intensive resources. The company needs to adjust the cluster's capacity to support the change in analytical and storage requirements.\nWhich solution meets these requirements?","discussion":[{"content":"Vote A.\n\"currently has a cluster of three dense storage nodes.\" means it is not single-node cluster, both resizing work, but classic resize take 2 hours–2 days or longer, depending on your data's size.\nDense Compute (DC) allow creation of very high performance data warehouses using fast CPUs, large amounts of RAM and solid-state disks (SSDs)","timestamp":"1632361740.0","poster":"lui","comments":[{"timestamp":"1632692760.0","poster":"skar21","upvote_count":"3","comments":[{"upvote_count":"5","comment_id":"207443","content":"I don't think this is correct . When you elastic upsize you'll get more slices but when you elastic downsize you won't loose any slices. (I tested it myself)","poster":"jove","timestamp":"1634567340.0"}],"comment_id":"169579","content":"A & C should be avoided. Elastic resize is for temporary size adjustment. When you do the Elastic resize, the \"SLICE\" count \"will not\" change and not good for long term data load and computation."}],"comment_id":"167603","upvote_count":"34"},{"content":"The answer is C.\n\nby https://aws.amazon.com/redshift/features/?nc1=h_ls\naccoranding to the question \"...combine all the user data and apply complex calculations that require I/O intensive resources\",\n\ndrop A, B sinice \"Dense Compute (DC) the best choice for less than 500GB of data\".\n\nDS2 (Dense Storage) nodes enable you to create large data warehouses using hard disk drives (HDDs).Most customers who run on DS2 clusters can migrate their workloads to RA3 clusters and get up to 2x performance and more storage for the same cost as DS2.\n\nby https://aws.amazon.com/redshift/pricing/\n\"What to expect\" section\nOnce you make your selection, you may wish to use elastic resize to easily adjust the amount of provisioned compute capacity within minutes for steady-state processing.","timestamp":"1632452040.0","comments":[{"comment_id":"174191","poster":"GauravM17","content":"how DS would manage complex calculations and IO intensive aspects?","comments":[{"comment_id":"608036","timestamp":"1653648240.0","upvote_count":"2","content":"is 500 GB limit for each node? Since we are doing resize, we are adding new nodes as well. So we should take care of compute i think. Hence A.","poster":"certificationJunkie"}],"upvote_count":"8","timestamp":"1632730620.0"}],"comment_id":"169237","poster":"DonaldCMLIN","upvote_count":"26"},{"poster":"rag_mat_80","content":"classic resize does not let you change node type ( note that original config for infra is nodetype = DS2 ) meaning option B is out . If you only want to increase the number of nodes of same type , then you can do that with classic resize ( offcource it will create a new cluster ) so in my opinion C is useless as i can use classic storage itself . Between A and D i vote for A since the use case talks about compute more than storage","upvote_count":"1","comments":[{"upvote_count":"1","poster":"rag_mat_80","content":"correction - read the use case again and the company already has data in the warehouse so one thing for sure that classic resize will not retain system tables and data . So option B and D are out . Between A and C , i still vote for A since as per redshift documentation dc2 is a good choice for data < 10 TB . From the use case we don't know what's existing size of the warehouse but we are 4 TB to X . We don't know X so dc2 seems logical","timestamp":"1712146920.0","comment_id":"1188652"}],"timestamp":"1712146260.0","comment_id":"1188645"},{"comment_id":"1185890","upvote_count":"1","timestamp":"1711778220.0","content":"Selected Answer: C\ndefinitively C","poster":"patou"},{"poster":"tsangckl","content":"Selected Answer: B\nBing answer B\nB. Resize the cluster using classic resize with dense compute nodes.\n\nExplanation:\n\nOption B is correct because classic resize allows you to change both the node type and number of nodes. In this case, switching to dense compute nodes would provide the I/O intensive resources needed for the complex calculations. The classic resize operation also redistributes the data and reclaims the space, which would be beneficial given the additional 4 TB of user data.\nOther options are not the best solutions for this scenario. For example, Options A and C involve using elastic resize, which allows you to quickly add or remove nodes, but it doesn’t allow you to change the node type. Option D involves using classic resize with dense storage nodes, but this might not provide the I/O intensive resources needed for the complex calculations.","timestamp":"1711437480.0","upvote_count":"1","comment_id":"1183095"},{"poster":"NarenKA","upvote_count":"2","content":"Selected Answer: A\nIt's important to note that while classic resize (Option B and D) allows for a change in node types (from DS to DC or vice versa), it involves a longer downtime as it copies data from the old cluster to a new one.\nElastic resize is faster than classic resize. it allows you to quickly add or remove nodes to the cluster while keeping the cluster online. This minimizes downtime and can be completed in minutes, as opposed to classic resize, which can take several hours or more, depending on the size of the dataset.\nDense compute (DC) nodes are optimized for performance-intensive workloads. They offer faster CPUs, increased I/O performance, and higher storage throughput compared to dense storage (DS) nodes. Given the engineering team's need to apply complex calculations that require I/O intensive resources, switching to dense compute nodes will provide the necessary computational power and I/O performance.","comment_id":"1155478","timestamp":"1708515780.0"},{"upvote_count":"1","content":"Vote A.\n\nThe questions specifies I/O as being important which goes right to compute nodes. The reccomendation from AWS is to start with Elastic Resize first. The DC node types have enough to store this amount of data and the number of resized nodes won't exceed the limitations of an elastic resize; primarily the either 2x increase or decrease max of a cluster. If you need to 4x or 8x you node size, then you need to use classic.\n\nThis was also a (similar) question in Jon Bosno's practice tests in tutorialdojos","timestamp":"1705672980.0","poster":"metkillas","comment_id":"1126721"},{"comment_id":"1119627","content":"You need to store 4TB of user data in Redshift (data that changes little) so the size of these datasets would vary little...\n\nThey will also execute extremely complex queries...\n\nThey need to adjust to support these requirements. The first point implies continuing with dense storage nodes, but they also need compute nodes. Given that they have storage nodes and need more computation, I would change the node type, increasing this quantity... Then I would opt for the classic resize.\n\nC agreed?","poster":"GCPereira","upvote_count":"1","timestamp":"1704970440.0"},{"content":"I cant understand this question what to ask me?\nvery abstract question.\nso I choose C . ChatCPT answerd hahaha.","upvote_count":"1","comment_id":"1067598","poster":"LocalHero","timestamp":"1699662600.0"},{"poster":"michalf84","comment_id":"1067164","upvote_count":"1","timestamp":"1699612560.0","content":"Selected Answer: C\nElastic as faster and dense storage due to size limit6"},{"comment_id":"1064014","timestamp":"1699285800.0","content":"Selected Answer: C\nI believe option C is the most suitable choice. The reason is that dc2 nodes have a relatively limited SSD capacity of 160GB, while your data size is 4TB. Therefore, you'll need to opt for a dense storage node type to handle the increased storage requirements effectively.","upvote_count":"2","poster":"markstudy"},{"upvote_count":"1","timestamp":"1694023380.0","comment_id":"1000864","poster":"leotoras","content":"A is correct: Elastic resize across node type automates the steps of taking a snapshot, creating a new cluster, deleting the old cluster, and renaming the new cluster into a simple, quick, and familiar operation. Elastic resize operation can be run at any time or can be scheduled to run at a future time. Customers can quickly upgrade their existing DS2 or DC2 node type-based cluster to the new RA3 node type with elastic resize."},{"comment_id":"996451","upvote_count":"3","content":"Selected Answer: B\nVote for B. Following is the experience I tried on Sept 1st 2023\n- Redshift currently allows me to create a dc2 or ra3 cluster. Although it shows the ds2 option on Resize option, the console will display “NumberOfNodesQuotaExceeded You do not have access to node type ds2.xlarge. Choose another node type” even the quota in the service is enough. In short, I don’t think the candidate can simulate this at home unless he/she currently works for some big companies with existing ds2 Redshift clusters.","timestamp":"1693609440.0","comments":[{"poster":"zanhsieh","timestamp":"1693609440.0","comment_id":"996452","comments":[{"content":"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#working-with-clusters-overview","timestamp":"1693609560.0","comment_id":"996453","poster":"zanhsieh","upvote_count":"1"}],"upvote_count":"1","content":"- The way how Redshift increases its computation power and storage is either adding more nodes (elastic) with uniform node type and storage, or snapshot whole cluster then upgrade (classic). The snapshot way can change node type.\n- Since AWS deprecated dense storage(ds2) cluster creation since 2021-08-01 and deprecated ds2 node type since 2021-12-31, no dense storage node can be added, which means we should drop C and D.\n- Since converting from ds to dc, there is no way to use elastic, so we drop A.\n- Can dc2 handle extra 4TB without exceeding its maximum number of nodes? Yes. dc2.l max nodes 32, total capacity 5.12TB; dc2.8xl max nodes 128, total capacity 326TB"}],"poster":"zanhsieh"},{"timestamp":"1693594560.0","poster":"r3mo","content":"The Answer is C:\nBecause... Dense storage nodes provide a balance between \"storage capacity and computational power\", making them suitable for analytical workloads that involve heavy I/O operations.","upvote_count":"2","comment_id":"996362"},{"timestamp":"1691058360.0","poster":"MLCL","comment_id":"970994","content":"Selected Answer: A\nElsatic resize allows to change node types.","upvote_count":"2"},{"comment_id":"954240","content":"Selected Answer: B\nClassic resize is for change the node types, we already got storage and we need compute","poster":"developeranfc","upvote_count":"3","timestamp":"1689598620.0"},{"timestamp":"1689359280.0","content":"Selected Answer: B\nClassic Resize can allow changing node type and no of nodes, + there is no time constraints added in the question.","poster":"rookiee1111","upvote_count":"2","comment_id":"951760"},{"poster":"Menyawy","upvote_count":"2","timestamp":"1685517060.0","comment_id":"910956","content":"Selected Answer: B\nClassic will enable you to reach the required storage capacity while elastic will not"},{"poster":"pk349","comment_id":"886312","timestamp":"1682948280.0","upvote_count":"3","content":"B: I passed the test"},{"timestamp":"1681930380.0","upvote_count":"1","comment_id":"875025","poster":"MeshterZYX","content":"Selected Answer: A\nIt's never C. I vote A"},{"upvote_count":"2","content":"Since the requirement is for I/O intensive calculations, it is best to use dense storage nodes as they are optimized for storage capacity rather than processing power. The cluster needs to accommodate an additional 4 TB of data, so resizing the cluster with elastic resize is the most cost-effective solution as it allows for a quick and seamless resizing operation without downtime.\n\nTherefore, the correct solution is option C: Resize the cluster using elastic resize with dense storage nodes.","timestamp":"1677419160.0","poster":"rags140882","comment_id":"822470"},{"comment_id":"811577","poster":"Gabba","content":"We can safely remove options related to DS2 now as DS2 is decommissioned. We can now either select DC2 or RA3 node types. So only 2 options valid are DC2 with elastic resize or classic resize where elastic is definitely better option. So A.","timestamp":"1676616780.0","upvote_count":"1"},{"content":"Selected Answer: A\nI also think it is A","comment_id":"810661","poster":"[Removed]","timestamp":"1676552280.0","upvote_count":"1"},{"upvote_count":"2","comment_id":"802436","timestamp":"1675883460.0","content":"Answer is A","poster":"Ashoks"},{"content":"Selected Answer: B\nThe correct answer is confirm B. Not A or C. \nI've tried myself create a redshift cluster 3 node with DS. Yes, you can elastic resize (A) to DC, but using elastic resize (A), the storage size will be limited and you cannot achieve the additional \"4TB\". The only way to resize to DC and achieve additional \"4TB\" requirement, is only through answer B classic resize. If you don't believe me, just go and create a quick redshift cluster to prove this.","upvote_count":"7","poster":"HeroX118","timestamp":"1675123500.0","comment_id":"793479"},{"upvote_count":"3","comment_id":"729872","content":"Answer- A\nElastic resize – Use elastic resize to change the node type, number of nodes, or both. If you only change the number of nodes, then queries are temporarily paused and connections are held open if possible. During the resize operation, the cluster is read-only. Typically, elastic resize takes 10–15 minutes. AWS recommends using elastic resize when possible.\n\nClassic resize – Use classic resize to change the node type, number of nodes, or both. Choose this option when you are resizing to a configuration that isn't available through elastic resize. An example is to or from a single-node cluster. During the resize operation, the cluster is read-only. Typically, classic resize takes 2 hours–2 days or longer, depending on your data's size.","poster":"henom","timestamp":"1669690980.0"},{"content":"A : Do elastic resize and change node type from ds to RA3\nElastic resize – You can add nodes to or remove nodes from your cluster. You can also change the node type, such as from DS2 nodes to RA3 nodes. \nChange the node type for a cluster – When you change the node type, a snapshot is created and data is redistributed from the source cluster to a cluster comprised of the new node type. On completion, running queries are dropped. Like the in-place resize, it completes quickly.\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html#elastic-resize\nConsider choosing RA3 node types in these cases:\nYour data volume is growing rapidly or is expected to grow rapidly.\nYou want the flexibility to size the cluster based only on your performance needs.\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-ra3-node-types","poster":"nadavw","upvote_count":"2","comment_id":"720546","timestamp":"1668696840.0"},{"comment_id":"711823","content":"Selected Answer: A\nCorrect option is A as you need to move to Dense Compute nodes to support resource intensive calculations. Also, Amazon Redshift now supports changing node types within minutes with elastic resize.\nhttps://aws.amazon.com/about-aws/whats-new/2020/04/amazon-redshift-now-supports-changing-node-types-within-minutes-with-elastic-resize/\n\nOption B is wrong as changing node type is possible via elastic resize. Also, classic resize may take hours, or even days, to perform the resize operation. AWS also recommends using elastic resize over the classic type whenever possible.\n\n\nOptions C & D are wrong as dense storage nodes are not ideal for resource intensive calculations.","timestamp":"1667659200.0","poster":"cloudlearnerhere","upvote_count":"1"},{"timestamp":"1666597080.0","poster":"Haimett","content":"Anyone has a clear solution for this one?? For me it´s B since node types need to be changed so classic resizing won´t work and I/O intensive operations suggest that cost is less relevant that performance. Any thoughts are welcome","upvote_count":"2","comment_id":"702792"},{"content":"Selected Answer: A\n\"apply complex calculations that require I/O intensive resources\" - This requires DC node, not DS. Also classic resize is time consuming and best avoided.","poster":"Arka_01","timestamp":"1664082780.0","comment_id":"678452","upvote_count":"3"},{"content":"Answer A - why?\nDense Compute (DC2) nodes allow you to have compute-intensive data warehouses with local SSD storage included. You choose the number of nodes, that you need based on data size and performance requirements. DC2 nodes store your data locally for high performance, and as the data size grows, you can add more compute nodes to increase the storage capacity of the cluster. DC2 nodes allow only up to 2.56TB storage per node but with a very high I/O performance of 7.50 GB/s\nhttps://aws.amazon.com/redshift/pricing/","timestamp":"1660193160.0","upvote_count":"2","poster":"rudramadhu","comment_id":"645259"},{"comments":[{"content":"why you cannot change node type?","poster":"JoellaLi","comment_id":"698235","upvote_count":"1","timestamp":"1666096260.0"},{"upvote_count":"2","poster":"pgf909","content":"https://aws.amazon.com/premiumsupport/knowledge-center/resize-redshift-cluster/\nIf elastic resize is available as an option, use elastic resize to change the node type, number of nodes, or both.","timestamp":"1667081520.0","comment_id":"707512"}],"timestamp":"1659081600.0","content":"Selected Answer: C\nA is imposible u cant resizw elastically and change node type, \nD is no senses to do clasic resize if you are not changing a type,\nfor 4TB+ data I would go for DS type","poster":"arboles","upvote_count":"3","comment_id":"639076"},{"comments":[{"upvote_count":"1","content":"Sorry Answer: A","timestamp":"1658381940.0","poster":"rocky48","comment_id":"634387"}],"poster":"rocky48","upvote_count":"1","content":"Selected Answer: A\nSelected Answer: B","comment_id":"634386","timestamp":"1658381880.0"},{"timestamp":"1657528560.0","upvote_count":"3","comment_id":"629920","poster":"dushmantha","content":"Selected Answer: B\nObjective is clear. We need to allow for I/O burst. So best out of given choices is Dense Compute. But elastic resize can't use to resize from Storage to Compute or vise versa. It can only change from same type. Therefore answer should be B.","comments":[{"timestamp":"1667081760.0","comment_id":"707514","poster":"pgf909","upvote_count":"1","content":"You can also change the node type, such as from DS2 nodes to RA3 nodes.\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html#elastic-resize \nWhy \"elastic resize can't use to resize from Storage to Compute or vise versa\"?"}]},{"comment_id":"616547","poster":"Booqq","upvote_count":"3","content":"Vote A. \n\"For customers with less than 500GB of data in their data warehouses, Dense Compute nodes are the most cost-effective and highest performance option. Above 500GB, customers whose primary focus is performance can continue with Dense Compute nodes up to hundreds of terabytes, giving them the highest ratio of CPU, Memory and I/O to storage. If performance isn’t as critical for a customer’s use case, or if customers want to prioritize reducing costs further, they can use the larger Dense Storage nodes and scale up to a petabyte or more of compressed user data for under $1,000/TB/Year \"\nsee https://aws.amazon.com/about-aws/whats-new/2014/01/23/amazon-redshift-ssd-node-type/","timestamp":"1655271600.0"},{"comment_id":"613420","poster":"CloudTimes","content":"A since computation need is increasing","upvote_count":"1","timestamp":"1654709340.0"},{"upvote_count":"1","timestamp":"1652783100.0","content":"Selected Answer: C\nin current mode, its RA3, I will go with Option -C\nas A is recommended only when Data is less than 1TB\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html","poster":"Shammy45","comment_id":"602879"},{"upvote_count":"1","comment_id":"595275","timestamp":"1651351080.0","content":"Answer - A","poster":"jrheen"},{"timestamp":"1651152360.0","upvote_count":"4","poster":"Teraxs","content":"Selected Answer: A\nWe recommend using elastic resize whenever possible, because it completes much more quickly than classic resize.\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html#rs-resize-tutorial\n\nDC2 has higher I/O than DS2 (and RA3) at least per $ paid\nhttps://aws.amazon.com/redshift/pricing/","comment_id":"593832"},{"poster":"AjayAws","comment_id":"574256","content":"Selected Answer: C\nC Seems correct","timestamp":"1648117620.0","upvote_count":"2"},{"poster":"RSSRAO","comment_id":"551434","upvote_count":"1","timestamp":"1645316340.0","content":"Selected Answer: B\nClassic Resize with dense compute nodes is correct."},{"content":"A\n\"accommodate changing analytical and storage requirements\"\nHere is not only the storage requirement changed, but also analytical one.\nSo this team must go with Dense Compute (DC).\nAfter Apr 6, 2020, elastic resize can change the node type.\nThe question does not mention the cost.\nBut I think this question will not appear in the exam, now there is RA3 note type, so in the actual exam there may be RA3 question.","upvote_count":"3","poster":"npt","comment_id":"506527","timestamp":"1640130900.0"},{"upvote_count":"2","timestamp":"1638144540.0","comments":[{"poster":"yogen","content":"incorrect..Using Elastic resize you change node type as well.\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html","comment_id":"510452","timestamp":"1640622360.0","upvote_count":"1"}],"comment_id":"489511","content":"i think B since with Elastic resize only can Quickly add or remove nodes of same type so we to need use classic way to resizing to add a compute node along with storage nodes","poster":"arun004"},{"poster":"aws2019","timestamp":"1637377680.0","upvote_count":"2","comment_id":"482235","content":"Ans is C\n\n\n\nvCPU Memory Addressable storage capacity I/O Price\nDense Compute DC2 \n\n\n\n\ndc2.large 2 15 GiB 0.16TB SSD 0.60 GB/s $0.25 per Hour\ndc2.8xlarge 32 244 GiB 2.56TB SSD 7.50 GB/s $4.80 per Hour\nDense Storage DS2 \n\n\n\n\nds2.xlarge 4 31 GiB 2TB HDD 0.40 GB/s $0.85 per Hour\nds2.8xlarge 36 244 GiB 16TB HDD 3.30 GB/s $6.80 per Hour\nRA3 with Redshift Managed Storage* \n\n\n\n\nra3.xlplus 4 32 GiB 32TB RMS 0.65 GB/s $1.086 per Hour\nra3.4xlarge 12 96 GiB 128TB RMS 2.00 GB/s $3.26 per Hour\nra3.16xlarge 48 384 GiB 128TB RMS 8.00 GB/s $13.04 per Hour"},{"comment_id":"482231","content":"ans is A","timestamp":"1637376780.0","poster":"aws2019","upvote_count":"2"},{"content":"This question has mentioned \"apply complex calculations that require I/O intensive resources\" which I think is a clue for DC node. Currently there are Storage node and need this node type be updated to DC, Classic re-size should happen since Elastic resize is ideal if number of nodes are changing. Therefore, I think answer is B","poster":"mickies9","timestamp":"1636215840.0","upvote_count":"1","comment_id":"420997"},{"content":"Ans A\nThis is a very bad question as it focuses on a property that gets updated recently. C and D are immediately out because DS nodes use HDD which does not have good I/O. Between classic and elastic resizing is difficult. There had been an update just a few days before the Data Analytics Certification launch, making elastic resizing the go-to solution for all resizing. It is better stick with the update as it happens before the launch, although there is no guarantee.","comment_id":"392154","upvote_count":"4","timestamp":"1636198380.0","poster":"Shraddha"},{"upvote_count":"2","poster":"Donell","content":"Answer is A.\nIt is mentioned in question that \"complex calculations that require I/O intensive resources\". Dense Compute Nodes uses SSD whereas Dense Storage uses HDD.\nHence Dense Compute nodes are suitable in this scenario,also elastic resize should be used whenever possible as suggested by AWS.","timestamp":"1636150380.0","comment_id":"386379"},{"upvote_count":"4","poster":"Dantehilary","comment_id":"373063","content":"its A. elastice resize with dense compute trust me. saw this question in Jon Bonso's prac questions.\n\nDC2 (dense compute) nodes allow you to have compute-intensive data warehouses with local SSD storage included. You choose the number of nodes you need based on data size and performance requirements. DC2 nodes store your data locally for high performance, and as the data size grows, you can add more compute nodes to increase the storage capacity of the cluster. DC2 nodes allow only up to 2.56TB storage per node but with a very high I/O performance of 7.50 GB/s.\n\nTherefore, the correct answer for this scenario is: Modify the cluster to use dense compute nodes and scale it using elastic resize to increase performance.","comments":[{"timestamp":"1655271420.0","content":"For customers with less than 500GB of data in their data warehouses, Dense Compute nodes are the most cost-effective and highest performance option. Above 500GB, customers whose primary focus is performance can continue with Dense Compute nodes up to hundreds of terabytes, giving them the highest ratio of CPU, Memory and I/O to storage. If performance isn’t as critical for a customer’s use case, or if customers want to prioritize reducing costs further, they can use the larger Dense Storage nodes and scale up to a petabyte or more of compressed user data for under $1,000/TB/Year.\nhttps://aws.amazon.com/about-aws/whats-new/2014/01/23/amazon-redshift-ssd-node-type/","comment_id":"616546","poster":"Booqq","upvote_count":"1"}],"timestamp":"1636081440.0"},{"upvote_count":"3","timestamp":"1636010940.0","comments":[{"comment_id":"573046","content":"To resize your cluster, use one of the following approaches:\n\nElastic resize – Use it to change the node type, number of nodes, or both. Elastic resize works quickly by changing or adding nodes to your existing cluster. If you change only the number of nodes, queries are temporarily paused and connections are held open, if possible. Typically, elastic resize takes 10–15 minutes. During the resize operation, the cluster is read-only.\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html","timestamp":"1647963240.0","poster":"CHRIS12722222","upvote_count":"1"}],"poster":"afantict","content":"Why not B?\n\"require I/O intensive resources\" maybe means DC.\nNow using DS, So we want change DS to DC, we must use classic resize.","comment_id":"353284"},{"upvote_count":"1","comment_id":"297327","content":"C\nFor datasets under 1 TB (compressed), we recommend DC2 node types for the best performance at the lowest price. If you expect your data to grow, we recommend using RA3 nodes\n\nas the data is huge we need DS nodes which lead us to elastic resize because we still with the same node type\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\nhttps://hevodata.com/learn/redshift-node-types/","timestamp":"1635872880.0","poster":"sayed"},{"upvote_count":"1","poster":"lostsoul07","comment_id":"279040","content":"A is the right answer","timestamp":"1635866760.0"},{"upvote_count":"1","content":"C is the right answer","poster":"lostsoul07","comment_id":"274292","timestamp":"1635785040.0"},{"comments":[{"content":"This is right, but here they didnt asked for cost optimization....","timestamp":"1639025340.0","comment_id":"497331","poster":"sanpak","upvote_count":"1"}],"poster":"Henning","content":"Answer C\nWhen you consider the issue of between dense compute and storage nodes consider the following assumptions:\n-currently 3 dense storage nodes for 6TB and ~2200$/month cost\n\nadding the additional 4 TB we would need around a capacity of 10TB\n-dc2.large has a max node size of 32 and 5.1TB of compressed storage\n-dc2.8xlarge needs 4 nodes for 10.2TB of storage for ~17500$/month cost\nvs.\n-ds2.xlarge with 10TB of storage and 3700$/month cost.\n\nTo keep in line with the current budget I would go for the current dense storage solution.\nFor refenrence: https://aws.amazon.com/redshift/pricing/","comment_id":"273639","timestamp":"1635664380.0","upvote_count":"1"},{"content":"Answer : A \n\nRefer : https://www.examtopics.com/discussions/amazon/view/29115-exam-aws-certified-data-analytics-specialty-topic-1-question/","comment_id":"263596","upvote_count":"1","timestamp":"1635424680.0","poster":"Roontha"},{"comment_id":"225844","poster":"Rajkbasu","upvote_count":"3","timestamp":"1635399180.0","content":"Option C\nAWS recommend elastic resize. So B & D removed\nDense compute for faster data processing which is not a requirement here. A removed."},{"comment_id":"219886","upvote_count":"1","poster":"beedle","content":"it should be A. The company already has 3 DS nodes. We definetly need comute nodes and a cluster resize (best practise is to use elastic as per aws docs)","timestamp":"1635363600.0"},{"timestamp":"1635293040.0","poster":"BillyC","comment_id":"216753","upvote_count":"2","content":"B is correct for me"},{"content":"Both A and B are correct as of now, but if the question was written before Apr 6, 2020 then the answer would be B. Before this date only classic resizing could change the node type.","poster":"jove","comment_id":"207456","upvote_count":"2","comments":[{"content":"Correct! Now both A and B are correct . Also, think about RA3 now; Now you may see one option with RA3 where you can choose more resource or more storage :)","comment_id":"208142","timestamp":"1634950740.0","poster":"Prodip","upvote_count":"1"}],"timestamp":"1634641680.0"},{"content":"For complex calculations, dense compute is needed. And 4TB with dc2.8xlarge node is achievable.As it supports a range of 2-128 nodes and so the total capacity supported is 326TB.\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\n\nBut I am confused whether it needs to be an elastic resize or classic?\nAs per AWS documentation elastic resize is doable for node type changes. I'd like to go with A. \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html","comment_id":"179796","poster":"aceit","timestamp":"1634519220.0","upvote_count":"2"},{"comment_id":"178819","upvote_count":"2","content":"It is A, pls read these documents https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html#rs-resize-tutorial","timestamp":"1633461660.0","comments":[{"timestamp":"1633749300.0","comment_id":"179146","comments":[{"timestamp":"1634141760.0","poster":"KoMo","upvote_count":"1","comments":[{"poster":"KoMo","upvote_count":"2","content":"you can find the announcement here https://aws.amazon.com/about-aws/whats-new/2020/04/amazon-redshift-now-supports-changing-node-types-within-minutes-with-elastic-resize/","comment_id":"179433","timestamp":"1634484240.0"}],"comment_id":"179431","content":"seems there are new updates .. please check the above link\n\nElastic resize – Use elastic resize to change the node type, number of nodes, or both.\nClassic resize – Use classic resize to change the node type, number of nodes, or both."}],"content":"you can't elastic resize a Redshift cluster changing the type of nodes. You can only do this with a standard resize. \nElastic resize: To quickly add or remove nodes from an existing cluster, use elastic resize. Elastic resize operations usually take a few minutes to complete. The cluster will be unavailable during that time\nClassic resize: To change the node type, the number of nodes, or both, use classic resize. A classic resize copies tables to a new cluster. The source cluster will be in read-only mode until the resize operation finishes.","upvote_count":"1","poster":"Phoenyx89"}],"poster":"abhineet"},{"comments":[{"timestamp":"1633736040.0","content":"it also talks about complex queries and IO","upvote_count":"1","poster":"abhineet","comments":[{"timestamp":"1634507040.0","poster":"Paitan","upvote_count":"1","comment_id":"179724","content":"I think this is the most confusing question in this forum. In discussions all 4 options have come up as potential answers. Not sure what will I do if this comes up in the actual exam :-)"}],"comment_id":"178821"}],"poster":"Paitan","content":"It talks about 4 TB additional data. So I will go with option C. Elastic resize with dense storage.","comment_id":"175536","upvote_count":"1","timestamp":"1632806640.0"},{"timestamp":"1632275220.0","poster":"awssp12345","comment_id":"164817","upvote_count":"2","content":"I think the answer is C - because as per the doc https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-node-type-info \nFor datasets under 1 TB (compressed), we recommend DC2 node types for the best performance at the lowest price. If you expect your data to grow, we recommend using RA3 nodes so you can size compute and storage independently to achieve improved price and performance.\nWhich means we will have to throw in DS nodes which can be achieved by elastic resize."},{"timestamp":"1632255120.0","poster":"zeronine","content":"My answer is D - it has to be dense storage as the existing ones are also using the same type","upvote_count":"1","comments":[{"upvote_count":"3","poster":"skar21","timestamp":"1632269880.0","comment_id":"162542","content":"It says \"apply complex calculations that require I/O intensive resources\". So compute node uses SDD to support high throughput. So it could be \"B\". any thoughts?"}],"comment_id":"162243"},{"timestamp":"1632196260.0","upvote_count":"1","poster":"ramozo","comment_id":"162099","content":"Regarding the video, the answer is in minute 15:00."},{"content":"It is B. You need to change to compute nodes for intensive I/O and this can only be done in Classic resize. See https://pages.awscloud.com/Best-Practices-for-Scaling-Amazon-Redshift_1111-ABD_OD.html?sc_icampaign=Event_pac_Q4-2018_sitemerch_ribbon_OTT_1111-ABD&sc_ichannel=ha&sc_icontent=awssm-1563&sc_ioutcome=Product_Adoption_Campaigns&sc_iplace=ribbon&trk=ha_a131L000005PUJiQAO~ha_awssm-1563&trkCampaign=November_1111-ABD","timestamp":"1632097980.0","poster":"ramozo","upvote_count":"5","comment_id":"162087"}],"answer_images":[],"choices":{"D":"Resize the cluster using classic resize with dense storage nodes.","C":"Resize the cluster using elastic resize with dense storage nodes.","B":"Resize the cluster using classic resize with dense compute nodes.","A":"Resize the cluster using elastic resize with dense compute nodes."},"topic":"1","answer_description":"","isMC":true,"exam_id":20,"unix_timestamp":1597912320},{"id":"WJxLiFLztMSdsSq9pz4I","answer_ET":"C","answers_community":["C (91%)","9%"],"question_id":93,"exam_id":20,"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/28811-exam-aws-certified-data-analytics-specialty-topic-1-question/","topic":"1","choices":{"B":"Check the security group of the EMR clusters regularly to ensure it does not allow inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0.","C":"Enable the block public access setting for Amazon EMR at the account level before any EMR cluster is created.","D":"Use AWS WAF to block public internet access to the EMR clusters across the board.","A":"Create an EMR security configuration and ensure the security configuration is associated with the EMR clusters when they are created."},"answer_images":[],"question_images":[],"question_text":"A company stores its sales and marketing data that includes personally identifiable information (PII) in Amazon S3. The company allows its analysts to launch their own Amazon EMR cluster and run analytics reports with the data. To meet compliance requirements, the company must ensure the data is not publicly accessible throughout this process. A data engineer has secured Amazon S3 but must ensure the individual EMR clusters created by the analysts are not exposed to the public internet.\nWhich solution should the data engineer to meet this compliance requirement with LEAST amount of effort?","isMC":true,"unix_timestamp":1597652160,"timestamp":"2020-08-17 10:16:00","discussion":[{"content":"C??\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html","timestamp":"1632111240.0","comments":[{"timestamp":"1632321660.0","content":"Agreed","comment_id":"164824","poster":"awssp12345","upvote_count":"1"},{"comment_id":"175387","upvote_count":"1","comments":[{"content":"my bad. I read again and is c","timestamp":"1632566880.0","comment_id":"175388","upvote_count":"2","poster":"bigollo"}],"poster":"bigollo","timestamp":"1632405360.0","content":"the cluster is already created, and you can not recreate it because\n is much effort"}],"upvote_count":"25","comment_id":"159755","poster":"Priyanka_01"},{"upvote_count":"1","poster":"kondi2309","comment_id":"1147086","content":"Selected Answer: C\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html","timestamp":"1707642360.0"},{"comment_id":"1119638","upvote_count":"2","timestamp":"1704971220.0","content":"--- workflow ---\n\ndata with PII -> s3\n\nanalyst 1 -> EMR 1\nanalyst 2 -> EMR 2\n...\nanalyst n -> EMR n\n\n(In fact, what company allows its analysts to create an individual EMR for each person?! o.O)\n\n--- objective ---\n\nensure the EMR is not accessible by public internet\n\n--- way to make this with the least effort and least cost ---\n\nblock all account emr public access\n\n--- have another way to make this? ---\n\nyes, if a data analyst specialist designs a AMI for all EMR clusters and schedules a daily job to create an EMR for all analysts... buuuuuuuuuut, have a lot of effort rsrsrs","poster":"GCPereira"},{"upvote_count":"1","comment_id":"1067136","poster":"monkeydba","content":"https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-emr-introduces-block-public-access-configuration-to-secure-emr-clusters-from-unintentional-network-exposure/","timestamp":"1699610220.0"},{"comment_id":"886315","timestamp":"1682948400.0","upvote_count":"4","content":"C: I passed the test","poster":"pk349"},{"comment_id":"802438","content":"Answer is C","timestamp":"1675883520.0","poster":"Ashoks","upvote_count":"1"},{"poster":"cloudlearnerhere","content":"Selected Answer: C\nCorrect answer is C as the EMR clusters can be configured with a block public access setting which is applied to all regions within an account.\n\nAmazon EMR block public access prevents a cluster in a public subnet from launching when any security group associated with the cluster has a rule that allows inbound traffic from IPv4 0.0.0.0/0 or IPv6 ::/0 (public access) on a port, unless the port has been specified as an exception. Port 22 is an exception by default. You can configure exceptions to allow public access on a port or range of ports. Block public access does not take effect in private subnets.\n\n\nA is wrong as security configurations can be used to configure data encryption, Kerberos authentication, and Amazon S3 authorization for EMRFS.\n\n\nB is wrong Although this approach is possible, it entails a management overhead of regularly updating the security groups of the EMR cluster.\n\nOption D is wrong as WAF does not work with EMR clusters.","timestamp":"1667659560.0","upvote_count":"4","comment_id":"711825"},{"content":"Selected Answer: B\nthe company must ensure the data is not publicly accessible throughout this process. How to ensure SG not be modified during the whole process if you choose C?","poster":"pgf909","comment_id":"707525","upvote_count":"1","timestamp":"1667082600.0"},{"upvote_count":"1","content":"B ---- as Block public access does not block IAM principals with appropriate permissions from updating security group configurations to allow public access on running clusters.... https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html\nI would suggest customer to use config to trigger auto mitigation if any port is opened to public access.","poster":"pgf909","comment_id":"707520","timestamp":"1667082360.0"},{"timestamp":"1664082840.0","upvote_count":"1","comment_id":"678455","content":"Selected Answer: C\n\"with LEAST amount of effort\" - this is the key statement here.","poster":"Arka_01"},{"content":"Selected Answer: C\nSelected Answer: C","timestamp":"1659815640.0","poster":"rocky48","comment_id":"643497","upvote_count":"1"},{"poster":"Ramshizzle","upvote_count":"2","timestamp":"1656249300.0","content":"Selected Answer: C\nB is obviously wrong. AWS Exams would never allow a compliance solution to manually check if the settings are correct every now and then. \nC is better","comment_id":"622564"},{"comment_id":"604850","upvote_count":"1","poster":"Bik000","timestamp":"1653132300.0","content":"Selected Answer: C\nMy Answer is C"},{"timestamp":"1648381200.0","poster":"CHRIS12722222","content":"Option C does not make sense since this is already enabled by default. \nOption B is better. I think the best solution is to use a custom config rule with SSM remediation\n\nhttps://asecure.cloud/a/cfgrule_c_emr_security_groups_restricted/","comment_id":"576143","comments":[{"content":"Also it does not prevent authorised persons from overriding the default EMR block public access settings when the cluster is running.\n\"Block public access is only applicable during cluster creation. Block public access does not block IAM principals with appropriate permissions from updating security group configurations to allow public access on running clusters.\"\nRef: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html","timestamp":"1648381320.0","poster":"CHRIS12722222","comment_id":"576146","upvote_count":"1"}],"upvote_count":"1"},{"comment_id":"392162","upvote_count":"3","timestamp":"1636030140.0","poster":"Shraddha","content":"Ans C\nThis is a textbook question.\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html"},{"content":"Ans C\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html","comment_id":"384036","upvote_count":"4","timestamp":"1636015560.0","poster":"Shraddha"},{"timestamp":"1634914620.0","content":"I think C is Default. The question is what we need to do to ensure that, and we have to make sure the ports are not open as public.. Do you think the correct answer is B?","comment_id":"348992","upvote_count":"4","poster":"AjithkumarSL"},{"comment_id":"274293","poster":"lostsoul07","timestamp":"1634637000.0","upvote_count":"1","content":"C is the right answer"},{"poster":"BillyC","comment_id":"216751","upvote_count":"1","timestamp":"1633980300.0","content":"C is right answer"},{"timestamp":"1633946700.0","comment_id":"175541","poster":"Paitan","content":"Option C.\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-block-public-access.html","upvote_count":"2"}],"answer_description":""},{"id":"CCpWp6VSQ0F88fWKnLOQ","topic":"1","answers_community":["D (100%)"],"answer_images":[],"question_id":94,"answer_description":"","timestamp":"2020-08-19 20:24:00","question_text":"A financial company uses Amazon S3 as its data lake and has set up a data warehouse using a multi-node Amazon Redshift cluster. The data files in the data lake are organized in folders based on the data source of each data file. All the data files are loaded to one table in the Amazon Redshift cluster using a separate\nCOPY command for each data file location. With this approach, loading all the data files into Amazon Redshift takes a long time to complete. Users want a faster solution with little or no increase in cost while maintaining the segregation of the data files in the S3 data lake.\nWhich solution meets these requirements?","question_images":[],"unix_timestamp":1597861440,"isMC":true,"answer":"D","url":"https://www.examtopics.com/discussions/amazon/view/29070-exam-aws-certified-data-analytics-specialty-topic-1-question/","choices":{"A":"Use Amazon EMR to copy all the data files into one folder and issue a COPY command to load the data into Amazon Redshift.","B":"Load all the data files in parallel to Amazon Aurora, and run an AWS Glue job to load the data into Amazon Redshift.","D":"Create a manifest file that contains the data file locations and issue a COPY command to load the data into Amazon Redshift.","C":"Use an AWS Glue job to copy all the data files into one folder and issue a COPY command to load the data into Amazon Redshift."},"exam_id":20,"answer_ET":"D","discussion":[{"poster":"carol1522","comment_id":"161734","upvote_count":"25","content":"D? https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html","timestamp":"1632272520.0"},{"timestamp":"1667659740.0","content":"Selected Answer: D\nCorrect answer is D as a manifest file can be used to load the data. Also, its recommended to have a single COPY command instead of multiple concurrent COPY commands for performance.\n\nUse the COPY command to load a table in parallel from data files on Amazon S3. You can specify the files to be loaded by using an Amazon S3 object prefix or by using a manifest file.\n\nAmazon Redshift can automatically load in parallel from multiple compressed data files.\n\nHowever, if you use multiple concurrent COPY commands to load one table from multiple files, Amazon Redshift is forced to perform a serialized load. This type of load is much slower and requires a VACUUM process at the end if the table has a sort column defined.\n\nOptions A, B & C are wrong as they add unnecessary work and cost.","comment_id":"711826","poster":"cloudlearnerhere","upvote_count":"9","comments":[{"content":"Can you share a link that gives more insight into \"However, if you use multiple concurrent COPY commands to load one table from multiple files, Amazon Redshift is forced to perform a serialized load. This type of load is much slower and requires a VACUUM process at the end if the table has a sort column defined\"?","upvote_count":"1","comment_id":"896369","timestamp":"1683944220.0","poster":"crs1234"}]},{"upvote_count":"1","content":"Selected Answer: D\nAns D, single COPY command for performance and manifest file for loading data","comment_id":"1148969","timestamp":"1707810180.0","poster":"kondi2309"},{"upvote_count":"1","content":"datalake s3 -> dw redshift\n\nproblems copying files using copy-by-prefix\n\nneed a solution without increasing costs\n\nA) emr is expansive\nb) aurora needs effort configuration and glue needs development effort\nc) glue job needs a development effort and copying all files to the same prefix will create a problem... which file goes to which table?\nd) manifest file is the best option because you can specify exactly the prefix/key to your copy command","comment_id":"1119653","timestamp":"1704973440.0","poster":"GCPereira"},{"upvote_count":"1","timestamp":"1688065680.0","content":"D: manifest file is valid option","poster":"tsk9921","comment_id":"938483"},{"timestamp":"1682948520.0","content":"D: I passed the test","poster":"pk349","comment_id":"886317","upvote_count":"2"},{"comment_id":"678457","content":"Selected Answer: D\nYou can use a single copy command with manifest file, containing different S3 locations. This will speed up the COPY process.","upvote_count":"1","poster":"Arka_01","timestamp":"1664082960.0"},{"upvote_count":"1","comment_id":"638486","poster":"rocky48","content":"Selected Answer: D\nSelected Answer: D","timestamp":"1658986440.0"},{"timestamp":"1653131580.0","comment_id":"604841","upvote_count":"1","content":"Selected Answer: D\nMy Answer is D","poster":"Bik000"},{"poster":"AWSRanger","upvote_count":"2","content":"Selected Answer: D\nD is correct","timestamp":"1650648780.0","comment_id":"590163"},{"comment_id":"384050","timestamp":"1636027860.0","upvote_count":"6","content":"Ans D\nA = wrong, no segregation, increased cost. B = wrong, no segregation, unnecessary work, increased cost. C = wrong, no segregation, increased cost. This is a question on how COPY command work. In general you should use only one COPY command because Redshift will load data in parallel, if you use many COPYs Redshift will have to load data in sequential manner.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_COPY_command_examples.html#copy-command-examples-manifest","poster":"Shraddha"},{"timestamp":"1635951660.0","comment_id":"274294","upvote_count":"2","poster":"lostsoul07","content":"D is the right answer"},{"upvote_count":"2","comment_id":"216750","content":"My answer is D","timestamp":"1634950560.0","poster":"BillyC"},{"poster":"sanjaym","upvote_count":"2","timestamp":"1634901360.0","content":"D is right answer.","comment_id":"204917"},{"comment_id":"191579","content":"From the link:https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html\n\"You can use a manifest to ensure that the COPY command loads all of the required files, and only the required files, for a data load\"\nSo answer is D","upvote_count":"7","timestamp":"1634230260.0","poster":"syu31svc"},{"upvote_count":"3","poster":"Paitan","comment_id":"175542","content":"Using manifest file is the right choice. So option D.","timestamp":"1633446480.0"},{"content":"Yes D is the right answer","timestamp":"1633295880.0","poster":"Saaho","comment_id":"162344","upvote_count":"4"},{"upvote_count":"8","comment_id":"162101","poster":"ramozo","content":"Agree, D.","timestamp":"1632592560.0"}]},{"id":"pcZQS1dNLgb5hivFpyD7","unix_timestamp":1599532920,"answer_images":[],"timestamp":"2020-09-08 04:42:00","topic":"1","question_text":"A company's marketing team has asked for help in identifying a high performing long-term storage service for their data based on the following requirements:\n✑ The data size is approximately 32 TB uncompressed.\n✑ There is a low volume of single-row inserts each day.\n✑ There is a high volume of aggregation queries each day.\n✑ Multiple complex joins are performed.\n✑ The queries typically involve a small subset of the columns in a table.\nWhich storage service will provide the MOST performant solution?","url":"https://www.examtopics.com/discussions/amazon/view/30835-exam-aws-certified-data-analytics-specialty-topic-1-question/","isMC":true,"discussion":[{"comment_id":"175543","timestamp":"1632162300.0","upvote_count":"28","content":"Redhift for sure.","poster":"Paitan"},{"content":"The simplest question in the exam.","comment_id":"207468","timestamp":"1632468720.0","upvote_count":"9","poster":"jove"},{"content":"Selected Answer: B\nAmazon Redshift meets all the requirements.","timestamp":"1707810300.0","upvote_count":"1","poster":"kondi2309","comment_id":"1148971"},{"upvote_count":"1","timestamp":"1682948580.0","comment_id":"886319","content":"B: I passed the test","poster":"pk349"},{"comment_id":"711827","timestamp":"1667659800.0","content":"Selected Answer: B\nCorrect answer is B as Redshift as it can be used for OLAP processing and meets all the requirements.\n\nAmazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes using AWS-designed hardware and machine learning to deliver the best price-performance at any scale.\nOption A is wrong as Amazon Aurora MySQL is ideal for OLTP solutions and not OLAP.\n\nOption C s wrong as Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications.\n\nOption D is wrong as Amazon Elasticsearch would not allow complex queries.","upvote_count":"5","poster":"cloudlearnerhere"},{"poster":"Arka_01","upvote_count":"1","timestamp":"1664083020.0","comment_id":"678458","content":"Selected Answer: B\nThe scenario here is indicating for a columnar data storage. So the answer will be Amazon Redshift."},{"timestamp":"1659818700.0","upvote_count":"1","content":"Selected Answer: B\nB is the right answer","comment_id":"643529","poster":"rocky48"},{"poster":"AWSRanger","upvote_count":"1","timestamp":"1650648840.0","content":"Selected Answer: B\nB is correct","comment_id":"590165"},{"comment_id":"504647","upvote_count":"1","poster":"sanpak","content":"why not D ? elastic search, search in subset of column works better in ES...","timestamp":"1639891080.0"},{"timestamp":"1635974100.0","comment_id":"442332","content":"Ans B","upvote_count":"1","poster":"Billhardy"},{"comment_id":"384052","content":"Ans B\nA = wrong, Aurora for OLTP not OLAP. C = wrong, graph database not relevant. D = wrong, complex joins in ES are expensive.","upvote_count":"4","timestamp":"1634601840.0","poster":"Shraddha"},{"comment_id":"274317","timestamp":"1633432620.0","upvote_count":"3","content":"B is the right answer","poster":"lostsoul07"},{"poster":"BillyC","content":"My answer is B","upvote_count":"3","timestamp":"1632963300.0","comment_id":"216749"}],"answer":"B","choices":{"A":"Amazon Aurora MySQL","C":"Amazon Neptune","B":"Amazon Redshift","D":"Amazon Elasticsearch"},"answers_community":["B (100%)"],"question_images":[],"answer_description":"","answer_ET":"B","exam_id":20,"question_id":95}],"exam":{"provider":"Amazon","lastUpdated":"11 Apr 2025","id":20,"isMCOnly":true,"name":"AWS Certified Data Analytics - Specialty","isBeta":false,"numberOfQuestions":164,"isImplemented":true},"currentPage":19},"__N_SSP":true}