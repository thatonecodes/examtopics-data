{"pageProps":{"questions":[{"id":"GOkhYo8JL8zfHifyWfdX","topic":"1","isMC":true,"choices":{"C":"Amazon Route 53","D":"Amazon S3 Transfer Acceleration","A":"Amazon CloudFront","B":"AWS Global Accelerator"},"discussion":[{"poster":"Nigma","content":"A is right\n\nYou can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin\n\nGlobal Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses","upvote_count":"45","timestamp":"1668519780.0","comment_id":"718763"},{"poster":"mr123dd","upvote_count":"12","comment_id":"1119033","timestamp":"1704921120.0","content":"Selected Answer: A\nwebsite = http = cloudfront, if it is UDP, then global accelerator"},{"timestamp":"1738866600.0","upvote_count":"1","content":"Selected Answer: A\nA because CloudFront can be used for video streaming. NOT GA because GA is good for services with lower level network protocols - and their speed ups.","poster":"Dharmarajan","comment_id":"1352572"},{"upvote_count":"2","comment_id":"1298935","timestamp":"1729109520.0","content":"Selected Answer: A\nAns A - CloudFront delivers video on demand or live streaming video using any HTTP origin","poster":"PaulGa"},{"poster":"dragongoseki","timestamp":"1718979120.0","upvote_count":"1","content":"Selected Answer: A\nA is right answer.","comment_id":"1234456"},{"content":"Selected Answer: A\nA for sure","timestamp":"1718928480.0","upvote_count":"1","poster":"ChymKuBoy","comment_id":"1234128"},{"timestamp":"1718349780.0","upvote_count":"1","poster":"trinh_le","comment_id":"1230294","content":"AD is correct. link here https://docs.aws.amazon.com/aws-backup/latest/devguide/creating-a-backup-plan.html"},{"poster":"lofzee","timestamp":"1716889620.0","content":"Selected Answer: A\nA.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html","comment_id":"1220119","upvote_count":"2"},{"timestamp":"1714837260.0","content":"Selected Answer: A\nuse CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin","comment_id":"1206552","poster":"ManikRoy","upvote_count":"2"},{"upvote_count":"3","poster":"lanceshen","timestamp":"1710204840.0","content":"Selected Answer: B\nOption A (Amazon CloudFront) is a content delivery network (CDN) service that can improve the performance of on-demand streaming by caching and delivering content from edge locations. While it can accelerate on-demand streaming, it may not provide the same level of optimization for real-time streaming as AWS Global Accelerator.","comment_id":"1171377"},{"timestamp":"1706685780.0","comment_id":"1136491","poster":"MrPCarrot","upvote_count":"3","content":"A is pperfect https://d1.awsstatic.com/whitepapers/amazon-cloudfront-for-media.pdf"},{"timestamp":"1704057000.0","poster":"awsgeek75","upvote_count":"2","comment_id":"1110816","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html\n\nCloudFront solves the problem of streaming over Amazon CDN on global scale. \n\n\"B\" AWS Global Accelerator won't be suitable for streaming from web server as it does not provide edge caching like CDN. Global Accelerator only points the user to nearest functioning node which is helpful for real-time streaming but not best for on-demand."},{"poster":"viru","content":"Selected Answer: A\nhttps://aws.amazon.com/cloudfront/streaming/","timestamp":"1702967520.0","comment_id":"1100378","upvote_count":"3"},{"poster":"yayaayzo","content":"A IS THE RIGHT ANS\n. Amazon CloudFront is a content delivery network (CDN) service offered by AWS. It is designed to deliver data, including videos and other media files, with low latency and high transfer speeds. This is a suitable option for optimizing website performance, especially for streaming content globally.","timestamp":"1702223760.0","upvote_count":"2","comment_id":"1092618"},{"poster":"MiniYang","comments":[{"timestamp":"1705433640.0","poster":"awsgeek75","upvote_count":"1","comment_id":"1124464","content":"https://aws.amazon.com/global-accelerator/\n\"AWS Global Accelerator is a networking service that helps you improve the availability, performance, and security of your public applications. Global Accelerator provides two global static public IPs that act as a fixed entry point to your application endpoints, such as Application Load Balancers, Network Load Balancers, Amazon Elastic Compute Cloud (EC2) instances, and elastic IPs.\"\nIt is Anycast which connect user to closest resource on your server like ALB etc or ergional services.\nCloudFront is CDN and pushes your content to edge locations near the user. This solves all issues of latency and performance"}],"upvote_count":"1","comment_id":"1083167","content":"Selected Answer: B\nAlthough CloudFront is a content delivery network (CDN) that can provide low-latency and high-performance content delivery, its performance for real-time streaming and on-demand streaming may not be as professional as AWS Global Accelerator","timestamp":"1701240120.0"},{"upvote_count":"3","comment_id":"1081482","content":"You can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin\n\nGlobal Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses","timestamp":"1701085440.0","poster":"Ruffyit"},{"comment_id":"1048748","poster":"mhka1988","content":"Selected Answer: A\nCloudFront offers several options for streaming your media to global viewers—both pre-recorded files and live events.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/IntroductionUseCases.html#IntroductionUseCasesStreaming\n\nFor video on demand (VOD) streaming, you can use CloudFront to stream in common formats such as MPEG DASH, Apple HLS, Microsoft Smooth Streaming, and CMAF, to any device.\n\nFor broadcasting a live stream, you can cache media fragments at the edge, so that multiple requests for the manifest file that delivers the fragments in the right order can be combined, to reduce the load on your origin server.","upvote_count":"4","timestamp":"1697803560.0"},{"comments":[{"content":"CloudFront for \"local audience\"???","comments":[{"poster":"xkosarf","timestamp":"1726932960.0","comment_id":"1287389","upvote_count":"1","content":"the question says Global audience"}],"comment_id":"1105979","poster":"pentium75","upvote_count":"1","timestamp":"1703593560.0"}],"timestamp":"1697214000.0","upvote_count":"3","content":"Selected Answer: B\nPlease stop posting answers from ChatGPT. \n\"The event is expected to attract a global online audience.\" \n\n Global Accelerator is a service that accelerates traffic to Google Cloud services from users around the world. If you're looking to stream audio content to a global audience, Global Accelerator may be more suitable due to its ability to route traffic through the nearest edge locations and reduce latency. However, if you're looking to stream audio content from a single source to a local audience, CloudFront may be a better option.","comment_id":"1042814","poster":"OlehKom"},{"content":"Selected Answer: A\nWas between A and B here but this link convinced me that A would be correct. \nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/IntroductionUseCases.html#IntroductionUseCasesStreaming","comments":[{"content":"thanks","poster":"Itsmetanmay","timestamp":"1713965700.0","comment_id":"1201407","upvote_count":"1"}],"timestamp":"1697136360.0","poster":"awashenko","comment_id":"1042023","upvote_count":"4"},{"comment_id":"1002100","poster":"TariqKipkemei","content":"Selected Answer: A\nAmazon CloudFront is a content delivery network (CDN) service that helps you distribute your static and dynamic content quickly and reliably with high speed.","upvote_count":"2","timestamp":"1694146800.0"},{"poster":"Chiquitabandita","upvote_count":"3","content":"chatgpt went with cloudfront on this question, so answer A","timestamp":"1693854060.0","comment_id":"998792"},{"timestamp":"1693389480.0","upvote_count":"3","poster":"coolkidsclubvip","comment_id":"993929","content":"Selected Answer: B\nhttps://aws.amazon.com/cn/blogs/networking-and-content-delivery/how-flowplayer-improved-live-video-ingest-with-aws-global-accelerator/"},{"poster":"Guru4Cloud","timestamp":"1692224460.0","content":"The reasons are:\n\nCloudFront is a content delivery network (CDN) that caches content at edge locations around the world.\nCaching the video content globally brings it closer to viewers, reducing latency.\nThis improves performance for both live streaming and on-demand playback for the global audience.\nRoute 53 provides DNS resolution but does not cache content locally.\nGlobal Accelerator improves TCP traffic routing performance but is not a caching CDN.\nS3 Transfer Acceleration optimizes uploads to S3 over long distances but does not help with content delivery.","comment_id":"983074","upvote_count":"3"},{"comments":[{"timestamp":"1698406380.0","content":"a video streaming is udp trafic","comment_id":"1055530","upvote_count":"1","poster":"fageroff"}],"timestamp":"1690158060.0","upvote_count":"2","content":"Selected Answer: B\nGlobal Accelerator good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP","poster":"Chan1010","comment_id":"960971"},{"timestamp":"1687606200.0","upvote_count":"4","content":"Amazon CloudFront is a content delivery network (CDN) that can deliver both real-time and on-demand streaming. It caches content at edge locations worldwide, providing low-latency delivery to a global audience.\n\nB. AWS Global Accelerator: Global Accelerator is more suitable for non-HTTP use cases or when static IP addresses are required.\nC. Amazon Route 53: Route 53 is a DNS service and not designed specifically for streaming video.\nD. Amazon S3 Transfer Acceleration: S3 Transfer Acceleration improves upload speeds to Amazon S3 but does not directly enhance streaming performance.","poster":"cookieMr","comment_id":"932494"},{"content":"Selected Answer: A\nServe video on demand or live streaming video\nCloudFront offers several options for streaming your media to global viewers—both pre-recorded files and live events.\n\nFor video on demand (VOD) streaming, you can use CloudFront to stream in common formats such as MPEG DASH, Apple HLS, Microsoft Smooth Streaming, and CMAF, to any device.\n\nFor broadcasting a live stream, you can cache media fragments at the edge, so that multiple requests for the manifest file that delivers the fragments in the right order can be combined, to reduce the load on your origin server.","upvote_count":"2","timestamp":"1685162280.0","poster":"Jeeva28","comment_id":"907753"},{"poster":"Kumaran1508","content":"Selected Answer: B\nI vote for B. Global Accelerator.\nCloudFront Video on Demand is specifically designed for delivering on-demand video content, meaning pre-recorded videos that can be streamed or downloaded. It is not suitable for streaming real-time videos or live video broadcasts.\nGlobal Accelerator help in reducing network hops between the user and AWS making real-time streams smoother.","timestamp":"1685094540.0","upvote_count":"3","comment_id":"907265","comments":[{"upvote_count":"2","content":"\"CloudFront offers several options for streaming your media to global viewers—both pre-recorded files and live events.\"","poster":"pentium75","timestamp":"1703593740.0","comment_id":"1105982"}]},{"upvote_count":"1","content":"Selected Answer: B\nTo get the benefit of CloudFront video needs to be cached, so requests should be frequent. On demand video - I vote for B","comments":[],"poster":"eugene_stalker","comment_id":"906834","timestamp":"1685040480.0"},{"timestamp":"1679518440.0","comments":[],"content":"How can Cloudfront help with real-time use case?","comment_id":"847519","poster":"warioverde","upvote_count":"2"},{"content":"Amazon CloudFront","comment_id":"775478","timestamp":"1673705100.0","upvote_count":"1","poster":"Mamiololo"},{"content":"Selected Answer: A\nCloudFront offers several options for streaming your media to global viewers—both pre-recorded files and live events. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/IntroductionUseCases.html#IntroductionUseCasesStreaming","timestamp":"1672838460.0","upvote_count":"3","poster":"aba2s","comment_id":"765679"},{"content":"Selected Answer: A\nA Cloudfront","timestamp":"1671311220.0","poster":"career360guru","comment_id":"748418","upvote_count":"1"},{"timestamp":"1671039420.0","content":"Selected Answer: A\nCloudfront is used for live streaming and video on-demand\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/IntroductionUseCases.html","comment_id":"745331","upvote_count":"3","poster":"Baba_Eni"},{"upvote_count":"2","content":"Selected Answer: A\nI thought the real-time streaming comes with rtsp protocol for which B is better.\nBut I realized now real-time streaming also has http way now (like HLS, etc.).\nSo the answer should be A.","poster":"leonnnn","timestamp":"1669638720.0","comment_id":"729081"},{"content":"A is correct","poster":"Wpcorgan","comment_id":"724450","upvote_count":"1","timestamp":"1669133520.0"},{"content":"Selected Answer: A\nCloudFront for sure","poster":"babaxoxo","upvote_count":"1","comment_id":"719468","timestamp":"1668586680.0"}],"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/87514-exam-aws-certified-solutions-architect-associate-saa-c03/","question_text":"A solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.\n\nWhich service will improve the performance of both the real-time and on-demand streaming?","answer_ET":"A","exam_id":31,"unix_timestamp":1668519780,"answer_description":"","question_images":[],"answers_community":["A (75%)","B (25%)"],"answer":"A","timestamp":"2022-11-15 14:43:00","question_id":86},{"id":"u0M4hSz45c776Sca5tBL","question_id":87,"isMC":true,"discussion":[{"comments":[{"poster":"debasishdtta","upvote_count":"2","comment_id":"1121430","timestamp":"1705136340.0","content":"Don't use API keys for authentication or authorization to control access to your APIs. If you have multiple APIs in a usage plan, a user with a valid API key for one API in that usage plan can access all APIs in that usage plan. Instead, to control access to your API, use an IAM role, a Lambda authorizer, or an Amazon Cognito user pool."},{"comment_id":"1105991","timestamp":"1703593920.0","content":"E \"An IAM role for EACH (!) user ATTEMPTING (!) to access the API\"? Hello no.","poster":"pentium75","upvote_count":"5"}],"timestamp":"1680933420.0","content":"Selected Answer: CE\nC) WAF has bot identification and remedial tools, so it's CORRECT.\n\nA) remember the question : \"...block requests from unauthorized users?\" -- an api key is involved in a authorization process. It's not the more secure process, but it's better than an totoally anonymous process. If you don't know the key, you can't authenticate. So the bots, at least the first days/weeks could not access the service (at the end they'll do, cos' the key will be spread informally). So it's CORRECT.\n\nB) Implement a logic in the Lambda to detect fraudulent ip's is almost impossible, cos' it's a dynamic and changing pattern that you cannot handle easily.\n\nD) creating a rol is not going to imply be more protected from unauth. request, because a rol is a \"principal\", it's not involved in the authorization process.","comment_id":"864446","poster":"jdr75","upvote_count":"10"},{"comment_id":"1042026","timestamp":"1697136600.0","content":"Selected Answer: AC\nAgree A and C\n\nI don't see how E is feasible as its a public API. How would you create an IAM role for each user?","poster":"awashenko","upvote_count":"6"},{"timestamp":"1738726920.0","comment_id":"1351690","upvote_count":"1","poster":"Mrigraj12","content":"Selected Answer: AC\nWrong choices: B-> Integrating logic in lambda is not feasable as you will have to feed data which will increase its size, execution time and therefore overall cost.\n\nD-> If it happens then the public will not be able to access it as it is private only clients inside VPC will be abe to access it.\n\nE-> You are just giving them access"},{"upvote_count":"2","poster":"PaulGa","content":"Selected Answer: AC\nAns A, C - \nA: using API keys and usage plans restricts access to your API to users who have the key, limiting fraudulent access.\nC: designed to fight bots","timestamp":"1729110060.0","comment_id":"1298937"},{"content":"Selected Answer: AC\nC) Everyone agrees on C\n\nB) Almost impossible cos how do you detect fraudulent IP address from a publicly accessible application\nD) It's a publicly accessible application, converting the API to a private one defeats the purpose\nE) IAM role for each user trying to access a publicly accessible API is impossible. It like creating an IAM for each user that tries to use google AUTH for their website\nA) By implementing API keys and usage plans, you can restrict access to your API to only those users who possess the key, helping to limit fraudulent access.","upvote_count":"2","comments":[{"timestamp":"1742777820.0","upvote_count":"1","content":"but i think both option A and E imply that the api access actually have a targeted user\nif A is possible, why cant E be an apporach?","comment_id":"1409256","poster":"jerryl"}],"timestamp":"1719146040.0","poster":"diddy99","comment_id":"1235820"},{"comment_id":"1234454","timestamp":"1718979120.0","upvote_count":"1","poster":"dragongoseki","content":"Selected Answer: AC\nAC is right answer."},{"upvote_count":"2","content":"Selected Answer: AC\nAC for sure","timestamp":"1718928720.0","poster":"ChymKuBoy","comment_id":"1234130"},{"timestamp":"1716574800.0","comment_id":"1217712","content":"Do the people voting E realize how insane that is? Creating a local IAM user in your account for every user that needs to access the API. No just... no.","poster":"hb0011","upvote_count":"3"},{"timestamp":"1713464820.0","poster":"EMPERBACH","upvote_count":"1","content":"Selected Answer: BC\nB. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses. -> you can think about CORS script write on Lambda to prevent fraudulent IP addresses.\nC. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out. -> No comment here as it can use to filter traffic","comment_id":"1198098"},{"comment_id":"1136494","timestamp":"1706686020.0","poster":"MrPCarrot","content":"C and D are the perfect answers","upvote_count":"2"},{"upvote_count":"1","content":"Don't use API keys for authentication or authorization to control access to your APIs. If you have multiple APIs in a usage plan, a user with a valid API key for one API in that usage plan can access all APIs in that usage plan. Instead, to control access to your API, use an IAM role, a Lambda authorizer, or an Amazon Cognito user pool.","timestamp":"1705136280.0","comment_id":"1121429","poster":"debasishdtta"},{"comment_id":"1110819","comments":[{"content":"It's a globally published API if you make it private how do other people access it ? A would be the better solution than D","upvote_count":"2","poster":"sidharthwader","comment_id":"1164700","timestamp":"1709461200.0"}],"upvote_count":"4","timestamp":"1704057780.0","poster":"awsgeek75","content":"Selected Answer: CD\nI'll throw a curveball over here. \"C\" is a given as WAF rules can target malicious usage. For example: https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/aws-waf-ip-reputation.html\n\n\"D\" Convert existing public API to a private API. This part is same as A. The additional bit over here is to change the DNS record to a new API endpoint which blocks the requests from unauthorised users also. The unauthorised users will not be redirected from public to private API endpoint. I am assuming that the public API endpoint will be used for authorisation and only authorised users will be redirected to private endpoint. This is more robust as the actual API (private endpoint) never gets hit with requests from unauthorised bots and WAF redirects it back to public URL.\n\nHappy to be corrected and challenged"},{"timestamp":"1702376340.0","upvote_count":"4","poster":"ale_brd_111","comment_id":"1094348","content":"Selected Answer: AC\nThe combination of using an API key and implementing an AWS WAF rule provides the most comprehensive and effective way to block requests from unauthorized users and protect the company's serverless application from botnet attacks."},{"timestamp":"1701240480.0","comment_id":"1083174","content":"Selected Answer: CE\nA. Create plans using API keys shared only with real users: While using API keys is a standard way to control access to APIs, using API keys alone may not completely prevent attacks from botnets. Malicious request.\n\nB. Incorporate logic in the Lambda function to ignore requests from fraudulent IP addresses: This may be a solution, but filtering that relies more on IP addresses may not be as flexible as using AWS WAF.\n\nD. Convert an existing public API to a private API. Update DNS records to redirect users to the new API endpoint: This approach makes the API private, but requires user redirects and may inconvenience existing users.","poster":"MiniYang","upvote_count":"1"},{"upvote_count":"2","content":"C) WAF has bot identification and remedial tools, so it's CORRECT.\n\nA) remember the question : \"...block requests from unauthorized users?\" -- an api key is involved in a authorization process. It's not the more secure process, but it's better than an totoally anonymous process. If you don't know the key, you can't authenticate. So the bots, at least the first days/weeks could not access the service (at the end they'll do, cos' the key will be spread informally). So it's CORRECT.","comment_id":"1081489","timestamp":"1701086580.0","poster":"Ruffyit"},{"upvote_count":"1","content":"Selected Answer: AC\nAWS WAF rule to target and filter out malicious requests and API key to authorize users.","poster":"TariqKipkemei","comment_id":"1002101","timestamp":"1694146980.0"},{"poster":"Guru4Cloud","content":"Selected Answer: AC\nThe reasons are:\n\nAn API key with a usage plan limits access to only authorized apps and users. This prevents general public access.\nWAF rules can identify and block malicious bot traffic through pattern matching and IP reputation lists.\nTogether, the API key and WAF provide preventative and detective controls against unauthorized requests.\nThe other options add complexity or are reactive. IAM roles per user is not feasible for a public API.\nIgnoring requests in Lambda and changing DNS are response actions after an attack.","upvote_count":"3","timestamp":"1692224700.0","comment_id":"983078"},{"content":"AC\n\n\nit's essential to note that while API keys are commonly associated with private APIs, they can also be used in conjunction with public APIs. In some cases, even public APIs may require API keys to control usage and monitor how the API is being utilized. The API provider might enforce usage limits, track API usage, or monitor for potential misuse, all of which can be managed effectively using API keys.\n\nIn summary, API keys are not exclusive to private APIs and can be used for both private and public APIs, depending on the specific requirements and use case of the API provider.","timestamp":"1690286100.0","comment_id":"962665","poster":"zjcorpuz","upvote_count":"2"},{"poster":"MutiverseAgent","upvote_count":"3","content":"Selected Answer: AC\nWhy option C) vs option E)\n- It's simpler\n- We want to protect general access to the API and not granular method/user access. The API is already public so If a user API key is in several usage plans that is not a problem (The API is currently public). The objective is to protect API from abuse from malicious internet users and to NOT protect granular method/user access from users that are using the API in the correct way.","timestamp":"1689859080.0","comment_id":"957603"},{"content":"Selected Answer: CE\nImportant\nDon't use API keys for authentication or authorization for your APIs. If you have multiple APIs in a usage plan, a user with a valid API key for one API in that usage plan can access all APIs in that usage plan. Instead, use an IAM role, a Lambda authorizer, or an Amazon Cognito user pool.","upvote_count":"2","comment_id":"944316","timestamp":"1688617260.0","poster":"Mia2009687"},{"poster":"Abrar2022","timestamp":"1686201600.0","content":"Selected Answer: AC\nIf you're wondering why A. It's because you can configure usage plans and API keys to allow customers to access selected APIs, and begin throttling requests to those APIs based on defined limits and quotas. As for C. It's because AWS WAF has bot detection capabilities.","upvote_count":"3","comment_id":"917826"},{"content":"It should be A and C \nBut API Key alone can not help \n\nAPI keys are alphanumeric string values that you distribute to application developer customers to grant access to your API. You can use API keys together with Lambda authorizers, IAM roles, or Amazon Cognito to control access to your APIs.","upvote_count":"2","comment_id":"828699","poster":"sachin","timestamp":"1677908820.0"},{"upvote_count":"1","timestamp":"1677515880.0","comment_id":"823954","content":"Selected Answer: CE\nHere https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html it says this:\n\nDon't use API keys for authentication or authorization for your APIs. If you have multiple APIs in a usage plan, a user with a valid API key for one API in that usage plan can access all APIs in that usage plan. Instead, use an IAM role, a Lambda authorizer, or an Amazon Cognito user pool.\n\nAPI keys are intended for software developers wanting to access an API from their application. This link then goes on to say an IAM role should be used instead.","poster":"Steve_4542636","comments":[{"poster":"Steve_4542636","content":"Nevermind my answer. I switch it to A/C because the question states the application is *using* the API Gateway so A will make sense","upvote_count":"2","comment_id":"823957","timestamp":"1677516120.0"}]},{"upvote_count":"4","content":"Selected Answer: AC\nA/C for security to prevent anonymous access","poster":"simplimarvelous","timestamp":"1674176820.0","comment_id":"781746"},{"content":"I'm thinking A and C\nA - the API is publicly accessible but there is nothing to stop the company requiring users to register for access.\nB - you can do this with Lambda, AWS Network Firewall and Amazon GuardDuty, see https://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/, but these components are not mentioned\nC - a WAF is the logical choice with it's bot detection capabilities\nD - a private API is only accessible within a VPC, so this would not work\nE - would be even more work than A","comment_id":"766394","timestamp":"1672907700.0","upvote_count":"4","poster":"JayBee65"},{"content":"Selected Answer: AC\nhttps://www.examtopics.com/discussions/amazon/view/61082-exam-aws-certified-solutions-architect-associate-saa-c02/","poster":"HayLLlHuK","comment_id":"765664","upvote_count":"4","timestamp":"1672837860.0"},{"content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\nhttps://medium.com/@tshemku/aws-waf-vs-firewall-manager-vs-shield-vs-shield-advanced-4c86911e94c6","timestamp":"1672248780.0","poster":"techhb","comment_id":"760155","upvote_count":"3"},{"poster":"SoluAWS","content":"I do not agree with A as it mentioned the application is publically accessible. \"A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda\". If this is public how can we ensure that genuine user?\n\nI will go with CD","timestamp":"1672106760.0","upvote_count":"3","comment_id":"758074"},{"poster":"techhb","upvote_count":"2","comment_id":"755824","content":"Selected Answer: AC\nA and C ,C is obivious ,however A is the only other which seems to put quota API keys are alphanumeric string values that you distribute to application developer customers to grant access to your API. You can use API keys together with Lambda authorizers, IAM roles, or Amazon Cognito to control access to your APIs","timestamp":"1671986040.0"},{"content":"Selected Answer: AC\nA and C","upvote_count":"1","comment_id":"748422","poster":"career360guru","timestamp":"1671311520.0"},{"poster":"Phinx","comment_id":"727995","timestamp":"1669534020.0","content":"Selected Answer: AC\nA and C are the correct choices.","upvote_count":"1"},{"timestamp":"1669456380.0","poster":"justtry","content":"Selected Answer: AC\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html","upvote_count":"2","comment_id":"727425"},{"poster":"5up3rm4n","timestamp":"1669363860.0","comment_id":"726492","content":"Only answer C is an obviouis choice. B and D are clearly not right and A is the only remotely viable other answer but even then the documentation on API Keys and Usage quotas states not to rely on it to block API requests;\n\nUsage plan throttling and quotas are not hard limits, and are applied on a best-effort basis. In some cases, clients can exceed the quotas that you set. Don’t rely on usage plan quotas or throttling to control costs or block access to an API. Consider using AWS Budgets to monitor costs and AWS WAF to manage API requests.\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html","upvote_count":"4"},{"comment_id":"725787","upvote_count":"1","timestamp":"1669290060.0","poster":"ds0321","content":"Selected Answer: AC\nA and C"},{"content":"Selected Answer: AC\nuse usage plan API key","poster":"babaxoxo","timestamp":"1668587040.0","comment_id":"719472","upvote_count":"2"},{"comment_id":"718766","content":"A and C","timestamp":"1668519900.0","poster":"Nigma","upvote_count":"3"}],"url":"https://www.examtopics.com/discussions/amazon/view/87516-exam-aws-certified-solutions-architect-associate-saa-c03/","question_images":[],"answer":"AC","topic":"1","answers_community":["AC (72%)","CE (21%)","6%"],"answer_description":"","answer_ET":"AC","unix_timestamp":1668519900,"answer_images":[],"timestamp":"2022-11-15 14:45:00","question_text":"A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application’s traffic recently spiked due to fraudulent requests from botnets.\n\nWhich steps should a solutions architect take to block requests from unauthorized users? (Choose two.)","exam_id":31,"choices":{"A":"Create a usage plan with an API key that is shared with genuine users only.","D":"Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint.","C":"Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.","B":"Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.","E":"Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call."}},{"id":"6MGiATQfedIG2gHh5fBD","timestamp":"2022-10-08 16:02:00","question_id":88,"answer":"B","answers_community":["B (86%)","8%"],"question_images":[],"isMC":true,"topic":"1","choices":{"B":"Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.","C":"Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.","D":"Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.","A":"Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles."},"question_text":"A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.\nWhich solution will meet these requirements?","discussion":[{"upvote_count":"70","poster":"rodriiviru","comment_id":"691510","timestamp":"1665436020.0","comments":[{"comment_id":"692617","content":"Agree with you","timestamp":"1665547320.0","upvote_count":"2","poster":"BoboChow"}],"content":"Selected Answer: B\nB"},{"timestamp":"1726905000.0","poster":"PhucVuu","upvote_count":"58","content":"Selected Answer: B\nKeywords:\n- Data lake on AWS.\n- Consists of data in Amazon S3 and Amazon RDS for PostgreSQL.\n- The company needs a reporting solution that provides data VISUALIZATION and includes ALL the data sources within the data lake.\n\nA - Incorrect: Amazon QuickSight only support users(standard version) and groups (enterprise version). users and groups only exists without QuickSight. QuickSight don't support IAM. We use users and groups to view the QuickSight dashboard\nB - Correct: as explained in answer A and QuickSight is used to created dashboard from S3, RDS, Redshift, Aurora, Athena, OpenSearch, Timestream\nC - Incorrect: This way don't support visulization and don't mention how to process RDS data\nD - Incorrect: This way don't support visulization and don't mention how to combine data RDS and S3","comment_id":"863412"},{"upvote_count":"1","content":"Selected Answer: B\nAmazon QuickSight is a data visualization service that allows you to create interactive dashboards and reports from various data sources, including Amazon S3 and Amazon RDS for PostgreSQL. You can connect all the data sources and create new datasets in QuickSight, and then publish dashboards to visualize the data. You can also share the dashboards with the appropriate users and groups, and control their access levels using IAM roles and permissions.","comment_id":"1419919","poster":"Sanki_26","timestamp":"1743562260.0"},{"timestamp":"1739812980.0","poster":"Vandaman","content":"Selected Answer: B\nIt was the use of users and groups that answered that for me","comment_id":"1357893","upvote_count":"1"},{"poster":"FlyingHawk","timestamp":"1736466960.0","content":"Selected Answer: B\nAmazon QuickSight can handle dynamic data through Direct Query Mode or scheduled data refreshes. It provides real-time or near-real-time visualizations, interactive dashboards, and fine-grained access control, making it the most suitable solution for the company’s requirements.\n\nhttps://aws.amazon.com/blogs/business-intelligence/best-practices-for-amazon-quicksight-spice-and-direct-query-mode/\n\nI saw the question in udemy practice test 5 from Neal Davis, he suggested the correct answer is D, his reasons: option B solves the problem of access sharing with resources but does not take care of delta in data.","comment_id":"1338585","upvote_count":"2"},{"comment_id":"1331627","upvote_count":"1","timestamp":"1735141920.0","content":"Selected Answer: B\nAmazon QuickSight is designed for creating visualizations and dashboards, and it can easily connect to a variety of data sources such as Amazon S3 and Amazon RDS for PostgreSQL.\nYou can create an analysis in QuickSight by connecting the required data sources, creating datasets, and then building dashboards.\nAccess control in QuickSight is robust. You can share dashboards with specific users and groups, ensuring that only the management team has full access to all the visualizations, while the rest of the company can have limited access.","poster":"MGKYAING"},{"comment_id":"1325131","timestamp":"1733938920.0","poster":"Tjazz04","upvote_count":"1","content":"Selected Answer: B\nKeyword: data visualization = QuickSight\nQuickSight defines Users (standard versions) and Groups (enterprise version)\nThese users & groups only exist within QuickSight, not IAM \nSo the correct answer is B."},{"comment_id":"1311777","upvote_count":"1","poster":"EzKkk","timestamp":"1731567480.0","content":"Selected Answer: B\nThe question requires a solution for data visualization, which means it focuses solely on downstream consumption. Therefore, any solution that includes upstream processing is off the table (C & D). This leaves us with two options: A and B. Essentially, they are the same with one key difference: A manages access through IAM Roles, while B manages it through an IAM group. Since only the management team is granted permission, using a group will be more efficient, as it scales better with changes in personnel and provides a centralized point for managing permissions."},{"comment_id":"1311776","upvote_count":"1","poster":"EzKkk","timestamp":"1731567420.0","content":"The question requires a solution for data visualization, which means it focuses solely on downstream consumption. Therefore, any solution that includes upstream processing is off the table (C & D). This leaves us with two options: A and B. Essentially, they are the same with one key difference: A manages access through IAM Roles, while B manages it through an IAM group. Since only the management team is granted permission, using a group will be more efficient, as it scales better with changes in personnel and provides a centralized point for managing permissions."},{"content":"Selected Answer: D\nIf you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.\n\nAthena uses data source connectors that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL under the Apache 2.0 license","poster":"hanen","comments":[{"comment_id":"1338586","poster":"FlyingHawk","content":"D relies on static reports stored in Amazon S3.\nIf the data changes, the reports would need to be regenerated and re-published, which is not ideal for dynamic data.\nThey also lack interactive visualization capabilities and fine-grained access control for dashboards.","upvote_count":"1","timestamp":"1736467140.0"},{"upvote_count":"1","comment_id":"1308655","content":"key phases: \" The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. \"","poster":"JA2018","timestamp":"1731048960.0"}],"timestamp":"1726905060.0","comment_id":"794874","upvote_count":"4"},{"poster":"linux_admin","content":"Selected Answer: B\nOption B is the correct answer because Amazon QuickSight's sharing mechanism is based on users and groups, not IAM roles. IAM roles are used for granting permissions to AWS resources, but they are not directly used for sharing QuickSight dashboards.\n\nIn option B, you create an analysis in Amazon QuickSight, connect all the data sources (Amazon S3 and Amazon RDS for PostgreSQL), and create new datasets. After publishing dashboards to visualize the data, you share them with appropriate users and groups. This approach allows you to control the access levels for different users, such as providing full access to the management team and limited access to the rest of the company. This solution meets the requirements specified in the question.","upvote_count":"7","timestamp":"1726905060.0","comment_id":"856354"},{"upvote_count":"4","timestamp":"1726905060.0","comment_id":"852190","poster":"elearningtakai","content":"Selected Answer: B\nAmazon QuickSight is a cloud-based business intelligence (BI) service that makes it easy to create and publish interactive dashboards that include data visualizations from multiple data sources. By using QuickSight, the company can connect to both Amazon S3 and Amazon RDS for PostgreSQL and create new datasets that combine data from both sources. The company can then use QuickSight to create interactive dashboards that visualize the data and provide data insights.\n\nTo limit access to the visualizations, the company can use QuickSight's built-in security features. QuickSight allows you to define fine-grained access control at the user or group level. This way, the management team can have full access to all the visualizations, while the rest of the company can have only limited access."},{"timestamp":"1726905060.0","upvote_count":"2","poster":"dszes","comment_id":"907502","content":"tricky question, Users, groups and roles can have access.\nViewing who has access to a dashboard\nUse the following procedure to see which users or groups have access to the dashboard.\n\nOpen the published dashboard and choose Share at upper right. Then choose Share dashboard.\n\nIn the Share dashboard page that opens, under Manage permissions, review the users and groups, and their roles and settings.\n\nYou can search to locate a specific user or group by entering their name or any part of their name in the search box at upper right. Searching is case-sensitive, and wildcards aren't supported. Delete the search term to return the view to all users."},{"upvote_count":"4","poster":"cookieMr","timestamp":"1726905000.0","content":"Selected Answer: B\nB. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.\n\nAmazon QuickSight is a business intelligence (BI) tool provided by AWS that allows you to create interactive dashboards and reports. It supports a variety of data sources, including Amazon S3 and Amazon RDS for PostgreSQL, which are the data sources in the company's data lake.\n\nOption A (Create an analysis in Amazon QuickSight and share with IAM roles) is incorrect because it suggests sharing with IAM roles, which are more suitable for managing access to AWS resources rather than granting access to specific users or groups within QuickSight.","comment_id":"926537"},{"timestamp":"1726905000.0","comment_id":"958341","upvote_count":"3","content":"Selected Answer: B\nExplanation:\n\nOption B involves using Amazon QuickSight, which is a business intelligence tool provided by AWS for data visualization and reporting. With this option, you can connect all the data sources within the data lake, including Amazon S3 and Amazon RDS for PostgreSQL. You can create datasets within QuickSight that pull data from these sources.\n\nThe solution allows you to publish dashboards in Amazon QuickSight, which will provide the required data visualization capabilities. To control access, you can use appropriate IAM (Identity and Access Management) roles, assigning full access only to the company's management team and limiting access for the rest of the company. You can share the dashboards selectively with the users and groups that need access.","poster":"Guru4Cloud"},{"comment_id":"1024734","timestamp":"1726905000.0","upvote_count":"2","poster":"IdanAWS","content":"My opinion is divided here, and I will explain:\nOption C can be correct because glue crawler is used to access S3, and athena federated query is used to access RDS.\nMy problem with answer C is that it says:\n\"Generate Reports by using athena\"\nAnd I think that is not true. athena alone does not generate reports, it has to integrate with services such as quickSight and then it generates reports, therefore the answer is not written properly and I think C is a mistake.\nSince C is wrong I think B is the correct answer."},{"content":"Explanation:\n\nOption B involves using Amazon QuickSight, which is a business intelligence tool provided by AWS for data visualization and reporting. With this option, you can connect all the data sources within the data lake, including Amazon S3 and Amazon RDS for PostgreSQL. You can create datasets within QuickSight that pull data from these sources.\n\nThe solution allows you to publish dashboards in Amazon QuickSight, which will provide the required data visualization capabilities. To control access, you can use appropriate IAM (Identity and Access Management) roles, assigning full access only to the company's management team and limiting access for the rest of the company. You can share the dashboards selectively with the users and groups that need access.","timestamp":"1726905000.0","upvote_count":"1","comment_id":"1054261","poster":"Ruffyit"}],"answer_description":"","exam_id":31,"answer_ET":"B","unix_timestamp":1665237720,"url":"https://www.examtopics.com/discussions/amazon/view/84732-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[]},{"id":"cJJai3Mnd0wgCbqjiifa","choices":{"A":"Amazon OpenSearch Service (Amazon Elasticsearch Service)","C":"Amazon S3 Standard","B":"Amazon S3 Glacier","D":"Amazon RDS for PostgreSQL"},"url":"https://www.examtopics.com/discussions/amazon/view/87632-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_ET":"C","answers_community":["C (90%)","10%"],"answer_description":"","answer":"C","unix_timestamp":1668587400,"discussion":[{"upvote_count":"14","poster":"babaxoxo","content":"Selected Answer: C\nAns C:\nCost-effective solution with milliseconds of retrieval -> it should be s3 standard","comment_id":"719482","timestamp":"1668587400.0"},{"timestamp":"1692224820.0","upvote_count":"6","content":"Selected Answer: C\nThe reasons are:\n\nS3 Standard provides high durability and availability for storage\nIt allows millisecond access to retrieve objects\nObjects can be stored for any duration, meeting the 30 day retention need\nStorage costs are low, around $0.023 per GB/month\nOpenSearch and RDS require running and managing a cluster for DR storage\nGlacier has lower cost but retrieval time is too high at 3-5 hours\nS3 Standard's simplicity, high speed access, and low cost make it optimal for this small DR dataset that needs to be accessed quickly","poster":"Guru4Cloud","comment_id":"983079"},{"upvote_count":"1","content":"Selected Answer: C\nS3 Standard is the most cheapest.","timestamp":"1738387500.0","poster":"satyaammm","comment_id":"1349757"},{"upvote_count":"2","poster":"PaulGa","comment_id":"1299619","timestamp":"1729247460.0","content":"Selected Answer: C\nAns C - S3 std: cost-effective, milliseconds of retrieval"},{"timestamp":"1729110300.0","comment_id":"1298940","poster":"PaulGa","content":"Selected Answer: C\nS3 Standard","upvote_count":"1"},{"comment_id":"1183474","poster":"soufiyane","timestamp":"1711472280.0","upvote_count":"1","content":"Selected Answer: C\nalso s3 can store any form of data"},{"poster":"pentium75","comment_id":"1106003","timestamp":"1703594280.0","upvote_count":"5","content":"Selected Answer: C\nOnly Glacier class that would meet the requirement is Instant Retrieval, but it has 90 days minimum storage time which would kill the cost savings."},{"upvote_count":"1","comments":[{"poster":"xdkonorek2","content":"but if 300 MB is divided into smaller files situation changes which is probably the case..\n\n300 MB / month storage without retrieval when files are 600x0.5MB :\nS3 Standard cost (Monthly): 0.01 USD\nS3 Standard cost (Upfront): 0.00 USD\nS3 Glacier Instant Retrieval cost (Monthly): 0.01 USD\nS3 Glacier Instant Retrieval cost (Upfront): 0.02 USD\n\n\n3 GB / month storage without retrieval when files are 6000x0.5 MB file:\nS3 Standard cost (Monthly): 0.10 USD\nS3 Standard cost (Upfront): 0.03 USD\nS3 Glacier Instant Retrieval cost (Monthly): 0.13 USD\nS3 Glacier Instant Retrieval cost (Upfront): 0.25 USD","upvote_count":"4","comment_id":"1065076","timestamp":"1699381620.0"},{"comment_id":"1105996","upvote_count":"2","poster":"pentium75","content":"Only Glacier Instant Retrieval (which is not mentioned in B) would meet the access requirement, but \"Objects that are archived to S3 Glacier Instant Retrieval and S3 Glacier Flexible Retrieval are charged for a minimum storage duration of 90 days.\"","timestamp":"1703594160.0"}],"comment_id":"1065074","timestamp":"1699381380.0","poster":"xdkonorek2","content":"Selected Answer: B\n300 MB / month storage without retrieval when file is single 300 MB file:\nS3 Standard cost (Monthly): 0.01 USD\nS3 Standard cost (Upfront): 0.00 USD\nS3 Glacier Instant Retrieval cost (Monthly): 0.00 USD\n\nif it was 3GB:\n3 GB / month storage without retrieval when file is single 3GB file:\nS3 Standard cost (Monthly): 0.07 USD\nS3 Standard cost (Upfront): 0.00 USD\nS3 Glacier Instant Retrieval cost (Monthly): 0.01 USD\n\nWhen assumed no retrieval is required because it's DR solution, and it's a single file, Glacier Instant Retrieval wins, and when they mention S3 glacier we must choose one of the sub-category"},{"upvote_count":"4","poster":"Its_SaKar","comment_id":"1004583","timestamp":"1694424480.0","content":"Selected Answer: C\nAnswer is not B because S3 glacier and S3 glacier instant storage are two different types of storage class. So, answer here is C: S3 standard"},{"comment_id":"1002102","timestamp":"1694147160.0","poster":"TariqKipkemei","content":"Selected Answer: C\nData must be accessible in milliseconds and must be kept for 30 days = Amazon S3 Standard","upvote_count":"2"},{"timestamp":"1693149000.0","upvote_count":"1","content":"Selected Answer: C\nANS - C","poster":"chanchal133","comment_id":"991521"},{"comments":[],"timestamp":"1690434480.0","comment_id":"964358","content":"Selected Answer: C\nhttps://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/","poster":"Nazmul123","upvote_count":"2"},{"content":"S3 Standard is a highly durable and scalable storage option suitable for backup and disaster recovery purposes. It offers millisecond access to data when needed and provides durability guarantees. It is also cost-effective compared to other storage options like OpenSearch Service, S3 Glacier, and RDS for PostgreSQL, which may have higher costs or longer access times for retrieving the data.\n\nA. OpenSearch Service (Elasticsearch Service): While it offers fast data retrieval, it may incur higher costs compared to storing data directly in S3, especially considering the amount of data being generated.\n\nB. S3 Glacier: While it provides long-term archival storage at a lower cost, it does not meet the requirement of immediate access in milliseconds. Retrieving data from Glacier typically takes several hours.\n\nD. RDS for PostgreSQL: While it can be used for data storage, it may be overkill and more expensive for a backup and disaster recovery solution compared to S3 Standard, which is more suitable and cost-effective for storing and retrieving data.","timestamp":"1687606800.0","comment_id":"932509","upvote_count":"3","poster":"cookieMr"},{"upvote_count":"4","timestamp":"1686380700.0","content":"Selected Answer: B\nhttps://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/","poster":"joehong","comment_id":"919858","comments":[{"timestamp":"1703594220.0","content":"\"Objects that are archived to S3 Glacier Instant Retrieval and S3 Glacier Flexible Retrieval are charged for a minimum storage duration of 90 days.\" Also Instant Retrieval is not mentioned in B.","upvote_count":"2","comment_id":"1106000","poster":"pentium75"}]},{"comment_id":"805326","timestamp":"1676127840.0","content":"A. Incorrect\nAmazon OpenSearch Service (Amazon Elasticsearch Service) is designed for full-text search and analytics, but it may not be the most cost-effective solution for this use case\nB. Incorrect\nS3 Glacier is a cold storage solution that is designed for long-term data retention and infrequent access.\nC. Correct\nS3 standard is cost-effective and meets the requirement. S3 Standard allows for data retention for a specific number of days.\n\nD. PostgreSQL is a relational database service and may not be the most cost-effective solution.","poster":"KZM","upvote_count":"5"},{"upvote_count":"4","comment_id":"767808","poster":"ngochieu276","timestamp":"1673016420.0","content":"Selected Answer: B\nS3 Glacier Instant Retrieval – Use for archiving data that is rarely accessed and requires milliseconds retrieval.\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html"},{"comment_id":"748354","upvote_count":"1","content":"Selected Answer: C\nOption C","timestamp":"1671306540.0","poster":"career360guru"},{"timestamp":"1670825940.0","content":"Selected Answer: C\nJSON is object notation. S3 stores objects.","poster":"lapaki","upvote_count":"2","comment_id":"742453"},{"comment_id":"734314","upvote_count":"1","poster":"hpipit","content":"Selected Answer: C\nc IS correct","timestamp":"1670059500.0"},{"poster":"Wpcorgan","comment_id":"724453","upvote_count":"1","timestamp":"1669133640.0","content":"C is correct"},{"upvote_count":"4","poster":"sdasdawa","timestamp":"1668598620.0","content":"Selected Answer: C\nIMHO \nNormally ElasticSearch would be ideal here, however as question states \"Most cost-effective\" \nS3 is the best choice in this case","comment_id":"719579","comments":[{"comment_id":"733109","upvote_count":"4","timestamp":"1669929960.0","poster":"Aamee","content":"ElasticSearch is a search service, the question states here about the backup service reqd. for the DR scenario."}]}],"question_id":89,"exam_id":31,"timestamp":"2022-11-16 09:30:00","topic":"1","question_images":[],"answer_images":[],"question_text":"An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to back up the data. The data must be accessible in milliseconds if it is needed, and the data must be kept for 30 days.\n\nWhich solution meets these requirements MOST cost-effectively?","isMC":true},{"id":"BuuF7GrtWqMtOvPDd5DK","question_id":90,"unix_timestamp":1668587580,"discussion":[{"poster":"babaxoxo","upvote_count":"18","comment_id":"719484","content":"Selected Answer: B\nsolution should remove operation overhead -> s3 -> lambda -> aurora","timestamp":"1668587580.0","comments":[{"content":"Aurora supports mysql and postgresql but question has database sql server. So, that eliminates B. So, the other logical answer is D. IMHO. Btw, i also thought the answer is B and started re-reading question carefully.","upvote_count":"6","timestamp":"1686969420.0","poster":"markw92","comments":[{"upvote_count":"6","content":"sql database, not sql server","comment_id":"1020565","timestamp":"1695971760.0","poster":"JIJIJIXI"}],"comment_id":"925708"}]},{"content":"Selected Answer: B\nBy placing the JSON documents in an S3 bucket, the documents will be stored in a highly durable and scalable object storage service. The use of AWS Lambda allows the company to run their Python code to process the documents as they arrive in the S3 bucket without having to worry about the underlying infrastructure. This also allows for horizontal scalability, as AWS Lambda will automatically scale the number of instances of the function based on the incoming rate of requests. The results can be stored in an Amazon Aurora DB cluster, which is a fully-managed, high-performance database service that is compatible with MySQL and PostgreSQL. This will provide the necessary durability and scalability for the results of the processing.","poster":"Zerotn3","timestamp":"1672623720.0","comment_id":"763445","upvote_count":"13"},{"poster":"satyaammm","upvote_count":"1","content":"Selected Answer: B\nUsing Lambda, S3 and Aurora is the most suitable and provides least operational overhead for this problem.","timestamp":"1739027760.0","comment_id":"1353464"},{"timestamp":"1729247940.0","content":"Selected Answer: D\nAns D - because none of the other answers guarantee FIFO. What if a subsequent JSON operation is intended to update an existing database record (which is highly likely) - then the wrong change would be applied","poster":"PaulGa","upvote_count":"2","comment_id":"1299622"},{"upvote_count":"1","poster":"soufiyane","timestamp":"1711814280.0","comment_id":"1186168","content":"Selected Answer: B\nb is the right answer it very obvious"},{"comment_id":"1112312","content":"Guys I have a question.\nWe dont know how long the processing of JSON documents is going to take. What if that processing takes more than 15 min ? Lambda can run only for 15 correct ? Based on this the answer could be D\n\n Please correct my understanding.","timestamp":"1704233280.0","upvote_count":"3","poster":"Anantvir"},{"content":"Selected Answer: B\n\"D\" has a lot of moving parts and more operational overhead even if each part is a managed service in itself. Also, if something can be done with Lambda, don't use an EC2 instance in any form as it always increases operational overhead (compared to Lambda).","timestamp":"1704058140.0","comment_id":"1110820","upvote_count":"3","poster":"awsgeek75"},{"timestamp":"1697793540.0","poster":"David_Ang","content":"Selected Answer: B\n\"D\" is just like the most complex one, sometimes the admin make mistakes and don't realize. lambda is a service make for this","comment_id":"1048586","upvote_count":"2"},{"timestamp":"1695719100.0","poster":"Mandar15","comment_id":"1017571","upvote_count":"1","content":"Selected Answer: B\nB is correc"},{"poster":"TariqKipkemei","content":"Selected Answer: B\nMain requirement is: 'scalability and minimized operational overhead' = serverless = Amazon S3 bucket, AWS Lambda function, Amazon Aurora DB cluster","comment_id":"1004473","timestamp":"1694411100.0","upvote_count":"2"},{"content":"Selected Answer: B\n- Using Lambda functions triggered by S3 events allows the Python code to automatically scale up and down based on the number of incoming JSON documents. This provides high availability and maximizes scalability.\n- Storing the results in an Amazon Aurora DB cluster provides a managed, scalable, and highly available database.\n- This serverless approach minimizes operational overhead since Lambda and Aurora handle provisioning infrastructure, deploying code, monitoring, patching, etc.","upvote_count":"3","timestamp":"1692271800.0","poster":"Guru4Cloud","comment_id":"983550"},{"content":"The answer is B. Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.\nThis solution is highly available because Lambda functions are automatically scaled up or down based on the number of requests they receive. It is also scalable because you can easily add more Lambda functions to process more documents. Finally, it minimizes operational overhead because you do not need to manage any EC2 instances.","upvote_count":"2","poster":"aadityaravi8","comment_id":"944211","timestamp":"1688606760.0"},{"content":"Selected Answer: B\nUsing Lambda eliminates the need to manage and provision servers, ensuring scalability and minimizing operational overhead. S3 provides durable and highly available storage for the JSON documents. Lambda can be triggered automatically whenever new documents are added to the S3 bucket, allowing for real-time processing. Storing the results in an Aurora DB cluster ensures high availability and scalability for the processed data. This solution leverages serverless architecture, allowing for automatic scaling and high availability without the need for managing infrastructure, making it the most suitable choice.\n\nA. This option requires manual management and scaling of EC2 instances, resulting in higher operational overhead and complexity.\n\nC. This approach still involves manual management and scaling of EC2 instances, increasing operational complexity and overhead.\n\nD. This solution requires managing and scaling an ECS cluster, adding operational overhead and complexity. Utilizing SQS adds complexity to the system, requiring custom handling of message consumption and processing in the Python code.","comment_id":"934077","upvote_count":"4","poster":"cookieMr","timestamp":"1687758840.0"},{"poster":"Bmarodi","upvote_count":"2","content":"Selected Answer: B\nKeywords here are : \"maximizes scalability and minimizes operational overhead, hence option B is correct answer.","comment_id":"905529","timestamp":"1684908300.0"},{"poster":"channn","timestamp":"1681090500.0","content":"Selected Answer: D\ni vote for D as 'on-premises SQL database' is not mysql/postgre which can replace by aurora","upvote_count":"2","comments":[{"content":"Why not? It's a \"SQL database\", NOT necessarily Microsoft SQL Server. But even if it would be SQL server, that could be migrated to Aurora.","timestamp":"1703595480.0","poster":"pentium75","upvote_count":"1","comment_id":"1106021"}],"comment_id":"865929"},{"timestamp":"1677121560.0","poster":"kerin","upvote_count":"1","comment_id":"818719","content":"B is the best option. https://aws.amazon.com/rds/aurora/"},{"comment_id":"761256","timestamp":"1672331160.0","content":"Selected Answer: B\nagree...B is the best option S3, Lambda , Aurora.","upvote_count":"2","poster":"mp165"},{"upvote_count":"2","comment_id":"755837","timestamp":"1671987300.0","poster":"techhb","content":"Selected Answer: B\nChoosing B as \"The company needs a highly available solution that maximizes scalability and minimizes operational overhead\""},{"poster":"studis","content":"B is tempting but this sentence \"runs thousands of times each day.\" If we use lambda as in B, won't this incur a high bill at the end?","comment_id":"748900","timestamp":"1671369540.0","comments":[{"upvote_count":"2","comment_id":"1106022","timestamp":"1703595540.0","content":"Why would Lambda incur a high bill? If it runs 5000 times each day, each time for 0.5 seconds, you'd pay for 2500 seconds in total.","poster":"pentium75"}],"upvote_count":"1"},{"content":"Selected Answer: B\nOption B","comment_id":"748356","upvote_count":"1","poster":"career360guru","timestamp":"1671306720.0"},{"upvote_count":"2","content":"Selected Answer: B\nD is incorrect because using ECS entails a lot of admin overhead. so B is the correct one.","timestamp":"1669534200.0","comment_id":"727998","poster":"Phinx"},{"comment_id":"724458","content":"B is correct","poster":"Wpcorgan","upvote_count":"1","timestamp":"1669134060.0"},{"comment_id":"722986","timestamp":"1668976080.0","content":"Selected Answer: B\nB is the answer\nhttps://aws.amazon.com/rds/aurora/","upvote_count":"2","poster":"EKA_CloudGod"},{"comment_id":"719702","comments":[{"comment_id":"720476","content":"ehhhhhh","timestamp":"1668691740.0","poster":"Nightducky","upvote_count":"4"}],"poster":"BENICE","upvote_count":"1","content":"D is correct option","timestamp":"1668606660.0"}],"topic":"1","answers_community":["B (94%)","6%"],"answer_images":[],"answer_description":"","exam_id":31,"question_images":[],"isMC":true,"question_text":"A company has a small Python application that processes JSON documents and outputs the results to an on-premises SQL database. The application runs thousands of times each day. The company wants to move the application to the AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational overhead.\n\nWhich solution will meet these requirements?","timestamp":"2022-11-16 09:33:00","choices":{"C":"Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-Attach feature to attach the volume to multiple Amazon EC2 instances. Run the Python code on the EC2 instances to process the documents. Store the results on an Amazon RDS DB instance.","D":"Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy the Python code as a container on an Amazon Elastic Container Service (Amazon ECS) cluster that is configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results on an Amazon RDS DB instance.","A":"Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon Aurora DB cluster.","B":"Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster."},"answer_ET":"B","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/87633-exam-aws-certified-solutions-architect-associate-saa-c03/"}],"exam":{"isMCOnly":true,"isBeta":false,"numberOfQuestions":1019,"isImplemented":true,"provider":"Amazon","name":"AWS Certified Solutions Architect - Associate SAA-C03","id":31,"lastUpdated":"11 Apr 2025"},"currentPage":18},"__N_SSP":true}