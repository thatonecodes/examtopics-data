{"pageProps":{"questions":[{"id":"VJJcpjZSJvhmj4wabvAe","question_id":906,"isMC":true,"answer_ET":"A","discussion":[{"timestamp":"1715240760.0","poster":"Hkayne","content":"Selected Answer: A\nB is wrong because the job takes 1 hour and the lambda maximum execution time is 15 minutes.\nC is wrong can't use spot instances because the job can not tolerate interruptions.\nD iswrong too because DataSync is not designed to lunch jobs.\nCorrect answer is A","upvote_count":"5","comment_id":"1208745"},{"timestamp":"1729659780.0","poster":"mk168898","upvote_count":"1","comment_id":"1301882","content":"B is not because lambda only max 15mins but job requires 1 hour\nC is not because cannot tolerate interruptions, spot instance might be gone anytime.\nD datasync doesn't seem correct\n\nA is most correct"},{"comment_id":"1262450","upvote_count":"1","timestamp":"1723115340.0","content":"I know datasync and lambda is event based, there are interruptions. C, doesnt address the scheduling requirement. Has to be A","poster":"744fdad"},{"upvote_count":"1","content":"Selected Answer: A\nA. Create a container for the job. Schedule the job to run as an AWS Fargate task on an Amazon Elastic Container Service (Amazon ECS) cluster by using Amazon EventBridge Scheduler.","poster":"muhammadahmer36","comment_id":"1249111","timestamp":"1721149440.0"},{"content":"Selected Answer: A\nAnswer: A\n\nFargate is compatibilie with ECS and is allowing for log running tasks","timestamp":"1716472380.0","upvote_count":"4","comment_id":"1216624","poster":"Scheldon"}],"unix_timestamp":1715240760,"question_text":"A company runs a critical data analysis job each week before the first day of the work week. The job requires at least 1 hour to complete the analysis. The job is stateful and cannot tolerate interruptions. The company needs a solution to run the job on AWS.\n\nWhich solution will meet these requirements?","answer_description":"","answer":"A","answers_community":["A (100%)"],"question_images":[],"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/140209-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"choices":{"D":"Configure an AWS DataSync task to run the job. Configure a cron expression to run the task on a schedule.","C":"Configure an Auto Scaling group of Amazon EC2 Spot Instances that run Amazon Linux. Configure a crontab entry on the instances to run the analysis.","B":"Configure the job to run in an AWS Lambda function. Create a scheduled rule in Amazon EventBridge to invoke the Lambda function.","A":"Create a container for the job. Schedule the job to run as an AWS Fargate task on an Amazon Elastic Container Service (Amazon ECS) cluster by using Amazon EventBridge Scheduler."},"timestamp":"2024-05-09 09:46:00","exam_id":31},{"id":"UcqAAApjwMngGDRU1KlU","unix_timestamp":1714495020,"discussion":[{"timestamp":"1714495020.0","upvote_count":"7","content":"Selected Answer: C\nA, B, D are senseless +\nAmazon Security Lake automatically centralizes security data from AWS environments, you can get a more complete understanding of your security data across your entire organization. You can also improve the protection.","poster":"sandordini","comment_id":"1204667"},{"poster":"LeonSauveterre","upvote_count":"1","timestamp":"1735811340.0","content":"Selected Answer: C\nLet's just focus on Security Lake.\n1. Purpose-built: Amazon Security Lake is specifically designed to collect and centralize security data with minimal configuration and development overhead.\n2. It integrates easily with AWS security services, providing a streamlined solution with less manual configuration compared to using AWS Glue or Lambda.\n3. This is also designed to scale automatically, making it the most efficient and effective solution for the needs mentioned in the question.","comment_id":"1335462"},{"content":"Selected Answer: C\nC. Configure a data lake in Amazon Security Lake to collect the security data. Upload the data to an Amazon S3 bucket.","comment_id":"1249119","upvote_count":"1","poster":"muhammadahmer36","timestamp":"1721150340.0"},{"content":"Selected Answer: C\nAnswer C\n\nhttps://aws.amazon.com/security-lake/\n\nAmazon Security Lake automatically centralizes security data from AWS environments, SaaS providers, on premises, and cloud sources into a purpose-built data lake stored in your account.","timestamp":"1716476940.0","comment_id":"1216674","poster":"Scheldon","upvote_count":"3"}],"question_id":907,"url":"https://www.examtopics.com/discussions/amazon/view/139811-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"C","answer_images":[],"answer_description":"","isMC":true,"exam_id":31,"answer_ET":"C","question_text":"A company runs workloads in the AWS Cloud. The company wants to centrally collect security data to assess security across the entire company and to improve workload protection.\n\nWhich solution will meet these requirements with the LEAST development effort?","choices":{"C":"Configure a data lake in Amazon Security Lake to collect the security data. Upload the data to an Amazon S3 bucket.","B":"Configure an AWS Lambda function to collect the security data in .csv format. Upload the data to an Amazon S3 bucket.","A":"Configure a data lake in AWS Lake Formation. Use AWS Glue crawlers to ingest the security data into the data lake.","D":"Configure an AWS Database Migration Service (AWS DMS) replication instance to load the security data into an Amazon RDS cluster."},"answers_community":["C (100%)"],"topic":"1","question_images":[],"timestamp":"2024-04-30 18:37:00"},{"id":"xtV5MRIzuX4gXW3EQiDi","question_images":[],"unix_timestamp":1715241840,"discussion":[{"poster":"Scheldon","upvote_count":"5","timestamp":"1716477840.0","content":"Selected Answer: D\nAnswer: D\n\nhttps://aws.amazon.com/transit-gateway/\n\nLooks like the best solution would be transit gateway. It will allow for inter-VPC communication for all 5 applications/VPC, reach shared resource/VPC and in the future it will be easy to allow for inter-communication between even 100 VPCs (applications)","comment_id":"1216683"},{"timestamp":"1735812120.0","poster":"LeonSauveterre","comment_id":"1335475","content":"Selected Answer: D\nA - VPN Tunnels: High administrative overhead. Requires individual tunnel setup and routing configuration for each VPC, making scaling to 100+ VPCs inefficient.\nB - VPC Peering: Moderate overhead. Requires manual peering for each pair of VPCs and routing configurations, which becomes harder to manage as more VPCs are added.\nC - Direct Connect: High cost and complexity. Direct Connect is typically used for hybrid cloud setups or data center connectivity but is costly and harder to manage at large scale. Not suitable for connecting numerous VPCs in the AWS Cloud.\nD - Transit Gateway: Lowest overhead. Acts as a centralized hub for managing inter-VPC communication and simplifies the process of routing between VPCs. It is highly scalable and reduces the complexity of adding new VPCs, and optimal for managing large-scale environments.","upvote_count":"1","comments":[{"comment_id":"1335476","content":"I copied this from somewhere else for your references.","upvote_count":"1","timestamp":"1735812120.0","poster":"LeonSauveterre"}]},{"content":"each application needs to be in their own VPC and can communicate with each other => transit gateway","comment_id":"1301890","upvote_count":"2","poster":"mk168898","timestamp":"1729660620.0"},{"content":"Selected Answer: D\nD. Deploy a transit gateway with associations between the transit gateway and the application VPCs and the shared services VPC. Add routes between the application VPCs in their subnets and the application VPCs to the shared services VPC through the transit gateway.","timestamp":"1721150700.0","comment_id":"1249121","poster":"muhammadahmer36","upvote_count":"2"},{"timestamp":"1719612960.0","comment_id":"1238945","upvote_count":"3","content":"Selected Answer: D\nAWS Transit Gateway:\n\n Centralized Connectivity: AWS Transit Gateway provides a hub-and-spoke model for connecting multiple VPCs, simplifying network management by providing a single point of connectivity for all VPCs.\n Scalability: It is designed to handle many VPCs, making it suitable for scaling beyond the initial five applications to more than 100 applications.\n Reduced Administrative Overhead: Managing VPC peering connections or VPN tunnels for each pair of VPCs would become complex and difficult to manage at scale. Transit Gateway simplifies this by providing centralized routing and connectivity.","poster":"emakid"},{"timestamp":"1719361260.0","content":"Selected Answer: D\nthe LEAST administrative overhead = transit gateway","upvote_count":"2","poster":"DanielWuTRT","comment_id":"1237139"},{"poster":"0bdf3af","comment_id":"1216367","timestamp":"1716451260.0","content":"D. https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html","upvote_count":"3"},{"comments":[{"content":"not possible, vpc peerings can have up to 125 connections and the request is for 100 apps, meaning 100(100-1)/2 connections...","poster":"dragossky","upvote_count":"1","timestamp":"1734464520.0","comment_id":"1328119"}],"content":"Selected Answer: B\nCorrect answer is B","timestamp":"1715241840.0","comment_id":"1208753","poster":"Hkayne","upvote_count":"1"}],"question_id":908,"answer_description":"","exam_id":31,"answer_images":[],"timestamp":"2024-05-09 10:04:00","topic":"1","choices":{"C":"Deploy an AWS Direct Connect connection between the application VPCs and the shared services VPAdd routes from the application VPCs in their subnets to the shared services VPC and the applications VPCs. Add routes from the shared services VPC subnets to the applications VPCs.","A":"Deploy software VPN tunnels between the application VPCs and the shared services VPC. Add routes between the application VPCs in their subnets to the shared services VPC.","D":"Deploy a transit gateway with associations between the transit gateway and the application VPCs and the shared services VPC. Add routes between the application VPCs in their subnets and the application VPCs to the shared services VPC through the transit gateway.","B":"Deploy VPC peering connections between the application VPCs and the shared services VPC. Add routes between the application VPCs in their subnets to the shared services VPC through the peering connection."},"answers_community":["D (93%)","7%"],"question_text":"A company is migrating five on-premises applications to VPCs in the AWS Cloud. Each application is currently deployed in isolated virtual networks on premises and should be deployed similarly in the AWS Cloud. The applications need to reach a shared services VPC. All the applications must be able to communicate with each other.\n\nIf the migration is successful, the company will repeat the migration process for more than 100 applications.\n\nWhich solution will meet these requirements with the LEAST administrative overhead?","url":"https://www.examtopics.com/discussions/amazon/view/140211-exam-aws-certified-solutions-architect-associate-saa-c03/","isMC":true,"answer":"D","answer_ET":"D"},{"id":"D1ZMcLM5Cli79gC1xAD1","answer_ET":"B","timestamp":"2022-10-08 10:11:00","choices":{"C":"Create an Amazon FSx for Windows File Server file system to extend the company's storage space.","A":"Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.","D":"Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.","B":"Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days."},"answers_community":["B (85%)","Other"],"answer_description":"","exam_id":31,"url":"https://www.examtopics.com/discussions/amazon/view/84680-exam-aws-certified-solutions-architect-associate-saa-c03/","question_id":909,"answer":"B","isMC":true,"topic":"1","unix_timestamp":1665216660,"question_images":[],"question_text":"A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.\nThe total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.\nWhich solution will meet these requirements?","discussion":[{"upvote_count":"54","comment_id":"689126","timestamp":"1726819020.0","comments":[{"comment_id":"1258029","upvote_count":"1","poster":"gianola","content":"I think B is wrong because they are not asking you to add more room, they are asking you to have more room available.","timestamp":"1722328260.0"},{"timestamp":"1669796640.0","comments":[{"upvote_count":"3","comment_id":"981838","timestamp":"1692116880.0","comments":[{"timestamp":"1693222740.0","upvote_count":"4","content":"It says low-latency is required for the most recently accessed files, not new ones. So if an older file is retrieved from deep archive, it should then readily be accessible, according to the question, which points toward Flexible retrieval. However the utility portion in the answer D is vague.","comment_id":"992106","poster":"Nava702","comments":[{"content":"D will require users to change how they access files, while B only needs the SMB server to mount the gateway, and then the server can read and write files to S3 just like a normal network drive.","poster":"francisizme","upvote_count":"1","comment_id":"1366137","timestamp":"1741320480.0"},{"comment_id":"1095538","content":"file gateway comes with a cache volume on-premise wich ensures the low latency for the most recently accessed files","timestamp":"1702478100.0","upvote_count":"7","poster":"Ander927"}]}],"poster":"SuperDuperPooperScooper","content":"Low-latency access is only required for the first 7 days, B maintains that fast access for 7 days and only then are the files sent to Glacier Archive"}],"content":"Yes it might be vague but how do we keep the low-latency access that only flexible can offer?","poster":"Udoyen","upvote_count":"5","comment_id":"731229"}],"poster":"Sinaneos","content":"Answer directly points towards file gateway with lifecycles, \n\nD is wrong because utility function is vague and there is no need for flexible storage."},{"timestamp":"1726819020.0","upvote_count":"36","poster":"javitech83","content":"Selected Answer: B\nB answwer is correct. low latency is only needed for newer files. Additionally, File GW provides low latency access by caching frequently accessed files locally so answer is B","comment_id":"734975"},{"upvote_count":"1","timestamp":"1739460900.0","poster":"ak240519","comment_id":"1356185","content":"Selected Answer: B\nFile Gateway helps with lower latency and the files after 7 days should be moved to Glacier as they are no longer being accessed."},{"comment_id":"1350826","content":"Selected Answer: B\nB extends the storage with s3 gateway and meets lifecycle policy requirements","poster":"adamatic","timestamp":"1738578960.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"1289456","timestamp":"1727355540.0","poster":"Sandy4v","content":"B is correct"},{"upvote_count":"5","timestamp":"1726904220.0","poster":"HayLLlHuK","comment_id":"758151","content":"The same question and answer explanation exists in a Udemy course.\nCorrect answer is B.\nAmazon S3 File Gateway provides a seamless way to connect to the cloud to store application data files and backup images as durable objects in Amazon S3 cloud storage. Amazon S3 File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.\nIt can be used for on-premises data-intensive Amazon EC2-based applications that need file protocol access to S3 object storage. Lifecycle policies can then transition the data to S3 Glacier Deep Archive after 7 days.\n\nD is wrong because is involves too much extra configuration which is unnecessary."},{"content":"Selected Answer: B\nOption B, creating an Amazon S3 File Gateway and an S3 Lifecycle policy to transition data to S3 Glacier Deep Archive, would meet the requirements specified in the prompt.\n\nThe S3 File Gateway allows you to store and retrieve objects in Amazon S3 using standard file system protocols, such as SMB and NFS. This would provide additional storage space for the company's data and allow for low-latency access to the most recently accessed files, as the data would still be stored on the SMB file server.","upvote_count":"6","comments":[{"content":"Option A, using AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS, would not provide additional storage space for the company's data and would not allow for low-latency access to the most recently accessed files. \n\nOption C, creating an FSx for Windows File Server file system, would provide additional storage space but would not include file lifecycle management. \n\nOption D, installing a utility on each user's computer to access Amazon S3 and creating an S3 Lifecycle policy, would not provide additional storage space on the company's file server.","comment_id":"758979","upvote_count":"5","timestamp":"1672170900.0","poster":"Buruguduystunstugudunstuy"}],"poster":"Buruguduystunstugudunstuy","comment_id":"758978","timestamp":"1726904220.0"},{"poster":"gx2222","upvote_count":"3","timestamp":"1726904220.0","content":"Selected Answer: B\nExplanation:\n\nSince the company needs to increase available storage space while maintaining low-latency access to recently accessed files and implement file lifecycle management to avoid future storage issues, the best solution is to use Amazon S3 with a File Gateway.\n\nUsing an Amazon S3 File Gateway, the company can access its SMB file server through an S3 bucket. This provides low-latency access to recently accessed files by caching them on the gateway appliance. The solution also supports file lifecycle management by using S3 Lifecycle policies to transition files to lower cost storage classes after they haven't been accessed for a certain period of time.\n\nIn this case, the company can create an S3 Lifecycle policy to transition files to S3 Glacier Deep Archive after 7 days of not being accessed. This would allow the company to store large amounts of data at a lower cost, while still having easy access to recently accessed files.","comment_id":"859073"},{"upvote_count":"12","comment_id":"860755","content":"Selected Answer: B\nKeywords:\n- After 7 days the files are rarely accessed.\n-The total data size is increasing and is close to the company's total storage capacity.\n- Increase the company's available storage space without losing low-latency access to the most recently accessed files. -> (for rarely accessed files we can access it with high-latency)\n- Must also provide file lifecycle management to avoid future storage issues.\n\nA: Incorrect - Don't mention how to increase company's available storage space.\nB: Correct - extend storage space and fast access with S3 File Gateway (cache recent access file), reduce cost and storage by move to S3 Glacier Deep Archive after 7 days.\nC: Incorrect - Didn't handle file lifecycle management.\nD: Incorrect - Don't mention about increase the company's available storage space.","poster":"PhucVuu","timestamp":"1726904220.0"},{"poster":"McLobster","upvote_count":"2","timestamp":"1726904160.0","comment_id":"965846","content":"Selected Answer: A\naccording to documentation the minimum storage timeframe for an object inside S3 before being able to transition using lifecycle policy is 30 days , so those 7 days policies kinda seem wrong to me\n\n\nTransition actions – These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them. For more information, see Using Amazon S3 storage classes. \n\n\nI was thinking of option A using DataSync as a scheduled task? am i wrong here?"},{"content":"A. DataSync is focused on transferring data when we need synchronization (for example, from an on-premises DB that updates daily to a DB in AWS). In this case, we don’t need to transfer data or synchronize data, we only need to increase the storage. So, this option is not correct.\nB. S3 File Gateway allows communication between a File System/Server and S3, and it supports SMB protocol. We can use S3 FGW to move files from the FS to the S3 bucket. Then, the question says that files older than seven days are rarely accessed, so we can transition to S3 Glacier for those files to archive, in a costly-efficient way. So, this option is correct.\nC. You have two different storages for saving the files, making the interoperability unnecessarily more complex (and you need constant data synchronization, regarding to option A). \nD. This is a mess. The solution is not scalable and It depends on the number of users, which is not a static number. Additionally, Flexible Retrieval is unnecessary.","upvote_count":"5","comment_id":"1117343","timestamp":"1726904160.0","poster":"Andreshere"},{"content":"Selected Answer: B\nanswwer is correct. low latency is only needed for newer files. Additionally, File GW provides low latency access by caching frequently accessed files locally so answer is B","upvote_count":"1","comment_id":"1254055","timestamp":"1726819020.0","poster":"akshay243007"},{"timestamp":"1724694300.0","content":"Selected Answer: B\nB is correct","comment_id":"1272935","poster":"CHM3","upvote_count":"1"},{"comment_id":"1263427","poster":"PaulGa","content":"Ans B: Low latency is only required for recent files in last 7 days; the rest can effectively be archived.","upvote_count":"1","timestamp":"1723284900.0"},{"comment_id":"1261009","content":"I'm new to this site, B is obviously correct, so how can D be shown as the \"Correct Answer\"? Makes no sense..","timestamp":"1722851220.0","upvote_count":"2","poster":"Sebbster"},{"poster":"WMF0187","upvote_count":"1","content":"B:\n\nExplanation:\nAmazon S3 File Gateway: This service provides a seamless and secure integration between on-premises environments and the Amazon S3 cloud storage, allowing users to store and retrieve objects in S3 using the standard file protocols.\nS3 Lifecycle policy: By creating a lifecycle policy, you can automatically transition data that is older than 7 days to a more cost-effective storage class like S3 Glacier Deep Archive, which is suitable for infrequently accessed data.","comment_id":"1246743","timestamp":"1720787640.0"},{"poster":"aquarian_ngc","timestamp":"1720748340.0","comment_id":"1246417","upvote_count":"1","content":"Best answer opt C"},{"content":"Selected Answer: B\nFile gateway has a cache and when hybrid is needed always think a storage gateway solution","upvote_count":"1","comment_id":"1245997","timestamp":"1720684500.0","poster":"jhg96"},{"comment_id":"1236554","content":"Selected Answer: B\nOption B: Use an Amazon S3 File Gateway to extend storage space and create an S3 Lifecycle policy to transition data to S3 Glacier Deep Archive after 7 days.\n\nThis solution provides seamless storage expansion, low-latency access for recent files, and cost-effective lifecycle management for older data.","timestamp":"1719254940.0","poster":"williamsmith95","upvote_count":"1"},{"content":"B is the correct answer","timestamp":"1719229860.0","comment_id":"1236303","poster":"chirag_a_parikh","upvote_count":"1"},{"content":"Selected Answer: B\nB for sure","comment_id":"1224548","upvote_count":"1","poster":"ChymKuBoy","timestamp":"1717567920.0"},{"poster":"shil_31","upvote_count":"2","timestamp":"1717302240.0","comment_id":"1223017","content":"Selected Answer: B\nAmazon S3 File Gateway: By deploying an S3 File Gateway, the company can extend its storage space by leveraging Amazon S3 as a scalable and durable storage solution. The gateway provides seamless access to S3 objects as files, allowing users to continue accessing data using standard file protocols like SMB.\n\nS3 Lifecycle Policy: By creating an S3 Lifecycle policy, the company can automatically transition data that is older than 7 days to Amazon S3 Glacier Deep Archive. This allows the company to reduce storage costs for infrequently accessed data while maintaining access to the most recently accessed files with low latency."},{"upvote_count":"1","timestamp":"1716423300.0","comment_id":"1216104","content":"Selected Answer: B\nBy creating an Amazon S3 File Gateway, the company can seamlessly extend its storage space while ensuring low-latency access to the most recently accessed files. Additionally, by implementing an S3 Lifecycle policy to transition data to S3 Glacier Deep Archive after 7 days, the company can effectively manage its storage resources and avoid future storage issues.","poster":"OBIOHAnze"},{"timestamp":"1713603000.0","upvote_count":"1","content":"Selected Answer: B\nB is the correct Answer","comment_id":"1199026","poster":"Vjshinde"},{"timestamp":"1712440080.0","poster":"soufiyane","upvote_count":"1","content":"Selected Answer: B\nB is the answer\nAws storage is used for hybird storage rather than using datasync whoch is for data transfer, also it mentions data needs lifecycle managment","comment_id":"1190629"},{"content":"Hint: SMB files : You can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB).","poster":"smalipeddi","timestamp":"1712423220.0","comment_id":"1190521","upvote_count":"2"},{"comment_id":"1182924","timestamp":"1711419000.0","poster":"TruthWS","upvote_count":"1","content":"B is correct because we need gateway, its data center mean it located on-premises"},{"poster":"ManikRoy","timestamp":"1711377780.0","comment_id":"1182574","content":"Selected Answer: B\nS3 File gateway with lifecycle policy for auto transition of old files.","upvote_count":"1"},{"comment_id":"1173231","timestamp":"1710404280.0","comments":[{"poster":"hro","timestamp":"1710898800.0","content":"AWS would not have you install a utility do something a service can do. The answer is B.","comment_id":"1177873","upvote_count":"4"}],"poster":"Liam_W","upvote_count":"4","content":"Selected Answer: D\nInitially when reading this question, i wanted to use S3 Inteligiant Tiering as the answer but it was not mention.\nI was then torn between B and D due to the life cycle policies that was required as stated by the question: I chose D because it covers the need to be accessed whenever, without reducing the latency. This answer is also the best choice as it would reduce any storage issues users could face."},{"poster":"cygao","comment_id":"1162364","content":"Selected Answer: B\nabsolutely B","upvote_count":"1","timestamp":"1709197920.0"},{"timestamp":"1709143680.0","comment_id":"1161892","content":"Selected Answer: B\nAmazon s3 file gateway is a solution to transfer data from storage on prem to storage in cloud and it supports SMB, ISCSI and NFS protocols so B is the correct answer.","upvote_count":"2","poster":"Rosy92"},{"upvote_count":"1","content":"Selected Answer: B\nB innit","comment_id":"1150815","poster":"cryptics","timestamp":"1707985740.0"},{"comment_id":"1143432","poster":"NachoDevOps","content":"Selected Answer: B\ngateway to connect on premise with cloud and lifecycle to prevent space issues in the future .","timestamp":"1707317940.0","upvote_count":"1"},{"upvote_count":"3","timestamp":"1705171560.0","comment_id":"1121969","content":"Selected Answer: B\nProblems: SMB + Out of space + require low latency access to recent file + 7 day old files can go in archive\nA: Can work but not a continuous process so storage problems not solved\nC: FSx needs to connect to something on other side to transfer data. This is not specified here\nD: There is not data on user computer.\nB File gateway is for extending on-prem to S3.","poster":"awsgeek75"},{"upvote_count":"1","content":"Selected Answer: B\nB seems correct","timestamp":"1705149000.0","comment_id":"1121616","poster":"A_jaa"},{"upvote_count":"1","content":"Selected Answer: B\nAfter 7 days the files are rarely accessed. Hence, B","poster":"thewalker","timestamp":"1704434640.0","comment_id":"1114300"},{"poster":"Debabrata1234","timestamp":"1703176080.0","content":"Selected Answer: D\ntest the prep","upvote_count":"1","comment_id":"1102737"},{"content":"Selected Answer: D\nB is the cheapest option, but I think the question is kind of vague and can be tricky because they specify that they need access to the most recently accessed files not the files that have been recently created, which points to the need of some flexible retrieval. Still I'll have to agree with some other comments regarding that the question directs to file gateway, but since the option is not available I would go for the D option.","upvote_count":"1","comment_id":"1096892","poster":"riadS","timestamp":"1702594620.0"},{"timestamp":"1701752940.0","upvote_count":"2","content":"Selected Answer: D\nNeed low latency to the most recently accessed files, not most recent created files. Also, retrieving files from Deep Archive is more complex.","poster":"zkaian","comment_id":"1088196"},{"comment_id":"1066216","content":"Selected Answer: B\nB is the correct answer again here. Why dont the moderator give some reasoning for the community's Most Voted answer a proper review and correct themselves or provide better reasoning ? Whoever agrees please upvote this.Thanks","upvote_count":"13","poster":"nathanss","timestamp":"1699515300.0"},{"timestamp":"1699110840.0","upvote_count":"1","comment_id":"1062201","poster":"pabloveintimilla","content":"Selected Answer: B\nWith File gateway can apply lifecycles"},{"poster":"chrisExam","timestamp":"1698663300.0","upvote_count":"1","comment_id":"1057570","content":"Selected Answer: B\nD. Installer un utilitaire sur chaque ordinateur de l'utilisateur n'est pas une solution évolutive et peut entraîner des problèmes de gestion et de compatibilité.bien que S3 Glacier Flexible Retrieval puisse être une option valable pour le stockage à long terme, cette solution ne prend pas en compte l'accès à faible latence aux fichiers les plus récents ou la gestion automatique du cycle de vie.\nParmi ces options, la meilleure solution qui répond aux exigences est :\nB."},{"comment_id":"1022995","content":"why the website always choose the wrong answer -__-","timestamp":"1696241520.0","poster":"Emanv","upvote_count":"3"},{"comments":[{"upvote_count":"1","content":"about automated processes you are right, but filegateway is used for migrations! there is no reference or keyword here that refers to a migration","poster":"cris93","comment_id":"1082386","timestamp":"1701165600.0"}],"content":"Installing a utility on each user computer is a MANUAL process. AWS will always favor an AUTOMATED process over a manual process. the correct answer is B.","comment_id":"1019437","upvote_count":"2","poster":"lowkey07","timestamp":"1695877860.0"},{"timestamp":"1695742740.0","comment_id":"1017936","upvote_count":"3","content":"Selected Answer: B\nption B: Amazon S3 File Gateway provides a hybrid cloud storage solution, integrating on-premises environments with cloud storage. Files written to the file share are automatically saved as S3 objects. With S3 Lifecycle policies, you can transition objects between storage classes. Transitioning to Glacier Deep Archive is suitable for rarely accessed files. This solution addresses both the storage capacity and lifecycle management requirements.","poster":"M0SHE"},{"upvote_count":"2","poster":"pakut2","timestamp":"1695370980.0","comments":[{"upvote_count":"4","content":"I was wrong. Turns out you are able to transition from standard to Glacier after just 7 days. I can't find it explicitly stated anywhere, but it works in the console. My confusion is from the `minimum storage duration` metric. As I understand it now, you are able to delete/move objects before minimum storage duration is exceeded, but you pay for the entire period nonetheless. So it's doable, but not cost effective. Anyway, it does not apply here since S3 Standard storage class does not have a minimum storage duration constraint. So, I would go with B. Both B and D achieve the same thing, but B is way simpler to setup and maintain","comment_id":"1013987","timestamp":"1695379860.0","poster":"pakut2"}],"content":"Selected Answer: A\nA is the only answer meeting the requirements. B and D are incorrect, since minimum storage duration needed for an object to be moved into Glacier is 90 days, not 7 https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-compare. C does not provide lifecycle management, which is a part of the requirements","comment_id":"1013828"},{"comment_id":"1012747","poster":"LR2023","content":"Selected Answer: A\nA\n\nwe caanot move objects to S3 glacier within 7 days","timestamp":"1695263100.0","upvote_count":"2"},{"poster":"sanjay_cloud_guy","timestamp":"1695009000.0","content":"D Correct answer.\nkeywords->low-latency is required for the most recently accessed files which will be from glacier having low latency\nutility will run as from multiple systems connected to SMB server to do the transfer.","comment_id":"1010222","upvote_count":"1"},{"upvote_count":"1","comment_id":"1003868","poster":"dbs6339","content":"Selected Answer: D\nExactly, what we needs to focus on that is increase the company's available storage space not total storage space with not losing the low-laytency access.\n\nAnswer D is the more exact. \n\nB/D both answer can be make more available storage space after 7 days send to the S3 galacier but B will losing access about most recently accessed files with the low-latency access.","timestamp":"1694337480.0"},{"upvote_count":"1","poster":"reema908516","content":"Selected Answer: B\nB answwer is correct","comment_id":"1003596","timestamp":"1694309400.0"},{"content":"Can anyone tell me what the test is going to use as the correct answer? I don't want to go into the test and just answer the ones that majority voted for but really they don't have it as the right answer. BTW the discussions are awesome and helps you learn a lot from other peoples intellect.","upvote_count":"3","comment_id":"983990","timestamp":"1692305880.0","poster":"BillyBlunts"},{"comment_id":"979839","poster":"Fresbie99","timestamp":"1691912100.0","content":"Acc to the question we need lifecycle policy and file gateway - Also the S3 flexible retrieval is not required here, Deep archive is the solution.\nhence B is correct","upvote_count":"1"},{"content":"Selected Answer: B\nIncrease the company's available storage space without losing low-latency access to the most recently accessed files = Amazon S3 File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. \nProvide file lifecycle management = S3 Lifecycle policy.","upvote_count":"2","poster":"TariqKipkemei","timestamp":"1690516500.0","comment_id":"965262"},{"comment_id":"963895","poster":"Zoro_","timestamp":"1690385400.0","content":"It says rarely accessed so Flexible retrieval can be a better option please clarify on this \nfor deep retrieval, it takes about 12 hours (maybe less than too)","upvote_count":"1"},{"content":"Selected Answer: B\nB answwer is correct.","poster":"hakim1977","comment_id":"962380","timestamp":"1690264560.0","upvote_count":"1"},{"poster":"Guru4Cloud","comment_id":"956804","upvote_count":"1","timestamp":"1689783240.0","content":"Selected Answer: B\nThe most appropriate solution for the given requirements would be:\n\nB. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days."},{"content":"Selected Answer: D\nThe question says, after 7 days the file are \"rarely accessed\", which means there are still possibilities that these filed would be needed in short period of time. Glacier Flexible Retrieval would fit this requirement.","timestamp":"1689288120.0","upvote_count":"3","poster":"premnick","comment_id":"951023"},{"timestamp":"1689226920.0","poster":"Kaab_B","upvote_count":"1","comment_id":"950364","content":"Selected Answer: B\nThe solution that will meet the requirements of increasing available storage space, maintaining low-latency access to recently accessed files, and providing file lifecycle management is option B: Create an Amazon S3 File Gateway and use an S3 Lifecycle policy to transition data to S3 Glacier Deep Archive after 7 days."},{"upvote_count":"1","timestamp":"1689177660.0","poster":"miki111","content":"Option B MET THE REQUIREMENT","comment_id":"949973"},{"content":"Selected Answer: B\nfile gateway is correct.","timestamp":"1688377800.0","comment_id":"941676","upvote_count":"1","poster":"animefan1"},{"timestamp":"1687879740.0","content":"Selected Answer: B\nA. Isn't a good option because it doesn't address the cost-saving requirement\nB. Correct option, but there are no requirements specified about the access pattern of the data. So S3 AI might be preferable here. \nC. This one is not possible for an on-prem solution.\nD. Vierd option, I cannot google this tool. Or we are talking about AWS CLI and customization scripts... I won't go with it.","comment_id":"935545","poster":"USalo","upvote_count":"1"},{"poster":"cookieMr","content":"Selected Answer: B\nOption B: Creating an Amazon S3 File Gateway is the recommended solution. It allows the company to extend their storage space by utilizing Amazon S3 as a scalable and durable storage service. The File Gateway provides low-latency access to the most recently accessed files by maintaining a local cache on-premises. The S3 Lifecycle policy can be configured to transition data to S3 Glacier Deep Archive after 7 days, which provides long-term archival storage at a lower cost. This approach addresses both the storage capacity issue and the need for file lifecycle management.","comment_id":"926518","upvote_count":"2","timestamp":"1687074360.0"},{"content":"This question is tricky...Because, the B and D sounds good, however, you can not transition to S3 Glacier after 7 days, you have to wait at least 90 days for the D case, and 180 Days for the B...the A and C option are more suitable, but the thing is, the C option not mention about lifecycle policy and the A option there is not a thing to extend the storage...Then....I think the answer should be the A, because you are doing like a back up","upvote_count":"2","comment_id":"915649","poster":"hypnozz","timestamp":"1685987640.0"},{"comment_id":"912199","timestamp":"1685631660.0","content":"Selected Answer: B\nOption B is right answer.","poster":"Bmarodi","upvote_count":"1"},{"timestamp":"1685484900.0","comment_id":"910617","poster":"GR0407","upvote_count":"1","content":"Selected Answer: B\nB is the correct answer"},{"poster":"LabLab","upvote_count":"1","comment_id":"907164","content":"B: correct, D:wrong ( why should install a utility to access S3? I have never seen this. The purpose from this question is how to find a solution (S3 Gateway) and use the fileshare and then lifecycycle policy.","timestamp":"1685086920.0"},{"upvote_count":"1","comment_id":"902512","timestamp":"1684578240.0","poster":"abhishek2021","content":"Selected Answer: B\nFile gateway provides caching and low latency access to the S3 bucket. S3 lifecycle policy can transfer file to Glacier after 7 days."},{"poster":"PrasanthVarada","timestamp":"1684401840.0","upvote_count":"1","content":"D is the right answer. In Question it says that files are accessed very rarely after 7 Days. But however, the main requirement is not to loose low-latency. So \"S3 Glacier Flexible Retrieval\" is the right answer.\nB is wrong answer for another reason - S3 File gateway cannot access Glacier.","comment_id":"900982"},{"content":"B is the correct answer , recently added items are cached at on-premesis while other can be stored at s3.","timestamp":"1684385880.0","poster":"nishant_pruthi","comment_id":"900736","upvote_count":"1"},{"timestamp":"1684046760.0","content":"Selected Answer: B\nB is correct","poster":"cheese929","upvote_count":"1","comment_id":"897322"},{"upvote_count":"1","poster":"darkknight23","timestamp":"1682544000.0","comment_id":"882030","content":"Selected Answer: B\nB it is."},{"timestamp":"1681293600.0","upvote_count":"2","comment_id":"868142","poster":"lawren","content":"Selected Answer: D\nwithout losing low-latency is key to answering the question."},{"timestamp":"1680416580.0","upvote_count":"3","comment_id":"858570","poster":"channn","content":"Selected Answer: B\nB: lower latency by accessing caching on local and life cycle for files after 7 days in S3"},{"upvote_count":"1","content":"Selected Answer: B\nB is correct. What I have problem with D is that.. what the heck is utility to access S3? Did you mean an URL?","poster":"acuaws","timestamp":"1680304500.0","comment_id":"857464"},{"poster":"arakokeba","comment_id":"852526","upvote_count":"1","timestamp":"1679955840.0","content":"Selected Answer: B\nB is correct to me."},{"content":"D is definitely the correct answer for this question. Answer D is the only one that meet this requirement After 7 days the files are rarely accessed.","comment_id":"849777","poster":"topdog","upvote_count":"4","timestamp":"1679715060.0"},{"content":"Selected Answer: B\nB good for me","timestamp":"1679556780.0","poster":"tienhoboss","comment_id":"847939","upvote_count":"1"},{"poster":"saransh_001","comment_id":"844944","timestamp":"1679319420.0","content":"Selected Answer: B\nB seems to be correct","upvote_count":"1"},{"comment_id":"836713","timestamp":"1678594740.0","upvote_count":"3","poster":"sanking","content":"Selected Answer: D\nWhy B is not correct? \nCreate an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.\nI think “ S3 Glacier Deep Archive” is not correct."},{"upvote_count":"1","poster":"sunnyninja","comment_id":"833401","content":"Selected Answer: B\nB is correct","timestamp":"1678313400.0"},{"poster":"suraj2045","upvote_count":"1","comment_id":"826537","timestamp":"1677737700.0","content":"B IS RIGHT"},{"timestamp":"1677294120.0","content":"Selected Answer: B\nIt's B, but half the answers incorrect. you have to hold the files for 30 days prior to transitioning any s3 life cycle policy","poster":"KittieHearts","comments":[{"content":"You don't have to. 30 days is duration of storage of particular object after moving it. You're confirming, that you're aware of storing object for minimum 30 days in the new storage class. In this case is respective only to Storage IA and Storage IA One-Zone","poster":"jakubzajac","timestamp":"1677956220.0","upvote_count":"1","comment_id":"829275"}],"upvote_count":"3","comment_id":"821126"},{"comment_id":"819291","content":"Selected Answer: B\nWithout losing low-latency access to the most recently accessed files. is the key point","timestamp":"1677162600.0","poster":"bilel500","upvote_count":"1"},{"content":"B can't be a solution because it takes 12-48 hours for retrieval for glacier deep dive archive.","comment_id":"814922","poster":"habibi03336","upvote_count":"2","timestamp":"1676877240.0"},{"timestamp":"1676272920.0","poster":"buiducvu","upvote_count":"1","comment_id":"807137","content":"Selected Answer: B\nB answwer is correct"},{"content":"Letter B\nA solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.\n\n\nDont make sense each user upload the file if you already have a server, the latency is the same.","upvote_count":"1","timestamp":"1676208840.0","poster":"MichaelCarrasco","comment_id":"806370"},{"poster":"devg198","timestamp":"1675789800.0","content":"Selected Answer: B\nwithout losing low-latency access to the most recently accessed files. is the key point\n\nAnd Option: D is wrong because installing utility on each Amazon S3 doesn't make sense.","upvote_count":"1","comment_id":"801163"},{"poster":"CaoMengde09","upvote_count":"2","content":"B is the most viable solution. But still the second half of the answer is wrong, because you need at least to store for 30 days data in S3 before transitioning to S3 Glacier Deep Archive","comment_id":"795833","comments":[{"comment_id":"821055","comments":[{"content":"When it said \"after 7 days the files are rarely access\" ... it's a McGuffin ... staying in S3 is not a problem the files would be stored in, for example, the Standart Class for 30 days. It's very cheap. If no one access the file in days 10, 11 , ... to 30, there's no problem. After 30 days, to save money, you can move to a cheaper S3 class.\nSo, no confused with it.","timestamp":"1680037140.0","comment_id":"853700","upvote_count":"2","poster":"jdr75"}],"timestamp":"1677285120.0","upvote_count":"1","content":"That's why I was also confused","poster":"KittieHearts"}],"timestamp":"1675322220.0"},{"upvote_count":"6","content":"Selected Answer: B\nD is Dead wrong\n\"install a utility on each user's computer\" doesn't seem logical...what if there are 99999999 users?","poster":"MoMoCh4n","timestamp":"1673567640.0","comment_id":"773956"},{"poster":"hahahumble","timestamp":"1673399940.0","comment_id":"771939","upvote_count":"2","content":"Selected Answer: B\nB is correct"},{"comments":[{"content":"I SUSPECT IT'S A LEGAL ISSUE, SOME SO OBVIOUSLY WRONG ! CONFUSES ONE.","upvote_count":"4","poster":"LuckyAro","comment_id":"771713","timestamp":"1673377440.0"}],"content":"I AM QUITE SUSPICIOUS WHY EXAMTOPICS ARE FIELDING OUT WRONG ANSWERS!???","poster":"shirleyson123","comment_id":"766659","upvote_count":"8","timestamp":"1672925340.0"},{"upvote_count":"2","content":"I am very confused between the 2 answers( B and D) => cuz - \"A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files\". and the S2 glacier deep can not provide low latency access.","poster":"Zerotn3","comments":[{"upvote_count":"1","comment_id":"869661","poster":"Robrobtutu","timestamp":"1681410060.0","content":"The files that go on Glacier are the old files, the new files, the ones you need low latency on, are not on Glacier."}],"comment_id":"755506","timestamp":"1671955800.0"},{"comment_id":"746321","poster":"yoben84","content":"It can't be response B because data cannot be put directly in Glacier it has to transition first in standard storage.","timestamp":"1671122580.0","upvote_count":"2"},{"comment_id":"736562","upvote_count":"3","content":"Selected Answer: B\nAnswer is B","poster":"BhushanMapuskar","timestamp":"1670307180.0"},{"content":"Selected Answer: B\nAnswer B make scene and satisfy the requirements . Answer D is too vague and not scalable.","timestamp":"1670222340.0","comment_id":"735704","poster":"AlaN652","upvote_count":"2"},{"upvote_count":"1","comment_id":"723472","poster":"Wpcorgan","content":"B is correct for me","timestamp":"1669033920.0"},{"poster":"ABCMail","comment_id":"721795","timestamp":"1668838260.0","upvote_count":"2","content":"Selected Answer: B\nUsing aws S3 gateway seems to be a logical option"},{"timestamp":"1668677040.0","comments":[{"timestamp":"1669710120.0","content":"No, you misread the question, the requirement is \"without losing low-latency access to the most recently accessed files\". Since files accessed in the last 7 days will not be transitioned to Glacier Deep Archive, they will still be available \"without losing low-latency access\" if B is selected.","poster":"JayBee65","upvote_count":"6","comment_id":"730094"}],"comment_id":"720359","upvote_count":"1","poster":"digitalsee","content":"Selected Answer: D\nAfter 7 days the files are rarely accessed, so they are still accessed and still need access within milliseconds. So B can't be correct.\nInstalling S3 clients on Clients is a weird solution but the others simply don't fit the case."},{"comments":[{"comment_id":"735702","poster":"AlaN652","content":"The question says \" without losing low-latency access to the most recently accessed files. \" this can be achieved by cashing most frequently accessed files in the file GW.","timestamp":"1670222280.0","upvote_count":"3"}],"timestamp":"1668577800.0","upvote_count":"3","content":"Selected Answer: D\nI think the answer is D because the question said without losing low-latency access.With glacier flexible retrieval, data can but access within 1-5 mins unlike glacier deep archival, it take 12 hours to access your data, 12 hours would defeat trying to get a low- latency access","comment_id":"719352","poster":"mikey2000"},{"content":"Selected Answer: D\nBecause it still requires \"low latency access\" and some files older than 7 days are infrequently, BUT still accessed only Flexible retrieval can retrieve the file in minutes. Deep archive can't. The other answers(A and C) are not any good.","poster":"Mihai7","upvote_count":"2","timestamp":"1668531240.0","comment_id":"718969","comments":[{"content":"Low latency access is for files less than 7 days old. File GW provides low latency access by caching frequently accessed files locally so answer is B.","poster":"Ptopics","upvote_count":"3","comment_id":"722093","timestamp":"1668873180.0"}]},{"timestamp":"1668397800.0","upvote_count":"2","poster":"leonnnn","comment_id":"717679","content":"Selected Answer: B\nA S3 gateway provides smb for end users and stores file in S3 so that it can use lifecycle management to transit old files."},{"timestamp":"1668279360.0","poster":"akellaaws","content":"Selected Answer: B\nUtility is not necessary","upvote_count":"1","comment_id":"716876"},{"timestamp":"1667956500.0","upvote_count":"2","content":"Selected Answer: B\nFile Gateway will be the best bet to replace the SMB storage server","poster":"Azeeza","comment_id":"714199"},{"timestamp":"1667920740.0","comment_id":"713922","poster":"VijayMeh","content":"Selected Answer: B\nB is the right answer","upvote_count":"1"},{"upvote_count":"1","timestamp":"1667612580.0","poster":"gokalpkocer3","comment_id":"711483","content":"Selected Answer: B\nlifecycle therefore B."},{"comment_id":"706998","upvote_count":"1","timestamp":"1667023260.0","poster":"keezbadger","content":"D is tricky. Though the questions says that \"A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files.\" Which falls under the category of S3 Glacier flexible retrieval. Amazon S3 File Gateway provides a seamless way to connect to the cloud in order to store application data files and backup images as durable objects in Amazon S3 cloud storage. Amazon S3 File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. Therefore :\nB is the right answer."},{"comment_id":"706793","poster":"17Master","timestamp":"1666997580.0","content":"Selected Answer: B\nAns is correct","upvote_count":"1"},{"content":"Selected Answer: B\nI go with B with this one,\nvolume gateway should support SMB, and with cache volume you can cache most recently accessed files locally for minimum latency","upvote_count":"1","timestamp":"1666620360.0","comment_id":"703090","poster":"Six_Fingered_Jose","comments":[{"timestamp":"1666620420.0","content":"oh wait this is about filegateway, ignore my comment but answer is still B","poster":"Six_Fingered_Jose","upvote_count":"1","comment_id":"703091"}]},{"timestamp":"1665543300.0","upvote_count":"4","content":"Selected Answer: B\nAmazon S3 File Gateway supports either SMB or NFS protocol","poster":"BoboChow","comment_id":"692585"},{"timestamp":"1665402180.0","content":"Selected Answer: B\nThe only answer which supports lifecycles","poster":"pooppants","comment_id":"691075","upvote_count":"1"},{"timestamp":"1665397920.0","poster":"D2w","upvote_count":"2","content":"Selected Answer: B\nB, amazon S3 File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. Then lifecycle","comment_id":"691009"},{"content":"Selected Answer: B\nAfter 7 day the files are not used","upvote_count":"2","comment_id":"690844","poster":"Rock08","timestamp":"1665388560.0"}],"answer_images":[]},{"id":"QTxI8316eS2TrjKNUg50","choices":{"D":"Use Amazon ElastiCache to cache the common queries that the script runs against the database.","C":"Instruct the development team to manually export the entries in the database at the end of each day.","A":"Modify the DB instance to be a Multi-AZ deployment.","B":"Create a read replica of the database. Configure the script to query only the read replica."},"answer_images":[],"discussion":[{"comment_id":"696424","poster":"alvarez100","upvote_count":"45","timestamp":"1665942480.0","content":"Selected Answer: B\nElasti Cache if for reading common results. The script is looking for new movies added. Read replica would be the best choice."},{"timestamp":"1667826900.0","poster":"Gil80","content":"Selected Answer: B\n• You have a production DB that is taking on a normal load\n • You want to run a reporting application to run some analytics\n • You create a read replica to run the new workload there\n • The prod application is unaffected\n • Read replicas are used for SELECT (=read) only kind of statements\nTherefore I believe B to be the better answer.\n\nAs for \"D\" - ElastiCache use cases are:\n1. Your data is slow or expensive to get when compared to cache retrieval.\n2. Users access your data often.\n3. Your data stays relatively the same, or if it changes quickly staleness is not a large issue.\n\n1 - Somewhat true.\n2 - Not true for our case.\n3 - Also not true. The data changes throughout the day.\n\nFor my understanding, caching has to do with millisecond results, high-performance reads. These are not the issues mentioned in the questions, therefore B.","comment_id":"713059","upvote_count":"18","comments":[{"content":"I will support this by point to the question : \" with the LEAST operational overhead?\" \n\nConfiguring the read replica is much easier than configuring and integrating new service.","poster":"NitiATOS","timestamp":"1675080300.0","upvote_count":"5","comment_id":"792714"}]},{"timestamp":"1736011740.0","upvote_count":"1","content":"Selected Answer: B\nRead replicas are replications of original databases and are the most suitable option for running scripts here.","comment_id":"1336464","poster":"satyaammm"},{"upvote_count":"3","timestamp":"1726406220.0","content":"Selected Answer: B\nAns B - as per Gil80 (1 year, 10mth ago)\n\"Selected Answer: B\n• You have a production DB that is taking on a normal load\n • You want to run a reporting application to run some analytics\n • You create a read replica to run the new workload there\n • The prod application is unaffected\n • Read replicas are used for SELECT (=read) only kind of statements\nTherefore I believe B to be the better answer.\n\nAs for \"D\" - ElastiCache use cases are:\n1. Your data is slow or expensive to get when compared to cache retrieval.\n2. Users access your data often.\n3. Your data stays relatively the same, or if it changes quickly staleness is not a large issue.\n\n1 - Somewhat true.\n2 - Not true for our case.\n3 - Also not true. The data changes throughout the day.\n\nFor my understanding, caching has to do with millisecond results, high-performance reads. These are not the issues mentioned in the questions, therefore B.\"","comment_id":"1284138","poster":"PaulGa"},{"content":"Selected Answer: B\noption B for sure, read replica is designed for this very use case, improving performance, on the other hand, enabling multi-AZ improves availability and not performance.","poster":"jaradat02","comment_id":"1252421","timestamp":"1721563320.0","upvote_count":"2"},{"comment_id":"1191808","upvote_count":"3","content":"Selected Answer: B\nA. Having multi-AZ database would increase availability, bu not performance.\nC. Not practical. Huge Operational Overhead. (Solution should be LEAST operational overhead)\nD. Good for fixed queries with fixed results. not a good fit in this case as script is looking for new results in DB. It has to scan the database.\n\nCorrect Answer B. Read replica ensure you have dedicated read instance with its own resources.","poster":"MehulKapadia","timestamp":"1712606820.0"},{"upvote_count":"3","poster":"awsgeek75","timestamp":"1705268580.0","comment_id":"1122886","content":"Selected Answer: B\nJust read from read replica. \nA: This will make it HA but won't solve any problems\nC: We want an AWS solution not change the development team's ways of working\nD: Elasticache is cache of read queries when data doesn't change. It's useless for finding new data."},{"comment_id":"1116686","timestamp":"1704724800.0","poster":"bujuman","content":"Selected Answer: A\nAnswer C is inconcevable according to LEAST operational overhead?\nWe will exclude answer D because question is about RDS databases and ElastiCache is not.\nBetween answers A and B , A is the most appropriate answer due to 2 foolowing points:\n- Possible to transfrom a Single-AZ RDS to Multi-AZ\n- LEAST operational overhead","upvote_count":"2"},{"comment_id":"1090331","timestamp":"1701956280.0","poster":"smdrouiss","comments":[{"comment_id":"1105275","poster":"pentium75","upvote_count":"3","timestamp":"1703508180.0","content":"Doesn't it become a multi-instance DB when you add a read replica? A can't be because you can't read from the passive replica of a multi-AZ DB."}],"content":"Selected Answer: A\nIt is A , because the scenario mention \"single db instance\" which is not possible to enable read replica","upvote_count":"1"},{"upvote_count":"7","timestamp":"1698901920.0","comment_id":"1060271","content":"Selected Answer: B\nlol seriously the person who wrote the answer wants us to fail","poster":"slimen"},{"comment_id":"1038921","upvote_count":"1","poster":"tom_cruise","content":"Selected Answer: B\nThis is what we do in the real world.","timestamp":"1696876140.0"},{"poster":"joshik","content":"Selected Answer: B\n- Cached data might not always be up-to-date, so you need to manage cache expiry and invalidation carefully.\n- It may require some code changes to implement caching logic in your script.\n- ElastiCache comes with additional costs, so you should assess the cost implications based on your usage.","timestamp":"1695767880.0","upvote_count":"2","comment_id":"1018228"},{"poster":"underdogpex","content":"Selected Answer: B\nWhy not D:\nWhile ElastiCache can be relatively easy to set up, it still requires ongoing management, monitoring, and potentially scaling as the dataset and query load grow. This introduces operational overhead that may not align with the goal of minimizing operational work.","upvote_count":"2","comment_id":"998105","timestamp":"1693793760.0"},{"comment_id":"994459","content":"the correct answer should be A, you can't create a read replica on a single-AZ DB instance","comments":[{"timestamp":"1720468920.0","poster":"effiecancode","content":"Actualy you can","upvote_count":"1","comment_id":"1244551"}],"timestamp":"1693428960.0","poster":"Router","upvote_count":"1"},{"timestamp":"1692679380.0","comment_id":"987075","poster":"TariqKipkemei","content":"Selected Answer: B\na read replica is always fit for these type of scenarios.","upvote_count":"2"},{"comment_id":"981010","timestamp":"1692035700.0","content":"Selected Answer: B\nThe key requirements are:\n\nThe script must report a final total during business hours\nResolve the issue of inadequate database performance for development tasks when the script is running\nWith the least operational overhead","upvote_count":"1","poster":"Guru4Cloud"},{"poster":"cookieMr","timestamp":"1687369380.0","upvote_count":"5","comment_id":"929785","content":"Selected Answer: B\nA. Modifying the DB to be a Multi-AZ deployment improves high availability and fault tolerance but does not directly address the performance issue during the script execution.\n\nC. Instructing the development team to manually export the entries in the database introduces manual effort and is not a scalable or efficient solution.\n\nD. While using ElastiCache for caching can improve read performance for common queries, it may not be the most suitable solution for the scenario described. Caching is effective for reducing the load on the database for frequently accessed data, but it may not directly address the performance issue during the script execution.\n\nCreating a read replica of the database (option B) provides a scalable solution that offloads read traffic from the primary database. The script can be configured to query the read replica, reducing the impact on the primary database during the script execution."},{"content":"Selected Answer: B\nFor LEAST operational overhead, I recommended to use read replica DB","comment_id":"904632","upvote_count":"2","poster":"MostafaWardany","timestamp":"1684823700.0"},{"upvote_count":"2","poster":"Bmarodi","content":"Selected Answer: B\nThe option B will reduce burden on DB, becase the script will read only from replica, not from DB, hence option B is correct answer.","timestamp":"1684657920.0","comment_id":"903036"},{"timestamp":"1684554240.0","poster":"Siva007","upvote_count":"2","content":"Selected Answer: B\nB is correct. Read replica for read only script any analytical loads.","comment_id":"902341"},{"comment_id":"881630","content":"Selected Answer: B\nB is correct. Run the script on the read replica.","poster":"cheese929","timestamp":"1682513760.0","upvote_count":"2"},{"upvote_count":"2","content":"B:\n read replica would be the best choice","poster":"alexiscloud","timestamp":"1680071880.0","comment_id":"854084"},{"content":"Selected Answer: B\nReason to have a Read Replica is improved performance (key word) which is native to RDS. Elastic Cache may have misses. \n\nThe other way of looking at this question is : Elastic Cache could be beneficial for development tasks (and hence improve the overall DB performance). But then, Option D mentions that the queries for scripts are cached, and not the DB content (or metadata). This may not necessarily improve the performance of the DB. \n\nSo, Option B is the best answer.","poster":"Mahadeva","comment_id":"766125","upvote_count":"2","timestamp":"1672872000.0"},{"timestamp":"1672823640.0","comment_id":"765426","poster":"DavidNamy","upvote_count":"2","content":"Selected Answer: B\nThe correct answer would be option B"},{"poster":"Nandan747","timestamp":"1672231080.0","upvote_count":"4","comment_id":"759816","content":"Selected Answer: B\nD is incorrect. The requirement says LEAST OPERATIONAL OVERHEAD. Here, using Elasticache you need to heavily modify your scripts/code to accommodate Elasticache into the architecture which is higher Operational overhead compared to turning DB into Muti-AZ mode."},{"content":"Selected Answer: B\n***CORRECT***\nThe best solution to meet the requirement with the least operational overhead would be to create a read replica of the database and configure the script to query only the read replica. Option B.\n\nA read replica is a fully managed database that is kept in sync with the primary database. Read replicas allow you to scale out read-heavy workloads by distributing read queries across multiple databases. This can help improve the performance of the database and reduce the impact on the primary database.\n\nBy configuring the script to query the read replica, the development team can continue to use the primary database for development tasks, while the script's queries will be directed to the read replica. This will reduce the load on the primary database and improve its performance.","comments":[{"timestamp":"1671598680.0","content":"***WRONG***\nOption A (modifying the DB instance to be a Multi-AZ deployment) would not address the issue of the script's queries impacting the primary database. \n\nOption C (instructing the development team to manually export the entries in the database at the end of each day) would not be an efficient solution as it would require manual effort and could lead to data loss if the export process is not done properly. \n\nOption D (using Amazon ElastiCache to cache the common queries) could improve the performance of the script's queries, but it would not address the issue of the script's queries impacting the primary database.","upvote_count":"5","comment_id":"751885","poster":"Buruguduystunstugudunstuy"}],"poster":"Buruguduystunstugudunstuy","timestamp":"1671598680.0","comment_id":"751884","upvote_count":"7"},{"poster":"duriselvan","upvote_count":"1","comment_id":"750913","timestamp":"1671541980.0","content":"b is correct \nAmazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora."},{"poster":"yoben84","comment_id":"749870","timestamp":"1671457560.0","upvote_count":"1","content":"D is not reducing operational overhead, since there is development effort to integrate the app to a cache. you have to manage the cluster of the elastic cache"},{"upvote_count":"2","poster":"yoben84","timestamp":"1671457320.0","comment_id":"749865","content":"Selected Answer: D\nIt's a DB instance not managed instance so you can't use a read replica."},{"comment_id":"749863","upvote_count":"1","poster":"juliansierra","timestamp":"1671457260.0","content":"The script makes two tasks. Firsts, the script runs queries RECORD the number of new movies that have been added to the database. In the second task, the script must report a final total. The question ask about how to improve the database behavior when this script is running. I don't know if B is a valid answer because you can not RECORD in a only-write database. But the other 3 options makes no sense for me too. So, it's difficult give a certain answer."},{"poster":"career360guru","content":"Selected Answer: B\nB - Add read replica and run the script against read replica endpoints.","upvote_count":"1","comment_id":"747607","timestamp":"1671226860.0"},{"poster":"shw1981","comment_id":"740058","timestamp":"1670587080.0","upvote_count":"1","content":"Selected Answer: B\nB is correct"},{"content":"Caching works best for static contents. When you run a total, you need to go through all the records in a table. The question is not constructed properly. Best solution is to create an index on the added date, it won't take long, nor heavy io/cpu to get the total number of newly added total for the day. This approach takes minimal effort, does not incur any extra charge, better than both B and D.","comment_id":"726990","timestamp":"1669398900.0","upvote_count":"1","poster":"Vesperia","comments":[{"timestamp":"1669399200.0","poster":"Vesperia","comment_id":"726994","content":"I would choose B as the answer. For the stated type of queries It's better than D .","upvote_count":"1"}]},{"content":"B is correct","comment_id":"723750","upvote_count":"1","timestamp":"1669048200.0","poster":"Wpcorgan"},{"timestamp":"1668678600.0","poster":"htangga","content":"Selected Answer: B\nB is more make sense for me","upvote_count":"1","comment_id":"720378"},{"upvote_count":"1","content":"Selected Answer: B\nNot D as apps have to be re-written to take advantage of elasticache APIs - that is too much overhead.","poster":"Bevemo","comment_id":"713504","timestamp":"1667885340.0"},{"upvote_count":"1","poster":"Cizzla7049","content":"Selected Answer: D\nEven though B is correct, it says least operational overhead which is D. Like the other person said, AWS used similar use cases.","timestamp":"1667671500.0","comment_id":"711935"},{"poster":"Az900500","upvote_count":"1","content":"When you want to free your database from occasional operation overhead, read replica is key ; the operations runs on the replica while the db run continue operation smoothly.. ElasticCahe would have been considered \"If DB has issue before or after running the script\"\n\nB is the answer","timestamp":"1667660280.0","comment_id":"711834"},{"upvote_count":"1","content":"Selected Answer: D\nFor me its D. It says least operation overhead. Its the use case of elastic cache.\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html#:~:text=Speed%20and%20expense,on%20other%0A%09%09%09%09factors","comment_id":"710173","timestamp":"1667439180.0","comments":[{"comment_id":"713503","poster":"Bevemo","upvote_count":"1","content":"Apps have to be re-written to take advantage of elasticache APIs - that is too much overhead.","timestamp":"1667885280.0"},{"comment_id":"713017","poster":"ArielSchivo","timestamp":"1667822700.0","upvote_count":"1","content":"Not sure about that since it says \"Your data stays relatively the same, or if it changes quickly staleness is not a large issue\", and this is a case related to new movies added to the database."}],"poster":"Hunkie"},{"upvote_count":"4","comment_id":"703361","poster":"Six_Fingered_Jose","timestamp":"1666645680.0","content":"Selected Answer: B\nagree with B, the script only reads the data to produce a report and the slowdown only occurs when the script is running, thus having a read replica solves the issue with the least overhead"},{"comment_id":"700557","content":"Selected Answer: B\nbbbbbbbbbbbbbb","poster":"tubtab","timestamp":"1666333620.0","upvote_count":"2"},{"poster":"Chunsli","comment_id":"696464","upvote_count":"2","content":"B seems to make a better sense","timestamp":"1665948120.0"},{"upvote_count":"2","poster":"brushek","timestamp":"1665632580.0","content":"Selected Answer: B\nIt should be read replica.","comment_id":"693570"}],"answer_ET":"B","url":"https://www.examtopics.com/discussions/amazon/view/85339-exam-aws-certified-solutions-architect-associate-saa-c03/","unix_timestamp":1665632580,"question_id":910,"exam_id":31,"answer":"B","question_text":"A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours.\nThe company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue.\nWhich solution will meet this requirement with the LEAST operational overhead?","timestamp":"2022-10-13 05:43:00","isMC":true,"topic":"1","answers_community":["B (95%)","3%"],"question_images":[],"answer_description":""}],"exam":{"id":31,"isBeta":false,"lastUpdated":"11 Apr 2025","isMCOnly":true,"name":"AWS Certified Solutions Architect - Associate SAA-C03","isImplemented":true,"numberOfQuestions":1019,"provider":"Amazon"},"currentPage":182},"__N_SSP":true}