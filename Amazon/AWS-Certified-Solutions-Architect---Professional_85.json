{"pageProps":{"questions":[{"id":"vZaw1ikrbVY51aQ51ESi","discussion":[{"poster":"amog","comment_id":"37900","content":"Answer is A\n\"as fast as possible\" => read replica","upvote_count":"12","timestamp":"1632083880.0"},{"comment_id":"1307230","poster":"student22","timestamp":"1730789760.0","content":"Selected Answer: A\nAnswer: A\nNOT C - Having to manage MySQL on EC2 is too much overhead.","upvote_count":"1"},{"timestamp":"1723735020.0","poster":"amministrazione","content":"A. For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region","upvote_count":"1","comment_id":"1266514"},{"content":"Selected Answer: A\nDefinitely A, the least complex, it is easy to implement the read replicas for all the RDS DBs, the application can read from the read replica. \nD is not good, having to manage the MySQL on EC2, and I don't consider that s3 is important here. 'Logistic optimization'? What is that?!\nFor ones that say that read-replica is regional check this https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RDS_Fea_Regions_DB-eng.Feature.CrossRegionReadReplicas.html","upvote_count":"1","timestamp":"1707155520.0","comment_id":"1141318","poster":"JPA210"},{"upvote_count":"1","content":"Selected Answer: D\nA is not possible because read replica should be in the same region as the master","poster":"tototo12","timestamp":"1704388140.0","comment_id":"1113918"},{"comments":[{"poster":"shammous","timestamp":"1704619560.0","upvote_count":"1","content":"\"but no s3 as in option D, which addresses the logistic optimization requirement.\" How does it address it, my friend? Would it be better for the batch job to query data from different read-replicas from each region in the HQ region, or extract data from files inside S3? It seems to me that the first option is more \"optimal\" and \"fast\"..","comment_id":"1115672"}],"upvote_count":"1","timestamp":"1689249000.0","comment_id":"950624","content":"Selected Answer: D\n:) Are you reading a question at all?\n\nD.\n\nA - possibly, but no s3 as in option D, which address the logistic optimization requirement.\nB - no, as no point to use self managed MySQL on EC2, A better, but don't address the logistic optimization requirement.\nC - no, as A is better offers read replicas, which is better for DB performance, although it's not the ask. The ask is logistic optimization.\nD - possibly not the best to use self managed MySQL on EC2, but s3 usage is a good solution for logistic optimization.\nE - no, Direct Connect makes sense to connect on-premise resources with Cloud. Here, all of the resources are deployed in the Cloud already.","poster":"kondratyevmn"},{"timestamp":"1684450080.0","poster":"rtguru","comment_id":"901540","upvote_count":"1","comments":[{"timestamp":"1689249300.0","upvote_count":"1","poster":"kondratyevmn","content":":) How so? You can't write to the read replica, because it's read only)","comment_id":"950630"}],"content":"Read replica seems to be the quickest way to get data from regions to headquarters. A is the correct answer"},{"poster":"ashii007","comments":[{"comment_id":"879967","upvote_count":"1","timestamp":"1682400300.0","poster":"fdpv","content":"Please, check you facts:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RDS_Fea_Regions_DB-eng.Feature.CrossRegionReadReplicas.html\n\nThe only objection could be that MySQL 5.6 is mentioned in the question. But for MySQL 5.6 and 8.0, cross-region read replicas with RDS for MySQL is supported. Maybe the question is somehow outdate."}],"content":"A implies - read replica in a different region that region of master. AWS RDS mysql supports multi -AZ deployment ONLY. It cannot span across multiple regions.","comment_id":"695850","timestamp":"1665887700.0","upvote_count":"2"},{"timestamp":"1648588260.0","content":"Selected Answer: A\nA. For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region","upvote_count":"1","comment_id":"577866","poster":"jj22222"},{"comment_id":"406219","upvote_count":"1","timestamp":"1636070760.0","poster":"Akhil254","content":"A Correct"},{"poster":"01037","content":"A\nEasy one","upvote_count":"1","comment_id":"346712","timestamp":"1635425460.0"},{"upvote_count":"1","timestamp":"1635270120.0","poster":"cldy","content":"A. \nRead Replicas for \"as fast as possible\" requirement.","comment_id":"324699"},{"content":"A is the right answer here.","upvote_count":"1","timestamp":"1634614920.0","poster":"bustedd","comment_id":"293651"},{"upvote_count":"2","timestamp":"1634375640.0","poster":"fullaws","content":"A is correct","comment_id":"143859"},{"poster":"noisonnoiton","comment_id":"131101","upvote_count":"2","content":"go with A","timestamp":"1633370400.0"},{"content":"A is Correct","poster":"BillyC","timestamp":"1632382800.0","comment_id":"49676","upvote_count":"4"}],"question_id":421,"answer":"A","timestamp":"2020-01-12 03:57:00","topic":"1","isMC":true,"question_images":[],"question_text":"Your company has HQ in Tokyo and branch offices all over the world and is using a logistics software with a multi-regional deployment on AWS in Japan, Europe and USA. The logistic software has a 3-tier architecture and currently uses MySQL 5.6 for data persistence. Each region has deployed its own database.\nIn the HQ region you run an hourly batch process reading data from every region to compute cross-regional reports that are sent by email to all offices this batch process must be completed as fast as possible to quickly optimize logistics.\nHow do you build the database architecture in order to meet the requirements?","url":"https://www.examtopics.com/discussions/amazon/view/11790-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"unix_timestamp":1578797820,"answer_images":[],"answers_community":["A (60%)","D (40%)"],"answer_description":"","choices":{"A":"For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region","B":"For each regional deployment, use MySQL on EC2 with a master in the region and send hourly EBS snapshots to the HQ region","E":"Use Direct Connect to connect all regional MySQL deployments to the HQ region and reduce network latency for the batch process","D":"For each regional deployment, use MySQL on EC2 with a master in the region and use S3 to copy data files hourly to the HQ region","C":"For each regional deployment, use RDS MySQL with a master in the region and send hourly RDS snapshots to the HQ region"},"answer_ET":"A"},{"id":"MPD5S1mRXbcz3hzU9frN","choices":{"C":"Ensure that all organizations in the partnership have AWS accounts. Configure buckets in each of the accounts with a bucket policy that allows the institute that owns the data the ability to write to the bucket. Periodically sync the data from the institute's account to the other organizations. Have the organizations use their AWS credentials when accessing the data using their accounts.","D":"Ensure that all organizations in the partnership have AWS accounts. In the account with the S3 bucket, create a cross-account role for each account in the partnership that allows read access to the data. Enable Requester Pays on the bucket. Have the organizations assume and use that read role when accessing the data.","B":"Ensure that all organizations in the partnership have AWS accounts. Create a bucket policy on the bucket that owns the data. The policy should allow the accounts in the partnership read access to the bucket. Enable Requester Pays on the bucket. Have the organizations use their AWS credentials when accessing the data.","A":"Ensure that all organizations in the partnership have AWS accounts. In the account with the S3 bucket, create a cross-account role for each account in the partnership that allows read access to the data. Have the organizations assume and use that read role when accessing the data."},"isMC":true,"answer_images":[],"question_id":422,"question_text":"A group of research institutions and hospitals are in a partnership to study 2 PBs of genomic data. The institute that owns the data stores it in an Amazon S3 bucket and updates it regularly. The institute would like to give all of the organizations in the partnership read access to the data. All members of the partnership are extremely cost-conscious, and the institute that owns the account with the S3 bucket is concerned about covering the costs for requests and data transfers from Amazon S3.\nWhich solution allows for secure datasharing without causing the institute that owns the bucket to assume all the costs for S3 requests and data transfers?","timestamp":"2019-09-14 17:41:00","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/5170-exam-aws-certified-solutions-architect-professional-topic-1/","answer_ET":"B","unix_timestamp":1568475660,"question_images":[],"answer_description":"","answer":"B","exam_id":32,"answers_community":["B (100%)"],"discussion":[{"upvote_count":"53","poster":"donathon","timestamp":"1632352740.0","content":"B\nIn general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their bucket. A bucket owner, however, can configure a bucket to be a Requester Pays bucket. With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data. If you enable Requester Pays on a bucket, anonymous access to that bucket is not allowed.\nA\\D: When the requester assumes an AWS Identity and Access Management (IAM) role prior to making their request, the account to which the role belongs is charged for the request.\nC: This would incur additional cost of storing the data.","comment_id":"12725","comments":[{"poster":"AJ41185","content":"Agree with the explaination","timestamp":"1635355140.0","upvote_count":"2","comment_id":"282067"}]},{"upvote_count":"10","timestamp":"1632780960.0","poster":"fw","comment_id":"80943","content":"B. \nD doesn't work as if another account use cross-account role created under the bucket owner account, the bucket owner account is charged for the request."},{"timestamp":"1733646900.0","upvote_count":"1","content":"Selected Answer: B\nAgree with everyone on B. Initially I thought it is D (assume IAM role) but later learnt that this will make the S3 bucket owner pay the cost!! Learnt this now.","poster":"mnsait","comment_id":"1323447"},{"comment_id":"929914","upvote_count":"1","content":"Selected Answer: B\nB.\nTo enable Requester Pays for an S3 bucket","timestamp":"1687380480.0","poster":"SkyZeroZx"},{"timestamp":"1664816160.0","comment_id":"685689","content":"Selected Answer: B\nBased on all comments","upvote_count":"1","poster":"dmscountera"},{"comment_id":"669059","poster":"bihani","timestamp":"1663162620.0","upvote_count":"1","content":"Selected Answer: B\nAnswer is B"},{"timestamp":"1638879840.0","upvote_count":"1","poster":"cldy","content":"B. Ensure that all organizations in the partnership have AWS accounts. Create a bucket policy on the bucket that owns the data. The policy should allow the accounts in the partnership read access to the bucket. Enable Requester Pays on the bucket. Have the organizations use their AWS credentials when accessing the data.","comment_id":"495961"},{"poster":"AzureDP900","upvote_count":"1","comment_id":"493896","content":"requester pay the price , my answer is B","timestamp":"1638643980.0"},{"comment_id":"483084","content":"Selected Answer: B\nB.\nTo enable Requester Pays for an S3 bucket","upvote_count":"1","timestamp":"1637480340.0","poster":"acloudguru"},{"upvote_count":"1","timestamp":"1636246560.0","content":"B.\nTo enable Requester Pays for an S3 bucket\n\nSign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n\nIn the Buckets list, choose the name of the bucket that you want to enable Requester Pays for.\n\nChoose Properties.\n\nUnder Requester pays, choose Edit.\n\nChoose Enable, and choose Save changes.\n\nAmazon S3 enables Requester Pays for your bucket and displays your Bucket overview. Under Requester pays, you see Enabled.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysExamples.html","poster":"seyik","comment_id":"462141"},{"poster":"andylogan","content":"It's B - If requester pays is enabled then the request has to be authenticated and cannot assume a role to access the S3 bucket.","upvote_count":"1","timestamp":"1636185600.0","comment_id":"451160"},{"poster":"WhyIronMan","timestamp":"1636100580.0","upvote_count":"1","content":"I'll go with B","comment_id":"409706"},{"timestamp":"1636015140.0","content":"B correct","upvote_count":"1","poster":"Akhil254","comment_id":"406693"},{"timestamp":"1635912120.0","content":"Answer is B.\n\nD -Cant be answer as the role owning account will have to pay for the requests which will defeat the purpose of transferring data transfer cost to requester account.","comment_id":"378137","poster":"Amitv2706","upvote_count":"1"},{"poster":"Waiweng","comment_id":"344493","content":"It's B","timestamp":"1635717720.0","upvote_count":"3"},{"content":"Answer is B. Option D defeats the purpose of creating individual AWS accounts if there is no policy to mandates its use.","comment_id":"312155","upvote_count":"1","poster":"Pupu86","timestamp":"1635560280.0"},{"content":"going with B","poster":"Kian1","timestamp":"1635385440.0","upvote_count":"2","comment_id":"290321"},{"poster":"Ebi","comment_id":"282653","upvote_count":"3","timestamp":"1635383160.0","content":"B is my choice"},{"upvote_count":"2","poster":"sanjaym","content":"B for sure.","comment_id":"268010","timestamp":"1635297540.0"},{"timestamp":"1635078480.0","content":"Correct answer is B. Requester Pays and without role","comment_id":"242496","poster":"T14102020","upvote_count":"2"},{"upvote_count":"2","poster":"jackdryan","timestamp":"1635007080.0","content":"I'll go with B","comment_id":"230033"},{"comment_id":"229907","content":"B no doubt when a role is created the account where it's created is the owner again no doubt about it that eliminates A & D and i really don't know what C is doing out there. \ndon't confuse your self with misunderstanding the definitions on requester pays web page.\nB is correct no doubt.","timestamp":"1634743800.0","poster":"YouYouYou","upvote_count":"2"},{"timestamp":"1634618580.0","poster":"Bulti","upvote_count":"3","comment_id":"229238","content":"Answer is B. If requester pays is enabled then the request has to be authenticated and cannot assume a role to access the S3 bucket."},{"timestamp":"1634553480.0","content":"Answer: B\nWhen the requester assumes an AWS Identity and Access Management (IAM) role prior to making their request, the account to which the role belongs is charged for the request. For more information about IAM roles, see IAM Roles in the IAM User Guide.\nThis makes D invalid\nReference: https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html","comment_id":"222737","upvote_count":"1","poster":"kj07"},{"poster":"petebear55","timestamp":"1634531400.0","content":"They should stop putting the correct answer down if it is wrong .. its almost like they are trying to give u wrong answers .. answer here should be B ... this is the most secure . it uses BEST PRACTICE by putting a policy on the bucket and then utilizes requester pays ... D is the only other answer tat could have came close .. but B is the answer ..","upvote_count":"2","comment_id":"216570"},{"upvote_count":"3","poster":"Paitan","content":"\"Enable Requester Pays\" works only if the requester logs in using its own AWS credentials and not use any assumed roles from other's account. So option B is the right choice.","timestamp":"1634415600.0","comment_id":"195649"},{"comments":[{"poster":"Nkem","timestamp":"1634149980.0","content":"B\nThe explanation that you provided points to B. If the requester assumes the owner identity through a role, then the owner pays. Therefore a bucket policy is required so that the requester keeps their own identity.","comment_id":"184345","upvote_count":"1"}],"comment_id":"183434","timestamp":"1634059020.0","upvote_count":"3","poster":"wsw","content":"Difficult to choose between B and D, but I'll go with D because of:\n1: IAM roles are always a preference when managing accesses and permissions\n2: IAM is usually recommended over bucket policies\n3: https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html clearly says \"When the requester assumes an AWS Identity and Access Management (IAM) role prior to making their request, the account to which the role belongs is charged for the request. For more information about IAM roles, see IAM Roles in the IAM User Guide.\" I admit that \"the account to which the role belongs is charged\" is a bit confusing, but they probably meant \"the account from which the role is assumed\""},{"content":"B is more flexible than D as partners wont lose their account privileges for woking with data","poster":"ipindado2020","timestamp":"1633930500.0","upvote_count":"1","comment_id":"182664"},{"poster":"fullaws","upvote_count":"2","comment_id":"149579","timestamp":"1633703340.0","content":"B is correct"},{"upvote_count":"2","comment_id":"134424","timestamp":"1633650540.0","content":"B for sure","poster":"NikkyDicky"},{"poster":"ramikhreim","comment_id":"119156","timestamp":"1633385700.0","content":"Answer is D : In this scenario, the bucket owner can create an AWS Identity and Access Management (IAM) role with permission to access objects, and grant another AWS account permission to assume the role temporarily enabling it to access objects in the bucket.\nhttps://docs.amazonaws.cn/en_us/AmazonS3/latest/dev/example-walkthroughs-managing-access-example4.html","upvote_count":"1"},{"content":"B is the right answer.","comment_id":"105591","poster":"meenu2225","upvote_count":"2","timestamp":"1633354140.0"},{"upvote_count":"1","comment_id":"102178","content":"D: also enable Enable Requester Pays but seems better with cross-account role for each account.","poster":"NKnab","timestamp":"1633257480.0"},{"comment_id":"94307","timestamp":"1633174440.0","poster":"3parusr","content":"\"When the requester assumes an AWS Identity and Access Management (IAM) role prior to making their request, the account to which the role belongs is charged for the request. For more information about IAM roles, see IAM Roles in the IAM User Guide.\" That would mean it is B as it uses the customer Amazon account. As u4x noted D would bill the account that grants the right","upvote_count":"2"},{"comment_id":"94057","content":"A\nWhen the requester assumes an AWS Identity and Access Management (IAM) role prior to making their request, the account to which the role belongs is charged for the request","upvote_count":"1","poster":"JAWS1600","timestamp":"1633162860.0"},{"timestamp":"1632944820.0","comments":[{"upvote_count":"3","timestamp":"1633089840.0","content":"requester pays only if they are direct users - Not if they assume a role.","comment_id":"94055","poster":"JAWS1600"}],"comment_id":"87188","poster":"Mkumar","upvote_count":"1","content":"D\nbased on https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html"},{"content":"It is B. Organisations should use their own credentials so that Requester will pay. If 'role' is used then bucket owner will be the requester to pay for the transfer.","timestamp":"1632902340.0","upvote_count":"4","poster":"[Removed]","comment_id":"87093"},{"content":"Guys read question carefully\nThe institute would like to give all of the organizations in the partnership read access to the data. Can you transfer data if you have read permission ? \nMaybe answer A ????","timestamp":"1632826740.0","comment_id":"83805","upvote_count":"3","poster":"ashp"},{"timestamp":"1632775980.0","comment_id":"80455","content":"Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects\nResource-based Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects\nCross-account IAM roles for programmatic and console access to S3 bucket objects","poster":"frankzeng","upvote_count":"1"},{"content":"D\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/","comment_id":"80452","poster":"frankzeng","upvote_count":"1","timestamp":"1632733320.0","comments":[{"content":"The reference provided does not support D. This link states \n“Use one of the following methods to grant cross-account access to objects that are stored in S3 buckets: \n- Resource-based policies and AWS Identity and Access Management (IAM) policies …”\nYou can add an “Identity and Access Management (IAM) policy” to the S3 bucket that restricts access to particular AWS accounts as per the example contained in the following link: https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html. That is what B does.\nIf a role created by the account that owns the data is used to access the data, then the account that created the role will pay the transfer costs associated with the data as referenced by donathon. That makes D incorrect.\n\nThe correct answer is B.","timestamp":"1633610100.0","poster":"LunchTime","upvote_count":"3","comment_id":"120599"}]},{"upvote_count":"1","content":"D: also enable Enable Requester Pays but seems better with cross-account role for each account.","comment_id":"61073","timestamp":"1632729480.0","poster":"virtual"},{"comment_id":"50745","poster":"Ming","content":"Should be D, partnership must own the role and assume the role for request pay to work.","upvote_count":"2","timestamp":"1632696060.0"},{"timestamp":"1632533940.0","content":"Answer is B\nRequester Pay will solve this","comment_id":"50036","upvote_count":"3","poster":"amog"},{"content":"I do agree with Donathon explanation. \"B\" is correct.","comments":[{"timestamp":"1632500220.0","poster":"Moon","comment_id":"13898","upvote_count":"1","content":"https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html"}],"timestamp":"1632409500.0","poster":"Moon","upvote_count":"4","comment_id":"13897"},{"upvote_count":"3","comment_id":"11082","timestamp":"1632099780.0","comments":[{"upvote_count":"11","comment_id":"11165","content":"B\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html\n\nWhen the requester assumes an AWS Identity and Access Management (IAM) role prior to making their request, the account to which the role belongs is charged for the request.","timestamp":"1632129960.0","comments":[{"comment_id":"11700","content":"thx. b","upvote_count":"1","timestamp":"1632298920.0","poster":"dpvnme"}],"poster":"u4x"}],"poster":"awsec2","content":"d my view"}]},{"id":"048QuI2yZCiUijTkQ0NU","question_text":"A company currently uses a single 1 Gbps AWS Direct Connect connection to establish connectivity between an AWS Region and its data center. The company has five Amazon VPCs, all of which are connected to the data center using the same Direct Connect connection. The Network team is worried about the single point of failure and is interested in improving the redundancy of the connections to AWS while keeping costs to a minimum.\nWhich solution would improve the redundancy of the connection to AWS while meeting the cost requirements?","answer_description":"","exam_id":32,"topic":"1","answer_images":[],"timestamp":"2019-09-14 17:44:00","discussion":[{"poster":"donathon","comment_id":"12726","upvote_count":"29","content":"B\nA: This is too costly.\nC: How will this help when direct connect is the issue?\nD; There is still a single point of failure and Direct Connect cannot be set to public. It is not publicily accessible.","timestamp":"1632827220.0"},{"timestamp":"1632838560.0","comment_id":"13896","content":"I agree with \"B\".\nA: is costly.\nC: MPLS can not be used without dedicated link. Also, AWS does not support it.\nD: using Public VIF over the same direct connect will be helpful, as it is not adding extra physical redundancy. Also, Public VIFs are not used to connect on-prim to VPCs. it is used to connect on-prim to AWS public services like S3, DynamoDB...etc.","poster":"Moon","upvote_count":"22"},{"poster":"SkyZeroZx","upvote_count":"1","timestamp":"1687380540.0","comment_id":"929915","content":"lowcost keyword = VPN"},{"poster":"SkyZeroZx","comment_id":"926293","timestamp":"1687037280.0","upvote_count":"1","content":"Selected Answer: B\nI agree with \"B\"."},{"content":"Selected Answer: B\nBased on all comments","comment_id":"685694","poster":"dmscountera","upvote_count":"1","timestamp":"1664816640.0"},{"timestamp":"1661506800.0","upvote_count":"1","content":"D: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html","poster":"Sumit_Kumar","comment_id":"652167"},{"poster":"cldy","timestamp":"1639114560.0","comment_id":"498325","content":"B. Set up VPN tunnels from the data center to each VPC. Terminate each VPN tunnel at the virtual private gateway (VGW) of the respective VPC and set up BGP for route management.","upvote_count":"1"},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1638644160.0","comment_id":"493898","content":"B is right answer"},{"content":"Answer is B","comment_id":"447665","upvote_count":"1","timestamp":"1636167420.0","poster":"moon2351"},{"content":"B\nredundency + minimum cost","comment_id":"445657","poster":"student22","upvote_count":"1","timestamp":"1636070580.0"},{"comment_id":"409722","upvote_count":"1","content":"I'll go with B","timestamp":"1635950460.0","poster":"WhyIronMan"},{"timestamp":"1635734040.0","poster":"Waiweng","comment_id":"344583","upvote_count":"3","content":"B is the answer"},{"poster":"Pupu86","comment_id":"312162","upvote_count":"2","timestamp":"1635587100.0","content":"Option A - too costly to subscribe to another 1Gbps DX link\nOption C - doesn't make sense\nOption D - Public VIF still make use of the same DX link but only for public AWS resource connections such as S3 DynamoDB etc..\n\nSo Answer is B"},{"content":"B is the answer","upvote_count":"3","poster":"Ebi","comment_id":"282643","timestamp":"1635568380.0"},{"upvote_count":"2","content":"I'll with B","comment_id":"268014","timestamp":"1635384180.0","poster":"sanjaym"},{"poster":"PAUGURU","comment_id":"244894","timestamp":"1635048600.0","comments":[{"timestamp":"1635207600.0","upvote_count":"1","content":"in case of Direct connect would still remain a single point of failure. B is correct","poster":"Aquavk","comment_id":"252491"}],"upvote_count":"1","content":"D as stated here: https://docs.aws.amazon.com/directconnect/latest/UserGuide/remote_regions.html\n\"You can create a Direct Connect gateway in any public Region. Use it to connect your AWS Direct Connect connection over a private virtual interface to VPCs in your account that are located in different Regions or to a transit gateway.\nAlternatively, you can create a public virtual interface for your AWS Direct Connect connection and then establish a VPN connection to your VPC in the remote Region. \""},{"poster":"T14102020","upvote_count":"1","content":"Correct answer is B. Not D because Public VIF","comment_id":"242498","timestamp":"1634967240.0"},{"comment_id":"230035","timestamp":"1634778720.0","upvote_count":"2","content":"I'll go with B","poster":"jackdryan"},{"poster":"YouYouYou","content":"the answer goes between A and B other answers are wrong \nadditional 1 GB per month is not too coasty around 216 $ while 5 vpns are 180 $ so b is more cost effective checked.","timestamp":"1634074620.0","upvote_count":"2","comments":[{"comment_id":"547843","timestamp":"1644939480.0","content":"5 vpns are 400$, additional 1G direct connect is around 1200.\nhttps://aws.amazon.com/vpn/pricing/\nhttps://aws.amazon.com/directconnect/pricing/#:~:text=AWS%20Direct%20Connect%20data%20transfer,per%20GB%20in%20all%20locations.","poster":"mikhailwang","upvote_count":"1"}],"comment_id":"229910"},{"timestamp":"1634037780.0","poster":"Bulti","content":"Answer is B. Not D because Public VIF is not used to connect to the VPC. Its used to connect to public services such as S3 and DynamoDB","comment_id":"229328","upvote_count":"1"},{"content":"Ok B is cheaper and is correct, but time consuming to implement and manage","timestamp":"1633614300.0","poster":"Edgecrusher77","comment_id":"226710","upvote_count":"1"},{"upvote_count":"3","timestamp":"1633475460.0","comment_id":"149586","content":"B is correct","poster":"fullaws"},{"comment_id":"134425","poster":"NikkyDicky","upvote_count":"2","timestamp":"1633101960.0","content":"B for sure"},{"timestamp":"1633069920.0","upvote_count":"1","comment_id":"107387","content":"they word Site to Site VPN as VPN tunnel .https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html","poster":"JAWS1600"},{"comment_id":"105593","content":"B is the one.","upvote_count":"2","poster":"meenu2225","timestamp":"1632975180.0"},{"comment_id":"102180","poster":"NKnab","content":"b is cheaper so it is correct answer","timestamp":"1632932580.0","upvote_count":"2"},{"upvote_count":"6","comment_id":"50037","timestamp":"1632850080.0","content":"Think about B","poster":"amog"},{"timestamp":"1632641280.0","content":"I also think B is the right answer.","poster":"Lee","upvote_count":"4","comment_id":"11588"},{"comment_id":"11083","upvote_count":"10","content":"b is right","poster":"awsec2","timestamp":"1632200220.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/5171-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":423,"choices":{"C":"Set up a new point-to-point Multiprotocol Label Switching (MPLS) connection to the AWS Region that's being used. Configure BGP to use this new circuit as passive, so that no traffic flows through this unless the AWS Direct Connect fails.","B":"Set up VPN tunnels from the data center to each VPC. Terminate each VPN tunnel at the virtual private gateway (VGW) of the respective VPC and set up BGP for route management.","A":"Provision another 1 Gbps Direct Connect connection and create new VIFs to each of the VPCs. Configure the VIFs in a load balancing fashion using BGP.","D":"Create a public VIF on the Direct Connect connection and set up a VPN tunnel which will terminate on the virtual private gateway (VGW) of the respective VPC using the public VIF. Use BGP to handle the failover to the VPN connection."},"answer_ET":"B","unix_timestamp":1568475840,"answer":"B","isMC":true,"answers_community":["B (100%)"],"question_images":[]},{"id":"n4a5vSqxE1HKsIYb24ur","answers_community":["A (67%)","D (33%)"],"discussion":[{"comments":[{"content":"Depending on the Amazon Regions involved and the amount of data to be copied, a cross-Region snapshot copy can take hours to complete. https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html","timestamp":"1636220100.0","poster":"TiredDad","upvote_count":"1","comment_id":"413822"},{"poster":"amithbti416","comment_id":"383189","content":"https://aws.amazon.com/blogs/database/cross-region-automatic-disaster-recovery-on-amazon-rds-for-oracle-database-using-db-snapshots-and-aws-lambda/","upvote_count":"1","timestamp":"1635826560.0"},{"comment_id":"42564","timestamp":"1632364320.0","content":"D\nsorry, this time may be u and Moon were wrong.\nEBS and RDS both support CRR.\nhttps://amazonaws-china.com/about-aws/whats-new/2013/06/11/amazon-announces-faster-cross-region-ebs-snapshot-copy/\nBTW, no matter how, using Lambda or ECS , bring extra fee, and there is no way to copy snapshot to another region since Lambda&ECS can not span across region","poster":"PacoDerek","upvote_count":"7","comments":[{"poster":"easytoo","comment_id":"88550","upvote_count":"3","content":"Hi PacoDerek you can use Lambda to copy snapshots across regions. \nhttps://stackoverflow.com/questions/41726536/aws-lambda-copy-ec2-snapshot-automatically-between-regions","comments":[{"upvote_count":"2","timestamp":"1632739800.0","poster":"easytoo","content":"https://stackoverflow.com/questions/58922949/aws-lambda-copy-monthly-snapshots-to-another-region","comment_id":"88553"}],"timestamp":"1632735000.0"},{"timestamp":"1632912840.0","comment_id":"94148","poster":"Ibranthovic","content":"So you are prefering using Active-actice to reduce cost ?\nA is the right answer","upvote_count":"2"},{"content":"D is wrong - it implies active-active so both environments are working, and pilot light means active-passive","poster":"rb39","comment_id":"501398","upvote_count":"1","timestamp":"1639490280.0"},{"comment_id":"102646","poster":"meenu2225","content":"Nope, thats not correct mate, Lambda is a regional service but it can copy snapahosts to different regions (https://timesofcloud.com/aws-lambda-copy-ec2-snapshot-automatically-regions/). I agree you can do it via option D, but how are you planning to do this? Manually? Option A does the same but provides an automated method which copies the snpashot to different region. with ASG= 0 instances in the secondary region, gives it enough time to spin the infrtructure in the backup region as RTO and RPO are long enough.","timestamp":"1633246140.0","upvote_count":"1","comments":[{"content":"My Bad, you can automate the cross region replication of snapshot via snapshot policies. I take my stance back option D is the right one.","comment_id":"105597","upvote_count":"3","comments":[{"timestamp":"1633941900.0","comment_id":"158015","upvote_count":"1","poster":"Stec1980","content":"A would be preferred because it's Active/Passive"}],"timestamp":"1633287840.0","poster":"meenu2225"}]},{"content":"A is correct...you can use lambda to create and copy snapshots and you would want to use an Active/Passive routing configuration, not Active/Active because the desired solution is Pilot Light and there's a 6 hour RTO.","comment_id":"158013","timestamp":"1633919220.0","poster":"Stec1980","upvote_count":"5"}]},{"timestamp":"1635146220.0","upvote_count":"3","poster":"shammous","comment_id":"279806","content":"Why ECS should not be used? ECS would be cheaper as there is a requirement for cost effectiveness. \"AWS Lambda is optimized for simple and quick functions to execute. Larger and more complex functions create execution complexity (and significant execution cost) to the user. Amazon ECS, on the other hand, can be used with any reasonable size and complexity container.\" So ECS can better handle \"long-running job\" like taking RDS and EBS snapshots. So C is a more suitable answer."}],"comment_id":"12727","poster":"donathon","timestamp":"1632185520.0","content":"A\nB\\D: Too costly and not pilot light.\nC: ECS should not be used.","upvote_count":"23"},{"timestamp":"1632212580.0","poster":"Moon","content":"I would go with \"A\".\nB: it is not pilot, as it has working nodes in DR region.\nC: comparing Lambda to ECS snapshot job, it is better to use Lambda.\nD: EBS does not have auto regional replication!!","comments":[{"timestamp":"1632453960.0","upvote_count":"1","content":"Agree with you","poster":"virtual","comment_id":"61119"},{"poster":"b3llman","upvote_count":"3","content":"EBS does have auto regional replication. It can be defined under Snapshot Lifecycle Policy.","comment_id":"183504","timestamp":"1634262660.0"}],"upvote_count":"11","comment_id":"13895"},{"poster":"evargasbrz","content":"Selected Answer: A\nA is the right answer\nD-> I think that active-actice will not reduce costs and it's not pilot light.","timestamp":"1672161300.0","comment_id":"758832","upvote_count":"2"},{"poster":"hobokabobo","comment_id":"746718","content":"Selected Answer: D\nA: write unnecessary lambda seems odd and incurs costs\nB: again a lamda\nC: long jobs on ec2 even more expensive\nD: cross region snapshots are possible and avoid costs for lambda/ec2. I do not like the active active as it bears the unnecessary risk of two systems beeing up at the same time and datacoruption... . but it does have the benefit that with autscaling at 0 the system is down and will immediately be used as soon as it comes up. Pull the primary down and pull the other up and dns will deliver the ip of the sendary. Difference to active passiv in dns is only in case both systems are up at the same time in which case both systems will be used for name resolution. So the whole setup is active-pasive, only dns is active-active which is ok as it will use only the up of the active system..\nSo cheapest is indeed D. (May bear a risk of datacorruption in case of misconfigured autoscaling and both systems beeing up at the same time. But correctly configured it works)","timestamp":"1671158460.0","upvote_count":"1"},{"content":"A is best answer due to pilot and cost.","timestamp":"1668372720.0","poster":"LrdKanien","upvote_count":"1","comment_id":"717518"},{"upvote_count":"2","timestamp":"1668271440.0","content":"Selected Answer: D\nIt's actually D. R53 Active-Active with 0 instances on the DR region will result in a pilot-light configuration. A B and C have \"daily\" snapshot copy that won't meet the RPO/RTO requirements. D does not specify how often the snapshots are created/copied on the DR region, and the minimum interval is 1 hour, which will meet RPO and RTO requirements.","poster":"Lorrendo","comment_id":"716809"},{"timestamp":"1667567220.0","poster":"resnef","comments":[{"comment_id":"711157","upvote_count":"1","content":"answer is A","poster":"resnef","timestamp":"1667567340.0"}],"comment_id":"711155","content":"trick of the question is \"pilot light\", which makes A better than D","upvote_count":"1"},{"poster":"SVJS","upvote_count":"1","comment_id":"694466","content":"A.\nA is correct , we need active-passive failover configuration in Route 53.","timestamp":"1665718080.0"},{"content":"Selected Answer: A\nBased on all comments","comment_id":"685696","timestamp":"1664816940.0","upvote_count":"1","poster":"dmscountera"},{"timestamp":"1662827100.0","comment_id":"665527","content":"i think it is easy to implement EBS cross-region rather than lambda, actually today i use AWS backup service\n\nbut \"'D\" answer has active-active which is not the pilot light strategy as active-active is the strategy of multi-site \n\nhttps://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-i-strategies-for-recovery-in-the-cloud/\n\nSo, Answer A is best choice in this case","poster":"engmohhamed","upvote_count":"1"},{"poster":"pixepe","timestamp":"1662287940.0","content":"As it's pilot light approach on backup region, anything which has active-active doesn't make sense","comment_id":"659174","upvote_count":"2"},{"poster":"Enigmaaaaaa","comment_id":"636718","upvote_count":"1","content":"Only A is correct.\nHow D can be correct? its active-active are you going to shift traffic to an instance which does not exists? also in the question states \"pilot light\" this is clearly a active-passive deployment. another point is that you cannot just cross-region copy snapshots by itself you need to have a lambda or a script that will do it.","timestamp":"1658754240.0"},{"poster":"KiraguJohn","timestamp":"1658663640.0","content":"Why i think A is wrong...The company's RTO is six hours and its RPO is twenty-four hours.\nA talks about a daily backup which means once after 24 hrs. What if the system fails at 23rd hour. Will you meet RTO of 6 hours? I prefer D although its Active-Active the autoscaling is set to 0.","upvote_count":"2","comment_id":"636028"},{"timestamp":"1654154640.0","content":"Selected Answer: A\nD is perfect, if it's not \"active-active failover\".\nSo it's A","comment_id":"610476","poster":"bobsmith2000","upvote_count":"1"},{"comment_id":"503821","timestamp":"1639762740.0","content":"EBS & EFS don't have cross-region replication in-built; you can use AWS Backup for that.","poster":"vbal","upvote_count":"1"},{"upvote_count":"1","timestamp":"1639024980.0","content":"A is correct. Pilot light.","poster":"CloudChef","comment_id":"497324"},{"upvote_count":"1","timestamp":"1638938280.0","content":"A. Use AWS Lambda to create daily EBS and RDS snapshots, and copy them to the disaster recovery region. Use Amazon Route 53 with active-passive failover configuration. Use Amazon EC2 in an Auto Scaling group with the capacity set to 0 in the disaster recovery region.","comment_id":"496528","poster":"cldy"},{"upvote_count":"2","poster":"acloudguru","comment_id":"483206","timestamp":"1637494620.0","content":"Selected Answer: A\nhttps://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-iii-pilot-light-and-warm-standby/"},{"upvote_count":"1","poster":"student22","comment_id":"434166","content":"D.\nIt's still pilot light because ASG is 0.","comments":[{"poster":"student22","upvote_count":"1","timestamp":"1636275240.0","comment_id":"455128","content":"Changing to A because lambda is a better solution to copy EBS snapshots."}],"timestamp":"1636256040.0"},{"timestamp":"1636208100.0","poster":"tiffanny","upvote_count":"4","content":"Active - active : Use this failover configuration when you want all of your resources to be available the majority of the time.\nActive - Passive : Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable.\n\nSo since this is pilot light, we need to go to Active - passive approach. If we use D, we need to run it actively on the other region which means it will cost again.","comment_id":"413281"},{"poster":"DerekKey","timestamp":"1636150860.0","upvote_count":"1","comment_id":"413109","content":"A ok \nB wrong - active-active - cost\nC wrong - cost (task running 24/7/365 with single job daily)\nD wrong - active-active (x instances - 0 instances)"},{"comment_id":"409726","content":"I'll go with A","upvote_count":"1","poster":"WhyIronMan","timestamp":"1636054260.0"},{"poster":"Akhil254","comment_id":"406695","upvote_count":"1","timestamp":"1635892800.0","content":"A correct"},{"upvote_count":"1","poster":"Kopa","content":"For me its A, how can the DR be active if you have ASG with zero instances configured?\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html","comment_id":"392838","timestamp":"1635850740.0"},{"content":"the answer is A, the key is the active/passive failover configuration, it you have active-active will send traffic to DR site","timestamp":"1635819240.0","poster":"zolthar_z","comment_id":"368786","upvote_count":"1"},{"comment_id":"344594","poster":"Waiweng","upvote_count":"3","timestamp":"1635776880.0","content":"go for A"},{"poster":"biki1985","content":"Here is the defination of Pilot Light- • Pilot light (RPO in minutes, RTO in hours): Replicate your data from one region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup such as databases and object storage are always on. Other elements such as application servers are loaded with application code and configurations but are switched off and are only used during testing or when Disaster Recovery failover is invoked. DR is is switch off mode that means Active_passive is the ecenario. Hence A is the Answer.","timestamp":"1635742980.0","comment_id":"334401","upvote_count":"2"},{"comment_id":"323870","poster":"MinasWang","timestamp":"1635605880.0","content":"D only use the native cross-region copy features of EBS/RDS, do not involved the Lambda cost, so from the “MINIMAL COST” perspective, D is more suitable for Pilot light approach.","upvote_count":"1","comments":[{"comment_id":"330637","content":"How do we copy EBS & RDS snapshots to a different region without lambda? there is no native replication feature available. Correct Answer : A","upvote_count":"1","timestamp":"1635675480.0","poster":"SD13"}]},{"upvote_count":"1","timestamp":"1635600840.0","poster":"wasabidev","comment_id":"311205","content":"A.\nD. wrong due to active-active and capacity of 0 instances"},{"content":"I would go with A because lambda is used just to copy snapshot and Route 53 is in an active passive failover. I won't go for D because there is nothing like EBS having copy capability.","comment_id":"298964","timestamp":"1635548340.0","upvote_count":"1","poster":"kiev"},{"comment_id":"290674","poster":"Kian1","content":"will go with A lambda + Active/Passive","timestamp":"1635480780.0","upvote_count":"2"},{"timestamp":"1635182400.0","content":"Definitely A,\nD cannot be the answer, active-active means traffic will be routed to both targets while auto-scaling group in DR region has 0 instance which does not make sense","upvote_count":"6","comment_id":"282417","comments":[{"content":"What if we do active-active and setup health check on the DR region where there are 0 instances and hence no traffic will be routed there.","timestamp":"1636227000.0","poster":"TiredDad","upvote_count":"1","comment_id":"413835"},{"content":"Also there is no such a thing as \"EBS and RDS cross-region snapshot copy capability\", you need to copy snapshots manually to another region","poster":"Ebi","upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1635512820.0","content":"No, here it is https://aws.amazon.com/about-aws/whats-new/2020/01/aws-backup-supports-cross-region-backup/","poster":"selva","comment_id":"291409"}],"timestamp":"1635202260.0","comment_id":"287879"}],"poster":"Ebi"},{"upvote_count":"1","timestamp":"1635141300.0","poster":"dat","comment_id":"276503","content":"D: RDS CRR not supported for SQL Server.\nA is my choice"},{"poster":"sanjaym","comment_id":"268017","upvote_count":"1","content":"I'll go with A","timestamp":"1635088200.0"},{"upvote_count":"1","poster":"vipgcp","content":"Between A and D\nA - Lambda which is costly + if Lambda is just to trigger then it will terminate the process after 15 min. Why to create design which has chance of error?\nD - Though D is active active, but there are no instance in secondary region.\n\nSO Do sounds more suitable","comment_id":"254504","timestamp":"1635047340.0","comments":[{"upvote_count":"1","timestamp":"1635545280.0","poster":"selva","comment_id":"291410","content":"With active-active, I think the traffic will be split, which will cause an issue here."}]},{"upvote_count":"1","content":"A \nActive-Passive is cost effective than Active-Active as in D\nAlso need lambda for ebs and rds multi region snapshot","timestamp":"1635039780.0","poster":"rscloud","comment_id":"246871"},{"timestamp":"1635024720.0","comment_id":"246109","upvote_count":"2","poster":"newme","content":"I'll go for D. It is the simplest and most cost-effective way.\nI don't quite understand why \"pilot light approach\" has to be \"active-passive failover\".\nThough the ASG in another region is included in active-active failover configuration, it's capacity is 0, so I think it is a pilot light approach."},{"upvote_count":"2","timestamp":"1635006480.0","content":"Answer is A. Lambda more cheap then ECS and active-passive","poster":"T14102020","comment_id":"242511"},{"content":"I meant to say D .... .. Once again guys your not READING the question !!! 'MINIMUM COST' is key here ... So D","timestamp":"1634881140.0","poster":"petebear55","upvote_count":"3","comment_id":"241064"},{"upvote_count":"1","comment_id":"241061","content":"Going for A .. Once again guys your not READING the question !!! 'MINIMUM COST' is key here","timestamp":"1634746680.0","poster":"petebear55"},{"upvote_count":"2","comment_id":"235707","poster":"cloudgc","timestamp":"1634687760.0","content":"A - Lambda will only be used to initiate the snapshot - so no need to bother about the 15 min runtime of Lambda.\nA similar (eventhough complex) case study : https://aws.amazon.com/blogs/database/%C2%AD%C2%AD%C2%ADautomating-cross-region-cross-account-snapshot-copies-with-the-snapshot-tool-for-amazon-aurora/"},{"content":"I'll go with A","comment_id":"230039","poster":"jackdryan","timestamp":"1634513640.0","upvote_count":"2"},{"content":"in more complex situations the AWS Backup does not provide enough granularity for operators on the cloud. For example, one common use case with backups is the ability to copy snapshots into other regions for the purposes of redundancy. In the event of a regional outage, your data will still be present in other regions; a common model of a pilot light disaster recovery scenario. While this feature is on the roadmap for AWS Backup, this is a feasible use case today with Lambda. Not only can you copy snapshots between regions with Lambda, but it is much more extensible as you can plug in a variety of other services such as Cloudwatch (monitoring), SNS (notification), and SQS (batching/queuing), and build your very own disaster recovery solution, all natively with Lambda. This is not only just theoretical, we have helped customers do exactly this, with a series of Lambda functions, SQS queues, SNS topics, and Cloudwatch alarms to build automated disaster recovery solutions that just work.\n\nwill go with A","timestamp":"1634387340.0","comment_id":"229943","poster":"YouYouYou","upvote_count":"1"},{"comment_id":"229339","timestamp":"1634337240.0","content":"Only A or C is possible because they are the ones that provide pilot light DR strategy. \nUsing A multiple Lambda functions will be required - one to create the EBS snapshot and the other to copy the snapshots to another region. These Lambda functions need to be scheduled at specific internals using Cloudwatch events. The questions doesn't mention creating multiple lambda functions and stitching them together via Cloudwatch events.\n\n On the other hand using ECS to schedule a long running task that creates the snapshot and then copies it to the other region is a much more elegant solution. So I will go with C.","poster":"Bulti","upvote_count":"1","comments":[{"timestamp":"1635572820.0","poster":"ele","upvote_count":"2","comment_id":"306306","content":"That's right, but the question mentions Lambda as aws service, not just one function. There should be different functions invoked: to start snapshots creation, to check if snapshot exist - then start copy. I think it's just not reasonable to keep running task, waiting for the end. It'll cost money. Correct is A - MIN COST."}]},{"content":"Both A and D will work.\nD also mentions the \"snapshot copy feature\" which means we are taking the snapshot in source region and then copying then to destination region. \nIf we solely go with minimum cost, then A involves lambda cost and copy cost, and D involve only copy cost. There is no mention of automation anywhere in question so i am not considering that. \nBased on solely cost consideration, i will go with D.","comment_id":"216398","upvote_count":"4","timestamp":"1634316060.0","poster":"vjt"},{"comment_id":"195654","upvote_count":"2","poster":"Paitan","timestamp":"1634316060.0","content":"Lambda should be used just to initiate the snapshot creation process. So we need not worry if the snapshot creation takes more than 15 minutes. Hence option A sounds legit."},{"timestamp":"1634313300.0","content":"Cross region replication using lambda is possible ,which may use async tech.\nhttps://aws.amazon.com/blogs/database/cross-region-automatic-disaster-recovery-on-amazon-rds-for-oracle-database-using-db-snapshots-and-aws-lambda/\nChoose A","comment_id":"185049","poster":"exergeng","upvote_count":"1"},{"upvote_count":"2","poster":"manoj101","comment_id":"184060","timestamp":"1634266800.0","content":"D is correct. \nLambda does not support longer than 15 minutes execution so can not be considered.\nhttps://amazonaws-china.com/about-aws/whats-new/2013/06/11/amazon-announces-faster-cross-region-ebs-snapshot-copy/"},{"comments":[{"content":"I agree with your opinion. I don't think there is any guarantee that a snapshot copy using Lambda will be completed within 15 minutes.","upvote_count":"2","comment_id":"177563","poster":"luki666","timestamp":"1633974720.0"}],"poster":"nameisreqd","comment_id":"167440","content":"C\nA: Lambda runtime limit s 15 minutes. Not enough to create snapshot and then copy to DR region\nB:Not pilot light\nC: ECS can run long enough to create snapshot and copy to DR region\nD:EBS and RDS snapshots do not support cross region creation of snapshots. They support cross region copy but snapshot first needs to be created in source region and then copied to target region using CLI or console. This option mentions to create snapshot in target (DR) region directly, that is not possible","upvote_count":"1","timestamp":"1633965960.0"},{"upvote_count":"1","poster":"fullaws","timestamp":"1633912620.0","content":"A is correct, DNS active-active too costly (need to provisioning the load balancer). LambdaEdge can perform copy snapshot to cross region.","comment_id":"149595"},{"content":"A for sure","comment_id":"134428","poster":"NikkyDicky","upvote_count":"1","timestamp":"1633789740.0"},{"poster":"mat2020","comment_id":"133082","content":"answer: A","timestamp":"1633782240.0","upvote_count":"1"},{"timestamp":"1633770480.0","comment_id":"113554","poster":"Shawn1","content":"I would go for D because the DR region has the capacity set to 0, which means no actual traffic/cost in DR. Answer A will have additional cost on Lambda. And Lambda has a 15 minute limit on execution, which may also be a problem to handle the daily snapshot.","upvote_count":"2"},{"comment_id":"110307","upvote_count":"3","content":"My apologies team, A is the right answer. Option D has Active-Active failover, which is not the requirement.","timestamp":"1633750440.0","poster":"meenu2225"},{"comment_id":"107392","content":"https://cloudacademy.com/lab/automating-ebs-snapshots-lambda-and-cloudwatch-events/","timestamp":"1633738440.0","poster":"JAWS1600","upvote_count":"1"},{"comment_id":"107391","upvote_count":"1","content":"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/","timestamp":"1633577100.0","poster":"JAWS1600"},{"comment_id":"107388","timestamp":"1633552860.0","upvote_count":"1","content":"EBS does not have cross region copy capability . We need lambda for it","poster":"JAWS1600"},{"upvote_count":"1","poster":"sunilrch","content":"A is the perfect answer","timestamp":"1633424820.0","comment_id":"105778"},{"timestamp":"1633402680.0","poster":"meenu2225","comment_id":"105599","content":"After going through this question few time, I can say option D is the right one.\nAlthough Option A won't be that expensive as compared to D, but it will be expensive and managing of lambda is a factor too. So, Option D is the right one.","upvote_count":"2"},{"content":"I would go for A, due to Route53 active-passive is for pilot light.","timestamp":"1633119240.0","comment_id":"102181","upvote_count":"1","poster":"NKnab"},{"upvote_count":"1","poster":"NKnab","content":"A is correct. https://mysteriouscode.io/blog/copying-rds-snapshot-to-another-region-for-cross-region-recovery/","comment_id":"101408","timestamp":"1633032480.0"},{"content":"A\nhttps://aws.amazon.com/blogs/database/cross-region-automatic-disaster-recovery-on-amazon-rds-for-oracle-database-using-db-snapshots-and-aws-lambda/","poster":"IsaacTeh","timestamp":"1632997500.0","upvote_count":"2","comment_id":"98902"},{"content":"The best choice is C","comment_id":"97384","timestamp":"1632996480.0","upvote_count":"2","poster":"Jeb"},{"timestamp":"1632908700.0","poster":"VrushaliD","content":"agree with D. Lambda involves additional cost.","comment_id":"93820","upvote_count":"3"},{"comments":[{"upvote_count":"2","poster":"fw","timestamp":"1632869220.0","comment_id":"90687","content":"Supporting C.\nDepending on the Regions involved and the amount of data to be copied, a cross-Region snapshot copy can take hours to complete.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html"}],"poster":"fw","timestamp":"1632758100.0","comment_id":"90540","content":"Although this link suggests A is the correct answer. I still have doubts regarding Lambda running limit of 15 minutes, is this enough time to handle both DB snapshot and copy to another region? Plus, option C did mention using ECS to handle \"long-running job\", makes me consider C as the answer \nhttps://aws.amazon.com/blogs/database/cross-region-automatic-disaster-recovery-on-amazon-rds-for-oracle-database-using-db-snapshots-and-aws-lambda/","upvote_count":"3"},{"comments":[{"timestamp":"1632710100.0","content":"Correct. the key is pilot light.R53 AA failover makes no sense","upvote_count":"1","comment_id":"82127","poster":"PacoDerek"}],"poster":"tauseef","comment_id":"56872","upvote_count":"6","timestamp":"1632450780.0","content":"@PacoDerek - The solution is asking for pilot light which is a active/passive solution. So, A should be the correct answer as mentioned by Donathon/Moon."},{"timestamp":"1632367440.0","comment_id":"50039","content":"Should be A for pilot","upvote_count":"4","poster":"amog"},{"content":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html","poster":"9Ow30","upvote_count":"1","comment_id":"29129","timestamp":"1632282780.0"},{"timestamp":"1632067200.0","comments":[{"timestamp":"1633827180.0","content":"Lambda can run for 15 minutes only. Copying RDS and EBS snapshots can easily take more than that. \nSo C (the only other active/passive option)","comments":[{"upvote_count":"1","timestamp":"1635149460.0","poster":"CanBe","content":"Lambda will initiate the jobs as shown here: https://aws.amazon.com/blogs/database/cross-region-automatic-disaster-recovery-on-amazon-rds-for-oracle-database-using-db-snapshots-and-aws-lambda/\n\nECS may also be a solution but I have not seen the use of ECS for this purpose in any doc.","comment_id":"281459"}],"upvote_count":"2","poster":"MultiAZ","comment_id":"143627"}],"upvote_count":"2","comment_id":"12075","poster":"huhupai","content":"I would go for A, due to Route53 active-passive is for pilot light."}],"answer_images":[],"answer_ET":"A","url":"https://www.examtopics.com/discussions/amazon/view/5554-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1","answer_description":"","question_images":[],"question_id":424,"unix_timestamp":1569118920,"isMC":true,"choices":{"D":"Use EBS and RDS cross-region snapshot copy capability to create snapshots in the disaster recovery region. Use Amazon Route 53 with active-active failover configuration. Use Amazon EC2 in an Auto Scaling group with the capacity set to 0 in the disaster recovery region.","A":"Use AWS Lambda to create daily EBS and RDS snapshots, and copy them to the disaster recovery region. Use Amazon Route 53 with active-passive failover configuration. Use Amazon EC2 in an Auto Scaling group with the capacity set to 0 in the disaster recovery region.","B":"Use AWS Lambda to create daily EBS and RDS snapshots, and copy them to the disaster recovery region. Use Amazon Route 53 with active-active failover configuration. Use Amazon EC2 in an Auto Scaling group configured in the same way as in the primary region.","C":"Use Amazon ECS to handle long-running tasks to create daily EBS and RDS snapshots, and copy to the disaster recovery region. Use Amazon Route 53 with active-passive failover configuration. Use Amazon EC2 in an Auto Scaling group with the capacity set to 0 in the disaster recovery region."},"timestamp":"2019-09-22 04:22:00","exam_id":32,"answer":"A","question_text":"A company currently uses Amazon EBS and Amazon RDS for storage purposes. The company intends to use a pilot light approach for disaster recovery in a different AWS Region. The company has an RTO of 6 hours and an RPO of 24 hours.\nWhich solution would achieve the requirements with MINIMAL cost?"},{"id":"DqAKyJUCdlAa03p336lj","isMC":true,"exam_id":32,"choices":{"A":"Use Amazon S3 to collect multiple records in one S3 object. Use a lifecycle configuration to move data to Amazon Glacier immediately after write. Use expedited retrievals when reading the data.","E":"Write the records to an Amazon ElastiCache for Redis. Configure the Redis append-only file (AOF) persistence logs to write to Amazon S3. Recover from the log if the ElastiCache instance has failed.","C":"Use an AWS Lambda function invoked via Amazon API Gateway to collect data for 5 minutes. Write data to Amazon S3 just before the Lambda execution stops.","B":"Write the records to Amazon Kinesis Data Firehose and configure Kinesis Data Firehose to deliver the data to Amazon S3 after 5 minutes. Set an expiration action at 30 days on the S3 bucket.","D":"Write the records to Amazon DynamoDB configured with a Time To Live (TTL) of 30 days. Read data using the GetItem or BatchGetItem call."},"answer_ET":"BD","question_images":[],"question_id":425,"topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/5174-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"unix_timestamp":1568479500,"question_text":"A company needs to cost-effectively persist small data records (up to 1 KiB) for up to 30 days. The data is read rarely. When reading the data, a 5-minute delay is acceptable.\nWhich of the following solutions achieve this goal? (Choose two.)","answers_community":["BD (78%)","AD (22%)"],"answer":"BD","discussion":[{"content":"BD\nA: After 30 days the data should be deleted instead of storing it.\nB: When an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously. There may be a delay between the expiration date and the date at which Amazon S3 removes an object. You are not charged for storage time associated with an object that has expired.\nC: Does not address the 30 days deletion.\nD: https://aws.amazon.com/blogs/aws/new-manage-dynamodb-items-using-time-to-live-ttl/\nE: This is for cache and not suitable for this use case.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-expire-general-considerations.html","upvote_count":"34","comment_id":"12729","comments":[{"poster":"gookseang","comment_id":"275107","timestamp":"1635250800.0","content":"you talk too much","upvote_count":"7"},{"upvote_count":"1","poster":"kirrim","content":"A: storing data for 90 days in Glacier still costs half as much as storing in S3 Standard for 30 days, this is the least expensive option\nB: valid, although it would cost twice as much as A if you used S3 Stanadard\nC: agree on no lifecycle out of the data, also Lambda execution time charges for 5 mins would be very high\nD: is technically valid but storage cost alone for DynamoDB would be 10x option B and 20x option A\nE: sizing the memory cache large enough to not overwrite data for 30 days would make this the most expensive option of all","timestamp":"1636186080.0","comment_id":"457243"}],"poster":"donathon","timestamp":"1632235020.0"},{"comments":[{"upvote_count":"2","content":"bd here","timestamp":"1632214020.0","poster":"dpvnme","comment_id":"11701"}],"content":"b,d mostly","comment_id":"11093","timestamp":"1632092640.0","upvote_count":"9","poster":"awsec2"},{"poster":"SkyZeroZx","comment_id":"926294","timestamp":"1687037700.0","upvote_count":"1","content":"Selected Answer: BD\nyou just need to persist data for 30 days, so: B,D"},{"poster":"evargasbrz","content":"Selected Answer: BD\nyou just need to persist data for 30 days, so: B,D","timestamp":"1672161720.0","upvote_count":"2","comment_id":"758840"},{"timestamp":"1665718620.0","upvote_count":"1","poster":"SVJS","content":"B&D.\nKey is to delete the data after 30 days. There should not be any cost for the data after 30 days.","comment_id":"694471"},{"poster":"dmscountera","content":"Selected Answer: BD\nBased on all comments","comment_id":"685697","timestamp":"1664817060.0","upvote_count":"1"},{"comment_id":"655221","poster":"Sizuma","content":"AB CORRECT","upvote_count":"3","timestamp":"1661959260.0"},{"timestamp":"1659495900.0","comment_id":"641541","upvote_count":"2","content":"Selected Answer: AD\nA is cheaper than B.\nS3 cost is 0.023$ / GB, S3 Glacier cost is 0.004$ / GB.\nSo,even if S3 Glacier kept paying 90 days, they pay 0.012$ /GB. So they pay half of S3.","poster":"MarkChoi"},{"comment_id":"641536","content":"\"after 5 minutes\"?????\nwhat a dull action~~\nwhy are they doing that????","timestamp":"1659494940.0","upvote_count":"1","poster":"MarkChoi"},{"content":"will go with ab, accessed seldom,so why you store them in dynamo or redis?","poster":"necsk","timestamp":"1653523860.0","comment_id":"607430","upvote_count":"1"},{"content":"B. Write the records to Amazon Kinesis Data Firehose and configure Kinesis Data Firehose to deliver the data to Amazon S3 after 5 minutes. Set an expiration action at 30 days on the S3 bucket.\nD. Write the records to Amazon DynamoDB configured with a Time To Live (TTL) of 30 days. Read data using the GetItem or BatchGetItem call.","poster":"cldy","timestamp":"1638882240.0","upvote_count":"1","comment_id":"496021"},{"upvote_count":"1","comment_id":"493924","content":"I will go with BD","poster":"AzureDP900","timestamp":"1638647340.0"},{"comment_id":"483123","upvote_count":"3","content":"Selected Answer: BD\nA: After 30 days the data should be deleted instead of storing it.\nB: When an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously. There may be a delay between the expiration date and the date at which Amazon S3 removes an object. You are not charged for storage time associated with an object that has expired.\nC: Does not address the 30 days deletion.\nD: https://aws.amazon.com/blogs/aws/new-manage-dynamodb-items-using-time-to-live-ttl/\nE: This is for cache and not suitable for this use case.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-expire-general-considerations.html","poster":"acloudguru","timestamp":"1637484300.0"},{"poster":"bad_syntax","comment_id":"414193","timestamp":"1636086420.0","upvote_count":"5","content":"A & D - Reluctantly\nThe question says the data should persist for up-to 30 days. Removal of the data there-after is not a requirement, only an assumption.\nA vs B\nA: Terrible option. But it meets the criteria technically. Even if the data is stored for 90 days, or forever for that matter, and even when there is a cost to retrieve. \nB: Incorrect. Question says a 5 minute delay is acceptable. However answer B wants to use Kinesis Data Firehose to write to S3 after 5 minutes. Where is the data supposed to be read from? Kinesis Data Firehose or S3? Kinesis Data Firehose only has data younger than 5min, and reading from S3, data younger than 5min is not available for at least 5min.","comments":[{"poster":"RVivek","comment_id":"539275","upvote_count":"2","content":"Data will be read from S3. New data will be available only after 5 minutes. 5 minute delay is acceptable","timestamp":"1643848800.0"}]},{"upvote_count":"2","timestamp":"1636020660.0","poster":"WhyIronMan","content":"I'll go with B,D","comment_id":"409733"},{"poster":"Waiweng","timestamp":"1635834120.0","upvote_count":"5","content":"B,D mostly","comment_id":"345024"},{"content":"will go with BD","timestamp":"1635527100.0","comment_id":"290676","poster":"Kian1","upvote_count":"3"},{"poster":"Ebi","content":"Answer is BD","upvote_count":"6","timestamp":"1635289020.0","comment_id":"282432"},{"poster":"sanjaym","content":"I'll go with BD.","comment_id":"268022","timestamp":"1635249540.0","upvote_count":"2"},{"content":"Have to be B&D.\nThe question is kind of wired, only B&D meet the requirement of 30 days expiration.\nAnd B \"deliver the data to Amazon S3 after 5 minutes\" is nonsense, why after 5 minutes? What's the merits?\nA is actually a good answer, only that it doesn't delete data.","comments":[{"upvote_count":"3","comment_id":"333325","poster":"sarah_t","timestamp":"1635807600.0","content":"glacier is not a good choice as you're charged for the data for a minimum of 90 days (you can delete it earlier but you pay regardless)."}],"comment_id":"246261","timestamp":"1635209700.0","poster":"newme","upvote_count":"3"},{"poster":"T14102020","comment_id":"242520","content":"Correct answer is BD.\nB: addresses the 30 days, and 5 minutes retrievals\nD: DynamoDB is suitable for small objects, 1KB.","timestamp":"1635036060.0","upvote_count":"1"},{"comment_id":"241071","upvote_count":"1","timestamp":"1635011640.0","poster":"petebear55","content":"The reason why not A which may seem feasible but is a typical shitty Amazon Red Herring to persecute us poor Plebs ;) !!! \n90 days\nUpload requests are priced from $0.05 per 1,000 requests. In addition, archives stored in S3 Glacier have a minimum 90 days of storage, and archives deleted before 90 days incur a pro-rated charge equal to the storage charge for the remaining days."},{"comment_id":"241068","timestamp":"1634829540.0","poster":"petebear55","upvote_count":"1","content":"B AND D FOR ME"},{"content":"I'll go with B,D","upvote_count":"2","comment_id":"230041","timestamp":"1634761200.0","poster":"jackdryan"},{"comment_id":"229344","content":"B &D- Not A because no deletion policy set on Glacier and besides Glacier is a long term archival storage- not suitable for a short term archival of 30 days. Storing such small size data in DynamoDB for 30 days would turn out to be cheaper than storing the same data in Glacier for 90 days and performing expedited retrieval even though rare.","upvote_count":"1","poster":"Bulti","timestamp":"1634725560.0"},{"comment_id":"226921","content":"The answer should be A&B.\nC: Collecting data for 5 minutes doesn't make sense. You wouldn't know the size of the data after 5 minutes. Payload for API gateway is 10MB. \nD: Elasticache typical usage is for read performance improvement.\nB,A in that order seem to be right solution","upvote_count":"2","timestamp":"1634483940.0","poster":"srinivasa"},{"timestamp":"1634323260.0","poster":"Possum4Fun","content":"you can use Expedited retrievals to access data in 1 – 5 minutes for a flat rate of $0.03 per GB retrieved. Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. Nov 21, 2016\nHence, A & D","comment_id":"224423","upvote_count":"5"},{"comment_id":"182671","upvote_count":"1","poster":"ipindado2020","timestamp":"1634228580.0","content":"agree with BD"},{"poster":"fullaws","timestamp":"1634211420.0","comment_id":"149599","upvote_count":"2","content":"B and D is correct"},{"poster":"NikkyDicky","upvote_count":"2","comment_id":"134432","content":"BD for sure","timestamp":"1634196060.0"},{"timestamp":"1634127240.0","poster":"easytoo","upvote_count":"3","content":"It's B,D for me.","comment_id":"129324"},{"comment_id":"61393","timestamp":"1633756680.0","upvote_count":"6","poster":"Jshuen","content":"one more anti-pattern for not choosing A is minimum storage for Glacier is 90 days, but this case the company just need to store for 30days"},{"poster":"amog","comments":[{"poster":"amog","timestamp":"1633676400.0","comment_id":"50042","upvote_count":"4","content":"Answer is B,D"}],"comment_id":"50041","content":"A: Slow retrieval when using Glacier\nC: No expire after 30 days\nE: ElasticCache is not the solution for storage","upvote_count":"2","timestamp":"1633424100.0"},{"content":"A is actually a good option: https://aws.amazon.com/about-aws/whats-new/2016/11/access-your-amazon-glacier-data-in-minutes-with-new-retrieval-options/","upvote_count":"4","comment_id":"33322","timestamp":"1632620160.0","poster":"chandler","comments":[{"upvote_count":"1","comment_id":"48320","timestamp":"1633352100.0","content":"Not cost effective","poster":"wolke89"},{"content":"A is wrong, you can't move objects immediately to glacier after its write into S3. You have to use bucket policy to wait for 30 days to move objects into glacier","upvote_count":"1","comment_id":"888980","poster":"Jesuisleon","timestamp":"1683149400.0"}]},{"content":"I do support answers \"B\" & \"D\".\nA: have a slow retrieval.\nB: addresses the 30 days, and 5 minutes retrievals\nC: does not delete in 30 days.\nD: DynamoDB is suitable for small objects, 1KB.\nE: this is not storing data solution.","upvote_count":"4","timestamp":"1632370680.0","poster":"Moon","comments":[{"timestamp":"1633907760.0","comment_id":"82431","poster":"kui","upvote_count":"6","content":"A: expedited retrieval can be within 1-5 minutes."},{"comment_id":"291119","upvote_count":"2","content":"Expedited retrievals are typically made available within 1–5 minutes. + The data is read rarely = A","timestamp":"1635806160.0","poster":"ele"}],"comment_id":"13893"}],"answer_description":"","timestamp":"2019-09-14 18:45:00"}],"exam":{"provider":"Amazon","numberOfQuestions":1019,"isMCOnly":false,"id":32,"isImplemented":true,"isBeta":false,"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025"},"currentPage":85},"__N_SSP":true}