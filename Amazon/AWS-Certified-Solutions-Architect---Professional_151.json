{"pageProps":{"questions":[{"id":"DH18i1QaFrPasIiEfdwB","answer_images":[],"isMC":true,"answer_ET":"C","topic":"1","answer":"C","exam_id":32,"question_id":751,"timestamp":"2021-08-19 16:16:00","discussion":[{"poster":"Jupi","comment_id":"432182","timestamp":"1633857720.0","upvote_count":"9","content":"A - Incorrect - It will allow instances to be accessed from internet\nB - Incorrect - NAT gateways are not supported for IPv6 traffic—use an outbound-only (egress-only) internet gateway instead.\nC- Correct. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html#vpc-migrate-ipv6-cidr\nD: Incorrect - NAT gateways are not supported for IPv6 traffic—use an outbound-only (egress-only) internet gateway instead."},{"poster":"TechX","timestamp":"1656569760.0","comment_id":"625063","content":"Selected Answer: C\nC, easy one","upvote_count":"1"},{"content":"Selected Answer: C\nC looks right","poster":"jj22222","upvote_count":"1","timestamp":"1648563540.0","comment_id":"577665"},{"timestamp":"1643133120.0","poster":"shotty1","upvote_count":"1","comment_id":"532310","content":"Selected Answer: C\nit is C"},{"poster":"AzureDP900","content":"I will go with C","comment_id":"497229","timestamp":"1639004580.0","upvote_count":"1"},{"poster":"andylogan","upvote_count":"1","comment_id":"448736","content":"It's C","timestamp":"1634978040.0"},{"upvote_count":"1","content":"key word egress gateway, so C is correct","poster":"Kopa","timestamp":"1634817600.0","comment_id":"444491"},{"upvote_count":"3","poster":"tgv","comment_id":"435490","content":"CCC\n---","timestamp":"1634486940.0"},{"upvote_count":"1","poster":"Cotter","comment_id":"431231","content":"Yes C. > https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html.","timestamp":"1633742580.0"},{"content":"Also go for C","poster":"denccc","upvote_count":"1","timestamp":"1633013280.0","comment_id":"429153"},{"timestamp":"1632552120.0","comment_id":"428524","upvote_count":"2","poster":"pablobairat","content":"C\nSources:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"},{"content":"It should be C","poster":"pkboy78","upvote_count":"1","comment_id":"427516","timestamp":"1632115320.0"}],"url":"https://www.examtopics.com/discussions/amazon/view/59938-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"answers_community":["C (100%)"],"choices":{"A":"Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway.","C":"Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway.","D":"Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPv6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway.","B":"Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway."},"unix_timestamp":1629382560,"answer_description":"","question_text":"A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.\nA solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.\nWhat should the solutions architect do to meet these requirements?"},{"id":"Iqx3tFmwawiYkmlK8aR4","answer_images":[],"answer_ET":"D","unix_timestamp":1629526560,"isMC":true,"exam_id":32,"discussion":[{"comment_id":"835971","content":"Selected Answer: D\nIt would be D","upvote_count":"1","poster":"Edan777","timestamp":"1678537860.0"},{"content":"Main point is \"The company's goal should be to reduce the quantity of idle infrastructure supporting online forms.\" => serverless or scheduled autoscaling, but has no option about autoscaling => answer is D","timestamp":"1660642020.0","poster":"kadev","comment_id":"647560","upvote_count":"2"},{"comment_id":"637656","upvote_count":"1","poster":"hilft","content":"B vs. D\nBut don't need kinesis here.\nD","timestamp":"1658874240.0"},{"timestamp":"1656850080.0","content":"D. Provision an Amazon Aurora Serverless cluster. Build multiple schemas for each web form's data storage. Use Amazon API Gateway and an AWS Lambda function to recreate the data input forms. Use Amazon Route 53 to point the DNS names of the web forms to their corresponding API Gateway endpoint.","upvote_count":"3","comment_id":"626541","poster":"awsgorilla"},{"content":"The reference is about using Kinesis for capturing changes to DynamoDB not about updating data in DynamoDB. Also how will customer supply the data?","timestamp":"1650913140.0","poster":"JYZ","comment_id":"591927","upvote_count":"1"},{"poster":"AMKazi","comment_id":"517119","content":"Kineses data streams will increase the cost exponentially. Also the volume is low and per qtr.\nD: is the answer","timestamp":"1641349140.0","upvote_count":"2"},{"timestamp":"1639213500.0","content":"D. Provision an Amazon Aurora Serverless cluster. Build multiple schemas for each web formג€™s data storage. Use Amazon API Gateway and an AWS Lambda function to recreate the data input forms. Use Amazon Route 53 to point the DNS names of the web forms to their corresponding API Gateway endpoint.","upvote_count":"1","comment_id":"499230","poster":"cldy"},{"content":"Selected Answer: D\nServerless API + Serverless Business Logic + Serverless DB","timestamp":"1638235140.0","comment_id":"490318","upvote_count":"4","poster":"acloudguru"},{"comment_id":"448758","content":"It's D","upvote_count":"1","poster":"andylogan","timestamp":"1635719400.0"},{"comment_id":"440229","upvote_count":"1","timestamp":"1635401460.0","poster":"student22","content":"D\nServerless API + Serverless Business Logic + Serverless DB"},{"content":"DDD\n---","timestamp":"1634885460.0","comment_id":"435494","poster":"tgv","upvote_count":"1"},{"comment_id":"434601","poster":"blackgamer","upvote_count":"1","timestamp":"1634653020.0","content":"It should be D."},{"upvote_count":"2","content":"Answer is D. Fargate Storage is ephemeral storage, so, if the cluster has some problem the data will be lost","comment_id":"430251","poster":"zolthar_z","timestamp":"1633375800.0"},{"comment_id":"429159","content":"Would go for D","upvote_count":"3","timestamp":"1633322100.0","poster":"denccc","comments":[{"comment_id":"431701","upvote_count":"2","poster":"denccc","content":"Because of \"respond to each event\" and \"minimize the amount of idle infrastructure\"","timestamp":"1633962120.0"}]},{"timestamp":"1633315020.0","comment_id":"429098","poster":"Rmukh","comments":[{"poster":"wassb","timestamp":"1665845820.0","upvote_count":"1","content":"Farget Task storage is an ephemeral storage","comment_id":"695491"}],"content":"I think it should be C. D says create multiple schemas for each data format storage which is unnecessary.","upvote_count":"1"},{"content":"D for sure","poster":"pablobairat","comment_id":"428525","upvote_count":"3","timestamp":"1632438300.0"}],"answers_community":["D (100%)"],"choices":{"B":"Create one Amazon DynamoDB table to store data for all the data input. Use the application form name as the table key to distinguish data items. Create an Amazon Kinesis data stream to receive the data input and store the input in DynamoDB. Use Amazon Route 53 to point the DNS names of the web forms to the Kinesis data stream's endpoint.","A":"Use Amazon EC2 Image Builder to create AMIs for the legacy servers. Use the AMIs to provision EC2 instances to recreate the applications in the AWS Cloud. Place an Application Load Balancer (ALB) in front of the EC2 instances. Use Amazon Route 53 to point the DNS names of the web forms to the ALB.","D":"Provision an Amazon Aurora Serverless cluster. Build multiple schemas for each web form's data storage. Use Amazon API Gateway and an AWS Lambda function to recreate the data input forms. Use Amazon Route 53 to point the DNS names of the web forms to their corresponding API Gateway endpoint.","C":"Create Docker images for each server of the legacy web form applications. Create an Amazon Elastic Container Service (Amazon EC2) cluster on AWS Fargate. Place an Application Load Balancer in front of the ECS cluster. Use Fargate task storage to store the web form data."},"question_id":752,"url":"https://www.examtopics.com/discussions/amazon/view/60121-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"","answer":"D","topic":"1","question_images":[],"timestamp":"2021-08-21 08:16:00","question_text":"A company is migrating applications from on premises to the AWS Cloud. These applications power the company's internal web forms. These web forms collect data for specific events several times each quarter. The web forms use simple SQL statements to save the data to a local relational database.\nData collection occurs for each event, and the on-premises servers are idle most of the time. The company needs to minimize the amount of idle infrastructure that supports the web forms.\nWhich solution will meet these requirements?"},{"id":"faysv3hH0rt4kpmYNmAv","url":"https://www.examtopics.com/discussions/amazon/view/59987-exam-aws-certified-solutions-architect-professional-topic-1/","exam_id":32,"question_text":"A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js applications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into reports. When the aggregation jobs run, some of the load jobs fail to run correctly.\nThe company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the company's customers.\nWhat should a solutions architect do to meet these requirements?","answer":"C","answers_community":["C (100%)"],"topic":"1","unix_timestamp":1629414480,"discussion":[{"content":"A - Incorrect - Network Load Balancers do not support the lambda target type. Application Load Balancers are the only load balancers that support the lambda target type\nB - InCorrect - This will not solve problem of load jobs fail, while the aggregate job runs\nC- Correct - Aurora Replica for aggregates job and RDS proxy for better RDS performance\nD- Incorrect - Kinesis Firehose ca not replicate data to aurora directly","comment_id":"432221","timestamp":"1634558280.0","poster":"Jupi","upvote_count":"22"},{"poster":"mericov","comment_id":"428198","content":"C - Migrate the database with DMS -> Create a read replica - aggregation jobs will read data from RR -> Endpoints with Lambda behind an ALB -> use Proxy to write to master DB. Once synchronized, stop the task and point collectors to ALB. For B, is too complicated to configure the apps as EC2 instances. Instead Lambda is suitable. A, Lambda cannot run behind NLB.","timestamp":"1632523740.0","upvote_count":"10"},{"timestamp":"1672676100.0","content":"Selected Answer: C\nI'll go with C","upvote_count":"1","comment_id":"763839","poster":"evargasbrz"},{"comment_id":"626601","poster":"aandc","upvote_count":"1","timestamp":"1656859860.0","content":"Selected Answer: C\nC adds read replica to solve aggregation jobs issue"},{"timestamp":"1639005180.0","content":"I will go with C","comment_id":"497230","poster":"AzureDP900","upvote_count":"3"},{"comment_id":"495100","timestamp":"1638792000.0","content":"C. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS.","poster":"cldy","upvote_count":"2"},{"comment_id":"488849","timestamp":"1638077040.0","poster":"backfringe","upvote_count":"2","content":"I'd go with C"},{"upvote_count":"3","timestamp":"1637455860.0","poster":"acloudguru","content":"Selected Answer: C\nB - InCorrect - This will not solve problem of load jobs fail, while the aggregate job runs","comment_id":"482950"},{"timestamp":"1635374880.0","content":"It''s C - \nAmazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66%","comment_id":"448759","upvote_count":"3","poster":"andylogan"},{"poster":"tgv","comment_id":"435500","timestamp":"1635309540.0","upvote_count":"2","content":"CCC\n---"},{"poster":"blackgamer","timestamp":"1634585520.0","content":"C for sure.","comment_id":"434605","upvote_count":"1"},{"content":"I think C.","upvote_count":"2","comment_id":"431233","poster":"Cotter","timestamp":"1634445900.0"},{"content":"Answer is C, even B works did not fix the reading issue from the aggregation task","comment_id":"430272","poster":"zolthar_z","timestamp":"1634037960.0","upvote_count":"4"},{"upvote_count":"1","timestamp":"1633738920.0","content":"it should be B","poster":"Abhiju2019","comment_id":"428584"},{"timestamp":"1632747420.0","poster":"pablobairat","comment_id":"428531","upvote_count":"4","content":"C it is"},{"timestamp":"1632256020.0","comment_id":"427763","upvote_count":"1","poster":"pkboy78","content":"I think it is B"}],"isMC":true,"answer_ET":"C","answer_description":"","answer_images":[],"question_images":[],"timestamp":"2021-08-20 01:08:00","choices":{"D":"Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream.","B":"Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS.","A":"Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB.","C":"Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS."},"question_id":753},{"id":"IEgxSwe5xRGKY3Yacjjd","unix_timestamp":1567787280,"question_text":"A company is building a voting system for a popular TV show, viewers win watch the performances then visit the show's website to vote for their favorite performer. It is expected that in a short period of time after the show has finished the site will receive millions of visitors. The visitors will first login to the site using their Amazon.com credentials and then submit their vote. After the voting is completed the page will display the vote totals. The company needs to build the site such that can handle the rapid influx of traffic while maintaining good performance but also wants to keep costs to a minimum.\nWhich of the design patterns below should they use?","url":"https://www.examtopics.com/discussions/amazon/view/4820-exam-aws-certified-solutions-architect-professional-topic-1/","answer_images":[],"timestamp":"2019-09-06 18:28:00","discussion":[{"poster":"Moon","comments":[{"upvote_count":"2","poster":"cldy","content":"wont extra servers to process SQS increase the cost?","comment_id":"328515","timestamp":"1635300000.0"},{"timestamp":"1652519040.0","upvote_count":"3","poster":"bobsmith2000","comment_id":"601503","content":"Why not B?\nDDB is able to sustain millions of writes.\nWith D you pay for ELB, numerous web servers, sqs, numerous of app servers. And how to make sure that all votes are written to DDB after the show is over? Watches have to wait untill app servers process all items in the queue and untill DDB lets them to write? \n\nIn the case of B: the JS can implement the exponential back-off to wait untill DDB finish scaling and lets writes (it maintains double capacity in case of a spike and uses some kind of predictive 1 scaling under the hood)"}],"timestamp":"1633666200.0","upvote_count":"24","comment_id":"12802","content":"D is correct. It would be more costly to scale the read and write of the DynamoDB for the million visitors peak time, while you can offload the read and write using SQS. That will reduce the high cost of read/write on DyDB."},{"poster":"Warrenn","content":"The answer is not C if DynamoDb is configured to auto-provision WCU then a burst in traffic is going to be very expense as million of users are going to make the WCU very high to handle the load, if DynamoDb has fixed WCU then the high sudden volume will cause errors. SQS is always better at dealing with normalizing bursts in traffic therefore D.","upvote_count":"7","comment_id":"17514","timestamp":"1633830480.0"},{"content":"D. Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers, the web servers will first call the Login With Amazon service to authenticate the user, the web servers win process the users vote and store the result into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the SQS queue. A set of application servers will then retrieve the items from the queue and store the result into a DynamoDB table.","comment_id":"1266696","timestamp":"1723755840.0","poster":"amministrazione","upvote_count":"1"},{"content":"Selected Answer: D\nDesign B is not as suitable for this scenario because it does not use any load balancing. This means that if the website receives a large number of visitors, the web servers could become overloaded. This could lead to performance problems, such as slow loading times or even outages.\n\nDesign D uses CloudFront and an Elastic Load Balancer to distribute traffic across the web servers. This helps to prevent any single web server from becoming overloaded, which can improve performance and prevent outages.\n\nIn addition, design D uses SQS to buffer the votes. This helps to prevent the web servers from being overloaded by a large number of votes. This can also improve performance by reducing the number of requests that need to be made to the web servers.\n\nTherefore, design D is a better choice for this scenario because it uses load balancing and SQS to improve performance and prevent outages.","upvote_count":"1","poster":"SkyZeroZx","timestamp":"1687201800.0","comment_id":"927831"},{"comment_id":"921637","content":"Selected Answer: D\nB how to comunicate with IAM Role ? is correct \nC is incorrect for cost insted of use SQL\nD is correct because use SQS for not provisioned WCU in DynamoDB is more cost","poster":"SkyZeroZx","upvote_count":"1","comments":[{"poster":"SkyZeroZx","comment_id":"921639","timestamp":"1686593880.0","content":"B is incorrect typo error","upvote_count":"1"}],"timestamp":"1686593820.0"},{"timestamp":"1671826680.0","content":"Selected Answer: D\nA. Read the question, DynamoDB is a component to be used in my mind. Quickly scan the answers, all other answers use DybnamDB. So I killed this answer first.\nB. Looks very good, but how to use IAM roles in this setup?\nC. Not a bad solution.\nD. This is a better solution than C, with SQS, to decouple data provider and consumer the DynamoDB can be provisioned without considering accommodating the peak traffic.","poster":"TigerInTheCloud","comment_id":"754505","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nDDDDDDDDD","timestamp":"1662121500.0","comment_id":"657410","poster":"welcomeYM"},{"upvote_count":"1","timestamp":"1652518140.0","poster":"bobsmith2000","comment_id":"601498","content":"Why not B?\nIt's totally feasible. \nMillions of visitors + keeping the cost low => Serverless, CloudFront -> S3 (JS) -> DDB."},{"content":"C is correct.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html \nLog data can take up to 12 hours to become available for export. For near real-time analysis of log data, see Analyzing log data with CloudWatch Logs Insights or Real-time processing of log data with subscriptions instead.\nOnly C meets the RPO/RTO requirements.","poster":"johnnsmith","comments":[{"content":"Sorry. This is meant for another question.","upvote_count":"1","poster":"johnnsmith","timestamp":"1645897080.0","comment_id":"556873"}],"timestamp":"1645895160.0","comment_id":"556817","upvote_count":"1"},{"comment_id":"496595","timestamp":"1638945600.0","content":"D. Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers, the web servers will first call the Login With Amazon service to authenticate the user, the web servers win process the users vote and store the result into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the SQS queue. A set of application servers will then retrieve the items from the queue and store the result into a DynamoDB table.","poster":"cldy","upvote_count":"1"},{"poster":"01037","upvote_count":"1","content":"OK, I'll go with D.\nBoth B and C can be expensive if using on-demand capacity mode of DynamoDB.\nBut by adding some retry logic, I think I can work.","comment_id":"361698","timestamp":"1635647820.0"},{"timestamp":"1635241800.0","comment_id":"218441","poster":"newme","comments":[{"timestamp":"1635682320.0","poster":"01037","comments":[{"timestamp":"1636118280.0","poster":"01037","comment_id":"361971","upvote_count":"1","content":"So why not B?"}],"comment_id":"361970","content":"Oh, so it's not that expensive!?\nAs to aggression, create a new table for every event, use performer as hash key and visitor as range key, I think it works.","upvote_count":"1"}],"content":"I'm not sure which is right.\nIt seems people don't choose A or C, because it is said to be expensive.\nBut using on-demand capacity mode of DynamoDB, in US East (N. Virginia) Region, it's only $1.25 per million write request units. So with millions of visitors and 1 vote for each visitor, it only costs less than $20 for each performance.\nAnd another thing, aggregation is needed after vote, but I don't think DynamoDB is good at aggregation. So is DynamoDB really a good choice?","upvote_count":"1"},{"timestamp":"1635037680.0","comment_id":"150488","poster":"kratnesh","content":"Answer D, using SQS the request rate will get flattened and make proper IOPS of DynamoDB to write and read data for millions of users.","upvote_count":"1"},{"comment_id":"144534","poster":"fullaws","timestamp":"1634692200.0","content":"D is correct, agree, SQS is better than auto-provision WCU for dynamodb","upvote_count":"1"},{"comment_id":"40490","poster":"CloudFloater","timestamp":"1634608140.0","content":"D\nhttp://jayendrapatil.com/aws-storage-options-whitepaper/","upvote_count":"2"},{"comments":[{"poster":"amog","comment_id":"37968","timestamp":"1634445360.0","upvote_count":"2","content":"For milions vote processing, we need SQS to do this"}],"timestamp":"1634407920.0","content":"Answer is D\nKeyword is \"After the voting is completed the page will display the vote totals\"","comment_id":"37967","upvote_count":"1","poster":"amog"},{"content":"Answer D: \nUse CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers, the web servers will first call the Login. With Amazon service to authenticate the user, the web servers would process the users vote and store the result into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the SQS queue. A set of application servers will then retrieve the items from the queue and store the result into a DynamoDB table","comment_id":"20030","upvote_count":"4","poster":"pra276","timestamp":"1634365980.0"},{"comment_id":"10305","poster":"awsec2","upvote_count":"1","content":"C is right .","timestamp":"1632454440.0"},{"poster":"DalianYifang","timestamp":"1632291600.0","upvote_count":"1","comments":[{"comment_id":"601501","timestamp":"1652518500.0","content":"Why not B? Even cheaper.","upvote_count":"1","poster":"bobsmith2000"}],"content":"Any comments on D to keep the cost to a minimum? Why not C? cheaper than D and it should be working correct. Any thoughts?","comment_id":"9952"}],"isMC":true,"topic":"1","exam_id":32,"choices":{"D":"Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers, the web servers will first call the Login With Amazon service to authenticate the user, the web servers win process the users vote and store the result into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the SQS queue. A set of application servers will then retrieve the items from the queue and store the result into a DynamoDB table.","C":"Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers, the web servers will first call the Login with Amazon service to authenticate the user, the web servers will process the users vote and store the result into a DynamoDB table using IAM Roles for EC2 instances to gain permissions to the DynamoDB table.","B":"Use CloudFront and the static website hosting feature of S3 with the Javascript SDK to call the Login With Amazon service to authenticate the user, use IAM Roles to gain permissions to a DynamoDB table to store the users vote.","A":"Use CloudFront and an Elastic Load balancer in front of an auto-scaled set of web servers, the web servers will first call the Login With Amazon service to authenticate the user then process the users vote and store the result into a multi-AZ Relational Database Service instance."},"question_id":754,"answers_community":["D (100%)"],"answer_description":"","question_images":[],"answer":"D","answer_ET":"D"},{"id":"19JU6f8LUIE3gfjee5I4","unix_timestamp":1650627300,"timestamp":"2022-04-22 13:35:00","question_text":"A company runs an application in the cloud that consists of a database and a website. Users can post data to the website, have the data processed, and have the data sent back to them in an email. Data is stored in a MySQL database running on an Amazon EC2 instance. The database is running in a VPC with two private subnets. The website is running on Apache Tomcat in a single EC2 instance in a different VPC with one public subnet. There is a single VPC peering connection between the database and website VPC.\nThe website has suffered several outages during the last month due to high traffic.\nWhich actions should a solutions architect take to increase the reliability of the application? (Choose three.)","answer":"ACF","discussion":[{"poster":"snakecharmer2","content":"Selected Answer: ACF\nA - autoscalling \nC - aurora with read replica\nF - multi-az","timestamp":"1650627300.0","upvote_count":"8","comment_id":"589940"},{"comment_id":"698388","upvote_count":"2","content":"Selected Answer: ACF\nACF looks good.","timestamp":"1666113720.0","poster":"Ni_yot"},{"upvote_count":"3","timestamp":"1651515780.0","content":"go with ACF","comment_id":"596178","poster":"Yamchi"}],"exam_id":32,"choices":{"B":"Provision an additional VPC peering connection.","A":"Place the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application Load Balancer.","D":"Provision two NAT gateways in the database VPC.","F":"Create an additional public subnet in a different Availability Zone in the website VPC.","E":"Move the Tomcat server to the database VPC.","C":"Migrate the MySQL database to Amazon Aurora with one Aurora Replica."},"question_id":755,"answers_community":["ACF (100%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/74122-exam-aws-certified-solutions-architect-professional-topic-1/","isMC":true,"question_images":[],"answer_ET":"ACF","answer_images":[],"topic":"1"}],"exam":{"isBeta":false,"numberOfQuestions":1019,"isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","provider":"Amazon","id":32,"isMCOnly":false},"currentPage":151},"__N_SSP":true}