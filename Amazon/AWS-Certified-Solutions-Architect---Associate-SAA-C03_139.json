{"pageProps":{"questions":[{"id":"dyRczNywTcSUAvHyMYRA","answer_ET":"B","question_images":[],"question_id":691,"exam_id":31,"choices":{"B":"Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an Amazon FSx for Lustre file system, and integrate it with the S3 bucket. Access the FSx for Lustre file system from the HPC cluster instances.","D":"Create an Amazon FSx for Lustre file system. Import the data directly into the FSx for Lustre file system. Access the FSx for Lustre file system from the HPC cluster instances.","C":"Create an Amazon S3 bucket and an Amazon Elastic File System (Amazon EFS) file system. Import the data into the S3 bucket. Copy the data from the S3 bucket to the EFS file system. Access the EFS file system from the HPC cluster instances.","A":"Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an AWS Storage Gateway file gateway to use the S3 bucket. Access the file gateway from the HPC cluster instances."},"question_text":"A company copies 200 TB of data from a recent ocean survey onto AWS Snowball Edge Storage Optimized devices. The company has a high performance computing (HPC) cluster that is hosted on AWS to look for oil and gas deposits. A solutions architect must provide the cluster with consistent sub-millisecond latency and high-throughput access to the data on the Snowball Edge Storage Optimized devices. The company is sending the devices back to AWS.\n\nWhich solution will meet these requirements?","isMC":true,"answers_community":["B (65%)","D (35%)"],"url":"https://www.examtopics.com/discussions/amazon/view/132866-exam-aws-certified-solutions-architect-associate-saa-c03/","timestamp":"2024-02-05 17:02:00","answer_images":[],"unix_timestamp":1707148920,"topic":"1","discussion":[{"comment_id":"1211267","timestamp":"1715670840.0","poster":"Linuslin","content":"Selected Answer: B\nNo direct integration between Snowball and Fsx for Lustre. It must be via S3.\nSnowball Edge (Storage Optimized) --> S3 --integrate--> FSx for Lustre\n\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html\n\nhttps://aws.amazon.com/tw/blogs/aws/enhanced-amazon-s3-integration-for-amazon-fsx-for-lustre/","upvote_count":"12"},{"comments":[{"comment_id":"1157053","content":"The format on the Snowball device would be s3 compatible only. The FSx for Lustre file system can be created and then linked to the S3 bucket. The Lustre file system can then be mounted on the HPC workloads that need sub-millisecond latency to storge data. Option B would be the correct option, assuming only S3 support on snowball.","upvote_count":"9","poster":"domper20232023","timestamp":"1708684740.0"}],"content":"Selected Answer: D\nOption D\nOption A, B, and C involve using Amazon S3 or Amazon EFS as an intermediary storage layer, which may introduce additional latency and overhead, not meeting the requirement of consistent sub-millisecond latency. Therefore, Option D is the most suitable solution for this scenario.","poster":"Cali182","comment_id":"1144484","timestamp":"1707399300.0","upvote_count":"12"},{"upvote_count":"1","timestamp":"1739195100.0","comment_id":"1354478","poster":"a8a1e0e","content":"Selected Answer: B\nIntegration from Snowall to Fsc only with S3"},{"upvote_count":"1","poster":"FlyingHawk","comment_id":"1333329","timestamp":"1735447740.0","content":"Selected Answer: B\nit is not possible to import data directly from Snowball Edge Storage Optimized devices to Amazon FSx for Lustre, \n FSx for Lustre is designed to work seamlessly with data stored in Amazon S3, providing integration for high-performance access and lazy loading.\nSnowball Edge devices are designed to securely transfer data to AWS services, particularly Amazon S3. https://aws.amazon.com/blogs/storage/automatically-import-objects-from-amazon-s3-into-amazon-fsx-for-lustre/"},{"comments":[{"comment_id":"1285475","content":"You cannot import data from snowball in any other destination other than S3. So no, D is INCORRECT. Don't get tricked just because they mentioned HPC.","poster":"MatAlves","timestamp":"1726612140.0","upvote_count":"2"}],"upvote_count":"1","content":"Selected Answer: D\nThe correct answer is D for sure","poster":"Abdullah2004","comment_id":"1267656","timestamp":"1723893420.0"},{"comment_id":"1257522","upvote_count":"4","poster":"n999","timestamp":"1722261780.0","content":"Selected Answer: B\nB for sure"},{"timestamp":"1718671560.0","content":"Selected Answer: D\nImport data to AWS services: When AWS receives the device, the data is automatically imported into the designated AWS service or Amazon S3 bucket based on your configuration. For example, if you need to access the data from an HPC cluster running on AWS, you would import the data into an Amazon FSx for Lustre file system or Amazon S3, and then access it from your HPC cluster instances.\nNo need S3 bucket","poster":"DZRomero","comment_id":"1232173","upvote_count":"1"},{"comment_id":"1205104","content":"Selected Answer: B\nYou cannot access the FSx for Lustre file system from the HPC cluster instances and this is only possible via S3","timestamp":"1714569840.0","poster":"trinh_le","upvote_count":"5"},{"content":"Selected Answer: D\nHPC = Lustre","timestamp":"1714053960.0","comment_id":"1202043","upvote_count":"3","comments":[{"comment_id":"1202059","timestamp":"1714055400.0","content":"Extension: HPC= Lustre, but Snowball = S3, therefore: B\nSync from Snowball to S3 -> Link/integrate with Lustre\n\nCorrect answer: C","poster":"sandordini","comments":[{"content":"Which is of course not C but B... :D Sorry...\nSo correct answer: :D","comment_id":"1202061","timestamp":"1714055460.0","poster":"sandordini","upvote_count":"2"}],"upvote_count":"2"}],"poster":"sandordini"},{"upvote_count":"3","comment_id":"1193111","timestamp":"1712762940.0","poster":"sukjubae","content":"B is right"},{"poster":"alawada","content":"Selected Answer: D is right answer because it mentions sub-millisecond latency and high-throughput access","comment_id":"1180155","upvote_count":"1","timestamp":"1711123440.0"},{"content":"B\nhttps://medium.com/@abylead/amazon-fsx-for-migration-and-certification-f3cb7b4dd843","upvote_count":"2","timestamp":"1710840240.0","comment_id":"1177163","poster":"mgrimandi"},{"upvote_count":"3","comment_id":"1173644","poster":"MattBJ","timestamp":"1710442800.0","content":"Selected Answer: B\nB is correct"},{"upvote_count":"3","timestamp":"1709548620.0","comment_id":"1165504","poster":"[Removed]","content":"Selected Answer: B\nAccording to Copilot: Transferring data directly from AWS Snowball Edge to Amazon FSx for Lustre is not a standard process supported directly by AWS."},{"content":"Selected Answer: D\nOption D, creating an Amazon FSx for Lustre file system and importing the data directly into it, is indeed the most suitable solution for this scenario. By bypassing an intermediary storage layer and directly importing the data into FSx for Lustre, the solution ensures optimal performance with consistent sub-millisecond latency and high throughput, meeting the requirements of the HPC cluster. Thank you for pointing out the clarity.","comment_id":"1157993","upvote_count":"1","poster":"iczcezar","timestamp":"1708785240.0"},{"comments":[{"upvote_count":"2","timestamp":"1708536660.0","comment_id":"1155745","poster":"FZA24","content":"It must be via S3"}],"comment_id":"1155744","upvote_count":"4","content":"Selected Answer: B\nIt should be B. \nNo direct integration between Snowball and Fsx for Lustre","poster":"FZA24","timestamp":"1708536660.0"},{"poster":"67a3f49","upvote_count":"1","timestamp":"1708350360.0","content":"Cali182 you cannot directly copy from Snowball Edge to FSx for luster","comment_id":"1153951"},{"comment_id":"1152537","timestamp":"1708172700.0","upvote_count":"4","poster":"1Alpha1","content":"Selected Answer: B\nIts B.\nSnowball Edge (Storage Optimized) --> S3 --integrate--> FSx for Lustre"},{"poster":"Darshan07","comment_id":"1148055","content":"Selected Answer: D\nD is the correct answer","upvote_count":"2","timestamp":"1707738540.0"},{"content":"My bad...it should be B","timestamp":"1707148980.0","poster":"Andy_09","upvote_count":"5","comment_id":"1141256"},{"content":"Correct answer D","timestamp":"1707148920.0","comment_id":"1141255","upvote_count":"1","poster":"Andy_09"}],"answer":"B","answer_description":""},{"id":"R3f5AzPC5PoQRDYGoIA1","question_images":[],"answer_images":[],"question_text":"A company has NFS servers in an on-premises data center that need to periodically back up small amounts of data to Amazon S3.\n\nWhich solution meets these requirements and is MOST cost-effective?","timestamp":"2024-02-05 17:05:00","answers_community":["B (100%)"],"answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/amazon/view/132867-exam-aws-certified-solutions-architect-associate-saa-c03/","answer":"B","answer_ET":"B","exam_id":31,"isMC":true,"unix_timestamp":1707149100,"discussion":[{"comment_id":"1148976","timestamp":"1707810900.0","poster":"BillaRanga","upvote_count":"9","content":"Selected Answer: B\nA -> Used for ETL not copying\nB -> Works\nC -> Works, but overkill for the described scenario of periodic small backups, high cost\nD -> Works but it may not be necessary for transferring small amounts of data periodically. High setup cost"},{"timestamp":"1707149100.0","comment_id":"1141260","content":"B is the correct option","poster":"Andy_09","upvote_count":"9"},{"poster":"Scheldon","upvote_count":"2","timestamp":"1719826020.0","content":"Selected Answer: B\nAnswerB\n\nShould be sufficient","comment_id":"1240075"},{"comment_id":"1148057","content":"Selected Answer: B\nB is the correct option","poster":"Darshan07","timestamp":"1707738660.0","upvote_count":"2"}],"question_id":692,"choices":{"B":"Set up an AWS DataSync agent on the on-premises servers, and sync the data to Amazon S3.","A":"Set up AWS Glue to copy the data from the on-premises servers to Amazon S3.","D":"Set up an AWS Direct Connect connection between the on-premises data center and a VPC, and copy the data to Amazon S3.","C":"Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3."}},{"id":"QawttvpmzDuIjQ6pvtfK","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/132868-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_images":[],"answers_community":["C (100%)"],"question_id":693,"isMC":true,"topic":"1","question_images":[],"unix_timestamp":1707149160,"answer_description":"","discussion":[{"timestamp":"1707149160.0","poster":"Andy_09","comment_id":"1141261","content":"UDP needs NLB","upvote_count":"9"},{"timestamp":"1731855120.0","upvote_count":"1","content":"UDP -> L4 Protocol -> NLB","poster":"echonesis","comment_id":"1313585"},{"poster":"MatAlves","comment_id":"1285480","timestamp":"1726612800.0","content":"Selected Answer: C\nUDP > NLB.","upvote_count":"2"},{"upvote_count":"1","content":"Ans : C\nOfCourse we can use both NLB and GLB balancers for UDP traffic but NLB is more cost effective than GLB that is why we choice C.","comment_id":"1192093","timestamp":"1712648400.0","poster":"zinabu"},{"poster":"asdfcdsxdfc","timestamp":"1709505720.0","content":"Selected Answer: C\nTCP/UDP = NLB","comment_id":"1165140","upvote_count":"3"},{"upvote_count":"4","poster":"osmk","comment_id":"1149476","content":"C ->https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html","timestamp":"1707849480.0"},{"poster":"Marunio","content":"Selected Answer: C\nUDP -> NLB. \n\nALB is for HTTP/HTTPS.\n\nGateway Load Balancer is for 3rd party virtual appliances like Firewalls etc not the traffic distribution.\n\nhttps://aws.amazon.com/compare/the-difference-between-the-difference-between-application-network-and-gateway-load-balancing/#:~:text=An%20NLB%20operates%20on%20layer,level%20along%20with%20gateway%20functionality.","upvote_count":"3","timestamp":"1707835680.0","comment_id":"1149243"},{"timestamp":"1707834900.0","content":"Selected Answer: C\nUDP, should use network load balancer","poster":"Gagg","comment_id":"1149225","upvote_count":"2"},{"poster":"nj1999","timestamp":"1707479760.0","comment_id":"1145426","upvote_count":"4","content":"C, NLB"}],"choices":{"D":"Launch an identical set of game servers on EC2 instances in separate AWS Regions. Route internet traffic to both sets of EC2 instances.","A":"Configure an Application Load Balancer with the required protocol and ports for the internet traffic. Specify the EC2 instances as the targets.","B":"Configure a Gateway Load Balancer for the internet traffic. Specify the EC2 instances as the targets.","C":"Configure a Network Load Balancer with the required protocol and ports for the internet traffic. Specify the EC2 instances as the targets."},"timestamp":"2024-02-05 17:06:00","question_text":"An online video game company must maintain ultra-low latency for its game servers. The game servers run on Amazon EC2 instances. The company needs a solution that can handle millions of UDP internet traffic requests each second.\n\nWhich solution will meet these requirements MOST cost-effectively?","exam_id":31,"answer":"C"},{"id":"mYffb9fq5hQyBYf7gxnd","question_id":694,"url":"https://www.examtopics.com/discussions/amazon/view/132870-exam-aws-certified-solutions-architect-associate-saa-c03/","discussion":[{"content":"Selected Answer: AD\nA: Correct. because need convert from MySQL to PostgreSQL\n\nB: Wrong. Schema Conversion does not create an Aurora read replica\n\nC: Wrong. Company wants to migrate to Aurora PostgreSQL, not Aurora MySQL\n\nD: Correct. CDC task helps to capture ongoing change from source data store\n\nE: Wrong. Although using Aurora Read Replica is an option for DB migration within the same Region, this question is asking for \"combination of steps\", which this option does not have another compatible option to pair with\n\nTherefore, answer is \"AD\"","timestamp":"1711343160.0","comment_id":"1182218","poster":"h0ng97_spare_002","upvote_count":"8"},{"content":"AD makes sense to me, but I am not sure if that's the best answer.","poster":"mestule","upvote_count":"6","timestamp":"1707259860.0","comments":[{"upvote_count":"4","poster":"Andy_09","content":"Agreed. AD makes more sense !!","comment_id":"1144628","timestamp":"1707406020.0"}],"comment_id":"1142816"},{"comment_id":"1240078","poster":"Scheldon","timestamp":"1719826380.0","content":"Selected Answer: AD\nAnswerAD","upvote_count":"2"},{"comment_id":"1173937","content":"Lag many never be zero, then it will ne er be promoted to primary","timestamp":"1710466740.0","poster":"xBUGx","upvote_count":"2"},{"timestamp":"1708106760.0","comment_id":"1152162","content":"Selected Answer: AD\nIt's quite similar with Q.235, based on that discussion A-D makes more sense.","comments":[{"upvote_count":"1","comment_id":"1262114","poster":"1e22522","timestamp":"1723037700.0","content":"of course, sin mas"}],"poster":"haci","upvote_count":"4"},{"upvote_count":"4","comment_id":"1141264","poster":"Andy_09","content":"Correct answer BE","timestamp":"1707149460.0"}],"answer_images":[],"choices":{"B":"Use AWS Database Migration Service (AWS DMS) Schema Conversion to create an Aurora PostgreSQL read replica on the RDS for MySQL DB instance.","E":"Promote the Aurora PostgreSQL read replica to a standalone Aurora PostgreSQL DB cluster when the replica lag is zero.","A":"Use AWS Database Migration Service (AWS DMS) Schema Conversion to transform the database objects.","D":"Define an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) to migrate the data.","C":"Configure an Aurora MySQL read replica for the RDS for MySQL DB instance."},"isMC":true,"question_images":[],"answer_description":"","answers_community":["AD (100%)"],"exam_id":31,"unix_timestamp":1707149460,"answer":"AD","timestamp":"2024-02-05 17:11:00","question_text":"A company runs a three-tier application in a VPC. The database tier uses an Amazon RDS for MySQL DB instance.\n\nThe company plans to migrate the RDS for MySQL DB instance to an Amazon Aurora PostgreSQL DB cluster. The company needs a solution that replicates the data changes that happen during the migration to the new database.\n\nWhich combination of steps will meet these requirements? (Choose two.)","answer_ET":"AD","topic":"1"},{"id":"wPMYLbwMGkMu1PHkoudr","exam_id":31,"answer":"B","answers_community":["B (100%)"],"question_id":695,"timestamp":"2024-02-06 23:54:00","topic":"1","discussion":[{"poster":"Moon239","timestamp":"1707410400.0","content":"Selected Answer: B\nRead replica","upvote_count":"5","comment_id":"1144721"},{"timestamp":"1719826500.0","comment_id":"1240079","poster":"Scheldon","upvote_count":"2","content":"Selected Answer: B\nAnswer B"},{"upvote_count":"3","comment_id":"1170641","timestamp":"1710106440.0","content":"Selected Answer: B\nB, read replica","poster":"giovanna_mag"},{"upvote_count":"2","timestamp":"1707260040.0","comment_id":"1142819","content":"Selected Answer: B\nB looks correct","poster":"mestule"}],"isMC":true,"unix_timestamp":1707260040,"choices":{"C":"Instruct the development team to manually export the new entries for the day in the database at the end of each day.","A":"Add functionality to the script to identify the instance that has the fewest active connections. Configure the script to read from that instance to report the total new entries.","D":"Use Amazon ElastiCache to cache the common queries that the script runs against the database.","B":"Create a read replica of the database. Configure the script to query only the read replica to report the total new entries."},"answer_images":[],"question_images":[],"answer_ET":"B","question_text":"A company hosts a database that runs on an Amazon RDS instance that is deployed to multiple Availability Zones. The company periodically runs a script against the database to report new entries that are added to the database. The script that runs against the database negatively affects the performance of a critical application. The company needs to improve application performance with minimal costs.\n\nWhich solution will meet these requirements with the LEAST operational overhead?","url":"https://www.examtopics.com/discussions/amazon/view/133216-exam-aws-certified-solutions-architect-associate-saa-c03/","answer_description":""}],"exam":{"id":31,"name":"AWS Certified Solutions Architect - Associate SAA-C03","lastUpdated":"11 Apr 2025","isBeta":false,"numberOfQuestions":1019,"isMCOnly":true,"isImplemented":true,"provider":"Amazon"},"currentPage":139},"__N_SSP":true}