{"pageProps":{"questions":[{"id":"rllt62hHgqXKgJ1yVF7G","topic":"1","answer_images":[],"choices":{"D":"It allows you to view items stored in a tables, add, update, and delete items.","A":"It allows you to add local secondary indexes to existing tables.","B":"It allows you to query a table.","C":"It allows you to set up alarms to monitor your table's capacity usage."},"answers_community":["A (100%)"],"unix_timestamp":1572829560,"timestamp":"2019-11-04 02:06:00","answer":"A","exam_id":32,"discussion":[{"content":"Answer is A: . It allows you to add local secondary indexes to existing tables.\nWe cannot create local secondary indexes(LSI) for existing tables. LSI can only be created during the table creation.","comment_id":"21681","upvote_count":"9","timestamp":"1633884840.0","poster":"pra276"},{"poster":"SkyZeroZx","timestamp":"1686941220.0","comment_id":"925482","upvote_count":"1","content":"Selected Answer: A\nLocal secondary indexes can only be created on table creation."},{"comment_id":"686894","content":"Selected Answer: A\nLocal secondary indexes can only be created on table creation.","upvote_count":"1","timestamp":"1664974740.0","poster":"astalavista1"},{"poster":"hilft","comment_id":"635760","timestamp":"1658614440.0","content":"I vote for A. You can't add local secondary indexes to existing tables. Only upon creation","upvote_count":"1"},{"upvote_count":"1","content":"Answer is A.\nLocal secondary indexes on a table are created when the table is created. When you delete a table, any local secondary indexes on that table are also deleted.\n\nOnly Global Secondary Instance can be created after the table creation\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html#LSI.Creating","timestamp":"1634426700.0","comment_id":"448662","poster":"lifebegins"},{"content":"more information here: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/creating-alarms.html","poster":"Denis_H","upvote_count":"1","comment_id":"43266","timestamp":"1634061600.0"},{"timestamp":"1632912840.0","content":"None of the answer are not wrong... \n\nu can use the console to do the following in DynamoDB:\n\nMonitor recent alerts, total capacity, service health, and the latest DynamoDB news on the DynamoDB dashboard.\n\nCreate, update, and delete tables. The capacity calculator provides estimates of how many capacity units to request based on the usage information you provide.\n\nManage streams.\n\nView, add, update, and delete items that are stored in tables. Manage Time to Live (TTL) to define when items in a table expire so that they can be automatically deleted from the database.\n\nQuery and scan a table.\n\nSet up and view alarms to monitor your table's capacity usage. View your table's top monitoring metrics on real-time graphs from CloudWatch.\n\nModify a table's provisioned capacity.\n\nCreate and delete global secondary indexes.\n\nCreate triggers to connect DynamoDB streams to AWS Lambda functions.\n\nApply tags to your resources to help organize and identify them.\n\nPurchase reserved capacity.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ConsoleDynamoDB.html","comment_id":"19104","upvote_count":"3","poster":"skywalker"}],"isMC":true,"question_text":"Which of the following is NOT true of the DynamoDB Console?","answer_description":"The DynamoDB Console lets you do the following: Create, update, and delete tables. The throughput calculator provides you with estimates of how many capacity units you will need to request based on the usage information you provide. View items stored in a tables, add, update, and delete items. Query a table. Set up alarms to monitor your table's capacity usage. View your table's top monitoring metrics on real-time graphs from CloudWatch. View alarms configured for each table and create custom alarms.html.","answer_ET":"A","question_id":306,"url":"https://www.examtopics.com/discussions/amazon/view/7669-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[]},{"id":"srAvWXzUVnxoa06nj3wo","timestamp":"2020-10-02 14:22:00","answer_description":"DynamoDB uses JSON only as a transport protocol, not as a storage format. The AWS SDKs use JSON to send data to DynamoDB, and DynamoDB responds with JSON, but DynamoDB does not store data persistently in JSON format.\nReference:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.LowLev elAPI.html","url":"https://www.examtopics.com/discussions/amazon/view/33430-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"answers_community":[],"question_id":307,"answer_ET":"D","isMC":true,"unix_timestamp":1601641320,"answer_images":[],"topic":"1","answer":"D","exam_id":32,"question_text":"DynamoDB uses only as a transport protocol, not as a storage format.","choices":{"A":"WDDX","C":"SGML","B":"XML","D":"JSON"},"discussion":[{"timestamp":"1639138440.0","poster":"cldy","content":"D. JSON","upvote_count":"1","comment_id":"498599"},{"timestamp":"1635217200.0","upvote_count":"1","poster":"lifebegins","comment_id":"448669","content":"Answer is D:\n\nDynamoDB uses JSON only as a transport protocol, not as a storage format. The AWS SDKs use JSON to send data to DynamoDB, and DynamoDB responds with JSON. DynamoDB does not store data persistently in JSON format.\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.LowLevelAPI.html"},{"timestamp":"1633042200.0","upvote_count":"2","comment_id":"233784","content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.LowLevelAPI.html","poster":"newme"}]},{"id":"uYmXRgDUJ1DtVUw59RFm","question_id":308,"answer_images":[],"answer_ET":"C","timestamp":"2021-03-26 17:01:00","unix_timestamp":1616774460,"answers_community":["C (100%)"],"question_images":[],"choices":{"A":"Alarm Signal","D":"DynamoDBALARM","B":"DynamoDB Analyzer","C":"CloudWatch"},"url":"https://www.examtopics.com/discussions/amazon/view/48259-exam-aws-certified-solutions-architect-professional-topic-1/","answer_description":"CloudWatch allows you to set alarms when you reach a specified threshold for a metric.\nReference:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/MonitoringDynamoDB.html","isMC":true,"exam_id":32,"answer":"C","question_text":"In DynamoDB, which of the following allows you to set alarms when you reach a specified threshold for a metric?","topic":"1","discussion":[{"poster":"SkyZeroZx","comment_id":"925483","content":"Selected Answer: C\nC. CloudWatch","upvote_count":"1","timestamp":"1686941280.0"},{"poster":"cldy","comment_id":"494144","content":"C. CloudWatch","upvote_count":"2","timestamp":"1638690540.0"},{"content":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/creating-alarms.html","timestamp":"1635111000.0","upvote_count":"1","poster":"ExtHo","comment_id":"321270"}]},{"id":"XgpxfYUfg7COXL0NnruH","url":"https://www.examtopics.com/discussions/amazon/view/62462-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":[],"timestamp":"2021-09-21 07:32:00","topic":"1","isMC":true,"answer_description":"Yes. When you copy data from an Amazon DynamoDB table into Amazon Redshift, you can perform complex data analysis queries on that data. This includes joins with other tables in your Amazon Redshift cluster.\nReference:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/RedshiftforDynamoDB.html","question_images":[],"question_id":309,"choices":{"C":"No, DynamoDB data types do not correspond directly with those of Amazon Redshift.","B":"No","D":"Yes","A":"No, you cannot load all the data from DynamoDB table to a Redshift table as it limited by size constraints."},"discussion":[{"poster":"hilft","upvote_count":"1","content":"skip this. probably won't be in SAP exam","timestamp":"1658872800.0","comment_id":"637643"},{"comment_id":"498699","poster":"cldy","content":"D. Yes. possible to Load Data From DynamoDB Into Amazon Redshift","timestamp":"1639146720.0","upvote_count":"1"},{"timestamp":"1635601620.0","content":"Answer should be D:\n\nYes, we can trasfer the data from Dynamod DB to redshift for Analysis\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/RedshiftforDynamoDB.html","comment_id":"448671","poster":"lifebegins","upvote_count":"2"}],"unix_timestamp":1632202320,"question_text":"Is it possible to load data from Amazon DynamoDB into Amazon Redshift?","answer_images":[],"answer_ET":"D","exam_id":32,"answer":"D"},{"id":"n0UGPTmDxIMbQqJqw7Er","timestamp":"2019-10-25 10:07:00","discussion":[{"timestamp":"1632155160.0","poster":"Warrenn","comment_id":"17394","content":"the question asks to improve the availability EBS volumes are attached to a single ec2 instance correct answer is A","upvote_count":"20"},{"timestamp":"1632131760.0","comment_id":"17335","upvote_count":"9","poster":"rasagulla","comments":[{"timestamp":"1724813340.0","poster":"tutrieuchau","content":"I think the problem of this issue is speed, EBS is faster than other service such as EFS or S3. So for speed, EBS is better than S3, but for cost or scaling, the S3 is bester than EBS","comment_id":"1273748","upvote_count":"1"}],"content":"I am trying to understand why option 'A' using S3 cannot be a possible solution for this question. Can some help me on that."},{"content":"D. EBS with Provisioned IOPS (PIOPS) to store I/O files SQS to distribute elaboration commands to a group of hosts working in parallel Auto Scaling to dynamically size the group ot hosts depending on the length of the SQS queue.","upvote_count":"1","comment_id":"1266483","timestamp":"1723730580.0","poster":"amministrazione"},{"content":"A. S3 to store I/O files. SQS to distribute elaboration commands to a group of hosts working in parallel. Auto scaling to dynamically size the group of hosts depending on the length of the SQS queue.\n\nExplanation:\nS3 for Storage: Using Amazon S3 to store input and output files allows for scalable and durable storage, which can handle a high volume of files efficiently.\nSQS for Command Distribution: Amazon SQS (Simple Queue Service) can be used to distribute elaboration commands to a group of EC2 hosts working in parallel. This ensures that tasks are processed efficiently and concurrently.\nAuto Scaling for Dynamic Sizing: Auto Scaling can dynamically adjust the size of the group of EC2 hosts based on the length of the SQS queue. This ensures that the processing capacity scales up or down based on the workload, improving both efficiency and availability.","poster":"Mahesh430","comment_id":"1196974","timestamp":"1713327000.0","upvote_count":"1"},{"comment_id":"1088428","poster":"nideesh","content":"Selected Answer: D\nSNS cannot be used.\nBetween A and D. D is better, EBS is available after Instance restarts. Writing to EBS is faster than S3. S3 in itself is a service written on top of EFS. To improve availability of EBS a better IOPS is needed instead of replacing EBS with S3. Changing the application to use S3 buket.","upvote_count":"1","timestamp":"1701775980.0"},{"upvote_count":"2","poster":"sujfil","content":"Question doesn't mention anything about reducing cost. I cannot understand why we would you choose S3 over EBS as quality of service is more important as per question","comment_id":"916718","timestamp":"1686095100.0"},{"poster":"TigerInTheCloud","comment_id":"747547","content":"Selected Answer: A\nA: Standard SQS/Auto-Scaling use case. \n * S3 is cheaper than EBS without provisioning capacity. It is also HA.\n * SQS decouples the producer and consumer. It is HA.\n * Auto-scaling for getting needed capacity on-demand (performance and cost)","upvote_count":"2","timestamp":"1671220860.0"},{"upvote_count":"1","comment_id":"704307","timestamp":"1666748040.0","content":"Selected Answer: A\nAnswer: A","poster":"Neftali"},{"timestamp":"1661300280.0","comment_id":"651035","poster":"rajvee","upvote_count":"1","content":"A. Tricky qn. \"S3 to store I/O files.\" = Increases availability/durability from storing in a single EC2 and EBS. \"SQS to distribute \n elaboration commands to a group of hosts working in parallel\" and\n\"Auto scaling to dynamically size the group of hosts depending on the length of the SQS queue\" = Increase the compute power and availability to support the processing."},{"upvote_count":"3","content":"Selected Answer: A\nAnswer: A\nExplanation:\nSNS doesn’t allow you to distribute tasks between group of hosts. It allows you sending notification but how do you decide which host will handle it?\nAlso PIOPS is good for performance but not for availability which this task is asking about.\nThere is no problem in using S3 as there is no frequently changing data, you process the file and write the result once and don’t change it later.","poster":"TechX","timestamp":"1656410340.0","comment_id":"623884"},{"comment_id":"622785","content":"Selected Answer: A\nAgree with A. S3 is better.","poster":"kangtamo","upvote_count":"1","timestamp":"1656272820.0"},{"comment_id":"520771","upvote_count":"2","poster":"pititcu667","content":"this is hard going with d simply because \"shorten the solution's development time\".","timestamp":"1641808380.0"},{"content":"A is better because the files in S3 can be read by multiple instances. EBS storage is attached to a single instance","comment_id":"519500","upvote_count":"2","poster":"Tokyoboy","timestamp":"1641643980.0"},{"upvote_count":"1","timestamp":"1638845580.0","content":"A. With S3 i do a lot to increase availability like cross-region replication etc. Even if you can do multi-attach on EBS Procisioned IOPS io1 it is still limited to 16 and can only be on a single -AZ","poster":"KiraguJohn","comment_id":"495553"},{"content":"a is better","timestamp":"1636284180.0","poster":"pavelurv","upvote_count":"1","comment_id":"444814"},{"upvote_count":"2","content":"D. Already solution is working with EC2 with EBS volume. Question asking to reduce elaboration time and increase availability. Implementing auto scaling elaboration time will get reduce, with multiple instance in ASG highly available.","timestamp":"1636134540.0","poster":"FERIN_01","comment_id":"423434"},{"poster":"Akhil254","comment_id":"405828","timestamp":"1635979560.0","content":"A Correct","upvote_count":"1"},{"content":"A\n Multi-Attach on Amazon EBS Provisioned IOPS io1 volumes allow a single volume to be concurrently attached to up to sixteen AWS Nitro System-based Amazon Elastic Compute Cloud (Amazon EC2) instances within the same Availability Zone.\nNote: the key words are Nitro based systems and Singe-AZ. So, with so many assumptions, D can not be the correct answer.","comment_id":"384462","upvote_count":"2","timestamp":"1635926820.0","poster":"DashL"},{"comment_id":"341789","upvote_count":"1","content":"Obviously A","poster":"01037","timestamp":"1635559860.0"},{"comment_id":"282398","content":"I ll go with A","upvote_count":"1","poster":"bnagaraja9099","timestamp":"1635401340.0"},{"poster":"Ajeeshpv","content":"A should be correct","comment_id":"272765","upvote_count":"1","timestamp":"1635379080.0"},{"upvote_count":"2","timestamp":"1635322500.0","content":"A is correct. The simple reason is that the hosts cannot share EBS volume as stated in option D","comment_id":"272476","poster":"AWSLeaerner"},{"timestamp":"1635259560.0","content":"I think it should be A because we can reduce time by doing autscaling EC2 based on number of files and s3 can be used by all EC2 at same time but in case of EBS it can not be used(EBS multi attach is limited functionality). So it has to be A.","upvote_count":"3","comment_id":"254599","poster":"Dara2315"},{"poster":"ChauPhan","content":"Between A and D, I'll go for D with the speed of read/write IO of EBS. A = S3 is in low cost situation. In this case, we need to reduce elaboration time.","upvote_count":"1","timestamp":"1634955960.0","comment_id":"232806","comments":[{"content":"Imaging that you have an application, getting the file from S3, analyze it, and write the output somewhere, maybe S3 again. The getting file's time from S3 is more higher than getting's time from your local disk, in this case EBS volume. So the processing time is much higher, especially you have high number of files as the questions.","timestamp":"1635189780.0","poster":"ChauPhan","comment_id":"232816","upvote_count":"1"}]},{"content":"The right answer should be option A.","poster":"Sagardonthineni","upvote_count":"1","comment_id":"206859","timestamp":"1634912580.0"},{"content":"Multi-Attach is supported exclusively on Provisioned IOPS SSD (io1) volumes. It is not supported on Provisioned IOPS SSD (io2) volumes.\nMulti-Attach is quite limited. The question doesn't mention the detail. I'll go with S3","timestamp":"1634878680.0","poster":"saddly","comment_id":"202393","upvote_count":"2"},{"timestamp":"1634717820.0","upvote_count":"2","content":"D is correct","poster":"srknbngl","comment_id":"190201"},{"poster":"dswd","comment_id":"181536","upvote_count":"3","content":"Definitely not D. Even though AWS allows you to attach EBS volumes multiple times (since 02/2020), you can not just run a normal filesystem on top as filesystems assume single ownership of the volume. This would just break in very spectacular ways. I would go with A.","timestamp":"1634675100.0"},{"timestamp":"1634670180.0","comment_id":"171297","upvote_count":"1","poster":"indranilatcal","content":"Option D talks about EBS being an I/O source for multiple source. To me, tats not possible as EBS volumes can't be attached to more than one EC2 instance. A seems to be correct."},{"comment_id":"170239","upvote_count":"1","poster":"Bulti","content":"EBS replication is confined to a single AZ whereas S3 Data can be replicated across multi-AZ ( up to 3) by default and if needed can be configured across multiple regions.\n\nAlso EBS volumes cannot be shared across multiple EC2 instances and therefore each EC2 instance will have to write the data to it's own EBS volume. The results therefore will be scattered across multiple EBS volumes which cannot survive an AZ failure. So the better option is A.","timestamp":"1634646420.0"},{"content":"D is correct, Write operation small and frequent (PIOPS), availability of the solution (intensive operation might kill the instance, autoscaling solve), Multi-Attach (PIOPS)","comment_id":"143425","upvote_count":"1","poster":"fullaws","timestamp":"1634503380.0"},{"poster":"guptas","upvote_count":"1","comment_id":"142879","timestamp":"1634429460.0","content":"I am so confused right now between A and D.\nI mean S3 anti-patterns - rapidly changing data, talking about files but it is available\nEBS - made for rapidly changing data and file system but supports 1 instance.\ncrazy"},{"upvote_count":"2","timestamp":"1634357880.0","comment_id":"127379","poster":"noisonnoiton","content":"go with A"},{"comment_id":"115840","poster":"oatif","comments":[{"poster":"manoj101","comments":[{"content":"i stand corrected, you can use multi attach to attach one large volume with multiple linux instances.","timestamp":"1634391900.0","comment_id":"131912","upvote_count":"1","poster":"oatif"},{"timestamp":"1634817120.0","comment_id":"202085","content":"Just a soft reminder that this behavior has to avoid when the scenario is a dual writer architecture. For instance, SQL on WFC.","upvote_count":"1","poster":"TerrenceC"}],"content":"You can attached. Read following:-\nAmazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1) volume to up to 16 Nitro-based instances that are in the same Availability Zone.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html","upvote_count":"1","timestamp":"1634259120.0","comment_id":"116918"}],"timestamp":"1634215920.0","upvote_count":"2","content":"A because you can only attach one EBS volume with one instance at a time"},{"content":"A EBS volume cannot be accessed by multiple host instances.","comments":[{"comment_id":"84570","timestamp":"1634058180.0","upvote_count":"2","content":"don't miss lead ... r u sure? the best id EfS for the IOPS， but the EFS is not in. We shall use the second-best EBS. And EBS may be cheaper as it only is used for a few hours per day. D is quite certain.","poster":"bernardtao"},{"comment_id":"106347","content":"You can but only Provisioned IOPS volume","timestamp":"1634158740.0","upvote_count":"2","poster":"roger8978"}],"timestamp":"1634021100.0","comment_id":"75180","poster":"Joeylee","upvote_count":"6"},{"upvote_count":"1","content":"How could you possibly think adding a call to S3 would be faster than Provisioned IOPS? EBS is MUCH FASTER THAN S3 and its cheaper... Why add the call and trip to and from S3 when you can increase flow on the disks themselves for more availability of the SOLUTION? https://www.google.com/search?client=firefox-b-1-d&q=what+is+faster+ebs+or+s3%3F","timestamp":"1633962180.0","poster":"GSH","comment_id":"71856"},{"comment_id":"66682","poster":"drneon","content":"A is correct answer","upvote_count":"2","timestamp":"1633846320.0"},{"comment_id":"66005","poster":"leanin","upvote_count":"2","content":"I got for \"D\" since EBS is faster than S3.","timestamp":"1633749300.0"},{"timestamp":"1633566720.0","upvote_count":"3","comment_id":"49659","poster":"BillyC","content":"A is Correct!"},{"comment_id":"48113","comments":[{"timestamp":"1634150880.0","comment_id":"98388","poster":"JAWS1600","content":"Why would you use SQS on EBS . EBS can directly be consumed by application . Not a good architecture","upvote_count":"1"}],"poster":"Jshuen","upvote_count":"2","content":"i will go for D, \nbecause the question says 'analysis application that gets some files as Input, analyzes them and for each file writes some data in output to a big file.' . \nI will interpret the application writes and append the output onto a single file, so S3 may not be a good choice to append information to an existing object","timestamp":"1633044660.0"},{"content":"If its about availability then A, otherwise D.","upvote_count":"2","timestamp":"1632507960.0","comment_id":"39061","poster":"ReggieR2","comments":[{"upvote_count":"4","timestamp":"1632894000.0","poster":"sarah1","content":"I don't think this is correct.\nI believe the key here is that we require a method to ingest the images and then distribute the work. 'a' allows them to be stored in s3 (ingest), then distribute the work via sqs, and scale the ASG based on queue depth. 'd' does not provide for a method of ingesting the files in a way that multiple workers could process (ie: s3).","comment_id":"43445"}]},{"timestamp":"1632503040.0","comment_id":"37325","upvote_count":"3","poster":"AnNguyen","content":"Answer is A\n\"improve the availability\": As D, we just replace EBS type. It's not mention about HA for EBS"},{"comment_id":"21868","poster":"Tamili","timestamp":"1632425160.0","content":"Answer is A , S3 trigger can support SQS","upvote_count":"4"},{"comment_id":"18812","content":"I go for \"A\" too.","upvote_count":"5","poster":"skywalker","timestamp":"1632421560.0"},{"upvote_count":"5","timestamp":"1632305400.0","poster":"TechGuru","content":"A is correct answer","comment_id":"18157"}],"url":"https://www.examtopics.com/discussions/amazon/view/7193-exam-aws-certified-solutions-architect-professional-topic-1/","topic":"1","question_id":310,"choices":{"A":"S3 to store I/O files. SQS to distribute elaboration commands to a group of hosts working in parallel. Auto scaling to dynamically size the group of hosts depending on the length of the SQS queue","C":"S3 to store I/O files, SNS to distribute evaporation commands to a group of hosts working in parallel. Auto scaling to dynamically size the group of hosts depending on the number of SNS notifications","D":"EBS with Provisioned IOPS (PIOPS) to store I/O files SQS to distribute elaboration commands to a group of hosts working in parallel Auto Scaling to dynamically size the group ot hosts depending on the length of the SQS queue.","B":"EBS with Provisioned IOPS (PIOPS) to store I/O files. SNS to distribute elaboration commands to a group of hosts working in parallel Auto Scaling to dynamically size the group of hosts depending on the number of SNS notifications"},"exam_id":32,"unix_timestamp":1571990820,"question_images":[],"isMC":true,"answer_ET":"A","answers_community":["A (88%)","13%"],"answer":"A","answer_description":"","answer_images":[],"question_text":"You have a periodic image analysis application that gets some files in input, analyzes them and tor each file writes some data in output to a ten file the number of files in input per day is high and concentrated in a few hours of the day.\nCurrently you have a server on EC2 with a large EBS volume that hosts the input data and the results. It takes almost 20 hours per day to complete the process.\nWhat services could be used to reduce the elaboration time and improve the availability of the solution?"}],"exam":{"lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isMCOnly":false,"isImplemented":true,"name":"AWS Certified Solutions Architect - Professional","isBeta":false,"id":32,"provider":"Amazon"},"currentPage":62},"__N_SSP":true}