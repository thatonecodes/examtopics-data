{"pageProps":{"questions":[{"id":"GhxTX45eRCToQBIbWUmV","discussion":[{"comment_id":"529375","content":"c of cause","timestamp":"1642798440.0","poster":"Ni_yot","upvote_count":"1"},{"poster":"wpinfo","comment_id":"513159","upvote_count":"1","content":"Selected Answer: C\nanswer should be C. The AWS Schema Conversion Tool (AWS SCT) makes heterogeneous database migrations. https://aws.amazon.com/dms/schema-conversion-tool/","timestamp":"1640853960.0"},{"timestamp":"1640826420.0","comment_id":"512901","upvote_count":"1","content":"I go with C","poster":"notabot2"},{"comment_id":"512635","timestamp":"1640805960.0","poster":"RamCrk","upvote_count":"1","content":"C , preference ,because must design a heterogeneous\nhttps://aws.amazon.com/dms/schema-conversion-tool/"},{"poster":"rootx","content":"Selected Answer: C\nC seems to be the best fit","comment_id":"511928","timestamp":"1640763960.0","upvote_count":"1"}],"isMC":true,"answer_ET":"A","question_id":626,"url":"https://www.examtopics.com/discussions/amazon/view/68879-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"topic":"1","unix_timestamp":1640763960,"answer":"C","timestamp":"2021-12-29 08:46:00","answer_description":"Reference:\nhttps://docs.aws.amazon.com/dms/latest/sbs/dms-sbs-welcome.html","answers_community":["C (100%)"],"exam_id":32,"choices":{"D":"Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.","A":"Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities.","B":"Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.","C":"Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MeSQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS."},"question_text":"A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises installation of a\nMicrosoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service. A solutions architect must design a heterogeneous database migration on AWS.\nWhich solution will meet these requirements?","answer_images":[]},{"id":"aDxKqcuypPKWeeGO4dRX","question_images":[],"answer_images":[],"answer_ET":"B","isMC":true,"question_text":"A company has an application that generates reports and stores them in an Amazon bucket Amazon S3 bucket. When a user accesses their report, the application generates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that anyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.\nWhich set of action will immediately remediate the security issue without impacting the application's normal workflow?","answers_community":["D (100%)"],"timestamp":"2020-11-04 02:59:00","topic":"1","discussion":[{"timestamp":"1634805960.0","comment_id":"259133","content":"I'll go with D","poster":"Ebi","upvote_count":"7"},{"upvote_count":"6","comments":[{"content":"from documentation \nSetting this option to TRUE causes Amazon S3 to ignore all public ACLs on a bucket and any objects that it contains. This setting enables you to safely block public access granted by ACLs while still allowing PUT Object calls that include a public ACL (as opposed to BlockPublicAcls, which rejects PUT Object calls that include a public ACL). Enabling this setting doesn't affect the persistence of any existing ACLs and doesn't prevent new public ACLs from being set.","poster":"user0001","timestamp":"1652806920.0","comment_id":"602995","upvote_count":"1"}],"poster":"Waiweng","comment_id":"356352","timestamp":"1635721440.0","content":"it's D"},{"content":"https://aws.amazon.com/about-aws/whats-new/2018/02/aws-trusted-advisors-s3-bucket-permissions-check-is-now-free/\nhttps://docs.aws.amazon.com/awssupport/latest/user/security-checks.html#amazon-s3-bucket-permissions","timestamp":"1673069820.0","upvote_count":"1","poster":"maxh8086","comment_id":"768258"},{"upvote_count":"2","content":"Selected Answer: D\nD sounds better.","timestamp":"1656370320.0","poster":"kangtamo","comment_id":"623524"},{"content":"\" The company's security staff determined that the files are accessible to the public and may be downloaded without authentication\" mean want public access right? why D? so confused","timestamp":"1642742460.0","comment_id":"528947","upvote_count":"1","poster":"GeniusMikeLiu"},{"poster":"cldy","upvote_count":"2","comment_id":"498315","timestamp":"1639113660.0","content":"D. Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcis option to TRUE on the bucket."},{"poster":"AzureDP900","upvote_count":"2","comment_id":"492018","timestamp":"1638402600.0","content":"D is right. \nThe S3 bucket is allowing public access and this must be immediately disabled. Setting the IgnorePublicAcls option\nto TRUE causes Amazon S3 to ignore all public ACLs on a bucket and any objects that it contains.\nThe other settings you can configure with the Block Public Access Feature are:\no BlockPublicAcls – PUT bucket ACL and PUT objects requests are blocked if granting public access.\no BlockPublicPolicy – Rejects requests to PUT a bucket policy if granting public access.\no RestrictPublicBuckets – Restricts access to principles in the bucket owners’ AWS account."},{"comment_id":"443378","upvote_count":"2","timestamp":"1636265520.0","poster":"andylogan","content":"It's D - pre-signed URL is to allows unauthenticated users access to the bucket in private"},{"poster":"tgv","timestamp":"1636182960.0","upvote_count":"1","content":"DDD\n---","comment_id":"436344"},{"poster":"WhyIronMan","comment_id":"413340","content":"I'll go with D","timestamp":"1636126860.0","upvote_count":"1"},{"poster":"Kian1","upvote_count":"5","comment_id":"293454","content":"going with D","timestamp":"1635646980.0"},{"upvote_count":"4","comment_id":"256091","poster":"Bulti","timestamp":"1634057040.0","content":"Answer is D. Remember that the purpose of creating a pre-signed URL is to allows unauthenticated users access to the bucket or the objects in the bucket which are private. So if someone can still access the bucket then the buckets or the objects in the bucket have been granted a public ACL which needs to be blocked and the way to do that is by using the IgnorePublicAcls setting."},{"poster":"petebear55","timestamp":"1633871880.0","upvote_count":"1","comments":[{"timestamp":"1635184200.0","poster":"shammous","comment_id":"277886","upvote_count":"2","content":"B won't \"immediately remediate the security issue\". D would."}],"comment_id":"255855","content":"B could be the answer .. however it would probably AWS Macie which does the needful. .. I will go for D in this case .. however i'm not hundred percent convinced and think the question is poorly written"},{"timestamp":"1633736700.0","content":"Correct is D.","poster":"T14102020","upvote_count":"1","comment_id":"244969"},{"timestamp":"1632947280.0","poster":"jackdryan","upvote_count":"2","comment_id":"232646","content":"I'll go with D"},{"comment_id":"217565","poster":"smartassX","timestamp":"1632136320.0","content":"D --> \"IgnorePublicAcis\" --> \"Setting this option to TRUE causes Amazon S3 to ignore all public ACLs on a bucket and any objects that it contains. This setting enables you to safely block public access granted by ACLs while still allowing PUT Object calls that include a public ACL (as opposed to BlockPublicAcls, which rejects PUT Object calls that include a public ACL). Enabling this setting doesn't affect the persistence of any existing ACLs and doesn't prevent new public ACLs from being set.\"","upvote_count":"4"},{"content":"D\nhttps://aws.amazon.com/s3/features/block-public-access/","timestamp":"1632099720.0","poster":"asldavid","comments":[{"comment_id":"218276","poster":"Gmail78","upvote_count":"1","comments":[{"timestamp":"1632731460.0","poster":"avland","content":"Pretty sure there's a typo there. Should be IgnorePublicAcls.\n\nBlock public access to buckets and objects granted through any access control lists (ACLs)\nS3 will ignore all ACLs that grant public access to buckets and objects.","upvote_count":"3","comment_id":"226771"}],"content":"what is IgnorePublicAcis? I would go with A instead","timestamp":"1632415920.0"},{"timestamp":"1632663360.0","comment_id":"225641","poster":"Kelvin1477","upvote_count":"2","content":"Support D too as mention pre-signed url that is shared to the user will not be block but the policy will block any other public access: \nhttps://acloud.guru/forums/s3-masterclass/discussion/-LsBZBXjnnNdi4dT1Czi/block%20public%20access%20vs%20pre-signed%20URL%20access"}],"comment_id":"212416","upvote_count":"3"}],"choices":{"A":"Create an AWS Lambda function that applies all policy for users who are not authenticated. Create a scheduled event to invoke the Lambda function.","B":"Review the AWS Trusted advisor bucket permissions check and implement the recommend actions.","C":"Run a script that puts a Private ACL on all of the object in the bucket.","D":"Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcis option to TRUE on the bucket."},"answer":"D","exam_id":32,"unix_timestamp":1604455140,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/35983-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":627},{"id":"acgP2Ad6HULwk9VyyTR0","unix_timestamp":1604565600,"question_images":[],"question_id":628,"answer_ET":"B","timestamp":"2020-11-05 09:40:00","url":"https://www.examtopics.com/discussions/amazon/view/36131-exam-aws-certified-solutions-architect-professional-topic-1/","answers_community":["B (55%)","D (45%)"],"discussion":[{"timestamp":"1632189180.0","poster":"liono","comments":[{"timestamp":"1635755640.0","comment_id":"396389","poster":"DashL","upvote_count":"4","content":"To connect to a VPC, it is required to connect to a Private Virtual interface over Direct connect. I guess an AWS document will be more accurate than any blog post:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html"},{"upvote_count":"5","content":"Private VIFs do not provide encryption.. Public VIFs can via IPSEC. you also cannot establish a VPN connection without a Public VIF.","comment_id":"638933","timestamp":"1659055260.0","poster":"helpaws"},{"timestamp":"1653330780.0","content":"it is D, there is no requirement to access public services so no need for public VIP","upvote_count":"2","poster":"user0001","comment_id":"606280","comments":[{"poster":"Byrney","content":"AWS S2S VPN is a public service, so a public VIF is required","timestamp":"1667868120.0","upvote_count":"2","comment_id":"713374"}]}],"upvote_count":"28","comment_id":"213291","content":"B\nhttps://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/"},{"timestamp":"1634561760.0","content":"Answer is B. https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/. Remember that to connect to services such as EC2 using just Direct Connect you need to create a private VIF. However if you want to encrypt the traffic flowing through DirectConnect, you will need to use the public VIF of DX to create a VPN connection that will allow access to AWS services such as S3, EC2 etc. The video describes this.","comment_id":"256104","upvote_count":"13","poster":"Bulti"},{"upvote_count":"1","timestamp":"1696681560.0","content":"Selected Answer: D\nIts explained here. Prevously public VIF was needed not now\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/private-ip-dx.html","comment_id":"1027324","poster":"nhorcajada"},{"comments":[{"timestamp":"1672315080.0","content":"Sorry I mean B. B is the right answer here.","upvote_count":"2","comments":[{"content":"Can a moderator change my vote to B, please?","poster":"evargasbrz","timestamp":"1672315140.0","comment_id":"760966","upvote_count":"2"}],"comment_id":"760965","poster":"evargasbrz"}],"poster":"evargasbrz","upvote_count":"2","content":"Selected Answer: D\nD is the right answer here.\nAs you can check in this document: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\n\nyou really need to use a Public VIFs to access all AWS public services such as Amazon virtual private gateway IPsec endpoint.","timestamp":"1672314960.0","comment_id":"760962"},{"upvote_count":"3","timestamp":"1666168020.0","comment_id":"698820","poster":"JohnPi","content":"Selected Answer: B\nyou need public VIF.\nTo implement a Private IP VPN with AWS Direct Connect you need a transit virtual interface, DXG, transit gateway"},{"poster":"Enigmaaaaaa","content":"This is clearly stated in AWS documentation \nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\nThe answer must B as IPSec tunnels are always public","comment_id":"640081","upvote_count":"2","timestamp":"1659266100.0"},{"timestamp":"1659055680.0","upvote_count":"1","poster":"hilft","comment_id":"638935","content":"got the DX. D > B"},{"poster":"aandc","comment_id":"625970","timestamp":"1656741420.0","content":"B you need to use the public VIF of DX to create a VPN connection \nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html","upvote_count":"1"},{"content":"Selected Answer: D\nD for me","comment_id":"624379","upvote_count":"1","comments":[{"upvote_count":"2","content":"Just ignore D, after asking my experienced senior SA. It should be B, cause now he is also doing a same solution for a company has the same case in this question","poster":"TechX","timestamp":"1656471720.0","comment_id":"624389"}],"poster":"TechX","timestamp":"1656470940.0"},{"comment_id":"621794","timestamp":"1656091980.0","poster":"Ddssssss","upvote_count":"1","content":"I don't understand why it cant be D?? Just because 90% of the time you would use the Public interface doesn't mean you cant use the private. Its a valid DX configuration option with IPSEC tunnel. \nPrivate virtual interface: A private virtual interface should be used to access an Amazon VPC using private IP addresses.\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html\n\nIt is also clearly explain in this blog which references all the details in any AWS doc. \nhttps://jayendrapatil.com/tag/direct-connect/\n\nThis doc is also only 2 days old. but with the use of a transit GW you can use Private IP and IPSEC. \n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/introducing-aws-site-to-site-vpn-private-ip-vpns/"},{"comment_id":"596272","content":"Selected Answer: B\no connect to services such as EC2 using just Direct Connect you need to create a private VIF. However if you want to encrypt the traffic flowing through DirectConnect, you will need to use the public VIF of DX to create a VPN connection that will allow access to AWS services such as S3, EC2.","poster":"Hasitha99","upvote_count":"1","timestamp":"1651541580.0"},{"comment_id":"576414","content":"Selected Answer: D\nI would choose D. There is no internet connection. And the traffic is between corporate network and VPC. Most likely, it only involves private IP addresses, which only requires privhttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-professional/view/14/#ate virtual interface over DX.","upvote_count":"1","timestamp":"1648408860.0","poster":"azure_kai"},{"timestamp":"1646120400.0","upvote_count":"1","comment_id":"558647","poster":"jyrajan69","content":"There is no debate, link from liono clearly shows step by step solution. Answer is B"},{"upvote_count":"1","timestamp":"1644823980.0","poster":"lifebegins","comment_id":"546958","content":"Answer is D:\nWe shoud go over the \nWith AWS Direct Connect and AWS Site-to-Site VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html"},{"comments":[{"timestamp":"1644751680.0","content":"Private VIFs do not provide encryption.. Public VIFs can via IPSEC. you also cannot establish a VPN connection without a Public VIF.","poster":"futen0326","comment_id":"546420","upvote_count":"2","comments":[{"upvote_count":"1","content":"You can with a Transit VIF \"Private IP VPN is deployed on top of Transit VIFs\" -- https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-aws-site-to-site-vpn-private-ip-vpns/ Answer is still B though.","comment_id":"689639","timestamp":"1665257400.0","poster":"Naj_64"}]}],"content":"My answer is D.\n\nWhy do we need public virtual interface for communication between laptop and VPC over DX? There are no requirements of accessing from internet. It should be PRIVATE virtual interface.","poster":"HellGate","comment_id":"543146","timestamp":"1644333360.0","upvote_count":"1"},{"poster":"GV19","content":"Selected Answer: B\nto establish VPN over DX, Public VIF is required, Only Option B has this detail;","upvote_count":"2","timestamp":"1642800780.0","comment_id":"529413"},{"timestamp":"1638862620.0","comment_id":"495736","upvote_count":"1","content":"VPC does not have internet connection.\nPrivate virtual interface: used to access an VPC using private IP addresses.\nPublic virtual interface: can access all AWS public services using public IP addresses.","poster":"KiraguJohn"},{"content":"B is right answer\nCreate a new public virtual interface for the existing DX connection, and create a new VPN that connects to the VPC over the DX public virtual interface.","timestamp":"1638402660.0","comment_id":"492019","poster":"AzureDP900","upvote_count":"1"},{"comment_id":"480738","upvote_count":"2","timestamp":"1637242200.0","content":"Using PRIVATE virtual interface to connect to legacy application in an Amazon EC2. Answer D is correct.","poster":"Ronon"},{"comment_id":"443380","content":"It's B as you need public VIF","poster":"andylogan","upvote_count":"1","timestamp":"1636285740.0"},{"poster":"blackgamer","timestamp":"1636140300.0","content":"The answer is B,","upvote_count":"1","comment_id":"435954"},{"poster":"tgv","comment_id":"434874","upvote_count":"1","timestamp":"1636056000.0","content":"BBB\n---"},{"timestamp":"1636022220.0","poster":"denccc","upvote_count":"1","content":"B: \nA private virtual interface should be used to access an Amazon VPC using private IP addresses. \nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html","comment_id":"434638"},{"poster":"WhyIronMan","comment_id":"413343","content":"I'll go with D","upvote_count":"2","timestamp":"1635817260.0"},{"content":"it's B","comment_id":"356358","upvote_count":"4","poster":"Waiweng","timestamp":"1635363240.0"},{"comment_id":"353681","poster":"tvs","upvote_count":"2","timestamp":"1635354900.0","content":"B - https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/"},{"upvote_count":"4","timestamp":"1635330720.0","comment_id":"343881","poster":"Amitv2706","content":"answer is B.\nD can not be correct due to this reason as per Tutorials Dojo : \"is incorrect because you must use a public virtual interface for your AWS Direct Connect (DX) connection and not a private one. You won't be able to establish an encrypted VPN along with your DX connection if you create a private virtual interface.\""},{"content":"Requirement \" all data in transit must be encrypted between users and the VPC\" only filled by Public VIF B","upvote_count":"1","timestamp":"1635274320.0","comment_id":"342985","poster":"ExtHo"},{"poster":"kiev","content":"B is the right answer. Question answered by Neal Davis","upvote_count":"2","comment_id":"296777","timestamp":"1635171600.0"},{"poster":"Kian1","upvote_count":"2","timestamp":"1635072120.0","comment_id":"293459","content":"going with B"},{"poster":"LB","timestamp":"1634994480.0","content":"B- you need public vif","comment_id":"292466","upvote_count":"2"},{"poster":"Ebi","content":"Answer is B","upvote_count":"3","comment_id":"281600","timestamp":"1634928780.0"},{"upvote_count":"2","poster":"kopper2019","comments":[{"comment_id":"277445","timestamp":"1634917920.0","content":"Its running over IPSEC, thus need a Public VIF.","poster":"rcher","upvote_count":"4"},{"content":"Your link says that it connects to the *public* VIF","timestamp":"1635234900.0","upvote_count":"1","poster":"sarah_t","comment_id":"334453"}],"content":"D is the answer, some says B but if you need to connect to S3 for example you would net a public virtual interface but for VPC you need a private virtual interface\n\nPrivate virtual interface: Access an Amazon VPC using private IP addresses.\n\nPublic virtual interface: Access AWS services from your on-premises data center. Allow AWS services, or AWS customers access to your public networks over the interface instead of traversing the internet.\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html","comment_id":"267606","timestamp":"1634744100.0"},{"content":"B - form the VPN connection with VGW at VPC using Public VIF\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html","timestamp":"1634196060.0","poster":"RLai","upvote_count":"2","comment_id":"252017"},{"timestamp":"1633784340.0","content":"C 100% is the correct answer. You can combine DX and site to site VPN for encryption. Read up information in this link and the pdf version of it for more details. https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html","comments":[{"comments":[{"timestamp":"1635403920.0","content":"what are you saying my friend","upvote_count":"1","comment_id":"395927","poster":"MrCarter"}],"comment_id":"252132","timestamp":"1634421960.0","upvote_count":"1","poster":"darthvoodoo","content":"Reading the documentation again, I would say A is the correct answer. AWS. The AWS client VPN to encrypt the traffic on the client side and send it through the Direct connect connection. This method has no negative impact on the throughput."}],"upvote_count":"1","comment_id":"251808","poster":"darthvoodoo"},{"upvote_count":"2","poster":"doubeguy","comment_id":"247247","timestamp":"1633566480.0","content":"B\nI use this configuration."},{"timestamp":"1633350660.0","content":"D\nIt has to be private VIF. Check out this article that clears up the confusion. https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/","comment_id":"239745","upvote_count":"2","poster":"SPV2117"},{"upvote_count":"3","comment_id":"235819","timestamp":"1633152900.0","content":"VPN is always over a public VIF","poster":"Crabs"},{"comment_id":"232653","timestamp":"1633094640.0","content":"I'll go with D","poster":"jackdryan","upvote_count":"3"},{"timestamp":"1633023180.0","comments":[{"upvote_count":"3","poster":"GopiSivanathan","comment_id":"236133","comments":[{"comments":[{"poster":"doubeguy","comment_id":"247250","timestamp":"1633666200.0","content":"\"consistent network performance\" means DX","upvote_count":"1"}],"timestamp":"1633546320.0","upvote_count":"1","comment_id":"246734","content":"Answer is A: The Laptop user should use Client VPN","poster":"GopiSivanathan"}],"timestamp":"1633267740.0","content":"DX Private VIF doesn't offer encryption https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html\n \nWith AWS Direct Connect and AWS Site-to-Site VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections."}],"upvote_count":"5","comment_id":"228241","content":"Answer is D\nA is INCORRECT because installing a VPN client on users desktops to connect to the EC2s in the VPC will decrease the network throughput and bypass the DX by going over the internet.\nB is INCORRECT because the question specifically states EC2 in a VPC. A Public VIF would be used for services like S3.\nC is INCORRECT because a site-to-site VPN will decrease network performance and bypass the DX.\nD is CORRECT because a Private VIF will allow users to connect to private resources like EC2.","poster":"XRiddlerX"},{"comment_id":"225648","timestamp":"1632897780.0","poster":"Kelvin1477","content":"either A or D,\nfor direct connect to vpc, usually use private virtual interface. public virtual interface is to connect to AWS managed service such as S3\nI would still prefer A, as question mention protect data in transit between user and vpc. if just using private virtual interface default option thru direct connect, the encryption is only happen between the interface and customer gateway","upvote_count":"2"},{"comment_id":"224041","timestamp":"1632856020.0","upvote_count":"8","poster":"arulrajjayaraj","content":"D \nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/images/direct_connect_overview.png"},{"upvote_count":"3","comment_id":"220348","comments":[{"timestamp":"1634854260.0","content":"Using existing direct connect connection,you gotta use a Public vif since its running over IPSEC","poster":"rcher","upvote_count":"1","comment_id":"277443"}],"content":"isn't D? To connect to your resources hosted in an Amazon Virtual Private Cloud (Amazon VPC) using their private IP addresses, use a private virtual interface?","timestamp":"1632820140.0","poster":"beso"},{"comments":[{"comment_id":"219213","content":"B\nagree with liono","poster":"SZU","timestamp":"1632572400.0","upvote_count":"1"}],"comment_id":"218821","content":"D\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/","timestamp":"1632528660.0","poster":"Raysquared","upvote_count":"1"}],"answer":"B","isMC":true,"question_text":"A company hosts a legacy application that runs on an Amazon EC2 instance inside a VPC without internet access. Users access the application with a desktop program installed on their corporate laptops. Communication between the laptops and the VPC flows through AWS Direct Connect (DX). A new requirement states that all data in transit must be encrypted between users and the VPC.\nWhich strategy should a solutions architect use to maintain consistent network performance while meeting this new requirement?","answer_description":"","topic":"1","exam_id":32,"answer_images":[],"choices":{"D":"Create a new private virtual interface for the existing DX connection, and create a new VPN that connects to the VPC over the DX private virtual interface.","B":"Create a new public virtual interface for the existing DX connection, and create a new VPN that connects to the VPC over the DX public virtual interface.","C":"Create a new Site-to-Site VPN that connects to the VPC over the internet.","A":"Create a client VPN endpoint and configure the laptops to use an AWS client VPN to connect to the VPC over the internet."}},{"id":"olDuzlZHeeEjSNADVPWo","discussion":[{"poster":"cloudgc","content":"A&C are correct.\nNLB will see traffic from interface endpoint subnet and logging service subnet.\nLogging service SG will see traffic only from NLB IP.","comments":[{"content":"NLB will Client IP preservation enabled by default. But Client IP preservation has no effect on inbound traffic from AWS PrivateLink. The source IP of the AWS PrivateLink traffic is always the private IP address of the Network Load Balancer. \nSo Logging service SG will see traffic only from NLB IP.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html","comment_id":"902936","upvote_count":"2","timestamp":"1684649940.0","poster":"hnsuper"}],"comment_id":"233622","upvote_count":"37","timestamp":"1633762440.0"},{"poster":"Bulti","content":"A&C. The client of the Logging service running on EC2 is NLB and not the interface endpoint. the flow is Client->VPCE(PrivateLink)->NLB->Logging service. So the answer is A & C 100%.","timestamp":"1634082540.0","comments":[{"content":"Agreed, and i implemented this architecture for my work.\n\nNLB sits in front of the Logging Services, so the NACL and Sec groups for the corresponding logging instances (and its subnet) need to check for the NLB ingress. A/C for me","poster":"rcher","timestamp":"1634481660.0","upvote_count":"2","comment_id":"277447"},{"upvote_count":"1","content":"you got it right.","comment_id":"313655","timestamp":"1634699460.0","poster":"nitinz"}],"comment_id":"256110","upvote_count":"13"},{"comment_id":"1107170","timestamp":"1703708520.0","upvote_count":"1","content":"A: NACLs for communication between NLB and logging service subnets are irrelevant as they reside within the same VPC.\nC: Security groups for communication between NLB and logging service EC2 instances are managed internally by the NLB.\nD: Clients don't directly communicate with logging service EC2 instances; they interact via the NLB.\nregarding B, For successful communication, NACLs attached to the logging service subnets must explicitly allow inbound traffic from the interface endpoint subnets.\nThis ensures that traffic originating from the client services, passing through the interface endpoints, is permitted to enter the logging service subnets and reach the EC2 instances hosting the logging application.","poster":"CProgrammer"},{"upvote_count":"1","timestamp":"1678059660.0","poster":"[Removed]","content":"Selected Answer: AC\nhttps://aws.amazon.com/premiumsupport/knowledge-center/security-network-acl-vpc-endpoint/\n\n1. The Inbound security group rules of the Network Load Balancer’s targets allow communication from the private IP address of the Network Load Balancer nodes\nThe rules within the network ACL associated with the Network Load Balancer’s targets allow communication from the private IP address of the Network Load Balancer nodes","comment_id":"830427"},{"comment_id":"689503","upvote_count":"1","content":"Selected Answer: BD\nB +D is corect","poster":"JohnPi","timestamp":"1665246060.0"},{"upvote_count":"1","content":"I think A&D and include C.\n\nWith NLB, for security group attached to target EC2 instance (front by NLB) need to allow not only IP of NLB but also IP from client (If target type is an instance), assume that we use EC2 only, so target type instance is fitted.\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/security-group-load-balancer/","timestamp":"1658886420.0","comment_id":"637735","poster":"foureye2004"},{"poster":"kangtamo","comment_id":"623521","content":"Selected Answer: AC\nAgree with AC: NLB","upvote_count":"2","timestamp":"1656369480.0"},{"upvote_count":"1","comment_id":"604939","timestamp":"1653147660.0","comments":[{"timestamp":"1655337300.0","upvote_count":"2","comment_id":"617005","content":"the singulars and plurals in A are off. First it says subnets, then subnet, then subnets, then subnet. I think A is correct, just bad grammar.","poster":"Ddssssss"}],"content":"A is not correct.\nThe Q states \"The logging service is deployed in many SUBNETS\", A states \"Check that the NACL is attached to the logging service SUBNET\"","poster":"bobsmith2000"},{"poster":"cldy","comment_id":"497787","timestamp":"1639059720.0","content":"A. Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.\nC. Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.","upvote_count":"2"},{"timestamp":"1638403560.0","upvote_count":"2","poster":"AzureDP900","content":"It seems B & D for me. I need to revisit this question again !","comment_id":"492022"},{"content":"From this resource https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#client-ip-preservation:\n\"Client IP preservation has no effect on AWS PrivateLink traffic. The source IP of the AWS PrivateLink traffic is always the private IP address of the Network Load Balancer.\" ... hence the answer is A&C","comment_id":"483611","timestamp":"1637523600.0","poster":"nsei","upvote_count":"3"},{"timestamp":"1636012140.0","upvote_count":"1","comment_id":"443428","poster":"andylogan","content":"It's A C since the client of the Logging service running on EC2 is NLB"},{"timestamp":"1635947700.0","poster":"wakame","content":"Hi guys,\nNLB does not do Source NAT unlike ALB, but is the correct answer still A & C?","upvote_count":"1","comment_id":"441629","comments":[{"comment_id":"441638","timestamp":"1635981840.0","content":"I found out that there are the following specifications, so I solved it.\n https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#client-ip-preservation\nIn the case of using PrivateLink, NLB has a specification that It transrates Source IP to NLB Private IP.","upvote_count":"3","poster":"wakame"}]},{"upvote_count":"1","poster":"tgv","content":"AAA CCC\n---\nI don't understand what NLB not having security group has to do with A/C. \nI'm thinking that the clients are sending traffic to the NLB (not some kind of round robin directly on the EC2 instances). \nThe communication between NLB and EC2 instances still has to be configured. It doesn't work out of the box","comment_id":"434880","timestamp":"1635876480.0"},{"upvote_count":"2","content":"A and C","timestamp":"1635847740.0","comment_id":"433723","poster":"blackgamer"},{"poster":"WhyIronMan","content":"I'll go with B,D","comment_id":"413344","timestamp":"1635762900.0","upvote_count":"4"},{"content":"B&D.\nNLB is not like ALB. it just passes the traffic to EC2. EC2 needs to allow ingress from outside.","upvote_count":"2","timestamp":"1635757500.0","comment_id":"403181","poster":"nopenope111"},{"comment_id":"383616","upvote_count":"3","poster":"kpcert","content":"I think the answer is B and D. NACL and Security group of EC2 logging service to allow traffic from client subnets, It is NLB in front of EC2 , NLB will preserve the client IPs and pass on the client details and source IPs of client to EC2, so the Network ACL and Security group of logging service should have the allow rule for the ip range of client subnets subnets.","timestamp":"1635658920.0"},{"content":"it's A&C","upvote_count":"3","timestamp":"1635077160.0","comment_id":"356365","poster":"Waiweng"},{"poster":"Amitv2706","content":"A and C.\n\nConsidering this flow : Client Services --> VPC Interface Endpoint -- > NLB(Its own subnet/SG) --> Logging Service (Its own subnet/SG)\n\nA- Correct\nB- Incorrect - NLB is between VPC Int End point and Logging Service\nC- Correct\nD- Incorrect - Logging Service never get traffic directly from Client Service\nE -Incorrect - Logging Service never get traffic directly from Interface Endpoint","upvote_count":"8","comment_id":"343890","comments":[{"upvote_count":"4","comment_id":"357011","poster":"Amitv2706","timestamp":"1635526200.0","content":"Correction - E - Incorrect - No Security Group applicable for NLB"}],"timestamp":"1634831580.0"},{"comment_id":"321733","content":"i go with A & C","timestamp":"1634805720.0","poster":"alisyech","upvote_count":"1"},{"upvote_count":"1","timestamp":"1634792820.0","poster":"wasabidev","content":"AC for me","comment_id":"313916"},{"poster":"Ebi","upvote_count":"5","timestamp":"1634607540.0","content":"AC for me","comment_id":"284588"},{"upvote_count":"4","timestamp":"1634417400.0","content":"Looks like A and C for me\nhttps://aws.amazon.com/premiumsupport/knowledge-center/elb-connectivity-troubleshooting/","poster":"kopper2019","comment_id":"269961"},{"timestamp":"1634270880.0","upvote_count":"2","poster":"Justu","comment_id":"267908","content":"Bulti is right, A&C is the correct answer"},{"upvote_count":"1","poster":"rkbala","timestamp":"1634130840.0","content":"question didn't mention about custom NACL. Default NACL allows all traffic. So need to check only security group configration. ANS could be DE","comment_id":"267261"},{"comment_id":"250540","timestamp":"1633854660.0","content":"BD : Network Load Balancers do not have associated security groups. Therefore, the security groups for your targets must use IP addresses to allow traffic from the load balancer. So If you do not want to grant access to the entire VPC CIDR, you can grant access to the private IP addresses used by the load balancer nodes","upvote_count":"3","poster":"spring21"},{"timestamp":"1633775640.0","content":"A and C","upvote_count":"2","poster":"joos","comment_id":"240086"},{"comments":[{"comment_id":"395930","poster":"MrCarter","upvote_count":"1","content":"my man","timestamp":"1635714060.0"}],"poster":"jackdryan","comment_id":"232659","upvote_count":"4","content":"I'll go with B,D","timestamp":"1633096740.0"},{"content":"B&D, since Network Load Balancers do not have associated security groups.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/security-group-load-balancer/","comments":[{"content":"It is EC2 security group, no where it is mention NLB security group","poster":"tvs","upvote_count":"1","comment_id":"353688","timestamp":"1635055740.0"}],"comment_id":"220352","timestamp":"1633004340.0","upvote_count":"1","poster":"beso"},{"upvote_count":"1","comment_id":"218286","content":"B & D\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/connect-endpoint-service-vpc/","poster":"AK2020","timestamp":"1632861660.0"},{"timestamp":"1632379800.0","content":"why B?\nThe logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets","upvote_count":"1","comments":[{"poster":"keos","timestamp":"1632791700.0","upvote_count":"8","comment_id":"214996","content":"go for A&C, \"the source IP addresses are the private IP addresses of the load balancer nodes\", https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html"}],"comment_id":"214994","poster":"keos"},{"content":"B & D NLB not has Security Group","upvote_count":"1","timestamp":"1632252300.0","poster":"A_New_Guy","comment_id":"214032"},{"timestamp":"1632228300.0","comments":[{"comment_id":"233598","content":"no security groups for NLB.","timestamp":"1633534320.0","upvote_count":"3","poster":"cloudgc"}],"comment_id":"213293","upvote_count":"3","content":"B & E, you need to allow ingress from interface endpoints towards NLB","poster":"liono"},{"timestamp":"1632070320.0","content":"B & D\nhttps://aws.amazon.com/premiumsupport/knowledge-center/security-network-acl-vpc-endpoint/","comment_id":"212828","poster":"asldavid","upvote_count":"7"}],"isMC":true,"answer_ET":"AC","question_id":629,"url":"https://www.examtopics.com/discussions/amazon/view/36058-exam-aws-certified-solutions-architect-professional-topic-1/","question_images":[],"topic":"1","unix_timestamp":1604503440,"answer_description":"","answer":"AC","timestamp":"2020-11-04 16:24:00","answers_community":["AC (75%)","BD (25%)"],"exam_id":32,"choices":{"A":"Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.","B":"Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances.","D":"Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the clients.","C":"Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.","E":"Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets."},"question_text":"A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between the client services and the logging service.\nIn each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.\nWhich combination of steps should a solutions architect take to resolve this issue? (Choose two.)","answer_images":[]},{"id":"DMapEXOSziS2Mnz0TPij","question_images":[],"answer_images":[],"answer_ET":"C","question_text":"A company is refactoring an existing web service that provides read and write access to structured data. The service must respond to short but significant spikes in the system load. The service must be fault tolerant across multiple AWS Regions.\nWhich actions should be taken to meet these requirements?","isMC":true,"answers_community":["C (83%)","D (17%)"],"timestamp":"2020-11-04 16:27:00","topic":"1","discussion":[{"timestamp":"1632870060.0","comment_id":"256375","upvote_count":"26","content":"Its between B and C. I think A and D are out. A is out because of DocumentDB and D is out because of multi value. Between B and C, I think C is a better because S3 is usually used as a static web site and not for writing dynamic data (in this case structured data). Option C is a standard way of designing an application using a middle tier and a data tier where the middle tier is load balanced and is in an auto scaling group. Moreover DynamoDb can be used for both structured and semi-structured data. The latency routing policy with health checks will result in routing the traffic to the region with low latency in case the ALB endpoint is considered healthy or else it will be routed to the other region. So I will go with C.","comments":[{"content":"D can work fine also\n- Aurora for structured value\n- Fault Tolerant because Route 53 with MVA policy allows health checks like it would w/ Failover policy : https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-multivalue.html\n- ASG for spikes\n- 1 ALB, simpler architecture than C where there are 2 ALB (one internal and one external)","poster":"tekkart","timestamp":"1635460440.0","comment_id":"424691","comments":[{"comments":[{"content":"Cloudfront can support API gateway and lambda.https://aws.amazon.com/tw/cloudfront/?nc=sn&loc=0","upvote_count":"1","comment_id":"484052","timestamp":"1637573760.0","poster":"acloudguru"}],"content":"Because of the word 'refactoring' it's right may be ECS in answer C where ASG and ALB are also available\n\nFor A and B, I don't see how CloudFront can have API Gateway as origin... A and B would be ruled out because CloudFront can have : Web server, S3 bucket, or Elemental Media PAckage/Store for VOD as origins.","comment_id":"424696","upvote_count":"1","poster":"tekkart","timestamp":"1635499140.0"}],"upvote_count":"3"},{"poster":"aws_arn_name","timestamp":"1635026760.0","upvote_count":"2","content":"\"short but significant spikes\" , i think Lambda is better with this than ASG so answer should be B","comment_id":"367733"},{"timestamp":"1636147380.0","poster":"joe16","content":"In S3 CRR is not immediate(AWS Docs - \"Most objects replicate within 15 minutes, but sometimes replication can take a couple hours or more\"). So B is not an option as solution. \nDDB Global tables have sync latency of less than a sec - \"In a global table, a newly written item is usually propagated to all replica tables within a second.\"\nI will go with C","comment_id":"456652","upvote_count":"2"}],"poster":"Bulti"},{"comment_id":"258986","comments":[{"timestamp":"1633124700.0","content":"You're overthinking it. We only need to write to the primary.","comment_id":"264338","poster":"nqobza","comments":[{"poster":"Ebi","timestamp":"1633776720.0","comment_id":"284555","upvote_count":"3","content":"With multi value routing in route 53 you should be able to write in each region, otherwise you need manage failover if primary fails which has not been mentioned in this answer, I still go with C"}],"upvote_count":"3"},{"timestamp":"1634103540.0","content":"This is another good point.","comment_id":"298181","poster":"gpark","upvote_count":"1"},{"comment_id":"334673","timestamp":"1634493720.0","content":"aurora does support multi master: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html\n'In a multi-master cluster, all DB instances can perform write operations.'","comments":[{"upvote_count":"1","timestamp":"1636135560.0","poster":"Liongeek","comment_id":"449824","content":"\"Currently, all DB instances in a multi-master cluster must be in the same AWS Region.\""},{"timestamp":"1635271860.0","poster":"MrCarter","content":"MULTI MASTER IS A REGIONAL SERVICE NOT MULTI REGION!!","comment_id":"395936","upvote_count":"8"}],"poster":"dart93","upvote_count":"3"}],"content":"Answer is C.\nD is not the right answer, although Aurora is better choice for structured data, but Aurora Global database supports one master only, so other regions do not support write.","timestamp":"1633027740.0","upvote_count":"18","poster":"Ebi"},{"timestamp":"1686405540.0","upvote_count":"2","content":"Selected Answer: C\nC is right.\nHere I just want to emphasize why B is WRONG.\n\"Create an Amazon CloudFront distribution in each Region\" this looks strange to me.\nYou CloudFront distrubtion don't have Region concept, and won't be deployed in Regions!\nYou need to specify in Price Class Options to tell what kinds of Edge locations you hope your distribtion to be deployed, see the link https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html\n\"Assign the company's domain as an alternate domain for both distributions, and configure Amazon Route 53 with a failover routing policy between them\" here also looks strange to me,\nit sounds like to use failover routing for two CloudFront distributions, but indeed it doesn't follow AWS best practices, you can use one CloudFront distributions with one origin group containing two origins, see https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html","comment_id":"920118","poster":"Jesuisleon"},{"content":"AWS never mentioned DynamoDB here so should be D\nhttps://aws.amazon.com/what-is/structured-data/#:~:text=Zipped%20files-,How%20can%20AWS%20help%20with%20structured%20data%3F,-You%20can%20set","upvote_count":"1","comment_id":"788596","timestamp":"1674727380.0","poster":"davidy2020"},{"upvote_count":"1","timestamp":"1673470740.0","poster":"syaldram","comment_id":"772891","content":"Selected Answer: C\nStructured data = DynamoDB!"},{"content":"Selected Answer: C\nThe answer is C\n\nA(wrong):Single Point of Failure, can't support fault tolerant across multiple regions.\n\nB(wrong):S3 CRR is not fast enough. AWS Docs - \"Most objects replicate within 15 minutes, but sometimes replication can take a couple hours or more\". By comparison, DynamoDB Global tables has sync latency of less than a sec - \"In a global table, a newly written item is usually propagated to all replica tables within a second.\".\n\nD(wrong):Unlike DynamoDB, Aurora Global database has only one master(only one writable node) in the case of multiple region deployment.","upvote_count":"2","timestamp":"1664951760.0","poster":"tomosabc1","comment_id":"686651"},{"upvote_count":"1","comment_id":"671166","poster":"linuxmaster007","content":"Answer is C ( as per tutorial dojo)","timestamp":"1663374480.0"},{"upvote_count":"1","content":"dynamo db can store both Structured and Semi Structured data. So C is correct.","comment_id":"653122","timestamp":"1661714640.0","poster":"ASC1"},{"poster":"CloudHandsOn","content":"D. - 'STRUCTURED' data. No other option is fully structured. I believe because of this, its the only viable option.","upvote_count":"2","timestamp":"1658605860.0","comment_id":"635729"},{"content":"Selected Answer: D\nThe Q states \"structured data\". So neither NoSQL nor S3 (file storage) fits the bill.\nThe only answer which complies to this situation is D. Multi-answer is not a problem for a web app.","timestamp":"1652708580.0","comment_id":"602609","poster":"bobsmith2000","upvote_count":"1"},{"content":"C: this is because The service must be able to react quickly to brief but large surges in system demand. Across many AWS Regions, the service must be fault resilient\nD can not scale fast \nA is not fault resilient across regions","comment_id":"601564","poster":"user0001","timestamp":"1652531820.0","upvote_count":"1"},{"timestamp":"1644758040.0","upvote_count":"1","content":"Answer is C ; The soloution should be able to quickly scale Fargate and fault resilient then both region should be active . Dynamodb glbal table and Route53 latency based rcords with health check","comment_id":"546459","poster":"RVivek"},{"upvote_count":"1","content":"C. Store the data in an Amazon DynamoDB global table in two Regions using on-demand capacity mode. In both Regions, run the web service as Amazon ECS Fargate tasks in an Auto Scaling ECS service behind an Application Load Balancer (ALB). In Amazon Route 53, configure an alias record in the companyג€™s domain and a Route 53 latency-based routing policy with health checks to distribute traffic between the two ALBs.","timestamp":"1639021920.0","comment_id":"497306","poster":"cldy"},{"content":"Answer is C.\nsee Ebi explanation, I am good to with it.\nD is not the right answer, although Aurora is better choice for structured data, but Aurora Global database supports one master only, so other regions do not support","poster":"AzureDP900","timestamp":"1638403740.0","upvote_count":"1","comment_id":"492025"},{"content":"I think it shoud be D.\nThe question says: \"Across many AWS Regions, the service must be fault resilient\". There is nothing about that in both regions database should be writable.\nhttps://aws.amazon.com/rds/aurora/faqs/ :\nAmazon Aurora Global Database is a feature that allows a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads in each region with typical latency of less than a second, and provides disaster recovery from region-wide outages. In the unlikely event of a regional degradation or outage, a secondary region can be promoted to full read/write capabilities in less than 1 minute.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html\nAurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload.","timestamp":"1637609580.0","comment_id":"484508","poster":"nerembo","upvote_count":"1"},{"timestamp":"1636284240.0","comment_id":"462797","upvote_count":"2","content":"This was clearly C in the past due to the lack of multi-region support in DocumentDB. But in 2021, AWS implemented support for DocumentDB global clusters to support automatic replication across up to 5 regions. So now A and C are both valid candidates.\n\nI would still lean towards C, even so, because one DocumentDB region must be primary, and the failover process to a secondary region is not seamless by any means. You have to stop application writes in the primary (failed) region, and then promote the secondary region to its own standalone master. Then you have to repoint your app to the secondary region. Not ideal.\n\nhttps://aws.amazon.com/documentdb/global-clusters/\n\nhttps://aws.amazon.com/blogs/database/introducing-amazon-documentdb-with-mongodb-compatibility-global-clusters/","poster":"kirrim"},{"poster":"andylogan","timestamp":"1636127940.0","content":"It's C for Dynamo","comment_id":"443430","upvote_count":"1"},{"upvote_count":"2","comment_id":"435960","timestamp":"1635764280.0","poster":"blackgamer","content":"C for me"},{"poster":"tgv","upvote_count":"3","comment_id":"434885","content":"CCC\n---\nA: it doesn't cover the fault-tolerant across multiple regions requirement\nB: CloudFront is a global service\nD: Creating multivalue answer alias records is not supported.","timestamp":"1635535740.0"},{"poster":"TomPaschenda","comment_id":"421118","content":"C for me:\nA - out because DocumentDB has no cross-region failover\nB - out because S3 would not support writes in both regions (replication only goes one way)\nC - only possible solution\nD - out because EC2 ASG is not great for \"short but significant spikes\". Also \"download web service code in user data\" - why? And as pointed out, read replica would require promotion for failover","timestamp":"1635379980.0","upvote_count":"1"},{"poster":"WhyIronMan","timestamp":"1635361560.0","upvote_count":"2","content":"I'll go with D","comment_id":"413346"},{"comment_id":"396406","upvote_count":"2","timestamp":"1635283980.0","poster":"DashL","content":"B\nA uses DocumentDB, but no failover option is provided for DocumentDB. So, A is ruled out.\nB uses S3. S3 can be used to store structured data. In this option, ROute 53 is configures with failover policy. This is the only valid option.\nC states that \"configure an alias record in the company's domain\". An alias record is a Route 53 specific entry - so it is not possible to create an alias record in company's domain.\nD states that \"In Amazon Route 53, configure an alias record for the company's domain\" - so, D is also ruled out."},{"poster":"hess","upvote_count":"2","comment_id":"389732","timestamp":"1635205080.0","content":"C. according to tutorials dojo"},{"poster":"zolthar_z","upvote_count":"2","timestamp":"1635204660.0","comment_id":"376668","content":"The answer is C, DynamoDB can handle structured data, is a no relational DB (maybe that is why is confusing), the spikes will be handle faster with fargate than EC2."},{"content":"D - Structural data + spike","timestamp":"1635129600.0","comment_id":"374846","upvote_count":"1","poster":"ss160700"},{"content":"it's C","upvote_count":"4","timestamp":"1634981880.0","poster":"Waiweng","comment_id":"356384"},{"content":"it's C","poster":"Waiweng","upvote_count":"2","comment_id":"356379","timestamp":"1634619600.0"},{"poster":"Amitv2706","content":"Answer is B.\n\nStructured Data - S3 \n\nSpikes in System Load - API Gtw with Lambda\nFault Tolerant Across Multiple Regions - Route 53 with a failover routing policy (Most important point when we compare to other options)","timestamp":"1634596680.0","comment_id":"343898","upvote_count":"1"},{"content":"Aurora appears better for structured data but it's not *fault tolerant* for Route 53 without health check, so D is not correct. The answer is C.","upvote_count":"1","poster":"Kelvin","comment_id":"339379","timestamp":"1634594220.0"},{"content":"The key Is term Refactor here, means converting data from structured SQL to No SQL Database.\nC is the correct answer as we use dynomoDB.","comment_id":"329399","poster":"AJBA","upvote_count":"1","timestamp":"1634438880.0"},{"comment_id":"328692","timestamp":"1634406300.0","content":"\"refactoring\"\n\"structured data\"\nhttps://docs.aws.amazon.com/whitepapers/latest/big-data-analytics-options/amazon-dynamodb.html\nA","poster":"hungran91","upvote_count":"3"},{"timestamp":"1633803420.0","content":"C is the answer as it addresses all the concerns of the question.\nDynamoDB stores structured data in tables, indexed by primary key, and allows low-latency read and write access to items ranging from 1 byte up to 400 KB. This storage alternative meets the latency and throughput requirements of highly demanding applications by providing single-digit millisecond latency and predictable performance with seamless throughput and storage scalability.\nhttps://docs.aws.amazon.com/whitepapers/latest/big-data-analytics-options/amazon-dynamodb.html","poster":"LB","comments":[{"upvote_count":"1","timestamp":"1634363460.0","content":"I will Go with C\nA+B: DocumentDB and S3 is not for structured data\nD: a multi-value is server-side routing( compared with simple routing for client-side routing) and Server choose any heathy record. Instead it should use latency-based route with health check. Or latency with failover routing policy","poster":"jerrySyd","comment_id":"300623"}],"upvote_count":"4","comment_id":"292729"},{"timestamp":"1633776480.0","comment_id":"283582","upvote_count":"2","poster":"Trap_D0_r","content":"D\nWeb service not webpage, B requires a lot of assumptions about the type of service and architecture. D will meet the question requirements and is compatible with almost any web service you could image."},{"comment_id":"277449","timestamp":"1633757040.0","poster":"rcher","upvote_count":"2","content":"DocumentDB - MongoDB , another kind of NoSql\nDynamoDB - NoSql\nS3 - Object storage\n\n:)"},{"timestamp":"1633007280.0","upvote_count":"2","comment_id":"258235","content":"it's said is structure data so no way DynamoDB fits here so it's D","poster":"kopper2019"},{"upvote_count":"1","comment_id":"255889","timestamp":"1632821760.0","poster":"petebear55","content":"kj07 1 week, 2 days ago\nAnswer: B ... This is wrong mate its D ... S3 buckets are only suitable in this scenario for reading structured data if the solution required Athena. And think of the study of \"Fluid dynamics\" when considering new Cloud technologies. B does not flow and is cumbersome and two structured like an old COBOL program. The solution is not dynamic enough for cloud computing. also the main clue is in the question \"Structured data\" when AWS mentions this it means MYSQL rather NoSQL databases .. so look for RDS etc as opposed to Dynamo DB"},{"content":"Answer is D. Structured data hence Aurora.","poster":"jaho","upvote_count":"2","comment_id":"255768","timestamp":"1632741000.0"},{"comments":[{"timestamp":"1633616520.0","comment_id":"267410","content":"Yes, this is correct. Think about it. You can store structured and unstructured data on S3, it doesn't say you need to have database.","upvote_count":"1","poster":"Justu"}],"upvote_count":"2","timestamp":"1632736020.0","comment_id":"249135","content":"Answer: B\nA&C not suitable because you can't use DynamoDB and DocumentDB for structured data.\nD is not correct because it will not scale fast enough and also you can't use multi value routing for load balancer\nhttps://aws.amazon.com/premiumsupport/knowledge-center/multivalue-versus-simple-policies/\nB meets all the requirements","poster":"kj07"},{"poster":"T14102020","comment_id":"240967","upvote_count":"2","timestamp":"1632646920.0","content":"I go with D"},{"comment_id":"232661","upvote_count":"3","timestamp":"1632618540.0","content":"I'll go with D","poster":"jackdryan"},{"upvote_count":"2","timestamp":"1632314460.0","content":"D, Amazon Aurora Global Database is structure data designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions,\nAWS documentDB, S3 and dynamodb are not structured DB.","poster":"beso","comment_id":"220363"},{"comments":[{"poster":"Kelvin1477","timestamp":"1632553320.0","upvote_count":"4","content":"Agree, D also saying using multivalue routing whereby it is fronted by ALB, multivalue route is only for AA records","comment_id":"225382"},{"timestamp":"1632623760.0","comment_id":"238544","content":"Agree with you both, even though Aurora seems the perfect choice, you cannot put a load balancer dns name in a multi-value DSN record","upvote_count":"1","poster":"PAUGURU"},{"content":"A bucket in each region is not a robust enough solution ... \"think fluid dynamics\" something more fluid is needed .... D","poster":"petebear55","timestamp":"1632767940.0","upvote_count":"1","comment_id":"255886"},{"comments":[{"timestamp":"1633907280.0","upvote_count":"2","poster":"ele","comment_id":"295197","content":"Agree on C. Regarding storing structured data in Dynamodb have a look here https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-relational-modeling.html"}],"content":"Agree the part that D can't be correct.\nAccording to \nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-multivalue.html\nCreating multivalue answer alias records is not supported.\n\nI think the answer is C.\nThough Dynamodb is NoSQL DB, I think is still can store structured data.","comment_id":"270209","timestamp":"1633731360.0","upvote_count":"5","poster":"01037"}],"timestamp":"1632299940.0","upvote_count":"3","comment_id":"218899","content":"D cannot be the answer bcz question ask for solution to sudden spike, \"Configure the instances to download the web service code in the user data\" will not scale well for sudden spike traffic. Lambda is meant for this kind of traffic as it can scale fast for upto 10000RPS. S3 can be used for both structured and unstructured data. Answer is B","poster":"cpd"},{"timestamp":"1632182700.0","content":"Structured data should be stored in RDB i.e. Aurora. D seems to be the right choice.","upvote_count":"4","poster":"liono","comment_id":"213315"},{"timestamp":"1632182520.0","upvote_count":"8","poster":"asldavid","content":"Answer is D.","comment_id":"212833"}],"choices":{"C":"Store the data in an Amazon DynamoDB global table in two Regions using on-demand capacity mode. In both Regions, run the web service as Amazon ECS Fargate tasks in an Auto Scaling ECS service behind an Application Load Balancer (ALB). In Amazon Route 53, configure an alias record in the company's domain and a Route 53 latency-based routing policy with health checks to distribute traffic between the two ALBs.","B":"Store the data in replicated Amazon S3 buckets in two Regions. Create an Amazon CloudFront distribution in each Region, with custom origins built on Amazon API Gateway and AWS Lambda launched in each Region. Assign the company's domain as an alternate domain for both distributions, and configure Amazon Route 53 with a failover routing policy between them.","A":"Store the data in Amazon DocumentDB. Create a single global Amazon CloudFront distribution with a custom origin built on edge-optimized Amazon API Gateway and AWS Lambda. Assign the company's domain as an alternate domain for the distribution, and configure Amazon Route 53 with an alias to the CloudFront distribution.","D":"Store the data in Amazon Aurora global databases. Add Auto Scaling replicas to both Regions. Run the web service on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer in each Region. Configure the instances to download the web service code in the user data. In Amazon Route 53, configure an alias record for the company's domain and a multi-value routing policy"},"answer":"C","unix_timestamp":1604503620,"exam_id":32,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/36059-exam-aws-certified-solutions-architect-professional-topic-1/","question_id":630}],"exam":{"id":32,"isImplemented":true,"isBeta":false,"provider":"Amazon","name":"AWS Certified Solutions Architect - Professional","lastUpdated":"11 Apr 2025","numberOfQuestions":1019,"isMCOnly":false},"currentPage":126},"__N_SSP":true}