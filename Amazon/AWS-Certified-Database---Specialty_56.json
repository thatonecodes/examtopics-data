{"pageProps":{"questions":[{"id":"TfaXxenkN1WJ5eRjfqfY","isMC":true,"exam_id":22,"url":"https://www.examtopics.com/discussions/amazon/view/127230-exam-aws-certified-database-specialty-topic-1-question-347/","answer_images":[],"answer_ET":"B","answer_description":"","timestamp":"2023-11-26 08:16:00","unix_timestamp":1700982960,"discussion":[{"timestamp":"1712816580.0","upvote_count":"1","comment_id":"1193485","content":"Selected Answer: B\nRunning an explain plan to analyze the queries on the tables. It helps to understand how the queries are being executed and where the performance bottlenecks might be.\n Considering recommendations from Amazon Redshift Advisor is a smart move. Redshift Advisor analyzes all the clusters in your account to generate tailored recommendations on how to optimize performance and decrease operating costs.\n Identifying the columns that are used in filter and group by conditions and converting them into sort keys can significantly improve query performance. Sort keys determine the order in which the data is physically stored in a table and can greatly reduce the amount of data that needs to be read from disk during query execution.\n Setting the compression encoding for the recommended columns to RAW and the rest of the column compression encoding to AZ64 is a good strategy. RAW encoding disables compression and can be useful for frequently accessed columns, while AZ64 is a high-performance compression encoding optimized for analytic queries and is suitable for the rest of the columns.","poster":"tsangckl"},{"timestamp":"1712781960.0","comment_id":"1193297","poster":"Doox","comments":[{"poster":"Doox","timestamp":"1712782380.0","upvote_count":"1","comment_id":"1193303","content":"You can compress the distribution key, <b>but you must avoid compressing the sort key column (especially the first column of the sort key). </b>"}],"content":"B\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/query-best-practices-redshift/best-practices-tables.html\n\nAvoid compressing the sort key column.","upvote_count":"1"},{"content":"D\nYou cannot change the compression of a column, without recreating the table","upvote_count":"1","timestamp":"1705156800.0","comments":[{"upvote_count":"1","timestamp":"1706980980.0","content":"wrong https://aws.amazon.com/about-aws/whats-new/2020/10/amazon-redshift-supports-modifying-column-comprression-encodings-to-optimize-storage-utilization-query-performance/?nc1=h_ls","comment_id":"1139490","poster":"Skarlex77"}],"poster":"MultiAZ","comment_id":"1121749"},{"content":"Selected Answer: B\nhttps://aws.amazon.com/about-aws/whats-new/2020/10/amazon-redshift-supports-modifying-column-comprression-encodings-to-optimize-storage-utilization-query-performance/\n\nAZ64, a new compression encoding that consumes 5-10% less storage than ZSTD and enables queries to run 70% faster. Previously, customers who wanted to take advantage of new encoding algorithms such as AZ64 needed to recreate the entire table. Since Redshift recommends that columns defined as SORT keys should not be compressed, previously customers who apply sort keys to existing tables needed to recreate the entire table.","timestamp":"1703435400.0","poster":"rrshah83","upvote_count":"3","comment_id":"1104726"},{"timestamp":"1701859260.0","poster":"calduck","comment_id":"1089201","content":"B. Redshift now supports modifying column compression encodings to optimize storage utilization and query performance. https://aws.amazon.com/about-aws/whats-new/2020/10/amazon-redshift-supports-modifying-column-comprression-encodings-to-optimize-storage-utilization-query-performance/","upvote_count":"2"},{"upvote_count":"1","poster":"marll88","timestamp":"1700982960.0","comment_id":"1080523","content":"I think D.\n\nNot B and C\nRedShift does not allow ALTER TABLE to change column compression type"}],"question_text":"A retail company uses Amazon Redshift for its 1 PB data warehouse. Several analytical workloads run on a Redshift cluster. The tables within the cluster have grown rapidly. End users are reporting poor performance of daily reports that run on the transaction fact tables.\n\nA database specialist must change the design of the tables to improve the reporting performance. All the changes must be applied dynamically. The changes must have the least possible impact on users and must optimize the overall table size.\n\nWhich solution will meet these requirements?","topic":"1","choices":{"A":"Use the STL_SCAN view to understand how the tables are getting scanned. Identify the columns that are used in filter and group by conditions. Create a temporary table with the identified columns as sort keys and compression as Zstandard (ZSTD) by copying the data from the original table. Drop the original table. Give the temporary table the same name that the original table had.","C":"Run an explain plan to analyze the queries on the tables. Consider recommendations from Amazon Redshift Advisor. Identify the columns that are used in filter and group by conditions. Convert the recommended columns from Redshift Advisor into sort keys with compression encoding set to LZO. Set the rest of the column compression encoding to Zstandard (ZSTD).","D":"Run an explain plan to analyze the queries on the tables. Consider recommendations from Amazon Redshift Advisor. Identify the columns that are used in filter and group by conditions. Create a deep copy of the table with the identified columns as sort keys and compression for all columns as Zstandard (ZSTD) by using a bulk insert. Drop the original table. Give the copy table the same name that the original table had.","B":"Run an explain plan to analyze the queries on the tables. Consider recommendations from Amazon Redshift Advisor. Identify the columns that are used in filter and group by conditions. Convert the recommended columns from Redshift Advisor into sort keys with compression encoding set to RAW. Set the rest of the column compression encoding to AZ64."},"question_images":[],"answers_community":["B (100%)"],"answer":"B","question_id":276},{"id":"7I0lwxBLFAQ0rEAFcgiE","answers_community":["A (100%)"],"question_id":277,"url":"https://www.examtopics.com/discussions/amazon/view/138107-exam-aws-certified-database-specialty-topic-1-question-348/","unix_timestamp":1712531160,"exam_id":22,"discussion":[{"upvote_count":"1","comment_id":"1193488","timestamp":"1712817060.0","content":"Selected Answer: A\nThe database administrator should add an inbound security group rule to the database security group that allows access from the developer account VPC CIDR on port 5432. This port is the default port for PostgreSQL databases.\n\nAdditionally, the administrator should add an outbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on port 5432. This will allow the reporting application running on the EC2 instance to reach the database in the production account.","poster":"tsangckl"},{"poster":"Doox","upvote_count":"1","content":"A.\n\nAs long as the DB permits traffic in, return traffic is permitted. Security Groups are stateful.\n\nSecurity groups are stateful. For example, if you send a request from an instance, the response traffic for that request is allowed to reach the instance regardless of the inbound security group rules. Responses to allowed inbound traffic are allowed to leave the instance, regardless of the outbound rules.\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html","comment_id":"1191242","timestamp":"1712531160.0"}],"timestamp":"2024-04-08 01:06:00","question_text":"A company has a reporting application that runs on an Amazon EC2 instance in an isolated developer account on AWS. The application needs to retrieve data during non-peak company hours from an Amazon Aurora PostgreSQL database that runs in the companyâ€™s production account. The company's security team requires that access to production resources complies with AWS best security practices.\n\nA database administrator needs to provide the reporting application with access to the production database. The company has already configured VPC peering between the production account and developer account. The company has also updated the route tables in both accounts with the necessary entries to correctly set up VPC peering.\n\nWhat must the database administrator do to finish providing connectivity to the reporting application?","answer_images":[],"topic":"1","question_images":[],"isMC":true,"answer_description":"","answer":"A","choices":{"C":"Add an inbound security group rule to the database security group that allows access from the developer account VPC CIDR on all TCP ports. Add an inbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on port 5432.","D":"Add an inbound security group rule to the database security group that allows access from the developer account VPC CIDR on port 5432. Add an outbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on all TCP ports.","A":"Add an inbound security group rule to the database security group that allows access from the developer account VPC CIDR on port 5432. Add an outbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on port 5432.","B":"Add an outbound security group rule to the database security group that allows access from the developer account VPC CIDR on port 5432. Add an outbound security group rule to the EC2 security group that allows access to the production account VPC CIDR on port 5432."},"answer_ET":"A"},{"id":"aISIrnoVNyxWNwX5JxVu","timestamp":"2024-02-18 11:15:00","choices":{"D":"Amazon DynamoDB with a global secondary index (GSI)","C":"Amazon DynamoDB global tables","A":"Amazon RDS for MariaDB with cross-Region read replicas","B":"Amazon RDS with a Multi-AZ deployment"},"answers_community":["C (100%)"],"url":"https://www.examtopics.com/discussions/amazon/view/134129-exam-aws-certified-database-specialty-topic-1-question-349/","unix_timestamp":1708251300,"discussion":[{"comment_id":"1193489","timestamp":"1712817300.0","upvote_count":"1","poster":"tsangckl","content":"Selected Answer: C\nAmazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution. When you create a global table, you specify the AWS regions where you want the table to be available. DynamoDB performs all of the necessary tasks to create identical tables in these regions, and propagate ongoing data changes to all of them.\n\nThe other options (A, B, and D) do not meet all the requirements:\n\n A. Amazon RDS for MariaDB with cross-Region read replicas: This option does not support active-active configuration. The read replicas are read-only.\n B. Amazon RDS with a Multi-AZ deployment: This option is for high availability and failover support for DB instances within a single region. It does not support multi-region, active-active configuration.\n D. Amazon DynamoDB with a global secondary index (GSI): GSIs are used to speed up access to data in DynamoDB, but they do not provide multi-region, active-active configuration."},{"poster":"PGGuy","upvote_count":"1","content":"Selected Answer: C\nsimilar to #249. So, I pick C","comment_id":"1153185","timestamp":"1708251300.0"}],"answer_description":"","answer_ET":"C","question_text":"A global company is creating an application. The application must be highly available. The company requires an RTO and an RPO of less than 5 minutes. The company needs a database that will provide the ability to set up an active-active configuration and near real-time synchronization of data across tables in multiple AWS Regions.\n\nWhich solution will meet these requirements?","topic":"1","isMC":true,"question_id":278,"question_images":[],"answer_images":[],"exam_id":22,"answer":"C"},{"id":"YJ9vp6JXWeatvA41qLOS","url":"https://www.examtopics.com/discussions/amazon/view/25721-exam-aws-certified-database-specialty-topic-1-question-35/","topic":"1","question_images":[],"exam_id":22,"question_id":279,"answer":"BE","unix_timestamp":1594733340,"answer_ET":"BE","answer_description":"","choices":{"A":"Enable Amazon CloudWatch Events and view the incoming T-SQL statements causing the CPU to spike.","B":"Enable Enhanced Monitoring metrics to view CPU utilization at the RDS SQL Server DB instance level.","C":"Implement a caching layer to help with repeated queries on the RDS SQL Server DB instance.","E":"Enable Amazon RDS Performance Insights to view the database load and filter the load by waits, SQL statements, hosts, or users.","D":"Use Amazon QuickSight to view the SQL statement being run."},"answers_community":["BE (100%)"],"answer_images":[],"question_text":"A company has a database monitoring solution that uses Amazon CloudWatch for its Amazon RDS for SQL Server environment. The cause of a recent spike in\nCPU utilization was not determined using the standard metrics that were collected. The CPU spike caused the application to perform poorly, impacting users. A\nDatabase Specialist needs to determine what caused the CPU spike.\nWhich combination of steps should be taken to provide more visibility into the processes and queries running during an increase in CPU load? (Choose two.)","timestamp":"2020-07-14 15:29:00","discussion":[{"content":"B&E as well\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-instance-high-cpu/\n\"Several factors can cause an increase in CPU utilization. For example, user-initiated heavy workloads, analytic queries, prolonged deadlocks and lock waits, multiple concurrent transactions, long-running transactions, or other processes that utilize CPU resources.\nFirst, you can identify the source of the CPU usage by:\nUsing Enhanced Monitoring\nUsing Performance Insights\"","upvote_count":"11","comment_id":"140250","timestamp":"1633382220.0","poster":"BillyMadison"},{"content":"B & E for me too","comment_id":"314841","timestamp":"1634437680.0","poster":"LMax","upvote_count":"7"},{"timestamp":"1651254840.0","upvote_count":"3","comment_id":"594629","poster":"novice_expert","content":"Selected Answer: BE\nyou can identify the source of the CPU usage by:\nUsing Enhanced Monitoring\nUsing Performance Insights\""},{"content":"Selected Answer: BE\nGets good visibility","upvote_count":"3","comment_id":"555555","timestamp":"1645736580.0","poster":"tugboat"},{"upvote_count":"1","timestamp":"1634234880.0","content":"Ans: BE","comment_id":"297986","poster":"myutran"},{"timestamp":"1633497840.0","content":"Answer is B,E","comment_id":"154322","poster":"Ebi","upvote_count":"2"},{"comment_id":"135664","content":"Yes B and E here","upvote_count":"2","timestamp":"1633320840.0","poster":"BillyC"},{"content":"B and E","upvote_count":"4","poster":"Mickysingh","comment_id":"134874","timestamp":"1632908040.0"}],"isMC":true},{"id":"bzurXMtn8cX2JVYmdlho","question_text":"A company has a hybrid environment in which a VPC connects to an on-premises network through an AWS Site-to-Site VPN connection. The VPC contains an application that is hosted on Amazon EC2 instances. The EC2 instances run in private subnets behind an Application Load Balancer (ALB) that is associated with multiple public subnets. The EC2 instances need to securely access an Amazon DynamoDB table.\n\nWhich solution will meet these requirements?","unix_timestamp":1709457360,"url":"https://www.examtopics.com/discussions/amazon/view/135099-exam-aws-certified-database-specialty-topic-1-question-350/","question_images":[],"answer_ET":"C","discussion":[{"upvote_count":"1","content":"Selected Answer: D\nA VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet. The EC2 instances do not require public IP addresses, and internet gateway, NAT device, VPN connection, or AWS Direct Connect connection is not required to communicate with DynamoDB.","comment_id":"1193492","timestamp":"1712817480.0","poster":"tsangckl"},{"timestamp":"1709457360.0","upvote_count":"1","comment_id":"1164633","poster":"fceb2c1","content":"Selected Answer: D\nD using VPC endpoint"}],"exam_id":22,"answer_images":[],"answer":"D","answers_community":["D (100%)"],"question_id":280,"isMC":true,"topic":"1","answer_description":"","choices":{"D":"Create a VPC endpoint for DynamoDB. Assign the endpoint to the route table of the private subnets that contain the EC2 instances.","A":"Use the internet gateway of the VPC to access the DynamoDB table. Use the ALB to route the traffic to the EC2 instances.","C":"Use the Site-to-Site VPN connection to route all DynamoDB network traffic through the on-premises network infrastructure to access the EC2 instances.","B":"Add a NAT gateway in one of the public subnets of the VPC. Configure the security groups of the EC2 instances to access the DynamoDB table through the NAT gateway."},"timestamp":"2024-03-03 10:16:00"}],"exam":{"name":"AWS Certified Database - Specialty","isBeta":false,"id":22,"lastUpdated":"11 Apr 2025","numberOfQuestions":359,"isMCOnly":false,"isImplemented":true,"provider":"Amazon"},"currentPage":56},"__N_SSP":true}