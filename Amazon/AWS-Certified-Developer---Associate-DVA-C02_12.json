{"pageProps":{"questions":[{"id":"8iJZu9S4GIV2ef2GeMQN","unix_timestamp":1696549320,"timestamp":"2023-10-06 01:42:00","question_text":"An organization is using Amazon CloudFront to ensure that its users experience low-latency access to its web application. The organization has identified a need to encrypt all traffic between users and CloudFront, and all traffic between CloudFront and the web application.\n\nHow can these requirements be met? (Choose two.)","isMC":true,"answers_community":["BD (100%)"],"exam_id":24,"choices":{"A":"Use AWS KMS to encrypt traffic between CloudFront and the web application.","C":"Set the Origin’s HTTP Port to 443.","B":"Set the Origin Protocol Policy to “HTTPS Only”.","D":"Set the Viewer Protocol Policy to “HTTPS Only” or “Redirect HTTP to HTTPS”.","E":"Enable the CloudFront option Restrict Viewer Access."},"answer":"BD","url":"https://www.examtopics.com/discussions/amazon/view/122567-exam-aws-certified-developer-associate-dva-c02-topic-1/","answer_ET":"BD","topic":"1","answer_images":[],"question_id":56,"discussion":[{"comment_id":"1125831","upvote_count":"5","timestamp":"1721298960.0","content":"Selected Answer: BD\nB. Set the Origin Protocol Policy to “HTTPS Only”: This setting ensures that all traffic between CloudFront and the web application (origin) is encrypted. By setting the Origin Protocol Policy to \"HTTPS Only,\" CloudFront will only connect to the origin over HTTPS, ensuring encryption of data in transit.\nD. Set the Viewer Protocol Policy to “HTTPS Only” or “Redirect HTTP to HTTPS”: This setting is crucial for ensuring that all traffic between the users (viewers) and CloudFront is encrypted. By setting the Viewer Protocol Policy to \"HTTPS Only\" or \"Redirect HTTP to HTTPS,\" CloudFront ensures that user requests are either only served over HTTPS or automatically redirected from HTTP to HTTPS.","poster":"SerialiDr"},{"content":"Selected Answer: BD\nBD is the correct answer.","timestamp":"1732333500.0","upvote_count":"1","comment_id":"1216154","poster":"65703c1"},{"timestamp":"1717202340.0","comment_id":"1084882","upvote_count":"2","poster":"Jeff1719","content":"Selected Answer: BD\nBD: Protocol and Viewer protocol policy, see\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html"},{"poster":"dilleman","upvote_count":"4","comment_id":"1040253","content":"Selected Answer: BD\nB and D are the correct ones.\nB: Setting the Origin Protocol Policy to “HTTPS Only” ensures that CloudFront always uses HTTPS to connect to the origin, which is the web application in this scenario.\nD: Setting the Viewer Protocol Policy to “HTTPS Only” ensures that CloudFront will only serve requests over HTTPS. Setting it to “Redirect HTTP to HTTPS” ensures that any HTTP request from viewers is redirected to HTTPS.","timestamp":"1712817660.0"},{"poster":"Digo30sp","content":"Selected Answer: BD\nThe correct answers are (B) and (D).\n\nTo meet the requirement to encrypt all traffic between users and CloudFront, your organization must set the Viewer Protocol Policy to “HTTPS Only” or “Redirect HTTP to HTTPS”. This will force users to use HTTPS to connect to CloudFront.\n\nTo meet the requirement to encrypt all traffic between CloudFront and the web application, your organization must set the Origin Protocol Policy to “HTTPS Only”. This will force CloudFront to use HTTPS to connect to the web application.","upvote_count":"3","comment_id":"1026432","timestamp":"1712399460.0"}],"answer_description":"","question_images":[]},{"id":"4OSVz3BHuPLYQpH8PQON","answer":"B","url":"https://www.examtopics.com/discussions/amazon/view/102742-exam-aws-certified-developer-associate-dva-c02-topic-1/","unix_timestamp":1678919700,"discussion":[{"comment_id":"845568","upvote_count":"7","content":"Selected Answer: B\nB\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html","timestamp":"1679398980.0","poster":"Untamables"},{"upvote_count":"2","comment_id":"1329814","content":"Selected Answer: B\nA) Eliminated - OpsWorks is unnecessary for Lambda function deployments.Blue/green deployments are more suited for EC2 or ECS applications, not for a simple Lambda rollback.\nB) Correct - Quickly switch the alias to an older version\nC) Eliminated - Requires manual work to download and redeploy older versions.\nD) Eliminated - CodePipeline introduces unnecessary complexity","timestamp":"1734755580.0","poster":"sumanshu"},{"content":"Selected Answer: B\nkeyword: LEAST operational overhead\n\n=> discard D: it does more work, so cost for many works also increase\n=> discard C: it makes effort for manual work, adding operational overhead and delays.\n=> Discard A: Designed for managing server configurations, not for managing lambda function\n\nB: most seamless, only change version by pointing an alias (eg: production) to specific Lambda Version. On the other hand, it's free","poster":"trieudo","comment_id":"1325923","timestamp":"1734049560.0","upvote_count":"1"},{"timestamp":"1733455860.0","upvote_count":"1","poster":"f271c23","comment_id":"1322608","content":"Selected Answer: A\nI am not sure if the question is about which deployment strategy to choose from if we want to roll back seamlessly once deployed. Or is the question about how to manage the deployment versions to be able to roll back. does anyone has similar doubts ? please help , thanks"},{"content":"Selected Answer: B\nB is the correct answer.","poster":"65703c1","comment_id":"1214993","upvote_count":"2","timestamp":"1716299520.0"},{"timestamp":"1711535820.0","comment_id":"1183999","upvote_count":"1","content":"c ra unga amma","poster":"mghectorenjoyer69"},{"comment_id":"897425","content":"B is the least overhead solution","upvote_count":"3","poster":"ubiqinon","timestamp":"1684055280.0"},{"timestamp":"1681220760.0","upvote_count":"2","poster":"zk1200","comment_id":"867361","content":"Selected Answer: B\nI considered D as well which refers to using CodeDeploy. however using codedeploy adds more work. So alias makes more sense."},{"content":"Selected Answer: B\nlambda function version => alias","timestamp":"1680359160.0","poster":"ihta_2031","upvote_count":"4","comment_id":"858008"},{"upvote_count":"3","timestamp":"1678919700.0","comment_id":"840375","content":"B\nhttps://www.examtopics.com/discussions/amazon/view/96149-exam-aws-certified-developer-associate-topic-1-question-441/","poster":"aragon_saa"}],"answer_ET":"B","choices":{"A":"Use AWS OpsWorks to perform blue/green deployments.","D":"Use AWS CodePipeline for deployments and rollbacks.","C":"Maintain deployment packages for older versions in Amazon S3.","B":"Use a function alias with different versions."},"answer_description":"","question_text":"A developer is deploying an AWS Lambda function The developer wants the ability to return to older versions of the function quickly and seamlessly.\nHow can the developer achieve this goal with the LEAST operational overhead?","answer_images":[],"exam_id":24,"timestamp":"2023-03-15 23:35:00","question_id":57,"answers_community":["B (95%)","5%"],"question_images":[],"isMC":true,"topic":"1"},{"id":"E9U6DgdiAKStBTPDYVul","isMC":true,"question_id":58,"exam_id":24,"choices":{"B":"Symmetric customer managed keys with key material that is generated by AWS","D":"Symmetric customer managed keys with imported key material","A":"Amazon S3 managed keys","C":"Asymmetric customer managed keys with key material that is generated by AWS"},"answer_ET":"B","answers_community":["B (72%)","A (28%)"],"timestamp":"2023-10-06 01:54:00","unix_timestamp":1696550040,"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/122571-exam-aws-certified-developer-associate-dva-c02-topic-1/","question_images":[],"question_text":"A developer is planning to migrate on-premises company data to Amazon S3. The data must be encrypted, and the encryption keys must support automatic annual rotation. The company must use AWS Key Management Service (AWS KMS) to encrypt the data.\n\nWhich type of keys should the developer use to meet these requirements?","answer_images":[],"answer":"B","discussion":[{"timestamp":"1713000840.0","upvote_count":"13","content":"Selected Answer: B\nAsymmetric keys (option C) are typically used for different use cases, such as digital signatures and key pairs, and may not be as suitable for automatic rotation in the described scenario.\n\nImported key material (option D) means that you bring your own key material, and AWS KMS doesn't support automatic rotation for such keys.\n\nAmazon S3 managed keys (option A) are used specifically for Amazon S3 and don't support automatic rotation.\n\nso, option B is correct","comment_id":"1042519","poster":"PrakashM14"},{"comment_id":"1216156","timestamp":"1732333680.0","upvote_count":"1","poster":"65703c1","content":"Selected Answer: B\nB is the correct answer."},{"comment_id":"1163612","timestamp":"1725198840.0","poster":"SerialiDr","upvote_count":"2","content":"Selected Answer: B\nThis option allows for automatic rotation of the keys, aligning with AWS best practices for key management and security. AWS KMS supports key rotation, which can be configured to occur automatically on an annual basis for customer managed keys. This ensures that data remains encrypted with a key that is periodically rotated, enhancing the security posture of the data stored in Amazon S3."},{"upvote_count":"1","timestamp":"1724583420.0","poster":"KarBiswa","comment_id":"1158715","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html Its a symmetric key rotation"},{"upvote_count":"3","poster":"konieczny69","content":"Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html\n\nServer-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. Amazon S3 server-side encryption uses 256-bit Advanced Encryption Standard Galois/Counter Mode (AES-GCM) to encrypt all uploaded objects.","timestamp":"1722603180.0","comment_id":"1138649"},{"timestamp":"1721299320.0","upvote_count":"2","comment_id":"1125840","poster":"SerialiDr","content":"Selected Answer: B\nB. Symmetric customer managed keys with key material that is generated by AWS: This option allows the developer to create and manage their own encryption keys in AWS KMS, with AWS generating the key material. AWS KMS supports automatic rotation of customer managed keys. You can configure the key to rotate automatically once per year."},{"timestamp":"1718218620.0","upvote_count":"1","poster":"Certified101","content":"Selected Answer: B\nB is correct, it must use KMS","comment_id":"1094960"},{"poster":"ShawnWon","comment_id":"1075143","content":"Option A (Amazon S3 managed keys) does not involve using AWS Key Management Service (AWS KMS) directly. Instead, it relies on Amazon S3 to manage the keys for server-side encryption. If the requirement is specifically to use AWS KMS for encryption, then Option A would not meet that requirement.","timestamp":"1716171840.0","upvote_count":"1"},{"timestamp":"1714264560.0","content":"Selected Answer: B\nOnly this option supports AWS KMS with the key rotation","poster":"wonder_man","comment_id":"1055925","upvote_count":"1"},{"comment_id":"1042517","upvote_count":"1","timestamp":"1713000720.0","poster":"PrakashM14","content":"Asymmetric keys (option C) are typically used for different use cases, such as digital signatures and key pairs, and may not be as suitable for automatic rotation in the described scenario.\n\nImported key material (option D) means that you bring your own key material, and AWS KMS doesn't support automatic rotation for such keys.\n\nAmazon S3 managed keys (option A) are used specifically for Amazon S3 and don't support automatic rotation.\n\nso, option B is correct"},{"poster":"dilleman","timestamp":"1712817840.0","content":"Selected Answer: A\nA: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html","upvote_count":"2","comment_id":"1040256"},{"poster":"Digo30sp","timestamp":"1712399880.0","content":"Selected Answer: A\nA) Amazon S3 Managed Keys\nhttps://docs.aws.amazon.com/pt_br/AmazonS3/latest/userguide/serv-side-encryption.html","comment_id":"1026438","upvote_count":"3"}],"topic":"1"},{"id":"RbARLpCBSBMJuWVQHonB","discussion":[{"comment_id":"1126076","upvote_count":"6","poster":"SerialiDr","timestamp":"1705598760.0","content":"Selected Answer: C\nThis is the most efficient and integrated approach. AWS CodeBuild is fully integrated with AWS CodePipeline and can be used to run unit tests as part of the CI/CD process. Placing the testing stage before deployment ensures that only tested code is deployed. The buildspec can be configured to fail the build if tests do not pass, and CodeBuild's test reports feature allows for easy viewing and analysis of test results."},{"comment_id":"1216458","upvote_count":"1","timestamp":"1716460320.0","poster":"65703c1","content":"Selected Answer: C\nC is the correct answer."},{"poster":"xxxx1","content":"c is the correct answer","timestamp":"1709795700.0","comment_id":"1167739","upvote_count":"1"},{"timestamp":"1698477660.0","upvote_count":"1","poster":"NinjaCloud","content":"Correct answer: B","comment_id":"1056030"},{"timestamp":"1697189460.0","poster":"Gold07","content":"c is the correct answer","comment_id":"1042516","upvote_count":"1"},{"poster":"Cerakoted","upvote_count":"2","timestamp":"1697035920.0","content":"Selected Answer: C\nI think C is correct.\nTypical consists of stages are..\nBuild -> Test -> Deploy(test) -> Load Test -> and others","comment_id":"1040782"},{"comment_id":"1039376","content":"Selected Answer: C\nC should be correct.","timestamp":"1696934160.0","upvote_count":"3","poster":"dilleman"},{"comment_id":"1026440","upvote_count":"1","poster":"Digo30sp","timestamp":"1696588740.0","comments":[{"content":"definitely not B, since nobody doing tests after deployment. No sense","timestamp":"1730675520.0","poster":"04075e0","comment_id":"1306677","upvote_count":"1"},{"upvote_count":"5","content":"This does not make sense. Why run the tests after the deploy when you can choose option C, to run the tests before the deploy? C should be best practice and the same amount of effort as B.","comment_id":"1039375","timestamp":"1696934100.0","comments":[{"comment_id":"1053828","poster":"Dibaal","timestamp":"1698247380.0","upvote_count":"1","content":"funny 😁"}],"poster":"dilleman"}],"content":"Selected Answer: B\nThe correct answer is (B).\n\nSolution (B) is the simplest and requires the least operational effort. It involves adding a new stage to the CodePipeline pipeline that uses AWS CodeBuild to run the unit tests. The CodeBuild stage can be configured to fail if any tests fail. The CodeBuild test report can be integrated into the CodeBuild console so that developers can view test results."}],"exam_id":24,"topic":"1","answer_images":[],"answers_community":["C (92%)","8%"],"choices":{"A":"Write a Git pre-commit hook that runs the tests before every commit. Ensure that each developer who is working on the project has the pre-commit hook installed locally. Review the test report and resolve any issues before pushing changes to AWS CodeCommit.","B":"Add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage after the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues.","C":"Add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage before the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues.","D":"Add a new stage to the pipeline. Use Jenkins as the provider. Configure CodePipeline to use Jenkins to run the unit tests. Write a Jenkinsfile that fails the stage if any test does not pass. Use the test report plugin for Jenkins to integrate the report with the Jenkins dashboard. View the test results in Jenkins. Resolve any issues."},"answer":"C","unix_timestamp":1696549560,"answer_description":"","question_images":[],"answer_ET":"C","question_id":59,"timestamp":"2023-10-06 01:46:00","url":"https://www.examtopics.com/discussions/amazon/view/122569-exam-aws-certified-developer-associate-dva-c02-topic-1/","question_text":"A team of developers is using an AWS CodePipeline pipeline as a continuous integration and continuous delivery (CI/CD) mechanism for a web application. A developer has written unit tests to programmatically test the functionality of the application code. The unit tests produce a test report that shows the results of each individual check. The developer now wants to run these tests automatically during the CI/CD process.\n\nWhich solution will meet this requirement with the LEAST operational effort?","isMC":true},{"id":"HeXvNqdzfIqM6Nb86Ki5","url":"https://www.examtopics.com/discussions/amazon/view/122572-exam-aws-certified-developer-associate-dva-c02-topic-1/","discussion":[{"content":"Selected Answer: D\nC works as well but It is a broad solution I think it's better practice to use D and specify the exact endpoints that the user can access from. \n\"aws:sourceVpce\": [\"vpce-id1\", \"vpce-id2\", \"...\"]","timestamp":"1696934700.0","upvote_count":"7","poster":"dilleman","comment_id":"1039386"},{"comment_id":"1070055","comments":[{"content":"I think the same \"A developer needs to configure an Amazon S3 bucket policy so users can access an S3 bucket only by using these VPC endpoints\"","poster":"shake76","timestamp":"1701884520.0","comment_id":"1089591","upvote_count":"1","comments":[{"upvote_count":"2","comments":[{"content":"for bucket policy, if vpce isnt explicitly allowed, it's by default denied anyway so it should have been allow string equal vpce?","timestamp":"1722375960.0","poster":"examtopics111","upvote_count":"1","comment_id":"1258424"}],"content":"StringNotEqual is for the deny of outher that mentioned vpce.\n\n{\n \"Version\": \"2012-10-17\",\n \"Id\": \"Policy1415115909152\",\n \"Statement\": [\n {\n \"Sid\": \"Access-to-specific-VPCE-only\",\n \"Principal\": \"*\",\n \"Action\": \"s3:*\",\n \"Effect\": \"Deny\",\n \"Resource\": [\"arn:aws:s3:::awsexamplebucket1\",\n \"arn:aws:s3:::awsexamplebucket1/*\"],\n \"Condition\": {\n \"StringNotEquals\": {\n \"aws:SourceVpce\": \"vpce-1a2b3c4d\"\n }\n }\n }\n ]\n}","timestamp":"1710325140.0","comment_id":"1172425","poster":"vipyodha"}]}],"timestamp":"1699942740.0","poster":"CrescentShared","upvote_count":"6","content":"I don't think any of the options is correct. Seriously StringNotEquals not StringEquals?"},{"timestamp":"1727323680.0","poster":"albert_kuo","content":"Selected Answer: D\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Sid\": \"DenyAccessIfNotFromSpecificVPCEndpoints\",\n \"Effect\": \"Deny\",\n \"Principal\": \"*\",\n \"Action\": \"s3:*\",\n \"Resource\": [\n \"arn:aws:s3:::your-bucket-name\",\n \"arn:aws:s3:::your-bucket-name/*\"\n ],\n \"Condition\": {\n \"StringNotEquals\": {\n \"aws:SourceVpce\": [\n \"vpce-0123456789abcdef0\",\n \"vpce-0fedcba9876543210\",\n \"vpce-0a1b2c3d4e5f6a7b8\"\n ]\n }\n }\n }\n ]\n}","comment_id":"1289279","upvote_count":"3"},{"upvote_count":"1","content":"Selected Answer: D\nD is the correct answer.","poster":"65703c1","comment_id":"1216460","timestamp":"1716460500.0"},{"comment_id":"1158875","content":"Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html typically explained the same scenario. D beyond doubt.","poster":"KarBiswa","upvote_count":"2","timestamp":"1708872660.0"},{"comment_id":"1138068","timestamp":"1706842440.0","upvote_count":"1","content":"Selected Answer: D\nD, based on the following documentation:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html#example-bucket-policies-restrict-accesss-vpc-endpoint","poster":"joshnort"},{"comment_id":"1132281","timestamp":"1706247720.0","upvote_count":"2","content":"Why it's StringNotEquals instead of StringEquals? Is the question wrong or my English is too bad to understand this?","poster":"CrescentShared","comments":[{"poster":"vipyodha","content":"It is StringNotEqual, means if source vpce is not this then deny access\n\n{\n \"Version\": \"2012-10-17\",\n \"Id\": \"Policy1415115909152\",\n \"Statement\": [\n {\n \"Sid\": \"Access-to-specific-VPCE-only\",\n \"Principal\": \"*\",\n \"Action\": \"s3:*\",\n \"Effect\": \"Deny\",\n \"Resource\": [\"arn:aws:s3:::awsexamplebucket1\",\n \"arn:aws:s3:::awsexamplebucket1/*\"],\n \"Condition\": {\n \"StringNotEquals\": {\n \"aws:SourceVpce\": \"vpce-1a2b3c4d\"\n }\n }\n }\n ]\n}","upvote_count":"3","timestamp":"1710324780.0","comment_id":"1172423"}]},{"comment_id":"1126079","poster":"SerialiDr","content":"Selected Answer: D\nThis option is the closest to being correct, but it should use StringEquals instead of StringNotEquals. The correct approach is to use a single S3 bucket policy with a condition that includes aws:SourceVpce with StringEquals for the specific VPC endpoint IDs. This will ensure that access is allowed only from those specified endpoints.","upvote_count":"2","timestamp":"1705599060.0"},{"upvote_count":"2","comment_id":"1111199","timestamp":"1704116040.0","poster":"rrshah83","content":"Selected Answer: B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html#example-bucket-policies-restrict-access-vpc"},{"poster":"Certified101","content":"Selected Answer: D\nD is correct","timestamp":"1702414740.0","upvote_count":"1","comment_id":"1094963"},{"timestamp":"1697203140.0","poster":"PrakashM14","comment_id":"1042699","upvote_count":"2","comments":[{"poster":"ekutas","comments":[{"upvote_count":"2","poster":"ekutas","content":"Od course if we use \"Effect\": \"Allow\"))","timestamp":"1698960120.0","comment_id":"1060920"}],"upvote_count":"1","content":"D says \"aws:sourceVpce value in the StringNotEquals condition\". StringNotEquals won't work, it deny access for specified VPC ids","comment_id":"1060918","timestamp":"1698959700.0"}],"content":"Selected Answer: D\nin option C :\nCondition\": {\n \"StringNotEqualsIfExists\": {\n \"aws:sourceVpce\": \"vpce*\",\n }\n}\nit might Deny access from all VPC endpoints.\n\nso the ans is D"},{"upvote_count":"1","timestamp":"1696588800.0","poster":"Digo30sp","content":"Selected Answer: C\nThe correct answer is (C).\n\nSolution (C) is the simplest and will meet the company's requirements. It creates a single S3 bucket policy that has the value aws:SourceVpce and the StringNotEquals condition to use vpce*. This will only allow users who are using a VPC endpoint in the same VPC to access the S3 bucket.","comment_id":"1026441"}],"question_text":"A company has multiple Amazon VPC endpoints in the same VPC. A developer needs to configure an Amazon S3 bucket policy so users can access an S3 bucket only by using these VPC endpoints.\n\nWhich solution will meet these requirements?","answer_images":[],"answer_ET":"D","isMC":true,"exam_id":24,"unix_timestamp":1696550100,"topic":"1","timestamp":"2023-10-06 01:55:00","choices":{"D":"Create a single S3 bucket policy that has multiple aws:sourceVpce value in the StringNotEquals condition. Repeat for all the VPC endpoint IDs.","B":"Create a single S3 bucket policy that has the aws:SourceVpc value and in the StringNotEquals condition to use VPC ID.","A":"Create multiple S3 bucket polices by using each VPC endpoint ID that have the aws:SourceVpce value in the StringNotEquals condition.","C":"Create a single S3 bucket policy that has the aws:SourceVpce value and in the StringNotEquals condition to use vpce*."},"question_id":60,"answer":"D","question_images":[],"answer_description":"","answers_community":["D (87%)","9%"]}],"exam":{"provider":"Amazon","lastUpdated":"11 Apr 2025","isImplemented":true,"isBeta":false,"numberOfQuestions":551,"id":24,"name":"AWS Certified Developer - Associate DVA-C02","isMCOnly":true},"currentPage":12},"__N_SSP":true}