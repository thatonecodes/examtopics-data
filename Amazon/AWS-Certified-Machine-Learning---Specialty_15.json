{"pageProps":{"questions":[{"id":"MtuAGFxZFzwV8aD2Y12a","question_id":71,"choices":{"A":"Change the notebook instance type to a memory optimized instance with the same vCPU number as the ml.m5.4xlarge instance has. Stop the notebook when it is not in use. Run both data preprocessing and feature engineering development on that instance.","D":"Change the notebook instance type to a smaller general purpose instance. Stop the notebook when it is not in use. Run data preprocessing on an R5 instance with the same memory size as the ml.m5.4xlarge instance by using the Reserved Instance option.","C":"Change the notebook instance type to a smaller general purpose instance. Stop the notebook when it is not in use. Run data preprocessing on an ml.r5 instance with the same memory size as the ml.m5.4xlarge instance by using Amazon SageMaker Processing.","B":"Keep the notebook instance type and size the same. Stop the notebook when it is not in use. Run data preprocessing on a P3 instance type with the same memory as the ml.m5.4xlarge instance by using Amazon SageMaker Processing."},"unix_timestamp":1651252980,"exam_id":26,"answer_ET":"C","answer_description":"","isMC":true,"answers_community":["C (67%)","D (33%)"],"discussion":[{"poster":"spaceexplorer","comment_id":"594614","upvote_count":"15","content":"Selected Answer: C\nB is wrong as it says it doesn't take advantage of GPUs","timestamp":"1651252980.0"},{"comment_id":"601547","content":"I believe answer should be C. a) Initial processing needs less cpu and memory so that can be done on a smaller instance. b) Second operation is memory intensive so instance type should be changed to R5 type instance.","upvote_count":"8","poster":"exam_prep","timestamp":"1652528340.0"},{"upvote_count":"2","poster":"data_sma","content":"Selected Answer: C\nIt's C:\n1. Change the notebook instance to a smaller one (fewer resources and lower cost), ideal for feature engineering work.\n2. Move data preprocessing to an ml.r5 instance, which is memory-optimized and therefore better suited to the preprocessing workload.\n3. Using Amazon SageMaker Processing to perform preprocessing allows you to allocate resources only when needed (2 hours per day), reducing operational costs.\nWHY IS NOT D?\nWhile using Reserved Instances can reduce costs, it involves a long-term commitment that may not be ideal for variable or seasonal workloads.","comment_id":"1324219","timestamp":"1733775780.0"},{"content":"Selected Answer: C\nI'd opt for C. A and B are wrong for obvious reasons.\n\nD sounds good but it doesn't have a ML instance and also it's just the development phase and we might not want to reserve an instance for too long.","poster":"Stokvisss","timestamp":"1709105400.0","upvote_count":"1","comment_id":"1161365"},{"poster":"geoan13","content":"Option A need only one instance\nall other options talks about 2 instances. so why can't it be A...","timestamp":"1700001120.0","upvote_count":"2","comment_id":"1070901"},{"comment_id":"1070898","timestamp":"1700000940.0","poster":"geoan13","upvote_count":"1","content":"C Memory-optimized instances means provide a high memory\nIn D they mention reserved instance. so it is costly"},{"comment_id":"1009207","poster":"jopaca1216","timestamp":"1694883600.0","upvote_count":"1","content":"C is correct.\nDue that B is wrong, is not to use a GPU Instance based."},{"upvote_count":"1","content":"Selected Answer: C\n\"Which solution will result in the MOST cost savings\"\nBecause of this, D is wrong: are you sure that allocating an instance for months / years for a 2h/day is cost saving?\nCorrect is C","timestamp":"1691590620.0","poster":"kaike_reis","comment_id":"976727"},{"comment_id":"967239","upvote_count":"1","content":"Selected Answer: C\noffers the best balance of cost savings and resource adequacy for both feature engineering and data preprocessing tasks.","timestamp":"1690730640.0","poster":"Mickey321"},{"content":"It' C, as Reserved Instance no good for only 2 hours of daily work.","timestamp":"1688649420.0","upvote_count":"1","comment_id":"944688","poster":"ADVIT"},{"upvote_count":"1","comment_id":"859312","content":"Selected Answer: D\nR instance with processes that uses lot of memory. Reserved instances for less cost","timestamp":"1680465600.0","comments":[{"content":"Selection of D is totally wrong, because you don't understand what \"Reserved Instance\" is!!!\nYou cannot reserve a instance only for hours a day!!!! this is like apartment rent, can you just rent an apartment for nap time????","poster":"ZSun","timestamp":"1683121260.0","comment_id":"888564","upvote_count":"3"}],"poster":"Mllb"},{"comment_id":"814177","upvote_count":"2","poster":"AjoseO","comments":[{"comment_id":"970152","content":"is better not use everytime chatgpt, and read AWS documentation about instances.","poster":"ccpmad","upvote_count":"2","timestamp":"1690979280.0"}],"timestamp":"1676817480.0","content":"Selected Answer: D\nD over C because if the EC2 instance is being used consistently for the same two hours each day, customers could consider using a Reserved Instance with a term of 1 or 3 years and payment option that aligns with their usage pattern. \n\nThis would provide significant cost savings compared to On-Demand pricing for those two hours each day."},{"comment_id":"787163","timestamp":"1674610980.0","upvote_count":"1","content":"B. \nC is wrong as ml.r5 is not stopped when not in use","poster":"jhonivy"},{"content":"Selected Answer: D\nIMO D is correct. The reserved instance option for an R5 instance, as in Option D, would provide the greatest cost savings, as reserved instances offer a discounted hourly rate in exchange for a one-time payment for a committed usage term.","timestamp":"1671649260.0","upvote_count":"4","comment_id":"752658","poster":"DeepakPg"},{"upvote_count":"4","comment_id":"683239","comments":[{"upvote_count":"1","comment_id":"884061","timestamp":"1682747340.0","poster":"Tony_1406","content":"\"Scheduled RIs: These are available to launch within the time windows you reserve. This option allows you to match your capacity reservation to a predictable recurring schedule that only requires a fraction of a day, a week, or a month.\"\nYou have the option to reserve for a fraction of a day. Since the question specify precisely how long the job is, it makes it suitable. \nhttps://aws.amazon.com/ec2/pricing/reserved-instances/"}],"poster":"ryuhei","timestamp":"1664515560.0","content":"Selected Answer: C\nI think C is correct.\nIt only runs for 2 hours once a day, so RI is wasted. So I think D is wrong."},{"poster":"Shailendraa","timestamp":"1663007940.0","upvote_count":"2","content":"12-sep exam","comment_id":"667357"},{"poster":"rafael_teste","comment_id":"644484","timestamp":"1660046280.0","content":"Selected Answer: D\nI think it is D. Using RIs the customer can have the greatest cost savings, as stated by the question","upvote_count":"2"},{"comment_id":"595662","upvote_count":"3","content":"Selected Answer: D\nI think the answer should be D: R5 instance is cheaper and appropriate for data processing under same memory size; reserved instance make it cheaper.","poster":"DJiang","comments":[{"content":"only use 2 hours per day, so no need for RI","timestamp":"1653741420.0","upvote_count":"8","comment_id":"608391","poster":"exam887"}],"timestamp":"1651422360.0"}],"answer_images":[],"answer":"C","url":"https://www.examtopics.com/discussions/amazon/view/74919-exam-aws-certified-machine-learning-specialty-topic-1/","topic":"1","question_text":"A company is building a demand forecasting model based on machine learning (ML). In the development stage, an ML specialist uses an Amazon SageMaker notebook to perform feature engineering during work hours that consumes low amounts of CPU and memory resources. A data engineer uses the same notebook to perform data preprocessing once a day on average that requires very high memory and completes in only 2 hours. The data preprocessing is not configured to use GPU. All the processes are running well on an ml.m5.4xlarge notebook instance.\nThe company receives an AWS Budgets alert that the billing for this month exceeds the allocated budget.\nWhich solution will result in the MOST cost savings?","question_images":[],"timestamp":"2022-04-29 19:23:00"},{"id":"CzKuK6JhLyi6LhvpVymP","question_id":72,"question_text":"A machine learning specialist is developing a regression model to predict rental rates from rental listings. A variable named Wall_Color represents the most prominent exterior wall color of the property. The following is the sample data, excluding all other variables:\n//IMG//\n\nThe specialist chose a model that needs numerical input data.\nWhich feature engineering approaches should the specialist use to allow the regression model to learn from the Wall_Color data? (Choose two.)","url":"https://www.examtopics.com/discussions/amazon/view/75059-exam-aws-certified-machine-learning-specialty-topic-1/","unix_timestamp":1651490580,"answer_description":"","isMC":true,"answer_images":[],"choices":{"A":"Apply integer transformation and set Red = 1, White = 5, and Green = 10.","C":"Replace the color name string by its length.","E":"Replace each color name by its training set frequency.","B":"Add new columns that store one-hot representation of colors.","D":"Create three columns to encode the color in RGB format."},"discussion":[{"timestamp":"1651753500.0","comment_id":"597299","content":"B, and E (frequency encoding)","upvote_count":"13","poster":"edvardo"},{"poster":"bluer1","comments":[{"comment_id":"601995","content":"I think D cannot be because distances in RGB format are not representive of points. CIELAB correlates numerical color values consistently with human visual perception.","timestamp":"1652604000.0","poster":"vanluigi","upvote_count":"1"}],"content":"BD？ any thought?","timestamp":"1651490580.0","upvote_count":"8","comment_id":"596013"},{"poster":"MultiCloudIronMan","comment_id":"1299910","upvote_count":"2","content":"Selected Answer: BD\nThese methods ensure that the color data is represented numerically while preserving the information’s integrity and relevance for the regression model","timestamp":"1729318920.0"},{"timestamp":"1719041460.0","poster":"pandkast","upvote_count":"1","content":"Selected Answer: BD\nUsing frequency encoding may help in some contexts but can introduce bias, especially if the frequency of a color is not related to the rental rate. This method does not leverage the actual differences between colors.","comment_id":"1235240"},{"upvote_count":"2","comment_id":"1147211","content":"Selected Answer: BD\nIn this scenario, the specialist should use one-hot encoding and RGB encoding to allow the regression model to learn from the Wall_Color data. One-hot encoding is a technique used to convert categorical data into numerical data. It creates new columns that store one-hot representation of colors. For example, a variable named color has three categories: red, green, and blue. After one-hot encoding\nRGB encoding can capture the intensity and hue of a color, but it may also introduce correlation\namong the three columns. Therefore, using both one-hot encoding and RGB encoding can providemore information to the regression model than using either one alone.","poster":"kyuhuck","timestamp":"1707654360.0"},{"upvote_count":"3","comment_id":"976737","poster":"kaike_reis","timestamp":"1691591160.0","content":"Selected Answer: BE\nHere we have a non-ordinal categorical variable to receive a numerical conversion for the model. Letter A is wrong as it is not an ordinal variable. Letter C is wrong as we are not going to retain any significant information for the model. The best solutions would be: Letter B and E. Letter D would be very interesting, but it would generate a problem of information fragmentation: most models consider the variables as being independent of each other, and these 3 columns by definition would not be independent."},{"upvote_count":"2","content":"Selected Answer: BE\nB, and E (frequency encoding)","comment_id":"967297","timestamp":"1690735200.0","poster":"Mickey321"},{"timestamp":"1688649540.0","poster":"ADVIT","content":"A+B make sense to me","upvote_count":"1","comment_id":"944690"},{"upvote_count":"1","poster":"WilianCB","timestamp":"1682861700.0","comment_id":"885269","content":"Selected Answer: BD\nB for sure \nD. This approach involves breaking down each color into its Red, Green, and Blue components and creating separate columns for each component. This allows the model to capture the information about the intensity of each color component, which can be useful in predicting the target variable.\n\nA, C, and E are not suitable for encoding color data in a way that can be used by a regression model. The integer transformation approach in option A arbitrarily assigns values to colors without any meaningful relationship between them. The approach in option C replaces the color names with their length, which does not provide any useful information for the model. Option E replaces each color name with its frequency in the training set, which does not capture any information about the color itself."},{"timestamp":"1678404960.0","poster":"alp_ileri","comment_id":"834509","content":"I think frequency encoding cannot be. What if some colors have same amount of frequency?","upvote_count":"7"},{"upvote_count":"7","poster":"AjoseO","comment_id":"814181","comments":[{"upvote_count":"5","comment_id":"814183","timestamp":"1676817960.0","content":"Frequency encoding is a feature engineering technique used to convert categorical variables into numerical ones by replacing each category with the frequency of its occurrence in the training set. This approach can be useful when dealing with high-cardinality categorical variables, which are categorical variables with a large number of distinct categories.","poster":"AjoseO"}],"timestamp":"1676817900.0","content":"Selected Answer: BE\nB. Add new columns that store one-hot representation of colors.\nOne-hot encoding is a common approach to represent categorical variables as numerical values. This approach creates new binary variables for each category and assigns a value of 1 to the corresponding category and 0 to the others. In this case, the specialist can create three new binary variables, one for each color (Red, White, and Green) and use them as input to the regression model.\n\nE. Replace each color name by its training set frequency.\nAnother approach to convert categorical variables into numerical ones is to replace each category with its frequency of occurrence in the training set. In this case, the specialist can replace the color names with their respective frequencies (1/3 for Red, 1/3 for White, and 1/3 for Green) to represent them numerically."},{"poster":"maxkm","upvote_count":"2","timestamp":"1675096080.0","comment_id":"793040","content":"Selected Answer: BD\nThese are the only options preserving what \"color\" is. One-hot encoding is a default standard for any categorical data to be fed to a model that takes in numeric input. RGB format is a good numeric representation of any color by preserving its nature"},{"upvote_count":"2","content":"Selected Answer: AB\nA&B\nhttps://victorzhou.com/blog/one-hot/#:~:text=One-Hot%20Encoding%20takes%20a%20single%20integer%20and%20produces,of%20colors%20are%20possible%3A%20red%2C%20green%2C%20or%20blue.","poster":"Sonoko","comments":[{"comment_id":"810566","upvote_count":"3","timestamp":"1676544900.0","poster":"wolfsong","content":"B & E. \nIt cannot be A because your URL specifically states that: \n\"This is known as integer encoding. For Machine Learning, this encoding can be problematic - in this example, we’re essentially saying “green” is the average of “red” and “blue”, which can lead to weird unexpected outcomes.\""}],"timestamp":"1671151560.0","comment_id":"746636"},{"timestamp":"1656298080.0","comment_id":"622903","poster":"ovokpus","upvote_count":"3","content":"Selected Answer: BE\nFrequency encoding"},{"upvote_count":"3","poster":"[Removed]","timestamp":"1655291700.0","comment_id":"616711","content":"B and E"},{"upvote_count":"5","comment_id":"610988","poster":"tgaos","timestamp":"1654244700.0","content":"BE is correct.\nFor e, please refer: https://medium.com/analytics-vidhya/different-type-of-feature-engineering-encoding-techniques-for-categorical-variable-encoding-214363a016fb#:~:text=One%20Hot%20Encoding%3A%20%E2%80%94%20In%20this,slows%20down%20the%20learning%20significantly."}],"answer":"BE","answers_community":["BE (60%)","BD (32%)","8%"],"answer_ET":"BE","question_images":["https://www.examtopics.com/assets/media/exam-media/04145/0009900001.png"],"topic":"1","exam_id":26,"timestamp":"2022-05-02 13:23:00"},{"id":"05DerY8nHqwQrQ9On6kN","url":"https://www.examtopics.com/discussions/amazon/view/77501-exam-aws-certified-machine-learning-specialty-topic-1/","question_text":"A data scientist is working on a public sector project for an urban traffic system. While studying the traffic patterns, it is clear to the data scientist that the traffic behavior at each light is correlated, subject to a small stochastic error term. The data scientist must model the traffic behavior to analyze the traffic patterns and reduce congestion.\nHow will the data scientist MOST effectively model the problem?","unix_timestamp":1657685820,"answer_description":"","answers_community":["A (100%)"],"topic":"1","discussion":[{"upvote_count":"14","comment_id":"653642","poster":"DD4","timestamp":"1693341060.0","content":"answer : A, because the setting needs multi agents and is constrained with traffic light correlation."},{"upvote_count":"12","poster":"AjoseO","comments":[{"poster":"ccpmad","upvote_count":"2","content":"thank you chatgpt","comment_id":"970155","timestamp":"1722601920.0"}],"timestamp":"1708354440.0","comment_id":"814191","content":"Selected Answer: A\nA. The data scientist should obtain a correlated equilibrium policy by formulating this problem as a multi-agent reinforcement learning problem.\n\nIn this scenario, where the traffic behavior at each light is correlated, a multi-agent reinforcement learning (MARL) approach is well-suited to model the problem. In MARL, multiple agents interact with each other and the environment, and their behavior is influenced by the behavior of other agents. This approach is particularly useful in modeling traffic systems, where the behavior of each vehicle is affected by the behavior of other vehicles and traffic lights.\n\nFormulating the problem as a MARL problem can help the data scientist obtain a correlated equilibrium policy, which can optimize traffic flow across multiple traffic lights by taking into account the correlations between them. By optimizing traffic flow across all traffic lights in a correlated way, it may be possible to reduce congestion and improve overall traffic efficiency."},{"comment_id":"1046849","timestamp":"1729250160.0","content":"i am wondering how is this actually implemented, i am learning deep RL right now","upvote_count":"1","poster":"wendaz"},{"timestamp":"1723213680.0","poster":"kaike_reis","content":"Selected Answer: A\nIt's too complex problem for supervised or unsupervised. It's a multi-agent problem.","comment_id":"976741","upvote_count":"1"},{"comment_id":"797420","timestamp":"1706997240.0","poster":"damaldon","content":"Answer is A","upvote_count":"1"},{"poster":"Skychaser","upvote_count":"6","comment_id":"630766","timestamp":"1689221820.0","content":"https://www.researchgate.net/publication/221456376_Multi-Agent_Reinforcement_Learning_for_Simulating_Pedestrian_Navigation"}],"exam_id":26,"question_images":[],"choices":{"D":"Rather than finding an equilibrium policy, the data scientist should obtain accurate predictors of traffic flow by using unlabeled simulated data representing the new traffic patterns in the city and applying an unsupervised learning approach.","A":"The data scientist should obtain a correlated equilibrium policy by formulating this problem as a multi-agent reinforcement learning problem.","C":"Rather than finding an equilibrium policy, the data scientist should obtain accurate predictors of traffic flow by using historical data through a supervised learning approach.","B":"The data scientist should obtain the optimal equilibrium policy by formulating this problem as a single-agent reinforcement learning problem."},"answer_ET":"A","timestamp":"2022-07-13 06:17:00","question_id":73,"answer_images":[],"isMC":true,"answer":"A"},{"id":"Bzl7B0ZsqUt7f36SQ3vo","question_images":[],"answer_ET":"D","choices":{"C":"Use the SageMaker built-in Object Detection algorithm instead of the NTM algorithm for the training job to process the blog post data.","A":"Use the Amazon Comprehend entity recognition API operations. Remove the detected words from the blog post data. Replace the blog post data source in the S3 bucket.","B":"Run the SageMaker built-in principal component analysis (PCA) algorithm with the blog post data from the S3 bucket as the data source. Replace the blog post data in the S3 bucket with the results of the training job.","D":"Remove the stopwords from the blog post data by using the CountVectorizer function in the scikit-learn library. Replace the blog post data in the S3 bucket with the results of the vectorizer."},"answers_community":["D (90%)","10%"],"exam_id":26,"url":"https://www.examtopics.com/discussions/amazon/view/76486-exam-aws-certified-machine-learning-specialty-topic-1/","discussion":[{"content":"D: Option A, B & C don't make sense. D removes the stop words and help in count vectors","comment_id":"610351","timestamp":"1685658000.0","poster":"exam_prep","upvote_count":"16"},{"content":"Selected Answer: D\nD\nThe only solution that solves our problem: remove stopwords ASAP","upvote_count":"1","timestamp":"1723213800.0","comment_id":"976744","poster":"kaike_reis"},{"upvote_count":"1","poster":"Mickey321","content":"Selected Answer: D\nD: Option A, B & C don't make sense. D removes the stop words and help in count vectors","timestamp":"1722369000.0","comment_id":"967464"},{"poster":"ADVIT","upvote_count":"2","timestamp":"1720272840.0","content":"D, ChatGPT confirm :)","comment_id":"944705"},{"content":"Selected Answer: D\nNeeds to remove stopwords and the rare worlds are feasible.","comment_id":"833389","timestamp":"1709933100.0","upvote_count":"3","poster":"Valcilio"},{"timestamp":"1702240140.0","poster":"Peeking","upvote_count":"4","content":"Selected Answer: D\nThe stop words need to be removed. The rare words don't need to be removed because it has been found that they are feasible tags.","comment_id":"741225"},{"upvote_count":"1","timestamp":"1695200820.0","comments":[{"upvote_count":"1","comment_id":"714230","content":"check the requirement in question: \"the generated model do not include the stopwords\"","timestamp":"1699497060.0","poster":"VinceCar"}],"poster":"HerbertK","comment_id":"673965","content":"Selected Answer: A\nWhy not A?"}],"topic":"1","answer_images":[],"isMC":true,"answer_description":"","unix_timestamp":1654122000,"timestamp":"2022-06-02 00:20:00","answer":"D","question_id":74,"question_text":"A data scientist is using the Amazon SageMaker Neural Topic Model (NTM) algorithm to build a model that recommends tags from blog posts. The raw blog post data is stored in an Amazon S3 bucket in JSON format. During model evaluation, the data scientist discovered that the model recommends certain stopwords such as \"a,\" \"an,\" and \"the\" as tags to certain blog posts, along with a few rare words that are present only in certain blog entries. After a few iterations of tag review with the content team, the data scientist notices that the rare words are unusual but feasible. The data scientist also must ensure that the tag recommendations of the generated model do not include the stopwords.\nWhat should the data scientist do to meet these requirements?"},{"id":"ao1LtxrpVGYWD0Zmt2SN","exam_id":26,"answers_community":["C (93%)","7%"],"discussion":[{"comment_id":"601247","content":"Selected Answer: C\nhttps://aws.amazon.com/datasync/faqs/\nBased on answer for the question - \"How do I use AWS DataSync to migrate data to AWS?\"","timestamp":"1683995520.0","upvote_count":"16","poster":"Sivadharan"},{"content":"Selected Answer: C\nit's DataSync","comment_id":"637916","poster":"matteocal","upvote_count":"5","timestamp":"1690444800.0"},{"comment_id":"1082454","upvote_count":"2","timestamp":"1732791540.0","content":"Selected Answer: C\nAll other options seem like they would require some manual coding to meet all requirements. DataSync appears as the best option as a result","poster":"endeesa"},{"upvote_count":"1","timestamp":"1722369180.0","content":"Selected Answer: C\nOption C, using AWS DataSync, is the most appropriate solution. AWS DataSync is a service designed for data transfer between on-premises storage and AWS, and it provides the features the company needs:","comment_id":"967469","poster":"Mickey321"},{"content":"Selected Answer: C\nC- DataSync is the answer","upvote_count":"1","comment_id":"944713","timestamp":"1720273200.0","poster":"ADVIT"},{"content":"Selected Answer: C\nAWS DataSync is a service that can be used to transfer large amounts of data between on-premises storage and Amazon S3, EFS, or FSx for Windows File Server. DataSync is optimized for fast, automated, and secure transfers of large amounts of data, and it supports scheduling, monitoring, and data integrity validation.\n\nIn this scenario, the company wants a solution that can transfer and automatically update data between the on-premises object storage and Amazon S3, with support for encryption, scheduling, monitoring, and data integrity validation. DataSync meets all of these requirements, as it can transfer data using secure network connections, schedule data transfers, verify data integrity, and encrypt data in transit and at rest.","upvote_count":"3","timestamp":"1708354740.0","poster":"AjoseO","comment_id":"814194"},{"poster":"edvardo","content":"Selected Answer: A\nA maybe?\n\nhttps://aws.amazon.com/datasync/faqs/","comments":[{"content":"I meant C","timestamp":"1683708840.0","poster":"edvardo","upvote_count":"4","comment_id":"599476"}],"comment_id":"599475","timestamp":"1683708780.0","upvote_count":"2"}],"answer":"C","question_text":"A company wants to create a data repository in the AWS Cloud for machine learning (ML) projects. The company wants to use AWS to perform complete ML lifecycles and wants to use Amazon S3 for the data storage. All of the company's data currently resides on premises and is 40 ׀¢׀’ in size.\nThe company wants a solution that can transfer and automatically update data between the on-premises object storage and Amazon S3. The solution must support encryption, scheduling, monitoring, and data integrity validation.\nWhich solution meets these requirements?","answer_images":[],"question_images":[],"timestamp":"2022-05-10 10:53:00","isMC":true,"url":"https://www.examtopics.com/discussions/amazon/view/75399-exam-aws-certified-machine-learning-specialty-topic-1/","question_id":75,"topic":"1","answer_ET":"C","answer_description":"","unix_timestamp":1652172780,"choices":{"B":"Use AWS Transfer for FTPS to transfer the files from the on-premises storage to Amazon S3.","D":"Use S3 Batch Operations to pull data periodically from the on-premises storage. Enable S3 Versioning on the S3 bucket to protect against accidental overwrites.","A":"Use the S3 sync command to compare the source S3 bucket and the destination S3 bucket. Determine which source files do not exist in the destination S3 bucket and which source files were modified.","C":"Use AWS DataSync to make an initial copy of the entire dataset. Schedule subsequent incremental transfers of changing data until the final cutover from on premises to AWS."}}],"exam":{"provider":"Amazon","isImplemented":true,"lastUpdated":"11 Apr 2025","isMCOnly":false,"id":26,"isBeta":false,"numberOfQuestions":369,"name":"AWS Certified Machine Learning - Specialty"},"currentPage":15},"__N_SSP":true}