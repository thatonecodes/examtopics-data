{"pageProps":{"questions":[{"id":"JbcTZHvixbiOIHsaz8VW","unix_timestamp":1741983600,"timestamp":"2025-03-14 21:20:00","question_id":286,"choices":{"A":"Enable AWS Config. Add the alb-waf-enabled managed rule. Create an AWS Systems Manager Automation document to add AWS WAF to an ALB. Edit the rule to automatically remediate. Select the Systems Manager Automation document as the remediation action.","D":"Configure an Amazon EventBridge rule to periodically call an AWS Lambda function that calls the detect-stack-drift API on the CloudFormation template. Configure the Lambda function to delete and redeploy the CloudFormation stack if the AWS::WAFv2::WebACLAssociation resource shows a status of drifted.","C":"Configure an Amazon EventBridge rule to periodically call an AWS Lambda function that calls the detect-stack-drift API on the CloudFormation template. Configure the Lambda function to modify the ALB attributes with waf.fail_open.enabled set to true if the AWS::WAFv2::WebACLAssociation resource shows a status of drifted.","B":"Enable AWS Config. Add the alb-waf-enabled managed rule. Create an Amazon EventBridge rule to send all AWS Config ConfigurationItemChangeNotification notification types to an AWS Lambda function. Configure the Lambda function to call the AWS Config start-resource-evaluation API in detective mode."},"exam_id":23,"answer":"A","answers_community":["A (100%)"],"isMC":true,"answer_ET":"A","question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/amazon/view/169071-exam-aws-certified-devops-engineer-professional-dop-c02/","topic":"1","answer_images":[],"question_text":"A DevOps engineer uses AWS WAF to manage web ACLs across an AWS account. The DevOps engineer must ensure that AWS WAF is enabled for all Application Load Balancers (ALBs) in the account. The DevOps engineer uses an AWS CloudFormation template to deploy an individual ALB and AWS WAF as part of each application stack's deployment process. If AWS WAF is removed from the ALB after the ALB is deployed, AWS WAF must be added to the ALB automatically.\n\nWhich solution will meet these requirements with the MOST operational efficiency?","discussion":[{"timestamp":"1741983600.0","upvote_count":"1","comment_id":"1395712","poster":"2b80c69","content":"Selected Answer: A\nAWS Config has elb-waf-enabled managed rule which solves the problem in a most operationally efficient manner.\nNot B coz, it focus more on sending notifications which is not the purpose of the question\nNot C & D coz, it has drift detection, which is again not the purpose of the question"}]},{"id":"GK400FCQU8gwNLemC17X","answer_ET":"BCE","question_images":[],"unix_timestamp":1681610040,"url":"https://www.examtopics.com/discussions/amazon/view/106307-exam-aws-certified-devops-engineer-professional-dop-c02/","question_text":"A company has multiple member accounts that are part of an organization in AWS Organizations. The security team needs to review every Amazon EC2 security group and their inbound and outbound rules. The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization.\nWhich combination of access changes will meet these requirements? (Choose three.)","answer_images":[],"answer_description":"","isMC":true,"question_id":287,"discussion":[{"poster":"tartarus23","timestamp":"1703026380.0","content":"Selected Answer: BCE\nExplanation:\n\n(B) The trust relationship enables an IAM entity (user, group, or role) to assume a role. In this case, the entities in the management account need to assume roles in the member accounts.\n\n(C) The IAM role in each member account should have a policy attached that grants read-only access to EC2 instances. The AmazonEC2ReadOnlyAccess managed policy provides this access.\n\n(E) An IAM role in the management account should be created that has the permission to perform the sts:AssumeRole action against the member account IAM role's ARN. This allows entities assuming this role to switch to the roles in the member accounts and perform actions according to the permissions of those roles.","upvote_count":"8","comment_id":"927930"},{"upvote_count":"4","content":"BCE are correct: \nB: create trust relationship for management to assume role in member accounts\nC: create role in member account that has access to AmazoneEC2\nE: Create IAM role in management account that allow access to member account IAM role","comment_id":"1135472","poster":"thanhnv142","timestamp":"1722303240.0"},{"comment_id":"1080792","content":"The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization.\n\nReadOnlyAccess and option B grant the assumeRole\nBesides that the correct resource is \"IAM\" not \"I AM\" So BCF is correct","upvote_count":"1","poster":"svjl","timestamp":"1716733260.0"},{"timestamp":"1709110080.0","poster":"RVivek","upvote_count":"1","content":"Selected Answer: BCE\nB- Member accounts should trust Management account\nC- Memeber accounts should have a Role athat has the necessary permission\nE- Managment account should have a IAM user account that has stsAssume role permission for the roles created in member accounts","comment_id":"991865"},{"upvote_count":"2","poster":"incorrigble_maverick","comments":[{"upvote_count":"1","timestamp":"1714677240.0","comment_id":"1060917","poster":"zain1258","content":"D is clearly wrong. You are running your lambda function to get details in management account. The IAM role should be in management account with sts:AssumeRole permission to assume IAM roles in member accounts"}],"timestamp":"1708383600.0","comment_id":"985439","content":"BCE is wrong. They want to programmatically therefore B is definitenly wrong. The Lambda function IAM Role ARN in the management account needs to be able to assume a role in the member account that has the AmazonEC2ReadOnlyAccess attached to it. Therefore, I will go with C, D, E"},{"comment_id":"962404","timestamp":"1706171760.0","content":"BCE correct","poster":"DavidPham","upvote_count":"1"},{"comment_id":"924226","timestamp":"1702653660.0","content":"Selected Answer: BCE\nBCE is right.","poster":"madperro","upvote_count":"1"},{"timestamp":"1701370980.0","poster":"bcx","comment_id":"910455","upvote_count":"1","content":"B, C and E"},{"content":"Selected Answer: BCE\nB, C and E","poster":"PhuocT","timestamp":"1700488260.0","comment_id":"902550","upvote_count":"2"},{"poster":"2pk","comment_id":"899577","upvote_count":"2","timestamp":"1700177520.0","content":"Selected Answer: ACE\nA:By creating a trust relationship that allows users in the member accounts to assume the IAM role in the management account, they will have the necessary permissions to access resources and retrieve the required information.\n\nC:To grant the necessary permissions for retrieving information about EC2 security groups, an IAM role should be created in each member account. This role should have the AmazonEC2ReadOnlyAccess managed policy attached, which provides the required permissions.\n\nE:In the management account, an IAM role should be created that allows assuming the IAM role in the member accounts. This role should have the necessary permissions to perform the sts:AssumeRole action against the ARN of the IAM roles in the member accounts."},{"comment_id":"896540","timestamp":"1699874400.0","upvote_count":"1","content":"Selected Answer: BCE\nBCE will create correct cross account permission","poster":"ele"},{"timestamp":"1699094640.0","poster":"vherman","upvote_count":"1","comment_id":"889305","content":"Selected Answer: ACD\nacd looks good)"},{"content":"Selected Answer: ACD\nACE I guess\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html","poster":"marcoforexam","upvote_count":"1","comment_id":"887320","timestamp":"1698934920.0"},{"content":"Selected Answer: BCE\nCorrect answer","comment_id":"886965","upvote_count":"1","timestamp":"1698898320.0","poster":"haazybanj"},{"upvote_count":"2","comment_id":"871404","poster":"alce2020","content":"Selected Answer: BCE\nIll go with BCF","timestamp":"1697421240.0"}],"answers_community":["BCE (80%)","10%","10%"],"choices":{"A":"Create a trust relationship that allows users in the member accounts to assume the management account IAM role.","D":"Create an I AM role in each member account to allow the sts:AssumeRole action against the management account IAM role's ARN.","E":"Create an I AM role in the management account that allows the sts:AssumeRole action against the member account IAM role's ARN.","C":"Create an IAM role in each member account that has access to the AmazonEC2ReadOnlyAccess managed policy.","B":"Create a trust relationship that allows users in the management account to assume the IAM roles of the member accounts.","F":"Create an IAM role in the management account that has access to the AmazonEC2ReadOnlyAccess managed policy."},"exam_id":23,"topic":"1","answer":"BCE","timestamp":"2023-04-16 03:54:00"},{"id":"opCMUmwzZdpEgXjrxrL2","answer_ET":"C","url":"https://www.examtopics.com/discussions/amazon/view/105438-exam-aws-certified-devops-engineer-professional-dop-c02/","answer_description":"","answer":"C","topic":"1","timestamp":"2023-04-06 18:30:00","discussion":[{"poster":"4555894","upvote_count":"2","comment_id":"1168822","timestamp":"1725794400.0","content":"Selected Answer: C\nCreate an SQS dead-letter queue. Modify the existing queue by including a re-drive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time."},{"timestamp":"1725723000.0","poster":"zijo","comments":[{"content":"This is not a good approach because it requires unifying the validation logic of the custom application and Lambda function, requires updating both the custom application and Lambda when data specifications change, and requires that the timing of those updates be the same from the SQS perspective, making the deployment process more complex and devops cost expensive. BTW, failed messages are reviewed by scientists, and there is no requirement that they be automatically fix by the program.","timestamp":"1725771780.0","poster":"dzn","upvote_count":"1","comment_id":"1168607"}],"upvote_count":"1","content":"Answer is A. Lambda function is required for automated fixing of the invalid message data and hence A is the right choice here.","comment_id":"1168226"},{"comment_id":"1155459","timestamp":"1724231700.0","upvote_count":"2","content":"Selected Answer: C\nA Dead Letter Queue (DLQ) can be the destination queue for messages that cannot be successfully processed by other queues. DLQs are used to analyze why a message failed or to isolate problem messages.","poster":"dzn"},{"upvote_count":"1","content":"C is correct: Use dead letter queue and config maximum receives is the right way","timestamp":"1722303420.0","poster":"thanhnv142","comment_id":"1135474"},{"poster":"Bans","timestamp":"1720371360.0","content":"definitely C","comment_id":"1116112","upvote_count":"1"},{"timestamp":"1718010180.0","comment_id":"1092440","content":"Selected Answer: C\nThis is DLQ use case. So, its 100% C","upvote_count":"3","poster":"harithzainudin"},{"poster":"SafranboluLokumu","upvote_count":"4","timestamp":"1717237140.0","content":"Selected Answer: C\neveryone votes C but answer seems as A. which one correct? should we trust to voters or examtopic? :D","comment_id":"1085199"},{"timestamp":"1714401480.0","content":"The correct answer is C . This is a use case for Dead Letter Queue","upvote_count":"2","poster":"xhi158","comment_id":"1056979"},{"poster":"bugincloud","comment_id":"1012421","content":"Selected Answer: C\nclassic DLQ usecase","timestamp":"1710952860.0","upvote_count":"1"},{"comment_id":"991404","timestamp":"1709039220.0","content":"Selected Answer: C\nC - DLQ","upvote_count":"1","poster":"Skshitiz"},{"poster":"FEEREWMWKA","content":"C - DLQ","comment_id":"990656","upvote_count":"1","timestamp":"1708952400.0"},{"poster":"andriit","timestamp":"1706368920.0","comment_id":"964756","upvote_count":"1","content":"DevOps is about automation! Variant C says: \"Instruct scientists... \" :D\nSo variant A is the best among other"},{"upvote_count":"4","timestamp":"1705048020.0","poster":"Just_Ninja","comment_id":"949526","content":"Selected Answer: C\nDLQ is the right solution. SQS is one to one! So Lambda make no sense."},{"comment_id":"946350","poster":"habros","upvote_count":"1","timestamp":"1704713220.0","content":"Selected Answer: C\nAlways do with DLQ for failed deliveries. C all the way"},{"content":"Selected Answer: C\nC answer with DLQ is a right solution.","upvote_count":"1","poster":"madperro","timestamp":"1702654020.0","comment_id":"924229"},{"content":"Selected Answer: C\nThe perfect case for a dead-letter queue","timestamp":"1701371040.0","comment_id":"910459","upvote_count":"1","poster":"bcx"},{"upvote_count":"1","content":"Selected Answer: C\nSQS DLQ needed","comment_id":"905518","poster":"Akaza","timestamp":"1700812320.0"},{"upvote_count":"2","content":"sure is C, but the parameter maximum receives looks too small and could create false negatives.\n https://aws.amazon.com/es/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/","comment_id":"899271","timestamp":"1700152140.0","poster":"mgonblan"},{"timestamp":"1698898740.0","content":"Selected Answer: C\nC is it","comment_id":"886969","upvote_count":"1","poster":"haazybanj"},{"comment_id":"870503","upvote_count":"2","content":"C is the correct answer","poster":"alce2020","timestamp":"1697321400.0"},{"content":"Selected Answer: C\nOption C is the correct solution. Creating a dead-letter queue and modifying the queue's redrive policy enables the messages that failed to be moved to the dead-letter queue instead of being discarded. Scientists can then review the data in the dead-letter queue and reprocess it later.\n\nOption A is incorrect because it involves the creation of a separate mechanism to replay failed messages. Although the use of Amazon S3 bucket allows scientists to review and correct the data, it doesn't provide a means of retaining failed messages.","comment_id":"870440","poster":"jqso234","upvote_count":"2","timestamp":"1697312880.0"},{"poster":"ma_rio","timestamp":"1697091780.0","upvote_count":"1","comment_id":"867971","content":"Selected Answer: C\nCreate an Amazon DynamoDB table and configure an SQS dead letter queue to send failed messages to the DynamoDB table."},{"poster":"Dimidrol","comment_id":"863145","timestamp":"1696609800.0","upvote_count":"1","content":"Selected Answer: C\nС for sure"}],"question_id":288,"unix_timestamp":1680798600,"question_text":"A space exploration company receives telemetry data from multiple satellites. Small packets of data are received through Amazon API Gateway and are placed directly into an Amazon Simple Queue Service (Amazon SQS) standard queue. A custom application is subscribed to the queue and transforms the data into a standard format.\nBecause of inconsistencies in the data that the satellites produce, the application is occasionally unable to transform the data. In these cases, the messages remain in the SQS queue. A DevOps engineer must develop a solution that retains the failed messages and makes them available to scientists for review and future processing.\nWhich solution will meet these requirements?","answers_community":["C (100%)"],"exam_id":23,"isMC":true,"question_images":[],"answer_images":[],"choices":{"A":"Configure AWS Lambda to poll the SQS queue and invoke a Lambda function to check whether the queue messages are valid. If validation fails, send a copy of the data that is not valid to an Amazon S3 bucket so that the scientists can review and correct the data. When the data is corrected, amend the message in the SQS queue by using a replay Lambda function with the corrected data.","C":"Create an SQS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time.","B":"Convert the SQS standard queue to an SQS FIFO queue. Configure AWS Lambda to poll the SQS queue every 10 minutes by using an Amazon EventBridge schedule. Invoke the Lambda function to identify any messages with a SentTimestamp value that is older than 5 minutes, push the data to the same location as the application's output location, and remove the messages from the queue.","D":"Configure API Gateway to send messages to different SQS virtual queues that are named for each of the satellites. Update the application to use a new virtual queue for any data that it cannot transform, and send the message to the new virtual queue. Instruct the scientists to use the virtual queue to review the data that is not valid. Reprocess this data at a later time."}},{"id":"9ZBxjU4YiYloLwPkCNmj","answer_ET":"D","url":"https://www.examtopics.com/discussions/amazon/view/106215-exam-aws-certified-devops-engineer-professional-dop-c02/","unix_timestamp":1681510380,"discussion":[{"content":"Selected Answer: D\n100% D.\n\nAWS Service Catalog lets you centrally manage your cloud resources to achieve governance at scale of your infrastructure as code (IaC) templates, written in CloudFormation or Terraform configurations. With AWS Service Catalog, you can meet your compliance requirements while making sure your customers can quickly deploy the cloud resources they need.\nhttps://aws.amazon.com/servicecatalog/\n\nall other service in other answer is not related.","timestamp":"1702206420.0","poster":"harithzainudin","upvote_count":"9","comment_id":"1092442"},{"content":"Service Catalogue is a like a internal marketplace for an organization in that accounts in that organization are limited to using only the resources describe in the product catalogue. For the use case described the best choice is using Service Catalogue.","comment_id":"1267882","upvote_count":"1","timestamp":"1723939140.0","poster":"flaacko"},{"comment_id":"1259250","content":"Selected Answer: D\nkeywords: strict tagging, resource requirements a, limit the deployment\nAWS Service Catalog","poster":"jamesf","timestamp":"1722498780.0","upvote_count":"1"},{"poster":"shammous","comment_id":"1256074","upvote_count":"1","timestamp":"1722059760.0","content":"D would be a better option, especially for developers, to abstract configuring the CloudFormation StackSets when launching applications with diverse versions. In AWS Service Catalog, they would just pick up the version and deploy. Everything would be set for them in the background including the CloudFormation StackSet with the version parameter and the tagging enforcement."},{"upvote_count":"1","comments":[{"content":"\"AWS Service Catalog enables you to launch a product in one or more accounts and AWS Regions. To do this, administrators must apply a stack set constraint to the product with the accounts and Regions, where it can launch as a stack set.\"\nhttps://docs.aws.amazon.com/servicecatalog/latest/userguide/launch-stacksets.html","timestamp":"1717206900.0","poster":"Gomer","comment_id":"1222426","upvote_count":"1"}],"poster":"Gomer","content":"Selected Answer: D\nI'd argue that the correct answer is to use Service Catalog and StackSets. Option \"D:\" doesn't preclude using StackSets, it just doesn't mention it as part of the solution. ServiceCatalog is the formal method to distribute standard solutions (such as CloudFormation StackSets)","timestamp":"1717203600.0","comment_id":"1222404"},{"poster":"stoy123","content":"\"A provisioned Service Catalog product is an AWS CloudFormation stack\"\nReally confusing. I go with D...","upvote_count":"1","comments":[{"poster":"vn_thanhtung","upvote_count":"1","timestamp":"1714637820.0","content":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html Please check topic this , correct answer is C","comment_id":"1205373"}],"timestamp":"1711529700.0","comment_id":"1183949"},{"timestamp":"1709833560.0","poster":"zijo","content":"AWS Service Catalog can be used to deploy resources to two regions (or even more) with the help of AWS CloudFormation StackSets. So Answer is C","comment_id":"1168239","upvote_count":"3"},{"comment_id":"1159617","content":"Answer C\nIf rules are applied across multiple accounts, the StackSets feature is more suitable. The service catalog is used for provisioning new accounts under the AWS control tower.","poster":"Shasha1","upvote_count":"3","timestamp":"1708945080.0"},{"poster":"Cert1Magic2","timestamp":"1708652460.0","comment_id":"1156826","content":"Selected Answer: D\nIt's D","upvote_count":"2"},{"timestamp":"1708515240.0","poster":"dzn","content":"Selected Answer: C\nService Catalog cannot ensures that the same application can be deployed to multiple regions.","comment_id":"1155467","upvote_count":"1"},{"comments":[{"upvote_count":"2","poster":"thanhnv142","comment_id":"1147302","content":"Correction: should be C, not D. The question mentions <deployment to two Regions>. Only stacksets can do this. Even AWS Service Catalog products. It cannot be used cross-region unless it is deployed by stackset","timestamp":"1707663660.0"}],"comment_id":"1135482","upvote_count":"1","poster":"thanhnv142","content":"D is correct: Catalog product impose strict requirements for app deloyment. If using stacksets, devs can deploy app to everywhere without any restrictions","timestamp":"1706586360.0"},{"upvote_count":"1","timestamp":"1705282380.0","content":"To restrict regions or accounts where catalog products can be deployed, refer to AWS Service Catalog Stack Set Constraints (https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-stackset.html),","comment_id":"1122996","poster":"EricFu"},{"poster":"robertohyena","upvote_count":"2","comment_id":"1072458","timestamp":"1700142960.0","content":"Selected Answer: D\nI will go with D.\nReference here:\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/manage-aws-service-catalog-products-in-multiple-aws-accounts-and-aws-regions.html"},{"content":"Selected Answer: D\nWhile stacksets and catalog can use in this case catalog can restrict the strick policy than stacksets. \nThe company has strict tagging and resource requirements.. So it's product catalog. in stacksets developers can modify the resources since they can get hold the template.","timestamp":"1699273140.0","upvote_count":"4","poster":"2pk","comment_id":"1063822"},{"comment_id":"1056338","poster":"rlf","upvote_count":"3","content":"Answer is D.\n\"Developers will need to deploy\" multiple versions of the same application. So service catalog products will be best for developers.","timestamp":"1698511560.0"},{"timestamp":"1697635980.0","comment_id":"1046953","poster":"BaburTurk","upvote_count":"2","content":"Selected Answer: C\nThe correct answer is: C. Create CloudFormation StackSets with approved CloudFormation templates.\n\nCloudFormation StackSets enable you to deploy stacks across multiple accounts and Regions using a single template. This allows you to enforce company policy by only allowing developers to use approved templates.\n\nAWS Service Catalog products can be used to launch approved CloudFormation templates, but they do not enforce the use of approved templates."},{"content":"Selected Answer: C\nThe best solution to ensure that resources are deployed in accordance with company policy is to create CloudFormation StackSets with approved CloudFormation templates.\n\nCloudFormation StackSets allow you to create and manage stacks across multiple AWS accounts and Regions. You can specify the template to use when creating a StackSet, as well as any parameters and capabilities that the template requires.\n\nBy using approved CloudFormation templates, you can ensure that all resources deployed by the StackSet meet your company's tagging and resource requirements. You can also use StackSets to limit the deployment to two Regions.\n\nThe other options are not as effective:","upvote_count":"2","timestamp":"1695369900.0","comment_id":"1013792","poster":"Dushank"},{"poster":"ProfXsamson","timestamp":"1693960620.0","content":"D\nService Catalog allows organizations to centrally manage commonly deployed IT services, and helps organizations achieve consistent governance and meet compliance requirements. End users can quickly deploy only the approved IT services they need, following the constraints set by your organization.\n\nService Catalog provides the following benefit:\n\n----Standardization:: Administer and manage approved assets by restricting where the product can be launched, the type of instance that can be used, and many other configuration options. The result is a standardized landscape for product provisioning for your entire organization.","upvote_count":"4","comment_id":"1000026"},{"timestamp":"1693437420.0","upvote_count":"3","content":"Selected Answer: D\nI think D is the correct. \nD: because of the specific requirements. Service catalogs allow to setup fixed products that will comply with the mandates. It is possible to its portfolios across accounts and regions, so de multi-account requirement would be met as well. (https://aws.amazon.com/blogs/mt/simplify-sharing-your-aws-service-catalog-portfolios-in-an-aws-organizations-setup/)\n\nThe option C: Yet stacksets allow to select the target account and regions, nothing blocks you select otherwise and dont follow the mandates. You can build anything and deploy as stackset","comment_id":"994542","poster":"cocegas"},{"upvote_count":"4","content":"Selected Answer: C\nI think it should be C.\n\"strict tagging and resource requirements\" is covered by the approved CFTs. Stacksets is the only options that covers multi region deployment.","poster":"Radeeka","comment_id":"985565","timestamp":"1692509400.0"},{"timestamp":"1691920440.0","poster":"ixdb","content":"I think C is right.\nOption D involves AWS Service Catalog, which is used for creating and managing a catalog of approved IT services and resources that developers can deploy, but it doesn't inherently address the requirement of deploying across specific regions, and it might be overcomplicating the solution for this specific scenario.","upvote_count":"2","comment_id":"979908"},{"timestamp":"1686835680.0","comment_id":"924231","upvote_count":"3","content":"Selected Answer: D\nD is the right answer.","poster":"madperro"},{"content":"answer is C","poster":"Flyingdagger","comments":[{"content":"However, it would not limit the deployment to two Regions, and it would not prevent developers from deploying unapproved versions of the application.","comment_id":"980011","upvote_count":"3","poster":"Aja1","timestamp":"1691929680.0"}],"timestamp":"1685379720.0","upvote_count":"2","comment_id":"909565"},{"upvote_count":"2","comment_id":"908919","timestamp":"1685315880.0","poster":"youonebe","content":"The correct answer is C."},{"timestamp":"1682994120.0","poster":"haazybanj","content":"Selected Answer: D\nC. Create CloudFormation StackSets with approved CloudFormation templates.\n\nCloudFormation StackSets allows you to create, update, or delete stacks across multiple accounts and Regions with a single CloudFormation template. By creating approved CloudFormation templates and deploying them through StackSets, you can ensure that resources are deployed in accordance with company policy. Additionally, you can limit the deployment to two Regions by specifying the Regions in the StackSet configuration. This approach provides a centralized and automated solution for deploying resources and can help enforce strict tagging and resource requirements.\n\n\nOption D, creating AWS Service Catalog products with approved CloudFormation templates, can help ensure that developers are using pre-approved templates for their deployments. However, it does not address the requirement to limit the deployment to two Regions","comments":[{"timestamp":"1691920320.0","poster":"ixdb","upvote_count":"3","content":"According to you words, it seems you prefer C","comment_id":"979906"}],"upvote_count":"4","comment_id":"886971"},{"timestamp":"1681510380.0","comment_id":"870504","content":"Selected Answer: D\nD is the correct but Stacksets could work too","upvote_count":"3","poster":"alce2020"}],"topic":"1","question_id":289,"exam_id":23,"question_images":[],"answers_community":["D (78%)","C (22%)"],"answer_images":[],"choices":{"C":"Create CloudFormation StackSets with approved CloudFormation templates.","D":"Create AWS Service Catalog products with approved CloudFormation templates.","A":"Create AWS Trusted Advisor checks to find and remediate unapproved CloudFormation StackSets.","B":"Create a Cloud Formation drift detection operation to find and remediate unapproved CloudFormation StackSets."},"timestamp":"2023-04-15 00:13:00","question_text":"A company wants to use AWS CloudFormation for infrastructure deployment. The company has strict tagging and resource requirements and wants to limit the deployment to two Regions. Developers will need to deploy multiple versions of the same application.\nWhich solution ensures resources are deployed in accordance with company policy?","isMC":true,"answer_description":"","answer":"D"},{"id":"VXfmeHaHE2PMB46JYHjx","isMC":true,"question_images":[],"timestamp":"2023-04-05 21:45:00","discussion":[{"content":"Selected Answer: D\nOption D:\nReal-Time Enforcement: Ensures tagging compliance as soon as a volume is created or modified.\nComprehensive Coverage: Captures both CreateVolume and ModifyVolume events.\nMinimal Overhead: Automates tagging without requiring manual audits or remediation actions","timestamp":"1735919640.0","upvote_count":"1","poster":"bb4f13b","comments":[{"poster":"dark4igi","content":"what will happen if dev decide to change tag to dally?\nAWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly","upvote_count":"1","comment_id":"1362296","timestamp":"1740603540.0"}],"comment_id":"1336072"},{"timestamp":"1727165460.0","poster":"lunt","content":"Selected Answer: B\nOnly takes few minutes to login > Config > Managed rulename = BACKUP_PLAN_MIN_FREQUENCY_AND_MIN_RETENTION_CHECK\nA = tags everything in EC2, thats EC2::* which includes ELB/EIP/etc. Nope.\nOption B you can specify the tags to match & expected values = answer.","upvote_count":"3","comment_id":"901454"},{"content":"Selected Answer: B\nA: It works, but it uses a custom rule.\nB: It is simpler than option A as it uses a managed rule which already exists.\nC: It only applies to new volumes and does not address existing resources.\nD: It is better than C but still does not fully meet the requirement to check all EBS volumes and enforce compliance.\nBest Answer is B.","timestamp":"1727165460.0","comment_id":"1274403","poster":"CristianoRosa","upvote_count":"4"},{"content":"Selected Answer: B\nBy leveraging the AWS Config managed rule and automated remediation action, the DevOps engineer can ensure that all EBS volumes in the account always have the required Backup_Frequency tag, enabling the company to perform backups at least weekly unless a different value is explicitly specified. This solution provides continuous monitoring and automated remediation, reducing the risk of human error and ensuring compliance with the company's backup policy.","poster":"c3518fc","upvote_count":"1","timestamp":"1727165460.0","comment_id":"1209014"},{"content":"Selected Answer: B\nOption B --> AWS config managed rule on EC2::Volume resource + custom SSM automation document\nNot Option A --> because it says custom config rule on all EC2::Instance + Managed SSM automation document\nNot options C & D --> As it says cloudtrail which is for logging API actions","upvote_count":"2","comment_id":"1239821","timestamp":"1727165460.0","comments":[{"content":"sorry, a typo.. Option A also says custom SSM automation document, but it is wrong where it says custom config rule on all Ec2::Instance","comment_id":"1239823","timestamp":"1719778740.0","upvote_count":"1","poster":"ajeeshb"}],"poster":"ajeeshb"},{"upvote_count":"1","comment_id":"1149607","poster":"Diego1414","timestamp":"1707860580.0","content":"Answer is A.\nChecks if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instance have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time.\n\nThe AWS-managed AWS Systems Manager automation document AWS-SetRequiredTags does not work as a remediation with this rule. You will need to create your own custom Systems Manager automation documentation for remediation\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/required-tags.html","comments":[{"content":"We don't need to create a custom AWS Config rule, we can utilize the managed rule to detect for non-compliance on the EBS volumes. Otherwise the options indicate to use a custom runbook for AWS Systems Manager to remediate the missing tags.","timestamp":"1708261620.0","comment_id":"1153280","upvote_count":"1","poster":"Hizumi"}]},{"comment_id":"1145264","timestamp":"1707458760.0","content":"B is correct: We should use AWS config for this task \nC and D: cloud trail is for auditing account activities, which is irrelevant\nA: <returns a compliance failure for all Amazon EC2 resources> : we need to remediate EC2 volumes only, not all EC2 resources","poster":"thanhnv142","upvote_count":"4"},{"poster":"Sisanda_giiven","content":"A is the correct answer \"The AWS-managed AWS Systems Manager automation document AWS-SetRequiredTags does not work as a remediation with this rule. You will need to create your own custom Systems Manager automation documentation for remediation.\" from this link : https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html","comment_id":"1131357","timestamp":"1706163540.0","upvote_count":"1"},{"content":"B is the best choice. If you look at Config Managed Rules you can find - ebs-in-backup-plan - Check if Amazon Elastic Block Store (Amazon EBS) volumes are added in backup plans of AWS Backup. The rule is NON_COMPLIANT if Amazon EBS volumes are not included in backup plans.","timestamp":"1701203520.0","comment_id":"1082927","poster":"zijo","upvote_count":"3"},{"comments":[{"timestamp":"1691305500.0","content":"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html","poster":"Aja1","comment_id":"973599","upvote_count":"4"}],"comment_id":"921051","timestamp":"1686530160.0","upvote_count":"3","poster":"SanChan","content":"Selected Answer: B\nB is the most straightforward and efficient solution to ensure that all EBS volumes always have the Backup_Frequency tag applied with the least amount of effort.\n\nA This approach requires more effort than using a managed rule provided by AWS."},{"timestamp":"1686214560.0","content":"Selected Answer: B\nB makes sense, you can use managed rule \"required-tags\" to identify non-compliant volumes and custom SSM document to fix it.","comment_id":"918046","upvote_count":"1","poster":"madperro"},{"comment_id":"888278","content":"Selected Answer: B\nB makes sense","timestamp":"1683100560.0","upvote_count":"1","poster":"vherman"},{"upvote_count":"1","comment_id":"870441","timestamp":"1681501800.0","content":"B. Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.","poster":"alce2020"},{"poster":"ele","content":"Selected Answer: B\nAnswer B: Config has a managed rule for type AWS EC2 Volume for tag compliance check.","comment_id":"863678","timestamp":"1680858600.0","upvote_count":"1"},{"upvote_count":"2","poster":"Dimidrol","comment_id":"862433","timestamp":"1680723900.0","comments":[{"timestamp":"1680723960.0","content":"Sorry A is the answer. This is custom rule","poster":"Dimidrol","upvote_count":"2","comment_id":"862434","comments":[{"poster":"Dimidrol","upvote_count":"1","comment_id":"862436","comments":[{"upvote_count":"4","content":"Option A creates a custom rule that applies to all EC2 resources, not just volumes, which may create additional overhead. The custom AWS Systems Manager Automation runbook is used to apply the Backup_Frequency tag with a value of weekly, but this approach can result in inconsistent tagging if the developers specify a different desired backup frequency. Therefore, Option A is not the correct answer.\n\nOption B is the correct answer because it uses a managed rule specifically for EC2 volumes, which simplifies the configuration effort and ensures that all volumes have the Backup_Frequency tag applied consistently. The custom AWS Systems Manager Automation runbook is used to automatically apply the Backup_Frequency tag with a value of weekly, which reduces the risk of data loss due to missing backups. Your comment that the managed rule should only apply to volumes is correct, and Option B addresses that requirement.","comment_id":"870525","timestamp":"1681514580.0","poster":"jqso234"}],"content":"But very strange that custom rule for all ec2 instances , it should be only ec2 volumes","timestamp":"1680724080.0"}]}],"content":"Selected Answer: B\nB for me. https://aws.amazon.com/ru/blogs/mt/build-an-aws-config-custom-rule-to-optimize-amazon-ebs-volume-types/"}],"topic":"1","choices":{"B":"Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.","C":"Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.","A":"Set up AWS Config in the account. Create a custom rule that returns a compliance failure for all Amazon EC2 resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.","D":"Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events or EBS ModifyVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule."},"answer":"B","unix_timestamp":1680723900,"question_text":"A company requires its developers to tag all Amazon Elastic Block Store (Amazon EBS) volumes in an account to indicate a desired backup frequency. This requirement Includes EBS volumes that do not require backups. The company uses custom tags named Backup_Frequency that have values of none, dally, or weekly that correspond to the desired backup frequency. An audit finds that developers are occasionally not tagging the EBS volumes.\nA DevOps engineer needs to ensure that all EBS volumes always have the Backup_Frequency tag so that the company can perform backups at least weekly unless a different value is specified.\nWhich solution will meet these requirements?","answer_description":"","answers_community":["B (95%)","5%"],"answer_ET":"B","exam_id":23,"question_id":290,"answer_images":[],"url":"https://www.examtopics.com/discussions/amazon/view/105332-exam-aws-certified-devops-engineer-professional-dop-c02/"}],"exam":{"numberOfQuestions":355,"isBeta":false,"id":23,"name":"AWS Certified DevOps Engineer - Professional DOP-C02","lastUpdated":"11 Apr 2025","isImplemented":true,"provider":"Amazon","isMCOnly":true},"currentPage":58},"__N_SSP":true}